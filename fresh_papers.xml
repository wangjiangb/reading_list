<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 11 Oct 2024 01:00:41 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications</title><link>http://arxiv.org/abs/2410.02952v3</link><description>We present a practical distillation approach to fine-tune LLMs for invokingtools in real-time applications. We focus on visual editing tasks;specifically, we modify images and videos by interpreting user stylisticrequests, specified in natural language ("golden hour"), using an LLM to selectthe appropriate tools and their parameters to achieve the desired visualeffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential inthis task, but their high cost and latency make them unsuitable for real-timeapplications. In our approach, we fine-tune a (smaller) student LLM withguidance from a (larger) teacher LLM and behavioral signals. We introduceoffline metrics to evaluate student LLMs. Both online and offline experimentsshow that our student models manage to match the performance of our teachermodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, weshow that fine-tuning was improved by 25% in low-data regimes usingaugmentation.</description><author>Oren Sultan, Alex Khasin, Guy Shiran, Asnat Greenstein-Messica, Dafna Shahaf</author><pubDate>Thu, 10 Oct 2024 11:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02952v3</guid></item><item><title>Context-Aware Command Understanding for Tabletop Scenarios</title><link>http://arxiv.org/abs/2410.06355v2</link><description>This paper presents a novel hybrid algorithm designed to interpret naturalhuman commands in tabletop scenarios. By integrating multiple sources ofinformation, including speech, gestures, and scene context, the system extractsactionable instructions for a robot, identifying relevant objects and actions.The system operates in a zero-shot fashion, without reliance on predefinedobject models, enabling flexible and adaptive use in various environments. Weassess the integration of multiple deep learning models, evaluating theirsuitability for deployment in real-world robotic setups. Our algorithm performsrobustly across different tasks, combining language processing with visualgrounding. In addition, we release a small dataset of video recordings used toevaluate the system. This dataset captures real-world interactions in which ahuman provides instructions in natural language to a robot, a contribution tofuture research on human-robot interaction. We discuss the strengths andlimitations of the system, with particular focus on how it handles multimodalcommand interpretation, and its ability to be integrated into symbolic roboticframeworks for safe and explainable decision-making.</description><author>Paul Gajewski, Antonio Galiza Cerdeira Gonzalez, Bipin Indurkhya</author><pubDate>Thu, 10 Oct 2024 10:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06355v2</guid></item><item><title>Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques</title><link>http://arxiv.org/abs/2410.06719v2</link><description>Diffusion models are powerful generative models, and this capability can alsobe applied to discrimination. The inner activations of a pre-trained diffusionmodel can serve as features for discriminative tasks, namely, diffusionfeature. We discover that diffusion feature has been hindered by a hidden yetuniversal phenomenon that we call content shift. To be specific, there arecontent differences between features and the input image, such as the exactshape of a certain object. We locate the cause of content shift as one inherentcharacteristic of diffusion models, which suggests the broad existence of thisphenomenon in diffusion feature. Further empirical study also indicates thatits negative impact is not negligible even when content shift is not visuallyperceivable. Hence, we propose to suppress content shift to enhance the overallquality of diffusion features. Specifically, content shift is related to theinformation drift during the process of recovering an image from the noisyinput, pointing out the possibility of turning off-the-shelf generationtechniques into tools for content shift suppression. We further propose apractical guideline named GATE to efficiently evaluate the potential benefit ofa technique and provide an implementation of our methodology. Despite thesimplicity, the proposed approach has achieved superior results on varioustasks and datasets, validating its potential as a generic booster for diffusionfeatures. Our code is available athttps://github.com/Darkbblue/diffusion-content-shift.</description><author>Benyuan Meng, Qianqian Xu, Zitai Wang, Zhiyong Yang, Xiaochun Cao, Qingming Huang</author><pubDate>Thu, 10 Oct 2024 10:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06719v2</guid></item><item><title>ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents</title><link>http://arxiv.org/abs/2410.06703v2</link><description>Recent advancements in LLM-based web agents have introduced novelarchitectures and benchmarks showcasing progress in autonomous web navigationand interaction. However, most existing benchmarks prioritize effectiveness andaccuracy, overlooking crucial factors like safety and trustworthiness which areessential for deploying web agents in enterprise settings. The risks of unsafeweb agent behavior, such as accidentally deleting user accounts or performingunintended actions in critical business operations, pose significant barriersto widespread adoption. In this paper, we present ST-WebAgentBench, a newonline benchmark specifically designed to evaluate the safety andtrustworthiness of web agents in enterprise contexts. This benchmark isgrounded in a detailed framework that defines safe and trustworthy (ST) agentbehavior, outlines how ST policies should be structured and introduces theCompletion under Policies metric to assess agent performance. Our evaluationreveals that current SOTA agents struggle with policy adherence and cannot yetbe relied upon for critical business applications. Additionally, we proposearchitectural principles aimed at improving policy awareness and compliance inweb agents. We open-source this benchmark and invite the community tocontribute, with the goal of fostering a new generation of safer, moretrustworthy AI agents. All code, data, environment reproduction resources, andvideo demonstrations are available athttps://sites.google.com/view/st-webagentbench/home.</description><author>Ido Levy, Ben Wiesel, Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov</author><pubDate>Thu, 10 Oct 2024 09:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06703v2</guid></item><item><title>Q-WSL:Leveraging Dynamic Programming for Weighted Supervised Learning in Goal-conditioned RL</title><link>http://arxiv.org/abs/2410.06648v2</link><description>A novel class of advanced algorithms, termed Goal-Conditioned WeightedSupervised Learning (GCWSL), has recently emerged to tackle the challengesposed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSLconsistently delivers strong performance across a diverse set of goal-reachingtasks due to its simplicity, effectiveness, and stability. However, GCWSLmethods lack a crucial capability known as trajectory stitching, which isessential for learning optimal policies when faced with unseen skills duringtesting. This limitation becomes particularly pronounced when the replay bufferis predominantly filled with sub-optimal trajectories. In contrast, traditionalTD-based RL methods, such as Q-learning, which utilize Dynamic Programming, donot face this issue but often experience instability due to the inherentdifficulties in value function approximation. In this paper, we proposeQ-learning Weighted Supervised Learning (Q-WSL), a novel framework designed toovercome the limitations of GCWSL by incorporating the strengths of DynamicProgramming found in Q-learning. Q-WSL leverages Dynamic Programming results tooutput the optimal action of (state, goal) pairs across different trajectorieswithin the replay buffer. This approach synergizes the strengths of bothQ-learning and GCWSL, effectively mitigating their respective weaknesses andenhancing overall performance. Empirical evaluations on challenginggoal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditionedapproaches in terms of both performance and sample efficiency. Additionally,Q-WSL exhibits notable robustness in environments characterized by binaryreward structures and environmental stochasticity.</description><author>Xing Lei, Xuetao Zhang, Zifeng Zhuang, Donglin Wang</author><pubDate>Thu, 10 Oct 2024 09:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06648v2</guid></item><item><title>Gridded Transformer Neural Processes for Large Unstructured Spatio-Temporal Data</title><link>http://arxiv.org/abs/2410.06731v2</link><description>Many important problems require modelling large-scale spatio-temporaldatasets, with one prevalent example being weather forecasting. Recently,transformer-based approaches have shown great promise in a range of weatherforecasting problems. However, these have mostly focused on gridded datasources, neglecting the wealth of unstructured, off-the-grid data fromobservational measurements such as those at weather stations. A promisingfamily of models suitable for such tasks are neural processes (NPs), notablythe family of transformer neural processes (TNPs). Although TNPs have shownpromise on small spatio-temporal datasets, they are unable to scale to thequantities of data used by state-of-the-art weather and climate models. Thislimitation stems from their lack of efficient attention mechanisms. We addressthis shortcoming through the introduction of gridded pseudo-token TNPs whichemploy specialised encoders and decoders to handle unstructured observationsand utilise a processor containing gridded pseudo-tokens that leverageefficient attention mechanisms. Our method consistently outperforms a range ofstrong baselines on various synthetic and real-world regression tasks involvinglarge-scale data, while maintaining competitive computational efficiency. Thereal-life experiments are performed on weather data, demonstrating thepotential of our approach to bring performance and computational benefits whenapplied at scale in a weather modelling pipeline.</description><author>Matthew Ashman, Cristiana Diaconu, Eric Langezaal, Adrian Weller, Richard E. Turner</author><pubDate>Thu, 10 Oct 2024 08:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06731v2</guid></item><item><title>Deep Correlated Prompting for Visual Recognition with Missing Modalities</title><link>http://arxiv.org/abs/2410.06558v2</link><description>Large-scale multimodal models have shown excellent performance over a seriesof tasks powered by the large corpus of paired multimodal training data.Generally, they are always assumed to receive modality-complete inputs.However, this simple assumption may not always hold in the real world due toprivacy constraints or collection difficulty, where models pretrained onmodality-complete data easily demonstrate degraded performance onmissing-modality cases. To handle this issue, we refer to prompt learning toadapt large pretrained multimodal models to handle missing-modality scenariosby regarding different missing cases as different types of input. Instead ofonly prepending independent prompts to the intermediate layers, we present toleverage the correlations between prompts and input features and excavate therelationships between different layers of prompts to carefully design theinstructions. We also incorporate the complementary semantics of differentmodalities to guide the prompting design for each modality. Extensiveexperiments on three commonly-used datasets consistently demonstrate thesuperiority of our method compared to the previous approaches upon differentmissing scenarios. Plentiful ablations are further given to show thegeneralizability and reliability of our method upon different modality-missingratios and types.</description><author>Lianyu Hu, Tongkai Shi, Wei Feng, Fanhua Shang, Liang Wan</author><pubDate>Thu, 10 Oct 2024 08:32:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06558v2</guid></item><item><title>Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators</title><link>http://arxiv.org/abs/2402.12365v4</link><description>Neural operators, serving as physics surrogate models, have recently gainedincreased interest. With ever increasing problem complexity, the naturalquestion arises: what is an efficient way to scale neural operators to largerand more complex simulations - most importantly by taking into accountdifferent types of simulation datasets. This is of special interest since, akinto their numerical counterparts, different techniques are used acrossapplications, even if the underlying dynamics of the systems are similar.Whereas the flexibility of transformers has enabled unified architecturesacross domains, neural operators mostly follow a problem specific design, whereGNNs are commonly used for Lagrangian simulations and grid-based modelspredominate Eulerian simulations. We introduce Universal Physics Transformers(UPTs), an efficient and unified learning paradigm for a wide range ofspatio-temporal problems. UPTs operate without grid- or particle-based latentstructures, enabling flexibility and scalability across meshes and particles.UPTs efficiently propagate dynamics in the latent space, emphasized by inverseencoding and decoding techniques. Finally, UPTs allow for queries of the latentspace representation at any point in space-time. We demonstrate diverseapplicability and efficacy of UPTs in mesh-based fluid simulations, andsteady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-baseddynamics.</description><author>Benedikt Alkin, Andreas Fürst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter</author><pubDate>Thu, 10 Oct 2024 07:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12365v4</guid></item><item><title>Reliable Probabilistic Human Trajectory Prediction for Autonomous Applications</title><link>http://arxiv.org/abs/2410.06905v2</link><description>Autonomous systems, like vehicles or robots, require reliable, accurate,fast, resource-efficient, scalable, and low-latency trajectory predictions toget initial knowledge about future locations and movements of surroundingobjects for safe human-machine interaction. Furthermore, they need to know theuncertainty of the predictions for risk assessment to provide safe pathplanning. This paper presents a lightweight method to address theserequirements, combining Long Short-Term Memory and Mixture Density Networks.Our method predicts probability distributions, including confidence levelestimations for positional uncertainty to support subsequent risk managementapplications and runs on a low-power embedded platform. We discuss essentialrequirements for human trajectory prediction in autonomous vehicle applicationsand demonstrate our method's performance using multiple traffic-relateddatasets. Furthermore, we explain reliability and sharpness metrics and showhow important they are to guarantee the correctness and robustness of a model'spredictions and uncertainty assessments. These essential evaluations have sofar received little attention for no good reason. Our approach focuses entirelyon real-world applicability. Verifying prediction uncertainties and a model'sreliability are central to autonomous real-world applications. Our frameworkand code are available at:https://github.com/kav-institute/mdn_trajectory_forecasting.</description><author>Manuel Hetzel, Hannes Reichert, Konrad Doll, Bernhard Sick</author><pubDate>Thu, 10 Oct 2024 07:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06905v2</guid></item><item><title>DAPE V2: Process Attention Score as Feature Map for Length Extrapolation</title><link>http://arxiv.org/abs/2410.04798v3</link><description>The attention mechanism is a fundamental component of the Transformer model,contributing to interactions among distinct tokens, in contrast to earlierfeed-forward neural networks. In general, the attention scores are determinedsimply by the key-query products. However, this work's occasional trial(combining DAPE and NoPE) of including additional MLPs on attention scoreswithout position encoding indicates that the classical key-query multiplicationmay limit the performance of Transformers. In this work, we conceptualizeattention as a feature map and apply the convolution operator (for neighboringattention scores across different heads) to mimic the processing methods incomputer vision. Specifically, the main contribution of this paper isidentifying and interpreting the Transformer length extrapolation problem as aresult of the limited expressiveness of the naive query and key dot product,and we successfully translate the length extrapolation issue into awell-understood feature map processing problem. The novel insight, which can beadapted to various attention-related models, reveals that the currentTransformer architecture has the potential for further evolution. Extensiveexperiments demonstrate that treating attention as a feature map and applyingconvolution as a processing method significantly enhances Transformerperformance.</description><author>Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li</author><pubDate>Thu, 10 Oct 2024 06:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.04798v3</guid></item><item><title>QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model</title><link>http://arxiv.org/abs/2410.06806v2</link><description>Recent advancements in State Space Models, notably Mamba, have demonstratedsuperior performance over the dominant Transformer models, particularly inreducing the computational complexity from quadratic to linear. Yet,difficulties in adapting Mamba from language to vision tasks arise due to thedistinct characteristics of visual data, such as the spatial locality andadjacency within images and large variations in information granularity acrossvisual tokens. Existing vision Mamba approaches either flatten tokens intosequences in a raster scan fashion, which breaks the local adjacency of images,or manually partition tokens into windows, which limits their long-rangemodeling and generalization capabilities. To address these limitations, wepresent a new vision Mamba model, coined QuadMamba, that effectively captureslocal dependencies of varying granularities via quadtree-based image partitionand scan. Concretely, our lightweight quadtree-based scan module learns topreserve the 2D locality of spatial regions within learned window quadrants.The module estimates the locality score of each token from their features,before adaptively partitioning tokens into window quadrants. An omnidirectionalwindow shifting scheme is also introduced to capture more intact andinformative features across different local regions. To make the discretizedquadtree partition end-to-end trainable, we further devise a sequence maskingstrategy based on Gumbel-Softmax and its straight-through gradient estimator.Extensive experiments demonstrate that QuadMamba achieves state-of-the-artperformance in various vision tasks, including image classification, objectdetection, instance segmentation, and semantic segmentation. The code is inhttps://github.com/VISION-SJTU/QuadMamba.</description><author>Fei Xie, Weijia Zhang, Zhongdao Wang, Chao Ma</author><pubDate>Thu, 10 Oct 2024 06:19:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06806v2</guid></item><item><title>LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs</title><link>http://arxiv.org/abs/2410.06062v2</link><description>We introduce a Retrieval-Augmented Generation (RAG) system for translatinguser questions into accurate federated SPARQL queries over bioinformaticsknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhanceaccuracy and reduce hallucinations in query generation, our system utilisesmetadata from the KGs, including query examples and schema information, andincorporates a validation step to correct generated queries. The system isavailable online at chat.expasy.org.</description><author>Vincent Emonet, Jerven Bolleman, Severine Duvaud, Tarcisio Mendes de Farias, Ana Claudia Sima</author><pubDate>Thu, 10 Oct 2024 06:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06062v2</guid></item><item><title>Image Super-Resolution with Text Prompt Diffusion</title><link>http://arxiv.org/abs/2311.14282v4</link><description>Image super-resolution (SR) methods typically model degradation to improvereconstruction accuracy in complex and unknown degradation scenarios. However,extracting degradation information from low-resolution images is challenging,which limits the model performance. To boost image SR performance, one feasibleapproach is to introduce additional priors. Inspired by advancements inmulti-modal methods and text prompt image processing, we introduce text promptsto image SR to provide degradation priors. Specifically, we first design atext-image generation pipeline to integrate text into the SR dataset throughthe text degradation representation and degradation model. The textrepresentation applies a discretization manner based on the binning method todescribe the degradation abstractly. This method maintains the flexibility ofthe text and is user-friendly. Meanwhile, we propose the PromptSR to realizethe text prompt SR. The PromptSR utilizes the pre-trained language model (e.g.,T5 or CLIP) to enhance restoration. We train the PromptSR on the generatedtext-image dataset. Extensive experiments indicate that introducing textprompts into SR, yields excellent results on both synthetic and real-worldimages. Code is available at: https://github.com/zhengchen1999/PromptSR.</description><author>Zheng Chen, Yulun Zhang, Jinjin Gu, Xin Yuan, Linghe Kong, Guihai Chen, Xiaokang Yang</author><pubDate>Thu, 10 Oct 2024 05:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14282v4</guid></item><item><title>RelitLRM: Generative Relightable Radiance for Large Reconstruction Models</title><link>http://arxiv.org/abs/2410.06231v2</link><description>We propose RelitLRM, a Large Reconstruction Model (LRM) for generatinghigh-quality Gaussian splatting representations of 3D objects under novelilluminations from sparse (4-8) posed images captured under unknown staticlighting. Unlike prior inverse rendering methods requiring dense captures andslow optimization, often causing artifacts like incorrect highlights or shadowbaking, RelitLRM adopts a feed-forward transformer-based model with a novelcombination of a geometry reconstructor and a relightable appearance generatorbased on diffusion. The model is trained end-to-end on synthetic multi-viewrenderings of objects under varying known illuminations. This architecturedesign enables to effectively decompose geometry and appearance, resolve theambiguity between material and lighting, and capture the multi-modaldistribution of shadows and specularity in the relit appearance. We show oursparse-view feed-forward RelitLRM offers competitive relighting results tostate-of-the-art dense-view optimization-based baselines while beingsignificantly faster. Our project page is available at:https://relit-lrm.github.io/.</description><author>Tianyuan Zhang, Zhengfei Kuang, Haian Jin, Zexiang Xu, Sai Bi, Hao Tan, He Zhang, Yiwei Hu, Milos Hasan, William T. Freeman, Kai Zhang, Fujun Luan</author><pubDate>Thu, 10 Oct 2024 05:41:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06231v2</guid></item><item><title>SONAR: A Synthetic AI-Audio Detection Framework and Benchmark</title><link>http://arxiv.org/abs/2410.04324v3</link><description>Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) usinggenerative Artificial Intelligence (AI) technology have made it possible togenerate high-quality and realistic human-like audio. This introducessignificant challenges to distinguishing AI-synthesized speech from theauthentic human voice and could raise potential issues of misuse for maliciouspurposes such as impersonation and fraud, spreading misinformation, deepfakes,and scams. However, existing detection techniques for AI-synthesized audio havenot kept pace and often exhibit poor generalization across diverse datasets. Inthis paper, we introduce SONAR, a synthetic AI-Audio Detection Framework andBenchmark, aiming to provide a comprehensive evaluation for distinguishingcutting-edge AI-synthesized auditory content. SONAR includes a novel evaluationdataset sourced from 9 diverse audio synthesis platforms, including leading TTSproviders and state-of-the-art TTS models. It is the first framework touniformly benchmark AI-audio detection across both traditional and foundationmodel-based deepfake detection systems. Through extensive experiments, wereveal the generalization limitations of existing detection methods anddemonstrate that foundation models exhibit stronger generalizationcapabilities, which can be attributed to their model size and the scale andquality of pretraining data. Additionally, we explore the effectiveness andefficiency of few-shot fine-tuning in improving generalization, highlightingits potential for tailored applications, such as personalized detection systemsfor specific entities or individuals. Code and dataset are available athttps://github.com/Jessegator/SONAR.</description><author>Xiang Li, Pin-Yu Chen, Wenqi Wei</author><pubDate>Thu, 10 Oct 2024 05:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.04324v3</guid></item><item><title>Large Language Models as Code Executors: An Exploratory Study</title><link>http://arxiv.org/abs/2410.06667v2</link><description>The capabilities of Large Language Models (LLMs) have significantly evolved,extending from natural language processing to complex tasks like codeunderstanding and generation. We expand the scope of LLMs' capabilities to abroader context, using LLMs to execute code snippets to obtain the output. Thispaper pioneers the exploration of LLMs as code executors, where code snippetsare directly fed to the models for execution, and outputs are returned. We arethe first to comprehensively examine this feasibility across various LLMs,including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, theo1 model achieved over 90% accuracy in code execution, while othersdemonstrated lower accuracy levels. Furthermore, we introduce an IterativeInstruction Prompting (IIP) technique that processes code snippets line byline, enhancing the accuracy of weaker models by an average of 7.22% (with thehighest improvement of 18.96%) and an absolute average improvement of 3.86%against CoT prompting (with the highest improvement of 19.46%). Our study notonly highlights the transformative potential of LLMs in coding but also laysthe groundwork for future advancements in automated programming and thecompletion of complex tasks.</description><author>Chenyang Lyu, Lecheng Yan, Rui Xing, Wenxi Li, Younes Samih, Tianbo Ji, Longyue Wang</author><pubDate>Thu, 10 Oct 2024 05:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06667v2</guid></item><item><title>Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology</title><link>http://arxiv.org/abs/2410.07087v2</link><description>Developing agents capable of navigating to a target location based onlanguage instructions and visual information, known as vision-languagenavigation (VLN), has attracted widespread interest. Most research has focusedon ground-based agents, while UAV-based VLN remains relatively underexplored.Recent efforts in UAV vision-language navigation predominantly adoptground-based VLN settings, relying on predefined discrete action spaces andneglecting the inherent disparities in agent movement dynamics and thecomplexity of navigation tasks between ground and aerial environments. Toaddress these disparities and challenges, we propose solutions from threeperspectives: platform, benchmark, and methodology. To enable realistic UAVtrajectory simulation in VLN tasks, we propose the OpenUAV platform, whichfeatures diverse environments, realistic flight control, and extensivealgorithmic support. We further construct a target-oriented VLN datasetconsisting of approximately 12k trajectories on this platform, serving as thefirst dataset specifically designed for realistic UAV VLN tasks. To tackle thechallenges posed by complex aerial environments, we propose an assistant-guidedUAV object search benchmark called UAV-Need-Help, which provides varying levelsof guidance information to help UAVs better accomplish realistic VLN tasks. Wealso propose a UAV navigation LLM that, given multi-view images, taskdescriptions, and assistant instructions, leverages the multimodalunderstanding capabilities of the MLLM to jointly process visual and textualinformation, and performs hierarchical trajectory generation. The evaluationresults of our method significantly outperform the baseline models, while thereremains a considerable gap between our results and those achieved by humanoperators, underscoring the challenge presented by the UAV-Need-Help task.</description><author>Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao, Si Liu</author><pubDate>Thu, 10 Oct 2024 05:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07087v2</guid></item><item><title>Task-oriented Time Series Imputation Evaluation via Generalized Representers</title><link>http://arxiv.org/abs/2410.06652v2</link><description>Time series analysis is widely used in many fields such as power energy,economics, and transportation, including different tasks such as forecasting,anomaly detection, classification, etc. Missing values are widely observed inthese tasks, and often leading to unpredictable negative effects on existingmethods, hindering their further application. In response to this situation,existing time series imputation methods mainly focus on restoring sequencesbased on their data characteristics, while ignoring the performance of therestored sequences in downstream tasks. Considering different requirements ofdownstream tasks (e.g., forecasting), this paper proposes an efficientdownstream task-oriented time series imputation evaluation approach. Bycombining time series imputation with neural network models used for downstreamtasks, the gain of different imputation strategies on downstream tasks isestimated without retraining, and the most favorable imputation value fordownstream tasks is given by combining different imputation strategiesaccording to the estimated gain.</description><author>Zhixian Wang, Linxiao Yang, Liang Sun, Qingsong Wen, Yi Wang</author><pubDate>Thu, 10 Oct 2024 04:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06652v2</guid></item><item><title>Grounding Robot Policies with Visuomotor Language Guidance</title><link>http://arxiv.org/abs/2410.06473v2</link><description>Recent advances in the fields of natural language processing and computervision have shown great potential in understanding the underlying dynamics ofthe world from large-scale internet data. However, translating this knowledgeinto robotic systems remains an open challenge, given the scarcity ofhuman-robot interactions and the lack of large-scale datasets of real-worldrobotic data. Previous robot learning approaches such as behavior cloning andreinforcement learning have shown great capabilities in learning robotic skillsfrom human demonstrations or from scratch in specific environments. However,these approaches often require task-specific demonstrations or designingcomplex simulation environments, which limits the development of generalizableand robust policies for new settings. Aiming to address these limitations, wepropose an agent-based framework for grounding robot policies to the currentcontext, considering the constraints of a current robot and its environmentusing visuomotor-grounded language guidance. The proposed framework is composedof a set of conversational agents designed for specific roles -- namely,high-level advisor, visual grounding, monitoring, and robotic agents. Given abase policy, the agents collectively generate guidance at run time to shift theaction distribution of the base policy towards more desirable future states. Wedemonstrate that our approach can effectively guide manipulation policies toachieve significantly higher success rates both in simulation and in real-worldexperiments without the need for additional human demonstrations or extensiveexploration. Project videos at https://sites.google.com/view/motorcortex/home.</description><author>Arthur Bucker, Pablo Ortega-Kral, Jonathan Francis, Jean Oh</author><pubDate>Thu, 10 Oct 2024 04:03:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06473v2</guid></item><item><title>EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models</title><link>http://arxiv.org/abs/2410.07133v2</link><description>Recent advancements in generation models have showcased remarkablecapabilities in generating fantastic content. However, most of them are trainedon proprietary high-quality data, and some models withhold their parameters andonly provide accessible application programming interfaces (APIs), limitingtheir benefits for downstream tasks. To explore the feasibility of training atext-to-image generation model comparable to advanced models using publiclyavailable resources, we introduce EvolveDirector. This framework interacts withadvanced models through their public APIs to obtain text-image data pairs totrain a base model. Our experiments with extensive data indicate that the modeltrained on generated data of the advanced model can approximate its generationcapability. However, it requires large-scale samples of 10 million or more.This incurs significant expenses in time, computational resources, andespecially the costs associated with calling fee-based APIs. To address thisproblem, we leverage pre-trained large vision-language models (VLMs) to guidethe evolution of the base model. VLM continuously evaluates the base modelduring training and dynamically updates and refines the training dataset by thediscrimination, expansion, deletion, and mutation operations. Experimentalresults show that this paradigm significantly reduces the required data volume.Furthermore, when approaching multiple advanced models, EvolveDirector canselect the best samples generated by them to learn powerful and balancedabilities. The final trained model Edgen is demonstrated to outperform theseadvanced models. The code and model weights are available athttps://github.com/showlab/EvolveDirector.</description><author>Rui Zhao, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Zhangjie Wu, Junhao Zhang, Yingya Zhang, Mike Zheng Shou</author><pubDate>Thu, 10 Oct 2024 04:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07133v2</guid></item><item><title>Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models</title><link>http://arxiv.org/abs/2405.14297v3</link><description>The Sparse Mixture of Experts (SMoE) has been widely employed to enhance theefficiency of training and inference for Transformer-based foundational models,yielding promising results. However, the performance of SMoE heavily depends onthe choice of hyper-parameters, such as the number of experts and the number ofexperts to be activated (referred to as top-k), resulting in significantcomputational overhead due to the extensive model training by searching overvarious hyper-parameter configurations. As a remedy, we introduce the DynamicMixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gatingmethod that enables each token to automatically determine the number of expertsto activate. (2) An adaptive process automatically adjusts the number ofexperts during training. Extensive numerical results across Vision, Language,and Vision-Language tasks demonstrate the effectiveness of our approach toachieve competitive performance compared to GMoE for vision and language tasks,and MoE-LLaVA for vision-language tasks, while maintaining efficiency byactivating fewer parameters. Our code is available athttps://github.com/LINs-lab/DynMoE.</description><author>Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, Tao Lin</author><pubDate>Thu, 10 Oct 2024 03:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14297v3</guid></item><item><title>Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</title><link>http://arxiv.org/abs/2410.06535v2</link><description>Constantly discovering novel concepts is crucial in evolving environments.This paper explores the underexplored task of Continual Generalized CategoryDiscovery (C-GCD), which aims to incrementally discover new classes fromunlabeled data while maintaining the ability to recognize previously learnedclasses. Although several settings are proposed to study the C-GCD task, theyhave limitations that do not reflect real-world scenarios. We thus study a morepractical C-GCD setting, which includes more new classes to be discovered overa longer period, without storing samples of past classes. In C-GCD, the modelis initially trained on labeled data of known classes, followed by multipleincremental stages where the model is fed with unlabeled data containing bothold and new classes. The core challenge involves two conflicting objectives:discover new classes and prevent forgetting old ones. We delve into theconflicts and identify that models are susceptible to prediction bias andhardness bias. To address these issues, we introduce a debiased learningframework, namely Happy, characterized by Hardness-aware prototype sampling andsoft entropy regularization. For the prediction bias, we first introduceclustering-guided initialization to provide robust features. In addition, wepropose soft entropy regularization to assign appropriate probabilities to newclasses, which can significantly enhance the clustering performance of newclasses. For the harness bias, we present the hardness-aware prototypesampling, which can effectively reduce the forgetting issue for previously seenclasses, especially for difficult classes. Experimental results demonstrate ourmethod proficiently manages the conflicts of C-GCD and achieves remarkableperformance across various datasets, e.g., 7.5% overall gains on ImageNet-100.Our code is publicly available at https://github.com/mashijie1028/Happy-CGCD.</description><author>Shijie Ma, Fei Zhu, Zhun Zhong, Wenzhuo Liu, Xu-Yao Zhang, Cheng-Lin Liu</author><pubDate>Thu, 10 Oct 2024 03:10:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06535v2</guid></item><item><title>Degree Distribution based Spiking Graph Networks for Domain Adaptation</title><link>http://arxiv.org/abs/2410.06883v2</link><description>Spiking Graph Networks (SGNs) have garnered significant attraction from bothresearchers and industry due to their ability to address energy consumptionchallenges in graph classification. However, SGNs are only effective forin-distribution data and cannot tackle out-of-distribution data. In this paper,we first propose the domain adaptation problem in SGNs, and introduce a novelframework named Degree-aware Spiking Graph Domain Adaptation forClassification. The proposed DeSGDA addresses the spiking graph domainadaptation problem by three aspects: node degree-aware personalized spikingrepresentation, adversarial feature distribution alignment, and pseudo-labeldistillation. First, we introduce the personalized spiking representationmethod for generating degree-dependent spiking signals. Specifically, thethreshold of triggering a spike is determined by the node degree, allowing thispersonalized approach to capture more expressive information forclassification. Then, we propose the graph feature distribution alignmentmodule that is adversarially trained using membrane potential against a domaindiscriminator. Such an alignment module can efficiently maintain highperformance and low energy consumption in the case of inconsistentdistribution. Additionally, we extract consistent predictions across two spacesto create reliable pseudo-labels, effectively leveraging unlabeled data toenhance graph classification performance. Extensive experiments on benchmarkdatasets validate the superiority of the proposed DeSGDA compared withcompetitive baselines.</description><author>Yingxu Wang, Siwei Liu, Mengzhu Wang, Shangsong Liang, Nan Yin</author><pubDate>Thu, 10 Oct 2024 02:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06883v2</guid></item><item><title>HBTP: Heuristic Behavior Tree Planning with Large Language Model Reasoning</title><link>http://arxiv.org/abs/2406.00965v4</link><description>Behavior Trees (BTs) are increasingly becoming a popular control structure inrobotics due to their modularity, reactivity, and robustness. In terms of BTgeneration methods, BT planning shows promise for generating reliable BTs.However, the scalability of BT planning is often constrained by prolongedplanning times in complex scenarios, largely due to a lack of domain knowledge.In contrast, pre-trained Large Language Models (LLMs) have demonstrated taskreasoning capabilities across various domains, though the correctness andsafety of their planning remain uncertain. This paper proposes integrating BTplanning with LLM reasoning, introducing Heuristic Behavior Tree Planning(HBTP)-a reliable and efficient framework for BT generation. The key idea inHBTP is to leverage LLMs for task-specific reasoning to generate a heuristicpath, which BT planning can then follow to expand efficiently. We firstintroduce the heuristic BT expansion process, along with two heuristic variantsdesigned for optimal planning and satisficing planning, respectively. Then, wepropose methods to address the inaccuracies of LLM reasoning, including actionspace pruning and reflective feedback, to further enhance both reasoningaccuracy and planning efficiency. Experiments demonstrate the theoreticalbounds of HBTP, and results from four datasets confirm its practicaleffectiveness in everyday service robot applications.</description><author>Yishuai Cai, Xinglin Chen, Yunxin Mao, Minglong Li, Shaowu Yang, Wenjing Yang, Ji Wang</author><pubDate>Thu, 10 Oct 2024 02:36:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00965v4</guid></item><item><title>IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers</title><link>http://arxiv.org/abs/2410.02592v3</link><description>Recently, in-car monitoring has emerged as a promising technology fordetecting early-stage abnormal status of the driver and providing timely alertsto prevent traffic accidents. Although training models with multimodal dataenhances the reliability of abnormal status detection, the scarcity of labeleddata and the imbalance of class distribution impede the extraction of criticalabnormal state features, significantly deteriorating training performance.Furthermore, missing modalities due to environment and hardware limitationsfurther exacerbate the challenge of abnormal status identification. Moreimportantly, monitoring abnormal health conditions of passengers, particularlyin elderly care, is of paramount importance but remains underexplored. Toaddress these challenges, we introduce our IC3M, an efficientcamera-rotation-based multimodal framework for monitoring both driver andpassengers in a car. Our IC3M comprises two key modules: an adaptive thresholdpseudo-labeling strategy and a missing modality reconstruction. The formercustomizes pseudo-labeling thresholds for different classes based on the classdistribution, generating class-balanced pseudo labels to guide model trainingeffectively, while the latter leverages crossmodality relationships learnedfrom limited labels to accurately recover missing modalities by distributiontransferring from available modalities. Extensive experimental resultsdemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,precision, and recall while exhibiting superior robustness under limitedlabeled data and severe missing modality.</description><author>Zihan Fang, Zheng Lin, Senkang Hu, Hangcheng Cao, Yiqin Deng, Xianhao Chen, Yuguang Fang</author><pubDate>Thu, 10 Oct 2024 02:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02592v3</guid></item><item><title>MM-Ego: Towards Building Egocentric Multimodal LLMs</title><link>http://arxiv.org/abs/2410.07177v1</link><description>This research aims to comprehensively explore building a multimodalfoundation model for egocentric video understanding. To achieve this goal, wework on three fronts. First, as there is a lack of QA data for egocentric videounderstanding, we develop a data engine that efficiently generates 7Mhigh-quality QA samples for egocentric videos ranging from 30 seconds to onehour long, based on human-annotated data. This is currently the largestegocentric QA dataset. Second, we contribute a challenging egocentric QAbenchmark with 629 videos and 7,026 questions to evaluate the models' abilityin recognizing and memorizing visual details across videos of varying lengths.We introduce a new de-biasing evaluation method to help mitigate theunavoidable language bias present in the models being evaluated. Third, wepropose a specialized multimodal architecture featuring a novel "Memory PointerPrompting" mechanism. This design includes a global glimpse step to gain anoverarching understanding of the entire video and identify key visualinformation, followed by a fallback step that utilizes the key visualinformation to generate responses. This enables the model to more effectivelycomprehend extended video content. With the data, benchmark, and model, wesuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerfulperformance on egocentric video understanding.</description><author>Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang</author><pubDate>Wed, 09 Oct 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07177v1</guid></item><item><title>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title><link>http://arxiv.org/abs/2410.07176v1</link><description>Retrieval-Augmented Generation (RAG), while effective in integrating externalknowledge to address the limitations of large language models (LLMs), can beundermined by imperfect retrieval, which may introduce irrelevant, misleading,or even malicious information. Despite its importance, previous studies haverarely explored the behavior of RAG through joint analysis on how errors fromimperfect retrieval attribute and propagate, and how potential conflicts arisebetween the LLMs' internal knowledge and external sources. We find thatimperfect retrieval augmentation might be inevitable and quite harmful, throughcontrolled analysis under realistic conditions. We identify the knowledgeconflicts between LLM-internal and external knowledge from retrieval as abottleneck to overcome in the post-retrieval stage of RAG. To render LLMsresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approachthat adaptively elicits essential information from LLMs' internal knowledge,iteratively consolidates internal and external knowledge with source-awareness,and finalizes the answer according to information reliability. Our experimentsusing Gemini and Claude demonstrate that Astute RAG significantly outperformsprevious robustness-enhanced RAG methods. Notably, Astute RAG is the onlyapproach that matches or exceeds the performance of LLMs without RAG underworst-case scenarios. Further analysis reveals that Astute RAG effectivelyresolves knowledge conflicts, improving the reliability and trustworthiness ofRAG systems.</description><author>Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık</author><pubDate>Wed, 09 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07176v1</guid></item><item><title>Neural Circuit Architectural Priors for Quadruped Locomotion</title><link>http://arxiv.org/abs/2410.07174v1</link><description>Learning-based approaches to quadruped locomotion commonly adopt genericpolicy architectures like fully connected MLPs. As such architectures containfew inductive biases, it is common in practice to incorporate priors in theform of rewards, training curricula, imitation data, or trajectory generators.In nature, animals are born with priors in the form of their nervous system'sarchitecture, which has been shaped by evolution to confer innate ability andefficient learning. For instance, a horse can walk within hours of birth andcan quickly improve with practice. Such architectural priors can also be usefulin ANN architectures for AI. In this work, we explore the advantages of abiologically inspired ANN architecture for quadruped locomotion based on neuralcircuits in the limbs and spinal cord of mammals. Our architecture achievesgood initial performance and comparable final performance to MLPs, while usingless data and orders of magnitude fewer parameters. Our architecture alsoexhibits better generalization to task variations, even admitting deployment ona physical robot without standard sim-to-real methods. This work shows thatneural circuits can provide valuable architectural priors for locomotion andencourages future work in other sensorimotor skills.</description><author>Nikhil X. Bhattasali, Venkatesh Pattabiraman, Lerrel Pinto, Grace W. Lindsay</author><pubDate>Wed, 09 Oct 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07174v1</guid></item><item><title>Do better language models have crisper vision?</title><link>http://arxiv.org/abs/2410.07173v1</link><description>How well do text-only Large Language Models (LLMs) grasp the visual world? AsLLMs are increasingly used in computer vision, addressing this question becomesboth fundamental and pertinent. However, existing studies have primarilyfocused on limited scenarios, such as their ability to generate visual contentor cluster multimodal data. To this end, we propose the Visual TextRepresentation Benchmark (ViTeRB) to isolate key properties that make languagemodels well-aligned with the visual world. With this, we identify large-scaledecoder-based LLMs as ideal candidates for representing text in vision-centriccontexts, counter to the current practice of utilizing text encoders. Buildingon these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.By leveraging precomputable frozen features from strong vision and languagemodels, ShareLock achieves an impressive 51% accuracy on ImageNet despiteutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPUhour (or 10 hours including the precomputation of features) - orders ofmagnitude less than prior methods. Code will be released.</description><author>Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano</author><pubDate>Wed, 09 Oct 2024 17:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07173v1</guid></item><item><title>Glider: Global and Local Instruction-Driven Expert Router</title><link>http://arxiv.org/abs/2410.07172v1</link><description>The availability of performant pre-trained models has led to a proliferationof fine-tuned expert models that are specialized to particular domains. Thishas enabled the creation of powerful and adaptive routing-based "ModelMoErging" methods with the goal of using expert modules to create an aggregatesystem with improved performance or generalization. However, existing MoErgingmethods often prioritize generalization to unseen tasks at the expense ofperformance on held-in tasks, which limits its practical applicability inreal-world deployment scenarios. We observe that current token-level routingmechanisms neglect the global semantic context of the input task. Thistoken-wise independence hinders effective expert selection for held-in tasks,as routing decisions fail to incorporate the semantic properties of the task.To address this, we propose, Global and Local Instruction Driven Expert Router(GLIDER) that integrates a multi-scale routing mechanism, encompassing asemantic global router and a learned local router. The global router leveragesLLM's advanced reasoning capabilities for semantic-related contexts to enhanceexpert selection. Given the input query and LLM, the router generates semantictask instructions that guide the retrieval of the most relevant experts acrossall layers. This global guidance is complemented by a local router thatfacilitates token-level routing decisions within each module, enabling finercontrol and enhanced performance on unseen tasks. Our experiments usingT5-based models for T0 and FLAN tasks demonstrate that GLIDER achievessubstantially improved held-in performance while maintaining stronggeneralization on held-out tasks. We also perform ablations experiments to divedeeper into the components of GLIDER. Our experiments highlight the importanceof our multi-scale routing that leverages LLM-driven semantic reasoning forMoErging methods.</description><author>Pingzhi Li, Prateek Yadav, Jaehong Yoon, Jie Peng, Yi-Lin Sung, Mohit Bansal, Tianlong Chen</author><pubDate>Wed, 09 Oct 2024 17:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07172v1</guid></item><item><title>IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</title><link>http://arxiv.org/abs/2410.07171v1</link><description>Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have madenotable strides in compositional text-to-image generation. However, thesemethods typically exhibit distinct strengths for compositional generation, withsome excelling in handling attribute binding and others in spatialrelationships. This disparity highlights the need for an approach that canleverage the complementary strengths of various models to comprehensivelyimprove the composition capability. To this end, we introduce IterComp, a novelframework that aggregates composition-aware model preferences from multiplemodels and employs an iterative feedback learning approach to enhancecompositional generation. Specifically, we curate a gallery of six powerfulopen-source diffusion models and evaluate their three key compositionalmetrics: attribute binding, spatial relationships, and non-spatialrelationships. Based on these metrics, we develop a composition-aware modelpreference dataset comprising numerous image-rank pairs to traincomposition-aware reward models. Then, we propose an iterative feedbacklearning method to enhance compositionality in a closed-loop manner, enablingthe progressive self-refinement of both the base diffusion model and rewardmodels over multiple iterations. Theoretical proof demonstrates theeffectiveness and extensive experiments show our significant superiority overprevious SOTA methods (e.g., Omost and FLUX), particularly in multi-categoryobject composition and complex semantic alignment. IterComp opens new researchavenues in reward feedback learning for diffusion models and compositionalgeneration. Code: https://github.com/YangLing0818/IterComp</description><author>Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui</author><pubDate>Wed, 09 Oct 2024 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07171v1</guid></item><item><title>One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation</title><link>http://arxiv.org/abs/2410.07170v1</link><description>Foundation models (FMs) are pre-trained on large-scale datasets and thenfine-tuned on a downstream task for a specific application. The most successfuland most commonly used fine-tuning method is to update the pre-trained weightsvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that areusually initialized at random with a uniform rank distribution across modelweights. Recent works focus on weight-driven initialization or learning ofadaptive ranks during training. Both approaches have only been investigated inisolation, resulting in slow convergence or a uniform rank distribution, inturn leading to sub-optimal performance. We propose to enhance LoRA byinitializing the new weights in a data-driven manner by computing singularvalue decomposition on minibatches of activation vectors. Then, we initializethe LoRA matrices with the obtained right-singular vectors and re-distributeranks among all weight matrices to explain the maximal amount of variance andcontinue the standard LoRA fine-tuning procedure. This results in our newmethod Explained Variance Adaptation (EVA). We apply EVA to a variety offine-tuning tasks ranging from language generation and understanding to imageclassification and reinforcement learning. EVA exhibits faster convergence thancompetitors and attains the highest average score across a multitude of tasksper domain.</description><author>Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, Sepp Hochreiter</author><pubDate>Wed, 09 Oct 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07170v1</guid></item><item><title>Sylber: Syllabic Embedding Representation of Speech from Raw Audio</title><link>http://arxiv.org/abs/2410.07168v1</link><description>Syllables are compositional units of spoken language that play a crucial rolein human speech perception and production. However, current neural speechrepresentations lack structure, resulting in dense token sequences that arecostly to process. To bridge this gap, we propose a new model, Sylber, thatproduces speech representations with clean and robust syllabic structure.Specifically, we propose a self-supervised model that regresses features onsyllabic segments distilled from a teacher model which is an exponential movingaverage of the model in training. This results in a highly structuredrepresentation of speech features, offering three key benefits: 1) a fast,linear-time syllable segmentation algorithm, 2) efficient syllabic tokenizationwith an average of 4.27 tokens per second, and 3) syllabic units better suitedfor lexical and syntactic understanding. We also train token-to-speechgenerative models with our syllabic units and show that fully intelligiblespeech can be reconstructed from these tokens. Lastly, we observe thatcategorical perception, a linguistic phenomenon of speech perception, emergesnaturally in our model, making the embedding space more categorical and sparsethan previous self-supervised learning approaches. Together, we present a novelself-supervised approach for representing speech as syllables, with significantpotential for efficient speech tokenization and spoken language modeling.</description><author>Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli</author><pubDate>Wed, 09 Oct 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07168v1</guid></item><item><title>Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate</title><link>http://arxiv.org/abs/2410.07167v1</link><description>We present the Modality Integration Rate (MIR), an effective, robust, andgeneralized metric to indicate the multi-modal pre-training quality of LargeVision Language Models (LVLMs). Large-scale pre-training plays a critical rolein building capable LVLMs, while evaluating its training quality without thecostly supervised fine-tuning stage is under-explored. Loss, perplexity, andin-context evaluation results are commonly used pre-training metrics for LargeLanguage Models (LLMs), while we observed that these metrics are lessindicative when aligning a well-trained LLM with a new modality. Due to thelack of proper metrics, the research of LVLMs in the critical pre-trainingstage is hindered greatly, including the training data choice, efficient moduledesign, etc. In this paper, we propose evaluating the pre-training quality fromthe inter-modal distribution distance perspective and present MIR, the ModalityIntegration Rate, which is 1) \textbf{Effective} to represent the pre-trainingquality and show a positive relation with the benchmark performance aftersupervised fine-tuning. 2) \textbf{Robust} toward different training/evaluationdata. 3) \textbf{Generalize} across training configurations and architecturechoices. We conduct a series of pre-training experiments to explore theeffectiveness of MIR and observe satisfactory results that MIR is indicativeabout training data selection, training strategy schedule, and modelarchitecture design to get better pre-training results. We hope MIR could be ahelpful metric for building capable LVLMs and inspire the following researchabout modality alignment in different areas. Our code is at:https://github.com/shikiw/Modality-Integration-Rate.</description><author>Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</author><pubDate>Wed, 09 Oct 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07167v1</guid></item><item><title>Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</title><link>http://arxiv.org/abs/2410.07166v1</link><description>We aim to evaluate Large Language Models (LLMs) for embodied decision making.While a significant body of work has been leveraging LLMs for decision makingin embodied environments, we still lack a systematic understanding of theirperformance because they are usually applied in different domains, fordifferent purposes, and built based on different inputs and outputs.Furthermore, existing evaluations tend to rely solely on a final success rate,making it difficult to pinpoint what ability is missing in LLMs and where theproblem lies, which in turn blocks embodied agents from leveraging LLMseffectively and selectively. To address these limitations, we propose ageneralized interface (Embodied Agent Interface) that supports theformalization of various types of tasks and input-output specifications ofLLM-based modules. Specifically, it allows us to unify 1) a broad set ofembodied decision-making tasks involving both state and temporally extendedgoals, 2) four commonly-used LLM-based modules for decision making: goalinterpretation, subgoal decomposition, action sequencing, and transitionmodeling, and 3) a collection of fine-grained metrics which break downevaluation into various types of errors, such as hallucination errors,affordance errors, various types of planning errors, etc. Overall, ourbenchmark offers a comprehensive assessment of LLMs' performance for differentsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AIsystems, and providing insights for effective and selective use of LLMs inembodied decision making.</description><author>Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu</author><pubDate>Wed, 09 Oct 2024 17:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07166v1</guid></item><item><title>A neural network-based approach to hybrid systems identification for control</title><link>http://arxiv.org/abs/2404.01814v2</link><description>We consider the problem of designing a machine learning-based model of anunknown dynamical system from a finite number of (state-input)-successor statedata points, such that the model obtained is also suitable for optimal controldesign. We adopt a neural network (NN) architecture that, once suitablytrained, yields a hybrid system with continuous piecewise-affine (PWA) dynamicsthat is differentiable with respect to the network's parameters, therebyenabling the use of derivative-based training procedures. We show that acareful choice of our NN's weights produces a hybrid system model withstructural properties that are highly favorable when used as part of a finitehorizon optimal control problem (OCP). Specifically, we rely on availableresults to establish that optimal solutions with strong local optimalityguarantees can be computed via nonlinear programming (NLP), in contrast toclassical OCPs for general hybrid systems which typically require mixed-integeroptimization. Besides being well-suited for optimal control design, numericalsimulations illustrate that our NN-based technique enjoys very similarperformance to state-of-the-art system identification methods for hybridsystems and it is competitive on nonlinear benchmarks.</description><author>Filippo Fabiani, Bartolomeo Stellato, Daniele Masti, Paul J. Goulart</author><pubDate>Wed, 09 Oct 2024 17:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01814v2</guid></item><item><title>AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation</title><link>http://arxiv.org/abs/2410.07164v1</link><description>Recent advancements in diffusion models have led to significant improvementsin the generation and animation of 4D full-body human-object interactions(HOI). Nevertheless, existing methods primarily focus on SMPL-based motiongeneration, which is limited by the scarcity of realistic large-scaleinteraction data. This constraint affects their ability to create everyday HOIscenes. This paper addresses this challenge using a zero-shot approach with apre-trained diffusion model. Despite this potential, achieving our goals isdifficult due to the diffusion model's lack of understanding of ''where'' and''how'' objects interact with the human body. To tackle these issues, weintroduce AvatarGO, a novel framework designed to generate animatable 4D HOIscenes directly from textual inputs. Specifically, 1) for the ''where''challenge, we propose LLM-guided contact retargeting, which employs Lang-SAM toidentify the contact body part from text prompts, ensuring preciserepresentation of human-object spatial relations. 2) For the ''how'' challenge,we introduce correspondence-aware motion optimization that constructs motionfields for both human and object models using the linear blend skinningfunction from SMPL-X. Our framework not only generates coherent compositionalmotions, but also exhibits greater robustness in handling penetration issues.Extensive experiments with existing methods validate AvatarGO's superiorgeneration and animation capabilities on a variety of human-object pairs anddiverse poses. As the first attempt to synthesize 4D avatars with objectinteractions, we hope AvatarGO could open new doors for human-centric 4Dcontent creation.</description><author>Yukang Cao, Liang Pan, Kai Han, Kwan-Yee K. Wong, Ziwei Liu</author><pubDate>Wed, 09 Oct 2024 17:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07164v1</guid></item><item><title>Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning</title><link>http://arxiv.org/abs/2410.07163v1</link><description>In this work, we address the problem of large language model (LLM)unlearning, aiming to remove unwanted data influences and associated modelcapabilities (e.g., copyrighted data or harmful content generation) whilepreserving essential model utilities, without the need for retraining fromscratch. Despite the growing need for LLM unlearning, a principled optimizationframework remains lacking. To this end, we revisit the state-of-the-artapproach, negative preference optimization (NPO), and identify the issue ofreference model bias, which could undermine NPO's effectiveness, particularlywhen unlearning forget data of varying difficulty. Given that, we propose asimple yet effective unlearning optimization framework, called SimNPO, showingthat 'simplicity' in removing the reliance on a reference model (through thelens of simple preference optimization) benefits unlearning. We also providedeeper insights into SimNPO's advantages, supported by analysis using mixturesof Markov chains. Furthermore, we present extensive experiments validatingSimNPO's superiority over existing unlearning baselines in benchmarks like TOFUand MUSE, and robustness against relearning attacks. Codes are available athttps://github.com/OPTML-Group/Unlearn-Simple.</description><author>Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, Sijia Liu</author><pubDate>Wed, 09 Oct 2024 17:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07163v1</guid></item><item><title>Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy</title><link>http://arxiv.org/abs/2407.06813v2</link><description>Diplomacy is one of the most sophisticated activities in human society. Thecomplex interactions among multiple parties/ agents involve various abilitieslike social reasoning, negotiation arts, and long-term strategy planning.Previous AI agents surely have proved their capability of handling multi-stepgames and larger action spaces on tasks involving multiple agents. However,diplomacy involves a staggering magnitude of decision spaces, especiallyconsidering the negotiation stage required. Recently, LLM agents have showntheir potential for extending the boundary of previous agents on a couple ofapplications, however, it is still not enough to handle a very long planningperiod in a complex multi-agent environment. Empowered with cutting-edge LLMtechnology, we make the first stab to explore AI's upper bound towards ahuman-like agent for such a highly comprehensive multi-agent mission bycombining three core and essential capabilities for stronger LLM-based societalagents: 1) strategic planner with memory and reflection; 2) goal-orientednegotiate with social reasoning; 3) augmenting memory by self-play games toself-evolving without any human in the loop.</description><author>Zhenyu Guan, Xiangyu Kong, Fangwei Zhong, Yizhou Wang</author><pubDate>Wed, 09 Oct 2024 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06813v2</guid></item><item><title>Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond</title><link>http://arxiv.org/abs/2410.07158v1</link><description>In recent years, training data attribution (TDA) methods have emerged as apromising direction for the interpretability of neural networks. While researcharound TDA is thriving, limited effort has been dedicated to the evaluation ofattributions. Similar to the development of evaluation metrics for traditionalfeature attribution approaches, several standalone metrics have been proposedto evaluate the quality of TDA methods across various contexts. However, thelack of a unified framework that allows for systematic comparison limits trustin TDA methods and stunts their widespread adoption. To address this researchgap, we introduce Quanda, a Python toolkit designed to facilitate theevaluation of TDA methods. Beyond offering a comprehensive set of evaluationmetrics, Quanda provides a uniform interface for seamless integration withexisting TDA implementations across different repositories, thus enablingsystematic benchmarking. The toolkit is user-friendly, thoroughly tested,well-documented, and available as an open-source library on PyPi and underhttps://github.com/dilyabareeva/quanda.</description><author>Dilyara Bareeva, Galip Ümit Yolcu, Anna Hedström, Niklas Schmolenski, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</author><pubDate>Wed, 09 Oct 2024 17:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07158v1</guid></item><item><title>InstructG2I: Synthesizing Images from Multimodal Attributed Graphs</title><link>http://arxiv.org/abs/2410.07157v1</link><description>In this paper, we approach an overlooked yet critical task Graph2Image:generating images from multimodal attributed graphs (MMAGs). This task posessignificant challenges due to the explosion in graph size, dependencies amonggraph entities, and the need for controllability in graph conditions. Toaddress these challenges, we propose a graph context-conditioned diffusionmodel called InstructG2I. InstructG2I first exploits the graph structure andmultimodal information to conduct informative neighbor sampling by combiningpersonalized page rank and re-ranking based on vision-language features. Then,a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliaryset of graph prompts to guide the denoising process of diffusion. Finally, wepropose graph classifier-free guidance, enabling controllable generation byvarying the strength of graph guidance and multiple connected edges to a node.Extensive experiments conducted on three datasets from different domainsdemonstrate the effectiveness and controllability of our approach. The code isavailable at https://github.com/PeterGriffinJin/InstructG2I.</description><author>Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han</author><pubDate>Wed, 09 Oct 2024 17:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07157v1</guid></item><item><title>Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis</title><link>http://arxiv.org/abs/2410.07155v1</link><description>Recent advances in diffusion models have demonstrated exceptionalcapabilities in image and video generation, further improving the effectivenessof 4D synthesis. Existing 4D generation methods can generate high-quality 4Dobjects or scenes based on user-friendly conditions, benefiting the gaming andvideo industries. However, these methods struggle to synthesize significantobject deformation of complex 4D transitions and interactions within scenes. Toaddress this challenge, we propose Trans4D, a novel text-to-4D synthesisframework that enables realistic complex scene transitions. Specifically, wefirst use multi-modal large language models (MLLMs) to produce a physic-awarescene description for 4D scene initialization and effective transition timingplanning. Then we propose a geometry-aware 4D transition network to realize acomplex scene-level 4D transition based on the plan, which involves expressivegeometrical object deformation. Extensive experiments demonstrate that Trans4Dconsistently outperforms existing state-of-the-art methods in generating 4Dscenes with accurate and high-quality transitions, validating itseffectiveness. Code: https://github.com/YangLing0818/Trans4D</description><author>Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, Stefano Ermon, Wentao Zhang</author><pubDate>Wed, 09 Oct 2024 17:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07155v1</guid></item><item><title>CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition</title><link>http://arxiv.org/abs/2410.07153v1</link><description>Skeleton-based multi-entity action recognition is a challenging task aimingto identify interactive actions or group activities involving multiple diverseentities. Existing models for individuals often fall short in this task due tothe inherent distribution discrepancies among entity skeletons, leading tosuboptimal backbone optimization. To this end, we introduce a Convex HullAdaptive Shift based multi-Entity action recognition method (CHASE), whichmitigates inter-entity distribution gaps and unbiases subsequent backbones.Specifically, CHASE comprises a learnable parameterized network and anauxiliary objective. The parameterized network achieves plausible,sample-adaptive repositioning of skeleton sequences through two key components.First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the neworigin of the coordinate system is within the skeleton convex hull. Second, theCoefficient Learning Block provides a lightweight parameterization of themapping from skeleton sequences to their specific coefficients in convexcombinations. Moreover, to guide the optimization of this network fordiscrepancy minimization, we propose the Mini-batch Pair-wise Maximum MeanDiscrepancy as the additional objective. CHASE operates as a sample-adaptivenormalization method to mitigate inter-entity distribution discrepancies,thereby reducing data bias and improving the subsequent classifier'smulti-entity action recognition performance. Extensive experiments on sixdatasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity andVolleyball, consistently verify our approach by seamlessly adapting tosingle-entity backbones and boosting their performance in multi-entityscenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .</description><author>Yuhang Wen, Mengyuan Liu, Songtao Wu, Beichen Ding</author><pubDate>Wed, 09 Oct 2024 17:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07153v1</guid></item><item><title>Towards Interpreting Visual Information Processing in Vision-Language Models</title><link>http://arxiv.org/abs/2410.07149v1</link><description>Vision-Language Models (VLMs) are powerful tools for processing andunderstanding text and images. We study the processing of visual tokens in thelanguage model component of LLaVA, a prominent VLM. Our approach focuses onanalyzing the localization of object information, the evolution of visual tokenrepresentations across layers, and the mechanism of integrating visualinformation for predictions. Through ablation studies, we demonstrated thatobject identification accuracy drops by over 70\% when object-specific tokensare removed. We observed that visual token representations become increasinglyinterpretable in the vocabulary space across layers, suggesting an alignmentwith textual tokens corresponding to image content. Finally, we found that themodel extracts object information from these refined representations at thelast token position for prediction, mirroring the process in text-only languagemodels for factual association tasks. These findings provide crucial insightsinto how VLMs process and integrate visual information, bridging the gapbetween our understanding of language and vision models, and paving the way formore interpretable and controllable multimodal systems.</description><author>Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez</author><pubDate>Wed, 09 Oct 2024 17:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07149v1</guid></item><item><title>Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy</title><link>http://arxiv.org/abs/2410.07147v1</link><description>Mental-health therapy involves a complex conversation flow in which patientsand therapists continuously negotiate what should be talked about next. Forexample, therapists might try to shift the conversation's direction to keep thetherapeutic process on track and avoid stagnation, or patients might push thediscussion towards issues they want to focus on. How do such patient and therapist redirections relate to the development andquality of their relationship? To answer this question, we introduce aprobabilistic measure of the extent to which a certain utterance immediatelyredirects the flow of the conversation, accounting for both the intention andthe actual realization of such a change. We apply this new measure tocharacterize the development of patient-therapist relationships over multiplesessions in a very large, widely-used online therapy platform. Our analysisreveals that (1) patient control of the conversation's direction generallyincreases relative to that of the therapist as their relationship progresses;and (2) patients who have less control in the first few sessions aresignificantly more likely to eventually express dissatisfaction with theirtherapist and terminate the relationship.</description><author>Vivian Nguyen, Sang Min Jung, Lillian Lee, Thomas D. Hull, Cristian Danescu-Niculescu-Mizil</author><pubDate>Wed, 09 Oct 2024 17:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07147v1</guid></item><item><title>Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling</title><link>http://arxiv.org/abs/2410.07145v1</link><description>One essential advantage of recurrent neural networks (RNNs) overtransformer-based language models is their linear computational complexityconcerning the sequence length, which makes them much faster in handling longsequences during inference. However, most publicly available RNNs (e.g., Mambaand RWKV) are trained on sequences with less than 10K tokens, and theireffectiveness in longer contexts remains largely unsatisfying so far. In thispaper, we study the cause of the inability to process long context for RNNs andsuggest critical mitigations. We examine two practical concerns when applyingstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate toinputs longer than the training length and (2) the upper bound of memorycapacity. Addressing the first concern, we first investigate *state collapse*(SC), a phenomenon that causes severe performance degradation on sequencelengths not encountered during training. With controlled experiments, weattribute this to overfitting due to the recurrent state beingoverparameterized for the training length. For the second concern, we train aseries of Mamba-2 models on long documents to empirically estimate therecurrent state capacity in language modeling and passkey retrieval. Then,three SC mitigation methods are proposed to improve Mamba-2's lengthgeneralizability, allowing the model to process more than 1M tokens without SC.We also find that the recurrent state capacity in passkey retrieval scalesexponentially to the state size, and we empirically train a Mamba-2 370M withnear-perfect passkey retrieval accuracy on 256K context length. This suggests apromising future for RNN-based long-context modeling.</description><author>Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun</author><pubDate>Wed, 09 Oct 2024 17:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07145v1</guid></item><item><title>Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates</title><link>http://arxiv.org/abs/2410.07137v1</link><description>Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, andMT-Bench, have become popular for evaluating language models due to theircost-effectiveness and scalability compared to human evaluation. Achieving highwin rates on these benchmarks can significantly boost the promotional impact ofnewly released language models. This promotional benefit may motivate tricks,such as manipulating model output length or style to game win rates, eventhough several mechanisms have been developed to control length and disentanglestyle to reduce gameability. Nonetheless, we show that even a "null model" thatalways outputs a constant response (irrelevant to input instructions) can cheatautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate onAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.Moreover, the crafted cheating outputs are transferable because we assume thatthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) areprivate and cannot be accessed. While our experiments are primarilyproof-of-concept, an adversary could use LLMs to generate more imperceptiblecheating responses, unethically benefiting from high win rates and promotionalimpact. Our findings call for the development of anti-cheating mechanisms forreliable automatic benchmarks. The code is available athttps://github.com/sail-sg/Cheating-LLM-Benchmarks.</description><author>Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin</author><pubDate>Wed, 09 Oct 2024 17:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07137v1</guid></item><item><title>EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models</title><link>http://arxiv.org/abs/2410.07133v1</link><description>Recent advancements in generation models have showcased remarkablecapabilities in generating fantastic content. However, most of them are trainedon proprietary high-quality data, and some models withhold their parameters andonly provide accessible application programming interfaces (APIs), limitingtheir benefits for downstream tasks. To explore the feasibility of training atext-to-image generation model comparable to advanced models using publiclyavailable resources, we introduce EvolveDirector. This framework interacts withadvanced models through their public APIs to obtain text-image data pairs totrain a base model. Our experiments with extensive data indicate that the modeltrained on generated data of the advanced model can approximate its generationcapability. However, it requires large-scale samples of 10 million or more.This incurs significant expenses in time, computational resources, andespecially the costs associated with calling fee-based APIs. To address thisproblem, we leverage pre-trained large vision-language models (VLMs) to guidethe evolution of the base model. VLM continuously evaluates the base modelduring training and dynamically updates and refines the training dataset by thediscrimination, expansion, deletion, and mutation operations. Experimentalresults show that this paradigm significantly reduces the required data volume.Furthermore, when approaching multiple advanced models, EvolveDirector canselect the best samples generated by them to learn powerful and balancedabilities. The final trained model Edgen is demonstrated to outperform theseadvanced models. The code and model weights are available athttps://github.com/showlab/EvolveDirector.</description><author>Rui Zhao, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Zhangjie Wu, Junhao Zhang, Yingya Zhang, Mike Zheng Shou</author><pubDate>Wed, 09 Oct 2024 17:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07133v1</guid></item><item><title>Mental Disorders Detection in the Era of Large Language Models</title><link>http://arxiv.org/abs/2410.07129v1</link><description>This paper compares the effectiveness of traditional machine learningmethods, encoder-based models, and large language models (LLMs) on the task ofdetecting depression and anxiety. Five datasets were considered, each differingin format and the method used to define the target pathology class. We testedAutoML models based on linguistic features, several variations of encoder-basedTransformers such as BERT, and state-of-the-art LLMs as pathologyclassification models. The results demonstrated that LLMs outperformtraditional methods, particularly on noisy and small datasets where trainingexamples vary significantly in text length and genre. However, psycholinguisticfeatures and encoder-based models can achieve performance comparable tolanguage models when trained on texts from individuals with clinicallyconfirmed depression, highlighting their potential effectiveness in targetedclinical applications.</description><author>Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Ivan Smirnov, Artem Shelmanov</author><pubDate>Wed, 09 Oct 2024 17:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07129v1</guid></item><item><title>ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models</title><link>http://arxiv.org/abs/2407.12877v2</link><description>Assessing the quality of outputs generated by generative models, such aslarge language models and vision language models, presents notable challenges.Traditional methods for evaluation typically rely on either human assessments,which are resource-intensive, or automatic metrics that often show a lowcorrelation with human judgment. Another common approach is to use deeplearning systems, which not only consume a substantial amount of compute andtime but also require extensive training data. In this study, we introduce atuning-free framework called ReFeR, designed to evaluate generative outputs,including both text and images, by leveraging a 2-level hierarchy of LLMs andVLMs themselves. We rigorously evaluate our framework, ReFeR, across fourdiverse evaluation tasks. The framework not only improves the accuracy of theseevaluations, surpassing previous benchmarks but also generates constructivefeedback. Interestingly, the framework is also applicable to reasoning tasks.Experiments on four reasoning tasks demonstrate superior collective reasoningabilities of the framework. We present two variants of the framework:ReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering amore cost-effective solution. ReFeR-Lite is $\sim7.7\times$ more efficientwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIPpackage publicly available. See this PIP URLhttps://pypi.org/project/refer-agents/ and this Git URLhttps://github.com/yaswanth-iitkgp/ReFeR_Code .</description><author>Yaswanth Narsupalli, Abhranil Chandra, Sreevatsa Muppirala, Manish Gupta, Pawan Goyal</author><pubDate>Wed, 09 Oct 2024 17:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12877v2</guid></item><item><title>Thing2Reality: Transforming 2D Content into Conditioned Multiviews and 3D Gaussian Objects for XR Communication</title><link>http://arxiv.org/abs/2410.07119v1</link><description>During remote communication, participants often share both digital andphysical content, such as product designs, digital assets, and environments, toenhance mutual understanding. Recent advances in augmented communication havefacilitated users to swiftly create and share digital 2D copies of physicalobjects from video feeds into a shared space. However, conventional 2Drepresentations of digital objects restricts users' ability to spatiallyreference items in a shared immersive environment. To address this, we proposeThing2Reality, an Extended Reality (XR) communication platform that enhancesspontaneous discussions of both digital and physical items during remotesessions. With Thing2Reality, users can quickly materialize ideas or physicalobjects in immersive environments and share them as conditioned multiviewrenderings or 3D Gaussians. Thing2Reality enables users to interact with remoteobjects or discuss concepts in a collaborative manner. Our user study revealedthat the ability to interact with and manipulate 3D representations of objectssignificantly enhances the efficiency of discussions, with the potential toaugment discussion of 2D artifacts.</description><author>Erzhen Hu, Mingyi Li, Jungtaek Hong, Xun Qian, Alex Olwal, David Kim, Seongkook Heo, Ruofei Du</author><pubDate>Wed, 09 Oct 2024 17:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07119v1</guid></item><item><title>Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy</title><link>http://arxiv.org/abs/2410.07118v1</link><description>The use of small language models (SLMs), herein defined as models with lessthan three billion parameters, is increasing across various domains andapplications. Due to their ability to run on more accessible hardware andpreserve user privacy, SLMs possess the potential to democratize access tolanguage models for individuals of different socioeconomic status and withdifferent privacy preferences. This study assesses several state-of-the-artSLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllamaproject) for use in the financial domain to support the development offinancial literacy LMs. Democratizing access to quality financial informationfor those who are financially under educated is greatly needed in society,particularly as new financial markets and products emerge and participation infinancial markets increases due to ease of access. We are the first to examinethe use of open-source SLMs to democratize access to financial questionanswering capabilities for individuals and students. To this end, we provide ananalysis of the memory usage, inference time, similarity comparisons toground-truth answers, and output readability of prominent SLMs to determinewhich models are most accessible and capable of supporting access to financialinformation. We analyze zero-shot and few-shot learning variants of the models.The results suggest that some off-the-shelf SLMs merit further exploration andfine-tuning to prepare them for individual use, while others may have limits totheir democratization.</description><author>Tagore Rao Kosireddy, Jeffrey D. Wall, Evan Lucas</author><pubDate>Wed, 09 Oct 2024 17:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07118v1</guid></item><item><title>The FIX Benchmark: Extracting Features Interpretable to eXperts</title><link>http://arxiv.org/abs/2409.13684v2</link><description>Feature-based methods are commonly used to explain model predictions, butthese methods often implicitly assume that interpretable features are readilyavailable. However, this is often not the case for high-dimensional data, andit can be hard even for domain experts to mathematically specify which featuresare important. Can we instead automatically extract collections or groups offeatures that are aligned with expert knowledge? To address this gap, wepresent FIX (Features Interpretable to eXperts), a benchmark for measuring howwell a collection of features aligns with expert knowledge. In collaborationwith domain experts, we propose FIXScore, a unified expert alignment measureapplicable to diverse real-world settings across cosmology, psychology, andmedicine domains in vision, language and time series data modalities. WithFIXScore, we find that popular feature-based explanation methods have pooralignment with expert-specified knowledge, highlighting the need for newmethods that can better identify features interpretable to experts.</description><author>Helen Jin, Shreya Havaldar, Chaehyeon Kim, Anton Xue, Weiqiu You, Helen Qu, Marco Gatti, Daniel A Hashimoto, Bhuvnesh Jain, Amin Madani, Masao Sako, Lyle Ungar, Eric Wong</author><pubDate>Wed, 09 Oct 2024 17:47:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13684v2</guid></item><item><title>Personalized Visual Instruction Tuning</title><link>http://arxiv.org/abs/2410.07113v1</link><description>Recent advancements in multimodal large language models (MLLMs) havedemonstrated significant progress; however, these models exhibit a notablelimitation, which we refer to as "face blindness". Specifically, they canengage in general conversations but fail to conduct personalized dialoguestargeting at specific individuals. This deficiency hinders the application ofMLLMs in personalized settings, such as tailored visual assistants on mobiledevices, or domestic robots that need to recognize members of the family. Inthis paper, we introduce Personalized Visual Instruction Tuning (PVIT), a noveldata curation and training framework designed to enable MLLMs to identifytarget individuals within an image and engage in personalized and coherentdialogues. Our approach involves the development of a sophisticated pipelinethat autonomously generates training data containing personalizedconversations. This pipeline leverages the capabilities of various visualexperts, image generation models, and (multi-modal) large language models. Toevaluate the personalized potential of MLLMs, we present a benchmark calledP-Bench, which encompasses various question types with different levels ofdifficulty. The experiments demonstrate a substantial personalized performanceenhancement after fine-tuning with our curated dataset.</description><author>Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, Tong Zhang</author><pubDate>Wed, 09 Oct 2024 17:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07113v1</guid></item><item><title>VHELM: A Holistic Evaluation of Vision Language Models</title><link>http://arxiv.org/abs/2410.07112v1</link><description>Current benchmarks for assessing vision-language models (VLMs) often focus ontheir perception or problem-solving capabilities and neglect other criticalaspects such as fairness, multilinguality, or toxicity. Furthermore, theydiffer in their evaluation procedures and the scope of the evaluation, makingit difficult to compare models. To address these issues, we extend the HELMframework to VLMs to present the Holistic Evaluation of Vision Language Models(VHELM). VHELM aggregates various datasets to cover one or more of the 9aspects: visual perception, knowledge, reasoning, bias, fairness,multilinguality, robustness, toxicity, and safety. In doing so, we produce acomprehensive, multi-dimensional view of the capabilities of the VLMs acrossthese important factors. In addition, we standardize the standard inferenceparameters, methods of prompting, and evaluation metrics to enable faircomparisons across models. Our framework is designed to be lightweight andautomatic so that evaluation runs are cheap and fast. Our initial run evaluates22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.We uncover new key findings, such as the fact that efficiency-focused models(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse thantheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmarkbut not when evaluated on the other aspects. For transparency, we release theraw model generations and complete results on our website(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a livingbenchmark, and we hope to continue adding new datasets and models over time.</description><author>Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, Percy Liang</author><pubDate>Wed, 09 Oct 2024 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07112v1</guid></item><item><title>Continual Learning: Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay</title><link>http://arxiv.org/abs/2410.07110v1</link><description>Machine learning models often suffer from catastrophic forgetting ofpreviously learned knowledge when learning new classes. Various methods havebeen proposed to mitigate this issue. However, rehearsal-based learning, whichretains samples from previous classes, typically achieves good performance buttends to memorize specific instances, struggling with Out-of-Distribution (OOD)generalization. This often leads to high forgetting rates and poorgeneralization. Surprisingly, the OOD generalization capabilities of thesemethods have been largely unexplored. In this paper, we highlight this issueand propose a simple yet effective strategy inspired by contrastive learningand data-centric principles to address it. We introduce Adaptive ContrastiveReplay (ACR), a method that employs dual optimization to simultaneously trainboth the encoder and the classifier. ACR adaptively populates the replay bufferwith misclassified samples while ensuring a balanced representation of classesand tasks. By refining the decision boundary in this way, ACR achieves abalance between stability and plasticity. Our method significantly outperformsprevious approaches in terms of OOD generalization, achieving an improvement of13.41\% on Split CIFAR-100, 9.91\% on Split Mini-ImageNet, and 5.98\% on SplitTiny-ImageNet.</description><author>Hossein Rezaei, Mohammad Sabokrou</author><pubDate>Wed, 09 Oct 2024 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07110v1</guid></item><item><title>I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy</title><link>http://arxiv.org/abs/2410.07109v1</link><description>As Large Language Model (LLM)-based agents become increasingly autonomous andwill more freely interact with each other, studying interactions between thembecomes crucial to anticipate emergent phenomena and potential risks. Drawinginspiration from the widely popular Stanford Prison Experiment, we contributeto this line of research by studying interaction patterns of LLM agents in acontext characterized by strict social hierarchy. We do so by specificallystudying two types of phenomena: persuasion and anti-social behavior insimulated scenarios involving a guard and a prisoner agent who seeks to achievea specific goal (i.e., obtaining additional yard time or escape from prison).Leveraging 200 experimental scenarios for a total of 2,000 machine-machineconversations across five different popular LLMs, we provide a set ofnoteworthy findings. We first document how some models consistently fail incarrying out a conversation in our multi-agent setup where power dynamics areat play. Then, for the models that were able to engage in successfulinteractions, we empirically show how the goal that an agent is set to achieveimpacts primarily its persuasiveness, while having a negligible effect withrespect to the agent's anti-social behavior. Third, we highlight how agents'personas, and particularly the guard's personality, drive both the likelihoodof successful persuasion from the prisoner and the emergence of anti-socialbehaviors. Fourth, we show that even without explicitly prompting for specificpersonalities, anti-social behavior emerges by simply assigning agents' roles.These results bear implications for the development of interactive LLM agentsas well as the debate on their societal impact.</description><author>Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano</author><pubDate>Wed, 09 Oct 2024 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07109v1</guid></item><item><title>Private prediction for large-scale synthetic text generation</title><link>http://arxiv.org/abs/2407.12108v2</link><description>We present an approach for generating differentially private synthetic textusing large language models (LLMs), via private prediction. In the privateprediction framework, we only require the output synthetic data to satisfydifferential privacy guarantees. This is in contrast to approaches that train agenerative model on potentially sensitive user-supplied source data and seek toensure the model itself is safe to release. We prompt a pretrained LLM with source data, but ensure that next-tokenpredictions are made with differential privacy guarantees. Previous work inthis paradigm reported generating a small number of examples (&lt;10) atreasonable privacy levels, an amount of data that is useful only for downstreamin-context learning or prompting. In contrast, we make changes that allow us togenerate thousands of high-quality synthetic data points, greatly expanding theset of potential applications. Our improvements come from an improved privacyanalysis and a better private selection mechanism, which makes use of theequivalence between the softmax layer for sampling tokens in LLMs and theexponential mechanism. Furthermore, we introduce a novel use of publicpredictions via the sparse vector technique, in which we do not pay privacycosts for tokens that are predictable without sensitive data; we find this tobe particularly effective for structured data.</description><author>Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii</author><pubDate>Wed, 09 Oct 2024 17:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12108v2</guid></item><item><title>Topologically Faithful Multi-class Segmentation in Medical Images</title><link>http://arxiv.org/abs/2403.11001v2</link><description>Topological accuracy in medical image segmentation is a highly importantproperty for downstream applications such as network analysis and flow modelingin vessels or cell counting. Recently, significant methodological advancementshave brought well-founded concepts from algebraic topology to binarysegmentation. However, these approaches have been underexplored in multi-classsegmentation scenarios, where topological errors are common. We propose ageneral loss function for topologically faithful multi-class segmentationextending the recent Betti matching concept, which is based on inducedmatchings of persistence barcodes. We project the N-class segmentation problemto N single-class segmentation tasks, which allows us to use 1-parameterpersistent homology, making training of neural networks computationallyfeasible. We validate our method on a comprehensive set of four medicaldatasets with highly variant topological characteristics. Our loss formulationsignificantly enhances topological correctness in cardiac, cell, artery-vein,and Circle of Willis segmentation.</description><author>Alexander H. Berger, Nico Stucki, Laurin Lux, Vincent Buergin, Suprosanna Shit, Anna Banaszak, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold</author><pubDate>Wed, 09 Oct 2024 17:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11001v2</guid></item><item><title>Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context</title><link>http://arxiv.org/abs/2410.07103v1</link><description>Multi-hop reasoning, which requires multi-step reasoning based on thesupporting documents within a given context, remains challenging for largelanguage models (LLMs). LLMs often struggle to filter out irrelevant documentswithin the context, and their performance is sensitive to the position ofsupporting documents within that context. In this paper, we identify anadditional challenge: LLMs' performance is also sensitive to the order in whichthe supporting documents are presented. We refer to this as the misorderedcontext problem. To address this issue, we propose a simple yet effectivemethod called context repetition (CoRe), which involves prompting the model byrepeatedly presenting the context to ensure the supporting documents arepresented in the optimal order for the model. Using CoRe, we improve the F1score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%pon a synthetic task. Additionally, CoRe helps mitigate the well-known"lost-in-the-middle" problem in LLMs and can be effectively combined withretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.</description><author>Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon</author><pubDate>Wed, 09 Oct 2024 17:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07103v1</guid></item><item><title>DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data Mining</title><link>http://arxiv.org/abs/2410.00260v2</link><description>Large Language Models (LLMs) have shown remarkable ability to generalizeeffectively across numerous industry domains while executing a range of tasks.Many of these competencies are obtained from the data utilized during thepre-training phase of the Language Models (LMs). However, these models exhibitlimitations when tasked with performing in specialized or low-resource industrydomains. More recent approaches use LLMs for generating domain-specificsynthetic data but most often they lack in truthfulness and complexity.Alternatively, in cases where domain data is available like healthcare andfinance most of the LMs are proprietary necessitating the need for a scalablemethod to curate real world industry specific pre-training data. In this work,we propose an automated and scalable framework - DoPAMine:Domain-specificPre-training Adaptation from seed-guided data Mining, to mine domain specifictraining data from a large data corpus for domain adaptation of a LM. Theframework leverages the parametric knowledge of a LLM to generate diverse andrepresentative seed data tailored to a specific domain which is then used tomine real world data from a large data corpus like Common Crawl. We evaluatedour framework's performance in the continual pre-training (CPT) setting bytraining two domain specific 7B parameter LMs in healthcare and finance withdata mined via DoPAMine. Our experiments show that DoPAMine boosts theperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA andPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settingsrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets whencompared to the baseline.</description><author>Vinayak Arannil, Neha Narwal, Sourav Sanjukta Bhabesh, Sai Nikhil Thirandas, Darren Yow-Bang Wang, Graham Horwood, Alex Anto Chirayath, Gouri Pandeshwar</author><pubDate>Wed, 09 Oct 2024 17:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00260v2</guid></item><item><title>Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings</title><link>http://arxiv.org/abs/2401.06112v3</link><description>Word embedding is one of the most important components in natural languageprocessing, but interpreting high-dimensional embeddings remains a challengingproblem. To address this problem, Independent Component Analysis (ICA) isidentified as an effective solution. ICA-transformed word embeddings revealinterpretable semantic axes; however, the order of these axes are arbitrary. Inthis study, we focus on this property and propose a novel method, Axis Tour,which optimizes the order of the axes. Inspired by Word Tour, a one-dimensionalword embedding method, we aim to improve the clarity of the word embeddingspace by maximizing the semantic continuity of the axes. Furthermore, we showthrough experiments on downstream tasks that Axis Tour yields better orcomparable low-dimensional embeddings compared to both PCA and ICA.</description><author>Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira</author><pubDate>Wed, 09 Oct 2024 17:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06112v3</guid></item><item><title>Identifying and Addressing Delusions for Target-Directed Decision-Making</title><link>http://arxiv.org/abs/2410.07096v1</link><description>We are interested in target-directed agents, which produce targets duringdecision-time planning, to guide their behaviors and achieve bettergeneralization during evaluation. Improper training of these agents can resultin delusions: the agent may come to hold false beliefs about the targets, whichcannot be properly rejected, leading to unwanted behaviors and damagingout-of-distribution generalization. We identify different types of delusions byusing intuitive examples in carefully controlled environments, and investigatetheir causes. We demonstrate how delusions can be addressed for agents trainedby hindsight relabeling, a mainstream approach in for training target-directedRL agents. We validate empirically the effectiveness of the proposed solutionsin correcting delusional behaviors and improving out-of-distributiongeneralization.</description><author>Mingde Zhao, Tristan Sylvain, Doina Precup, Yoshua Bengio</author><pubDate>Wed, 09 Oct 2024 17:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07096v1</guid></item><item><title>MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering</title><link>http://arxiv.org/abs/2410.07095v1</link><description>We introduce MLE-bench, a benchmark for measuring how well AI agents performat machine learning engineering. To this end, we curate 75 MLengineering-related competitions from Kaggle, creating a diverse set ofchallenging tasks that test real-world ML engineering skills such as trainingmodels, preparing datasets, and running experiments. We establish humanbaselines for each competition using Kaggle's publicly available leaderboards.We use open-source agent scaffolds to evaluate several frontier language modelson our benchmark, finding that the best-performing setup--OpenAI's o1-previewwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in16.9% of competitions. In addition to our main results, we investigate variousforms of resource scaling for AI agents and the impact of contamination frompre-training. We open-source our benchmark code (github.com/openai/mle-bench/)to facilitate future research in understanding the ML engineering capabilitiesof AI agents.</description><author>Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry</author><pubDate>Wed, 09 Oct 2024 17:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07095v1</guid></item><item><title>An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots</title><link>http://arxiv.org/abs/2410.07094v1</link><description>Software engineering (SE) chatbots are increasingly gaining attention fortheir role in enhancing development processes. At the core of chatbots are theNatural Language Understanding platforms (NLUs), which enable them tocomprehend and respond to user queries. Before deploying NLUs, there is a needto train them with labeled data. However, acquiring such labeled data for SEchatbots is challenging due to the scarcity of high-quality datasets. Thischallenge arises because training SE chatbots requires specialized vocabularyand phrases not found in typical language datasets. Consequently, chatbotdevelopers often resort to manually annotating user queries to gather the datanecessary for training effective chatbots, a process that is bothtime-consuming and resource-intensive. Previous studies propose approaches tosupport chatbot practitioners in annotating users' posed queries. However,these approaches require human intervention to generate rules, called labelingfunctions (LFs), that identify and categorize user queries based on specificpatterns in the data. To address this issue, we propose an approach toautomatically generate LFs by extracting patterns from labeled user queries. Weevaluate the effectiveness of our approach by applying it to the queries offour diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)and measure the performance improvement gained from training the NLU on thequeries labeled by the generated LFs. We find that the generated LFseffectively label data with AUC scores of up to 85.3%, and NLU's performanceimprovement of up to 27.2% across the studied datasets. Furthermore, ourresults show that the number of LFs used to generate LFs affects the labelingperformance. We believe that our approach can save time and resources inlabeling users' queries, allowing practitioners to focus on core chatbotfunctionalities.</description><author>Ebube Alor, Ahmad Abdellatif, SayedHassan Khatoonabadi, Emad Shihab</author><pubDate>Wed, 09 Oct 2024 17:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07094v1</guid></item><item><title>LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning</title><link>http://arxiv.org/abs/2410.07093v1</link><description>Language plays a vital role in the realm of human motion. Existing methodshave largely depended on CLIP text embeddings for motion generation, yet theyfall short in effectively aligning language and motion due to CLIP'spretraining on static image-text pairs. This work introduces LaMP, a novelLanguage-Motion Pretraining model, which transitions from a language-vision toa more suitable language-motion latent space. It addresses key limitations bygenerating motion-informative text embeddings, significantly enhancing therelevance and semantics of generated motion sequences. With LaMP, we advancethree key tasks: text-to-motion generation, motion-text retrieval, and motioncaptioning through aligned language-motion representation learning. Forgeneration, we utilize LaMP to provide the text condition instead of CLIP, andan autoregressive masked prediction is designed to achieve mask modelingwithout rank collapse in transformers. For retrieval, motion features fromLaMP's motion transformer interact with query tokens to retrieve text featuresfrom the text transformer, and vice versa. For captioning, we finetune a largelanguage model with the language-informative motion features to develop astrong motion captioning model. In addition, we introduce the LaMP-BertScoremetric to assess the alignment of generated motions with textual descriptions.Extensive experimental results on multiple datasets demonstrate substantialimprovements over previous methods across all three tasks. The code of ourmethod will be made public.</description><author>Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang</author><pubDate>Wed, 09 Oct 2024 17:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07093v1</guid></item><item><title>Collusion Detection with Graph Neural Networks</title><link>http://arxiv.org/abs/2410.07091v1</link><description>Collusion is a complex phenomenon in which companies secretly collaborate toengage in fraudulent practices. This paper presents an innovative methodologyfor detecting and predicting collusion patterns in different national marketsusing neural networks (NNs) and graph neural networks (GNNs). GNNs areparticularly well suited to this task because they can exploit the inherentnetwork structures present in collusion and many other economic problems. Ourapproach consists of two phases: In Phase I, we develop and train models onindividual market datasets from Japan, the United States, two regions inSwitzerland, Italy, and Brazil, focusing on predicting collusion in singlemarkets. In Phase II, we extend the models' applicability through zero-shotlearning, employing a transfer learning approach that can detect collusion inmarkets in which training data is unavailable. This phase also incorporatesout-of-distribution (OOD) generalization to evaluate the models' performance onunseen datasets from other countries and regions. In our empirical study, weshow that GNNs outperform NNs in detecting complex collusive patterns. Thisresearch contributes to the ongoing discourse on preventing collusion andoptimizing detection methodologies, providing valuable guidance on the use ofNNs and GNNs in economic applications to enhance market fairness and economicwelfare.</description><author>Lucas Gomes, Jannis Kueck, Mara Mattes, Martin Spindler, Alexey Zaytsev</author><pubDate>Wed, 09 Oct 2024 17:31:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07091v1</guid></item><item><title>Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology</title><link>http://arxiv.org/abs/2410.07087v1</link><description>Developing agents capable of navigating to a target location based onlanguage instructions and visual information, known as vision-languagenavigation (VLN), has attracted widespread interest. Most research has focusedon ground-based agents, while UAV-based VLN remains relatively underexplored.Recent efforts in UAV vision-language navigation predominantly adoptground-based VLN settings, relying on predefined discrete action spaces andneglecting the inherent disparities in agent movement dynamics and thecomplexity of navigation tasks between ground and aerial environments. Toaddress these disparities and challenges, we propose solutions from threeperspectives: platform, benchmark, and methodology. To enable realistic UAVtrajectory simulation in VLN tasks, we propose the OpenUAV platform, whichfeatures diverse environments, realistic flight control, and extensivealgorithmic support. We further construct a target-oriented VLN datasetconsisting of approximately 12k trajectories on this platform, serving as thefirst dataset specifically designed for realistic UAV VLN tasks. To tackle thechallenges posed by complex aerial environments, we propose an assistant-guidedUAV object search benchmark called UAV-Need-Help, which provides varying levelsof guidance information to help UAVs better accomplish realistic VLN tasks. Wealso propose a UAV navigation LLM that, given multi-view images, taskdescriptions, and assistant instructions, leverages the multimodalunderstanding capabilities of the MLLM to jointly process visual and textualinformation, and performs hierarchical trajectory generation. The evaluationresults of our method significantly outperform the baseline models, while thereremains a considerable gap between our results and those achieved by humanoperators, underscoring the challenge presented by the UAV-Need-Help task.</description><author>Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao, Si Liu</author><pubDate>Wed, 09 Oct 2024 17:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07087v1</guid></item><item><title>Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments</title><link>http://arxiv.org/abs/2407.12040v4</link><description>This study extensively evaluated You Only Look Once (YOLO) object detectionalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, andYOLO11 for green fruit detection in commercial orchards. The research alsovalidated in-field fruitlet counting using an iPhone and machine vision sensorsacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.Among the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-baseoutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. Interms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9configurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,significantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,and YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. Thiscomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,offering researchers essential insights to choose the best-suited model forfruitlet detection and possible automation in commercial orchards. Forreal-time automation related work in relevant datasets, we recommend usingYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,YOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, FruitletDetection, Greenfruit Detection, Green Apple Detection, AgriculturalAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shotDetection</description><author>Ranjan Sapkota, Zhichao Meng, Martin Churuvija, Xiaoqiang Du, Zenghong Ma, Manoj Karkee</author><pubDate>Wed, 09 Oct 2024 17:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12040v4</guid></item><item><title>Stanceformer: Target-Aware Transformer for Stance Detection</title><link>http://arxiv.org/abs/2410.07083v1</link><description>The task of Stance Detection involves discerning the stance expressed in atext towards a specific subject or target. Prior works have relied on existingtransformer models that lack the capability to prioritize targets effectively.Consequently, these models yield similar performance regardless of whether weutilize or disregard target information, undermining the task's significance.To address this challenge, we introduce Stanceformer, a target-awaretransformer model that incorporates enhanced attention towards the targetsduring both training and inference. Specifically, we design a \textit{TargetAwareness} matrix that increases the self-attention scores assigned to thetargets. We demonstrate the efficacy of the Stanceformer with variousBERT-based models, including state-of-the-art models and Large Language Models(LLMs), and evaluate its performance across three stance detection datasets,alongside a zero-shot dataset. Our approach Stanceformer not only providessuperior performance but also generalizes even to other domains, such asAspect-based Sentiment Analysis. We make the code publiclyavailable.\footnote{\scriptsize\url{https://github.com/kgarg8/Stanceformer}}</description><author>Krishna Garg, Cornelia Caragea</author><pubDate>Wed, 09 Oct 2024 17:24:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07083v1</guid></item><item><title>JPEG Inspired Deep Learning</title><link>http://arxiv.org/abs/2410.07081v1</link><description>Although it is traditionally believed that lossy image compression, such asJPEG compression, has a negative impact on the performance of deep neuralnetworks (DNNs), it is shown by recent works that well-crafted JPEG compressioncan actually improve the performance of deep learning (DL). Inspired by this,we propose JPEG-DL, a novel DL framework that prepends any underlying DNNarchitecture with a trainable JPEG compression layer. To make the quantizationoperation in JPEG compression trainable, a new differentiable soft quantizer isemployed at the JPEG layer, and then the quantization operation and underlyingDNN are jointly trained. Extensive experiments show that in comparison with thestandard DL, JPEG-DL delivers significant accuracy improvements across variousdatasets and model architectures while enhancing robustness against adversarialattacks. Particularly, on some fine-grained image classification datasets,JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code isavailable on https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git.</description><author>Ahmed H. Salamah, Kaixiang Zheng, Yiwen Liu, En-Hui Yang</author><pubDate>Wed, 09 Oct 2024 17:23:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07081v1</guid></item><item><title>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses</title><link>http://arxiv.org/abs/2410.07076v1</link><description>Scientific discovery contributes largely to human society's prosperity, andrecent progress shows that LLMs could potentially catalyze this process.However, it is still unclear whether LLMs can discover novel and validhypotheses in chemistry. In this work, we investigate this central researchquestion: Can LLMs automatically discover novel and valid chemistry researchhypotheses given only a chemistry research background (consisting of a researchquestion and/or a background survey), without limitation on the domain of theresearch question? After extensive discussions with chemistry experts, wepropose an assumption that a majority of chemistry hypotheses can be resultedfrom a research background and several inspirations. With this key insight, webreak the central question into three smaller fundamental questions. In brief,they are: (1) given a background question, whether LLMs can retrieve goodinspirations; (2) with background and inspirations, whether LLMs can lead tohypothesis; and (3) whether LLMs can identify good hypotheses to rank themhigher. To investigate these questions, we construct a benchmark consisting of51 chemistry papers published in Nature, Science, or a similar level in 2024(all papers are only available online since 2024). Every paper is divided bychemistry PhD students into three components: background, inspirations, andhypothesis. The goal is to rediscover the hypothesis, given only the backgroundand a large randomly selected chemistry literature corpus consisting the groundtruth inspiration papers, with LLMs trained with data up to 2023. We alsodevelop an LLM-based multi-agent framework that leverages the assumption,consisting of three stages reflecting the three smaller questions. The proposedmethod can rediscover many hypotheses with very high similarity with the groundtruth ones, covering the main innovations.</description><author>Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou</author><pubDate>Wed, 09 Oct 2024 17:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07076v1</guid></item><item><title>ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using LLMs</title><link>http://arxiv.org/abs/2401.14279v2</link><description>Technical Q&amp;A sites are valuable for software developers seeking knowledge,but the code snippets they provide are often uncompilable and incomplete due tounresolved types and missing libraries. This poses a challenge for users whowish to reuse or analyze these snippets. Existing methods either do not focuson creating compilable code or have low success rates. To address this, wepropose ZS4C, a lightweight approach for zero-shot synthesis of compilable codefrom incomplete snippets using Large Language Models (LLMs). ZS4C operates intwo stages: first, it uses an LLM, like GPT-3.5, to identify missing importstatements in a snippet; second, it collaborates with a validator (e.g.,compiler) to fix compilation errors caused by incorrect imports and syntaxissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,Python-SO, which includes 539 Python snippets from Stack Overflow across the 20most popular Python libraries. ZS4C significantly outperforms existing methods,improving the compilation rate from 63% to 95.1% compared to thestate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infermore accurate import statements (with an F1 score of 0.98) than SnR, with animprovement of 8.5% in the F1.</description><author>Azmain Kabir, Shaowei Wang, Yuan Tian, Tse-Hsun Chen, Muhammad Asaduzzaman, Wenbin Zhang</author><pubDate>Wed, 09 Oct 2024 17:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14279v2</guid></item><item><title>Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning</title><link>http://arxiv.org/abs/2410.07074v1</link><description>Textual Attributed Graphs (TAGs) are crucial for modeling complex real-worldsystems, yet leveraging large language models (LLMs) for TAGs presents uniquechallenges due to the gap between sequential text processing andgraph-structured data. We introduce AskGNN, a novel approach that bridges thisgap by leveraging In-Context Learning (ICL) to integrate graph data andtask-specific information into LLMs. AskGNN employs a Graph Neural Network(GNN)-powered structure-enhanced retriever to select labeled nodes acrossgraphs, incorporating complex graph structures and their supervision signals.Our learning-to-retrieve algorithm optimizes the retriever to select examplenodes that maximize LLM performance on graph. Experiments across three tasksand seven LLMs demonstrate AskGNN's superior effectiveness in graph taskperformance, opening new avenues for applying LLMs to graph-structured datawithout extensive fine-tuning.</description><author>Zhengyu Hu, Yichuan Li, Zhengyu Chen, Jingang Wang, Han Liu, Kyumin Lee, Kaize Ding</author><pubDate>Wed, 09 Oct 2024 17:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07074v1</guid></item><item><title>Pixtral 12B</title><link>http://arxiv.org/abs/2410.07073v1</link><description>We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.Pixtral-12B is trained to understand both natural images and documents,achieving leading performance on various multimodal benchmarks, surpassing anumber of larger models. Unlike many open-source models, Pixtral is also acutting-edge text model for its size, and does not compromise on naturallanguage performance to excel in multimodal tasks. Pixtral uses a new visionencoder trained from scratch, which allows it to ingest images at their naturalresolution and aspect ratio. This gives users flexibility on the number oftokens used to process an image. Pixtral is also able to process any number ofimages in its long context window of 128K tokens. Pixtral 12B substaniallyoutperforms other open models of similar sizes (Llama-3.2 11B \&amp; Qwen-2-VL 7B).It also outperforms much larger open models like Llama-3.2 90B while being 7xsmaller. We further contribute an open-source benchmark, MM-MT-Bench, forevaluating vision-language models in practical scenarios, and provide detailedanalysis and code for standardized evaluation protocols for multimodal LLMs.Pixtral-12B is released under Apache 2.0 license.</description><author>Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang</author><pubDate>Wed, 09 Oct 2024 17:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07073v1</guid></item><item><title>Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation</title><link>http://arxiv.org/abs/2404.06809v3</link><description>The rapid development of large language models has led to the widespreadadoption of Retrieval-Augmented Generation (RAG), which integrates externalknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.However, the existing RAG paradigm inevitably suffers from the impact of flawedinformation introduced during the retrieval phrase, thereby diminishing thereliability and correctness of the generated outcomes. In this paper, wepropose Credibility-aware Generation (CAG), a universally applicable frameworkdesigned to mitigate the impact of flawed information in RAG. At its core, CAGaims to equip models with the ability to discern and process information basedon its credibility. To this end, we propose an innovative data transformationframework that generates data based on credibility, thereby effectivelyendowing models with the capability of CAG. Furthermore, to accurately evaluatethe models' capabilities of CAG, we construct a comprehensive benchmarkcovering three critical real-world scenarios. Experimental results demonstratethat our model can effectively understand and utilize credibility forgeneration, significantly outperform other models with retrieval augmentation,and exhibit resilience against the disruption caused by noisy documents,thereby maintaining robust performance. Moreover, our model supports customizedcredibility, offering a wide range of potential applications.</description><author>Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun</author><pubDate>Wed, 09 Oct 2024 17:16:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06809v3</guid></item><item><title>Towards xAI: Configuring RNN Weights using Domain Knowledge for MIMO Receive Processing</title><link>http://arxiv.org/abs/2410.07072v1</link><description>Deep learning is making a profound impact in the physical layer of wirelesscommunications. Despite exhibiting outstanding empirical performance in taskssuch as MIMO receive processing, the reasons behind the demonstrated superiorperformance improvement remain largely unclear. In this work, we advance thefield of Explainable AI (xAI) in the physical layer of wireless communicationsutilizing signal processing principles. Specifically, we focus on the task ofMIMO-OFDM receive processing (e.g., symbol detection) using reservoir computing(RC), a framework within recurrent neural networks (RNNs), which outperformsboth conventional and other learning-based MIMO detectors. Our analysisprovides a signal processing-based, first-principles understanding of thecorresponding operation of the RC. Building on this fundamental understanding,we are able to systematically incorporate the domain knowledge of wirelesssystems (e.g., channel statistics) into the design of the underlying RNN bydirectly configuring the untrained RNN weights for MIMO-OFDM symbol detection.The introduced RNN weight configuration has been validated through extensivesimulations demonstrating significant performance improvements. Thisestablishes a foundation for explainable RC-based architectures in MIMO-OFDMreceive processing and provides a roadmap for incorporating domain knowledgeinto the design of neural networks for NextG systems.</description><author>Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu</author><pubDate>Wed, 09 Oct 2024 17:16:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07072v1</guid></item><item><title>Retrieval-Augmented Decision Transformer: External Memory for In-context RL</title><link>http://arxiv.org/abs/2410.07071v1</link><description>In-context learning (ICL) is the ability of a model to learn a new task byobserving a few exemplars in its context. While prevalent in NLP, thiscapability has recently also been observed in Reinforcement Learning (RL)settings. Prior in-context RL methods, however, require entire episodes in theagent's context. Given that complex environments typically lead to longepisodes with sparse rewards, these methods are constrained to simpleenvironments with short episodes. To address these challenges, we introduceRetrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an externalmemory mechanism to store past experiences from which it retrieves onlysub-trajectories relevant for the current situation. The retrieval component inRA-DT does not require training and can be entirely domain-agnostic. Weevaluate the capabilities of RA-DT on grid-world environments, roboticssimulations, and procedurally-generated video games. On grid-worlds, RA-DToutperforms baselines, while using only a fraction of their context length.Furthermore, we illuminate the limitations of current in-context RL methods oncomplex environments and discuss future directions. To facilitate futureresearch, we release datasets for four of the considered environments.</description><author>Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, Sepp Hochreiter</author><pubDate>Wed, 09 Oct 2024 17:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07071v1</guid></item><item><title>ReIFE: Re-evaluating Instruction-Following Evaluation</title><link>http://arxiv.org/abs/2410.07069v1</link><description>The automatic evaluation of instruction following typically involves usinglarge language models (LLMs) to assess response quality. However, there is alack of comprehensive evaluation of these LLM-based evaluators across twodimensions: the base LLMs and the evaluation protocols. Therefore, we present athorough meta-evaluation of instruction following, including 25 base LLMs and15 recently proposed evaluation protocols, on 4 human-annotated datasets,assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allowsus to identify the best-performing base LLMs and evaluation protocols with ahigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)Base LLM performance ranking remains largely consistent across evaluationprotocols, with less capable LLMs showing greater improvement from protocolenhancements; (2) Robust evaluation of evaluation protocols requires many baseLLMs with varying capability levels, as protocol effectiveness can depend onthe base LLM used; (3) Evaluation results on different datasets are not alwaysconsistent, so a rigorous evaluation requires multiple datasets withdistinctive features. We release our meta-evaluation suite ReIFE, whichprovides the codebase and evaluation result collection for more than 500LLM-evaluator configurations, to support future research ininstruction-following evaluation.</description><author>Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan</author><pubDate>Wed, 09 Oct 2024 17:14:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07069v1</guid></item><item><title>Counterfactuals As a Means for Evaluating Faithfulness of Attribution Methods in Autoregressive Language Models</title><link>http://arxiv.org/abs/2408.11252v3</link><description>Despite the widespread adoption of autoregressive language models,explainability evaluation research has predominantly focused on span infillingand masked language models. Evaluating the faithfulness of an explanationmethod -- how accurately it explains the inner workings and decision-making ofthe model -- is challenging because it is difficult to separate the model fromits explanation. Most faithfulness evaluation techniques corrupt or removeinput tokens deemed important by a particular attribution (feature importance)method and observe the resulting change in the model's output. However, forautoregressive language models, this approach creates out-of-distributioninputs due to their next-token prediction training objective. In this study, wepropose a technique that leverages counterfactual generation to evaluate thefaithfulness of attribution methods for autoregressive language models. Ourtechnique generates fluent, in-distribution counterfactuals, making theevaluation protocol more reliable.</description><author>Sepehr Kamahi, Yadollah Yaghoobzadeh</author><pubDate>Wed, 09 Oct 2024 17:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11252v3</guid></item><item><title>A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research</title><link>http://arxiv.org/abs/2410.07066v1</link><description>Deep Generative Models (DGMs) have rapidly advanced in recent years, becomingessential tools in various fields due to their ability to learn complex datadistributions and generate synthetic data. Their importance in transportationresearch is increasingly recognized, particularly for applications like trafficdata generation, prediction, and feature extraction. This paper offers acomprehensive introduction and tutorial on DGMs, with a focus on theirapplications in transportation. It begins with an overview of generativemodels, followed by detailed explanations of fundamental models, a systematicreview of the literature, and practical tutorial code to aid implementation.The paper also discusses current challenges and opportunities, highlighting howthese models can be effectively utilized and further developed intransportation research. This paper serves as a valuable reference, guidingresearchers and practitioners from foundational knowledge to advancedapplications of DGMs in transportation research.</description><author>Seongjin Choi, Zhixiong Jin, Seungwoo Ham, Jiwon Kim, Lijun Sun</author><pubDate>Wed, 09 Oct 2024 17:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07066v1</guid></item><item><title>Population Transformer: Learning Population-level Representations of Neural Activity</title><link>http://arxiv.org/abs/2406.03044v2</link><description>We present a self-supervised framework that learns population-level codes forarbitrary ensembles of neural recordings at scale. We address two keychallenges in scaling models with neural time-series data: sparse and variableelectrode distribution across subjects and datasets. The Population Transformer(PopT) stacks on top of pretrained representations and enhances downstreamdecoding by enabling learned aggregation of multiple spatially-sparse datachannels. The pretrained PopT lowers the amount of data required for downstreamdecoding experiments, while increasing accuracy, even on held-out subjects andtasks. Compared to end-to-end methods, this approach is computationallylightweight and more interpretable, while still retaining competitiveperformance. We further show how our framework is generalizable to multipletime-series embeddings and neural data modalities. Beyond decoding, weinterpret the pretrained PopT and fine-tuned models to show how they can beused to extract neuroscience insights from massive amounts of data. We releaseour code as well as a pretrained PopT to enable off-the-shelf improvements inmulti-channel intracranial data decoding and interpretability.</description><author>Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, Andrei Barbu</author><pubDate>Wed, 09 Oct 2024 17:07:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03044v2</guid></item><item><title>Data Selection via Optimal Control for Language Models</title><link>http://arxiv.org/abs/2410.07064v1</link><description>This work investigates the selection of high-quality pre-training data frommassive corpora to enhance LMs' capabilities for downstream usage. We formulatedata selection as a generalized Optimal Control problem, which can be solvedtheoretically by Pontryagin's Maximum Principle (PMP), yielding a set ofnecessary conditions that characterize the relationship between optimal dataselection and LM training dynamics. Based on these theoretical results, weintroduce PMP-based Data Selection (PDS), a framework that approximates optimaldata selection by solving the PMP conditions. In our experiments, we adopt PDSto select data from CommmonCrawl and show that the PDS-selected corpusaccelerates the learning of LMs and constantly boosts their performance on awide range of downstream tasks across various model sizes. Moreover, thebenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced bythe extrapolation of the test loss curves according to the Scaling Laws. PDSalso improves data utilization when the pre-training data is limited, byreducing the data demand by 1.8 times, which mitigates the quick exhaustion ofavailable web-crawled corpora. Our code, data, and model checkpoints can befound in https://github.com/microsoft/LMOps/tree/main/data_selection.</description><author>Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, Minlie Huang</author><pubDate>Wed, 09 Oct 2024 17:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07064v1</guid></item><item><title>InAttention: Linear Context Scaling for Transformers</title><link>http://arxiv.org/abs/2410.07063v1</link><description>VRAM requirements for transformer models scale quadratically with contextlength due to the self-attention mechanism. In this paper we modify thedecoder-only transformer, replacing self-attention with InAttention, whichscales linearly with context length during inference by having tokens attendonly to initial states. Benchmarking shows that InAttention significantlyreduces VRAM usage during inference, enabling handling of long sequences onconsumer GPUs. We corroborate that fine-tuning extends context lengthefficiently, improving performance on long sequences without high trainingcosts. InAttention offers a scalable solution for long-range dependencies intransformer models, paving the way for further optimization.</description><author>Joseph Eisner</author><pubDate>Wed, 09 Oct 2024 17:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07063v1</guid></item><item><title>TinyEmo: Scaling down Emotional Reasoning via Metric Projection</title><link>http://arxiv.org/abs/2410.07062v1</link><description>This paper introduces TinyEmo, a family of small multi-modal language modelsfor emotional reasoning and classification. Our approach features: (1) asynthetic emotional instruct dataset for both pre-training and fine-tuningstages, (2) a Metric Projector that delegates classification from the languagemodel allowing for more efficient training and inference, (3) a multi-modallarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automatedframework for bias detection. TinyEmo is able to perform emotion classificationand emotional reasoning, all while using substantially fewer parameters thancomparable models. This efficiency allows us to freely incorporate more diverseemotional datasets, enabling strong performance on classification tasks, withour smallest model (700M parameters) outperforming larger state-of-the-artmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,the Metric Projector allows for interpretability and indirect bias detection inlarge models without additional training, offering an approach to understandand improve AI systems. We release code, models, and dataset at https://github.com/ggcr/TinyEmo</description><author>Cristian Gutierrez</author><pubDate>Wed, 09 Oct 2024 17:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07062v1</guid></item><item><title>Online Epsilon Net and Piercing Set for Geometric Concepts</title><link>http://arxiv.org/abs/2410.07059v1</link><description>VC-dimension and $\varepsilon$-nets are key concepts in Statistical LearningTheory. Intuitively, VC-dimension is a measure of the size of a class of sets.The famous $\varepsilon$-net theorem, a fundamental result in DiscreteGeometry, asserts that if the VC-dimension of a set system is bounded, then asmall sample exists that intersects all sufficiently large sets. In online learning scenarios where data arrives sequentially, theVC-dimension helps to bound the complexity of the set system, and$\varepsilon$-nets ensure the selection of a small representative set. Thissampling framework is crucial in various domains, including spatial dataanalysis, motion planning in dynamic environments, optimization of sensornetworks, and feature extraction in computer vision, among others. Motivated bythese applications, we study the online $\varepsilon$-net problem for geometricconcepts with bounded VC-dimension. While the offline version of this problemhas been extensively studied, surprisingly, there are no known theoreticalresults for the online version to date. We present the first deterministiconline algorithm with an optimal competitive ratio for intervals in$\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimalcompetitive ratio for axis-aligned boxes in $\mathbb{R}^d$, for $d\le 3$.Furthermore, we introduce a novel technique to analyze similar-sized objects ofconstant description complexity in $\mathbb{R}^d$, which may be of independentinterest. Next, we focus on the continuous version of this problem, whereranges of the set system are geometric concepts in $\mathbb{R}^d$ arriving inan online manner, but the universe is the entire space, and the objective is tochoose a small sample that intersects all the ranges.</description><author>Sujoy Bhore, Devdan Dey, Satyam Singh</author><pubDate>Wed, 09 Oct 2024 16:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07059v1</guid></item><item><title>Predictability maximization and the origins of word order harmony</title><link>http://arxiv.org/abs/2408.16570v5</link><description>We address the linguistic problem of the sequential arrangement of a head andits dependents from an information theoretic perspective. In particular, weconsider the optimal placement of a head that maximizes the predictability ofthe sequence. We assume that dependents are statistically independent given ahead, in line with the open-choice principle and the core assumptions ofdependency grammar. We demonstrate the optimality of harmonic order, i.e.,placing the head last maximizes the predictability of the head whereas placingthe head first maximizes the predictability of dependents. We also show thatpostponing the head is the optimal strategy to maximize its predictabilitywhile bringing it forward is the optimal strategy to maximize thepredictability of dependents. We unravel the advantages of the strategy ofmaximizing the predictability of the head over maximizing the predictability ofdependents. Our findings shed light on the placements of the head adopted byreal languages or emerging in different kinds of experiments.</description><author>Ramon Ferrer-i-Cancho</author><pubDate>Wed, 09 Oct 2024 16:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16570v5</guid></item><item><title>Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing</title><link>http://arxiv.org/abs/2410.07054v1</link><description>Large Language Models (LLMs) have recently revolutionized the NLP field,while they still fall short in some specific down-stream tasks. In the work, wefocus on utilizing LLMs to perform machine translation, where we observe thattwo patterns of errors frequently occur and drastically affect the translationquality: language mismatch and repetition. The work sets out to explore thepotential for mitigating these two issues by leveraging model editing methods,e.g., by locating Feed-Forward Network (FFN) neurons or something that areresponsible for the errors and deactivating them in the inference time. We findthat directly applying such methods either limited effect on the targetederrors or has significant negative side-effect on the general translationquality, indicating that the located components may also be crucial forensuring machine translation with LLMs on the rails. To this end, we propose torefine the located components by fetching the intersection of the locatingresults under different language settings, filtering out the aforementionedinformation that is irrelevant to targeted errors. The experiment resultsempirically demonstrate that our methods can effectively reduce the languagemismatch and repetition ratios and meanwhile enhance or keep the generaltranslation quality in most cases.</description><author>Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei</author><pubDate>Wed, 09 Oct 2024 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07054v1</guid></item><item><title>Robots in the Middle: Evaluating LLMs in Dispute Resolution</title><link>http://arxiv.org/abs/2410.07053v1</link><description>Mediation is a dispute resolution method featuring a neutral third-party(mediator) who intervenes to help the individuals resolve their dispute. Inthis paper, we investigate to which extent large language models (LLMs) areable to act as mediators. We investigate whether LLMs are able to analyzedispute conversations, select suitable intervention types, and generateappropriate intervention messages. Using a novel, manually created dataset of50 dispute scenarios, we conduct a blind evaluation comparing LLMs with humanannotators across several key metrics. Overall, the LLMs showed strongperformance, even outperforming our human annotators across dimensions.Specifically, in 62% of the cases, the LLMs chose intervention types that wererated as better than or equivalent to those chosen by humans. Moreover, in 84%of the cases, the intervention messages generated by the LLMs were rated asbetter than or equal to the intervention messages written by humans. LLMslikewise performed favourably on metrics such as impartiality, understandingand contextualization. Our results demonstrate the potential of integrating AIin online dispute resolution (ODR) platforms.</description><author>Jinzhe Tan, Hannes Westermann, Nikhil Reddy Pottanigari, Jaromír Šavelka, Sébastien Meeùs, Mia Godet, Karim Benyekhlef</author><pubDate>Wed, 09 Oct 2024 16:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07053v1</guid></item><item><title>CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling</title><link>http://arxiv.org/abs/2312.05412v2</link><description>We introduce a multi-modal diffusion model tailored for the bi-directionalconditional generation of video and audio. We propose a joint contrastivetraining loss to improve the synchronization between visual and auditoryoccurrences. We present experiments on two datasets to evaluate the efficacy ofour proposed model. The assessment of generation quality and alignmentperformance is carried out from various angles, encompassing both objective andsubjective metrics. Our findings demonstrate that the proposed modeloutperforms the baseline in terms of quality and generation speed throughintroduction of our novel cross-modal easy fusion architectural block.Furthermore, the incorporation of the contrastive loss results in improvementsin audio-visual alignment, particularly in the high-correlation video-to-audiogeneration task.</description><author>Ruihan Yang, Hannes Gamper, Sebastian Braun</author><pubDate>Wed, 09 Oct 2024 16:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05412v2</guid></item><item><title>S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning</title><link>http://arxiv.org/abs/2410.07046v1</link><description>Recently, differentiable mask pruning methods optimize the continuousrelaxation architecture (soft network) as the proxy of the pruned discretenetwork (hard network) for superior sub-architecture search. However, due tothe agnostic impact of the discretization process, the hard network struggleswith the equivalent representational capacity as the soft network, namelydiscretization gap, which severely spoils the pruning performance. In thispaper, we first investigate the discretization gap and propose a novelstructural differentiable mask pruning framework named S2HPruner to bridge thediscretization gap in a one-stage manner. In the training procedure, SH2Prunerforwards both the soft network and its corresponding hard network, thendistills the hard network under the supervision of the soft network. Tooptimize the mask and prevent performance degradation, we propose a decoupledbidirectional knowledge distillation. It blocks the weight updating from thehard to the soft network while maintaining the gradient corresponding to themask. Compared with existing pruning arts, S2HPruner achieves surpassingpruning performance without fine-tuning on comprehensive benchmarks, includingCIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures.Besides, investigation and analysis experiments explain the effectiveness ofS2HPruner. Codes will be released soon.</description><author>Weihao Lin, Shengji Tang, Chong Yu, Peng Ye, Tao Chen</author><pubDate>Wed, 09 Oct 2024 16:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07046v1</guid></item><item><title>Z-upscaling: Optical Flow Guided Frame Interpolation for Isotropic Reconstruction of 3D EM Volumes</title><link>http://arxiv.org/abs/2410.07043v1</link><description>We propose a novel optical flow based approach to enhance the axialresolution of anisotropic 3D EM volumes to achieve isotropic 3D reconstruction.Assuming spatial continuity of 3D biological structures in well aligned EMvolumes, we reasoned that optical flow estimation techniques, often applied fortemporal resolution enhancement in videos, can be utilized. Pixel level motionis estimated between neighboring 2D slices along z, using spatial gradient flowestimates to interpolate and generate new 2D slices resulting in isotropicvoxels. We leverage recent state-of-the-art learning methods for video frameinterpolation and transfer learning techniques, and demonstrate the success ofour approach on publicly available ultrastructure EM volumes.</description><author>Fisseha A. Ferede, Ali Khalighifar, Jaison John, Krishnan Venkataraman, Khaled Khairy</author><pubDate>Wed, 09 Oct 2024 16:34:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07043v1</guid></item><item><title>Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention</title><link>http://arxiv.org/abs/2407.05649v3</link><description>Graph Neural Networks (GNNs) have become important tools for machine learningon graph-structured data. In this paper, we explore the synergistic combinationof graph encoding, graph rewiring, and graph attention, by introducing GraphAttention with Stochastic Structures (GRASS), a novel GNN architecture. GRASSutilizes relative random walk probabilities (RRWP) encoding and a noveldecomposed variant (D-RRWP) to efficiently capture structural information. Itrewires the input graph by superimposing a random regular graph to enhancelong-range information propagation. It also employs a novel additive attentionmechanism tailored for graph-structured data. Our empirical evaluationsdemonstrate that GRASS achieves state-of-the-art performance on multiplebenchmark datasets, including a 20.3% reduction in mean absolute error on theZINC dataset.</description><author>Tongzhou Liao, Barnabás Póczos</author><pubDate>Wed, 09 Oct 2024 16:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05649v3</guid></item><item><title>Emergent properties with repeated examples</title><link>http://arxiv.org/abs/2410.07041v1</link><description>We study the performance of transformers as a function of the number ofrepetitions of training examples with algorithmically generated datasets. Onthree problems of mathematics: the greatest common divisor, modularmultiplication, and matrix eigenvalues, we show that for a fixed number oftraining steps, models trained on smaller sets of repeated examples outperformmodels trained on larger sets of single-use examples. We also demonstrate thattwo-set training - repeated use of a small random subset of examples, alongnormal sampling on the rest of the training set - provides for faster learningand better performance. This highlights that the benefits of repetition canoutweigh those of data diversity. These datasets and problems provide acontrolled setting to shed light on the still poorly understood interplaybetween generalization and memorization in deep learning.</description><author>François Charton, Julia Kempe</author><pubDate>Wed, 09 Oct 2024 16:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07041v1</guid></item><item><title>A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs</title><link>http://arxiv.org/abs/2311.10610v3</link><description>Large-scale graph machine learning is challenging as the complexity oflearning models scales with the graph size. Subsampling the graph is a viablealternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.Existing graph sampling techniques require not only computing the spectra oflarge matrices but also repeating these computations when the graph changes,e.g., grows. In this paper, we introduce a signal sampling theory for a type ofgraph limit -- the graphon. We prove a Poincar\'e inequality for graphonsignals and show that complements of node subsets satisfying this inequalityare unique sampling sets for Paley-Wiener spaces of graphon signals. Exploitingconnections with spectral clustering and Gaussian elimination, we prove thatsuch sampling sets are consistent in the sense that unique sampling sets on aconvergent graph sequence converge to unique sampling sets on the graphon. Wethen propose a related graphon signal sampling algorithm for large graphs, anddemonstrate its good empirical performance on graph machine learning tasks.</description><author>Thien Le, Luana Ruiz, Stefanie Jegelka</author><pubDate>Wed, 09 Oct 2024 16:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10610v3</guid></item><item><title>Distributionally Robust Clustered Federated Learning: A Case Study in Healthcare</title><link>http://arxiv.org/abs/2410.07039v1</link><description>In this paper, we address the challenge of heterogeneous data distributionsin cross-silo federated learning by introducing a novel algorithm, which weterm Cross-silo Robust Clustered Federated Learning (CS-RCFL). Our approachleverages the Wasserstein distance to construct ambiguity sets around eachclient's empirical distribution that capture possible distribution shifts inthe local data, enabling evaluation of worst-case model performance. We thenpropose a model-agnostic integer fractional program to determine the optimaldistributionally robust clustering of clients into coalitions so that possiblebiases in the local models caused by statistically heterogeneous clientdatasets are avoided, and analyze our method for linear and logistic regressionmodels. Finally, we discuss a federated learning protocol that ensures theprivacy of client distributions, a critical consideration, for instance, whenclients are healthcare institutions. We evaluate our algorithm on synthetic andreal-world healthcare data.</description><author>Xenia Konti, Hans Riess, Manos Giannopoulos, Yi Shen, Michael J. Pencina, Nicoleta J. Economou-Zavlanos, Michael M. Zavlanos</author><pubDate>Wed, 09 Oct 2024 16:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07039v1</guid></item><item><title>PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness</title><link>http://arxiv.org/abs/2410.07035v1</link><description>Large Language Models (LLMs) demonstrate impressive capabilities acrossvarious domains, including role-playing, creative writing, mathematicalreasoning, and coding. Despite these advancements, LLMs still encounterchallenges with length control, frequently failing to adhere to specific lengthconstraints due to their token-level operations and insufficient training ondata with strict length limitations. We identify this issue as stemming from alack of positional awareness and propose novel approaches--PositionID Promptingand PositionID Fine-Tuning--to address it. These methods enhance the model'sability to continuously monitor and manage text length during generation.Additionally, we introduce PositionID CP Prompting to enable LLMs to performcopy and paste operations accurately. Furthermore, we develop two benchmarksfor evaluating length control and copy-paste abilities. Our experimentsdemonstrate that our methods significantly improve the model's adherence tolength constraints and copy-paste accuracy without compromising responsequality.</description><author>Zekun Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu</author><pubDate>Wed, 09 Oct 2024 16:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07035v1</guid></item><item><title>Clean Evaluations on Contaminated Visual Language Models</title><link>http://arxiv.org/abs/2410.07030v1</link><description>How to evaluate large language models (LLMs) cleanly has been established asan important research era to genuinely report the performance of possiblycontaminated LLMs. Yet, how to cleanly evaluate the visual language models(VLMs) is an under-studied problem. We propose a novel approach to achieve suchgoals through data augmentation methods on the visual input information. Wethen craft a new visual clean evaluation benchmark with thousands of datainstances. Through extensive experiments, we found that the traditional visualdata augmentation methods are useful, but they are at risk of being used as apart of the training data as a workaround. We further propose using BGRaugmentation to switch the colour channel of the visual information. We foundthat it is a simple yet effective method for reducing the effect of datacontamination and fortunately, it is also harmful to be used as a dataaugmentation method during training. It means that it is hard to integrate suchdata augmentation into training by malicious trainers and it could be apromising technique to cleanly evaluate visual LLMs. Our code, data, and modelweights will be released upon publication.</description><author>Hongyuan Lu, Shujie Miao, Wai Lam</author><pubDate>Wed, 09 Oct 2024 16:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07030v1</guid></item><item><title>Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models</title><link>http://arxiv.org/abs/2405.15234v3</link><description>Diffusion models (DMs) have achieved remarkable success in text-to-imagegeneration, but they also pose safety risks, such as the potential generationof harmful content and copyright violations. The techniques of machineunlearning, also known as concept erasing, have been developed to address theserisks. However, these techniques remain vulnerable to adversarial promptattacks, which can prompt DMs post-unlearning to regenerate undesired imagescontaining concepts (such as nudity) meant to be erased. This work aims toenhance the robustness of concept erasing by integrating the principle ofadversarial training (AT) into machine unlearning, resulting in the robustunlearning framework referred to as AdvUnlearn. However, achieving thiseffectively and efficiently is highly nontrivial. First, we find that astraightforward implementation of AT compromises DMs' image generation qualitypost-unlearning. To address this, we develop a utility-retaining regularizationon an additional retain set, optimizing the trade-off between concept erasurerobustness and model utility in AdvUnlearn. Moreover, we identify the textencoder as a more suitable module for robustification compared to UNet,ensuring unlearning effectiveness. And the acquired text encoder can serve as aplug-and-play robust unlearner for various DM types. Empirically, we performextensive experiments to demonstrate the robustness advantage of AdvUnlearnacross various DM unlearning scenarios, including the erasure of nudity,objects, and style concepts. In addition to robustness, AdvUnlearn alsoachieves a balanced tradeoff with model utility. To our knowledge, this is thefirst work to systematically explore robust DM unlearning through AT, settingit apart from existing methods that overlook robustness in concept erasing.Codes are available at: https://github.com/OPTML-Group/AdvUnlearn</description><author>Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu</author><pubDate>Wed, 09 Oct 2024 16:12:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15234v3</guid></item><item><title>Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback</title><link>http://arxiv.org/abs/2410.07025v1</link><description>Radiologists play a crucial role by translating medical images into medicalreports. However, the field faces staffing shortages and increasing workloads.While automated approaches using vision-language models (VLMs) show promise asassistants, they require exceptionally high accuracy. Most current VLMs inradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in thegeneral domain, additional preference fine-tuning has become standard practice.The challenge in radiology lies in the prohibitive cost of obtainingradiologist feedback. We propose a scalable automated preference alignmenttechnique for VLMs in radiology, focusing on chest X-ray (CXR) reportgeneration. Our method leverages publicly available datasets with anLLM-as-a-Judge mechanism, eliminating the need for additional expertradiologist feedback. We evaluate and benchmark five direct alignmentalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREENscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase inan average across six metrics (domain specific and general), compared to theSFT baseline. We study reward overoptimization via length exploitation, withreports lengthening by up to 3.2x. To assess a potential alignment tax, webenchmark on six additional diverse tasks, finding no significant degradations.A reader study involving four board-certified radiologists indicates win ratesof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.Our analysis provides actionable insights for the development of VLMs inhigh-stakes fields like radiology.</description><author>Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari</author><pubDate>Wed, 09 Oct 2024 16:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07025v1</guid></item></channel></rss>