<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 15 Sep 2023 06:00:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects</title><link>http://arxiv.org/abs/2309.07921v1</link><description>We introduce OpenIllumination, a real-world dataset containing over 108Kimages of 64 objects with diverse materials, captured under 72 camera views anda large number of different illuminations. For each image in the dataset, weprovide accurate camera parameters, illumination ground truth, and foregroundsegmentation masks. Our dataset enables the quantitative evaluation of mostinverse rendering and material decomposition methods for real objects. Weexamine several state-of-the-art inverse rendering methods on our dataset andcompare their performances. The dataset and code can be found on the projectpage: https://oppo-us-research.github.io/OpenIllumination.</description><author>Isabella Liu, Linghao Chen, Ziyang Fu, Liwen Wu, Haian Jin, Zhong Li, Chin Ming Ryan Wong, Yi Xu, Ravi Ramamoorthi, Zexiang Xu, Hao Su</author><pubDate>Thu, 14 Sep 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07921v1</guid></item><item><title>Large-Vocabulary 3D Diffusion Model with Transformer</title><link>http://arxiv.org/abs/2309.07920v1</link><description>Creating diverse and high-quality 3D assets with an automatic generativemodel is highly desirable. Despite extensive efforts on 3D generation, mostexisting works focus on the generation of a single category or a fewcategories. In this paper, we introduce a diffusion-based feed-forwardframework for synthesizing massive categories of real-world 3D objects with asingle generative model. Notably, there are three major challenges for thislarge-vocabulary 3D generation: a) the need for expressive yet efficient 3Drepresentation; b) large diversity in geometry and texture across categories;c) complexity in the appearances of real-world objects. To this end, we proposea novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, forhandling challenges via three aspects. 1) Considering efficiency androbustness, we adopt a revised triplane representation and improve the fittingspeed and accuracy. 2) To handle the drastic variations in geometry andtexture, we regard the features of all 3D objects as a combination ofgeneralized 3D knowledge and specialized 3D features. To extract generalized 3Dknowledge from diverse categories, we propose a novel 3D-aware transformer withshared cross-plane attention. It learns the cross-plane relations acrossdifferent planes and aggregates the generalized 3D knowledge with specialized3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhancethe generalized 3D knowledge in the encoded triplanes for handling categorieswith complex appearances. Extensive experiments on ShapeNet and OmniObject3D(over 200 diverse real-world categories) convincingly demonstrate that a singleDiffTF model achieves state-of-the-art large-vocabulary 3D object generationperformance with large diversity, rich semantics, and high quality.</description><author>Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, Ziwei Liu</author><pubDate>Thu, 14 Sep 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07920v1</guid></item><item><title>Unified Human-Scene Interaction via Prompted Chain-of-Contacts</title><link>http://arxiv.org/abs/2309.07918v1</link><description>Human-Scene Interaction (HSI) is a vital component of fields like embodied AIand virtual reality. Despite advancements in motion quality and physicalplausibility, two pivotal factors, versatile interaction control and thedevelopment of a user-friendly interface, require further exploration beforethe practical application of HSI. This paper presents a unified HSI framework,UniHSI, which supports unified control of diverse interactions through languagecommands. This framework is built upon the definition of interaction as Chainof Contacts (CoC): steps of human joint-object part pairs, which is inspired bythe strong correlation between interaction types and human-object contactregions. Based on the definition, UniHSI constitutes a Large Language Model(LLM) Planner to translate language prompts into task plans in the form of CoC,and a Unified Controller that turns CoC into uniform task execution. Tofacilitate training and evaluation, we collect a new dataset named ScenePlanthat encompasses thousands of task plans generated by LLMs based on diversescenarios. Comprehensive experiments demonstrate the effectiveness of ourframework in versatile task execution and generalizability to real scannedscenes. The project page is at https://github.com/OpenRobotLab/UniHSI .</description><author>Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, Jiangmiao Pang</author><pubDate>Thu, 14 Sep 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07918v1</guid></item><item><title>Looking at words and points with attention: a benchmark for text-to-shape coherence</title><link>http://arxiv.org/abs/2309.07917v1</link><description>While text-conditional 3D object generation and manipulation have seen rapidprogress, the evaluation of coherence between generated 3D shapes and inputtextual descriptions lacks a clear benchmark. The reason is twofold: a) the lowquality of the textual descriptions in the only publicly available dataset oftext-shape pairs; b) the limited effectiveness of the metrics used toquantitatively assess such coherence. In this paper, we propose a comprehensivesolution that addresses both weaknesses. Firstly, we employ large languagemodels to automatically refine textual descriptions associated with shapes.Secondly, we propose a quantitative metric to assess text-to-shape coherence,through cross-attention mechanisms. To validate our approach, we conduct a userstudy and compare quantitatively our metric with existing ones. The refineddataset, the new metric and a set of text-shape pairs validated by the userstudy comprise a novel, fine-grained benchmark that we publicly release tofoster research on text-to-shape coherence of text-conditioned 3D generativemodels. Benchmark available athttps://cvlab-unibo.github.io/CrossCoherence-Web/.</description><author>Andrea Amaduzzi, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano</author><pubDate>Thu, 14 Sep 2023 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07917v1</guid></item><item><title>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</title><link>http://arxiv.org/abs/2309.07915v1</link><description>Starting from the resurgence of deep learning, vision-language models (VLMs)benefiting from large language models (LLMs) have never been so popular.However, while LLMs can utilize extensive background knowledge and taskinformation with in-context learning, most VLMs still struggle withunderstanding complex multi-modal prompts with multiple images. The issue cantraced back to the architectural design of VLMs or pre-training data.Specifically, the current VLMs primarily emphasize utilizing multi-modal datawith a single image some, rather than multi-modal prompts with interleavedmultiple images and text. Even though some newly proposed VLMs could handleuser prompts with multiple images, pre-training data does not provide moresophisticated multi-modal prompts than interleaved image and text crawled fromthe web. We propose MMICL to address the issue by considering both the modeland data perspectives. We introduce a well-designed architecture capable ofseamlessly integrating visual and textual context in an interleaved manner andMIC dataset to reduce the gap between the training data and the complex userprompts in real-world applications, including: 1) multi-modal context withinterleaved images and text, 2) textual references for each image, and 3)multi-image data with spatial, logical, or temporal relationships. Ourexperiments confirm that MMICL achieves new stat-of-the-art zero-shot andfew-shot performance on a wide range of general vision-language tasks,especially for complex reasoning benchmarks including MME and MMBench. Ouranalysis demonstrates that MMICL effectively deals with the challenge ofcomplex multi-modal prompt understanding. The experiments on ScienceQA-IMG alsoshow that MMICL successfully alleviates the issue of language bias in VLMs,which we believe is the reason behind the advanced performance of MMICL.</description><author>Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang</author><pubDate>Thu, 14 Sep 2023 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07915v1</guid></item><item><title>ALWOD: Active Learning for Weakly-Supervised Object Detection</title><link>http://arxiv.org/abs/2309.07914v1</link><description>Object detection (OD), a crucial vision task, remains challenged by the lackof large training datasets with precise object localization labels. In thiswork, we propose ALWOD, a new framework that addresses this problem by fusingactive learning (AL) with weakly and semi-supervised object detectionparadigms. Because the performance of AL critically depends on the modelinitialization, we propose a new auxiliary image generator strategy thatutilizes an extremely small labeled set, coupled with a large weakly tagged setof images, as a warm-start for AL. We then propose a new AL acquisitionfunction, another critical factor in AL success, that leverages thestudent-teacher OD pair disagreement and uncertainty to effectively propose themost informative images to annotate. Finally, to complete the AL loop, weintroduce a new labeling task delegated to human annotators, based on selectionand correction of model-proposed detections, which is both rapid and effectivein labeling the informative images. We demonstrate, across several challengingbenchmarks, that ALWOD significantly narrows the gap between the ODs trained onfew partially labeled but strategically selected image instances and those thatrely on the fully-labeled data. Our code is publicly available onhttps://github.com/seqam-lab/ALWOD.</description><author>Yuting Wang, Velibor Ilic, Jiatong Li, Branislav Kisacanin, Vladimir Pavlovic</author><pubDate>Thu, 14 Sep 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07914v1</guid></item><item><title>Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning</title><link>http://arxiv.org/abs/2309.07911v1</link><description>Recently, large-scale pre-trained language-image models like CLIP have shownextraordinary capabilities for understanding spatial contents, but naivelytransferring such models to video recognition still suffers from unsatisfactorytemporal modeling capabilities. Existing methods insert tunable structures intoor in parallel with the pre-trained model, which either requiresback-propagation through the whole pre-trained model and is thusresource-demanding, or is limited by the temporal reasoning capability of thepre-trained structure. In this work, we present DiST, which disentangles thelearning of spatial and temporal aspects of videos. Specifically, DiST uses adual-encoder structure, where a pre-trained foundation model acts as thespatial encoder, and a lightweight network is introduced as the temporalencoder. An integration branch is inserted between the encoders to fusespatio-temporal information. The disentangled spatial and temporal learning inDiST is highly efficient because it avoids the back-propagation of massivepre-trained parameters. Meanwhile, we empirically show that disentangledlearning with an extra network for integration benefits both spatial andtemporal understanding. Extensive experiments on five benchmarks show that DiSTdelivers better performance than existing state-of-the-art methods byconvincing gaps. When pre-training on the large-scale Kinetics-710, we achieve89.7% on Kinetics-400 with a frozen ViT-L model, which verifies the scalabilityof DiST. Codes and models can be found inhttps://github.com/alibaba-mmai-research/DiST.</description><author>Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang</author><pubDate>Thu, 14 Sep 2023 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07911v1</guid></item><item><title>Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget</title><link>http://arxiv.org/abs/2304.10520v2</link><description>Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),efficiently learn a rich representation of the input. However, for adapting todownstream tasks, they require a sufficient amount of labeled data since theirrich features code not only objects but also less relevant image background. Incontrast, Instance Discrimination (ID) methods focus on objects. In this work,we study how to combine the efficiency and scalability of MIM with the abilityof ID to perform downstream classification in the absence of large amounts oflabeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning(MAE-CT), a sequential approach that utilizes the implicit clustering of theNearest Neighbor Contrastive Learning (NNCLR) objective to induce abstractionin the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features suchthat they form semantic clusters of objects without using any labels. Notably,MAE-CT does not rely on hand-crafted augmentations and frequently achieves itsbest performances while using only minimal augmentations (crop &amp; flip).Further, MAE-CT is compute efficient as it requires at most 10% overheadcompared to MAE re-training. Applied to large and huge Vision Transformer (ViT)models, MAE-CT excels over previous self-supervised methods trained on ImageNetin linear probing, k-NN and low-shot classification accuracy as well as inunsupervised clustering accuracy. With ViT-H/16 MAE-CT achieves a newstate-of-the-art in linear probing of 82.2%.</description><author>Johannes Lehner, Benedikt Alkin, Andreas Fürst, Elisabeth Rumetshofer, Lukas Miklautz, Sepp Hochreiter</author><pubDate>Thu, 14 Sep 2023 18:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10520v2</guid></item><item><title>TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting</title><link>http://arxiv.org/abs/2309.07910v1</link><description>Existing volumetric methods for predicting 3D human pose estimation areaccurate, but computationally expensive and optimized for single time-stepprediction. We present TEMPO, an efficient multi-view pose estimation modelthat learns a robust spatiotemporal representation, improving pose accuracywhile also tracking and forecasting human pose. We significantly reducecomputation compared to the state-of-the-art by recurrently computingper-person 2D pose features, fusing both spatial and temporal information intoa single representation. In doing so, our model is able to use spatiotemporalcontext to predict more accurate human poses without sacrificing efficiency. Wefurther use this representation to track human poses over time as well aspredict future poses. Finally, we demonstrate that our model is able togeneralize across datasets without scene-specific fine-tuning. TEMPO achieves10$\%$ better MPJPE with a 33$\times$ improvement in FPS compared to TesseTrackon the challenging CMU Panoptic Studio dataset.</description><author>Rohan Choudhury, Kris Kitani, Laszlo A. Jeni</author><pubDate>Thu, 14 Sep 2023 18:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07910v1</guid></item><item><title>Physically Plausible Full-Body Hand-Object Interaction Synthesis</title><link>http://arxiv.org/abs/2309.07907v1</link><description>We propose a physics-based method for synthesizing dexterous hand-objectinteractions in a full-body setting. While recent advancements have addressedspecific facets of human-object interactions, a comprehensive physics-basedapproach remains a challenge. Existing methods often focus on isolated segmentsof the interaction process and rely on data-driven techniques that may resultin artifacts. In contrast, our proposed method embraces reinforcement learning(RL) and physics simulation to mitigate the limitations of data-drivenapproaches. Through a hierarchical framework, we first learn skill priors forboth body and hand movements in a decoupled setting. The generic skill priorslearn to decode a latent skill embedding into the motion of the underlyingpart. A high-level policy then controls hand-object interactions in thesepretrained latent spaces, guided by task objectives of grasping and 3D targettrajectory following. It is trained using a novel reward function that combinesan adversarial style term with a task reward, encouraging natural motions whilefulfilling the task incentives. Our method successfully accomplishes thecomplete interaction task, from approaching an object to grasping andsubsequent manipulation. We compare our approach against kinematics-basedbaselines and show that it leads to more physically plausible motions.</description><author>Jona Braun, Sammy Christen, Muhammed Kocabas, Emre Aksan, Otmar Hilliges</author><pubDate>Thu, 14 Sep 2023 18:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07907v1</guid></item><item><title>Generative Image Dynamics</title><link>http://arxiv.org/abs/2309.07906v1</link><description>We present an approach to modeling an image-space prior on scene dynamics.Our prior is learned from a collection of motion trajectories extracted fromreal video sequences containing natural, oscillating motion such as trees,flowers, candles, and clothes blowing in the wind. Given a single image, ourtrained model uses a frequency-coordinated diffusion sampling process topredict a per-pixel long-term motion representation in the Fourier domain,which we call a neural stochastic motion texture. This representation can beconverted into dense motion trajectories that span an entire video. Along withan image-based rendering module, these trajectories can be used for a number ofdownstream applications, such as turning still images into seamlessly loopingdynamic videos, or allowing users to realistically interact with objects inreal pictures.</description><author>Zhengqi Li, Richard Tucker, Noah Snavely, Aleksander Holynski</author><pubDate>Thu, 14 Sep 2023 18:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07906v1</guid></item><item><title>Ambiguity-Aware In-Context Learning with Large Language Models</title><link>http://arxiv.org/abs/2309.07900v1</link><description>In-context learning (ICL) i.e. showing LLMs only a few task-specificdemonstrations has led to downstream gains with no task-specific fine-tuningrequired. However, LLMs are sensitive to the choice of prompts, and therefore acrucial research question is how to select good demonstrations for ICL. Oneeffective strategy is leveraging semantic similarity between the ICLdemonstrations and test inputs by using a text retriever, which however issub-optimal as that does not consider the LLM's existing knowledge about thattask. From prior work (Min et al., 2022), we already know that labels pairedwith the demonstrations bias the model predictions. This leads us to ourhypothesis whether considering LLM's existing knowledge about the task,especially with respect to the output label space can help in a betterdemonstration selection strategy. Through extensive experimentation on threetext classification tasks, we find that it is beneficial to not only choosesemantically similar ICL demonstrations but also to choose those demonstrationsthat help resolve the inherent label ambiguity surrounding the test example.Interestingly, we find that including demonstrations that the LLM previouslymis-classified and also fall on the test example's decision boundary, bringsthe most performance gain.</description><author>Lingyu Gao, Aditi Chaudhary, Krishna Srinivasan, Kazuma Hashimoto, Karthik Raman, Michael Bendersky</author><pubDate>Thu, 14 Sep 2023 18:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07900v1</guid></item><item><title>Improving physics-informed DeepONets with hard constraints</title><link>http://arxiv.org/abs/2309.07899v1</link><description>Current physics-informed (standard or operator) neural networks still rely onaccurately learning the initial conditions of the system they are solving. Incontrast, standard numerical methods evolve such initial conditions withoutneeding to learn these. In this study, we propose to improve currentphysics-informed deep learning strategies such that initial conditions do notneed to be learned and are represented exactly in the predicted solution.Moreover, this method guarantees that when a DeepONet is applied multiple timesto time step a solution, the resulting function is continuous.</description><author>Rüdiger Brecht, Dmytro R. Popovych, Alex Bihlo, Roman O. Popovych</author><pubDate>Thu, 14 Sep 2023 18:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07899v1</guid></item><item><title>Machine Learning-Assisted Discovery of Novel Reactor Designs</title><link>http://arxiv.org/abs/2308.08841v2</link><description>Additive manufacturing has enabled the fabrication of advanced reactorgeometries, permitting larger, more complex design spaces. Identifyingpromising configurations within such spaces presents a significant challengefor current approaches. Furthermore, existing parameterisations of reactorgeometries are low-dimensional with expensive optimisation limiting morecomplex solutions. To address this challenge, we establish a machinelearning-assisted approach for the design of the next-generation of chemicalreactors, combining the application of high-dimensional parameterisations,computational fluid dynamics, and multi-fidelity Bayesian optimisation. Weassociate the development of mixing-enhancing vortical flow structures in novelcoiled reactors with performance, and use our approach to identify keycharacteristics of optimal designs. By appealing to fluid mechanicalprinciples, we rationalise the selection of novel design features that lead toexperimental performance improvements of ~60% over conventional designs. Ourresults demonstrate that coupling advanced manufacturing techniques with`augmented-intelligence' approaches can lead to superior design performanceand, consequently, emissions-reduction and sustainability.</description><author>Tom Savage, Nausheen Basha, Jonathan McDonough, Omar K Matar, Ehecatl Antonio del Rio Chanona</author><pubDate>Thu, 14 Sep 2023 18:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08841v2</guid></item><item><title>Investigating HLB control strategies using Genetic Algorithms: A two-orchard model approach with ACP Dispersal</title><link>http://arxiv.org/abs/2309.07895v1</link><description>This study focuses on the use of genetic algorithms to optimize controlparameters in two potential strategies called mechanical and chemical control,for mitigating the spread of Huanglongbing (HLB) in citrus orchards. Bydeveloping a two-orchard model that incorporates the dispersal of the AsianCitrus Psyllid (ACP), the cost functions and objective function are explored toassess the effectiveness of the proposed control strategies. The mobility ofACP is also taken into account to capture the disease dynamics morerealistically. Additionally, a mathematical expression for the globalreproduction number ($R_{0}$) is derived, allowing for sensitivity analysis ofthe model parameters when ACP mobility is present. Furthermore, wemathematically express the cost function and efficiency of the strategy interms of the final size and individual $R_{0}$ of each patch (i.e., when ACPmobility is absent). The results obtained through the genetic algorithms revealoptimal parameters for each control strategy, providing valuable insights fordecision-making in implementing effective control measures against HLB incitrus orchards. This study highlights the importance of optimizing controlparameters in disease management in agriculture and provides a solid foundationfor future research in developing disease control strategies based on geneticalgorithms.</description><author>Andrés Anzo Hernández, Uvencio José Giménez Mujica, Carlos Hernández Gracidas, José Jacobo Oliveros Oliveros</author><pubDate>Thu, 14 Sep 2023 18:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07895v1</guid></item><item><title>Choosing a Proxy Metric from Past Experiments</title><link>http://arxiv.org/abs/2309.07893v1</link><description>In many randomized experiments, the treatment effect of the long-term metric(i.e. the primary outcome of interest) is often difficult or infeasible tomeasure. Such long-term metrics are often slow to react to changes andsufficiently noisy they are challenging to faithfully estimate in short-horizonexperiments. A common alternative is to measure several short-term proxymetrics in the hope they closely track the long-term metric -- so they can beused to effectively guide decision-making in the near-term. We introduce a newstatistical framework to both define and construct an optimal proxy metric foruse in a homogeneous population of randomized experiments. Our procedure firstreduces the construction of an optimal proxy metric in a given experiment to aportfolio optimization problem which depends on the true latent treatmenteffects and noise level of experiment under consideration. We then denoise theobserved treatment effects of the long-term metric and a set of proxies in ahistorical corpus of randomized experiments to extract estimates of the latenttreatment effects for use in the optimization problem. One key insight derivedfrom our approach is that the optimal proxy metric for a given experiment isnot apriori fixed; rather it should depend on the sample size (or effectivenoise level) of the randomized experiment for which it is deployed. Toinstantiate and evaluate our framework, we employ our methodology in a largecorpus of randomized experiments from an industrial recommendation system andconstruct proxy metrics that perform favorably relative to several baselines.</description><author>Nilesh Tripuraneni, Lee Richardson, Alexander D'Amour, Jacopo Soriano, Steve Yadlowsky</author><pubDate>Thu, 14 Sep 2023 18:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07893v1</guid></item><item><title>HandNeRF: Learning to Reconstruct Hand-Object Interaction Scene from a Single RGB Image</title><link>http://arxiv.org/abs/2309.07891v1</link><description>This paper presents a method to learn hand-object interaction prior forreconstructing a 3D hand-object scene from a single RGB image. The inference aswell as training-data generation for 3D hand-object scene reconstruction ischallenging due to the depth ambiguity of a single image and occlusions by thehand and object. We turn this challenge into an opportunity by utilizing thehand shape to constrain the possible relative configuration of the hand andobject geometry. We design a generalizable implicit function, HandNeRF, thatexplicitly encodes the correlation of the 3D hand shape features and 2D objectfeatures to predict the hand and object scene geometry. With experiments onreal-world datasets, we show that HandNeRF is able to reconstruct hand-objectscenes of novel grasp configurations more accurately than comparable methods.Moreover, we demonstrate that object reconstruction from HandNeRF ensures moreaccurate execution of a downstream task, such as grasping for robotichand-over.</description><author>Hongsuk Choi, Nikhil Chavan-Dafle, Jiacheng Yuan, Volkan Isler, Hyunsoo Park</author><pubDate>Thu, 14 Sep 2023 18:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07891v1</guid></item><item><title>A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors</title><link>http://arxiv.org/abs/2309.07888v1</link><description>We present a novel local-global feature fusion framework for body-weightexercise recognition with floor-based dynamic pressure maps. One step furtherfrom the existing studies using deep neural networks mainly focusing on globalfeature extraction, the proposed framework aims to combine local and globalfeatures using image processing techniques and the YOLO object detection tolocalize pressure profiles from different body parts and consider physicalconstraints. The proposed local feature extraction method generates two sets ofhigh-level local features consisting of cropped pressure mapping and numericalfeatures such as angular orientation, location on the mat, and pressure area.In addition, we adopt a knowledge distillation for regularization to preservethe knowledge of the global feature extraction and improve the performance ofthe exercise recognition. Our experimental results demonstrate a notable 11percent improvement in F1 score for exercise recognition while preservinglabel-specific features.</description><author>Davinder Pal Singh, Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz</author><pubDate>Thu, 14 Sep 2023 18:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07888v1</guid></item><item><title>Some notes concerning a generalized KMM-type optimization method for density ratio estimation</title><link>http://arxiv.org/abs/2309.07887v1</link><description>In the present paper we introduce new optimization algorithms for the task ofdensity ratio estimation. More precisely, we consider extending the well-knownKMM method using the construction of a suitable loss function, in order toencompass more general situations involving the estimation of density ratiowith respect to subsets of the training data and test data, respectively. Theassociated codes can be found at https://github.com/CDAlecsa/Generalized-KMM.</description><author>Cristian Daniel Alecsa</author><pubDate>Thu, 14 Sep 2023 18:36:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07887v1</guid></item><item><title>Kernel Conditional Moment Constraints for Confounding Robust Inference</title><link>http://arxiv.org/abs/2302.13348v2</link><description>We study policy evaluation of offline contextual bandits subject tounobserved confounders. Sensitivity analysis methods are commonly used toestimate the policy value under the worst-case confounding over a givenuncertainty set. However, existing work often resorts to some coarse relaxationof the uncertainty set for the sake of tractability, leading to overlyconservative estimation of the policy value. In this paper, we propose ageneral estimator that provides a sharp lower bound of the policy value. It canbe shown that our estimator contains the recently proposed sharp estimator byDorn and Guo (2022) as a special case, and our method enables a novel extensionof the classical marginal sensitivity model using f-divergence. To constructour estimator, we leverage the kernel method to obtain a tractableapproximation to the conditional moment constraints, which traditionalnon-sharp estimators failed to take into account. In the theoretical analysis,we provide a condition for the choice of the kernel which guarantees nospecification error that biases the lower bound estimation. Furthermore, weprovide consistency guarantees of policy evaluation and learning. In theexperiments with synthetic and real-world data, we demonstrate theeffectiveness of the proposed method.</description><author>Kei Ishikawa, Niao He</author><pubDate>Thu, 14 Sep 2023 18:31:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13348v2</guid></item><item><title>Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats</title><link>http://arxiv.org/abs/2308.01921v2</link><description>Fast screening of drug molecules based on the ligand binding affinity is animportant step in the drug discovery pipeline. Graph neural fingerprint is apromising method for developing molecular docking surrogates with highthroughput and great fidelity. In this study, we built a COVID-19 drug dockingdataset of about 300,000 drug candidates on 23 coronavirus protein targets.With this dataset, we trained graph neural fingerprint docking models forhigh-throughput virtual COVID-19 drug screening. The graph neural fingerprintmodels yield high prediction accuracy on docking scores with the mean squarederror lower than $0.21$ kcal/mol for most of the docking targets, showingsignificant improvement over conventional circular fingerprint methods. To makethe neural fingerprints transferable for unknown targets, we also propose atransferable graph neural fingerprint method trained on multiple targets. Withcomparable accuracy to target-specific graph neural fingerprint models, thetransferable model exhibits superb training and data efficiency. We highlightthat the impact of this study extends beyond COVID-19 dataset, as our approachfor fast virtual ligand screening can be easily adapted and integrated into ageneral machine learning-accelerated pipeline to battle future bio-threats.</description><author>Wei Chen, Yihui Ren, Ai Kagawa, Matthew R. Carbone, Samuel Yen-Chi Chen, Xiaohui Qu, Shinjae Yoo, Austin Clyde, Arvind Ramanathan, Rick L. Stevens, Hubertus J. J. van Dam, Deyu Liu</author><pubDate>Thu, 14 Sep 2023 18:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01921v2</guid></item><item><title>Scalable Model-Based Gaussian Process Clustering</title><link>http://arxiv.org/abs/2309.07882v1</link><description>Gaussian process is an indispensable tool in clustering functional data,owing to it's flexibility and inherent uncertainty quantification. However,when the functional data is observed over a large grid (say, of length $p$),Gaussian process clustering quickly renders itself infeasible, incurring$O(p^2)$ space complexity and $O(p^3)$ time complexity per iteration; and thusprohibiting it's natural adaptation to large environmental applications. Toensure scalability of Gaussian process clustering in such applications, wepropose to embed the popular Vecchia approximation for Gaussian processes atthe heart of the clustering task, provide crucial theoretical insights towardsalgorithmic design, and finally develop a computationally efficient expectationmaximization (EM) algorithm. Empirical evidence of the utility of our proposalis provided via simulations and analysis of polar temperature anomaly(\href{https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/global/time-series}{noaa.gov})data-sets.</description><author>Anirban Chakraborty, Abhisek Chakraborty</author><pubDate>Thu, 14 Sep 2023 18:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07882v1</guid></item><item><title>mEBAL2 Database and Benchmark: Image-based Multispectral Eyeblink Detection</title><link>http://arxiv.org/abs/2309.07880v1</link><description>This work introduces a new multispectral database and novel approaches foreyeblink detection in RGB and Near-Infrared (NIR) individual images. Ourcontributed dataset (mEBAL2, multimodal Eye Blink and Attention Levelestimation, Version 2) is the largest existing eyeblink database, representinga great opportunity to improve data-driven multispectral approaches for blinkdetection and related applications (e.g., attention level estimation andpresentation attack detection in face biometrics). mEBAL2 includes 21,100 imagesequences from 180 different students (more than 2 million labeled images intotal) while conducting a number of e-learning tasks of varying difficulty ortaking a real course on HTML initiation through the edX MOOC platform. mEBAL2uses multiple sensors, including two Near-Infrared (NIR) and one RGB camera tocapture facial gestures during the execution of the tasks, as well as anElectroencephalogram (EEG) band to get the cognitive activity of the user andblinking events. Furthermore, this work proposes a Convolutional Neural Networkarchitecture as benchmark for blink detection on mEBAL2 with performances up to97%. Different training methodologies are implemented using the RGB spectrum,NIR spectrum, and the combination of both to enhance the performance onexisting eyeblink detectors. We demonstrate that combining NIR and RGB imagesduring training improves the performance of RGB eyeblink detectors (i.e.,detection based only on a RGB image). Finally, the generalization capacity ofthe proposed eyeblink detectors is validated in wilder and more challengingenvironments like the HUST-LEBW dataset to show the usefulness of mEBAL2 totrain a new generation of data-driven approaches for eyeblink detection.</description><author>Roberto Daza, Aythami Morales, Julian Fierrez, Ruben Tolosana, Ruben Vera-Rodriguez</author><pubDate>Thu, 14 Sep 2023 18:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07880v1</guid></item><item><title>Using network metrics to explore the community structure that underlies movement patterns</title><link>http://arxiv.org/abs/2309.07878v1</link><description>This work aims to explore the community structure of Santiago de Chile byanalyzing the movement patterns of its residents. We use a dataset containingthe approximate locations of home and work places for a subset of anonymizedresidents to construct a network that represents the movement patterns withinthe city. Through the analysis of this network, we aim to identify thecommunities or sub-cities that exist within Santiago de Chile and gain insightsinto the factors that drive the spatial organization of the city. We employmodularity optimization algorithms and clustering techniques to identify thecommunities within the network. Our results present that the novelty ofcombining community detection algorithms with segregation tools provides newinsights to further the understanding of the complex geography of segregationduring working hours.</description><author>Anh Pham Thi Minh, Abhishek Kumar Singh, Soumya Snigdha Kundu</author><pubDate>Thu, 14 Sep 2023 18:24:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07878v1</guid></item><item><title>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions</title><link>http://arxiv.org/abs/2309.07875v1</link><description>Training large language models to follow instructions makes them performbetter on a wide range of tasks, generally becoming more helpful. However, aperfectly helpful model will follow even the most malicious instructions andreadily generate harmful content. In this paper, we raise concerns over thesafety of models that only emphasize helpfulness, not safety, in theirinstruction-tuning. We show that several popular instruction-tuned models arehighly unsafe. Moreover, we show that adding just 3% safety examples (a fewhundred demonstrations) in the training set when fine-tuning a model like LLaMAcan substantially improve their safety. Our safety-tuning does not make modelssignificantly less capable or helpful as measured by standard benchmarks.However, we do find a behavior of exaggerated safety, where too muchsafety-tuning makes models refuse to respond to reasonable prompts thatsuperficially resemble unsafe ones. Our study sheds light on trade-offs intraining LLMs to follow instructions and exhibit safe behavior.</description><author>Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou</author><pubDate>Thu, 14 Sep 2023 18:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07875v1</guid></item><item><title>Agents: An Open-source Framework for Autonomous Language Agents</title><link>http://arxiv.org/abs/2309.07870v1</link><description>Recent advances on large language models (LLMs) enable researchers anddevelopers to build autonomous language agents that can automatically solvevarious tasks and interact with environments, humans, and other agents usingnatural language interfaces. We consider language agents as a promisingdirection towards artificial general intelligence and release Agents, anopen-source library with the goal of opening up these advances to a widernon-specialist audience. Agents is carefully engineered to support importantfeatures including planning, memory, tool usage, multi-agent communication, andfine-grained symbolic control. Agents is user-friendly as it enablesnon-specialists to build, customize, test, tune, and deploy state-of-the-artautonomous language agents without much coding. The library is alsoresearch-friendly as its modularized design makes it easily extensible forresearchers. Agents is available at https://github.com/aiwaves-cn/agents.</description><author>Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, Mrinmaya Sachan</author><pubDate>Thu, 14 Sep 2023 18:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07870v1</guid></item><item><title>Beta Diffusion</title><link>http://arxiv.org/abs/2309.07867v1</link><description>We introduce beta diffusion, a novel generative modeling method thatintegrates demasking and denoising to generate data within bounded ranges.Using scaled and shifted beta distributions, beta diffusion utilizesmultiplicative transitions over time to create both forward and reversediffusion processes, maintaining beta distributions in both the forwardmarginals and the reverse conditionals, given the data at any point in time.Unlike traditional diffusion-based generative models relying on additiveGaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion ismultiplicative and optimized with KL-divergence upper bounds (KLUBs) derivedfrom the convexity of the KL divergence. We demonstrate that the proposed KLUBsare more effective for optimizing beta diffusion compared to negative ELBOs,which can also be derived as the KLUBs of the same KL divergence with its twoarguments swapped. The loss function of beta diffusion, expressed in terms ofBregman divergence, further supports the efficacy of KLUBs for optimization.Experimental results on both synthetic data and natural images demonstrate theunique capabilities of beta diffusion in generative modeling of range-boundeddata and validate the effectiveness of KLUBs in optimizing diffusion models,thereby making them valuable additions to the family of diffusion-basedgenerative models and the optimization techniques used to train them.</description><author>Mingyuan Zhou, Tianqi Chen, Zhendong Wang, Huangjie Zheng</author><pubDate>Thu, 14 Sep 2023 18:14:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07867v1</guid></item><item><title>Gradient constrained sharpness-aware prompt learning for vision-language models</title><link>http://arxiv.org/abs/2309.07866v1</link><description>This paper targets a novel trade-off problem in generalizable prompt learningfor vision-language models (VLM), i.e., improving the performance on unseenclasses while maintaining the performance on seen classes. Comparing withexisting generalizable methods that neglect the seen classes degradation, thesetting of this problem is more strict and fits more closely with practicalapplications. To solve this problem, we start from the optimizationperspective, and leverage the relationship between loss landscape geometry andmodel generalization ability. By analyzing the loss landscape of thestate-of-the-art method and the widely-used Sharpness-aware Minimization (SAM),we conclude that the trade-off performance correlates to both loss value andloss sharpness, while each of them are indispensable. However, we find theoptimizing gradient of existing methods cannot always maintain high consistencywith both loss value and loss sharpness during the whole optimizationprocedure. To this end, we propose an novel SAM-based method for promptlearning, denoted as Gradient Constrained Sharpness-aware Context Optimization(GCSCoOp), to dynamically constrains the optimizing gradient, thus achievingabove two-fold optimization objective simultaneously. Extensive experimentsverify the effectiveness of GCSCoOp in the trade-off problem.</description><author>Liangchen Liu, Nannan Wang, Dawei Zhou, Xinbo Gao, Decheng Liu, Xi Yang, Tongliang Liu</author><pubDate>Thu, 14 Sep 2023 18:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07866v1</guid></item><item><title>The Rise and Potential of Large Language Model Based Agents: A Survey</title><link>http://arxiv.org/abs/2309.07864v1</link><description>For a long time, humanity has pursued artificial intelligence (AI) equivalentto or surpassing the human level, with AI agents considered a promising vehiclefor this pursuit. AI agents are artificial entities that sense theirenvironment, make decisions, and take actions. Many efforts have been made todevelop intelligent AI agents since the mid-20th century. However, theseefforts have mainly focused on advancement in algorithms or training strategiesto enhance specific capabilities or performance on particular tasks. Actually,what the community lacks is a sufficiently general and powerful model to serveas a starting point for designing AI agents that can adapt to diversescenarios. Due to the versatile and remarkable capabilities they demonstrate,large language models (LLMs) are regarded as potential sparks for ArtificialGeneral Intelligence (AGI), offering hope for building general AI agents. Manyresearch efforts have leveraged LLMs as the foundation to build AI agents andhave achieved significant progress. We start by tracing the concept of agentsfrom its philosophical origins to its development in AI, and explain why LLMsare suitable foundations for AI agents. Building upon this, we present aconceptual framework for LLM-based agents, comprising three main components:brain, perception, and action, and the framework can be tailored to suitdifferent applications. Subsequently, we explore the extensive applications ofLLM-based agents in three aspects: single-agent scenarios, multi-agentscenarios, and human-agent cooperation. Following this, we delve into agentsocieties, exploring the behavior and personality of LLM-based agents, thesocial phenomena that emerge when they form societies, and the insights theyoffer for human society. Finally, we discuss a range of key topics and openproblems within the field.</description><author>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, Tao Gui</author><pubDate>Thu, 14 Sep 2023 18:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07864v1</guid></item><item><title>CiwaGAN: Articulatory information exchange</title><link>http://arxiv.org/abs/2309.07861v1</link><description>Humans encode information into sounds by controlling articulators and decodeinformation from sounds using the auditory apparatus. This paper introducesCiwaGAN, a model of human spoken language acquisition that combinesunsupervised articulatory modeling with an unsupervised model of informationexchange through the auditory modality. While prior research includesunsupervised articulatory modeling and information exchange separately, ourmodel is the first to combine the two components. The paper also proposes animproved articulatory model with more interpretable internal representations.The proposed CiwaGAN model is the most realistic approximation of human spokenlanguage acquisition using deep learning. As such, it is useful for cognitivelyplausible simulations of the human speech act.</description><author>Gašper Beguš, Thomas Lu, Alan Zhou, Peter Wu, Gopala K. Anumanchipalli</author><pubDate>Thu, 14 Sep 2023 18:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07861v1</guid></item><item><title>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</title><link>http://arxiv.org/abs/2308.12966v2</link><description>We introduce the Qwen-VL series, a set of large-scale vision-language models(LVLMs) designed to perceive and understand both text and images. ComprisingQwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in taskslike image captioning, question answering, visual localization, and flexibleinteraction. The evaluation covers a wide range of tasks including zero-shotcaptioning, visual or document visual question answering, and grounding. Wedemonstrate the Qwen-VL outperforms existing LVLMs. We present theirarchitecture, training, capabilities, and performance, highlighting theircontributions to advancing multimodal artificial intelligence. Code, demo andmodels are available at https://github.com/QwenLM/Qwen-VL.</description><author>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou</author><pubDate>Thu, 14 Sep 2023 18:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12966v2</guid></item><item><title>Identifying the Group-Theoretic Structure of Machine-Learned Symmetries</title><link>http://arxiv.org/abs/2309.07860v1</link><description>Deep learning was recently successfully used in deriving symmetrytransformations that preserve important physics quantities. Being completelyagnostic, these techniques postpone the identification of the discoveredsymmetries to a later stage. In this letter we propose methods for examiningand identifying the group-theoretic structure of such machine-learnedsymmetries. We design loss functions which probe the subalgebra structureeither during the deep learning stage of symmetry discovery or in a subsequentpost-processing stage. We illustrate the new methods with examples from theU(n) Lie group family, obtaining the respective subalgebra decompositions. Asan application to particle physics, we demonstrate the identification of theresidual symmetries after the spontaneous breaking of non-Abelian gaugesymmetries like SU(3) and SU(5) which are commonly used in model building.</description><author>Roy T. Forestano, Konstantin T. Matchev, Katia Matcheva, Alexander Roman, Eyup B. Unlu, Sarunas Verner</author><pubDate>Thu, 14 Sep 2023 18:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07860v1</guid></item><item><title>Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing</title><link>http://arxiv.org/abs/2309.04612v2</link><description>Feature generation aims to generate new and meaningful features to create adiscriminative representation space.A generated feature is meaningful when thegenerated feature is from a feature pair with inherent feature interaction. Inthe real world, experienced data scientists can identify potentially usefulfeature-feature interactions, and generate meaningful dimensions from anexponentially large search space, in an optimal crossing form over an optimalgeneration path. But, machines have limited human-like abilities.We generalizesuch learning tasks as self-optimizing feature generation. Self-optimizingfeature generation imposes several under-addressed challenges on existingsystems: meaningful, robust, and efficient generation. To tackle thesechallenges, we propose a principled and generic representation-crossingframework to solve self-optimizing feature generation.To achieve hashingrepresentation, we propose a three-step approach: feature discretization,feature hashing, and descriptive summarization. To achieve reinforcementcrossing, we develop a hierarchical reinforcement feature crossing approach.Wepresent extensive experimental results to demonstrate the effectiveness andefficiency of the proposed method. The code is available athttps://github.com/yingwangyang/HRC_feature_cross.git.</description><author>Wangyang Ying, Dongjie Wang, Kunpeng Liu, Leilei Sun, Yanjie Fu</author><pubDate>Thu, 14 Sep 2023 17:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04612v2</guid></item><item><title>ExpertQA: Expert-Curated Questions and Attributed Answers</title><link>http://arxiv.org/abs/2309.07852v1</link><description>As language models are adapted by a more sophisticated and diverse set ofusers, the importance of guaranteeing that they provide factually correctinformation supported by verifiable sources is critical across fields of study&amp; professions. This is especially the case for high-stakes fields, such asmedicine and law, where the risk of propagating false information is high andcan lead to undesirable societal consequences. Previous work studyingfactuality and attribution has not focused on analyzing these characteristicsof language model outputs in domain-specific scenarios. In this work, wepresent an evaluation study analyzing various axes of factuality andattribution provided in responses from a few systems, by bringing domainexperts in the loop. Specifically, we first collect expert-curated questionsfrom 484 participants across 32 fields of study, and then ask the same expertsto evaluate generated responses to their own questions. We also ask experts torevise answers produced by language models, which leads to ExpertQA, ahigh-quality long-form QA dataset with 2177 questions spanning 32 fields, alongwith verified answers and attributions for claims in the answers.</description><author>Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, Dan Roth</author><pubDate>Thu, 14 Sep 2023 17:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07852v1</guid></item><item><title>TFNet: Exploiting Temporal Cues for Fast and Accurate LiDAR Semantic Segmentation</title><link>http://arxiv.org/abs/2309.07849v1</link><description>LiDAR semantic segmentation plays a crucial role in enabling autonomousdriving and robots to understand their surroundings accurately and robustly.There are different types of methods, such as point-based, range image-based,and polar-based. Among these, range image-based methods are widely used due totheir balance between accuracy and speed. However, they face a significantchallenge known as the ``many-to-one'' problem caused by the range image'slimited horizontal and vertical angular resolution, where around 20% of the 3Dpoints are occluded during model inference based on our observation. In thispaper, we present TFNet, a range image-based LiDAR semantic segmentation methodthat utilizes temporal information to address this issue. Specifically, weincorporate a temporal fusion layer to extract useful information from previousscans and integrate it with the current scan. We then design a max-voting-basedpost-processing technique to correct false predictions, particularly thosecaused by the ``many-to-one'' issue. Experiments on two benchmarks and sevenbackbones of three modalities demonstrate the effectiveness and scalability ofour proposed method.</description><author>Rong Li, ShiJie Li, Xieyuanli Chen, Teli Ma, Wang Hao, Juergen Gall, Junwei Liang</author><pubDate>Thu, 14 Sep 2023 17:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07849v1</guid></item><item><title>MC-NeRF: Muti-Camera Neural Radiance Fields for Muti-Camera Image Acquisition Systems</title><link>http://arxiv.org/abs/2309.07846v1</link><description>Neural Radiance Fields (NeRF) employ multi-view images for 3D scenerepresentation and have shown remarkable performance. As one of the primarysources of multi-view images, multi-camera systems encounter challenges such asvarying intrinsic parameters and frequent pose changes. Most previousNeRF-based methods often assume a global unique camera and seldom considerscenarios with multiple cameras. Besides, some pose-robust methods still remainsusceptible to suboptimal solutions when poses are poor initialized. In thispaper, we propose MC-NeRF, a method can jointly optimize both intrinsic andextrinsic parameters for bundle-adjusting Neural Radiance Fields. Firstly, weconduct a theoretical analysis to tackle the degenerate case and coupling issuethat arise from the joint optimization between intrinsic and extrinsicparameters. Secondly, based on the proposed solutions, we introduce anefficient calibration image acquisition scheme for multi-camera systems,including the design of calibration object. Lastly, we present a globalend-to-end network with training sequence that enables the regression ofintrinsic and extrinsic parameters, along with the rendering network. Moreover,most existing datasets are designed for unique camera, we create a new datasetthat includes four different styles of multi-camera acquisition systems,allowing readers to generate custom datasets. Experiments confirm theeffectiveness of our method when each image corresponds to different cameraparameters. Specifically, we adopt up to 110 images with 110 differentintrinsic and extrinsic parameters, to achieve 3D scene representation withoutproviding initial poses. The Code and supplementary materials are available athttps://in2-viaun.github.io/MC-NeRF.</description><author>Yu Gao, Lutong Su, Hao Liang, Yufeng Yue, Yi Yang, Mengyin Fu</author><pubDate>Thu, 14 Sep 2023 17:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07846v1</guid></item><item><title>Applying Deep Learning to Calibrate Stochastic Volatility Models</title><link>http://arxiv.org/abs/2309.07843v1</link><description>Stochastic volatility models, where the volatility is a stochastic process,can capture most of the essential stylized facts of implied volatility surfacesand give more realistic dynamics of the volatility smile or skew. However, theycome with the significant issue that they take too long to calibrate. Alternative calibration methods based on Deep Learning (DL) techniques havebeen recently used to build fast and accurate solutions to the calibrationproblem. Huge and Savine developed a Differential Deep Learning (DDL) approach,where Machine Learning models are trained on samples of not only features andlabels but also differentials of labels to features. The present work aims toapply the DDL technique to price vanilla European options (i.e. the calibrationinstruments), more specifically, puts when the underlying asset follows aHeston model and then calibrate the model on the trained network. DDL allowsfor fast training and accurate pricing. The trained neural network dramaticallyreduces Heston calibration's computation time. In this work, we also introduce different regularisation techniques, and weapply them notably in the case of the DDL. We compare their performance inreducing overfitting and improving the generalisation error. The DDLperformance is also compared to the classical DL (without differentiation) onein the case of Feed-Forward Neural Networks. We show that the DDL outperformsthe DL.</description><author>Abir Sridi, Paul Bilokon</author><pubDate>Thu, 14 Sep 2023 17:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07843v1</guid></item><item><title>SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction</title><link>http://arxiv.org/abs/2305.11322v3</link><description>Spiking neural networks (SNNs) process time-series data via internalevent-driven neural dynamics whose energy consumption depends on the number ofspikes exchanged between neurons over the course of the input presentation. Intypical implementations of an SNN classifier, decisions are produced after theentire input sequence has been processed, resulting in latency and energyconsumption levels that are fairly uniform across inputs. Recently introduceddelay-adaptive SNNs tailor the inference latency -- and, with it, the energyconsumption -- to the difficulty of each example, by producing an earlydecision when the SNN model is sufficiently ``confident''. In this paper, westart by observing that, as an SNN processes input samples, its classificationdecisions tend to be first under-confident and then over-confident with respectto the decision's ground-truth, unknown, test accuracy. This makes it difficultto determine a stopping time that ensures a desired level of accuracy. Toaddress this problem, we introduce a novel delay-adaptive SNN-based inferencemethodology that, wrapping around any pre-trained SNN classifier, providesguaranteed reliability for the decisions produced at input-dependent stoppingtimes. The approach entails minimal added complexity as compared to theunderlying SNN, requiring only thresholding and counting operations at runtime, and it leverages tools from conformal prediction (CP).</description><author>Jiechen Chen, Sangwoo Park, Osvaldo Simeone</author><pubDate>Thu, 14 Sep 2023 17:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11322v3</guid></item><item><title>Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness</title><link>http://arxiv.org/abs/2111.01996v2</link><description>Adversarial robustness, which primarily comprises sensitivity-basedrobustness and spatial robustness, plays an integral part in achieving robustgeneralization. In this paper, we endeavor to design strategies to achieveuniversal adversarial robustness. To achieve this, we first investigate therelatively less-explored realm of spatial robustness. Then, we integrate theexisting spatial robustness methods by incorporating both local and globalspatial vulnerability into a unified spatial attack and adversarial trainingapproach. Furthermore, we present a comprehensive relationship between naturalaccuracy, sensitivity-based robustness, and spatial robustness, supported bystrong evidence from the perspective of robust representation. Crucially, toreconcile the interplay between the mutual impacts of various robustnesscomponents into one unified framework, we incorporate the \textit{Paretocriterion} into the adversarial robustness analysis, yielding a novel strategycalled Pareto Adversarial Training for achieving universal robustness. Theresulting Pareto front, which delineates the set of optimal solutions, providesan optimal balance between natural accuracy and various adversarial robustness.This sheds light on solutions for achieving universal robustness in the future.To the best of our knowledge, we are the first to consider universaladversarial robustness via multi-objective optimization.</description><author>Ke Sun, Mingjie Li, Zhouchen Lin</author><pubDate>Thu, 14 Sep 2023 17:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.01996v2</guid></item><item><title>Two Timin': Repairing Smart Contracts With A Two-Layered Approach</title><link>http://arxiv.org/abs/2309.07841v1</link><description>Due to the modern relevance of blockchain technology, smart contracts presentboth substantial risks and benefits. Vulnerabilities within them can trigger acascade of consequences, resulting in significant losses. Many current papersprimarily focus on classifying smart contracts for malicious intent, oftenrelying on limited contract characteristics, such as bytecode or opcode. Thispaper proposes a novel, two-layered framework: 1) classifying and 2) directlyrepairing malicious contracts. Slither's vulnerability report is combined withsource code and passed through a pre-trained RandomForestClassifier (RFC) andLarge Language Models (LLMs), classifying and repairing each suggestedvulnerability. Experiments demonstrate the effectiveness of fine-tuned andprompt-engineered LLMs. The smart contract repair models, built frompre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overallvulnerability count by 97.5% and 96.7% respectively. A manual inspection ofrepaired contracts shows that all retain functionality, indicating that theproposed method is appropriate for automatic batch classification and repair ofvulnerabilities in smart contracts.</description><author>Abhinav Jain, Ehan Masud, Michelle Han, Rohan Dhillon, Sumukh Rao, Arya Joshi, Salar Cheema, Saurav Kumar</author><pubDate>Thu, 14 Sep 2023 17:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07841v1</guid></item><item><title>Gaussian Process Surrogate Models for Neural Networks</title><link>http://arxiv.org/abs/2208.06028v2</link><description>Not being able to understand and predict the behavior of deep learningsystems makes it hard to decide what architecture and algorithm to use for agiven problem. In science and engineering, modeling is a methodology used tounderstand complex systems whose internal processes are opaque. Modelingreplaces a complex system with a simpler, more interpretable surrogate. Drawinginspiration from this, we construct a class of surrogate models for neuralnetworks using Gaussian processes. Rather than deriving kernels for infiniteneural networks, we learn kernels empirically from the naturalistic behavior offinite neural networks. We demonstrate our approach captures existing phenomenarelated to the spectral bias of neural networks, and then show that oursurrogate models can be used to solve practical problems such as identifyingwhich points most influence the behavior of specific neural networks andpredicting which architectures and algorithms will generalize well for specificdatasets.</description><author>Michael Y. Li, Erin Grant, Thomas L. Griffiths</author><pubDate>Thu, 14 Sep 2023 17:37:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.06028v2</guid></item><item><title>Learning to Warm-Start Fixed-Point Optimization Algorithms</title><link>http://arxiv.org/abs/2309.07835v1</link><description>We introduce a machine-learning framework to warm-start fixed-pointoptimization algorithms. Our architecture consists of a neural network mappingproblem parameters to warm starts, followed by a predefined number offixed-point iterations. We propose two loss functions designed to eitherminimize the fixed-point residual or the distance to a ground truth solution.In this way, the neural network predicts warm starts with the end-to-end goalof minimizing the downstream loss. An important feature of our architecture isits flexibility, in that it can predict a warm start for fixed-point algorithmsrun for any number of steps, without being limited to the number of steps ithas been trained on. We provide PAC-Bayes generalization bounds on unseen datafor common classes of fixed-point operators: contractive, linearly convergent,and averaged. Applying this framework to well-known applications in control,statistics, and signal processing, we observe a significant reduction in thenumber of iterations and solution time required to solve these problems,through learned warm starts.</description><author>Rajiv Sambharya, Georgina Hall, Brandon Amos, Bartolomeo Stellato</author><pubDate>Thu, 14 Sep 2023 17:22:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07835v1</guid></item><item><title>VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2309.07832v1</link><description>We present VAPOR, a novel method for autonomous legged robot navigation inunstructured, densely vegetated outdoor environments using OfflineReinforcement Learning (RL). Our method trains a novel RL policy from unlabeleddata collected in real outdoor vegetation. This policy uses height andintensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map,and processed proprioception data as state inputs, and learns the physical andgeometric properties of the surrounding vegetation such as height, density, andsolidity/stiffness for navigation. Instead of using end-to-end policy actions,the fully-trained RL policy's Q network is used to evaluate dynamicallyfeasible robot actions generated from a novel adaptive planner capable ofnavigating through dense narrow passages and preventing entrapment invegetation such as tall grass and bushes. We demonstrate our method'scapabilities on a legged robot in complex outdoor vegetation. We observe animprovement in success rates, a decrease in average power consumption, anddecrease in normalized trajectory length compared to both existing end-to-endoffline RL and outdoor navigation methods.</description><author>Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Mohamed Elnoor, Dinesh Manocha</author><pubDate>Thu, 14 Sep 2023 17:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07832v1</guid></item><item><title>Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery</title><link>http://arxiv.org/abs/2309.07823v1</link><description>Automatic road extraction from satellite imagery using deep learning is aviable alternative to traditional manual mapping. Therefore it has receivedconsiderable attention recently. However, most of the existing methods aresupervised and require pixel-level labeling, which is tedious and error-prone.To make matters worse, the earth has a diverse range of terrain, vegetation,and man-made objects. It is well known that models trained in one areageneralize poorly to other areas. Various shooting conditions such as light andangel, as well as different image processing techniques further complicate theissue. It is impractical to develop training data to cover all image styles.This paper proposes to leverage OpenStreetMap road data as weak labels andlarge scale satellite imagery to pre-train semantic segmentation models. Ourextensive experimental results show that the prediction accuracy increases withthe amount of the weakly labeled data, as well as the road density in the areaschosen for training. Using as much as 100 times more data than the widely usedDeepGlobe road dataset, our model with the D-LinkNet architecture and theResNet-50 backbone exceeds the top performer of the current DeepGlobeleaderboard. Furthermore, due to large-scale pre-training, our modelgeneralizes much better than those trained with only the curated datasets,implying great application potential.</description><author>Shiqiao Meng, Zonglin Di, Siwei Yang, Yin Wang</author><pubDate>Thu, 14 Sep 2023 17:16:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07823v1</guid></item><item><title>CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration</title><link>http://arxiv.org/abs/2309.07822v1</link><description>In recent years, large language models (LLMs) have shown remarkablecapabilities at scale, particularly at generating text conditioned on a prompt.In our work, we investigate the use of LLMs to augment training data of smalllanguage models~(SLMs) with automatically generated counterfactual~(CF)instances -- i.e. minimally altered inputs -- in order to improveout-of-domain~(OOD) performance of SLMs in the extractive questionanswering~(QA) setup. We show that, across various LLM generators, such dataaugmentation consistently enhances OOD performance and improves modelcalibration for both confidence-based and rationale-augmented calibratormodels. Furthermore, these performance improvements correlate with higherdiversity of CF instances in terms of their surface form and semantic content.Finally, we show that CF augmented models which are easier to calibrate alsoexhibit much lower entropy when assigning importance, indicating thatrationale-augmented calibrators prefer concise explanations.</description><author>Rachneet Sachdeva, Martin Tutek, Iryna Gurevych</author><pubDate>Thu, 14 Sep 2023 17:16:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07822v1</guid></item><item><title>Decomposition of linear tensor transformations</title><link>http://arxiv.org/abs/2309.07819v1</link><description>One of the main issues in computing a tensor decomposition is how to choosethe number of rank-one components, since there is no finite algorithms fordetermining the rank of a tensor. A commonly used approach for this purpose isto find a low-dimensional subspace by solving an optimization problem andassuming the number of components is fixed. However, even though this algorithmis efficient and easy to implement, it often converges to poor local minima andsuffers from outliers and noise. The aim of this paper is to develop amathematical framework for exact tensor decomposition that is able to representa tensor as the sum of a finite number of low-rank tensors. In the paper threedifferent problems will be carried out to derive: i) the decomposition of anon-negative self-adjoint tensor operator; ii) the decomposition of a lineartensor transformation; iii) the decomposition of a generic tensor.</description><author>Claudio Turchetti</author><pubDate>Thu, 14 Sep 2023 17:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07819v1</guid></item><item><title>Effective Latent Differential Equation Models via Attention and Multiple Shooting</title><link>http://arxiv.org/abs/2307.05735v3</link><description>Scientific Machine Learning (SciML) is a burgeoning field thatsynergistically combines domain-aware and interpretable models with agnosticmachine learning techniques. In this work, we introduce GOKU-UI, an evolutionof the SciML generative model GOKU-nets. GOKU-UI not only broadens the originalmodel's spectrum to incorporate other classes of differential equations, suchas Stochastic Differential Equations (SDEs), but also integrates attentionmechanisms and a novel multiple shooting training strategy in the latent space.These modifications have led to a significant increase in its performance inboth reconstruction and forecast tasks, as demonstrated by our evaluation ofsimulated and empirical data. Specifically, GOKU-UI outperformed all baselinemodels on synthetic datasets even with a training set 16-fold smaller,underscoring its remarkable data efficiency. Furthermore, when applied toempirical human brain data, while incorporating stochastic Stuart-Landauoscillators into its dynamical core, our proposed enhancements markedlyincreased the model's effectiveness in capturing complex brain dynamics. Thisaugmented version not only surpassed all baseline methods in the reconstructiontask, but also demonstrated lower prediction error of future brain activity upto 15 seconds ahead. By training GOKU-UI on resting state fMRI data, we encodedwhole-brain dynamics into a latent representation, learning a low-dimensionaldynamical system model that could offer insights into brain functionality andopen avenues for practical applications such as the classification of mentalstates or psychiatric conditions. Ultimately, our research provides furtherimpetus for the field of Scientific Machine Learning, showcasing the potentialfor advancements when established scientific insights are interwoven withmodern machine learning.</description><author>Germán Abrevaya, Mahta Ramezanian-Panahi, Jean-Christophe Gagnon-Audet, Pablo Polosecki, Irina Rish, Silvina Ponce Dawson, Guillermo Cecchi, Guillaume Dumas</author><pubDate>Thu, 14 Sep 2023 17:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05735v3</guid></item><item><title>Meta-Learning Regrasping Strategies for Physical-Agnostic Objects</title><link>http://arxiv.org/abs/2205.11110v2</link><description>Grasping inhomogeneous objects in real-world applications remains achallenging task due to the unknown physical properties such as massdistribution and coefficient of friction. In this study, we propose ameta-learning algorithm called ConDex, which incorporates Conditional NeuralProcesses (CNP) with DexNet-2.0 to autonomously discern the underlying physicalproperties of objects using depth images. ConDex efficiently acquires physicalembeddings from limited trials, enabling precise grasping point estimation.Furthermore, ConDex is capable of updating the predicted grasping qualityiteratively from new trials in an online fashion. To the best of our knowledge,we are the first who generate two object datasets focusing on inhomogeneousphysical properties with varying mass distributions and friction coefficients.Extensive evaluations in simulation demonstrate ConDex's superior performanceover DexNet-2.0 and existing meta-learning-based grasping pipelines.Furthermore, ConDex shows robust generalization to previously unseen real-worldobjects despite training solely in the simulation. The synthetic and real-worlddatasets will be published as well.</description><author>Ning Gao, Jingyu Zhang, Ruijie Chen, Ngo Anh Vien, Hanna Ziesche, Gerhard Neumann</author><pubDate>Thu, 14 Sep 2023 17:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.11110v2</guid></item><item><title>Domain Generalization for Crop Segmentation with Knowledge Distillation</title><link>http://arxiv.org/abs/2304.01029v2</link><description>In recent years, precision agriculture has gradually oriented farming closerto automation processes to support all the activities related to fieldmanagement. Service robotics plays a predominant role in this evolution bydeploying autonomous agents that can navigate fields while performing taskswithout human intervention, such as monitoring, spraying, and harvesting. Toexecute these precise actions, mobile robots need a real-time perception systemthat understands their surroundings and identifies their targets in the wild.Generalizing to new crops and environmental conditions is critical forpractical applications, as labeled samples are rarely available. In this paper,we investigate the problem of crop segmentation and propose a novel approach toenhance domain generalization using knowledge distillation. In the proposedframework, we transfer knowledge from an ensemble of models individuallytrained on source domains to a student model that can adapt to unseen targetdomains. To evaluate the proposed method, we present a synthetic multi-domaindataset for crop segmentation containing plants of variegate shapes andcovering different terrain styles, weather conditions, and light scenarios formore than 50,000 samples. We demonstrate significant improvements inperformance over state-of-the-art methods and superior sim-to-realgeneralization. Our approach provides a promising solution for domaingeneralization in crop segmentation and has the potential to enhance a widevariety of precision agriculture applications.</description><author>Simone Angarano, Mauro Martini, Alessandro Navone, Marcello Chiaberge</author><pubDate>Thu, 14 Sep 2023 17:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01029v2</guid></item><item><title>MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids</title><link>http://arxiv.org/abs/2303.08447v2</link><description>Integrating variable renewable energy into the grid has posed challenges tosystem operators in achieving optimal trade-offs among energy availability,cost affordability, and pollution controllability. This paper proposes amulti-agent reinforcement learning framework for managing energy transactionsin microgrids. The framework addresses the challenges above: it seeks tooptimize the usage of available resources by minimizing the carbon footprintwhile benefiting all stakeholders. The proposed architecture consists of threelayers of agents, each pursuing different objectives. The first layer,comprised of prosumers and consumers, minimizes the total energy cost. Theother two layers control the energy price to decrease the carbon impact whilebalancing the consumption and production of both renewable and conventionalenergy. This framework also takes into account fluctuations in energy demandand supply.</description><author>Nicolas Cuadrado, Roberto Gutierrez, Yongli Zhu, Martin Takac</author><pubDate>Thu, 14 Sep 2023 17:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08447v2</guid></item><item><title>Directed Scattering for Knowledge Graph-based Cellular Signaling Analysis</title><link>http://arxiv.org/abs/2309.07813v1</link><description>Directed graphs are a natural model for many phenomena, in particularscientific knowledge graphs such as molecular interaction or chemical reactionnetworks that define cellular signaling relationships. In these situations,source nodes typically have distinct biophysical properties from sinks. Due totheir ordered and unidirectional relationships, many such networks also havehierarchical and multiscale structure. However, the majority of methodsperforming node- and edge-level tasks in machine learning do not take theseproperties into account, and thus have not been leveraged effectively forscientific tasks such as cellular signaling network inference. We propose a newframework called Directed Scattering Autoencoder (DSAE) which uses a directedversion of a geometric scattering transform, combined with the non-lineardimensionality reduction properties of an autoencoder and the geometricproperties of the hyperbolic space to learn latent hierarchies. We show thismethod outperforms numerous others on tasks such as embedding directed graphsand learning cellular signaling networks.</description><author>Aarthi Venkat, Joyce Chew, Ferran Cardoso Rodriguez, Christopher J. Tape, Michael Perlmutter, Smita Krishnaswamy</author><pubDate>Thu, 14 Sep 2023 16:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07813v1</guid></item><item><title>Text Classification of Cancer Clinical Trial Eligibility Criteria</title><link>http://arxiv.org/abs/2309.07812v1</link><description>Automatic identification of clinical trials for which a patient is eligibleis complicated by the fact that trial eligibility is stated in naturallanguage. A potential solution to this problem is to employ text classificationmethods for common types of eligibility criteria. In this study, we focus onseven common exclusion criteria in cancer trials: prior malignancy, humanimmunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phaseIII cancer trials with these exclusions annotated at the trial level. Weexperiment with common transformer models as well as a new pre-trained clinicaltrial BERT model. Our results demonstrate the feasibility of automaticallyclassifying common exclusion criteria. Additionally, we demonstrate the valueof a pre-trained language model specifically for clinical trials, which yieldsthe highest average performance across all criteria.</description><author>Yumeng Yang, Soumya Jayaraj, Ethan B Ludmir, Kirk Roberts</author><pubDate>Thu, 14 Sep 2023 16:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07812v1</guid></item><item><title>Spectrum-Aware Adjustment: A New Debiasing Framework with Applications to Principal Components Regression</title><link>http://arxiv.org/abs/2309.07810v1</link><description>We introduce a new debiasing framework for high-dimensional linear regressionthat bypasses the restrictions on covariate distributions imposed by moderndebiasing technology. We study the prevalent setting where the number offeatures and samples are both large and comparable. In this context,state-of-the-art debiasing technology uses a degrees-of-freedom correction toremove shrinkage bias of regularized estimators and conduct inference. However,this method requires that the observed samples are i.i.d., the covariatesfollow a mean zero Gaussian distribution, and reliable covariance matrixestimates for observed features are available. This approach struggles when (i)covariates are non-Gaussian with heavy tails or asymmetric distributions, (ii)rows of the design exhibit heterogeneity or dependencies, and (iii) reliablefeature covariance estimates are lacking. To address these, we develop a new strategy where the debiasing correction isa rescaled gradient descent step (suitably initialized) with step sizedetermined by the spectrum of the sample covariance matrix. Unlike prior work,we assume that eigenvectors of this matrix are uniform draws from theorthogonal group. We show this assumption remains valid in diverse situationswhere traditional debiasing fails, including designs with complex row-columndependencies, heavy tails, asymmetric properties, and latent low-rankstructures. We establish asymptotic normality of our proposed estimator(centered and scaled) under various convergence notions. Moreover, we develop aconsistent estimator for its asymptotic variance. Lastly, we introduce adebiased Principal Component Regression (PCR) technique using ourSpectrum-Aware approach. In varied simulations and real data experiments, weobserve that our method outperforms degrees-of-freedom debiasing by a margin.</description><author>Yufan Li, Pragya Sur</author><pubDate>Thu, 14 Sep 2023 16:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07810v1</guid></item><item><title>Communication Efficient Private Federated Learning Using Dithering</title><link>http://arxiv.org/abs/2309.07809v1</link><description>The task of preserving privacy while ensuring efficient communication is afundamental challenge in federated learning. In this work, we tackle thischallenge in the trusted aggregator model, and propose a solution that achievesboth objectives simultaneously. We show that employing a quantization schemebased on subtractive dithering at the clients can effectively replicate thenormal noise addition process at the aggregator. This implies that we canguarantee the same level of differential privacy against other clients whilesubstantially reducing the amount of communication required, as opposed totransmitting full precision gradients and using central noise addition. We alsoexperimentally demonstrate that the accuracy of our proposed approach matchesthat of the full precision gradient method.</description><author>Burak Hasircioglu, Deniz Gunduz</author><pubDate>Thu, 14 Sep 2023 16:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07809v1</guid></item><item><title>What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving</title><link>http://arxiv.org/abs/2309.07808v1</link><description>More research attention has recently been given to end-to-end autonomousdriving technologies where the entire driving pipeline is replaced with asingle neural network because of its simpler structure and faster inferencetime. Despite this appealing approach largely reducing the components indriving pipeline, its simplicity also leads to interpretability problems andsafety issues arXiv:2003.06404. The trained policy is not always compliant withthe traffic rules and it is also hard to discover the reason for themisbehavior because of the lack of intermediate outputs. Meanwhile, Sensors arealso critical to autonomous driving's security and feasibility to perceive thesurrounding environment under complex driving scenarios. In this paper, weproposed P-CSG, a novel penalty-based imitation learning approach with crosssemantics generation sensor fusion technologies to increase the overallperformance of End-to-End Autonomous Driving. We conducted an assessment of ourmodel's performance using the Town 05 Long benchmark, achieving an impressivedriving score improvement of over 15%. Furthermore, we conducted robustnessevaluations against adversarial attacks like FGSM and Dot attacks, revealing asubstantial increase in robustness compared to baseline models.More detailedinformation, such as code-based resources, ablation studies and videos can befound at https://hk-zh.github.io/p-csg-plus.</description><author>Hongkuan Zhou, Aifen Sui, Wei Cao, Letian Shi</author><pubDate>Thu, 14 Sep 2023 16:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07808v1</guid></item><item><title>Pop Quiz! Do Pre-trained Code Models Possess Knowledge of Correct API Names?</title><link>http://arxiv.org/abs/2309.07804v1</link><description>Recent breakthroughs in pre-trained code models, such as CodeBERT and Codex,have shown their superior performance in various downstream tasks. Thecorrectness and unambiguity of API usage among these code models are crucialfor achieving desirable program functionalities, requiring them to learnvarious API fully qualified names structurally and semantically. Recent studiesreveal that even state-of-the-art pre-trained code models struggle withsuggesting the correct APIs during code generation. However, the reasons forsuch poor API usage performance are barely investigated. To address thischallenge, we propose using knowledge probing as a means of interpreting codemodels, which uses cloze-style tests to measure the knowledge stored in models.Our comprehensive study examines a code model's capability of understanding APIfully qualified names from two different perspectives: API call and API import.Specifically, we reveal that current code models struggle with understandingAPI names, with pre-training strategies significantly affecting the quality ofAPI name learning. We demonstrate that natural language context can assist codemodels in locating Python API names and generalize Python API name knowledge tounseen data. Our findings provide insights into the limitations andcapabilities of current pre-trained code models, and suggest that incorporatingAPI structure into the pre-training process can improve automated API usage andcode representations. This work provides significance for advancing codeintelligence practices and direction for future studies. All experimentresults, data and source code used in this work are available at\url{https://doi.org/10.5281/zenodo.7902072}.</description><author>Terry Yue Zhuo, Xiaoning Du, Zhenchang Xing, Jiamou Sun, Haowei Quan, Li Li, Liming Zhu</author><pubDate>Thu, 14 Sep 2023 16:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07804v1</guid></item><item><title>TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering</title><link>http://arxiv.org/abs/2212.04953v2</link><description>Basecalling is an essential step in nanopore sequencing analysis where theraw signals of nanopore sequencers are converted into nucleotide sequences,i.e., reads. State-of-the-art basecallers employ complex deep learning modelsto achieve high basecalling accuracy. This makes basecallingcomputationally-inefficient and memory-hungry; bottlenecking the entire genomeanalysis pipeline. However, for many applications, the majority of reads do nomatch the reference genome of interest (i.e., target reference) and thus arediscarded in later steps in the genomics pipeline, wasting the basecallingcomputation. To overcome this issue, we propose TargetCall, the firstpre-basecalling filter to eliminate the wasted computation in basecalling.TargetCall's key idea is to discard reads that will not match the targetreference (i.e., off-target reads) prior to basecalling. TargetCall consists oftwo main components: (1) LightCall, a lightweight neural network basecallerthat produces noisy reads; and (2) Similarity Check, which labels each of thesenoisy reads as on-target or off-target by matching them to the targetreference. TargetCall aims to filter out all off-target reads beforebasecalling. The highly-accurate but slow basecalling is performed only on theraw signals whose noisy reads are labeled as on-target. Our thoroughexperimental evaluations using both real and simulated data show thatTargetCall 1) improves the end-to-end basecalling performance while maintaininghigh sensitivity in keeping on-target reads, 2) maintains high accuracy indownstream analysis, 3) precisely filters out up to 94.71% of off-target reads,and 4) achieves better performance, throughput, sensitivity, precision, andgenerality compared to prior works. We open-source TargetCall athttps://github.com/CMU-SAFARI/TargetCall</description><author>Meryem Banu Cavlak, Gagandeep Singh, Mohammed Alser, Can Firtina, Joël Lindegger, Mohammad Sadrosadati, Nika Mansouri Ghiasi, Can Alkan, Onur Mutlu</author><pubDate>Thu, 14 Sep 2023 16:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04953v2</guid></item><item><title>Self-supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive</title><link>http://arxiv.org/abs/2305.19862v3</link><description>Modern consumer cameras usually employ the rolling shutter (RS) mechanism,where images are captured by scanning scenes row-by-row, yielding RSdistortions for dynamic scenes. To correct RS distortions, existing methodsadopt a fully supervised learning manner, where high framerate global shutter(GS) images should be collected as ground-truth supervision. In this paper, wepropose a Self-supervised learning framework for Dual reversed RS distortionsCorrection (SelfDRSC), where a DRSC network can be learned to generate a highframerate GS video only based on dual RS images with reversed distortions. Inparticular, a bidirectional distortion warping module is proposed forreconstructing dual reversed RS images, and then a self-supervised loss can bedeployed to train DRSC network by enhancing the cycle consistency between inputand reconstructed dual reversed RS images. Besides start and end RS scanningtime, GS images at arbitrary intermediate scanning time can also be supervisedin SelfDRSC, thus enabling the learned DRSC network to generate a highframerate GS video. Moreover, a simple yet effective self-distillation strategyis introduced in self-supervised loss for mitigating boundary artifacts ingenerated GS images. On synthetic dataset, SelfDRSC achieves better orcomparable quantitative metrics in comparison to state-of-the-art methodstrained in the full supervision manner. On real-world RS cases, our SelfDRSCcan produce high framerate GS videos with finer correction textures and bettertemporary consistency. The source code and trained models are made publiclyavailable at https://github.com/shangwei5/SelfDRSC. We also provide animplementation in HUAWEI Mindspore athttps://github.com/Hunter-Will/SelfDRSC-mindspore.</description><author>Wei Shang, Dongwei Ren, Chaoyu Feng, Xiaotao Wang, Lei Lei, Wangmeng Zuo</author><pubDate>Thu, 14 Sep 2023 16:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19862v3</guid></item><item><title>The Dynamical Principles of Storytelling</title><link>http://arxiv.org/abs/2309.07797v1</link><description>When considering the opening part of 1800 short stories, we find that thefirst dozen paragraphs of the average narrative follow an action principle asdefined in arXiv:2309.06600. When the order of the paragraphs is shuffled, theaverage no longer exhibits this property. The findings show that there is apreferential direction we take in semantic space when starting a story,possibly related to a common Western storytelling tradition as implied byAristotle in Poetics.</description><author>Isidoros Doxas, James Meiss, Steven Bottone, Tom Strelich, Andrew Plummer, Adrienne Breland, Simon Dennis, Kathy Garvin-Doxas, Michael Klymkowsky</author><pubDate>Thu, 14 Sep 2023 16:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07797v1</guid></item><item><title>For A More Comprehensive Evaluation of 6DoF Object Pose Tracking</title><link>http://arxiv.org/abs/2309.07796v1</link><description>Previous evaluations on 6DoF object pose tracking have presented obviouslimitations along with the development of this area. In particular, theevaluation protocols are not unified for different methods, the widely-usedYCBV dataset contains significant annotation error, and the error metrics alsomay be biased. As a result, it is hard to fairly compare the methods, which hasbecame a big obstacle for developing new algorithms. In this paper wecontribute a unified benchmark to address the above problems. For more accurateannotation of YCBV, we propose a multi-view multi-object global pose refinementmethod, which can jointly refine the poses of all objects and view cameras,resulting in sub-pixel sub-millimeter alignment errors. The limitations ofprevious scoring methods and error metrics are analyzed, based on which weintroduce our improved evaluation methods. The unified benchmark takes bothYCBV and BCOT as base datasets, which are shown to be complementary in scenecategories. In experiments, we validate the precision and reliability of theproposed global pose refinement method with a realistic semi-synthesizeddataset particularly for YCBV, and then present the benchmark results unifyinglearning&amp;non-learning and RGB&amp;RGBD methods, with some finds not discovered inprevious studies.</description><author>Yang Li, Fan Zhong, Xin Wang, Shuangbing Song, Jiachen Li, Xueying Qin, Changhe Tu</author><pubDate>Thu, 14 Sep 2023 16:35:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07796v1</guid></item><item><title>Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks</title><link>http://arxiv.org/abs/2309.07794v1</link><description>Effectively leveraging multimodal information from social media posts isessential to various downstream tasks such as sentiment analysis, sarcasmdetection and hate speech classification. However, combining text and imageinformation is challenging because of the idiosyncratic cross-modal semanticswith hidden or complementary information present in matching image-text pairs.In this work, we aim to directly model this by proposing the use of twoauxiliary losses jointly with the main task when fine-tuning any pre-trainedmultimodal model. Image-Text Contrastive (ITC) brings image-textrepresentations of a post closer together and separates them from differentposts, capturing underlying dependencies. Image-Text Matching (ITM) facilitatesthe understanding of semantic correspondence between images and text bypenalizing unrelated pairs. We combine these objectives with five multimodalmodels, demonstrating consistent improvements across four popular social mediadatasets. Furthermore, through detailed analysis, we shed light on the specificscenarios and cases where each auxiliary task proves to be most effective.</description><author>Danae Sánchez Villegas, Daniel Preoţiuc-Pietro, Nikolaos Aletras</author><pubDate>Thu, 14 Sep 2023 16:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07794v1</guid></item><item><title>A Multi-In and Multi-Out Dendritic Neuron Model and its Optimization</title><link>http://arxiv.org/abs/2309.07791v1</link><description>Artificial neural networks (ANNs), inspired by the interconnection of realneurons, have achieved unprecedented success in various fields such as computervision and natural language processing. Recently, a novel mathematical ANNmodel, known as the dendritic neuron model (DNM), has been proposed to addressnonlinear problems by more accurately reflecting the structure of real neurons.However, the single-output design limits its capability to handle multi-outputtasks, significantly lowering its applications. In this paper, we propose anovel multi-in and multi-out dendritic neuron model (MODN) to tacklemulti-output tasks. Our core idea is to introduce a filtering matrix to thesoma layer to adaptively select the desired dendrites to regress each output.Because such a matrix is designed to be learnable, MODN can explore therelationship between each dendrite and output to provide a better solution todownstream tasks. We also model a telodendron layer into MODN to simulatebetter the real neuron behavior. Importantly, MODN is a more general andunified framework that can be naturally specialized as the DNM by customizingthe filtering matrix. To explore the optimization of MODN, we investigate bothheuristic and gradient-based optimizers and introduce a 2-step training methodfor MODN. Extensive experimental results performed on 11 datasets on bothbinary and multi-class classification tasks demonstrate the effectiveness ofMODN, with respect to accuracy, convergence, and generality.</description><author>Yu Ding, Jun Yu, Chunzhi Gu, Shangce Gao, Chao Zhang</author><pubDate>Thu, 14 Sep 2023 16:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07791v1</guid></item><item><title>Sequential decomposition of propositional logic programs</title><link>http://arxiv.org/abs/2304.13522v2</link><description>The sequential composition of propositional logic programs has been recentlyintroduced. This paper studies the sequential {\em decomposition} of programsby studying Green's relations $\mathcal{L,R,J}$ -- well-known in semigrouptheory -- between programs. In a broader sense, this paper is a further steptowards an algebraic theory of logic programming.</description><author>Christian Antić</author><pubDate>Thu, 14 Sep 2023 16:25:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13522v2</guid></item><item><title>Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction</title><link>http://arxiv.org/abs/2308.08518v3</link><description>Traditional geometric registration based estimation methods only exploit theCAD model implicitly, which leads to their dependence on observation qualityand deficiency to occlusion. To address the problem,the paper proposes abidirectional correspondence prediction network with a point-wiseattention-aware mechanism. This network not only requires the model points topredict the correspondence but also explicitly models the geometricsimilarities between observations and the model prior. Our key insight is thatthe correlations between each model point and scene point provide essentialinformation for learning point-pair matches. To further tackle the correlationnoises brought by feature distribution divergence, we design a simple buteffective pseudo-siamese network to improve feature homogeneity. Experimentalresults on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show thatthe proposed method achieves better performance than other state-of-the-artmethods under the same evaluation criteria. Its robustness in estimating posesis greatly improved, especially in an environment with severe occlusions.</description><author>Yuhao Yang, Jun Wu, Yue Wang, Guangjian Zhang, Rong Xiong</author><pubDate>Thu, 14 Sep 2023 16:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08518v3</guid></item><item><title>Convergence analysis of online algorithms for vector-valued kernel regression</title><link>http://arxiv.org/abs/2309.07779v1</link><description>We consider the problem of approximating the regression function from noisyvector-valued data by an online learning algorithm using an appropriatereproducing kernel Hilbert space (RKHS) as prior. In an online algorithm,i.i.d. samples become available one by one by a random process and aresuccessively processed to build approximations to the regression function. Weare interested in the asymptotic performance of such online approximationalgorithms and show that the expected squared error in the RKHS norm can bebounded by $C^2 (m+1)^{-s/(2+s)}$, where $m$ is the current number of processeddata, the parameter $0&lt;s\leq 1$ expresses an additional smoothness assumptionon the regression function and the constant $C$ depends on the variance of theinput noise, the smoothness of the regression function and further parametersof the algorithm.</description><author>Michael Griebel, Peter Oswald</author><pubDate>Thu, 14 Sep 2023 16:10:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07779v1</guid></item><item><title>Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks</title><link>http://arxiv.org/abs/2302.07260v5</link><description>Several fundamental problems in science and engineering consist of globaloptimization tasks involving unknown high-dimensional (black-box) functionsthat map a set of controllable variables to the outcomes of an expensiveexperiment. Bayesian Optimization (BO) techniques are known to be effective intackling global optimization problems using a relatively small number objectivefunction evaluations, but their performance suffers when dealing withhigh-dimensional outputs. To overcome the major challenge of dimensionality,here we propose a deep learning framework for BO and sequential decision makingbased on bootstrapped ensembles of neural architectures with randomized priors.Using appropriate architecture choices, we show that the proposed framework canapproximate functional relationships between design variables and quantities ofinterest, even in cases where the latter take values in high-dimensional vectorspaces or even infinite-dimensional function spaces. In the context of BO, weaugmented the proposed probabilistic surrogates with re-parameterized MonteCarlo approximations of multiple-point (parallel) acquisition functions, aswell as methodological extensions for accommodating black-box constraints andmulti-fidelity information sources. We test the proposed framework againststate-of-the-art methods for BO and demonstrate superior performance acrossseveral challenging tasks with high-dimensional outputs, including aconstrained multi-fidelity optimization task involving shape optimization ofrotor blades in turbo-machinery.</description><author>Mohamed Aziz Bhouri, Michael Joly, Robert Yu, Soumalya Sarkar, Paris Perdikaris</author><pubDate>Thu, 14 Sep 2023 16:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07260v5</guid></item><item><title>Virchow: A Million-Slide Digital Pathology Foundation Model</title><link>http://arxiv.org/abs/2309.07778v1</link><description>Computational pathology uses artificial intelligence to enable precisionmedicine and decision support systems through the analysis of whole slideimages. It has the potential to revolutionize the diagnosis and treatment ofcancer. However, a major challenge to this objective is that for many specificcomputational pathology tasks the amount of data is inadequate for development.To address this challenge, we created Virchow, a 632 million parameter deepneural network foundation model for computational pathology. Usingself-supervised learning, Virchow is trained on 1.5 million hematoxylin andeosin stained whole slide images from diverse tissue groups, which is orders ofmagnitude more data than previous works. When evaluated on downstream tasksincluding tile-level pan-cancer detection and subtyping and slide-levelbiomarker prediction, Virchow outperforms state-of-the-art systems both oninternal datasets drawn from the same population as the pretraining data aswell as external public datasets. Virchow achieves 93% balanced accuracy forpancancer tile classification, and AUCs of 0.983 for colon microsatelliteinstability status prediction and 0.967 for breast CDH1 status prediction. Thegains in performance highlight the importance of pretraining on massivepathology image datasets, suggesting pretraining on even larger datasets couldcontinue improving performance for many high-impact applications where limitedamounts of training data are available, such as drug outcome prediction.</description><author>Eugene Vorontsov, Alican Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Siqi Liu, Philippe Mathieu, Alexander van Eck, Donghun Lee, Julian Viret, Eric Robert, Yi Kan Wang, Jeremy D. Kun, Matthew C. H. Le, Jan Bernhard, Ran A. Godrich, Gerard Oakley, Ewan Millar, Matthew Hanna, Juan Retamero, William A. Moye, Razik Yousfi, Christopher Kanan, David Klimstra, Brandon Rothrock, Thomas J. Fuchs</author><pubDate>Thu, 14 Sep 2023 16:09:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07778v1</guid></item><item><title>Usability Evaluation of Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games</title><link>http://arxiv.org/abs/2309.07773v1</link><description>This paper presents an empirical investigation of the extent to which spokenHumanoid Embodied Conversational Agents (HECAs) can foster usability in mobileserious game (MSG) applications. The aim of the research is to assess theimpact of multiple agents and illusion of humanness on the quality of theinteraction. The experiment investigates two styles of agent presentation: anagent of high human-likeness (HECA) and an agent of low human-likeness (text).The purpose of the experiment is to assess whether and how agents of highhumanlikeness can evoke the illusion of humanness and affect usability. Agentsof high human-likeness were designed by following the ECA design model that isa proposed guide for ECA development. The results of the experiment with 90participants show that users prefer to interact with the HECAs. The differencebetween the two versions is statistically significant with a large effect size(d=1.01), with many of the participants justifying their choice by saying thatthe human-like characteristics of the HECA made the version more appealing.This research provides key information on the potential effect of HECAs onserious games, which can provide insight into the design of future mobileserious games.</description><author>Danai Korre, Judy Robertson</author><pubDate>Thu, 14 Sep 2023 16:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07773v1</guid></item><item><title>Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2308.13437v2</link><description>Recently, Multimodal Large Language Models (MLLMs) that enable Large LanguageModels (LLMs) to interpret images through visual instruction tuning haveachieved significant success. However, existing visual instruction tuningmethods only utilize image-language instruction data to align the language andimage modalities, lacking a more fine-grained cross-modal alignment. In thispaper, we propose Position-enhanced Visual Instruction Tuning (PVIT), whichextends the functionality of MLLMs by integrating an additional region-levelvision encoder. This integration promotes a more detailed comprehension ofimages for the MLLM. In addition, to efficiently achieve a fine-grainedalignment between the vision modules and the LLM, we design multiple datageneration strategies to construct an image-region-language instructiondataset. Finally, we present both quantitative experiments and qualitativeanalysis that demonstrate the superiority of the proposed model. Code and datawill be released at https://github.com/PVIT-official/PVIT.</description><author>Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, Yang Liu</author><pubDate>Thu, 14 Sep 2023 16:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13437v2</guid></item><item><title>Variational Quantum Linear Solver enhanced Quantum Support Vector Machine</title><link>http://arxiv.org/abs/2309.07770v1</link><description>Quantum Support Vector Machines (QSVM) play a vital role in using quantumresources for supervised machine learning tasks, such as classification.However, current methods are strongly limited in terms of scalability on NoisyIntermediate Scale Quantum (NISQ) devices. In this work, we propose a novelapproach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM.This is built upon our idea of utilizing the variational quantum linear solverto solve system of linear equations of a least squares-SVM on a NISQ device.The implementation of our approach is evaluated by an extensive series ofnumerical experiments with the Iris dataset, which consists of three distinctiris plant species. Based on this, we explore the practicality andeffectiveness of our algorithm by constructing a classifier capable ofclassification in a feature space ranging from one to seven dimensions.Furthermore, by strategically exploiting both classical and quantum computingfor various subroutines of our algorithm, we effectively mitigate practicalchallenges associated with the implementation. These include significantimprovement in the trainability of the variational ansatz and notablereductions in run-time for cost calculations. Based on the numericalexperiments, our approach exhibits the capability of identifying a separatinghyperplane in an 8-dimensional feature space. Moreover, it consistentlydemonstrated strong performance across various instances with the same dataset.</description><author>Jianming Yi, Kalyani Suresh, Ali Moghiseh, Norbert Wehn</author><pubDate>Thu, 14 Sep 2023 15:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07770v1</guid></item><item><title>TrojViT: Trojan Insertion in Vision Transformers</title><link>http://arxiv.org/abs/2208.13049v4</link><description>Vision Transformers (ViTs) have demonstrated the state-of-the-art performancein various vision-related tasks. The success of ViTs motivates adversaries toperform backdoor attacks on ViTs. Although the vulnerability of traditionalCNNs to backdoor attacks is well-known, backdoor attacks on ViTs areseldom-studied. Compared to CNNs capturing pixel-wise local features byconvolutions, ViTs extract global context information through patches andattentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTsyields only a low clean data accuracy and a low attack success rate. In thispaper, we propose a stealth and practical ViT-specific backdoor attack$TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoorattacks, TrojViT generates a patch-wise trigger designed to build a Trojancomposed of some vulnerable bits on the parameters of a ViT stored in DRAMmemory through patch salience ranking and attention-target loss. TrojViTfurther uses minimum-tuned parameter update to reduce the bit number of theTrojan. Once the attacker inserts the Trojan into the ViT model by flipping thevulnerable bits, the ViT model still produces normal inference accuracy withbenign inputs. But when the attacker embeds a trigger into an input, the ViTmodel is forced to classify the input to a predefined target class. We showthat flipping only few vulnerable bits identified by TrojViT on a ViT modelusing the well-known RowHammer can transform the model into a backdoored one.We perform extensive experiments of multiple datasets on various ViT models.TrojViT can classify $99.64\%$ of test images to a target class by flipping$345$ bits on a ViT for ImageNet.Our codes are available athttps://github.com/mxzheng/TrojViT</description><author>Mengxin Zheng, Qian Lou, Lei Jiang</author><pubDate>Thu, 14 Sep 2023 15:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.13049v4</guid></item><item><title>Echotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks</title><link>http://arxiv.org/abs/2309.07765v1</link><description>The Transformer architecture has proven to be highly effective for AutomaticSpeech Recognition (ASR) tasks, becoming a foundational component for aplethora of research in the domain. Historically, many approaches have leanedon fixed-length attention windows, which becomes problematic for varied speechsamples in duration and complexity, leading to data over-smoothing and neglectof essential long-term connectivity. Addressing this limitation, we introduceEcho-MSA, a nimble module equipped with a variable-length attention mechanismthat accommodates a range of speech sample complexities and durations. Thismodule offers the flexibility to extract speech features across variousgranularities, spanning from frames and phonemes to words and discourse. Theproposed design captures the variable length feature of speech and addressesthe limitations of fixed-length attention. Our evaluation leverages a parallelattention architecture complemented by a dynamic gating mechanism thatamalgamates traditional attention with the Echo-MSA module output. Empiricalevidence from our study reveals that integrating Echo-MSA into the primarymodel's training regime significantly enhances the word error rate (WER)performance, all while preserving the intrinsic stability of the originalmodel.</description><author>Sizhou Chen, Songyang Gao, Sen Fang</author><pubDate>Thu, 14 Sep 2023 15:51:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07765v1</guid></item><item><title>PRE: Vision-Language Prompt Learning with Reparameterization Encoder</title><link>http://arxiv.org/abs/2309.07760v1</link><description>Large pre-trained vision-language models such as CLIP have demonstrated greatpotential in zero-shot transferability to downstream tasks. However, to attainoptimal performance, the manual selection of prompts is necessary to improvealignment between the downstream image distribution and the textual classdescriptions. This manual prompt engineering is the major challenge fordeploying such models in practice since it requires domain expertise and isextremely time-consuming. To avoid non-trivial prompt engineering, recent workContext Optimization (CoOp) introduced the concept of prompt learning to thevision domain using learnable textual tokens. While CoOp can achievesubstantial improvements over manual prompts, its learned context is worsegeneralizable to wider unseen classes within the same dataset. In this work, wepresent Prompt Learning with Reparameterization Encoder (PRE) - a simple andefficient method that enhances the generalization ability of the learnableprompt to unseen classes while maintaining the capacity to learn Base classes.Instead of directly optimizing the prompts, PRE employs a prompt encoder toreparameterize the input prompt embeddings, enhancing the exploration oftask-specific knowledge from few-shot samples. Experiments and extensiveablation studies on 8 benchmarks demonstrate that our approach is an efficientmethod for prompt learning. Specifically, PRE achieves a notable enhancement of5.60% in average accuracy on New classes and 3% in Harmonic mean compared toCoOp in the 16-shot setting, all achieved within a good training time.</description><author>Anh Pham Thi Minh</author><pubDate>Thu, 14 Sep 2023 15:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07760v1</guid></item><item><title>PROGrasp: Pragmatic Human-Robot Communication for Object Grasping</title><link>http://arxiv.org/abs/2309.07759v1</link><description>Interactive Object Grasping (IOG) is the task of identifying and grasping thedesired object via human-robot natural language interaction. Current IOGsystems assume that a human user initially specifies the target object'scategory (e.g., bottle). Inspired by pragmatics, where humans often conveytheir intentions by relying on context to achieve goals, we introduce a new IOGtask, Pragmatic-IOG, and the corresponding dataset, Intention-orientedMulti-modal Dialogue (IM-Dial). In our proposed task scenario, anintention-oriented utterance (e.g., "I am thirsty") is initially given to therobot. The robot should then identify the target object by interacting with ahuman user. Based on the task setup, we propose a new robotic system that caninterpret the user's intention and pick up the target object, Pragmatic ObjectGrasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modulesfor visual grounding, question asking, object grasping, and most importantly,answer interpretation for pragmatic inference. Experimental results show thatPROGrasp is effective in offline (i.e., target object discovery) and online(i.e., IOG with a physical robot arm) settings.</description><author>Gi-Cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang</author><pubDate>Thu, 14 Sep 2023 15:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07759v1</guid></item><item><title>Machine Learning and Computer Vision Techniques in Continuous Beehive Monitoring Applications: A survey</title><link>http://arxiv.org/abs/2208.00085v3</link><description>Wide use and availability of the machine learning and computer visiontechniques allows development of relatively complex monitoring systems in manydomains. Besides the traditional industrial domain, new application appearsalso in biology and agriculture, where we could speak about the detection ofinfections, parasites and weeds, but also about automated monitoring and earlywarning systems. This is also connected with the introduction of the easilyaccessible hardware and development kits such as Arduino, or RaspberryPifamily. In this paper, we survey 50 existing papers focusing on the methods ofautomated beehive monitoring methods using the computer vision techniques,particularly on the pollen and Varroa mite detection together with the beetraffic monitoring. Such systems could also be used for the monitoring of thehoneybee colonies and for the inspection of their health state, which couldidentify potentially dangerous states before the situation is critical, or tobetter plan periodic bee colony inspections and therefore save significantcosts. Later, we also include analysis of the research trends in thisapplication field and we outline the possible direction of the newexplorations. Our paper is aimed also at veterinary and apidology professionalsand experts, who might not be familiar with machine learning to introduce themto its possibilities, therefore each family of applications is opened by abrief theoretical introduction and motivation related to its base method. Wehope that this paper will inspire other scientists to use machine learningtechniques for other applications in beehive monitoring.</description><author>Simon Bilik, Tomas Zemcik, Lukas Kratochvila, Dominik Ricanek, Milos Richter, Sebastian Zambanini, Karel Horak</author><pubDate>Thu, 14 Sep 2023 15:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.00085v3</guid></item><item><title>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</title><link>http://arxiv.org/abs/2306.14565v2</link><description>Despite the promising progress in multi-modal tasks, current largemulti-modal models (LMM) are prone to hallucinating inconsistent descriptionswith respect to the associated image and human instructions. This paperaddresses this issue by introducing the first large and diverse visualinstruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.Our dataset consists of 120k visual instructions generated by GPT4, covering 16vision-and-language tasks with open-ended instructions and answers. Unlikeexisting studies that primarily focus on positive instruction samples, wedesign LRV-Instruction to include both positive and negative instructions formore robust visual instruction tuning. Our negative instructions are designedat two semantic levels: (i) Nonexistent Element Manipulation and (ii) ExistentElement Manipulation. To efficiently measure the hallucination generated byLMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novelapproach to evaluate visual instruction tuning without the need forhuman-annotated groundtruth answers and can adapt to diverse instructionformats. We conduct comprehensive experiments to investigate the hallucinationof LMMs. Our results demonstrate that existing LMMs exhibit significanthallucination when presented with our negative instructions, particularly withExistent Element Manipulation instructions. Moreover, by finetuning MiniGPT4 onLRV-Instruction, we successfully mitigate hallucination while improvingperformance on public datasets using less training data compared tostate-of-the-art methods. Additionally, we observed that a balanced ratio ofpositive and negative instances in the training data leads to a more robustmodel. Updates of our project are available athttps://fuxiaoliu.github.io/LRV/.</description><author>Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang</author><pubDate>Thu, 14 Sep 2023 15:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14565v2</guid></item><item><title>Generative AI Text Classification using Ensemble LLM Approaches</title><link>http://arxiv.org/abs/2309.07755v1</link><description>Large Language Models (LLMs) have shown impressive performance across avariety of Artificial Intelligence (AI) and natural language processing tasks,such as content creation, report generation, etc. However, unregulated malignapplication of these models can create undesirable consequences such asgeneration of fake news, plagiarism, etc. As a result, accurate detection ofAI-generated language can be crucial in responsible usage of LLMs. In thiswork, we explore 1) whether a certain body of text is AI generated or writtenby human, and 2) attribution of a specific language model in generating a bodyof text. Texts in both English and Spanish are considered. The datasets used inthis study are provided as part of the Automated Text Identification(AuTexTification) shared task. For each of the research objectives statedabove, we propose an ensemble neural model that generates probabilities fromdifferent pre-trained LLMs which are used as features to a Traditional MachineLearning (TML) classifier following it. For the first task of distinguishingbetween AI and human generated text, our model ranked in fifth and thirteenthplace (with macro $F1$ scores of 0.733 and 0.649) for English and Spanishtexts, respectively. For the second task on model attribution, our model rankedin first place with macro $F1$ scores of 0.625 and 0.653 for English andSpanish texts, respectively.</description><author>Harika Abburi, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen, Sanmitra Bhattacharya</author><pubDate>Thu, 14 Sep 2023 15:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07755v1</guid></item><item><title>Co-Salient Object Detection with Semantic-Level Consensus Extraction and Dispersion</title><link>http://arxiv.org/abs/2309.07753v1</link><description>Given a group of images, co-salient object detection (CoSOD) aims tohighlight the common salient object in each image. There are two factorsclosely related to the success of this task, namely consensus extraction, andthe dispersion of consensus to each image. Most previous works represent thegroup consensus using local features, while we instead utilize a hierarchicalTransformer module for extracting semantic-level consensus. Therefore, it canobtain a more comprehensive representation of the common object category, andexclude interference from other objects that share local similarities with thetarget object. In addition, we propose a Transformer-based dispersion modulethat takes into account the variation of the co-salient object in differentscenes. It distributes the consensus to the image feature maps in animage-specific way while making full use of interactions within the group.These two modules are integrated with a ViT encoder and an FPN-like decoder toform an end-to-end trainable network, without additional branch and auxiliaryloss. The proposed method is evaluated on three commonly used CoSOD datasetsand achieves state-of-the-art performance.</description><author>Peiran Xu, Yadong Mu</author><pubDate>Thu, 14 Sep 2023 15:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07753v1</guid></item><item><title>DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</title><link>http://arxiv.org/abs/2309.07752v1</link><description>In this paper, we present the decomposed triplane-hash neural radiance fields(DT-NeRF), a framework that significantly improves the photorealistic renderingof talking faces and achieves state-of-the-art results on key evaluationdatasets. Our architecture decomposes the facial region into two specializedtriplanes: one specialized for representing the mouth, and the other for thebroader facial features. We introduce audio features as residual terms andintegrate them as query vectors into our model through an audio-mouth-facetransformer. Additionally, our method leverages the capabilities of NeuralRadiance Fields (NeRF) to enrich the volumetric representation of the entireface through additive volumetric rendering techniques. Comprehensiveexperimental evaluations corroborate the effectiveness and superiority of ourproposed approach.</description><author>Yaoyu Su, Shaohui Wang, Haoqian Wang</author><pubDate>Thu, 14 Sep 2023 15:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07752v1</guid></item><item><title>OmnimatteRF: Robust Omnimatte with 3D Background Modeling</title><link>http://arxiv.org/abs/2309.07749v1</link><description>Video matting has broad applications, from adding interesting effects tocasually captured movies to assisting video production professionals. Mattingwith associated effects such as shadows and reflections has also attractedincreasing research activity, and methods like Omnimatte have been proposed toseparate dynamic foreground objects of interest into their own layers. However,prior works represent video backgrounds as 2D image layers, limiting theircapacity to express more complicated scenes, thus hindering application toreal-world videos. In this paper, we propose a novel video matting method,OmnimatteRF, that combines dynamic 2D foreground layers and a 3D backgroundmodel. The 2D layers preserve the details of the subjects, while the 3Dbackground robustly reconstructs scenes in real-world videos. Extensiveexperiments demonstrate that our method reconstructs scenes with better qualityon various videos.</description><author>Geng Lin, Chen Gao, Jia-Bin Huang, Changil Kim, Yipeng Wang, Matthias Zwicker, Ayush Saraf</author><pubDate>Thu, 14 Sep 2023 15:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07749v1</guid></item><item><title>A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET/CT Images</title><link>http://arxiv.org/abs/2309.05446v2</link><description>Fluorodeoxyglucose (FDG) positron emission tomography (PET) combined withcomputed tomography (CT) is considered the primary solution for detecting somecancers, such as lung cancer and melanoma. Automatic segmentation of tumors inPET/CT images can help reduce doctors' workload, thereby improving diagnosticquality. However, precise tumor segmentation is challenging due to the smallsize of many tumors and the similarity of high-uptake normal areas to the tumorregions. To address these issues, this paper proposes alocalization-to-segmentation framework (L2SNet) for precise tumor segmentation.L2SNet first localizes the possible lesions in the lesion localization phaseand then uses the location cues to shape the segmentation results in the lesionsegmentation phase. To further improve the segmentation performance of L2SNet,we design an adaptive threshold scheme that takes the segmentation results ofthe two phases into consideration. The experiments with the MICCAI 2023Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge dataset showthat our method achieved a competitive result and was ranked in the top 7methods on the preliminary test set. Our work is available at:https://github.com/MedCAI/L2SNet.</description><author>Linghan Cai, Jianhao Huang, Zihang Zhu, Jinpeng Lu, Yongbing Zhang</author><pubDate>Thu, 14 Sep 2023 15:30:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05446v2</guid></item><item><title>Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning</title><link>http://arxiv.org/abs/2309.07742v1</link><description>Focus in Explainable AI is shifting from explanations defined in terms oflow-level elements, such as input features, to explanations encoded in terms ofinterpretable concepts learned from data. How to reliably acquire such conceptsis, however, still fundamentally unclear. An agreed-upon notion of conceptinterpretability is missing, with the result that concepts used by bothpost-hoc explainers and concept-based neural networks are acquired through avariety of mutually incompatible strategies. Critically, most of these neglectthe human side of the problem: a representation is understandable only insofaras it can be understood by the human at the receiving end. The key challenge inHuman-interpretable Representation Learning (HRL) is how to model andoperationalize this human element. In this work, we propose a mathematicalframework for acquiring interpretable representations suitable for bothpost-hoc explainers and concept-based neural networks. Our formalization of HRLbuilds on recent advances in causal representation learning and explicitlymodels a human stakeholder as an external observer. This allows us to derive aprincipled notion of alignment between the machine representation and thevocabulary of concepts understood by the human. In doing so, we link alignmentand interpretability through a simple and intuitive name transfer game, andclarify the relationship between alignment and a well-known property ofrepresentations, namely disentanglment. We also show that alignment is linkedto the issue of undesirable correlations among concepts, also known as conceptleakage, and to content-style separation, all through a generalinformation-theoretic reformulation of these properties. Our conceptualizationaims to bridge the gap between the human and algorithmic sides ofinterpretability and establish a stepping stone for new research onhuman-interpretable representations.</description><author>Emanuele Marconato, Andrea Passerini, Stefano Teso</author><pubDate>Thu, 14 Sep 2023 15:26:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07742v1</guid></item><item><title>Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports</title><link>http://arxiv.org/abs/2309.00917v2</link><description>The way we analyse clinical texts has undergone major changes over the lastyears. The introduction of language models such as BERT led to adaptations forthe (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely onlarge databases of archived medical documents. While performing well in termsof accuracy, both the lack of interpretability and limitations to transferacross languages limit their use in clinical setting. We introduce a novellight-weight graph-based embedding method specifically catering radiologyreports. It takes into account the structure and composition of the report,while also connecting medical terms in the report through the multi-lingualSNOMED Clinical Terms knowledge base. The resulting graph embedding uncoversthe underlying relationships among clinical terms, achieving a representationthat is better understandable for clinicians and clinically more accurate,without reliance on large pre-training datasets. We show the use of thisembedding on two tasks namely disease classification of X-ray reports and imageclassification. For disease classification our model is competitive with itsBERT-based counterparts, while being magnitudes smaller in size and trainingdata requirements. For image classification, we show the effectiveness of thegraph embedding leveraging cross-modal knowledge transfer and show how thismethod is usable across different languages.</description><author>Tom van Sonsbeek, Xiantong Zhen, Marcel Worring</author><pubDate>Thu, 14 Sep 2023 15:25:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00917v2</guid></item><item><title>The complementary roles of non-verbal cues for Robust Pronunciation Assessment</title><link>http://arxiv.org/abs/2309.07739v1</link><description>Research on pronunciation assessment systems focuses on utilizing phoneticand phonological aspects of non-native (L2) speech, often neglecting the richlayer of information hidden within the non-verbal cues. In this study, weproposed a novel pronunciation assessment framework, IntraVerbalPA. % Theframework innovatively incorporates both fine-grained frame- and abstractutterance-level non-verbal cues, alongside the conventional speech and phonemerepresentations. Additionally, we introduce ''Goodness of phonemic-duration''metric to effectively model duration distribution within the framework. Ourresults validate the effectiveness of the proposed IntraVerbalPA framework andits individual components, yielding performance that either matches oroutperforms existing research works.</description><author>Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</author><pubDate>Thu, 14 Sep 2023 15:18:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07739v1</guid></item><item><title>Explaining Speech Classification Models via Word-Level Audio Segments and Paralinguistic Features</title><link>http://arxiv.org/abs/2309.07733v1</link><description>Recent advances in eXplainable AI (XAI) have provided new insights into howmodels for vision, language, and tabular data operate. However, few approachesexist for understanding speech models. Existing work focuses on a few spokenlanguage understanding (SLU) tasks, and explanations are difficult to interpretfor most users. We introduce a new approach to explain speech classificationmodels. We generate easy-to-interpret explanations via input perturbation ontwo information levels. 1) Word-level explanations reveal how each word-relatedaudio segment impacts the outcome. 2) Paralinguistic features (e.g., prosodyand background noise) answer the counterfactual: ``What would the modelprediction be if we edited the audio signal in this way?'' We validate ourapproach by explaining two state-of-the-art SLU models on two speechclassification tasks in English and Italian. Our findings demonstrate that theexplanations are faithful to the model's inner workings and plausible tohumans. Our method and findings pave the way for future research oninterpreting speech models.</description><author>Eliana Pastor, Alkis Koudounas, Giuseppe Attanasio, Dirk Hovy, Elena Baralis</author><pubDate>Thu, 14 Sep 2023 15:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07733v1</guid></item><item><title>AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks</title><link>http://arxiv.org/abs/2309.07730v1</link><description>Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used forunderwater environments and find applications in many areas. However, a lack ofsecurity considerations, the unstable and challenging nature of the underwaterenvironment, and the resource-constrained nature of the sensor nodes used forUW-ASNs (which makes them incapable of adopting security primitives) make theUW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralisedIntrusion Detection and Prevention System called AIDPS for UW-ASNs. Theproposed AIDPS can improve the security of the UW-ASNs so that they canefficiently detect underwater-related attacks (e.g., blackhole, grayhole andflooding attacks). To determine the most effective configuration of theproposed construction, we conduct a number of experiments using severalstate-of-the-art machine learning algorithms (e.g., Adaptive Random Forest(ARF), light gradient-boosting machine, and K-nearest neighbours) and conceptdrift detection algorithms (e.g., ADWIN, kdqTree, and Page-Hinkley). Ourexperimental results show that incremental ARF using ADWIN provides optimalperformance when implemented with One-class support vector machine (SVM)anomaly-based detectors. Furthermore, our extensive evaluation results alsoshow that the proposed scheme outperforms state-of-the-art bench-markingmethods while providing a wider range of desirable features such as scalabilityand complexity.</description><author>Soumadeep Das, Aryan Mohammadi Pasikhani, Prosanta Gope, John A. Clark, Chintan Patel, Biplab Sikdar</author><pubDate>Thu, 14 Sep 2023 15:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07730v1</guid></item><item><title>GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning</title><link>http://arxiv.org/abs/2309.00923v2</link><description>This paper investigates a challenging problem of zero-shot learning in themulti-label scenario (MLZSL), wherein, the model is trained to recognizemultiple unseen classes within a sample (e.g., an image) based on seen classesand auxiliary knowledge, e.g., semantic information. Existing methods usuallyresort to analyzing the relationship of various seen classes residing in asample from the dimension of spatial or semantic characteristics, and transferthe learned model to unseen ones. But they ignore the effective integration oflocal and global features. That is, in the process of inferring unseen classes,global features represent the principal direction of the image in the featurespace, while local features should maintain uniqueness within a certain range.This integrated neglect will make the model lose its grasp of the maincomponents of the image. Relying only on the local existence of seen classesduring the inference stage introduces unavoidable bias. In this paper, wepropose a novel and effective group bi-enhancement framework for MLZSL, dubbedGBE-MLZSL, to fully make use of such properties and enable a more accurate androbust visual-semantic projection. Specifically, we split the feature maps intoseveral feature groups, of which each feature group can be trainedindependently with the Local Information Distinguishing Module (LID) to ensureuniqueness. Meanwhile, a Global Enhancement Module (GEM) is designed topreserve the principal direction. Besides, a static graph structure is designedto construct the correlation of local features. Experiments on large-scaleMLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that theproposed GBE-MLZSL outperforms other state-of-the-art methods with largemargins.</description><author>Ziming Liu, Jingcai Guo, Xiaocheng Lu, Song Guo, Peiran Dong, Jiewei Zhang</author><pubDate>Thu, 14 Sep 2023 15:05:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00923v2</guid></item><item><title>PerPLM: Personalized Fine-tuning of Pretrained Language Models via Writer-specific Intermediate Learning and Prompts</title><link>http://arxiv.org/abs/2309.07727v1</link><description>The meanings of words and phrases depend not only on where they are used(contexts) but also on who use them (writers). Pretrained language models(PLMs) are powerful tools for capturing context, but they are typicallypretrained and fine-tuned for universal use across different writers. Thisstudy aims to improve the accuracy of text understanding tasks by personalizingthe fine-tuning of PLMs for specific writers. We focus on a general settingwhere only the plain text from target writers are available forpersonalization. To avoid the cost of fine-tuning and storing multiple copiesof PLMs for different users, we exhaustively explore using writer-specificprompts to personalize a unified PLM. Since the design and evaluation of theseprompts is an underdeveloped area, we introduce and compare different types ofprompts that are possible in our setting. To maximize the potential ofprompt-based personalized fine-tuning, we propose a personalized intermediatelearning based on masked language modeling to extract task-independent traitsof writers' text. Our experiments, using multiple tasks, datasets, and PLMs,reveal the nature of different prompts and the effectiveness of ourintermediate learning approach.</description><author>Daisuke Oba, Naoki Yoshinaga, Masashi Toyoda</author><pubDate>Thu, 14 Sep 2023 15:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07727v1</guid></item><item><title>Probing in Context: Toward Building Robust Classifiers via Probing Large Language Models</title><link>http://arxiv.org/abs/2305.14171v2</link><description>Large language models are able to learn new tasks in context, where they areprovided with instructions and a few annotated examples. However, theeffectiveness of in-context learning is dependent on the provided context, andthe performance on a downstream task can vary considerably, depending on theinstruction. Importantly, such dependency on the context can surface inunpredictable ways, e.g., a seemingly more informative instruction might leadto a worse performance. In this paper, we propose an alternative approach,which we term in-context probing. Similar to in-context learning, wecontextualize the representation of the input with an instruction, but insteadof decoding the output prediction, we probe the contextualized representationto predict the label. Through a series of experiments on a diverse set ofclassification tasks, we show that in-context probing is significantly morerobust to changes in instructions. We further show that probing performscompetitive or superior to finetuning and can be particularly helpful to buildclassifiers on top of smaller models, and with only a hundred trainingexamples.</description><author>Afra Amini, Massimiliano Ciaramita</author><pubDate>Thu, 14 Sep 2023 14:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14171v2</guid></item><item><title>L1-aware Multilingual Mispronunciation Detection Framework</title><link>http://arxiv.org/abs/2309.07719v1</link><description>The phonological discrepancies between a speaker's native (L1) and thenon-native language (L2) serves as a major factor for mispronunciation. Thispaper introduces a novel multilingual MDD architecture, L1-MultiMDD, enrichedwith L1-aware speech representation. An end-to-end speech encoder is trained onthe input signal and its corresponding reference phoneme sequence. First, anattention mechanism is deployed to align the input audio with the referencephoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from anauxiliary model, pretrained in a multi-task setup identifying L1 and L2language, and are infused with the primary network. Finally, the L1-MultiMDD isthen optimized for a unified multilingual phoneme recognition task usingconnectionist temporal classification (CTC) loss for the target languages:English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness ofthe proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, andAraVoiceL2v2; and unseen -- EpaDB and Speechocean762 datasets. The consistentgains in PER, and false rejection rate (FRR) across all target languagesconfirm our approach's robustness, efficacy, and generalizability.</description><author>Yassine El Kheir, Shammur Absar Chwodhury, Ahmed Ali</author><pubDate>Thu, 14 Sep 2023 14:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07719v1</guid></item><item><title>Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks</title><link>http://arxiv.org/abs/2309.07716v1</link><description>Despite the many successful applications of deep learning models formultidimensional signal and image processing, most traditional neural networksprocess data represented by (multidimensional) arrays of real numbers. Theintercorrelation between feature channels is usually expected to be learnedfrom the training data, requiring numerous parameters and careful training. Incontrast, vector-valued neural networks are conceived to process arrays ofvectors and naturally consider the intercorrelation between feature channels.Consequently, they usually have fewer parameters and often undergo more robusttraining than traditional neural networks. This paper aims to present a broadframework for vector-valued neural networks, referred to as V-nets. In thiscontext, hypercomplex-valued neural networks are regarded as vector-valuedmodels with additional algebraic properties. Furthermore, this paper explainsthe relationship between vector-valued and traditional neural networks.Precisely, a vector-valued neural network can be obtained by placingrestrictions on a real-valued model to consider the intercorrelation betweenfeature channels. Finally, we show how V-nets, including hypercomplex-valuedneural networks, can be implemented in current deep-learning libraries asreal-valued networks.</description><author>Marcos Eduardo Valle</author><pubDate>Thu, 14 Sep 2023 14:48:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07716v1</guid></item><item><title>Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version)</title><link>http://arxiv.org/abs/2307.14799v3</link><description>Modern semiconductor manufacturing involves intricate production processesconsisting of hundreds of operations, which can take several months from lotrelease to completion. The high-tech machines used in these processes arediverse, operate on individual wafers, lots, or batches in multiple stages, andnecessitate product-specific setups and specialized maintenance procedures.This situation is different from traditional job-shop scheduling scenarios,which have less complex production processes and machines, and mainly focus onsolving highly combinatorial but abstract scheduling problems. In this work, weaddress the scheduling of realistic semiconductor manufacturing processes bymodeling their specific requirements using hybrid Answer Set Programming withdifference logic, incorporating flexible machine processing, setup, batchingand maintenance operations. Unlike existing methods that schedule semiconductormanufacturing processes locally with greedy heuristics or by independentlyoptimizing specific machine group allocations, we examine the potentials oflarge-scale scheduling subject to multiple optimization objectives.</description><author>Mohammed M. S. El-Kholany, Ramsha Ali, Martin Gebser</author><pubDate>Thu, 14 Sep 2023 14:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14799v3</guid></item><item><title>Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context</title><link>http://arxiv.org/abs/2309.07708v1</link><description>Financial simulators play an important role in enhancing forecastingaccuracy, managing risks, and fostering strategic financial decision-making.Despite the development of financial market simulation methodologies, existingframeworks often struggle with adapting to specialized simulation context. Wepinpoint the challenges as i) current financial datasets do not contain contextlabels; ii) current techniques are not designed to generate financial data withcontext as control, which demands greater precision compared to othermodalities; iii) the inherent difficulties in generating context-aligned,high-fidelity data given the non-stationary, noisy nature of financial data. Toaddress these challenges, our contributions are: i) we proposed the ContextualMarket Dataset with market dynamics, stock ticker, and history state ascontext, leveraging a market dynamics modeling method that combines linearregression and Dynamic Time Warping clustering to extract market dynamics; ii)we present Market-GAN, a novel architecture incorporating a GenerativeAdversarial Networks (GAN) for the controllable generation with context, anautoencoder for learning low-dimension features, and supervisors for knowledgetransfer; iii) we introduce a two-stage training scheme to ensure thatMarket-GAN captures the intrinsic market distribution with multiple objectives.In the pertaining stage, with the use of the autoencoder and supervisors, weprepare the generator with a better initialization for the adversarial trainingstage. We propose a set of holistic evaluation metrics that consider alignment,fidelity, data usability on downstream tasks, and market facts. We evaluateMarket-GAN with the Dow Jones Industrial Average data from 2000 to 2023 andshowcase superior performance in comparison to 4 state-of-the-art time-seriesgenerative models.</description><author>Haochong Xia, Shuo Sun, Xinrun Wang, Bo An</author><pubDate>Thu, 14 Sep 2023 14:42:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07708v1</guid></item><item><title>CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders</title><link>http://arxiv.org/abs/2309.07707v1</link><description>Large-scale self-supervised pre-trained speech encoders outperformconventional approaches in speech recognition and translation tasks. Due to thehigh cost of developing these large models, building new encoders for new tasksand deploying them to on-device applications are infeasible. Prior studiespropose model compression methods to address this issue, but those works focuson smaller models and less realistic tasks. Thus, we propose ContrastiveLayer-to-layer Distillation (CoLLD), a novel knowledge distillation method tocompress pre-trained speech encoders by leveraging masked prediction andcontrastive learning to train student models to copy the behavior of a largeteacher model. CoLLD outperforms prior methods and closes the gap between smalland large models on multilingual speech-to-text translation and recognitionbenchmarks.</description><author>Heng-Jui Chang, Ning Dong, Ruslan Mavlyutov, Sravya Popuri, Yu-An Chung</author><pubDate>Thu, 14 Sep 2023 14:38:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07707v1</guid></item><item><title>Towards Language-guided Visual Recognition via Dynamic Convolutions</title><link>http://arxiv.org/abs/2110.08797v2</link><description>In this paper, we are committed to establishing an unified and end-to-endmulti-modal network via exploring the language-guided visual recognition. Toapproach this target, we first propose a novel multi-modal convolution modulecalled Language-dependent Convolution (LaConv). Its convolution kernels aredynamically generated based on natural language information, which can helpextract differentiated visual features for different multi-modal examples.Based on the LaConv module, we further build the first fully language-drivenconvolution network, termed as LaConvNet, which can unify the visualrecognition and multi-modal reasoning in one forward structure. To validateLaConv and LaConvNet, we conduct extensive experiments on four benchmarkdatasets of two vision-and-language tasks, i.e., visual question answering(VQA) and referring expression comprehension (REC). The experimental resultsnot only shows the performance gains of LaConv compared to the existingmulti-modal modules, but also witness the merits of LaConvNet as an unifiednetwork, including compact network, high generalization ability and excellentperformance, e.g., +4.7% on RefCOCO+.</description><author>Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yongjian Wu, Yue Gao, Rongrong Ji</author><pubDate>Thu, 14 Sep 2023 14:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.08797v2</guid></item><item><title>A Survivor in the Era of Large-Scale Pretraining: An Empirical Study of One-Stage Referring Expression Comprehension</title><link>http://arxiv.org/abs/2204.07913v2</link><description>Most of the existing work in one-stage referring expression comprehension(REC) mainly focuses on multi-modal fusion and reasoning, while the influenceof other factors in this task lacks in-depth exploration. To fill this gap, weconduct an empirical study in this paper. Concretely, we first build a verysimple REC network called SimREC, and ablate 42 candidate designs/settings,which covers the entire process of one-stage REC from network design to modeltraining. Afterwards, we conduct over 100 experimental trials on threebenchmark datasets of REC. The extensive experimental results not only show thekey factors that affect REC performance in addition to multi-modal fusion,e.g., multi-scale features and data augmentation, but also yield some findingsthat run counter to conventional understanding. For example, as a vision andlanguage (V&amp;L) task, REC does is less impacted by language prior. In addition,with a proper combination of these findings, we can improve the performance ofSimREC by a large margin, e.g., +27.12% on RefCOCO+, which outperforms allexisting REC methods. But the most encouraging finding is that with much lesstraining overhead and parameters, SimREC can still achieve better performancethan a set of large-scale pre-trained models, e.g., UNITER and VILLA,portraying the special role of REC in existing V&amp;L research.</description><author>Gen Luo, Yiyi Zhou, Jiamu Sun, Xiaoshuai Sun, Rongrong Ji</author><pubDate>Thu, 14 Sep 2023 14:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.07913v2</guid></item><item><title>NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches</title><link>http://arxiv.org/abs/2309.07704v1</link><description>Accurate dietary intake estimation is critical for informing policies andprograms to support healthy eating, as malnutrition has been directly linked todecreased quality of life. However self-reporting methods such as food diariessuffer from substantial bias. Other conventional dietary assessment techniquesand emerging alternative approaches such as mobile applications incur high timecosts and may necessitate trained personnel. Recent work has focused on usingcomputer vision and machine learning to automatically estimate dietary intakefrom food images, but the lack of comprehensive datasets with diverseviewpoints, modalities and food annotations hinders the accuracy and realism ofsuch methods. To address this limitation, we introduce NutritionVerse-Synth,the first large-scale dataset of 84,984 photorealistic synthetic 2D food imageswith associated dietary information and multimodal annotations (including depthimages, instance masks, and semantic masks). Additionally, we collect a realimage dataset, NutritionVerse-Real, containing 889 images of 251 dishes toevaluate realism. Leveraging these novel datasets, we develop and benchmarkNutritionVerse, an empirical study of various dietary intake estimationapproaches, including indirect segmentation-based and direct predictionnetworks. We further fine-tune models pretrained on synthetic data with realimages to provide insights into the fusion of synthetic and real data. Finally,we release both datasets (NutritionVerse-Synth, NutritionVerse-Real) onhttps://www.kaggle.com/nutritionverse/datasets as part of an open initiative toaccelerate machine learning for dietary sensing.</description><author>Chi-en Amy Tai, Matthew Keller, Saeejith Nair, Yuhao Chen, Yifan Wu, Olivia Markham, Krish Parmar, Pengcheng Xi, Heather Keller, Sharon Kirkpatrick, Alexander Wong</author><pubDate>Thu, 14 Sep 2023 14:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07704v1</guid></item><item><title>Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture</title><link>http://arxiv.org/abs/2302.10848v2</link><description>In Changjun Fan et al. [Nature Communicationshttps://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deepreinforced learning approach to augment combinatorial optimization heuristics.In particular, they present results for several spin glass ground stateproblems, for which instances on non-planar networks are generally NP-hard, incomparison with several Monte Carlo based methods, such as simulated annealing(SA) or parallel tempering (PT). Indeed, those results demonstrate that thereinforced learning improves the results over those obtained with SA or PT, orat least allows for reduced runtimes for the heuristics before results ofcomparable quality have been obtained relative to those other methods. Tofacilitate the conclusion that their method is ''superior'', the authors pursuetwo basic strategies: (1) A commercial GUROBI solver is called on to procure asample of exact ground states as a testbed to compare with, and (2) ahead-to-head comparison between the heuristics is given for a sample of largerinstances where exact ground states are hard to ascertain. Here, we put thesestudies into a larger context, showing that the claimed superiority is at bestmarginal for smaller samples and becomes essentially irrelevant with respect toany sensible approximation of true ground states in the larger samples. Forexample, this method becomes irrelevant as a means to determine stiffnessexponents $\theta$ in $d&gt;2$, as mentioned by the authors, where the problem isnot only NP-hard but requires the subtraction of two almost equal ground-stateenergies and systemic errors in each of $\approx 1\%$ found here areunacceptable. This larger picture on the method arises from a straightforwardfinite-size corrections study over the spin glass ensembles the authors employ,using data that has been available for decades.</description><author>Stefan Boettcher</author><pubDate>Thu, 14 Sep 2023 14:29:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10848v2</guid></item><item><title>Causal Entropy and Information Gain for Measuring Causal Control</title><link>http://arxiv.org/abs/2309.07703v1</link><description>Artificial intelligence models and methods commonly lack causalinterpretability. Despite the advancements in interpretable machine learning(IML) methods, they frequently assign importance to features which lack causalinfluence on the outcome variable. Selecting causally relevant features amongthose identified as relevant by these methods, or even before model training,would offer a solution. Feature selection methods utilizing informationtheoretical quantities have been successful in identifying statisticallyrelevant features. However, the information theoretical quantities they arebased on do not incorporate causality, rendering them unsuitable for suchscenarios. To address this challenge, this article proposes informationtheoretical quantities that incorporate the causal structure of the system,which can be used to evaluate causal importance of features for some givenoutcome variable. Specifically, we introduce causal versions of entropy andmutual information, termed causal entropy and causal information gain, whichare designed to assess how much control a feature provides over the outcomevariable. These newly defined quantities capture changes in the entropy of avariable resulting from interventions on other variables. Fundamental resultsconnecting these quantities to the existence of causal effects are derived. Theuse of causal information gain in feature selection is demonstrated,highlighting its superiority over standard mutual information in revealingwhich features provide control over a chosen outcome variable. Ourinvestigation paves the way for the development of methods with improvedinterpretability in domains involving causation.</description><author>Francisco Nunes Ferreira Quialheiro Simoes, Mehdi Dastani, Thijs van Ommen</author><pubDate>Thu, 14 Sep 2023 14:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07703v1</guid></item><item><title>Dataset Condensation via Generative Model</title><link>http://arxiv.org/abs/2309.07698v1</link><description>Dataset condensation aims to condense a large dataset with a lot of trainingsamples into a small set. Previous methods usually condense the dataset intothe pixels format. However, it suffers from slow optimization speed and largenumber of parameters to be optimized. When increasing image resolutions andclasses, the number of learnable parameters grows accordingly, prohibitingcondensation methods from scaling up to large datasets with diverse classes.Moreover, the relations among condensed samples have been neglected and hencethe feature distribution of condensed samples is often not diverse. To solvethese problems, we propose to condense the dataset into another format, agenerative model. Such a novel format allows for the condensation of largedatasets because the size of the generative model remains relatively stable asthe number of classes or image resolution increases. Furthermore, anintra-class and an inter-class loss are proposed to model the relation ofcondensed samples. Intra-class loss aims to create more diverse samples foreach class by pushing each sample away from the others of the same class.Meanwhile, inter-class loss increases the discriminability of samples bywidening the gap between the centers of different classes. Extensivecomparisons with state-of-the-art methods and our ablation studies confirm theeffectiveness of our method and its individual component. To our bestknowledge, we are the first to successfully conduct condensation onImageNet-1k.</description><author>David Junhao Zhang, Heng Wang, Chuhui Xue, Rui Yan, Wenqing Zhang, Song Bai, Mike Zheng Shou</author><pubDate>Thu, 14 Sep 2023 14:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07698v1</guid></item></channel></rss>