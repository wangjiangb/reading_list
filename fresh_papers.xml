<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 19 Jul 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Forecasting the steam mass flow in a powerplant using the parallel hybrid network</title><link>http://arxiv.org/abs/2307.09483v1</link><description>Efficient and sustainable power generation is a crucial concern in the energysector. In particular, thermal power plants grapple with accurately predictingsteam mass flow, which is crucial for operational efficiency and costreduction. In this study, we use a parallel hybrid neural network architecturethat combines a parametrized quantum circuit and a conventional feed-forwardneural network specifically designed for time-series prediction in industrialsettings to enhance predictions of steam mass flow 15 minutes into the future.Our results show that the parallel hybrid model outperforms standaloneclassical and quantum models, achieving more than 5.7 and 4.9 times lower meansquared error (MSE) loss on the test set after training compared to pureclassical and pure quantum networks, respectively. Furthermore, the hybridmodel demonstrates smaller relative errors between the ground truth and themodel predictions on the test set, up to 2 times better than the pure classicalmodel. These findings contribute to the broader scientific understanding of howintegrating quantum and classical machine learning techniques can be applied toreal-world challenges faced by the energy sector, ultimately leading tooptimized power plant operations.</description><author>Andrii Kurkin, Jonas Hegemann, Mo Kordzanganeh, Alexey Melnikov</author><pubDate>Tue, 18 Jul 2023 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09483v1</guid></item><item><title>AnyDoor: Zero-shot Object-level Image Customization</title><link>http://arxiv.org/abs/2307.09481v1</link><description>This work presents AnyDoor, a diffusion-based image generator with the powerto teleport target objects to new scenes at user-specified locations in aharmonious way. Instead of tuning parameters for each object, our model istrained only once and effortlessly generalizes to diverse object-scenecombinations at the inference stage. Such a challenging zero-shot settingrequires an adequate characterization of a certain object. To this end, wecomplement the commonly used identity feature with detail features, which arecarefully designed to maintain texture details yet allow versatile localvariations (e.g., lighting, orientation, posture, etc.), supporting the objectin favorably blending with different surroundings. We further propose to borrowknowledge from video datasets, where we can observe various forms (i.e., alongthe time axis) of a single object, leading to stronger model generalizabilityand robustness. Extensive experiments demonstrate the superiority of ourapproach over existing alternatives as well as its great potential inreal-world applications, such as virtual try-on and object moving. Project pageis https://damo-vilab.github.io/AnyDoor-Page/.</description><author>Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, Hengshuang Zhao</author><pubDate>Tue, 18 Jul 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09481v1</guid></item><item><title>FACTS: Facial Animation Creation using the Transfer of Styles</title><link>http://arxiv.org/abs/2307.09480v1</link><description>The ability to accurately capture and express emotions is a critical aspectof creating believable characters in video games and other forms ofentertainment. Traditionally, this animation has been achieved with artisticeffort or performance capture, both requiring costs in time and labor. Morerecently, audio-driven models have seen success, however, these often lackexpressiveness in areas not correlated to the audio signal. In this paper, wepresent a novel approach to facial animation by taking existing animations andallowing for the modification of style characteristics. Specifically, weexplore the use of a StarGAN to enable the conversion of 3D facial animationsinto different emotions and person-specific styles. We are able to maintain thelip-sync of the animations with this method thanks to the use of a novelviseme-preserving loss.</description><author>Jack Saunders, Steven Caulkin, Vinay Namboodiri</author><pubDate>Tue, 18 Jul 2023 18:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09480v1</guid></item><item><title>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</title><link>http://arxiv.org/abs/2307.09476v1</link><description>Modern language models can imitate complex patterns through few-shotlearning, enabling them to complete challenging tasks without fine-tuning.However, imitation can also lead models to reproduce inaccuracies or harmfulcontent if present in the context. We study harmful imitation through the lensof a model's internal representations, and identify two related phenomena:overthinking and false induction heads. The first phenomenon, overthinking,appears when we decode predictions from intermediate layers, given correct vs.incorrect few-shot demonstrations. At early layers, both demonstrations inducesimilar model behavior, but the behavior diverges sharply at some "criticallayer", after which the accuracy given incorrect demonstrations progressivelydecreases. The second phenomenon, false induction heads, are a possiblemechanistic cause of overthinking: these are heads in late layers that attendto and copy false information from previous demonstrations, and whose ablationreduces overthinking. Beyond scientific understanding, our results suggest thatstudying intermediate model computations could be a promising avenue forunderstanding and guarding against harmful model behaviors.</description><author>Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt</author><pubDate>Tue, 18 Jul 2023 18:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09476v1</guid></item><item><title>ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning</title><link>http://arxiv.org/abs/2307.09474v1</link><description>Human-AI interactivity is a critical aspect that reflects the usability ofmultimodal large language models (MLLMs). However, existing end-to-end MLLMsonly allow users to interact with them through language instructions, leadingto the limitation of the interactive accuracy and efficiency. In this study, wepresent precise referring instructions that utilize diverse referencerepresentations such as points and boxes as referring prompts to refer to thespecial region. This enables MLLMs to focus on the region of interest andachieve finer-grained interaction. Based on precise referring instruction, wepropose ChatSpot, a unified end-to-end multimodal large language model thatsupports diverse forms of interactivity including mouse clicks, drag-and-drop,and drawing boxes, which provides a more flexible and seamless interactiveexperience. We also construct a multi-grained vision-languageinstruction-following dataset based on existing datasets and GPT-4 generating.Furthermore, we design a series of evaluation tasks to assess the effectivenessof region recognition and interaction. Experimental results showcase ChatSpot'spromising performance.</description><author>Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, Xiangyu Zhang</author><pubDate>Tue, 18 Jul 2023 18:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09474v1</guid></item><item><title>GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping</title><link>http://arxiv.org/abs/2307.09472v1</link><description>Efficiency is quite important for 3D lane detection due to practicaldeployment demand. In this work, we propose a simple, fast, and end-to-enddetector that still maintains high detection precision. Specifically, we devisea set of fully convolutional heads based on row-wise classification. Incontrast to previous counterparts, ours supports recognizing both vertical andhorizontal lanes. Besides, our method is the first one to perform row-wiseclassification in bird-eye-view. In the heads, we split feature into multiplegroups and every group of feature corresponds to a lane instance. Duringtraining, the predictions are associated with lane labels using the proposedsingle-win one-to-one matching to compute loss, and no post-processingoperation is demanded for inference. In this way, our proposed fullyconvolutional detector, GroupLane, realizes end-to-end detection like DETR.Evaluated on 3 real world 3D lane benchmarks, OpenLane, Once-3DLanes, andOpenLane-Huawei, GroupLane adopting ConvNext-Base as the backbone outperformsthe published state-of-the-art PersFormer by 13.6% F1 score in the OpenLanevalidation set. Besides, GroupLane with ResNet18 still surpasses PersFormer by4.9% F1 score, while the inference speed is nearly 7x faster and the FLOPs isonly 13.3% of it.</description><author>Zhuoling Li, Chunrui Han, Zheng Ge, Jinrong Yang, En Yu, Haoqian Wang, Hengshuang Zhao, Xiangyu Zhang</author><pubDate>Tue, 18 Jul 2023 18:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09472v1</guid></item><item><title>SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design</title><link>http://arxiv.org/abs/2306.15656v3</link><description>This paper introduces SparseOptimizer, a novel deep learning optimizer thatexploits Moreau-Yosida regularization to naturally induce sparsity in largelanguage models such as BERT, ALBERT and GPT. Key to the design ofSparseOptimizer is an embedded shrinkage operator, which imparts sparsitydirectly within the optimization process. This operator, backed by a soundtheoretical framework, includes an analytical solution, thereby reinforcing theoptimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-playfunctionality eradicates the need for code modifications, making it auniversally adaptable tool for a wide array of large language models. Empiricalevaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2confirm that SparseBERT and SparseALBERT, when sparsified usingSparseOptimizer, achieve performance comparable to their dense counterparts,BERT and ALBERT, while significantly reducing their parameter count. Further,this work proposes an innovative optimizer-compiler co-design strategy,demonstrating the potential of inference acceleration (\textbf{3.37x},\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, andLLVM generic compile, respectively) in SparseBERT when paired with anappropriately designed compiler. This study represents a significant stepforward in the evolution of efficient, scalable, and high-performing largelanguage models, setting a precedent for future exploration and optimization inthis domain. The SparseOptimizer code and SparseALBERT model will be publiclyavailable upon paper acceptance.</description><author>Fu-Ming Guo</author><pubDate>Tue, 18 Jul 2023 18:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15656v3</guid></item><item><title>Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning</title><link>http://arxiv.org/abs/2212.03181v2</link><description>Signal Temporal Logic (STL) is a powerful framework for describing thecomplex temporal and logical behaviour of the dynamical system. Numerousstudies have attempted to employ reinforcement learning to learn a controllerthat enforces STL specifications; however, they have been unable to effectivelytackle the challenges of ensuring robust satisfaction in continuous state spaceand maintaining tractability. In this paper, leveraging the concept of funnelfunctions, we propose a tractable reinforcement learning algorithm to learn atime-dependent policy for robust satisfaction of STL specification incontinuous state space. We demonstrate the utility of our approach on severalSTL tasks using different environments.</description><author>Naman Saxena, Gorantla Sandeep, Pushpak Jagtap</author><pubDate>Tue, 18 Jul 2023 18:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03181v2</guid></item><item><title>Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection</title><link>http://arxiv.org/abs/2307.09465v1</link><description>Given that approximately half of science, technology, engineering, andmathematics (STEM) undergraduate students in U.S. colleges and universitiesleave by the end of the first year [15], it is crucial to improve the qualityof classroom environments. This study focuses on monitoring students' emotionsin the classroom as an indicator of their engagement and proposes an approachto address this issue. The impact of different facial parts on the performanceof an emotional recognition model is evaluated through experimentation. To testthe proposed model under partial occlusion, an artificially occluded dataset isintroduced. The novelty of this work lies in the proposal of an occlusion-awarearchitecture for facial action units (AUs) extraction, which employs attentionmechanism and adaptive feature learning. The AUs can be used later to classifyfacial expressions in classroom settings. This research paper's findings provide valuable insights into handlingocclusion in analyzing facial images for emotional engagement analysis. Theproposed experiments demonstrate the significance of considering occlusion andenhancing the reliability of facial analysis models in classroom environments.These findings can also be extended to other settings where occlusions areprevalent.</description><author>Shrouk Wally, Ahmed Elsayed, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 18:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09465v1</guid></item><item><title>A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction</title><link>http://arxiv.org/abs/2307.09463v1</link><description>Neural decoders for quantum error correction (QEC) rely on neural networks toclassify syndromes extracted from error correction codes and find appropriaterecovery operators to protect logical information against errors. Despite thegood performance of neural decoders, important practical requirements remain tobe achieved, such as minimizing the decoding time to meet typical rates ofsyndrome generation in repeated error correction schemes, and ensuring thescalability of the decoding approach as the code distance increases. Designinga dedicated integrated circuit to perform the decoding task in co-integrationwith a quantum processor appears necessary to reach these decoding time andscalability requirements, as routing signals in and out of a cryogenicenvironment to be processed externally leads to unnecessary delays and aneventual wiring bottleneck. In this work, we report the design and performanceanalysis of a neural decoder inference accelerator based on an in-memorycomputing (IMC) architecture, where crossbar arrays of resistive memory devicesare employed to both store the synaptic weights of the decoder neural networkand perform analog matrix-vector multiplications during inference. Inproof-of-concept numerical experiments supported by experimental measurements,we investigate the impact of TiO$_\textrm{x}$-based memristive devices'non-idealities on decoding accuracy. Hardware-aware training methods aredeveloped to mitigate the loss in accuracy, allowing the memristive neuraldecoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for thedistance-three surface code, whereas the equivalent digital neural decoderachieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides apathway to scalable, fast, and low-power cryogenic IMC hardware for integratedQEC.</description><author>Frédéric Marcotte, Pierre-Antoine Mouny, Victor Yon, Gebremedhin A. Dagnew, Bohdan Kulchytskyy, Sophie Rochette, Yann Beilliard, Dominique Drouin, Pooya Ronagh</author><pubDate>Tue, 18 Jul 2023 18:46:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09463v1</guid></item><item><title>Machine Learning Enhanced Hankel Dynamic-Mode Decomposition</title><link>http://arxiv.org/abs/2303.06289v3</link><description>While the acquisition of time series has become more straightforward,developing dynamical models from time series is still a challenging andevolving problem domain. Within the last several years, to address thisproblem, there has been a merging of machine learning tools with what is calledthe dynamic mode decomposition (DMD). This general approach has been shown tobe an especially promising avenue for accurate model development. Building onthis prior body of work, we develop a deep learning DMD based method whichmakes use of the fundamental insight of Takens' Embedding Theorem to build anadaptive learning scheme that better approximates higher dimensional andchaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). Welikewise explore how our method learns mappings which tend, after successfultraining, to significantly change the mutual information between dimensions inthe dynamics. This appears to be a key feature in enhancing the DMD overall,and it should help provide further insight for developing other deep learningmethods for time series analysis and model generation.</description><author>Christopher W. Curtis, D. Jay Alford-Lago, Erik Bollt, Andrew Tuma</author><pubDate>Tue, 18 Jul 2023 18:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06289v3</guid></item><item><title>Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla</title><link>http://arxiv.org/abs/2307.09458v1</link><description>\emph{Circuit analysis} is a promising technique for understanding theinternal mechanisms of language models. However, existing analyses are done insmall models far from the state of the art. To address this, we present a casestudy of circuit analysis in the 70B Chinchilla model, aiming to test thescalability of circuit analysis. In particular, we study multiple-choicequestion answering, and investigate Chinchilla's capability to identify thecorrect answer \emph{label} given knowledge of the correct answer \emph{text}.We find that the existing techniques of logit attribution, attention patternvisualization, and activation patching naturally scale to Chinchilla, allowingus to identify and categorize a small set of `output nodes' (attention headsand MLPs). We further study the `correct letter' category of attention heads aiming tounderstand the semantics of their features, with mixed results. For normalmultiple-choice question answers, we significantly compress the query, key andvalue subspaces of the head without loss of performance when operating on theanswer labels for multiple-choice questions, and we show that the query and keysubspaces represent an `Nth item in an enumeration' feature to at least someextent. However, when we attempt to use this explanation to understand theheads' behaviour on a more general distribution including randomized answerlabels, we find that it is only a partial explanation, suggesting there is moreto learn about the operation of `correct letter' heads on multiple choicequestion answering.</description><author>Tom Lieberum, Matthew Rahtz, János Kramár, Geoffrey Irving, Rohin Shah, Vladimir Mikulik</author><pubDate>Tue, 18 Jul 2023 18:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09458v1</guid></item><item><title>Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</title><link>http://arxiv.org/abs/2307.09457v1</link><description>Multiple Instance Learning (MIL) has been widely applied to medical imagingdiagnosis, where bag labels are known and instance labels inside bags areunknown. Traditional MIL assumes that instances in each bag are independentsamples from a given distribution. However, instances are often spatially orsequentially ordered, and one would expect similar diagnostic importance forneighboring instances. To address this, in this study, we propose a smoothattention deep MIL (SA-DMIL) model. Smoothness is achieved by the introductionof first and second order constraints on the latent function encoding theattention paid to each instance in a bag. The method is applied to thedetection of intracranial hemorrhage (ICH) on head CT scans. The results showthat this novel SA-DMIL: (a) achieves better performance than the non-smoothattention MIL at both scan (bag) and slice (instance) levels; (b) learnsspatial dependencies between slices; and (c) outperforms currentstate-of-the-art MIL methods on the same ICH test set.</description><author>Yunan Wu, Francisco M. Castro-Macías, Pablo Morales-Álvarez, Rafael Molina, Aggelos K. Katsaggelos</author><pubDate>Tue, 18 Jul 2023 18:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09457v1</guid></item><item><title>A comparative analysis of SR-GAN models</title><link>http://arxiv.org/abs/2307.09456v1</link><description>In this study, we evaluate the performance of multiple state-of-the-art SRGAN (Super Resolution Generative Adversarial Network) models, ESRGAN,Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergodegradation using a pipeline. Our results show that some models seem tosignificantly increase the resolution of the input images while preservingtheir visual quality, this is assessed using Tesseract OCR engine. We observethat EDSR-BASE model from huggingface outperforms the remaining candidatemodels in terms of both quantitative metrics and subjective visual qualityassessments with least compute overhead. Specifically, EDSR generates imageswith higher peak signal-to-noise ratio (PSNR) and structural similarity index(SSIM) values and are seen to return high quality OCR results with TesseractOCR engine. These findings suggest that EDSR is a robust and effective approachfor single-image super-resolution and may be particularly well-suited forapplications where high-quality visual fidelity is critical and optimizedcompute.</description><author>Fatemeh Rezapoor Nikroo, Ajinkya Deshmukh, Anantha Sharma, Adrian Tam, Kaarthik Kumar, Cleo Noris</author><pubDate>Tue, 18 Jul 2023 18:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09456v1</guid></item><item><title>Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers</title><link>http://arxiv.org/abs/2307.09455v1</link><description>For real-world language applications, detecting an out-of-distribution (OOD)sample is helpful to alert users or reject such unreliable samples. However,modern over-parameterized language models often produce overconfidentpredictions for both in-distribution (ID) and OOD samples. In particular,language models suffer from OOD samples with a similar semantic representationto ID samples since these OOD samples lie near the ID manifold. A rejectionnetwork can be trained with ID and diverse outlier samples to detect test OODsamples, but explicitly collecting auxiliary OOD datasets brings an additionalburden for data collection. In this paper, we propose a simple but effectivemethod called Pseudo Outlier Exposure (POE) that constructs a surrogate OODdataset by sequentially masking tokens related to ID classes. The surrogate OODsample introduced by POE shows a similar representation to ID data, which ismost effective in training a rejection network. Our method does not require anyexternal OOD data and can be easily implemented within off-the-shelfTransformers. A comprehensive comparison with state-of-the-art algorithmsdemonstrates POE's competitiveness on several text classification benchmarks.</description><author>Jaeyoung Kim, Kyuheon Jung, Dongbin Na, Sion Jang, Eunbin Park, Sungchul Choi</author><pubDate>Tue, 18 Jul 2023 18:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09455v1</guid></item><item><title>Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality</title><link>http://arxiv.org/abs/2307.06915v2</link><description>Stochastic Gradient Descent (SGD) is one of the simplest and most popularalgorithms in modern statistical and machine learning due to its computationaland memory efficiency. Various averaging schemes have been proposed toaccelerate the convergence of SGD in different settings. In this paper, weexplore a general averaging scheme for SGD. Specifically, we establish theasymptotic normality of a broad range of weighted averaged SGD solutions andprovide asymptotically valid online inference approaches. Furthermore, wepropose an adaptive averaging scheme that exhibits both optimal statisticalrate and favorable non-asymptotic convergence, drawing insights from theoptimal weight for the linear model in terms of non-asymptotic mean squarederror (MSE).</description><author>Ziyang Wei, Wanrong Zhu, Wei Biao Wu</author><pubDate>Tue, 18 Jul 2023 18:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06915v2</guid></item><item><title>Robustness Analysis of Video-Language Models Against Visual and Language Perturbations</title><link>http://arxiv.org/abs/2207.02159v4</link><description>Joint visual and language modeling on large-scale datasets has recently showngood progress in multi-modal tasks when compared to single modal learning.However, robustness of these approaches against real-world perturbations hasnot been studied. In this work, we perform the first extensive robustness studyof video-language models against various real-world perturbations. We focus ontext-to-video retrieval and propose two large-scale benchmark datasets,MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 differenttext perturbations. The study reveals some interesting initial findings fromthe studied models: 1) models are generally more susceptible when only video isperturbed as opposed to when only text is perturbed, 2) models that arepre-trained are more robust than those trained from scratch, 3) models attendmore to scene and objects rather than motion and action. We hope this studywill serve as a benchmark and guide future research in robust video-languagelearning. The benchmark introduced in this study along with the code anddatasets is available at https://bit.ly/3CNOly4.</description><author>Madeline C. Schiappa, Shruti Vyas, Hamid Palangi, Yogesh S. Rawat, Vibhav Vineet</author><pubDate>Tue, 18 Jul 2023 18:23:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02159v4</guid></item><item><title>Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT</title><link>http://arxiv.org/abs/2307.01740v2</link><description>Stroke lesion volume is a key radiologic measurement for assessing theprognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to beautomatically measured on Non-Contrast CT (NCCT) scans. Recent diffusionprobabilistic models have shown potentials of being used for imagesegmentation. In this paper, a novel Synchronous image-label DiffusionProbability Model (SDPM) is proposed for stroke lesion segmentation on NCCTusing Markov diffusion process. The proposed SDPM is fully based on a LatentVariable Model (LVM), offering a complete probabilistic elaboration. Anadditional net-stream, parallel with a noise prediction stream, is introducedto obtain initial noisy label estimates for efficiently inferring the finallabels. By optimizing the specified variational boundaries, the trained modelcan infer multiple label estimates for reference given the input images withnoises. The proposed model was assessed on three stroke lesion datasetsincluding one public and two private datasets. Compared to several U-net andtransformer-based segmentation methods, our proposed SDPM model is able toachieve state-of-the-art performance. The code is publicly available.</description><author>Jianhai Zhang, Tonghua Wan, Ethan MacDonald, Bijoy Menon, Aravind Ganesh, Qiu Wu</author><pubDate>Tue, 18 Jul 2023 18:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01740v2</guid></item><item><title>Convergent regularization in inverse problems and linear plug-and-play denoisers</title><link>http://arxiv.org/abs/2307.09441v1</link><description>Plug-and-play (PnP) denoising is a popular iterative framework for solvingimaging inverse problems using off-the-shelf image denoisers. Their empiricalsuccess has motivated a line of research that seeks to understand theconvergence of PnP iterates under various assumptions on the denoiser. While asignificant amount of research has gone into establishing the convergence ofthe PnP iteration for different regularity conditions on the denoisers, notmuch is known about the asymptotic properties of the converged solution as thenoise level in the measurement tends to zero, i.e., whether PnP methods areprovably convergent regularization schemes under reasonable assumptions on thedenoiser. This paper serves two purposes: first, we provide an overview of theclassical regularization theory in inverse problems and survey a few notablerecent data-driven methods that are provably convergent regularization schemes.We then continue to discuss PnP algorithms and their established convergenceguarantees. Subsequently, we consider PnP algorithms with linear denoisers andpropose a novel spectral filtering technique to control the strength ofregularization arising from the denoiser. Further, by relating the implicitregularization of the denoiser to an explicit regularization functional, werigorously show that PnP with linear denoisers leads to a convergentregularization scheme. More specifically, we prove that in the limit as thenoise vanishes, the PnP reconstruction converges to the minimizer of aregularization potential subject to the solution satisfying the noiselessoperator equation. The theoretical analysis is corroborated by numericalexperiments for the classical inverse problem of tomographic imagereconstruction.</description><author>Andreas Hauptmann, Subhadip Mukherjee, Carola-Bibiane Schönlieb, Ferdia Sherry</author><pubDate>Tue, 18 Jul 2023 18:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09441v1</guid></item><item><title>Unsupervised Conditional Slot Attention for Object Centric Learning</title><link>http://arxiv.org/abs/2307.09437v1</link><description>Extracting object-level representations for downstream reasoning tasks is anemerging area in AI. Learning object-centric representations in an unsupervisedsetting presents multiple challenges, a key one being binding an arbitrarynumber of object instances to a specialized object slot. Recent object-centricrepresentation methods like Slot Attention utilize iterative attention to learncomposable representations with dynamic inference level binding but fail toachieve specialized slot level binding. To address this, in this paper wepropose Unsupervised Conditional Slot Attention using a novel ProbabilisticSlot Dictionary (PSD). We define PSD with (i) abstract object-level propertyvectors as key and (ii) parametric Gaussian distribution as its correspondingvalue. We demonstrate the benefits of the learnt specific object-levelconditioning distributions in multiple downstream tasks, namely objectdiscovery, compositional scene generation, and compositional visual reasoning.We show that our method provides scene composition capabilities and asignificant boost in a few shot adaptability tasks of compositional visualreasoning, while performing similarly or better than slot attention in objectdiscovery tasks</description><author>Avinash Kori, Francesco Locatello, Francesca Toni, Ben Glocker</author><pubDate>Tue, 18 Jul 2023 18:11:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09437v1</guid></item><item><title>SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs</title><link>http://arxiv.org/abs/2307.09435v1</link><description>In recent years, large-scale pre-trained speech language models (SLMs) havedemonstrated remarkable advancements in various generative speech modelingapplications, such as text-to-speech synthesis, voice conversion, and speechenhancement. These applications typically involve mapping text or speech inputsto pre-trained SLM representations, from which target speech is decoded. Thispaper introduces a new approach, SLMGAN, to leverage SLM representations fordiscriminative tasks within the generative adversarial network (GAN) framework,specifically for voice conversion. Building upon StarGANv2-VC, we add our novelSLM-based WavLM discriminators on top of the mel-based discriminators alongwith our newly designed SLM feature matching loss function, resulting in anunsupervised zero-shot voice conversion system that does not require textlabels during training. Subjective evaluation results show that SLMGANoutperforms existing state-of-the-art zero-shot voice conversion models interms of naturalness and achieves comparable similarity, highlighting thepotential of SLM-based discriminators for related applications.</description><author>Yinghao Aaron Li, Cong Han, Nima Mesgarani</author><pubDate>Tue, 18 Jul 2023 18:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09435v1</guid></item><item><title>Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education</title><link>http://arxiv.org/abs/2307.09426v1</link><description>The advancement of biomedical research heavily relies on access to largeamounts of medical data. In the case of histopathology, Whole Slide Images(WSI) and clinicopathological information are valuable for developingArtificial Intelligence (AI) algorithms for Digital Pathology (DP).Transferring medical data "as open as possible" enhances the usability of thedata for secondary purposes but poses a risk to patient privacy. At the sametime, existing regulations push towards keeping medical data "as closed asnecessary" to avoid re-identification risks. Generally, these legal regulationsrequire the removal of sensitive data but do not consider the possibility ofdata linkage attacks due to modern image-matching algorithms. In addition, thelack of standardization in DP makes it harder to establish a single solutionfor all formats of WSIs. These challenges raise problems for bio-informaticsresearchers in balancing privacy and progress while developing AI algorithms.This paper explores the legal regulations and terminologies for medicaldata-sharing. We review existing approaches and highlight challenges from thehistopathological perspective. We also present a data-sharing guideline forhistological data to foster multidisciplinary research and education.</description><author>Neel Kanwal, Emiel A. M. Janssen, Kjersti Engan</author><pubDate>Tue, 18 Jul 2023 17:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09426v1</guid></item><item><title>Execution-based Code Generation using Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2301.13816v3</link><description>The utilization of programming language (PL) models, pre-trained onlarge-scale code corpora, as a means of automating software engineeringprocesses has demonstrated considerable potential in streamlining various codegeneration tasks such as code completion, code translation, and programsynthesis. However, current approaches mainly rely on supervised fine-tuningobjectives borrowed from text generation, neglecting unique sequence-levelcharacteristics of code, including but not limited to compilability as well assyntactic and functional correctness. To address this limitation, we proposePPOCoder, a new framework for code generation that synergistically combinespre-trained PL models with Proximal Policy Optimization (PPO) which is a widelyused deep reinforcement learning technique. By utilizing non-differentiablefeedback from code execution and structure alignment, PPOCoder seamlesslyintegrates external code-specific knowledge into the model optimizationprocess. It's important to note that PPOCoder is a task-agnostic andmodel-agnostic framework that can be used across different code generationtasks and PLs. Extensive experiments on three code generation tasks demonstratethe effectiveness of our proposed approach compared to SOTA methods, achievingsignificant improvements in compilation success rates and functionalcorrectness across different PLs.</description><author>Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, Chandan K. Reddy</author><pubDate>Tue, 18 Jul 2023 17:49:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13816v3</guid></item><item><title>Scaling Laws for Imitation Learning in NetHack</title><link>http://arxiv.org/abs/2307.09423v1</link><description>Imitation Learning (IL) is one of the most widely used methods in machinelearning. Yet, while powerful, many works find it is often not able to fullyrecover the underlying expert behavior. However, none of these works deeplyinvestigate the role of scaling up the model and data size. Inspired by recentwork in Natural Language Processing (NLP) where "scaling up" has resulted inincreasingly more capable LLMs, we investigate whether carefully scaling upmodel and data size can bring similar improvements in the imitation learningsetting. To demonstrate our findings, we focus on the game of NetHack, achallenging environment featuring procedural generation, stochasticity,long-term dependencies, and partial observability. We find IL loss and meanreturn scale smoothly with the compute budget and are strongly correlated,resulting in power laws for training compute-optimal IL agents with respect tomodel size and number of samples. We forecast and train several NetHack agentswith IL and find they outperform prior state-of-the-art by at least 2x in allsettings. Our work both demonstrates the scaling behavior of imitation learningin a challenging domain, as well as the viability of scaling up currentapproaches for increasingly capable agents in NetHack, a game that remainselusively hard for current AI systems.</description><author>Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade</author><pubDate>Tue, 18 Jul 2023 17:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09423v1</guid></item><item><title>On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach</title><link>http://arxiv.org/abs/2104.06474v2</link><description>In recent years, word embeddings have been widely used to measure biases intexts. Even if they have proven to be effective in detecting a wide variety ofbiases, metrics based on word embeddings lack transparency andinterpretability. We analyze an alternative PMI-based metric to quantify biasesin texts. It can be expressed as a function of conditional probabilities, whichprovides a simple interpretation in terms of word co-occurrences. We also provethat it can be approximated by an odds ratio, which allows estimatingconfidence intervals and statistical significance of textual biases. Thisapproach produces similar results to metrics based on word embeddings whencapturing gender gaps of the real world embedded in large corpora.</description><author>Francisco Valentini, Germán Rosati, Damián Blasi, Diego Fernandez Slezak, Edgar Altszyler</author><pubDate>Tue, 18 Jul 2023 17:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.06474v2</guid></item><item><title>Measuring Student Behavioral Engagement using Histogram of Actions</title><link>http://arxiv.org/abs/2307.09420v1</link><description>In this paper, we propose a novel technique for measuring behavioralengagement through students' actions recognition. The proposed approachrecognizes student actions then predicts the student behavioral engagementlevel. For student action recognition, we use human skeletons to model studentpostures and upper body movements. To learn the dynamics of student upper body,a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actionswithin every 2minute video segment then these actions are used to build ahistogram of actions which encodes the student actions and their frequencies.This histogram is utilized as an input to SVM classifier to classify whetherthe student is engaged or disengaged. To evaluate the proposed framework, webuild a dataset consisting of 1414 2-minute video segments annotated with 13actions and 112 video segments annotated with two engagement levels.Experimental results indicate that student actions can be recognized with top 1accuracy 83.63% and the proposed framework can capture the average engagementof the class.</description><author>Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09420v1</guid></item><item><title>Ultra-Fast and Ultra-Low-Power In-Sensor Edge Vision for Gaze Estimation</title><link>http://arxiv.org/abs/2307.07813v2</link><description>Intelligent edge vision tasks encounter the critical challenge of ensuringpower and latency efficiency due to the typically heavy computational load theyimpose on edge platforms.This work leverages one of the first "AI in sensor"vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-powerend-to-end edge vision applications. We evaluate the IMX500 and compare it toother edge platforms, such as the Google Coral Dev Micro and Sony Spresense, byexploring gaze estimation as a case study. We propose TinyTracker, a highlyefficient, fully quantized model for 2D gaze estimation designed to maximizethe performance of the edge vision systems considered in this study.TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]without significant loss in gaze estimation accuracy (maximum of 0.16 cm whenfully quantized). TinyTracker's deployment on the Sony IMX500 vision sensorresults in end-to-end latency of around 19ms. The camera takes around 17.9ms toread, process and transmit the pixels to the accelerator. The inference time ofthe network is 0.86ms with an additional 0.24 ms for retrieving the resultsfrom the sensor. The overall energy consumption of the end-to-end system is 4.9mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJVS 34.2mJ)</description><author>Pietro Bonazzi, Thomas Ruegg, Sizhen Bian, Yawei Li, Michele Magno</author><pubDate>Tue, 18 Jul 2023 17:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07813v2</guid></item><item><title>Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation</title><link>http://arxiv.org/abs/2307.09416v1</link><description>Research in Image Generation has recently made significant progress,particularly boosted by the introduction of Vision-Language models which areable to produce high-quality visual content based on textual inputs. Despiteongoing advancements in terms of generation quality and realism, no methodicalframeworks have been defined yet to quantitatively measure the quality of thegenerated content and the adherence with the prompted requests: so far, onlyhuman-based evaluations have been adopted for quality satisfaction and forcomparing different generative methods. We introduce a novel automated methodfor Visual Concept Evaluation (ViCE), i.e. to assess consistency between agenerated/edited image and the corresponding prompt/instructions, with aprocess inspired by the human cognitive behaviour. ViCE combines the strengthsof Large Language Models (LLMs) and Visual Question Answering (VQA) into aunified pipeline, aiming to replicate the human cognitive process in qualityassessment. This method outlines visual concepts, formulates image-specificverification questions, utilizes the Q&amp;A system to investigate the image, andscores the combined outcome. Although this brave new hypothesis of mimickinghumans in the image evaluation process is in its preliminary assessment stage,results are promising and open the door to a new form of automatic evaluationwhich could have significant impact as the image generation or the image targetediting tasks become more and more sophisticated.</description><author>Federico Betti, Jacopo Staiano, Lorenzo Baraldi, Lorenzo Baraldi, Rita Cucchiara, Nicu Sebe</author><pubDate>Tue, 18 Jul 2023 17:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09416v1</guid></item><item><title>Mitigating Transformer Overconfidence via Lipschitz Regularization</title><link>http://arxiv.org/abs/2306.06849v2</link><description>Though Transformers have achieved promising results in many computer visiontasks, they tend to be over-confident in predictions, as the standard DotProduct Self-Attention (DPSA) can barely preserve distance for the unboundedinput domain. In this work, we fill this gap by proposing a novel LipschitzRegularized Transformer (LRFormer). Specifically, we present a new similarityfunction with the distance within Banach Space to ensure the Lipschitzness andalso regularize the term by a contractive Lipschitz Bound. The proposed methodis analyzed with a theoretical guarantee, providing a rigorous basis for itseffectiveness and reliability. Extensive experiments conducted on standardvision benchmarks demonstrate that our method outperforms the state-of-the-artsingle forward pass approaches in prediction, calibration, and uncertaintyestimation.</description><author>Wenqian Ye, Yunsheng Ma, Xu Cao, Kun Tang</author><pubDate>Tue, 18 Jul 2023 17:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06849v2</guid></item><item><title>Online Learning with Costly Features in Non-stationary Environments</title><link>http://arxiv.org/abs/2307.09388v1</link><description>Maximizing long-term rewards is the primary goal in sequentialdecision-making problems. The majority of existing methods assume that sideinformation is freely available, enabling the learning agent to observe allfeatures' states before making a decision. In real-world problems, however,collecting beneficial information is often costly. That implies that, besidesindividual arms' reward, learning the observations of the features' states isessential to improve the decision-making strategy. The problem is aggravated ina non-stationary environment where reward and cost distributions undergo abruptchanges over time. To address the aforementioned dual learning problem, weextend the contextual bandit setting and allow the agent to observe subsets offeatures' states. The objective is to maximize the long-term average gain,which is the difference between the accumulated rewards and the paid costs onaverage. Therefore, the agent faces a trade-off between minimizing the cost ofinformation acquisition and possibly improving the decision-making processusing the obtained information. To this end, we develop an algorithm thatguarantees a sublinear regret in time. Numerical results demonstrate thesuperiority of our proposed policy in a real-world scenario.</description><author>Saeed Ghoorchian, Evgenii Kortukov, Setareh Maghsudi</author><pubDate>Tue, 18 Jul 2023 17:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09388v1</guid></item><item><title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.14581v2</link><description>2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable tomodel the human skeletal topology. However, current GCN-based 3D HPE methodsupdate the node features by aggregating their neighbors' information withoutconsidering the interaction of joints in different motion patterns. Althoughsome studies import limb information to learn the movement patterns, the latentsynergies among joints, such as maintaining balance in the motion are seldominvestigated. We propose a hop-wise GraphFormer with intragroup jointrefinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists ofa novel Hop-wise GraphFormer(HGF) module and an Intragroup JointRefinement(IJR) module which leverages the prior limb information forperipheral joints refinement. The HGF module groups the joints by $k$-hopneighbors and utilizes a hop-wise transformer-like attention mechanism amongthese groups to discover latent joint synergy. Extensive experimental resultsshow that HopFIR outperforms the SOTA methods with a large margin (on theHuman3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).Furthermore, it is also demonstrated that previous SOTA GCN-based methods canbenefit from the proposed hop-wise attention mechanism efficiently withsignificant performance promotion, such as SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.</description><author>Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, ShanLin Yang</author><pubDate>Tue, 18 Jul 2023 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14581v2</guid></item><item><title>Zero-shot Query Reformulation for Conversational Search</title><link>http://arxiv.org/abs/2307.09384v1</link><description>As the popularity of voice assistants continues to surge, conversationalsearch has gained increased attention in Information Retrieval. However, datasparsity issues in conversational search significantly hinder the progress ofsupervised conversational search methods. Consequently, researchers arefocusing more on zero-shot conversational search approaches. Nevertheless,existing zero-shot methods face three primary limitations: they are notuniversally applicable to all retrievers, their effectiveness lacks sufficientexplainability, and they struggle to resolve common conversational ambiguitiescaused by omission. To address these limitations, we introduce a novelZero-shot Query Reformulation (ZeQR) framework that reformulates queries basedon previous dialogue contexts without requiring supervision from conversationalsearch data. Specifically, our framework utilizes language models designed formachine reading comprehension tasks to explicitly resolve two commonambiguities: coreference and omission, in raw queries. In comparison toexisting zero-shot methods, our approach is universally applicable to anyretriever without additional adaptation or indexing. It also provides greaterexplainability and effectively enhances query intent understanding becauseambiguities are explicitly and proactively resolved. Through extensiveexperiments on four TREC conversational datasets, we demonstrate theeffectiveness of our method, which consistently outperforms state-of-the-artbaselines.</description><author>Dayu Yang, Yue Zhang, Hui Fang</author><pubDate>Tue, 18 Jul 2023 17:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09384v1</guid></item><item><title>Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training</title><link>http://arxiv.org/abs/2307.07909v2</link><description>We introduce DualMind, a generalist agent designed to tackle variousdecision-making tasks that addresses challenges posed by current methods, suchas overfitting behaviors and dependence on task-specific fine-tuning. DualMinduses a novel "Dual-phase" training strategy that emulates how humans learn toact in the world. The model first learns fundamental common knowledge through aself-supervised objective tailored for control tasks and then learns how tomake decisions based on different contexts through imitating behaviorsconditioned on given prompts. DualMind can handle tasks across domains, scenes,and embodiments using just a single set of model weights and can executezero-shot prompting without requiring task-specific fine-tuning. We evaluateDualMind on MetaWorld and Habitat through extensive experiments and demonstrateits superior generalizability compared to previous techniques, outperformingother generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld,respectively. On the 45 tasks in MetaWorld, DualMind achieves over 30 tasks ata 90$\%$ success rate.</description><author>Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba, Ashish Kapoor, Shuang Ma</author><pubDate>Tue, 18 Jul 2023 17:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07909v2</guid></item><item><title>Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models</title><link>http://arxiv.org/abs/2305.03829v3</link><description>Image-based precision medicine aims to personalize treatment decisions basedon an individual's unique imaging features so as to improve their clinicaloutcome. Machine learning frameworks that integrate uncertainty estimation aspart of their treatment recommendations would be safer and more reliable.However, little work has been done in adapting uncertainty estimationtechniques and validation metrics for precision medicine. In this paper, we useBayesian deep learning for estimating the posterior distribution over factualand counterfactual outcomes on several treatments. This allows for estimatingthe uncertainty for each treatment option and for the individual treatmenteffects (ITE) between any two treatments. We train and evaluate this model topredict future new and enlarging T2 lesion counts on a large, multi-centerdataset of MR brain images of patients with multiple sclerosis, exposed toseveral treatments during randomized controlled trials. We evaluate thecorrelation of the uncertainty estimate with the factual error, and, given thelack of ground truth counterfactual outcomes, demonstrate how uncertainty forthe ITE prediction relates to bounds on the ITE error. Lastly, we demonstratehow knowledge of uncertainty could modify clinical decision-making to improveindividual patient and clinical trial outcomes.</description><author>Joshua Durso-Finley, Jean-Pierre Falet, Raghav Mehta, Douglas L. Arnold, Nick Pawlowski, Tal Arbel</author><pubDate>Tue, 18 Jul 2023 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03829v3</guid></item><item><title>Batched Predictors Generalize within Distribution</title><link>http://arxiv.org/abs/2307.09379v1</link><description>We study the generalization properties of batched predictors, i.e., modelstasked with predicting the mean label of a small set (or batch) of examples.The batched prediction paradigm is particularly relevant for models deployed todetermine the quality of a group of compounds in preparation for offlinetesting. By utilizing a suitable generalization of the Rademacher complexity,we prove that batched predictors come with exponentially strongergeneralization guarantees as compared to the standard per-sample approach.Surprisingly, the proposed bound holds independently of overparametrization.Our theoretical insights are validated experimentally for various tasks,architectures, and applications.</description><author>Andreas Loukas, Pan Kessel</author><pubDate>Tue, 18 Jul 2023 17:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09379v1</guid></item><item><title>Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading</title><link>http://arxiv.org/abs/2307.09377v1</link><description>The use of machine learning in algorithmic trading systems is increasinglycommon. In a typical set-up, supervised learning is used to predict the futureprices of assets, and those predictions drive a simple trading and executionstrategy. This is quite effective when the predictions have sufficient signal,markets are liquid, and transaction costs are low. However, those conditionsoften do not hold in thinly traded financial markets and markets fordifferentiated assets such as real estate or vehicles. In these markets, thetrading strategy must consider the long-term effects of taking positions thatare relatively more difficult to change. In this work, we propose aReinforcement Learning (RL) algorithm that trades based on signals from alearned predictive model and addresses these challenges. We test our algorithmon 20+ years of equity data from Bursa Malaysia.</description><author>Vikram Duvvur, Aashay Mehta, Edward Sun, Bo Wu, Ken Yew Chan, Jeff Schneider</author><pubDate>Tue, 18 Jul 2023 17:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09377v1</guid></item><item><title>CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space</title><link>http://arxiv.org/abs/2307.09375v1</link><description>Deep neural networks (DNNs) have demonstrated their outperformance in varioussoftware systems, but also exhibit misbehavior and even result in irreversibledisasters. Therefore, it is crucial to identify the misbehavior of DNN-basedsoftware and improve DNNs' quality. Test input prioritization is one of themost appealing ways to guarantee DNNs' quality, which prioritizes test inputsso that more bug-revealing inputs can be identified earlier with limited timeand manual labeling efforts. However, the existing prioritization methods arestill limited from three aspects: certifiability, effectiveness, andgeneralizability. To overcome the challenges, we propose CertPri, a test inputprioritization technique designed based on a movement cost perspective of testinputs in DNNs' feature space. CertPri differs from previous works in three keyaspects: (1) certifiable: it provides a formal robustness guarantee for themovement cost; (2) effective: it leverages formally guaranteed movement coststo identify malicious bug-revealing inputs; and (3) generic: it can be appliedto various tasks, data, models, and scenarios. Extensive evaluations across 2tasks (i.e., classification and regression), 6 data forms, 4 model structures,and 2 scenarios (i.e., white-box and black-box) demonstrate CertPri's superiorperformance. For instance, it significantly improves 53.97% prioritizationeffectiveness on average compared with baselines. Its robustness andgeneralizability are 1.41~2.00 times and 1.33~3.39 times that of baselines onaverage, respectively.</description><author>Haibin Zheng, Jinyin Chen, Haibo Jin</author><pubDate>Tue, 18 Jul 2023 16:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09375v1</guid></item><item><title>Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation</title><link>http://arxiv.org/abs/2307.09372v1</link><description>Support Vector Machines (SVM) have gathered significant acclaim asclassifiers due to their successful implementation of Statistical LearningTheory. However, in the context of multiclass and multilabel settings, thereliance on vector-based formulations in existing SVM-based models poseslimitations regarding flexibility and ease of incorporating additional terms tohandle specific challenges. To overcome these limitations, our research paperfocuses on introducing a matrix formulation for SVM that effectively addressesthese constraints. By employing the Accelerated Gradient Descent method in thedual, we notably enhance the efficiency of solving the Matrix-SVM problem.Experimental evaluations on multilabel and multiclass datasets demonstrate thatMatrix SVM achieves superior time efficacy while delivering similar results toBinary Relevance SVM. Moreover, our matrix formulation unveils crucial insights and advantages thatmay not be readily apparent in traditional vector-based notations. We emphasizethat numerous multilabel models can be viewed as extensions of SVM, withcustomised modifications to meet specific requirements. The matrix formulationpresented in this paper establishes a solid foundation for developing moresophisticated models capable of effectively addressing the distinctivechallenges encountered in multilabel learning.</description><author>Sambhav Jain Reshma Rastogi</author><pubDate>Tue, 18 Jul 2023 16:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09372v1</guid></item><item><title>Plug the Leaks: Advancing Audio-driven Talking Face Generation by Preventing Unintended Information Flow</title><link>http://arxiv.org/abs/2307.09368v1</link><description>Audio-driven talking face generation is the task of creating alip-synchronized, realistic face video from given audio and reference frames.This involves two major challenges: overall visual quality of generated imageson the one hand, and audio-visual synchronization of the mouth part on theother hand. In this paper, we start by identifying several problematic aspectsof synchronization methods in recent audio-driven talking face generationapproaches. Specifically, this involves unintended flow of lip and poseinformation from the reference to the generated image, as well as instabilitiesduring model training. Subsequently, we propose various techniques forobviating these issues: First, a silent-lip reference image generator preventsleaking of lips from the reference to the generated image. Second, an adaptivetriplet loss handles the pose leaking problem. Finally, we propose a stabilizedformulation of synchronization loss, circumventing aforementioned traininginstabilities while additionally further alleviating the lip leaking issue.Combining the individual improvements, we present state-of-the art performanceon LRS2 and LRW in both synchronization and visual quality. We further validateour design in various ablation experiments, confirming the individualcontributions as well as their complementary effects.</description><author>Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Bärmann, Hazim Kemal Ekenel, Alexander Waibel</author><pubDate>Tue, 18 Jul 2023 16:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09368v1</guid></item><item><title>Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives</title><link>http://arxiv.org/abs/2307.09366v1</link><description>We consider the problem of learning a sparse graph underlying an undirectedGaussian graphical model, a key problem in statistical machine learning. Given$n$ samples from a multivariate Gaussian distribution with $p$ variables, thegoal is to estimate the $p \times p$ inverse covariance matrix (aka precisionmatrix), assuming it is sparse (i.e., has a few nonzero entries). We proposeGraphL0BnB, a new estimator based on an $\ell_0$-penalized version of thepseudolikelihood function, while most earlier approaches are based on the$\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integerprogram (MIP) which can be difficult to compute at scale using off-the-shelfcommercial solvers. To solve the MIP, we propose a custom nonlinearbranch-and-bound (BnB) framework that solves node relaxations with tailoredfirst-order methods. As a by-product of our BnB framework, we proposelarge-scale solvers for obtaining good primal solutions that are of independentinterest. We derive novel statistical guarantees (estimation and variableselection) for our estimator and discuss how our approach improves uponexisting estimators. Our numerical experiments on real/synthetic datasetssuggest that our method can solve, to near-optimality, problem instances with$p = 10^4$ -- corresponding to a symmetric matrix of size $p \times p$ with$p^2/2$ binary variables. We demonstrate the usefulness of GraphL0BnB versusvarious state-of-the-art approaches on a range of datasets.</description><author>Kayhan Behdin, Wenyu Chen, Rahul Mazumder</author><pubDate>Tue, 18 Jul 2023 16:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09366v1</guid></item><item><title>An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness</title><link>http://arxiv.org/abs/2307.09365v1</link><description>Zero-cost proxies are nowadays frequently studied and used to search forneural architectures. They show an impressive ability to predict theperformance of architectures by making use of their untrained weights. Thesetechniques allow for immense search speed-ups. So far the joint search forwell-performing and robust architectures has received much less attention inthe field of NAS. Therefore, the main focus of zero-cost proxies is the cleanaccuracy of architectures, whereas the model robustness should play an evenlyimportant part. In this paper, we analyze the ability of common zero-costproxies to serve as performance predictors for robustness in the popularNAS-Bench-201 search space. We are interested in the single prediction task forrobustness and the joint multi-objective of clean and robust accuracy. Wefurther analyze the feature importance of the proxies and show that predictingthe robustness makes the prediction task from existing zero-cost proxies morechallenging. As a result, the joint consideration of several proxies becomesnecessary to predict a model's robustness while the clean accuracy can beregressed from a single such feature.</description><author>Jovita Lukasik, Michael Moeller, Margret Keuper</author><pubDate>Tue, 18 Jul 2023 16:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09365v1</guid></item><item><title>Local Minima Drive Communications in Cooperative Interaction</title><link>http://arxiv.org/abs/2307.09364v1</link><description>An important open question in human-robot interaction (HRI) is precisely whenan agent should decide to communicate, particularly in a cooperative task.Perceptual Control Theory (PCT) tells us that agents are able to cooperate on ajoint task simply by sharing the same 'intention', thereby distributing theeffort required to complete the task among the agents. This is even true foragents that do not possess the same abilities, so long as the goal isobservable, the combined actions are sufficient to complete the task, and thereis no local minimum in the search space. If these conditions hold, then acooperative task can be accomplished without any communication between thecontributing agents. However, for tasks that do contain local minima, theglobal solution can only be reached if at least one of the agents adapts itsintention at the appropriate moments, and this can only be achieved byappropriately timed communication. In other words, it is hypothesised that incooperative tasks, the function of communication is to coordinate actions in acomplex search space that contains local minima. These principles have beenverified in a computer-based simulation environment in which two independentone-dimensional agents are obliged to cooperate in order to solve atwo-dimensional path-finding task.</description><author>Roger K. Moore</author><pubDate>Tue, 18 Jul 2023 16:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09364v1</guid></item><item><title>Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement</title><link>http://arxiv.org/abs/2307.09362v1</link><description>Most prior semantic segmentation methods have been developed for day-timescenes, while typically underperforming in night-time scenes due toinsufficient and complicated lighting conditions. In this work, we tackle thischallenge by proposing a novel night-time semantic segmentation paradigm, i.e.,disentangle then parse (DTP). DTP explicitly disentangles night-time imagesinto light-invariant reflectance and light-specific illumination components andthen recognizes semantics based on their adaptive fusion. Concretely, theproposed DTP comprises two key components: 1) Instead of processinglighting-entangled features as in prior works, our Semantic-OrientedDisentanglement (SOD) framework enables the extraction of reflectance componentwithout being impeded by lighting, allowing the network to consistentlyrecognize the semantics under cover of varying and complicated lightingconditions. 2) Based on the observation that the illumination component canserve as a cue for some semantically confused regions, we further introduce anIllumination-Aware Parser (IAParser) to explicitly learn the correlationbetween semantics and lighting, and aggregate the illumination features toyield more precise predictions. Extensive experiments on the night-timesegmentation task with various settings demonstrate that DTP significantlyoutperforms state-of-the-art methods. Furthermore, with negligible additionalparameters, DTP can be directly used to benefit existing day-time methods fornight-time segmentation.</description><author>Zhixiang Wei, Lin Chen, Tao Tu, Huaian Chen, Pengyang Ling, Yi Jin</author><pubDate>Tue, 18 Jul 2023 16:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09362v1</guid></item><item><title>MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments</title><link>http://arxiv.org/abs/2307.09361v1</link><description>Self-supervised learning can be used for mitigating the greedy needs ofVision Transformer networks for very large fully-annotated datasets. Differentclasses of self-supervised learning offer representations with either goodcontextual reasoning properties, e.g., using masked image modeling strategies,or invariance to image perturbations, e.g., with contrastive methods. In thiswork, we propose a single-stage and standalone method, MOCA, which unifies bothdesired properties using novel mask-and-predict objectives defined withhigh-level features (instead of pixel-level details). Moreover, we show how toeffectively employ both learning paradigms in a synergistic andcomputation-efficient way. Doing so, we achieve new state-of-the-art results onlow-shot settings and strong experimental results in various evaluationprotocols with a training that is at least 3 times faster than prior methods.</description><author>Spyros Gidaris, Andrei Bursuc, Oriane Simeoni, Antonin Vobecky, Nikos Komodakis, Matthieu Cord, Patrick Pérez</author><pubDate>Tue, 18 Jul 2023 16:46:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09361v1</guid></item><item><title>Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference</title><link>http://arxiv.org/abs/2307.09357v1</link><description>Analog In-Memory Computing (AIMC) is a promising approach to reduce thelatency and energy consumption of Deep Neural Network (DNN) inference andtraining. However, the noisy and non-linear device characteristics, and thenon-ideal peripheral circuitry in AIMC chips, require adapting DNNs to bedeployed on such hardware to achieve equivalent accuracy to digital computing.In this tutorial, we provide a deep dive into how such adaptations can beachieved and evaluated using the recently released IBM Analog HardwareAcceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit.The AIHWKit is a Python library that simulates inference and training of DNNsusing AIMC. We present an in-depth description of the AIHWKit design,functionality, and best practices to properly perform inference and training.We also present an overview of the Analog AI Cloud Composer, that provides thebenefits of using the AIHWKit simulation platform in a fully managed cloudsetting. Finally, we show examples on how users can expand and customizeAIHWKit for their own needs. This tutorial is accompanied by comprehensiveJupyter Notebook code examples that can be run using AIHWKit, which can bedownloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial.</description><author>Manuel Le Gallo, Corey Lammie, Julian Buechel, Fabio Carta, Omobayode Fagbohungbe, Charles Mackin, Hsinyu Tsai, Vijay Narayanan, Abu Sebastian, Kaoutar El Maghraoui, Malte J. Rasch</author><pubDate>Tue, 18 Jul 2023 16:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09357v1</guid></item><item><title>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.09356v1</link><description>Referring video object segmentation (RVOS) aims at segmenting an object in avideo following human instruction. Current state-of-the-art methods fall intoan offline pattern, in which each clip independently interacts with textembedding for cross-modal understanding. They usually present that the offlinepattern is necessary for RVOS, yet model limited temporal association withineach clip. In this work, we break up the previous offline belief and propose asimple yet effective online model using explicit query propagation, namedOnlineRefer. Specifically, our approach leverages target cues that gathersemantic information and position prior to improve the accuracy and ease ofreferring predictions for the current frame. Furthermore, we generalize ouronline model into a semi-online framework to be compatible with video-basedbackbones. To show the effectiveness of our method, we evaluate it on fourbenchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, andJHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-Lbackbone achieves 63.5 J&amp;F and 64.8 J&amp;F on Refer-Youtube-VOS and Refer-DAVIS17,outperforming all other offline methods.</description><author>Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, Jianbing Shen</author><pubDate>Tue, 18 Jul 2023 16:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09356v1</guid></item><item><title>Evaluating Open-QA Evaluation</title><link>http://arxiv.org/abs/2305.12421v2</link><description>This study focuses on the evaluation of the Open Question Answering (Open-QA)task, which can directly estimate the factuality of large language models(LLMs). Current automatic evaluation methods have shown limitations, indicatingthat human evaluation still remains the most reliable approach. We introduce anew task, Evaluating QA Evaluation (QA-Eval) and the corresponding datasetEVOUNA, designed to assess the accuracy of AI-generated answers in relation tostandard answers within Open-QA. Our evaluation of these methods utilizeshuman-annotated results to measure their performance. Specifically, the workinvestigates methods that show high correlation with human evaluations, deemingthem more reliable. We also discuss the pitfalls of current methods and methodsto improve LLM-based evaluators. We believe this new QA-Eval task andcorresponding dataset EVOUNA will facilitate the development of more effectiveautomatic evaluation tools and prove valuable for future research in this area.All resources are available at \url{https://github.com/wangcunxiang/QA-Eval}and it is under the Apache-2.0 License.</description><author>Cunxiang Wang, Sirui Cheng, Qipeng Guo, Zhikun Xu, Bowen Ding, Yidong Wang, Xiangkun Hu, Zheng Zhang, Yue Zhang</author><pubDate>Tue, 18 Jul 2023 16:41:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12421v2</guid></item><item><title>SphereNet: Learning a Noise-Robust and General Descriptor for Point Cloud Registration</title><link>http://arxiv.org/abs/2307.09351v1</link><description>Point cloud registration is to estimate a transformation to align pointclouds collected in different perspectives. In learning-based point cloudregistration, a robust descriptor is vital for high-accuracy registration.However, most methods are susceptible to noise and have poor generalizationability on unseen datasets. Motivated by this, we introduce SphereNet to learna noise-robust and unseen-general descriptor for point cloud registration. Inour method, first, the spheroid generator builds a geometric domain based onspherical voxelization to encode initial features. Then, the sphericalinterpolation of the sphere is introduced to realize robustness against noise.Finally, a new spherical convolutional neural network with spherical integritypadding completes the extraction of descriptors, which reduces the loss offeatures and fully captures the geometric features. To evaluate our methods, anew benchmark 3DMatch-noise with strong noise is introduced. Extensiveexperiments are carried out on both indoor and outdoor datasets. Underhigh-intensity noise, SphereNet increases the feature matching recall by morethan 25 percentage points on 3DMatch-noise. In addition, it sets a newstate-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with93.5\% and 75.6\% registration recall and also has the best generalizationability on unseen datasets.</description><author>Guiyu Zhao, Zhentao Guo, Xin Wang, Hongbin Ma</author><pubDate>Tue, 18 Jul 2023 16:37:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09351v1</guid></item><item><title>Robust online active learning</title><link>http://arxiv.org/abs/2302.00422v6</link><description>In many industrial applications, obtaining labeled observations is notstraightforward as it often requires the intervention of human experts or theuse of expensive testing equipment. In these circumstances, active learning canbe highly beneficial in suggesting the most informative data points to be usedwhen fitting a model. Reducing the number of observations needed for modeldevelopment alleviates both the computational burden required for training andthe operational expenses related to labeling. Online active learning, inparticular, is useful in high-volume production processes where the decisionabout the acquisition of the label for a data point needs to be taken within anextremely short time frame. However, despite the recent efforts to developonline active learning strategies, the behavior of these methods in thepresence of outliers has not been thoroughly examined. In this work, weinvestigate the performance of online active linear regression in contaminateddata streams. Our study shows that the currently available query strategies areprone to sample outliers, whose inclusion in the training set eventuallydegrades the predictive performance of the models. To address this issue, wepropose a solution that bounds the search area of a conditional D-optimalalgorithm and uses a robust estimator. Our approach strikes a balance betweenexploring unseen regions of the input space and protecting against outliers.Through numerical simulations, we show that the proposed method is effective inimproving the performance of online active learning in the presence ofoutliers, thus expanding the potential applications of this powerful tool.</description><author>Davide Cacciarelli, Murat Kulahci, John Sølve Tyssedal</author><pubDate>Tue, 18 Jul 2023 16:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00422v6</guid></item><item><title>The Score-Difference Flow for Implicit Generative Modeling</title><link>http://arxiv.org/abs/2304.12906v2</link><description>Implicit generative modeling (IGM) aims to produce samples of synthetic datamatching the characteristics of a target data distribution. Recent work (e.g.score-matching networks, diffusion models) has approached the IGM problem fromthe perspective of pushing synthetic source data toward the target distributionvia dynamical perturbations or flows in the ambient space. In this direction,we present the score difference (SD) between arbitrary target and sourcedistributions as a flow that optimally reduces the Kullback-Leibler divergencebetween them while also solving the Schroedinger bridge problem. We apply theSD flow to convenient proxy distributions, which are aligned if and only if theoriginal distributions are aligned. We demonstrate the formal equivalence ofthis formulation to denoising diffusion models under certain conditions. Wealso show that the training of generative adversarial networks includes ahidden data-optimization sub-problem, which induces the SD flow under certainchoices of loss function when the discriminator is optimal. As a result, the SDflow provides a theoretical link between model classes that individuallyaddress the three challenges of the "generative modeling trilemma" -- highsample quality, mode coverage, and fast sampling -- thereby setting the stagefor a unified approach.</description><author>Romann M. Weber</author><pubDate>Tue, 18 Jul 2023 16:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12906v2</guid></item><item><title>Gradient Surgery for One-shot Unlearning on Generative Model</title><link>http://arxiv.org/abs/2307.04550v2</link><description>Recent regulation on right-to-be-forgotten emerges tons of interest inunlearning pre-trained machine learning models. While approximating astraightforward yet expensive approach of retrain-from-scratch, recent machineunlearning methods unlearn a sample by updating weights to remove its influenceon the weight parameters. In this paper, we introduce a simple yet effectiveapproach to remove a data influence on the deep generative model. Inspired byworks in multi-task learning, we propose to manipulate gradients to regularizethe interplay of influence among samples by projecting gradients onto thenormal plane of the gradients to be retained. Our work is agnostic tostatistics of the removal samples, outperforming existing baselines whileproviding theoretical analysis for the first time in unlearning a generativemodel.</description><author>Seohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim</author><pubDate>Tue, 18 Jul 2023 16:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04550v2</guid></item><item><title>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</title><link>http://arxiv.org/abs/2307.08674v2</link><description>Tables are prevalent in real-world databases, requiring significant time andeffort for humans to analyze and manipulate. The advancements in large languagemodels (LLMs) have made it possible to interact with tables using naturallanguage input, bringing this capability closer to reality. In this paper, wepresent TableGPT, a unified fine-tuned framework that enables LLMs tounderstand and operate on tables using external functional commands. Itintroduces the capability to seamlessly interact with tables, enabling a widerange of functionalities such as question answering, data manipulation (e.g.,insert, delete, query, and modify operations), data visualization, analysisreport generation, and automated prediction. TableGPT aims to provideconvenience and accessibility to users by empowering them to effortlesslyleverage tabular data. At the core of TableGPT lies the novel concept of globaltabular representations, which empowers LLMs to gain a comprehensiveunderstanding of the entire table beyond meta-information. By jointly trainingLLMs on both table and text modalities, TableGPT achieves a deep understandingof tabular data and the ability to perform complex operations on tables throughchain-of-command instructions. Importantly, TableGPT offers the advantage ofbeing a self-contained system rather than relying on external API interfaces.Moreover, it supports efficient data process flow, query rejection (whenappropriate) and private deployment, enabling faster domain data fine-tuningand ensuring data privacy, which enhances the framework's adaptability tospecific use cases.</description><author>Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao</author><pubDate>Tue, 18 Jul 2023 16:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08674v2</guid></item><item><title>A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework</title><link>http://arxiv.org/abs/2204.03719v2</link><description>Class imbalance poses new challenges when it comes to classifying datastreams. Many algorithms recently proposed in the literature tackle thisproblem using a variety of data-level, algorithm-level, and ensembleapproaches. However, there is a lack of standardized and agreed-upon proceduresand benchmarks on how to evaluate these algorithms. This work proposes astandardized, exhaustive, and comprehensive experimental framework to evaluatealgorithms in a collection of diverse and challenging imbalanced data streamscenarios. The experimental study evaluates 24 state-of-the-art data streamsalgorithms on 515 imbalanced data streams that combine static and dynamic classimbalance ratios, instance-level difficulties, concept drift, real-world andsemi-synthetic datasets in binary and multi-class scenarios. This leads to alarge-scale experimental study comparing state-of-the-art classifiers in thedata stream mining domain. We discuss the advantages and disadvantages ofstate-of-the-art classifiers in each of these scenarios and we provide generalrecommendations to end-users for selecting the best algorithms for imbalanceddata streams. Additionally, we formulate open challenges and future directionsfor this domain. Our experimental framework is fully reproducible and easy toextend with new methods. This way, we propose a standardized approach toconducting experiments in imbalanced data streams that can be used by otherresearchers to create complete, trustworthy, and fair evaluation of newlyproposed methods. Our experimental framework can be downloaded fromhttps://github.com/canoalberto/imbalanced-streams.</description><author>Gabriel Aguiar, Bartosz Krawczyk, Alberto Cano</author><pubDate>Tue, 18 Jul 2023 16:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.03719v2</guid></item><item><title>Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints</title><link>http://arxiv.org/abs/2307.09342v1</link><description>Many constraint satisfaction and optimisation problems can be solvedeffectively by encoding them as instances of the Boolean Satisfiability problem(SAT). However, even the simplest types of constraints have many encodings inthe literature with widely varying performance, and the problem of selectingsuitable encodings for a given problem instance is not trivial. We explore theproblem of selecting encodings for pseudo-Boolean and linear constraints usinga supervised machine learning approach. We show that it is possible to selectencodings effectively using a standard set of features for constraint problems;however we obtain better performance with a new set of features specificallydesigned for the pseudo-Boolean and linear constraints. In fact, we achievegood results when selecting encodings for unseen problem classes. Our resultscompare favourably to AutoFolio when using the same feature set. We discuss therelative importance of instance features to the task of selecting the bestencodings, and compare several variations of the machine learning method.</description><author>Felix Ulrich-Oltean, Peter Nightingale, James Alfred Walker</author><pubDate>Tue, 18 Jul 2023 16:26:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09342v1</guid></item><item><title>Adaptively Optimised Adaptive Importance Samplers</title><link>http://arxiv.org/abs/2307.09341v1</link><description>We introduce a new class of adaptive importance samplers leveraging adaptiveoptimisation tools, which we term AdaOAIS. We build on Optimised AdaptiveImportance Samplers (OAIS), a class of techniques that adapt proposals toimprove the mean-squared error of the importance sampling estimators byparameterising the proposal and optimising the $\chi^2$-divergence between thetarget and the proposal. We show that a naive implementation of OAIS usingstochastic gradient descent may lead to unstable estimators despite itsconvergence guarantees. To remedy this shortcoming, we instead propose to useadaptive optimisers (such as AdaGrad and Adam) to improve the stability of theOAIS. We provide convergence results for AdaOAIS in a similar manner to OAIS.We also provide empirical demonstration on a variety of examples and show thatAdaOAIS lead to stable importance sampling estimators in practice.</description><author>Carlos A. C. C. Perello, Ömer Deniz Akyildiz</author><pubDate>Tue, 18 Jul 2023 16:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09341v1</guid></item><item><title>Deep Learning with Passive Optical Nonlinear Mapping</title><link>http://arxiv.org/abs/2307.08558v2</link><description>Deep learning has fundamentally transformed artificial intelligence, but theever-increasing complexity in deep learning models calls for specializedhardware accelerators. Optical accelerators can potentially offer enhancedperformance, scalability, and energy efficiency. However, achieving nonlinearmapping, a critical component of neural networks, remains challengingoptically. Here, we introduce a design that leverages multiple scattering in areverberating cavity to passively induce optical nonlinear random mapping,without the need for additional laser power. A key advantage emerging from ourwork is that we show we can perform optical data compression, facilitated bymultiple scattering in the cavity, to efficiently compress and retain vitalinformation while also decreasing data dimensionality. This allows rapidoptical information processing and generation of low dimensional mixtures ofhighly nonlinear features. These are particularly useful for applicationsdemanding high-speed analysis and responses such as in edge computing devices.Utilizing rapid optical information processing capabilities, our opticalplatforms could potentially offer more efficient and real-time processingsolutions for a broad range of applications. We demonstrate the efficacy of ourdesign in improving computational performance across tasks, includingclassification, image reconstruction, key-point detection, and objectdetection, all achieved through optical data compression combined with adigital decoder. Notably, we observed high performance, at an extremecompression ratio, for real-time pedestrian detection. Our findings pave theway for novel algorithms and architectural designs for optical computing.</description><author>Fei Xia, Kyungduk Kim, Yaniv Eliezer, Liam Shaughnessy, Sylvain Gigan, Hui Cao</author><pubDate>Tue, 18 Jul 2023 16:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08558v2</guid></item><item><title>Capturing Emerging Complexity in Lenia</title><link>http://arxiv.org/abs/2305.09378v3</link><description>This research project investigates Lenia, an artificial life platform thatsimulates ecosystems of digital creatures. Lenia's ecosystem consists ofsimple, artificial organisms that can move, consume, grow, and reproduce. Theplatform is important as a tool for studying artificial life and evolution, asit provides a scalable and flexible environment for creating a diverse range oforganisms with varying abilities and behaviors. Measuring complexity in Leniais a key aspect of the study, which identifies the metrics for measuringlong-term complex emerging behavior of rules, with the aim of evolving betterLenia behaviors which are yet not discovered. The Genetic Algorithm usesneighborhoods or kernels as genotype while keeping the rest of the parametersof Lenia as fixed, for example growth function, to produce different behaviorsrespective to the population and then measures fitness value to decide thecomplexity of the resulting behavior. First, we use Variation over Time as afitness function where higher variance between the frames are rewarded. Second,we use Auto-encoder based fitness where variation of the list of reconstructionloss for the frames is rewarded. Third, we perform combined fitness wherehigher variation of the pixel density of reconstructed frames is rewarded. Allthree experiments are tweaked with pixel alive threshold and frames used.Finally, after performing nine experiments of each fitness for 500 generations,we pick configurations from all experiments such that there is a scope offurther evolution, and run it for 2500 generations. Results show that thekernel's center of mass increases with a specific set of pixels and togetherwith borders the kernel try to achieve a Gaussian distribution.</description><author>Sanyam Jain, Aarati Shrestha, Stefano Nichele</author><pubDate>Tue, 18 Jul 2023 16:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09378v3</guid></item><item><title>Approximating nonlinear functions with latent boundaries in low-rank excitatory-inhibitory spiking networks</title><link>http://arxiv.org/abs/2307.09334v1</link><description>Deep feedforward and recurrent rate-based neural networks have becomesuccessful functional models of the brain, but they neglect obvious biologicaldetails such as spikes and Dale's law. Here we argue that these details arecrucial in order to understand how real neural circuits operate. Towards thisaim, we put forth a new framework for spike-based computation in low-rankexcitatory-inhibitory spiking networks. By considering populations with rank-1connectivity, we cast each neuron's spiking threshold as a boundary in alow-dimensional input-output space. We then show how the combined thresholds ofa population of inhibitory neurons form a stable boundary in this space, andthose of a population of excitatory neurons form an unstable boundary.Combining the two boundaries results in a rank-2 excitatory-inhibitory (EI)network with inhibition-stabilized dynamics at the intersection of the twoboundaries. The computation of the resulting networks can be understood as thedifference of two convex functions, and is thereby capable of approximatingarbitrary non-linear input-output mappings. We demonstrate several propertiesof these networks, including noise suppression and amplification, irregularactivity and synaptic balance, as well as how they relate to rate networkdynamics in the limit that the boundary becomes soft. Finally, while our workfocuses on small networks (5-50 neurons), we discuss potential avenues forscaling up to much larger networks. Overall, our work proposes a newperspective on spiking networks that may serve as a starting point for amechanistic understanding of biological spike-based computation.</description><author>William F. Podlaski, Christian K. Machens</author><pubDate>Tue, 18 Jul 2023 16:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09334v1</guid></item><item><title>Company2Vec -- German Company Embeddings based on Corporate Websites</title><link>http://arxiv.org/abs/2307.09332v1</link><description>With Company2Vec, the paper proposes a novel application in representationlearning. The model analyzes business activities from unstructured companywebsite data using Word2Vec and dimensionality reduction. Company2Vec maintainssemantic language structures and thus creates efficient company embeddings infine-granular industries. These semantic embeddings can be used for variousapplications in banking. Direct relations between companies and words allowsemantic business analytics (e.g. top-n words for a company). Furthermore,industry prediction is presented as a supervised learning application andevaluation method. The vectorized structure of the embeddings allows measuringcompanies similarities with the cosine distance. Company2Vec hence offers amore fine-grained comparison of companies than the standard industry labels(NACE). This property is relevant for unsupervised learning tasks, such asclustering. An alternative industry segmentation is shown with k-meansclustering on the company embeddings. Finally, this paper proposes threealgorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centricpeer-firm identification.</description><author>Christopher Gerling</author><pubDate>Tue, 18 Jul 2023 16:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09332v1</guid></item><item><title>Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots</title><link>http://arxiv.org/abs/2307.09330v1</link><description>We investigate the ability of individuals to visually validate statisticalmodels in terms of their fit to the data. While visual model estimation hasbeen studied extensively, visual model validation remains under-investigated.It is unknown how well people are able to visually validate models, and howtheir performance compares to visual and computational estimation. As astarting point, we conducted a study across two populations (crowdsourced andvolunteers). Participants had to both visually estimate (i.e, draw) andvisually validate (i.e., accept or reject) the frequently studied model ofaverages. Across both populations, the level of accuracy of the models thatwere considered valid was lower than the accuracy of the estimated models. Wefind that participants' validation and estimation were unbiased. Moreover,their natural critical point between accepting and rejecting a given mean valueis close to the boundary of its 95% confidence interval, indicating that thevisually perceived confidence interval corresponds to a common statisticalstandard. Our work contributes to the understanding of visual model validationand opens new research opportunities.</description><author>Daniel Braun, Ashley Suh, Remco Chang, Michael Gleicher, Tatiana von Landesberger</author><pubDate>Tue, 18 Jul 2023 16:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09330v1</guid></item><item><title>Towards a performance analysis on pre-trained Visual Question Answering models for autonomous driving</title><link>http://arxiv.org/abs/2307.09329v1</link><description>This short paper presents a preliminary analysis of three popular VisualQuestion Answering (VQA) models, namely ViLBERT, ViLT, and LXMERT, in thecontext of answering questions relating to driving scenarios. The performanceof these models is evaluated by comparing the similarity of responses toreference answers provided by computer vision experts. Model selection ispredicated on the analysis of transformer utilization in multimodalarchitectures. The results indicate that models incorporating cross-modalattention and late fusion techniques exhibit promising potential for generatingimproved answers within a driving perspective. This initial analysis serves asa launchpad for a forthcoming comprehensive comparative study involving nineVQA models and sets the scene for further investigations into the effectivenessof VQA model queries in self-driving scenarios. Supplementary material isavailable athttps://github.com/KaavyaRekanar/Towards-a-performance-analysis-on-pre-trained-VQA-models-for-autonomous-driving.</description><author>Kaavya Rekanar, Ciarán Eising, Ganesh Sistu, Martin Hayes</author><pubDate>Tue, 18 Jul 2023 16:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09329v1</guid></item><item><title>A Recursive Bateson-Inspired Model for the Generation of Semantic Formal Concepts from Spatial Sensory Data</title><link>http://arxiv.org/abs/2307.08087v2</link><description>Neural-symbolic approaches to machine learning incorporate the advantagesfrom both connectionist and symbolic methods. Typically, these models employ afirst module based on a neural architecture to extract features from complexdata. Then, these features are processed as symbols by a symbolic engine thatprovides reasoning, concept structures, composability, better generalizationand out-of-distribution learning among other possibilities. However, neuralapproaches to the grounding of symbols in sensory data, albeit powerful, stillrequire heavy training and tedious labeling for the most part. This paperpresents a new symbolic-only method for the generation of hierarchical conceptstructures from complex spatial sensory data. The approach is based onBateson's notion of difference as the key to the genesis of an idea or aconcept. Following his suggestion, the model extracts atomic features from rawdata by computing elemental sequential comparisons in a stream of multivariatenumerical values. Higher-level constructs are built from these features bysubjecting them to further comparisons in a recursive process. At any stage inthe recursion, a concept structure may be obtained from these constructs andfeatures by means of Formal Concept Analysis. Results show that the model isable to produce fairly rich yet human-readable conceptual representationswithout training. Additionally, the concept structures obtained through themodel (i) present high composability, which potentially enables the generationof 'unseen' concepts, (ii) allow formal reasoning, and (iii) have inherentabilities for generalization and out-of-distribution learning. Consequently,this method may offer an interesting angle to current neural-symbolic research.Future work is required to develop a training methodology so that the model canbe tested against a larger dataset.</description><author>Jaime de Miguel-Rodriguez, Fernando Sancho-Caparrini</author><pubDate>Tue, 18 Jul 2023 16:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08087v2</guid></item><item><title>Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</title><link>http://arxiv.org/abs/2307.09323v1</link><description>This paper presents ER-NeRF, a novel conditional Neural Radiance Fields(NeRF) based architecture for talking portrait synthesis that can concurrentlyachieve fast convergence, real-time rendering, and state-of-the-art performancewith small model size. Our idea is to explicitly exploit the unequalcontribution of spatial regions to guide talking portrait modeling.Specifically, to improve the accuracy of dynamic head reconstruction, a compactand expressive NeRF-based Tri-Plane Hash Representation is introduced bypruning empty spatial regions with three planar hash encoders. For speechaudio, we propose a Region Attention Module to generate region-aware conditionfeature via an attention mechanism. Different from existing methods thatutilize an MLP-based encoder to learn the cross-modal relation implicitly, theattention mechanism builds an explicit connection between audio features andspatial regions to capture the priors of local motions. Moreover, a direct andfast Adaptive Pose Encoding is introduced to optimize the head-torso separationproblem by mapping the complex transformation of the head pose into spatialcoordinates. Extensive experiments demonstrate that our method renders betterhigh-fidelity and audio-lips synchronized talking portrait videos, withrealistic details and high efficiency compared to previous methods.</description><author>Jiahe Li, Jiawei Zhang, Xiao Bai, Jun Zhou, Lin Gu</author><pubDate>Tue, 18 Jul 2023 16:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09323v1</guid></item><item><title>High Fidelity Image Counterfactuals with Probabilistic Causal Models</title><link>http://arxiv.org/abs/2306.15764v2</link><description>We present a general causal generative modelling framework for accurateestimation of high fidelity image counterfactuals with deep structural causalmodels. Estimation of interventional and counterfactual queries forhigh-dimensional structured variables, such as images, remains a challengingtask. We leverage ideas from causal mediation analysis and advances ingenerative modelling to design new deep causal mechanisms for structuredvariables in causal models. Our experiments demonstrate that our proposedmechanisms are capable of accurate abduction and estimation of direct, indirectand total effects as measured by axiomatic soundness of counterfactuals.</description><author>Fabio De Sousa Ribeiro, Tian Xia, Miguel Monteiro, Nick Pawlowski, Ben Glocker</author><pubDate>Tue, 18 Jul 2023 16:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15764v2</guid></item><item><title>Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders</title><link>http://arxiv.org/abs/2305.16189v2</link><description>Unsupervised source separation involves unraveling an unknown set of sourcesignals recorded through a mixing operator, with limited prior knowledge aboutthe sources, and only access to a dataset of signal mixtures. This problem isinherently ill-posed and is further challenged by the variety of time-scalesexhibited by sources in time series data. Existing methods typically rely on apreselected window size that limits their capacity to handle multi-scalesources. To address this issue, instead of operating in the time domain, wepropose an unsupervised multi-scale clustering and source separation frameworkby leveraging wavelet scattering covariances that provide a low-dimensionalrepresentation of stochastic processes, capable of distinguishing betweendifferent non-Gaussian stochastic processes. Nested within this representationspace, we develop a factorial Gaussian-mixture variational autoencoder that istrained to (1) probabilistically cluster sources at different time-scales and(2) independently sample scattering covariance representations associated witheach cluster. Using samples from each cluster as prior information, weformulate source separation as an optimization problem in the waveletscattering covariance representation space, resulting in separated sources inthe time domain. When applied to seismic data recorded during the NASA InSightmission on Mars, our multi-scale nested approach proves to be a powerful toolfor discriminating between sources varying greatly in time-scale, e.g.,minute-long transient one-sided pulses (known as ``glitches'') and structuredambient noises resulting from atmospheric activities that typically last fortens of minutes. These results provide an opportunity to conduct furtherinvestigations into the isolated sources related to atmospheric-surfaceinteractions, thermal relaxations, and other complex phenomena.</description><author>Ali Siahkoohi, Rudy Morel, Randall Balestriero, Erwan Allys, Grégory Sainton, Taichi Kawamura, Maarten V. de Hoop</author><pubDate>Tue, 18 Jul 2023 16:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16189v2</guid></item><item><title>Exploiting Field Dependencies for Learning on Categorical Data</title><link>http://arxiv.org/abs/2307.09321v1</link><description>Traditional approaches for learning on categorical data underexploit thedependencies between columns (\aka fields) in a dataset because they rely onthe embedding of data points driven alone by the classification/regressionloss. In contrast, we propose a novel method for learning on categorical datawith the goal of exploiting dependencies between fields. Instead of modellingstatistics of features globally (i.e., by the covariance matrix of features),we learn a global field dependency matrix that captures dependencies betweenfields and then we refine the global field dependency matrix at theinstance-wise level with different weights (so-called local dependencymodelling) w.r.t. each field to improve the modelling of the fielddependencies. Our algorithm exploits the meta-learning paradigm, i.e., thedependency matrices are refined in the inner loop of the meta-learningalgorithm without the use of labels, whereas the outer loop intertwines theupdates of the embedding matrix (the matrix performing projection) and globaldependency matrix in a supervised fashion (with the use of labels). Our methodis simple yet it outperforms several state-of-the-art methods on six populardataset benchmarks. Detailed ablation studies provide additional insights intoour method.</description><author>Zhibin Li, Piotr Koniusz, Lu Zhang, Daniel Edward Pagendam, Peyman Moghadam</author><pubDate>Tue, 18 Jul 2023 16:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09321v1</guid></item><item><title>Biomaker CA: a Biome Maker project using Cellular Automata</title><link>http://arxiv.org/abs/2307.09320v1</link><description>We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA).In Biomaker CA, morphogenesis is a first class citizen and small seeds need togrow into plant-like organisms to survive in a nutrient starved environment andeventually reproduce with variation so that a biome survives for longtimelines. We simulate complex biomes by means of CA rules in 2D grids andparallelize all of its computation on GPUs through the Python JAX framework. Weshow how this project allows for several different kinds of environments andlaws of 'physics', alongside different model architectures and mutationstrategies. We further analyze some configurations to show how plant agents cangrow, survive, reproduce, and evolve, forming stable and unstable biomes. Wethen demonstrate how one can meta-evolve models to survive in a harshenvironment either through end-to-end meta-evolution or by a more surgical andefficient approach, called Petri dish meta-evolution. Finally, we show how toperform interactive evolution, where the user decides how to evolve a plantmodel interactively and then deploys it in a larger environment. We open sourceBiomaker CA at: https://tinyurl.com/2x8yu34s .</description><author>Ettore Randazzo, Alexander Mordvintsev</author><pubDate>Tue, 18 Jul 2023 16:03:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09320v1</guid></item><item><title>MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds</title><link>http://arxiv.org/abs/2307.09316v1</link><description>3D semantic segmentation on multi-scan large-scale point clouds plays animportant role in autonomous systems. Unlike the single-scan-based semanticsegmentation task, this task requires distinguishing the motion states ofpoints in addition to their semantic categories. However, methods designed forsingle-scan-based segmentation tasks perform poorly on the multi-scan task dueto the lacking of an effective way to integrate temporal information. Wepropose MarS3D, a plug-and-play motion-aware module for semantic segmentationon multi-scan 3D point clouds. This module can be flexibly combined withsingle-scan models to allow them to have multi-scan perception abilities. Themodel encompasses two key designs: the Cross-Frame Feature Embedding module forenriching representation learning and the Motion-Aware Feature Learning modulefor enhancing motion awareness. Extensive experiments show that MarS3D canimprove the performance of the baseline model by a large margin. The code isavailable at https://github.com/CVMI-Lab/MarS3D.</description><author>Jiahui Liu, Chirui Chang, Jianhui Liu, Xiaoyang Wu, Lan Ma, Xiaojuan Qi</author><pubDate>Tue, 18 Jul 2023 15:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09316v1</guid></item><item><title>Untargeted Near-collision Attacks in Biometric Recognition</title><link>http://arxiv.org/abs/2304.01580v3</link><description>A biometric recognition system can operate in two distinct modes,identification or verification. In the first mode, the system recognizes anindividual by searching the enrolled templates of all the users for a match. Inthe second mode, the system validates a user's identity claim by comparing thefresh provided template with the enrolled template. The biometrictransformation schemes usually produce binary templates that are better handledby cryptographic schemes, and the comparison is based on a distance that leaksinformation about the similarities between two biometric templates. Both theexperimentally determined false match rate and false non-match rate throughrecognition threshold adjustment define the recognition accuracy, and hence thesecurity of the system. To the best of our knowledge, few works provide aformal treatment of the security under minimum leakage of information, i.e.,the binary outcome of a comparison with a threshold. In this paper, we rely onprobabilistic modelling to quantify the security strength of binary templates.We investigate the influence of template size, database size and threshold onthe probability of having a near-collision. We highlight several untargetedattacks on biometric systems considering naive and adaptive adversaries.Interestingly, these attacks can be launched both online and offline and, bothin the identification mode and in the verification mode. We discuss the choiceof parameters through the generic presented attacks.</description><author>Axel Durbet, Paul-Marie Grollemund, Kevin Thiry-Atighehchi</author><pubDate>Tue, 18 Jul 2023 15:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01580v3</guid></item><item><title>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media</title><link>http://arxiv.org/abs/2307.09312v1</link><description>We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modalgraph-based transformer model for detecting hate speech in online socialnetworks. In contrast to traditional text-only methods, our approach tolabelling a comment as hate speech centers around the holistic analysis of textand images. This is done by leveraging graph transformers to capture thecontextual relationships in the entire discussion that surrounds a comment,with interwoven fusion layers to combine text and image embeddings instead ofprocessing different modalities separately. We compare the performance of ourmodel to baselines that only process text; we also conduct extensive ablationstudies. We conclude with future work for multimodal solutions to deliversocial value in online contexts, arguing that capturing a holistic view of aconversation greatly advances the effort to detect anti-social behavior.</description><author>Liam Hebert, Gaurav Sahu, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen</author><pubDate>Tue, 18 Jul 2023 15:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09312v1</guid></item><item><title>Automatic Differentiation for Inverse Problems with Applications in Quantum Transport</title><link>http://arxiv.org/abs/2307.09311v1</link><description>A neural solver and differentiable simulation of the quantum transmittingboundary model is presented for the inverse quantum transport problem. Theneural solver is used to engineer continuous transmission properties and thedifferentiable simulation is used to engineer current-voltage characteristics.</description><author>Ivan Williams, Eric Polizzi</author><pubDate>Tue, 18 Jul 2023 15:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09311v1</guid></item><item><title>EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting</title><link>http://arxiv.org/abs/2307.09306v1</link><description>Capturing high-dimensional social interactions and feasible futures isessential for predicting trajectories. To address this complex nature, severalattempts have been devoted to reducing the dimensionality of the outputvariables via parametric curve fitting such as the B\'ezier curve and B-splinefunction. However, these functions, which originate in computer graphicsfields, are not suitable to account for socially acceptable human dynamics. Inthis paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory predictionapproach that uses a novel trajectory descriptor to form a compact space, knownhere as $\mathbb{ET}$ space, in place of Euclidean space, for representingpedestrian movements. We first reduce the complexity of the trajectorydescriptor via a low-rank approximation. We transform the pedestrians' historypaths into our $\mathbb{ET}$ space represented by spatio-temporal principlecomponents, and feed them into off-the-shelf trajectory forecasting models. Theinputs and outputs of the models as well as social interactions are allgathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, wepropose a trajectory anchor-based refinement method to cover all possiblefutures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstratethat our EigenTrajectory predictor can significantly improve both theprediction accuracy and reliability of existing trajectory forecasting modelson public benchmarks, indicating that the proposed descriptor is suited torepresent pedestrian behaviors. Code is publicly available athttps://github.com/inhwanbae/EigenTrajectory .</description><author>Inhwan Bae, Jean Oh, Hae-Gon Jeon</author><pubDate>Tue, 18 Jul 2023 15:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09306v1</guid></item><item><title>FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer</title><link>http://arxiv.org/abs/2304.02011v2</link><description>Particle localization and -classification constitute two of the mostfundamental problems in computational microscopy. In recent years, deeplearning based approaches have been introduced for these tasks with greatsuccess. A key shortcoming of these supervised learning methods is their needfor large training data sets, typically generated from particle models inconjunction with complex numerical forward models simulating the physics oftransmission electron microscopes. Computer implementations of such forwardmodels are computationally extremely demanding and limit the scope of theirapplicability. In this paper we propose a method for simulating the forwardoperator of an electron microscope based on additive noise and Neural StyleTransfer techniques. We evaluate the method on localization and classificationtasks using one of the established state-of-the-art architectures showingperformance on par with the benchmark. In contrast to previous approaches, ourmethod accelerates the data generation process by a factor of 750 while using33 times less memory and scales well to typical transmission electronmicroscope detector sizes. It utilizes GPU acceleration and parallelprocessing. It can be used to adapt a synthetic training data set according toreference data from any transmission electron microscope. The source code isavailable at https://gitlab.com/deepet/faket.</description><author>Pavol Harar, Lukas Herrmann, Philipp Grohs, David Haselbach</author><pubDate>Tue, 18 Jul 2023 15:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02011v2</guid></item><item><title>Conformal prediction under ambiguous ground truth</title><link>http://arxiv.org/abs/2307.09302v1</link><description>In safety-critical classification tasks, conformal prediction allows toperform rigorous uncertainty quantification by providing confidence setsincluding the true class with a user-specified probability. This generallyassumes the availability of a held-out calibration set with access to groundtruth labels. Unfortunately, in many domains, such labels are difficult toobtain and usually approximated by aggregating expert opinions. In fact, thisholds true for almost all datasets, including well-known ones such as CIFAR andImageNet. Applying conformal prediction using such labels underestimatesuncertainty. Indeed, when expert opinions are not resolvable, there is inherentambiguity present in the labels. That is, we do not have ``crisp'', definitiveground truth labels and this uncertainty should be taken into account duringcalibration. In this paper, we develop a conformal prediction framework forsuch ambiguous ground truth settings which relies on an approximation of theunderlying posterior distribution of labels given inputs. We demonstrate ourmethodology on synthetic and real datasets, including a case study of skincondition classification in dermatology.</description><author>David Stutz, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, Arnaud Doucet</author><pubDate>Tue, 18 Jul 2023 15:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09302v1</guid></item><item><title>Rumor Detection with Diverse Counterfactual Evidence</title><link>http://arxiv.org/abs/2307.09296v1</link><description>The growth in social media has exacerbated the threat of fake news toindividuals and communities. This draws increasing attention to developingefficient and timely rumor detection methods. The prevailing approaches resortto graph neural networks (GNNs) to exploit the post-propagation patterns of therumor-spreading process. However, these methods lack inherent interpretation ofrumor detection due to the black-box nature of GNNs. Moreover, these methodssuffer from less robust results as they employ all the propagation patterns forrumor detection. In this paper, we address the above issues with the proposedDiverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Ourintuition is to exploit the diverse counterfactual evidence of an event graphto serve as multi-view interpretations, which are further aggregated for robustrumor detection results. Specifically, our method first designs a subgraphgeneration strategy to efficiently generate different subgraphs of the eventgraph. We constrain the removal of these subgraphs to cause the change in rumordetection results. Thus, these subgraphs naturally serve as counterfactualevidence for rumor detection. To achieve multi-view interpretation, we design adiversity loss inspired by Determinantal Point Processes (DPP) to encouragediversity among the counterfactual evidence. A GNN-based rumor detection modelfurther aggregates the diverse counterfactual evidence discovered by theproposed DCE-RD to achieve interpretable and robust rumor detection results.Extensive experiments on two real-world datasets show the superior performanceof our method. Our code is available at https://github.com/Vicinity111/DCE-RD.</description><author>Kaiwei Zhang, Junchi Yu, Haichao Shi, Jian Liang, Xiao-Yu Zhang</author><pubDate>Tue, 18 Jul 2023 15:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09296v1</guid></item><item><title>Llama 2: Open Foundation and Fine-Tuned Chat Models</title><link>http://arxiv.org/abs/2307.09288v1</link><description>In this work, we develop and release Llama 2, a collection of pretrained andfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized fordialogue use cases. Our models outperform open-source chat models on mostbenchmarks we tested, and based on our human evaluations for helpfulness andsafety, may be a suitable substitute for closed-source models. We provide adetailed description of our approach to fine-tuning and safety improvements ofLlama 2-Chat in order to enable the community to build on our work andcontribute to the responsible development of LLMs.</description><author>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing </author><pubDate>Tue, 18 Jul 2023 15:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09288v1</guid></item><item><title>FlexiAST: Flexibility is What AST Needs</title><link>http://arxiv.org/abs/2307.09286v1</link><description>The objective of this work is to give patch-size flexibility to AudioSpectrogram Transformers (AST). Recent advancements in ASTs have shown superiorperformance in various audio-based tasks. However, the performance of standardASTs degrades drastically when evaluated using different patch sizes from thatused during training. As a result, AST models are typically re-trained toaccommodate changes in patch sizes. To overcome this limitation, this paperproposes a training procedure to provide flexibility to standard AST modelswithout architectural changes, allowing them to work with various patch sizesat the inference stage - FlexiAST. This proposed training approach simplyutilizes random patch size selection and resizing of patch and positionalembedding weights. Our experiments show that FlexiAST gives similar performanceto standard AST models while maintaining its evaluation ability at variouspatch sizes on different datasets for audio classification tasks.</description><author>Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, Arda Senocak</author><pubDate>Tue, 18 Jul 2023 15:30:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09286v1</guid></item><item><title>RepViT: Revisiting Mobile CNN From ViT Perspective</title><link>http://arxiv.org/abs/2307.09283v1</link><description>Recently, lightweight Vision Transformers (ViTs) demonstrate superiorperformance and lower latency compared with lightweight Convolutional NeuralNetworks (CNNs) on resource-constrained mobile devices. This improvement isusually attributed to the multi-head self-attention module, which enables themodel to learn global representations. However, the architectural disparitiesbetween lightweight ViTs and lightweight CNNs have not been adequatelyexamined. In this study, we revisit the efficient design of lightweight CNNsand emphasize their potential for mobile devices. We incrementally enhance themobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, byintegrating the efficient architectural choices of lightweight ViTs. This endsup with a new family of pure lightweight CNNs, namely RepViT. Extensiveexperiments show that RepViT outperforms existing state-of-the-art lightweightViTs and exhibits favorable latency in various vision tasks. On ImageNet,RepViT achieves over 80\% top-1 accuracy with nearly 1ms latency on an iPhone12, which is the first time for a lightweight model, to the best of ourknowledge. Our largest model, RepViT-M3, obtains 81.4\% accuracy with only1.3ms latency. The code and trained models are available at\url{https://github.com/jameslahm/RepViT}.</description><author>Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, Guiguang Ding</author><pubDate>Tue, 18 Jul 2023 15:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09283v1</guid></item><item><title>Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe</title><link>http://arxiv.org/abs/2210.14348v3</link><description>Privacy concerns have attracted increasing attention in data-driven productsdue to the tendency of machine learning models to memorize sensitive trainingdata. Generating synthetic versions of such data with a formal privacyguarantee, such as differential privacy (DP), provides a promising path tomitigating these privacy concerns, but previous approaches in this directionhave typically failed to produce synthetic data of high quality. In this work,we show that a simple and practical recipe in the text domain is effective:simply fine-tuning a pretrained generative language model with DP enables themodel to generate useful synthetic text with strong privacy protection. Throughextensive empirical analyses on both benchmark and private customer data, wedemonstrate that our method produces synthetic text that is competitive interms of utility with its non-private counterpart, meanwhile providing strongprotection against potential privacy leakages.</description><author>Xiang Yue, Huseyin A. Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan, Robert Sim</author><pubDate>Tue, 18 Jul 2023 15:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.14348v3</guid></item><item><title>Regression-free Blind Image Quality Assessment</title><link>http://arxiv.org/abs/2307.09279v1</link><description>Regression-based blind image quality assessment (IQA) models are susceptibleto biased training samples, leading to a biased estimation of model parameters.To mitigate this issue, we propose a regression-free framework for imagequality evaluation, which is founded upon retrieving similar instances byincorporating semantic and distortion features. The motivation behind thisapproach is rooted in the observation that the human visual system (HVS) hasanalogous visual responses to semantically similar image contents degraded bythe same distortion. The proposed framework comprises two classification-basedmodules: semantic-based classification (SC) module and distortion-basedclassification (DC) module. Given a test image and an IQA database, the SCmodule retrieves multiple pristine images based on semantic similarity. The DCmodule then retrieves instances based on distortion similarity from thedistorted images that correspond to each retrieved pristine image. Finally, thepredicted quality score is derived by aggregating the subjective quality scoresof multiple retrieved instances. Experimental results on four benchmarkdatabases validate that the proposed model can remarkably outperform thestate-of-the-art regression-based models.</description><author>Xiaoqi Wang, Jian Xiong, Hao Gao, Weisi Lin</author><pubDate>Tue, 18 Jul 2023 15:19:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09279v1</guid></item><item><title>High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance</title><link>http://arxiv.org/abs/2302.00999v2</link><description>During recent years the interest of optimization and machine learningcommunities in high-probability convergence of stochastic optimization methodshas been growing. One of the main reasons for this is that high-probabilitycomplexity bounds are more accurate and less studied than in-expectation ones.However, SOTA high-probability non-asymptotic convergence results are derivedunder strong assumptions such as the boundedness of the gradient noise varianceor of the objective's gradient itself. In this paper, we propose severalalgorithms with high-probability convergence results under less restrictiveassumptions. In particular, we derive new high-probability convergence resultsunder the assumption that the gradient/operator noise has bounded central$\alpha$-th moment for $\alpha \in (1,2]$ in the following setups: (i) smoothnon-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-stronglyconvex minimization problems, (ii) Lipschitz / star-cocoercive and monotone /quasi-strongly monotone variational inequalities. These results justify theusage of the considered methods for solving problems that do not fit standardfunctional classes studied in stochastic optimization.</description><author>Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horváth, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, Peter Richtárik</author><pubDate>Tue, 18 Jul 2023 15:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00999v2</guid></item><item><title>Improving Text Semantic Similarity Modeling through a 3D Siamese Network</title><link>http://arxiv.org/abs/2307.09274v1</link><description>Siamese networks have gained popularity as a method for modeling textsemantic similarity. Traditional methods rely on pooling operation to compressthe semantic representations from Transformer blocks in encoding, resulting intwo-dimensional semantic vectors and the loss of hierarchical semanticinformation from Transformer blocks. Moreover, this limited structure ofsemantic vectors is akin to a flattened landscape, which restricts the methodsthat can be applied in downstream modeling, as they can only navigate this flatterrain. To address this issue, we propose a novel 3D Siamese network for textsemantic similarity modeling, which maps semantic information to ahigher-dimensional space. The three-dimensional semantic tensors not onlyretains more precise spatial and feature domain information but also providesthe necessary structural condition for comprehensive downstream modelingstrategies to capture them. Leveraging this structural advantage, we introduceseveral modules to reinforce this 3D framework, focusing on three aspects:feature extraction, attention, and feature fusion. Our extensive experiments onfour text semantic similarity benchmarks demonstrate the effectiveness andefficiency of our 3D Siamese Network.</description><author>Jianxiang Zang, Hui Liu</author><pubDate>Tue, 18 Jul 2023 15:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09274v1</guid></item><item><title>Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images</title><link>http://arxiv.org/abs/2307.08535v2</link><description>Cine magnetic resonance imaging (MRI) is the current gold standard for theassessment of cardiac anatomy and function. However, it typically only acquiresa set of two-dimensional (2D) slices of the underlying three-dimensional (3D)anatomy of the heart, thus limiting the understanding and analysis of bothhealthy and pathological cardiac morphology and physiology. In this paper, wepropose a novel fully automatic surface reconstruction pipeline capable ofreconstructing multi-class 3D cardiac anatomy meshes from raw cine MRIacquisitions. Its key component is a multi-class point cloud completion network(PCCN) capable of correcting both the sparsity and misalignment issues of the3D reconstruction task in a unified model. We first evaluate the PCCN on alarge synthetic dataset of biventricular anatomies and observe Chamferdistances between reconstructed and gold standard anatomies below or similar tothe underlying image resolution for multiple levels of slice misalignment.Furthermore, we find a reduction in reconstruction error compared to abenchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and meansurface distance, respectively. We then apply the PCCN as part of our automatedreconstruction pipeline to 1000 subjects from the UK Biobank study in across-domain transfer setting and demonstrate its ability to reconstructaccurate and topologically plausible biventricular heart meshes with clinicalmetrics comparable to the previous literature. Finally, we investigate therobustness of our proposed approach and observe its capacity to successfullyhandle multiple common outlier conditions.</description><author>Marcel Beetz, Abhirup Banerjee, Julius Ossenberg-Engels, Vicente Grau</author><pubDate>Tue, 18 Jul 2023 15:11:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08535v2</guid></item><item><title>K-Tensors: Clustering Positive Semi-Definite Matrices</title><link>http://arxiv.org/abs/2306.06534v3</link><description>This paper introduces a novel self-consistency clustering algorithm($K$-Tensors) designed for {partitioning a distribution of}positive-semidefinite matrices based on their eigenstructures. As positivesemi-definite matrices can be represented as ellipsoids in $\mathbb R^p$, $p\ge 2$, it is critical to maintain their structural information to performeffective clustering. However, traditional clustering algorithms {applied tomatrices} often {involve vectorization of} the matrices, resulting in a loss ofessential structural information. To address this issue, we propose a distancemetric {for clustering} that is specifically based on the structuralinformation of positive semi-definite matrices. This distance metric enablesthe clustering algorithm to consider the differences between positivesemi-definite matrices and their projections onto {a} common space spanned by\thadJulyTen{orthonormal vectors defined from a set of} positive semi-definitematrices. This innovative approach to clustering positive semi-definitematrices has broad applications in several domains including financial andbiomedical research, such as analyzing functional connectivity data. Bymaintaining the structural information of positive semi-definite matrices, ourproposed algorithm promises to cluster the positive semi-definite matrices in amore meaningful way, thereby facilitating deeper insights into the underlyingdata in various applications.</description><author>Hanchao Zhang, Thaddeus Tarpey</author><pubDate>Tue, 18 Jul 2023 15:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06534v3</guid></item><item><title>Linearized Relative Positional Encoding</title><link>http://arxiv.org/abs/2307.09270v1</link><description>Relative positional encoding is widely used in vanilla and lineartransformers to represent positional information. However, existing encodingmethods of a vanilla transformer are not always directly applicable to a lineartransformer, because the latter requires a decomposition of the query and keyrepresentations into separate kernel functions. Nevertheless, principles fordesigning encoding methods suitable for linear transformers remainunderstudied. In this work, we put together a variety of existing linearrelative positional encoding approaches under a canonical form and furtherpropose a family of linear relative positional encoding algorithms via unitarytransformation. Our formulation leads to a principled framework that can beused to develop new relative positional encoding methods that preserve linearspace-time complexity. Equipped with different models, the proposed linearizedrelative positional encoding (LRPE) family derives effective encoding forvarious applications. Experiments show that compared with existing methods,LRPE achieves state-of-the-art performance in language modeling, textclassification, and image classification. Meanwhile, it emphasizes a generalparadigm for designing broadly more relative positional encoding methods thatare applicable to linear transformers. The code is available athttps://github.com/OpenNLPLab/Lrpe.</description><author>Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng Kong, Yiran Zhong</author><pubDate>Tue, 18 Jul 2023 14:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09270v1</guid></item><item><title>TabText: A Flexible and Contextual Approach to Tabular Data Representation</title><link>http://arxiv.org/abs/2206.10381v3</link><description>Tabular data is essential for applying machine learning tasks across variousindustries. However, traditional data processing methods do not fully utilizeall the information available in the tables, ignoring important contextualinformation such as column header descriptions. In addition, pre-processingdata into a tabular format can remain a labor-intensive bottleneck in modeldevelopment. This work introduces TabText, a processing and feature extractionframework that extracts contextual information from tabular data structures.TabText addresses processing difficulties by converting the content intolanguage and utilizing pre-trained large language models (LLMs). We evaluateour framework on nine healthcare prediction tasks ranging from patientdischarge, ICU admission, and mortality. We show that 1) applying our TabTextframework enables the generation of high-performing and simple machine learningbaseline models with minimal data pre-processing, and 2) augmentingpre-processed tabular data with TabText representations improves the averageand worst-case AUC performance of standard machine learning models by as muchas 6%.</description><author>Kimberly Villalobos Carballo, Liangyuan Na, Yu Ma, Léonard Boussioux, Cynthia Zeng, Luis R. Soenksen, Dimitris Bertsimas</author><pubDate>Tue, 18 Jul 2023 14:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10381v3</guid></item><item><title>End-to-End Neural Network Training for Hyperbox-Based Classification</title><link>http://arxiv.org/abs/2307.09269v1</link><description>Hyperbox-based classification has been seen as a promising technique in whichdecisions on the data are represented as a series of orthogonal,multidimensional boxes (i.e., hyperboxes) that are often interpretable andhuman-readable. However, existing methods are no longer capable of efficientlyhandling the increasing volume of data many application domains face nowadays.We address this gap by proposing a novel, fully differentiable framework forhyperbox-based classification via neural networks. In contrast to previouswork, our hyperbox models can be efficiently trained in an end-to-end fashion,which leads to significantly reduced training times and superior classificationresults.</description><author>Denis Mayr Lima Martins, Christian Lülf, Fabian Gieseke</author><pubDate>Tue, 18 Jul 2023 14:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09269v1</guid></item><item><title>Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding</title><link>http://arxiv.org/abs/2307.09267v1</link><description>3D visual grounding involves finding a target object in a 3D scene thatcorresponds to a given sentence query. Although many approaches have beenproposed and achieved impressive performance, they all require denseobject-sentence pair annotations in 3D point clouds, which are bothtime-consuming and expensive. To address the problem that fine-grainedannotated data is difficult to obtain, we propose to leverage weakly supervisedannotations to learn the 3D visual grounding model, i.e., only coarsescene-sentence correspondences are used to learn object-sentence links. Toaccomplish this, we design a novel semantic matching model that analyzes thesemantic similarity between object proposals and sentences in a coarse-to-finemanner. Specifically, we first extract object proposals and coarsely select thetop-K candidates based on feature and class similarity matrices. Next, wereconstruct the masked keywords of the sentence using each candidate one byone, and the reconstructed accuracy finely reflects the semantic similarity ofeach candidate to the query. Additionally, we distill the coarse-to-finesemantic matching knowledge into a typical two-stage 3D visual grounding model,which reduces inference costs and improves performance by taking full advantageof the well-studied structure of the existing architectures. We conductextensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate theeffectiveness of our proposed method.</description><author>Zehan Wang, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao</author><pubDate>Tue, 18 Jul 2023 14:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09267v1</guid></item><item><title>Knowledge Distillation for Object Detection: from generic to remote sensing datasets</title><link>http://arxiv.org/abs/2307.09264v1</link><description>Knowledge distillation, a well-known model compression technique, is anactive research area in both computer vision and remote sensing communities. Inthis paper, we evaluate in a remote sensing context various off-the-shelfobject detection knowledge distillation methods which have been originallydeveloped on generic computer vision datasets such as Pascal VOC. Inparticular, methods covering both logit mimicking and feature imitationapproaches are applied for vehicle detection using the well-known benchmarkssuch as xView and VEDAI datasets. Extensive experiments are performed tocompare the relative performance and interrelationships of the methods.Experimental results show high variations and confirm the importance of resultaggregation and cross validation on remote sensing datasets.</description><author>Hoàng-Ân Lê, Minh-Tan Pham</author><pubDate>Tue, 18 Jul 2023 14:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09264v1</guid></item><item><title>Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning</title><link>http://arxiv.org/abs/2307.09263v1</link><description>As an efficient distributed machine learning approach, Federated learning(FL) can obtain a shared model by iterative local model training at the userside and global model aggregating at the central server side, therebyprotecting privacy of users. Mobile users in FL systems typically communicatewith base stations (BSs) via wireless channels, where training performancecould be degraded due to unreliable access caused by user mobility. However,existing work only investigates a static scenario or random initialization ofuser locations, which fail to capture mobility in real-world networks. Totackle this issue, we propose a practical model for user mobility in FL acrossmultiple BSs, and develop a user scheduling and resource allocation method tominimize the training delay with constrained communication resources.Specifically, we first formulate an optimization problem with user mobilitythat jointly considers user selection, BS assignment to users, and bandwidthallocation to minimize the latency in each communication round. Thisoptimization problem turned out to be NP-hard and we proposed a delay-awaregreedy search algorithm (DAGSA) to solve it. Simulation results show that theproposed algorithm achieves better performance than the state-of-the-artbaselines and a certain level of user mobility could improve trainingperformance.</description><author>Kecheng Fan, Wen Chen, Jun Li, Xiumei Deng, Xuefeng Han, Ming Ding</author><pubDate>Tue, 18 Jul 2023 14:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09263v1</guid></item><item><title>Neuromorphic spintronics simulated using an unconventional data-driven Thiele equation approach</title><link>http://arxiv.org/abs/2307.09262v1</link><description>In this study, we developed a quantitative description of the dynamics ofspin-torque vortex nano-oscillators (STVOs) through an unconventional modelbased on the combination of the Thiele equation approach (TEA) and data frommicromagnetic simulations (MMS). Solving the STVO dynamics with our analyticalmodel allows to accelerate the simulations by 9 orders of magnitude compared toMMS while reaching the same level of accuracy. Here, we showcase our model bysimulating a STVO-based neural network for solving a classification task. Weassess its performance with respect to the input signal current intensity andthe level of noise that might affect such a system. Our approach is promisingfor accelerating the design of STVO-based neuromorphic computing devices whiledecreasing drastically its computational cost.</description><author>Anatole Moureaux, Simon de Wergifosse, Chloé Chopin, Flavio Abreu Araujo</author><pubDate>Tue, 18 Jul 2023 14:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09262v1</guid></item><item><title>Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds</title><link>http://arxiv.org/abs/2307.09259v1</link><description>Machine learning for point clouds has been attracting much attention, withmany applications in various fields, such as shape recognition and materialscience. To enhance the accuracy of such machine learning methods, it is knownto be effective to incorporate global topological features, which are typicallyextracted by persistent homology. In the calculation of persistent homology fora point cloud, we need to choose a filtration for the point clouds, anincreasing sequence of spaces. Because the performance of machine learningmethods combined with persistent homology is highly affected by the choice of afiltration, we need to tune it depending on data and tasks. In this paper, wepropose a framework that learns a filtration adaptively with the use of neuralnetworks. In order to make the resulting persistent homologyisometry-invariant, we develop a neural network architecture with suchinvariance. Additionally, we theoretically show a finite-dimensionalapproximation result that justifies our architecture. Experimental resultsdemonstrated the efficacy of our framework in several classification tasks.</description><author>Naoki Nishikawa, Yuichi Ike, Kenji Yamanishi</author><pubDate>Tue, 18 Jul 2023 14:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09259v1</guid></item><item><title>Text vectorization via transformer-based language models and n-gram perplexities</title><link>http://arxiv.org/abs/2307.09255v1</link><description>As the probability (and thus perplexity) of a text is calculated based on theproduct of the probabilities of individual tokens, it may happen that oneunlikely token significantly reduces the probability (i.e., increase theperplexity) of some otherwise highly probable input, while potentiallyrepresenting a simple typographical error. Also, given that perplexity is ascalar value that refers to the entire input, information about the probabilitydistribution within it is lost in the calculation (a relatively good text thathas one unlikely token and another text in which each token is equally likelythey can have the same perplexity value), especially for longer texts. As analternative to scalar perplexity this research proposes a simple algorithm usedto calculate vector values based on n-gram perplexities within the input. Suchrepresentations consider the previously mentioned aspects, and instead of aunique value, the relative perplexity of each text token is calculated, andthese values are combined into a single vector representing the input.</description><author>Mihailo Škorić</author><pubDate>Tue, 18 Jul 2023 14:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09255v1</guid></item><item><title>PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models</title><link>http://arxiv.org/abs/2307.09254v1</link><description>Uncertainty learning and quantification of models are crucial tasks toenhance the trustworthiness of the models. Importantly, the recent surge ofgenerative language models (GLMs) emphasizes the need for reliable uncertaintyquantification due to the concerns on generating hallucinated facts. In thispaper, we propose to learn neural prediction set models that comes with theprobably approximately correct (PAC) guarantee for quantifying the uncertaintyof GLMs. Unlike existing prediction set models, which are parameterized by ascalar value, we propose to parameterize prediction sets via neural networks,which achieves more precise uncertainty quantification but still satisfies thePAC guarantee. We demonstrate the efficacy of our method on four types oflanguage datasets and six types of models by showing that our method improvesthe quantified uncertainty by $63\%$ on average, compared to a standardbaseline method.</description><author>Sangdon Park, Taesoo Kim</author><pubDate>Tue, 18 Jul 2023 14:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09254v1</guid></item><item><title>Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese</title><link>http://arxiv.org/abs/2306.15788v2</link><description>We investigate the effectiveness of GPT-3.5 and GPT-4, two large languagemodels, as Grammatical Error Correction (GEC) tools for Brazilian Portugueseand compare their performance against Microsoft Word and Google Docs. Weintroduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,Spelling, Internet, and Fast typing. Our results show that while GPT-4 hashigher recall than other methods, LLMs tend to have lower precision, leading toovercorrection. This study demonstrates the potential of LLMs as practical GECtools for Brazilian Portuguese and encourages further exploration of LLMs fornon-English languages and other educational settings.</description><author>Maria Carolina Penteado, Fábio Perez</author><pubDate>Tue, 18 Jul 2023 14:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15788v2</guid></item><item><title>SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model</title><link>http://arxiv.org/abs/2303.05118v2</link><description>The goal of continual learning is to improve the performance of recognitionmodels in learning sequentially arrived data. Although most existing works areestablished on the premise of learning from scratch, growing efforts have beendevoted to incorporating the benefits of pre-training. However, how toadaptively exploit the pre-trained knowledge for each incremental task whilemaintaining its generalizability remains an open question. In this work, wepresent an extensive analysis for continual learning on a pre-trained model(CLPM), and attribute the key challenge to a progressive overfitting problem.Observing that selectively reducing the learning rate can almost resolve thisissue in the representation layer, we propose a simple but extremely effectiveapproach named Slow Learner with Classifier Alignment (SLCA), which furtherimproves the classification layer by modeling the class-wise distributions andaligning the classification layers in a post-hoc fashion. Across a variety ofscenarios, our proposal provides substantial improvements for CLPM (e.g., up to49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, SplitCUB-200 and Split Cars-196, respectively), and thus outperformsstate-of-the-art approaches by a large margin. Based on such a strong baseline,critical factors and promising directions are analyzed in-depth to facilitatesubsequent research.</description><author>Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei</author><pubDate>Tue, 18 Jul 2023 14:29:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05118v2</guid></item><item><title>UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data</title><link>http://arxiv.org/abs/2307.09249v1</link><description>Recent advancements in Natural Language Processing (NLP) have witnessed thegroundbreaking impact of pretrained models, yielding impressive outcomes acrossvarious tasks. This study seeks to extend the power of pretrainingmethodologies to tabular data, a domain traditionally overlooked, yetinherently challenging due to the plethora of table schemas intrinsic todifferent tasks. The primary research questions underpinning this work revolvearound the adaptation to heterogeneous table structures, the establishment of auniversal pretraining protocol for tabular data, the generalizability andtransferability of learned knowledge across tasks, the adaptation to diversedownstream applications, and the incorporation of incremental columns overtime. In response to these challenges, we introduce UniTabE, a pioneeringmethod designed to process tables in a uniform manner, devoid of constraintsimposed by specific table structures. UniTabE's core concept relies onrepresenting each basic table element with a module, termed TabUnit. This issubsequently followed by a Transformer encoder to refine the representation.Moreover, our model is designed to facilitate pretraining and finetuningthrough the utilization of free-form prompts. In order to implement thepretraining phase, we curated an expansive tabular dataset comprisingapproximately 13 billion samples, meticulously gathered from the Kaggleplatform. Rigorous experimental testing and analyses were performed under amyriad of scenarios to validate the effectiveness of our methodology. Theexperimental results demonstrate UniTabE's superior performance against severalbaseline models across a multitude of benchmark datasets. This, therefore,underscores UniTabE's potential to significantly enhance the semanticrepresentation of tabular data, thereby marking a significant stride in thefield of tabular data analysis.</description><author>Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, Qi Liu</author><pubDate>Tue, 18 Jul 2023 14:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09249v1</guid></item><item><title>Application of BERT in Wind Power Forecasting-Teletraan's Solution in Baidu KDD Cup 2022</title><link>http://arxiv.org/abs/2307.09248v1</link><description>Nowadays, wind energy has drawn increasing attention as its important role incarbon neutrality and sustainable development. When wind power is integratedinto the power grid, precise forecasting is necessary for the sustainabilityand security of the system. However, the unpredictable nature and long sequenceprediction make it especially challenging. In this technical report, weintroduce the BERT model applied for Baidu KDD Cup 2022, and the dailyfluctuation is added by post-processing to make the predicted results in linewith daily periodicity. Our solution achieves 3rd place of 2490 teams. The codeis released athttps://github.com/LongxingTan/KDD2022-Baidu</description><author>Longxing Tan, Hongying Yue</author><pubDate>Tue, 18 Jul 2023 14:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09248v1</guid></item><item><title>Towards Sustainable Deep Learning for Multi-Label Classification on NILM</title><link>http://arxiv.org/abs/2307.09244v1</link><description>Non-intrusive load monitoring (NILM) is the process of obtainingappliance-level data from a single metering point, measuring total electricityconsumption of a household or a business. Appliance-level data can be directlyused for demand response applications and energy management systems as well asfor awareness raising and motivation for improvements in energy efficiency andreduction in the carbon footprint. Recently, classical machine learning anddeep learning (DL) techniques became very popular and proved as highlyeffective for NILM classification, but with the growing complexity thesemethods are faced with significant computational and energy demands during boththeir training and operation. In this paper, we introduce a novel DL modelaimed at enhanced multi-label classification of NILM with improved computationand energy efficiency. We also propose a testing methodology for comparison ofdifferent models using data synthesized from the measurement datasets so as tobetter represent real-world scenarios. Compared to the state-of-the-art, theproposed model has its carbon footprint reduced by more than 23% whileproviding on average approximately 8 percentage points in performanceimprovement when testing on data derived from REFIT and UK-DALE datasets.</description><author>Anže Pirnat, Blaž Bertalanič, Gregor Cerar, Mihael Mohorčič, Carolina Fortuna</author><pubDate>Tue, 18 Jul 2023 14:23:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09244v1</guid></item><item><title>Generation of High Spatial Resolution Terrestrial Surface from Low Spatial Resolution Elevation Contour Maps via Hierarchical Computation of Median Elevation Regions</title><link>http://arxiv.org/abs/2307.09239v1</link><description>We proposed a simple yet effective morphological approach to convert a sparseDigital Elevation Model (DEM) to a dense Digital Elevation Model. Theconversion is similar to that of the generation of high-resolution DEM from itslow-resolution DEM. The approach involves the generation of median contours toachieve the purpose. It is a sequential step of the I) decomposition of theexisting sparse Contour map into the maximum possible Threshold ElevationRegion (TERs). II) Computing all possible non-negative and non-weighted MedianElevation Region (MER) hierarchically between the successive TER decomposedfrom a sparse contour map. III) Computing the gradient of all TER, and MERcomputed from previous steps would yield the predicted intermediate elevationcontour at a higher spatial resolution. We presented this approach initiallywith some self-made synthetic data to show how the contour prediction works andthen experimented with the available contour map of Washington, NH to justifyits usefulness. This approach considers the geometric information of existingcontours and interpolates the elevation contour at a new spatial region of atopographic surface until no elevation contours are necessary to generate. Thisnovel approach is also very low-cost and robust as it uses elevation contours.</description><author>Geetika Barman, B. S. Daya Sagar</author><pubDate>Tue, 18 Jul 2023 14:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09239v1</guid></item></channel></rss>