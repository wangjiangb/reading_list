<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 01 Apr 2024 06:00:34 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models</title><link>http://arxiv.org/abs/2403.20331v1</link><description>This paper introduces a novel and significant challenge for Vision LanguageModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines theVLM's ability to withhold answers when faced with unsolvable problems in thecontext of Visual Question Answering (VQA) tasks. UPD encompasses threedistinct settings: Absent Answer Detection (AAD), Incompatible Answer SetDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeplyinvestigate the UPD problem, extensive experiments indicate that most VLMs,including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varyingextents, highlighting significant room for the improvements. To address UPD, weexplore both training-free and training-based solutions, offering new insightsinto their effectiveness and limitations. We hope our insights, together withfuture efforts within the proposed UPD settings, will enhance the broaderunderstanding and development of more practical and reliable VLMs.</description><author>Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, Kiyoharu Aizawa</author><pubDate>Fri, 29 Mar 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20331v1</guid></item><item><title>Are We on the Right Way for Evaluating Large Vision-Language Models?</title><link>http://arxiv.org/abs/2403.20330v1</link><description>Large vision-language models (LVLMs) have recently achieved rapid progress,sparking numerous studies to evaluate their multi-modal capabilities. However,we dig into current evaluation works and identify two primary issues: 1) Visualcontent is unnecessary for many samples. The answers can be directly inferredfrom the questions and options, or the world knowledge embedded in LLMs. Thisphenomenon is prevalent across current benchmarks. For instance, GeminiProachieves 42.9% on the MMMU benchmark without any visual input, and outperformsthe random choice baseline across six benchmarks over 20% on average. 2)Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM couldstill answer some visual-necessary questions without visual content, indicatingthe memorizing of these samples within large-scale training data. For example,Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLMbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modalgains and potentially misguide the study of LVLM. To this end, we presentMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500samples meticulously selected by humans. MMStar benchmarks 6 core capabilitiesand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities withcarefully balanced and purified samples. These samples are first roughlyselected from current benchmarks with an automated pipeline, human review isthen involved to ensure each curated sample exhibits visual dependency, minimaldata leakage, and requires advanced multi-modal capabilities. Moreover, twometrics are developed to measure data leakage and actual performance gain inmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess theirmulti-modal capabilities, and on 7 benchmarks with the proposed metrics toinvestigate their data leakage and actual multi-modal gain.</description><author>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, Feng Zhao</author><pubDate>Fri, 29 Mar 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20330v1</guid></item><item><title>SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes</title><link>http://arxiv.org/abs/2403.07726v3</link><description>This paper presents the results of the SHROOM, a shared task focused ondetecting hallucinations: outputs from natural language generation (NLG)systems that are fluent, yet inaccurate. Such cases of overgeneration put injeopardy many NLG applications, where correctness is often mission-critical.The shared task was conducted with a newly constructed dataset of 4000 modeloutputs labeled by 5 annotators each, spanning 3 NLP tasks: machinetranslation, paraphrase generation and definition modeling. The shared task was tackled by a total of 58 different users grouped in 42teams, out of which 27 elected to write a system description paper;collectively, they submitted over 300 prediction sets on both tracks of theshared task. We observe a number of key trends in how this approach was tackled-- many participants rely on a handful of model, and often rely either onsynthetic data for fine-tuning or zero-shot prompting strategies. While amajority of the teams did outperform our proposed baseline system, theperformances of top-scoring systems are still consistent with a random handlingof the more challenging items.</description><author>Timothee Mickus, Elaine Zosa, Raúl Vázquez, Teemu Vahtola, Jörg Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki</author><pubDate>Fri, 29 Mar 2024 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07726v3</guid></item><item><title>ReALM: Reference Resolution As Language Modeling</title><link>http://arxiv.org/abs/2403.20329v1</link><description>Reference resolution is an important problem, one that is essential tounderstand and successfully handle context of different kinds. This contextincludes both previous turns and context that pertains to non-conversationalentities, such as entities on the user's screen or those running in thebackground. While LLMs have been shown to be extremely powerful for a varietyof tasks, their use in reference resolution, particularly fornon-conversational entities, remains underutilized. This paper demonstrates howLLMs can be used to create an extremely effective system to resolve referencesof various types, by showing how reference resolution can be converted into alanguage modeling problem, despite involving forms of entities like those onscreen that are not traditionally conducive to being reduced to a text-onlymodality. We demonstrate large improvements over an existing system withsimilar functionality across different types of references, with our smallestmodel obtaining absolute gains of over 5% for on-screen references. We alsobenchmark against GPT-3.5 and GPT-4, with our smallest model achievingperformance comparable to that of GPT-4, and our larger models substantiallyoutperforming it.</description><author>Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu, Nidhi Rajshree</author><pubDate>Fri, 29 Mar 2024 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20329v1</guid></item><item><title>Learning Visual Quadrupedal Loco-Manipulation from Demonstrations</title><link>http://arxiv.org/abs/2403.20328v1</link><description>Quadruped robots are progressively being integrated into human environments.Despite the growing locomotion capabilities of quadrupedal robots, theirinteraction with objects in realistic scenes is still limited. While additionalrobotic arms on quadrupedal robots enable manipulating objects, they aresometimes redundant given that a quadruped robot is essentially a mobile unitequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,we aim to empower a quadruped robot to execute real-world manipulation tasksusing only its legs. We decompose the loco-manipulation process into alow-level reinforcement learning (RL)-based controller and a high-levelBehavior Cloning (BC)-based planner. By parameterizing the manipulationtrajectory, we synchronize the efforts of the upper and lower layers, therebyleveraging the advantages of both RL and BC. Our approach is validated throughsimulations and real-world experiments, demonstrating the robot's ability toperform tasks that demand mobility and high precision, such as lifting a basketfrom the ground while moving, closing a dishwasher, pressing a button, andpushing a door. Project website: https://zhengmaohe.github.io/leg-manip</description><author>Zhengmao He, Kun Lei, Yanjie Ze, Koushil Sreenath, Zhongyu Li, Huazhe Xu</author><pubDate>Fri, 29 Mar 2024 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20328v1</guid></item><item><title>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</title><link>http://arxiv.org/abs/2311.17245v5</link><description>Recent advancements in real-time neural rendering using point-basedtechniques have paved the way for the widespread adoption of 3Drepresentations. However, foundational approaches like 3D Gaussian Splattingcome with a substantial storage overhead caused by growing the SfM points tomillions, often demanding gigabyte-level disk space for a single unboundedscene, posing significant scalability challenges and hindering the splattingefficiency. To address this challenge, we introduce LightGaussian, a novel methoddesigned to transform 3D Gaussians into a more efficient and compact format.Drawing inspiration from the concept of Network Pruning, LightGaussianidentifies Gaussians that are insignificant in contributing to the scenereconstruction and adopts a pruning and recovery process, effectively reducingredundancy in Gaussian counts while preserving visual effects. Additionally,LightGaussian employs distillation and pseudo-view augmentation to distillspherical harmonics to a lower degree, allowing knowledge transfer to morecompact representations while maintaining reflectance. Furthermore, we proposea hybrid scheme, VecTree Quantization, to quantize all attributes, resulting inlower bitwidth representations with minimal accuracy losses. In summary, LightGaussian achieves an averaged compression rate over 15xwhile boosting the FPS from 139 to 215, enabling an efficient representation ofcomplex scenes on Mip-NeRF 360, Tank and Temple datasets. Project website: https://lightgaussian.github.io/</description><author>Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang</author><pubDate>Fri, 29 Mar 2024 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17245v5</guid></item><item><title>Gecko: Versatile Text Embeddings Distilled from Large Language Models</title><link>http://arxiv.org/abs/2403.20327v1</link><description>We present Gecko, a compact and versatile text embedding model. Geckoachieves strong retrieval performance by leveraging a key idea: distillingknowledge from large language models (LLMs) into a retriever. Our two-stepdistillation process begins with generating diverse, synthetic paired datausing an LLM. Next, we further refine the data quality by retrieving a set ofcandidate passages for each query, and relabeling the positive and hardnegative passages using the same LLM. The effectiveness of our approach isdemonstrated by the compactness of the Gecko. On the Massive Text EmbeddingBenchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existingentries with 768 embedding size. Gecko with 768 embedding dimensions achievesan average score of 66.31, competing with 7x larger models and 5x higherdimensional embeddings.</description><author>Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, Iftekhar Naim</author><pubDate>Fri, 29 Mar 2024 18:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20327v1</guid></item><item><title>Safe Explicable Planning</title><link>http://arxiv.org/abs/2304.03773v4</link><description>Human expectations arise from their understanding of others and the world. Inthe context of human-AI interaction, this understanding may not align withreality, leading to the AI agent failing to meet expectations and compromisingteam performance. Explicable planning, introduced as a method to bridge thisgap, aims to reconcile human expectations with the agent's optimal behavior,facilitating interpretable decision-making. However, an unresolved criticalissue is ensuring safety in explicable planning, as it could result inexplicable behaviors that are unsafe. To address this, we propose SafeExplicable Planning (SEP), which extends the prior work to support thespecification of a safety bound. The goal of SEP is to find behaviors thatalign with human expectations while adhering to the specified safety criterion.Our approach generalizes the consideration of multiple objectives stemming frommultiple models rather than a single model, yielding a Pareto set of safeexplicable policies. We present both an exact method, guaranteeing finding thePareto set, and a more efficient greedy method that finds one of the policiesin the Pareto set. Additionally, we offer approximate solutions based on stateaggregation to improve scalability. We provide formal proofs that validate thedesired theoretical properties of these methods. Evaluation through simulationsand physical robot experiments confirms the effectiveness of our approach forsafe explicable planning.</description><author>Akkamahadevi Hanni, Andrew Boateng, Yu Zhang</author><pubDate>Fri, 29 Mar 2024 18:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03773v4</guid></item><item><title>Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer</title><link>http://arxiv.org/abs/2403.20324v1</link><description>Epilepsy is one of the most common neurological disorders, and many patientsrequire surgical intervention when medication fails to control seizures. Foreffective surgical outcomes, precise localisation of the epileptogenic focus -often approximated through the Seizure Onset Zone (SOZ) - is critical yetremains a challenge. Active probing through electrical stimulation is alreadystandard clinical practice for identifying epileptogenic areas. This paperadvances the application of deep learning for SOZ localisation using SinglePulse Electrical Stimulation (SPES) responses. We achieve this by introducingTransformer models that incorporate cross-channel attention. We evaluate thesemodels on held-out patient test sets to assess their generalisability to unseenpatients and electrode placements. Our study makes three key contributions: Firstly, we implement an existingdeep learning model to compare two SPES analysis paradigms - namely, divergentand convergent. These paradigms evaluate outward and inward effectiveconnections, respectively. Our findings reveal a notable improvement in movingfrom a divergent (AUROC: 0.574) to a convergent approach (AUROC: 0.666),marking the first application of the latter in this context. Secondly, wedemonstrate the efficacy of the Transformer models in handling heterogeneouselectrode placements, increasing the AUROC to 0.730. Lastly, by incorporatinginter-trial variability, we further refine the Transformer models, with anAUROC of 0.745, yielding more consistent predictions across patients. Theseadvancements provide a deeper insight into SOZ localisation and represent asignificant step in modelling patient-specific intracranial EEG electrodeplacements in SPES. Future work will explore integrating these models intoclinical decision-making processes to bridge the gap between deep learningresearch and practical healthcare applications.</description><author>Jamie Norris, Aswin Chari, Gerald Cooray, Martin Tisdall, Karl Friston, Richard Rosch</author><pubDate>Fri, 29 Mar 2024 18:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20324v1</guid></item><item><title>What Generative Artificial Intelligence Means for Terminological Definitions</title><link>http://arxiv.org/abs/2402.16139v2</link><description>This paper examines the impact of Generative Artificial Intelligence (GenAI)tools like ChatGPT on the creation and consumption of terminologicaldefinitions. From the terminologist's point of view, the strategic use of GenAItools can streamline the process of crafting definitions, reducing both timeand effort, while potentially enhancing quality. GenAI tools enable AI-assistedterminography, notably post-editing terminography, where the machine produces adefinition that the terminologist then corrects or refines. However, thepotential of GenAI tools to fulfill all the terminological needs of a user,including term definitions, challenges the very existence of terminologicaldefinitions and resources as we know them. Unlike terminological definitions,GenAI tools can describe the knowledge activated by a term in a specificcontext. However, a main drawback of these tools is that their output cancontain errors. For this reason, users requiring reliability will likely stillresort to terminological resources for definitions. Nevertheless, with theinevitable integration of AI into terminology work, the distinction betweenhuman-created and AI-created content will become increasingly blurred.</description><author>Antonio San Martín</author><pubDate>Fri, 29 Mar 2024 18:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16139v2</guid></item><item><title>Towards a Framework for Evaluating Explanations in Automated Fact Verification</title><link>http://arxiv.org/abs/2403.20322v1</link><description>As deep neural models in NLP become more complex, and as a consequenceopaque, the necessity to interpret them becomes greater. A burgeoning interesthas emerged in rationalizing explanations to provide short and coherentjustifications for predictions. In this position paper, we advocate for aformal framework for key concepts and properties about rationalizingexplanations to support their evaluation systematically. We also outline onesuch formal framework, tailored to rationalizing explanations of increasinglycomplex structures, from free-form explanations to deductive explanations, toargumentative explanations (with the richest structure). Focusing on theautomated fact verification task, we provide illustrations of the use andusefulness of our formalization for evaluating explanations, tailored to theirvarying structures.</description><author>Neema Kotonya, Francesca Toni</author><pubDate>Fri, 29 Mar 2024 18:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20322v1</guid></item><item><title>Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space</title><link>http://arxiv.org/abs/2310.11256v2</link><description>The Gromov-Wasserstein (GW) distance is frequently used in machine learningto compare distributions across distinct metric spaces. Despite its utility, itremains computationally intensive, especially for large-scale problems.Recently, a novel Wasserstein distance specifically tailored for Gaussianmixture models and known as MW (mixture Wasserstein) has been introduced byseveral authors. In scenarios where data exhibit clustering, this approachsimplifies to a small-scale discrete optimal transport problem, whichcomplexity depends solely on the number of Gaussian components in the GMMs.This paper aims to extend MW by introducing new Gromov-type distances. Thesedistances are designed to be isometry-invariant in Euclidean spaces and areapplicable for comparing GMMs across different dimensional spaces. Our firstcontribution is the Mixture Gromov Wasserstein distance (MGW), which can beviewed as a Gromovized version of MW. This new distance has a straightforwarddiscrete formulation, making it highly efficient for estimating distancesbetween GMMs in practical applications. To facilitate the derivation of atransport plan between GMMs, we present a second distance, the EmbeddedWasserstein distance (EW). This distance turns out to be closely related toseveral recent alternatives to Gromov-Wasserstein. We show that EW can beadapted to derive a distance as well as optimal transportation plans betweenGMMs. We demonstrate the efficiency of these newly proposed distances on mediumto large-scale problems, including shape matching and hyperspectral image colortransfer.</description><author>Antoine Salmona, Julie Delon, Agnès Desolneux</author><pubDate>Fri, 29 Mar 2024 18:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11256v2</guid></item><item><title>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</title><link>http://arxiv.org/abs/2310.05737v3</link><description>While Large Language Models (LLMs) are the dominant models for generativetasks in language, they do not perform as well as diffusion models on image andvideo generation. To effectively use LLMs for visual generation, one crucialcomponent is the visual tokenizer that maps pixel-space inputs to discretetokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, avideo tokenizer designed to generate concise and expressive tokens for bothvideos and images using a common token vocabulary. Equipped with this newtokenizer, we show that LLMs outperform diffusion models on standard image andvideo generation benchmarks including ImageNet and Kinetics. In addition, wedemonstrate that our tokenizer surpasses the previously top-performing videotokenizer on two more tasks: (1) video compression comparable to thenext-generation video codec (VCC) according to human evaluations, and (2)learning effective representations for action recognition tasks.</description><author>Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang</author><pubDate>Fri, 29 Mar 2024 18:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05737v3</guid></item><item><title>MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning</title><link>http://arxiv.org/abs/2403.20320v1</link><description>Adapting models pre-trained on large-scale datasets to a variety ofdownstream tasks is a common strategy in deep learning. Consequently,parameter-efficient fine-tuning methods have emerged as a promising way toadapt pre-trained models to different tasks while training only a minimalnumber of parameters. While most of these methods are designed for single-taskadaptation, parameter-efficient training in Multi-Task Learning (MTL)architectures is still unexplored. In this paper, we introduce MTLoRA, a novelframework for parameter-efficient training of MTL models. MTLoRA employsTask-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectivelydisentangle the parameter space in MTL fine-tuning, thereby enabling the modelto adeptly handle both task specialization and interaction within MTL contexts.We applied MTLoRA to hierarchical-transformer-based MTL architectures, adaptingthem to multiple downstream dense prediction tasks. Our extensive experimentson the PASCAL dataset show that MTLoRA achieves higher accuracy on downstreamtasks compared to fully fine-tuning the MTL model while reducing the number oftrainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimaltrade-off between the number of trainable parameters and the accuracy of thedownstream tasks, outperforming current state-of-the-art parameter-efficienttraining methods in both accuracy and efficiency. Our code is publiclyavailable.</description><author>Ahmed Agiza, Marina Neseem, Sherief Reda</author><pubDate>Fri, 29 Mar 2024 18:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20320v1</guid></item><item><title>SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks</title><link>http://arxiv.org/abs/2401.15741v4</link><description>Improving the efficiency of state-of-the-art methods in semantic segmentationrequires overcoming the increasing computational cost as well as issues such asfusing semantic information from global and local contexts. Based on the recentsuccess and problems that convolutional neural networks (CNNs) encounter insemantic segmentation, this research proposes an encoder-decoder architecturewith a unique efficient residual network, Efficient-ResNet. Attention-boostinggates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming tofuse the equivariant and feature-based semantic information with the equivalentsizes of the output of global context of the efficient residual network in theencoder. Respectively, the decoder network is developed with the additionalattention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improvethe efficiency in the one-to-one conversion of the semantic information bydeploying additional convolution layers in the decoder part. Our network istested on the challenging CamVid and Cityscapes datasets, and the proposedmethods reveal significant improvements on the residual networks. To the bestof our knowledge, the developed network, SERNet-Former, achievesstate-of-the-art results (84.62 % mean IoU) on CamVid dataset and challengingresults (87.35 % mean IoU) on Cityscapes validation dataset.</description><author>Serdar Erisen</author><pubDate>Fri, 29 Mar 2024 18:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15741v4</guid></item><item><title>SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular 3D Detection of Large Objects</title><link>http://arxiv.org/abs/2403.20318v1</link><description>Monocular 3D detectors achieve remarkable performance on cars and smallerobjects. However, their performance drops on larger objects, leading to fatalaccidents. Some attribute the failures to training data scarcity or theirreceptive field requirements of large objects. In this paper, we highlight thisunderstudied problem of generalization to large objects. We find that modernfrontal detectors struggle to generalize to large objects even on nearlybalanced datasets. We argue that the cause of failure is the sensitivity ofdepth regression losses to noise of larger objects. To bridge this gap, wecomprehensively investigate regression and dice losses, examining theirrobustness under varying error levels and object sizes. We mathematically provethat the dice loss leads to superior noise-robustness and model convergence forlarge objects compared to regression losses for a simplified case. Leveragingour theoretical insights, we propose SeaBird (Segmentation in Bird's View) asthe first step towards generalizing to large objects. SeaBird effectivelyintegrates BEV segmentation on foreground objects for 3D detection, with thesegmentation head trained with the dice loss. SeaBird achieves SoTA results onthe KITTI-360 leaderboard and improves existing detectors on the nuScenesleaderboard, particularly for large objects. Code and models athttps://github.com/abhi1kumar/SeaBird</description><author>Abhinav Kumar, Yuliang Guo, Xinyu Huang, Liu Ren, Xiaoming Liu</author><pubDate>Fri, 29 Mar 2024 18:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20318v1</guid></item><item><title>Convolutional Prompting meets Language Models for Continual Learning</title><link>http://arxiv.org/abs/2403.20317v1</link><description>Continual Learning (CL) enables machine learning models to learn fromcontinuously shifting new training data in absence of data from old tasks.Recently, pretrained vision transformers combined with prompt tuning have shownpromise for overcoming catastrophic forgetting in CL. These approaches rely ona pool of learnable prompts which can be inefficient in sharing knowledgeacross tasks leading to inferior performance. In addition, the lack offine-grained layer specific prompts does not allow these to fully express thestrength of the prompts for CL. We address these limitations by proposingConvPrompt, a novel convolutional prompt creation mechanism that maintainslayer-wise shared embeddings, enabling both layer-specific learning and betterconcept transfer across tasks. The intelligent use of convolution enables us tomaintain a low parameter overhead without compromising performance. We furtherleverage Large Language Models to generate fine-grained text descriptions ofeach category which are used to get task similarity and dynamically decide thenumber of prompts to be learned. Extensive experiments demonstrate thesuperiority of ConvPrompt and improves SOTA by ~3% with significantly lessparameter overhead. We also perform strong ablation over various modules todisentangle the importance of different components.</description><author>Anurag Roy, Riddhiman Moulick, Vinay K. Verma, Saptarshi Ghosh, Abir Das</author><pubDate>Fri, 29 Mar 2024 18:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20317v1</guid></item><item><title>Learning to Count without Annotations</title><link>http://arxiv.org/abs/2307.08727v2</link><description>While recent supervised methods for reference-based object counting continueto improve the performance on benchmark datasets, they have to rely on smalldatasets due to the cost associated with manually annotating dozens of objectsin images. We propose UnCounTR, a model that can learn this task withoutrequiring any manual annotations. To this end, we construct "Self-Collages",images with various pasted objects as training samples, that provide a richlearning signal covering arbitrary object types and counts. Our method buildson existing unsupervised representations and segmentation techniques tosuccessfully demonstrate for the first time the ability of reference-basedcounting without manual supervision. Our experiments show that our method notonly outperforms simple baselines and generic models such as FasterRCNN andDETR, but also matches the performance of supervised counting models in somedomains.</description><author>Lukas Knobel, Tengda Han, Yuki M. Asano</author><pubDate>Fri, 29 Mar 2024 18:38:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08727v2</guid></item><item><title>TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning</title><link>http://arxiv.org/abs/2304.05301v2</link><description>The surge of artificial intelligence, specifically large language models, hasled to a rapid advent towards the development of large-scale machine learningtraining clusters. Collective communications within these clusters tend to beheavily bandwidth-bound, necessitating techniques to optimally utilize theavailable network bandwidth. This puts the routing algorithm for the collectiveat the forefront of determining the performance. Unfortunately, communicationlibraries used in distributed machine learning today are limited by a fixed setof routing algorithms. This constraints collective performance within thedomain of next-generation training clusters that employ intricate,heterogeneous, and asymmetric, large-scale topologies. Further, the emergenceof irregular topologies attributed to runtime phenomena such as device failuresserves to compound the complexity of the challenge. To this end, this paperintroduces TACOS, an automated synthesizer that generates topology-awarecollective algorithms for common distributed machine learning collectivesacross arbitrary input network topologies. TACOS was able to synthesizeAll-Reduce algorithm for a heterogeneous 512-NPU system in just 6.09 minuteswhile achieving performance improvement up to 4.27x over state-of-the-art priorwork. TACOS exhibits high scalability, with synthesis time scalingquadratically with the number of NPUs. In contrast to prior works' NP-hardapproaches, TACOS with 40K NPUs completes in 2.52 hours.</description><author>William Won, Midhilesh Elavazhagan, Sudarshan Srinivasan, Ajaya Durg, Samvit Kaul, Swati Gupta, Tushar Krishna</author><pubDate>Fri, 29 Mar 2024 18:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05301v2</guid></item><item><title>LipSim: A Provably Robust Perceptual Similarity Metric</title><link>http://arxiv.org/abs/2310.18274v2</link><description>Recent years have seen growing interest in developing and applying perceptualsimilarity metrics. Research has shown the superiority of perceptual metricsover pixel-wise metrics in aligning with human perception and serving as aproxy for the human visual system. On the other hand, as perceptual metricsrely on neural networks, there is a growing concern regarding their resilience,given the established vulnerability of neural networks to adversarial attacks.It is indeed logical to infer that perceptual metrics may inherit both thestrengths and shortcomings of neural networks. In this work, we demonstrate thevulnerability of state-of-the-art perceptual similarity metrics based on anensemble of ViT-based feature extractors to adversarial attacks. We thenpropose a framework to train a robust perceptual similarity metric calledLipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging1-Lipschitz neural networks as the backbone, LipSim provides guarded areasaround each data point and certificates for all perturbations within an$\ell_2$ ball. Finally, a comprehensive set of experiments shows theperformance of LipSim in terms of natural and certified scores and on the imageretrieval application. The code is available athttps://github.com/SaraGhazanfari/LipSim.</description><author>Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg</author><pubDate>Fri, 29 Mar 2024 18:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18274v2</guid></item><item><title>Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations</title><link>http://arxiv.org/abs/2403.20312v1</link><description>Existing vision-language models (VLMs) treat text descriptions as a unit,confusing individual concepts in a prompt and impairing visual semanticmatching and reasoning. An important aspect of reasoning in logic and languageis negations. This paper highlights the limitations of popular VLMs such asCLIP, at understanding the implications of negations, i.e., the effect of theword "not" in a given prompt. To enable evaluation of VLMs on fluent promptswith negations, we present CC-Neg, a dataset containing 228,246 images, truecaptions and their corresponding negated captions. Using CC-Neg along withmodifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework,has an improved understanding of negations. This training paradigm improvesCoN-CLIP's ability to encode semantics reliably, resulting in 3.85% averagegain in top-1 accuracy for zero-shot image classification across 8 datasets.Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarkssuch as SugarCREPE by 4.4%, showcasing emergent compositional understanding ofobjects, relations, and attributes in text. Overall, our work addresses acrucial limitation of VLMs by introducing a dataset and framework thatstrengthens semantic associations between images and text, demonstratingimproved large-scale foundation models with significantly reduced computationalcost, promoting efficiency and accessibility.</description><author>Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, Aparna Bharati</author><pubDate>Fri, 29 Mar 2024 18:33:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20312v1</guid></item><item><title>RNb-NeuS: Reflectance and Normal-based Multi-View 3D Reconstruction</title><link>http://arxiv.org/abs/2312.01215v2</link><description>This paper introduces a versatile paradigm for integrating multi-viewreflectance (optional) and normal maps acquired through photometric stereo. Ourapproach employs a pixel-wise joint re-parameterization of reflectance andnormal, considering them as a vector of radiances rendered under simulated,varying illumination. This re-parameterization enables the seamless integrationof reflectance and normal maps as input data in neural volume rendering-based3D reconstruction while preserving a single optimization objective. Incontrast, recent multi-view photometric stereo (MVPS) methods depend onmultiple, potentially conflicting objectives. Despite its apparent simplicity,our proposed approach outperforms state-of-the-art approaches in MVPSbenchmarks across F-score, Chamfer distance, and mean angular error metrics.Notably, it significantly improves the detailed 3D reconstruction of areas withhigh curvature or low visibility.</description><author>Baptiste Brument, Robin Bruneau, Yvain Quéau, Jean Mélou, François Bernard Lauze, Jean-Denis, Jean-Denis Durou, Lilian Calvet</author><pubDate>Fri, 29 Mar 2024 18:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01215v2</guid></item><item><title>InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds</title><link>http://arxiv.org/abs/2403.20309v1</link><description>While novel view synthesis (NVS) has made substantial progress in 3D computervision, it typically requires an initial estimation of camera intrinsics andextrinsics from dense viewpoints. This pre-processing is usually conducted viaa Structure-from-Motion (SfM) pipeline, a procedure that can be slow andunreliable, particularly in sparse-view scenarios with insufficient matchedfeatures for accurate reconstruction. In this work, we integrate the strengthsof point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) withend-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolvedissues in NVS under unconstrained settings, which encompasses pose-free andsparse view challenges. Our framework, InstantSplat, unifies dense stereopriors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &amp;pose-free images in less than 1 minute. Specifically, InstantSplat comprises aCoarse Geometric Initialization (CGI) module that swiftly establishes apreliminary scene structure and camera parameters across all training views,utilizing globally-aligned 3D point maps derived from a pre-trained densestereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)module, which jointly optimizes the 3D Gaussian attributes and the initializedposes with pose regularization. Experiments conducted on the large-scaleoutdoor Tanks &amp; Temples datasets demonstrate that InstantSplat significantlyimproves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error(ATE) by 80%. These establish InstantSplat as a viable solution for scenariosinvolving posefree and sparse-view conditions. Project page:instantsplat.github.io.</description><author>Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang</author><pubDate>Fri, 29 Mar 2024 18:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20309v1</guid></item><item><title>ChainNet: Structured Metaphor and Metonymy in WordNet</title><link>http://arxiv.org/abs/2403.20308v1</link><description>The senses of a word exhibit rich internal structure. In a typical lexicon,this structure is overlooked: a word's senses are encoded as a list withoutinter-sense relations. We present ChainNet, a lexical resource which for thefirst time explicitly identifies these structures. ChainNet expresses howsenses in the Open English Wordnet are derived from one another: every nominalsense of a word is either connected to another sense by metaphor or metonymy,or is disconnected in the case of homonymy. Because WordNet senses are linkedto resources which capture information about their meaning, ChainNet representsthe first dataset of grounded metaphor and metonymy.</description><author>Rowan Hall Maudslay, Simone Teufel, Francis Bond, James Pustejovsky</author><pubDate>Fri, 29 Mar 2024 18:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20308v1</guid></item><item><title>Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference</title><link>http://arxiv.org/abs/2403.20306v1</link><description>With the ubiquitous use of modern large language models (LLMs) acrossindustries, the inference serving for these models is ever expanding. Given thehigh compute and memory requirements of modern LLMs, more and moretop-of-the-line GPUs are being deployed to serve these models. Energyavailability has come to the forefront as the biggest challenge for data centerexpansion to serve these models. In this paper, we present the trade-offsbrought up by making energy efficiency the primary goal of LLM serving underperformance SLOs. We show that depending on the inputs, the model, and theservice-level agreements, there are several knobs available to the LLMinference provider to use for being energy efficient. We characterize theimpact of these knobs on the latency, throughput, as well as the energy. Byexploring these trade-offs, we offer valuable insights into optimizing energyusage without compromising on performance, thereby paving the way forsustainable and cost-effective LLM deployment in data center environments.</description><author>Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, Josep Torrellas</author><pubDate>Fri, 29 Mar 2024 18:22:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20306v1</guid></item><item><title>Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes</title><link>http://arxiv.org/abs/2403.19432v2</link><description>Data accuracy is essential for scientific research and policy development.The National Violent Death Reporting System (NVDRS) data is widely used fordiscovering the patterns and causes of death. Recent studies suggested theannotation inconsistencies within the NVDRS and the potential impact onerroneous suicide-cause attributions. We present an empirical Natural LanguageProcessing (NLP) approach to detect annotation inconsistencies and adopt across-validation-like paradigm to identify problematic instances. We analyzed267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Ourresults showed that incorporating the target state's data into training thesuicide-crisis classifier brought an increase of 5.4% to the F-1 score on thetarget state's test set and a decrease of 1.1% on other states' test set. Toconclude, we demonstrated the annotation inconsistencies in NVDRS's deathinvestigation notes, identified problematic instances, evaluated theeffectiveness of correcting problematic instances, and eventually proposed anNLP improvement solution.</description><author>Song Wang, Yiliang Zhou, Ziqiang Han, Cui Tao, Yunyu Xiao, Ying Ding, Joydeep Ghosh, Yifan Peng</author><pubDate>Fri, 29 Mar 2024 18:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19432v2</guid></item><item><title>Improving Learnt Local MAPF Policies with Heuristic Search</title><link>http://arxiv.org/abs/2403.20300v1</link><description>Multi-agent path finding (MAPF) is the problem of finding collision-freepaths for a team of agents to reach their goal locations. State-of-the-artclassical MAPF solvers typically employ heuristic search to find solutions forhundreds of agents but are typically centralized and can struggle to scale whenrun with short timeouts. Machine learning (ML) approaches that learn policiesfor each agent are appealing as these could enable decentralized systems andscale well while maintaining good solution quality. Current ML approaches toMAPF have proposed methods that have started to scratch the surface of thispotential. However, state-of-the-art ML approaches produce "local" policiesthat only plan for a single timestep and have poor success rates andscalability. Our main idea is that we can improve a ML local policy by usingheuristic search methods on the output probability distribution to resolvedeadlocks and enable full horizon planning. We show several model-agnostic waysto use heuristic search with learnt policies that significantly improve thepolicies' success rates and scalability. To our best knowledge, we demonstratethe first time ML-based MAPF approaches have scaled to high congestionscenarios (e.g. 20% agent density).</description><author>Rishi Veerapaneni, Qian Wang, Kevin Ren, Arthur Jakobsson, Jiaoyang Li, Maxim Likhachev</author><pubDate>Fri, 29 Mar 2024 18:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20300v1</guid></item><item><title>Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement</title><link>http://arxiv.org/abs/2403.20298v1</link><description>The issue of data sparsity poses a significant challenge to recommendersystems. In response to this, algorithms that leverage side information such asreview texts have been proposed. Furthermore, Cross-Domain Recommendation(CDR), which captures domain-shareable knowledge and transfers it from a richerdomain (source) to a sparser one (target), has received notable attention.Nevertheless, the majority of existing methodologies assume a Euclideanembedding space, encountering difficulties in accurately representing richertext information and managing complex interactions between users and items.This paper advocates a hyperbolic CDR approach based on review texts formodeling user-item relationships. We first emphasize that conventionaldistance-based domain alignment techniques may cause problems because smallmodifications in hyperbolic geometry result in magnified perturbations,ultimately leading to the collapse of hierarchical structures. To address thischallenge, we propose hierarchy-aware embedding and domain alignment schemesthat adjust the scale to extract domain-shareable information withoutdisrupting structural forms. The process involves the initial embedding ofreview texts in hyperbolic space, followed by feature extraction incorporatingdegree-based normalization and structure alignment. We conducted extensiveexperiments to substantiate the efficiency, robustness, and scalability of ourproposed model in comparison to state-of-the-art baselines.</description><author>Yoonhyuk Choi</author><pubDate>Fri, 29 Mar 2024 18:15:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20298v1</guid></item><item><title>A Strong Baseline for Point Cloud Registration via Direct Superpoints Matching</title><link>http://arxiv.org/abs/2307.01362v4</link><description>Deep neural networks endow the downsampled superpoints with highlydiscriminative feature representations. Previous dominant point cloudregistration approaches match these feature representations as the first step,e.g., using the Sinkhorn algorithm. A RANSAC-like method is then usuallyadopted as a post-processing refinement to filter the outliers. Other dominantmethod is to directly predict the superpoint matchings using learned MLPlayers. Both of them have drawbacks: RANSAC-based methods are computationallyintensive and prediction-based methods suffer from outputing non-existingpoints in the point cloud. In this paper, we propose a straightforward andeffective baseline to find correspondences of superpoints in a global matchingmanner. We employ the normalized matching scores as weights for eachcorrespondence, allowing us to reject the outliers and further weigh the restinliers when fitting the transformation matrix without relying on thecumbersome RANSAC. Moreover, the entire model can be trained in an end-to-endfashion, leading to better accuracy. Our simple yet effective baseline showscomparable or even better results than state-of-the-art methods on threedatasets including ModelNet, 3DMatch, and KITTI. We do not advocate ourapproach to be \emph{the} solution for point cloud registration but use theresults to emphasize the role of matching strategy for point cloudregistration. The code and models are available athttps://github.com/neu-vi/Superpoints_Registration.</description><author>Aniket Gupta, Yiming Xie, Hanumant Singh, Huaizu Jiang</author><pubDate>Fri, 29 Mar 2024 18:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01362v4</guid></item><item><title>Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values</title><link>http://arxiv.org/abs/2311.04855v2</link><description>Non-negative matrix factorization (NMF) is a dimensionality reductiontechnique that has shown promise for analyzing noisy data, especiallyastronomical data. For these datasets, the observed data may contain negativevalues due to noise even when the true underlying physical signal is strictlypositive. Prior NMF work has not treated negative data in a statisticallyconsistent manner, which becomes problematic for low signal-to-noise data withmany negative values. In this paper we present two algorithms, Shift-NMF andNearly-NMF, that can handle both the noisiness of the input data and also anyintroduced negativity. Both of these algorithms use the negative data spacewithout clipping, and correctly recover non-negative signals without anyintroduced positive offset that occurs when clipping negative data. Wedemonstrate this numerically on both simple and more realistic examples, andprove that both algorithms have monotonically decreasing update rules.</description><author>Dylan Green, Stephen Bailey</author><pubDate>Fri, 29 Mar 2024 18:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04855v2</guid></item><item><title>Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation</title><link>http://arxiv.org/abs/2403.20289v1</link><description>Emotion Recognition in Conversation (ERC) involves detecting the underlyingemotion behind each utterance within a conversation. Effectively generatingrepresentations for utterances remains a significant challenge in this task.Recent works propose various models to address this issue, but they stillstruggle with differentiating similar emotions such as excitement andhappiness. To alleviate this problem, We propose an Emotion-AnchoredContrastive Learning (EACL) framework that can generate more distinguishableutterance representations for similar emotions. To achieve this, we utilizelabel encodings as anchors to guide the learning of utterance representationsand design an auxiliary loss to ensure the effective separation of anchors forsimilar emotions. Moreover, an additional adaptation process is proposed toadapt anchors to serve as effective classifiers to improve classificationperformance. Across extensive experiments, our proposed EACL achievesstate-of-the-art emotion recognition performance and exhibits superiorperformance on similar emotions. Our code is available athttps://github.com/Yu-Fangxu/EACL.</description><author>Fangxu Yu, Junjie Guo, Zhen Wu, Xinyu Dai</author><pubDate>Fri, 29 Mar 2024 18:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20289v1</guid></item><item><title>Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain</title><link>http://arxiv.org/abs/2403.20288v1</link><description>We explore the potential of Large Language Models (LLMs) to assist andpotentially correct physicians in medical decision-making tasks. We evaluateseveral LLMs, including Meditron, Llama2, and Mistral, to analyze the abilityof these models to interact effectively with physicians across differentscenarios. We consider questions from PubMedQA and several tasks, ranging frombinary (yes/no) responses to long answer generation, where the answer of themodel is produced after an interaction with a physician. Our findings suggestthat prompt design significantly influences the downstream accuracy of LLMs andthat LLMs can provide valuable feedback to physicians, challenging incorrectdiagnoses and contributing to more accurate decision-making. For example, whenthe physician is accurate 38% of the time, Mistral can produce the correctanswer, improving accuracy up to 74% depending on the prompt being used, whileLlama2 and Meditron models exhibit greater sensitivity to prompt choice. Ouranalysis also uncovers the challenges of ensuring that LLM-generatedsuggestions are pertinent and useful, emphasizing the need for further researchin this area.</description><author>Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini</author><pubDate>Fri, 29 Mar 2024 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20288v1</guid></item><item><title>Benchmarking Counterfactual Image Generation</title><link>http://arxiv.org/abs/2403.20287v1</link><description>Counterfactual image generation is pivotal for understanding the causalrelations of variables, with applications in interpretability and generation ofunbiased synthetic data. However, evaluating image generation is along-standing challenge in itself. The need to evaluate counterfactualgeneration compounds on this challenge, precisely because counterfactuals, bydefinition, are hypothetical scenarios without observable ground truths. Inthis paper, we present a novel comprehensive framework aimed at benchmarkingcounterfactual image generation methods. We incorporate metrics that focus onevaluating diverse aspects of counterfactuals, such as composition,effectiveness, minimality of interventions, and image realism. We assess theperformance of three distinct conditional image generation model types, basedon the Structural Causal Model paradigm. Our work is accompanied by auser-friendly Python package which allows to further evaluate and benchmarkexisting and future counterfactual image generation methods. Our framework isextendable to additional SCM and other causal methods, generative models, anddatasets.</description><author>Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Giorgos Papanastasiou, Sotirios A. Tsaftaris</author><pubDate>Fri, 29 Mar 2024 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20287v1</guid></item><item><title>VicTR: Video-conditioned Text Representations for Activity Recognition</title><link>http://arxiv.org/abs/2304.02560v2</link><description>Vision-Language models (VLMs) have excelled in the image-domain -- especiallyin zero-shot settings -- thanks to the availability of vast pretraining data(i.e., paired image-text samples). However for videos, such paired data is notas abundant. Therefore, video-VLMs are usually designed by adapting pretrainedimage-VLMs to the video-domain, instead of training from scratch. All suchrecipes rely on augmenting visual embeddings with temporal information (i.e.,image $\rightarrow$ video), often keeping text embeddings unchanged or evenbeing discarded. In this paper, we argue the contrary, that better video-VLMscan be designed by focusing more on augmenting text, rather than visualinformation. More specifically, we introduce Video-conditioned TextRepresentations (VicTR): a form of text embeddings optimized w.r.t. visualembeddings, creating a more-flexible contrastive latent space. Our model canfurther make use of freely-available semantic information, in the form ofvisually-grounded auxiliary text (e.g. object or scene information). Weevaluate our model on few-shot, zero-shot (HMDB-51, UCF-101), short-form(Kinetics-400) and long-form (Charades) activity recognition benchmarks,showing strong performance among video-VLMs.</description><author>Kumara Kahatapitiya, Anurag Arnab, Arsha Nagrani, Michael S. Ryoo</author><pubDate>Fri, 29 Mar 2024 17:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02560v2</guid></item><item><title>Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices</title><link>http://arxiv.org/abs/2403.15905v4</link><description>The personalization of machine learning (ML) models to address data drift isa significant challenge in the context of Internet of Things (IoT)applications. Presently, most approaches focus on fine-tuning either the fullbase model or its last few layers to adapt to new data, while often neglectingenergy costs. However, various types of data drift exist, and fine-tuning thefull base model or the last few layers may not result in optimal performance incertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energyadaptive personalization framework designed for resource-constrained devices.We categorize data drift and personalization into three types: input-level,feature-level, and output-level. For each type, we fine-tune different blocksof the model to achieve optimal performance with reduced energy costs.Specifically, input-, feature-, and output-level correspond to fine-tuning thefront, middle, and rear blocks of the model. We evaluate TBFT on a ResNetmodel, three datasets, three different training sizes, and a Raspberry Pi.Compared with the $Block Avg$, where each block is fine-tuned individually andtheir performance improvements are averaged, TBFT exhibits an improvement inmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumptionon average compared with full fine-tuning.</description><author>Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Haddadi</author><pubDate>Fri, 29 Mar 2024 17:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15905v4</guid></item><item><title>LayerNorm: A key component in parameter-efficient fine-tuning</title><link>http://arxiv.org/abs/2403.20284v1</link><description>Fine-tuning a pre-trained model, such as Bidirectional EncoderRepresentations from Transformers (BERT), has been proven to be an effectivemethod for solving many natural language processing (NLP) tasks. However, dueto the large number of parameters in many state-of-the-art NLP models,including BERT, the process of fine-tuning is computationally expensive. Oneattractive solution to this issue is parameter-efficient fine-tuning, whichinvolves modifying only a minimal segment of the model while keeping theremainder unchanged. Yet, it remains unclear which segment of the BERT model iscrucial for fine-tuning. In this paper, we first analyze different componentsin the BERT model to pinpoint which one undergoes the most significant changesafter fine-tuning. We find that output LayerNorm changes more than any othercomponents when fine-tuned for different General Language UnderstandingEvaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm canreach comparable, or in some cases better, performance to full fine-tuning andother parameter-efficient fine-tuning methods. Moreover, we use Fisherinformation to determine the most critical subset of LayerNorm and demonstratethat many NLP tasks in the GLUE benchmark can be solved by fine-tuning only asmall portion of LayerNorm with negligible performance degradation.</description><author>Taha ValizadehAslani, Hualou Liang</author><pubDate>Fri, 29 Mar 2024 17:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20284v1</guid></item><item><title>GlitchBench: Can large multimodal models detect video game glitches?</title><link>http://arxiv.org/abs/2312.05291v2</link><description>Large multimodal models (LMMs) have evolved from large language models (LLMs)to integrate multiple input modalities, such as visual inputs. This integrationaugments the capacity of LLMs for tasks requiring visual comprehension andreasoning. However, the extent and limitations of their enhanced abilities arenot fully understood, especially when it comes to real-world tasks. To addressthis gap, we introduce GlitchBench, a novel benchmark derived from video gamequality assurance tasks, to test and evaluate the reasoning capabilities ofLMMs. Our benchmark is curated from a variety of unusual and glitched scenariosfrom video games and aims to challenge both the visual and linguistic reasoningpowers of LMMs in detecting and interpreting out-of-the-ordinary events. Weevaluate multiple state-of-the-art LMMs, and we show that GlitchBench presentsa new challenge for these models. Code and data are available at:https://glitchbench.github.io/</description><author>Mohammad Reza Taesiri, Tianjun Feng, Anh Nguyen, Cor-Paul Bezemer</author><pubDate>Fri, 29 Mar 2024 17:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05291v2</guid></item><item><title>Sparse multimodal fusion with modal channel attention</title><link>http://arxiv.org/abs/2403.20280v1</link><description>The ability of masked multimodal transformer architectures to learn a robustembedding space when modality samples are sparsely aligned is studied bymeasuring the quality of generated embedding spaces as a function of modalsparsity. An extension to the masked multimodal transformer model is proposedwhich incorporates modal-incomplete channels in the multihead attentionmechanism called modal channel attention (MCA). Two datasets with 4 modalitiesare used, CMU-MOSEI for multimodal sentiment recognition and TCGA formultiomics. Models are shown to learn uniform and aligned embedding spaces withonly two out of four modalities in most samples. It was found that, even withno modal sparsity, the proposed MCA mechanism improves the quality of generatedembedding spaces, recall metrics, and subsequent performance on downstreamtasks.</description><author>Josiah Bjorgaard</author><pubDate>Fri, 29 Mar 2024 17:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20280v1</guid></item><item><title>LUQ: Long-text Uncertainty Quantification for LLMs</title><link>http://arxiv.org/abs/2403.20279v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capability in avariety of NLP tasks. Despite their effectiveness, these models are prone togenerate nonfactual content. Uncertainty Quantification (UQ) is pivotal inenhancing our understanding of a model's confidence in its generated content,thereby aiding in the mitigation of nonfactual outputs. Existing research on UQpredominantly targets short text generation, typically yielding brief,word-limited responses. However, real-world applications frequently necessitatemuch longer responses. Our study first highlights the limitations of current UQmethods in handling long text generation. We then introduce \textsc{Luq}, anovel sampling-based UQ approach specifically designed for long text. Ourfindings reveal that \textsc{Luq} outperforms existing baseline methods incorrelating with the model's factuality scores (negative coefficient of -0.85observed for Gemini Pro). With \textsc{Luq} as the tool for UQ, we investigatebehavior patterns of several popular LLMs' response confidence spectrum and howthat interplays with the response' factuality. We identify that LLMs lackconfidence in generating long text for rare facts and a factually strong model(i.e. GPT-4) tends to reject questions it is not sure about. To further improvethe factual accuracy of LLM responses, we propose a method called\textsc{Luq-Ensemble} that ensembles responses from multiple models and selectsthe response with the least uncertainty. The ensembling method greatly improvesthe response factuality upon the best standalone LLM.</description><author>Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier</author><pubDate>Fri, 29 Mar 2024 17:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20279v1</guid></item><item><title>Sāmayik: A Benchmark and Dataset for English-Sanskrit Translation</title><link>http://arxiv.org/abs/2305.14004v2</link><description>We release S\={a}mayik, a dataset of around 53,000 parallel English-Sanskritsentences, written in contemporary prose. Sanskrit is a classical languagestill in sustenance and has a rich documented heritage. However, due to thelimited availability of digitized content, it still remains a low-resourcelanguage. Existing Sanskrit corpora, whether monolingual or bilingual, havepredominantly focused on poetry and offer limited coverage of contemporarywritten materials. S\={a}mayik is curated from a diverse range of domains,including language instruction material, textual teaching pedagogy, and onlinetutorials, among others. It stands out as a unique resource that specificallycaters to the contemporary usage of Sanskrit, with a primary emphasis on prosewriting. Translation models trained on our dataset demonstrate statisticallysignificant improvements when translating out-of-domain contemporary corpora,outperforming models trained on older classical-era poetry datasets. Finally,we also release benchmark models by adapting four multilingual pre-trainedmodels, three of them have not been previously exposed to Sanskrit fortranslating between English and Sanskrit while one of them is multi-lingualpre-trained translation model including English and Sanskrit. The dataset andsource code is present at https://github.com/ayushbits/saamayik.</description><author>Ayush Maheshwari, Ashim Gupta, Amrith Krishna, Atul Kumar Singh, Ganesh Ramakrishnan, G. Anil Kumar, Jitin Singla</author><pubDate>Fri, 29 Mar 2024 17:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14004v2</guid></item><item><title>Rapid Motor Adaptation for Robotic Manipulator Arms</title><link>http://arxiv.org/abs/2312.04670v2</link><description>Developing generalizable manipulation skills is a core challenge in embodiedAI. This includes generalization across diverse task configurations,encompassing variations in object shape, density, friction coefficient, andexternal disturbances such as forces applied to the robot. Rapid MotorAdaptation (RMA) offers a promising solution to this challenge. It posits thatessential hidden variables influencing an agent's task performance, such asobject mass and shape, can be effectively inferred from the agent's action andproprioceptive history. Drawing inspiration from RMA in locomotion and in-handrotation, we use depth perception to develop agents tailored for rapid motoradaptation in a variety of manipulation tasks. We evaluated our agents on fourchallenging tasks from the Maniskill2 benchmark, namely pick-and-placeoperations with hundreds of objects from the YCB and EGAD datasets, peginsertion with precise position and orientation, and operating a variety offaucets and handles, with customized environment variations. Empirical resultsdemonstrate that our agents surpass state-of-the-art methods like automaticdomain randomization and vision-based policies, obtaining better generalizationperformance and sample efficiency.</description><author>Yichao Liang, Kevin Ellis, João Henriques</author><pubDate>Fri, 29 Mar 2024 17:39:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04670v2</guid></item><item><title>FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding</title><link>http://arxiv.org/abs/2312.02214v2</link><description>We propose FlashAvatar, a novel and lightweight 3D animatable avatarrepresentation that could reconstruct a digital avatar from a short monocularvideo sequence in minutes and render high-fidelity photo-realistic images at300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3DGaussian field embedded in the surface of a parametric face model and learnextra spatial offset to model non-surface regions and subtle facial details.While full use of geometric priors can capture high-frequency facial detailsand preserve exaggerated expressions, proper initialization can help reduce thenumber of Gaussians, thus enabling super-fast rendering speed. Extensiveexperimental results demonstrate that FlashAvatar outperforms existing worksregarding visual quality and personalized details and is almost an order ofmagnitude faster in rendering speed. Project page:https://ustc3dv.github.io/FlashAvatar/</description><author>Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang</author><pubDate>Fri, 29 Mar 2024 17:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02214v2</guid></item><item><title>Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces</title><link>http://arxiv.org/abs/2403.20275v1</link><description>Touch and vision go hand in hand, mutually enhancing our ability tounderstand the world. From a research perspective, the problem of mixing touchand vision is underexplored and presents interesting challenges. To this end,we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data(local depth maps) with multi-view vision data to achieve surfacereconstruction and novel view synthesis. Our method optimises 3D Gaussianprimitives to accurately model the object's geometry at points of contact. Bycreating a framework that decreases the transmittance at touch locations, weachieve a refined surface reconstruction, ensuring a uniformly smooth depthmap. Touch is particularly useful when considering non-Lambertian objects (e.g.shiny or reflective surfaces) since contemporary methods tend to fail toreconstruct with fidelity specular highlights. By combining vision and tactilesensing, we achieve more accurate geometry reconstructions with fewer imagesthan prior methods. We conduct evaluation on objects with glossy and reflectivesurfaces and demonstrate the effectiveness of our approach, offeringsignificant improvements in reconstruction quality.</description><author>Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</author><pubDate>Fri, 29 Mar 2024 17:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20275v1</guid></item><item><title>CATSNet: a context-aware network for Height Estimation in a Forested Area based on Pol-TomoSAR data</title><link>http://arxiv.org/abs/2403.20273v1</link><description>Tropical forests are a key component of the global carbon cycle. With plansfor upcoming space-borne missions like BIOMASS to monitor forestry, severalairborne missions, including TropiSAR and AfriSAR campaigns, have beensuccessfully launched and experimented. Typical Synthetic Aperture RadarTomography (TomoSAR) methods involve complex models with low accuracy and highcomputation costs. In recent years, deep learning methods have also gainedattention in the TomoSAR framework, showing interesting performance. Recently,a solution based on a fully connected Tomographic Neural Network (TSNN) hasdemonstrated its effectiveness in accurately estimating forest and groundheights by exploiting the pixel-wise elements of the covariance matrix derivedfrom TomoSAR data. This work instead goes beyond the pixel-wise approach todefine a context-aware deep learning-based solution named CATSNet. Aconvolutional neural network is considered to leverage patch-based informationand extract features from a neighborhood rather than focus on a single pixel.The training is conducted by considering TomoSAR data as the input and LightDetection and Ranging (LiDAR) values as the ground truth. The experimentalresults show striking advantages in both performance and generalization abilityby leveraging context information within Multiple Baselines (MB) TomoSAR dataacross different polarimetric modalities, surpassing existing techniques.</description><author>Wenyu Yang, Sergio Vitale, Hossein Aghababaei, Giampaolo Ferraioli, Vito Pascazio, Gilda Schirinzi</author><pubDate>Fri, 29 Mar 2024 17:27:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20273v1</guid></item><item><title>Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</title><link>http://arxiv.org/abs/2403.20271v1</link><description>The interaction between humans and artificial intelligence (AI) is a crucialfactor that reflects the effectiveness of multimodal large language models(MLLMs). However, current MLLMs primarily focus on image-level comprehensionand limit interaction to textual instructions, thereby constraining theirflexibility in usage and depth of response. In this paper, we introduce theDraw-and-Understand project: a new model, a multi-domain dataset, and achallenging benchmark for visual prompting. Specifically, we propose SPHINX-V,a new end-to-end trained Multimodal Large Language Model (MLLM) that connects avision encoder, a visual prompt encoder and an LLM for various visual prompts(points, bounding boxes, and free-form shape) and language understanding. Toadvance visual prompting research for MLLMs, we introduce MDVP-Data andMDVP-Bench. MDVP-Data features a multi-domain dataset containing 1.6M uniqueimage-visual prompt-text instruction-following samples, including naturalimages, document images, OCR images, mobile screenshots, web screenshots, andmulti-panel images. Furthermore, we present MDVP-Bench, a comprehensive andchallenging benchmark to assess a model's capability in understanding visualprompting instructions. Our experiments demonstrate SPHINX-V's impressivemultimodal interaction capabilities through visual prompting, revealingsignificant improvements in detailed pixel-level description andquestion-answering abilities.</description><author>Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, Hongsheng Li</author><pubDate>Fri, 29 Mar 2024 17:26:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20271v1</guid></item><item><title>Latxa: An Open Language Model and Evaluation Suite for Basque</title><link>http://arxiv.org/abs/2403.20266v1</link><description>We introduce Latxa, a family of large language models for Basque ranging from7 to 70 billion parameters. Latxa is based on Llama 2, which we continuepretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.Addressing the scarcity of high-quality benchmarks for Basque, we furtherintroduce 4 multiple choice evaluation datasets: EusProficiency, comprising5,169 questions from official language proficiency exams; EusReading,comprising 352 reading comprehension questions; EusTrivia, comprising 1,715trivia questions from 5 knowledge areas; and EusExams, comprising 16,774questions from public examinations. In our extensive evaluation, Latxaoutperforms all previous open models we compare to by a large margin. Inaddition, it is competitive with GPT-4 Turbo in language proficiency andunderstanding, despite lagging behind in reading comprehension andknowledge-intensive tasks. Both the Latxa family of models, as well as our newpretraining corpora and evaluation datasets, are publicly available under openlicenses at https://github.com/hitz-zentroa/latxa. Our suite enablesreproducible research on methods to build LLMs for low-resource languages.</description><author>Julen Etxaniz, Oscar Sainz, Naiara Perez, Itziar Aldabe, German Rigau, Eneko Agirre, Aitor Ormazabal, Mikel Artetxe, Aitor Soroa</author><pubDate>Fri, 29 Mar 2024 17:16:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20266v1</guid></item><item><title>Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability</title><link>http://arxiv.org/abs/2403.16970v2</link><description>As deep learning has become the state-of-the-art for computer-assisteddiagnosis, interpretability of the automatic decisions is crucial for clinicaldeployment. While various methods were proposed in this domain, visualattention maps of clinicians during radiological screening offer a unique assetto provide important insights and can potentially enhance the quality ofcomputer-assisted diagnosis. With this paper, we introduce a noveldeep-learning framework for joint disease diagnosis and prediction ofcorresponding visual saliency maps for chest X-ray scans. Specifically, wedesigned a novel dual-encoder multi-task UNet, which leverages both aDenseNet201 backbone and a Residual and Squeeze-and-Excitation block-basedencoder to extract diverse features for saliency map prediction, and amulti-scale feature-fusion classifier to perform disease classification. Totackle the issue of asynchronous training schedules of individual tasks inmulti-task learning, we proposed a multi-stage cooperative learning strategy,with contrastive learning for feature encoder pretraining to boost performance.Experiments show that our proposed method outperformed existing techniques forchest X-ray diagnosis and the quality of visual saliency map prediction.</description><author>Zirui Qiu, Hassan Rivaz, Yiming Xiao</author><pubDate>Fri, 29 Mar 2024 17:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16970v2</guid></item><item><title>ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models</title><link>http://arxiv.org/abs/2403.20262v1</link><description>Research on Large Language Models (LLMs) has recently witnessed an increasinginterest in extending models' context size to better capture dependencieswithin long documents. While benchmarks have been proposed to assess long-rangeabilities, existing efforts primarily considered generic tasks that are notnecessarily aligned with real-world applications. In contrast, our workproposes a new benchmark for long-context LLMs focused on a practical meetingassistant scenario. In this scenario, the long contexts consist of transcriptsobtained by automatic speech recognition, presenting unique challenges for LLMsdue to the inherent noisiness and oral nature of such data. Our benchmark,named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271manually crafted questions and their ground-truth answers. Our experiments withrecent long-context LLMs on ELITR-Bench highlight a gap between open-source andproprietary models, especially when questions are asked sequentially within aconversation. We also provide a thorough analysis of our GPT-4-based evaluationmethod, encompassing insights from a crowdsourcing study. Our findings suggestthat while GPT-4's evaluation scores are correlated with human judges', itsability to differentiate among more than three score levels may be limited.</description><author>Thibaut Thonet, Jos Rozen, Laurent Besacier</author><pubDate>Fri, 29 Mar 2024 17:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20262v1</guid></item><item><title>EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation</title><link>http://arxiv.org/abs/2403.01482v3</link><description>Semantic segmentation has innately relied on extensive pixel-level annotateddata, leading to the emergence of unsupervised methodologies. Among them,leveraging self-supervised Vision Transformers for unsupervised semanticsegmentation (USS) has been making steady progress with expressive deepfeatures. Yet, for semantically segmenting images with complex objects, apredominant challenge remains: the lack of explicit object-level semanticencoding in patch-level features. This technical limitation often leads toinadequate segmentation of complex objects with diverse structures. To addressthis gap, we present a novel approach, EAGLE, which emphasizes object-centricrepresentation learning for unsupervised semantic segmentation. Specifically,we introduce EiCue, a spectral technique providing semantic and structural cuesthrough an eigenbasis derived from the semantic similarity matrix of deep imagefeatures and color affinity from an image. Further, by incorporating ourobject-centric contrastive loss with EiCue, we guide our model to learnobject-level representations with intra- and inter-image object-featureconsistency, thereby enhancing semantic accuracy. Extensive experiments onCOCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-artUSS results of EAGLE with accurate and consistent semantic segmentation acrosscomplex scenes.</description><author>Chanyoung Kim, Woojung Han, Dayun Ju, Seong Jae Hwang</author><pubDate>Fri, 29 Mar 2024 17:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01482v3</guid></item><item><title>FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation</title><link>http://arxiv.org/abs/2403.20261v1</link><description>Molecular docking is a pivotal process in drug discovery. While traditionaltechniques rely on extensive sampling and simulation governed by physicalprinciples, these methods are often slow and costly. The advent of deeplearning-based approaches has shown significant promise, offering increases inboth accuracy and efficiency. Building upon the foundational work of FABind, amodel designed with a focus on speed and accuracy, we present FABind+, anenhanced iteration that largely boosts the performance of its predecessor. Weidentify pocket prediction as a critical bottleneck in molecular docking andpropose a novel methodology that significantly refines pocket prediction,thereby streamlining the docking process. Furthermore, we introducemodifications to the docking module to enhance its pose generationcapabilities. In an effort to bridge the gap with conventionalsampling/generative methods, we incorporate a simple yet effective samplingtechnique coupled with a confidence model, requiring only minor adjustments tothe regression framework of FABind. Experimental results and analysis revealthat FABind+ remarkably outperforms the original FABind, achieves competitivestate-of-the-art performance, and delivers insightful modeling strategies. Thisdemonstrates FABind+ represents a substantial step forward in molecular dockingand drug discovery. Our code is in https://github.com/QizhiPei/FABind.</description><author>Kaiyuan Gao, Qizhi Pei, Jinhua Zhu, Tao Qin, Kun He, Tie-Yan Liu, Lijun Wu</author><pubDate>Fri, 29 Mar 2024 17:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20261v1</guid></item><item><title>Prototype-based Interpretable Breast Cancer Prediction Models: Analysis and Challenges</title><link>http://arxiv.org/abs/2403.20260v1</link><description>Deep learning models have achieved high performance in medical applications,however, their adoption in clinical practice is hindered due to their black-boxnature. Self-explainable models, like prototype-based models, can be especiallybeneficial as they are interpretable by design. However, if the learntprototypes are of low quality then the prototype-based models are as good asblack-box. Having high quality prototypes is a pre-requisite for a trulyinterpretable model. In this work, we propose a prototype evaluation frameworkfor coherence (PEF-C) for quantitatively evaluating the quality of theprototypes based on domain knowledge. We show the use of PEF-C in the contextof breast cancer prediction using mammography. Existing works onprototype-based models on breast cancer prediction using mammography havefocused on improving the classification performance of prototype-based modelscompared to black-box models and have evaluated prototype quality throughanecdotal evidence. We are the first to go beyond anecdotal evidence andevaluate the quality of the mammography prototypes systematically using ourPEF-C. Specifically, we apply three state-of-the-art prototype-based models,ProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancerprediction and evaluate these models w.r.t. i) classification performance, andii) quality of the prototypes, on three public datasets. Our results show thatprototype-based models are competitive with black-box models in terms ofclassification performance, and achieve a higher score in detecting ROIs.However, the quality of the prototypes are not yet sufficient and can beimproved in aspects of relevance, purity and learning a variety of prototypes.We call the XAI community to systematically evaluate the quality of theprototypes to check their true usability in high stake decisions and improvesuch models further.</description><author>Shreyasi Pathak, Jörg Schlötterer, Jeroen Veltman, Jeroen Geerdink, Maurice van Keulen, Christin Seifert</author><pubDate>Fri, 29 Mar 2024 17:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20260v1</guid></item><item><title>Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions</title><link>http://arxiv.org/abs/2403.20254v1</link><description>Temporal action detection (TAD) aims to locate action positions and recognizeaction categories in long-term untrimmed videos. Although many methods haveachieved promising results, their robustness has not been thoroughly studied.In practice, we observe that temporal information in videos can be occasionallycorrupted, such as missing or blurred frames. Interestingly, existing methodsoften incur a significant performance drop even if only one frame is affected.To formally evaluate the robustness, we establish two temporal corruptionrobustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper,we extensively analyze the robustness of seven leading TAD methods and obtainsome interesting findings: 1) Existing methods are particularly vulnerable totemporal corruptions, and end-to-end methods are often more susceptible thanthose with a pre-trained feature extractor; 2) Vulnerability mainly comes fromlocalization error rather than classification error; 3) When corruptions occurin the middle of an action instance, TAD models tend to yield the largestperformance drop. Besides building a benchmark, we further develop a simple buteffective robust training method to defend against temporal corruptions,through the FrameDrop augmentation and Temporal-Robust Consistency loss.Remarkably, our approach not only improves robustness but also yields promisingimprovements on clean data. We believe that this study will serve as abenchmark for future research in robust video analysis. Source code and modelsare available at https://github.com/Alvin-Zeng/temporal-robustness-benchmark.</description><author>Runhao Zeng, Xiaoyong Chen, Jiaming Liang, Huisi Wu, Guangzhong Cao, Yong Guo</author><pubDate>Fri, 29 Mar 2024 17:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20254v1</guid></item><item><title>MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation</title><link>http://arxiv.org/abs/2403.20253v1</link><description>Medical image segmentation of anatomical structures and pathology is crucialin modern clinical diagnosis, disease study, and treatment planning. To date,great progress has been made in deep learning-based segmentation techniques,but most methods still lack data efficiency, generalizability, andinteractability. Consequently, the development of new, precise segmentationmethods that demand fewer labeled datasets is of utmost importance in medicalimage analysis. Recently, the emergence of foundation models, such as CLIP andSegment-Anything-Model (SAM), with comprehensive cross-domain representationopened the door for interactive and universal image segmentation. However,exploration of these models for data-efficient medical image segmentation isstill limited, but is highly necessary. In this paper, we propose a novelframework, called MedCLIP-SAM that combines CLIP and SAM models to generatesegmentation of clinical scans using text prompts in both zero-shot and weaklysupervised settings. To achieve this, we employed a new Decoupled Hard NegativeNoise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP modeland the recent gScoreCAM to generate prompts to obtain segmentation masks fromSAM in a zero-shot setting. Additionally, we explored the use of zero-shotsegmentation labels in a weakly supervised paradigm to improve the segmentationquality further. By extensively testing three diverse segmentation tasks andmedical image modalities (breast tumor ultrasound, brain tumor MRI, and lungX-ray), our proposed framework has demonstrated excellent accuracy.</description><author>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</author><pubDate>Fri, 29 Mar 2024 16:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20253v1</guid></item><item><title>Using LLMs to Model the Beliefs and Preferences of Targeted Populations</title><link>http://arxiv.org/abs/2403.20252v1</link><description>We consider the problem of aligning a large language model (LLM) to model thepreferences of a human population. Modeling the beliefs, preferences, andbehaviors of a specific population can be useful for a variety of differentapplications, such as conducting simulated focus groups for new products,conducting virtual surveys, and testing behavioral interventions, especiallyfor interventions that are expensive, impractical, or unethical. Existing workhas had mixed success using LLMs to accurately model human behavior indifferent contexts. We benchmark and evaluate two well-known fine-tuningapproaches and evaluate the resulting populations on their ability to match thepreferences of real human respondents on a survey of preferences for batteryelectric vehicles (BEVs). We evaluate our models against their ability to matchpopulation-wide statistics as well as their ability to match individualresponses, and we investigate the role of temperature in controlling thetrade-offs between these two. Additionally, we propose and evaluate a novelloss term to improve model performance on responses that require a numericresponse.</description><author>Keiichi Namikoshi, Alex Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Arechiga</author><pubDate>Fri, 29 Mar 2024 16:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20252v1</guid></item><item><title>Latent Embedding Clustering for Occlusion Robust Head Pose Estimation</title><link>http://arxiv.org/abs/2403.20251v1</link><description>Head pose estimation has become a crucial area of research in computer visiongiven its usefulness in a wide range of applications, including robotics,surveillance, or driver attention monitoring. One of the most difficultchallenges in this field is managing head occlusions that frequently take placein real-world scenarios. In this paper, we propose a novel and efficientframework that is robust in real world head occlusion scenarios. In particular,we propose an unsupervised latent embedding clustering with regression andclassification components for each pose angle. The model optimizes latentfeature representations for occluded and non-occluded images through aclustering term while improving fine-grained angle predictions. Experimentalevaluation on in-the-wild head pose benchmark datasets reveal competitiveperformance in comparison to state-of-the-art methodologies with the advantageof having a significant data reduction. We observe a substantial improvement inoccluded head pose estimation. Also, an ablation study is conducted toascertain the impact of the clustering term within our proposed framework.</description><author>José Celestino, Manuel Marques, Jacinto C. Nascimento</author><pubDate>Fri, 29 Mar 2024 16:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20251v1</guid></item><item><title>Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning</title><link>http://arxiv.org/abs/2311.13612v2</link><description>Over the past year, a large body of multimodal research has emerged aroundzero-shot evaluation using GPT descriptors. These studies boost the zero-shotaccuracy of pretrained VL models with an ensemble of label-specific textgenerated by GPT. A recent study, WaffleCLIP, demonstrated that similarzero-shot accuracy can be achieved with an ensemble of random descriptors.However, both zero-shot methods are un-trainable and consequently sub-optimalwhen some few-shot out-of-distribution (OOD) training data is available.Inspired by these prior works, we present two more flexible methods calleddescriptor and word soups, which do not require an LLM at test time and canleverage training data to increase OOD target accuracy. Descriptor soupgreedily selects a small set of textual descriptors using generic few-shottraining data, then calculates robust class embeddings using the selecteddescriptors. Word soup greedily assembles a chain of words in a similar manner.Compared to existing few-shot soft prompt tuning methods, word soup requiresfewer parameters by construction and less GPU memory, since it does not requirebackpropagation. Both soups outperform current published few-shot methods, evenwhen combined with SoTA zero-shot methods, on cross-dataset and domaingeneralization benchmarks. Compared with SoTA prompt and descriptor ensemblingmethods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracywith fewer ensemble members. Please checkout our code:github.com/Chris210634/word_soups</description><author>Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis</author><pubDate>Fri, 29 Mar 2024 16:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13612v2</guid></item><item><title>Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures</title><link>http://arxiv.org/abs/2403.20250v1</link><description>This paper deals with optimal policy learning (OPL) with observational data,i.e. data-driven optimal decision-making, in multi-action (or multi-arm)settings, where a finite set of decision options is available. It is organizedin three parts, where I discuss respectively: estimation, risk preference, andpotential failures. The first part provides a brief review of the keyapproaches to estimating the reward (or value) function and optimal policywithin this context of analysis. Here, I delineate the identificationassumptions and statistical properties related to offline optimal policylearning estimators. In the second part, I delve into the analysis of decisionrisk. This analysis reveals that the optimal choice can be influenced by thedecision maker's attitude towards risks, specifically in terms of the trade-offbetween reward conditional mean and conditional variance. Here, I present anapplication of the proposed model to real data, illustrating that the averageregret of a policy with multi-valued treatment is contingent on thedecision-maker's attitude towards risk. The third part of the paper discussesthe limitations of optimal data-driven decision-making by highlightingconditions under which decision-making can falter. This aspect is linked to thefailure of the two fundamental assumptions essential for identifying theoptimal choice: (i) overlapping, and (ii) unconfoundedness. Some conclusionsend the paper.</description><author>Giovanni Cerulli</author><pubDate>Fri, 29 Mar 2024 16:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20250v1</guid></item><item><title>Relation Rectification in Diffusion Model</title><link>http://arxiv.org/abs/2403.20249v1</link><description>Despite their exceptional generative abilities, large text-to-image diffusionmodels, much like skilled but careless artists, often struggle with accuratelydepicting visual relationships between objects. This issue, as we uncoverthrough careful analysis, arises from a misaligned text encoder that strugglesto interpret specific relationships and differentiate the logical order ofassociated objects. To resolve this, we introduce a novel task termed RelationRectification, aiming to refine the model to accurately represent a givenrelationship it initially fails to generate. To address this, we propose aninnovative solution utilizing a Heterogeneous Graph Convolutional Network(HGCN). It models the directional relationships between relation terms andcorresponding objects within the input prompts. Specifically, we optimize theHGCN on a pair of prompts with identical relational words but reversed objectorders, supplemented by a few reference images. The lightweight HGCN adjuststhe text embeddings generated by the text encoder, ensuring the accuratereflection of the textual relation in the embedding space. Crucially, ourmethod retains the parameters of the text encoder and diffusion model,preserving the model's robust performance on unrelated descriptions. Wevalidated our approach on a newly curated dataset of diverse relational data,demonstrating both quantitative and qualitative enhancements in generatingimages with precise visual relations. Project page:https://wuyinwei-hah.github.io/rrnet.github.io/.</description><author>Yinwei Wu, Xingyi Yang, Xinchao Wang</author><pubDate>Fri, 29 Mar 2024 16:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20249v1</guid></item><item><title>Energy Efficient Deep Multi-Label ON/OFF Classification of Low Frequency Metered Home Appliances</title><link>http://arxiv.org/abs/2307.09244v2</link><description>Non-intrusive load monitoring (NILM) is the process of obtainingappliance-level data from a single metering point, measuring total electricityconsumption of a household or a business. Appliance-level data can be directlyused for demand response applications and energy management systems as well asfor awareness raising and motivation for improvements in energy efficiency.Recently, classical machine learning and deep learning (DL) techniques becamevery popular and proved as highly effective for NILM classification, but withthe growing complexity these methods are faced with significant computationaland energy demands during both their training and operation. In this paper, weintroduce a novel DL model aimed at enhanced multi-label classification of NILMwith improved computation and energy efficiency. We also propose an evaluationmethodology for comparison of different models using data synthesized from themeasurement datasets so as to better represent real-world scenarios. Comparedto the state-of-the-art, the proposed model has its energy consumption reducedby more than 23% while providing on average approximately 8 percentage pointsin performance improvement when evaluating on data derived from REFIT andUK-DALE datasets. We also show a 12 percentage point performance advantage ofthe proposed DL based model over a random forest model and observe performancedegradation with the increase of the number of devices in the household, namelywith each additional 5 devices, the average performance degrades byapproximately 7 percentage points.</description><author>Anže Pirnat, Blaž Bertalanič, Gregor Cerar, Mihael Mohorčič, Carolina Fortuna</author><pubDate>Fri, 29 Mar 2024 16:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09244v2</guid></item><item><title>Enhancing Dimension-Reduced Scatter Plots with Class and Feature Centroids</title><link>http://arxiv.org/abs/2403.20246v1</link><description>Dimension reduction is increasingly applied to high-dimensional biomedicaldata to improve its interpretability. When datasets are reduced to twodimensions, each observation is assigned an x and y coordinates and isrepresented as a point on a scatter plot. A significant challenge lies ininterpreting the meaning of the x and y axes due to the complexities inherentin dimension reduction. This study addresses this challenge by using the x andy coordinates derived from dimension reduction to calculate class and featurecentroids, which can be overlaid onto the scatter plots. This method connectsthe low-dimension space to the original high-dimensional space. We illustratethe utility of this approach with data derived from the phenotypes of threeneurogenetic diseases and demonstrate how the addition of class and featurecentroids increases the interpretability of scatter plots.</description><author>Daniel B. Hier, Tayo Obafemi-Ajayi, Gayla R. Olbricht, Devin M. Burns, Sasha Petrenko, Donald C. Wunsch II</author><pubDate>Fri, 29 Mar 2024 16:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20246v1</guid></item><item><title>LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos</title><link>http://arxiv.org/abs/2312.05269v2</link><description>In this paper we introduce LifelongMemory, a new framework for accessinglong-form egocentric videographic memory through natural language questionanswering and retrieval. LifelongMemory generates concise video activitydescriptions of the camera wearer and leverages the zero-shot capabilities ofpretrained large language models to perform reasoning over long-form videocontext. Furthermore, Lifelong Memory uses a confidence and explanation moduleto produce confident, high-quality, and interpretable answers. Our approachachieves state-of-the-art performance on the EgoSchema benchmark for questionanswering and is highly competitive on the natural language query (NLQ)challenge of Ego4D. Code is available athttps://github.com/Agentic-Learning-AI-Lab/lifelong-memory.</description><author>Ying Wang, Yanlai Yang, Mengye Ren</author><pubDate>Fri, 29 Mar 2024 16:44:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05269v2</guid></item><item><title>DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset</title><link>http://arxiv.org/abs/2212.04119v2</link><description>As sharing images in an instant message is a crucial factor, there has beenactive research on learning an image-text multi-modal dialogue models. However,training a well-generalized multi-modal dialogue model remains challenging dueto the low quality and limited diversity of images per dialogue in existingmulti-modal dialogue datasets. In this paper, we propose an automated pipelineto construct a multi-modal dialogue dataset, ensuring both dialogue quality andimage diversity without requiring minimum human effort. In our pipeline, toguarantee the coherence between images and dialogue, we prompt GPT-4 to inferpotential image-sharing moments - specifically, the utterance, speaker,rationale, and image description. Furthermore, we leverage CLIP similarity tomaintain consistency between aligned multiple images to the utterance. Throughthis pipeline, we introduce DialogCC, a high-quality and diverse multi-modaldialogue dataset that surpasses existing datasets in terms of quality anddiversity in human evaluation. Our comprehensive experiments highlight thatwhen multi-modal dialogue models are trained using our dataset, theirgeneralization performance on unseen dialogue datasets is significantlyenhanced. We make our source code and dataset publicly available.</description><author>Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Jonghwan Hyeon, Ho-Jin Choi</author><pubDate>Fri, 29 Mar 2024 16:27:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04119v2</guid></item><item><title>Long-Tailed Anomaly Detection with Learnable Class Names</title><link>http://arxiv.org/abs/2403.20236v1</link><description>Anomaly detection (AD) aims to identify defective images and localize theirdefects (if any). Ideally, AD models should be able to detect defects over manyimage classes; without relying on hard-coded class names that can beuninformative or inconsistent across datasets; learn without anomalysupervision; and be robust to the long-tailed distributions of real-worldapplications. To address these challenges, we formulate the problem oflong-tailed AD by introducing several datasets with different levels of classimbalance and metrics for performance evaluation. We then propose a novelmethod, LTAD, to detect defects from multiple and long-tailed classes, withoutrelying on dataset class names. LTAD combines AD by reconstruction and semanticAD modules. AD by reconstruction is implemented with a transformer-basedreconstruction module. Semantic AD is implemented with a binary classifier,which relies on learned pseudo class names and a pretrained foundation model.These modules are learned over two phases. Phase 1 learns the pseudo-classnames and a variational autoencoder (VAE) for feature synthesis that augmentsthe training data to combat long-tails. Phase 2 then learns the parameters ofthe reconstruction and classification modules of LTAD. Extensive experimentsusing the proposed long-tailed datasets show that LTAD substantiallyoutperforms the state-of-the-art methods for most forms of dataset imbalance.The long-tailed dataset split is available athttps://zenodo.org/records/10854201 .</description><author>Chih-Hui Ho, Kuan-Chuan Peng, Nuno Vasconcelos</author><pubDate>Fri, 29 Mar 2024 16:26:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20236v1</guid></item><item><title>Artificial Neural Networks-based Real-time Classification of ENG Signals for Implanted Nerve Interfaces</title><link>http://arxiv.org/abs/2403.20234v1</link><description>Neuropathies are gaining higher relevance in clinical settings, as they riskpermanently jeopardizing a person's life. To support the recovery of patients,the use of fully implanted devices is emerging as one of the most promisingsolutions. However, these devices, even if becoming an integral part of a fullycomplex neural nanonetwork system, pose numerous challenges. In this article,we address one of them, which consists of the classification of motor/sensorystimuli. The task is performed by exploring four different types of artificialneural networks (ANNs) to extract various sensory stimuli from theelectroneurographic (ENG) signal measured in the sciatic nerve of rats.Different sizes of the data sets are considered to analyze the feasibility ofthe investigated ANNs for real-time classification through a comparison oftheir performance in terms of accuracy, F1-score, and prediction time. Thedesign of the ANNs takes advantage of the modelling of the ENG signal as amultiple-input multiple-output (MIMO) system to describe the measures taken bystate-of-the-art implanted nerve interfaces. These are based on the use ofmulti-contact cuff electrodes to achieve nanoscale spatial discrimination ofthe nerve activity. The MIMO ENG signal model is another contribution of thispaper. Our results show that some ANNs are more suitable for real-timeapplications, being capable of achieving accuracies over $90\%$ for signalwindows of $100$ and $200\,$ms with a low enough processing time to beeffective for pathology recovery.</description><author>ntonio Coviello, Francesco Linsalata, Umberto Spagnolini, Maurizio Magarini</author><pubDate>Fri, 29 Mar 2024 16:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20234v1</guid></item><item><title>Functional Bilevel Optimization for Machine Learning</title><link>http://arxiv.org/abs/2403.20233v1</link><description>In this paper, we introduce a new functional point of view on bileveloptimization problems for machine learning, where the inner objective isminimized over a function space. These types of problems are most often solvedby using methods developed in the parametric setting, where the inner objectiveis strongly convex with respect to the parameters of the prediction function.The functional point of view does not rely on this assumption and notablyallows using over-parameterized neural networks as the inner predictionfunction. We propose scalable and efficient algorithms for the functionalbilevel optimization problem and illustrate the benefits of our approach oninstrumental regression and reinforcement learning tasks, which admit naturalfunctional bilevel structures.</description><author>Ieva Petrulionyte, Julien Mairal, Michael Arbel</author><pubDate>Fri, 29 Mar 2024 16:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20233v1</guid></item><item><title>U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation</title><link>http://arxiv.org/abs/2403.20231v1</link><description>Concept personalization methods enable large text-to-image models to learnspecific subjects (e.g., objects/poses/3D models) and synthesize renditions innew contexts. Given that the image references are highly biased towards visualattributes, state-of-the-art personalization models tend to overfit the wholesubject and cannot disentangle visual characteristics in pixel space. In thisstudy, we proposed a more challenging setting, namely fine-grained visualappearance personalization. Different from existing methods, we allow users toprovide a sentence describing the desired attributes. A novel decoupledself-augmentation strategy is proposed to generate target-related andnon-target samples to learn user-specified visual attributes. These augmenteddata allow for refining the model's understanding of the target attribute whilemitigating the impact of unrelated attributes. At the inference stage,adjustments are conducted on semantic space through the learned target andnon-target embeddings to further enhance the disentanglement of targetattributes. Extensive experiments on various kinds of visual attributes withSOTA personalization methods show the ability of the proposed method to mimictarget visual appearance in novel contexts, thus improving the controllabilityand flexibility of personalization.</description><author>You Wu, Kean Liu, Xiaoyue Mi, Fan Tang, Juan Cao, Jintao Li</author><pubDate>Fri, 29 Mar 2024 16:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20231v1</guid></item><item><title>An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT</title><link>http://arxiv.org/abs/2403.20230v1</link><description>Vision Transformers (ViTs) have achieved significant success in computervision. However, their intensive computations and massive memory footprintchallenge ViTs' deployment on embedded devices, calling for efficient ViTs.Among them, EfficientViT, the state-of-the-art one, features aConvolution-Transformer hybrid architecture, enhancing both accuracy andhardware efficiency. Unfortunately, existing accelerators cannot fully exploitthe hardware benefits of EfficientViT due to its unique architecture. In thispaper, we propose an FPGA-based accelerator for EfficientViT to advance thehardware efficiency frontier of ViTs. Specifically, we design a reconfigurablearchitecture to efficiently support various operation types, includinglightweight convolutions and attention, boosting hardware utilization.Additionally, we present a time-multiplexed and pipelined dataflow tofacilitate both intra- and inter-layer fusions, reducing off-chip data accesscosts. Experimental results show that our accelerator achieves up to 780.2 GOPSin throughput and 105.1 GOPS/W in energy efficiency at 200MHz on the XilinxZCU102 FPGA, which significantly outperforms prior works.</description><author>Haikuo Shao, Huihong Shi, Wendong Mao, Zhongfeng Wang</author><pubDate>Fri, 29 Mar 2024 16:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20230v1</guid></item><item><title>3DInAction: Understanding Human Actions in 3D Point Clouds</title><link>http://arxiv.org/abs/2303.06346v2</link><description>We propose a novel method for 3D point cloud action recognition.Understanding human actions in RGB videos has been widely studied in recentyears, however, its 3D point cloud counterpart remains under-explored. This ismostly due to the inherent limitation of the point cloud data modality -- lackof structure, permutation invariance, and varying number of points -- whichmakes it difficult to learn a spatio-temporal representation. To address thislimitation, we propose the 3DinAction pipeline that first estimates patchesmoving in time (t-patches) as a key building block, alongside a hierarchicalarchitecture that learns an informative spatio-temporal representation. We showthat our method achieves improved performance on existing datasets, includingDFAUST and IKEA ASM. Code is publicly available athttps://github.com/sitzikbs/3dincaction.</description><author>Yizhak Ben-Shabat, Oren Shrout, Stephen Gould</author><pubDate>Fri, 29 Mar 2024 16:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06346v2</guid></item><item><title>MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark</title><link>http://arxiv.org/abs/2403.20225v1</link><description>Multi-target multi-camera tracking is a crucial task that involvesidentifying and tracking individuals over time using video streams frommultiple cameras. This task has practical applications in various fields, suchas visual surveillance, crowd behavior analysis, and anomaly detection.However, due to the difficulty and cost of collecting and labeling data,existing datasets for this task are either synthetically generated orartificially constructed within a controlled camera network setting, whichlimits their ability to model real-world dynamics and generalize to diversecamera configurations. To address this issue, we present MTMMC, a real-world,large-scale dataset that includes long video sequences captured by 16multi-modal cameras in two different environments - campus and factory - acrossvarious time, weather, and season conditions. This dataset provides achallenging test-bed for studying multi-camera tracking under diversereal-world complexities and includes an additional input modality of spatiallyaligned and temporally synchronized RGB and thermal cameras, which enhances theaccuracy of multi-camera tracking. MTMMC is a super-set of existing datasets,benefiting independent fields such as person detection, re-identification, andmultiple object tracking. We provide baselines and new learning setups on thisdataset and set the reference scores for future studies. The datasets, models,and test server will be made publicly available.</description><author>Sanghyun Woo, Kwanyong Park, Inkyu Shin, Myungchul Kim, In So Kweon</author><pubDate>Fri, 29 Mar 2024 16:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20225v1</guid></item><item><title>Shallow Cross-Encoders for Low-Latency Retrieval</title><link>http://arxiv.org/abs/2403.20222v1</link><description>Transformer-based Cross-Encoders achieve state-of-the-art effectiveness intext retrieval. However, Cross-Encoders based on large transformer models (suchas BERT or T5) are computationally expensive and allow for scoring only a smallnumber of documents within a reasonably small latency window. However, keepingsearch latencies low is important for user satisfaction and energy usage. Inthis paper, we show that weaker shallow transformer models (i.e., transformerswith a limited number of layers) actually perform better than full-scale modelswhen constrained to these practical low-latency settings since they canestimate the relevance of more documents in the same time budget. We furthershow that shallow transformers may benefit from the generalized BinaryCross-Entropy (gBCE) training scheme, which has recently demonstrated successfor recommendation tasks. Our experiments with TREC Deep Learning passageranking query sets demonstrate significant improvements in shallow andfull-scale models in low-latency scenarios. For example, when the latency limitis 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERTmodel) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, whileTinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reachesNDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallowCross-Encoders are effective even when used without a GPU (e.g., with CPUinference, NDCG@10 decreases only by 3% compared to GPU inference with 50mslatency), which makes Cross-Encoders practical to run even without specializedhardware acceleration.</description><author>Aleksandr V. Petrov, Sean MacAvaney, Craig Macdonald</author><pubDate>Fri, 29 Mar 2024 16:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20222v1</guid></item><item><title>Graph Neural Aggregation-diffusion with Metastability</title><link>http://arxiv.org/abs/2403.20221v1</link><description>Continuous graph neural models based on differential equations have expandedthe architecture of graph neural networks (GNNs). Due to the connection betweengraph diffusion and message passing, diffusion-based models have been widelystudied. However, diffusion naturally drives the system towards an equilibriumstate, leading to issues like over-smoothing. To this end, we propose GRADEinspired by graph aggregation-diffusion equations, which includes the delicatebalance between nonlinear diffusion and aggregation induced by interactionpotentials. The node representations obtained through aggregation-diffusionequations exhibit metastability, indicating that features can aggregate intomultiple clusters. In addition, the dynamics within these clusters can persistfor long time periods, offering the potential to alleviate over-smoothingeffects. This nonlinear diffusion in our model generalizes existingdiffusion-based models and establishes a connection with classical GNNs. Weprove that GRADE achieves competitive performance across various benchmarks andalleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichletenergy.</description><author>Kaiyuan Cui, Xinyan Wang, Zicheng Zhang, Weichen Zhao</author><pubDate>Fri, 29 Mar 2024 16:05:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20221v1</guid></item><item><title>A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models</title><link>http://arxiv.org/abs/2402.11676v2</link><description>Counter narratives - informed responses to hate speech contexts designed torefute hateful claims and de-escalate encounters - have emerged as an effectivehate speech intervention strategy. While previous work has proposed automaticcounter narrative generation methods to aid manual interventions, theevaluation of these approaches remains underdeveloped. Previous automaticmetrics for counter narrative evaluation lack alignment with human judgment asthey rely on superficial reference comparisons instead of incorporating keyaspects of counter narrative quality as evaluation criteria. To address priorevaluation limitations, we propose a novel evaluation framework prompting LLMsto provide scores and feedback for generated counter narrative candidates using5 defined aspects derived from guidelines from counter narrative specializedNGOs. We found that LLM evaluators achieve strong alignment to human-annotatedscores and feedback and outperform alternative metrics, indicating theirpotential as multi-aspect, reference-free and interpretable evaluators forcounter narrative evaluation.</description><author>Jaylen Jones, Lingbo Mo, Eric Fosler-Lussier, Huan Sun</author><pubDate>Fri, 29 Mar 2024 16:01:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11676v2</guid></item><item><title>DragVideo: Interactive Drag-style Video Editing</title><link>http://arxiv.org/abs/2312.02216v2</link><description>Video generation models have shown their superior ability to generatephoto-realistic video. However, how to accurately control (or edit) the videoremains a formidable challenge. The main issues are: 1) how to perform directand accurate user control in editing; 2) how to execute editings like changingshape, expression, and layout without unsightly distortion and artifacts to theedited content; and 3) how to maintain spatio-temporal consistency of videoafter editing. To address the above issues, we propose DragVideo, a generaldrag-style video editing framework. Inspired by DragGAN, DragVideo addressesissues 1) and 2) by proposing the drag-style video latent optimization methodwhich gives desired control by updating noisy video latent according to draginstructions through video-level drag objective function. We amend issue 3) byintegrating the video diffusion model with sample-specific LoRA and MutualSelf-Attention in DragVideo to ensure the edited result is spatio-temporallyconsistent. We also present a series of testing examples for drag-style videoediting and conduct extensive experiments across a wide array of challengingediting tasks, such as motion, skeleton editing, etc, underscoring DragVideocan edit video in an intuitive, faithful to the user's intention manner, withnearly unnoticeable distortion and artifacts, while maintaining spatio-temporalconsistency. While traditional prompt-based video editing fails to do theformer two and directly applying image drag editing fails in the last,DragVideo's versatility and generality are emphasized. Github link:https://github.com/RickySkywalker/DragVideo-Official.</description><author>Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, Chi-Keung Tang</author><pubDate>Fri, 29 Mar 2024 15:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02216v2</guid></item><item><title>CroissantLLM: A Truly Bilingual French-English Language Model</title><link>http://arxiv.org/abs/2402.00786v4</link><description>We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3TEnglish and French tokens, to bring to the research and industrial community ahigh-performance, fully open-sourced bilingual model that runs swiftly onconsumer-grade local hardware. To that end, we pioneer the approach of trainingan intrinsically bilingual model with a 1:1 English-to-French pretraining dataratio, a custom tokenizer, and bilingual finetuning datasets. We release thetraining dataset, notably containing a French split with manually curated,high-quality, and varied data sources. To assess performance outside ofEnglish, we craft a novel benchmark, FrenchBench, consisting of an array ofclassification and generation tasks, covering various orthogonal aspects ofmodel performance in the French Language. Additionally, rooted in transparencyand to foster further Large Language Model research, we release codebases, anddozens of checkpoints across various model sizes, training data distributions,and training steps, as well as fine-tuned Chat models, and strong translationmodels. We evaluate our model through the FMTI framework, and validate 81 % ofthe transparency criteria, far beyond the scores of even most open initiatives.This work enriches the NLP landscape, breaking away from previousEnglish-centric work in order to strengthen our understanding ofmultilinguality in language models.</description><author>Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, António Loison, Duarte M. Alves, Caio Corro, Nicolas Boizard, João Alves, Ricardo Rei, Pedro H. Martins, Antoni Bigata Casademunt, François Yvon, André F. T. Martins, Gautier Viaud, Céline Hudelot, Pierre Colombo</author><pubDate>Fri, 29 Mar 2024 15:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00786v4</guid></item><item><title>Self-learning Canonical Space for Multi-view 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2403.12440v2</link><description>Multi-view 3D human pose estimation is naturally superior to single view one,benefiting from more comprehensive information provided by images of multipleviews. The information includes camera poses, 2D/3D human poses, and 3Dgeometry. However, the accurate annotation of these information is hard toobtain, making it challenging to predict accurate 3D human pose from multi-viewimages. To deal with this issue, we propose a fully self-supervised framework,named cascaded multi-view aggregating network (CMANet), to construct acanonical parameter space to holistically integrate and exploit multi-viewinformation. In our framework, the multi-view information is grouped into twocategories: 1) intra-view information , 2) inter-view information. Accordingly,CMANet consists of two components: intra-view module (IRV) and inter-viewmodule (IEV). IRV is used for extracting initial camera pose and 3D human poseof each view; IEV is to fuse complementary pose information and cross-view 3Dgeometry for a final 3D human pose. To facilitate the aggregation of the intra-and inter-view, we define a canonical parameter space, depicted by per-viewcamera pose and human pose and shape parameters ($\theta$ and $\beta$) of SMPLmodel, and propose a two-stage learning procedure. At first stage, IRV learnsto estimate camera pose and view-dependent 3D human pose supervised byconfident output of an off-the-shelf 2D keypoint detector. At second stage, IRVis frozen and IEV further refines the camera pose and optimizes the 3D humanpose by implicitly encoding the cross-view complement and 3D geometryconstraint, achieved by jointly fitting predicted multi-view 2D keypoints. Theproposed framework, modules, and learning strategy are demonstrated to beeffective by comprehensive experiments and CMANet is superior tostate-of-the-art methods in extensive quantitative and qualitative analysis.</description><author>Xiaoben Li, Mancheng Meng, Ziyan Wu, Terrence Chen, Fan Yang, Dinggang Shen</author><pubDate>Fri, 29 Mar 2024 15:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12440v2</guid></item><item><title>Distributed agency in second language learning and teaching through generative AI</title><link>http://arxiv.org/abs/2403.20216v1</link><description>Generative AI offers significant opportunities for language learning. Toolslike ChatGPT can provide informal second language practice through chats inwritten or voice forms, with the learner specifying through promptsconversational parameters such as proficiency level, language register, anddiscussion topics. AI can be instructed to give corrective feedback, createpractice exercises, or develop an extended study plan. Instructors can use AIto build learning and assessment materials in a variety of media. AI is likelyto make immersive technologies more powerful and versatile, moving away fromscripted interactions. For both learners and teachers, it is important tounderstand the limitations of AI systems that arise from their purelystatistical model of human language, which limits their ability to deal withnuanced social and cultural aspects of language use. Additionally, there areethical concerns over how AI systems are created as well as practicalconstraints in their use, especially for less privileged populations. The powerand versatility of AI tools are likely to turn them into valuable and constantcompanions in many peoples lives (akin to smartphones), creating a closeconnection that goes beyond simple tool use. Ecological theories such associomaterialism are helpful in examining the shared agency that developsthrough close user-AI interactions, as are the perspectives on human-objectrelations from Indigenous cultures.</description><author>Robert Godwin-Jones</author><pubDate>Fri, 29 Mar 2024 15:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20216v1</guid></item><item><title>Advancing the Arabic WordNet: Elevating Content Quality</title><link>http://arxiv.org/abs/2403.20215v1</link><description>High-quality WordNets are crucial for achieving high-quality results in NLPapplications that rely on such resources. However, the wordnets of mostlanguages suffer from serious issues of correctness and completeness withrespect to the words and word meanings they define, such as incorrect lemmas,missing glosses and example sentences, or an inadequate, Western-centricrepresentation of the morphology and the semantics of the language. Previousefforts have largely focused on increasing lexical coverage while ignoringother qualitative aspects. In this paper, we focus on the Arabic language andintroduce a major revision of the Arabic WordNet that addresses multipledimensions of lexico-semantic resource quality. As a result, we updated morethan 58% of the synsets of the existing Arabic WordNet by adding missinginformation and correcting errors. In order to address issues of languagediversity and untranslatability, we also extended the wordnet structure by newelements: phrasets and lexical gaps.</description><author>Abed Alhakim Freihat, Hadi Khalilia, Gábor Bella, Fausto Giunchiglia</author><pubDate>Fri, 29 Mar 2024 15:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20215v1</guid></item><item><title>H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model</title><link>http://arxiv.org/abs/2403.20213v1</link><description>The generic large Vision-Language Models (VLMs) is rapidly developing, butstill perform poorly in Remote Sensing (RS) domain, which is due to the uniqueand specialized nature of RS imagery and the comparatively limited spatialperception of current VLMs. Existing Remote Sensing specific Vision LanguageModels (RSVLMs) still have considerable potential for improvement, primarilyowing to the lack of large-scale, high-quality RS vision-language datasets. Weconstructed HqDC-1.4M, the large scale High quality and Detailed Captions forRS images, containing 1.4 million image-caption pairs, which not only enhancethe RSVLM's understanding of RS images but also significantly improve themodel's spatial perception abilities, such as localization and counting,thereby increasing the helpfulness of the RSVLM. Moreover, to address theinevitable "hallucination" problem in RSVLM, we developed RSSA, the firstdataset aimed at enhancing the Self-Awareness capability of RSVLMs. Byincorporating a variety of unanswerable questions into typical RS visualquestion-answering tasks, RSSA effectively improves the truthfulness andreduces the hallucinations of the model's outputs, thereby enhancing thehonesty of the RSVLM. Based on these datasets, we proposed the H2RSVLM, theHelpful and Honest Remote Sensing Vision Language Model. H2RSVLM has achievedoutstanding performance on multiple RS public datasets and is capable ofrecognizing and refusing to answer the unanswerable questions, effectivelymitigating the incorrect generations. We will release the code, data and modelweights at https://github.com/opendatalab/H2RSVLM .</description><author>Chao Pang, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Xingxing Weng, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He</author><pubDate>Fri, 29 Mar 2024 15:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20213v1</guid></item><item><title>Rethinking Multi-view Representation Learning via Distilled Disentangling</title><link>http://arxiv.org/abs/2403.10897v2</link><description>Multi-view representation learning aims to derive robust representations thatare both view-consistent and view-specific from diverse data sources. Thispaper presents an in-depth analysis of existing approaches in this domain,highlighting a commonly overlooked aspect: the redundancy betweenview-consistent and view-specific representations. To this end, we propose aninnovative framework for multi-view representation learning, which incorporatesa technique we term 'distilled disentangling'. Our method introduces theconcept of masked cross-view prediction, enabling the extraction of compact,high-quality view-consistent representations from various sources withoutincurring extra computational overhead. Additionally, we develop a distilleddisentangling module that efficiently filters out consistency-relatedinformation from multi-view representations, resulting in purer view-specificrepresentations. This approach significantly reduces redundancy betweenview-consistent and view-specific representations, enhancing the overallefficiency of the learning process. Our empirical evaluations reveal thathigher mask ratios substantially improve the quality of view-consistentrepresentations. Moreover, we find that reducing the dimensionality ofview-consistent representations relative to that of view-specificrepresentations further refines the quality of the combined representations.Our code is accessible at: https://github.com/Guanzhou-Ke/MRDD.</description><author>Guanzhou Ke, Bo Wang, Xiaoli Wang, Shengfeng He</author><pubDate>Fri, 29 Mar 2024 15:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10897v2</guid></item><item><title>You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs</title><link>http://arxiv.org/abs/2403.12931v2</link><description>We introduce YOSO, a novel generative model designed for rapid, scalable, andhigh-fidelity one-step image synthesis. This is achieved by integrating thediffusion process with GANs. Specifically, we smooth the distribution by thedenoising generator itself, performing self-cooperative learning. We show thatour method can serve as a one-step generation model training from scratch withcompetitive performance. Moreover, we show that our method can be extended tofinetune pre-trained text-to-image diffusion for high-quality one-steptext-to-image synthesis even with LoRA fine-tuning. In particular, we providethe first diffusion transformer that can generate images in one step trained on512 resolution, with the capability of adapting to 1024 resolution withoutexplicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.</description><author>Yihong Luo, Xiaolong Chen, Jing Tang</author><pubDate>Fri, 29 Mar 2024 15:48:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12931v2</guid></item><item><title>On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem</title><link>http://arxiv.org/abs/2403.20212v1</link><description>We study the generalization capability of Unsupervised Learning in solvingthe Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN)trained with a surrogate loss function to generate an embedding for each node.We use these embeddings to construct a heat map that indicates the likelihoodof each edge being part of the optimal route. We then apply local search togenerate our final predictions. Our investigation explores how differenttraining instance sizes, embedding dimensions, and distributions influence theoutcomes of Unsupervised Learning methods. Our results show that training withlarger instance sizes and increasing embedding dimensions can build a moreeffective representation, enhancing the model's ability to solve TSP.Furthermore, in evaluating generalization across different distributions, wefirst determine the hardness of various distributions and explore how differenthardnesses affect the final results. Our findings suggest that models trainedon harder instances exhibit better generalization capabilities, highlightingthe importance of selecting appropriate training instances in solving TSP usingUnsupervised Learning.</description><author>Yimeng Min, Carla P. Gomes</author><pubDate>Fri, 29 Mar 2024 15:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20212v1</guid></item><item><title>Restricting to the chip architecture maintains the quantum neural network accuracy</title><link>http://arxiv.org/abs/2212.14426v2</link><description>In the era of noisy intermediate-scale quantum devices, variational quantumalgorithms (VQAs) stand as a prominent strategy for constructing quantummachine learning models. These models comprise both a quantum and a classicalcomponent. The quantum facet is characterized by a parametrization $U$,typically derived from the composition of various quantum gates. On the otherhand, the classical component involves an optimizer that adjusts the parametersof $U$ to minimize a cost function $C$. Despite the extensive applications ofVQAs, several critical questions persist, such as determining the optimal gatesequence, devising efficient parameter optimization strategies, selectingappropriate cost functions, and understanding the influence of quantum chiparchitectures on the final results. This article aims to address the lastquestion, emphasizing that, in general, the cost function tends to convergetowards an average value as the utilized parameterization approaches a$2$-design. Consequently, when the parameterization closely aligns with a$2$-design, the quantum neural network model's outcome becomes less dependenton the specific parametrization. This insight leads to the possibility ofleveraging the inherent architecture of quantum chips to define theparametrization for VQAs. By doing so, the need for additional swap gates ismitigated, consequently reducing the depth of VQAs and minimizing associatederrors.</description><author>Lucas Friedrich, Jonas Maziero</author><pubDate>Fri, 29 Mar 2024 15:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14426v2</guid></item><item><title>Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science</title><link>http://arxiv.org/abs/2403.20208v1</link><description>In the domain of data science, the predictive tasks of classification,regression, and imputation of missing values are commonly encounteredchallenges associated with tabular data. This research endeavors to apply LargeLanguage Models (LLMs) towards addressing these predictive tasks. Despite theirproficiency in comprehending natural language, LLMs fall short in dealing withstructured tabular data. This limitation stems from their lacking exposure tothe intricacies of tabular data during their foundational training. Ourresearch aims to mitigate this gap by compiling a comprehensive corpus oftables annotated with instructions and executing large-scale training ofLlama-2 on this enriched dataset. Furthermore, we investigate the practicalapplication of applying the trained model to zero-shot prediction, few-shotprediction, and in-context learning scenarios. Through extensive experiments,our methodology has shown significant improvements over existing benchmarks.These advancements highlight the efficacy of tailoring LLM training to solvetable-related problems in data science, thereby establishing a new benchmark inthe utilization of LLMs for enhancing tabular intelligence.</description><author>Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu</author><pubDate>Fri, 29 Mar 2024 15:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20208v1</guid></item><item><title>Two-sample Test using Projected Wasserstein Distance</title><link>http://arxiv.org/abs/2010.11970v4</link><description>We develop a projected Wasserstein distance for the two-sample test, afundamental problem in statistics and machine learning: given two sets ofsamples, to determine whether they are from the same distribution. Inparticular, we aim to circumvent the curse of dimensionality in Wassersteindistance: when the dimension is high, it has diminishing testing power, whichis inherently due to the slow concentration property of Wasserstein metrics inthe high dimension space. A key contribution is to couple optimal projection tofind the low dimensional linear mapping to maximize the Wasserstein distancebetween projected probability distributions. We characterize the theoreticalproperty of the finite-sample convergence rate on IPMs and present practicalalgorithms for computing this metric. Numerical examples validate ourtheoretical results.</description><author>Jie Wang, Rui Gao, Yao Xie</author><pubDate>Fri, 29 Mar 2024 15:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2010.11970v4</guid></item><item><title>RLSynC: Offline-Online Reinforcement Learning for Synthon Completion</title><link>http://arxiv.org/abs/2309.02671v3</link><description>Retrosynthesis is the process of determining the set of reactant moleculesthat can react to form a desired product. Semi-template-based retrosynthesismethods, which imitate the reverse logic of synthesis reactions, first predictthe reaction centers in the products, and then complete the resulting synthonsback into reactants. We develop a new offline-online reinforcement learningmethod RLSynC for synthon completion in semi-template-based methods. RLSynCassigns one agent to each synthon, all of which complete the synthons byconducting actions step by step in a synchronized fashion. RLSynC learns thepolicy from both offline training episodes and online interactions, whichallows RLSynC to explore new reaction spaces. RLSynC uses a standalone forwardsynthesis model to evaluate the likelihood of the predicted reactants insynthesizing a product, and thus guides the action search. Our resultsdemonstrate that RLSynC can outperform state-of-the-art synthon completionmethods with improvements as high as 14.9%, highlighting its potential insynthesis planning.</description><author>Frazier N. Baker, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning</author><pubDate>Fri, 29 Mar 2024 15:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02671v3</guid></item><item><title>The Future of Combating Rumors? Retrieval, Discrimination, and Generation</title><link>http://arxiv.org/abs/2403.20204v1</link><description>Artificial Intelligence Generated Content (AIGC) technology development hasfacilitated the creation of rumors with misinformation, impacting societal,economic, and political ecosystems, challenging democracy. Current rumordetection efforts fall short by merely labeling potentially misinformation(classification task), inadequately addressing the issue, and it is unrealisticto have authoritative institutions debunk every piece of information on socialmedia. Our proposed comprehensive debunking process not only detects rumors butalso provides explanatory generated content to refute the authenticity of theinformation. The Expert-Citizen Collective Wisdom (ECCW) module we designedaensures high-precision assessment of the credibility of information and theretrieval module is responsible for retrieving relevant knowledge from aReal-time updated debunking database based on information keywords. By usingprompt engineering techniques, we feed results and knowledge into a LLM (LargeLanguage Model), achieving satisfactory discrimination and explanatory effectswhile eliminating the need for fine-tuning, saving computational costs, andcontributing to debunking efforts.</description><author>Junhao Xu, Longdi Xian, Zening Liu, Mingliang Chen, Qiuyang Yin, Fenghua Song</author><pubDate>Fri, 29 Mar 2024 15:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20204v1</guid></item><item><title>Voice Signal Processing for Machine Learning. The Case of Speaker Isolation</title><link>http://arxiv.org/abs/2403.20202v1</link><description>The widespread use of automated voice assistants along with other recenttechnological developments have increased the demand for applications thatprocess audio signals and human voice in particular. Voice recognition tasksare typically performed using artificial intelligence and machine learningmodels. Even though end-to-end models exist, properly pre-processing the signalcan greatly reduce the complexity of the task and allow it to be solved with asimpler ML model and fewer computational resources. However, ML engineers whowork on such tasks might not have a background in signal processing which is anentirely different area of expertise. The objective of this work is to provide a concise comparative analysis ofFourier and Wavelet transforms that are most commonly used as signaldecomposition methods for audio processing tasks. Metrics for evaluating speechintelligibility are also discussed, namely Scale-Invariant Signal-to-DistortionRatio (SI-SDR), Perceptual Evaluation of Speech Quality (PESQ), and Short-TimeObjective Intelligibility (STOI). The level of detail in the exposition ismeant to be sufficient for an ML engineer to make informed decisions whenchoosing, fine-tuning, and evaluating a decomposition method for a specific MLmodel. The exposition contains mathematical definitions of the relevantconcepts accompanied with intuitive non-mathematical explanations in order tomake the text more accessible to engineers without deep expertise in signalprocessing. Formal mathematical definitions and proofs of theorems areintentionally omitted in order to keep the text concise.</description><author>Radan Ganchev</author><pubDate>Fri, 29 Mar 2024 15:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20202v1</guid></item><item><title>A multiobjective continuation method to compute the regularization path of deep neural networks</title><link>http://arxiv.org/abs/2308.12044v5</link><description>Sparsity is a highly desired feature in deep neural networks (DNNs) since itensures numerical efficiency, improves the interpretability of models (due tothe smaller number of relevant features), and robustness. For linear models, itis well known that there exists a \emph{regularization path} connecting thesparsest solution in terms of the $\ell^1$ norm, i.e., zero weights and thenon-regularized solution. Very recently, there was a first attempt to extendthe concept of regularization paths to DNNs by means of treating the empiricalloss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving theresulting multiobjective optimization problem for low-dimensional DNN. However,due to the non-smoothness of the $\ell^1$ norm and the high number ofparameters, this approach is not very efficient from a computationalperspective for high-dimensional DNNs. To overcome this limitation, we presentan algorithm that allows for the approximation of the entire Pareto front forthe above-mentioned objectives in a very efficient manner for high-dimensionalDNNs with millions of parameters. We present numerical examples using bothdeterministic and stochastic gradients. We furthermore demonstrate thatknowledge of the regularization path allows for a well-generalizing networkparametrization. To the best of our knowledge, this is the first algorithm tocompute the regularization path for non-convex multiobjective optimizationproblems (MOPs) with millions of degrees of freedom.</description><author>Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</author><pubDate>Fri, 29 Mar 2024 15:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12044v5</guid></item><item><title>High-dimensional analysis of ridge regression for non-identically distributed data with a variance profile</title><link>http://arxiv.org/abs/2403.20200v1</link><description>High-dimensional linear regression has been thoroughly studied in the contextof independent and identically distributed data. We propose to investigatehigh-dimensional regression models for independent but non-identicallydistributed data. To this end, we suppose that the set of observed predictors(or features) is a random matrix with a variance profile and with dimensionsgrowing at a proportional rate. Assuming a random effect model, we study thepredictive risk of the ridge estimator for linear regression with such avariance profile. In this setting, we provide deterministic equivalents of thisrisk and of the degree of freedom of the ridge estimator. For certain class ofvariance profile, our work highlights the emergence of the well-known doubledescent phenomenon in high-dimensional regression for the minimum normleast-squares estimator when the ridge regularization parameter goes to zero.We also exhibit variance profiles for which the shape of this predictive riskdiffers from double descent. The proofs of our results are based on tools fromrandom matrix theory in the presence of a variance profile that have not beenconsidered so far to study regression models. Numerical experiments areprovided to show the accuracy of the aforementioned deterministic equivalentson the computation of the predictive risk of ridge regression. We alsoinvestigate the similarities and differences that exist with the standardsetting of independent and identically distributed data.</description><author>Jérémie Bigot, Issa-Mbenard Dabo, Camille Male</author><pubDate>Fri, 29 Mar 2024 15:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20200v1</guid></item><item><title>NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks</title><link>http://arxiv.org/abs/2403.20199v1</link><description>Space Communication poses challenges such as severe delays, hard-to-predictroutes and communication disruptions. The Delay Tolerant Network architecture,having been specifically designed keeping such scenarios in mind, is suitableto address some challenges. The traditional DTN routing protocols fall short ofdelivering optimal performance, due to the inherent complexities of spacecommunication. Researchers have aimed at using recent advancements in AI tomitigate some routing challenges [9]. We propose utilising a feedforward neuralnetwork to develop a novel protocol NeuraLunaDTNet, which enhances theefficiency of the PRoPHET routing protocol for lunar communication, by learningcontact plans in dynamically changing spatio-temporal graph.</description><author>Parth Patel, Milena Radenkovic</author><pubDate>Fri, 29 Mar 2024 15:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20199v1</guid></item><item><title>Dual-Channel Multiplex Graph Neural Networks for Recommendation</title><link>http://arxiv.org/abs/2403.11624v3</link><description>Efficient recommender systems play a crucial role in accurately capturinguser and item attributes that mirror individual preferences. Some existingrecommendation techniques have started to shift their focus towards modelingvarious types of interaction relations between users and items in real-worldrecommendation scenarios, such as clicks, marking favorites, and purchases ononline shopping platforms. Nevertheless, these approaches still grapple withtwo significant shortcomings: (1) Insufficient modeling and exploitation of theimpact of various behavior patterns formed by multiplex relations between usersand items on representation learning, and (2) ignoring the effect of differentrelations in the behavior patterns on the target relation in recommender systemscenarios. In this study, we introduce a novel recommendation framework,Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses theaforementioned challenges. It incorporates an explicit behavior patternrepresentation learner to capture the behavior patterns composed of multiplexuser-item interaction relations, and includes a relation chain representationlearning and a relation chain-aware encoder to discover the impact of variousauxiliary relations on the target relation, the dependencies between differentrelations, and mine the appropriate order of relations in a behavior pattern.Extensive experiments on three real-world datasets demonstrate that our \modelsurpasses various state-of-the-art recommendation methods. It outperforms thebest baselines by 10.06\% and 12.15\% on average across all datasets in termsof R@10 and N@10 respectively.</description><author>Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu Dong, Yanwei Yu</author><pubDate>Fri, 29 Mar 2024 15:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11624v3</guid></item><item><title>V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions</title><link>http://arxiv.org/abs/2403.11371v4</link><description>Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perceptionsystems have shown the significant success on 3D object detection. While thesemodels perform well in the trained clean weather, they struggle in unseenadverse weather conditions with the real-world domain gap. In this paper, wepropose a domain generalization approach, named V2X-DGW, for LiDAR-based 3Dobject detection on multi-agent perception system under adverse weatherconditions. Not only in the clean weather does our research aim to ensurefavorable multi-agent performance, but also in the unseen adverse weatherconditions by learning only on the clean weather data. To advance research inthis area, we have simulated the impact of three prevalent adverse weatherconditions on two widely-used multi-agent datasets, resulting in the creationof two novel benchmark datasets: OPV2V-w and V2XSet-w. To this end, we first introduce the Adaptive Weather Augmentation (AWA) tomimic the unseen adverse weather conditions, and then propose two alignmentsfor generalizable representation learning: Trust-region Weather-invariantAlignment (TWA) and Agent-aware Contrastive Alignment (ACA). Extensiveexperimental results demonstrate that our V2X-DGW achieved improvements in theunseen adverse weather conditions.</description><author>Baolu Li, Jinlong Li, Xinyu Liu, Runsheng Xu, Zhengzhong Tu, Jiacheng Guo, Xiaopeng Li, Hongkai Yu</author><pubDate>Fri, 29 Mar 2024 15:19:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11371v4</guid></item><item><title>Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization</title><link>http://arxiv.org/abs/2403.20197v1</link><description>Simplex-structured matrix factorization (SSMF) is a generalization ofnonnegative matrix factorization, a fundamental interpretable data analysismodel, and has applications in hyperspectral unmixing and topic modeling. Toobtain identifiable solutions, a standard approach is to find minimum-volumesolutions. By taking advantage of the duality/polarity concept for polytopes,we convert minimum-volume SSMF in the primal space to a maximum-volume problemin the dual space. We first prove the identifiability of this maximum-volumedual problem. Then, we use this dual formulation to provide a noveloptimization approach which bridges the gap between two existing families ofalgorithms for SSMF, namely volume minimization and facet identification.Numerical experiments show that the proposed approach performs favorablycompared to the state-of-the-art SSMF algorithms.</description><author>Maryam Abdolali, Giovanni Barbarino, Nicolas Gillis</author><pubDate>Fri, 29 Mar 2024 15:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20197v1</guid></item><item><title>Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks</title><link>http://arxiv.org/abs/2403.20196v1</link><description>Existing discourse corpora are annotated based on different frameworks, whichshow significant dissimilarities in definitions of arguments and relations andstructural constraints. Despite surface differences, these frameworks sharebasic understandings of discourse relations. The relationship between theseframeworks has been an open research question, especially the correlationbetween relation inventories utilized in different frameworks. Betterunderstanding of this question is helpful for integrating discourse theoriesand enabling interoperability of discourse corpora annotated under differentframeworks. However, studies that explore correlations between discourserelation inventories are hindered by different criteria of discoursesegmentation, and expert knowledge and manual examination are typically needed.Some semi-automatic methods have been proposed, but they rely on corporaannotated in multiple frameworks in parallel. In this paper, we introduce afully automatic approach to address the challenges. Specifically, we extend thelabel-anchored contrastive learning method introduced by Zhang et al. (2022b)to learn label embeddings during a classification task. These embeddings arethen utilized to map discourse relations from different frameworks. We showexperimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad etal., 2018).</description><author>Yingxue Fu</author><pubDate>Fri, 29 Mar 2024 15:18:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20196v1</guid></item><item><title>Enhancing Lithological Mapping with Spatially Constrained Bayesian Network (SCB-Net): An Approach for Field Data-Constrained Predictions with Uncertainty Evaluation</title><link>http://arxiv.org/abs/2403.20195v1</link><description>Geological maps are an extremely valuable source of information for the Earthsciences. They provide insights into mineral exploration, vulnerability tonatural hazards, and many other applications. These maps are created usingnumerical or conceptual models that use geological observations to extrapolatedata. Geostatistical techniques have traditionally been used to generatereliable predictions that take into account the spatial patterns inherent inthe data. However, as the number of auxiliary variables increases, thesemethods become more labor-intensive. Additionally, traditional machine learningmethods often struggle with spatially correlated data and extracting valuablenon-linear information from geoscientific datasets. To address theselimitations, a new architecture called the Spatially Constrained BayesianNetwork (SCB-Net) has been developed. The SCB-Net aims to effectively exploitthe information from auxiliary variables while producing spatially constrainedpredictions. It is made up of two parts, the first part focuses on learningunderlying patterns in the auxiliary variables while the second part integratesground-truth data and the learned embeddings from the first part. Moreover, toassess model uncertainty, a technique called Monte Carlo dropout is used as aBayesian approximation. The SCB-Net has been applied to two selected areas innorthern Quebec, Canada, and has demonstrated its potential in generatingfield-data-constrained lithological maps while allowing assessment ofprediction uncertainty for decision-making. This study highlights the promisingadvancements of deep neural networks in geostatistics, particularly in handlingcomplex spatial feature learning tasks, leading to improved spatial informationtechniques.</description><author>Victor Silva dos Santos, Erwan Gloaguen, Shiva Tirdad</author><pubDate>Fri, 29 Mar 2024 15:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20195v1</guid></item><item><title>Motion Inversion for Video Customization</title><link>http://arxiv.org/abs/2403.20193v1</link><description>In this research, we present a novel approach to motion customization invideo generation, addressing the widespread gap in the thorough exploration ofmotion representation within video generative models. Recognizing the uniquechallenges posed by video's spatiotemporal nature, our method introduces MotionEmbeddings, a set of explicit, temporally coherent one-dimensional embeddingsderived from a given video. These embeddings are designed to integrateseamlessly with the temporal transformer modules of video diffusion models,modulating self-attention computations across frames without compromisingspatial integrity. Our approach offers a compact and efficient solution tomotion representation and enables complex manipulations of motioncharacteristics through vector arithmetic in the embedding space. Furthermore,we identify the Temporal Discrepancy in video generative models, which refersto variations in how different motion modules process temporal relationshipsbetween frames. We leverage this understanding to optimize the integration ofour motion embeddings. Our contributions include the introduction of a tailoredmotion embedding for customization tasks, insights into the temporal processingdifferences in video models, and a demonstration of the practical advantagesand effectiveness of our method through extensive experiments.</description><author>Luozhou Wang, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, Yingcong Chen</author><pubDate>Fri, 29 Mar 2024 15:14:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20193v1</guid></item><item><title>Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers</title><link>http://arxiv.org/abs/2403.19591v2</link><description>Non-linear functions are prevalent in Transformers and their lightweightvariants, incurring substantial and frequently underestimated hardware costs.Previous state-of-the-art works optimize these operations by piece-wise linearapproximation and store the parameters in look-up tables (LUT), but most ofthem require unfriendly high-precision arithmetics such as FP/INT 32 and lackconsideration of integer-only INT quantization. This paper proposed a geneticLUT-Approximation algorithm namely GQA-LUT that can automatically determine theparameters with quantization awareness. The results demonstrate that GQA-LUTachieves negligible degradation on the challenging semantic segmentation taskfor both vanilla and linear Transformer models. Besides, proposed GQA-LUTenables the employment of INT8-based LUT-Approximation that achieves an areasavings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to thehigh-precision FP/INT 32 alternatives. Code is available at https://github.com/PingchengDong/GQA-LUT.</description><author>Pingcheng Dong, Yonghao Tan, Dong Zhang, Tianwei Ni, Xuejiao Liu, Yu Liu, Peng Luo, Luhong Liang, Shih-Yang Liu, Xijie Huang, Huaiyu Zhu, Yun Pan, Fengwei An, Kwang-Ting Cheng</author><pubDate>Fri, 29 Mar 2024 15:13:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19591v2</guid></item><item><title>Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data</title><link>http://arxiv.org/abs/2403.20190v1</link><description>The widespread application of machine learning algorithms is a matter ofincreasing concern for the data privacy research community, and many havesought to develop privacy-preserving techniques for it. Among existingapproaches, the homomorphic evaluation of ML algorithms stands out byperforming operations directly over encrypted data, enabling strong guaranteesof confidentiality. The homomorphic evaluation of inference algorithms ispractical even for relatively deep Convolution Neural Networks (CNNs). However,training is still a major challenge, with current solutions often resorting tolightweight algorithms that can be unfit for solving more complex problems,such as image recognition. This work introduces the homomorphic evaluation ofWilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequentWeightless Neural Networks (WNNs) for training and inference on encrypted data.Compared to CNNs, WNNs offer better performance with a relatively smallaccuracy drop. We develop a complete framework for it, including severalbuilding blocks that can be of independent interest. Our framework achieves91.7% accuracy on the MNIST dataset after only 3.5 minutes of encryptedtraining (multi-threaded), going up to 93.8% in 3.5 hours. For the HAM10000dataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after1 hour. Compared to the state of the art on the HE evaluation of CNN training,Glyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to1200 times with an accuracy loss of at most 5.4%. For HAM10000, we evenachieved a 0.65% accuracy improvement while being 60 times faster than Glyph.We also provide solutions for small-scale encrypted training. In a singlethread on a desktop machine using less than 200MB of memory, we train over 1000MNIST images in 12 minutes or over the entire Wisconsin Breast Cancer datasetin just 11 seconds.</description><author>Leonardo Neumann, Antonio Guimarães, Diego F. Aranha, Edson Borin</author><pubDate>Fri, 29 Mar 2024 15:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20190v1</guid></item><item><title>Distributed Swarm Learning for Edge Internet of Things</title><link>http://arxiv.org/abs/2403.20188v1</link><description>The rapid growth of Internet of Things (IoT) has led to the widespreaddeployment of smart IoT devices at wireless edge for collaborative machinelearning tasks, ushering in a new era of edge learning. With a huge number ofhardware-constrained IoT devices operating in resource-limited wirelessnetworks, edge learning encounters substantial challenges, includingcommunication and computation bottlenecks, device and data heterogeneity,security risks, privacy leakages, non-convex optimization, and complex wirelessenvironments. To address these issues, this article explores a novel frameworkknown as distributed swarm learning (DSL), which combines artificialintelligence and biological swarm intelligence in a holistic manner. Byharnessing advanced signal processing and communications, DSL providesefficient solutions and robust tools for large-scale IoT at the edge ofwireless networks.</description><author>Yue Wang, Zhi Tian, FXin Fan, Zhipeng Cai, Cameron Nowzari, Kai Zeng</author><pubDate>Fri, 29 Mar 2024 15:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20188v1</guid></item><item><title>Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey</title><link>http://arxiv.org/abs/2305.18703v7</link><description>Large language models (LLMs) have significantly advanced the field of naturallanguage processing (NLP), providing a highly useful, task-agnostic foundationfor a wide range of applications. However, directly applying LLMs to solvesophisticated problems in specific domains meets many hurdles, caused by theheterogeneity of domain data, the sophistication of domain knowledge, theuniqueness of domain objectives, and the diversity of the constraints (e.g.,various social norms, cultural conformity, religious beliefs, and ethicalstandards in the domain applications). Domain specification techniques are keyto make large language models disruptive in many applications. Specifically, tosolve these hurdles, there has been a notable increase in research andpractices conducted in recent years on the domain specialization of LLMs. Thisemerging field of study, with its substantial potential for impact,necessitates a comprehensive and systematic review to better summarize andguide ongoing work in this area. In this article, we present a comprehensivesurvey on domain specification techniques for large language models, anemerging direction critical for large language model applications. First, wepropose a systematic taxonomy that categorizes the LLM domain-specializationtechniques based on the accessibility to LLMs and summarizes the framework forall the subcategories as well as their relations and differences to each other.Second, we present an extensive taxonomy of critical application domains thatcan benefit dramatically from specialized LLMs, discussing their practicalsignificance and open challenges. Last, we offer our insights into the currentresearch status and future trends in this area.</description><author>Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Dhagash Mehta, Stefano Pasquali, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, Carl Yang, Liang Zhao</author><pubDate>Fri, 29 Mar 2024 15:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18703v7</guid></item></channel></rss>