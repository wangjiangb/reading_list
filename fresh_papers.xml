<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 30 Nov 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A Simple Recipe for Language-guided Domain Generalized Segmentation</title><link>http://arxiv.org/abs/2311.17922v1</link><description>Generalization to new domains not seen during training is one of thelong-standing goals and challenges in deploying neural networks in real-worldapplications. Existing generalization techniques necessitate substantial dataaugmentation, potentially sourced from external datasets, and aim at learninginvariant representations by imposing various alignment constraints.Large-scale pretraining has recently shown promising generalizationcapabilities, along with the potential of bridging different modalities. Forinstance, the recent advent of vision-language models like CLIP has opened thedoorway for vision models to exploit the textual modality. In this paper, weintroduce a simple framework for generalizing semantic segmentation networks byemploying language as the source of randomization. Our recipe comprises threekey ingredients: i) the preservation of the intrinsic CLIP robustness throughminimal fine-tuning, ii) language-driven local style augmentation, and iii)randomization by locally mixing the source and augmented styles duringtraining. Extensive experiments report state-of-the-art results on variousgeneralization benchmarks. The code will be made available.</description><author>Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick PÃ©rez, Raoul de Charette</author><pubDate>Wed, 29 Nov 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17922v1</guid></item><item><title>Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models</title><link>http://arxiv.org/abs/2311.17919v1</link><description>We address the problem of synthesizing multi-view optical illusions: imagesthat change appearance upon a transformation, such as a flip or rotation. Wepropose a simple, zero-shot method for obtaining these illusions fromoff-the-shelf text-to-image diffusion models. During the reverse diffusionprocess, we estimate the noise from different views of a noisy image. We thencombine these noise estimates together and denoise the image. A theoreticalanalysis suggests that this method works precisely for views that can bewritten as orthogonal transformations, of which permutations are a subset. Thisleads to the idea of a visual anagram--an image that changes appearance undersome rearrangement of pixels. This includes rotations and flips, but also moreexotic pixel permutations such as a jigsaw rearrangement. Our approach alsonaturally extends to illusions with more than two views. We provide bothqualitative and quantitative results demonstrating the effectiveness andflexibility of our method. Please see our project webpage for additionalvisualizations and results: https://dangeng.github.io/visual_anagrams/</description><author>Daniel Geng, Inbum Park, Andrew Owens</author><pubDate>Wed, 29 Nov 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17919v1</guid></item><item><title>Do text-free diffusion models learn discriminative visual representations?</title><link>http://arxiv.org/abs/2311.17921v1</link><description>While many unsupervised learning models focus on one family of tasks, eithergenerative or discriminative, we explore the possibility of a unifiedrepresentation learner: a model which addresses both families of taskssimultaneously. We identify diffusion models, a state-of-the-art method forgenerative tasks, as a prime candidate. Such models involve training a U-Net toiteratively predict and remove noise, and the resulting model can synthesizehigh-fidelity, diverse, novel images. We find that the intermediate featuremaps of the U-Net are diverse, discriminative feature representations. Wepropose a novel attention mechanism for pooling feature maps and furtherleverage this mechanism as DifFormer, a transformer feature fusion of featuresfrom different diffusion U-Net blocks and noise steps. We also develop DifFeed,a novel feedback mechanism tailored to diffusion. We find that diffusion modelsare better than GANs, and, with our fusion and feedback mechanisms, can competewith state-of-the-art unsupervised image representation learning methods fordiscriminative tasks - image classification with full and semi-supervision,transfer for fine-grained classification, object detection and segmentation,and semantic segmentation. Our project website(https://mgwillia.github.io/diffssl/) and code(https://github.com/soumik-kanad/diffssl) are available publicly.</description><author>Soumik Mukhopadhyay, Matthew Gwilliam, Yosuke Yamaguchi, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Tianyi Zhou, Abhinav Shrivastava</author><pubDate>Wed, 29 Nov 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17921v1</guid></item><item><title>Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving</title><link>http://arxiv.org/abs/2311.17918v1</link><description>In autonomous driving, predicting future events in advance and evaluating theforeseeable risks empowers autonomous vehicles to better plan their actions,enhancing safety and efficiency on the road. To this end, we propose Drive-WM,the first driving world model compatible with existing end-to-end planningmodels. Through a joint spatial-temporal modeling facilitated by viewfactorization, our model generates high-fidelity multiview videos in drivingscenes. Building on its powerful generation ability, we showcase the potentialof applying the world model for safe driving planning for the first time.Particularly, our Drive-WM enables driving into multiple futures based ondistinct driving maneuvers, and determines the optimal trajectory according tothe image-based rewards. Evaluation on real-world driving datasets verifiesthat our method could generate high-quality, consistent, and controllablemultiview videos, opening up possibilities for real-world simulations and safeplanning.</description><author>Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, Zhaoxiang Zhang</author><pubDate>Wed, 29 Nov 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17918v1</guid></item><item><title>AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text</title><link>http://arxiv.org/abs/2311.17917v1</link><description>We study the problem of creating high-fidelity and animatable 3D avatars fromonly textual descriptions. Existing text-to-avatar methods are either limitedto static avatars which cannot be animated or struggle to generate animatableavatars with promising quality and precise pose control. To address theselimitations, we propose AvatarStudio, a coarse-to-fine generative model thatgenerates explicit textured 3D meshes for animatable human avatars.Specifically, AvatarStudio begins with a low-resolution NeRF-basedrepresentation for coarse generation, followed by incorporating SMPL-guidedarticulation into the explicit mesh representation to support avatar animationand high resolution rendering. To ensure view consistency and posecontrollability of the resulting avatars, we introduce a 2D diffusion modelconditioned on DensePose for Score Distillation Sampling supervision. Byeffectively leveraging the synergy between the articulated mesh representationand the DensePose-conditional diffusion model, AvatarStudio can createhigh-quality avatars from text that are ready for animation, significantlyoutperforming previous methods. Moreover, it is competent for manyapplications, e.g., multimodal avatar animations and style-guided avatarcreation. For more results, please refer to our project page:http://jeff95.me/projects/avatarstudio.html</description><author>Jianfeng Zhang, Xuanmeng Zhang, Huichao Zhang, Jun Hao Liew, Chenxu Zhang, Yi Yang, Jiashi Feng</author><pubDate>Wed, 29 Nov 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17917v1</guid></item><item><title>Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces</title><link>http://arxiv.org/abs/2305.19891v3</link><description>Large discrete action spaces (LDAS) remain a central challenge inreinforcement learning. Existing solution approaches can handle unstructuredLDAS with up to a few million actions. However, many real-world applications inlogistics, production, and transportation systems have combinatorial actionspaces, whose size grows well beyond millions of actions, even on smallinstances. Fortunately, such action spaces exhibit structure, e.g., equallyspaced discrete resource units. With this work, we focus on handling structuredLDAS (SLDAS) with sizes that cannot be handled by current benchmarks: wepropose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigmfor SLDAS. We present a scalable neighborhood exploration heuristic thatutilizes this paradigm and efficiently explores the discrete neighborhoodaround the continuous proxy action in structured action spaces with up to$10^{73}$ actions. We demonstrate the performance of our method by benchmarkingit against three state-of-the-art approaches designed for large discrete actionspaces across two distinct environments. Our results show that DNC matches oroutperforms state-of-the-art approaches while being computationally moreefficient. Furthermore, our method scales to action spaces that so far remainedcomputationally intractable for existing methodologies.</description><author>Fabian Akkerman, Julius Luy, Wouter van Heeswijk, Maximilian Schiffer</author><pubDate>Wed, 29 Nov 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19891v3</guid></item><item><title>Algorithmic Assistance with Recommendation-Dependent Preferences</title><link>http://arxiv.org/abs/2208.07626v2</link><description>When we use algorithms to produce risk assessments, we typically think ofthese predictions as providing helpful input to human decisions, such as whenrisk scores are presented to judges or doctors. But when a decision-makerobtains algorithmic assistance, they may not only react to the information. Thedecision-maker may view the input of the algorithm as recommending a defaultaction, making it costly for them to deviate, such as when a judge is reluctantto overrule a high-risk assessment of a defendant or a doctor fears theconsequences of deviating from recommended procedures. In this article, wepropose a principal-agent model of joint human-machine decision-making. Withinthis model, we consider the effect and design of algorithmic recommendationswhen they affect choices not just by shifting beliefs, but also by alteringpreferences. We motivate this assumption from institutional factors, such as adesire to avoid audits, as well as from well-established models in behavioralscience that predict loss aversion relative to a reference point, which here isset by the algorithm. We show that recommendation-dependent preferences createinefficiencies where the decision-maker is overly responsive to therecommendation. As a potential remedy, we discuss algorithms that strategicallywithhold recommendations, and show how they can improve the quality of finaldecisions.</description><author>Bryce McLaughlin, Jann Spiess</author><pubDate>Wed, 29 Nov 2023 18:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07626v2</guid></item><item><title>OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation</title><link>http://arxiv.org/abs/2311.17911v1</link><description>Hallucination, posed as a pervasive challenge of multi-modal large languagemodels (MLLMs), has significantly impeded their real-world usage that demandsprecise judgment. Existing methods mitigate this issue with either trainingwith specific designed data or inferencing with external knowledge from othersources, incurring inevitable additional costs. In this paper, we presentOPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and aRetrospection-Allocation strategy, serving as a nearly free lunch to alleviatethe hallucination issue without additional data, knowledge, or training. Ourapproach begins with an interesting observation that, most hallucinations areclosely tied to the knowledge aggregation patterns manifested in theself-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on afew summary tokens, but not all the previous tokens. Such partial over-trustinclination results in the neglecting of image tokens and describes the imagecontent with hallucination. Statistically, we observe an 80%$\sim$95%co-currency rate between hallucination contents and such knowledge aggregationpatterns. Based on the observation, OPERA introduces a penalty term on themodel logits during the beam-search decoding to mitigate the over-trust issue,along with a rollback strategy that retrospects the presence of summary tokensin the previously generated tokens, and re-allocate the token selection ifnecessary. With extensive experiments, OPERA shows significanthallucination-mitigating performance on different MLLMs and metrics, provingits effectiveness and generality. Our code is available at:https://github.com/shikiw/OPERA.</description><author>Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</author><pubDate>Wed, 29 Nov 2023 18:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17911v1</guid></item><item><title>HUGS: Human Gaussian Splats</title><link>http://arxiv.org/abs/2311.17910v1</link><description>Recent advances in neural rendering have improved both training and renderingtimes by orders of magnitude. While these methods demonstrate state-of-the-artquality and speed, they are designed for photogrammetry of static scenes and donot generalize well to freely moving humans in the environment. In this work,we introduce Human Gaussian Splats (HUGS) that represents an animatable humantogether with the scene using 3D Gaussian Splatting (3DGS). Our method takesonly a monocular video with a small number of (50-100) frames, and itautomatically learns to disentangle the static scene and a fully animatablehuman avatar within 30 minutes. We utilize the SMPL body model to initializethe human Gaussians. To capture details that are not modeled by SMPL (e.g.cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.Utilizing 3D Gaussians for animated humans brings new challenges, including theartifacts created when articulating the Gaussians. We propose to jointlyoptimize the linear blend skinning weights to coordinate the movements ofindividual Gaussians during animation. Our approach enables novel-posesynthesis of human and novel view synthesis of both the human and the scene. Weachieve state-of-the-art rendering quality with a rendering speed of 60 FPSwhile being ~100x faster to train over previous work. Our code will beannounced here: https://github.com/apple/ml-hugs</description><author>Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan</author><pubDate>Wed, 29 Nov 2023 18:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17910v1</guid></item><item><title>CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting</title><link>http://arxiv.org/abs/2311.17907v1</link><description>With the onset of diffusion-based generative models and their ability togenerate text-conditioned images, content generation has received a massiveinvigoration. Recently, these models have been shown to provide useful guidancefor the generation of 3D graphics assets. However, existing work intext-conditioned 3D generation faces fundamental constraints: (i) inability togenerate detailed, multi-object scenes, (ii) inability to textually controlmulti-object configurations, and (iii) physically realistic scene composition.In this work, we propose CG3D, a method for compositionally generating scalable3D assets that resolves these constraints. We find that explicit Gaussianradiance fields, parameterized to allow for compositions of objects, possessthe capability to enable semantically and physically consistent scenes. Byutilizing a guidance framework built around this explicit representation, weshow state of the art results, capable of even exceeding the guiding diffusionmodel in terms of object combinations and physics accuracy.</description><author>Alexander Vilesov, Pradyumna Chari, Achuta Kadambi</author><pubDate>Wed, 29 Nov 2023 18:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17907v1</guid></item><item><title>Language-conditioned Detection Transformer</title><link>http://arxiv.org/abs/2311.17902v1</link><description>We present a new open-vocabulary detection framework. Our framework uses bothimage-level labels and detailed detection annotations when available. Ourframework proceeds in three steps. We first train a language-conditioned objectdetector on fully-supervised detection data. This detector gets to see thepresence or absence of ground truth classes during training, and conditionsprediction on the set of present classes. We use this detector to pseudo-labelimages with image-level labels. Our detector provides much more accuratepseudo-labels than prior approaches with its conditioning mechanism. Finally,we train an unconditioned open-vocabulary detector on the pseudo-annotatedimages. The resulting detector, named DECOLA, shows strong zero-shotperformance in open-vocabulary LVIS benchmark as well as direct zero-shottransfer benchmarks on LVIS, COCO, Object365, and OpenImages. DECOLAoutperforms the prior arts by 17.1 AP-rare and 9.4 mAP on zero-shot LVISbenchmark. DECOLA achieves state-of-the-art results in various model sizes,architectures, and datasets by only training on open-sourced data andacademic-scale computing. Code is available athttps://github.com/janghyuncho/DECOLA.</description><author>Jang Hyun Cho, Philipp KrÃ¤henbÃ¼hl</author><pubDate>Wed, 29 Nov 2023 18:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17902v1</guid></item><item><title>SODA: Bottleneck Diffusion Models for Representation Learning</title><link>http://arxiv.org/abs/2311.17901v1</link><description>We introduce SODA, a self-supervised diffusion model, designed forrepresentation learning. The model incorporates an image encoder, whichdistills a source view into a compact representation, that, in turn, guides thegeneration of related novel views. We show that by imposing a tight bottleneckbetween the encoder and a denoising decoder, and leveraging novel viewsynthesis as a self-supervised objective, we can turn diffusion models intostrong representation learners, capable of capturing visual semantics in anunsupervised manner. To the best of our knowledge, SODA is the first diffusionmodel to succeed at ImageNet linear-probe classification, and, at the sametime, it accomplishes reconstruction, editing and synthesis tasks across a widerange of datasets. Further investigation reveals the disentangled nature of itsemergent latent space, that serves as an effective interface to control andmanipulate the model's produced images. All in all, we aim to shed light on theexciting and promising potential of diffusion models, not only for imagegeneration, but also for learning rich and robust representations.</description><author>Drew A. Hudson, Daniel Zoran, Mateusz Malinowski, Andrew K. Lampinen, Andrew Jaegle, James L. McClelland, Loic Matthey, Felix Hill, Alexander Lerchner</author><pubDate>Wed, 29 Nov 2023 18:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17901v1</guid></item><item><title>Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis</title><link>http://arxiv.org/abs/2311.17898v1</link><description>Hallucinations and unfaithful synthesis due to inaccurate prompts withinsufficient semantic details are widely observed in multimodal generativemodels. A prevalent strategy to align multiple modalities is to fine-tune thegenerator with a large number of annotated text-image pairs. However, such aprocedure is labor-consuming and resource-draining. The key question we ask is:can we enhance the quality and faithfulness of text-driven generative modelsbeyond extensive text-image pair annotations? To address this question, wepropose Knowledge Pursuit Prompting (KPP), a zero-shot framework thatiteratively incorporates external knowledge to help generators produce reliablevisual content. Instead of training generators to handle generic prompts, KPPemploys a recursive knowledge query process to gather informative externalfacts from the knowledge base, instructs a language model to compress theacquired knowledge for prompt refinement, and utilizes text-driven generatorsfor visual synthesis. The entire process is zero-shot, without accessing thearchitectures and parameters of generative models. We evaluate the frameworkacross multiple text-driven generative tasks (image, 3D rendering, and video)on datasets of different domains. We further demonstrate the extensibility andadaptability of KPP through varying foundation model bases and instructions.Our results show that KPP is capable of generating faithful and semanticallyrich content across diverse visual domains, offering a promising solution toimprove multimodal generative models.</description><author>Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, RenÃ© Vidal</author><pubDate>Wed, 29 Nov 2023 18:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17898v1</guid></item><item><title>Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</title><link>http://arxiv.org/abs/2311.17893v1</link><description>In this paper, we propose a simple yet effective approach for self-supervisedvideo object segmentation (VOS). Our key insight is that the inherentstructural dependencies present in DINO-pretrained Transformers can beleveraged to establish robust spatio-temporal correspondences in videos.Furthermore, simple clustering on this correspondence cue is sufficient toyield competitive segmentation results. Previous self-supervised VOS techniquesmajorly resort to auxiliary modalities or utilize iterative slot attention toassist in object discovery, which restricts their general applicability andimposes higher computational requirements. To deal with these challenges, wedevelop a simplified architecture that capitalizes on the emerging objectnessfrom DINO-pretrained Transformers, bypassing the need for additional modalitiesor slot attention. Specifically, we first introduce a single spatio-temporalTransformer block to process the frame-wise DINO features and establishspatio-temporal dependencies in the form of self-attention. Subsequently,utilizing these attention maps, we implement hierarchical clustering togenerate object segmentation masks. To train the spatio-temporal block in afully self-supervised manner, we employ semantic and dynamic motion consistencycoupled with entropy normalization. Our method demonstrates state-of-the-artperformance across multiple unsupervised VOS benchmarks and particularly excelsin complex real-world multi-object video segmentation tasks such asDAVIS-17-Unsupervised and YouTube-VIS-19. The code and model checkpoints willbe released at https://github.com/shvdiwnkozbw/SSL-UVOS.</description><author>Shuangrui Ding, Rui Qian, Haohang Xu, Dahua Lin, Hongkai Xiong</author><pubDate>Wed, 29 Nov 2023 18:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17893v1</guid></item><item><title>A Pipeline For Discourse Circuits From CCG</title><link>http://arxiv.org/abs/2311.17892v1</link><description>There is a significant disconnect between linguistic theory and modern NLPpractice, which relies heavily on inscrutable black-box architectures.DisCoCirc is a newly proposed model for meaning that aims to bridge thisdivide, by providing neuro-symbolic models that incorporate linguisticstructure. DisCoCirc represents natural language text as a `circuit' thatcaptures the core semantic information of the text. These circuits can then beinterpreted as modular machine learning models. Additionally, DisCoCirc fulfilsanother major aim of providing an NLP model that can be implemented onnear-term quantum computers. In this paper we describe a software pipeline that converts English text toits DisCoCirc representation. The pipeline achieves coverage over a largefragment of the English language. It relies on Combinatory Categorial Grammar(CCG) parses of the input text as well as coreference resolution information.This semantic and syntactic information is used in several steps to convert thetext into a simply-typed $\lambda$-calculus term, and then into a circuitdiagram. This pipeline will enable the application of the DisCoCirc frameworkto NLP tasks, using both classical and quantum approaches.</description><author>Jonathon Liu, Razin A. Shaikh, Benjamin Rodatz, Richie Yeung, Bob Coecke</author><pubDate>Wed, 29 Nov 2023 18:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17892v1</guid></item><item><title>LibSignal: An Open Library for Traffic Signal Control</title><link>http://arxiv.org/abs/2211.10649v2</link><description>This paper introduces a library for cross-simulator comparison ofreinforcement learning models in traffic signal control tasks. This library isdeveloped to implement recent state-of-the-art reinforcement learning modelswith extensible interfaces and unified cross-simulator evaluation metrics. Itsupports commonly-used simulators in traffic signal control tasks, includingSimulation of Urban MObility(SUMO) and CityFlow, and multiple benchmarkdatasets for fair comparisons. We conducted experiments to validate ourimplementation of the models and to calibrate the simulators so that theexperiments from one simulator could be referential to the other. Based on thevalidated models and calibrated environments, this paper compares and reportsthe performance of current state-of-the-art RL algorithms across differentdatasets and simulators. This is the first time that these methods have beencompared fairly under the same datasets with different simulators.</description><author>Hao Mei, Xiaoliang Lei, Longchao Da, Bin Shi, Hua Wei</author><pubDate>Wed, 29 Nov 2023 18:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10649v2</guid></item><item><title>Pose Anything: A Graph-Based Approach for Category-Agnostic Pose Estimation</title><link>http://arxiv.org/abs/2311.17891v1</link><description>Traditional 2D pose estimation models are limited by their category-specificdesign, making them suitable only for predefined object categories. Thisrestriction becomes particularly challenging when dealing with novel objectsdue to the lack of relevant training data. To address this limitation, category-agnostic pose estimation (CAPE) wasintroduced. CAPE aims to enable keypoint localization for arbitrary objectcategories using a single model, requiring minimal support images withannotated keypoints. This approach not only enables object pose generationbased on arbitrary keypoint definitions but also significantly reduces theassociated costs, paving the way for versatile and adaptable pose estimationapplications. We present a novel approach to CAPE that leverages the inherent geometricalrelations between keypoints through a newly designed Graph Transformer Decoder.By capturing and incorporating this crucial structural information, our methodenhances the accuracy of keypoint localization, marking a significant departurefrom conventional CAPE techniques that treat keypoints as isolated entities. We validate our approach on the MP-100 benchmark, a comprehensive datasetcomprising over 20,000 images spanning more than 100 categories. Our methodoutperforms the prior state-of-the-art by substantial margins, achievingremarkable improvements of 2.16% and 1.82% under 1-shot and 5-shot settings,respectively. Furthermore, our method's end-to-end training demonstrates bothscalability and efficiency compared to previous CAPE approaches.</description><author>Or Hirschorn, Shai Avidan</author><pubDate>Wed, 29 Nov 2023 18:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17891v1</guid></item><item><title>Uncertainty-aware Traffic Prediction under Missing Data</title><link>http://arxiv.org/abs/2309.06800v5</link><description>Traffic prediction is a crucial topic because of its broad scope ofapplications in the transportation domain. Recently, various studies haveachieved promising results. However, most studies assume the predictionlocations have complete or at least partial historical records and cannot beextended to non-historical recorded locations. In real-life scenarios, thedeployment of sensors could be limited due to budget limitations andinstallation availability, which makes most current models not applicable.Though few pieces of literature tried to impute traffic states at the missinglocations, these methods need the data simultaneously observed at the locationswith sensors, making them not applicable to prediction tasks. Another drawbackis the lack of measurement of uncertainty in prediction, making prior worksunsuitable for risk-sensitive tasks or involving decision-making. To fill thegap, inspired by the previous inductive graph neural network, this workproposed an uncertainty-aware framework with the ability to 1) extendprediction to missing locations with no historical records and significantlyextend spatial coverage of prediction locations while reducing deployment ofsensors and 2) generate probabilistic prediction with uncertaintyquantification to help the management of risk and decision making in thedown-stream tasks. Through extensive experiments on real-life datasets, theresult shows our method achieved promising results on prediction tasks, and theuncertainty quantification gives consistent results which highly correlatedwith the locations with and without historical data. We also show that ourmodel could help support sensor deployment tasks in the transportation field toachieve higher accuracy with a limited sensor deployment budget.</description><author>Hao Mei, Junxian Li, Zhiming Liang, Guanjie Zheng, Bin Shi, Hua Wei</author><pubDate>Wed, 29 Nov 2023 18:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06800v5</guid></item><item><title>Are ensembles getting better all the time?</title><link>http://arxiv.org/abs/2311.17885v1</link><description>Ensemble methods combine the predictions of several base models. We studywhether or not including more models in an ensemble always improve its averageperformance. Such a question depends on the kind of ensemble considered, aswell as the predictive metric chosen. We focus on situations where all membersof the ensemble are a priori expected to perform as well, which is the case ofseveral popular methods like random forests or deep ensembles. In this setting,we essentially show that ensembles are getting better all the time if, and onlyif, the considered loss function is convex. More precisely, in that case, theaverage loss of the ensemble is a decreasing function of the number of models.When the loss function is nonconvex, we show a series of results that can besummarised by the insight that ensembles of good models keep getting better,and ensembles of bad models keep getting worse. To this end, we prove a newresult on the monotonicity of tail probabilities that may be of independentinterest. We illustrate our results on a simple machine learning problem(diagnosing melanomas using neural nets).</description><author>Pierre-Alexandre Mattei, Damien Garreau</author><pubDate>Wed, 29 Nov 2023 18:32:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17885v1</guid></item><item><title>D-CIPHER: Discovery of Closed-form Partial Differential Equations</title><link>http://arxiv.org/abs/2206.10586v3</link><description>Closed-form differential equations, including partial differential equationsand higher-order ordinary differential equations, are one of the most importanttools used by scientists to model and better understand natural phenomena.Discovering these equations directly from data is challenging because itrequires modeling relationships between various derivatives that are notobserved in the data (equation-data mismatch) and it involves searching acrossa huge space of possible equations. Current approaches make strong assumptionsabout the form of the equation and thus fail to discover many well-knownsystems. Moreover, many of them resolve the equation-data mismatch byestimating the derivatives, which makes them inadequate for noisy andinfrequently sampled systems. To this end, we propose D-CIPHER, which is robustto measurement artifacts and can uncover a new and very general class ofdifferential equations. We further design a novel optimization procedure,CoLLie, to help D-CIPHER search through this class efficiently. Finally, wedemonstrate empirically that it can discover many well-known equations that arebeyond the capabilities of current methods.</description><author>Krzysztof Kacprzyk, Zhaozhi Qian, Mihaela van der Schaar</author><pubDate>Wed, 29 Nov 2023 18:23:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10586v3</guid></item><item><title>TSDF-Sampling: Efficient Sampling for Neural Surface Field using Truncated Signed Distance Field</title><link>http://arxiv.org/abs/2311.17878v1</link><description>Multi-view neural surface reconstruction has exhibited impressive results.However, a notable limitation is the prohibitively slow inference time whencompared to traditional techniques, primarily attributed to the dense sampling,required to maintain the rendering quality. This paper introduces a novelapproach that substantially reduces the number of samplings by incorporatingthe Truncated Signed Distance Field (TSDF) of the scene. While prior works haveproposed importance sampling, their dependence on initial uniform samples overthe entire space makes them unable to avoid performance degradation when tryingto use less number of samples. In contrast, our method leverages the TSDFvolume generated only by the trained views, and it proves to provide areasonable bound on the sampling from upcoming novel views. As a result, weachieve high rendering quality by fully exploiting the continuous neural SDFestimation within the bounds given by the TSDF volume. Notably, our method isthe first approach that can be robustly plug-and-play into a diverse array ofneural surface field models, as long as they use the volume renderingtechnique. Our empirical results show an 11-fold increase in inference speedwithout compromising performance. The result videos are available at ourproject page: https://tsdf-sampling.github.io/</description><author>Chaerin Min, Sehyun Cha, Changhee Won, Jongwoo Lim</author><pubDate>Wed, 29 Nov 2023 18:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17878v1</guid></item><item><title>Enhancing Post-Hoc Explanation Benchmark Reliability for Image Classification</title><link>http://arxiv.org/abs/2311.17876v1</link><description>Deep neural networks, while powerful for image classification, often operateas "black boxes," complicating the understanding of their decision-makingprocesses. Various explanation methods, particularly those generating saliencymaps, aim to address this challenge. However, the inconsistency issues offaithfulness metrics hinder reliable benchmarking of explanation methods. Thispaper employs an approach inspired by psychometrics, utilizing Krippendorf'salpha to quantify the benchmark reliability of post-hoc methods in imageclassification. The study proposes model training modifications, includingfeeding perturbed samples and employing focal loss, to enhance robustness andcalibration. Empirical evaluations demonstrate significant improvements inbenchmark reliability across metrics, datasets, and post-hoc methods. Thispioneering work establishes a foundation for more reliable evaluation practicesin the realm of post-hoc explanation methods, emphasizing the importance ofmodel robustness in the assessment process.</description><author>Tristan Gomez, Harold MouchÃ¨re</author><pubDate>Wed, 29 Nov 2023 18:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17876v1</guid></item><item><title>FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information</title><link>http://arxiv.org/abs/2311.17874v1</link><description>This study addresses the challenging problem of active view selection anduncertainty quantification within the domain of Radiance Fields. NeuralRadiance Fields (NeRF) have greatly advanced image rendering andreconstruction, but the limited availability of 2D images poses uncertaintiesstemming from occlusions, depth ambiguities, and imaging errors. Efficientlyselecting informative views becomes crucial, and quantifying NeRF modeluncertainty presents intricate challenges. Existing approaches either depend onmodel architecture or are based on assumptions regarding density distributionsthat are not generally applicable. By leveraging Fisher Information, weefficiently quantify observed information within Radiance Fields without groundtruth data. This can be used for the next best view selection and pixel-wiseuncertainty quantification. Our method overcomes existing limitations on modelarchitecture and effectiveness, achieving state-of-the-art results in both viewselection and uncertainty quantification, demonstrating its potential toadvance the field of Radiance Fields. Our method with the 3D Gaussian Splattingbackend could perform view selections at 70 fps.</description><author>Wen Jiang, Boshu Lei, Kostas Daniilidis</author><pubDate>Wed, 29 Nov 2023 18:20:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17874v1</guid></item><item><title>SAIBench: A Structural Interpretation of AI for Science Through Benchmarks</title><link>http://arxiv.org/abs/2311.17869v1</link><description>Artificial Intelligence for Science (AI4S) is an emerging research field thatutilizes machine learning advancements to tackle complex scientificcomputational issues, aiming to enhance computational efficiency and accuracy.However, the data-driven nature of AI4S lacks the correctness or accuracyassurances of conventional scientific computing, posing challenges whendeploying AI4S models in real-world applications. To mitigate these, morecomprehensive benchmarking procedures are needed to better understand AI4Smodels. This paper introduces a novel benchmarking approach, known asstructural interpretation, which addresses two key requirements: identifyingthe trusted operating range in the problem space and tracing errors back totheir computational components. This method partitions both the problem andmetric spaces, facilitating a structural exploration of these spaces. Thepractical utility and effectiveness of structural interpretation areillustrated through its application to three distinct AI4S workloads:machine-learning force fields (MLFF), jet tagging, and precipitationnowcasting. The benchmarks effectively model the trusted operating range, traceerrors, and reveal novel perspectives for refining the model, training process,and data sampling strategy. This work is part of the SAIBench project, an AI4Sbenchmarking suite.</description><author>Yatao Li, Jianfeng Zhan</author><pubDate>Wed, 29 Nov 2023 18:17:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17869v1</guid></item><item><title>Universalizing Weak Supervision</title><link>http://arxiv.org/abs/2112.03865v3</link><description>Weak supervision (WS) frameworks are a popular way to bypass hand-labelinglarge datasets for training data-hungry models. These approaches synthesizemultiple noisy but cheaply-acquired estimates of labels into a set ofhigh-quality pseudolabels for downstream training. However, the synthesistechnique is specific to a particular kind of label, such as binary labels orsequences, and each new label type requires manually designing a new synthesisalgorithm. Instead, we propose a universal technique that enables weaksupervision over any label type while still offering desirable properties,including practical flexibility, computational efficiency, and theoreticalguarantees. We apply this technique to important problems previously nottackled by WS frameworks including learning to rank, regression, and learningin hyperbolic space. Theoretically, our synthesis approach produces aconsistent estimators for learning some challenging but importantgeneralizations of the exponential family model. Experimentally, we validateour framework and show improvement over baselines in diverse settings includingreal-world learning-to-rank and regression problems along with learning onhyperbolic manifolds.</description><author>Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, Frederic Sala</author><pubDate>Wed, 29 Nov 2023 18:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.03865v3</guid></item><item><title>Mitigating Source Bias for Fairer Weak Supervision</title><link>http://arxiv.org/abs/2303.17713v3</link><description>Weak supervision enables efficient development of training sets by reducingthe need for ground truth labels. However, the techniques that make weaksupervision attractive -- such as integrating any source of signal to estimateunknown labels -- also entail the danger that the produced pseudolabels arehighly biased. Surprisingly, given everyday use and the potential for increasedbias, weak supervision has not been studied from the point of view of fairness.We begin such a study, starting with the observation that even when a fairmodel can be built from a dataset with access to ground-truth labels, thecorresponding dataset labeled via weak supervision can be arbitrarily unfair.To address this, we propose and empirically validate a model for sourceunfairness in weak supervision, then introduce a simple counterfactualfairness-based technique that can mitigate these biases. Theoretically, we showthat it is possible for our approach to simultaneously improve both accuracyand fairness -- in contrast to standard fairness approaches that suffer fromtradeoffs. Empirically, we show that our technique improves accuracy on weaksupervision baselines by as much as 32\% while reducing demographic parity gapby 82.5\%. A simple extension of our method aimed at maximizing performanceproduces state-of-the-art performance in five out of ten datasets in the WRENCHbenchmark.</description><author>Changho Shin, Sonia Cromp, Dyah Adila, Frederic Sala</author><pubDate>Wed, 29 Nov 2023 18:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17713v3</guid></item><item><title>Gaussian Shell Maps for Efficient 3D Human Generation</title><link>http://arxiv.org/abs/2311.17857v1</link><description>Efficient generation of 3D digital humans is important in several industries,including virtual reality, social media, and cinematic production. 3Dgenerative adversarial networks (GANs) have demonstrated state-of-the-art(SOTA) quality and diversity for generated assets. Current 3D GANarchitectures, however, typically rely on volume representations, which areslow to render, thereby hampering the GAN training and requiringmulti-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps(GSMs) as a framework that connects SOTA generator network architectures withemerging 3D Gaussian rendering primitives using an articulable multishell--based scaffold. In this setting, a CNN generates a 3D texture stack withfeatures that are mapped to the shells. The latter represent inflated anddeflated versions of a template surface of a digital human in a canonical bodypose. Instead of rasterizing the shells directly, we sample 3D Gaussians on theshells whose attributes are encoded in the texture features. These Gaussiansare efficiently and differentiably rendered. The ability to articulate theshells is important during GAN training and, at inference time, to deform abody into arbitrary user-defined poses. Our efficient rendering scheme bypassesthe need for view-inconsistent upsamplers and achieves high-quality multi-viewconsistent renderings at a native resolution of $512 \times 512$ pixels. Wedemonstrate that GSMs successfully generate 3D humans when trained onsingle-view datasets, including SHHQ and DeepFashion.</description><author>Rameen Abdal, Wang Yifan, Zifan Shi, Yinghao Xu, Ryan Po, Zhengfei Kuang, Qifeng Chen, Dit-Yan Yeung, Gordon Wetzstein</author><pubDate>Wed, 29 Nov 2023 18:04:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17857v1</guid></item><item><title>Leveraging Graph Diffusion Models for Network Refinement Tasks</title><link>http://arxiv.org/abs/2311.17856v1</link><description>Most real-world networks are noisy and incomplete samples from an unknowntarget distribution. Refining them by correcting corruptions or inferringunobserved regions typically improves downstream performance. Inspired by theimpressive generative capabilities that have been used to correct corruptionsin images, and the similarities between "in-painting" and filling in missingnodes and edges conditioned on the observed graph, we propose a novel graphgenerative framework, SGDM, which is based on subgraph diffusion. Our frameworknot only improves the scalability and fidelity of graph diffusion models, butalso leverages the reverse process to perform novel, conditional generationtasks. In particular, through extensive empirical analysis and a set of novelmetrics, we demonstrate that our proposed model effectively supports thefollowing refinement tasks for partially observable networks: T1: denoisingextraneous subgraphs, T2: expanding existing subgraphs and T3: performing"style" transfer by regenerating a particular subgraph to match thecharacteristics of a different node or subgraph.</description><author>Puja Trivedi, Ryan Rossi, David Arbour, Tong Yu, Franck Dernoncourt, Sungchul Kim, Nedim Lipka, Namyong Park, Nesreen K. Ahmed, Danai Koutra</author><pubDate>Wed, 29 Nov 2023 18:02:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17856v1</guid></item><item><title>Maximum Entropy Model Correction in Reinforcement Learning</title><link>http://arxiv.org/abs/2311.17855v1</link><description>We propose and theoretically analyze an approach for planning with anapproximate model in reinforcement learning that can reduce the adverse impactof model error. If the model is accurate enough, it accelerates the convergenceto the true value function too. One of its key components is the MaxEnt ModelCorrection (MoCo) procedure that corrects the model's next-state distributionsbased on a Maximum Entropy density estimation formulation. Based on MoCo, weintroduce the Model Correcting Value Iteration (MoCoVI) algorithm, and itssampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergencecan be much faster than the conventional model-free algorithms. Unliketraditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize anapproximate model and still converge to the correct value function.</description><author>Amin Rakhsha, Mete Kemertas, Mohammad Ghavamzadeh, Amir-massoud Farahmand</author><pubDate>Wed, 29 Nov 2023 18:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17855v1</guid></item><item><title>On the Adversarial Robustness of Graph Contrastive Learning Methods</title><link>http://arxiv.org/abs/2311.17853v1</link><description>Contrastive learning (CL) has emerged as a powerful framework for learningrepresentations of images and text in a self-supervised manner while enhancingmodel robustness against adversarial attacks. More recently, researchers haveextended the principles of contrastive learning to graph-structured data,giving birth to the field of graph contrastive learning (GCL). However, whetherGCL methods can deliver the same advantages in adversarial robustness as theircounterparts in the image and text domains remains an open question. In thispaper, we introduce a comprehensive robustness evaluation protocol tailored toassess the robustness of GCL models. We subject these models to adaptiveadversarial attacks targeting the graph structure, specifically in the evasionscenario. We evaluate node and graph classification tasks using diversereal-world datasets and attack strategies. With our work, we aim to offerinsights into the robustness of GCL methods and hope to open avenues forpotential future research directions.</description><author>Filippo Guerranti, Zinuo Yi, Anna Starovoit, Rafiq Kamel, Simon Geisler, Stephan GÃ¼nnemann</author><pubDate>Wed, 29 Nov 2023 17:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17853v1</guid></item><item><title>Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects</title><link>http://arxiv.org/abs/2311.17851v1</link><description>Unlabeled 3D objects present an opportunity to leverage pretrained visionlanguage models (VLMs) on a range of annotation tasks -- from describing objectsemantics to physical properties. An accurate response must take into accountthe full appearance of the object in 3D, various ways of phrasing thequestion/prompt, and changes in other factors that affect the response. Wepresent a method to marginalize over any factors varied across VLM queries,utilizing the VLM's scores for sampled responses. We first show that thisprobabilistic aggregation can outperform a language model (e.g., GPT4) forsummarization, for instance avoiding hallucinations when there are contrastingdetails between responses. Secondly, we show that aggregated annotations areuseful for prompt-chaining; they help improve downstream VLM predictions (e.g.,of object material when the object's type is specified as an auxiliary input inthe prompt). Such auxiliary inputs allow ablating and measuring thecontribution of visual reasoning over language-only reasoning. Using theseevaluations, we show how VLMs can approach, without additional training orin-context learning, the quality of human-verified type and materialannotations on the large-scale Objaverse dataset.</description><author>Rishabh Kabra, Loic Matthey, Alexander Lerchner, Niloy J. Mitra</author><pubDate>Wed, 29 Nov 2023 17:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17851v1</guid></item><item><title>SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</title><link>http://arxiv.org/abs/2311.12775v2</link><description>We propose a method to allow precise and extremely fast mesh extraction from3D Gaussian Splatting. Gaussian Splatting has recently become very popular asit yields realistic rendering while being significantly faster to train thanNeRFs. It is however challenging to extract a mesh from the millions of tiny 3Dgaussians as these gaussians tend to be unorganized after optimization and nomethod has been proposed so far. Our first key contribution is a regularizationterm that encourages the gaussians to align well with the surface of the scene.We then introduce a method that exploits this alignment to extract a mesh fromthe Gaussians using Poisson reconstruction, which is fast, scalable, andpreserves details, in contrast to the Marching Cubes algorithm usually appliedto extract meshes from Neural SDFs. Finally, we introduce an optionalrefinement strategy that binds gaussians to the surface of the mesh, andjointly optimizes these Gaussians and the mesh through Gaussian splattingrendering. This enables easy editing, sculpting, rigging, animating,compositing and relighting of the Gaussians using traditional softwares bymanipulating the mesh instead of the gaussians themselves. Retrieving such aneditable mesh for realistic rendering is done within minutes with our method,compared to hours with the state-of-the-art methods on neural SDFs, whileproviding a better rendering quality. Our project page is the following:https://imagine.enpc.fr/~guedona/sugar/</description><author>Antoine GuÃ©don, Vincent Lepetit</author><pubDate>Wed, 29 Nov 2023 17:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12775v2</guid></item><item><title>Towards Real-World Focus Stacking with Deep Learning</title><link>http://arxiv.org/abs/2311.17846v1</link><description>Focus stacking is widely used in micro, macro, and landscape photography toreconstruct all-in-focus images from multiple frames obtained with focusbracketing, that is, with shallow depth of field and different focus planes.Existing deep learning approaches to the underlying multi-focus image fusionproblem have limited applicability to real-world imagery since they aredesigned for very short image sequences (two to four images), and are typicallytrained on small, low-resolution datasets either acquired by light-fieldcameras or generated synthetically. We introduce a new dataset consisting of 94high-resolution bursts of raw images with focus bracketing, with pseudo groundtruth computed from the data using state-of-the-art commercial software. Thisdataset is used to train the first deep learning algorithm for focus stackingcapable of handling bursts of sufficient length for real-world applications.Qualitative experiments demonstrate that it is on par with existing commercialsolutions in the long-burst, realistic regime while being significantly moretolerant to noise. The code and dataset are available athttps://github.com/araujoalexandre/FocusStackingDataset.</description><author>Alexandre Araujo, Jean Ponce, Julien Mairal</author><pubDate>Wed, 29 Nov 2023 17:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17846v1</guid></item><item><title>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning</title><link>http://arxiv.org/abs/2311.17842v1</link><description>In this study, we are interested in imbuing robots with the capability ofphysically-grounded task planning. Recent advancements have shown that largelanguage models (LLMs) possess extensive knowledge useful in robotic tasks,especially in reasoning and planning. However, LLMs are constrained by theirlack of world grounding and dependence on external affordance models toperceive environmental information, which cannot jointly reason with LLMs. Weargue that a task planner should be an inherently grounded, unified multimodalsystem. To this end, we introduce Robotic Vision-Language Planning (ViLa), anovel approach for long-horizon robotic planning that leverages vision-languagemodels (VLMs) to generate a sequence of actionable steps. ViLa directlyintegrates perceptual data into its reasoning and planning process, enabling aprofound understanding of commonsense knowledge in the visual world, includingspatial layouts and object attributes. It also supports flexible multimodalgoal specification and naturally incorporates visual feedback. Our extensiveevaluation, conducted in both real-robot and simulated environments,demonstrates ViLa's superiority over existing LLM-based planners, highlightingits effectiveness in a wide array of open-world manipulation tasks.</description><author>Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao</author><pubDate>Wed, 29 Nov 2023 17:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17842v1</guid></item><item><title>A quasi-polynomial time algorithm for Multi-Dimensional Scaling via LP hierarchies</title><link>http://arxiv.org/abs/2311.17840v1</link><description>Multi-dimensional Scaling (MDS) is a family of methods for embeddingpair-wise dissimilarities between $n$ objects into low-dimensional space. MDSis widely used as a data visualization tool in the social and biologicalsciences, statistics, and machine learning. We study the Kamada-Kawaiformulation of MDS: given a set of non-negative dissimilarities $\{d_{i,j}\}_{i, j \in [n]}$ over $n$ points, the goal is to find an embedding$\{x_1,\dots,x_n\} \subset \mathbb{R}^k$ that minimizes \[ \text{OPT} =\min_{x} \mathbb{E}_{i,j \in [n]} \left[ \left(1-\frac{\|x_i -x_j\|}{d_{i,j}}\right)^2 \right] \] Despite its popularity, our theoretical understanding of MDS is extremelylimited. Recently, Demaine, Hesterberg, Koehler, Lynch, and Urschel(arXiv:2109.11505) gave the first approximation algorithm with provableguarantees for Kamada-Kawai, which achieves an embedding with cost $\text{OPT}+\epsilon$ in $n^2 \cdot 2^{\tilde{\mathcal{O}}(k \Delta^4 / \epsilon^2)}$time, where $\Delta$ is the aspect ratio of the input dissimilarities. In thiswork, we give the first approximation algorithm for MDS with quasi-polynomialdependency on $\Delta$: for target dimension $k$, we achieve a solution withcost $\mathcal{O}(\text{OPT}^{ \hspace{0.04in}1/k } \cdot \log(\Delta/\epsilon))+ \epsilon$ in time $n^{ \mathcal{O}(1)} \cdot 2^{\tilde{\mathcal{O}}( k^2(\log(\Delta)/\epsilon)^{k/2 + 1} ) }$. Our approach is based on a novel analysis of a conditioning-based roundingscheme for the Sherali-Adams LP Hierarchy. Crucially, our analysis exploits thegeometry of low-dimensional Euclidean space, allowing us to avoid anexponential dependence on the aspect ratio $\Delta$. We believe ourgeometry-aware treatment of the Sherali-Adams Hierarchy is an important steptowards developing general-purpose techniques for efficient metric optimizationalgorithms.</description><author>Ainesh Bakshi, Vincent Cohen-Addad, Samuel B. Hopkins, Rajesh Jayaram, Silvio Lattanzi</author><pubDate>Wed, 29 Nov 2023 17:42:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17840v1</guid></item><item><title>Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability</title><link>http://arxiv.org/abs/2310.11518v3</link><description>Self-play is a technique for machine learning in multi-agent systems where alearning algorithm learns by interacting with copies of itself. Self-play isuseful for generating large quantities of data for learning, but has thedrawback that the agents the learner will face post-training may havedramatically different behavior than the learner came to expect by interactingwith itself. For the special case of two-player constant-sum games, self-playthat reaches Nash equilibrium is guaranteed to produce strategies that performwell against any post-training opponent; however, no such guarantee exists formultiplayer games. We show that in games that approximately decompose into aset of two-player constant-sum games (called constant-sum polymatrix games)where global $\epsilon$-Nash equilibria are boundedly far from Nash equilibriain each subgame (called subgame stability), any no-external-regret algorithmthat learns by self-play will produce a strategy with bounded vulnerability.For the first time, our results identify a structural property of multiplayergames that enable performance guarantees for the strategies produced by a broadclass of self-play algorithms. We demonstrate our findings through experimentson Leduc poker.</description><author>Revan MacQueen, James R. Wright</author><pubDate>Wed, 29 Nov 2023 17:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11518v3</guid></item><item><title>SPiC-E : Structural Priors in 3D Diffusion Models using Cross Entity Attention</title><link>http://arxiv.org/abs/2311.17834v1</link><description>We are witnessing rapid progress in automatically generating and manipulating3D assets due to the availability of pretrained text-image diffusion models.However, time-consuming optimization procedures are required for synthesizingeach sample, hindering their potential for democratizing 3D content creation.Conversely, 3D diffusion models now train on million-scale 3D datasets,yielding high-quality text-conditional 3D samples within seconds. In this work,we present SPiC-E - a neural network that adds structural guidance to 3Ddiffusion models, extending their usage beyond text-conditional generation. Atits core, our framework introduces a cross-entity attention mechanism thatallows for multiple entities (in particular, paired input and guidance 3Dshapes) to interact via their internal representations within the denoisingnetwork. We utilize this mechanism for learning task-specific structural priorsin 3D diffusion models from auxiliary guidance shapes. We show that ourapproach supports a variety of applications, including 3D stylization, semanticshape editing and text-conditional abstraction-to-3D, which transformsprimitive-based abstractions into highly-expressive shapes. Extensiveexperiments demonstrate that SPiC-E achieves SOTA performance over these taskswhile often being considerably faster than alternative methods. Importantly,this is accomplished without tailoring our approach for any specific task.</description><author>Etai Sella, Gal Fiebelman, Noam Atia, Hadar Averbuch-Elor</author><pubDate>Wed, 29 Nov 2023 17:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17834v1</guid></item><item><title>Analyzing and Explaining Image Classifiers via Diffusion Guidance</title><link>http://arxiv.org/abs/2311.17833v1</link><description>While deep learning has led to huge progress in complex image classificationtasks like ImageNet, unexpected failure modes, e.g. via spurious features, callinto question how reliably these classifiers work in the wild. Furthermore, forsafety-critical tasks the black-box nature of their decisions is problematic,and explanations or at least methods which make decisions plausible are neededurgently. In this paper, we address these problems by generating images thatoptimize a classifier-derived objective using a framework for guided imagegeneration. We analyze the behavior and decisions of image classifiers byvisual counterfactual explanations (VCEs), detection of systematic mistakes byanalyzing images where classifiers maximally disagree, and visualization ofneurons to verify potential spurious features. In this way, we validateexisting observations, e.g. the shape bias of adversarially robust models, aswell as novel failure modes, e.g. systematic errors of zero-shot CLIPclassifiers, or identify harmful spurious features. Moreover, our VCEsoutperform previous work while being more versatile.</description><author>Maximilian Augustin, Yannic Neuhaus, Matthias Hein</author><pubDate>Wed, 29 Nov 2023 17:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17833v1</guid></item><item><title>Anomalous Behavior Detection in Trajectory Data of Older Drivers</title><link>http://arxiv.org/abs/2311.17822v1</link><description>Given a road network and a set of trajectory data, the anomalous behaviordetection (ABD) problem is to identify drivers that show significantdirectional deviations, hardbrakings, and accelerations in their trips. The ABDproblem is important in many societal applications, including Mild CognitiveImpairment (MCI) detection and safe route recommendations for older drivers.The ABD problem is computationally challenging due to the large size oftemporally-detailed trajectories dataset. In this paper, we propose anEdge-Attributed Matrix that can represent the key properties oftemporally-detailed trajectory datasets and identify abnormal drivingbehaviors. Experiments using real-world datasets demonstrated that our approachidentifies abnormal driving behaviors.</description><author>Seyedeh Gol Ara Ghoreishi, Sonia Moshfeghi, Muhammad Tanveer Jan, Joshua Conniff, KwangSoo Yang, Jinwoo Jang, Borko Furht, Ruth Tappen, David Newman, Monica Rosselli, Jiannan Zhai</author><pubDate>Wed, 29 Nov 2023 17:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17822v1</guid></item><item><title>Building Open Knowledge Graph for Metal-Organic Frameworks (MOF-KG): Challenges and Case Studies</title><link>http://arxiv.org/abs/2207.04502v2</link><description>Metal-Organic Frameworks (MOFs) are a class of modular, porous crystallinematerials that have great potential to revolutionize applications such as gasstorage, molecular separations, chemical sensing, catalysis, and drug delivery.The Cambridge Structural Database (CSD) reports 10,636 synthesized MOF crystalswhich in addition contains ca. 114,373 MOF-like structures. The sheer number ofsynthesized (plus potentially synthesizable) MOF structures requiresresearchers pursue computational techniques to screen and isolate MOFcandidates. In this demo paper, we describe our effort on leveraging knowledgegraph methods to facilitate MOF prediction, discovery, and synthesis. Wepresent challenges and case studies about (1) construction of a MOF knowledgegraph (MOF-KG) from structured and unstructured sources and (2) leveraging theMOF-KG for discovery of new or missing knowledge.</description><author>Yuan An, Jane Greenberg, Xintong Zhao, Xiaohua Hu, Scott McCLellan, Alex Kalinowski, Fernando J. Uribe-Romo, Kyle Langlois, Jacob Furst, Diego A. GÃ³mez-GualdrÃ³n, Fernando Fajardo-Rojas, Katherine Ardila</author><pubDate>Wed, 29 Nov 2023 17:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04502v2</guid></item><item><title>A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures</title><link>http://arxiv.org/abs/2311.17815v1</link><description>In recent years, the field of Deep Learning has seen many disruptive andimpactful advancements. Given the increasing complexity of deep neuralnetworks, the need for efficient hardware accelerators has become more and morepressing to design heterogeneous HPC platforms. The design of Deep Learningaccelerators requires a multidisciplinary approach, combining expertise fromseveral areas, spanning from computer architecture to approximate computing,computational models, and machine learning algorithms. Several methodologiesand tools have been proposed to design accelerators for Deep Learning,including hardware-software co-design approaches, high-level synthesis methods,specific customized compilers, and methodologies for design space exploration,modeling, and simulation. These methodologies aim to maximize the exploitableparallelism and minimize data movement to achieve high performance and energyefficiency. This survey provides a holistic review of the most influentialdesign methodologies and EDA tools proposed in recent years to implement DeepLearning accelerators, offering the reader a wide perspective in this rapidlyevolving field. In particular, this work complements the previous surveyproposed by the same authors in [203], which focuses on Deep Learning hardwareaccelerators for heterogeneous HPC platforms.</description><author>Fabrizio Ferrandi, Serena Curzel, Leandro Fiorin, Daniele Ielmini, Cristina Silvano, Francesco Conti, Alessio Burrello, Francesco Barchi, Luca Benini, Luciano Lavagno, Teodoro Urso, Enrico Calore, Sebastiano Fabio Schifano, Cristian Zambelli, Maurizio Palesi, Giuseppe Ascia, Enrico Russo, Nicola Petra, Davide De Caro, Gennaro Di Meo, Valeria Cardellini, Salvatore Filippone, Francesco Lo Presti, Francesco Silvestri, Paolo Palazzari, Stefania Perri</author><pubDate>Wed, 29 Nov 2023 17:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17815v1</guid></item><item><title>Higher-Order DisCoCat (Peirce-Lambek-Montague semantics)</title><link>http://arxiv.org/abs/2311.17813v1</link><description>We propose a new definition of higher-order DisCoCat (categoricalcompositional distributional) models where the meaning of a word is not adiagram, but a diagram-valued higher-order function. Our models can be seen asa variant of Montague semantics based on a lambda calculus where the primitivesact on string diagrams rather than logical formulae. As a special case, we showhow to translate from the Lambek calculus into Peirce's system beta forfirst-order logic. This allows us to give a purely diagrammatic treatment ofhigher-order and non-linear processes in natural language semantics: adverbs,prepositions, negation and quantifiers. The theoretical definition presented inthis article comes with a proof-of-concept implementation in DisCoPy, thePython library for string diagrams.</description><author>Alexis Toumi, Giovanni de Felice</author><pubDate>Wed, 29 Nov 2023 17:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17813v1</guid></item><item><title>DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2311.17812v1</link><description>Following language instructions to navigate in unseen environments is achallenging task for autonomous embodied agents. With strong representationcapabilities, pretrained vision-and-language models are widely used in VLN.However, most of them are trained on web-crawled general-purpose datasets,which incurs a considerable domain gap when used for VLN tasks. To address theproblem, we propose a novel and model-agnostic domain-aware prompt learning(DAP) framework. For equipping the pretrained models with specific object-leveland scene-level cross-modal alignment in VLN tasks, DAP applies a low-costprompt tuning paradigm to learn soft visual prompts for extracting in-domainimage semantics. Specifically, we first generate a set of in-domain image-textpairs with the help of the CLIP model. Then we introduce soft visual prompts inthe input space of the visual encoder in a pretrained model. DAP injectsin-domain visual knowledge into the visual encoder of the pretrained model inan efficient way. Experimental results on both R2R and REVERIE show thesuperiority of DAP compared to existing state-of-the-art methods.</description><author>Ting Liu, Yue Hu, Wansen Wu, Youkai Wang, Kai Xu, Quanjun Yin</author><pubDate>Wed, 29 Nov 2023 17:03:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17812v1</guid></item><item><title>Cross-Axis Transformer with 2D Rotary Embeddings</title><link>http://arxiv.org/abs/2311.07184v2</link><description>Despite lagging behind their modal cousins in many respects, VisionTransformers have provided an interesting opportunity to bridge the gap betweensequence modeling and image modeling. Up until now however, vision transformershave largely been held back, due to both computational inefficiency, and lackof proper handling of spatial dimensions. In this paper, we introduce theCross-Axis Transformer. CAT is a model inspired by both Axial Transformers, andMicrosoft's recent Retentive Network, that drastically reduces the requirednumber of floating point operations required to process an image, whilesimultaneously converging faster and more accurately than the VisionTransformers it replaces.</description><author>Lily Erickson</author><pubDate>Wed, 29 Nov 2023 17:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07184v2</guid></item><item><title>Coloring the Past: Neural Historical Buildings Reconstruction from Archival Photography</title><link>http://arxiv.org/abs/2311.17810v1</link><description>Historical buildings are a treasure and milestone of human cultural heritage.Reconstructing the 3D models of these building hold significant value. Therapid development of neural rendering methods makes it possible to recover the3D shape only based on archival photographs. However, this task presentsconsiderable challenges due to the limitations of such datasets. Historicalphotographs are often limited in number and the scenes in these photos mighthave altered over time. The radiometric quality of these images is also oftensub-optimal. To address these challenges, we introduce an approach toreconstruct the geometry of historical buildings, employing volumetricrendering techniques. We leverage dense point clouds as a geometric prior andintroduce a color appearance embedding loss to recover the color of thebuilding given limited available color images. We aim for our work to sparkincreased interest and focus on preserving historical buildings. Thus, we alsointroduce a new historical dataset of the Hungarian National Theater, providinga new benchmark for the reconstruction method.</description><author>David Komorowicz, Lu Sang, Ferdinand Maiwald, Daniel Cremers</author><pubDate>Wed, 29 Nov 2023 16:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17810v1</guid></item><item><title>Aggregation Model Hyperparameters Matter in Digital Pathology</title><link>http://arxiv.org/abs/2311.17804v1</link><description>Digital pathology has significantly advanced disease detection andpathologist efficiency through the analysis of gigapixel whole-slide images(WSI). In this process, WSIs are first divided into patches, for which afeature extractor model is applied to obtain feature vectors, which aresubsequently processed by an aggregation model to predict the respective WSIlabel. With the rapid evolution of representation learning, numerous newfeature extractor models, often termed foundational models, have emerged.Traditional evaluation methods, however, rely on fixed aggregation modelhyperparameters, a framework we identify as potentially biasing the results.Our study uncovers a co-dependence between feature extractor models andaggregation model hyperparameters, indicating that performance comparabilitycan be skewed based on the chosen hyperparameters. By accounting for thisco-dependency, we find that the performance of many current feature extractormodels is notably similar. We support this insight by evaluating seven featureextractor models across three different datasets with 162 different aggregationmodel configurations. This comprehensive approach provides a more nuancedunderstanding of the relationship between feature extractors and aggregationmodels, leading to a fairer and more accurate assessment of feature extractormodels in digital pathology.</description><author>Gustav Bredell, Marcel Fischer, Przemyslaw Szostak, Samaneh Abbasi-Sureshjani, Alvaro Gomariz</author><pubDate>Wed, 29 Nov 2023 16:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17804v1</guid></item><item><title>The Rise of the AI Co-Pilot: Lessons for Design from Aviation and Beyond</title><link>http://arxiv.org/abs/2311.14713v2</link><description>The fast pace of advances in AI promises to revolutionize various aspects ofknowledge work, extending its influence to daily life and professional fieldsalike. We advocate for a paradigm where AI is seen as a collaborative co-pilot,working under human guidance rather than as a mere tool. Drawing from relevantresearch and literature in the disciplines of Human-Computer Interaction andHuman Factors Engineering, we highlight the criticality of maintaining humanoversight in AI interactions. Reflecting on lessons from aviation, we addressthe dangers of over-relying on automation, such as diminished human vigilanceand skill erosion. Our paper proposes a design approach that emphasizes activehuman engagement, control, and skill enhancement in the AI partnership, aimingto foster a harmonious, effective, and empowering human-AI relationship. Weparticularly call out the critical need to design AI interaction capabilitiesand software applications to enable and celebrate the primacy of human agency.This calls for designs for human-AI partnership that cede ultimate control andresponsibility to the human user as pilot, with the AI co-pilot acting in awell-defined supporting role.</description><author>Abigail Sellen, Eric Horvitz</author><pubDate>Wed, 29 Nov 2023 16:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14713v2</guid></item><item><title>Towards Efficient Hyperdimensional Computing Using Photonics</title><link>http://arxiv.org/abs/2311.17801v1</link><description>Over the past few years, silicon photonics-based computing has emerged as apromising alternative to CMOS-based computing for Deep Neural Networks (DNN).Unfortunately, the non-linear operations and the high-precision requirements ofDNNs make it extremely challenging to design efficient silicon photonics-basedsystems for DNN inference and training. Hyperdimensional Computing (HDC) is anemerging, brain-inspired machine learning technique that enjoys severaladvantages over existing DNNs, including being lightweight, requiringlow-precision operands, and being robust to noise introduced by thenonidealities in the hardware. For HDC, computing in-memory (CiM) approacheshave been widely used, as CiM reduces the data transfer cost if the operandscan fit into the memory. However, inefficient multi-bit operations, high writelatency, and low endurance make CiM ill-suited for HDC. On the other hand, theexisting electro-photonic DNN accelerators are inefficient for HDC because theyare specifically optimized for matrix multiplication in DNNs and consume a lotof power with high-precision data converters. In this paper, we argue that photonic computing and HDC complement each otherbetter than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,the first-ever electro-photonic accelerator for HDC training and inference,supporting the basic, record-based, and graph encoding schemes. Evaluating withpopular datasets, we show that our accelerator can achieve two to five ordersof magnitude lower EDP than the state-of-the-art electro-photonic DNNaccelerators for implementing HDC training and inference. PhotoHDC alsoachieves four orders of magnitude lower energy-delay product than CiM-basedaccelerators for both HDC training and inference.</description><author>Farbin Fayza, Cansu Demirkiran, Hanning Chen, Che-Kai Liu, Avi Mohan, Hamza Errahmouni, Sanggeon Yun, Mohsen Imani, David Zhang, Darius Bunandar, Ajay Joshi</author><pubDate>Wed, 29 Nov 2023 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17801v1</guid></item><item><title>Learning to Simulate: Generative Metamodeling via Quantile Regression</title><link>http://arxiv.org/abs/2311.17797v1</link><description>Stochastic simulation models, while effective in capturing the dynamics ofcomplex systems, are often too slow to run for real-time decision-making.Metamodeling techniques are widely used to learn the relationship between asummary statistic of the outputs (e.g., the mean or quantile) and the inputs ofthe simulator, so that it can be used in real time. However, this methodologyrequires the knowledge of an appropriate summary statistic in advance, makingit inflexible for many practical situations. In this paper, we propose a newmetamodeling concept, called generative metamodeling, which aims to construct a"fast simulator of the simulator". This technique can generate random outputssubstantially faster than the original simulation model, while retaining anapproximately equal conditional distribution given the same inputs. Onceconstructed, a generative metamodel can instantaneously generate a large amountof random outputs as soon as the inputs are specified, thereby facilitating theimmediate computation of any summary statistic for real-time decision-making.Furthermore, we propose a new algorithm -- quantile-regression-based generativemetamodeling (QRGMM) -- and study its convergence and rate of convergence.Extensive numerical experiments are conducted to investigate the empiricalperformance of QRGMM, compare it with other state-of-the-art generativealgorithms, and demonstrate its usefulness in practical real-timedecision-making.</description><author>L. Jeff Hong, Yanxi Hou, Qingkai Zhang, Xiaowei Zhang</author><pubDate>Wed, 29 Nov 2023 16:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17797v1</guid></item><item><title>Marginal Laplacian Score</title><link>http://arxiv.org/abs/2311.17795v1</link><description>High-dimensional imbalanced data poses a machine learning challenge. In theabsence of sufficient or high-quality labels, unsupervised feature selectionmethods are crucial for the success of subsequent algorithms. Therefore, thereis a growing need for unsupervised feature selection algorithms focused onimbalanced data. Thus, we propose a Marginal Laplacian Score (MLS) amodification of the well-known Laplacian Score (LS) to be better suited forimbalance data. We introduce an assumption that the minority class or anomalousappear more frequently in the margin of the features. Consequently, MLS aims topreserve the local structure of the data set's margin. As MLS is better suitedfor handling imbalanced data, we propose its integration into modern featureselection methods that utilize the Laplacian score. We integrate the MLSalgorithm into the Differentiable Unsupervised Feature Selection (DUFS),resulting in DUFS-MLS. The proposed methods demonstrate robust and improvedperformance on synthetic and public data sets.</description><author>Guy Hay, Ohad Volk</author><pubDate>Wed, 29 Nov 2023 16:45:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17795v1</guid></item><item><title>BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer</title><link>http://arxiv.org/abs/2305.12534v3</link><description>We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL)based fuzzer aimed at finding security vulnerabilities for Web applications.BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performsgrammar-adhering and attack-provoking mutation operations on them to generatecandidate attack vectors. The key insight of BertRLFuzzer is the use of RL witha BERT model as an agent to guide the fuzzer to efficiently learngrammar-adhering and attack-provoking mutation operators. In order to establishthe efficacy of BertRLFuzzer we compare it against a total of 13 black box andwhite box fuzzers over a benchmark of 9 victim websites with over 16K LOC. Weobserved a significant improvement relative to the nearest competing tool interms of time to first attack (54% less), new vulnerabilities found (17 newvulnerabilities), and attack rate (4.4% more attack vectors generated).</description><author>Piyush Jha, Joseph Scott, Jaya Sriram Ganeshna, Mudit Singh, Vijay Ganesh</author><pubDate>Wed, 29 Nov 2023 16:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12534v3</guid></item><item><title>U-Net v2: Rethinking the Skip Connections of U-Net for Medical Image Segmentation</title><link>http://arxiv.org/abs/2311.17791v1</link><description>In this paper, we introduce U-Net v2, a new robust and efficient U-Netvariant for medical image segmentation. It aims to augment the infusion ofsemantic information into low-level features while simultaneously refininghigh-level features with finer details. For an input image, we begin byextracting multi-level features with a deep neural network encoder. Next, weenhance the feature map of each level by infusing semantic information fromhigher-level features and integrating finer details from lower-level featuresthrough Hadamard product. Our novel skip connections empower features of allthe levels with enriched semantic characteristics and intricate details. Theimproved features are subsequently transmitted to the decoder for furtherprocessing and segmentation. Our method can be seamlessly integrated into anyEncoder-Decoder network. We evaluate our method on several public medical imagesegmentation datasets for skin lesion segmentation and polyp segmentation, andthe experimental results demonstrate the segmentation accuracy of our newmethod over state-of-the-art methods, while preserving memory and computationalefficiency. Code is available at: https://github.com/yaoppeng/U-Net\_v2</description><author>Yaopeng Peng, Milan Sonka, Danny Z. Chen</author><pubDate>Wed, 29 Nov 2023 16:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17791v1</guid></item><item><title>Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models</title><link>http://arxiv.org/abs/2310.09949v3</link><description>A Retrieval-Augmented Language Model (RALM) augments a generative languagemodel by retrieving context-specific knowledge from an external database. Thisstrategy facilitates impressive text generation quality even with smallermodels, thus reducing orders of magnitude of computational demands. However,RALMs introduce unique system design challenges due to (a) the diverse workloadcharacteristics between LM inference and retrieval and (b) the various systemrequirements and bottlenecks for different RALM configurations such as modelsizes, database sizes, and retrieval frequencies. We propose Chameleon, aheterogeneous accelerator system that integrates both LM and retrievalaccelerators in a disaggregated architecture. The heterogeneity ensuresefficient acceleration of both LM inference and retrieval, while theaccelerator disaggregation enables the system to independently scale both typesof accelerators to fulfill diverse RALM requirements. Our Chameleon prototypeimplements retrieval accelerators on FPGAs and assigns LM inference to GPUs,with a CPU server orchestrating these accelerators over the network. Comparedto CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72xspeedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleonexhibits up to 2.16x reduction in latency and 3.18x speedup in throughputcompared to the hybrid CPU-GPU architecture. These promising results pave theway for bringing accelerator heterogeneity and disaggregation into future RALMsystems.</description><author>Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo Alonso</author><pubDate>Wed, 29 Nov 2023 16:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09949v3</guid></item><item><title>DSS: Synthesizing long Digital Ink using Data augmentation, Style encoding and Split generation</title><link>http://arxiv.org/abs/2311.17786v1</link><description>As text generative models can give increasingly long answers, we tackle theproblem of synthesizing long text in digital ink. We show that the commonlyused models for this task fail to generalize to long-form data and how thisproblem can be solved by augmenting the training data, changing the modelarchitecture and the inference procedure. These methods use contrastivelearning technique and are tailored specifically for the handwriting domain.They can be applied to any encoder-decoder model that works with digital ink.We demonstrate that our method reduces the character error rate on long-formEnglish data by half compared to baseline RNN and by 16% compared to theprevious approach that aims at addressing the same problem. We show that allthree parts of the method improve recognizability of generated inks. Inaddition, we evaluate synthesized data in a human study and find that peopleperceive most of generated data as real.</description><author>Aleksandr Timofeev, Anastasiia Fadeeva, Andrei Afonin, Claudiu Musat, Andrii Maksai</author><pubDate>Wed, 29 Nov 2023 16:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17786v1</guid></item><item><title>Propagate &amp; Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs</title><link>http://arxiv.org/abs/2311.17781v1</link><description>Recent studies attempted to utilize multilayer perceptrons (MLPs) to solvesemisupervised node classification on graphs, by training a student MLP byknowledge distillation from a teacher graph neural network (GNN). Whileprevious studies have focused mostly on training the student MLP by matchingthe output probability distributions between the teacher and student modelsduring distillation, it has not been systematically studied how to inject thestructural information in an explicit and interpretable manner. Inspired byGNNs that separate feature transformation $T$ and propagation $\Pi$, were-frame the distillation process as making the student MLP learn both $T$ and$\Pi$. Although this can be achieved by applying the inverse propagation$\Pi^{-1}$ before distillation from the teacher, it still comes with a highcomputational cost from large matrix multiplications during training. To solvethis problem, we propose Propagate &amp; Distill (P&amp;D), which propagates the outputof the teacher before distillation, which can be interpreted as an approximateprocess of the inverse propagation. We demonstrate that P&amp;D can readily improvethe performance of the student MLP.</description><author>Yong-Min Shin, Won-Yong Shin</author><pubDate>Wed, 29 Nov 2023 16:26:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17781v1</guid></item><item><title>Unified Binary and Multiclass Margin-Based Classification</title><link>http://arxiv.org/abs/2311.17778v1</link><description>The notion of margin loss has been central to the development and analysis ofalgorithms for binary classification. To date, however, there remains noconsensus as to the analogue of the margin loss for multiclass classification.In this work, we show that a broad range of multiclass loss functions,including many popular ones, can be expressed in the relative margin form, ageneralization of the margin form of binary losses. The relative margin form isbroadly useful for understanding and analyzing multiclass losses as shown byour prior work (Wang and Scott, 2020, 2021). To further demonstrate the utilityof this way of expressing multiclass losses, we use it to extend the seminalresult of Bartlett et al. (2006) on classification-calibration of binary marginlosses to multiclass. We then analyze the class of Fenchel-Young losses, andexpand the set of these losses that are known to be classification-calibrated.</description><author>Yutong Wang, Clayton Scott</author><pubDate>Wed, 29 Nov 2023 16:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17778v1</guid></item><item><title>HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation</title><link>http://arxiv.org/abs/2310.01406v2</link><description>Recent text-to-3D methods employing diffusion models have made significantadvancements in 3D human generation. However, these approaches face challengesdue to the limitations of text-to-image diffusion models, which lack anunderstanding of 3D structures. Consequently, these methods struggle to achievehigh-quality human generation, resulting in smooth geometry and cartoon-likeappearances. In this paper, we propose HumanNorm, a novel approach forhigh-quality and realistic 3D human generation. The main idea is to enhance themodel's 2D perception of 3D geometry by learning a normal-adapted diffusionmodel and a normal-aligned diffusion model. The normal-adapted diffusion modelcan generate high-fidelity normal maps corresponding to user prompts withview-dependent and body-aware text. The normal-aligned diffusion model learnsto generate color images aligned with the normal maps, thereby transformingphysical geometry details into realistic appearance. Leveraging the proposednormal diffusion model, we devise a progressive geometry generation strategyand a multi-step Score Distillation Sampling (SDS) loss to enhance theperformance of 3D human generation. Comprehensive experiments substantiateHumanNorm's ability to generate 3D humans with intricate geometry and realisticappearances. HumanNorm outperforms existing text-to-3D methods in both geometryand texture quality. The project page of HumanNorm ishttps://humannorm.github.io/.</description><author>Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, Qing Wang</author><pubDate>Wed, 29 Nov 2023 16:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01406v2</guid></item><item><title>One-Shot Open Affordance Learning with Foundation Models</title><link>http://arxiv.org/abs/2311.17776v1</link><description>We introduce One-shot Open Affordance Learning (OOAL), where a model istrained with just one example per base object category, but is expected toidentify novel objects and affordances. While vision-language models excel atrecognizing novel objects and scenes, they often struggle to understand finerlevels of granularity such as affordances. To handle this issue, we conduct acomprehensive analysis of existing foundation models, to explore their inherentunderstanding of affordances and assess the potential for data-limitedaffordance learning. We then propose a vision-language framework with simpleand effective designs that boost the alignment between visual features andaffordance text embeddings. Experiments on two affordance segmentationbenchmarks show that the proposed method outperforms state-of-the-art modelswith less than 1% of the full training data, and exhibits reasonablegeneralization capability on unseen objects and affordances.</description><author>Gen Li, Deqing Sun, Laura Sevilla-Lara, Varun Jampani</author><pubDate>Wed, 29 Nov 2023 16:23:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17776v1</guid></item><item><title>Corruption-Robust Lipschitz Contextual Search</title><link>http://arxiv.org/abs/2307.13903v3</link><description>I study the problem of learning a Lipschitz function with corrupted binarysignals. The learner tries to learn a $L$-Lipschitz function $f: [0,1]^d\rightarrow [0, L]$ that the adversary chooses. There is a total of $T$ rounds.In each round $t$, the adversary selects a context vector $x_t$ in the inputspace, and the learner makes a guess to the true function value $f(x_t)$ andreceives a binary signal indicating whether the guess is high or low. In atotal of $C$ rounds, the signal may be corrupted, though the value of $C$ is\emph{unknown} to the learner. The learner's goal is to incur a smallcumulative loss. This work introduces the new algorithmic technique\emph{agnostic checking} as well as new analysis techniques. I designalgorithms which: for the symmetric loss, the learner achieves regret $L\cdotO(C\log T)$ with $d = 1$ and $L\cdot O_d(C\log T + T^{(d-1)/d})$ with $d &gt; 1$;for the pricing loss, the learner achieves regret $L\cdot \widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.</description><author>Shiliang Zuo</author><pubDate>Wed, 29 Nov 2023 16:19:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13903v3</guid></item><item><title>Fast and Expressive Gesture Recognition using a Combination-Homomorphic Electromyogram Encoder</title><link>http://arxiv.org/abs/2311.14675v2</link><description>We study the task of gesture recognition from electromyography (EMG), withthe goal of enabling expressive human-computer interaction at high accuracy,while minimizing the time required for new subjects to provide calibrationdata. To fulfill these goals, we define combination gestures consisting of adirection component and a modifier component. New subjects only demonstrate thesingle component gestures and we seek to extrapolate from these to all possiblesingle or combination gestures. We extrapolate to unseen combination gesturesby combining the feature vectors of real single gestures to produce synthetictraining data. This strategy allows us to provide a large and flexible gesturevocabulary, while not requiring new subjects to demonstrate combinatoriallymany example gestures. We pre-train an encoder and a combination operator usingself-supervision, so that we can produce useful synthetic training data forunseen test subjects. To evaluate the proposed method, we collect a real-worldEMG dataset, and measure the effect of augmented supervision against twobaselines: a partially-supervised model trained with only single gesture datafrom the unseen subject, and a fully-supervised model trained with real singleand real combination gesture data from the unseen subject. We find that theproposed method provides a dramatic improvement over the partially-supervisedmodel, and achieves a useful classification accuracy that in some casesapproaches the performance of the fully-supervised model.</description><author>Niklas Smedemark-Margulies, Yunus Bicer, Elifnur Sunger, Tales Imbiriba, Eugene Tunik, Deniz Erdogmus, Mathew Yarossi, Robin Walters</author><pubDate>Wed, 29 Nov 2023 16:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14675v2</guid></item><item><title>LM-Cocktail: Resilient Tuning of Language Models via Model Merging</title><link>http://arxiv.org/abs/2311.13534v3</link><description>The pre-trained language models are continually fine-tuned to better supportdownstream applications. However, this operation may result in significantperformance degeneration on general tasks beyond the targeted domain. Toovercome this problem, we propose LM-Cocktail which enables the fine-tunedmodel to stay resilient in general perspectives. Our method is conducted in theform of model merging, where the fine-tuned language model is merged with thepre-trained base model or the peer models from other domains through weightedaverage. Despite simplicity, LM-Cocktail is surprisingly effective: theresulted model is able to achieve a strong empirical performance in the wholescope of general tasks while preserving a superior capacity in its targeteddomain. We conduct comprehensive experiments with LLama and BGE model onpopular benchmarks, including FLAN, MMLU, MTEB, whose results validate theefficacy of our proposed method. The code and checkpoints are available athttps://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.</description><author>Shitao Xiao, Zheng Liu, Peitian Zhang, Xingrun Xing</author><pubDate>Wed, 29 Nov 2023 16:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13534v3</guid></item><item><title>CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra</title><link>http://arxiv.org/abs/2309.03060v2</link><description>Many areas of machine learning and science involve large linear algebraproblems, such as eigendecompositions, solving linear systems, computing matrixexponentials, and trace estimation. The matrices involved often have Kronecker,convolutional, block diagonal, sum, or product structure. In this paper, wepropose a simple but general framework for large-scale linear algebra problemsin machine learning, named CoLA (Compositional Linear Algebra). By combining alinear operator abstraction with compositional dispatch rules, CoLAautomatically constructs memory and runtime efficient numerical algorithms.Moreover, CoLA provides memory efficient automatic differentiation, lowprecision computation, and GPU acceleration in both JAX and PyTorch, while alsoaccommodating new objects, operations, and rules in downstream packages viamultiple dispatch. CoLA can accelerate many algebraic operations, while makingit easy to prototype matrix structures and algorithms, providing an appealingdrop-in tool for virtually any computational effort that requires linearalgebra. We showcase its efficacy across a broad range of applications,including partial differential equations, Gaussian processes, equivariant modelconstruction, and unsupervised learning.</description><author>Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson</author><pubDate>Wed, 29 Nov 2023 16:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03060v2</guid></item><item><title>Supervising the Centroid Baseline for Extractive Multi-Document Summarization</title><link>http://arxiv.org/abs/2311.17771v1</link><description>The centroid method is a simple approach for extractive multi-documentsummarization and many improvements to its pipeline have been proposed. Wefurther refine it by adding a beam search process to the sentence selection andalso a centroid estimation attention model that leads to improved results. Wedemonstrate this in several multi-document summarization datasets, including ina multilingual scenario.</description><author>SimÃ£o GonÃ§alves, GonÃ§alo Correia, Diogo Pernes, Afonso Mendes</author><pubDate>Wed, 29 Nov 2023 16:11:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17771v1</guid></item><item><title>PillarNeSt: Embracing Backbone Scaling and Pretraining for Pillar-based 3D Object Detection</title><link>http://arxiv.org/abs/2311.17770v1</link><description>This paper shows the effectiveness of 2D backbone scaling and pretraining forpillar-based 3D object detectors. Pillar-based methods mainly employ randomlyinitialized 2D convolution neural network (ConvNet) for feature extraction andfail to enjoy the benefits from the backbone scaling and pretraining in theimage domain. To show the scaling-up capacity in point clouds, we introduce thedense ConvNet pretrained on large-scale image datasets (e.g., ImageNet) as the2D backbone of pillar-based detectors. The ConvNets are adaptively designedbased on the model size according to the specific features of point clouds,such as sparsity and irregularity. Equipped with the pretrained ConvNets, ourproposed pillar-based detector, termed PillarNeSt, outperforms the existing 3Dobject detectors by a large margin on the nuScenes and Argoversev2 datasets.Our code shall be released upon acceptance.</description><author>Weixin Mao, Tiancai Wang, Diankun Zhang, Junjie Yan, Osamu Yoshie</author><pubDate>Wed, 29 Nov 2023 16:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17770v1</guid></item><item><title>Robustness Approaches for the Examination Timetabling Problem under Data Uncertainty</title><link>http://arxiv.org/abs/2311.17766v1</link><description>In the literature the examination timetabling problem (ETTP) is oftenconsidered a post-enrollment problem (PE-ETTP). In the real world, universitiesoften schedule their exams before students register using information fromprevious terms. A direct consequence of this approach is the uncertaintypresent in the resulting models. In this work we discuss several approachesavailable in the robust optimization literature. We consider the implicationsof each approach in respect to the examination timetabling problem and presenthow the most favorable approaches can be applied to the ETTP. Afterwards weanalyze the impact of some possible implementations of the given robustnessapproaches on two real world instances and several random instances generatedby our instance generation framework which we introduce in this work.</description><author>Bernd Bassimir, Rolf Wanka</author><pubDate>Wed, 29 Nov 2023 16:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17766v1</guid></item><item><title>ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?</title><link>http://arxiv.org/abs/2311.16989v2</link><description>Upon its release in late 2022, ChatGPT has brought a seismic shift in theentire landscape of AI, both in research and commerce. Throughinstruction-tuning a large language model (LLM) with supervised fine-tuning andreinforcement learning from human feedback, it showed that a model could answerhuman questions and follow instructions on a broad panel of tasks. Followingthis success, interests in LLMs have intensified, with new LLMs flourishing atfrequent interval across academia and industry, including many start-upsfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic'sClaude) generally outperform their open-source counterparts, the progress onthe latter has been rapid with claims of achieving parity or even better oncertain tasks. This has crucial implications not only on research but also onbusiness. In this work, on the first anniversary of ChatGPT, we provide anexhaustive overview of this success, surveying all tasks where an open-sourceLLM has claimed to be on par or better than ChatGPT.</description><author>Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty</author><pubDate>Wed, 29 Nov 2023 16:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16989v2</guid></item><item><title>Cinematic Behavior Transfer via NeRF-based Differentiable Filming</title><link>http://arxiv.org/abs/2311.17754v1</link><description>In the evolving landscape of digital media and video production, the precisemanipulation and reproduction of visual elements like camera movements andcharacter actions are highly desired. Existing SLAM methods face limitations indynamic scenes and human pose estimation often focuses on 2D projections,neglecting 3D statuses. To address these issues, we first introduce a reversefilming behavior estimation technique. It optimizes camera trajectories byleveraging NeRF as a differentiable renderer and refining SMPL tracks. We thenintroduce a cinematic transfer pipeline that is able to transfer various shottypes to a new 2D video or a 3D virtual environment. The incorporation of 3Dengine workflow enables superior rendering and control abilities, which alsoachieves a higher rating in the user study.</description><author>Xuekun Jiang, Anyi Rao, Jingbo Wang, Dahua Lin, Bo Dai</author><pubDate>Wed, 29 Nov 2023 15:56:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17754v1</guid></item><item><title>A Unified Approach for Text- and Image-guided 4D Scene Generation</title><link>http://arxiv.org/abs/2311.16854v2</link><description>Large-scale diffusion generative models are greatly simplifying image, videoand 3D asset creation from user-provided text prompts and images. However, thechallenging problem of text-to-4D dynamic 3D scene generation with diffusionguidance remains largely unexplored. We propose Dream-in-4D, which features anovel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2Ddiffusion guidance to effectively learn a high-quality static 3D asset in thefirst stage; (2) a deformable neural radiance field that explicitlydisentangles the learned static asset from its deformation, preserving qualityduring motion learning; and (3) a multi-resolution feature grid for thedeformation field with a displacement total variation loss to effectively learnmotion with video diffusion guidance in the second stage. Through a userpreference study, we demonstrate that our approach significantly advances imageand motion quality, 3D consistency and text fidelity for text-to-4D generationcompared to baseline approaches. Thanks to its motion-disentangledrepresentation, Dream-in-4D can also be easily adapted for controllablegeneration where appearance is defined by one or multiple images, without theneed to modify the motion learning stage. Thus, our method offers, for thefirst time, a unified approach for text-to-4D, image-to-4D and personalized 4Dgeneration tasks.</description><author>Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Karsten Kreis, Otmar Hilliges, Shalini De Mello</author><pubDate>Wed, 29 Nov 2023 15:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16854v2</guid></item><item><title>BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment</title><link>http://arxiv.org/abs/2311.17752v1</link><description>Banding, also known as staircase-like contours, frequently occurs in flatareas of images/videos processed by the compression or quantization algorithms.As undesirable artifacts, banding destroys the original image structure, thusdegrading users' quality of experience (QoE). In this paper, we systematicallyinvestigate the banding image quality assessment (IQA) problem, aiming todetect the image banding artifacts and evaluate their perceptual visualquality. Considering that the existing image banding databases only containlimited content sources and banding generation methods, and lack perceptualquality labels (i.e. mean opinion scores), we first build the largest bandingIQA database so far, named Banding Artifact Noticeable Database (BAND-2k),which consists of 2,000 banding images generated by 15 compression andquantization schemes. A total of 23 workers participated in the subjective IQAexperiment, yielding over 214,000 patch-level banding class labels and 44,371reliable image-level quality ratings. Subsequently, we develop an effectiveno-reference (NR) banding evaluator for banding detection and qualityassessment by leveraging frequency characteristics of banding artifacts. A dualconvolutional neural network is employed to concurrently learn the featurerepresentation from the high-frequency and low-frequency maps, therebyenhancing the ability to discern banding artifacts. The quality score of abanding image is generated by pooling the banding detection maps masked by thespatial frequency filters. Experiments demonstrate that our banding evaluatorachieves a remarkably high accuracy in banding detection and also exhibits highSRCC and PLCC results with the perceptual quality labels. These findings unveilthe strong correlations between the intensity of banding artifacts and theperceptual visual quality, thus validating the necessity of banding qualityassessment.</description><author>Zijian Chen, Wei Sun, Jun Jia, Fangfang Lu, Zicheng Zhang, Jing Liu, Ru Huang, Xiongkuo Min, Guangtao Zhai</author><pubDate>Wed, 29 Nov 2023 15:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17752v1</guid></item><item><title>Addressing Membership Inference Attack in Federated Learning with Model Compression</title><link>http://arxiv.org/abs/2311.17750v1</link><description>Federated Learning (FL) has been proposed as a privacy-preserving solutionfor machine learning. However, recent works have shown that Federated Learningcan leak private client data through membership attacks. In this paper, we showthat the effectiveness of these attacks on the clients negatively correlateswith the size of the client datasets and model complexity. Based on thisfinding, we propose model-agnostic Federated Learning as a privacy-enhancingsolution because it enables the use of models of varying complexity in theclients. To this end, we present $\texttt{MaPP-FL}$, a novel privacy-aware FLapproach that leverages model compression on the clients while keeping a fullmodel on the server. We compare the performance of $\texttt{MaPP-FL}$ againststate-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, andFEMNIST vision datasets. Our experiments show the effectiveness of$\texttt{MaPP-FL}$ in preserving the clients' and the server's privacy whileachieving competitive classification accuracies.</description><author>Gergely DÃ¡niel NÃ©meth, Miguel Ãngel Lozano, Novi Quadrianto, Nuria Oliver</author><pubDate>Wed, 29 Nov 2023 15:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17750v1</guid></item><item><title>Efficient In-Context Learning in Vision-Language Models for Egocentric Videos</title><link>http://arxiv.org/abs/2311.17041v2</link><description>Recent advancements in text-only large language models (LLMs) havehighlighted the benefit of in-context learning for adapting to new tasks with afew demonstrations. However, extending in-context learning to largevision-language models (VLMs) using a huge amount of naturalisticvision-language data has shown limited success, particularly for egocentricvideos, due to high data collection costs. We propose a novel training method$\mathbb{E}$fficient $\mathbb{I}$n-context $\mathbb{L}$earning on$\mathbb{E}$gocentric $\mathbb{V}$ideos ($\mathbb{EILEV}$), which elicitsin-context learning in VLMs for egocentric videos without requiring massive,naturalistic egocentric video datasets. $\mathbb{EILEV}$ involves architecturaland training data adaptations to allow the model to process contextsinterleaved with video clips and narrations, sampling of in-context exampleswith clusters of similar verbs and nouns, use of data with skewed marginaldistributions with a long tail of infrequent verbs and nouns, as well ashomonyms and synonyms. Our evaluations show that $\mathbb{EILEV}$-trainedmodels outperform larger VLMs trained on a huge amount of naturalistic data inin-context learning. Furthermore, they can generalize to not onlyout-of-distribution, but also novel, rare egocentric videos and texts viain-context learning, demonstrating potential for applications requiringcost-effective training, and rapid post-deployment adaptability. Our code anddemo are available at \url{https://github.com/yukw777/EILEV}.</description><author>Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Joyce Chai</author><pubDate>Wed, 29 Nov 2023 15:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17041v2</guid></item><item><title>Variational Bayes image restoration with compressive autoencoders</title><link>http://arxiv.org/abs/2311.17744v1</link><description>Regularization of inverse problems is of paramount importance incomputational imaging. The ability of neural networks to learn efficient imagerepresentations has been recently exploited to design powerful data-drivenregularizers. While state-of-the-art plug-and-play methods rely on an implicitregularization provided by neural denoisers, alternative Bayesian approachesconsider Maximum A Posteriori (MAP) estimation in the latent space of agenerative model, thus with an explicit regularization. However,state-of-the-art deep generative models require a huge amount of training datacompared to denoisers. Besides, their complexity hampers the optimization ofthe latent MAP. In this work, we propose to use compressive autoencoders forlatent estimation. These networks, which can be seen as variationalautoencoders with a flexible latent prior, are smaller and easier to train thanstate-of-the-art generative models. We then introduce the Variational BayesLatent Estimation (VBLE) algorithm, which performs this estimation within theframework of variational inference. This allows for fast and easy (approximate)posterior sampling. Experimental results on image datasets BSD and FFHQdemonstrate that VBLE reaches similar performance than state-of-the-artplug-and-play methods, while being able to quantify uncertainties faster thanother existing posterior sampling techniques.</description><author>Maud Biquard, Marie Chabert, Thomas Oberlin</author><pubDate>Wed, 29 Nov 2023 15:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17744v1</guid></item><item><title>Mukhyansh: A Headline Generation Dataset for Indic Languages</title><link>http://arxiv.org/abs/2311.17743v1</link><description>The task of headline generation within the realm of Natural LanguageProcessing (NLP) holds immense significance, as it strives to distill the trueessence of textual content into concise and attention-grabbing summaries. Whilenoteworthy progress has been made in headline generation for widely spokenlanguages like English, there persist numerous challenges when it comes togenerating headlines in low-resource languages, such as the rich and diverseIndian languages. A prominent obstacle that specifically hinders headlinegeneration in Indian languages is the scarcity of high-quality annotated data.To address this crucial gap, we proudly present Mukhyansh, an extensivemultilingual dataset, tailored for Indian language headline generation.Comprising an impressive collection of over 3.39 million article-headlinepairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu,Tamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present acomprehensive evaluation of several state-of-the-art baseline models.Additionally, through an empirical analysis of existing works, we demonstratethat Mukhyansh outperforms all other models, achieving an impressive averageROUGE-L score of 31.43 across all 8 languages.</description><author>Lokesh Madasu, Gopichand Kanumolu, Nirmal Surange, Manish Shrivastava</author><pubDate>Wed, 29 Nov 2023 15:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17743v1</guid></item><item><title>Equivariant Parameter Sharing for Porous Crystalline Materials</title><link>http://arxiv.org/abs/2304.01628v3</link><description>Efficiently predicting properties of porous crystalline materials has greatpotential to accelerate the high throughput screening process for developingnew materials, as simulations carried out using first principles model areoften computationally expensive. To effectively make use of Deep Learningmethods to model these materials, we need to utilize the symmetries present inthe crystals, which are defined by their space group. Existing methods forcrystal property prediction either have symmetry constraints that are toorestrictive or only incorporate symmetries between unit cells. In addition,these models do not explicitly model the porous structure of the crystal. Inthis paper, we develop a model which incorporates the symmetries of the unitcell of a crystal in its architecture and explicitly models the porousstructure. We evaluate our model by predicting the heat of adsorption of CO$_2$for different configurations of the mordenite zeolite. Our results confirm thatour method performs better than existing methods for crystal propertyprediction and that the inclusion of pores results in a more efficient model.</description><author>Marko PetkoviÄ, Pablo Romero-Marimon, Vlado Menkovski, Sofia Calero</author><pubDate>Wed, 29 Nov 2023 15:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01628v3</guid></item><item><title>Physics-informed neural networks for transformed geometries and manifolds</title><link>http://arxiv.org/abs/2311.15940v2</link><description>Physics-informed neural networks (PINNs) effectively embed physicalprinciples into machine learning, but often struggle with complex oralternating geometries. We propose a novel method for integrating geometrictransformations within PINNs to robustly accommodate geometric variations. Ourmethod incorporates a diffeomorphism as a mapping of a reference domain andadapts the derivative computation of the physics-informed loss function. Thisgeneralizes the applicability of PINNs not only to smoothly deformed domains,but also to lower-dimensional manifolds and allows for direct shapeoptimization while training the network. We demonstrate the effectivity of ourapproach on several problems: (i) Eikonal equation on Archimedean spiral, (ii)Poisson problem on surface manifold, (iii) Incompressible Stokes flow indeformed tube, and (iv) Shape optimization with Laplace operator. Through theseexamples, we demonstrate the enhanced flexibility over traditional PINNs,especially under geometric variations. The proposed framework presents anoutlook for training deep neural operators over parametrized geometries, pavingthe way for advanced modeling with PDEs on complex geometries in science andengineering.</description><author>Samuel Burbulla</author><pubDate>Wed, 29 Nov 2023 15:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15940v2</guid></item><item><title>End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data</title><link>http://arxiv.org/abs/2311.17741v1</link><description>Joint rich and normalized automatic speech recognition (ASR), that producestranscriptions both with and without punctuation and capitalization, remains achallenge. End-to-end (E2E) ASR models offer both convenience and the abilityto perform such joint transcription of speech. Training such models requirespaired speech and rich text data, which is not widely available. In this paper,we compare two different approaches to train a stateless Transducer-based E2Ejoint rich and normalized ASR system, ready for streaming applications, with alimited amount of rich labeled data. The first approach uses a language modelto generate pseudo-rich transcriptions of normalized training data. The secondapproach uses a single decoder conditioned on the type of the output. The firstapproach leads to E2E rich ASR which perform better on out-of-domain data, withup to 9% relative reduction in errors. The second approach demonstrates thefeasibility of an E2E joint rich and normalized ASR system using as low as 5%rich training data with moderate (2.42% absolute) increase in errors.</description><author>Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent</author><pubDate>Wed, 29 Nov 2023 15:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17741v1</guid></item><item><title>A transductive few-shot learning approach for classification of digital histopathological slides from liver cancer</title><link>http://arxiv.org/abs/2311.17740v1</link><description>This paper presents a new approach for classifying 2D histopathology patchesusing few-shot learning. The method is designed to tackle a significantchallenge in histopathology, which is the limited availability of labeled data.By applying a sliding window technique to histopathology slides, we illustratethe practical benefits of transductive learning (i.e., making joint predictionson patches) to achieve consistent and accurate classification. Our approachinvolves an optimization-based strategy that actively penalizes the predictionof a large number of distinct classes within each window. We conductedexperiments on histopathological data to classify tissue classes in digitalslides of liver cancer, specifically hepatocellular carcinoma. The initialresults show the effectiveness of our method and its potential to enhance theprocess of automated cancer diagnosis and treatment, all while reducing thetime and effort required for expert annotation.</description><author>Aymen Sadraoui, SÃ©golÃ¨ne Martin, Eliott Barbot, Astrid Laurent-Bellue, Jean-Christophe Pesquet, Catherine Guettier, Ismail Ben Ayed</author><pubDate>Wed, 29 Nov 2023 15:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17740v1</guid></item><item><title>GenZI: Zero-Shot 3D Human-Scene Interaction Generation</title><link>http://arxiv.org/abs/2311.17737v1</link><description>Can we synthesize 3D humans interacting with scenes without learning from any3D human-scene interaction data? We propose GenZI, the first zero-shot approachto generating 3D human-scene interactions. Key to GenZI is our distillation ofinteraction priors from large vision-language models (VLMs), which have learneda rich semantic space of 2D human-scene compositions. Given a natural languagedescription and a coarse point location of the desired interaction in a 3Dscene, we first leverage VLMs to imagine plausible 2D human interactionsinpainted into multiple rendered views of the scene. We then formulate a robustiterative optimization to synthesize the pose and shape of a 3D human model inthe scene, guided by consistency with the 2D interaction hypotheses. Incontrast to existing learning-based approaches, GenZI circumvents theconventional need for captured 3D interaction data, and allows for flexiblecontrol of the 3D interaction synthesis with easy-to-use text prompts.Extensive experiments show that our zero-shot approach has high flexibility andgenerality, making it applicable to diverse scene types, including both indoorand outdoor environments.</description><author>Lei Li, Angela Dai</author><pubDate>Wed, 29 Nov 2023 15:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17737v1</guid></item><item><title>To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning</title><link>http://arxiv.org/abs/2311.07574v2</link><description>Existing visual instruction tuning methods typically prompt large languagemodels with textual descriptions to generate instruction-following data.Despite the promising performance achieved, these descriptions are derived fromimage annotations, which are oftentimes coarse-grained. Furthermore, theinstructions might even contradict the visual content without observing theentire visual context. To address this challenge, we introduce a fine-grainedvisual instruction dataset, LVIS-Instruct4V, which contains 220K visuallyaligned and context-aware instructions produced by prompting the powerfulGPT-4V with images from LVIS. Through experimental validation and case studies,we demonstrate that high-quality visual instructional data could improve theperformance of LLaVA-1.5, a state-of-the-art large multimodal model, across awide spectrum of benchmarks by clear margins. Notably, by simply replacing theLLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVAon most challenging LMM benchmarks, e.g., LLaVA$^w$ (76.7 vs. 70.7) and MM-Vet(40.2 vs. 35.4). We release our data and model athttps://github.com/X2FD/LVIS-INSTRUCT4V.</description><author>Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Wed, 29 Nov 2023 15:37:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07574v2</guid></item><item><title>Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object</title><link>http://arxiv.org/abs/2311.13562v2</link><description>Image style transfer occupies an important place in both computer graphicsand computer vision. However, most current methods require reference tostylized images and cannot individually stylize specific objects. To overcomethis limitation, we propose the "Soulstyler" framework, which allows users toguide the stylization of specific objects in an image through simple textualdescriptions. We introduce a large language model to parse the text andidentify stylization goals and specific styles. Combined with a CLIP-basedsemantic visual embedding encoder, the model understands and matches text andimage content. We also introduce a novel localized text-image block matchingloss that ensures that style transfer is performed only on specified targetobjects, while non-target regions remain in their original style. Experimentalresults demonstrate that our model is able to accurately perform style transferon target objects according to textual descriptions without affecting the styleof background regions. Our code will be available athttps://github.com/yisuanwang/Soulstyler.</description><author>Junhao Chen, Peng Rong, Jingbo Sun, Chao Li, Xiang Li, Hongwu Lv</author><pubDate>Wed, 29 Nov 2023 15:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13562v2</guid></item><item><title>Consistent Video-to-Video Transfer Using Synthetic Dataset</title><link>http://arxiv.org/abs/2311.00213v2</link><description>We introduce a novel and efficient approach for text-based video-to-videoediting that eliminates the need for resource-intensive per-video-per-modelfinetuning. At the core of our approach is a synthetic paired video datasettailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix'simage transfer via editing instruction, we adapt this paradigm to the videodomain. Extending the Prompt-to-Prompt to videos, we efficiently generatepaired samples, each with an input video and its edited counterpart. Alongsidethis, we introduce the Long Video Sampling Correction during sampling, ensuringconsistent long videos across batches. Our method surpasses current methodslike Tune-A-Video, heralding substantial progress in text-based video-to-videoediting and suggesting exciting avenues for further exploration and deployment.</description><author>Jiaxin Cheng, Tianjun Xiao, Tong He</author><pubDate>Wed, 29 Nov 2023 15:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00213v2</guid></item><item><title>SenTest: Evaluating Robustness of Sentence Encoders</title><link>http://arxiv.org/abs/2311.17722v1</link><description>Contrastive learning has proven to be an effective method for pre-trainingmodels using weakly labeled data in the vision domain. Sentence transformersare the NLP counterparts to this architecture, and have been growing inpopularity due to their rich and effective sentence representations. Havingeffective sentence representations is paramount in multiple tasks, such asinformation retrieval, retrieval augmented generation (RAG), and sentencecomparison. Keeping in mind the deployability factor of transformers,evaluating the robustness of sentence transformers is of utmost importance.This work focuses on evaluating the robustness of the sentence encoders. Weemploy several adversarial attacks to evaluate its robustness. This system usescharacter-level attacks in the form of random character substitution,word-level attacks in the form of synonym replacement, and sentence-levelattacks in the form of intra-sentence word order shuffling. The results of theexperiments strongly undermine the robustness of sentence encoders. The modelsproduce significantly different predictions as well as embeddings on perturbeddatasets. The accuracy of the models can fall up to 15 percent on perturbeddatasets as compared to unperturbed datasets. Furthermore, the experimentsdemonstrate that these embeddings does capture the semantic and syntacticstructure (sentence order) of sentences. However, existing supervisedclassification strategies fail to leverage this information, and merelyfunction as n-gram detectors.</description><author>Tanmay Chavan, Shantanu Patankar, Aditya Kane, Omkar Gokhale, Geetanjali Kale, Raviraj Joshi</author><pubDate>Wed, 29 Nov 2023 15:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17722v1</guid></item><item><title>Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers</title><link>http://arxiv.org/abs/2311.17717v1</link><description>Concept erasure in text-to-image diffusion models aims to disable pre-traineddiffusion models from generating images related to a target concept. To performreliable concept erasure, the properties of robustness and locality aredesirable. The former refrains the model from producing images associated withthe target concept for any paraphrased or learned prompts, while the latterpreserves the model ability in generating images for non-target concepts. Inthis paper, we propose Reliable Concept Erasing via Lightweight Erasers(Receler), which learns a lightweight Eraser to perform concept erasing andenhances locality and robustness with the proposed concept-localizedregularization and adversarial prompt learning, respectively. Comprehensivequantitative and qualitative experiments with various concept prompts verifythe superiority of Receler over the previous erasing methods on the above twodesirable properties.</description><author>Chi-Pin Huang, Kai-Po Chang, Chung-Ting Tsai, Yung-Hsuan Lai, Yu-Chiang Frank Wang</author><pubDate>Wed, 29 Nov 2023 15:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17717v1</guid></item><item><title>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</title><link>http://arxiv.org/abs/2311.12754v2</link><description>3D occupancy prediction is an important task for the robustness ofvision-centric autonomous driving, which aims to predict whether each point isoccupied in the surrounding 3D space. Existing methods usually require 3Doccupancy labels to produce meaningful results. However, it is very laboriousto annotate the occupancy status of each voxel. In this paper, we proposeSelfOcc to explore a self-supervised way to learn 3D occupancy using only videosequences. We first transform the images into the 3D space (e.g., bird's eyeview) to obtain 3D representation of the scene. We directly impose constraintson the 3D representations by treating them as signed distance fields. We canthen render 2D images of previous and future frames as self-supervision signalsto learn the 3D representations. We propose an MVS-embedded strategy todirectly optimize the SDF-induced weights with multiple depth proposals. OurSelfOcc outperforms the previous best method SceneRF by 58.7% using a singleframe as input on SemanticKITTI and is the first self-supervised work thatproduces reasonable 3D occupancy for surround cameras on nuScenes. SelfOccproduces high-quality depth and achieves state-of-the-art results on noveldepth synthesis, monocular depth estimation, and surround-view depth estimationon the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:https://github.com/huang-yh/SelfOcc.</description><author>Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, Jiwen Lu</author><pubDate>Wed, 29 Nov 2023 15:19:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12754v2</guid></item><item><title>An Attribution Method for Siamese Encoders</title><link>http://arxiv.org/abs/2310.05703v3</link><description>Despite the success of Siamese encoder models such as sentence transformers(ST), little is known about the aspects of inputs they pay attention to. Abarrier is that their predictions cannot be attributed to individual features,as they compare two inputs rather than processing a single one. This paperderives a local attribution method for Siamese encoders by generalizing theprinciple of integrated gradients to models with multiple inputs. The solutiontakes the form of feature-pair attributions, and can be reduced to atoken-token matrix for STs. Our method involves the introduction of integratedJacobians and inherits the advantageous formal properties of integratedgradients: it accounts for the model's full computation graph and is guaranteedto converge to the actual prediction. A pilot study shows that in an ST fewtoken-pairs can often explain large fractions of predictions, and it focuses onnouns and verbs. For accurate predictions, it however needs to attend to themajority of tokens and parts of speech.</description><author>Lucas MÃ¶ller, Dmitry Nikolaev, Sebastian PadÃ³</author><pubDate>Wed, 29 Nov 2023 15:12:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05703v3</guid></item><item><title>SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation</title><link>http://arxiv.org/abs/2311.17707v1</link><description>We introduce SAMPro3D for zero-shot 3D indoor scene segmentation. Given the3D point cloud and multiple posed 2D frames of 3D scenes, our approach segments3D scenes by applying the pretrained Segment Anything Model (SAM) to 2D frames.Our key idea involves locating 3D points in scenes as natural 3D prompts toalign their projected pixel prompts across frames, ensuring frame-consistencyin both pixel prompts and their SAM-predicted masks. Moreover, we suggestfiltering out low-quality 3D prompts based on feedback from all 2D frames, forenhancing segmentation quality. We also propose to consolidate different 3Dprompts if they are segmenting the same object, bringing a more comprehensivesegmentation. Notably, our method does not require any additional training ondomain-specific data, enabling us to preserve the zero-shot power of SAM.Extensive qualitative and quantitative results show that our methodconsistently achieves higher quality and more diverse segmentation thanprevious zero-shot or fully supervised approaches, and in many cases evensurpasses human-level annotations. The project page can be accessed athttps://mutianxu.github.io/sampro3d/.</description><author>Mutian Xu, Xingyilang Yin, Lingteng Qiu, Yang Liu, Xin Tong, Xiaoguang Han</author><pubDate>Wed, 29 Nov 2023 15:11:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17707v1</guid></item><item><title>Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models</title><link>http://arxiv.org/abs/2310.01929v2</link><description>Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, havedemonstrated remarkable prompt-based image generation capabilities.Multilingual encoders may have a substantial impact on the cultural agency ofthese models, as language is a conduit of culture. In this study, we explorethe cultural perception embedded in TTI models by characterizing culture acrossthree hierarchical tiers: cultural dimensions, cultural domains, and culturalconcepts. Based on this ontology, we derive prompt templates to unlock thecultural knowledge in TTI models, and propose a comprehensive suite ofevaluation techniques, including intrinsic evaluations using the CLIP space,extrinsic evaluations with a Visual-Question-Answer (VQA) model and humanassessments, to evaluate the cultural content of TTI-generated images. Tobolster our research, we introduce the CulText2I dataset, derived from fourdiverse TTI models and spanning ten languages. Our experiments provide insightsregarding Do, What, Which and How research questions about the nature ofcultural encoding in TTI models, paving the way for cross-cultural applicationsof these models.</description><author>Mor Ventura, Eyal Ben-David, Anna Korhonen, Roi Reichart</author><pubDate>Wed, 29 Nov 2023 15:11:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01929v2</guid></item><item><title>How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2311.17696v1</link><description>Artificial intelligence is transforming education through data-driven,personalized learning solutions. This paper introduces AI Tutor, an innovativeweb application that provides personalized tutoring in any subject usingstate-of-the-art Large Language Model (LLM). AI Tutor ingests course materialsto construct an adaptive knowledge base tailored to the course. When studentspose questions, it retrieves the most relevant information and generatesdetailed, conversational responses citing supporting evidence. The system ispowered by advanced large language models and Retrieval-Augmented Generation(RAG) techniques for accurate, natural question answering. We present afully-functional web interface and video demonstration that showcase AI Tutor'sversatility across diverse subjects and its ability to produce pedagogicallycogent responses. While an initial prototype, this work represents a pioneeringstep toward AI-enabled tutoring systems that can democratize access tohigh-quality, customized educational support.</description><author>Chenxi Dong</author><pubDate>Wed, 29 Nov 2023 15:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17696v1</guid></item><item><title>Fair Text-to-Image Diffusion via Fair Mapping</title><link>http://arxiv.org/abs/2311.17695v1</link><description>In this paper, we address the limitations of existing text-to-image diffusionmodels in generating demographically fair results when given human-relateddescriptions. These models often struggle to disentangle the target languagecontext from sociocultural biases, resulting in biased image generation. Toovercome this challenge, we propose Fair Mapping, a general, model-agnostic,and lightweight approach that modifies a pre-trained text-to-image model bycontrolling the prompt to achieve fair image generation. One key advantage ofour approach is its high efficiency. The training process only requiresupdating a small number of parameters in an additional linear mapping network.This not only reduces the computational cost but also accelerates theoptimization process. We first demonstrate the issue of bias in generatedresults caused by language biases in text-guided diffusion models. Bydeveloping a mapping network that projects language embeddings into an unbiasedspace, we enable the generation of relatively balanced demographic resultsbased on a keyword specified in the prompt. With comprehensive experiments onface image generation, we show that our method significantly improves imagegeneration performance when prompted with descriptions related to human faces.By effectively addressing the issue of bias, we produce more fair and diverseimage outputs. This work contributes to the field of text-to-image generationby enhancing the ability to generate images that accurately reflect theintended demographic characteristics specified in the text.</description><author>Jia Li, Lijie Hu, Jingfeng Zhang, Tianhang Zheng, Hua Zhang, Di Wang</author><pubDate>Wed, 29 Nov 2023 15:02:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17695v1</guid></item><item><title>Rigorous dynamical mean field theory for stochastic gradient descent methods</title><link>http://arxiv.org/abs/2210.06591v3</link><description>We prove closed-form equations for the exact high-dimensional asymptotics ofa family of first order gradient-based methods, learning an estimator (e.g.M-estimator, shallow neural network, ...) from observations on Gaussian datawith empirical risk minimization. This includes widely used algorithms such asstochastic gradient descent (SGD) or Nesterov acceleration. The obtainedequations match those resulting from the discretization of dynamical mean-fieldtheory (DMFT) equations from statistical physics when applied to gradient flow.Our proof method allows us to give an explicit description of how memorykernels build up in the effective dynamics, and to include non-separable updatefunctions, allowing datasets with non-identity covariance matrices. Finally, weprovide numerical implementations of the equations for SGD with genericextensive batch-size and with constant learning rates.</description><author>Cedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, Lenka Zdeborova</author><pubDate>Wed, 29 Nov 2023 15:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06591v3</guid></item><item><title>Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning</title><link>http://arxiv.org/abs/2311.17693v1</link><description>Robotic-assisted surgical systems have demonstrated significant potential inenhancing surgical precision and minimizing human errors. However, existingsystems lack the ability to accommodate the unique preferences and requirementsof individual surgeons. Additionally, they primarily focus on general surgeries(e.g., laparoscopy) and are not suitable for highly precise microsurgeries,such as ophthalmic procedures. Thus, we propose a simulation-based image-guidedapproach for surgeon-centered autonomous agents that can adapt to theindividual surgeon's skill level and preferred surgical techniques duringophthalmic cataract surgery. Our approach utilizes a simulated environment totrain reinforcement and imitation learning agents guided by image data toperform all tasks of the incision phase of cataract surgery. By integrating thesurgeon's actions and preferences into the training process with thesurgeon-in-the-loop, our approach enables the robot to implicitly learn andadapt to the individual surgeon's unique approach through demonstrations. Thisresults in a more intuitive and personalized surgical experience for thesurgeon. Simultaneously, it ensures consistent performance for the autonomousrobotic apprentice. We define and evaluate the effectiveness of our approachusing our proposed metrics; and highlight the trade-off between a generic agentand a surgeon-centered adapted agent. Moreover, our approach has the potentialto extend to other ophthalmic surgical procedures, opening the door to a newgeneration of surgeon-in-the-loop autonomous surgical robots. We provide anopen-source simulation framework for future development and reproducibility.</description><author>Amr Gomaa, Bilal Mahdy, Niko Kleer, Antonio KrÃ¼ger</author><pubDate>Wed, 29 Nov 2023 15:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17693v1</guid></item><item><title>Learning End-to-End Channel Coding with Diffusion Models</title><link>http://arxiv.org/abs/2302.01714v2</link><description>It is a known problem that deep-learning-based end-to-end (E2E) channelcoding systems depend on a known and differentiable channel model, due to thelearning process and based on the gradient-descent optimization methods. Thisplaces the challenge to approximate or generate the channel or its derivativefrom samples generated by pilot signaling in real-world scenarios. Currently,there are two prevalent methods to solve this problem. One is to generate thechannel via a generative adversarial network (GAN), and the other is to, inessence, approximate the gradient via reinforcement learning methods. Othermethods include using score-based methods, variational autoencoders, ormutual-information-based methods. In this paper, we focus on generative modelsand, in particular, on a new promising method called diffusion models, whichhave shown a higher quality of generation in image-based tasks. We will showthat diffusion models can be used in wireless E2E scenarios and that they workas good as Wasserstein GANs while having a more stable training procedure and abetter generalization ability in testing.</description><author>Muah Kim, Rick Fritschek, Rafael F. Schaefer</author><pubDate>Wed, 29 Nov 2023 14:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01714v2</guid></item><item><title>AviationGPT: A Large Language Model for the Aviation Domain</title><link>http://arxiv.org/abs/2311.17686v1</link><description>The advent of ChatGPT and GPT-4 has captivated the world with large languagemodels (LLMs), demonstrating exceptional performance in question-answering,summarization, and content generation. The aviation industry is characterizedby an abundance of complex, unstructured text data, replete with technicaljargon and specialized terminology. Moreover, labeled data for model buildingare scarce in this domain, resulting in low usage of aviation text data. Theemergence of LLMs presents an opportunity to transform this situation, butthere is a lack of LLMs specifically designed for the aviation domain. Toaddress this gap, we propose AviationGPT, which is built on open-source LLaMA-2and Mistral architectures and continuously trained on a wealth of carefullycurated aviation datasets. Experimental results reveal that AviationGPT offersusers multiple advantages, including the versatility to tackle diverse naturallanguage processing (NLP) problems (e.g., question-answering, summarization,document writing, information extraction, report querying, data cleaning, andinteractive data exploration). It also provides accurate and contextuallyrelevant responses within the aviation domain and significantly improvesperformance (e.g., over a 40% performance gain in tested cases). WithAviationGPT, the aviation industry is better equipped to address more complexresearch problems and enhance the efficiency and safety of National AirspaceSystem (NAS) operations.</description><author>Liya Wang, Jason Chou, Xin Zhou, Alex Tien, Diane M Baumgartner</author><pubDate>Wed, 29 Nov 2023 14:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17686v1</guid></item><item><title>Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback</title><link>http://arxiv.org/abs/2310.05199v5</link><description>Reinforcement learning from human feedback serves as a crucial bridge,aligning large language models with human and societal values. This alignmentrequires a vast corpus of human feedback to learn a reward model, which issubsequently used to finetune language models. However, we have identified thatthe reward model often finds shortcuts to bypass its intended objectives,misleadingly assuming that humans prefer longer responses. The emergence oflength bias often induces the model to favor longer outputs, yet it doesn'tequate to an increase in helpful information within these outputs. In thispaper, we propose an innovative solution, applying the Product-of-Experts (PoE)technique to separate reward modeling from the influence of sequence length. Inour framework, the main expert concentrates on understanding human intents,while the biased expert targets the identification and capture of length bias.To further enhance the learning of bias, we introduce perturbations into thebias-focused expert, disrupting the flow of semantic information. Experimentalresults validate the effectiveness of our approach, indicating that languagemodel performance is improved, irrespective of sequence length.</description><author>Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Wed, 29 Nov 2023 14:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05199v5</guid></item><item><title>Adapting Sentence Transformers for the Aviation Domain</title><link>http://arxiv.org/abs/2305.09556v2</link><description>Learning effective sentence representations is crucial for many NaturalLanguage Processing (NLP) tasks, including semantic search, semantic textualsimilarity (STS), and clustering. While multiple transformer models have beendeveloped for sentence embedding learning, these models may not performoptimally when dealing with specialized domains like aviation, which has uniquecharacteristics such as technical jargon, abbreviations, and unconventionalgrammar. Furthermore, the absence of labeled datasets makes it difficult totrain models specifically for the aviation domain. To address these challenges,we propose a novel approach for adapting sentence transformers for the aviationdomain. Our method is a two-stage process consisting of pre-training followedby fine-tuning. During pre-training, we use Transformers and SequentialDenoising AutoEncoder (TSDAE) with aviation text data as input to improve theinitial model performance. Subsequently, we fine-tune our models using aNatural Language Inference (NLI) dataset in the Sentence Bidirectional EncoderRepresentations from Transformers (SBERT) architecture to mitigate overfittingissues. Experimental results on several downstream tasks show that our adaptedsentence transformers significantly outperform general-purpose transformers,demonstrating the effectiveness of our approach in capturing the nuances of theaviation domain. Overall, our work highlights the importance of domain-specificadaptation in developing high-quality NLP solutions for specialized industrieslike aviation.</description><author>Liya Wang, Jason Chou, Dave Rouck, Alex Tien, Diane M Baumgartner</author><pubDate>Wed, 29 Nov 2023 14:45:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09556v2</guid></item><item><title>Compressing the Backward Pass of Large-Scale Neural Architectures by Structured Activation Pruning</title><link>http://arxiv.org/abs/2311.16883v2</link><description>The rise of Deep Neural Networks (DNNs) has led to an increase in model sizeand complexity, straining the memory capacity of GPUs. Sparsity in DNNs,characterized as structural or ephemeral, has gained attention as a solution.This work focuses on ephemeral sparsity, aiming to reduce memory consumptionduring training. It emphasizes the significance of activations, an oftenoverlooked component, and their role in memory usage. This work employsstructured pruning in Block Sparse Compressed Row (BSR) format in combinationwith a magnitude-based criterion to efficiently prune activations. Wefurthermore introduce efficient block-sparse operators for GPUs and showcasetheir effectiveness, as well as the superior compression offered by blocksparsity. We report the effectiveness of activation pruning by evaluatingtraining speed, accuracy, and memory usage of large-scale neural architectureson the example of ResMLP on image classification tasks. As a result, we observea memory reduction of up to 32% while maintaining accuracy. Ultimately, ourapproach aims to democratize large-scale model training, reduce GPUrequirements, and address ecological concerns.</description><author>Daniel Barley, Holger FrÃ¶ning</author><pubDate>Wed, 29 Nov 2023 14:41:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16883v2</guid></item><item><title>FedAgg: Adaptive Federated Learning with Aggregated Gradients</title><link>http://arxiv.org/abs/2303.15799v3</link><description>Federated Learning (FL) has become an emerging norm for distributed modeltraining, which enables multiple devices cooperatively to train a shared modelutilizing their own datasets scheduled by a central server while keepingprivate data localized. However, during the training process, thenon-independent-and-identically-distributed (Non-IID) data generated onheterogeneous clients and frequent communication across participants maysignificantly influence the training performance, slow down the convergentrate, and increase communication consumption. In this paper, we ameliorate thestandard stochastic gradient descent approach by introducing the aggregatedgradients at each local update epoch and propose an adaptive learning rateiterative algorithm that further takes the deviation between the localparameter and global parameter into account. The aforementioned adaptivelearning rate design mechanism requires local information of all clients, whichis challenging as there is no communication during the local update epochs. Toobtain a decentralized adaptive learning rate for each client, we introduce themean-field approach by utilizing two mean-field terms to estimate the averagelocal parameters and gradients respectively without exchanging clients' localinformation with each other over time. Through theoretical analysis, we provethat our method can provide the convergence guarantee for model training andderive a convergent upper bound for the client drifting term. Extensivenumerical results show that our proposed framework is superior to thestate-of-the-art FL schemes in both model accuracy and convergent rate onreal-world datasets with IID and Non-IID data distribution.</description><author>Wenhao Yuan, Xuehe Wang</author><pubDate>Wed, 29 Nov 2023 14:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15799v3</guid></item><item><title>COVIDx CXR-4: An Expanded Multi-Institutional Open-Source Benchmark Dataset for Chest X-ray Image-Based Computer-Aided COVID-19 Diagnostics</title><link>http://arxiv.org/abs/2311.17677v1</link><description>The global ramifications of the COVID-19 pandemic remain significant,exerting persistent pressure on nations even three years after its initialoutbreak. Deep learning models have shown promise in improving COVID-19diagnostics but require diverse and larger-scale datasets to improveperformance. In this paper, we introduce COVIDx CXR-4, an expandedmulti-institutional open-source benchmark dataset for chest X-ray image-basedcomputer-aided COVID-19 diagnostics. COVIDx CXR-4 expands significantly on theprevious COVIDx CXR-3 dataset by increasing the total patient cohort size bygreater than 2.66 times, resulting in 84,818 images from 45,342 patients acrossmultiple institutions. We provide extensive analysis on the diversity of thepatient demographic, imaging metadata, and disease distributions to highlightpotential dataset biases. To the best of the authors' knowledge, COVIDx CXR-4is the largest and most diverse open-source COVID-19 CXR dataset and is madepublicly available as part of an open initiative to advance research to aidclinicians against the COVID-19 disease.</description><author>Yifan Wu, Hayden Gunraj, Chi-en Amy Tai, Alexander Wong</author><pubDate>Wed, 29 Nov 2023 14:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17677v1</guid></item><item><title>Improving Minority Stress Detection with Emotions</title><link>http://arxiv.org/abs/2311.17676v1</link><description>Psychological stress detection is an important task for mental healthcareresearch, but there has been little prior work investigating the effectivenessof psychological stress models on minority individuals, who are especiallyvulnerable to poor mental health outcomes. In this work, we use the relatedtask of minority stress detection to evaluate the ability of psychologicalstress models to understand the language of sexual and gender minorities. Wefind that traditional psychological stress models underperform on minoritystress detection, and we propose using emotion-infused models to reduce thatperformance disparity. We further demonstrate that multi-task psychologicalstress models outperform the current state-of-the-art for minority stressdetection without directly training on minority stress data. We provideexplanatory analysis showing that minority communities have differentdistributions of emotions than the general population and that emotion-infusedmodels improve the performance of stress models on underrepresented groupsbecause of their effectiveness in low-data environments, and we propose thatintegrating emotions may benefit underrepresented groups in other mental healthdetection tasks.</description><author>Jonathan Ivey, Susan Gauch</author><pubDate>Wed, 29 Nov 2023 14:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17676v1</guid></item><item><title>SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks</title><link>http://arxiv.org/abs/2310.03684v3</link><description>Despite efforts to align large language models (LLMs) with human values,widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible tojailbreaking attacks, wherein an adversary fools a targeted LLM into generatingobjectionable content. To address this vulnerability, we propose SmoothLLM, thefirst algorithm designed to mitigate jailbreaking attacks on LLMs. Based on ourfinding that adversarially-generated prompts are brittle to character-levelchanges, our defense first randomly perturbs multiple copies of a given inputprompt, and then aggregates the corresponding predictions to detect adversarialinputs. SmoothLLM reduces the attack success rate on numerous popular LLMs tobelow one percentage point, avoids unnecessary conservatism, and admitsprovable guarantees on attack mitigation. Moreover, our defense usesexponentially fewer queries than existing attacks and is compatible with anyLLM. Our code is publicly available at the following link:https://github.com/arobey1/smooth-llm.</description><author>Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas</author><pubDate>Wed, 29 Nov 2023 14:39:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03684v3</guid></item></channel></rss>