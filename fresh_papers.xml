<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 04 Aug 2023 06:00:27 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World</title><link>http://arxiv.org/abs/2308.01907v1</link><description>We present the All-Seeing (AS) project: a large-scale data and model forrecognizing and understanding everything in the open world. Using a scalabledata engine that incorporates human feedback and efficient models in the loop,we create a new dataset (AS-1B) with over 1 billion regions annotated withsemantic tags, question-answering pairs, and detailed captions. It covers awide range of 3.5 million common and rare concepts in the real world, and has132.2 billion tokens that describe the concepts and their attributes.Leveraging this new dataset, we develop the All-Seeing model (ASM), a unifiedframework for panoptic visual recognition and understanding. The model istrained with open-ended language prompts and locations, which allows it togeneralize to various vision and language tasks with remarkable zero-shotperformance, including region-text retrieval, region recognition, captioning,and question-answering. We hope that this project can serve as a foundation forvision-language artificial general intelligence research. Models and thedataset shall be released at https://github.com/OpenGVLab/All-Seeing, and democan be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.</description><author>Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, Yushi Chen, Tong Lu, Jifeng Dai, Yu Qiao</author><pubDate>Thu, 03 Aug 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01907v1</guid></item><item><title>Reasoning in Large Language Models Through Symbolic Math Word Problems</title><link>http://arxiv.org/abs/2308.01906v1</link><description>Large language models (LLMs) have revolutionized NLP by solving downstreamtasks with little to no labeled data. Despite their versatile abilities, thelarger question of their ability to reason remains ill-understood. This paperaddresses reasoning in math word problems (MWPs) by studying symbolic versionsof the numeric problems, since a symbolic expression is a "concise explanation"of the numeric answer. We create and use a symbolic version of the SVAMPdataset and find that GPT-3's davinci-002 model also has good zero-shotaccuracy on symbolic MWPs. To evaluate the faithfulness of the model'sreasoning, we go beyond accuracy and additionally evaluate the alignmentbetween the final answer and the outputted reasoning, which correspond tonumeric and symbolic answers respectively for MWPs. We explore a self-promptingapproach to encourage the symbolic reasoning to align with the numeric answer,thus equipping the LLM with the ability to provide a concise and verifiablereasoning and making it more interpretable. Surprisingly, self-prompting alsoimproves the symbolic accuracy to be higher than both the numeric and symbolicaccuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will bereleased for future research on symbolic math problems.</description><author>Vedant Gaur, Nikunj Saunshi</author><pubDate>Thu, 03 Aug 2023 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01906v1</guid></item><item><title>Revisiting Deformable Convolution for Depth Completion</title><link>http://arxiv.org/abs/2308.01905v1</link><description>Depth completion, which aims to generate high-quality dense depth maps fromsparse depth maps, has attracted increasing attention in recent years. Previouswork usually employs RGB images as guidance, and introduces iterative spatialpropagation to refine estimated coarse depth maps. However, most of thepropagation refinement methods require several iterations and suffer from afixed receptive field, which may contain irrelevant and useless informationwith very sparse input. In this paper, we address these two challengessimultaneously by revisiting the idea of deformable convolution. We propose aneffective architecture that leverages deformable kernel convolution as asingle-pass refinement module, and empirically demonstrate its superiority. Tobetter understand the function of deformable convolution and exploit it fordepth completion, we further systematically investigate a variety ofrepresentative strategies. Our study reveals that, different from prior work,deformable convolution needs to be applied on an estimated depth map with arelatively high density for better performance. We evaluate our model on thelarge-scale KITTI dataset and achieve state-of-the-art level performance inboth accuracy and inference speed. Our code is available athttps://github.com/AlexSunNik/ReDC.</description><author>Xinglong Sun, Jean Ponce, Yu-Xiong Wang</author><pubDate>Thu, 03 Aug 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01905v1</guid></item><item><title>DETR Doesn't Need Multi-Scale or Locality Design</title><link>http://arxiv.org/abs/2308.01904v1</link><description>This paper presents an improved DETR detector that maintains a "plain"nature: using a single-scale feature map and global cross-attentioncalculations without specific locality constraints, in contrast to previousleading DETR-based detectors that reintroduce architectural inductive biases ofmulti-scale and locality into the decoder. We show that two simple technologiesare surprisingly effective within a plain design to compensate for the lack ofmulti-scale feature maps and locality constraints. The first is a box-to-pixelrelative position bias (BoxRPB) term added to the cross-attention formulation,which well guides each query to attend to the corresponding object region whilealso providing encoding flexibility. The second is masked image modeling(MIM)-based backbone pre-training which helps learn representation withfine-grained localization ability and proves crucial for remedying dependencieson the multi-scale feature maps. By incorporating these technologies and recentadvancements in training and problem formation, the improved "plain" DETRshowed exceptional improvements over the original DETR detector. By leveragingthe Object365 dataset for pre-training, it achieved 63.9 mAP accuracy using aSwin-L backbone, which is highly competitive with state-of-the-art detectorswhich all heavily rely on multi-scale feature maps and region-based featureextraction. Code is available at https://github.com/impiga/Plain-DETR .</description><author>Yutong Lin, Yuhui Yuan, Zheng Zhang, Chen Li, Nanning Zheng, Han Hu</author><pubDate>Thu, 03 Aug 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01904v1</guid></item><item><title>Higher Chest X-ray Resolution Improves Classification Performance</title><link>http://arxiv.org/abs/2306.06051v2</link><description>Deep learning models for image classification are often trained at aresolution of 224 x 224 pixels for historical and efficiency reasons. However,chest X-rays are acquired at a much higher resolution to display subtlepathologies. This study investigates the effect of training resolution on chestX-ray classification performance, using the chest X-ray 14 dataset. The resultsshow that training with a higher image resolution, specifically 1024 x 1024pixels, results in the best overall classification performance with a mean AUCof 84.2 % compared to 82.7 % when trained with 256 x 256 pixel images.Additionally, comparison of bounding boxes and GradCAM saliency maps suggestthat low resolutions, such as 256 x 256 pixels, are insufficient foridentifying small pathologies and force the model to use spuriousdiscriminating features. Our code is publicly available athttps://gitlab.lrz.de/IP/cxr-resolution</description><author>Alessandro Wollek, Sardi Hyska, Bastian Sabel, Michael Ingrisch, Tobias Lasser</author><pubDate>Thu, 03 Aug 2023 18:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06051v2</guid></item><item><title>How many preprints have actually been printed and why: a case study of computer science preprints on arXiv</title><link>http://arxiv.org/abs/2308.01899v1</link><description>Preprints play an increasingly critical role in academic communities. Thereare many reasons driving researchers to post their manuscripts to preprintservers before formal submission to journals or conferences, but the use ofpreprints has also sparked considerable controversy, especially surrounding theclaim of priority. In this paper, a case study of computer science preprintssubmitted to arXiv from 2008 to 2017 is conducted to quantify how manypreprints have eventually been printed in peer-reviewed venues. Among thosepublished manuscripts, some are published under different titles and without anupdate to their preprints on arXiv. In the case of these manuscripts, thetraditional fuzzy matching method is incapable of mapping the preprint to thefinal published version. In view of this issue, we introduce a semantics-basedmapping method with the employment of Bidirectional Encoder Representationsfrom Transformers (BERT). With this new mapping method and a plurality of datasources, we find that 66% of all sampled preprints are published underunchanged titles and 11% are published under different titles and with othermodifications. A further analysis was then performed to investigate why thesepreprints but not others were accepted for publication. Our comparison revealsthat in the field of computer science, published preprints feature adequaterevisions, multiple authorship, detailed abstract and introduction, extensiveand authoritative references and available source code.</description><author>Jialiang Lin, Yao Yu, Yu Zhou, Zhiyang Zhou, Xiaodong Shi</author><pubDate>Thu, 03 Aug 2023 18:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01899v1</guid></item><item><title>UniSim: A Neural Closed-Loop Sensor Simulator</title><link>http://arxiv.org/abs/2308.01898v1</link><description>Rigorously testing autonomy systems is essential for making safe self-drivingvehicles (SDV) a reality. It requires one to generate safety critical scenariosbeyond what can be collected safely in the world, as many scenarios happenrarely on public roads. To accurately evaluate performance, we need to test theSDV on these scenarios in closed-loop, where the SDV and other actors interactwith each other at each timestep. Previously recorded driving logs provide arich resource to build these new scenarios from, but for closed loopevaluation, we need to modify the sensor data based on the new sceneconfiguration and the SDV's decisions, as actors might be added or removed andthe trajectories of existing actors and the SDV will differ from the originallog. In this paper, we present UniSim, a neural sensor simulator that takes asingle recorded log captured by a sensor-equipped vehicle and converts it intoa realistic closed-loop multi-sensor simulation. UniSim builds neural featuregrids to reconstruct both the static background and dynamic actors in thescene, and composites them together to simulate LiDAR and camera data at newviewpoints, with actors added or removed and at new placements. To betterhandle extrapolated views, we incorporate learnable priors for dynamic objects,and leverage a convolutional network to complete unseen regions. Ourexperiments show UniSim can simulate realistic sensor data with small domaingap on downstream tasks. With UniSim, we demonstrate closed-loop evaluation ofan autonomy system on safety-critical scenarios as if it were in the realworld.</description><author>Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, Raquel Urtasun</author><pubDate>Thu, 03 Aug 2023 18:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01898v1</guid></item><item><title>Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning</title><link>http://arxiv.org/abs/2308.01895v1</link><description>Continual learning seeks to enable deep learners to train on a series oftasks of unknown length without suffering from the catastrophic forgetting ofprevious tasks. One effective solution is replay, which involves storing fewprevious experiences in memory and replaying them when learning the currenttask. However, there is still room for improvement when it comes to selectingthe most informative samples for storage and determining the optimal number ofsamples to be stored. This study aims to address these issues with a novelcomparison of the commonly used reservoir sampling to various alternativepopulation strategies and providing a novel detailed analysis of how to findthe optimal number of stored samples.</description><author>Daniel Brignac, Niels Lobo, Abhijit Mahalanobis</author><pubDate>Thu, 03 Aug 2023 18:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01895v1</guid></item><item><title>LISA: Reasoning Segmentation via Large Language Model</title><link>http://arxiv.org/abs/2308.00692v2</link><description>Although perception systems have made remarkable advancements in recentyears, they still rely on explicit human instruction to identify the targetobjects or categories before executing visual recognition tasks. Such systemslack the ability to actively reason and comprehend implicit user intentions. Inthis work, we propose a new segmentation task -- reasoning segmentation. Thetask is designed to output a segmentation mask given a complex and implicitquery text. Furthermore, we establish a benchmark comprising over one thousandimage-instruction pairs, incorporating intricate reasoning and world knowledgefor evaluation purposes. Finally, we present LISA: large Language InstructedSegmentation Assistant, which inherits the language generation capabilities ofthe multi-modal Large Language Model (LLM) while also possessing the ability toproduce segmentation masks. We expand the original vocabulary with a &lt;SEG&gt;token and propose the embedding-as-mask paradigm to unlock the segmentationcapability. Remarkably, LISA can handle cases involving: 1) complex reasoning;2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also,it demonstrates robust zero-shot capability when trained exclusively onreasoning-free datasets. In addition, fine-tuning the model with merely 239reasoning segmentation image-instruction pairs results in further performanceenhancement. Experiments show our method not only unlocks new reasoningsegmentation capabilities but also proves effective in both complex reasoningsegmentation and standard referring segmentation tasks. Code, models, and demoare at https://github.com/dvlab-research/LISA.</description><author>Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia</author><pubDate>Thu, 03 Aug 2023 18:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00692v2</guid></item><item><title>Exact identification of nonlinear dynamical systems by Trimmed Lasso</title><link>http://arxiv.org/abs/2308.01891v1</link><description>Identification of nonlinear dynamical systems has been popularized by sparseidentification of the nonlinear dynamics (SINDy) via the sequentiallythresholded least squares (STLS) algorithm. Many extensions SINDy have emergedin the literature to deal with experimental data which are finite in length andnoisy. Recently, the computationally intensive method of ensemblingbootstrapped SINDy models (E-SINDy) was proposed for model identification,handling finite, highly noisy data. While the extensions of SINDy are numerous,their sparsity-promoting estimators occasionally provide sparse approximationsof the dynamics as opposed to exact recovery. Furthermore, these estimatorssuffer under multicollinearity, e.g. the irrepresentable condition for theLasso. In this paper, we demonstrate that the Trimmed Lasso for robustidentification of models (TRIM) can provide exact recovery under more severenoise, finite data, and multicollinearity as opposed to E-SINDy. Additionally,the computational cost of TRIM is asymptotically equal to STLS since thesparsity parameter of the TRIM can be solved efficiently by convex solvers. Wecompare these methodologies on challenging nonlinear systems, specifically theLorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmarkof No\"el and Schoukens, 2016, and a time delay system describing tool cuttingdynamics. This study emphasizes the comparisons between STLS, reweighted$\ell_1$ minimization, and Trimmed Lasso in identification with respect toproblems faced by practitioners: the problem of finite and noisy data, theperformance of the sparse regression of when the library grows in dimension(multicollinearity), and automatic methods for choice of regularizationparameters.</description><author>Shawn L. Kiser, Mikhail Guskov, Marc Rébillat, Nicolas Ranc</author><pubDate>Thu, 03 Aug 2023 18:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01891v1</guid></item><item><title>DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations</title><link>http://arxiv.org/abs/2308.01890v1</link><description>Multi-label image recognition in the low-label regime is a task of greatchallenge and practical significance. Previous works have focused on learningthe alignment between textual and visual spaces to compensate for limited imagelabels, yet may suffer from reduced accuracy due to the scarcity ofhigh-quality multi-label annotations. In this research, we leverage thepowerful alignment between textual and visual features pretrained with millionsof auxiliary image-text pairs. We introduce an efficient and effectiveframework called Evidence-guided Dual Context Optimization (DualCoOp++), whichserves as a unified approach for addressing partial-label and zero-shotmulti-label recognition. In DualCoOp++ we separately encode evidential,positive, and negative contexts for target classes as parametric components ofthe linguistic input (i.e., prompts). The evidential context aims to discoverall the related visual content for the target class, and serves as guidance toaggregate positive and negative contexts from the spatial domain of the image,enabling better distinguishment between similar categories. Additionally, weintroduce a Winner-Take-All module that promotes inter-class interaction duringtraining, while avoiding the need for extra parameters and costs. As DualCoOp++imposes minimal additional learnable overhead on the pretrained vision-languageframework, it enables rapid adaptation to multi-label recognition tasks withlimited annotations and even unseen classes. Experiments on standardmulti-label recognition benchmarks across two challenging low-label settingsdemonstrate the superior performance of our approach compared tostate-of-the-art methods.</description><author>Ping Hu, Ximeng Sun, Stan Sclaroff, Kate Saenko</author><pubDate>Thu, 03 Aug 2023 18:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01890v1</guid></item><item><title>FROD: Robust Object Detection for Free</title><link>http://arxiv.org/abs/2308.01888v1</link><description>Object detection is a vital task in computer vision and has become anintegral component of numerous critical systems. However, state-of-the-artobject detectors, similar to their classification counterparts, are susceptibleto small adversarial perturbations that can significantly alter their normalbehavior. Unlike classification, the robustness of object detectors has notbeen thoroughly explored. In this work, we take the initial step towardsbridging the gap between the robustness of classification and object detectionby leveraging adversarially trained classification models. Merely utilizingadversarially trained models as backbones for object detection does not resultin robustness. We propose effective modifications to the classification-basedbackbone to instill robustness in object detection without incurring anycomputational overhead. To further enhance the robustness achieved by theproposed modified backbone, we introduce two lightweight components: imitationloss and delayed adversarial training. Extensive experiments on the MS-COCO andPascal VOC datasets are conducted to demonstrate the effectiveness of ourproposed approach.</description><author>Muhammad, Awais, Weiming, Zhuang, Lingjuan, Lyu, Sung-Ho, Bae</author><pubDate>Thu, 03 Aug 2023 18:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01888v1</guid></item><item><title>Athena 2.0: Discourse and User Modeling in Open Domain Dialogue</title><link>http://arxiv.org/abs/2308.01887v1</link><description>Conversational agents are consistently growing in popularity and many peopleinteract with them every day. While many conversational agents act as personalassistants, they can have many different goals. Some are task-oriented, such asproviding customer support for a bank or making a reservation. Others aredesigned to be empathetic and to form emotional connections with the user. TheAlexa Prize Challenge aims to create a socialbot, which allows the user toengage in coherent conversations, on a range of popular topics that willinterest the user. Here we describe Athena 2.0, UCSC's conversational agent forAmazon's Socialbot Grand Challenge 4. Athena 2.0 utilizes a novelknowledge-grounded discourse model that tracks the entity links that Athenaintroduces into the dialogue, and uses them to constrain named-entityrecognition and linking, and coreference resolution. Athena 2.0 also relies ona user model to personalize topic selection and other aspects of theconversation to individual users.</description><author>Omkar Patil, Lena Reed, Kevin K. Bowden, Juraj Juraska, Wen Cui, Vrindavan Harrison, Rishi Rajasekaran, Angela Ramirez, Cecilia Li, Eduardo Zamora, Phillip Lee, Jeshwanth Bheemanpally, Rohan Pandey, Adwait Ratnaparkhi, Marilyn Walker</author><pubDate>Thu, 03 Aug 2023 18:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01887v1</guid></item><item><title>Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2306.05357v2</link><description>Text-to-image generative models have enabled high-resolution image synthesisacross different domains, but require users to specify the content they wish togenerate. In this paper, we consider the inverse problem -- given a collectionof different images, can we discover the generative concepts that representeach image? We present an unsupervised approach to discover generative conceptsfrom a collection of images, disentangling different art styles in paintings,objects, and lighting from kitchen scenes, and discovering image classes givenImageNet images. We show how such generative concepts can accurately representthe content of images, be recombined and composed to generate new artistic andhybrid images, and be further used as a representation for downstreamclassification tasks.</description><author>Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, Antonio Torralba</author><pubDate>Thu, 03 Aug 2023 18:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05357v2</guid></item><item><title>Distributed Online Private Learning of Convex Nondecomposable Objectives</title><link>http://arxiv.org/abs/2206.07944v4</link><description>We deal with a general distributed constrained online learning problem withprivacy over time-varying networks, where a class of nondecomposable objectivesare considered. Under this setting, each node only controls a part of theglobal decision, and the goal of all nodes is to collaboratively minimize theglobal cost over a time horizon $T$ while guarantees the security of thetransmitted information. For such problems, we first design a novel genericalgorithm framework, named as DPSDA, of differentially private distributedonline learning using the Laplace mechanism and the stochastic variants of dualaveraging method. Note that in the dual updates, all nodes of DPSDA employ thenoise-corrupted gradients for more generality. Then, we propose two algorithms,named as DPSDA-C and DPSDA-PS, under this framework. In DPSDA-C, the nodesimplement a circulation-based communication in the primal updates so as toalleviate the disagreements over time-varying undirected networks. In addition,for the extension to time-varying directed ones, the nodes implement thebroadcast-based push-sum dynamics in DPSDA-PS, which can achieve averageconsensus over arbitrary directed networks. Theoretical results show that bothalgorithms attain an expected regret upper bound in $\mathcal{O}( \sqrt{T} )$when the objective function is convex, which matches the best utilityachievable by cutting-edge algorithms. Finally, numerical experiment results onboth synthetic and real-world datasets verify the effectiveness of ouralgorithms.</description><author>Huqiang Cheng, Xiaofeng Liao, Huaqing Li</author><pubDate>Thu, 03 Aug 2023 18:02:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.07944v4</guid></item><item><title>Masked Diffusion Models Are Fast and Privacy-Aware Learners</title><link>http://arxiv.org/abs/2306.11363v2</link><description>Diffusion models have emerged as the \emph{de-facto} technique for imagegeneration, yet they entail significant computational overhead, hindering thetechnique's broader application in the research community. We propose aprior-based denoising training framework, the first to incorporate thepre-train and fine-tune paradigm into the diffusion model training process,which substantially improves training efficiency and shows potential infacilitating various downstream tasks. Our approach centers on masking a highproportion (e.g., up to 90\%) of the input image and employing masked denoisingscore matching to denoise the visible areas, thereby guiding the diffusionmodel to learn more salient features from training data as prior knowledge. Byutilizing masked learning in a pre-training stage, we efficiently train theViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space,achieving a 4x acceleration and enhancing the quality of generated imagescompared to denoising diffusion probabilistic model (DDPM). Moreover, ourmasked pre-training technique can be universally applied to various diffusionmodels that directly generate images in the pixel space, aiding in the learningof pre-trained models with superior generalizability. For instance, a diffusionmodel pre-trained on VGGFace2 attains a 46\% quality improvement throughfine-tuning with merely 10\% data from a different distribution. Moreover, ourmethod shows the potential to serve as a training paradigm for enhancing theprivacy protection capabilities of diffusion models. Our code is available at\url{https://github.com/jiachenlei/maskdm}.</description><author>Jiachen Lei, Peng Cheng, Zhongjie Ba, Kui Ren</author><pubDate>Thu, 03 Aug 2023 17:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11363v2</guid></item><item><title>Thespian: Multi-Character Text Role-Playing Game Agents</title><link>http://arxiv.org/abs/2308.01872v1</link><description>Text-adventure games and text role-playing games are grand challenges forreinforcement learning game playing agents. Text role-playing games areopen-ended environments where an agent must faithfully play a particularcharacter. We consider the distinction between characters and actors, where anactor agent has the ability to play multiple characters. We present a frameworkwe call a thespian agent that can learn to emulate multiple characters alongwith a soft prompt that can be used to direct it as to which character to playat any time. We further describe an attention mechanism that allows the agentto learn new characters that are based on previously learned characters in afew-shot fashion. We show that our agent outperforms the state of the art agentframework in multi-character learning and few-shot learning.</description><author>Christopher Cui, Xiangyu Peng, Mark Riedl</author><pubDate>Thu, 03 Aug 2023 17:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01872v1</guid></item><item><title>No Agreement Without Loss: Learning and Social Choice in Peer Review</title><link>http://arxiv.org/abs/2211.02144v2</link><description>In peer review systems, reviewers are often asked to evaluate variousfeatures of submissions, such as technical quality or novelty. A score is givento each of the predefined features and based on these the reviewer has toprovide an overall quantitative recommendation. It may be assumed that eachreviewer has her own mapping from the set of features to a recommendation, andthat different reviewers have different mappings in mind. This introduces anelement of arbitrariness known as commensuration bias. In this paper we discussa framework, introduced by Noothigattu, Shah and Procaccia, and then applied bythe organizers of the AAAI 2022 conference. Noothigattu, Shah and Procacciaproposed to aggregate reviewer's mapping by minimizing certain loss functions,and studied axiomatic properties of this approach, in the sense of socialchoice theory. We challenge several of the results and assumptions used intheir work and report a number of negative results. On the one hand, we study atrade-off between some of the axioms proposed and the ability of the method toproperly capture agreements of the majority of reviewers. On the other hand, weshow that dropping a certain unrealistic assumption has dramatic effects,including causing the method to be discontinuous.</description><author>Pablo Barceló, Mauricio Duarte, Cristóbal Rojas, Tomasz Steifer</author><pubDate>Thu, 03 Aug 2023 17:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02144v2</guid></item><item><title>Tag Prediction of Competitive Programming Problems using Deep Learning Techniques</title><link>http://arxiv.org/abs/2308.01863v1</link><description>In the past decade, the amount of research being done in the fields ofmachine learning and deep learning, predominantly in the area of naturallanguage processing (NLP), has risen dramatically. A well-liked method fordeveloping programming abilities like logic building and problem solving iscompetitive programming. It can be tough for novices and even veteranprogrammers to traverse the wide collection of questions due to the massivenumber of accessible questions and the variety of themes, levels of difficulty,and questions offered. In order to help programmers find questions that areappropriate for their knowledge and interests, there is a need for an automatedmethod. This can be done using automated tagging of the questions using TextClassification. Text classification is one of the important tasks widelyresearched in the field of Natural Language Processing. In this paper, wepresent a way to use text classification techniques to determine the domain ofa competitive programming problem. A variety of models, including areimplemented LSTM, GRU, and MLP. The dataset has been scraped from Codeforces, amajor competitive programming website. A total of 2400 problems were scrapedand preprocessed, which we used as a dataset for our training and testing ofmodels. The maximum accuracy reached using our model is 78.0% by MLP(MultiLayer Perceptron).</description><author>Taha Lokat, Divyam Prajapati, Shubhada Labde</author><pubDate>Thu, 03 Aug 2023 17:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01863v1</guid></item><item><title>Wider and Deeper LLM Networks are Fairer LLM Evaluators</title><link>http://arxiv.org/abs/2308.01862v1</link><description>Measuring the quality of responses generated by LLMs is a challenging task,particularly when it comes to evaluating whether the response is aligned withhuman preference. A novel approach involves using the LLM itself to makeevaluation and stabilizing the results through multiple independentevaluations, similar to a single-layer narrow LLM network. This networkconsists of a fixed number of neurons, with each neuron being the same LLM. Inthis paper, we draw upon the extensive research on deep neural networks toexplore whether deeper and wider networks can lead to fairer evaluations.Specifically, inspired by the observation that different neurons in a neuralnetwork are responsible for detecting different concepts, we first adaptivelygenerate as many neuron roles as possible for each evaluation sample. Eachperspective corresponds to the role of a specific LLM neuron in the firstlayer. In subsequent layers, we follow the idea that higher layers in deepnetworks are responsible for more comprehensive features, each layer receivesrepresentations from all neurons in the previous layer, integrating the locallylearned evaluation information to obtain a more comprehensive evaluationresult. Interestingly, this network design resembles the process of academicpaper reviewing. To validate the effectiveness of our method, we construct thelargest and most diverse English evaluation benchmark LLMEval$^2$ for LLMevaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimentalresults demonstrate that a wider network (involving many reviewers) with 2layers (one round of discussion) performs the best, improving kappa correlationcoefficient from 0.28 to 0.34. We also leverage WideDeep to aid in theassessment of Chinese LLMs, which has accelerated the evaluation time by 4.6times, resulting in a 60% cost saving. WideDeep achieves a remarkable 93%agreement level among humans.</description><author>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li</author><pubDate>Thu, 03 Aug 2023 17:38:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01862v1</guid></item><item><title>ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation</title><link>http://arxiv.org/abs/2308.01861v1</link><description>In this work, we make the first attempt to evaluate LLMs in a morechallenging code generation scenario, i.e. class-level code generation. Wefirst manually construct the first class-level code generation benchmarkClassEval of 100 class-level Python code generation tasks with approximately500 person-hours. Based on it, we then perform the first study of 11state-of-the-art LLMs on class-level code generation. Based on our results, wehave the following main findings. First, we find that all existing LLMs showmuch worse performance on class-level code generation compared to on standalonemethod-level code generation benchmarks like HumanEval; and the method-levelcoding ability cannot equivalently reflect the class-level coding ability amongLLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superiorthan other LLMs on class-level code generation, and the second-tier modelsincludes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with verysimilar performance. Third, we find that generating the entire class all atonce (i.e. holistic generation strategy) is the best generation strategy onlyfor GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental andcompositional) is better strategies for the other models with limited abilityof understanding long instructions and utilizing the middle information.Lastly, we find the limited model ability of generating method-dependent codeand discuss the frequent error types in generated classes. Our benchmark isavailable at https://github.com/FudanSELab/ClassEval.</description><author>Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou</author><pubDate>Thu, 03 Aug 2023 17:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01861v1</guid></item><item><title>WindowNet: Learnable Windows for Chest X-ray Classification</title><link>http://arxiv.org/abs/2306.06038v2</link><description>Chest X-ray (CXR) images are commonly compressed to a lower resolution andbit depth to reduce their size, potentially altering subtle diagnosticfeatures. Radiologists use windowing operations to enhance image contrast, but theimpact of such operations on CXR classification performance is unclear. In this study, we show that windowing can improve CXR classificationperformance, and propose WindowNet, a model that learns optimal windowsettings. We first investigate the impact of bit-depth on classification performanceand find that a higher bit-depth (12-bit) leads to improved performance. We then evaluate different windowing settings and show that training with adistinct window generally improves pathology-wise classification performance. Finally, we propose and evaluate WindowNet, a model that learns optimalwindow settings, and show that it significantly improves performance comparedto the baseline model without windowing.</description><author>Alessandro Wollek, Sardi Hyska, Bastian Sabel, Michael Ingrisch, Tobias Lasser</author><pubDate>Thu, 03 Aug 2023 17:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06038v2</guid></item><item><title>Reconstructing Three-Dimensional Models of Interacting Humans</title><link>http://arxiv.org/abs/2308.01854v1</link><description>Understanding 3d human interactions is fundamental for fine-grained sceneanalysis and behavioural modeling. However, most of the existing models predictincorrect, lifeless 3d estimates, that miss the subtle human contactaspects--the essence of the event--and are of little use for detailedbehavioral understanding. This paper addresses such issues with severalcontributions: (1) we introduce models for interaction signature estimation(ISP) encompassing contact detection, segmentation, and 3d contact signatureprediction; (2) we show how such components can be leveraged to ensure contactconsistency during 3d reconstruction; (3) we construct several large datasetsfor learning and evaluating 3d contact prediction and reconstruction methods;specifically, we introduce CHI3D, a lab-based accurate 3d motion capturedataset with 631 sequences containing $2,525$ contact events, $728,664$ groundtruth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with$14,081$ processed pairs of people, and $81,233$ facet-level surfacecorrespondences. Finally, (4) we propose methodology for recovering theground-truth pose and shape of interacting people in a controlled setup and (5)annotate all 3d interaction motions in CHI3D with textual descriptions. Motiondata in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) ismade available for research purposes at \url{https://ci3d.imar.ro}, togetherwith an evaluation server and a public benchmark.</description><author>Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, Cristian Sminchisescu</author><pubDate>Thu, 03 Aug 2023 17:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01854v1</guid></item><item><title>Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory</title><link>http://arxiv.org/abs/2308.01853v1</link><description>Distribution shifts are a serious concern in modern statistical learning asthey can systematically change the properties of the data away from the truth.We focus on Wasserstein distribution shifts, where every data point may undergoa slight perturbation, as opposed to the Huber contamination model where afraction of observations are outliers. We formulate and study shifts beyondindependent perturbations, exploring Joint Distribution Shifts, where theper-observation perturbations can be coordinated. We analyze several importantstatistical problems, including location estimation, linear regression, andnon-parametric density estimation. Under a squared loss for mean estimation andprediction error in linear regression, we find the exact minimax risk, a leastfavorable perturbation, and show that the sample mean and least squaresestimators are respectively optimal. This holds for both independent and jointshifts, but the least favorable perturbations and minimax risks differ. Forother problems, we provide nearly optimal estimators and precise finite-samplebounds. We also introduce several tools for bounding the minimax risk underdistribution shift, such as a smoothing technique for location families, andgeneralizations of classical tools including least favorable sequences ofpriors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.</description><author>Patrick Chao, Edgar Dobriban</author><pubDate>Thu, 03 Aug 2023 17:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01853v1</guid></item><item><title>Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling</title><link>http://arxiv.org/abs/2308.01850v1</link><description>Text-to-motion generation has gained increasing attention, but most existingmethods are limited to generating short-term motions that correspond to asingle sentence describing a single action. However, when a text streamdescribes a sequence of continuous motions, the generated motions correspondingto each sentence may not be coherently linked. Existing long-term motiongeneration methods face two main issues. Firstly, they cannot directly generatecoherent motions and require additional operations such as interpolation toprocess the generated actions. Secondly, they generate subsequent actions in anautoregressive manner without considering the influence of future actions onprevious ones. To address these issues, we propose a novel approach thatutilizes a past-conditioned diffusion model with two optional coherent samplingmethods: Past Inpainting Sampling and Compositional Transition Sampling. PastInpainting Sampling completes subsequent motions by treating previous motionsas conditions, while Compositional Transition Sampling models the distributionof the transition as the composition of two adjacent motions guided bydifferent text prompts. Our experimental results demonstrate that our proposedmethod is capable of generating compositional and coherent long-term 3D humanmotions controlled by a user-instructed long text stream. The code is availableat\href{https://github.com/yangzhao1230/PCMDM}{https://github.com/yangzhao1230/PCMDM}.</description><author>Zhao Yang, Bing Su, Ji-Rong Wen</author><pubDate>Thu, 03 Aug 2023 17:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01850v1</guid></item><item><title>Curricular Transfer Learning for Sentence Encoded Tasks</title><link>http://arxiv.org/abs/2308.01849v1</link><description>Fine-tuning language models in a downstream task is the standard approach formany state-of-the-art methodologies in the field of NLP. However, when thedistribution between the source task and target task drifts, \textit{e.g.},conversational environments, these gains tend to be diminished. This articleproposes a sequence of pre-training steps (a curriculum) guided by "datahacking" and grammar analysis that allows further gradual adaptation betweenpre-training distributions. In our experiments, we acquire a considerableimprovement from our method compared to other known pre-training approaches forthe MultiWoZ task.</description><author>Jader Martins Camboim de Sá, Matheus Ferraroni Sanches, Rafael Roque de Souza, Júlio Cesar dos Reis, Leandro Aparecido Villas</author><pubDate>Thu, 03 Aug 2023 17:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01849v1</guid></item><item><title>XNLP: An Interactive Demonstration System for Universal Structured NLP</title><link>http://arxiv.org/abs/2308.01846v1</link><description>Structured Natural Language Processing (XNLP) is an important subset of NLPthat entails understanding the underlying semantic or syntactic structure oftexts, which serves as a foundational component for many downstreamapplications. Despite certain recent efforts to explore universal solutions forspecific categories of XNLP tasks, a comprehensive and effective approach forunifying all XNLP tasks long remains underdeveloped. In the meanwhile, whileXNLP demonstration systems are vital for researchers exploring various XNLPtasks, existing platforms can be limited to, e.g., supporting few XNLP tasks,lacking interactivity and universalness. To this end, we propose an advancedXNLP demonstration platform, where we propose leveraging LLM to achieveuniversal XNLP, with one model for all with high generalizability. Overall, oursystem advances in multiple aspects, including universal XNLP modeling, highperformance, interpretability, scalability, and interactivity, providing aunified platform for exploring diverse XNLP tasks in the community. XNLP isonline: https://xnlp.haofei.vip</description><author>Hao Fei, Meishan Zhang, Min Zhang, Tat-Seng Chua</author><pubDate>Thu, 03 Aug 2023 17:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01846v1</guid></item><item><title>Mlinear: Rethink the Linear Model for Time-series Forecasting</title><link>http://arxiv.org/abs/2305.04800v2</link><description>Recently, significant advancements have been made in time-series forecastingresearch, with an increasing focus on analyzing the nature of time-series data,e.g, channel-independence (CI) and channel-dependence (CD), rather than solelyfocusing on designing sophisticated forecasting models. However, currentresearch has primarily focused on either CI or CD in isolation, and thechallenge of effectively combining these two opposing properties to achieve asynergistic effect remains an unresolved issue. In this paper, we carefullyexamine the opposing properties of CI and CD, and raise a practical questionthat has not been effectively answered, e.g.,"How to effectively mix the CI andCD properties of time series to achieve better predictive performance?" Toanswer this question, we propose Mlinear (MIX-Linear), a simple yet effectivemethod based mainly on linear layers. The design philosophy of Mlinear mainlyincludes two aspects:(1) dynamically tuning the CI and CD properties based onthe time semantics of different input time series, and (2) providing deepsupervision to adjust the individual performance of the "CI predictor" and "CDpredictor". In addition, empirically, we introduce a new loss function thatsignificantly outperforms the widely used mean squared error (MSE) on multipledatasets. Experiments on time-series datasets covering multiple fields andwidely used have demonstrated the superiority of our method over PatchTST whichis the lateset Transformer-based method in terms of the MSE and MAE metrics on7 datasets with identical sequence inputs (336 or 512). Specifically, ourmethod significantly outperforms PatchTST with a ratio of 21:3 at 336 sequencelength input and 29:10 at 512 sequence length input. Additionally, our approachhas a 10 $\times$ efficiency advantage at the unit level, taking into accountboth training and inference times.</description><author>Wei Li, Xiangxu Meng, Chuhao Chen, Jianing Chen</author><pubDate>Thu, 03 Aug 2023 17:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04800v2</guid></item><item><title>Matrix Estimation for Individual Fairness</title><link>http://arxiv.org/abs/2302.02096v2</link><description>In recent years, multiple notions of algorithmic fairness have arisen. Onesuch notion is individual fairness (IF), which requires that individuals whoare similar receive similar treatment. In parallel, matrix estimation (ME) hasemerged as a natural paradigm for handling noisy data with missing values. Inthis work, we connect the two concepts. We show that pre-processing data usingME can improve an algorithm's IF without sacrificing performance. Specifically,we show that using a popular ME method known as singular value thresholding(SVT) to pre-process the data provides a strong IF guarantee under appropriateconditions. We then show that, under analogous conditions, SVT pre-processingalso yields estimates that are consistent and approximately minimax optimal. Assuch, the ME pre-processing step does not, under the stated conditions,increase the prediction error of the base algorithm, i.e., does not impose afairness-performance trade-off. We verify these results on synthetic and realdata.</description><author>Cindy Y. Zhang, Sarah H. Cen, Devavrat Shah</author><pubDate>Thu, 03 Aug 2023 17:06:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02096v2</guid></item><item><title>URET: Universal Robustness Evaluation Toolkit (for Evasion)</title><link>http://arxiv.org/abs/2308.01840v1</link><description>Machine learning models are known to be vulnerable to adversarial evasionattacks as illustrated by image classification models. Thoroughly understandingsuch attacks is critical in order to ensure the safety and robustness ofcritical AI tasks. However, most evasion attacks are difficult to deployagainst a majority of AI systems because they have focused on image domain withonly few constraints. An image is composed of homogeneous, numerical,continuous, and independent features, unlike many other input types to AIsystems used in practice. Furthermore, some input types include additionalsemantic and functional constraints that must be observed to generate realisticadversarial inputs. In this work, we propose a new framework to enable thegeneration of adversarial inputs irrespective of the input type and taskdomain. Given an input and a set of pre-defined input transformations, ourframework discovers a sequence of transformations that result in a semanticallycorrect and functional adversarial input. We demonstrate the generality of ourapproach on several diverse machine learning tasks with various inputrepresentations. We also show the importance of generating adversarial examplesas they enable the deployment of mitigation techniques.</description><author>Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy, Masha Zorin</author><pubDate>Thu, 03 Aug 2023 17:05:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01840v1</guid></item><item><title>Causal Discovery from Temporal Data: An Overview and New Perspectives</title><link>http://arxiv.org/abs/2303.10112v3</link><description>Temporal data, representing chronological observations of complex systems,has always been a typical data structure that can be widely generated by manydomains, such as industry, medicine and finance. Analyzing this type of data isextremely valuable for various applications. Thus, different temporal dataanalysis tasks, eg, classification, clustering and prediction, have beenproposed in the past decades. Among them, causal discovery, learning the causalrelations from temporal data, is considered an interesting yet critical taskand has attracted much research attention. Existing causal discovery works canbe divided into two highly correlated categories according to whether thetemporal data is calibrated, ie, multivariate time series causal discovery, andevent sequence causal discovery. However, most previous surveys are onlyfocused on the time series causal discovery and ignore the second category. Inthis paper, we specify the correlation between the two categories and provide asystematical overview of existing solutions. Furthermore, we provide publicdatasets, evaluation metrics and new perspectives for temporal data causaldiscovery.</description><author>Chang Gong, Di Yao, Chuzhe Zhang, Wenbin Li, Jingping Bi</author><pubDate>Thu, 03 Aug 2023 17:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10112v3</guid></item><item><title>Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data</title><link>http://arxiv.org/abs/2308.01839v1</link><description>Single-cell data integration can provide a comprehensive molecular view ofcells, and many algorithms have been developed to remove unwanted technical orbiological variations and integrate heterogeneous single-cell datasets. Despitetheir wide usage, existing methods suffer from several fundamental limitations.In particular, we lack a rigorous statistical test for whether twohigh-dimensional single-cell datasets are alignable (and therefore should evenbe aligned). Moreover, popular methods can substantially distort the dataduring alignment, making the aligned data and downstream analysis difficult tointerpret. To overcome these limitations, we present a spectral manifoldalignment and inference (SMAI) framework, which enables principled andinterpretable alignability testing and structure-preserving integration ofsingle-cell data. SMAI provides a statistical test to robustly determine thealignability between datasets to avoid misleading inference, and is justifiedby high-dimensional statistical theory. On a diverse range of real andsimulated benchmark datasets, it outperforms commonly used alignment methods.Moreover, we show that SMAI improves various downstream analyses such asidentification of differentially expressed genes and imputation of single-cellspatial transcriptomics, providing further biological insights. SMAI'sinterpretability also enables quantification and a deeper understanding of thesources of technical confounders in single-cell data.</description><author>Rong Ma, Eric D. Sun, David Donoho, James Zou</author><pubDate>Thu, 03 Aug 2023 17:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01839v1</guid></item><item><title>Subspace-Constrained Continuous Methane Leak Monitoring and Optimal Sensor Placement</title><link>http://arxiv.org/abs/2308.01836v1</link><description>This work presents a procedure that can quickly identify and isolate methaneemission sources leading to expedient remediation. Minimizing the time requiredto identify a leak and the subsequent time to dispatch repair crews cansignificantly reduce the amount of methane released into the atmosphere. Theprocedure developed utilizes permanently installed low-cost methane sensors atan oilfield facility to continuously monitor leaked gas concentration abovebackground levels. The methods developed for optimal sensor placement and leakinversion in consideration of predefined subspaces and restricted zones arepresented. In particular, subspaces represent regions comprising one or moreequipment items that may leak, and restricted zones define regions in which asensor may not be placed due to site restrictions by design. Thus, subspacesconstrain the inversion problem to specified locales, while restricted zonesconstrain sensor placement to feasible zones. The development of synthetic windmodels, and those based on historical data, are also presented as a means toaccommodate optimal sensor placement under wind uncertainty. The wind modelsserve as realizations for planning purposes, with the aim of maximizing themean coverage measure for a given number of sensors. Once the optimal design isestablished, continuous real-time monitoring permits localization andquantification of a methane leak source. The necessary methods, mathematicalformulation and demonstrative test results are presented.</description><author>Kashif Rashid, Lukasz Zielinski, Junyi Yuan, Andrew Speck</author><pubDate>Thu, 03 Aug 2023 16:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01836v1</guid></item><item><title>Distribution-Free Inference for the Regression Function of Binary Classification</title><link>http://arxiv.org/abs/2308.01835v1</link><description>One of the key objects of binary classification is the regression function,i.e., the conditional expectation of the class labels given the inputs. Withthe regression function not only a Bayes optimal classifier can be defined, butit also encodes the corresponding misclassification probabilities. The paperpresents a resampling framework to construct exact, distribution-free andnon-asymptotically guaranteed confidence regions for the true regressionfunction for any user-chosen confidence level. Then, specific algorithms aresuggested to demonstrate the framework. It is proved that the constructedconfidence regions are strongly consistent, that is, any false model isexcluded in the long run with probability one. The exclusion is quantified withprobably approximately correct type bounds, as well. Finally, the algorithmsare validated via numerical experiments, and the methods are compared toapproximate asymptotic confidence ellipsoids.</description><author>Ambrus Tamás, Balázs Csanád Csáji</author><pubDate>Thu, 03 Aug 2023 16:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01835v1</guid></item><item><title>The Capability of Large Language Models to Measure Psychiatric Functioning</title><link>http://arxiv.org/abs/2308.01834v1</link><description>The current work investigates the capability of Large language models (LLMs)that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)to predict psychiatric functioning from patient interviews and clinicaldescriptions without being trained to do so. To assess this, n = 145 depressionand n =115 PTSD assessments and n = 46 clinical case studies across highprevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, traumaand stress, Addictive disorders) were analyzed using prompts to extractestimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 iscapable of assessing psychiatric functioning across a range of psychiatricconditions with the strongest performance being the prediction of depressionscores based on standardized assessments (Accuracy range= 0.80 - 0.84) whichwere statistically indistinguishable from human clinical raters t(1,144) =1.20; p = 0.23. Results show the potential for general clinical language modelsto flexibly predict psychiatric risk based on free descriptions of functioningfrom both patients and clinicians.</description><author>Isaac R. Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, Matteo Malgaroli</author><pubDate>Thu, 03 Aug 2023 16:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01834v1</guid></item><item><title>Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation</title><link>http://arxiv.org/abs/2308.01831v1</link><description>In this paper, we propose a method to learn unified representations ofmultilingual speech and text with a single model, especially focusing on thepurpose of speech synthesis. We represent multilingual speech audio with speechunits, the quantized representations of speech features encoded from aself-supervised speech model. Therefore, we can focus on their linguisticcontent by treating the audio as pseudo text and can build a unifiedrepresentation of speech and text. Then, we propose to train an encoder-decoderstructured model with a Unit-to-Unit Translation (UTUT) objective onmultilingual data. Specifically, by conditioning the encoder with the sourcelanguage token and the decoder with the target language token, the model isoptimized to translate the spoken language into that of the target language, ina many-to-many language translation setting. Therefore, the model can build theknowledge of how spoken languages are comprehended and how to relate them todifferent languages. A single pre-trained model with UTUT can be employed fordiverse multilingual speech- and text-related tasks, such as Speech-to-SpeechTranslation (STS), multilingual Text-to-Speech Synthesis (TTS), andText-to-Speech Translation (TTST). By conducting comprehensive experimentsencompassing various languages, we validate the efficacy of the proposed methodacross diverse multilingual tasks. Moreover, we show UTUT can performmany-to-many language STS, which has not been previously explored in theliterature. Samples are available on https://choijeongsoo.github.io/utut.</description><author>Minsu Kim, Jeongsoo Choi, Dahun Kim, Yong Man Ro</author><pubDate>Thu, 03 Aug 2023 16:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01831v1</guid></item><item><title>Learning beyond sensations: how dreams organize neuronal representations</title><link>http://arxiv.org/abs/2308.01830v1</link><description>Semantic representations in higher sensory cortices form the basis forrobust, yet flexible behavior. These representations are acquired over thecourse of development in an unsupervised fashion and continuously maintainedover an organism's lifespan. Predictive learning theories propose that theserepresentations emerge from predicting or reconstructing sensory inputs.However, brains are known to generate virtual experiences, such as duringimagination and dreaming, that go beyond previously experienced inputs. Here,we suggest that virtual experiences may be just as relevant as actual sensoryinputs in shaping cortical representations. In particular, we discuss twocomplementary learning principles that organize representations through thegeneration of virtual experiences. First, "adversarial dreaming" proposes thatcreative dreams support a cortical implementation of adversarial learning inwhich feedback and feedforward pathways engage in a productive game of tryingto fool each other. Second, "contrastive dreaming" proposes that the invarianceof neuronal representations to irrelevant factors of variation is acquired bytrying to map similar virtual experiences together via a contrastive learningprocess. These principles are compatible with known cortical structure anddynamics and the phenomenology of sleep thus providing promising directions toexplain cortical learning beyond the classical predictive learning paradigm.</description><author>Nicolas Deperrois, Mihai A. Petrovici, Walter Senn, Jakob Jordan</author><pubDate>Thu, 03 Aug 2023 16:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01830v1</guid></item><item><title>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2308.01825v1</link><description>Mathematical reasoning is a challenging task for large language models(LLMs), while the scaling relationship of it with respect to LLM capacity isunder-explored. In this paper, we investigate how the pre-training loss,supervised data amount, and augmented data amount influence the reasoningperformances of a supervised LLM. We find that pre-training loss is a betterindicator of the model's performance than the model's parameter count. We applysupervised fine-tuning (SFT) with different amounts of supervised data andempirically find a log-linear relation between data amount and modelperformance, and we find better models improve less with enlarged superviseddatasets. To augment more data samples for improving model performances withoutany human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFTuses supervised models to generate and collect correct reasoning paths asaugmented fine-tuning datasets. We find with augmented samples containing moredistinct reasoning paths, RFT improves mathematical reasoning performance morefor LLMs. We also find RFT brings more improvement for less performant LLMs.Furthermore, we combine rejection samples from multiple models which pushLLaMA-7B to an accuracy of 49.3% and outperforms the supervised fine-tuning(SFT) accuracy of 35.9% significantly.</description><author>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou</author><pubDate>Thu, 03 Aug 2023 16:34:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01825v1</guid></item><item><title>Hard Adversarial Example Mining for Improving Robust Fairness</title><link>http://arxiv.org/abs/2308.01823v1</link><description>Adversarial training (AT) is widely considered the state-of-the-art techniquefor improving the robustness of deep neural networks (DNNs) against adversarialexamples (AE). Nevertheless, recent studies have revealed that adversariallytrained models are prone to unfairness problems, restricting theirapplicability. In this paper, we empirically observe that this limitation maybe attributed to serious adversarial confidence overfitting, i.e., certainadversarial examples with overconfidence. To alleviate this problem, we proposeHAM, a straightforward yet effective framework via adaptive Hard Adversarialexample Mining.HAM concentrates on mining hard adversarial examples whilediscarding the easy ones in an adaptive fashion. Specifically, HAM identifieshard AEs in terms of their step sizes needed to cross the decision boundarywhen calculating loss value. Besides, an early-dropping mechanism isincorporated to discard the easy examples at the initial stages of AEgeneration, resulting in efficient AT. Extensive experimental results onCIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significantimprovement in robust fairness while reducing computational cost compared toseveral state-of-the-art adversarial training methods. The code will be madepublicly available.</description><author>Chenhao Lin, Xiang Ji, Yulong Yang, Qian Li, Chao Shen, Run Wang, Liming Fang</author><pubDate>Thu, 03 Aug 2023 16:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01823v1</guid></item><item><title>Are LLMs All You Need for Task-Oriented Dialogue?</title><link>http://arxiv.org/abs/2304.06556v2</link><description>Instructions-tuned Large Language Models (LLMs) gained recently hugepopularity thanks to their ability to interact with users through conversation.In this work we aim to evaluate their ability to complete multi-turn tasks andinteract with external databases in the context of established task-orienteddialogue benchmarks. We show that for explicit belief state tracking, LLMsunderperform compared to specialized task-specific models. Nevertheless, theyshow ability to guide the dialogue to successful ending if given correct slotvalues. Furthermore this ability improves with access to true belief statedistribution or in-domain examples.</description><author>Vojtěch Hudeček, Ondřej Dušek</author><pubDate>Thu, 03 Aug 2023 16:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06556v2</guid></item><item><title>Random Planted Forest: a directly interpretable tree ensemble</title><link>http://arxiv.org/abs/2012.14563v3</link><description>We introduce a novel interpretable tree based algorithm for prediction in aregression setting. Our motivation is to estimate the unknown regressionfunction from a functional decomposition perspective in which the functionalcomponents correspond to lower order interaction terms. The idea is to modifythe random forest algorithm by keeping certain leaves after they are splitinstead of deleting them. This leads to non-binary trees which we refer to asplanted trees. An extension to a forest leads to our random planted forestalgorithm. Additionally, the maximum number of covariates which can interactwithin a leaf can be bounded. If we set this interaction bound to one, theresulting estimator is a sum of one-dimensional functions. In the other extremecase, if we do not set a limit, the resulting estimator and corresponding modelplace no restrictions on the form of the regression function. In a simulationstudy we find encouraging prediction and visualisation properties of our randomplanted forest method. We also develop theory for an idealized version ofrandom planted forests in cases where the interaction bound is low. We showthat if it is smaller than three, the idealized version achieves asymptoticallyoptimal convergence rates up to a logarithmic factor. Code is available onGitHub https://github.com/PlantedML/randomPlantedForest.</description><author>Munir Hiabu, Enno Mammen, Joseph T. Meyer</author><pubDate>Thu, 03 Aug 2023 16:27:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.14563v3</guid></item><item><title>Inductive reasoning in humans and large language models</title><link>http://arxiv.org/abs/2306.06548v2</link><description>The impressive recent performance of large language models has led many towonder to what extent they can serve as models of general intelligence or aresimilar to human cognition. We address this issue by applying GPT-3.5 and GPT-4to a classic problem in human inductive reasoning known as property induction.Over two experiments, we elicit human judgments on a range of propertyinduction tasks spanning multiple domains. Although GPT-3.5 struggles tocapture many aspects of human behaviour, GPT-4 is much more successful: for themost part, its performance qualitatively matches that of humans, and the onlynotable exception is its failure to capture the phenomenon of premisenon-monotonicity. Our work demonstrates that property induction allows forinteresting comparisons between human and machine intelligence and provides twolarge datasets that can serve as benchmarks for future work in this vein.</description><author>Simon J. Han, Keith Ransom, Andrew Perfors, Charles Kemp</author><pubDate>Thu, 03 Aug 2023 16:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06548v2</guid></item><item><title>Merging satellite and gauge-measured precipitation using LightGBM with an emphasis on extreme quantiles</title><link>http://arxiv.org/abs/2302.03606v2</link><description>Knowing the actual precipitation in space and time is critical inhydrological modelling applications, yet the spatial coverage with rain gaugestations is limited due to economic constraints. Gridded satelliteprecipitation datasets offer an alternative option for estimating the actualprecipitation by covering uniformly large areas, albeit related estimates arenot accurate. To improve precipitation estimates, machine learning is appliedto merge rain gauge-based measurements and gridded satellite precipitationproducts. In this context, observed precipitation plays the role of thedependent variable, while satellite data play the role of predictor variables.Random forests is the dominant machine learning algorithm in relevantapplications. In those spatial predictions settings, point predictions (mostlythe mean or the median of the conditional distribution) of the dependentvariable are issued. The aim of the manuscript is to solve the problem ofprobabilistic prediction of precipitation with an emphasis on extreme quantilesin spatial interpolation settings. Here we propose, issuing probabilisticspatial predictions of precipitation using Light Gradient Boosting Machine(LightGBM). LightGBM is a boosting algorithm, highlighted by prize-winningentries in prediction and forecasting competitions. To assess LightGBM, wecontribute a large-scale application that includes merging daily precipitationmeasurements in contiguous US with PERSIANN and GPM-IMERG satelliteprecipitation data. We focus on extreme quantiles of the probabilitydistribution of the dependent variable, where LightGBM outperforms quantileregression forests (QRF, a variant of random forests) in terms of quantilescore at extreme quantiles. Our study offers understanding of probabilisticpredictions in spatial settings using machine learning.</description><author>Hristos Tyralis, Georgia Papacharalampous, Nikolaos Doulamis, Anastasios Doulamis</author><pubDate>Thu, 03 Aug 2023 16:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03606v2</guid></item><item><title>Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit</title><link>http://arxiv.org/abs/2308.01814v1</link><description>Going beyond stochastic gradient descent (SGD), what new phenomena emerge inwide neural networks trained by adaptive optimizers like Adam? Here we show:The same dichotomy between feature learning and kernel behaviors (as in SGD)holds for general optimizers as well, including Adam -- albeit with a nonlinearnotion of "kernel." We derive the corresponding "neural tangent" and "maximalupdate" limits for any architecture. Two foundational advances underlie theabove results: 1) A new Tensor Program language, NEXORT, that can express howadaptive optimizers process gradients into updates. 2) The introduction ofbra-ket notation to drastically simplify expressions and calculations in TensorPrograms. This work summarizes and generalizes all previous results in theTensor Programs series of papers.</description><author>Greg Yang, Etai Littwin</author><pubDate>Thu, 03 Aug 2023 16:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01814v1</guid></item><item><title>Variational Classification</title><link>http://arxiv.org/abs/2305.10406v2</link><description>We present a latent variable generalisation of neural network softmaxclassification trained with cross-entropy loss, referred to as variationalclassification (VC). Our approach offers a novel probabilistic perspective onthe highly familiar softmax classification model, to which it relates similarlyto how variational and traditional autoencoders relate. We derive a trainingobjective based on the evidence lower bound (ELBO) that is non-trivial tooptimize, and therefore propose an adversarial approach to maximise it. We showthat VC addresses an inherent inconsistency within softmax classification,whilst also allowing more flexible choices of prior distributions in the latentspace in place of implicit assumptions revealed within off-the-shelf softmaxclassifiers. Empirical evaluation on image and text classification datasetsdemonstrates that variational classification maintains prediction accuracywhile improving other desirable properties such as calibration and adversarialrobustness, particularly under distribution shift and low data settings.</description><author>Shehzaad Dhuliawala, Mrinmaya Sachan, Carl Allen</author><pubDate>Thu, 03 Aug 2023 16:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10406v2</guid></item><item><title>Deep Neural Networks Fused with Textures for Image Classification</title><link>http://arxiv.org/abs/2308.01813v1</link><description>Fine-grained image classification (FGIC) is a challenging task in computervision for due to small visual differences among inter-subcategories, but,large intra-class variations. Deep learning methods have achieved remarkablesuccess in solving FGIC. In this paper, we propose a fusion approach to addressFGIC by combining global texture with local patch-based information. The firstpipeline extracts deep features from various fixed-size non-overlapping patchesand encodes features by sequential modelling using the long short-term memory(LSTM). Another path computes image-level textures at multiple scales using thelocal binary patterns (LBP). The advantages of both streams are integrated torepresent an efficient feature vector for image classification. The method istested on eight datasets representing the human faces, skin lesions, fooddishes, marine lives, etc. using four standard backbone CNNs. Our method hasattained better classification accuracy over existing methods with notablemargins.</description><author>Asish Bera, Debotosh Bhattacharjee, Mita Nasipuri</author><pubDate>Thu, 03 Aug 2023 16:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01813v1</guid></item><item><title>An End-to-end Food Portion Estimation Framework Based on Shape Reconstruction from Monocular Image</title><link>http://arxiv.org/abs/2308.01810v1</link><description>Dietary assessment is a key contributor to monitoring health status. Existingself-report methods are tedious and time-consuming with substantial biases anderrors. Image-based food portion estimation aims to estimate food energy valuesdirectly from food images, showing great potential for automated dietaryassessment solutions. Existing image-based methods either use a single-viewimage or incorporate multi-view images and depth information to estimate thefood energy, which either has limited performance or creates user burdens. Inthis paper, we propose an end-to-end deep learning framework for food energyestimation from a monocular image through 3D shape reconstruction. We leveragea generative model to reconstruct the voxel representation of the food objectfrom the input image to recover the missing 3D information. Our method isevaluated on a publicly available food image dataset Nutrition5k, resulting aMean Absolute Error (MAE) of 40.05 kCal and Mean Absolute Percentage Error(MAPE) of 11.47% for food energy estimation. Our method uses RGB image as theonly input at the inference stage and achieves competitive results compared tothe existing method requiring both RGB and depth information.</description><author>Zeman Shao, Gautham Vinod, Jiangpeng He, Fengqing Zhu</author><pubDate>Thu, 03 Aug 2023 16:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01810v1</guid></item><item><title>QUEST: Query Stream for Vehicle-Infrastructure Cooperative Perception</title><link>http://arxiv.org/abs/2308.01804v1</link><description>Cooperative perception can effectively enhance individual perceptionperformance by providing additional viewpoint and expanding the sensing field.Existing cooperation paradigms are either interpretable (result cooperation) orflexible (feature cooperation). In this paper, we propose the concept of querycooperation to enable interpretable instance-level flexible featureinteraction. To specifically explain the concept, we propose a cooperativeperception framework, termed QUEST, which let query stream flow among agents.The cross-agent queries are interacted via fusion for co-aware instances andcomplementation for individual unaware instances. Taking camera-basedvehicle-infrastructure perception as a typical practical application scene, theexperimental results on the real-world dataset, DAIR-V2X-Seq, demonstrate theeffectiveness of QUEST and further reveal the advantage of the querycooperation paradigm on transmission flexibility and robustness to packetdropout. We hope our work can further facilitate the cross-agent representationinteraction for better cooperative perception in practice.</description><author>Siqi Fan, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie</author><pubDate>Thu, 03 Aug 2023 16:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01804v1</guid></item><item><title>RAB: Provable Robustness Against Backdoor Attacks</title><link>http://arxiv.org/abs/2003.08904v8</link><description>Recent studies have shown that deep neural networks (DNNs) are vulnerable toadversarial attacks, including evasion and backdoor (poisoning) attacks. On thedefense side, there have been intensive efforts on improving both empirical andprovable robustness against evasion attacks; however, the provable robustnessagainst backdoor attacks still remains largely unexplored. In this paper, wefocus on certifying the machine learning model robustness against generalthreat models, especially backdoor attacks. We first provide a unifiedframework via randomized smoothing techniques and show how it can beinstantiated to certify the robustness against both evasion and backdoorattacks. We then propose the first robust training process, RAB, to smooth thetrained model and certify its robustness against backdoor attacks. We prove therobustness bound for machine learning models trained with RAB and prove thatour robustness bound is tight. In addition, we theoretically show that it ispossible to train the robust smoothed models efficiently for simple models suchas K-nearest neighbor classifiers, and we propose an exact smooth-trainingalgorithm that eliminates the need to sample from a noise distribution for suchmodels. Empirically, we conduct comprehensive experiments for different machinelearning (ML) models such as DNNs, support vector machines, and K-NN models onMNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark forcertified robustness against backdoor attacks. In addition, we evaluate K-NNmodels on a spambase tabular dataset to demonstrate the advantages of theproposed exact algorithm. Both the theoretic analysis and the comprehensiveevaluation on diverse ML models and datasets shed light on further robustlearning strategies against general training time attacks.</description><author>Maurice Weber, Xiaojun Xu, Bojan Karlaš, Ce Zhang, Bo Li</author><pubDate>Thu, 03 Aug 2023 15:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.08904v8</guid></item><item><title>Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach</title><link>http://arxiv.org/abs/2308.01797v1</link><description>Job scheduling is a well-known Combinatorial Optimization problem withendless applications. Well planned schedules bring many benefits in the contextof automated systems: among others, they limit production costs and waste.Nevertheless, the NP-hardness of this problem makes it essential to useheuristics whose design is difficult, requires specialized knowledge and oftenproduces methods tailored to the specific task. This paper presents an originalend-to-end Deep Reinforcement Learning approach to scheduling thatautomatically learns dispatching rules. Our technique is inspired by naturallanguage encoder-decoder models for sequence processing and has never beenused, to the best of our knowledge, for scheduling purposes. We applied andtested our method in particular to some benchmark instances of Job ShopProblem, but this technique is general enough to be potentially used to tackleother different optimal job scheduling tasks with minimal intervention. Resultsdemonstrate that we outperform many classical approaches exploiting prioritydispatching rules and show competitive results on state-of-the-art DeepReinforcement Learning ones.</description><author>Giovanni Bonetta, Davide Zago, Rossella Cancelliere, Andrea Grosso</author><pubDate>Thu, 03 Aug 2023 15:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01797v1</guid></item><item><title>A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography</title><link>http://arxiv.org/abs/2308.00471v2</link><description>Contrast Enhanced Spectral Mammography (CESM) is a dual-energy mammographicimaging technique that first needs intravenously administration of an iodinatedcontrast medium; then, it collects both a low-energy image, comparable tostandard mammography, and a high-energy image. The two scans are then combinedto get a recombined image showing contrast enhancement. Despite CESM diagnosticadvantages for breast cancer diagnosis, the use of contrast medium can causeside effects, and CESM also beams patients with a higher radiation dosecompared to standard mammography. To address these limitations this workproposes to use deep generative models for virtual contrast enhancement onCESM, aiming to make the CESM contrast-free as well as to reduce the radiationdose. Our deep networks, consisting of an autoencoder and two GenerativeAdversarial Networks, the Pix2Pix, and the CycleGAN, generate syntheticrecombined images solely from low-energy images. We perform an extensivequantitative and qualitative analysis of the model's performance, alsoexploiting radiologists' assessments, on a novel CESM dataset that includes1138 images that, as a further contribution of this work, we make publiclyavailable. The results show that CycleGAN is the most promising deep network togenerate synthetic recombined images, highlighting the potential of artificialintelligence techniques for virtual contrast enhancement in this field.</description><author>Aurora Rofena, Valerio Guarrasi, Marina Sarli, Claudia Lucia Piccolo, Matteo Sammarra, Bruno Beomonte Zobel, Paolo Soda</author><pubDate>Thu, 03 Aug 2023 15:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00471v2</guid></item><item><title>Enhancement of Novel View Synthesis Using Omnidirectional Image Completion</title><link>http://arxiv.org/abs/2203.09957v3</link><description>In this study, we present a method for synthesizing novel views from a single360-degree RGB-D image based on the neural radiance field (NeRF) . Priorstudies relied on the neighborhood interpolation capability of multi-layerperceptrons to complete missing regions caused by occlusion and zooming, whichleads to artifacts. In the method proposed in this study, the input image isreprojected to 360-degree RGB images at other camera positions, the missingregions of the reprojected images are completed by a 2D image generative model,and the completed images are utilized to train the NeRF. Because multiplecompleted images contain inconsistencies in 3D, we introduce a method to learnthe NeRF model using a subset of completed images that cover the target scenewith less overlap of completed regions. The selection of such a subset ofimages can be attributed to the maximum weight independent set problem, whichis solved through simulated annealing. Experiments demonstrated that theproposed method can synthesize plausible novel views while preserving thefeatures of the scene for both artificial and real-world data.</description><author>Takayuki Hara, Tatsuya Harada</author><pubDate>Thu, 03 Aug 2023 15:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.09957v3</guid></item><item><title>Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances</title><link>http://arxiv.org/abs/2308.01789v1</link><description>In recent years, Variational Quantum Algorithms (VQAs) have emerged as apromising approach for solving optimization problems on quantum computers inthe NISQ era. However, one limitation of VQAs is their reliance onfixed-structure circuits, which may not be taylored for specific problems orhardware configurations. A leading strategy to address this issue areAdaptative VQAs, which dynamically modify the circuit structure by adding andremoving gates, and optimize their parameters during the training. SeveralAdaptative VQAs, based on heuristics such as circuit shallowness, entanglementcapability and hardware compatibility, have already been proposed in theliterature, but there is still lack of a systematic comparison between thedifferent methods. In this paper, we aim to fill this gap by analyzing threeAdaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), VariableAnsatz (VAns), already proposed in the literature, and Random Adapt-VQE(RA-VQE), a random approach we introduce as a baseline. In order to comparethese algorithms to traditional VQAs, we also include the Quantum ApproximateOptimization Algorithm (QAOA) in our analysis. We apply these algorithms toQUBO problems and study their performance by examining the quality of thesolutions found and the computational times required. Additionally, weinvestigate how the choice of the hyperparameters can impact the overallperformance of the algorithms, highlighting the importance of selecting anappropriate methodology for hyperparameter tuning. Our analysis sets benchmarksfor Adaptative VQAs designed for near-term quantum devices and providesvaluable insights to guide future research in this area.</description><author>Gloria Turati, Maurizio Ferrari Dacrema, Paolo Cremonesi</author><pubDate>Thu, 03 Aug 2023 15:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01789v1</guid></item><item><title>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</title><link>http://arxiv.org/abs/2305.02897v2</link><description>Emergent chain-of-thought (CoT) reasoning capabilities promise to improveperformance and explainability of large language models (LLMs). However,uncertainties remain about how reasoning strategies formulated for previousmodel generations generalize to new model generations and different datasets.In this small-scale study, we compare different reasoning strategies induced byzero-shot prompting across six recently released LLMs (davinci-002,davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on amixture of six question-answering datasets, including datasets from scientificand medical domains. Our findings demonstrate that while some variations ineffectiveness occur, gains from CoT reasoning strategies remain robust acrossdifferent models and datasets. GPT-4 has the most benefit from currentstate-of-the-art reasoning strategies and exhibits the best performance byapplying a prompt previously discovered through automated discovery.</description><author>Konstantin Hebenstreit, Robert Praas, Louis P Kiesewetter, Matthias Samwald</author><pubDate>Thu, 03 Aug 2023 15:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02897v2</guid></item><item><title>Lexicon and Rule-based Word Lemmatization Approach for the Somali Language</title><link>http://arxiv.org/abs/2308.01785v1</link><description>Lemmatization is a Natural Language Processing (NLP) technique used tonormalize text by changing morphological derivations of words to their rootforms. It is used as a core pre-processing step in many NLP tasks includingtext indexing, information retrieval, and machine learning for NLP, amongothers. This paper pioneers the development of text lemmatization for theSomali language, a low-resource language with very limited or no prioreffective adoption of NLP methods and datasets. We especially develop a lexiconand rule-based lemmatizer for Somali text, which is a starting point for afull-fledged Somali lemmatization system for various NLP tasks. Withconsideration of the language morphological rules, we have developed an initiallexicon of 1247 root words and 7173 derivationally related terms enriched withrules for lemmatizing words not present in the lexicon. We have tested thealgorithm on 120 documents of various lengths including news articles, socialmedia posts, and text messages. Our initial results demonstrate that thealgorithm achieves an accuracy of 57\% for relatively long documents (e.g. fullnews articles), 60.57\% for news article extracts, and high accuracy of 95.87\%for short texts such as social media messages.</description><author>Shafie Abdi Mohamed, Muhidin Abdullahi Mohamed</author><pubDate>Thu, 03 Aug 2023 15:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01785v1</guid></item><item><title>atTRACTive: Semi-automatic white matter tract segmentation using active learning</title><link>http://arxiv.org/abs/2305.18905v3</link><description>Accurately identifying white matter tracts in medical images is essential forvarious applications, including surgery planning and tract-specific analysis.Supervised machine learning models have reached state-of-the-art solving thistask automatically. However, these models are primarily trained on healthysubjects and struggle with strong anatomical aberrations, e.g. caused by braintumors. This limitation makes them unsuitable for tasks such as preoperativeplanning, wherefore time-consuming and challenging manual delineation of thetarget tract is typically employed. We propose semi-automatic entropy-basedactive learning for quick and intuitive segmentation of white matter tractsfrom whole-brain tractography consisting of millions of streamlines. The methodis evaluated on 21 openly available healthy subjects from the Human ConnectomeProject and an internal dataset of ten neurosurgical cases. With only a fewannotations, the proposed approach enables segmenting tracts on tumor casescomparable to healthy subjects (dice=0.71), while the performance of automaticmethods, like TractSeg dropped substantially (dice=0.34) in comparison tohealthy subjects. The method is implemented as a prototype named atTRACTive inthe freely available software MITK Diffusion. Manual experiments on tumor datashowed higher efficiency due to lower segmentation times compared totraditional ROI-based segmentation.</description><author>Robin Peretzke, Klaus Maier-Hein, Jonas Bohn, Yannick Kirchhoff, Saikat Roy, Sabrina Oberli-Palma, Daniela Becker, Pavlina Lenga, Peter Neher</author><pubDate>Thu, 03 Aug 2023 15:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18905v3</guid></item><item><title>Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport</title><link>http://arxiv.org/abs/2308.01779v1</link><description>Weakly-supervised image segmentation has recently attracted increasingresearch attentions, aiming to avoid the expensive pixel-wise labeling. In thispaper, we present an effective method, namely Point2Mask, to achievehigh-quality panoptic prediction using only a single random point annotationper target for training. Specifically, we formulate the panoptic pseudo-maskgeneration as an Optimal Transport (OT) problem, where each ground-truth (gt)point label and pixel sample are defined as the label supplier and consumer,respectively. The transportation cost is calculated by the introducedtask-oriented maps, which focus on the category-wise and instance-wisedifferences among the various thing and stuff targets. Furthermore, acentroid-based scheme is proposed to set the accurate unit number for each gtpoint supplier. Hence, the pseudo-mask generation is converted into finding theoptimal transport plan at a globally minimal transportation cost, which can besolved via the Sinkhorn-Knopp Iteration. Experimental results on Pascal VOC andCOCO demonstrate the promising performance of our proposed Point2Mask approachto point-supervised panoptic segmentation. Source code is available at:https://github.com/LiWentomng/Point2Mask.</description><author>Wentong Li, Yuqian Yuan, Song Wang, Jianke Zhu, Jianshu Li, Jian Liu, Lei Zhang</author><pubDate>Thu, 03 Aug 2023 15:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01779v1</guid></item><item><title>Does Correction Remain An Problem For Large Language Models?</title><link>http://arxiv.org/abs/2308.01776v1</link><description>As large language models, such as GPT, continue to advance the capabilitiesof natural language processing (NLP), the question arises: does the problem ofcorrection still persist? This paper investigates the role of correction in thecontext of large language models by conducting two experiments. The firstexperiment focuses on correction as a standalone task, employing few-shotlearning techniques with GPT-like models for error correction. The secondexperiment explores the notion of correction as a preparatory task for otherNLP tasks, examining whether large language models can tolerate and performadequately on texts containing certain levels of noise or errors. By addressingthese experiments, we aim to shed light on the significance of correction inthe era of large language models and its implications for various NLPapplications.</description><author>Xiaowu Zhang, Xiaotian Zhang, Cheng Yang, Hang Yan, Xipeng Qiu</author><pubDate>Thu, 03 Aug 2023 15:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01776v1</guid></item><item><title>Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship</title><link>http://arxiv.org/abs/2112.15402v3</link><description>Continual learning is a promising machine learning paradigm to learn newtasks while retaining previously learned knowledge over streaming trainingdata. Till now, rehearsal-based methods, keeping a small part of data from oldtasks as a memory buffer, have shown good performance in mitigatingcatastrophic forgetting for previously learned knowledge. However, most ofthese methods typically treat each new task equally, which may not adequatelyconsider the relationship or similarity between old and new tasks. Furthermore,these methods commonly neglect sample importance in the continual trainingprocess and result in sub-optimal performance on certain tasks. To address thischallenging problem, we propose Relational Experience Replay (RER), a bi-levellearning framework, to adaptively tune task-wise relationships and sampleimportance within each task to achieve a better `stability' and `plasticity'trade-off. As such, the proposed method is capable of accumulating newknowledge while consolidating previously learned old knowledge during continuallearning. Extensive experiments conducted on three publicly available datasets(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed methodcan consistently improve the performance of all baselines and surpass currentstate-of-the-art methods.</description><author>Quanziang Wang, Renzhen Wang, Yuexiang Li, Dong Wei, Kai Ma, Yefeng Zheng, Deyu Meng</author><pubDate>Thu, 03 Aug 2023 15:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.15402v3</guid></item><item><title>Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment</title><link>http://arxiv.org/abs/2308.01771v1</link><description>This study investigated the potential of end-to-end deep learning tools as amore effective substitute for FEM in predicting stress-strain fields within 2Dcross sections of arterial wall. We first proposed a U-Net based fullyconvolutional neural network (CNN) to predict the von Mises stress and straindistribution based on the spatial arrangement of calcification within arterialwall cross-sections. Further, we developed a conditional generative adversarialnetwork (cGAN) to enhance, particularly from the perceptual perspective, theprediction accuracy of stress and strain field maps for arterial walls withvarious calcification quantities and spatial configurations. On top of U-Netand cGAN, we also proposed their ensemble approaches, respectively, to furtherimprove the prediction accuracy of field maps. Our dataset, consisting of inputand output images, was generated by implementing boundary conditions andextracting stress-strain field maps. The trained U-Net models can accuratelypredict von Mises stress and strain fields, with structural similarity indexscores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 forstress and strain, respectively, on a reserved test set. Meanwhile, the cGANmodels in a combination of ensemble and transfer learning techniquesdemonstrate high accuracy in predicting von Mises stress and strain fields, asevidenced by SSIM scores of 0.890 for stress and 0.803 for strain.Additionally, mean squared errors of 0.008 for stress and 0.017 for strainfurther support the model's performance on a designated test set. Overall, thisstudy developed a surrogate model for finite element analysis, which canaccurately and efficiently predict stress-strain fields of arterial wallsregardless of complex geometries and boundary conditions.</description><author>Yasin Shokrollahi1, Pengfei Dong1, Xianqi Li, Linxia Gu</author><pubDate>Thu, 03 Aug 2023 15:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01771v1</guid></item><item><title>Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN</title><link>http://arxiv.org/abs/2308.01769v1</link><description>Annotating nuclei in microscopy images for the training of neural networks isa laborious task that requires expert knowledge and suffers from inter- andintra-rater variability, especially in fluorescence microscopy. Generativenetworks such as CycleGAN can inverse the process and generate syntheticmicroscopy images for a given mask, thereby building a synthetic dataset.However, past works report content inconsistencies between the mask andgenerated image, partially due to CycleGAN minimizing its loss by hidingshortcut information for the image reconstruction in high frequencies ratherthan encoding the desired image content and learning the target task. In thiswork, we propose to remove the hidden shortcut information, calledsteganography, from generated images by employing a low pass filtering based onthe DCT. We show that this increases coherence between generated images andcycled masks and evaluate synthetic datasets on a downstream nucleisegmentation task. Here we achieve an improvement of 5.4 percentage points inthe F1-score compared to a vanilla CycleGAN. Integrating advancedregularization techniques into the CycleGAN architecture may help mitigatesteganography-related issues and produce more accurate synthetic datasets fornuclei segmentation.</description><author>Jonas Utz, Tobias Weise, Maja Schlereth, Fabian Wagner, Mareike Thies, Mingxuan Gu, Stefan Uderhardt, Katharina Breininger</author><pubDate>Thu, 03 Aug 2023 14:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01769v1</guid></item><item><title>A Novel Tensor Decomposition of arbitrary order based on Block Convolution with Reflective Boundary Conditions for Multi-Dimensional Data Analysis</title><link>http://arxiv.org/abs/2308.01768v1</link><description>Tensor decompositions are powerful tools for analyzing multi-dimensional datain their original format. Besides tensor decompositions like Tucker and CP,Tensor SVD (t-SVD) which is based on the t-product of tensors is anotherextension of SVD to tensors that recently developed and has found numerousapplications in analyzing high dimensional data. This paper offers a newinsight into the t-Product and shows that this product is a block convolutionof two tensors with periodic boundary conditions. Based on this viewpoint, wepropose a new tensor-tensor product called the $\star_c{}\text{-Product}$ basedon Block convolution with reflective boundary conditions. Using a tensorframework, this product can be easily extended to tensors of arbitrary order.Additionally, we introduce a tensor decomposition based on our$\star_c{}\text{-Product}$ for arbitrary order tensors. Compared to t-SVD, ournew decomposition has lower complexity, and experiments show that it yieldshigher-quality results in applications such as classification and compression.</description><author>Mahdi Molavi, Mansoor Rezghi, Tayyebeh Saeedi</author><pubDate>Thu, 03 Aug 2023 14:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01768v1</guid></item><item><title>PoissonNet: Resolution-Agnostic 3D Shape Reconstruction using Fourier Neural Operators</title><link>http://arxiv.org/abs/2308.01766v1</link><description>We introduce PoissonNet, an architecture for shape reconstruction thataddresses the challenge of recovering 3D shapes from points. Traditional deepneural networks face challenges with common 3D shape discretization techniquesdue to their computational complexity at higher resolutions. To overcome this,we leverage Fourier Neural Operators (FNOs) to solve the Poisson equation andreconstruct a mesh from oriented point cloud measurements. PoissonNet exhibitstwo main advantages. First, it enables efficient training on low-resolutiondata while achieving comparable performance at high-resolution evaluation,thanks to the resolution-agnostic nature of FNOs. This feature allows forone-shot super-resolution. Second, our method surpasses existing approaches inreconstruction quality while being differentiable. Overall, our proposed methodnot only improves upon the limitations of classical deep neural networks inshape reconstruction but also achieves superior results in terms ofreconstruction quality, running time, and resolution flexibility. Furthermore,we demonstrate that the Poisson surface reconstruction problem is well-posed inthe limit case by showing a universal approximation theorem for the solutionoperator of the Poisson equation with distributional data utilizing the FourierNeuronal Operator, which provides a theoretical foundation for our numericalresults. The code to reproduce the experiments is available on:\url{https://github.com/arsenal9971/PoissonNet}.</description><author>Hector Andrade-Loarca, Aras Bacho, Julius Hege, Gitta Kutyniok</author><pubDate>Thu, 03 Aug 2023 14:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01766v1</guid></item><item><title>LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On</title><link>http://arxiv.org/abs/2305.13501v3</link><description>The rapidly evolving fields of e-commerce and metaverse continue to seekinnovative approaches to enhance the consumer experience. At the same time,recent advancements in the development of diffusion models have enabledgenerative networks to create remarkably realistic images. In this context,image-based virtual try-on, which consists in generating a novel image of atarget model wearing a given in-shop garment, has yet to capitalize on thepotential of these powerful generative solutions. This work introducesLaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for theVirtual Try-ON task. The proposed architecture relies on a latent diffusionmodel extended with a novel additional autoencoder module that exploitslearnable skip connections to enhance the generation process preserving themodel's characteristics. To effectively maintain the texture and details of thein-shop garment, we propose a textual inversion component that can map thevisual features of the garment to the CLIP token embedding space and thusgenerate a set of pseudo-word token embeddings capable of conditioning thegeneration process. Experimental results on Dress Code and VITON-HD datasetsdemonstrate that our approach outperforms the competitors by a consistentmargin, achieving a significant milestone for the task. Source code and trainedmodels are publicly available at: https://github.com/miccunifi/ladi-vton.</description><author>Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, Rita Cucchiara</author><pubDate>Thu, 03 Aug 2023 14:51:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13501v3</guid></item><item><title>Auto-COP: Adaptation Generation in Context-Oriented Programming using Reinforcement Learning Options</title><link>http://arxiv.org/abs/2103.06757v2</link><description>Self-adaptive software systems continuously adapt in response to internal andexternal changes in their execution environment, captured as contexts. The COPparadigm posits a technique for the development of self-adaptive systems,capturing their main characteristics with specialized programming languageconstructs. COP adaptations are specified as independent modules composed inand out of the base system as contexts are activated and deactivated inresponse to sensed circumstances from the surrounding environment. However, thedefinition of adaptations, their contexts and associated specialized behavior,need to be specified at design time. In complex CPS this is intractable due tonew unpredicted operating conditions. We propose Auto-COP, a new technique toenable generation of adaptations at run time. Auto-COP uses RL options to buildaction sequences, based on the previous instances of the system execution.Options are explored in interaction with the environment, and the most suitableoptions for each context are used to generate adaptations exploiting COP. Tovalidate Auto-COP, we present two case studies exhibiting different systemcharacteristics and application domains: a driving assistant and a robotdelivery system. We present examples of Auto-COP code generated at run time, toillustrate the types of circumstances (contexts) requiring adaptation, and thecorresponding generated adaptations for each context. We confirm that thegenerated adaptations exhibit correct system behavior measured bydomain-specific performance metrics, while reducing the number of requiredexecution/actuation steps by a factor of two showing that the adaptations areregularly selected by the running system as adaptive behavior is moreappropriate than the execution of primitive actions.</description><author>Nicolás Cardozo, Ivana Dusparic</author><pubDate>Thu, 03 Aug 2023 14:47:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.06757v2</guid></item><item><title>From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module</title><link>http://arxiv.org/abs/2305.16174v2</link><description>Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks(GNNs) on a given graph topology by dynamically learning it. However, most ofLGI methods assume to have a (noisy, incomplete, improvable, ...) input graphto rewire and can solely learn regular graph topologies. In the wake of thesuccess of Topological Deep Learning (TDL), we study Latent Topology Inference(LTI) for learning higher-order cell complexes (with sparse and not regulartopology) describing multi-way interactions between data points. To this aim,we introduce the Differentiable Cell Complex Module (DCM), a novel learnablefunction that computes cell probabilities in the complex to improve thedownstream task. We show how to integrate DCM with cell complex message passingnetworks layers and train it in a end-to-end fashion, thanks to a two-stepinference procedure that avoids an exhaustive search across all possible cellsin the input, thus maintaining scalability. Our model is tested on severalhomophilic and heterophilic graph datasets and it is shown to outperform otherstate-of-the-art techniques, offering significant improvements especially incases where an input graph is not provided.</description><author>Claudio Battiloro, Indro Spinelli, Lev Telyatnikov, Michael Bronstein, Simone Scardapane, Paolo Di Lorenzo</author><pubDate>Thu, 03 Aug 2023 14:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16174v2</guid></item><item><title>NuInsSeg: A Fully Annotated Dataset for Nuclei Instance Segmentation in H&amp;E-Stained Histological Images</title><link>http://arxiv.org/abs/2308.01760v1</link><description>In computational pathology, automatic nuclei instance segmentation plays anessential role in whole slide image analysis. While many computerizedapproaches have been proposed for this task, supervised deep learning (DL)methods have shown superior segmentation performances compared to classicalmachine learning and image processing techniques. However, these models needfully annotated datasets for training which is challenging to acquire,especially in the medical domain. In this work, we release one of the biggestfully manually annotated datasets of nuclei in Hematoxylin and Eosin(H&amp;E)-stained histological images, called NuInsSeg. This dataset contains 665image patches with more than 30,000 manually segmented nuclei from 31 human andmouse organs. Moreover, for the first time, we provide additional ambiguousarea masks for the entire dataset. These vague areas represent the parts of theimages where precise and deterministic manual annotations are impossible, evenfor human experts. The dataset and detailed step-by-step instructions togenerate related segmentation masks are publicly available athttps://www.kaggle.com/datasets/ipateam/nuinsseg andhttps://github.com/masih4/NuInsSeg, respectively.</description><author>Amirreza Mahbod, Christine Polak, Katharina Feldmann, Rumsha Khan, Katharina Gelles, Georg Dorffner, Ramona Woitek, Sepideh Hatamikia, Isabella Ellinger</author><pubDate>Thu, 03 Aug 2023 14:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01760v1</guid></item><item><title>Bag of Policies for Distributional Deep Exploration</title><link>http://arxiv.org/abs/2308.01759v1</link><description>Efficient exploration in complex environments remains a major challenge forreinforcement learning (RL). Compared to previous Thompson sampling-inspiredmechanisms that enable temporally extended exploration, i.e., deep exploration,we focus on deep exploration in distributional RL. We develop here a generalpurpose approach, Bag of Policies (BoP), that can be built on top of any returndistribution estimator by maintaining a population of its copies. BoP consistsof an ensemble of multiple heads that are updated independently. Duringtraining, each episode is controlled by only one of the heads and the collectedstate-action pairs are used to update all heads off-policy, leading to distinctlearning signals for each head which diversify learning and behaviour. To testwhether optimistic ensemble method can improve on distributional RL as did onscalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with apopulation of distributional actor-critics using Bayesian Distributional PolicyGradients (BDPG). The population thus approximates a posterior distribution ofreturn distributions along with a posterior distribution of policies. Anotherbenefit of building upon BDPG is that it allows to analyze global posterioruncertainty along with local curiosity bonus simultaneously for exploration. AsBDPG is already an optimistic method, this pairing helps to investigate ifoptimism is accumulatable in distributional RL. Overall BoP results in greaterrobustness and speed during learning as demonstrated by our experimentalresults on ALE Atari games.</description><author>Asen Nachkov, Luchen Li, Giulia Luise, Filippo Valdettaro, Aldo Faisal</author><pubDate>Thu, 03 Aug 2023 14:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01759v1</guid></item><item><title>An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection</title><link>http://arxiv.org/abs/2212.03188v2</link><description>Clustering analysis of sequence data continues to address many applicationsin engineering design, aided with the rapid growth of machine learning inapplied science. This paper presents an unsupervised machine learning algorithmto extract defining characteristics of earthquake ground-motion spectra, alsocalled latent features, to aid in ground-motion selection (GMS). In thiscontext, a latent feature is a low-dimensional machine-discovered spectralcharacteristic learned through nonlinear relationships of a neural networkautoencoder. Machine discovered latent features can be combined withtraditionally defined intensity measures and clustering can be performed toselect a representative subgroup from a large ground-motion suite. Theobjective of efficient GMS is to choose characteristic records representativeof what the structure will probabilistically experience in its lifetime. Threeexamples are presented to validate this approach, including the use ofsynthetic and field recorded ground-motion datasets. The presented deepembedding clustering of ground-motion spectra has three main advantages: 1.defining characteristics the represent the sparse spectral content ofground-motions are discovered efficiently through training of the autoencoder,2. domain knowledge is incorporated into the machine learning framework withconditional variables in the deep embedding scheme, and 3. method exhibitsexcellent performance when compared to a benchmark seismic hazard analysis.</description><author>R. Bailey Bond, Pu Ren, Jerome F. Hajjar, Hao Sun</author><pubDate>Thu, 03 Aug 2023 14:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03188v2</guid></item><item><title>Automatically Predict Material Properties with Microscopic Image Example Polymer Compatibility</title><link>http://arxiv.org/abs/2303.12360v2</link><description>Many material properties are manifested in the morphological appearance andcharacterized with microscopic image, such as scanning electron microscopy(SEM). Polymer miscibility is a key physical quantity of polymer material andcommonly and intuitively judged by SEM images. However, human observation andjudgement for the images is time-consuming, labor-intensive and hard to bequantified. Computer image recognition with machine learning method can make upthe defects of artificial judging, giving accurate and quantitative judgement.We achieve automatic miscibility recognition utilizing convolution neuralnetwork and transfer learning method, and the model obtains up to 94% accuracy.We also put forward a quantitative criterion for polymer miscibility with thismodel. The proposed method can be widely applied to the quantitativecharacterization of the microstructure and properties of various materials.</description><author>Zhilong Liang, Zhenzhi Tan, Ruixin Hong, Wanli Ouyang, Jinying Yuan, Changshui Zhang</author><pubDate>Thu, 03 Aug 2023 14:36:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12360v2</guid></item><item><title>Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis</title><link>http://arxiv.org/abs/2305.19569v4</link><description>Extensive research has been conducted on fault diagnosis of planetarygearboxes using vibration signals and deep learning (DL) approaches. However,DL-based methods are susceptible to the domain shift problem caused by varyingoperating conditions of the gearbox. Although domain adaptation and datasynthesis methods have been proposed to overcome such domain shifts, they areoften not directly applicable in real-world situations where only healthy datais available in the target domain. To tackle the challenge of extreme domainshift scenarios where only healthy data is available in the target domain, thispaper proposes two novel domain knowledge-informed data synthesis methodsutilizing the health data map (HDMap). The two proposed approaches are referredto as scaled CutPaste and FaultPaste. The HDMap is used to physically representthe vibration signal of the planetary gearbox as an image-like matrix, allowingfor visualization of fault-related features. CutPaste and FaultPaste are thenapplied to generate faulty samples based on the healthy data in the targetdomain, using domain knowledge and fault signatures extracted from the sourcedomain, respectively. In addition to generating realistic faults, the proposedmethods introduce scaling of fault signatures for controlled synthesis offaults with various severity levels. A case study is conducted on a planetarygearbox testbed to evaluate the proposed approaches. The results show that theproposed methods are capable of accurately diagnosing faults, even in cases ofextreme domain shift, and can estimate the severity of faults that have notbeen previously observed in the target domain.</description><author>Jong Moon Ha, Olga Fink</author><pubDate>Thu, 03 Aug 2023 14:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19569v4</guid></item><item><title>Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants</title><link>http://arxiv.org/abs/2308.01746v1</link><description>How to enable learnability for new classes while keeping the capability wellon old classes has been a crucial challenge for class incremental learning.Beyond the normal case, long-tail class incremental learning and few-shot classincremental learning are also proposed to consider the data imbalance and datascarcity, respectively, which are common in real-world implementations andfurther exacerbate the well-known problem of catastrophic forgetting. Existingmethods are specifically proposed for one of the three tasks. In this paper, weoffer a unified solution to the misalignment dilemma in the three tasks.Concretely, we propose neural collapse terminus that is a fixed structure withthe maximal equiangular inter-class separation for the whole label space. Itserves as a consistent target throughout the incremental training to avoiddividing the feature space incrementally. For CIL and LTCIL, we further proposea prototype evolving scheme to drive the backbone features into our neuralcollapse terminus smoothly. Our method also works for FSCIL with only minoradaptations. Theoretical analysis indicates that our method holds the neuralcollapse optimality in an incremental fashion regardless of data imbalance ordata scarcity. We also design a generalized case where we do not know the totalnumber of classes and whether the data distribution is normal, long-tail, orfew-shot for each coming session, to test the generalizability of our method.Extensive experiments with multiple datasets are conducted to demonstrate theeffectiveness of our unified solution to all the three tasks and thegeneralized case.</description><author>Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip Torr, Dacheng Tao, Bernard Ghanem</author><pubDate>Thu, 03 Aug 2023 14:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01746v1</guid></item><item><title>Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning</title><link>http://arxiv.org/abs/2308.01744v1</link><description>Multitask learning is a powerful framework that enables one to simultaneouslylearn multiple related tasks by sharing information between them. Quantifyinguncertainty in the estimated tasks is of pivotal importance for many downstreamapplications, such as online or active learning. In this work, we provide novelmultitask confidence intervals in the challenging agnostic setting, i.e., whenneither the similarity between tasks nor the tasks' features are available tothe learner. The obtained intervals do not require i.i.d. data and can bedirectly applied to bound the regret in online learning. Through a refinedanalysis of the multitask information gain, we obtain new regret guaranteesthat, depending on a task similarity parameter, can significantly improve overtreating tasks independently. We further propose a novel online learningalgorithm that achieves such improved regret without knowing this parameter inadvance, i.e., automatically adapting to task similarity. As a second keyapplication of our results, we introduce a novel multitask active learningsetup where several tasks must be simultaneously optimized, but only one ofthem can be queried for feedback by the learner at each round. For thisproblem, we design a no-regret algorithm that uses our confidence intervals todecide which task should be queried. Finally, we empirically validate ourbounds and algorithms on synthetic and real-world (drug discovery) data.</description><author>Pier Giuseppe Sessa, Pierre Laforgue, Nicolò Cesa-Bianchi, Andreas Krause</author><pubDate>Thu, 03 Aug 2023 14:08:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01744v1</guid></item><item><title>Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization</title><link>http://arxiv.org/abs/2308.01743v1</link><description>The turbulent jet ignition concept using prechambers is a promising solutionto achieve stable combustion at lean conditions in large gas engines, leadingto high efficiency at low emission levels. Due to the wide range of design andoperating parameters for large gas engine prechambers, the preferred method forevaluating different designs is computational fluid dynamics (CFD), as testingin test bed measurement campaigns is time-consuming and expensive. However, thesignificant computational time required for detailed CFD simulations due to thecomplexity of solving the underlying physics also limits its applicability. Inoptimization settings similar to the present case, i.e., where the evaluationof the objective function(s) is computationally costly, Bayesian optimizationhas largely replaced classical design-of-experiment. Thus, the present studydeals with the computationally efficient Bayesian optimization of large gasengine prechambers design using CFD simulation. Reynolds-averaged-Navier-Stokessimulations are used to determine the target values as a function of theselected prechamber design parameters. The results indicate that the chosenstrategy is effective to find a prechamber design that achieves the desiredtarget values.</description><author>Stefan Posch, Clemens Gößnitzer, Franz Rohrhofer, Bernhard C. Geiger, Andreas Wimmer</author><pubDate>Thu, 03 Aug 2023 14:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01743v1</guid></item><item><title>Exploiting Multi-Label Correlation in Label Distribution Learning</title><link>http://arxiv.org/abs/2308.01742v1</link><description>Label Distribution Learning (LDL) is a novel machine learning paradigm thatassigns label distribution to each instance. Many LDL methods proposed toleverage label correlation in the learning process to solve theexponential-sized output space; among these, many exploited the low-rankstructure of label distribution to capture label correlation. However, recentstudies disclosed that label distribution matrices are typically full-rank,posing challenges to those works exploiting low-rank label correlation. Notethat multi-label is generally low-rank; low-rank label correlation is widelyadopted in multi-label learning (MLL) literature. Inspired by that, weintroduce an auxiliary MLL process in LDL and capture low-rank labelcorrelation on that MLL rather than LDL. In such a way, low-rank labelcorrelation is appropriately exploited in our LDL methods. We conductcomprehensive experiments and demonstrate that our methods are superior toexisting LDL methods. Besides, the ablation studies justify the advantages ofexploiting low-rank label correlation in the auxiliary MLL.</description><author>Zhiqiang Kou jing wang yuheng jia xin geng</author><pubDate>Thu, 03 Aug 2023 14:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01742v1</guid></item><item><title>Supply chain emission estimation using large language models</title><link>http://arxiv.org/abs/2308.01741v1</link><description>Large enterprises face a crucial imperative to achieve the SustainableDevelopment Goals (SDGs), especially goal 13, which focuses on combatingclimate change and its impacts. To mitigate the effects of climate change,reducing enterprise Scope 3 (supply chain emissions) is vital, as it accountsfor more than 90\% of total emission inventories. However, tracking Scope 3emissions proves challenging, as data must be collected from thousands ofupstream and downstream suppliers.To address the above mentioned challenges, wepropose a first-of-a-kind framework that uses domain-adapted NLP foundationmodels to estimate Scope 3 emissions, by utilizing financial transactions as aproxy for purchased goods and services. We compared the performance of theproposed framework with the state-of-art text classification models such asTF-IDF, word2Vec, and Zero shot learning. Our results show that thedomain-adapted foundation model outperforms state-of-the-art text miningtechniques and performs as well as a subject matter expert (SME). The proposedframework could accelerate the Scope 3 estimation at Enterprise scale and willhelp to take appropriate climate actions to achieve SDG 13.</description><author>Ayush Jain, Manikandan Padmanaban, Jagabondhu Hazra, Shantanu Godbole, Kommy Weldemariam</author><pubDate>Thu, 03 Aug 2023 14:06:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01741v1</guid></item><item><title>Optimal Training of Mean Variance Estimation Neural Networks</title><link>http://arxiv.org/abs/2302.08875v2</link><description>This paper focusses on the optimal implementation of a Mean VarianceEstimation network (MVE network) (Nix and Weigend, 1994). This type of networkis often used as a building block for uncertainty estimation methods in aregression setting, for instance Concrete dropout (Gal et al., 2017) and DeepEnsembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumesthat the data is produced from a normal distribution with a mean function andvariance function. The MVE network outputs a mean and variance estimate andoptimizes the network parameters by minimizing the negative loglikelihood. Inour paper, we present two significant insights. Firstly, the convergencedifficulties reported in recent work can be relatively easily prevented byfollowing the simple yet often overlooked recommendation from the originalauthors that a warm-up period should be used. During this period, only the meanis optimized with a fixed variance. We demonstrate the effectiveness of thisstep through experimentation, highlighting that it should be standard practice.As a sidenote, we examine whether, after the warm-up, it is beneficial to fixthe mean while optimizing the variance or to optimize both simultaneously.Here, we do not observe a substantial difference. Secondly, we introduce anovel improvement of the MVE network: separate regularization of the mean andthe variance estimate. We demonstrate, both on toy examples and on a number ofbenchmark UCI regression data sets, that following the original recommendationsand the novel separate regularization can lead to significant improvements.</description><author>Laurens Sluijterman, Eric Cator, Tom Heskes</author><pubDate>Thu, 03 Aug 2023 13:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08875v2</guid></item><item><title>Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution</title><link>http://arxiv.org/abs/2308.01738v1</link><description>Visibility in hazy nighttime scenes is frequently reduced by multiplefactors, including low light, intense glow, light scattering, and the presenceof multicolored light sources. Existing nighttime dehazing methods oftenstruggle with handling glow or low-light conditions, resulting in eitherexcessively dark visuals or unsuppressed glow outputs. In this paper, weenhance the visibility from a single nighttime haze image by suppressing glowand enhancing low-light regions. To handle glow effects, our framework learnsfrom the rendered glow pairs. Specifically, a light source aware network isproposed to detect light sources of night images, followed by the APSF (AngularPoint Spread Function)-guided glow rendering. Our framework is then trained onthe rendered images, resulting in glow suppression. Moreover, we utilizegradient-adaptive convolution, to capture edges and textures in hazy scenes. Byleveraging extracted edges and textures, we enhance the contrast of the scenewithout losing important structural details. To boost low-light intensity, ournetwork learns an attention map, then adjusted by gamma correction. Thisattention has high values on low-light regions and low values on haze and glowregions. Extensive evaluation on real nighttime haze images, demonstrates theeffectiveness of our method. Our experiments demonstrate that our methodachieves a PSNR of 30.72dB, outperforming state-of-the-art methods by 14$\%$ onGTA5 nighttime haze dataset. Our data and code is available at:\url{https://github.com/jinyeying/nighttime_dehaze}.</description><author>Yeying Jin, Beibei Lin, Wending Yan, Wei Ye, Yuan Yuan, Robby T. Tan</author><pubDate>Thu, 03 Aug 2023 13:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01738v1</guid></item><item><title>MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction</title><link>http://arxiv.org/abs/2308.01737v1</link><description>With the widespread application of personalized online services,click-through rate (CTR) prediction has received more and more attention andresearch. The most prominent features of CTR prediction are its multi-fieldcategorical data format, and vast and daily-growing data volume. The largecapacity of neural models helps digest such massive amounts of data under thesupervised learning paradigm, yet they fail to utilize the substantial data toits full potential, since the 1-bit click signal is not sufficient to guide themodel to learn capable representations of features and instances. Theself-supervised learning paradigm provides a more promising pretrain-finetunesolution to better exploit the large amount of user click logs, and learn moregeneralized and effective representations. However, self-supervised learningfor CTR prediction is still an open question, since current works on this lineare only preliminary and rudimentary. To this end, we propose a Model-agnosticpretraining (MAP) framework that applies feature corruption and recovery onmulti-field categorical data, and more specifically, we derive two practicalalgorithms: masked feature prediction (MFP) and replaced feature detection(RFD). MFP digs into feature interactions within each instance through maskingand predicting a small portion of input features, and introduces noisecontrastive estimation (NCE) to handle large feature spaces. RFD further turnsMFP into a binary classification mode through replacing and detecting changesin input features, making it even simpler and more effective for CTRpretraining. Our extensive experiments on two real-world large-scale datasets(i.e., Avazu, Criteo) demonstrate the advantages of these two methods onseveral strong backbones (e.g., DCNv2, DeepFM), and achieve newstate-of-the-art performance in terms of both effectiveness and efficiency forCTR prediction.</description><author>Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, Weinan Zhang</author><pubDate>Thu, 03 Aug 2023 13:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01737v1</guid></item><item><title>How to Evaluate Uncertainty Estimates in Machine Learning for Regression?</title><link>http://arxiv.org/abs/2106.03395v2</link><description>As neural networks become more popular, the need for accompanying uncertaintyestimates increases. There are currently two main approaches to test thequality of these estimates. Most methods output a density. They can be comparedby evaluating their loglikelihood on a test set. Other methods output aprediction interval directly. These methods are often tested by examining thefraction of test points that fall inside the corresponding predictionintervals. Intuitively both approaches seem logical. However, we demonstratethrough both theoretical arguments and simulations that both ways of evaluatingthe quality of uncertainty estimates have serious flaws. Firstly, bothapproaches cannot disentangle the separate components that jointly create thepredictive uncertainty, making it difficult to evaluate the quality of theestimates of these components. Secondly, a better loglikelihood does notguarantee better prediction intervals, which is what the methods are often usedfor in practice. Moreover, the current approach to test prediction intervalsdirectly has additional flaws. We show why it is fundamentally flawed to test aprediction or confidence interval on a single test set. At best, marginalcoverage is measured, implicitly averaging out overconfident and underconfidentpredictions. A much more desirable property is pointwise coverage, requiringthe correct coverage for each prediction. We demonstrate through practicalexamples that these effects can result in favoring a method, based on thepredictive uncertainty, that has undesirable behaviour of the confidence orprediction intervals. Finally, we propose a simulation-based testing approachthat addresses these problems while still allowing easy comparison betweendifferent methods.</description><author>Laurens Sluijterman, Eric Cator, Tom Heskes</author><pubDate>Thu, 03 Aug 2023 13:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.03395v2</guid></item><item><title>Ambient Adventures: Teaching ChatGPT on Developing Complex Stories</title><link>http://arxiv.org/abs/2308.01734v1</link><description>Imaginative play is an area of creativity that could allow robots to engagewith the world around them in a much more personified way. Imaginary play canbe seen as taking real objects and locations and using them as imaginaryobjects and locations in virtual scenarios. We adopted the story generationcapability of large language models (LLMs) to obtain the stories used forimaginary play with human-written prompts. Those generated stories will besimplified and mapped into action sequences that can guide the agent inimaginary play. To evaluate whether the agent can successfully finish theimaginary play, we also designed a text adventure game to simulate a house asthe playground for the agent to interact.</description><author>Zexin Chen, Eric Zhou, Kenneth Eaton, Xiangyu Peng, Mark Riedl</author><pubDate>Thu, 03 Aug 2023 13:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01734v1</guid></item><item><title>Uni-Fusion: Universal Continuous Mapping</title><link>http://arxiv.org/abs/2303.12678v2</link><description>We present Uni-Fusion, a universal continuous mapping framework for surfaces,surface properties (color, infrared, etc.) and more (latent features in CLIPembedding space, etc.). We propose the first universal implicit encoding modelthat supports encoding of both geometry and different types of properties (RGB,infrared, features, etc.) without requiring any training. Based on this, ourframework divides the point cloud into regular grid voxels and generates alatent feature in each voxel to form a Latent Implicit Map (LIM) for geometriesand arbitrary properties. Then, by fusing a local LIM frame-wisely into aglobal LIM, an incremental reconstruction is achieved. Encoded withcorresponding types of data, our Latent Implicit Map is capable of generatingcontinuous surfaces, surface property fields, surface feature fields, and allother possible options. To demonstrate the capabilities of our model, weimplement three applications: (1) incremental reconstruction for surfaces andcolor (2) 2D-to-3D transfer of fabricated properties (3) open-vocabulary sceneunderstanding by creating a text CLIP feature field on surfaces. We evaluateUni-Fusion by comparing it in corresponding applications, from which Uni-Fusionshows high-flexibility in various applications while performing best or beingcompetitive. The project page of Uni-Fusion is available athttps://jarrome.github.io/Uni-Fusion/ .</description><author>Yijun Yuan, Andreas Nuechter</author><pubDate>Thu, 03 Aug 2023 13:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12678v2</guid></item><item><title>Towards Self-organizing Personal Knowledge Assistants in Evolving Corporate Memories</title><link>http://arxiv.org/abs/2308.01732v1</link><description>This paper presents a retrospective overview of a decade of research in ourdepartment towards self-organizing personal knowledge assistants in evolvingcorporate memories. Our research is typically inspired by real-world problemsand often conducted in interdisciplinary collaborations with research andindustry partners. We summarize past experiments and results comprising topicslike various ways of knowledge graph construction in corporate and personalsettings, Managed Forgetting and (Self-organizing) Context Spaces as a novelapproach to Personal Information Management (PIM) and knowledge work support.Past results are complemented by an overview of related work and some of ourlatest findings not published so far. Last, we give an overview of our relatedindustry use cases including a detailed look into CoMem, a Corporate Memorybased on our presented research already in productive use and providingchallenges for further research. Many contributions are only first steps in newdirections with still a lot of untapped potential, especially with regard tofurther increasing the automation in PIM and knowledge work support.</description><author>Christian Jilek, Markus Schröder, Heiko Maus, Sven Schwarz, Andreas Dengel</author><pubDate>Thu, 03 Aug 2023 13:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01732v1</guid></item><item><title>TemporalStereo: Efficient Spatial-Temporal Stereo Matching Network</title><link>http://arxiv.org/abs/2211.13755v2</link><description>We present TemporalStereo, a coarse-to-fine stereo matching network that ishighly efficient, and able to effectively exploit the past geometry and contextinformation to boost matching accuracy. Our network leverages sparse costvolume and proves to be effective when a single stereo pair is given. However,its peculiar ability to use spatio-temporal information across stereo sequencesallows TemporalStereo to alleviate problems such as occlusions and reflectiveregions while enjoying high efficiency also in this latter case. Notably, ourmodel -- trained once with stereo videos -- can run in both single-pair andtemporal modes seamlessly. Experiments show that our network relying on cameramotion is robust even to dynamic objects when running on videos. We validateTemporalStereo through extensive experiments on synthetic (SceneFlow,TartanAir) and real (KITTI 2012, KITTI 2015) datasets. Our model achievesstate-of-the-art performance on any of these datasets. Code is available at\url{https://github.com/youmi-zym/TemporalStereo.git}.</description><author>Youmin Zhang, Matteo Poggi, Stefano Mattoccia</author><pubDate>Thu, 03 Aug 2023 13:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13755v2</guid></item><item><title>Two Approaches to Supervised Image Segmentation</title><link>http://arxiv.org/abs/2307.10123v2</link><description>Though performed almost effortlessly by humans, segmenting 2D gray-scale orcolor images in terms of regions of interest (e.g.~background, objects, orportions of objects) constitutes one of the greatest challenges in science andtechnology as a consequence of the involved dimensionality reduction(3D to 2D),noise, reflections, shades, and occlusions, among many other possible effects.While a large number of interesting related approaches have been suggestedalong the last decades, it was mainly thanks to the recent development of deeplearning that more effective and general solutions have been obtained,currently constituting the basic comparison reference for this type ofoperation. Also developed recently, a multiset-based methodology has beendescribed that is capable of encouraging image segmentation performance whilecombining spatial accuracy, stability, and robustness while requiring littlecomputational resources (hardware and/or training and recognition time). Theinteresting features of the multiset neurons methodology mostly follow from theenhanced selectivity and sensitivity, as well as good robustness to dataperturbations and outliers, allowed by the coincidence similarity index onwhich the multiset approach to supervised image segmentation is based. Afterdescribing the deep learning and multiset neurons approaches, the present workdevelops two comparison experiments between them which are primarily aimed atillustrating their respective main interesting features when applied to theadopted specific type of data and parameter configurations. While the deeplearning approach confirmed its potential for performing image segmentation,the alternative multiset methodology allowed for enhanced accuracy whilerequiring little computational resources.</description><author>Alexandre Benatti, Luciano da F. Costa</author><pubDate>Thu, 03 Aug 2023 13:45:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10123v2</guid></item><item><title>Normative framework for deriving neural networks with multi-compartmental neurons and non-Hebbian plasticity</title><link>http://arxiv.org/abs/2302.10051v2</link><description>An established normative approach for understanding the algorithmic basis ofneural computation is to derive online algorithms from principled computationalobjectives and evaluate their compatibility with anatomical and physiologicalobservations. Similarity matching objectives have served as successful startingpoints for deriving online algorithms that map onto neural networks (NNs) withpoint neurons and Hebbian/anti-Hebbian plasticity. These NN models account formany anatomical and physiological observations; however, the objectives havelimited computational power and the derived NNs do not explainmulti-compartmental neuronal structures and non-Hebbian forms of plasticitythat are prevalent throughout the brain. In this article, we unify andgeneralize recent extensions of the similarity matching approach to addressmore complex objectives, including a large class of unsupervised andself-supervised learning tasks that can be formulated as symmetric generalizedeigenvalue problems or nonnegative matrix factorization problems.Interestingly, the online algorithms derived from these objectives naturallymap onto NNs with multi-compartmental neurons and local, non-Hebbian learningrules. Therefore, this unified extension of the similarity matching approachprovides a normative framework that facilitates understandingmulti-compartmental neuronal structures and non-Hebbian plasticity foundthroughout the brain.</description><author>David Lipshutz, Yanis Bahroun, Siavash Golkar, Anirvan M. Sengupta, Dmitri B. Chklovskii</author><pubDate>Thu, 03 Aug 2023 13:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10051v2</guid></item><item><title>Quantification of Predictive Uncertainty via Inference-Time Sampling</title><link>http://arxiv.org/abs/2308.01731v1</link><description>Predictive variability due to data ambiguities has typically been addressedvia construction of dedicated models with built-in probabilistic capabilitiesthat are trained to predict uncertainty estimates as variables of interest.These approaches require distinct architectural components and trainingmechanisms, may include restrictive assumptions and exhibit overconfidence,i.e., high confidence in imprecise predictions. In this work, we propose apost-hoc sampling strategy for estimating predictive uncertainty accounting fordata ambiguity. The method can generate different plausible outputs for a giveninput and does not assume parametric forms of predictive distributions. It isarchitecture agnostic and can be applied to any feed-forward deterministicnetwork without changes to the architecture or training procedure. Experimentson regression tasks on imaging and non-imaging input data show the method'sability to generate diverse and multi-modal predictive distributions, and adesirable correlation of the estimated uncertainty with the prediction error.</description><author>Katarína Tóthová, Ľubor Ladický, Daniel Thul, Marc Pollefeys, Ender Konukoglu</author><pubDate>Thu, 03 Aug 2023 13:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01731v1</guid></item><item><title>Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data</title><link>http://arxiv.org/abs/2308.01729v1</link><description>We present novel cross-sectional and longitudinal claim count models forvehicle insurance built upon the Combined Actuarial Neural Network (CANN)framework proposed by Mario W\"uthrich and Michael Merz. The CANN approachcombines a classical actuarial model, such as a generalized linear model, witha neural network. This blending of models results in a two-component modelcomprising a classical regression model and a neural network part. The CANNmodel leverages the strengths of both components, providing a solid foundationand interpretability from the classical model while harnessing the flexibilityand capacity to capture intricate relationships and interactions offered by theneural network. In our proposed models, we use well-known log-linear claimcount regression models for the classical regression part and a multilayerperceptron (MLP) for the neural network part. The MLP part is used to processtelematics car driving data given as a vector characterizing the drivingbehavior of each insured driver. In addition to the Poisson and negativebinomial distributions for cross-sectional data, we propose a procedure fortraining our CANN model with a multivariate negative binomial (MVNB)specification. By doing so, we introduce a longitudinal model that accounts forthe dependence between contracts from the same insured. Our results reveal thatthe CANN models exhibit superior performance compared to log-linear models thatrely on manually engineered telematics features.</description><author>Francis Duval, Jean-Philippe Boucher, Mathieu Pigeon</author><pubDate>Thu, 03 Aug 2023 13:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01729v1</guid></item><item><title>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</title><link>http://arxiv.org/abs/2307.11067v2</link><description>We propose a simple three-stage approach to segment unseen objects in RGBimages using their CAD models. Leveraging recent powerful foundation models,DINOv2 and Segment Anything, we create descriptors and generate proposals,including binary masks for a given input RGB image. By matching proposals withreference descriptors created from CAD models, we achieve precise object IDassignment along with modal masks. We experimentally demonstrate that ourmethod achieves state-of-the-art results in CAD-based novel objectsegmentation, surpassing existing approaches on the seven core datasets of theBOP challenge by 19.8% AP using the same BOP evaluation protocol. Our sourcecode is available at https://github.com/nv-nguyen/cnos.</description><author>Van Nguyen Nguyen, Tomas Hodan, Georgy Ponimatkin, Thibault Groueix, Vincent Lepetit</author><pubDate>Thu, 03 Aug 2023 13:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11067v2</guid></item><item><title>Local Large Language Models for Complex Structured Medical Tasks</title><link>http://arxiv.org/abs/2308.01727v1</link><description>This paper introduces an approach that combines the language reasoningcapabilities of large language models (LLMs) with the benefits of localtraining to tackle complex, domain-specific tasks. Specifically, the authorsdemonstrate their approach by extracting structured condition codes frompathology reports. The proposed approach utilizes local LLMs, which can befine-tuned to respond to specific generative instructions and providestructured outputs. The authors collected a dataset of over 150k uncuratedsurgical pathology reports, containing gross descriptions, final diagnoses, andcondition codes. They trained different model architectures, including LLaMA,BERT and LongFormer and evaluated their performance. The results show that theLLaMA-based models significantly outperform BERT-style models across allevaluated metrics, even with extremely reduced precision. The LLaMA modelsperformed especially well with large datasets, demonstrating their ability tohandle complex, multi-label tasks. Overall, this work presents an effectiveapproach for utilizing LLMs to perform domain-specific tasks using accessiblehardware, with potential applications in the medical domain, where complex dataextraction and classification are required.</description><author>V. K. Cody Bumgardner, Aaron Mullen, Sam Armstrong, Caylin Hickey, Jeff Talbert</author><pubDate>Thu, 03 Aug 2023 13:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01727v1</guid></item><item><title>Weakly Supervised 3D Instance Segmentation without Instance-level Annotations</title><link>http://arxiv.org/abs/2308.01721v1</link><description>3D semantic scene understanding tasks have achieved great success with theemergence of deep learning, but often require a huge amount of manuallyannotated training data. To alleviate the annotation cost, we propose the firstweakly-supervised 3D instance segmentation method that only requirescategorical semantic labels as supervision, and we do not need instance-levellabels. The required semantic annotations can be either dense or extreme sparse(e.g. 0.02% of total points). Even without having any instance-relatedground-truth, we design an approach to break point clouds into raw fragmentsand find the most confident samples for learning instance centroids.Furthermore, we construct a recomposed dataset using pseudo instances, which isused to learn our defined multilevel shape-aware objectness signal. Anasymmetrical object inference algorithm is followed to process core points andboundary points with different strategies, and generate high-quality pseudoinstance labels to guide iterative training. Experiments demonstrate that ourmethod can achieve comparable results with recent fully supervised methods. Bygenerating pseudo instance labels from categorical semantic labels, ourdesigned approach can also assist existing methods for learning 3D instancesegmentation at reduced annotation cost.</description><author>Shichao Dong, Guosheng Lin</author><pubDate>Thu, 03 Aug 2023 13:30:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01721v1</guid></item><item><title>Confident Neural Network Regression with Bootstrapped Deep Ensembles</title><link>http://arxiv.org/abs/2202.10903v2</link><description>With the rise of the popularity and usage of neural networks, trustworthyuncertainty estimation is becoming increasingly essential. One of the mostprominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan etal., 2017) . A classical parametric model has uncertainty in the parameters dueto the fact that the data on which the model is build is a random sample. Amodern neural network has an additional uncertainty component since theoptimization of the network is random. Lakshminarayanan et al. (2017) notedthat Deep Ensembles do not incorporate the classical uncertainty induced by theeffect of finite data. In this paper, we present a computationally cheapextension of Deep Ensembles for the regression setting, called BootstrappedDeep Ensembles, that explicitly takes this classical effect of finite data intoaccount using a modified version of the parametric bootstrap. We demonstratethrough an experimental study that our method significantly improves uponstandard Deep Ensembles</description><author>Laurens Sluijterman, Eric Cator, Tom Heskes</author><pubDate>Thu, 03 Aug 2023 13:28:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.10903v2</guid></item><item><title>ProMix: Combating Label Noise via Maximizing Clean Sample Utility</title><link>http://arxiv.org/abs/2207.10276v4</link><description>Learning with Noisy Labels (LNL) has become an appealing topic, asimperfectly annotated data are relatively cheaper to obtain. Recentstate-of-the-art approaches employ specific selection mechanisms to separateclean and noisy samples and then apply Semi-Supervised Learning (SSL)techniques for improved performance. However, the selection step mostlyprovides a medium-sized and decent-enough clean subset, which overlooks a richset of clean samples. To fulfill this, we propose a novel LNL framework ProMixthat attempts to maximize the utility of clean samples for boosted performance.Key to our method, we propose a matched high confidence selection techniquethat selects those examples with high confidence scores and matched predictionswith given labels to dynamically expand a base clean sample set. To overcomethe potential side effect of excessive clean set selection procedure, wefurther devise a novel SSL framework that is able to train balanced andunbiased classifiers on the separated clean and noisy samples. Extensiveexperiments demonstrate that ProMix significantly advances the currentstate-of-the-art results on multiple benchmarks with different types and levelsof noise. It achieves an average improvement of 2.48\% on the CIFAR-N dataset.The code is available at https://github.com/Justherozen/ProMix</description><author>Ruixuan Xiao, Yiwen Dong, Haobo Wang, Lei Feng, Runze Wu, Gang Chen, Junbo Zhao</author><pubDate>Thu, 03 Aug 2023 13:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.10276v4</guid></item><item><title>A Masked Face Classification Benchmark on Low-Resolution Surveillance Images</title><link>http://arxiv.org/abs/2211.13061v2</link><description>We propose a novel image dataset focused on tiny faces wearing face masks formask classification purposes, dubbed Small Face MASK (SF-MASK), composed of acollection made from 20k low-resolution images exported from diverse andheterogeneous datasets, ranging from 7 x 7 to 64 x 64 pixel resolution. Anaccurate visualization of this collection, through counting grids, made itpossible to highlight gaps in the variety of poses assumed by the heads of thepedestrians. In particular, faces filmed by very high cameras, in which thefacial features appear strongly skewed, are absent. To address this structuraldeficiency, we produced a set of synthetic images which resulted in asatisfactory covering of the intra-class variance. Furthermore, a smallsubsample of 1701 images contains badly worn face masks, opening to multi-classclassification challenges. Experiments on SF-MASK focus on face maskclassification using several classifiers. Results show that the richness ofSF-MASK (real + synthetic images) leads all of the tested classifiers toperform better than exploiting comparative face mask datasets, on a fixed 1077images testing set. Dataset and evaluation code are publicly available here:https://github.com/HumaticsLAB/sf-mask</description><author>Federico Cunico, Andrea Toaiari, Marco Cristani</author><pubDate>Thu, 03 Aug 2023 13:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13061v2</guid></item><item><title>Classification and Online Clustering of Zero-Day Malware</title><link>http://arxiv.org/abs/2305.00605v2</link><description>A large amount of new malware is constantly being generated, which must notonly be distinguished from benign samples, but also classified into malwarefamilies. For this purpose, investigating how existing malware families aredeveloped and examining emerging families need to be explored. This paperfocuses on the online processing of incoming malicious samples to assign themto existing families or, in the case of samples from new families, to clusterthem. We experimented with seven prevalent malware families from the EMBERdataset, four in the training set and three additional new families in the testset. Based on the classification score of the multilayer perceptron, wedetermined which samples would be classified and which would be clustered intonew malware families. We classified 97.21% of streaming data with a balancedaccuracy of 95.33%. Then, we clustered the remaining data using aself-organizing map, achieving a purity from 47.61% for four clusters to 77.68%for ten clusters. These results indicate that our approach has the potential tobe applied to the classification and clustering of zero-day malware intomalware families.</description><author>Olha Jurečková, Martin Jureček, Mark Stamp, Fabio Di Troia, Róbert Lórencz</author><pubDate>Thu, 03 Aug 2023 13:04:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00605v2</guid></item><item><title>CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patient With Suspected Ischemic Stroke</title><link>http://arxiv.org/abs/2303.08757v2</link><description>Precise and fast prediction methods for ischemic areas comprised of deadtissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)patients are of significant clinical interest. They play an essential role inimproving diagnosis and treatment planning. Computed Tomography (CT) scan isone of the primary modalities for early assessment in patients with suspectedAIS. CT Perfusion (CTP) is often used as a primary assessment to determinestroke location, severity, and volume of ischemic lesions. Current automaticsegmentation methods for CTP mostly use already processed 3D parametric mapsconventionally used for clinical interpretation by radiologists as input.Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+timeinput, where the spatial information over the volume is ignored. In addition,these methods are only interested in segmenting core regions, while predictingpenumbra can be essential for treatment planning. This paper investigatesdifferent methods to utilize the entire 4D CTP as input to fully exploit thespatio-temporal information, leading us to propose a novel 4D convolutionlayer. Our comprehensive experiments on a local dataset of 152 patients dividedinto three groups show that our proposed models generate more precise resultsthan other methods explored. Adopting the proposed 4D mJ-Net, a DiceCoefficient of 0.53 and 0.23 is achieved for segmenting penumbra and coreareas, respectively. The code is available onhttps://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.</description><author>Luca Tomasetti, Kjersti Engan, Liv Jorunn Høllesli, Kathinka Dæhli Kurz, Mahdieh Khanmohammadi</author><pubDate>Thu, 03 Aug 2023 13:00:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08757v2</guid></item><item><title>A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem</title><link>http://arxiv.org/abs/2212.08736v3</link><description>We consider the inverse acoustic obstacle problem for sound-soft star-shapedobstacles in two dimensions wherein the boundary of the obstacle is determinedfrom measurements of the scattered field at a collection of receivers outsidethe object. One of the standard approaches for solving this problem is toreformulate it as an optimization problem: finding the boundary of the domainthat minimizes the $L^2$ distance between computed values of the scatteredfield and the given measurement data. The optimization problem iscomputationally challenging since the local set of convexity shrinks withincreasing frequency and results in an increasing number of local minima in thevicinity of the true solution. In many practical experimental settings, lowfrequency measurements are unavailable due to limitations of the experimentalsetup or the sensors used for measurement. Thus, obtaining a good initial guessfor the optimization problem plays a vital role in this environment. We present a neural network warm-start approach for solving the inversescattering problem, where an initial guess for the optimization problem isobtained using a trained neural network. We demonstrate the effectiveness ofour method with several numerical examples. For high frequency problems, thisapproach outperforms traditional iterative methods such as Gauss-Newtoninitialized without any prior (i.e., initialized using a unit circle), orinitialized using the solution of a direct method such as the linear samplingmethod. The algorithm remains robust to noise in the scattered fieldmeasurements and also converges to the true solution for limited aperture data.However, the number of training samples required to train the neural networkscales exponentially in frequency and the complexity of the obstaclesconsidered. We conclude with a discussion of this phenomenon and potentialdirections for future research.</description><author>Mo Zhou, Jiequn Han, Manas Rachh, Carlos Borges</author><pubDate>Thu, 03 Aug 2023 12:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08736v3</guid></item><item><title>Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition</title><link>http://arxiv.org/abs/2202.07901v3</link><description>Cross-modal representation learning learns a shared embedding between two ormore modalities to improve performance in a given task compared to using onlyone of the modalities. Cross-modal representation learning from different datatypes -- such as images and time-series data (e.g., audio or text data) --requires a deep metric learning loss that minimizes the distance between themodality embeddings. In this paper, we propose to use the contrastive ortriplet loss, which uses positive and negative identities to create samplepairs with different labels, for cross-modal representation learning betweenimage and time-series modalities (CMR-IS). By adapting the triplet loss forcross-modal representation learning, higher accuracy in the main (time-seriesclassification) task can be achieved by exploiting additional information ofthe auxiliary (image classification) task. We present a triplet loss with adynamic margin for single label and sequence-to-sequence classification tasks.We perform extensive evaluations on synthetic image and time-series data, andon data for offline handwriting recognition (HWR) and on online HWR fromsensor-enhanced pens for classifying written words. Our experiments show animproved classification accuracy, faster convergence, and bettergeneralizability due to an improved cross-modal representation. Furthermore,the more suitable generalizability leads to a better adaptability betweenwriters for online HWR.</description><author>Felix Ott, David Rügamer, Lucas Heublein, Bernd Bischl, Christopher Mutschler</author><pubDate>Thu, 03 Aug 2023 12:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.07901v3</guid></item><item><title>Reconstructing Pruned Filters using Cheap Spatial Transformations</title><link>http://arxiv.org/abs/2110.12844v2</link><description>We present an efficient alternative to the convolutional layer using cheapspatial transformations. This construction exploits an inherent spatialredundancy of the learned convolutional filters to enable a much greaterparameter efficiency, while maintaining the top-end accuracy of their densecounter-parts. Training these networks is modelled as a generalised pruningproblem, whereby the pruned filters are replaced with cheap transformationsfrom the set of non-pruned filters. We provide an efficient implementation ofthe proposed layer, followed by two natural extensions to avoid excessivefeature compression and to improve the expressivity of the transformedfeatures. We show that these networks can achieve comparable or improvedperformance to state-of-the-art pruning models across both the CIFAR-10 andImageNet-1K datasets.</description><author>Roy Miles, Krystian Mikolajczyk</author><pubDate>Thu, 03 Aug 2023 12:35:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.12844v2</guid></item><item><title>Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition</title><link>http://arxiv.org/abs/2308.01700v1</link><description>Feature selection could be defined as an optimization problem and solved bybio-inspired algorithms. Bees Algorithm (BA) shows decent performance infeature selection optimization tasks. On the other hand, Local PhaseQuantization (LPQ) is a frequency domain feature which has excellentperformance on Depth images. Here, after extracting LPQ features out of RGB(colour) and Depth images from the Iranian Kinect Face Database (IKFDB), theBees feature selection algorithm applies to select the desired number offeatures for final classification tasks. IKFDB is recorded with Kinect sensorV.2 and contains colour and depth images for facial and facialmicro-expressions recognition purposes. Here five facial expressions of Anger,Joy, Surprise, Disgust and Fear are used for final validation. The proposedBees LPQ method is compared with Particle Swarm Optimization (PSO) LPQ, PCALPQ, Lasso LPQ, and just LPQ features for classification tasks with SupportVector Machines (SVM), K-Nearest Neighbourhood (KNN), Shallow Neural Networkand Ensemble Subspace KNN. Returned results, show a decent performance of theproposed algorithm (99 % accuracy) in comparison with others.</description><author>Seyed Muhammad Hossein Mousavi, Atiye Ilanloo</author><pubDate>Thu, 03 Aug 2023 12:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01700v1</guid></item></channel></rss>