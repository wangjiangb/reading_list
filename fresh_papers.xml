<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 06 Feb 2025 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling</title><link>http://arxiv.org/abs/2502.02590v1</link><description>3D articulated objects modeling has long been a challenging problem, since itrequires to capture both accurate surface geometries and semanticallymeaningful and spatially precise structures, parts, and joints. Existingmethods heavily depend on training data from a limited set of handcraftedarticulated object categories (e.g., cabinets and drawers), which restrictstheir ability to model a wide range of articulated objects in anopen-vocabulary context. To address these limitations, we propose ArticulateAnymesh, an automated framework that is able to convert any rigid 3D mesh intoits articulated counterpart in an open-vocabulary manner. Given a 3D mesh, ourframework utilizes advanced Vision-Language Models and visual promptingtechniques to extract semantic information, allowing for both the segmentationof object parts and the construction of functional joints. Our experiments showthat Articulate Anymesh can generate large-scale, high-quality 3D articulatedobjects, including tools, toys, mechanical devices, and vehicles, significantlyexpanding the coverage of existing 3D articulated object datasets.Additionally, we show that these generated assets can facilitate theacquisition of new articulated object manipulation skills in simulation, whichcan then be transferred to a real robotic system. Our Github website ishttps://articulate-anymesh.github.io.</description><author>Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan</author><pubDate>Tue, 04 Feb 2025 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02590v1</guid></item><item><title>COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation</title><link>http://arxiv.org/abs/2502.02589v1</link><description>This paper introduces the COCONut-PanCap dataset, created to enhance panopticsegmentation and grounded image captioning. Building upon the COCO dataset withadvanced COCONut panoptic masks, this dataset aims to overcome limitations inexisting image-text datasets that often lack detailed, scene-comprehensivedescriptions. The COCONut-PanCap dataset incorporates fine-grained,region-level captions grounded in panoptic segmentation masks, ensuringconsistency and improving the detail of generated captions. Throughhuman-edited, densely annotated descriptions, COCONut-PanCap supports improvedtraining of vision-language models (VLMs) for image understanding andgenerative models for text-to-image tasks. Experimental results demonstratethat COCONut-PanCap significantly boosts performance across understanding andgeneration tasks, offering complementary benefits to large-scale datasets. Thisdataset sets a new benchmark for evaluating models on joint panopticsegmentation and grounded captioning tasks, addressing the need forhigh-quality, detailed image-text annotations in multi-modal learning.</description><author>Xueqing Deng, Qihang Yu, Ali Athar, Chenglin Yang, Linjie Yang, Xiaojie Jin, Xiaohui Shen, Liang-Chieh Chen</author><pubDate>Tue, 04 Feb 2025 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02589v1</guid></item><item><title>Calibrated Multi-Preference Optimization for Aligning Diffusion Models</title><link>http://arxiv.org/abs/2502.02588v1</link><description>Aligning text-to-image (T2I) diffusion models with preference optimization isvaluable for human-annotated datasets, but the heavy cost of manual datacollection limits scalability. Using reward models offers an alternative,however, current preference optimization methods fall short in exploiting therich information, as they only consider pairwise preference distribution.Furthermore, they lack generalization to multi-preference scenarios andstruggle to handle inconsistencies between rewards. To address this, we presentCalibrated Preference Optimization (CaPO), a novel method to align T2Idiffusion models by incorporating the general preference from multiple rewardmodels without human annotated data. The core of our approach involves a rewardcalibration method to approximate the general preference by computing theexpected win-rate against the samples generated by the pretrained models.Additionally, we propose a frontier-based pair selection method thateffectively manages the multi-preference distribution by selecting pairs fromPareto frontiers. Finally, we use regression loss to fine-tune diffusion modelsto match the difference between calibrated rewards of a selected pair.Experimental results show that CaPO consistently outperforms prior methods,such as Direct Preference Optimization (DPO), in both single and multi-rewardsettings validated by evaluation on T2I benchmarks, including GenEval andT2I-Compbench.</description><author>Kyungmin Lee, Xiaohang Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, Yinxiao Li</author><pubDate>Tue, 04 Feb 2025 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02588v1</guid></item><item><title>Spatio-temporal transformer to support automatic sign language translation</title><link>http://arxiv.org/abs/2502.02587v1</link><description>Sign Language Translation (SLT) systems support hearing-impaired peoplecommunication by finding equivalences between signed and spoken languages. Thistask is however challenging due to multiple sign variations, complexity inlanguage and inherent richness of expressions. Computational approaches haveevidenced capabilities to support SLT. Nonetheless, these approaches remainlimited to cover gestures variability and support long sequence translations.This paper introduces a Transformer-based architecture that encodesspatio-temporal motion gestures, preserving both local and long-range spatialinformation through the use of multiple convolutional and attention mechanisms.The proposed approach was validated on the Colombian Sign Language TranslationDataset (CoL-SLTD) outperforming baseline approaches, and achieving a BLEU4 of46.84%. Additionally, the proposed approach was validated on theRWTH-PHOENIX-Weather-2014T (PHOENIX14T), achieving a BLEU4 score of 30.77%,demonstrating its robustness and effectiveness in handling real-worldvariations</description><author>Christian Ruiz, Fabio Martinez</author><pubDate>Tue, 04 Feb 2025 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02587v1</guid></item><item><title>QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search</title><link>http://arxiv.org/abs/2502.02584v1</link><description>Language agents have become a promising solution to complex interactivetasks. One of the key ingredients to the success of language agents is thereward model on the trajectory of the agentic workflow, which provides valuableguidance during training or inference. However, due to the lack of annotationsof intermediate interactions, most existing works use an outcome reward modelto optimize policies across entire trajectories. This may lead to sub-optimalpolicies and hinder the overall performance. To address this, we propose QLASS(Q-guided Language Agent Stepwise Search), to automatically generateannotations by estimating Q-values in a stepwise manner for open languageagents. By introducing a reasoning tree and performing process reward modeling,QLASS provides effective intermediate guidance for each step. With the stepwiseguidance, we propose a Q-guided generation strategy to enable language agentsto better adapt to long-term value, resulting in significant performanceimprovement during model inference on complex interactive agent tasks. Notably,even with almost half the annotated data, QLASS retains strong performance,demonstrating its efficiency in handling limited supervision. We alsoempirically demonstrate that QLASS can lead to more effective decision makingthrough qualitative analysis. We will release our code and data.</description><author>Zongyu Lin, Yao Tang, Xingcheng Yao, Da Yin, Ziniu Hu, Yizhou Sun, Kai-Wei Chang</author><pubDate>Tue, 04 Feb 2025 18:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02584v1</guid></item><item><title>Open Materials Generation with Stochastic Interpolants</title><link>http://arxiv.org/abs/2502.02582v1</link><description>The discovery of new materials is essential for enabling technologicaladvancements. Computational approaches for predicting novel materials musteffectively learn the manifold of stable crystal structures within an infinitedesign space. We introduce Open Materials Generation (OMG), a unifyingframework for the generative design and discovery of inorganic crystallinematerials. OMG employs stochastic interpolants (SI) to bridge an arbitrary basedistribution to the target distribution of inorganic crystals via a broad classof tunable stochastic processes, encompassing both diffusion models and flowmatching as special cases. In this work, we adapt the SI framework byintegrating an equivariant graph representation of crystal structures andextending it to account for periodic boundary conditions in unit cellrepresentations. Additionally, we couple the SI flow over spatial coordinatesand lattice vectors with discrete flow matching for atomic species. Webenchmark OMG's performance on two tasks: Crystal Structure Prediction (CSP)for specified compositions, and 'de novo' generation (DNG) aimed at discoveringstable, novel, and unique structures. In our ground-up implementation of OMG,we refine and extend both CSP and DNG metrics compared to previous works. OMGestablishes a new state-of-the-art in generative modeling for materialsdiscovery, outperforming purely flow-based and diffusion-based implementations.These results underscore the importance of designing flexible deep learningframeworks to accelerate progress in materials science.</description><author>Philipp Hoellmer, Thomas Egg, Maya M. Martirossyan, Eric Fuemmeler, Amit Gupta, Zeren Shui, Pawan Prakash, Adrian Roitberg, Mingjie Liu, George Karypis, Mark Transtrum, Richard G. Hennig, Ellad B. Tadmor, Stefano Martiniani</author><pubDate>Tue, 04 Feb 2025 18:56:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02582v1</guid></item><item><title>Minimax-Optimal Covariance Projected Spectral Clustering for High-Dimensional Nonspherical Mixtures</title><link>http://arxiv.org/abs/2502.02580v1</link><description>In mixture models, nonspherical (anisotropic) noise within each cluster iswidely present in real-world data. We study both the minimax rate and optimalstatistical procedure for clustering under high-dimensional nonsphericalmixture models. In high-dimensional settings, we first establish theinformation-theoretic limits for clustering under Gaussian mixtures. Theminimax lower bound unveils an intriguing informational dimension-reductionphenomenon: there exists a substantial gap between the minimax rate and theoracle clustering risk, with the former determined solely by the projectedcenters and projected covariance matrices in a low-dimensional space. Motivatedby the lower bound, we propose a novel computationally efficient clusteringmethod: Covariance Projected Spectral Clustering (COPO). Its key step is toproject the high-dimensional data onto the low-dimensional space spanned by thecluster centers and then use the projected covariance matrices in this space toenhance clustering. We establish tight algorithmic upper bounds for COPO, bothfor Gaussian noise with flexible covariance and general noise with localdependence. Our theory indicates the minimax-optimality of COPO in the Gaussiancase and highlights its adaptivity to a broad spectrum of dependent noise.Extensive simulation studies under various noise structures and real dataanalysis demonstrate our method's superior performance.</description><author>Chengzhu Huang, Yuqi Gu</author><pubDate>Tue, 04 Feb 2025 18:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02580v1</guid></item><item><title>A comparison of translation performance between DeepL and Supertext</title><link>http://arxiv.org/abs/2502.02577v1</link><description>As strong machine translation (MT) systems are increasingly based on largelanguage models (LLMs), reliable quality benchmarking requires methods thatcapture their ability to leverage extended context. This study compares twocommercial MT systems -- DeepL and Supertext -- by assessing their performanceon unsegmented texts. We evaluate translation quality across four languagedirections with professional translators assessing segments with fulldocument-level context. While segment-level assessments indicate no strongpreference between the systems in most cases, document-level analysis reveals apreference for Supertext in three out of four language directions, suggestingsuperior consistency across longer texts. We advocate for morecontext-sensitive evaluation methodologies to ensure that MT qualityassessments reflect real-world usability. We release all evaluation data andscripts for further analysis and reproduction athttps://github.com/supertext/evaluation_deepl_supertext.</description><author>Alex Flückiger, Chantal Amrhein, Tim Graf, Philippe Schläpfer, Florian Schottmann, Samuel Läubli</author><pubDate>Tue, 04 Feb 2025 18:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02577v1</guid></item><item><title>Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement</title><link>http://arxiv.org/abs/2502.02573v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities acrossnumerous fields, presenting an opportunity to revolutionize optimizationproblem-solving, a crucial, ubiquitous, and complex domain. This paper exploresthe proficiency of LLMs in handling Sequential Optimization Problems (SOPs). Weintroduce WorldGen, a dynamic framework for generating unseen SOPs withcontrollable complexities, to evaluate LLM performance. Our initialobservations reveal that while LLMs perform well on simple SOPs, theirperformance significantly degrades with increased complexity. Motivated bythis, we revisit philosophical hypotheses on reasoning to enhance LLMperformance. Inspired by the influential framework of Hegelian Dialectics, wepropose ACE, demonstrating how the performance of LLMs in SOP contexts can besignificantly improved without any retraining or further fine-tuning.</description><author>Soheil Abbasloo</author><pubDate>Tue, 04 Feb 2025 18:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02573v1</guid></item><item><title>LoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation</title><link>http://arxiv.org/abs/2501.16559v2</link><description>The rising popularity of large foundation models has led to a heighteneddemand for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation(LoRA), which offer performance comparable to full model fine-tuning whilerequiring only a few additional parameters tailored to the specific base model.When such base models are deprecated and replaced, all associated LoRA modulesmust be retrained, requiring access to either the original training data or asubstantial amount of synthetic data that mirrors the original distribution.However, the original data is often inaccessible due to privacy or licensingissues, and generating synthetic data may be impractical and insufficientlyrepresentative. These factors complicate the fine-tuning process considerably.To address this challenge, we introduce a new adapter, Cross-Model Low-RankAdaptation (LoRA-X), which enables the training-free transfer of LoRAparameters across source and target models, eliminating the need for originalor synthetic training data. Our approach imposes the adapter to operate withinthe subspace of the source base model. This constraint is necessary because ourprior knowledge of the target model is limited to its weights, and the criteriafor ensuring the adapter's transferability are restricted to the target basemodel's weights and subspace. To facilitate the transfer of LoRA parameters ofthe source model to a target model, we employ the adapter only in the layers ofthe target model that exhibit an acceptable level of subspace similarity. Ourextensive experiments demonstrate the effectiveness of LoRA-X for text-to-imagegeneration, including Stable Diffusion v1.5 and Stable Diffusion XL.</description><author>Farzad Farhadzadeh, Debasmit Das, Shubhankar Borse, Fatih Porikli</author><pubDate>Tue, 04 Feb 2025 18:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.16559v2</guid></item><item><title>Fairness in Survival Analysis: A Novel Conditional Mutual Information Augmentation Approach</title><link>http://arxiv.org/abs/2502.02567v1</link><description>Survival analysis, a vital tool for predicting the time to event, has beenused in many domains such as healthcare, criminal justice, and finance. Likeclassification tasks, survival analysis can exhibit bias against disadvantagedgroups, often due to biases inherent in data or algorithms. Several studies inboth the IS and CS communities have attempted to address fairness in survivalanalysis. However, existing methods often overlook the importance of predictionfairness at pre-defined evaluation time points, which is crucial in real-worldapplications where decision making often hinges on specific time frames. Toaddress this critical research gap, we introduce a new fairness concept:equalized odds (EO) in survival analysis, which emphasizes prediction fairnessat pre-defined time points. To achieve the EO fairness in survival analysis, wepropose a Conditional Mutual Information Augmentation (CMIA) approach, whichfeatures a novel fairness regularization term based on conditional mutualinformation and an innovative censored data augmentation technique. Our CMIAapproach can effectively balance prediction accuracy and fairness, and it isapplicable to various survival models. We evaluate the CMIA approach againstseveral state-of-the-art methods within three different application domains,and the results demonstrate that CMIA consistently reduces prediction disparitywhile maintaining good accuracy and significantly outperforms the othercompeting methods across multiple datasets and survival models (e.g., linearCOX, deep AFT).</description><author>Tianyang Xie, Yong Ge</author><pubDate>Tue, 04 Feb 2025 18:40:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02567v1</guid></item><item><title>Revisiting Expected Possession Value in Football: Introducing a Benchmark, U-Net Architecture, and Reward and Risk for Passes</title><link>http://arxiv.org/abs/2502.02565v1</link><description>This paper introduces the first Expected Possession Value (EPV) benchmark anda new and improved EPV model for football. Through the introduction of theOJN-Pass-EPV benchmark, we present a novel method to quantitatively assess thequality of EPV models by using pairs of game states with given relative EPVs.Next, we attempt to replicate the results of Fern\'andez et al. (2021) using adataset containing Dutch Eredivisie and World Cup matches. Following ourfailure to do so, we propose a new architecture based on U-net-typeconvolutional neural networks, achieving good results in model loss andExpected Calibration Error. Finally, we present an improved pass model thatincorporates ball height and contains a new dual-component pass value modelthat analyzes reward and risk. The resulting EPV model correctly identifies thehigher value state in 78% of the game state pairs in the OJN-Pass-EPVbenchmark, demonstrating its ability to accurately assess goal-scoringpotential. Our findings can help assess the quality of EPV models, improve EPVpredictions, help assess potential reward and risk of passing decisions, andimprove player and team performance.</description><author>Thijs Overmeer, Tim Janssen, Wim P. M. Nuijten</author><pubDate>Tue, 04 Feb 2025 18:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02565v1</guid></item><item><title>Learning the RoPEs: Better 2D and 3D Position Encodings with STRING</title><link>http://arxiv.org/abs/2502.02562v1</link><description>We introduce STRING: Separable Translationally Invariant Position Encodings.STRING extends Rotary Position Encodings, a recently proposed and widely usedalgorithm in large language models, via a unifying theoretical framework.Importantly, STRING still provides exact translation invariance, includingtoken coordinates of arbitrary dimensionality, whilst maintaining a lowcomputational footprint. These properties are especially important in robotics,where efficient 3D token representation is key. We integrate STRING into VisionTransformers with RGB(-D) inputs (color plus optional depth), showingsubstantial gains, e.g. in open-vocabulary object detection and for roboticscontrollers. We complement our experiments with a rigorous mathematicalanalysis, proving the universality of our methods.</description><author>Connor Schenck, Isaac Reid, Mithun George Jacob, Alex Bewley, Joshua Ainslie, David Rendleman, Deepali Jain, Mohit Sharma, Avinava Dubey, Ayzaan Wahid, Sumeet Singh, Rene Wagner, Tianli Ding, Chuyuan Fu, Arunkumar Byravan, Jake Varley, Alexey Gritsenko, Matthias Minderer, Dmitry Kalashnikov, Jonathan Tompson, Vikas Sindhwani, Krzysztof Choromanski</author><pubDate>Tue, 04 Feb 2025 18:37:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02562v1</guid></item><item><title>Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents</title><link>http://arxiv.org/abs/2502.02561v1</link><description>A fundamental question in data-driven decision making is how to quantify theuncertainty of predictions in ways that can usefully inform downstream action.This interface between prediction uncertainty and decision-making is especiallyimportant in risk-sensitive domains, such as medicine. In this paper, wedevelop decision-theoretic foundations that connect uncertainty quantificationusing prediction sets with risk-averse decision-making. Specifically, we answerthree fundamental questions: (1) What is the correct notion of uncertaintyquantification for risk-averse decision makers? We prove that prediction setsare optimal for decision makers who wish to optimize their value at risk. (2)What is the optimal policy that a risk averse decision maker should use to mapprediction sets to actions? We show that a simple max-min decision policy isoptimal for risk-averse decision makers. Finally, (3) How can we deriveprediction sets that are optimal for such decision makers? We provide an exactcharacterization in the population regime and a distribution free finite-sampleconstruction. Answering these questions naturally leads to an algorithm,Risk-Averse Calibration (RAC), which follows a provably optimal design forderiving action policies from predictions. RAC is designed to be bothpractical-capable of leveraging the quality of predictions in a black-boxmanner to enhance downstream utility-and safe-adhering to a user-defined riskthreshold and optimizing the corresponding risk quantile of the user'sdownstream utility. Finally, we experimentally demonstrate the significantadvantages of RAC in applications such as medical diagnosis and recommendationsystems. Specifically, we show that RAC achieves a substantially improvedtrade-off between safety and utility, offering higher utility compared toexisting methods while maintaining the safety guarantee.</description><author>Shayan Kiyani, George Pappas, Aaron Roth, Hamed Hassani</author><pubDate>Tue, 04 Feb 2025 18:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02561v1</guid></item><item><title>Particle Trajectory Representation Learning with Masked Point Modeling</title><link>http://arxiv.org/abs/2502.02558v1</link><description>Effective self-supervised learning (SSL) techniques have been key tounlocking large datasets for representation learning. While many promisingmethods have been developed using online corpora and captioned photographs,their application to scientific domains, where data encodes highly specializedknowledge, remains in its early stages. We present a self-supervised maskedmodeling framework for 3D particle trajectory analysis in Time ProjectionChambers (TPCs). These detectors produce globally sparse (&lt;1% occupancy) butlocally dense point clouds, capturing meter-scale particle trajectories atmillimeter resolution. Starting with PointMAE, this work proposes volumetrictokenization to group sparse ionization points into resolution-agnosticpatches, as well as an auxiliary energy infilling task to improve trajectorysemantics. This approach -- which we call Point-based Liquid Argon MaskedAutoencoder (PoLAr-MAE) -- achieves 99.4% track and 97.7% shower classificationF-scores, matching that of supervised baselines without any labeled data. Whilethe model learns rich particle trajectory representations, it struggles withsub-token phenomena like overlapping or short-lived particle trajectories. Tosupport further research, we release PILArNet-M -- the largest open LArTPCdataset (1M+ events, 5.2B labeled points) -- to advance SSL in high energyphysics (HEP). Project site: https://youngsm.com/polarmae/</description><author>Sam Young, Yeon-jae Jwa, Kazuhiro Terao</author><pubDate>Tue, 04 Feb 2025 18:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02558v1</guid></item><item><title>SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</title><link>http://arxiv.org/abs/2501.01320v3</link><description>Video restoration poses non-trivial challenges in maintaining fidelity whilerecovering temporally consistent details from unknown degradations in the wild.Despite recent advances in diffusion-based restoration, these methods oftenface limitations in generation capability and sampling efficiency. In thiswork, we present SeedVR, a diffusion transformer designed to handle real-worldvideo restoration with arbitrary length and resolution. The core design ofSeedVR lies in the shifted window attention that facilitates effectiverestoration on long video sequences. SeedVR further supports variable-sizedwindows near the boundary of both spatial and temporal dimensions, overcomingthe resolution constraints of traditional window attention. Equipped withcontemporary practices, including causal video autoencoder, mixed image andvideo training, and progressive training, SeedVR achieves highly-competitiveperformance on both synthetic and real-world benchmarks, as well asAI-generated videos. Extensive experiments demonstrate SeedVR's superiorityover existing methods for generic video restoration.</description><author>Jianyi Wang, Zhijie Lin, Meng Wei, Yang Zhao, Ceyuan Yang, Fei Xiao, Chen Change Loy, Lu Jiang</author><pubDate>Tue, 04 Feb 2025 18:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01320v3</guid></item><item><title>AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late Dynamic Contrast Enhanced Prostate MRI Synthesis</title><link>http://arxiv.org/abs/2502.02555v1</link><description>Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medicalimaging technique that plays a crucial role in the detailed visualization andidentification of tissue perfusion in abnormal lesions and radiologicalsuggestions for biopsy. However, DCE-MRI involves the administration of aGadolinium based (Gad) contrast agent, which is associated with a risk oftoxicity in the body. Previous deep learning approaches that synthesize DCE-MRimages employ unimodal non-contrast or low-dose contrast MRI images lackingfocus on the local perfusion information within the anatomy of interest. Wepropose AAD-DCE, a generative adversarial network (GAN) with an aggregatedattention discriminator module consisting of global and local discriminators.The discriminators provide a spatial embedded attention map to drive thegenerator to synthesize early and late response DCE-MRI images. Our methodemploys multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient(ADC), and T1 pre-contrast for image synthesis. Extensive comparative andablation studies on the ProstateX dataset show that our model (i) is agnosticto various generator benchmarks and (ii) outperforms other DCE-MRI synthesisapproaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAEfor early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for lateresponse, and (ii) emphasize the importance of attention ensembling. Our codeis available at https://github.com/bhartidivya/AAD-DCE.</description><author>Divya Bharti, Sriprabha Ramanarayanan, Sadhana S, Kishore Kumar M, Keerthi Ram, Harsh Agarwal, Ramesh Venkatesan, Mohanasankar Sivaprakasam</author><pubDate>Tue, 04 Feb 2025 18:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02555v1</guid></item><item><title>Hierarchical Sparse Bayesian Multitask Model with Scalable Inference for Microbiome Analysis</title><link>http://arxiv.org/abs/2502.02552v1</link><description>This paper proposes a hierarchical Bayesian multitask learning model that isapplicable to the general multi-task binary classification learning problemwhere the model assumes a shared sparsity structure across different tasks. Wederive a computationally efficient inference algorithm based on variationalinference to approximate the posterior distribution. We demonstrate thepotential of the new approach on various synthetic datasets and for predictinghuman health status based on microbiome profile. Our analysis incorporates datapooled from multiple microbiome studies, along with a comprehensive comparisonwith other benchmark methods. Results in synthetic datasets show that theproposed approach has superior support recovery property when the underlyingregression coefficients share a common sparsity structure across differenttasks. Our experiments on microbiome classification demonstrate the utility ofthe method in extracting informative taxa while providing well-calibratedpredictions with uncertainty quantification and achieving competitiveperformance in terms of prediction metrics. Notably, despite the heterogeneityof the pooled datasets (e.g., different experimental objectives, laboratorysetups, sequencing equipment, patient demographics), our method delivers robustresults.</description><author>Haonan Zhu, Andre R. Goncalves, Camilo Valdes, Hiranmayi Ranganathan, Boya Zhang, Jose Manuel Martí, Car Reen Kok, Monica K. Borucki, Nisha J. Mulakken, James B. Thissen, Crystal Jaing, Alfred Hero, Nicholas A. Be</author><pubDate>Tue, 04 Feb 2025 18:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02552v1</guid></item><item><title>Anytime Incremental $ρ$POMDP Planning in Continuous Spaces</title><link>http://arxiv.org/abs/2502.02549v1</link><description>Partially Observable Markov Decision Processes (POMDPs) provide a robustframework for decision-making under uncertainty in applications such asautonomous driving and robotic exploration. Their extension, $\rho$POMDPs,introduces belief-dependent rewards, enabling explicit reasoning aboutuncertainty. Existing online $\rho$POMDP solvers for continuous spaces rely onfixed belief representations, limiting adaptability and refinement - criticalfor tasks such as information-gathering. We present $\rho$POMCPOW, an anytimesolver that dynamically refines belief representations, with formal guaranteesof improvement over time. To mitigate the high computational cost of updatingbelief-dependent rewards, we propose a novel incremental computation approach.We demonstrate its effectiveness for common entropy estimators, reducingcomputational cost by orders of magnitude. Experimental results show that$\rho$POMCPOW outperforms state-of-the-art solvers in both efficiency andsolution quality.</description><author>Ron Benchetrit, Idan Lev-Yehudi, Andrey Zhitnikov, Vadim Indelman</author><pubDate>Tue, 04 Feb 2025 18:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02549v1</guid></item><item><title>Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation</title><link>http://arxiv.org/abs/2502.02548v1</link><description>We tackle open-vocabulary 3D scene understanding by introducing a novel datageneration pipeline and training framework. Our method addresses three criticalrequirements for effective training: precise 3D region segmentation,comprehensive textual descriptions, and sufficient dataset scale. By leveragingstate-of-the-art open-vocabulary image segmentation models and region-awareVision-Language Models, we develop an automatic pipeline that generateshigh-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scenedatasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with5.6M mask-text pairs, significantly larger than existing datasets. Buildingupon this data, we propose Mosaic3D, a foundation model combining a 3D encodertrained with contrastive learning and a lightweight mask decoder foropen-vocabulary 3D semantic and instance segmentation. Our approach achievesstate-of-the-art results on open-vocabulary 3D semantic and instancesegmentation tasks including ScanNet200, Matterport3D, and ScanNet++, withablation studies validating the effectiveness of our large-scale training data.</description><author>Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Chris Choy</author><pubDate>Tue, 04 Feb 2025 18:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02548v1</guid></item><item><title>UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models</title><link>http://arxiv.org/abs/2404.01101v2</link><description>Diffusion models are vulnerable to backdoor attacks, where maliciousattackers inject backdoors by poisoning certain training samples during thetraining stage. This poses a significant threat to real-world applications inthe Model-as-a-Service (MaaS) scenario, where users query diffusion modelsthrough APIs or directly download them from the internet. To mitigate thethreat of backdoor attacks under MaaS, black-box input-level backdoor detectionhas drawn recent interest, where defenders aim to build a firewall that filtersout backdoor samples in the inference stage, with access only to input queriesand the generated results from diffusion models. Despite some preliminaryexplorations on the traditional classification tasks, these methods cannot bedirectly applied to the generative tasks due to two major challenges: (1) morediverse failures and (2) a multi-modality attack surface. In this paper, wepropose a black-box input-level backdoor detection framework on diffusionmodels, called UFID. Our defense is motivated by an insightful causal analysis:Backdoor attacks serve as the confounder, introducing a spurious path frominput to target images, which remains consistent even when we perturb the inputsamples with Gaussian noise. We further validate the intuition with theoreticalanalysis. Extensive experiments across different datasets on both conditionaland unconditional diffusion models show that our method achieves superbperformance on detection effectiveness and run-time efficiency.</description><author>Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti</author><pubDate>Tue, 04 Feb 2025 18:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01101v2</guid></item><item><title>Optimal Spectral Transitions in High-Dimensional Multi-Index Models</title><link>http://arxiv.org/abs/2502.02545v1</link><description>We consider the problem of how many samples from a Gaussian multi-index modelare required to weakly reconstruct the relevant index subspace. Despite itsincreasing popularity as a testbed for investigating the computationalcomplexity of neural networks, results beyond the single-index setting remainelusive. In this work, we introduce spectral algorithms based on thelinearization of a message passing scheme tailored to this problem. Our maincontribution is to show that the proposed methods achieve the optimalreconstruction threshold. Leveraging a high-dimensional characterization of thealgorithms, we show that above the critical threshold the leading eigenvectorcorrelates with the relevant index subspace, a phenomenon reminiscent of theBaik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrixtheory. Supported by numerical experiments and a rigorous theoreticalframework, our work bridges critical gaps in the computational limits of weaklearnability in multi-index model.</description><author>Leonardo Defilippis, Yatin Dandi, Pierre Mergny, Florent Krzakala, Bruno Loureiro</author><pubDate>Tue, 04 Feb 2025 18:15:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02545v1</guid></item><item><title>Addressing Label Shift in Distributed Learning via Entropy Regularization</title><link>http://arxiv.org/abs/2502.02544v1</link><description>We address the challenge of minimizing true risk in multi-node distributedlearning. These systems are frequently exposed to both inter-node andintra-node label shifts, which present a critical obstacle to effectivelyoptimizing model performance while ensuring that data remains confined to eachnode. To tackle this, we propose the Versatile Robust Label Shift (VRLS)method, which enhances the maximum likelihood estimation of the test-to-trainlabel density ratio. VRLS incorporates Shannon entropy-based regularization andadjusts the density ratio during training to better handle label shifts at thetest time. In multi-node learning environments, VRLS further extends itscapabilities by learning and adapting density ratios across nodes, effectivelymitigating label shifts and improving overall model performance. Experimentsconducted on MNIST, Fashion MNIST, and CIFAR-10 demonstrate the effectivenessof VRLS, outperforming baselines by up to 20% in imbalanced settings. Theseresults highlight the significant improvements VRLS offers in addressing labelshifts. Our theoretical analysis further supports this by establishinghigh-probability bounds on estimation errors.</description><author>Zhiyuan Wu, Changkyu Choi, Xiangcheng Cao, Volkan Cevher, Ali Ramezani-Kebrya</author><pubDate>Tue, 04 Feb 2025 18:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02544v1</guid></item><item><title>OVERTHINKING: Slowdown Attacks on Reasoning LLMs</title><link>http://arxiv.org/abs/2502.02542v1</link><description>We increase overhead for applications that rely on reasoning LLMs-we forcemodels to spend an amplified number of reasoning tokens, i.e., "overthink", torespond to the user query while providing contextually correct answers. Theadversary performs an OVERTHINK attack by injecting decoy reasoning problemsinto the public content that is used by the reasoning LLM (e.g., for RAGapplications) during inference time. Due to the nature of our decoy problems(e.g., a Markov Decision Process), modified texts do not violate safetyguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)and open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuADdatasets. Our results show up to 46x slowdown and high transferability of theattack across models. To protect applications, we discuss and implementdefenses leveraging LLM-based and system design approaches. Finally, we discusssocietal, financial, and energy impacts of OVERTHINK attack which could amplifythe costs for third party applications operating reasoning models.</description><author>Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian</author><pubDate>Tue, 04 Feb 2025 18:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02542v1</guid></item><item><title>CUQDS: Conformal Uncertainty Quantification under Distribution Shift for Trajectory Prediction</title><link>http://arxiv.org/abs/2406.12100v4</link><description>Trajectory prediction models that can infer both finite future trajectoriesand their associated uncertainties of the target vehicles in an online setting(e.g., real-world application scenarios) is crucial for ensuring the safe androbust navigation and path planning of autonomous vehicle motion. However, themajority of existing trajectory prediction models have neither consideredreducing the uncertainty as one objective during the training stage norprovided reliable uncertainty quantification during inference stage underpotential distribution shift. Therefore, in this paper, we propose theConformal Uncertainty Quantification under Distribution Shift framework, CUQDS,to quantify the uncertainty of the predicted trajectories of existingtrajectory prediction models under potential data distribution shift, whileconsidering improving the prediction accuracy of the models and reducing theestimated uncertainty during the training stage. Specifically, CUQDS includes1) a learning-based Gaussian process regression module that models the outputdistribution of the base model (any existing trajectory prediction or timeseries forecasting neural networks) and reduces the estimated uncertainty byadditional loss term, and 2) a statistical-based Conformal P control module tocalibrate the estimated uncertainty from the Gaussian process regression modulein an online setting under potential distribution shift between training andtesting data.</description><author>Huiqun Huang, Sihong He, Fei Miao</author><pubDate>Tue, 04 Feb 2025 18:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12100v4</guid></item><item><title>Flow Q-Learning</title><link>http://arxiv.org/abs/2502.02538v1</link><description>We present flow Q-learning (FQL), a simple and performant offlinereinforcement learning (RL) method that leverages an expressive flow-matchingpolicy to model arbitrarily complex action distributions in data. Training aflow policy with RL is a tricky problem, due to the iterative nature of theaction generation process. We address this challenge by training an expressiveone-step policy with RL, rather than directly guiding an iterative flow policyto maximize values. This way, we can completely avoid unstable recursivebackpropagation, eliminate costly iterative action generation at test time, yetstill mostly maintain expressivity. We experimentally show that FQL leads tostrong performance across 73 challenging state- and pixel-based OGBench andD4RL tasks in offline RL and offline-to-online RL. Project page:https://seohong.me/projects/fql/</description><author>Seohong Park, Qiyang Li, Sergey Levine</author><pubDate>Tue, 04 Feb 2025 18:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02538v1</guid></item><item><title>Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks</title><link>http://arxiv.org/abs/2502.02537v1</link><description>Collaborative Object Detection (COD) and collaborative perception canintegrate data or features from various entities, and improve object detectionaccuracy compared with individual perception. However, adversarial attacks posea potential threat to the deep learning COD models, and introduce high outputuncertainty. With unknown attack models, it becomes even more challenging toimprove COD resiliency and quantify the output uncertainty for highly dynamicperception scenes such as autonomous vehicles. In this study, we propose theTrusted Uncertainty Quantification in Collaborative Perception framework(TUQCP). TUQCP leverages both adversarial training and uncertaintyquantification techniques to enhance the adversarial robustness of existing CODmodels. More specifically, TUQCP first adds perturbations to the sharedinformation of randomly selected agents during object detection collaborationby adversarial training. TUQCP then alleviates the impacts of adversarialattacks by providing output uncertainty estimation through learning-basedmodule and uncertainty calibration through conformal prediction. Our frameworkworks for early and intermediate collaboration COD models and single-agentobject detection models. We evaluate TUQCP on V2X-Sim, a comprehensivecollaborative perception dataset for autonomous driving, and demonstrate a80.41% improvement in object detection accuracy compared to the baselines underthe same adversarial attacks. TUQCP demonstrates the importance of uncertaintyquantification to COD under adversarial attacks.</description><author>Huiqun Huang, Cong Chen, Jean-Philippe Monteuuis, Jonathan Petit, Fei Miao</author><pubDate>Tue, 04 Feb 2025 18:03:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02537v1</guid></item><item><title>Adaptive Self-improvement LLM Agentic System for ML Library Development</title><link>http://arxiv.org/abs/2502.02534v1</link><description>ML libraries, often written in architecture-specific programming languages(ASPLs) that target domain-specific architectures, are key to efficient MLsystems. However, writing these high-performance ML libraries is challengingbecause it requires expert knowledge of ML algorithms and the ASPL. Largelanguage models (LLMs), on the other hand, have shown general codingcapabilities. However, challenges remain when using LLMs for generating MLlibraries using ASPLs because 1) this task is complicated even for experiencedhuman programmers and 2) there are limited code examples because of theesoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoningwith limited data in order to complete this task. To address these challenges,we introduce an adaptive self-improvement agentic system. In order to evaluatethe effectiveness of our system, we construct a benchmark of a typical MLlibrary and generate ASPL code with both open and closed-source LLMs on thisbenchmark. Our results show improvements of up to $3.9\times$ over a baselinesingle LLM.</description><author>Genghan Zhang, Weixin Liang, Olivia Hsu, Kunle Olukotun</author><pubDate>Tue, 04 Feb 2025 17:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02534v1</guid></item><item><title>Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies</title><link>http://arxiv.org/abs/2502.02533v1</link><description>Large language models, employed as multiple agents that interact andcollaborate with each other, have excelled at solving complex tasks. The agentsare programmed with prompts that declare their functionality, along with thetopologies that orchestrate interactions across agents. Designing prompts andtopologies for multi-agent systems (MAS) is inherently complex. To automate theentire design process, we first conduct an in-depth analysis of the designspace aiming to understand the factors behind building effective MAS. We revealthat prompts together with topologies play critical roles in enabling moreeffective MAS design. Based on the insights, we propose Multi-Agent SystemSearch (MASS), a MAS optimization framework that efficiently exploits thecomplex MAS design space by interleaving its optimization stages, from local toglobal, from prompts to topologies, over three stages: 1) block-level (local)prompt optimization; 2) workflow topology optimization; 3) workflow-level(global) prompt optimization, where each stage is conditioned on theiteratively optimized prompts/topologies from former stages. We show thatMASS-optimized multi-agent systems outperform a spectrum of existingalternatives by a substantial margin. Based on the MASS-found systems, wefinally propose design principles behind building effective multi-agentsystems.</description><author>Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan Vulić, Anna Korhonen, Sercan Ö. Arık</author><pubDate>Tue, 04 Feb 2025 17:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02533v1</guid></item><item><title>Trajectory Flow Matching with Applications to Clinical Time Series Modeling</title><link>http://arxiv.org/abs/2410.21154v2</link><description>Modeling stochastic and irregularly sampled time series is a challengingproblem found in a wide range of applications, especially in medicine. Neuralstochastic differential equations (Neural SDEs) are an attractive modelingtechnique for this problem, which parameterize the drift and diffusion terms ofan SDE with neural networks. However, current algorithms for training NeuralSDEs require backpropagation through the SDE dynamics, greatly limiting theirscalability and stability. To address this, we propose Trajectory Flow Matching(TFM), which trains a Neural SDE in a simulation-free manner, bypassingbackpropagation through the dynamics. TFM leverages the flow matching techniquefrom generative modeling to model time series. In this work we first establishnecessary conditions for TFM to learn time series data. Next, we present areparameterization trick which improves training stability. Finally, we adaptTFM to the clinical time series setting, demonstrating improved performance onthree clinical time series datasets both in terms of absolute performance anduncertainty prediction.</description><author>Xi Zhang, Yuan Pu, Yuki Kawamura, Andrew Loza, Yoshua Bengio, Dennis L. Shung, Alexander Tong</author><pubDate>Tue, 04 Feb 2025 17:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21154v2</guid></item><item><title>Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer</title><link>http://arxiv.org/abs/2502.02531v1</link><description>We theoretically characterize gradient descent dynamics in deep linearnetworks trained at large width from random initialization and on largequantities of random data. Our theory captures the ``wider is better" effect ofmean-field/maximum-update parameterized networks as well as hyperparametertransfer effects, which can be contrasted with the neural-tangentparameterization where optimal learning rates shift with model width. Weprovide asymptotic descriptions of both non-residual and residual neuralnetworks, the latter of which enables an infinite depth limit when branches arescaled as $1/\sqrt{\text{depth}}$. We also compare training with one-passstochastic gradient descent to the dynamics when training data are repeated ateach iteration. Lastly, we show that this model recovers the accelerated powerlaw training dynamics for power law structured data in the rich regime observedin recent works.</description><author>Blake Bordelon, Cengiz Pehlevan</author><pubDate>Tue, 04 Feb 2025 17:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02531v1</guid></item><item><title>A weak convergence approach to large deviations for stochastic approximations</title><link>http://arxiv.org/abs/2502.02529v1</link><description>The theory of stochastic approximations form the theoretical foundation forstudying convergence properties of many popular recursive learning algorithmsin statistics, machine learning and statistical physics. Large deviations forstochastic approximations provide asymptotic estimates of the probability thatthe learning algorithm deviates from its expected path, given by a limit ODE,and the large deviation rate function gives insights to the most likely waythat such deviations occur. In this paper we prove a large deviation principle for general stochasticapproximations with state-dependent Markovian noise and decreasing step size.Using the weak convergence approach to large deviations, we generalize previousresults for stochastic approximations and identify the appropriate scalingsequence for the large deviation principle. We also give a new representationfor the rate function, in which the rate function is expressed as an actionfunctional involving the family of Markov transition kernels. Examples oflearning algorithms that are covered by the large deviation principle includestochastic gradient descent, persistent contrastive divergence and theWang-Landau algorithm.</description><author>Henrik Hult, Adam Lindhe, Pierre Nyquist, Guo-Jhen Wu</author><pubDate>Tue, 04 Feb 2025 17:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02529v1</guid></item><item><title>Why human-AI relationships need socioaffective alignment</title><link>http://arxiv.org/abs/2502.02528v1</link><description>Humans strive to design safe AI systems that align with our goals and remainunder our control. However, as AI capabilities advance, we face a newchallenge: the emergence of deeper, more persistent relationships betweenhumans and AI systems. We explore how increasingly capable AI agents maygenerate the perception of deeper relationships with users, especially as AIbecomes more personalised and agentic. This shift, from transactionalinteraction to ongoing sustained social engagement with AI, necessitates a newfocus on socioaffective alignment-how an AI system behaves within the socialand psychological ecosystem co-created with its user, where preferences andperceptions evolve through mutual influence. Addressing these dynamics involvesresolving key intrapersonal dilemmas, including balancing immediate versuslong-term well-being, protecting autonomy, and managing AI companionshipalongside the desire to preserve human social bonds. By framing thesechallenges through a notion of basic psychological needs, we seek AI systemsthat support, rather than exploit, our fundamental nature as social andemotional beings.</description><author>Hannah Rose Kirk, Iason Gabriel, Chris Summerfield, Bertie Vidgen, Scott A. Hale</author><pubDate>Tue, 04 Feb 2025 17:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02528v1</guid></item><item><title>TabPFN Unleashed: A Scalable and Effective Solution to Tabular Classification Problems</title><link>http://arxiv.org/abs/2502.02527v1</link><description>TabPFN has emerged as a promising in-context learning model for tabular data,capable of directly predicting the labels of test samples given labeledtraining examples. It has demonstrated competitive performance, particularly onsmall-scale classification tasks. However, despite its effectiveness, TabPFNstill requires further refinement in several areas, including handlinghigh-dimensional features, aligning with downstream datasets, and scaling tolarger datasets. In this paper, we revisit existing variants of TabPFN andobserve that most approaches focus either on reducing bias or variance, oftenneglecting the need to address the other side, while also increasing inferenceoverhead. To fill this gap, we propose Beta (Bagging and Encoder-basedFine-tuning for TabPFN Adaptation), a novel and effective method designed tominimize both bias and variance. To reduce bias, we introduce a lightweightencoder to better align downstream tasks with the pre-trained TabPFN. Byincreasing the number of encoders in a lightweight manner, Beta mitigatevariance, thereby further improving the model's performance. Additionally,bootstrapped sampling is employed to further reduce the impact of dataperturbations on the model, all while maintaining computational efficiencyduring inference. Our approach enhances TabPFN's ability to handlehigh-dimensional data and scale to larger datasets. Experimental results onover 200 benchmark classification datasets demonstrate that Beta eitheroutperforms or matches state-of-the-art methods.</description><author>Si-Yang Liu, Han-Jia Ye</author><pubDate>Tue, 04 Feb 2025 17:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02527v1</guid></item><item><title>Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation</title><link>http://arxiv.org/abs/2502.02525v1</link><description>Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucialfor enabling augmented reality and robotic manipulation. Category-level methodshave received extensive research attention due to their potential forgeneralization to intra-class unknown objects. However, these methods requiremanual collection and labeling of large-scale real-world training data. Toaddress this problem, we introduce a diffusion-based paradigm fordomain-generalized category-level 9-DoF object pose estimation. Our motivationis to leverage the latent generalization ability of the diffusion model toaddress the domain generalization challenge in object pose estimation. Thisentails training the model exclusively on rendered synthetic data to achievegeneralization to real-world scenes. We propose an effective diffusion model toredefine 9-DoF object pose estimation from a generative perspective. Our modeldoes not require any 3D shape priors during training or inference. By employingthe Denoising Diffusion Implicit Model, we demonstrate that the reversediffusion process can be executed in as few as 3 steps, achieving nearreal-time performance. Finally, we design a robotic grasping system comprisingboth hardware and software components. Through comprehensive experiments on twobenchmark datasets and the real-world robotic system, we show that our methodachieves state-of-the-art domain generalization performance. Our code will bemade public at https://github.com/CNJianLiu/Diff9D.</description><author>Jian Liu, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, Ajmal Mian</author><pubDate>Tue, 04 Feb 2025 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02525v1</guid></item><item><title>Brief analysis of DeepSeek R1 and it's implications for Generative AI</title><link>http://arxiv.org/abs/2502.02523v1</link><description>In late January 2025, DeepSeek released their new reasoning model (DeepSeekR1); which was developed at a fraction of the cost yet remains competitive withOpenAI's models, despite the US's GPU export ban. This report discusses themodel, and what its release means for the field of Generative AI more widely.We briefly discuss other models released from China in recent weeks, theirsimilarities; innovative use of Mixture of Experts (MoE), ReinforcementLearning (RL) and clever engineering appear to be key factors in thecapabilities of these models. This think piece has been written to a tighttime-scale, providing broad coverage of the topic, and serves as introductorymaterial for those looking to understand the model's technical advancements, aswell as it's place in the ecosystem. Several further areas of research areidentified.</description><author>Sarah Mercer, Samuel Spillard, Daniel P. Martin</author><pubDate>Tue, 04 Feb 2025 17:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02523v1</guid></item><item><title>Adaptive Exploration for Multi-Reward Multi-Policy Evaluation</title><link>http://arxiv.org/abs/2502.02516v1</link><description>We study the policy evaluation problem in an online multi-reward multi-policydiscounted setting, where multiple reward functions must be evaluatedsimultaneously for different policies. We adopt an $(\epsilon,\delta)$-PACperspective to achieve $\epsilon$-accurate estimates with high confidenceacross finite or convex sets of rewards, a setting that has not beeninvestigated in the literature. Building on prior work on Multi-Reward BestPolicy Identification, we adapt the MR-NaS exploration scheme to jointlyminimize sample complexity for evaluating different policies across differentreward sets. Our approach leverages an instance-specific lower bound revealinghow the sample complexity scales with a measure of value deviation, guiding thedesign of an efficient exploration policy. Although computing this boundentails a hard non-convex optimization, we propose an efficient convexapproximation that holds for both finite and convex reward sets. Experiments intabular domains demonstrate the effectiveness of this adaptive explorationscheme.</description><author>Alessio Russo, Aldo Pacchiano</author><pubDate>Tue, 04 Feb 2025 17:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02516v1</guid></item><item><title>Privacy Attacks on Image AutoRegressive Models</title><link>http://arxiv.org/abs/2502.02514v1</link><description>Image autoregressive (IAR) models have surpassed diffusion models (DMs) inboth image quality (FID: 1.48 vs. 1.58) and generation speed. However, theirprivacy risks remain largely unexplored. To address this, we conduct acomprehensive privacy analysis comparing IARs to DMs. We develop a novelmembership inference attack (MIA) that achieves a significantly higher successrate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% forDMs). Using this MIA, we perform dataset inference (DI) and find that IARsrequire as few as six samples to detect dataset membership, compared to 200 forDMs, indicating higher information leakage. Additionally, we extract hundredsof training images from an IAR (e.g., 698 from VAR-d30). Our findings highlighta fundamental privacy-utility trade-off: while IARs excel in generation qualityand speed, they are significantly more vulnerable to privacy attacks. Thissuggests that incorporating techniques from DMs, such as per-token probabilitymodeling using diffusion, could help mitigate IARs' privacy risks. Our code isavailable at https://github.com/sprintml/privacy_attacks_against_iars.</description><author>Antoni Kowalczuk, Jan Dubiński, Franziska Boenisch, Adam Dziedzic</author><pubDate>Tue, 04 Feb 2025 17:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02514v1</guid></item><item><title>Generative Modeling on Lie Groups via Euclidean Generalized Score Matching</title><link>http://arxiv.org/abs/2502.02513v1</link><description>We extend Euclidean score-based diffusion processes to generative modeling onLie groups. Through the formalism of Generalized Score Matching, our approachyields a Langevin dynamics which decomposes as a direct sum of Lie algebrarepresentations, enabling generative processes on Lie groups while operating inEuclidean space. Unlike equivariant models, which restrict the space oflearnable functions by quotienting out group orbits, our method can model anytarget distribution on any (non-Abelian) Lie group. Standard score matchingemerges as a special case of our framework when the Lie group is thetranslation group. We prove that our generalized generative processes arise assolutions to a new class of paired stochastic differential equations (SDEs),introduced here for the first time. We validate our approach throughexperiments on diverse data types, demonstrating its effectiveness inreal-world applications such as SO(3)-guided molecular conformer generation andmodeling ligand-specific global SE(3) transformations for molecular docking,showing improvement in comparison to Riemannian diffusion on the group itself.We show that an appropriate choice of Lie group enhances learning efficiency byreducing the effective dimensionality of the trajectory space and enables themodeling of transitions between complex data distributions. Additionally, wedemonstrate the universality of our approach by deriving how it extends to flowmatching.</description><author>Marco Bertolini, Tuan Le, Djork-Arné Clevert</author><pubDate>Tue, 04 Feb 2025 17:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02513v1</guid></item><item><title>Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding</title><link>http://arxiv.org/abs/2501.17053v2</link><description>In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding(WSTVG). It is a multimodal task aimed at localizing specific subjectsspatio-temporally based on textual queries without bounding box supervision.Motivated by recent advancements in multi-modal foundation models for groundingtasks, we first explore the potential of state-of-the-art object detectionmodels for WSTVG. Despite their robust zero-shot capabilities, our adaptationreveals significant limitations, including inconsistent temporal predictions,inadequate understanding of complex queries, and challenges in adapting todifficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), anovel approach which is designed to overcome these limitations. CoSPaLintegrates three core components: (1) Tubelet Phrase Grounding (TPG), whichintroduces spatio-temporal prediction by linking textual queries to tubelets;(2) Contextual Referral Grounding (CRG), which improves comprehension ofcomplex queries by extracting contextual information to refine objectidentification over time; and (3) Self-Paced Scene Understanding (SPS), atraining paradigm that progressively increases task difficulty, enabling themodel to adapt to complex scenarios by transitioning from coarse tofine-grained understanding.</description><author>Akash Kumar, Zsolt Kira, Yogesh Singh Rawat</author><pubDate>Tue, 04 Feb 2025 17:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.17053v2</guid></item><item><title>Boundary Constraint-free Biomechanical Model-Based Surface Matching for Intraoperative Liver Deformation Correction</title><link>http://arxiv.org/abs/2403.09964v3</link><description>In image-guided liver surgery, 3D-3D non-rigid registration methods play acrucial role in estimating the mapping between the preoperative model and theintraoperative surface represented as point clouds, addressing the challenge oftissue deformation. Typically, these methods incorporate a biomechanical model,represented as a finite element model (FEM), used to regularize a surfacematching term. This paper introduces a novel 3D-3D non-rigid registrationmethod. In contrast to the preceding techniques, our method uniquelyincorporates the FEM within the surface matching term itself, ensuring that theestimated deformation maintains geometric consistency throughout theregistration process. Additionally, we eliminate the need to determinezero-boundary conditions and applied force locations in the FEM. We achievethis by integrating soft springs into the stiffness matrix and allowing forcesto be distributed across the entire liver surface. To further improverobustness, we introduce a regularization technique focused on the gradient ofthe force magnitudes. This regularization imposes spatial smoothness and helpsprevent the overfitting of irregular noise in intraoperative data. Optimizationis achieved through an accelerated proximal gradient algorithm, furtherenhanced by our proposed method for determining the optimal step size. Ourmethod is evaluated and compared to both a learning-based method and atraditional method that features FEM regularization using data collected on ourcustom-developed phantom, as well as two publicly available datasets. Ourmethod consistently outperforms or is comparable to the baseline techniques.Our code and datasets will be available athttps://github.com/zixinyang9109/BCF-FEM.</description><author>Zixin Yang, Richard Simon, Kelly Merrell, Cristian. A. Linte</author><pubDate>Tue, 04 Feb 2025 17:29:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09964v3</guid></item><item><title>Exploring Empty Spaces: Human-in-the-Loop Data Augmentation</title><link>http://arxiv.org/abs/2410.01088v2</link><description>Data augmentation is crucial to make machine learning models more robust andsafe. However, augmenting data can be challenging as it requires generatingdiverse data points to rigorously evaluate model behavior on edge cases andmitigate potential harms. Creating high-quality augmentations that cover these"unknown unknowns" is a time- and creativity-intensive task. In this work, weintroduce Amplio, an interactive tool to help practitioners navigate "unknownunknowns" in unstructured text datasets and improve data diversity bysystematically identifying empty data spaces to explore. Amplio includes threehuman-in-the-loop data augmentation techniques: Augment With Concepts, Augmentby Interpolation, and Augment with Large Language Model. In a user study with18 professional red teamers, we demonstrate the utility of our augmentationmethods in helping generate high-quality, diverse, and relevant model safetyprompts. We find that Amplio enabled red teamers to augment data quickly andcreatively, highlighting the transformative potential of interactiveaugmentation workflows.</description><author>Catherine Yeh, Donghao Ren, Yannick Assogba, Dominik Moritz, Fred Hohman</author><pubDate>Tue, 04 Feb 2025 17:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01088v2</guid></item><item><title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</title><link>http://arxiv.org/abs/2502.02508v1</link><description>Large language models (LLMs) have demonstrated remarkable reasoningcapabilities across diverse domains. Recent studies have shown that increasingtest-time computation enhances LLMs' reasoning capabilities. This typicallyinvolves extensive sampling at inference time guided by an external LLMverifier, resulting in a two-player system. Despite external guidance, theeffectiveness of this system demonstrates the potential of a single LLM totackle complex tasks. Thus, we pose a new research problem: Can we internalizethe searching capabilities to fundamentally enhance the reasoning abilities ofa single LLM? This work explores an orthogonal direction focusing onpost-training LLMs for autoregressive searching (i.e., an extended reasoningprocess with self-reflection and self-exploration of new strategies). Toachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and atwo-stage training paradigm: 1) a small-scale format tuning stage tointernalize the COAT reasoning format and 2) a large-scale self-improvementstage leveraging reinforcement learning. Our approach results in Satori, a 7BLLM trained on open-source models and data. Extensive empirical evaluationsdemonstrate that Satori achieves state-of-the-art performance on mathematicalreasoning benchmarks while exhibits strong generalization to out-of-domaintasks. Code, data, and models will be fully open-sourced.</description><author>Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan</author><pubDate>Tue, 04 Feb 2025 17:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02508v1</guid></item><item><title>ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding</title><link>http://arxiv.org/abs/2409.03277v2</link><description>Automatic chart understanding is crucial for content comprehension anddocument parsing. Multimodal large language models (MLLMs) have demonstratedremarkable capabilities in chart understanding through domain-specificalignment and fine-tuning. However, the application of alignment trainingwithin the chart domain is still underexplored. To address this, we proposeChartMoE, which employs the mixture of expert (MoE) architecture to replace thetraditional linear projector to bridge the modality gap. Specifically, we trainmultiple linear connectors through distinct alignment tasks, which are utilizedas the foundational initialization parameters for different experts.Additionally, we introduce ChartMoE-Align, a dataset with over 900Kchart-table-JSON-code quadruples to conduct three alignment tasks(chart-table/JSON/code). Combined with the vanilla connector, we initializedifferent experts in four distinct ways and adopt high-quality knowledgelearning to further refine the MoE connector and LLM parameters. Extensiveexperiments demonstrate the effectiveness of the MoE connector and ourinitialization strategy, e.g., ChartMoE improves the accuracy of the previousstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.</description><author>Zhengzhuo Xu, Bowen Qu, Yiyan Qi, Sinan Du, Chengjin Xu, Chun Yuan, Jian Guo</author><pubDate>Tue, 04 Feb 2025 17:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03277v2</guid></item><item><title>Unified Spatial-Temporal Edge-Enhanced Graph Networks for Pedestrian Trajectory Prediction</title><link>http://arxiv.org/abs/2502.02504v1</link><description>Pedestrian trajectory prediction aims to forecast future movements based onhistorical paths. Spatial-temporal (ST) methods often separately model spatialinteractions among pedestrians and temporal dependencies of individuals. Theyoverlook the direct impacts of interactions among different pedestrians acrossvarious time steps (i.e., high-order cross-time interactions). This limitstheir ability to capture ST inter-dependencies and hinders predictionperformance. To address these limitations, we propose UniEdge with three majordesigns. Firstly, we introduce a unified ST graph data structure thatsimplifies high-order cross-time interactions into first-order relationships,enabling the learning of ST inter-dependencies in a single step. This avoidsthe information loss caused by multi-step aggregation. Secondly, traditionalGNNs focus on aggregating pedestrian node features, neglecting the propagationof implicit interaction patterns encoded in edge features. We propose theEdge-to-Edge-Node-to-Node Graph Convolution (E2E-N2N-GCN), a novel dual-graphnetwork that jointly models explicit N2N social interactions among pedestriansand implicit E2E influence propagation across these interaction patterns.Finally, to overcome the limited receptive fields and challenges in capturinglong-range dependencies of auto-regressive architectures, we introduce atransformer encoder-based predictor that enables global modeling of temporalcorrelation. UniEdge outperforms state-of-the-arts on multiple datasets,including ETH, UCY, and SDD.</description><author>Ruochen Li, Tanqiu Qiao, Stamos Katsigiannis, Zhanxing Zhu, Hubert P. H. Shum</author><pubDate>Tue, 04 Feb 2025 17:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02504v1</guid></item><item><title>Graph-based Document Structure Analysis</title><link>http://arxiv.org/abs/2502.02501v1</link><description>When reading a document, glancing at the spatial layout of a document is aninitial step to understand it roughly. Traditional document layout analysis(DLA) methods, however, offer only a superficial parsing of documents, focusingon basic instance detection and often failing to capture the nuanced spatialand logical relations between instances. These limitations hinder DLA-basedmodels from achieving a gradually deeper comprehension akin to human reading.In this work, we propose a novel graph-based Document Structure Analysis (gDSA)task. This task requires that model not only detects document elements but alsogenerates spatial and logical relations in form of a graph structure, allowingto understand documents in a holistic and intuitive manner. For this new task,we construct a relation graph-based document structure analysis dataset(GraphDoc) with 80K document images and 4.13M relation annotations, enablingtraining models to complete multiple tasks like reading order, hierarchicalstructures analysis, and complex inter-element relation inference. Furthermore,a document relation graph generator (DRGG) is proposed to address the gDSAtask, which achieves performance with 57.6% at mAP$_g$@0.5 for a strongbenchmark baseline on this novel task and dataset. We hope this graphicalrepresentation of document structure can mark an innovative advancement indocument structure analysis and understanding. The new dataset and code will bemade publicly available at https://yufanchen96.github.io/projects/GraphDoc.</description><author>Yufan Chen, Ruiping Liu, Junwei Zheng, Di Wen, Kunyu Peng, Jiaming Zhang, Rainer Stiefelhagen</author><pubDate>Tue, 04 Feb 2025 17:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02501v1</guid></item><item><title>The Skin Game: Revolutionizing Standards for AI Dermatology Model Comparison</title><link>http://arxiv.org/abs/2502.02500v1</link><description>Deep Learning approaches in dermatological image classification have shownpromising results, yet the field faces significant methodological challengesthat impede proper evaluation. This paper presents a dual contribution: first,a systematic analysis of current methodological practices in skin diseaseclassification research, revealing substantial inconsistencies in datapreparation, augmentation strategies, and performance reporting; second, acomprehensive training and evaluation framework demonstrated throughexperiments with the DINOv2-Large vision transformer across three benchmarkdatasets (HAM10000, DermNet, ISIC Atlas). The analysis identifies concerningpatterns, including pre-split data augmentation and validation-based reporting,potentially leading to overestimated metrics, while highlighting the lack ofunified methodology standards. The experimental results demonstrate DINOv2'sperformance in skin disease classification, achieving macro-averaged F1-scoresof 0.85 (HAM10000), 0.71 (DermNet), and 0.84 (ISIC Atlas). Attention mapanalysis reveals critical patterns in the model's decision-making, showingsophisticated feature recognition in typical presentations but significantvulnerabilities with atypical cases and composite images. Our findingshighlight the need for standardized evaluation protocols and carefulimplementation strategies in clinical settings. We propose comprehensivemethodological recommendations for model development, evaluation, and clinicaldeployment, emphasizing rigorous data preparation, systematic error analysis,and specialized protocols for different image types. To promotereproducibility, we provide our implementation code through GitHub. This workestablishes a foundation for rigorous evaluation standards in dermatologicalimage classification and provides insights for responsible AI implementation inclinical dermatology.</description><author>Łukasz Miętkiewicz, Leon Ciechanowski, Dariusz Jemielniak</author><pubDate>Tue, 04 Feb 2025 17:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02500v1</guid></item><item><title>Learning to generate physical ocean states: Towards hybrid climate modeling</title><link>http://arxiv.org/abs/2502.02499v1</link><description>Ocean General Circulation Models require extensive computational resources toreach equilibrium states, while deep learning emulators, despite offering fastpredictions, lack the physical interpretability and long-term stabilitynecessary for climate scientists to understand climate sensitivity (togreenhouse gas emissions) and mechanisms of abrupt % variability such astipping points. We propose to take the best from both worlds by leveraging deepgenerative models to produce physically consistent oceanic states that canserve as initial conditions for climate projections. We assess the viability ofthis hybrid approach through both physical metrics and numerical experiments,and highlight the benefits of enforcing physical constraints during generation.Although we train here on ocean variables from idealized numerical simulations,we claim that this hybrid approach, combining the computational efficiency ofdeep learning with the physical accuracy of numerical models, can effectivelyreduce the computational burden of running climate models to equilibrium, andreduce uncertainties in climate projections by minimizing drifts in baselinesimulations.</description><author>Etienne Meunier, David Kamm, Guillaume Gachon, Redouane Lguensat, Julie Deshayes</author><pubDate>Tue, 04 Feb 2025 17:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02499v1</guid></item><item><title>Decentralized Federated Learning with Model Caching on Mobile Agents</title><link>http://arxiv.org/abs/2408.14001v2</link><description>Federated Learning (FL) trains a shared model using data and computationpower on distributed agents coordinated by a central server. Decentralized FL(DFL) utilizes local model exchange and aggregation between agents to reducethe communication and computation overheads on the central server. However,when agents are mobile, the communication opportunity between agents can besporadic, largely hindering the convergence and accuracy of DFL. In this paper,we propose Cached Decentralized Federated Learning (Cached-DFL) to investigatedelay-tolerant model spreading and aggregation enabled by model caching onmobile agents. Each agent stores not only its own model, but also models ofagents encountered in the recent past. When two agents meet, they exchangetheir own models as well as the cached models. Local model aggregation utilizesall models stored in the cache. We theoretically analyze the convergence ofCached-DFL, explicitly taking into account the model staleness introduced bycaching. We design and compare different model caching algorithms for differentDFL and mobility scenarios. We conduct detailed case studies in a vehicularnetwork to systematically investigate the interplay between agent mobility,cache staleness, and model convergence. In our experiments, Cached-DFLconverges quickly, and significantly outperforms DFL without caching.</description><author>Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu</author><pubDate>Tue, 04 Feb 2025 17:14:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14001v2</guid></item><item><title>Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries</title><link>http://arxiv.org/abs/2502.02496v1</link><description>Sparse regularization techniques are well-established in machine learning,yet their application in neural networks remains challenging due to thenon-differentiability of penalties like the $L_1$ norm, which is incompatiblewith stochastic gradient descent. A promising alternative is shallow weightfactorization, where weights are decomposed into two factors, allowing forsmooth optimization of $L_1$-penalized neural networks by adding differentiable$L_2$ regularization to the factors. In this work, we introduce deep weightfactorization, extending previous shallow approaches to more than two factors.We theoretically establish equivalence of our deep factorization withnon-convex sparse regularization and analyze its impact on training dynamicsand optimization. Due to the limitations posed by standard training practices,we propose a tailored initialization scheme and identify important learningrate requirements necessary for training factorized networks. We demonstratethe effectiveness of our deep weight factorization through experiments onvarious architectures and datasets, consistently outperforming its shallowcounterpart and widely used pruning methods.</description><author>Chris Kolb, Tobias Weber, Bernd Bischl, David Rügamer</author><pubDate>Tue, 04 Feb 2025 17:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02496v1</guid></item><item><title>The Causal-Effect Score in Data Management</title><link>http://arxiv.org/abs/2502.02495v1</link><description>The Causal Effect (CE) is a numerical measure of causal influence ofvariables on observed results. Despite being widely used in many areas, onlypreliminary attempts have been made to use CE as an attribution score in datamanagement, to measure the causal strength of tuples for query answering indatabases. In this work, we introduce, generalize and investigate the so-calledCausal-Effect Score in the context of classical and probabilistic databases.</description><author>Felipe Azua, Leopoldo Bertossi</author><pubDate>Tue, 04 Feb 2025 17:12:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02495v1</guid></item><item><title>Analyzing Similarity Metrics for Data Selection for Language Model Pretraining</title><link>http://arxiv.org/abs/2502.02494v1</link><description>Similarity between training examples is used to curate pretraining datasetsfor language models by many methods -- for diversification and to selectexamples similar to high-quality data. However, similarity is typicallymeasured with off-the-shelf embedding models that are generic or trained fortasks such as retrieval. This paper introduces a framework to analyze thesuitability of embedding models specifically for data curation in the languagemodel pretraining setting. We quantify the correlation between similarity inthe embedding space to similarity in pretraining loss between differenttraining examples, and how diversifying in the embedding space affectspretraining quality. We analyze a variety of embedding models in our framework,with experiments using the Pile dataset for pretraining a 1.7B parameterdecoder-only language model. We find that the embedding models we consider areall useful for pretraining data curation. Moreover, a simple approach ofaveraging per-token embeddings proves to be surprisingly competitive with moresophisticated embedding models -- likely because the latter are not designedspecifically for pretraining data curation. Indeed, we believe our analysis andevaluation framework can serve as a foundation for the design of embeddingmodels that specifically reason about similarity in pretraining datasets.</description><author>Dylan Sam, Ayan Chakrabarti, Afshin Rostamizadeh, Srikumar Ramalingam, Gui Citovsky, Sanjiv Kumar</author><pubDate>Tue, 04 Feb 2025 17:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02494v1</guid></item><item><title>EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization</title><link>http://arxiv.org/abs/2502.02493v1</link><description>Speculative decoding is an effective and lossless method for Large LanguageModel (LLM) inference acceleration. It employs a smaller model to generate adraft token sequence, which is then verified by the original base model. Inmulti-GPU systems, inference latency can be further reduced through tensorparallelism (TP), while the optimal TP size of the draft model is typicallysmaller than that of the base model, leading to GPU idling during the draftingstage. To solve this problem, we propose EasySpec, a layer-parallel speculationstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaksthe sequential execution order of layers in the drafting model, enablingmulti-layer parallelization across devices, albeit with some inducedapproximation errors. After each drafting-and-verification iteration, the draftmodel's key-value (KV) cache is calibrated in a single forward pass, preventinglong-term error accumulation at minimal additional latency. We evaluatedEasySpec on several mainstream open-source LLMs, using smaller versions ofmodels from the same series as drafters. The results demonstrate that EasySpeccan achieve a peak speedup of 4.17x compared to vanilla decoding, whilepreserving the original distribution of the base LLMs. Specifically, thedrafting stage can be accelerated by up to 1.62x with a maximum accuracy dropof only 7%, requiring no training or fine-tuning on the draft models.</description><author>Yize Wu, Ke Gao, Yanjun Wu</author><pubDate>Tue, 04 Feb 2025 17:09:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02493v1</guid></item><item><title>The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs</title><link>http://arxiv.org/abs/2501.18626v3</link><description>We present a novel class of jailbreak adversarial attacks on LLMs, termedTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks(e.g., cipher decoding, riddles, code execution) into the model's prompt toindirectly generate prohibited inputs. To systematically assess theeffectiveness of these attacks, we introduce the PHRYGE benchmark. Wedemonstrate that our techniques successfully circumvent safeguards in sixstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findingshighlight critical weaknesses in current LLM safety alignments and underscorethe urgent need for more sophisticated defence strategies. Warning: this paper contains examples of unethical inquiries used solely forresearch purposes.</description><author>Sergey Berezin, Reza Farahbakhsh, Noel Crespi</author><pubDate>Tue, 04 Feb 2025 17:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18626v3</guid></item><item><title>VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</title><link>http://arxiv.org/abs/2502.02492v1</link><description>Despite tremendous recent progress, generative video models still struggle tocapture real-world motion, dynamics, and physics. We show that this limitationarises from the conventional pixel reconstruction objective, which biasesmodels toward appearance fidelity at the expense of motion coherence. Toaddress this, we introduce VideoJAM, a novel framework that instills aneffective motion prior to video generators, by encouraging the model to learn ajoint appearance-motion representation. VideoJAM is composed of twocomplementary units. During training, we extend the objective to predict boththe generated pixels and their corresponding motion from a single learnedrepresentation. During inference, we introduce Inner-Guidance, a mechanism thatsteers the generation toward coherent motion by leveraging the model's ownevolving motion prediction as a dynamic guidance signal. Notably, our frameworkcan be applied to any video model with minimal adaptations, requiring nomodifications to the training data or scaling of the model. VideoJAM achievesstate-of-the-art performance in motion coherence, surpassing highly competitiveproprietary models while also enhancing the perceived visual quality of thegenerations. These findings emphasize that appearance and motion can becomplementary and, when effectively integrated, enhance both the visual qualityand the coherence of video generation. Project website:https://hila-chefer.github.io/videojam-paper.github.io/</description><author>Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, Shelly Sheynin</author><pubDate>Tue, 04 Feb 2025 17:07:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02492v1</guid></item><item><title>A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation</title><link>http://arxiv.org/abs/2502.02489v1</link><description>Ultrasound (US) imaging is clinically invaluable due to its noninvasive andsafe nature. However, interpreting US images is challenging, requiressignificant expertise, and time, and is often prone to errors. Deep learningoffers assistive solutions such as segmentation. Supervised methods rely onlarge, high-quality, and consistently labeled datasets, which are challengingto curate. Moreover, these methods tend to underperform on out-of-distributiondata, limiting their clinical utility. Self-supervised learning (SSL) hasemerged as a promising alternative, leveraging unlabeled data to enhance modelperformance and generalisability. We introduce a contrastive SSL approachtailored for B-mode US images, incorporating a novel Relation Contrastive Loss(RCL). RCL encourages learning of distinct features by differentiating positiveand negative sample pairs through a learnable metric. Additionally, we proposespatial and frequency-based augmentation strategies for the representationlearning on US images. Our approach significantly outperforms traditionalsupervised segmentation methods across three public breast US datasets,particularly in data-limited scenarios. Notable improvements on the Dicesimilarity metric include a 4% increase on 20% and 50% of the BUSI dataset,nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4%and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively.Furthermore, we demonstrate superior generalisability on theout-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6%compared to the supervised baseline using 20% and 50% of the BUSI and BrEaSTtraining data, respectively. Our research highlights that domain-inspired SSLcan improve US segmentation, especially under data-limited conditions.</description><author>Edward Ellis, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali</author><pubDate>Tue, 04 Feb 2025 17:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02489v1</guid></item><item><title>Do Graph Diffusion Models Accurately Capture and Generate Substructure Distributions?</title><link>http://arxiv.org/abs/2502.02488v1</link><description>Diffusion models have gained popularity in graph generation tasks; however,the extent of their expressivity concerning the graph distributions they canlearn is not fully understood. Unlike models in other domains, popularbackbones for graph diffusion models, such as Graph Transformers, do notpossess universal expressivity to accurately model the distribution scores ofcomplex graph data. Our work addresses this limitation by focusing on thefrequency of specific substructures as a key characteristic of target graphdistributions. When evaluating existing models using this metric, we find thatthey fail to maintain the distribution of substructure counts observed in thetraining set when generating new graphs. To address this issue, we establish atheoretical connection between the expressivity of Graph Neural Networks (GNNs)and the overall performance of graph diffusion models, demonstrating that moreexpressive GNN backbones can better capture complex distribution patterns. Byintegrating advanced GNNs into the backbone architecture, we achievesignificant improvements in substructure generation.</description><author>Xiyuan Wang, Yewei Liu, Lexi Pang, Siwei Chen, Muhan Zhang</author><pubDate>Tue, 04 Feb 2025 17:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02488v1</guid></item><item><title>Hier-EgoPack: Hierarchical Egocentric Video Understanding with Diverse Task Perspectives</title><link>http://arxiv.org/abs/2502.02487v1</link><description>Our comprehension of video streams depicting human activities is naturallymultifaceted: in just a few moments, we can grasp what is happening, identifythe relevance and interactions of objects in the scene, and forecast what willhappen soon, everything all at once. To endow autonomous systems with such aholistic perception, learning how to correlate concepts, abstract knowledgeacross diverse tasks, and leverage tasks synergies when learning novel skillsis essential. A significant step in this direction is EgoPack, a unifiedframework for understanding human activities across diverse tasks with minimaloverhead. EgoPack promotes information sharing and collaboration amongdownstream tasks, essential for efficiently learning new skills. In this paper,we introduce Hier-EgoPack, which advances EgoPack by enabling reasoning alsoacross diverse temporal granularities, which expands its applicability to abroader range of downstream tasks. To achieve this, we propose a novelhierarchical architecture for temporal reasoning equipped with a GNN layerspecifically designed to tackle the challenges of multi-granularity reasoningeffectively. We evaluate our approach on multiple Ego4d benchmarks involvingboth clip-level and frame-level reasoning, demonstrating how our hierarchicalunified architecture effectively solves these diverse tasks simultaneously.</description><author>Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Tatiana Tommasi, Giuseppe Averta</author><pubDate>Tue, 04 Feb 2025 17:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02487v1</guid></item><item><title>Catoni Contextual Bandits are Robust to Heavy-tailed Rewards</title><link>http://arxiv.org/abs/2502.02486v1</link><description>Typical contextual bandit algorithms assume that the rewards at each roundlie in some fixed range $[0, R]$, and their regret scales polynomially withthis reward range $R$. However, many practical scenarios naturally involveheavy-tailed rewards or rewards where the worst-case range can be substantiallylarger than the variance. In this paper, we develop an algorithmic approachbuilding on Catoni's estimator from robust statistics, and apply it tocontextual bandits with general function approximation. When the variance ofthe reward at each round is known, we use a variance-weighted regressionapproach and establish a regret bound that depends only on the cumulativereward variance and logarithmically on the reward range $R$ as well as thenumber of rounds $T$. For the unknown-variance case, we further propose acareful peeling-based algorithm and remove the need for cumbersome varianceestimation. With additional dependence on the fourth moment, our algorithm alsoenjoys a variance-based bound with logarithmic reward-range dependence.Moreover, we demonstrate the optimality of the leading-order term in our regretbound through a matching lower bound.</description><author>Chenlu Ye, Yujia Jin, Alekh Agarwal, Tong Zhang</author><pubDate>Tue, 04 Feb 2025 17:03:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02486v1</guid></item><item><title>Prostate-Specific Foundation Models for Enhanced Detection of Clinically Significant Cancer</title><link>http://arxiv.org/abs/2502.00366v2</link><description>Accurate prostate cancer diagnosis remains challenging. Even when using MRI,radiologists exhibit low specificity and significant inter-observervariability, leading to potential delays or inaccuracies in identifyingclinically significant cancers. This leads to numerous unnecessary biopsies andrisks of missing clinically significant cancers. Here we present prostatevision contrastive network (ProViCNet), prostate organ-specific visionfoundation models for Magnetic Resonance Imaging (MRI) and Trans-RectalUltrasound imaging (TRUS) for comprehensive cancer detection. ProViCNet wastrained and validated using 4,401 patients across six institutions, as aprostate cancer detection model on radiology images relying on patch-levelcontrastive learning guided by biopsy confirmed radiologist annotations.ProViCNet demonstrated consistent performance across multiple internal andexternal validation cohorts with area under the receiver operating curve valuesranging from 0.875 to 0.966, significantly outperforming radiologists in thereader study (0.907 versus 0.805, p&lt;0.001) for mpMRI, while achieving 0.670 to0.740 for TRUS. We also integrated ProViCNet with standard PSA to develop avirtual screening test, and we showed that we can maintain the high sensitivityfor detecting clinically significant cancers while more than doublingspecificity from 15% to 38% (p&lt;0.001), thereby substantially reducingunnecessary biopsies. These findings highlight that ProViCNet's potential forenhancing prostate cancer diagnosis accuracy and reduce unnecessary biopsies,thereby optimizing diagnostic pathways.</description><author>Jeong Hoon Lee, Cynthia Xinran Li, Hassan Jahanandish, Indrani Bhattacharya, Sulaiman Vesal, Lichun Zhang, Shengtian Sang, Moon Hyung Choi, Simon John Christoph Soerensen, Steve Ran Zhou, Elijah Richard Sommer, Richard Fan, Pejman Ghanouni, Yuze Song, Tyler M. Seibert, Geoffrey A. Sonn, Mirabela Rusu</author><pubDate>Tue, 04 Feb 2025 17:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.00366v2</guid></item><item><title>Point-Level Topological Representation Learning on Point Clouds</title><link>http://arxiv.org/abs/2406.02300v3</link><description>Topological Data Analysis (TDA) allows us to extract powerful topological andhigher-order information on the global shape of a data set or point cloud.Tools like Persistent Homology or the Euler Transform give a single complexdescription of the global structure of the point cloud. However, common machinelearning applications like classification require point-level information andfeatures to be available. In this paper, we bridge this gap and propose a novelmethod to extract node-level topological features from complex point cloudsusing discrete variants of concepts from algebraic topology and differentialgeometry. We verify the effectiveness of these topological point features(TOPF) on both synthetic and real-world data and study their robustness undernoise and heterogeneous sampling.</description><author>Vincent P. Grande, Michael T. Schaub</author><pubDate>Tue, 04 Feb 2025 16:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02300v3</guid></item><item><title>Distributional Diffusion Models with Scoring Rules</title><link>http://arxiv.org/abs/2502.02483v1</link><description>Diffusion models generate high-quality synthetic data. They operate bydefining a continuous-time forward process which gradually adds Gaussian noiseto data until fully corrupted. The corresponding reverse process progressively"denoises" a Gaussian sample into a sample from the data distribution. However,generating high-quality outputs requires many discretization steps to obtain afaithful approximation of the reverse process. This is expensive and hasmotivated the development of many acceleration methods. We propose toaccomplish sample generation by learning the posterior {\em distribution} ofclean data samples given their noisy versions, instead of only the mean of thisdistribution. This allows us to sample from the probability transitions of thereverse process on a coarse time scale, significantly accelerating inferencewith minimal degradation of the quality of the output. This is accomplished byreplacing the standard regression loss used to estimate conditional means witha scoring rule. We validate our method on image and robot trajectorygeneration, where we consistently outperform standard diffusion models at fewdiscretization steps.</description><author>Valentin De Bortoli, Alexandre Galashov, J. Swaroop Guntupalli, Guangyao Zhou, Kevin Murphy, Arthur Gretton, Arnaud Doucet</author><pubDate>Tue, 04 Feb 2025 16:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02483v1</guid></item><item><title>Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study</title><link>http://arxiv.org/abs/2502.02481v1</link><description>Large language models (LLMs) have shown continuously improving multilingualcapabilities, and even small-scale open-source models have demonstrated rapidperformance enhancement. In this paper, we systematically explore the abilitiesof open LLMs with less than ten billion parameters to handle multilingualmachine translation (MT) tasks. We conduct comprehensive evaluations on sixpopular LLMs and find that models like Gemma2-9B exhibit impressivemultilingual translation capabilities. We then introduce the Parallel-FirstMonolingual-Second (PFMS) data mixing strategy in the continual pretrainingstage to further enhance the MT performance and present GemmaX2-28, a 9B modelachieving top-tier multilingual translation performance across 28 languages.Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)models such as TowerInstruct and XALMA and achieves competitive performancewith Google Translate and GPT-4-turbo.</description><author>Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, BinWang</author><pubDate>Tue, 04 Feb 2025 16:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02481v1</guid></item><item><title>Stable Port-Hamiltonian Neural Networks</title><link>http://arxiv.org/abs/2502.02480v1</link><description>In recent years, nonlinear dynamic system identification using artificialneural networks has garnered attention due to its manifold potentialapplications in virtually all branches of science and engineering. However,purely data-driven approaches often struggle with extrapolation and may yieldphysically implausible forecasts. Furthermore, the learned dynamics can exhibitinstabilities, making it difficult to apply such models safely and robustly.This article proposes stable port-Hamiltonian neural networks, a machinelearning architecture that incorporates the physical biases of energyconservation or dissipation while guaranteeing global Lyapunov stability of thelearned dynamics. Evaluations with illustrative examples and real-worldmeasurement data demonstrate the model's ability to generalize from sparsedata, outperforming purely data-driven approaches and avoiding instabilityissues. In addition, the model's potential for data-driven surrogate modelingis highlighted in application to multi-physics simulation data.</description><author>Fabian J. Roth, Dominik K. Klein, Maximilian Kannapinn, Jan Peters, Oliver Weeger</author><pubDate>Tue, 04 Feb 2025 16:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02480v1</guid></item><item><title>From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare</title><link>http://arxiv.org/abs/2409.09727v2</link><description>Federated learning holds great potential for enabling large-scale healthcareresearch and collaboration across multiple centres while ensuring data privacyand security are not compromised. Although numerous recent studies suggest orutilize federated learning based methods in healthcare, it remains unclearwhich ones have potential clinical utility. This review paper considers andanalyzes the most recent studies up to May 2024 that describe federatedlearning based methods in healthcare. After a thorough review, we find that thevast majority are not appropriate for clinical use due to their methodologicalflaws and/or underlying biases which include but are not limited to privacyconcerns, generalization issues, and communication costs. As a result, theeffectiveness of federated learning in healthcare is significantly compromised.To overcome these challenges, we provide recommendations and promisingopportunities that might be implemented to resolve these problems and improvethe quality of model development in federated learning with healthcare.</description><author>Ming Li, Pengcheng Xu, Junjie Hu, Zeyu Tang, Guang Yang</author><pubDate>Tue, 04 Feb 2025 16:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09727v2</guid></item><item><title>Discovery of Spatter Constitutive Models in Additive Manufacturing Using Machine Learning</title><link>http://arxiv.org/abs/2501.08922v2</link><description>Additive manufacturing (AM) is a rapidly evolving technology that hasattracted applications across a wide range of fields due to its ability tofabricate complex geometries. However, one of the key challenges in AM isachieving consistent print quality. This inconsistency is often attributed touncontrolled melt pool dynamics, partly caused by spatter which can lead todefects. Therefore, capturing and controlling the evolution of the melt pool iscrucial for enhancing process stability and part quality. In this study, wedeveloped a framework to support decision-making towards efficient AM processoperations, capable of facilitating quality control and minimizing defects viamachine learning (ML) and polynomial symbolic regression models. We implementedexperimentally validated computational tools, specifically for laser powder bedfusion (LPBF) processes as a cost-effective approach to collect large datasets.For a dataset consisting of 281 varying process conditions, parameters such asmelt pool dimensions (length, width, depth), melt pool geometry (area, volume),and volume indicated as spatter were extracted. Using machine learning (ML) andpolynomial symbolic regression models, a high R2 of over 95 % was achieved inpredicting the melt pool dimensions and geometry features on both the trainingand testing datasets, with either process conditions (power and velocity) ormelt pool dimensions as the model inputs. In the case of volume indicated asspatter the value of the R2 improved after logarithmic transforming the modelinputs, which were either the process conditions or the melt pool dimensions.Among the investigated ML models, the ExtraTree model achieved the highest R2values of 96.7 % and 87.5 %.</description><author>Olabode T. Ajenifujah, Amir Barati Farimani</author><pubDate>Tue, 04 Feb 2025 16:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08922v2</guid></item><item><title>Broadcasting in random recursive dags</title><link>http://arxiv.org/abs/2306.01727v3</link><description>A uniform $k$-{\sc dag} generalizes the uniform random recursive tree bypicking $k$ parents uniformly at random from the existing nodes. It starts with$k$ ''roots''. Each of the $k$ roots is assigned a bit. These bits arepropagated by a noisy channel. The parents' bits are flipped with probability$p$, and a majority vote is taken. When all nodes have received their bits, the$k$-{\sc dag} is shown without identifying the roots. The goal is to estimatethe majority bit among the roots. We identify the threshold for $p$ as afunction of $k$ below which the majority rule among all nodes yields an error$c+o(1)$ with $c&lt;1/2$. Above the threshold the majority rule errs withprobability $1/2+o(1)$.</description><author>Simon Briend, Luc Devroye, Gabor Lugosi</author><pubDate>Tue, 04 Feb 2025 16:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01727v3</guid></item><item><title>Using Random Noise Equivariantly to Boost Graph Neural Networks Universally</title><link>http://arxiv.org/abs/2502.02479v1</link><description>Recent advances in Graph Neural Networks (GNNs) have explored the potentialof random noise as an input feature to enhance expressivity across diversetasks. However, naively incorporating noise can degrade performance, whilearchitectures tailored to exploit noise for specific tasks excel yet lack broadapplicability. This paper tackles these issues by laying down a theoreticalframework that elucidates the increased sample complexity when introducingrandom noise into GNNs without careful design. We further propose EquivariantNoise GNN (ENGNN), a novel architecture that harnesses the symmetricalproperties of noise to mitigate sample complexity and bolster generalization.Our experiments demonstrate that using noise equivariantly significantlyenhances performance on node-level, link-level, subgraph, and graph-level tasksand achieves comparable performance to models designed for specific tasks,thereby offering a general method to boost expressivity across various graphtasks.</description><author>Xiyuan Wang, Muhan Zhang</author><pubDate>Tue, 04 Feb 2025 16:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02479v1</guid></item><item><title>Style transfer as data augmentation: evaluating unpaired image-to-image translation models in mammography</title><link>http://arxiv.org/abs/2502.02475v1</link><description>Several studies indicate that deep learning models can learn to detect breastcancer from mammograms (X-ray images of the breasts). However, challenges withoverfitting and poor generalisability prevent their routine use in the clinic.Models trained on data from one patient population may not perform well onanother due to differences in their data domains, emerging due to variations inscanning technology or patient characteristics. Data augmentation techniquescan be used to improve generalisability by expanding the diversity of featurerepresentations in the training data by altering existing examples.Image-to-image translation models are one approach capable of imposing thecharacteristic feature representations (i.e. style) of images from one datasetonto another. However, evaluating model performance is non-trivial,particularly in the absence of ground truths (a common reality in medicalimaging). Here, we describe some key aspects that should be considered whenevaluating style transfer algorithms, highlighting the advantages anddisadvantages of popular metrics, and important factors to be mindful of whenimplementing them in practice. We consider two types of generative models: acycle-consistent generative adversarial network (CycleGAN) and adiffusion-based SynDiff model. We learn unpaired image-to-image translationacross three mammography datasets. We highlight that undesirable aspects ofmodel performance may determine the suitability of some metrics, and alsoprovide some analysis indicating the extent to which various metrics assessunique aspects of model performance. We emphasise the need to use severalmetrics for a comprehensive assessment of model performance.</description><author>Emir Ahmed, Spencer A. Thomas, Ciaran Bench</author><pubDate>Tue, 04 Feb 2025 16:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02475v1</guid></item><item><title>Monocular Per-Object Distance Estimation with Masked Object Modeling</title><link>http://arxiv.org/abs/2401.03191v2</link><description>Per-object distance estimation is critical in surveillance and autonomousdriving, where safety is crucial. While existing methods rely on geometric ordeep supervised features, only a few attempts have been made to leverageself-supervised learning. In this respect, our paper draws inspiration fromMasked Image Modeling (MiM) and extends it to multi-object tasks. While MiMfocuses on extracting global image-level representations, it struggles withindividual objects within the image. This is detrimental for distanceestimation, as objects far away correspond to negligible portions of the image.Conversely, our strategy, termed Masked Object Modeling (MoM), enables a novelapplication of masking techniques. In a few words, we devise an auxiliaryobjective that reconstructs the portions of the image pertaining to the objectsdetected in the scene. The training phase is performed in a single unifiedstage, simultaneously optimizing the masking objective and the downstream loss(i.e., distance estimation). We evaluate the effectiveness of MoM on a novel reference architecture(DistFormer) on the standard KITTI, NuScenes, and MOTSynth datasets. Ourevaluation reveals that our framework surpasses the SoTA and highlights itsrobust regularization properties. The MoM strategy enhances both zero-shot andfew-shot capabilities, from synthetic to real domain. Finally, it furthers therobustness of the model in the presence of occluded or poorly detected objects.Code is available at https://github.com/apanariello4/DistFormer</description><author>Aniello Panariello, Gianluca Mancusi, Fedy Haj Ali, Angelo Porrello, Simone Calderara, Rita Cucchiara</author><pubDate>Tue, 04 Feb 2025 16:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03191v2</guid></item><item><title>SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations</title><link>http://arxiv.org/abs/2502.02472v1</link><description>The Latent Stochastic Differential Equation (SDE) is a powerful tool for timeseries and sequence modeling. However, training Latent SDEs typically relies onadjoint sensitivity methods, which depend on simulation and backpropagationthrough approximate SDE solutions, which limit scalability. In this work, wepropose SDE Matching, a new simulation-free method for training Latent SDEs.Inspired by modern Score- and Flow Matching algorithms for learning generativedynamics, we extend these ideas to the domain of stochastic dynamics for timeseries and sequence modeling, eliminating the need for costly numericalsimulations. Our results demonstrate that SDE Matching achieves performancecomparable to adjoint sensitivity methods while drastically reducingcomputational complexity.</description><author>Grigory Bartosh, Dmitry Vetrov, Christian A. Naesseth</author><pubDate>Tue, 04 Feb 2025 16:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02472v1</guid></item><item><title>Internal Activation as the Polar Star for Steering Unsafe LLM Behavior</title><link>http://arxiv.org/abs/2502.01042v2</link><description>Large language models (LLMs) have demonstrated exceptional capabilitiesacross a wide range of tasks but also pose significant risks due to theirpotential to generate harmful content. Although existing safety mechanisms canimprove model safety, they often lead to overly cautious behavior and fail tofully utilize LLMs' internal cognitive processes. Drawing inspiration fromcognitive science, where humans rely on reflective reasoning (System 2thinking) to regulate language and behavior, we empirically demonstrate thatLLMs also possess a similar capacity for internal assessment and regulation,which can be actively detected. Building on this insight, we introduce SafeSwitch, a framework thatdynamically regulates unsafe outputs by monitoring and utilizing the model'sinternal states. Our empirical results show that SafeSwitch reduces harmfuloutputs by over 80% on safety benchmarks while maintaining strong utility.Compared to traditional safety alignment methods, SafeSwitch delivers moreinformative and context-aware refusals, demonstrates resilience to unseenqueries, and achieves these benefits while only tuning less than 6% of theoriginal parameters. These features make SafeSwitch a promising approach forimplementing nuanced safety controls in LLMs.</description><author>Peixuan Han, Cheng Qian, Xiusi Chen, Yuji Zhang, Denghui Zhang, Heng Ji</author><pubDate>Tue, 04 Feb 2025 16:47:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.01042v2</guid></item><item><title>Mind the Gap: Evaluating Patch Embeddings from General-Purpose and Histopathology Foundation Models for Cell Segmentation and Classification</title><link>http://arxiv.org/abs/2502.02471v1</link><description>Recent advancements in foundation models have transformed computer vision,driving significant performance improvements across diverse domains, includingdigital histopathology. However, the advantages of domain-specifichistopathology foundation models over general-purpose models for specializedtasks such as cell analysis remain underexplored. This study investigates therepresentation learning gap between these two categories by analyzingmulti-level patch embeddings applied to cell instance segmentation andclassification. We implement an encoder-decoder architecture with a consistentdecoder and various encoders. These include convolutional, vision transformer(ViT), and hybrid encoders pre-trained on ImageNet-22K or LVD-142M,representing general-purpose foundation models. These are compared against ViTencoders from the recently released UNI, Virchow2, and Prov-GigaPath foundationmodels, trained on patches extracted from hundreds of thousands ofhistopathology whole-slide images. The decoder integrates patch embeddings fromdifferent encoder depths via skip connections to generate semantic and distancemaps. These maps are then post-processed to create instance segmentation maskswhere each label corresponds to an individual cell and to perform cell-typeclassification. All encoders remain frozen during training to assess theirpre-trained feature extraction capabilities. Using the PanNuke and CoNIChistopathology datasets, and the newly introduced Nissl-stained CytoDArk0dataset for brain cytoarchitecture studies, we evaluate instance-leveldetection, segmentation accuracy, and cell-type classification. This studyprovides insights into the comparative strengths and limitations ofgeneral-purpose vs. histopathology foundation models, offering guidance formodel selection in cell-focused histopathology and brain cytoarchitectureanalysis workflows.</description><author>Valentina Vadori, Antonella Peruffo, Jean-Marie Graïc, Livio Finos, Enrico Grisan</author><pubDate>Tue, 04 Feb 2025 16:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02471v1</guid></item><item><title>Modular Training of Neural Networks aids Interpretability</title><link>http://arxiv.org/abs/2502.02470v1</link><description>An approach to improve neural network interpretability is via clusterability,i.e., splitting a model into disjoint clusters that can be studiedindependently. We define a measure for clusterability and show that pre-trainedmodels form highly enmeshed clusters via spectral graph clustering. We thustrain models to be more modular using a ``clusterability loss'' function thatencourages the formation of non-interacting clusters. Using automatedinterpretability techniques, we show that our method can help train models thatare more modular and learn different, disjoint, and smaller circuits. Weinvestigate CNNs trained on MNIST and CIFAR, small transformers trained onmodular addition, and language models. Our approach provides a promisingdirection for training neural networks that learn simpler functions and areeasier to interpret.</description><author>Satvik Golechha, Maheep Chaudhary, Joan Velja, Alessandro Abate, Nandi Schoots</author><pubDate>Tue, 04 Feb 2025 16:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02470v1</guid></item><item><title>High-Fidelity Human Avatars from Laptop Webcams using Edge Compute</title><link>http://arxiv.org/abs/2502.02468v1</link><description>Applications of generating photo-realistic human avatars are many, however,high-fidelity avatar generation traditionally required expensive professionalcamera rigs and artistic labor, but recent research has enabled constructingthem automatically from smartphones with RGB and IR sensors. However, these newmethods still rely on the presence of high-resolution cameras on modernsmartphones and often require offloading the processing to powerful serverswith GPUs. Modern applications such as video conferencing call for the abilityto generate these avatars from consumer-grade laptop webcams using limitedcompute available on-device. In this work, we develop a novel method based on3D morphable models, landmark detection, photo-realistic texture GANs, anddifferentiable rendering to tackle the problem of low webcam image quality andedge computation. We build an automatic system to generate high-fidelityanimatable avatars under these limitations, leveraging the neural computecapabilities of mobile chips.</description><author>Akash Haridas Imran N. Junejo</author><pubDate>Tue, 04 Feb 2025 16:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02468v1</guid></item><item><title>A Differential Equation Approach for Wasserstein GANs and Beyond</title><link>http://arxiv.org/abs/2405.16351v2</link><description>This paper proposes a new theoretical lens to view Wasserstein generativeadversarial networks (WGANs). To minimize the Wasserstein-1 distance betweenthe true data distribution and our estimate of it, we derive adistribution-dependent ordinary differential equation (ODE) which representsthe gradient flow of the Wasserstein-1 loss, and show that a forward Eulerdiscretization of the ODE converges. This inspires a new class of generativemodels that naturally integrates persistent training (which we call W1-FE).When persistent training is turned off, we prove that W1-FE reduces to WGAN.When we intensify persistent training, W1-FE is shown to outperform WGAN intraining experiments from low to high dimensions, in terms of both convergencespeed and training results. Intriguingly, one can reap the benefits only whenpersistent training is carefully integrated through our ODE perspective. Asdemonstrated numerically, a naive inclusion of persistent training in WGAN(without relying on our ODE framework) can significantly worsen trainingresults.</description><author>Zachariah Malik, Yu-Jui Huang</author><pubDate>Tue, 04 Feb 2025 16:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16351v2</guid></item><item><title>Towards Consistent and Controllable Image Synthesis for Face Editing</title><link>http://arxiv.org/abs/2502.02465v1</link><description>Current face editing methods mainly rely on GAN-based techniques, but recentfocus has shifted to diffusion-based models due to their success in imagereconstruction. However, diffusion models still face challenges in manipulatingfine-grained attributes and preserving consistency of attributes that shouldremain unchanged. To address these issues and facilitate more convenientediting of face images, we propose a novel approach that leverages the power ofStable-Diffusion models and crude 3D face models to control the lighting,facial expression and head pose of a portrait photo. We observe that this taskessentially involve combinations of target background, identity and differentface attributes. We aim to sufficiently disentangle the control of thesefactors to enable high-quality of face editing. Specifically, our method,coined as RigFace, contains: 1) A Spatial Arrtibute Encoder that providespresise and decoupled conditions of background, pose, expression and lighting;2) An Identity Encoder that transfers identity features to the denoising UNetof a pre-trained Stable-Diffusion model; 3) An Attribute Rigger that injectsthose conditions into the denoising UNet. Our model achieves comparable or evensuperior performance in both identity preservation and photorealism compared toexisting face editing models.</description><author>Mengting Wei, Tuomas Varanka, Yante Li, Xingxun Jiang, Huai-Qian Khor, Guoying Zhao</author><pubDate>Tue, 04 Feb 2025 16:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02465v1</guid></item><item><title>Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2502.02464v1</link><description>Retrieval, re-ranking, and retrieval-augmented generation (RAG) are criticalcomponents of modern natural language processing (NLP) applications ininformation retrieval, question answering, and knowledge-based text generation.However, existing solutions are often fragmented, lacking a unified frameworkthat easily integrates these essential processes. The absence of a standardizedimplementation, coupled with the complexity of retrieval and re-rankingworkflows, makes it challenging for researchers to compare and evaluatedifferent approaches in a consistent environment. While existing toolkits suchas Rerankers and RankLLM provide general-purpose reranking pipelines, theyoften lack the flexibility required for fine-grained experimentation andbenchmarking. In response to these challenges, we introduce \textbf{Rankify}, apowerful and modular open-source toolkit designed to unify retrieval,re-ranking, and RAG within a cohesive framework. Rankify supports a wide rangeof retrieval techniques, including dense and sparse retrievers, whileincorporating state-of-the-art re-ranking models to enhance retrieval quality.Additionally, Rankify includes a collection of pre-retrieved datasets tofacilitate benchmarking, available at Huggingface(https://huggingface.co/datasets/abdoelsayed/reranking-datasets). To encourageadoption and ease of integration, we provide comprehensive documentation(http://rankify.readthedocs.io/), an open-source implementation onGitHub(https://github.com/DataScienceUIBK/rankify), and a PyPI package foreffortless installation(https://pypi.org/project/rankify/). By providing aunified and lightweight framework, Rankify allows researchers and practitionersto advance retrieval and re-ranking methodologies while ensuring consistency,scalability, and ease of use.</description><author>Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed Ali, Adam Jatowt</author><pubDate>Tue, 04 Feb 2025 16:33:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02464v1</guid></item><item><title>Distribution Transformers: Fast Approximate Bayesian Inference With On-The-Fly Prior Adaptation</title><link>http://arxiv.org/abs/2502.02463v1</link><description>While Bayesian inference provides a principled framework for reasoning underuncertainty, its widespread adoption is limited by the intractability of exactposterior computation, necessitating the use of approximate inference. However,existing methods are often computationally expensive, or demand costlyretraining when priors change, limiting their utility, particularly insequential inference problems such as real-time sensor fusion. To address thesechallenges, we introduce the Distribution Transformer -- a novel architecturethat can learn arbitrary distribution-to-distribution mappings. Our method canbe trained to map a prior to the corresponding posterior, conditioned on somedataset -- thus performing approximate Bayesian inference. Our novelarchitecture represents a prior distribution as a (universally-approximating)Gaussian Mixture Model (GMM), and transforms it into a GMM representation ofthe posterior. The components of the GMM attend to each other viaself-attention, and to the datapoints via cross-attention. We demonstrate thatDistribution Transformers both maintain flexibility to vary the prior, andsignificantly reduces computation times-from minutes to milliseconds-whileachieving log-likelihood performance on par with or superior to existingapproximate inference methods across tasks such as sequential inference,quantum system parameter inference, and Gaussian Process predictive posteriorinference with hyperpriors.</description><author>George Whittle, Juliusz Ziomek, Jacob Rawling, Michael A Osborne</author><pubDate>Tue, 04 Feb 2025 16:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02463v1</guid></item><item><title>SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency</title><link>http://arxiv.org/abs/2502.02458v1</link><description>Multimodal Large Language Models (MLLMs) mainly fall into two architectures,each involving a trade-off between training and inference efficiency: embeddingspace alignment (e.g., LLaVA-1.5) is inefficient during inference, whilecross-attention space alignment (e.g., Flamingo) is inefficient in training. Inthis paper, we compare these two architectures and identify the key factors forbuilding efficient MLLMs. A primary difference between them lies in howattention is applied to visual tokens, particularly in their interactions witheach other. To investigate whether attention among visual tokens is necessary,we propose a new self-attention mechanism, NAAViT (\textbf{N}o\textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okens), whicheliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows thatattention among visual tokens is highly redundant. Based on these insights, weintroduce SAISA (\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace\textbf{A}lignment), a novel architecture that enhance both training andinference efficiency. SAISA directly aligns visual features with the inputspaces of NAAViT self-attention blocks, reducing computational overhead in bothself-attention blocks and feed-forward networks (FFNs). Using the sameconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\% and trainingbudget by 26\%, while achieving superior performance in terms of accuracy.Comprehensive ablation studies further validate the effectiveness of SAISAacross various LLMs and visual encoders. The code and model will be publiclyavailable at https://github.com/icip-cas/SAISA.</description><author>Qianhao Yuan, Yanjiang Liu, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun</author><pubDate>Tue, 04 Feb 2025 16:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02458v1</guid></item><item><title>Physics-Informed Echo State Networks for Modeling Controllable Dynamical Systems</title><link>http://arxiv.org/abs/2409.19140v2</link><description>Echo State Networks (ESNs) are recurrent neural networks usually employed formodeling nonlinear dynamic systems with relatively ease of training. Byincorporating physical laws into the training of ESNs, Physics-Informed ESNs(PI-ESNs) were proposed initially to model chaotic dynamic systems withoutexternal inputs. They require less data for training since OrdinaryDifferential Equations (ODEs) of the considered system help to regularize theESN. In this work, the PI-ESN is extended with external inputs to modelcontrollable nonlinear dynamic systems. Additionally, an existing self-adaptivebalancing loss method is employed to balance the contributions of the residualregression term and the physics-informed loss term in the total loss function.The experiments with two nonlinear systems modeled by ODEs, the Van der Poloscillator and the four-tank system, and with one differential-algebraic (DAE)system, an electric submersible pump, revealed that the proposed PI-ESNoutperforms the conventional ESN, especially in scenarios with limited dataavailability, showing that PI-ESNs can regularize an ESN model with externalinputs previously trained on just a few datapoints, reducing its overfittingand improving its generalization error (up to 92% relative reduction in thetest error). Further experiments demonstrated that the proposed PI-ESN isrobust to parametric uncertainties in the ODE equations and that modelpredictive control using PI-ESN outperforms the one using plain ESN,particularly when training data is scarce.</description><author>Eric Mochiutti, Eric Aislan Antonelo, Eduardo Camponogara</author><pubDate>Tue, 04 Feb 2025 16:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19140v2</guid></item><item><title>Uncovering Challenges of Solving the Continuous Gromov-Wasserstein Problem</title><link>http://arxiv.org/abs/2303.05978v3</link><description>Recently, the Gromov-Wasserstein Optimal Transport (GWOT) problem hasattracted the special attention of the ML community. In this problem, given twodistributions supported on two (possibly different) spaces, one has to find themost isometric map between them. In the discrete variant of GWOT, the task isto learn an assignment between given discrete sets of points. In the moreadvanced continuous formulation, one aims at recovering a parametric mappingbetween unknown continuous distributions based on i.i.d. samples derived fromthem. The clear geometrical intuition behind the GWOT makes it a natural choicefor several practical use cases, giving rise to a number of proposed solvers.Some of them claim to solve the continuous version of the problem. At the sametime, GWOT is notoriously hard, both theoretically and numerically. Moreover,all existing continuous GWOT solvers still heavily rely on discrete techniques.Natural questions arise: to what extent do existing methods unravel the GWOTproblem, what difficulties do they encounter, and under which conditions theyare successful? Our benchmark paper is an attempt to answer these questions. Wespecifically focus on the continuous GWOT as the most interesting and debatablesetup. We crash-test existing continuous GWOT approaches on differentscenarios, carefully record and analyze the obtained results, and identifyissues. Our findings experimentally testify that the scientific community isstill missing a reliable continuous GWOT solver, which necessitates furtherresearch efforts. As the first step in this direction, we propose a newcontinuous GWOT method which does not rely on discrete techniques and partiallysolves some of the problems of the competitors.</description><author>Xavier Aramayo Carrasco, Maksim Nekrashevich, Petr Mokrov, Evgeny Burnaev, Alexander Korotin</author><pubDate>Tue, 04 Feb 2025 16:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05978v3</guid></item><item><title>Orientation-aware interaction-based deep material network in polycrystalline materials modeling</title><link>http://arxiv.org/abs/2502.02457v1</link><description>Multiscale simulations are indispensable for connecting microstructuralfeatures to the macroscopic behavior of polycrystalline materials, but theirhigh computational demands limit their practicality. Deep material networks(DMNs) have been proposed as efficient surrogate models, yet they fall short ofcapturing texture evolution. To address this limitation, we propose theorientation-aware interaction-based deep material network (ODMN), whichincorporates an orientation-aware mechanism and an interaction mechanismgrounded in the Hill-Mandel principle. The orientation-aware mechanism learnsthe crystallographic textures, while the interaction mechanism capturesstress-equilibrium directions among representative volume element (RVE)subregions, offering insight into internal microstructural mechanics. Notably,ODMN requires only linear elastic data for training yet generalizes effectivelyto complex nonlinear and anisotropic responses. Our results show that ODMNaccurately predicts both mechanical responses and texture evolution undercomplex plastic deformation, thus expanding the applicability of DMNs topolycrystalline materials. By balancing computational efficiency withpredictive fidelity, ODMN provides a robust framework for multiscalesimulations of polycrystalline materials.</description><author>Ting-Ju Wei, Tung-Huan Su, Chuin-Shan Chen</author><pubDate>Tue, 04 Feb 2025 16:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02457v1</guid></item><item><title>Model Human Learners: Computational Models to Guide Instructional Design</title><link>http://arxiv.org/abs/2502.02456v1</link><description>Instructional designers face an overwhelming array of design choices, makingit challenging to identify the most effective interventions. To address thisissue, I propose the concept of a Model Human Learner, a unified computationalmodel of learning that can aid designers in evaluating candidate interventions.This paper presents the first successful demonstration of this concept, showingthat a computational model can accurately predict the outcomes of two human A/Bexperiments -- one testing a problem sequencing intervention and the othertesting an item design intervention. It also demonstrates that such a model cangenerate learning curves without requiring human data and provide theoreticalinsights into why an instructional intervention is effective. These findingslay the groundwork for future Model Human Learners that integrate cognitive andlearning theories to support instructional design across diverse tasks andinterventions.</description><author>Christopher J. MacLellan</author><pubDate>Tue, 04 Feb 2025 16:24:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02456v1</guid></item><item><title>The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking</title><link>http://arxiv.org/abs/2501.19358v2</link><description>This work identifies the Energy Loss Phenomenon in Reinforcement Learningfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,energy loss in the final layer of a Large Language Model (LLM) graduallyincreases during the RL process, with an excessive increase in energy losscharacterizing reward hacking. Beyond empirical analysis, we further provide atheoretical foundation by proving that, under mild conditions, the increasedenergy loss reduces the upper bound of contextual relevance in LLMs, which is acritical aspect of reward hacking as the reduced contextual relevance typicallyindicates overfitting to reward model-favored patterns in RL. To address thisissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes theincrease in energy loss in the LLM's final layer during reward calculation toprevent excessive energy loss, thereby mitigating reward hacking. Wetheoretically show that EPPO can be conceptually interpreted as anentropy-regularized RL algorithm, which provides deeper insights into itseffectiveness. Extensive experiments across various LLMs and tasks demonstratethe commonality of the energy loss phenomenon, as well as the effectiveness ofEPPO in mitigating reward hacking and improving RLHF performance.</description><author>Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao</author><pubDate>Tue, 04 Feb 2025 16:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19358v2</guid></item><item><title>AI Reliance and Decision Quality: Fundamentals, Interdependence, and the Effects of Interventions</title><link>http://arxiv.org/abs/2304.08804v4</link><description>In AI-assisted decision-making, a central promise of having ahuman-in-the-loop is that they should be able to complement the AI system byoverriding its wrong recommendations. In practice, however, we often see thathumans cannot assess the correctness of AI recommendations and, as a result,adhere to wrong or override correct advice. Different ways of relying on AIrecommendations have immediate, yet distinct, implications for decisionquality. Unfortunately, reliance and decision quality are often inappropriatelyconflated in the current literature on AI-assisted decision-making. In thiswork, we disentangle and formalize the relationship between reliance anddecision quality, and we characterize the conditions under which human-AIcomplementarity is achievable. To illustrate how reliance and decision qualityrelate to one another, we propose a visual framework and demonstrate itsusefulness for interpreting empirical findings, including the effects ofinterventions like explanations. Overall, our research highlights theimportance of distinguishing between reliance behavior and decision quality inAI-assisted decision-making.</description><author>Jakob Schoeffer, Johannes Jakubik, Michael Voessing, Niklas Kuehl, Gerhard Satzger</author><pubDate>Tue, 04 Feb 2025 16:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08804v4</guid></item><item><title>IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning</title><link>http://arxiv.org/abs/2502.02454v1</link><description>Using extensive training data from SA-1B, the Segment Anything Model (SAM)has demonstrated exceptional generalization and zero-shot capabilities,attracting widespread attention in areas such as medical image segmentation andremote sensing image segmentation. However, its performance in the field ofimage manipulation detection remains largely unexplored and unconfirmed. Thereare two main challenges in applying SAM to image manipulation detection: a)reliance on manual prompts, and b) the difficulty of single-view information insupporting cross-dataset generalization. To address these challenges, wedevelops a cross-view prompt learning paradigm called IMDPrompter based on SAM.Benefiting from the design of automated prompts, IMDPrompter no longer relieson manual guidance, enabling automated detection and localization.Additionally, we propose components such as Cross-view Feature Perception,Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitatecross-view perceptual learning and guide SAM to generate accurate masks.Extensive experimental results from five datasets (CASIA, Columbia, Coverage,IMD2020, and NIST16) validate the effectiveness of our proposed method.</description><author>Quan Zhang, Yuxin Qi, Xi Tang, Jinwei Fang, Xi Lin, Ke Zhang, Chun Yuan</author><pubDate>Tue, 04 Feb 2025 16:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02454v1</guid></item><item><title>Personalization Toolkit: Training Free Personalization of Large Vision Language Models</title><link>http://arxiv.org/abs/2502.02452v1</link><description>Large Vision Language Models (LVLMs) have significant potential to deliverpersonalized assistance by adapting to individual users' unique needs andpreferences. Personalization of LVLMs is an emerging area that involvescustomizing models to recognize specific object instances and provide tailoredresponses. However, existing approaches rely on time-consuming test-timetraining for each user and object, rendering them impractical. This paperproposes a novel, training-free approach to LVLM personalization by leveragingpre-trained vision foundation models to extract distinct features,retrieval-augmented generation (RAG) techniques to recognize instances in thevisual input, and visual prompting methods. Our model-agnostic vision toolkitenables flexible and efficient personalization without extensive retraining. Wedemonstrate state-of-the-art results, outperforming conventional training-basedapproaches and establish a new standard for LVLM personalization.</description><author>Soroush Seifi, Vaggelis Dorovatas, Daniel Olmeda Reino, Rahaf Aljundi</author><pubDate>Tue, 04 Feb 2025 16:19:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02452v1</guid></item><item><title>Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study</title><link>http://arxiv.org/abs/2502.02451v1</link><description>This study explores computational approaches for measuring moral foundations(MFs) in non-English corpora. Since most resources are developed primarily forEnglish, cross-linguistic applications of moral foundation theory remainlimited. Using Chinese as a case study, this paper evaluates the effectivenessof applying English resources to machine translated text, local languagelexicons, multilingual language models, and large language models (LLMs) inmeasuring MFs in non-English texts. The results indicate that machinetranslation and local lexicon approaches are insufficient for complex moralassessments, frequently resulting in a substantial loss of culturalinformation. In contrast, multilingual models and LLMs demonstrate reliablecross-language performance with transfer learning, with LLMs excelling in termsof data efficiency. Importantly, this study also underscores the need forhuman-in-the-loop validation of automated MF assessment, as the most advancedmodels may overlook cultural nuances in cross-language measurements. Thefindings highlight the potential of LLMs for cross-language MF measurements andother complex multilingual deductive coding tasks.</description><author>Calvin Yixiang Cheng, Scott A Hale</author><pubDate>Tue, 04 Feb 2025 16:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02451v1</guid></item><item><title>Robust and Conjugate Spatio-Temporal Gaussian Processes</title><link>http://arxiv.org/abs/2502.02450v1</link><description>State-space formulations allow for Gaussian process (GP) regression withlinear-in-time computational cost in spatio-temporal settings, but performancetypically suffers in the presence of outliers. In this paper, we adapt andspecialise the robust and conjugate GP (RCGP) framework of Altamirano et al.(2024) to the spatio-temporal setting. In doing so, we obtain an outlier-robustspatio-temporal GP with a computational cost comparable to classicalspatio-temporal GPs. We also overcome the three main drawbacks of RCGPs: theirunreliable performance when the prior mean is chosen poorly, their lack ofreliable uncertainty quantification, and the need to carefully select ahyperparameter by hand. We study our method extensively in finance and weatherforecasting applications, demonstrating that it provides a reliable approach tospatio-temporal modelling in the presence of outliers.</description><author>William Laplante, Matias Altamirano, Andrew Duncan, Jeremias Knoblauch, François-Xavier Briol</author><pubDate>Tue, 04 Feb 2025 16:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02450v1</guid></item><item><title>Embracing Dialectic Intersubjectivity: Coordination of Different Perspectives in Content Analysis with LLM Persona Simulation</title><link>http://arxiv.org/abs/2502.00903v2</link><description>This study attempts to advancing content analysis methodology fromconsensus-oriented to coordination-oriented practices, thereby embracingdiverse coding outputs and exploring the dynamics among differentialperspectives. As an exploratory investigation of this approach, we evaluate sixGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts onBiden and Trump during the 2020 U.S. presidential campaign, examining patternsacross these models. By assessing each model's alignment with ideologicalperspectives, we explore how partisan selective processing could be identifiedin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan personaLLMs exhibit stronger ideological biases when processing politically congruentcontent. Additionally, intercoder reliability is higher among same-partisanpersonas compared to cross-partisan pairs. This approach enhances the nuancedunderstanding of LLM outputs and advances the integrity of AI-driven socialscience research, enabling simulations of real-world implications.</description><author>Taewoo Kang, Kjerstin Thorson, Tai-Quan Peng, Dan Hiaeshutter-Rice, Sanguk Lee, Stuart Soroka</author><pubDate>Tue, 04 Feb 2025 16:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.00903v2</guid></item><item><title>TUMTraffic-VideoQA: A Benchmark for Unified Spatio-Temporal Video Understanding in Traffic Scenes</title><link>http://arxiv.org/abs/2502.02449v1</link><description>We present TUMTraffic-VideoQA, a novel dataset and benchmark designed forspatio-temporal video understanding in complex roadside traffic scenarios. Thedataset comprises 1,000 videos, featuring 85,000 multiple-choice QA pairs,2,300 object captioning, and 5,700 object grounding annotations, encompassingdiverse real-world conditions such as adverse weather and traffic anomalies. Byincorporating tuple-based spatio-temporal object expressions,TUMTraffic-VideoQA unifies three essential tasks-multiple-choice video questionanswering, referred object captioning, and spatio-temporal objectgrounding-within a cohesive evaluation framework. We further introduce theTUMTraffic-Qwen baseline model, enhanced with visual token sampling strategies,providing valuable insights into the challenges of fine-grained spatio-temporalreasoning. Extensive experiments demonstrate the dataset's complexity,highlight the limitations of existing models, and position TUMTraffic-VideoQAas a robust foundation for advancing research in intelligent transportationsystems. The dataset and benchmark are publicly available to facilitate furtherexploration.</description><author>Xingcheng Zhou, Konstantinos Larintzakis, Hao Guo, Walter Zimmer, Mingyu Liu, Hu Cao, Jiajie Zhang, Venkatnarayanan Lakshminarasimhan, Leah Strand, Alois C. Knoll</author><pubDate>Tue, 04 Feb 2025 16:14:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02449v1</guid></item><item><title>MIDI-GPT: A Controllable Generative Model for Computer-Assisted Multitrack Music Composition</title><link>http://arxiv.org/abs/2501.17011v2</link><description>We present and release MIDI-GPT, a generative system based on the Transformerarchitecture that is designed for computer-assisted music compositionworkflows. MIDI-GPT supports the infilling of musical material at the track andbar level, and can condition generation on attributes including: instrumenttype, musical style, note density, polyphony level, and note duration. In orderto integrate these features, we employ an alternative representation formusical material, creating a time-ordered sequence of musical events for eachtrack and concatenating several tracks into a single sequence, rather thanusing a single time-ordered sequence where the musical events corresponding todifferent tracks are interleaved. We also propose a variation of ourrepresentation allowing for expressiveness. We present experimental resultsthat demonstrate that MIDI-GPT is able to consistently avoid duplicating themusical material it was trained on, generate music that is stylisticallysimilar to the training dataset, and that attribute controls allow enforcingvarious constraints on the generated material. We also outline severalreal-world applications of MIDI-GPT, including collaborations with industrypartners that explore the integration and evaluation of MIDI-GPT intocommercial products, as well as several artistic works produced using it.</description><author>Philippe Pasquier, Jeff Ens, Nathan Fradet, Paul Triana, Davide Rizzotti, Jean-Baptiste Rolland, Maryam Safi</author><pubDate>Tue, 04 Feb 2025 16:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.17011v2</guid></item><item><title>Sparse Data Generation Using Diffusion Models</title><link>http://arxiv.org/abs/2502.02448v1</link><description>Sparse data is ubiquitous, appearing in numerous domains, from economics andrecommender systems to astronomy and biomedical sciences. However, efficientlyand realistically generating sparse data remains a significant challenge. Weintroduce Sparse Data Diffusion (SDD), a novel method for generating sparsedata. SDD extends continuous state-space diffusion models by explicitlymodeling sparsity through the introduction of Sparsity Bits. Empiricalvalidation on image data from various domains-including two scientificapplications, physics and biology-demonstrates that SDD achieves high fidelityin representing data sparsity while preserving the quality of the generateddata.</description><author>Phil Ostheimer, Mayank Nagda, Marius Kloft, Sophie Fellenz</author><pubDate>Tue, 04 Feb 2025 16:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02448v1</guid></item><item><title>Towards graph neural networks for provably solving convex optimization problems</title><link>http://arxiv.org/abs/2502.02446v1</link><description>Recently, message-passing graph neural networks (MPNNs) have shown potentialfor solving combinatorial and continuous optimization problems due to theirability to capture variable-constraint interactions. While existing approachesleverage MPNNs to approximate solutions or warm-start traditional solvers, theyoften lack guarantees for feasibility, particularly in convex optimizationsettings. Here, we propose an iterative MPNN framework to solve convexoptimization problems with provable feasibility guarantees. First, wedemonstrate that MPNNs can provably simulate standard interior-point methodsfor solving quadratic problems with linear constraints, covering relevantproblems such as SVMs. Secondly, to ensure feasibility, we introduce a variantthat starts from a feasible point and iteratively restricts the search withinthe feasible region. Experimental results show that our approach outperformsexisting neural baselines in solution quality and feasibility, generalizes wellto unseen problem sizes, and, in some cases, achieves faster solution timesthan state-of-the-art solvers such as Gurobi.</description><author>Chendi Qian, Christopher Morris</author><pubDate>Tue, 04 Feb 2025 16:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02446v1</guid></item><item><title>Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models</title><link>http://arxiv.org/abs/2502.02444v1</link><description>Values are core drivers of individual and collective perception, cognition,and behavior. Value systems, such as Schwartz's Theory of Basic Human Values,delineate the hierarchy and interplay among these values, enablingcross-disciplinary investigations into decision-making and societal dynamics.Recently, the rise of Large Language Models (LLMs) has raised concernsregarding their elusive intrinsic values. Despite growing efforts inevaluating, understanding, and aligning LLM values, a psychologically groundedLLM value system remains underexplored. This study addresses the gap byintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,adaptable, and theoretically informed method for constructing value systems.Leveraging GPLA, we propose a psychologically grounded five-factor value systemtailored for LLMs. For systematic validation, we present three benchmarkingtasks that integrate psychological principles with cutting-edge AI priorities.Our results reveal that the proposed value system meets standard psychologicalcriteria, better captures LLM values, improves LLM safety prediction, andenhances LLM alignment, when compared to the canonical Schwartz's values.</description><author>Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song</author><pubDate>Tue, 04 Feb 2025 16:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02444v1</guid></item><item><title>LLMER: Crafting Interactive Extended Reality Worlds with JSON Data Generated by Large Language Models</title><link>http://arxiv.org/abs/2502.02441v1</link><description>The integration of Large Language Models (LLMs) like GPT-4 with ExtendedReality (XR) technologies offers the potential to build truly immersive XRenvironments that interact with human users through natural language, e.g.,generating and animating 3D scenes from audio inputs. However, the complexityof XR environments makes it difficult to accurately extract relevant contextualdata and scene/object parameters from an overwhelming volume of XR artifacts.It leads to not only increased costs with pay-per-use models, but also elevatedlevels of generation errors. Moreover, existing approaches focusing on codingscript generation are often prone to generation errors, resulting in flawed orinvalid scripts, application crashes, and ultimately a degraded userexperience. To overcome these challenges, we introduce LLMER, a novel frameworkthat creates interactive XR worlds using JSON data generated by LLMs. Unlikeprior approaches focusing on coding script generation, LLMER translates naturallanguage inputs into JSON data, significantly reducing the likelihood ofapplication crashes and processing latency. It employs a multi-stage strategyto supply only the essential contextual information adapted to the user'srequest and features multiple modules designed for various XR tasks. Ourpreliminary user study reveals the effectiveness of the proposed system, withover 80% reduction in consumed tokens and around 60% reduction in taskcompletion time compared to state-of-the-art approaches. The analysis of users'feedback also illuminates a series of directions for further optimization.</description><author>Jiangong Chen, Xiaoyi Wu, Tian Lan, Bin Li</author><pubDate>Tue, 04 Feb 2025 16:08:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02441v1</guid></item><item><title>SelfFed: Self-Supervised Federated Learning for Data Heterogeneity and Label Scarcity in Medical Images</title><link>http://arxiv.org/abs/2307.01514v3</link><description>Self-supervised learning in the federated learning paradigm has been gaininga lot of interest both in industry and research due to the collaborativelearning capability on unlabeled yet isolated data. However, self-supervisedbased federated learning strategies suffer from performance degradation due tolabel scarcity and diverse data distributions, i.e., data heterogeneity. Inthis paper, we propose the SelfFed framework for medical images to overcomedata heterogeneity and label scarcity issues. The first phase of the SelfFedframework helps to overcome the data heterogeneity issue by leveraging thepre-training paradigm that performs augmentative modeling using SwinTransformer-based encoder in a decentralized manner. The label scarcity issueis addressed by fine-tuning paradigm that introduces a contrastive network anda novel aggregation strategy. We perform our experimental analysis on publiclyavailable medical imaging datasets to show that SelfFed performs better whencompared to existing baselines and works. Our method achieves a maximumimprovement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IIDdatasets. Further, our proposed method outperforms existing baselines even whentrained on a few (10%) labeled instances.</description><author>Sunder Ali Khowaja, Kapal Dev, Syed Muhammad Anwar, Marius George Linguraru</author><pubDate>Tue, 04 Feb 2025 16:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01514v3</guid></item><item><title>Beemo: Benchmark of Expert-edited Machine-generated Outputs</title><link>http://arxiv.org/abs/2411.04032v2</link><description>The rapid proliferation of large language models (LLMs) has increased thevolume of machine-generated texts (MGTs) and blurred text authorship in variousdomains. However, most existing MGT benchmarks include single-author texts(human-written and machine-generated). This conventional design fails tocapture more practical multi-author scenarios, where the user refines the LLMresponse for natural flow, coherence, and factual correctness. Our paperintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),which includes 6.5k texts written by humans, generated by teninstruction-finetuned LLMs, and edited by experts for various use cases,ranging from creative writing to summarization. Beemo additionally comprises13.1k machine-generated and LLM-edited texts, allowing for diverse MGTdetection evaluation across various edit types. We document Beemo's creationprotocol and present the results of benchmarking 33 configurations of MGTdetectors in different experimental setups. We find that expert-based editingevades MGT detection, while LLM-edited texts are unlikely to be recognized ashuman-written. Beemo and all materials are publicly available.</description><author>Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov</author><pubDate>Tue, 04 Feb 2025 16:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04032v2</guid></item><item><title>Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment</title><link>http://arxiv.org/abs/2502.02438v1</link><description>Medical multimodal large language models (MLLMs) are becoming an instrumentalpart of healthcare systems, assisting medical personnel with decision makingand results analysis. Models for radiology report generation are able tointerpret medical imagery, thus reducing the workload of radiologists. Asmedical data is scarce and protected by privacy regulations, medical MLLMsrepresent valuable intellectual property. However, these assets are potentiallyvulnerable to model stealing, where attackers aim to replicate theirfunctionality via black-box access. So far, model stealing for the medicaldomain has focused on classification; however, existing attacks are noteffective against MLLMs. In this paper, we introduce Adversarial DomainAlignment (ADA-STEAL), the first stealing attack against medical MLLMs.ADA-STEAL relies on natural images, which are public and widely available, asopposed to their medical counterparts. We show that data augmentation withadversarial noise is sufficient to overcome the data distribution gap betweennatural images and the domain-specific distribution of the victim MLLM.Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate thatAdversarial Domain Alignment enables attackers to steal the medical MLLMwithout any access to medical data.</description><author>Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, Mario Fritz</author><pubDate>Tue, 04 Feb 2025 16:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02438v1</guid></item></channel></rss>