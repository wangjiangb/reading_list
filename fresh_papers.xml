<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 13 Jul 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DreamWaltz: Make a Scene with Complex 3D Animatable Avatars</title><link>http://arxiv.org/abs/2305.12529v2</link><description>We present DreamWaltz, a novel framework for generating and animating complex3D avatars given text guidance and parametric human body prior. While recentmethods have shown encouraging results for text-to-3D generation of commonobjects, creating high-quality and animatable 3D avatars remains challenging.To create high-quality 3D avatars, DreamWaltz proposes 3D-consistentocclusion-aware Score Distillation Sampling (SDS) to optimize implicit neuralrepresentations with canonical poses. It provides view-aligned supervision via3D-aware skeleton conditioning which enables complex avatar generation withoutartifacts and multiple faces. For animation, our method learns an animatableand generalizable avatar representation which could map arbitrary poses to thecanonical pose representation. Extensive evaluations demonstrate thatDreamWaltz is an effective and robust approach for creating 3D avatars that cantake on complex shapes and appearances as well as novel poses for animation.The proposed framework further enables the creation of complex scenes withdiverse compositions, including avatar-avatar, avatar-object and avatar-sceneinteractions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar andanimation results.</description><author>Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, Lei Zhang</author><pubDate>Wed, 12 Jul 2023 18:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12529v2</guid></item><item><title>Neural Free-Viewpoint Relighting for Glossy Indirect Illumination</title><link>http://arxiv.org/abs/2307.06335v1</link><description>Precomputed Radiance Transfer (PRT) remains an attractive solution forreal-time rendering of complex light transport effects such as glossy globalillumination. After precomputation, we can relight the scene with newenvironment maps while changing viewpoint in real-time. However, practical PRTmethods are usually limited to low-frequency spherical harmonic lighting.All-frequency techniques using wavelets are promising but have so far hadlittle practical impact. The curse of dimensionality and much higher datarequirements have typically limited them to relighting with fixed view or onlydirect lighting with triple product integrals. In this paper, we demonstrate ahybrid neural-wavelet PRT solution to high-frequency indirect illumination,including glossy reflection, for relighting with changing view. Specifically,we seek to represent the light transport function in the Haar wavelet basis.For global illumination, we learn the wavelet transport using a smallmulti-layer perceptron (MLP) applied to a feature field as a function ofspatial location and wavelet index, with reflected direction and materialparameters being other MLP inputs. We optimize/learn the feature field(compactly represented by a tensor decomposition) and MLP parameters frommultiple images of the scene under different lighting and viewing conditions.We demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputedrendering of challenging scenes involving view-dependent reflections and evencaustics.</description><author>Nithin Raghavan, Yan Xiao, Kai-En Lin, Tiancheng Sun, Sai Bi, Zexiang Xu, Tzu-Mao Li, Ravi Ramamoorthi</author><pubDate>Wed, 12 Jul 2023 18:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06335v1</guid></item><item><title>Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation</title><link>http://arxiv.org/abs/2307.06333v1</link><description>Policies often fail due to distribution shift -- changes in the state andreward that occur when a policy is deployed in new environments. Dataaugmentation can increase robustness by making the model invariant totask-irrelevant changes in the agent's observation. However, designers don'tknow which concepts are irrelevant a priori, especially when different endusers have different preferences about how the task is performed. We propose aninteractive framework to leverage feedback directly from the user to identifypersonalized task-irrelevant concepts. Our key idea is to generatecounterfactual demonstrations that allow users to quickly identify possibletask-relevant and irrelevant concepts. The knowledge of task-irrelevantconcepts is then used to perform data augmentation and thus obtain a policyadapted to personalized user objectives. We present experiments validating ourframework on discrete and continuous control tasks with real human users. Ourmethod (1) enables users to better understand agent failure, (2) reduces thenumber of demonstrations required for fine-tuning, and (3) aligns the agent toindividual user task preferences.</description><author>Andi Peng, Aviv Netanyahu, Mark Ho, Tianmin Shu, Andreea Bobu, Julie Shah, Pulkit Agrawal</author><pubDate>Wed, 12 Jul 2023 18:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06333v1</guid></item><item><title>Budgeting Counterfactual for Offline RL</title><link>http://arxiv.org/abs/2307.06328v1</link><description>The main challenge of offline reinforcement learning, where data is limited,arises from a sequence of counterfactual reasoning dilemmas within the realm ofpotential actions: What if we were to choose a different course of action?These circumstances frequently give rise to extrapolation errors, which tend toaccumulate exponentially with the problem horizon. Hence, it becomes crucial toacknowledge that not all decision steps are equally important to the finaloutcome, and to budget the number of counterfactual decisions a policy make inorder to control the extrapolation. Contrary to existing approaches that useregularization on either the policy or value function, we propose an approachto explicitly bound the amount of out-of-distribution actions during training.Specifically, our method utilizes dynamic programming to decide where toextrapolate and where not to, with an upper bound on the decisions differentfrom behavior policy. It balances between the potential for improvement fromtaking out-of-distribution actions and the risk of making errors due toextrapolation. Theoretically, we justify our method by the constrainedoptimality of the fixed point solution to our $Q$ updating rules. Empirically,we show that the overall performance of our method is better than thestate-of-the-art offline RL methods on tasks in the widely-used D4RLbenchmarks.</description><author>Yao Liu, Pratik Chaudhari, Rasool Fakoor</author><pubDate>Wed, 12 Jul 2023 18:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06328v1</guid></item><item><title>Synthesizing Artistic Cinemagraphs from Text</title><link>http://arxiv.org/abs/2307.03190v2</link><description>We introduce Text2Cinemagraph, a fully automated method for creatingcinemagraphs from text descriptions - an especially challenging task whenprompts feature imaginary elements and artistic styles, given the complexity ofinterpreting the semantics and motions of these images. Existing single-imageanimation methods fall short on artistic inputs, and recent text-based videomethods frequently introduce temporal inconsistencies, struggling to keepcertain regions static. To address these challenges, we propose an idea ofsynthesizing image twins from a single text prompt - a pair of an artisticimage and its pixel-aligned corresponding natural-looking twin. While theartistic image depicts the style and appearance detailed in our text prompt,the realistic counterpart greatly simplifies layout and motion analysis.Leveraging existing natural image and video datasets, we can accurately segmentthe realistic image and predict plausible motion given the semanticinformation. The predicted motion can then be transferred to the artistic imageto create the final cinemagraph. Our method outperforms existing approaches increating cinemagraphs for natural landscapes as well as artistic andother-worldly scenes, as validated by automated metrics and user studies.Finally, we demonstrate two extensions: animating existing paintings andcontrolling motion directions using text.</description><author>Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov, Jun-Yan Zhu</author><pubDate>Wed, 12 Jul 2023 18:45:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03190v2</guid></item><item><title>Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network</title><link>http://arxiv.org/abs/2305.12493v5</link><description>Contextual information plays a crucial role in speech recognitiontechnologies and incorporating it into the end-to-end speech recognition modelshas drawn immense interest recently. However, previous deep bias methods lackedexplicit supervision for bias tasks. In this study, we introduce a contextualphrase prediction network for an attention-based deep bias method. This networkpredicts context phrases in utterances using contextual embeddings andcalculates bias loss to assist in the training of the contextualized model. Ourmethod achieved a significant word error rate (WER) reduction across variousend-to-end speech recognition models. Experiments on the LibriSpeech corpusshow that our proposed model obtains a 12.1% relative WER improvement over thebaseline model, and the WER of the context phrases decreases relatively by40.5%. Moreover, by applying a context phrase filtering strategy, we alsoeffectively eliminate the WER degradation when using a larger biasing list.</description><author>Kaixun Huang, Ao Zhang, Zhanheng Yang, Pengcheng Guo, Bingshen Mu, Tianyi Xu, Lei Xie</author><pubDate>Wed, 12 Jul 2023 18:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12493v5</guid></item><item><title>Provably Faster Gradient Descent via Long Steps</title><link>http://arxiv.org/abs/2307.06324v1</link><description>This work establishes provably faster convergence rates for gradient descentvia a computer-assisted analysis technique. Our theory allows nonconstantstepsize policies with frequent long steps potentially violating descent byanalyzing the overall effect of many iterations at once rather than the typicalone-iteration inductions used in most first-order method analyses. We show thatlong steps, which may increase the objective value in the short term, lead toprovably faster convergence in the long term. A conjecture towards proving afaster $O(1/T\log T)$ rate for gradient descent is also motivated along withsimple numerical validation.</description><author>Benjamin Grimmer</author><pubDate>Wed, 12 Jul 2023 18:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06324v1</guid></item><item><title>Deep Learning of Crystalline Defects from TEM images: A Solution for the Problem of "Never Enough Training Data"</title><link>http://arxiv.org/abs/2307.06322v1</link><description>Crystalline defects, such as line-like dislocations, play an important rolefor the performance and reliability of many metallic devices. Their interactionand evolution still poses a multitude of open questions to materials scienceand materials physics. In-situ TEM experiments can provide important insightsinto how dislocations behave and move. During such experiments, the dislocationmicrostructure is captured in form of videos. The analysis of individual videoframes can provide useful insights but is limited by the capabilities ofautomated identification, digitization, and quantitative extraction of thedislocations as curved objects. The vast amount of data also makes manualannotation very time consuming, thereby limiting the use of DeepLearning-based, automated image analysis and segmentation of the dislocationmicrostructure. In this work, a parametric model for generating synthetictraining data for segmentation of dislocations is developed. Even though domainscientists might dismiss synthetic training images sometimes as too artificial,our findings show that they can result in superior performance, particularlyregarding the generalizing of the Deep Learning models with respect todifferent microstructures and imaging conditions. Additionally, we propose anenhanced deep learning method optimized for segmenting overlapping orintersecting dislocation lines. Upon testing this framework on four distinctreal datasets, we find that our synthetic training data are able to yieldhigh-quality results also on real images-even more so if fine-tune on a fewreal images was done.</description><author>Kishan Govind, Daniela Oliveros, Antonin Dlouhy, Marc Legros, Stefan Sandfeld</author><pubDate>Wed, 12 Jul 2023 18:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06322v1</guid></item><item><title>Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2307.06312v1</link><description>Semi-supervised learning has become increasingly popular in medical imagesegmentation due to its ability to leverage large amounts of unlabeled data toextract additional information. However, most existing semi-supervisedsegmentation methods only focus on extracting information from unlabeled data,disregarding the potential of labeled data to further improve the performanceof the model. In this paper, we propose a novel Correlation Aware MutualLearning (CAML) framework that leverages labeled data to guide the extractionof information from unlabeled data. Our approach is based on a mutual learningstrategy that incorporates two modules: the Cross-sample Mutual AttentionModule (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA moduleestablishes dense cross-sample correlations among a group of samples, enablingthe transfer of label prior knowledge to unlabeled data. The OCC moduleconstructs omni-correlations between the unlabeled and labeled datasets andregularizes dual models by constraining the omni-correlation matrix of eachsub-model to be consistent. Experiments on the Atrial Segmentation Challengedataset demonstrate that our proposed approach outperforms state-of-the-artmethods, highlighting the effectiveness of our framework in medical imagesegmentation tasks. The codes, pre-trained weights, and data are publiclyavailable.</description><author>Shengbo Gao, Ziji Zhang, Jiechao Ma, Zihao Li, Shu Zhang</author><pubDate>Wed, 12 Jul 2023 18:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06312v1</guid></item><item><title>Facial Reenactment Through a Personalized Generator</title><link>http://arxiv.org/abs/2307.06307v1</link><description>In recent years, the role of image generative models in facial reenactmenthas been steadily increasing. Such models are usually subject-agnostic andtrained on domain-wide datasets. The appearance of the reenacted individual islearned from a single image, and hence, the entire breadth of the individual'sappearance is not entirely captured, leading these methods to resort tounfaithful hallucination. Thanks to recent advancements, it is now possible totrain a personalized generative model tailored specifically to a givenindividual. In this paper, we propose a novel method for facial reenactmentusing a personalized generator. We train the generator using frames from ashort, yet varied, self-scan video captured using a simple commodity camera.Images synthesized by the personalized generator are guaranteed to preserveidentity. The premise of our work is that the task of reenactment is thusreduced to accurately mimicking head poses and expressions. To this end, welocate the desired frames in the latent space of the personalized generatorusing carefully designed latent optimization. Through extensive evaluation, wedemonstrate state-of-the-art performance for facial reenactment. Furthermore,we show that since our reenactment takes place in a semantic latent space, itcan be semantically edited and stylized in post-processing.</description><author>Ariel Elazary, Yotam Nitzan, Daniel Cohen-Or</author><pubDate>Wed, 12 Jul 2023 18:09:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06307v1</guid></item><item><title>VampNet: Music Generation via Masked Acoustic Token Modeling</title><link>http://arxiv.org/abs/2307.04686v2</link><description>We introduce VampNet, a masked acoustic token modeling approach to musicsynthesis, compression, inpainting, and variation. We use a variable maskingschedule during training which allows us to sample coherent music from themodel by applying a variety of masking approaches (called prompts) duringinference. VampNet is non-autoregressive, leveraging a bidirectionaltransformer architecture that attends to all tokens in a forward pass. Withjust 36 sampling passes, VampNet can generate coherent high-fidelity musicalwaveforms. We show that by prompting VampNet in various ways, we can apply itto tasks like music compression, inpainting, outpainting, continuation, andlooping with variation (vamping). Appropriately prompted, VampNet is capable ofmaintaining style, genre, instrumentation, and other high-level aspects of themusic. This flexible prompting capability makes VampNet a powerful musicco-creation tool. Code and audio samples are available online.</description><author>Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo</author><pubDate>Wed, 12 Jul 2023 18:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04686v2</guid></item><item><title>Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes</title><link>http://arxiv.org/abs/2307.06306v1</link><description>State-of-the-art federated learning algorithms such as FedAvg requirecarefully tuned stepsizes to achieve their best performance. The improvementsproposed by existing adaptive federated methods involve tuning of additionalhyperparameters such as momentum parameters, and consider adaptivity only inthe server aggregation round, but not locally. These methods can be inefficientin many practical scenarios because they require excessive tuning ofhyperparameters and do not capture local geometric information. In this work,we extend the recently proposed stochastic Polyak stepsize (SPS) to thefederated learning setting, and propose new locally adaptive and nearlyparameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove thatFedSPS converges linearly in strongly convex and sublinearly in convex settingswhen the interpolation condition (overparametrization) is satisfied, andconverges to a neighborhood of the solution in the general case. We extend ourproposed method to a decreasing stepsize version FedDecSPS, that converges alsowhen the interpolation condition does not hold. We validate our theoreticalclaims by performing illustrative convex experiments. Our proposed algorithmsmatch the optimization performance of FedAvg with the best tunedhyperparameters in the i.i.d. case, and outperform FedAvg in the non-i.i.d.case.</description><author>Sohom Mukherjee, Nicolas Loizou, Sebastian U. Stich</author><pubDate>Wed, 12 Jul 2023 18:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06306v1</guid></item><item><title>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</title><link>http://arxiv.org/abs/2307.06304v1</link><description>The ubiquitous and demonstrably suboptimal choice of resizing images to afixed resolution before processing them with computer vision models has not yetbeen successfully challenged. However, models such as the Vision Transformer(ViT) offer flexible sequence-based modeling, and hence varying input sequencelengths. We take advantage of this with NaViT (Native Resolution ViT) whichuses sequence packing during training to process inputs of arbitraryresolutions and aspect ratios. Alongside flexible model usage, we demonstrateimproved training efficiency for large-scale supervised and contrastiveimage-text pretraining. NaViT can be efficiently transferred to standard taskssuch as image and video classification, object detection, and semanticsegmentation and leads to improved results on robustness and fairnessbenchmarks. At inference time, the input resolution flexibility can be used tosmoothly navigate the test-time cost-performance trade-off. We believe thatNaViT marks a departure from the standard, CNN-designed, input and modellingpipeline used by most computer vision models, and represents a promisingdirection for ViTs.</description><author>Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lučić, Neil Houlsby</author><pubDate>Wed, 12 Jul 2023 18:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06304v1</guid></item><item><title>Towards a Certified Proof Checker for Deep Neural Network Verification</title><link>http://arxiv.org/abs/2307.06299v1</link><description>Recent developments in deep neural networks (DNNs) have led to their adoptionin safety-critical systems, which in turn has heightened the need forguaranteeing their safety. These safety properties of DNNs can be proven usingtools developed by the verification community. However, these tools arethemselves prone to implementation bugs and numerical stability problems, whichmake their reliability questionable. To overcome this, some verifiers produceproofs of their results which can be checked by a trusted checker. In thiswork, we present a novel implementation of a proof checker for DNNverification. It improves on existing implementations by offering numericalstability and greater verifiability. To achieve this, we leverage two keycapabilities of Imandra, an industrial theorem prover: its support of infiniteprecision real arithmetic and its formal verification infrastructure. So far,we have implemented a proof checker in Imandra, specified its correctnessproperties and started to verify the checker's compliance with them. Ourongoing work focuses on completing the formal verification of the checker andfurther optimizing its performance.</description><author>Remi Desmartin, Omri Isac, Grant Passmore, Kathrin Stark, Guy Katz, Ekaterina Komendantskaya</author><pubDate>Wed, 12 Jul 2023 17:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06299v1</guid></item><item><title>Improved Real-time Image Smoothing with Weak Structures Preserved and High-contrast Details Removed</title><link>http://arxiv.org/abs/2307.06298v1</link><description>Image smoothing is by reducing pixel-wise gradients to smooth out details. Asexisting methods always rely on gradients to determine smoothing manners, it isdifficult to distinguish structures and details to handle distinctively due tothe overlapped ranges of gradients for structures and details. Thus, it isstill challenging to achieve high-quality results, especially on preservingweak structures and removing high-contrast details. In this paper, we addressthis challenge by improving the real-time optimization-based method viaiterative least squares (called ILS). We observe that 1) ILS uses gradients asthe independent variable in its penalty function for determining smoothingmanners, and 2) the framework of ILS can still work for image smoothing when weuse some values instead of gradients in the penalty function. Thus,corresponding to the properties of pixels on structures or not, we compute somevalues to use in the penalty function to determine smoothing manners, and so wecan handle structures and details distinctively, no matter whether theirgradients are high or low. As a result, we can conveniently removehigh-contrast details while preserving weak structures. Moreover, such valuescan be adjusted to accelerate optimization computation, so that we can usefewer iterations than the original ILS method for efficiency. This also reducesthe changes onto structures to help structure preservation. Experimentalresults show our advantages over existing methods on efficiency and quality.</description><author>Shengchun Wang, Wencheng Wang, Fei Hou</author><pubDate>Wed, 12 Jul 2023 17:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06298v1</guid></item><item><title>Uncertain Machine Ethical Decisions Using Hypothetical Retrospection</title><link>http://arxiv.org/abs/2305.01424v2</link><description>We propose the use of the hypothetical retrospection argumentation procedure,developed by Sven Ove Hansson to improve existing approaches to machine ethicalreasoning by accounting for probability and uncertainty from a position ofPhilosophy that resonates with humans. Actions are represented with a branchingset of potential outcomes, each with a state, utility, and either a numeric orpoetic probability estimate. Actions are chosen based on comparisons betweensets of arguments favouring actions from the perspective of their branches,even those branches that led to an undesirable outcome. This use of argumentsallows a variety of philosophical theories for ethical reasoning to be used,potentially in flexible combination with each other. We implement theprocedure, applying consequentialist and deontological ethical theories,independently and concurrently, to an autonomous library system use case. Weintroduce a preliminary framework that seems to meet the varied requirements ofa machine ethics system: versatility under multiple theories and a resonancewith humans that enables transparency and explainability.</description><author>Simon Kolker, Louise Dennis, Ramon Fraga Pereira, Mengwei Xu</author><pubDate>Wed, 12 Jul 2023 17:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01424v2</guid></item><item><title>Probabilistic Counterexample Guidance for Safer Reinforcement Learning (Extended Version)</title><link>http://arxiv.org/abs/2307.04927v2</link><description>Safe exploration aims at addressing the limitations of Reinforcement Learning(RL) in safety-critical scenarios, where failures during trial-and-errorlearning may incur high costs. Several methods exist to incorporate externalknowledge or to use proximal sensor data to limit the exploration of unsafestates. However, reducing exploration risks in unknown environments, where anagent must discover safety threats during exploration, remains challenging. Inthis paper, we target the problem of safe exploration by guiding the trainingwith counterexamples of the safety requirement. Our method abstracts bothcontinuous and discrete state-space systems into compact abstract modelsrepresenting the safety-relevant knowledge acquired by the agent duringexploration. We then exploit probabilistic counterexample generation toconstruct minimal simulation submodels eliciting safety requirement violations,where the agent can efficiently train offline to refine its policy towardsminimising the risk of safety violations during the subsequent onlineexploration. We demonstrate our method's effectiveness in reducing safetyviolations during online exploration in preliminary experiments by an averageof 40.3% compared with QL and DQN standard algorithms and 29.1% compared withprevious related work, while achieving comparable cumulative rewards withrespect to unrestricted exploration and alternative approaches.</description><author>Xiaotong Ji, Antonio Filieri</author><pubDate>Wed, 12 Jul 2023 17:39:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04927v2</guid></item><item><title>Instruction Mining: High-Quality Instruction Data Selection for Large Language Models</title><link>http://arxiv.org/abs/2307.06290v1</link><description>Large language models typically undergo two training stages, pretraining andfinetuning. Despite that large-scale pretraining endows the model with strongcapabilities to generate natural language responses, these pretrained modelscan still fail to understand human instructions at times. To enhance languagemodels' ability of interpreting and responding to instructions, instructionfinetuning has emerged as a critical method in this area. Recent studies foundthat large language models can be finetuned to perform well even with a smallamount of high-quality instruction-following data. However, the selection ofhigh-quality datasets for finetuning language models still lacks clearguidelines to follow. In this paper, we propose InstructMining, a linear rulefor evaluating instruction-following data quality. We formulate InstructMiningusing specific natural language indicators. To investigate the relationshipbetween data quality and these indicators, we further conduct extensivefinetuning experiments. The experiment results are then applied to estimatingparameters in InstructMining. To further investigate its performance, we useInstructMining to select high-quality data from unseen datasets. Resultsdemonstrate that InstructMining can help select relatively high-quality samplesfrom various instruction-following datasets. Compared to models finetuned onunfiltered datasets, models finetuned on InstructMining selected datasetsperform better on 42.5% cases.</description><author>Yihan Cao, Yanbin Kang, Lichao Sun</author><pubDate>Wed, 12 Jul 2023 17:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06290v1</guid></item><item><title>Rational Neural Network Controllers</title><link>http://arxiv.org/abs/2307.06287v1</link><description>Neural networks have shown great success in many machine learning relatedtasks, due to their ability to act as general function approximators. Recentwork has demonstrated the effectiveness of neural networks in control systems(known as neural feedback loops), most notably by using a neural network as acontroller. However, one of the big challenges of this approach is that neuralnetworks have been shown to be sensitive to adversarial attacks. This meansthat, unless they are designed properly, they are not an ideal candidate forcontrollers due to issues with robustness and uncertainty, which are pivotalaspects of control systems. There has been initial work on robustness to bothanalyse and design dynamical systems with neural network controllers. However,one prominent issue with these methods is that they use existing neural networkarchitectures tailored for traditional machine learning tasks. These structuresmay not be appropriate for neural network controllers and it is important toconsider alternative architectures. This paper considers rational neuralnetworks and presents novel rational activation functions, which can be usedeffectively in robustness problems for neural feedback loops. Rationalactivation functions are replaced by a general rational neural networkstructure, which is convex in the neural network's parameters. A method isproposed to recover a stabilising controller from a Sum of Squares feasibilitytest. This approach is then applied to a refined rational neural network whichis more compatible with Sum of Squares programming. Numerical examples showthat this method can successfully recover stabilising rational neural networkcontrollers for neural feedback loops with non-linear plants with noise andparametric uncertainty.</description><author>Matthew Newton, Antonis Papachristodoulou</author><pubDate>Wed, 12 Jul 2023 17:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06287v1</guid></item><item><title>CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments</title><link>http://arxiv.org/abs/2304.06848v2</link><description>Robots operating in real-world environments must reason about possibleoutcomes of stochastic actions and make decisions based on partial observationsof the true world state. A major challenge for making accurate and robustaction predictions is the problem of confounding, which if left untreated canlead to prediction errors. The partially observable Markov decision process(POMDP) is a widely-used framework to model these stochastic andpartially-observable decision-making problems. However, due to a lack ofexplicit causal semantics, POMDP planning methods are prone to confounding biasand thus in the presence of unobserved confounders may produce underperformingpolicies. This paper presents a novel causally-informed extension of "anytimeregularized determinized sparse partially observable tree" (AR-DESPOT), amodern anytime online POMDP planner, using causal modelling and inference toeliminate errors caused by unmeasured confounder variables. We further proposea method to learn offline the partial parameterisation of the causal model forplanning, from ground truth model data. We evaluate our methods on a toyproblem with an unobserved confounder and show that the learned causal model ishighly accurate, while our planning method is more robust to confounding andproduces overall higher performing policies than AR-DESPOT.</description><author>Ricardo Cannizzaro, Lars Kunze</author><pubDate>Wed, 12 Jul 2023 17:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06848v2</guid></item><item><title>Evaluating Human-Language Model Interaction</title><link>http://arxiv.org/abs/2212.09746v3</link><description>Many real-world applications of language models (LMs), such as writingassistance and code autocomplete, involve human-LM interaction. However, mostbenchmarks are non-interactive in that a model produces output without humaninvolvement. To evaluate human-LM interaction, we develop a new framework,Human-AI Language-based Interaction Evaluation (HALIE), that defines thecomponents of interactive systems and dimensions to consider when designingevaluation metrics. Compared to standard, non-interactive evaluation, HALIEcaptures (i) the interactive process, not only the final output; (ii) thefirst-person subjective experience, not just a third-party assessment; and(iii) notions of preference beyond quality (e.g., enjoyment and ownership). Wethen design five tasks to cover different forms of interaction: socialdialogue, question answering, crossword puzzles, summarization, and metaphorgeneration. With four state-of-the-art LMs (three variants of OpenAI's GPT-3and AI21 Labs' Jurassic-1), we find that better non-interactive performancedoes not always translate to better human-LM interaction. In particular, wehighlight three cases where the results from non-interactive and interactivemetrics diverge and underscore the importance of human-LM interaction for LMevaluation.</description><author>Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E. Wang, Minae Kwon, Joon Sung Park, Hancheng Cao, Tony Lee, Rishi Bommasani, Michael Bernstein, Percy Liang</author><pubDate>Wed, 12 Jul 2023 17:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09746v3</guid></item><item><title>Tackling Computational Heterogeneity in FL: A Few Theoretical Insights</title><link>http://arxiv.org/abs/2307.06283v1</link><description>The future of machine learning lies in moving data collection along withtraining to the edge. Federated Learning, for short FL, has been recentlyproposed to achieve this goal. The principle of this approach is to aggregatemodels learned over a large number of distributed clients, i.e.,resource-constrained mobile devices that collect data from their environment,to obtain a new more general model. The latter is subsequently redistributed toclients for further training. A key feature that distinguishes federatedlearning from data-center-based distributed training is the inherentheterogeneity. In this work, we introduce and analyse a novel aggregationframework that allows for formalizing and tackling computational heterogeneityin federated optimization, in terms of both heterogeneous data and localupdates. Proposed aggregation algorithms are extensively analyzed from atheoretical, and an experimental prospective.</description><author>Adnan Ben Mansour, Gaia Carenini, Alexandre Duplessis</author><pubDate>Wed, 12 Jul 2023 17:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06283v1</guid></item><item><title>MMBench: Is Your Multi-modal Model an All-around Player?</title><link>http://arxiv.org/abs/2307.06281v1</link><description>Large vision-language models have recently achieved remarkable progress,exhibiting great perception and reasoning abilities concerning visualinformation. However, how to effectively evaluate these large vision-languagemodels remains a major obstacle, hindering future model development.Traditional benchmarks like VQAv2 or COCO Caption provide quantitativeperformance measurements but suffer from a lack of fine-grained abilityassessment and non-robust evaluation metrics. Recent subjective benchmarks,such as OwlEval, offer comprehensive evaluations of a model's abilities byincorporating human labor, but they are not scalable and display significantbias. In response to these challenges, we propose MMBench, a novelmulti-modality benchmark. MMBench methodically develops a comprehensiveevaluation pipeline, primarily comprised of two elements. The first element isa meticulously curated dataset that surpasses existing similar benchmarks interms of the number and variety of evaluation questions and abilities. Thesecond element introduces a novel CircularEval strategy and incorporates theuse of ChatGPT. This implementation is designed to convert free-formpredictions into pre-defined choices, thereby facilitating a more robustevaluation of the model's predictions. MMBench is a systematically-designedobjective benchmark for robustly evaluating the various abilities ofvision-language models. We hope MMBench will assist the research community inbetter evaluating their models and encourage future advancements in thisdomain. Project page: https://opencompass.org.cn/mmbench.</description><author>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin</author><pubDate>Wed, 12 Jul 2023 17:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06281v1</guid></item><item><title>Stochastic Light Field Holography</title><link>http://arxiv.org/abs/2307.06277v1</link><description>The Visual Turing Test is the ultimate goal to evaluate the realism ofholographic displays. Previous studies have focused on addressing challengessuch as limited \'etendue and image quality over a large focal volume, but theyhave not investigated the effect of pupil sampling on the viewing experience infull 3D holograms. In this work, we tackle this problem with a novel hologramgeneration algorithm motivated by matching the projection operators ofincoherent Light Field and coherent Wigner Function light transport. To thisend, we supervise hologram computation using synthesized photographs, which arerendered on-the-fly using Light Field refocusing from stochastically sampledpupil states during optimization. The proposed method produces holograms withcorrect parallax and focus cues, which are important for passing the VisualTuring Test. We validate that our approach compares favorably tostate-of-the-art CGH algorithms that use Light Field and Focal Stacksupervision. Our experiments demonstrate that our algorithm significantlyimproves the realism of the viewing experience for a variety of different pupilstates.</description><author>Florian Schiffers, Praneeth Chakravarthula, Nathan Matsuda, Grace Kuo, Ethan Tseng, Douglas Lanman, Felix Heide, Oliver Cossairt</author><pubDate>Wed, 12 Jul 2023 17:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06277v1</guid></item><item><title>Exposing the Fake: Effective Diffusion-Generated Images Detection</title><link>http://arxiv.org/abs/2307.06272v1</link><description>Image synthesis has seen significant advancements with the advent ofdiffusion-based generative models like Denoising Diffusion Probabilistic Models(DDPM) and text-to-image diffusion models. Despite their efficacy, there is adearth of research dedicated to detecting diffusion-generated images, whichcould pose potential security and privacy risks. This paper addresses this gapby proposing a novel detection method called Stepwise Error forDiffusion-generated Image Detection (SeDID). Comprising statistical-based$\text{SeDID}_{\text{Stat}}$ and neural network-based$\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusionmodels, namely deterministic reverse and deterministic denoising computationerrors. Our evaluations demonstrate SeDID's superior performance over existingmethods when applied to diffusion models. Thus, our work makes a pivotalcontribution to distinguishing diffusion model-generated images, marking asignificant step in the domain of artificial intelligence security.</description><author>Ruipeng Ma, Jinhao Duan, Fei Kong, Xiaoshuang Shi, Kaidi Xu</author><pubDate>Wed, 12 Jul 2023 17:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06272v1</guid></item><item><title>Local Grammar-Based Coding Revisited</title><link>http://arxiv.org/abs/2209.13636v2</link><description>We revisit the problem of minimal local grammar-based coding. In thissetting, the local grammar encoder encodes grammars symbol by symbol, whereasthe minimal grammar transform minimizes the grammar length in a preset class ofgrammars as given by the length of local grammar encoding. It has been knownthat such minimal codes are strongly universal for a strictly positive entropyrate, whereas the number of rules in the minimal grammar constitutes an upperbound for the mutual information of the source. Whereas the fully minimal codeis likely intractable, the constrained minimal block code can be efficientlycomputed. In this article, we present a new, simpler, and more general proof ofstrong universality of the minimal block code, regardless of the entropy rate.The proof is based on a simple Zipfian bound for ranked probabilities. By theway, we also show empirically that the number of rules in the minimal blockcode cannot clearly discriminate between long-memory and memoryless sources,such as a text in English and a random permutation of its characters. Thiscontradicts our previous expectations.</description><author>Łukasz Dębowski</author><pubDate>Wed, 12 Jul 2023 17:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.13636v2</guid></item><item><title>Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya</title><link>http://arxiv.org/abs/2305.19779v2</link><description>Model-based disease mapping remains a fundamental policy-informing tool inpublic health and disease surveillance. Hierarchical Bayesian models havebecome the state-of-the-art approach for disease mapping since they are able tocapture structure in the data, as well as to characterise uncertainty. Whenworking with areal data, e.g.~aggregates at the administrative unit level suchas district or province, routinely used models rely on the adjacency structureof areal units to account for spatial correlations. The goal of diseasesurveillance systems is to track disease outcomes over time. This task provideschallenging in situations of crises, such as political changes, leading tochanges of administrative boundaries. Kenya is an example of a country wherechange of boundaries took place in 2010. Moreover, the adjacency-based approachignores the continuous nature of spatial processes and cannot solve thechange-of-support problem, i.e.~when administrative boundaries change or whenestimates must be produced at a different administrative level. We present anovel, practical, and easy to implement solution relying on a methodologycombining deep generative modelling and fully Bayesian inference: we build onthe recently proposed PriorVAE method able to encode spatial priors over smallareas with variational autoencoders, to map malaria prevalence in Kenya.</description><author>Elizaveta Semenova, Swapnil Mishra, Samir Bhatt, Seth Flaxman, H Juliette T Unwin</author><pubDate>Wed, 12 Jul 2023 17:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19779v2</guid></item><item><title>Physics-informed Machine Learning for Calibrating Macroscopic Traffic Flow Models</title><link>http://arxiv.org/abs/2307.06267v1</link><description>Well-calibrated traffic flow models are fundamental to understanding trafficphenomena and designing control strategies. Traditional calibration has beendeveloped base on optimization methods. In this paper, we propose a novelphysics-informed, learning-based calibration approach that achievesperformances comparable to and even better than those of optimization-basedmethods. To this end, we combine the classical deep autoencoder, anunsupervised machine learning model consisting of one encoder and one decoder,with traffic flow models. Our approach informs the decoder of the physicaltraffic flow models and thus induces the encoder to yield reasonable trafficparameters given flow and speed measurements. We also introduce the denoisingautoencoder into our method so that it can handles not only with normal databut also with corrupted data with missing values. We verified our approach witha case study of I-210 E in California.</description><author>Yu Tang, Li Jin, Kaan Ozbay</author><pubDate>Wed, 12 Jul 2023 17:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06267v1</guid></item><item><title>From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning</title><link>http://arxiv.org/abs/2302.12559v3</link><description>We study differentially private (DP) machine learning algorithms as instancesof noisy fixed-point iterations, in order to derive privacy and utility resultsfrom this well-studied framework. We show that this new perspective recoverspopular private gradient-based methods like DP-SGD and provides a principledway to design and analyze new private optimization algorithms in a flexiblemanner. Focusing on the widely-used Alternating Directions Method ofMultipliers (ADMM) method, we use our general framework to derive novel privateADMM algorithms for centralized, federated and fully decentralized learning.For these three algorithms, we establish strong privacy guarantees leveragingprivacy amplification by iteration and by subsampling. Finally, we provideutility guarantees using a unified analysis that exploits a recent linearconvergence result for noisy fixed-point iterations.</description><author>Edwige Cyffers, Aurélien Bellet, Debabrota Basu</author><pubDate>Wed, 12 Jul 2023 17:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12559v3</guid></item><item><title>Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds</title><link>http://arxiv.org/abs/2302.02988v2</link><description>We investigate the problem of fixed-budget best arm identification (BAI) forminimizing expected simple regret. In an adaptive experiment, a decision makerdraws one of multiple treatment arms based on past observations and observesthe outcome of the drawn arm. After the experiment, the decision makerrecommends the treatment arm with the highest expected outcome. We evaluate thedecision based on the expected simple regret, which is the difference betweenthe expected outcomes of the best arm and the recommended arm. Due to inherentuncertainty, we evaluate the regret using the minimax criterion. First, wederive asymptotic lower bounds for the worst-case expected simple regret, whichare characterized by the variances of potential outcomes (leading factor).Based on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder(HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) inrecommending the best arm. Our theoretical analysis shows that the TS-HIRstrategy is asymptotically minimax optimal, meaning that the leading factor ofits worst-case expected simple regret matches our derived worst-case lowerbound. Additionally, we consider extensions of our method, such as theasymptotic optimality for the probability of misidentification. Finally, wevalidate the proposed method's effectiveness through simulations.</description><author>Masahiro Kato, Masaaki Imaizumi, Takuya Ishihara, Toru Kitagawa</author><pubDate>Wed, 12 Jul 2023 17:06:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02988v2</guid></item><item><title>On the hierarchical Bayesian modelling of frequency response functions</title><link>http://arxiv.org/abs/2307.06263v1</link><description>Population-based structural health monitoring (PBSHM) aims to share valuableinformation among members of a population, such as normal- and damage-conditiondata, to improve inferences regarding the health states of the members. Evenwhen the population is comprised of nominally-identical structures, benignvariations among the members will exist as a result of slight differences inmaterial properties, geometry, boundary conditions, or environmental effects(e.g., temperature changes). These discrepancies can affect modal propertiesand present as changes in the characteristics of the resonance peaks of thefrequency response function (FRF). Many SHM strategies depend on monitoring thedynamic properties of structures, so benign variations can be challenging forthe practical implementation of these systems. Another common challenge withvibration-based SHM is data loss, which may result from transmission issues,sensor failure, a sample-rate mismatch between sensors, and other causes.Missing data in the time domain will result in decreased resolution in thefrequency domain, which can impair dynamic characterisation. The hierarchicalBayesian approach provides a useful modelling structure for PBSHM, becausestatistical distributions at the population and individual (or domain) levelare learnt simultaneously to bolster statistical strength among the parameters.As a result, variance is reduced among the parameter estimates, particularlywhen data are limited. In this paper, combined probabilistic FRF models aredeveloped for a small population of nominally-identical helicopter blades undervarying temperature conditions, using a hierarchical Bayesian structure. Thesemodels address critical challenges in SHM, by accommodating benign variationsthat present as differences in the underlying dynamics, while also considering(and utilising), the similarities among the blades.</description><author>T. A. Dardeno, R. S. Mills, N. Dervilis, K. Worden, L. A. Bull</author><pubDate>Wed, 12 Jul 2023 17:03:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06263v1</guid></item><item><title>UGCANet: A Unified Global Context-Aware Transformer-based Network with Feature Alignment for Endoscopic Image Analysis</title><link>http://arxiv.org/abs/2307.06260v1</link><description>Gastrointestinal endoscopy is a medical procedure that utilizes a flexibletube equipped with a camera and other instruments to examine the digestivetract. This minimally invasive technique allows for diagnosing and managingvarious gastrointestinal conditions, including inflammatory bowel disease,gastrointestinal bleeding, and colon cancer. The early detection andidentification of lesions in the upper gastrointestinal tract and theidentification of malignant polyps that may pose a risk of cancer developmentare critical components of gastrointestinal endoscopy's diagnostic andtherapeutic applications. Therefore, enhancing the detection rates ofgastrointestinal disorders can significantly improve a patient's prognosis byincreasing the likelihood of timely medical intervention, which may prolong thepatient's lifespan and improve overall health outcomes. This paper presents anovel Transformer-based deep neural network designed to perform multiple taskssimultaneously, thereby enabling accurate identification of both uppergastrointestinal tract lesions and colon polyps. Our approach proposes a uniqueglobal context-aware module and leverages the powerful MiT backbone, along witha feature alignment block, to enhance the network's representation capability.This novel design leads to a significant improvement in performance acrossvarious endoscopic diagnosis tasks. Extensive experiments demonstrate thesuperior performance of our method compared to other state-of-the-artapproaches.</description><author>Pham Vu Hung, Nguyen Duy Manh, Nguyen Thi Oanh, Nguyen Thi Thuy, Dinh Viet Sang</author><pubDate>Wed, 12 Jul 2023 17:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06260v1</guid></item><item><title>DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks</title><link>http://arxiv.org/abs/2303.04878v2</link><description>Deep neural networks (DNNs) are widely used in various application domainssuch as image processing, speech recognition, and natural language processing.However, testing DNN models may be challenging due to the complexity and sizeof their input domain. Particularly, testing DNN models often requiresgenerating or exploring large unlabeled datasets. In practice, DNN testoracles, which identify the correct outputs for inputs, often require expensivemanual effort to label test data, possibly involving multiple experts to ensurelabeling correctness. In this paper, we propose DeepGD, a black-boxmulti-objective test selection approach for DNN models. It reduces the cost oflabeling by prioritizing the selection of test inputs with high fault revealingpower from large unlabeled datasets. DeepGD not only selects test inputs withhigh uncertainty scores to trigger as many mispredicted inputs as possible butalso maximizes the probability of revealing distinct faults in the DNN model byselecting diverse mispredicted inputs. The experimental results conducted onfour widely used datasets and five DNN models show that in terms offault-revealing ability: (1) White-box, coverage-based approaches fare poorly,(2) DeepGD outperforms existing black-box test selection approaches in terms offault detection, and (3) DeepGD also leads to better guidance for DNN modelretraining when using selected inputs to augment the training set.</description><author>Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah, Lionel Briand</author><pubDate>Wed, 12 Jul 2023 16:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04878v2</guid></item><item><title>Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat</title><link>http://arxiv.org/abs/2307.05350v2</link><description>ML model design either starts with an interpretable model or a Blackbox andexplains it post hoc. Blackbox models are flexible but difficult to explain,while interpretable models are inherently explainable. Yet, interpretablemodels require extensive ML knowledge and tend to be less flexible andunderperforming than their Blackbox variants. This paper aims to blur thedistinction between a post hoc explanation of a Blackbox and constructinginterpretable models. Beginning with a Blackbox, we iteratively carve out amixture of interpretable experts (MoIE) and a residual network. Eachinterpretable model specializes in a subset of samples and explains them usingFirst Order Logic (FOL), providing basic reasoning on concepts from theBlackbox. We route the remaining samples through a flexible residual. We repeatthe method on the residual network until all the interpretable models explainthe desired proportion of data. Our extensive experiments show that our route,interpret, and repeat approach (1) identifies a diverse set ofinstance-specific concepts with high concept completeness via MoIE withoutcompromising in performance, (2) identifies the relatively ``harder'' samplesto explain via residuals, (3) outperforms the interpretable by-design models bysignificant margins during test-time interventions, and (4) fixes the shortcutlearned by the original Blackbox. The code for MoIE is publicly available at:\url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}</description><author>Shantanu Ghosh, Ke Yu, Forough Arabshahi, Kayhan Batmanghelich</author><pubDate>Wed, 12 Jul 2023 16:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05350v2</guid></item><item><title>Machine learning and Topological data analysis identify unique features of human papillae in 3D scans</title><link>http://arxiv.org/abs/2307.06255v1</link><description>The tongue surface houses a range of papillae that are integral to themechanics and chemistry of taste and textural sensation. Although gustatoryfunction of papillae is well investigated, the uniqueness of papillae withinand across individuals remains elusive. Here, we present the first machinelearning framework on 3D microscopic scans of human papillae (n = 2092),uncovering the uniqueness of geometric and topological features of papillae.The finer differences in shapes of papillae are investigated computationallybased on a number of features derived from discrete differential geometry andcomputational topology. Interpretable machine learning techniques show thatpersistent homology features of the papillae shape are the most effective inpredicting the biological variables. Models trained on these features withsmall volumes of data samples predict the type of papillae with an accuracy of85%. The papillae type classification models can map the spatial arrangement offiliform and fungiform papillae on a surface. Remarkably, the papillae arefound to be distinctive across individuals and an individual can be identifiedwith an accuracy of 48% among the 15 participants from a single papillae.Collectively, this is the first unprecedented evidence demonstrating thattongue papillae can serve as a unique identifier inspiring new researchdirection for food preferences and oral diagnostics.</description><author>Rayna Andreeva, Anwesha Sarkar, Rik Sarkar</author><pubDate>Wed, 12 Jul 2023 16:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06255v1</guid></item><item><title>Latent Graph Attention for Enhanced Spatial Context</title><link>http://arxiv.org/abs/2307.04149v2</link><description>Global contexts in images are quite valuable in image-to-image translationproblems. Conventional attention-based and graph-based models capture theglobal context to a large extent, however, these are computationally expensive.Moreover, the existing approaches are limited to only learning the pairwisesemantic relation between any two points on the image. In this paper, wepresent Latent Graph Attention (LGA) a computationally inexpensive (linear tothe number of nodes) and stable, modular framework for incorporating the globalcontext in the existing architectures, especially empowering small-scalearchitectures to give performance closer to large size architectures, thusmaking the light-weight architectures more useful for edge devices with lowercompute power and lower energy needs. LGA propagates information spatiallyusing a network of locally connected graphs, thereby facilitating to constructa semantically coherent relation between any two spatially distant points thatalso takes into account the influence of the intermediate pixels. Moreover, thedepth of the graph network can be used to adapt the extent of contextual spreadto the target dataset, thereby being able to explicitly control the addedcomputational cost. To enhance the learning mechanism of LGA, we also introducea novel contrastive loss term that helps our LGA module to couple well with theoriginal architecture at the expense of minimal additional computational load.We show that incorporating LGA improves the performance on three challengingapplications, namely transparent object segmentation, image restoration fordehazing and optical flow estimation.</description><author>Ayush Singh, Yash Bhambhu, Himanshu Buckchash, Deepak K. Gupta, Dilip K. Prasad</author><pubDate>Wed, 12 Jul 2023 16:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04149v2</guid></item><item><title>Efficient Dynamics Modeling in Interactive Environments with Koopman Theory</title><link>http://arxiv.org/abs/2306.11941v2</link><description>The accurate modeling of dynamics in interactive environments is critical forsuccessful long-range prediction. Such a capability could advance ReinforcementLearning (RL) and Planning algorithms, but achieving it is challenging.Inaccuracies in model estimates can compound, resulting in increased errorsover long horizons. We approach this problem from the lens of Koopman theory,where the nonlinear dynamics of the environment can be linearized in ahigh-dimensional latent space. This allows us to efficiently parallelize thesequential problem of long-range prediction using convolution, while accountingfor the agent's action at every time step. Our approach also enables stabilityanalysis and better control over gradients through time. Taken together, theseadvantages result in significant improvement over the existing approaches, bothin the efficiency and the accuracy of modeling dynamics over extended horizons.We also report promising experimental results in dynamics modeling for thescenarios of both model-based planning and model-free RL.</description><author>Arnab Kumar Mondal, Siba Smarak Panigrahi, Sai Rajeswar, Kaleem Siddiqi, Siamak Ravanbakhsh</author><pubDate>Wed, 12 Jul 2023 16:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11941v2</guid></item><item><title>A Survey on Evaluation of Large Language Models</title><link>http://arxiv.org/abs/2307.03109v3</link><description>Large language models (LLMs) are gaining increasing popularity in bothacademia and industry, owing to their unprecedented performance in variousapplications. As LLMs continue to play a vital role in both research and dailyuse, their evaluation becomes increasingly critical, not only at the tasklevel, but also at the society level for better understanding of theirpotential risks. Over the past years, significant efforts have been made toexamine LLMs from various perspectives. This paper presents a comprehensivereview of these evaluation methods for LLMs, focusing on three key dimensions:what to evaluate, where to evaluate, and how to evaluate. Firstly, we providean overview from the perspective of evaluation tasks, encompassing generalnatural language processing tasks, reasoning, medical usage, ethics,educations, natural and social sciences, agent applications, and other areas.Secondly, we answer the `where' and `how' questions by diving into theevaluation methods and benchmarks, which serve as crucial components inassessing performance of LLMs. Then, we summarize the success and failure casesof LLMs in different tasks. Finally, we shed light on several future challengesthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights toresearchers in the realm of LLMs evaluation, thereby aiding the development ofmore proficient LLMs. Our key point is that evaluation should be treated as anessential discipline to better assist the development of LLMs. We consistentlymaintain the related open-source materials at:https://github.com/MLGroupJLU/LLM-eval-survey.</description><author>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie</author><pubDate>Wed, 12 Jul 2023 16:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03109v3</guid></item><item><title>MARBLE: Music Audio Representation Benchmark for Universal Evaluation</title><link>http://arxiv.org/abs/2306.10548v3</link><description>In the era of extensive intersection between art and Artificial Intelligence(AI), such as image generation and fiction co-creation, AI for music remainsrelatively nascent, particularly in music understanding. This is evident in thelimited work on deep music representations, the scarcity of large-scaledatasets, and the absence of a universal and community-driven benchmark. Toaddress this issue, we introduce the Music Audio Representation Benchmark foruniversaL Evaluation, termed MARBLE. It aims to provide a benchmark for variousMusic Information Retrieval (MIR) tasks by defining a comprehensive taxonomywith four hierarchy levels, including acoustic, performance, score, andhigh-level description. We then establish a unified protocol based on 14 taskson 8 public-available datasets, providing a fair and standard assessment ofrepresentations of all open-sourced pre-trained models developed on musicrecordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, andreproducible suite for the community, with a clear statement on copyrightissues on datasets. Results suggest recently proposed large-scale pre-trainedmusical language models perform the best in most tasks, with room for furtherimprovement. The leaderboard and toolkit repository are published athttps://marble-bm.shef.ac.uk to promote future music AI research.</description><author>Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu</author><pubDate>Wed, 12 Jul 2023 16:40:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10548v3</guid></item><item><title>Identifiability Guarantees for Causal Disentanglement from Soft Interventions</title><link>http://arxiv.org/abs/2307.06250v1</link><description>Causal disentanglement aims to uncover a representation of data using latentvariables that are interrelated through a causal model. Such a representationis identifiable if the latent model that explains the data is unique. In thispaper, we focus on the scenario where unpaired observational and interventionaldata are available, with each intervention changing the mechanism of a latentvariable. When the causal variables are fully observed, statisticallyconsistent algorithms have been developed to identify the causal model underfaithfulness assumptions. We here show that identifiability can still beachieved with unobserved causal variables, given a generalized notion offaithfulness. Our results guarantee that we can recover the latent causal modelup to an equivalence class and predict the effect of unseen combinations ofinterventions, in the limit of infinite data. We implement our causaldisentanglement framework by developing an autoencoding variational Bayesalgorithm and apply it to the problem of predicting combinatorial perturbationeffects in genomics.</description><author>Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, Caroline Uhler</author><pubDate>Wed, 12 Jul 2023 16:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06250v1</guid></item><item><title>Diffusion Based Multi-Agent Adversarial Tracking</title><link>http://arxiv.org/abs/2307.06244v1</link><description>Target tracking plays a crucial role in real-world scenarios, particularly indrug-trafficking interdiction, where the knowledge of an adversarial target'slocation is often limited. Improving autonomous tracking systems will enableunmanned aerial, surface, and underwater vehicles to better assist ininterdicting smugglers that use manned surface, semi-submersible, and aerialvessels. As unmanned drones proliferate, accurate autonomous target estimationis even more crucial for security and safety. This paper presents ConstrainedAgent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approachaimed at generating comprehensive predictions of adversary locations byleveraging past sparse state information. To assess the effectiveness of thisapproach, we evaluate predictions on single-target and multi-target pursuitenvironments, employing Monte-Carlo sampling of the diffusion model to estimatethe probability associated with each generated trajectory. We propose a novelcross-attention based diffusion model that utilizes constraint-based samplingto generate multimodal track hypotheses. Our single-target model surpasses theperformance of all baseline methods on Average Displacement Error (ADE) forpredictions across all time horizons.</description><author>Sean Ye, Manisha Natarajan, Zixuan Wu, Matthew Gombolay</author><pubDate>Wed, 12 Jul 2023 16:34:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06244v1</guid></item><item><title>Reconstructing Spatiotemporal Data with C-VAEs</title><link>http://arxiv.org/abs/2307.06243v1</link><description>The continuous representation of spatiotemporal data commonly relies on usingabstract data types, such as \textit{moving regions}, to represent entitieswhose shape and position continuously change over time. Creating thisrepresentation from discrete snapshots of real-world entities requires usinginterpolation methods to compute in-between data representations and estimatethe position and shape of the object of interest at arbitrary temporal points.Existing region interpolation methods often fail to generate smooth andrealistic representations of a region's evolution. However, recent advancementsin deep learning techniques have revealed the potential of deep models trainedon discrete observations to capture spatiotemporal dependencies throughimplicit feature learning. In this work, we explore the capabilities of Conditional VariationalAutoencoder (C-VAE) models to generate smooth and realistic representations ofthe spatiotemporal evolution of moving regions. We evaluate our proposedapproach on a sparsely annotated dataset on the burnt area of a forest fire. Weapply compression operations to sample from the dataset and use the C-VAE modeland other commonly used interpolation algorithms to generate in-between regionrepresentations. To evaluate the performance of the methods, we compare theirinterpolation results with manually annotated data and regions generated by aU-Net model. We also assess the quality of generated data considering temporalconsistency metrics. The proposed C-VAE-based approach demonstrates competitive results ingeometric similarity metrics. It also exhibits superior temporal consistency,suggesting that C-VAE models may be a viable alternative to modelling thespatiotemporal evolution of 2D moving regions.</description><author>Tiago F. R. Ribeiro, Fernando Silva, Rogério Luís de C. Costa</author><pubDate>Wed, 12 Jul 2023 16:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06243v1</guid></item><item><title>DSSE: a drone swarm search environment</title><link>http://arxiv.org/abs/2307.06240v1</link><description>The Drone Swarm Search project is an environment, based on PettingZoo, thatis to be used in conjunction with multi-agent (or single-agent) reinforcementlearning algorithms. It is an environment in which the agents (drones), have tofind the targets (shipwrecked people). The agents do not know the position ofthe target and do not receive rewards related to their own distance to thetarget(s). However, the agents receive the probabilities of the target(s) beingin a certain cell of the map. The aim of this project is to aid in the study ofreinforcement learning algorithms that require dynamic probabilities as inputs.</description><author>Manuel Castanares, Luis F. S. Carrete, Enrico F. Damiani, Leonardo D. M. de Abreu, José Fernando B. Brancalion, Fabrício J. Barth</author><pubDate>Wed, 12 Jul 2023 16:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06240v1</guid></item><item><title>Unified Molecular Modeling via Modality Blending</title><link>http://arxiv.org/abs/2307.06235v1</link><description>Self-supervised molecular representation learning is critical formolecule-based tasks such as AI-assisted drug discovery. Recent studiesconsider leveraging both 2D and 3D information for representation learning,with straightforward alignment strategies that treat each modality separately.In this work, we introduce a novel "blend-then-predict" self-supervisedlearning method (MoleBLEND), which blends atom relations from differentmodalities into one unified relation matrix for encoding, then recoversmodality-specific information for both 2D and 3D structures. By treating atomrelationships as anchors, seemingly dissimilar 2D and 3D manifolds are alignedand integrated at fine-grained relation-level organically. Extensiveexperiments show that MoleBLEND achieves state-of-the-art performance acrossmajor 2D/3D benchmarks. We further provide theoretical insights from theperspective of mutual-information maximization, demonstrating that our methodunifies contrastive, generative (inter-modal prediction) and mask-then-predict(intra-modal prediction) objectives into a single cohesive blend-then-predictframework.</description><author>Qiying Yu, Yudi Zhang, Yuyan Ni, Shikun Feng, Yanyan Lan, Hao Zhou, Jingjing Liu</author><pubDate>Wed, 12 Jul 2023 16:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06235v1</guid></item><item><title>On the Importance of Denoising when Learning to Compress Images</title><link>http://arxiv.org/abs/2307.06233v1</link><description>Image noise is ubiquitous in photography. However, image noise is notcompressible nor desirable, thus attempting to convey the noise in compressedimage bitstreams yields sub-par results in both rate and distortion. We proposeto explicitly learn the image denoising task when training a codec. Therefore,we leverage the Natural Image Noise Dataset, which offers a wide variety ofscenes captured with various ISO numbers, leading to different noise levels,including insignificant ones. Given this training set, we supervise the codecwith noisy-clean image pairs, and show that a single model trained based on amixture of images with variable noise levels appears to yield best-in-classresults with both noisy and clean images, achieving better rate-distortion thana compression-only model or even than a pair of denoising-then-compressionmodels with almost one order of magnitude fewer GMac operations.</description><author>Benoit Brummer, Christophe De Vleeschouwer</author><pubDate>Wed, 12 Jul 2023 16:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06233v1</guid></item><item><title>Structured mutation inspired by evolutionary theory enriches population performance and diversity</title><link>http://arxiv.org/abs/2302.00559v2</link><description>Grammar-Guided Genetic Programming (GGGP) employs a variety of insights fromevolutionary theory to autonomously design solutions for a given task. Recentinsights from evolutionary biology can lead to further improvements in GGGPalgorithms. In this paper, we apply principles from the theory of FacilitatedVariation and knowledge about heterogeneous mutation rates and mutation effectsto improve the variation operators. We term this new method of variationFacilitated Mutation (FM). We test FM performance on the evolution of neuralnetwork optimizers for image classification, a relevant task in evolutionarycomputation, with important implications for the field of machine learning. Wecompare FM and FM combined with crossover (FMX) against a typical mutationregime to assess the benefits of the approach. We find that FMX in particularprovides statistical improvements in key metrics, creating a superior optimizeroverall (+0.48\% average test accuracy), improving the average quality ofsolutions (+50\% average population fitness), and discovering more diversehigh-quality behaviors (+400 high-quality solutions discovered per run onaverage). Additionally, FM and FMX can reduce the number of fitness evaluationsin an evolutionary run, reducing computational costs in some scenarios.</description><author>Stefano Tiso, Pedro Carvalho, Nuno Lourenço, Penousal Machado</author><pubDate>Wed, 12 Jul 2023 16:13:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00559v2</guid></item><item><title>Analytical reconstructions of full-scan multiple source-translation computed tomography under large field of views</title><link>http://arxiv.org/abs/2305.19767v3</link><description>This paper is to investigate the high-quality analytical reconstructions ofmultiple source-translation computed tomography (mSTCT) under an extended fieldof view (FOV). Under the larger FOVs, the previously proposed backprojectionfiltration (BPF) algorithms for mSTCT, including D-BPF and S-BPF (theirdifferences are different derivate directions along the detector and source,respectively), make some errors and artifacts in the reconstructed images dueto a backprojection weighting factor and the half-scan mode, which deviatesfrom the intention of mSTCT imaging. In this paper, to achieve reconstructionwith as little error as possible under the extremely extended FOV, we combinethe full-scan mSTCT (F-mSTCT) geometry with the previous BPF algorithms tostudy the performance and derive a suitable redundancy-weighted function forF-mSTCT. The experimental results indicate FS-BPF can get high-quality, stableimages under the extremely extended FOV of imaging a large object, though itrequires more projections than FD-BPF. Finally, for different practicalrequirements in extending FOV imaging, we give suggestions on algorithmselection.</description><author>Zhisheng Wang, Yue Liu, Shunli Wang, Xingyuan Bian, Zongfeng Li, Junning Cui</author><pubDate>Wed, 12 Jul 2023 16:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19767v3</guid></item><item><title>spred: Solving $L_1$ Penalty with SGD</title><link>http://arxiv.org/abs/2210.01212v5</link><description>We propose to minimize a generic differentiable objective with $L_1$constraint using a simple reparametrization and straightforward stochasticgradient descent. Our proposal is the direct generalization of previous ideasthat the $L_1$ penalty may be equivalent to a differentiable reparametrizationwith weight decay. We prove that the proposed method, \textit{spred}, is anexact differentiable solver of $L_1$ and that the reparametrization trick iscompletely ``benign" for a generic nonconvex function. Practically, wedemonstrate the usefulness of the method in (1) training sparse neural networksto perform gene selection tasks, which involves finding relevant features in avery high dimensional space, and (2) neural network compression task, to whichprevious attempts at applying the $L_1$-penalty have been unsuccessful.Conceptually, our result bridges the gap between the sparsity in deep learningand conventional statistical learning.</description><author>Liu Ziyin, Zihao Wang</author><pubDate>Wed, 12 Jul 2023 16:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01212v5</guid></item><item><title>Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches</title><link>http://arxiv.org/abs/2307.06218v1</link><description>Poetry holds immense significance within the cultural and traditional fabricof any nation. It serves as a vehicle for poets to articulate their emotions,preserve customs, and convey the essence of their culture. Arabic poetry is noexception, having played a cherished role in the heritage of the Arabiccommunity throughout history and maintaining its relevance in the present era.Typically, comprehending Arabic poetry necessitates the expertise of a linguistwho can analyze its content and assess its quality. This paper presents theintroduction of a framework called \textit{Ashaar}https://github.com/ARBML/Ashaar, which encompasses a collection of datasets andpre-trained models designed specifically for the analysis and generation ofArabic poetry. The pipeline established within our proposed approachencompasses various aspects of poetry, such as meter, theme, and eraclassification. It also incorporates automatic poetry diacritization, enablingmore intricate analyses like automated extraction of the \textit{Arudi} style.Additionally, we explore the feasibility of generating conditional poetrythrough the pre-training of a character-based GPT model. Furthermore, as partof this endeavor, we provide four datasets: one for poetry generation, anotherfor diacritization, and two for Arudi-style prediction. These datasets aim tofacilitate research and development in the field of Arabic poetry by enablingresearchers and enthusiasts to delve into the nuances of this rich literarytradition.</description><author>Zaid Alyafeai, Maged S. Al-Shaibani, Moataz Ahmed</author><pubDate>Wed, 12 Jul 2023 16:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06218v1</guid></item><item><title>Weakly-supervised positional contrastive learning: application to cirrhosis classification</title><link>http://arxiv.org/abs/2307.04617v2</link><description>Large medical imaging datasets can be cheaply and quickly annotated withlow-confidence, weak labels (e.g., radiological scores). Access tohigh-confidence labels, such as histology-based diagnoses, is rare and costly.Pretraining strategies, like contrastive learning (CL) methods, can leverageunlabeled or weakly-annotated datasets. These methods typically require largebatch sizes, which poses a difficulty in the case of large 3D images at fullresolution, due to limited GPU memory. Nevertheless, volumetric positionalinformation about the spatial context of each 2D slice can be very importantfor some medical applications. In this work, we propose an efficientweakly-supervised positional (WSP) contrastive learning strategy where weintegrate both the spatial context of each 2D slice and a weak label via ageneric kernel-based loss function. We illustrate our method on cirrhosisprediction using a large volume of weakly-labeled images, namely radiologicallow-confidence annotations, and small strongly-labeled (i.e., high-confidence)datasets. The proposed model improves the classification AUC by 5% with respectto a baseline model on our internal dataset, and by 26% on the public LIHCdataset from the Cancer Genome Atlas. The code is available at:https://github.com/Guerbet-AI/wsp-contrastive.</description><author>Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</author><pubDate>Wed, 12 Jul 2023 16:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04617v2</guid></item><item><title>Closing the gap between SVRG and TD-SVRG with Gradient Splitting</title><link>http://arxiv.org/abs/2211.16237v2</link><description>Temporal difference (TD) learning is a policy evaluation in reinforcementlearning whose performance can be enhanced by variance reduction techniques.Recently, multiple works have sought to fuse TD learning with SVRG to obtain apolicy evaluation method with a geometric rate of convergence. However, theresulting convergence rate is significantly weaker than what is achieved bySVRG in the setting of convex optimization. In this work we utilize a recentinterpretation of TD-learning as the splitting of the gradient of anappropriately chosen function, thus simplifying the algorithm and fusing TDwith SVRG. Our main result is a geometric convergence bound with predeterminedlearning rate of $1/8$, which is identical to the convergence bound availablefor SVRG in the convex setting. Our theoretical findings are supported by a setof experiments.</description><author>Arsenii Mustafin, Alex Olshevsky, Ioannis Ch. Paschalidis</author><pubDate>Wed, 12 Jul 2023 16:03:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16237v2</guid></item><item><title>Testing different Log Bases For Vector Model Weighting Technique</title><link>http://arxiv.org/abs/2307.06213v1</link><description>Information retrieval systems retrieves relevant documents based on a querysubmitted by the user. The documents are initially indexed and the words in thedocuments are assigned weights using a weighting technique called TFIDF whichis the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TFrepresents the number of occurrences of a term in a document. IDF measureswhether the term is common or rare across all documents. It is computed bydividing the total number of documents in the system by the number of documentscontaining the term and then computing the logarithm of the quotient. Bydefault, we use base 10 to calculate the logarithm. In this paper, we are goingto test this weighting technique by using a range of log bases from 0.1 to100.0 to calculate the IDF. Testing different log bases for vector modelweighting technique is to highlight the importance of understanding theperformance of the system at different weighting values. We use the documentsof MED, CRAN, NPL, LISA, and CISI test collections that scientists assembledexplicitly for experiments in data information retrieval systems.</description><author>Kamel Assaf</author><pubDate>Wed, 12 Jul 2023 16:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06213v1</guid></item><item><title>Meta-multigraph Search: Rethinking Meta-structure on Heterogeneous Information Networks</title><link>http://arxiv.org/abs/2304.11574v2</link><description>Meta-structures are widely used to define which subset of neighbors toaggregate information in heterogeneous information networks (HINs). In thiswork, we investigate existing meta-structures, including meta-path andmeta-graph, and observe that they are initially designed manually with fixedpatterns and hence are insufficient to encode various rich semantic informationon diverse HINs. Through reflection on their limitation, we define a newconcept called meta-multigraph as a more expressive and flexible generalizationof meta-graph, and propose a stable differentiable search method toautomatically optimize the meta-multigraph for specific HINs and tasks. As theflexibility of meta-multigraphs may propagate redundant messages, we furtherintroduce a complex-to-concise (C2C) meta-multigraph that propagates messagesfrom complex to concise along the depth of meta-multigraph. Moreover, weobserve that the differentiable search typically suffers from unstable searchand a significant gap between the meta-structures in search and evaluation. Tothis end, we propose a progressive search algorithm by implicitly narrowing thesearch space to improve search stability and reduce inconsistency. Extensiveexperiments are conducted on six medium-scale benchmark datasets and onelarge-scale benchmark dataset over two representative tasks, i.e., nodeclassification and recommendation. Empirical results demonstrate that oursearch methods can automatically find expressive meta-multigraphs and C2Cmeta-multigraphs, enabling our model to outperform state-of-the-artheterogeneous graph neural networks.</description><author>Chao Li, Hao Xu, Kun He</author><pubDate>Wed, 12 Jul 2023 15:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11574v2</guid></item><item><title>Local Conditional Neural Fields for Versatile and Generalizable Large-Scale Reconstructions in Computational Imaging</title><link>http://arxiv.org/abs/2307.06207v1</link><description>Deep learning has transformed computational imaging, but traditionalpixel-based representations limit their ability to capture continuous,multiscale details of objects. Here we introduce a novel Local ConditionalNeural Fields (LCNF) framework, leveraging a continuous implicit neuralrepresentation to address this limitation. LCNF enables flexible objectrepresentation and facilitates the reconstruction of multiscale information. Wedemonstrate the capabilities of LCNF in solving the highly ill-posed inverseproblem in Fourier ptychographic microscopy (FPM) with multiplexedmeasurements, achieving robust, scalable, and generalizable large-scale phaseretrieval. Unlike traditional neural fields frameworks, LCNF incorporates alocal conditional representation that promotes model generalization, learningmultiscale information, and efficient processing of large-scale imaging data.By combining an encoder and a decoder conditioned on a learned latent vector,LCNF achieves versatile continuous-domain super-resolution imagereconstruction. We demonstrate accurate reconstruction of wide field-of-view,high-resolution phase images using only a few multiplexed measurements. LCNFrobustly captures the continuous object priors and eliminates various phaseartifacts, even when it is trained on imperfect datasets. The frameworkexhibits strong generalization, reconstructing diverse objects even withlimited training data. Furthermore, LCNF can be trained on a physics simulatorusing natural images and successfully applied to experimental measurements onbiological samples. Our results highlight the potential of LCNF for solvinglarge-scale inverse problems in computational imaging, with broad applicabilityin various deep-learning-based techniques.</description><author>Hao Wang, Lei Tian</author><pubDate>Wed, 12 Jul 2023 15:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06207v1</guid></item><item><title>SepVAE: a contrastive VAE to separate pathological patterns from healthy ones</title><link>http://arxiv.org/abs/2307.06206v1</link><description>Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders(VAEs) that aims at separating the common factors of variation between abackground dataset (BG) (i.e., healthy subjects) and a target dataset (TG)(i.e., patients) from the ones that only exist in the target dataset. To do so,these methods separate the latent space into a set of salient features (i.e.,proper to the target dataset) and a set of common features (i.e., exist in bothdatasets). Currently, all models fail to prevent the sharing of informationbetween latent spaces effectively and to capture all salient factors ofvariation. To this end, we introduce two crucial regularization losses: adisentangling term between common and salient representations and aclassification term between background and target samples in the salient space.We show a better performance than previous CA-VAEs methods on three medicalapplications and a natural images dataset (CelebA). Code and datasets areavailable on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.</description><author>Robin Louiset, Edouard Duchesnay, Antoine Grigis, Benoit Dufumier, Pietro Gori</author><pubDate>Wed, 12 Jul 2023 15:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06206v1</guid></item><item><title>Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems</title><link>http://arxiv.org/abs/2307.06187v1</link><description>In autonomic computing, self-adaptation has been proposed as a fundamentalparadigm to manage the complexity of multiagent systems (MASs). This achievedby extending a system with support to monitor and adapt itself to achievespecific concerns of interest. Communication in these systems is key given thatin scenarios involving agent interaction, it enhances cooperation and reducescoordination challenges by enabling direct, clear information exchange.However, improving the expressiveness of the interaction communication withMASs is not without challenges. In this sense, the interplay betweenself-adaptive systems and effective communication is crucial for future MASadvancements. In this paper, we propose the integration of large languagemodels (LLMs) such as GPT-based technologies into multiagent systems. We anchorour methodology on the MAPE-K model, which is renowned for its robust supportin monitoring, analyzing, planning, and executing system adaptations inresponse to dynamic environments. We also present a practical illustration ofthe proposed approach, in which we implement and assess a basic MAS-basedapplication. The approach significantly advances the state-of-the-art ofself-adaptive systems by proposing a new paradigm for MAS self-adaptation ofautonomous systems based on LLM capabilities.</description><author>Nathalia Nascimento, Paulo Alencar, Donald Cowan</author><pubDate>Wed, 12 Jul 2023 15:26:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06187v1</guid></item><item><title>VAE-Loco: Versatile Quadruped Locomotion by Learning a Disentangled Gait Representation</title><link>http://arxiv.org/abs/2205.01179v2</link><description>Quadruped locomotion is rapidly maturing to a degree where robots are able torealise highly dynamic manoeuvres. However, current planners are unable to varykey gait parameters of the in-swing feet midair. In this work we address thislimitation and show that it is pivotal in increasing controller robustness bylearning a latent space capturing the key stance phases constituting aparticular gait. This is achieved via a generative model trained on a singletrot style, which encourages disentanglement such that application of a drivesignal to a single dimension of the latent state induces holistic planssynthesising a continuous variety of trot styles. We demonstrate that specificproperties of the drive signal map directly to gait parameters such as cadence,footstep height and full stance duration. Due to the nature of our approachthese synthesised gaits are continuously variable online during robotoperation. The use of a generative model facilitates the detection andmitigation of disturbances to provide a versatile and robust planningframework. We evaluate our approach on two versions of the real ANYmalquadruped robots and demonstrate that our method achieves a continuous blend ofdynamic trot styles whilst being robust and reactive to external perturbations.</description><author>Alexander L. Mitchell, Wolfgang Merkt, Mathieu Geisert, Siddhant Gangapurwala, Martin Engelcke, Oiwi Parker Jones, Ioannis Havoutis, Ingmar Posner</author><pubDate>Wed, 12 Jul 2023 15:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.01179v2</guid></item><item><title>CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification</title><link>http://arxiv.org/abs/2307.06182v1</link><description>Automatic examination of thin-prep cytologic test (TCT) slides can assistpathologists in finding cervical abnormality for accurate and efficient cancerscreening. Current solutions mostly need to localize suspicious cells andclassify abnormality based on local patches, concerning the fact that wholeslide images of TCT are extremely large. It thus requires many annotations ofnormal and abnormal cervical cells, to supervise the training of thepatch-level classifier for promising performance. In this paper, we proposeCellGAN to synthesize cytopathological images of various cervical cell typesfor augmenting patch-level cell classification. Built upon a lightweightbackbone, CellGAN is equipped with a non-linear class mapping network toeffectively incorporate cell type information into image generation. We alsopropose the Skip-layer Global Context module to model the complex spatialrelationship of the cells, and attain high fidelity of the synthesized imagesthrough adversarial learning. Our experiments demonstrate that CellGAN canproduce visually plausible TCT cytopathological images for different celltypes. We also validate the effectiveness of using CellGAN to greatly augmentpatch-level cell classification performance.</description><author>Zhenrong Shen, Maosong Cao, Sheng Wang, Lichi Zhang, Qian Wang</author><pubDate>Wed, 12 Jul 2023 15:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06182v1</guid></item><item><title>The Deep Generative Decoder: MAP estimation of representations improves modeling of single-cell RNA data</title><link>http://arxiv.org/abs/2110.06672v3</link><description>Learning low-dimensional representations of single-cell transcriptomics hasbecome instrumental to its downstream analysis. The state of the art iscurrently represented by neural network models such as variational autoencoders(VAEs) which use a variational approximation of the likelihood for inference.We here present the Deep Generative Decoder (DGD), a simple generative modelthat computes model parameters and representations directly via maximum aposteriori (MAP) estimation. The DGD handles complex parameterized latentdistributions naturally unlike VAEs which typically use a fixed Gaussiandistribution, because of the complexity of adding other types. We first showits general functionality on a commonly used benchmark set, Fashion-MNIST.Secondly, we apply the model to multiple single-cell data sets. Here the DGDlearns low-dimensional, meaningful and well-structured latent representationswith sub-clustering beyond the provided labels. The advantages of this approachare its simplicity and its capability to provide representations of muchsmaller dimensionality than a comparable VAE.</description><author>Viktoria Schuster, Anders Krogh</author><pubDate>Wed, 12 Jul 2023 15:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.06672v3</guid></item><item><title>Large Class Separation is not what you need for Relational Reasoning-based OOD Detection</title><link>http://arxiv.org/abs/2307.06179v1</link><description>Standard recognition approaches are unable to deal with novel categories attest time. Their overconfidence on the known classes makes the predictionsunreliable for safety-critical applications such as healthcare or autonomousdriving. Out-Of-Distribution (OOD) detection methods provide a solution byidentifying semantic novelty. Most of these methods leverage a learning stageon the known data, which means training (or fine-tuning) a model to capture theconcept of normality. This process is clearly sensitive to the amount ofavailable samples and might be computationally expensive for on-board systems.A viable alternative is that of evaluating similarities in the embedding spaceproduced by large pre-trained models without any further learning effort. Wefocus exactly on such a fine-tuning-free OOD detection setting. This workspresents an in-depth analysis of the recently introduced relational reasoningpre-training and investigates the properties of the learned embedding,highlighting the existence of a correlation between the inter-class featuredistance and the OOD detection accuracy. As the class separation depends on thechosen pre-training objective, we propose an alternative loss function tocontrol the inter-class margin, and we show its advantage with thoroughexperiments.</description><author>Lorenzo Li Lu, Giulia D'Ascenzi, Francesco Cappio Borlino, Tatiana Tommasi</author><pubDate>Wed, 12 Jul 2023 15:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06179v1</guid></item><item><title>Linearization Algorithms for Fully Composite Optimization</title><link>http://arxiv.org/abs/2302.12808v2</link><description>This paper studies first-order algorithms for solving fully compositeoptimization problems over convex and compact sets. We leverage the structureof the objective by handling its differentiable and non-differentiablecomponents separately, linearizing only the smooth parts. This provides us withnew generalizations of the classical Frank-Wolfe method and the ConditionalGradient Sliding algorithm, that cater to a subclass of non-differentiableproblems. Our algorithms rely on a stronger version of the linear minimizationoracle, which can be efficiently implemented in several practical applications.We provide the basic version of our method with an affine-invariant analysisand prove global convergence rates for both convex and non-convex objectives.Furthermore, in the convex case, we propose an accelerated method withcorrespondingly improved complexity. Finally, we provide illustrativeexperiments to support our theoretical results.</description><author>Maria-Luiza Vladarean, Nikita Doikov, Martin Jaggi, Nicolas Flammarion</author><pubDate>Wed, 12 Jul 2023 15:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12808v2</guid></item><item><title>Smart Infrastructure: A Research Junction</title><link>http://arxiv.org/abs/2307.06177v1</link><description>Complex inner-city junctions are among the most critical traffic areas forinjury and fatal accidents. The development of highly automated driving (HAD)systems struggles with the complex and hectic everyday life within those areas.Sensor-equipped smart infrastructures, which can communicate and cooperate withvehicles, are essential to enable a holistic scene understanding to resolveocclusions drivers and vehicle perception systems for themselves can not cover.We introduce an intelligent research infrastructure equipped with visual sensortechnology, located at a public inner-city junction in Aschaffenburg, Germany.A multiple-view camera system monitors the traffic situation to perceive roadusers' behavior. Both motorized and non-motorized traffic is considered. Thesystem is used for research in data generation, evaluating new HAD sensorssystems, algorithms, and Artificial Intelligence (AI) training strategies usingreal-, synthetic- and augmented data. In addition, the junction features ahighly accurate digital twin. Real-world data can be taken into the digitaltwin for simulation purposes and synthetic data generation.</description><author>Manuel Hetzel, Hannes Reichert, Konrad Doll, Bernhard Sick</author><pubDate>Wed, 12 Jul 2023 15:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06177v1</guid></item><item><title>Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior</title><link>http://arxiv.org/abs/2307.06175v1</link><description>Recent reinforcement learning (RL) methods have achieved success in variousdomains. However, multi-agent RL (MARL) remains a challenge in terms ofdecentralization, partial observability and scalability to many agents.Meanwhile, collective behavior requires resolution of the aforementionedchallenges, and remains of importance to many state-of-the-art applicationssuch as active matter physics, self-organizing systems, opinion dynamics, andbiological or robotic swarms. Here, MARL via mean field control (MFC) offers apotential solution to scalability, but fails to consider decentralized andpartially observable systems. In this paper, we enable decentralized behaviorof agents under partial information by proposing novel models for decentralizedpartially observable MFC (Dec-POMFC), a broad class of problems withpermutation-invariant agents allowing for reduction to tractable single-agentMarkov decision processes (MDP) with single-agent RL solution. We providerigorous theoretical results, including a dynamic programming principle,together with optimality guarantees for Dec-POMFC solutions applied to finiteswarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradientmethods for MARL via centralized training and decentralized execution, togetherwith policy gradient approximation guarantees. In addition, we improve uponstate-of-the-art histogram-based MFC by kernel methods, which is of separateinterest also for fully observable MFC. We evaluate numerically onrepresentative collective behavior tasks such as adapted Kuramoto and Vicsekswarming models, being on par with state-of-the-art MARL. Overall, ourframework takes a step towards RL-based engineering of artificial collectivebehavior via MFC.</description><author>Kai Cui, Sascha Hauck, Christian Fabian, Heinz Koeppl</author><pubDate>Wed, 12 Jul 2023 15:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06175v1</guid></item><item><title>Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain</title><link>http://arxiv.org/abs/2307.03042v2</link><description>Adapting pretrained language models to novel domains, such as clinicalapplications, traditionally involves retraining their entire set of parameters.However, this approach is increasingly proven to be impractical owing to thesubstantial computational requirements associated with training such largelanguage models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)techniques offer a viable solution by selectively fine-tuning a small subset ofadditional parameters, significantly reducing the computational requirementsfor domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFTadapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA istrained using clinical notes obtained from the MIMIC-IV database, therebycreating a specialised adapter designed for the clinical domain. Additionally,we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA withDownstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.We evaluate this framework on multiple clinical outcome prediction datasets,comparing it to clinically trained language models. Our proposed frameworkachieves a state-of-the-art AUROC score averaged across all clinical downstreamtasks. We observe substantial improvements of 6-9% AUROC score in thelarge-scale multilabel classification tasks, such as diagnoses and proceduresclassification.</description><author>Aryo Pradipta Gema, Luke Daines, Pasquale Minervini, Beatrice Alex</author><pubDate>Wed, 12 Jul 2023 14:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03042v2</guid></item><item><title>Neuromorphic Optical Flow and Real-time Implementation with Event Cameras</title><link>http://arxiv.org/abs/2304.07139v2</link><description>Optical flow provides information on relative motion that is an importantcomponent in many computer vision pipelines. Neural networks provide highaccuracy optical flow, yet their complexity is often prohibitive forapplication at the edge or in robots, where efficiency and latency play crucialrole. To address this challenge, we build on the latest developments inevent-based vision and spiking neural networks. We propose a new networkarchitecture, inspired by Timelens, that improves the state-of-the-artself-supervised optical flow accuracy when operated both in spiking andnon-spiking mode. To implement a real-time pipeline with a physical eventcamera, we propose a methodology for principled model simplification based onactivity and latency analysis. We demonstrate high speed optical flowprediction with almost two orders of magnitude reduced complexity whilemaintaining the accuracy, opening the path for real-time deployments.</description><author>Yannick Schnider, Stanislaw Wozniak, Mathias Gehrig, Jules Lecomte, Axel von Arnim, Luca Benini, Davide Scaramuzza, Angeliki Pantazi</author><pubDate>Wed, 12 Jul 2023 14:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07139v2</guid></item><item><title>Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based Partial Differential Equations Solving</title><link>http://arxiv.org/abs/2307.06167v1</link><description>Physics-informed neural networks (PINNs) have emerged as promising surrogatemodes for solving partial differential equations (PDEs). Their effectivenesslies in the ability to capture solution-related features through neuralnetworks. However, original PINNs often suffer from bottlenecks, such as lowaccuracy and non-convergence, limiting their applicability in complex physicalcontexts. To alleviate these issues, we proposed auxiliary-task learning-basedphysics-informed neural networks (ATL-PINNs), which provide four differentauxiliary-task learning modes and investigate their performance compared withoriginal PINNs. We also employ the gradient cosine similarity algorithm tointegrate auxiliary problem loss with the primary problem loss in ATL-PINNs,which aims to enhance the effectiveness of the auxiliary-task learning modes.To the best of our knowledge, this is the first study to introduceauxiliary-task learning modes in the context of physics-informed learning. Weconduct experiments on three PDE problems across different fields andscenarios. Our findings demonstrate that the proposed auxiliary-task learningmodes can significantly improve solution accuracy, achieving a maximumperformance boost of 96.62% (averaging 28.23%) compared to the originalsingle-task PINNs. The code and dataset are open source athttps://github.com/junjun-yan/ATL-PINN.</description><author>Junjun Yan, Xinhai Chen, Zhichao Wang, Enqiang Zhou, Jie Liu</author><pubDate>Wed, 12 Jul 2023 14:46:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06167v1</guid></item><item><title>Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning</title><link>http://arxiv.org/abs/2307.06166v1</link><description>Vision-Language Models (VLMs) are expected to be capable of reasoning withcommonsense knowledge as human beings. One example is that humans can reasonwhere and when an image is taken based on their knowledge. This makes us wonderif, based on visual cues, Vision-Language Models that are pre-trained withlarge-scale image-text resources can achieve and even outperform human'scapability in reasoning times and location. To address this question, wepropose a two-stage \recognition\space and \reasoning\space probing task,applied to discriminative and generative VLMs to uncover whether VLMs canrecognize times and location-relevant features and further reason about it. Tofacilitate the investigation, we introduce WikiTiLo, a well-curated imagedataset compromising images with rich socio-cultural cues. In the extensiveexperimental studies, we find that although VLMs can effectively retainrelevant features in visual encoders, they still fail to make perfectreasoning. We will release our dataset and codes to facilitate future studies.</description><author>Gengyuan Zhang, Yurui Zhang, Kerui Zhang, Volker Tresp</author><pubDate>Wed, 12 Jul 2023 14:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06166v1</guid></item><item><title>The IMPTC Dataset: An Infrastructural Multi-Person Trajectory and Context Dataset</title><link>http://arxiv.org/abs/2307.06165v1</link><description>Inner-city intersections are among the most critical traffic areas for injuryand fatal accidents. Automated vehicles struggle with the complex and hecticeveryday life within those areas. Sensor-equipped smart infrastructures, whichcan cooperate with vehicles, can benefit automated traffic by extending theperception capabilities of drivers and vehicle perception systems.Additionally, they offer the opportunity to gather reproducible and precisedata of a holistic scene understanding, including context information as abasis for training algorithms for various applications in automated traffic.Therefore, we introduce the Infrastructural Multi-Person Trajectory and ContextDataset (IMPTC). We use an intelligent public inner-city intersection inGermany with visual sensor technology. A multi-view camera and LiDAR systemperceives traffic situations and road users' behavior. Additional sensorsmonitor contextual information like weather, lighting, and traffic light signalstatus. The data acquisition system focuses on Vulnerable Road Users (VRUs) andmulti-agent interaction. The resulting dataset consists of eight hours ofmeasurement data. It contains over 2,500 VRU trajectories, includingpedestrians, cyclists, e-scooter riders, strollers, and wheelchair users, andover 20,000 vehicle trajectories at different day times, weather conditions,and seasons. In addition, to enable the entire stack of research capabilities,the dataset includes all data, starting from the sensor-, calibration- anddetection data until trajectory and context data. The dataset is continuouslyexpanded and is available online for non-commercial research athttps://github.com/kav-institute/imptc-dataset.</description><author>Manuel Hetzel, Hannes Reichert, Günther Reitberger, Erich Fuchs, Konrad Doll, Bernhard Sick</author><pubDate>Wed, 12 Jul 2023 14:46:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06165v1</guid></item><item><title>Deep Generative Models for Physiological Signals: A Systematic Literature Review</title><link>http://arxiv.org/abs/2307.06162v1</link><description>In this paper, we present a systematic literature review on deep generativemodels for physiological signals, particularly electrocardiogram,electroencephalogram, photoplethysmogram and electromyogram. Compared to theexisting review papers, we present the first review that summarizes the recentstate-of-the-art deep generative models. By analysing the state-of-the-artresearch related to deep generative models along with their main applicationsand challenges, this review contributes to the overall understanding of thesemodels applied to physiological signals. Additionally, by highlighting theemployed evaluation protocol and the most used physiological databases, thisreview facilitates the assessment and benchmarking of deep generative models.</description><author>Nour Neifar, Afef Mdhaffar, Achraf Ben-Hamadou, Mohamed Jmaiel</author><pubDate>Wed, 12 Jul 2023 14:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06162v1</guid></item><item><title>One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data</title><link>http://arxiv.org/abs/2302.06375v2</link><description>There is a recent growing interest in applying Deep Learning techniques totabular data, in order to replicate the success of other ArtificialIntelligence areas in this structured domain. Specifically interesting is thecase in which tabular data have a time dependence, such as, for instancefinancial transactions. However, the heterogeneity of the tabular values, inwhich categorical elements are mixed with numerical items, makes thisadaptation difficult. In this paper we propose a Transformer architecture torepresent heterogeneous time-dependent tabular data, in which numericalfeatures are represented using a set of frequency functions and the wholenetwork is uniformly trained with a unique loss function.</description><author>Simone Luetto, Fabrizio Garuti, Enver Sangineto, Lorenzo Forni, Rita Cucchiara</author><pubDate>Wed, 12 Jul 2023 14:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06375v2</guid></item><item><title>Reflective Hybrid Intelligence for Meaningful Human Control in Decision-Support Systems</title><link>http://arxiv.org/abs/2307.06159v1</link><description>With the growing capabilities and pervasiveness of AI systems, societies mustcollectively choose between reduced human autonomy, endangered democracies andlimited human rights, and AI that is aligned to human and social values,nurturing collaboration, resilience, knowledge and ethical behaviour. In thischapter, we introduce the notion of self-reflective AI systems for meaningfulhuman control over AI systems. Focusing on decision support systems, we proposea framework that integrates knowledge from psychology and philosophy withformal reasoning methods and machine learning approaches to create AI systemsresponsive to human values and social norms. We also propose a possibleresearch approach to design and develop self-reflective capability in AIsystems. Finally, we argue that self-reflective AI systems can lead toself-reflective hybrid systems (human + AI), thus increasing meaningful humancontrol and empowering human moral reasoning by providing comprehensibleinformation and insights on possible human moral blind spots.</description><author>Catholijn M. Jonker, Luciano Cavalcante Siebert, Pradeep K. Murukannaiah</author><pubDate>Wed, 12 Jul 2023 14:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06159v1</guid></item><item><title>Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement</title><link>http://arxiv.org/abs/2303.01388v2</link><description>Over recent years, Reinforcement Learning combined with Deep Learningtechniques has successfully proven to solve complex problems in variousdomains, including robotics, self-driving cars, and finance. In this paper, weare introducing Reinforcement Learning (RL) to label placement, a complex taskin data visualization that seeks optimal positioning for labels to avoidoverlap and ensure legibility. Our novel point-feature label placement methodutilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the labelplacement strategy, which is the first machine-learning-driven labeling methodin contrast to existing hand-crafted algorithms designed by human experts. Tofacilitate RL learning, we developed an environment where an agent acts as aproxy for a label, a short textual annotation that augments visualization. Ourresults show that the strategy trained by our method significantly outperformsthe random strategy of an untrained agent and compared methods designed byhuman experts in terms of completeness (i.e., the number of placed labels). Thetrade-off is increased computation time, making the proposed method slower thancompared methods. Nevertheless, our method is ideal for scenarios where thelabeling can be computed in advance, and completeness is essential, such ascartographic maps, technical drawings, and medical atlases. Additionally, weconducted a user study to assess the perceived performance. The outcomesrevealed that the participants considered the proposed method to besignificantly better than the other examined methods. This indicates that theimproved completeness is not just reflected in the quantitative metrics butalso in the subjective evaluation of the participants.</description><author>Petr Bobák, Ladislav Čmolík, Martin Čadík</author><pubDate>Wed, 12 Jul 2023 14:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01388v2</guid></item><item><title>Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions</title><link>http://arxiv.org/abs/2307.06152v1</link><description>Maneuver decision-making is the core of unmanned combat aerial vehicle forautonomous air combat. To solve this problem, we propose an automaticcurriculum reinforcement learning method, which enables agents to learneffective decisions in air combat from scratch. The range of initial states areused for distinguishing curricula of different difficulty levels, therebymaneuver decision is divided into a series of sub-tasks from easy to difficult,and test results are used to change sub-tasks. As sub-tasks change, agentsgradually learn to complete a series of sub-tasks from easy to difficult,enabling them to make effective maneuvering decisions to cope with variousstates without the need to spend effort designing reward functions. Theablation studied show that the automatic curriculum learning proposed in thisarticle is an essential component for training through reinforcement learning,namely, agents cannot complete effective decisions without curriculum learning.Simulation experiments show that, after training, agents are able to makeeffective decisions given different states, including tracking, attacking andescaping, which are both rational and interpretable.</description><author>Zhang Hong-Peng</author><pubDate>Wed, 12 Jul 2023 14:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06152v1</guid></item><item><title>Deep Learning for Survival Analysis: A Review</title><link>http://arxiv.org/abs/2305.14961v2</link><description>The influx of deep learning (DL) techniques into the field of survivalanalysis in recent years has led to substantial methodological progress; forinstance, learning from unstructured or high-dimensional data such as images,text or omics data. In this work, we conduct a comprehensive systematic reviewof DL-based methods for time-to-event analysis, characterizing them accordingto both survival- and DL-related attributes. In summary, the reviewed methodsoften address only a small subset of tasks relevant to time-to-event data -e.g., single-risk right-censored data - and neglect to incorporate more complexsettings. Our findings are summarized in an editable, open-source, interactivetable: https://survival-org.github.io/DL4Survival. As this research area isadvancing rapidly, we encourage community contribution in order to keep thisdatabase up to date.</description><author>Simon Wiegrebe, Philipp Kopper, Raphael Sonabend, Bernd Bischl, Andreas Bender</author><pubDate>Wed, 12 Jul 2023 14:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14961v2</guid></item><item><title>NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services</title><link>http://arxiv.org/abs/2307.06148v1</link><description>Large language models (LLMs) have triggered tremendous success to empowerdaily life by generative information, and the personalization of LLMs couldfurther contribute to their applications due to better alignment with humanintents. Towards personalized generative services, a collaborative cloud-edgemethodology sounds promising, as it facilitates the effective orchestration ofheterogeneous distributed communication and computing resources. In thisarticle, after discussing the pros and cons of several candidate cloud-edgecollaboration techniques, we put forward NetGPT to capably deploy appropriateLLMs at the edge and the cloud in accordance with their computing capacity. Inaddition, edge LLMs could efficiently leverage location-based information forpersonalized prompt completion, thus benefiting the interaction with cloudLLMs. After deploying representative open-source LLMs (e.g., GPT-2-base andLLaMA model) at the edge and the cloud, we present the feasibility of NetGPT onthe basis of low-rank adaptation-based light-weight fine-tuning. Subsequently,we highlight substantial essential changes required for a native artificialintelligence (AI) network architecture towards NetGPT, with special emphasis ondeeper integration of communications and computing resources and carefulcalibration of logical AI workflow. Furthermore, we demonstrate severalby-product benefits of NetGPT, given edge LLM's astonishing capability topredict trends and infer intents, which possibly leads to a unified solutionfor intelligent network management \&amp; orchestration. In a nutshell, we arguethat NetGPT is a promising native-AI network architecture beyond provisioningpersonalized generative services.</description><author>Yuxuan Chen, Rongpeng Li, Zhifeng Zhao, Chenghui Peng, Jianjun Wu, Ekram Hossain, Honggang Zhang</author><pubDate>Wed, 12 Jul 2023 14:10:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06148v1</guid></item><item><title>Towards Fleet-wide Sharing of Wind Turbine Condition Information through Privacy-preserving Federated Learning</title><link>http://arxiv.org/abs/2212.03529v3</link><description>Terabytes of data are collected by wind turbine manufacturers from theirfleets every day. And yet, a lack of data access and sharing impedes exploitingthe full potential of the data. We present a distributed machine learningapproach that preserves the data privacy by leaving the data on the windturbines while still enabling fleet-wide learning on those local data. We showthat through federated fleet-wide learning, turbines with little or norepresentative training data can benefit from more accurate normal behaviormodels. Customizing the global federated model to individual turbines yieldsthe highest fault detection accuracy in cases where the monitored targetvariable is distributed heterogeneously across the fleet. We demonstrate thisfor bearing temperatures, a target variable whose normal behavior can varywidely depending on the turbine. We show that no turbine experiences a loss inmodel performance from participating in the federated learning process,resulting in superior performance of the federated learning strategy in ourcase studies. The distributed learning increases the normal behavior modeltraining times by about a factor of ten due to increased communication overheadand slower model convergence.</description><author>Lorin Jenkel, Stefan Jonas, Angela Meyer</author><pubDate>Wed, 12 Jul 2023 14:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03529v3</guid></item><item><title>Learning Kernel-Modulated Neural Representation for Efficient Light Field Compression</title><link>http://arxiv.org/abs/2307.06143v1</link><description>Light field is a type of image data that captures the 3D scene information byrecording light rays emitted from a scene at various orientations. It offers amore immersive perception than classic 2D images but at the cost of huge datavolume. In this paper, we draw inspiration from the visual characteristics ofSub-Aperture Images (SAIs) of light field and design a compact neural networkrepresentation for the light field compression task. The network backbone takesrandomly initialized noise as input and is supervised on the SAIs of the targetlight field. It is composed of two types of complementary kernels: descriptivekernels (descriptors) that store scene description information learned duringtraining, and modulatory kernels (modulators) that control the rendering ofdifferent SAIs from the queried perspectives. To further enhance compactness ofthe network meanwhile retain high quality of the decoded light field, weaccordingly introduce modulator allocation and kernel tensor decompositionmechanisms, followed by non-uniform quantization and lossless entropy codingtechniques, to finally form an efficient compression pipeline. Extensiveexperiments demonstrate that our method outperforms other state-of-the-art(SOTA) methods by a significant margin in the light field compression task.Moreover, after aligning descriptors, the modulators learned from one lightfield can be transferred to new light fields for rendering dense views,indicating a potential solution for view synthesis task.</description><author>Jinglei Shi, Yihong Xu, Christine Guillemot</author><pubDate>Wed, 12 Jul 2023 13:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06143v1</guid></item><item><title>MOPO-LSI: A User Guide</title><link>http://arxiv.org/abs/2307.01719v2</link><description>MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library forSustainable Investments. This document provides a user guide for MOPO-LSIversion 1.0, including problem setup, workflow and the hyper-parameters inconfigurations.</description><author>Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu, David, Wang, Michael O'Leary</author><pubDate>Wed, 12 Jul 2023 13:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01719v2</guid></item><item><title>SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning</title><link>http://arxiv.org/abs/2307.06135v1</link><description>Large language models (LLMs) have demonstrated impressive results indeveloping generalist planning agents for diverse tasks. However, groundingthese plans in expansive, multi-floor, and multi-room environments presents asignificant challenge for robotics. We introduce SayPlan, a scalable approachto LLM-based, large-scale task planning for robotics using 3D scene graph(3DSG) representations. To ensure the scalability of our approach, we: (1)exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semanticsearch for task-relevant subgraphs from a smaller, collapsed representation ofthe full graph; (2) reduce the planning horizon for the LLM by integrating aclassical path planner and (3) introduce an iterative replanning pipeline thatrefines the initial plan using feedback from a scene graph simulator,correcting infeasible actions and avoiding planning failures. We evaluate ourapproach on two large-scale environments spanning up to 3 floors, 36 rooms and140 objects, and show that our approach is capable of grounding large-scale,long-horizon task plans from abstract, and natural language instruction for amobile manipulator robot to execute.</description><author>Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf</author><pubDate>Wed, 12 Jul 2023 13:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06135v1</guid></item><item><title>RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration</title><link>http://arxiv.org/abs/2305.11474v3</link><description>Although many recent works have made advancements in the image restoration(IR) field, they often suffer from an excessive number of parameters. Anotherissue is that most Transformer-based IR methods focus only on either local orglobal features, leading to limited receptive fields or deficient parameterissues. To address these problems, we propose a lightweight IR network,Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposeddimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, whichcompute bi-dimensional (spatial and channel) self-attentions in parallel withdifferent numbers of multi-heads. The bi-dimensional attentions help each otherto complement their counterpart's drawbacks and are then mixed. Additionally,we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer thatcompensates for pixel-level information losses and utilizes semanticinformation while maintaining an efficient hierarchical structure. Furthermore,we revisit and modify MobileNet V1 and V2 to attach efficient convolutions toour proposed components. The experimental results demonstrate that RAMiTachieves state-of-the-art performance on multiple lightweight IR tasks,including super-resolution, color denoising, grayscale denoising, low-lightenhancement, and deraining. Codes are available athttps://github.com/rami0205/RAMiT.</description><author>Haram Choi, Cheolwoong Na, Jihyeon Oh, Seungjae Lee, Jinseop Kim, Subeen Choe, Jeongmin Lee, Taehoon Kim, Jihoon Yang</author><pubDate>Wed, 12 Jul 2023 13:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11474v3</guid></item><item><title>On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion</title><link>http://arxiv.org/abs/2307.04518v2</link><description>This document chronicles this author's attempt to explore how words come tomean what they do, with a particular focus on child language acquisition andwhat that means for models of language understanding.\footnote{I say\emph{historical} because I synthesize the ideas based on when I discoveredthem and how those ideas influenced my later thinking.} I explain the settingfor child language learning, how embodiment -- being able to perceive and enactin the world, including knowledge of concrete and abstract concepts -- iscrucial, and how emotion and cognition relate to each other and the languagelearning process. I end with what I think are some of the requirements for alanguage-learning agent that learns language in a setting similar to that ofchildren. This paper can act as a potential guide for ongoing and future workin modeling language.</description><author>Casey Kennington</author><pubDate>Wed, 12 Jul 2023 13:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04518v2</guid></item><item><title>Disentangled Contrastive Collaborative Filtering</title><link>http://arxiv.org/abs/2305.02759v3</link><description>Recent studies show that graph neural networks (GNNs) are prevalent to modelhigh-order relationships for collaborative filtering (CF). Towards thisresearch line, graph contrastive learning (GCL) has exhibited powerfulperformance in addressing the supervision label shortage issue by learningaugmented user and item representations. While many of them show theireffectiveness, two key questions still remain unexplored: i) Most existingGCL-based CF models are still limited by ignoring the fact that user-iteminteraction behaviors are often driven by diverse latent intent factors (e.g.,shopping for family party, preferred color or brand of products); ii) Theirintroduced non-adaptive augmentation techniques are vulnerable to noisyinformation, which raises concerns about the model's robustness and the risk ofincorporating misleading self-supervised signals. In light of theselimitations, we propose a Disentangled Contrastive Collaborative Filteringframework (DCCF) to realize intent disentanglement with self-supervisedaugmentation in an adaptive fashion. With the learned disentangledrepresentations with global context, our DCCF is able to not only distillfiner-grained latent factors from the entangled self-supervision signals butalso alleviate the augmentation-induced noise. Finally, the cross-viewcontrastive learning task is introduced to enable adaptive augmentation withour parameterized interaction mask generator. Experiments on various publicdatasets demonstrate the superiority of our method compared to existingsolutions. Our model implementation is released at the linkhttps://github.com/HKUDS/DCCF.</description><author>Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang</author><pubDate>Wed, 12 Jul 2023 13:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02759v3</guid></item><item><title>SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation</title><link>http://arxiv.org/abs/2211.00923v3</link><description>The lack of labeled second language (L2) speech data is a major challenge indesigning mispronunciation detection models. We introduce SpeechBlender - afine-grained data augmentation pipeline for generating mispronunciation errorsto overcome such data scarcity. The SpeechBlender utilizes varieties of masksto target different regions of phonetic units, and use the mixing factors tolinearly interpolate raw speech signals while augmenting pronunciation. Themasks facilitate smooth blending of the signals, generating more effectivesamples than the `Cut/Paste' method. Our proposed technique achievesstate-of-the-art results, with Speechocean762, on ASR dependentmispronunciation detection models at phoneme level, with a 2.0% gain in PearsonCorrelation Coefficient (PCC) compared to the previous state-of-the-art [1].Additionally, we demonstrate a 5.0% improvement at the phoneme level comparedto our baseline. We also observed a 4.6% increase in F1-score with ArabicAraVoiceL2 testset.</description><author>Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali, Hamdy Mubarak, Shazia Afzal</author><pubDate>Wed, 12 Jul 2023 13:28:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00923v3</guid></item><item><title>Guided Bottom-Up Interactive Constraint Acquisition</title><link>http://arxiv.org/abs/2307.06126v1</link><description>Constraint Acquisition (CA) systems can be used to assist in the modeling ofconstraint satisfaction problems. In (inter)active CA, the system is given aset of candidate constraints and posts queries to the user with the goal offinding the right constraints among the candidates. Current interactive CAalgorithms suffer from at least two major bottlenecks. First, in order toconverge, they require a large number of queries to be asked to the user.Second, they cannot handle large sets of candidate constraints, since theselead to large waiting times for the user. For this reason, the user must havefairly precise knowledge about what constraints the system should consider. Inthis paper, we alleviate these bottlenecks by presenting two novel methods thatimprove the efficiency of CA. First, we introduce a bottom-up approach namedGrowAcq that reduces the maximum waiting time for the user and allows thesystem to handle much larger sets of candidate constraints. It also reduces thetotal number of queries for problems in which the target constraint network isnot sparse. Second, we propose a probability-based method to guide querygeneration and show that it can significantly reduce the number of queriesrequired to converge. We also propose a new technique that allows the use ofopenly accessible CP solvers in query generation, removing the dependency ofexisting methods on less well-maintained custom solvers that are not publiclyavailable. Experimental results show that our proposed methods outperformstate-of-the-art CA methods, reducing the number of queries by up to 60%. Ourmethods work well even in cases where the set of candidate constraints is 50times larger than the ones commonly used in the literature.</description><author>Dimos Tsouros, Senne Berden, Tias Guns</author><pubDate>Wed, 12 Jul 2023 13:25:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06126v1</guid></item><item><title>Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation</title><link>http://arxiv.org/abs/2307.06125v1</link><description>Existing object-search approaches enable robots to search through freepathways, however, robots operating in unstructured human-centered environmentsfrequently also have to manipulate the environment to their needs. In thiswork, we introduce a novel interactive multi-object search task in which arobot has to open doors to navigate rooms and search inside cabinets anddrawers to find target objects. These new challenges require combiningmanipulation and navigation skills in unexplored environments. We presentHIMOS, a hierarchical reinforcement learning approach that learns to composeexploration, navigation, and manipulation skills. To achieve this, we design anabstract high-level action space around a semantic map memory and leverage theexplored environment as instance navigation points. We perform extensiveexperiments in simulation and the real-world that demonstrate that HIMOSeffectively transfers to new environments in a zero-shot manner. It showsrobustness to unseen subpolicies, failures in their execution, and differentrobot kinematics. These capabilities open the door to a wide range ofdownstream tasks across embodied AI and real-world use cases.</description><author>Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada</author><pubDate>Wed, 12 Jul 2023 13:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06125v1</guid></item><item><title>Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing</title><link>http://arxiv.org/abs/2307.06124v1</link><description>Current signing avatars are often described as unnatural as they cannotaccurately reproduce all the subtleties of synchronized body behaviors of ahuman signer. In this paper, we propose a new dynamic approach for transitionsbetween signs, focusing on mouthing animations for Portuguese Sign Language.Although native signers preferred animations with dynamic transitions, we didnot find significant differences in comprehension and perceived naturalnessscores. On the other hand, we show that including mouthing behaviors improvedcomprehension and perceived naturalness for novice sign language learners.Results have implications in computational linguistics, human-computerinteraction, and synthetic animation of signing avatars.</description><author>Inês Lacerda, Hugo Nicolau, Luisa Coheur</author><pubDate>Wed, 12 Jul 2023 13:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06124v1</guid></item><item><title>SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark</title><link>http://arxiv.org/abs/2307.06123v1</link><description>Membership inference (MI) attacks threaten user privacy through determiningif a given data example has been used to train a target model. However, it hasbeen increasingly recognized that the "comparing different MI attacks"methodology used in the existing works has serious limitations. Due to theselimitations, we found (through the experiments in this work) that somecomparison results reported in the literature are quite misleading. In thispaper, we seek to develop a comprehensive benchmark for comparing different MIattacks, called MIBench, which consists not only the evaluation metrics, butalso the evaluation scenarios. And we design the evaluation scenarios from fourperspectives: the distance distribution of data samples in the target dataset,the distance between data samples of the target dataset, the differentialdistance between two datasets (i.e., the target dataset and a generated datasetwith only nonmembers), and the ratio of the samples that are made no inferencesby an MI attack. The evaluation metrics consist of ten typical evaluationmetrics. We have identified three principles for the proposed "comparingdifferent MI attacks" methodology, and we have designed and implemented theMIBench benchmark with 84 evaluation scenarios for each dataset. In total, wehave used our benchmark to fairly and systematically compare 15state-of-the-art MI attack algorithms across 588 evaluation scenarios, andthese evaluation scenarios cover 7 widely used datasets and 7 representativetypes of models. All codes and evaluations of MIBench are publicly available athttps://github.com/MIBench/MIBench.github.io/blob/main/README.md.</description><author>Jun Niu, Xiaoyan Zhu, Moxuan Zeng, Ge Zhang, Qingyang Zhao, Chunhui Huang, Yangming Zhang, Suyu An, Yangzhong Wang, Xinghui Yue, Zhipeng He, Weihao Guo, Kuo Shen, Peng Liu, Yulong Shen, Xiaohong Jiang, Jianfeng Ma, Yuqing Zhang</author><pubDate>Wed, 12 Jul 2023 13:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06123v1</guid></item><item><title>Recognizing student identification numbers from the matrix templates using a modified U-net architecture</title><link>http://arxiv.org/abs/2307.06120v1</link><description>This paper presents an innovative approach to student identification duringexams and knowledge tests, which overcomes the limitations of the traditionalpersonal information entry method. The proposed method employs a matrixtemplate on the designated section of the exam, where squares containingnumbers are selectively blackened. The methodology involves the development ofa neural network specifically designed for recognizing students' personalidentification numbers. The neural network utilizes a specially adapted U-Netarchitecture, trained on an extensive dataset comprising images of blackenedtables. The network demonstrates proficiency in recognizing the patterns andarrangement of blackened squares, accurately interpreting the informationinscribed within them. Additionally, the model exhibits high accuracy incorrectly identifying entered student personal numbers and effectivelydetecting erroneous entries within the table. This approach offers multipleadvantages. Firstly, it significantly accelerates the exam marking process byautomatically extracting identifying information from the blackened tables,eliminating the need for manual entry and minimizing the potential for errors.Secondly, the method automates the identification process, thereby reducingadministrative effort and expediting data processing. The introduction of thisinnovative identification system represents a notable advancement in the fieldof exams and knowledge tests, replacing the conventional manual entry ofpersonal data with a streamlined, efficient, and accurate identificationprocess.</description><author>Filip Pavičić</author><pubDate>Wed, 12 Jul 2023 13:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06120v1</guid></item><item><title>TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image</title><link>http://arxiv.org/abs/2307.06118v1</link><description>Automatic tree density estimation and counting using single aerial andsatellite images is a challenging task in photogrammetry and remote sensing,yet has an important role in forest management. In this paper, we propose thefirst semisupervised transformer-based framework for tree counting whichreduces the expensive tree annotations for remote sensing images. Our method,termed as TreeFormer, first develops a pyramid tree representation module basedon transformer blocks to extract multi-scale features during the encodingstage. Contextual attention-based feature fusion and tree density regressormodules are further designed to utilize the robust features from the encoder toestimate tree density maps in the decoder. Moreover, we propose a pyramidlearning strategy that includes local tree density consistency and local treecount ranking losses to utilize unlabeled images into the training process.Finally, the tree counter token is introduced to regulate the network bycomputing the global tree counts for both labeled and unlabeled images. Ourmodel was evaluated on two benchmark tree counting datasets, Jiangsu, andYosemite, as well as a new dataset, KCL-London, created by ourselves. OurTreeFormer outperforms the state of the art semi-supervised methods under thesame setting and exceeds the fully-supervised methods using the same number oflabeled images. The codes and datasets are available athttps://github.com/HAAClassic/TreeFormer.</description><author>Hamed Amini Amirkolaee, Miaojing Shi, Mark Mulligan</author><pubDate>Wed, 12 Jul 2023 13:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06118v1</guid></item><item><title>Supervised topological data analysis for MALDI mass spectrometry imaging applications</title><link>http://arxiv.org/abs/2302.13948v2</link><description>Background: Matrix-assisted laser desorption/ionization mass spectrometryimaging (MALDI MSI) displays significant potential for applications in cancerresearch, especially in tumor typing and subtyping. Lung cancer is the primarycause of tumor-related deaths, where the most lethal entities areadenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing betweenthese two common subtypes is crucial for therapy decisions and successfulpatient management. Results: We propose a new algebraic topological framework, which obtainsintrinsic information from MALDI data and transforms it to reflect topologicalpersistence. Our framework offers two main advantages. Firstly, topologicalpersistence aids in distinguishing the signal from noise. Secondly, itcompresses the MALDI data, saving storage space and optimizes computationaltime for subsequent classification tasks. We present an algorithm thatefficiently implements our topological framework, relying on a single tuningparameter. Afterwards, logistic regression and random forest classifiers areemployed on the extracted persistence features, thereby accomplishing anautomated tumor (sub-)typing process. To demonstrate the competitiveness of ourproposed framework, we conduct experiments on a real-world MALDI dataset usingcross-validation. Furthermore, we showcase the effectiveness of the singledenoising parameter by evaluating its performance on synthetic MALDI imageswith varying levels of noise. Conclusion: Our empirical experiments demonstrate that the proposed algebraictopological framework successfully captures and leverages the intrinsicspectral information from MALDI data, leading to competitive results inclassifying lung cancer subtypes. Moreover, the frameworks ability to befine-tuned for denoising highlights its versatility and potential for enhancingdata analysis in MALDI applications.</description><author>Gideon Klaila, Vladimir Vutov, Anastasios Stefanou</author><pubDate>Wed, 12 Jul 2023 13:19:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13948v2</guid></item><item><title>REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction</title><link>http://arxiv.org/abs/2306.15724v2</link><description>The ability to detect and analyze failed executions automatically is crucialfor an explainable and robust robotic system. Recently, Large Language Models(LLMs) have demonstrated strong reasoning abilities on textual inputs. Toleverage the power of LLM for robot failure explanation, we introduce aframework REFLECT, which queries LLM to identify and explain robot failuresgiven a hierarchical summary of robot past experiences generated frommulti-sensory data. Conditioned on the explanation, a task planner willgenerate an executable plan for the robot to correct the failure and completethe task. To systematically evaluate the framework, we create the RoboFaildataset with a variety of tasks and failure scenarios. We demonstrate that theLLM-based framework is able to generate informative failure explanations thatassist successful correction planning. Videos and code available at:https://roboreflect.github.io/.</description><author>Zeyi Liu, Arpit Bahety, Shuran Song</author><pubDate>Wed, 12 Jul 2023 13:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15724v2</guid></item><item><title>Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features</title><link>http://arxiv.org/abs/2307.03544v2</link><description>Roman Numeral analysis is the important task of identifying chords and theirfunctional context in pieces of tonal music. This paper presents a new approachto automatic Roman Numeral analysis in symbolic music. While existingtechniques rely on an intermediate lossy representation of the score, wepropose a new method based on Graph Neural Networks (GNNs) that enable thedirect description and processing of each individual note in the score. Theproposed architecture can leverage notewise features and interdependenciesbetween notes but yield onset-wise representation by virtue of our novel edgecontraction algorithm. Our results demonstrate that ChordGNN outperformsexisting state-of-the-art models, achieving higher accuracy in Roman Numeralanalysis on the reference datasets. In addition, we investigate variants of ourmodel using proposed techniques such as NADE, and post-processing of the chordpredictions. The full source code for this work is available athttps://github.com/manoskary/chordgnn</description><author>Emmanouil Karystinaios, Gerhard Widmer</author><pubDate>Wed, 12 Jul 2023 13:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03544v2</guid></item><item><title>Deep learning for dynamic graphs: models and benchmarks</title><link>http://arxiv.org/abs/2307.06104v1</link><description>Recent progress in research on Deep Graph Networks (DGNs) has led to amaturation of the domain of learning on graphs. Despite the growth of thisresearch field, there are still important challenges that are yet unsolved.Specifically, there is an urge of making DGNs suitable for predictive tasks onrealworld systems of interconnected entities, which evolve over time. With theaim of fostering research in the domain of dynamic graphs, at first, we surveyrecent advantages in learning both temporal and spatial information, providinga comprehensive overview of the current state-of-the-art in the domain ofrepresentation learning for dynamic graphs. Secondly, we conduct a fairperformance comparison among the most popular proposed approaches, leveragingrigorous model selection and assessment for all the methods, thus establishinga sound baseline for evaluating new architectures and approaches</description><author>Alessio Gravina, Davide Bacciu</author><pubDate>Wed, 12 Jul 2023 13:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06104v1</guid></item><item><title>A Deep Learning Method for Comparing Bayesian Hierarchical Models</title><link>http://arxiv.org/abs/2301.11873v2</link><description>Bayesian model comparison (BMC) offers a principled approach for assessingthe relative merits of competing computational models and propagatinguncertainty into model selection decisions. However, BMC is often intractablefor the popular class of hierarchical models due to their high-dimensionalnested parameter structure. To address this intractability, we propose a deeplearning method for performing BMC on any set of hierarchical models which canbe instantiated as probabilistic programs. Since our method enables amortizedinference, it allows efficient re-estimation of posterior model probabilitiesand fast performance validation prior to any real-data application. In a seriesof extensive validation studies, we benchmark the performance of our methodagainst the state-of-the-art bridge sampling method and demonstrate excellentamortized inference across all BMC settings. We then showcase our method bycomparing four hierarchical evidence accumulation models that have previouslybeen deemed intractable for BMC due to partly implicit likelihoods. In thisapplication, we corroborate evidence for the recently proposed L\'evy flightmodel of decision-making and show how transfer learning can be leveraged toenhance training efficiency. We provide reproducible code for all analyses andan open-source implementation of our method.</description><author>Lasse Elsemüller, Martin Schnuerch, Paul-Christian Bürkner, Stefan T. Radev</author><pubDate>Wed, 12 Jul 2023 12:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11873v2</guid></item><item><title>Divide-and-Conquer Fusion</title><link>http://arxiv.org/abs/2110.07265v2</link><description>Combining several (sample approximations of) distributions, which we termsub-posteriors, into a single distribution proportional to their product, is acommon challenge. Occurring, for instance, in distributed 'big data' problems,or when working under multi-party privacy constraints. Many existing approachesresort to approximating the individual sub-posteriors for practical necessity,then find either an analytical approximation or sample approximation of theresulting (product-pooled) posterior. The quality of the posteriorapproximation for these approaches is poor when the sub-posteriors fallout-with a narrow range of distributional form, such as being approximatelyGaussian. Recently, a Fusion approach has been proposed which finds an exactMonte Carlo approximation of the posterior, circumventing the drawbacks ofapproximate approaches. Unfortunately, existing Fusion approaches have a numberof computational limitations, particularly when unifying a large number ofsub-posteriors. In this paper, we generalise the theory underpinning existingFusion approaches, and embed the resulting methodology within a recursivedivide-and-conquer sequential Monte Carlo paradigm. This ultimately leads to acompetitive Fusion approach, which is robust to increasing numbers ofsub-posteriors.</description><author>Ryan S. Y. Chan, Murray Pollock, Adam M. Johansen, Gareth O. Roberts</author><pubDate>Wed, 12 Jul 2023 12:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.07265v2</guid></item><item><title>RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation</title><link>http://arxiv.org/abs/2307.06099v1</link><description>Glass-like objects are widespread in daily life but remain intractable to besegmented for most existing methods. The transparent property makes itdifficult to be distinguished from background, while the tiny separationboundary further impedes the acquisition of their exact contour. In this paper,by revealing the key co-evolution demand of semantic and boundary learning, wepropose a Selective Mutual Evolution (SME) module to enable the reciprocalfeature learning between them. Then to exploit the global shape context, wepropose a Structurally Attentive Refinement (SAR) module to conduct afine-grained feature refinement for those ambiguous points around the boundary.Finally, to further utilize the multi-scale representation, we integrate theabove two modules into a cascaded structure and then introduce a ReciprocalFeature Evolution Network (RFENet) for effective glass-like objectsegmentation. Extensive experiments demonstrate that our RFENet achievesstate-of-the-art performance on three popular public datasets.</description><author>Ke Fan, Changan Wang, Yabiao Wang, Chengjie Wang, Ran Yi, Lizhuang Ma</author><pubDate>Wed, 12 Jul 2023 12:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06099v1</guid></item><item><title>Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses</title><link>http://arxiv.org/abs/2302.01241v2</link><description>Many visualizations have been developed for explainable AI (XAI), but theyoften require further reasoning by users to interpret. We argue that XAI shouldsupport diagrammatic and abductive reasoning for the AI to perform hypothesisgeneration and evaluation to reduce the interpretability gap. We proposeDiagrammatization to i) perform Peircean abductive-deductive reasoning, ii)follow domain conventions, and iii) explain with diagrams visually or verbally.We implemented DiagramNet for a clinical application to predict cardiacdiagnoses from heart auscultation, and explain with shape-based murmurdiagrams. In modeling studies, we found that DiagramNet not only providesfaithful murmur shape explanations, but also has better prediction performancethan baseline models. We further demonstrate the interpretability andtrustworthiness of diagrammatic explanations in a qualitative user study withmedical students, showing that clinically-relevant, diagrammatic explanationsare preferred over technical saliency map explanations. This work contributesinsights into providing domain-conventional abductive explanations foruser-centric XAI.</description><author>Brian Y. Lim, Joseph P. Cahaly, Chester Y. F. Sng, Adam Chew</author><pubDate>Wed, 12 Jul 2023 12:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01241v2</guid></item><item><title>Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks</title><link>http://arxiv.org/abs/2307.06097v1</link><description>Stochastic Gumbel graph networks are proposed to learn high-dimensional timeseries, where the observed dimensions are often spatially correlated. To thatend, the observed randomness and spatial-correlations are captured by learningthe drift and diffusion terms of the stochastic differential equation with aGumble matrix embedding, respectively. In particular, this novel frameworkenables us to investigate the implicit regularization effect of the noise termsin S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs byderiving the difference between the two corresponding loss functions in a smallneighborhood of weight. Then, we employ Kuramoto's model to generate data forcomparing the spectral density from the Hessian Matrix of the two lossfunctions. Experimental results on real-world data, demonstrate that S-GGNsexhibit superior convergence, robustness, and generalization, compared withstate-of-the-arts.</description><author>Jin Guo, Ting Gao, Yufu Lan, Peng Zhang, Sikun Yang, Jinqiao Duan</author><pubDate>Wed, 12 Jul 2023 12:38:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06097v1</guid></item><item><title>Online Laplace Model Selection Revisited</title><link>http://arxiv.org/abs/2307.06093v1</link><description>The Laplace approximation provides a closed-form model selection objectivefor neural networks (NN). Online variants, which optimise NN parameters jointlywith hyperparameters, like weight decay strength, have seen renewed interest inthe Bayesian deep learning community. However, these methods violate Laplace'smethod's critical assumption that the approximation is performed around a modeof the loss, calling into question their soundness. This work re-derives onlineLaplace methods, showing them to target a variational bound on a mode-correctedvariant of the Laplace evidence which does not make stationarity assumptions.Online Laplace and its mode-corrected counterpart share stationary points where1. the NN parameters are a maximum a posteriori, satisfying the Laplacemethod's assumption, and 2. the hyperparameters maximise the Laplace evidence,motivating online methods. We demonstrate that these optima are roughlyattained in practise by online algorithms using full-batch gradient descent onUCI regression datasets. The optimised hyperparameters prevent overfitting andoutperform validation-based early stopping.</description><author>Jihao Andreas Lin, Javier Antorán, José Miguel Hernández-Lobato</author><pubDate>Wed, 12 Jul 2023 12:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06093v1</guid></item><item><title>Quantitative CLTs in Deep Neural Networks</title><link>http://arxiv.org/abs/2307.06092v1</link><description>We study the distribution of a fully connected neural network with randomGaussian weights and biases in which the hidden layer widths are proportionalto a large constant $n$. Under mild assumptions on the non-linearity, we obtainquantitative bounds on normal approximations valid at large but finite $n$ andany fixed network depth. Our theorems show, both for the finite-dimensionaldistributions and the entire process, that the distance between a random fullyconnected network (and its derivatives) to the corresponding infinite widthGaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponentdepending on the metric used to measure discrepancy. Our bounds are stronger interms of their dependence on network width than any previously available in theliterature.</description><author>Stefano Favaro, Boris Hanin, Domenico Marinucci, Ivan Nourdin, Giovanni Peccati</author><pubDate>Wed, 12 Jul 2023 12:35:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06092v1</guid></item></channel></rss>