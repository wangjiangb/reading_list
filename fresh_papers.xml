<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 05 May 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Multi-Space Alignments Towards Universal LiDAR Segmentation</title><link>http://arxiv.org/abs/2405.01538v1</link><description>A unified and versatile LiDAR segmentation model with strong robustness andgeneralizability is desirable for safe autonomous driving perception. This workpresents M3Net, a one-of-a-kind framework for fulfilling multi-task,multi-dataset, multi-modality LiDAR segmentation in a universal manner usingjust a single set of parameters. To better exploit data volume and diversity,we first combine large-scale driving datasets acquired by different types ofsensors from diverse scenes and then conduct alignments in three spaces, namelydata, feature, and label spaces, during the training. As a result, M3Net iscapable of taming heterogeneous data for training state-of-the-art LiDARsegmentation models. Extensive experiments on twelve LiDAR segmentationdatasets verify our effectiveness. Notably, using a shared set of parameters,M3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on theofficial benchmarks of SemanticKITTI, nuScenes, and Waymo Open.</description><author>Youquan Liu, Lingdong Kong, Xiaoyang Wu, Runnan Chen, Xin Li, Liang Pan, Ziwei Liu, Yuexin Ma</author><pubDate>Thu, 02 May 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01538v1</guid></item><item><title>Diffusive Gibbs Sampling</title><link>http://arxiv.org/abs/2402.03008v3</link><description>The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methodsfor multi-modal distributions presents a significant challenge in practicalapplications such as Bayesian inference and molecular dynamics. Addressingthis, we propose Diffusive Gibbs Sampling (DiGS), an innovative family ofsampling methods designed for effective sampling from distributionscharacterized by distant and disconnected modes. DiGS integrates recentdevelopments in diffusion models, leveraging Gaussian convolution to create anauxiliary noisy distribution that bridges isolated modes in the original spaceand applying Gibbs sampling to alternately draw samples from both spaces. Anovel Metropolis-within-Gibbs scheme is proposed to enhance mixing in thedenoising sampling step. DiGS exhibits a better mixing property for samplingmulti-modal distributions than state-of-the-art methods such as paralleltempering, attaining substantially improved performance across various tasks,including mixtures of Gaussians, Bayesian neural networks and moleculardynamics.</description><author>Wenlin Chen, Mingtian Zhang, Brooks Paige, José Miguel Hernández-Lobato, David Barber</author><pubDate>Thu, 02 May 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03008v3</guid></item><item><title>Customizing Text-to-Image Models with a Single Image Pair</title><link>http://arxiv.org/abs/2405.01536v1</link><description>Art reinterpretation is the practice of creating a variation of a referencework, making a paired artwork that exhibits a distinct artistic style. We askif such an image pair can be used to customize a generative model to capturethe demonstrated stylistic difference. We propose Pair Customization, a newcustomization method that learns stylistic difference from a single image pairand then applies the acquired style to the generation process. Unlike existingmethods that learn to mimic a single concept from a collection of images, ourmethod captures the stylistic difference between paired images. This allows usto apply a stylistic change without overfitting to the specific image contentin the examples. To address this new task, we employ a joint optimizationmethod that explicitly separates the style and content into distinct LoRAweight spaces. We optimize these style and content weights to reproduce thestyle and content images while encouraging their orthogonality. Duringinference, we modify the diffusion process via a new style guidance based onour learned weights. Both qualitative and quantitative experiments show thatour method can effectively learn style while avoiding overfitting to imagecontent, highlighting the potential of modeling such stylistic differences froma single image pair.</description><author>Maxwell Jones, Sheng-Yu Wang, Nupur Kumari, David Bau, Jun-Yan Zhu</author><pubDate>Thu, 02 May 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01536v1</guid></item><item><title>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</title><link>http://arxiv.org/abs/2405.01535v1</link><description>Proprietary LMs such as GPT-4 are often employed to assess the quality ofresponses from various LMs. However, concerns including transparency,controllability, and affordability strongly motivate the development ofopen-source LMs specialized in evaluations. On the other hand, existing openevaluator LMs exhibit critical shortcomings: 1) they issue scores thatsignificantly diverge from those assigned by humans, and 2) they lack theflexibility to perform both direct assessment and pairwise ranking, the twomost prevalent forms of assessment. Additionally, they do not possess theability to evaluate based on custom evaluation criteria, focusing instead ongeneral attributes like helpfulness and harmlessness. To address these issues,we introduce Prometheus 2, a more powerful evaluator LM than its predecessorthat closely mirrors human and GPT-4 judgements. Moreover, it is capable ofprocessing both direct assessment and pair-wise ranking formats grouped with auser-defined evaluation criteria. On four direct assessment benchmarks and fourpairwise ranking benchmarks, Prometheus 2 scores the highest correlation andagreement with humans and proprietary LM judges among all tested open evaluatorLMs. Our models, code, and data are all publicly available athttps://github.com/prometheus-eval/prometheus-eval.</description><author>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo</author><pubDate>Thu, 02 May 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01535v1</guid></item><item><title>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks</title><link>http://arxiv.org/abs/2405.01534v1</link><description>Large Language Models (LLMs) have been shown to be capable of performinghigh-level planning for long-horizon robotics tasks, yet existing methodsrequire access to a pre-defined skill library (e.g. picking, placing, pulling,pushing, navigating). However, LLM planning does not address how to design orlearn those behaviors, which remains challenging particularly in long-horizonsettings. Furthermore, for many tasks of interest, the robot needs to be ableto adjust its behavior in a fine-grained manner, requiring the agent to becapable of modifying low-level control actions. Can we instead use theinternet-scale knowledge from LLMs for high-level policies, guidingreinforcement learning (RL) policies to efficiently solve robotic control tasksonline without requiring a pre-determined set of skills? In this paper, wepropose Plan-Seq-Learn (PSL): a modular approach that uses motion planning tobridge the gap between abstract language and learned low-level control forsolving long-horizon robotics tasks from scratch. We demonstrate that PSLachieves state-of-the-art results on over 25 challenging robotics tasks with upto 10 stages. PSL solves long-horizon tasks from raw visual input spanning fourbenchmarks at success rates of over 85%, out-performing language-based,classical, and end-to-end approaches. Video results and code athttps://mihdalal.github.io/planseqlearn/</description><author>Murtaza Dalal, Tarun Chiruvolu, Devendra Chaplot, Ruslan Salakhutdinov</author><pubDate>Thu, 02 May 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01534v1</guid></item><item><title>OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning</title><link>http://arxiv.org/abs/2405.01533v1</link><description>The advances in multimodal large language models (MLLMs) have led to growinginterests in LLM-based autonomous driving agents to leverage their strongreasoning capabilities. However, capitalizing on MLLMs' strong reasoningcapabilities for improved planning behavior is challenging since planningrequires full 3D situational awareness beyond 2D reasoning. To address thischallenge, our work proposes a holistic framework for strong alignment betweenagent models and 3D driving tasks. Our framework starts with a novel 3D MLLMarchitecture that uses sparse queries to lift and compress visualrepresentations into 3D before feeding them into an LLM. This query-basedrepresentation allows us to jointly encode dynamic objects and static mapelements (e.g., traffic lanes), providing a condensed world model forperception-action alignment in 3D. We further propose OmniDrive-nuScenes, a newvisual question-answering dataset challenging the true 3D situational awarenessof a model with comprehensive visual question-answering (VQA) tasks, includingscene description, traffic regulation, 3D grounding, counterfactual reasoning,decision making and planning. Extensive studies show the effectiveness of theproposed architecture as well as the importance of the VQA tasks for reasoningand planning in complex 3D scenes.</description><author>Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, Jose M. Alvarez</author><pubDate>Thu, 02 May 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01533v1</guid></item><item><title>Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models</title><link>http://arxiv.org/abs/2405.01531v1</link><description>Concept Bottleneck Models (CBMs) ground image classification onhuman-understandable concepts to allow for interpretable model decisions.Crucially, the CBM design inherently allows for human interventions, in whichexpert users are given the ability to modify potentially misaligned conceptchoices to influence the decision behavior of the model in an interpretablefashion. However, existing approaches often require numerous humaninterventions per image to achieve strong performances, posing practicalchallenges in scenarios where obtaining human feedback is expensive. In thispaper, we find that this is noticeably driven by an independent treatment ofconcepts during intervention, wherein a change of one concept does notinfluence the use of other ones in the model's final decision. To address thisissue, we introduce a trainable concept intervention realignment module, whichleverages concept relations to realign concept assignments post-intervention.Across standard, real-world benchmarks, we find that concept realignment cansignificantly improve intervention efficacy; significantly reducing the numberof interventions needed to reach a target classification performance or conceptprediction accuracy. In addition, it easily integrates into existingconcept-based architectures without requiring changes to the models themselves.This reduced cost of human-model collaboration is crucial to enhancing thefeasibility of CBMs in resource-constrained environments.</description><author>Nishad Singhi, Jae Myung Kim, Karsten Roth, Zeynep Akata</author><pubDate>Thu, 02 May 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01531v1</guid></item><item><title>Track2Act: Predicting Point Tracks from Internet Videos enables Diverse Zero-shot Robot Manipulation</title><link>http://arxiv.org/abs/2405.01527v1</link><description>We seek to learn a generalizable goal-conditioned policy that enableszero-shot robot manipulation: interacting with unseen objects in novel sceneswithout test-time adaptation. While typical approaches rely on a large amountof demonstration data for such generalization, we propose an approach thatleverages web videos to predict plausible interaction plans and learns atask-agnostic transformation to obtain robot actions in the real world. Ourframework,Track2Act predicts tracks of how points in an image should move infuture time-steps based on a goal, and can be trained with diverse videos onthe web including those of humans and robots manipulating everyday objects. Weuse these 2D track predictions to infer a sequence of rigid transforms of theobject to be manipulated, and obtain robot end-effector poses that can beexecuted in an open-loop manner. We then refine this open-loop plan bypredicting residual actions through a closed loop policy trained with a fewembodiment-specific demonstrations. We show that this approach of combiningscalably learned track prediction with a residual policy requiring minimalin-domain robot-specific data enables zero-shot robot manipulation, and presenta wide array of real-world robot manipulation results across unseen tasks,objects, and scenes. https://homangab.github.io/track2act/</description><author>Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, Shubham Tulsiani</author><pubDate>Thu, 02 May 2024 18:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01527v1</guid></item><item><title>FLAME: Factuality-Aware Alignment for Large Language Models</title><link>http://arxiv.org/abs/2405.01525v1</link><description>Alignment is a standard procedure to fine-tune pre-trained large languagemodels (LLMs) to follow natural language instructions and serve as helpful AIassistants. We have observed, however, that the conventional alignment processfails to enhance the factual accuracy of LLMs, and often leads to thegeneration of more false facts (i.e. hallucination). In this paper, we studyhow to make the LLM alignment process more factual, by first identifyingfactors that lead to hallucination in both alignment steps:\ supervisedfine-tuning (SFT) and reinforcement learning (RL). In particular, we find thattraining the LLM on new knowledge or unfamiliar texts can encouragehallucination. This makes SFT less factual as it trains on human labeled datathat may be novel to the LLM. Furthermore, reward functions used in standard RLcan also encourage hallucination, because it guides the LLM to provide morehelpful responses on a diverse set of instructions, often preferring longer andmore detailed responses. Based on these observations, we proposefactuality-aware alignment, comprised of factuality-aware SFT andfactuality-aware RL through direct preference optimization. Experiments showthat our proposed factuality-aware alignment guides LLMs to output more factualresponses while maintaining instruction-following capability.</description><author>Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, Xilun Chen</author><pubDate>Thu, 02 May 2024 18:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01525v1</guid></item><item><title>A separability-based approach to quantifying generalization: which layer is best?</title><link>http://arxiv.org/abs/2405.01524v1</link><description>Generalization to unseen data remains poorly understood for deep learningclassification and foundation models. How can one assess the ability ofnetworks to adapt to new or extended versions of their input space in thespirit of few-shot learning, out-of-distribution generalization, and domainadaptation? Which layers of a network are likely to generalize best? We providea new method for evaluating the capacity of networks to represent a sampleddomain, regardless of whether the network has been trained on all classes inthe domain. Our approach is the following: after fine-tuning state-of-the-artpre-trained models for visual classification on a particular domain, we assesstheir performance on data from related but distinct variations in that domain.Generalization power is quantified as a function of the latent embeddings ofunseen data from intermediate layers for both unsupervised and supervisedsettings. Working throughout all stages of the network, we find that (i) highclassification accuracy does not imply high generalizability; and (ii) deeperlayers in a model do not always generalize the best, which has implications forpruning. Since the trends observed across datasets are largely consistent, weconclude that our approach reveals (a function of) the intrinsic capacity ofthe different layers of a model to generalize.</description><author>Luciano Dyballa, Evan Gerritz, Steven W. Zucker</author><pubDate>Thu, 02 May 2024 18:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01524v1</guid></item><item><title>Transformer-Aided Semantic Communications</title><link>http://arxiv.org/abs/2405.01521v1</link><description>The transformer structure employed in large language models (LLMs), as aspecialized category of deep neural networks (DNNs) featuring attentionmechanisms, stands out for their ability to identify and highlight the mostrelevant aspects of input data. Such a capability is particularly beneficial inaddressing a variety of communication challenges, notably in the realm ofsemantic communication where proper encoding of the relevant data is criticalespecially in systems with limited bandwidth. In this work, we employ visiontransformers specifically for the purpose of compression and compactrepresentation of the input image, with the goal of preserving semanticinformation throughout the transmission process. Through the use of theattention mechanism inherent in transformers, we create an attention mask. Thismask effectively prioritizes critical segments of images for transmission,ensuring that the reconstruction phase focuses on key objects highlighted bythe mask. Our methodology significantly improves the quality of semanticcommunication and optimizes bandwidth usage by encoding different parts of thedata in accordance with their semantic information content, thus enhancingoverall efficiency. We evaluate the effectiveness of our proposed frameworkusing the TinyImageNet dataset, focusing on both reconstruction quality andaccuracy. Our evaluation results demonstrate that our framework successfullypreserves semantic information, even when only a fraction of the encoded datais transmitted, according to the intended compression rates.</description><author>Matin Mortaheb, Erciyes Karakaya, Mohammad A. Amir Khojastepour, Sennur Ulukus</author><pubDate>Thu, 02 May 2024 18:50:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01521v1</guid></item><item><title>Large language models can accurately predict searcher preferences</title><link>http://arxiv.org/abs/2309.10621v2</link><description>Relevance labels, which indicate whether a search result is valuable to asearcher, are key to evaluating and optimising search systems. The best way tocapture the true preferences of users is to ask them for their careful feedbackon which results would be useful, but this approach does not scale to produce alarge number of labels. Getting relevance labels at scale is usually done withthird-party labellers, who judge on behalf of the user, but there is a risk oflow-quality data if the labeller doesn't understand user needs. To improvequality, one standard approach is to study real users through interviews, userstudies and direct feedback, find areas where labels are systematicallydisagreeing with users, then educate labellers about user needs through judgingguidelines, training and monitoring. This paper introduces an alternateapproach for improving label quality. It takes careful feedback from realusers, which by definition is the highest-quality first-party gold data thatcan be derived, and develops an large language model prompt that agrees withthat data. We present ideas and observations from deploying language models forlarge-scale relevance labelling at Bing, and illustrate with data from TREC. Wehave found large language models can be effective, with accuracy as good ashuman labellers and similar capability to pick the hardest queries, best runs,and best groups. Systematic changes to the prompts make a difference inaccuracy, but so too do simple paraphrases. To measure agreement with realsearchers needs high-quality ``gold'' labels, but with these we find thatmodels produce better labels than third-party workers, for a fraction of thecost, and these labels let us train notably better rankers.</description><author>Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra</author><pubDate>Thu, 02 May 2024 18:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10621v2</guid></item><item><title>Towards Real-time Learning in Large Language Models: A Critical Review</title><link>http://arxiv.org/abs/2404.18311v3</link><description>Real-time learning concerns the ability of learning systems to acquireknowledge over time, enabling their adaptation and generalization to noveltasks. It is a critical ability for intelligent, real-world systems, especiallywhen data may be insufficient or difficult to obtain. This review provides acomprehensive analysis of real-time learning in Large Language Models. Itsynthesizes the state-of-the-art real-time learning paradigms, includingcontinual learning, meta-learning, parameter-efficient learning, andmixture-of-experts learning. We demonstrate their utility for real-timelearning by describing specific achievements from these related topics andtheir critical factors. Finally, the paper highlights current problems andchallenges for future research in the field. By consolidating the latestrelevant research developments, this review offers a comprehensiveunderstanding of real-time learning and its implications for designing anddeveloping LLM-based learning systems addressing real-world problems.</description><author>Mladjan Jovanovic, Peter Voss</author><pubDate>Thu, 02 May 2024 18:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18311v3</guid></item><item><title>D2PO: Discriminator-Guided DPO with Response Evaluation Models</title><link>http://arxiv.org/abs/2405.01511v1</link><description>Varied approaches for aligning language models have been proposed, includingsupervised fine-tuning, RLHF, and direct optimization methods such as DPO.Although DPO has rapidly gained popularity due to its straightforward trainingprocess and competitive results, there is an open question of whether thereremain practical advantages of using a discriminator, like a reward model, toevaluate responses. We propose D2PO, discriminator-guided DPO, an approach forthe online setting where preferences are being collected throughout learning.As we collect gold preferences, we use these not only to train our policy, butto train a discriminative response evaluation model to silver-label even moresynthetic data for policy training. We explore this approach across a set ofdiverse tasks, including a realistic chat setting, we find that our approachleads to higher-quality outputs compared to DPO with the same data budget, andgreater efficiency in terms of preference data requirements. Furthermore, weshow conditions under which silver labeling is most helpful: it is mosteffective when training the policy with DPO, outperforming traditional PPO, andbenefits from maintaining a separate discriminator from the policy model.</description><author>Prasann Singhal, Nathan Lambert, Scott Niekum, Tanya Goyal, Greg Durrett</author><pubDate>Thu, 02 May 2024 18:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01511v1</guid></item><item><title>Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA</title><link>http://arxiv.org/abs/2304.06027v2</link><description>Recent works demonstrate a remarkable ability to customize text-to-imagediffusion models while only providing a few example images. What happens if youtry to customize such models using multiple, fine-grained concepts in asequential (i.e., continual) manner? In our work, we show that recentstate-of-the-art customization of text-to-image models suffer from catastrophicforgetting when new concepts arrive sequentially. Specifically, when adding anew concept, the ability to generate high quality images of past, similarconcepts degrade. To circumvent this forgetting, we propose a new method,C-LoRA, composed of a continually self-regularized low-rank adaptation in crossattention layers of the popular Stable Diffusion model. Furthermore, we usecustomization prompts which do not include the word of the customized object(i.e., "person" for a human face dataset) and are initialized as completelyrandom embeddings. Importantly, our method induces only marginal additionalparameter costs and requires no storage of user data for replay. We show thatC-LoRA not only outperforms several baselines for our proposed setting oftext-to-image continual customization, which we refer to as ContinualDiffusion, but that we achieve a new state-of-the-art in the well-establishedrehearsal-free continual learning setting for image classification. The highachieving performance of C-LoRA in two separate domains positions it as acompelling solution for a wide range of applications, and we believe it hassignificant potential for practical impact. Project page:https://jamessealesmith.github.io/continual-diffusion/</description><author>James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, Hongxia Jin</author><pubDate>Thu, 02 May 2024 18:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06027v2</guid></item><item><title>Accelerating Convergence in Bayesian Few-Shot Classification</title><link>http://arxiv.org/abs/2405.01507v1</link><description>Bayesian few-shot classification has been a focal point in the field offew-shot learning. This paper seamlessly integrates mirror descent-basedvariational inference into Gaussian process-based few-shot classification,addressing the challenge of non-conjugate inference. By leveragingnon-Euclidean geometry, mirror descent achieves accelerated convergence byproviding the steepest descent direction along the corresponding manifold. Italso exhibits the parameterization invariance property concerning thevariational distribution. Experimental results demonstrate competitiveclassification accuracy, improved uncertainty quantification, and fasterconvergence compared to baseline models. Additionally, we investigate theimpact of hyperparameters and components. Code is publicly available athttps://github.com/keanson/MD-BSFC.</description><author>Tianjun Ke, Haoqun Cao, Feng Zhou</author><pubDate>Thu, 02 May 2024 18:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01507v1</guid></item><item><title>Fourier Neural Operator with Learned Deformations for PDEs on General Geometries</title><link>http://arxiv.org/abs/2207.05209v2</link><description>Deep learning surrogate models have shown promise in solving partialdifferential equations (PDEs). Among them, the Fourier neural operator (FNO)achieves good accuracy, and is significantly faster compared to numericalsolvers, on a variety of PDEs, such as fluid flows. However, the FNO uses theFast Fourier transform (FFT), which is limited to rectangular domains withuniform grids. In this work, we propose a new framework, viz., geo-FNO, tosolve PDEs on arbitrary geometries. Geo-FNO learns to deform the input(physical) domain, which may be irregular, into a latent space with a uniformgrid. The FNO model with the FFT is applied in the latent space. The resultinggeo-FNO model has both the computation efficiency of FFT and the flexibility ofhandling arbitrary geometries. Our geo-FNO is also flexible in terms of itsinput formats, viz., point clouds, meshes, and design parameters are all validinputs. We consider a variety of PDEs such as the Elasticity, Plasticity,Euler's, and Navier-Stokes equations, and both forward modeling and inversedesign problems. Geo-FNO is $10^5$ times faster than the standard numericalsolvers and twice more accurate compared to direct interpolation on existingML-based PDE solvers such as the standard FNO.</description><author>Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, Anima Anandkumar</author><pubDate>Thu, 02 May 2024 18:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.05209v2</guid></item><item><title>BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</title><link>http://arxiv.org/abs/2310.04420v2</link><description>Understanding the functional organization of higher visual cortex is acentral focus in neuroscience. Past studies have primarily mapped the visualand semantic selectivity of neural populations using hand-selected stimuli,which may potentially bias results towards pre-existing hypotheses of visualcortex functionality. Moving beyond conventional approaches, we introduce adata-driven method that generates natural language descriptions for imagespredicted to maximally activate individual voxels of interest. Our method --Semantic Captioning Using Brain Alignments ("BrainSCUBA") -- builds upon therich embedding space learned by a contrastive vision-language model andutilizes a pre-trained large language model to generate interpretable captions.We validate our method through fine-grained voxel-level captioning acrosshigher-order visual regions. We further perform text-conditioned imagesynthesis with the captions, and show that our images are semantically coherentand yield high predicted activations. Finally, to demonstrate how our methodenables scientific discovery, we perform exploratory investigations on thedistribution of "person" representations in the brain, and discoverfine-grained semantic selectivity in body-selective areas. Unlike earlierstudies that decode text, our method derives voxel-wise captions of semanticselectivity. Our results show that BrainSCUBA is a promising means forunderstanding functional preferences in the brain, and provides motivation forfurther hypothesis-driven investigation of visual cortex.</description><author>Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe</author><pubDate>Thu, 02 May 2024 18:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04420v2</guid></item><item><title>PAM-UNet: Shifting Attention on Region of Interest in Medical Images</title><link>http://arxiv.org/abs/2405.01503v1</link><description>Computer-aided segmentation methods can assist medical personnel in improvingdiagnostic outcomes. While recent advancements like UNet and its variants haveshown promise, they face a critical challenge: balancing accuracy withcomputational efficiency. Shallow encoder architectures in UNets often struggleto capture crucial spatial features, leading in inaccurate and sparsesegmentation. To address this limitation, we propose a novel\underline{P}rogressive \underline{A}ttention based \underline{M}obile\underline{UNet} (\underline{PAM-UNet}) architecture. The inverted residual(IR) blocks in PAM-UNet help maintain a lightweight framework, while layerwise\textit{Progressive Luong Attention} ($\mathcal{PLA}$) promotes precisesegmentation by directing attention toward regions of interest duringsynthesis. Our approach prioritizes both accuracy and speed, achieving acommendable balance with a mean IoU of 74.65 and a dice score of 82.87, whilerequiring only 1.32 floating-point operations per second (FLOPS) on the LiverTumor Segmentation Benchmark (LiTS) 2017 dataset. These results highlight theimportance of developing efficient segmentation models to accelerate theadoption of AI in clinical practice.</description><author>Abhijit Das, Debesh Jha, Vandan Gorade, Koushik Biswas, Hongyi Pan, Zheyuan Zhang, Daniela P. Ladner, Yury Velichko, Amir Borhani, Ulas Bagci</author><pubDate>Thu, 02 May 2024 18:33:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01503v1</guid></item><item><title>Analyzing the Role of Semantic Representations in the Era of Large Language Models</title><link>http://arxiv.org/abs/2405.01502v1</link><description>Traditionally, natural language processing (NLP) models often use a rich setof features created by linguistic expertise, such as semantic representations.However, in the era of large language models (LLMs), more and more tasks areturned into generic, end-to-end sequence generation problems. In this paper, weinvestigate the question: what is the role of semantic representations in theera of LLMs? Specifically, we investigate the effect of Abstract MeaningRepresentation (AMR) across five diverse NLP tasks. We propose an AMR-drivenchain-of-thought prompting method, which we call AMRCoT, and find that itgenerally hurts performance more than it helps. To investigate what AMR mayhave to offer on these tasks, we conduct a series of analysis experiments. Wefind that it is difficult to predict which input examples AMR may help or hurton, but errors tend to arise with multi-word expressions, named entities, andin the final inference step where the LLM must connect its reasoning over theAMR to its prediction. We recommend focusing on these areas for future work insemantic representations for LLMs. Our code:https://github.com/causalNLP/amr_llm.</description><author>Zhijing Jin, Yuen Chen, Fernando Gonzalez, Jiarui Liu, Jiayi Zhang, Julian Michael, Bernhard Schölkopf, Mona Diab</author><pubDate>Thu, 02 May 2024 18:32:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01502v1</guid></item><item><title>Perception and Localization of Macular Degeneration Applying Convolutional Neural Network, ResNet and Grad-CAM</title><link>http://arxiv.org/abs/2404.15918v2</link><description>A well-known retinal disease that sends blurry visions to the affectedpatients is Macular Degeneration. This research is based on classifying thehealthy and macular degeneration fundus by localizing the affected region ofthe fundus. A CNN architecture and CNN with ResNet architecture (ResNet50,ResNet50v2, ResNet101, ResNet101v2, ResNet152, ResNet152v2) as the backbone areused to classify the two types of fundus. The data are split into threecategories including (a) Training set is 90% and Testing set is 10% (b)Training set is 80% and Testing set is 20%, (c) Training set is 50% and Testingset is 50%. After the training, the best model has been selected from theevaluation metrics. Among the models, CNN with a backbone of ResNet50 performsbest which gives the training accuracy of 98.7% for 90% train and 10% test datasplit. With this model, we have performed the Grad-CAM visualization to get theregion of the affected area of the fundus.</description><author>Tahmim Hossain, Sagor Chandro Bakchy</author><pubDate>Thu, 02 May 2024 18:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15918v2</guid></item><item><title>LocInv: Localization-aware Inversion for Text-Guided Image Editing</title><link>http://arxiv.org/abs/2405.01496v1</link><description>Large-scale Text-to-Image (T2I) diffusion models demonstrate significantgeneration capabilities based on textual prompts. Based on the T2I diffusionmodels, text-guided image editing research aims to empower users to manipulategenerated images by altering the text prompts. However, existing image editingtechniques are prone to editing over unintentional regions that are beyond theintended target area, primarily due to inaccuracies in cross-attention maps. Toaddress this problem, we propose Localization-aware Inversion (LocInv), whichexploits segmentation maps or bounding boxes as extra localization priors torefine the cross-attention maps in the denoising phases of the diffusionprocess. Through the dynamic updating of tokens corresponding to noun words inthe textual input, we are compelling the cross-attention maps to closely alignwith the correct noun and adjective words in the text prompt. Based on thistechnique, we achieve fine-grained image editing over particular objects whilepreventing undesired changes to other regions. Our method LocInv, based on thepublicly available Stable Diffusion, is extensively evaluated on a subset ofthe COCO dataset, and consistently obtains superior results both quantitativelyand qualitatively.The code will be released athttps://github.com/wangkai930418/DPL</description><author>Chuanming Tang, Kai Wang, Fei Yang, Joost van de Weijer</author><pubDate>Thu, 02 May 2024 18:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01496v1</guid></item><item><title>Navigating Heterogeneity and Privacy in One-Shot Federated Learning with Diffusion Models</title><link>http://arxiv.org/abs/2405.01494v1</link><description>Federated learning (FL) enables multiple clients to train models collectivelywhile preserving data privacy. However, FL faces challenges in terms ofcommunication cost and data heterogeneity. One-shot federated learning hasemerged as a solution by reducing communication rounds, improving efficiency,and providing better security against eavesdropping attacks. Nevertheless, dataheterogeneity remains a significant challenge, impacting performance. This workexplores the effectiveness of diffusion models in one-shot FL, demonstratingtheir applicability in addressing data heterogeneity and improving FLperformance. Additionally, we investigate the utility of our diffusion modelapproach, FedDiff, compared to other one-shot FL methods under differentialprivacy (DP). Furthermore, to improve generated sample quality under DPsettings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method,enhancing the effectiveness of generated data for global model training.</description><author>Matias Mendieta, Guangyu Sun, Chen Chen</author><pubDate>Thu, 02 May 2024 18:26:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01494v1</guid></item><item><title>FeNNol: an Efficient and Flexible Library for Building Force-field-enhanced Neural Network Potentials</title><link>http://arxiv.org/abs/2405.01491v1</link><description>Neural network interatomic potentials (NNPs) have recently proven to bepowerful tools to accurately model complex molecular systems while bypassingthe high numerical cost of ab-initio molecular dynamics simulations. In recentyears, numerous advances in model architectures as well as the development ofhybrid models combining machine-learning (ML) with more traditional,physically-motivated, force-field interactions have considerably increased thedesign space of ML potentials. In this paper, we present FeNNol, a new libraryfor building, training and running force-field-enhanced neural networkpotentials. It provides a flexible and modular system for building hybridmodels, allowing to easily combine state-of-the-art embeddings withML-parameterized physical interaction terms without the need for explicitprogramming. Furthermore, FeNNol leverages the automatic differentiation andjust-in-time compilation features of the Jax Python library to enable fastevaluation of NNPs, shrinking the performance gap between ML potentials andstandard force-fields. This is demonstrated with the popular ANI-2x modelreaching simulation speeds nearly on par with the AMOEBA polarizableforce-field on commodity GPUs (GPU=Graphics processing unit). We hope thatFeNNol will facilitate the development and application of new hybrid NNParchitectures for a wide range of molecular simulation problems.</description><author>Thomas Plé, Olivier Adjoua, Louis Lagardère, Jean-Philip Piquemal</author><pubDate>Thu, 02 May 2024 18:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01491v1</guid></item><item><title>Controllable Text Generation in the Instruction-Tuning Era</title><link>http://arxiv.org/abs/2405.01490v1</link><description>While most research on controllable text generation has focused on steeringbase Language Models, the emerging instruction-tuning and prompting paradigmoffers an alternate approach to controllability. We compile and releaseConGenBench, a testbed of 17 different controllable generation tasks, using asubset of it to benchmark the performance of 9 different baselines and methodson Instruction-tuned Language Models. To our surprise, we find thatprompting-based approaches outperform controllable text generation methods onmost datasets and tasks, highlighting a need for research on controllable textgeneration with Instruction-tuned Language Models in specific. Prompt-basedapproaches match human performance on most stylistic tasks while lagging onstructural tasks, foregrounding a need to study more varied constraints andmore challenging stylistic tasks. To facilitate such research, we provide analgorithm that uses only a task dataset and a Large Language Model within-context capabilities to automatically generate a constraint dataset. Thismethod eliminates the fields dependence on pre-curated constraint datasets,hence vastly expanding the range of constraints that can be studied in thefuture.</description><author>Dhananjay Ashok, Barnabas Poczos</author><pubDate>Thu, 02 May 2024 18:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01490v1</guid></item><item><title>Digital Twin Generators for Disease Modeling</title><link>http://arxiv.org/abs/2405.01488v1</link><description>A patient's digital twin is a computational model that describes theevolution of their health over time. Digital twins have the potential torevolutionize medicine by enabling individual-level computer simulations ofhuman health, which can be used to conduct more efficient clinical trials or torecommend personalized treatment options. Due to the overwhelming complexity ofhuman biology, machine learning approaches that leverage large datasets ofhistorical patients' longitudinal health records to generate patients' digitaltwins are more tractable than potential mechanistic models. In this manuscript,we describe a neural network architecture that can learn conditional generativemodels of clinical trajectories, which we call Digital Twin Generators (DTGs),that can create digital twins of individual patients. We show that the sameneural network architecture can be trained to generate accurate digital twinsfor patients across 13 different indications simply by changing the trainingset and tuning hyperparameters. By introducing a general purpose architecture,we aim to unlock the ability to scale machine learning approaches to largerdatasets and across more indications so that a digital twin could be createdfor any patient in the world.</description><author>Nameyeh Alam, Jake Basilico, Daniele Bertolini, Satish Casie Chetty, Heather D'Angelo, Ryan Douglas, Charles K. Fisher, Franklin Fuller, Melissa Gomes, Rishabh Gupta, Alex Lang, Anton Loukianov, Rachel Mak-McCully, Cary Murray, Hanalei Pham, Susanna Qiao, Elena Ryapolova-Webb, Aaron Smith, Dimitri Theoharatos, Anil Tolwani, Eric W. Tramel, Anna Vidovszky, Judy Viduya, Jonathan R. Walsh</author><pubDate>Thu, 02 May 2024 18:23:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01488v1</guid></item><item><title>Compact 3D Scene Representation via Self-Organizing Gaussian Grids</title><link>http://arxiv.org/abs/2312.13299v2</link><description>3D Gaussian Splatting has recently emerged as a highly promising techniquefor modeling of static 3D scenes. In contrast to Neural Radiance Fields, itutilizes efficient rasterization allowing for very fast rendering athigh-quality. However, the storage size is significantly higher, which hinderspractical deployment, e.g. on resource constrained devices. In this paper, weintroduce a compact scene representation organizing the parameters of 3DGaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring adrastic reduction in storage requirements without compromising visual qualityduring rendering. Central to our idea is the explicit exploitation ofperceptual redundancies present in natural scenes. In essence, the inherentnature of a scene allows for numerous permutations of Gaussian parameters toequivalently represent it. To this end, we propose a novel highly parallelalgorithm that regularly arranges the high-dimensional Gaussian parameters intoa 2D grid while preserving their neighborhood structure. During training, wefurther enforce local smoothness between the sorted parameters in the grid. Theuncompressed Gaussians use the same structure as 3DGS, ensuring a seamlessintegration with established renderers. Our method achieves a reduction factorof 17x to 42x in size for complex scenes with no increase in training time,marking a substantial leap forward in the domain of 3D scene distribution andconsumption. Additional information can be found on our project page:https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/</description><author>Wieland Morgenstern, Florian Barthel, Anna Hilsmann, Peter Eisert</author><pubDate>Thu, 02 May 2024 18:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13299v2</guid></item><item><title>Learning finitely correlated states: stability of the spectral reconstruction</title><link>http://arxiv.org/abs/2312.07516v2</link><description>We show that marginals of blocks of $t$ systems of any finitely correlatedtranslation invariant state on a chain can be learned, in trace distance, with$O(t^2)$ copies -- with an explicit dependence on local dimension, memorydimension and spectral properties of a certain map constructed from the state-- and computational complexity polynomial in $t$. The algorithm requires onlythe estimation of a marginal of a controlled size, in the worst case bounded bythe minimum bond dimension, from which it reconstructs a translation invariantmatrix product operator. In the analysis, a central role is played by thetheory of operator systems. A refined error bound can be proven for$C^*$-finitely correlated states, which have an operational interpretation interms of sequential quantum channels applied to the memory system. We can alsoobtain an analogous error bound for a class of matrix product density operatorsreconstructible by local marginals. In this case, a linear number of marginalsmust be estimated, obtaining a sample complexity of $\tilde{O}(t^3)$. Thelearning algorithm also works for states that are only close to a finitelycorrelated state, with the potential of providing competitive algorithms forother interesting families of states.</description><author>Marco Fanizza, Niklas Galke, Josep Lumbreras, Cambyse Rouzé, Andreas Winter</author><pubDate>Thu, 02 May 2024 18:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07516v2</guid></item><item><title>Neural Operator: Learning Maps Between Function Spaces</title><link>http://arxiv.org/abs/2108.08481v6</link><description>The classical development of neural networks has primarily focused onlearning mappings between finite dimensional Euclidean spaces or finite sets.We propose a generalization of neural networks to learn operators, termedneural operators, that map between infinite dimensional function spaces. Weformulate the neural operator as a composition of linear integral operators andnonlinear activation functions. We prove a universal approximation theorem forour proposed neural operator, showing that it can approximate any givennonlinear continuous operator. The proposed neural operators are alsodiscretization-invariant, i.e., they share the same model parameters amongdifferent discretization of the underlying function spaces. Furthermore, weintroduce four classes of efficient parameterization, viz., graph neuraloperators, multi-pole graph neural operators, low-rank neural operators, andFourier neural operators. An important application for neural operators islearning surrogate maps for the solution operators of partial differentialequations (PDEs). We consider standard PDEs such as the Burgers, Darcysubsurface flow, and the Navier-Stokes equations, and show that the proposedneural operators have superior performance compared to existing machinelearning based methodologies, while being several orders of magnitude fasterthan conventional PDE solvers.</description><author>Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar</author><pubDate>Thu, 02 May 2024 18:19:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.08481v6</guid></item><item><title>A Careful Examination of Large Language Model Performance on Grade School Arithmetic</title><link>http://arxiv.org/abs/2405.00332v2</link><description>Large language models (LLMs) have achieved impressive success on manybenchmarks for mathematical reasoning. However, there is growing concern thatsome of this performance actually reflects dataset contamination, where dataclosely resembling benchmark questions leaks into the training data, instead oftrue reasoning ability. To investigate this claim rigorously, we commissionGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style andcomplexity of the established GSM8k benchmark, the gold standard for measuringelementary mathematical reasoning. We ensure that the two benchmarks arecomparable across important metrics such as human solve rates, number of stepsin solution, answer magnitude, and more. When evaluating leading open- andclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, withseveral families of models (e.g., Phi and Mistral) showing evidence ofsystematic overfitting across almost all model sizes. At the same time, manymodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) showminimal signs of overfitting. Further analysis suggests a positive relationship(Spearman's r^2=0.32) between a model's probability of generating an examplefrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting thatmany models may have partially memorized GSM8k.</description><author>Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue</author><pubDate>Thu, 02 May 2024 18:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00332v2</guid></item><item><title>Designing Algorithmic Recommendations to Achieve Human-AI Complementarity</title><link>http://arxiv.org/abs/2405.01484v1</link><description>Algorithms frequently assist, rather than replace, human decision-makers.However, the design and analysis of algorithms often focus on predictingoutcomes and do not explicitly model their effect on human decisions. Thisdiscrepancy between the design and role of algorithmic assistants becomes ofparticular concern in light of empirical evidence that suggests thatalgorithmic assistants again and again fail to improve human decisions. In thisarticle, we formalize the design of recommendation algorithms that assist humandecision-makers without making restrictive ex-ante assumptions about howrecommendations affect decisions. We formulate an algorithmic-design problemthat leverages the potential-outcomes framework from causal inference to modelthe effect of recommendations on a human decision-maker's binary treatmentchoice. Within this model, we introduce a monotonicity assumption that leads toan intuitive classification of human responses to the algorithm. Under thismonotonicity assumption, we can express the human's response to algorithmicrecommendations in terms of their compliance with the algorithm and thedecision they would take if the algorithm sends no recommendation. We showcasethe utility of our framework using an online experiment that simulates a hiringtask. We argue that our approach explains the relative performance of differentrecommendation algorithms in the experiment, and can help design solutions thatrealize human-AI complementarity.</description><author>Bryce McLaughlin, Jann Spiess</author><pubDate>Thu, 02 May 2024 18:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01484v1</guid></item><item><title>MANTIS: Interleaved Multi-Image Instruction Tuning</title><link>http://arxiv.org/abs/2405.01483v1</link><description>The recent years have witnessed a great array of large multimodal models(LMMs) to effectively solve single-image vision language tasks. However, theirabilities to solve multi-image visual language tasks is yet to be improved. Theexisting multi-image LMMs (e.g. OpenFlamingo, Emu, Idefics, etc) mostly gaintheir multi-image ability through pre-training on hundreds of millions of noisyinterleaved image-text data from web, which is neither efficient nor effective.In this paper, we aim at building strong multi-image LMMs via instructiontuning with academic-level resources. Therefore, we meticulously constructMantis-Instruct containing 721K instances from 14 multi-image datasets. Wedesign Mantis-Instruct to cover different multi-image skills like co-reference,reasoning, comparing, temporal understanding. We combine Mantis-Instruct withseveral single-image visual-language datasets to train our model Mantis tohandle any interleaved image-text inputs. We evaluate the trained Mantis onfive multi-image benchmarks and eight single-image benchmarks. Though onlyrequiring academic-level resources (i.e. 36 hours on 16xA100-40G), Mantis-8Bcan achieve state-of-the-art performance on all the multi-image benchmarks andbeats the existing best multi-image LMM Idefics2-8B by an average of 9 absolutepoints. We observe that Mantis performs equivalently well on the held-in andheld-out evaluation benchmarks. We further evaluate Mantis on single-imagebenchmarks and demonstrate that Mantis can maintain a strong single-imageperformance on par with CogVLM and Emu2. Our results are particularlyencouraging as it shows that low-cost instruction tuning is indeed much moreeffective than intensive pre-training in terms of building multi-image LMMs.</description><author>Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, Wenhu Chen</author><pubDate>Thu, 02 May 2024 18:14:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01483v1</guid></item><item><title>NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment</title><link>http://arxiv.org/abs/2405.01481v1</link><description>Aligning Large Language Models (LLMs) with human values and preferences isessential for making them helpful and safe. However, building efficient toolsto perform alignment can be challenging, especially for the largest and mostcompetent LLMs which often contain tens or hundreds of billions of parameters.We create NeMo-Aligner, a toolkit for model alignment that can efficientlyscale to using hundreds of GPUs for training. NeMo-Aligner comes with highlyoptimized and scalable implementations for major paradigms of model alignmentsuch as: Reinforcement Learning from Human Feedback (RLHF), Direct PreferenceOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,our toolkit supports running most of the alignment techniques in a ParameterEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed forextensibility, allowing support for other alignment techniques with minimaleffort. It is open-sourced with Apache 2.0 License and we invite communitycontributions at https://github.com/NVIDIA/NeMo-Aligner</description><author>Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin, Ashwath Aithal, Oleksii Kuchaiev</author><pubDate>Thu, 02 May 2024 18:13:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01481v1</guid></item><item><title>Common pitfalls to avoid while using multiobjective optimization in machine learning</title><link>http://arxiv.org/abs/2405.01480v1</link><description>Recently, there has been an increasing interest in exploring the applicationof multiobjective optimization (MOO) in machine learning (ML). The interest isdriven by the numerous situations in real-life applications where multipleobjectives need to be optimized simultaneously. A key aspect of MOO is theexistence of a Pareto set, rather than a single optimal solution, whichillustrates the inherent trade-offs between objectives. Despite its potential,there is a noticeable lack of satisfactory literature that could serve as anentry-level guide for ML practitioners who want to use MOO. Hence, our goal inthis paper is to produce such a resource. We critically review previousstudies, particularly those involving MOO in deep learning (usingPhysics-Informed Neural Networks (PINNs) as a guiding example), and identifymisconceptions that highlight the need for a better grasp of MOO principles inML. Using MOO of PINNs as a case study, we demonstrate the interplay betweenthe data loss and the physics loss terms. We highlight the most common pitfallsone should avoid while using MOO techniques in ML. We begin by establishing thegroundwork for MOO, focusing on well-known approaches such as the weighted sum(WS) method, alongside more complex techniques like the multiobjective gradientdescent algorithm (MGDA). Additionally, we compare the results obtained fromthe WS and MGDA with one of the most common evolutionary algorithms, NSGA-II.We emphasize the importance of understanding the specific problem, theobjective space, and the selected MOO method, while also noting that neglectingfactors such as convergence can result in inaccurate outcomes and,consequently, a non-optimal solution. Our goal is to offer a clear andpractical guide for ML practitioners to effectively apply MOO, particularly inthe context of DL.</description><author>Junaid Akhter, Paul David Fährmann, Konstantin Sonntag, Sebastian Peitz</author><pubDate>Thu, 02 May 2024 18:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01480v1</guid></item><item><title>Fewer Truncations Improve Language Modeling</title><link>http://arxiv.org/abs/2404.10830v2</link><description>In large language model training, input documents are typically concatenatedtogether and then split into sequences of equal length to avoid padding tokens.Despite its efficiency, the concatenation approach compromises data integrity-- it inevitably breaks many documents into incomplete pieces, leading toexcessive truncations that hinder the model from learning to compose logicallycoherent and factually consistent content that is grounded on the completecontext. To address the issue, we propose Best-fit Packing, a scalable andefficient method that packs documents into training sequences throughlength-aware combinatorial optimization. Our method completely eliminatesunnecessary truncations while retaining the same training efficiency asconcatenation. Empirical results from both text and code pre-training show thatour method achieves superior performance (e.g., relatively +4.7% on readingcomprehension; +16.8% in context following; and +9.2% on program synthesis),and reduces closed-domain hallucination effectively by up to 58.3%.</description><author>Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, Stefano Soatto</author><pubDate>Thu, 02 May 2024 18:10:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10830v2</guid></item><item><title>V-FLUTE: Visual Figurative Language Understanding with Textual Explanations</title><link>http://arxiv.org/abs/2405.01474v1</link><description>Large Vision-Language models (VLMs) have demonstrated strong reasoningcapabilities in tasks requiring a fine-grained understanding of literal imagesand text, such as visual question-answering or visual entailment. However,there has been little exploration of these models' capabilities when presentedwith images and captions containing figurative phenomena such as metaphors orhumor, the meaning of which is often implicit. To close this gap, we propose anew task and a high-quality dataset: Visual Figurative Language Understandingwith Textual Explanations (V-FLUTE). We frame the visual figurative languageunderstanding problem as an explainable visual entailment task, where the modelhas to predict whether the image (premise) entails a claim (hypothesis) andjustify the predicted label with a textual explanation. Using a human-AIcollaboration framework, we build a high-quality dataset, V-FLUTE, thatcontains 6,027 &lt;image, claim, label, explanation&gt; instances spanning fivediverse multimodal figurative phenomena: metaphors, similes, idioms, sarcasm,and humor. The figurative phenomena can be present either in the image, thecaption, or both. We further conduct both automatic and human evaluations toassess current VLMs' capabilities in understanding figurative phenomena.</description><author>Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan</author><pubDate>Thu, 02 May 2024 18:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01474v1</guid></item><item><title>IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning</title><link>http://arxiv.org/abs/2405.01472v1</link><description>Imitation learning is a promising paradigm for training robot controlpolicies, but these policies can suffer from distribution shift, where theconditions at evaluation time differ from those in the training data. A popularapproach for increasing policy robustness to distribution shift is interactiveimitation learning (i.e., DAgger and variants), where a human operator providescorrective interventions during policy rollouts. However, collecting asufficient amount of interventions to cover the distribution of policy mistakescan be burdensome for human operators. We propose IntervenGen (I-Gen), a noveldata generation system that can autonomously produce a large set of correctiveinterventions with rich coverage of the state space from a small number ofhuman interventions. We apply I-Gen to 4 simulated environments and 1 physicalenvironment with object pose estimation error and show that it can increasepolicy robustness by up to 39x with only 10 human interventions. Videos andmore results are available at https://sites.google.com/view/intervengen2024.</description><author>Ryan Hoque, Ajay Mandlekar, Caelan Garrett, Ken Goldberg, Dieter Fox</author><pubDate>Thu, 02 May 2024 18:06:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01472v1</guid></item><item><title>Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges</title><link>http://arxiv.org/abs/2311.18044v3</link><description>Transfer learning is a conceptually-enticing paradigm in pursuit of trulyintelligent embodied agents. The core concept -- reusing prior knowledge tolearn in and from novel situations -- is successfully leveraged by humans tohandle novel situations. In recent years, transfer learning has receivedrenewed interest from the community from different perspectives, includingimitation learning, domain adaptation, and transfer of experience fromsimulation to the real world, among others. In this paper, we unify the conceptof transfer learning in robotics and provide the first taxonomy of its kindconsidering the key concepts of robot, task, and environment. Through a reviewof the promises and challenges in the field, we identify the need oftransferring at different abstraction levels, the need of quantifying thetransfer gap and the quality of transfer, as well as the dangers of negativetransfer. Via this position paper, we hope to channel the effort of thecommunity towards the most significant roadblocks to realize the full potentialof transfer learning in robotics.</description><author>Noémie Jaquier, Michael C. Welle, Andrej Gams, Kunpeng Yao, Bernardo Fichera, Aude Billard, Aleš Ude, Tamim Asfour, Danica Kragic</author><pubDate>Thu, 02 May 2024 18:03:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18044v3</guid></item><item><title>WildChat: 1M ChatGPT Interaction Logs in the Wild</title><link>http://arxiv.org/abs/2405.01470v1</link><description>Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despitetheir widespread use, there remains a lack of public datasets showcasing howthese tools are used by a population of users in practice. To bridge this gap,we offered free access to ChatGPT for online users in exchange for theiraffirmative, consensual opt-in to anonymously collect their chat transcriptsand request headers. From this, we compiled WildChat, a corpus of 1 millionuser-ChatGPT conversations, which consists of over 2.5 million interactionturns. We compare WildChat with other popular user-chatbot interactiondatasets, and find that our dataset offers the most diverse user prompts,contains the largest number of languages, and presents the richest variety ofpotentially toxic use-cases for researchers to study. In addition totimestamped chat transcripts, we enrich the dataset with demographic data,including state, country, and hashed IP addresses, alongside request headers.This augmentation allows for more detailed analysis of user behaviors acrossdifferent geographical regions and temporal dimensions. Finally, because itcaptures a broad range of use cases, we demonstrate the dataset's potentialutility in fine-tuning instruction-following models. WildChat is released athttps://wildchat.allen.ai under AI2 ImpACT Licenses.</description><author>Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng</author><pubDate>Thu, 02 May 2024 18:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01470v1</guid></item><item><title>Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning</title><link>http://arxiv.org/abs/2405.01469v1</link><description>AI Foundation models are gaining traction in various applications, includingmedical fields like radiology. However, medical foundation models are oftentested on limited tasks, leaving their generalisability and biases unexplored.We present RayDINO, a large visual encoder trained by self-supervision on 873kchest X-rays. We compare RayDINO to previous state-of-the-art models acrossnine radiology tasks, from classification and dense segmentation to textgeneration, and provide an in depth analysis of population, age and sex biasesof our model. Our findings suggest that self-supervision allows patient-centricAI proving useful in clinical workflows and interpreting X-rays holistically.With RayDINO and small task-specific adapters, we reach state-of-the-artresults and improve generalization to unseen populations while mitigating bias,illustrating the true promise of foundation models: versatility and robustness.</description><author>Théo Moutakanni, Piotr Bojanowski, Guillaume Chassagnon, Céline Hudelot, Armand Joulin, Yann LeCun, Matthew Muckley, Maxime Oquab, Marie-Pierre Revel, Maria Vakalopoulou</author><pubDate>Thu, 02 May 2024 17:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01469v1</guid></item><item><title>Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models</title><link>http://arxiv.org/abs/2405.01468v1</link><description>Pre-trained contrastive vision-language models have demonstrated remarkableperformance across a wide range of tasks. However, they often struggle onfine-trained datasets with categories not adequately represented duringpre-training, which makes adaptation necessary. Recent works have shownpromising results by utilizing samples from web-scale databases forretrieval-augmented adaptation, especially in low-data regimes. Despite theempirical success, understanding how retrieval impacts the adaptation ofvision-language models remains an open research question. In this work, weadopt a reflective perspective by presenting a systematic study to understandthe roles of key components in retrieval-augmented adaptation. We unveil newinsights on uni-modal and cross-modal retrieval and highlight the critical roleof logit ensemble for effective adaptation. We further present theoreticalunderpinnings that directly support our empirical observations.</description><author>Yifei Ming, Yixuan Li</author><pubDate>Thu, 02 May 2024 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01468v1</guid></item><item><title>Chronos: Learning the Language of Time Series</title><link>http://arxiv.org/abs/2403.07815v2</link><description>We introduce Chronos, a simple yet effective framework for pretrainedprobabilistic time series models. Chronos tokenizes time series values usingscaling and quantization into a fixed vocabulary and trains existingtransformer-based language model architectures on these tokenized time seriesvia the cross-entropy loss. We pretrained Chronos models based on the T5 family(ranging from 20M to 710M parameters) on a large collection of publiclyavailable datasets, complemented by a synthetic dataset that we generated viaGaussian processes to improve generalization. In a comprehensive benchmarkconsisting of 42 datasets, and comprising both classical local models and deeplearning methods, we show that Chronos models: (a) significantly outperformother methods on datasets that were part of the training corpus; and (b) havecomparable and occasionally superior zero-shot performance on new datasets,relative to methods that were trained specifically on them. Our resultsdemonstrate that Chronos models can leverage time series data from diversedomains to improve zero-shot accuracy on unseen forecasting tasks, positioningpretrained models as a viable tool to greatly simplify forecasting pipelines.</description><author>Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, Yuyang Wang</author><pubDate>Thu, 02 May 2024 17:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07815v2</guid></item><item><title>Dynamic Local Average Treatment Effects</title><link>http://arxiv.org/abs/2405.01463v1</link><description>We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliancethat arise in applications such as digital recommendations and adaptive medicaltrials. These are settings where decision makers encourage individuals to taketreatments over time, but adapt encouragements based on previousencouragements, treatments, states, and outcomes. Importantly, individuals maychoose to (not) comply with a treatment recommendation, whenever it is madeavailable to them, based on unobserved confounding factors. We providenon-parametric identification, estimation, and inference for Dynamic LocalAverage Treatment Effects, which are expected values of multi-period treatmentcontrasts among appropriately defined complier subpopulations. Under standardassumptions in the Instrumental Variable and DTR literature, we show that onecan identify local average effects of contrasts that correspond to offeringtreatment at any single time step. Under an additional cross-periodeffect-compliance independence assumption, which is satisfied in StaggeredAdoption settings and a generalization of them, which we define as StaggeredCompliance settings, we identify local average treatment effects of treating inmultiple time periods.</description><author>Ravi B. Sojitra, Vasilis Syrgkanis</author><pubDate>Thu, 02 May 2024 17:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01463v1</guid></item><item><title>Uncertainty for Active Learning on Graphs</title><link>http://arxiv.org/abs/2405.01462v1</link><description>Uncertainty Sampling is an Active Learning strategy that aims to improve thedata efficiency of machine learning models by iteratively acquiring labels ofdata points with the highest uncertainty. While it has proven effective forindependent data its applicability to graphs remains under-explored. We proposethe first extensive study of Uncertainty Sampling for node classification: (1)We benchmark Uncertainty Sampling beyond predictive uncertainty and highlight asignificant performance gap to other Active Learning strategies. (2) We developground-truth Bayesian uncertainty estimates in terms of the data generatingprocess and prove their effectiveness in guiding Uncertainty Sampling towardoptimal queries. We confirm our results on synthetic data and design anapproximate approach that consistently outperforms other uncertainty estimatorson real datasets. (3) Based on this analysis, we relate pitfalls in modelinguncertainty to existing methods. Our analysis enables and informs thedevelopment of principled uncertainty estimation on graphs.</description><author>Dominik Fuchsgruber, Tom Wollschläger, Bertrand Charpentier, Antonio Oroz, Stephan Günnemann</author><pubDate>Thu, 02 May 2024 17:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01462v1</guid></item><item><title>SATO: Stable Text-to-Motion Framework</title><link>http://arxiv.org/abs/2405.01461v1</link><description>Is the Text to Motion model robust? Recent advancements in Text to Motionmodels primarily stem from more accurate predictions of specific actions.However, the text modality typically relies solely on pre-trained ContrastiveLanguage-Image Pretraining (CLIP) models. Our research has uncovered asignificant issue with the text-to-motion model: its predictions often exhibitinconsistent outputs, resulting in vastly different or even incorrect poseswhen presented with semantically similar or identical text inputs. In thispaper, we undertake an analysis to elucidate the underlying causes of thisinstability, establishing a clear link between the unpredictability of modeloutputs and the erratic attention patterns of the text encoder module.Consequently, we introduce a formal framework aimed at addressing this issue,which we term the Stable Text-to-Motion Framework (SATO). SATO consists ofthree modules, each dedicated to stable attention, stable prediction, andmaintaining a balance between accuracy and robustness trade-off. We present amethodology for constructing an SATO that satisfies the stability of attentionand prediction. To verify the stability of the model, we introduced a newtextual synonym perturbation dataset based on HumanML3D and KIT-ML. Resultsshow that SATO is significantly more stable against synonyms and other slightperturbations while keeping its high accuracy performance.</description><author>Wenshuo Chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, Chen Chen</author><pubDate>Thu, 02 May 2024 17:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01461v1</guid></item><item><title>Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders</title><link>http://arxiv.org/abs/2405.01460v1</link><description>Unlearnable examples (UEs) seek to maximize testing error by making subtlemodifications to training examples that are correctly labeled. Defenses againstthese poisoning attacks can be categorized based on whether specificinterventions are adopted during training. The first approach is training-timedefense, such as adversarial training, which can mitigate poisoning effects butis computationally intensive. The other approach is pre-training purification,e.g., image short squeezing, which consists of several simple compressions butoften encounters challenges in dealing with various UEs. Our work provides anovel disentanglement mechanism to build an efficient pre-training purificationmethod. Firstly, we uncover rate-constrained variational autoencoders (VAEs),demonstrating a clear tendency to suppress the perturbations in UEs. Wesubsequently conduct a theoretical analysis for this phenomenon. Building uponthese insights, we introduce a disentangle variational autoencoder (D-VAE),capable of disentangling the perturbations with learnable class-wiseembeddings. Based on this network, a two-stage purification approach isnaturally developed. The first stage focuses on roughly eliminatingperturbations, while the second stage produces refined, poison-free results,ensuring effectiveness and robustness across various scenarios. Extensiveexperiments demonstrate the remarkable performance of our method acrossCIFAR-10, CIFAR-100, and a 100-class ImageNet-subset. Code is available athttps://github.com/yuyi-sd/D-VAE.</description><author>Yi Yu, Yufei Wang, Song Xia, Wenhan Yang, Shijian Lu, Yap-Peng Tan, Alex C. Kot</author><pubDate>Thu, 02 May 2024 17:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01460v1</guid></item><item><title>BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine</title><link>http://arxiv.org/abs/2405.00465v2</link><description>Large Language Models (LLMs) have swiftly emerged as vital resources fordifferent applications in the biomedical and healthcare domains; however, thesemodels encounter issues such as generating inaccurate information orhallucinations. Retrieval-augmented generation provided a solution for thesemodels to update knowledge and enhance their performance. In contrast toprevious retrieval-augmented LMs, which utilize specialized cross-attentionmechanisms to help LLM encode retrieved text, BiomedRAG adopts a simplerapproach by directly inputting the retrieved chunk-based documents into theLLM. This straightforward design is easily applicable to existing retrieval andlanguage models, effectively bypassing noise information in retrieveddocuments, particularly in noise-intensive tasks. Moreover, we demonstrate thepotential for utilizing the LLM to supervise the retrieval model in thebiomedical domain, enabling it to retrieve the document that assists the LM inimproving its predictions. Our experiments reveal that with the tunedscorer,\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLPtasks, encompassing information extraction (triple extraction, relationextraction), text classification, link prediction, and question-answering,leveraging over 9 datasets. For instance, in the triple extraction task,\textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1scores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively.</description><author>Mingchen Li, Halil Kilicoglu, Hua Xu, Rui Zhang</author><pubDate>Thu, 02 May 2024 17:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00465v2</guid></item><item><title>UQA: Corpus for Urdu Question Answering</title><link>http://arxiv.org/abs/2405.01458v1</link><description>This paper introduces UQA, a novel dataset for question answering and textcomprehension in Urdu, a low-resource language with over 70 million nativespeakers. UQA is generated by translating the Stanford Question AnsweringDataset (SQuAD2.0), a large-scale English QA dataset, using a technique calledEATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans inthe translated context paragraphs. The paper describes the process of selectingand evaluating the best translation model among two candidates: GoogleTranslator and Seamless M4T. The paper also benchmarks several state-of-the-artmultilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, andreports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and74.56 EM. UQA is a valuable resource for developing and testing multilingualNLP systems for Urdu and for enhancing the cross-lingual transferability ofexisting models. Further, the paper demonstrates the effectiveness of EATS forcreating high-quality datasets for other languages and domains. The UQA datasetand the code are publicly available at www.github.com/sameearif/UQA.</description><author>Samee Arif, Sualeha Farid, Awais Athar, Agha Ali Raza</author><pubDate>Thu, 02 May 2024 17:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01458v1</guid></item><item><title>CPLLM: Clinical Prediction with Large Language Models</title><link>http://arxiv.org/abs/2309.11295v2</link><description>We present Clinical Prediction with Large Language Models (CPLLM), a methodthat involves fine-tuning a pre-trained Large Language Model (LLM) for clinicaldisease and readmission prediction. We utilized quantization and fine-tuned theLLM using prompts. For diagnosis prediction, we predict whether patients willbe diagnosed with a target disease during their next visit or in the subsequentdiagnosis, leveraging their historical diagnosis records. We compared ourresults to various baselines, including RETAIN, and Med-BERT, the currentstate-of-the-art model for disease prediction using temporal structured EHRdata. In addition, We also evaluated CPLLM for patient hospital readmissionprediction and compared our method's performance with benchmark baselines. Ourexperiments have shown that our proposed method, CPLLM, surpasses all thetested models in terms of PR-AUC and ROC-AUC metrics, showing state-of-the-artresults for diagnosis prediction and patient hospital readmission prediction.Such a method can be easily implemented and integrated into the clinicalprocess to help care providers estimate the next steps of patients</description><author>Ofir Ben Shoham, Nadav Rappoport</author><pubDate>Thu, 02 May 2024 17:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11295v2</guid></item><item><title>Conformal online model aggregation</title><link>http://arxiv.org/abs/2403.15527v2</link><description>Conformal prediction equips machine learning models with a reasonable notionof uncertainty quantification without making strong distributional assumptions.It wraps around any black-box prediction model and converts point predictionsinto set predictions that have a predefined marginal coverage guarantee.However, conformal prediction only works if we fix the underlying machinelearning model in advance. A relatively unaddressed issue in conformalprediction is that of model selection and/or aggregation: for a given problem,which of the plethora of prediction methods (random forests, neural nets,regularized linear models, etc.) should we conformalize? This paper proposes anew approach towards conformal model aggregation in online settings that isbased on combining the prediction sets from several algorithms by voting, whereweights on the models are adapted over time based on past performance.</description><author>Matteo Gasparin, Aaditya Ramdas</author><pubDate>Thu, 02 May 2024 17:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15527v2</guid></item><item><title>Creative Problem Solving in Large Language and Vision Models -- What Would it Take?</title><link>http://arxiv.org/abs/2405.01453v1</link><description>In this paper, we discuss approaches for integrating Computational Creativity(CC) with research in large language and vision models (LLVMs) to address a keylimitation of these models, i.e., creative problem solving. We presentpreliminary experiments showing how CC principles can be applied to addressthis limitation through augmented prompting. With this work, we hope to fosterdiscussions of Computational Creativity in the context of ML algorithms forcreative problem solving in LLVMs. Our code is at:https://github.com/lnairGT/creative-problem-solving-LLMs</description><author>Lakshmi Nair, Evana Gizzi, Jivko Sinapov</author><pubDate>Thu, 02 May 2024 17:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01453v1</guid></item><item><title>Test-time Assessment of a Model's Performance on Unseen Domains via Optimal Transport</title><link>http://arxiv.org/abs/2405.01451v1</link><description>Gauging the performance of ML models on data from unseen domains at test-timeis essential yet a challenging problem due to the lack of labels in thissetting. Moreover, the performance of these models on in-distribution data is apoor indicator of their performance on data from unseen domains. Thus, it isessential to develop metrics that can provide insights into the model'sperformance at test time and can be computed only with the informationavailable at test time (such as their model parameters, the training data orits statistics, and the unlabeled test data). To this end, we propose a metricbased on Optimal Transport that is highly correlated with the model'sperformance on unseen domains and is efficiently computable only usinginformation available at test time. Concretely, our metric characterizes themodel's performance on unseen domains using only a small amount of unlabeleddata from these domains and data or statistics from the training (source)domain(s). Through extensive empirical evaluation using standard benchmarkdatasets, and their corruptions, we demonstrate the utility of our metric inestimating the model's performance in various practical applications. Theseinclude the problems of selecting the source data and architecture that leadsto the best performance on data from an unseen domain and the problem ofpredicting a deployed model's performance at test time on unseen domains. Ourempirical results show that our metric, which uses information from both thesource and the unseen domain, is highly correlated with the model'sperformance, achieving a significantly better correlation than that obtainedvia the popular prediction entropy-based metric, which is computed solely usingthe data from the unseen domain.</description><author>Akshay Mehra, Yunbei Zhang, Jihun Hamm</author><pubDate>Thu, 02 May 2024 17:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01451v1</guid></item><item><title>Disentangled Representation Learning</title><link>http://arxiv.org/abs/2211.11695v3</link><description>Disentangled Representation Learning (DRL) aims to learn a model capable ofidentifying and disentangling the underlying factors hidden in the observabledata in representation form. The process of separating underlying factors ofvariation into variables with semantic meaning benefits in learning explainablerepresentations of data, which imitates the meaningful understanding process ofhumans when observing an object or relation. As a general learning strategy,DRL has demonstrated its power in improving the model explainability,controlability, robustness, as well as generalization capacity in a wide rangeof scenarios such as computer vision, natural language processing, and datamining. In this article, we comprehensively investigate DRL from variousaspects including motivations, definitions, methodologies, evaluations,applications, and model designs. We first present two well-recognizeddefinitions, i.e., Intuitive Definition and Group Theory Definition fordisentangled representation learning. We further categorize the methodologiesfor DRL into four groups from the following perspectives, the model type,representation structure, supervision signal, and independence assumption. Wealso analyze principles to design different DRL models that may benefitdifferent tasks in practical applications. Finally, we point out challenges inDRL as well as potential research directions deserving future investigations.We believe this work may provide insights for promoting the DRL research in thecommunity.</description><author>Xin Wang, Hong Chen, Si'ao Tang, Zihao Wu, Wenwu Zhu</author><pubDate>Thu, 02 May 2024 17:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11695v3</guid></item><item><title>Improving Domain Generalization on Gaze Estimation via Branch-out Auxiliary Regularization</title><link>http://arxiv.org/abs/2405.01439v1</link><description>Despite remarkable advancements, mainstream gaze estimation techniques,particularly appearance-based methods, often suffer from performancedegradation in uncontrolled environments due to variations in illumination andindividual facial attributes. Existing domain adaptation strategies, limited bytheir need for target domain samples, may fall short in real-worldapplications. This letter introduces Branch-out Auxiliary Regularization (BAR),an innovative method designed to boost gaze estimation's generalizationcapabilities without requiring direct access to target domain data.Specifically, BAR integrates two auxiliary consistency regularization branches:one that uses augmented samples to counteract environmental variations, andanother that aligns gaze directions with positive source domain samples toencourage the learning of consistent gaze features. These auxiliary pathwaysstrengthen the core network and are integrated in a smooth, plug-and-playmanner, facilitating easy adaptation to various other models. Comprehensiveexperimental evaluations on four cross-dataset tasks demonstrate thesuperiority of our approach.</description><author>Ruijie Zhao, Pinyan Tang, Sihui Luo</author><pubDate>Thu, 02 May 2024 17:26:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01439v1</guid></item><item><title>Sparse is Enough in Fine-tuning Pre-trained Large Language Models</title><link>http://arxiv.org/abs/2312.11875v2</link><description>With the prevalence of pre-training-fine-tuning paradigm, how to efficientlyadapt the pre-trained model to the downstream tasks has been an intriguingissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed forlow-cost adaptation. Although PEFT has demonstrated effectiveness and beenwidely applied, the underlying principles are still unclear. In this paper, weadopt the PAC-Bayesian generalization error bound, viewing pre-training as ashift of prior distribution which leads to a tighter bound for generalizationerror. We validate this shift from the perspectives of oscillations in the losslandscape and the quasi-sparsity in gradient distribution. Based on this, wepropose a gradient-based sparse fine-tuning algorithm, named Sparse IncrementFine-Tuning (SIFT), and validate its effectiveness on a range of tasksincluding the GLUE Benchmark and Instruction-tuning. The code is accessible athttps://github.com/song-wx/SIFT/.</description><author>Weixi Song, Zuchao Li, Lefei Zhang, Hai Zhao, Bo Du</author><pubDate>Thu, 02 May 2024 17:25:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11875v2</guid></item><item><title>Retrieval-Augmented Generation for AI-Generated Content: A Survey</title><link>http://arxiv.org/abs/2402.19473v4</link><description>Advancements in model algorithms, the growth of foundational models, andaccess to high-quality datasets have propelled the evolution of ArtificialIntelligence Generated Content (AIGC). Despite its notable successes, AIGCstill faces hurdles such as updating knowledge, handling long-tail data,mitigating data leakage, and managing high training and inference costs.Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm toaddress such challenges. In particular, RAG introduces the informationretrieval process, which enhances the generation process by retrieving relevantobjects from available data stores, leading to higher accuracy and betterrobustness. In this paper, we comprehensively review existing efforts thatintegrate RAG technique into AIGC scenarios. We first classify RAG foundationsaccording to how the retriever augments the generator, distilling thefundamental abstractions of the augmentation methodologies for variousretrievers and generators. This unified perspective encompasses all RAGscenarios, illuminating advancements and pivotal technologies that help withpotential future progress. We also summarize additional enhancements methodsfor RAG, facilitating effective engineering and implementation of RAG systems.Then from another view, we survey on practical applications of RAG acrossdifferent modalities and tasks, offering valuable references for researchersand practitioners. Furthermore, we introduce the benchmarks for RAG, discussthe limitations of current RAG systems, and suggest potential directions forfuture research. Github: https://github.com/PKU-DAIR/RAG-Survey.</description><author>Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui</author><pubDate>Thu, 02 May 2024 17:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19473v4</guid></item><item><title>StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</title><link>http://arxiv.org/abs/2405.01434v1</link><description>For recent diffusion-based generative models, maintaining consistent contentacross a series of generated images, especially those containing subjects andcomplex details, presents a significant challenge. In this paper, we propose anew way of self-attention calculation, termed Consistent Self-Attention, thatsignificantly boosts the consistency between the generated images and augmentsprevalent pretrained diffusion-based text-to-image models in a zero-shotmanner. To extend our method to long-range video generation, we furtherintroduce a novel semantic space temporal motion prediction module, namedSemantic Motion Predictor. It is trained to estimate the motion conditionsbetween two provided images in the semantic spaces. This module converts thegenerated sequence of images into videos with smooth transitions and consistentsubjects that are significantly more stable than the modules based on latentspaces only, especially in the context of long video generation. By mergingthese two novel components, our framework, referred to as StoryDiffusion, candescribe a text-based story with consistent images or videos encompassing arich variety of contents. The proposed StoryDiffusion encompasses pioneeringexplorations in visual story generation with the presentation of images andvideos, which we hope could inspire more research from the aspect ofarchitectural modifications. Our code is made publicly available athttps://github.com/HVision-NKU/StoryDiffusion.</description><author>Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, Qibin Hou</author><pubDate>Thu, 02 May 2024 17:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01434v1</guid></item><item><title>Learning linear dynamical systems under convex constraints</title><link>http://arxiv.org/abs/2303.15121v3</link><description>We consider the problem of finite-time identification of linear dynamicalsystems from $T$ samples of a single trajectory. Recent results havepredominantly focused on the setup where no structural assumption is made onthe system matrix $A^* \in \mathbb{R}^{n \times n}$, and have consequentlyanalyzed the ordinary least squares (OLS) estimator in detail. We assume priorstructural information on $A^*$ is available, which can be captured in the formof a convex set $\mathcal{K}$ containing $A^*$. For the solution of the ensuingconstrained least squares estimator, we derive non-asymptotic error bounds inthe Frobenius norm that depend on the local size of $\mathcal{K}$ at $A^*$. Toillustrate the usefulness of these results, we instantiate them for fourexamples, namely when (i) $A^*$ is sparse and $\mathcal{K}$ is a suitablyscaled $\ell_1$ ball; (ii) $\mathcal{K}$ is a subspace; (iii) $\mathcal{K}$consists of matrices each of which is formed by sampling a bivariate convexfunction on a uniform $n \times n$ grid (convex regression); (iv) $\mathcal{K}$consists of matrices each row of which is formed by uniform sampling (with stepsize $1/T$) of a univariate Lipschitz function. In all these situations, weshow that $A^*$ can be reliably estimated for values of $T$ much smaller thanwhat is needed for the unconstrained setting.</description><author>Hemant Tyagi, Denis Efimov</author><pubDate>Thu, 02 May 2024 17:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15121v3</guid></item><item><title>KAN: Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2404.19756v2</link><description>Inspired by the Kolmogorov-Arnold representation theorem, we proposeKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-LayerPerceptrons (MLPs). While MLPs have fixed activation functions on nodes("neurons"), KANs have learnable activation functions on edges ("weights").KANs have no linear weights at all -- every weight parameter is replaced by aunivariate function parametrized as a spline. We show that this seeminglysimple change makes KANs outperform MLPs in terms of accuracy andinterpretability. For accuracy, much smaller KANs can achieve comparable orbetter accuracy than much larger MLPs in data fitting and PDE solving.Theoretically and empirically, KANs possess faster neural scaling laws thanMLPs. For interpretability, KANs can be intuitively visualized and can easilyinteract with human users. Through two examples in mathematics and physics,KANs are shown to be useful collaborators helping scientists (re)discovermathematical and physical laws. In summary, KANs are promising alternatives forMLPs, opening opportunities for further improving today's deep learning modelswhich rely heavily on MLPs.</description><author>Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljačić, Thomas Y. Hou, Max Tegmark</author><pubDate>Thu, 02 May 2024 17:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19756v2</guid></item><item><title>FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated Objects</title><link>http://arxiv.org/abs/2205.04382v6</link><description>We explore a novel method to perceive and manipulate 3D articulated objectsthat generalizes to enable a robot to articulate unseen classes of objects. Wepropose a vision-based system that learns to predict the potential motions ofthe parts of a variety of articulated objects to guide downstream motionplanning of the system to articulate the objects. To predict the objectmotions, we train a neural network to output a dense vector field representingthe point-wise motion direction of the points in the point cloud underarticulation. We then deploy an analytical motion planner based on this vectorfield to achieve a policy that yields maximum articulation. We train the visionsystem entirely in simulation, and we demonstrate the capability of our systemto generalize to unseen object instances and novel categories in bothsimulation and the real world, deploying our policy on a Sawyer robot with nofinetuning. Results show that our system achieves state-of-the-art performancein both simulated and real-world experiments.</description><author>Ben Eisner, Harry Zhang, David Held</author><pubDate>Thu, 02 May 2024 17:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04382v6</guid></item><item><title>In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies</title><link>http://arxiv.org/abs/2405.01425v1</link><description>We present a new random walk for uniformly sampling high-dimensional convexbodies. It achieves state-of-the-art runtime complexity with strongerguarantees on the output than previously known, namely in R\'enyi divergence(which implies TV, $\mathcal{W}_2$, KL, $\chi^2$). The proof departs from knownapproaches for polytime algorithms for the problem -- we utilize a stochasticdiffusion perspective to show contraction to the target distribution with therate of convergence determined by functional isoperimetric constants of thestationary density.</description><author>Yunbum Kook, Santosh S. Vempala, Matthew S. Zhang</author><pubDate>Thu, 02 May 2024 17:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01425v1</guid></item><item><title>Natural Language to Verilog: Design of a Recurrent Spiking Neural Network using Large Language Models and ChatGPT</title><link>http://arxiv.org/abs/2405.01419v1</link><description>This paper investigates the use of Large Language Models (LLMs) forautomating the generation of hardware description code, aiming to explore theirpotential in supporting and enhancing the development of efficient neuromorphiccomputing architectures. Building on our prior work, we employ OpenAI'sChatGPT4 and natural language prompts to synthesize a RTL Verilog module of aprogrammable recurrent spiking neural network, while also generating testbenches to assess the system's correctness. The resultant design was validatedin three case studies, the exclusive OR,the IRIS flower classification and theMNIST hand-written digit classification, achieving accuracies of up to 96.6%.To verify its synthesizability and implementability, the design was prototypedon a field-programmable gate array and implemented on SkyWater 130 nmtechnology by using an open-source electronic design automation flow.Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication programto further evaluate the system on-chip performance in the future.</description><author>Paola Vitolo, George Psaltakis, Michael Tomlinson, Gian Domenico Licciardo, Andreas G. Andreou</author><pubDate>Thu, 02 May 2024 17:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01419v1</guid></item><item><title>MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors</title><link>http://arxiv.org/abs/2405.01413v1</link><description>Large 2D vision-language models (2D-LLMs) have gained significant attentionby bridging Large Language Models (LLMs) with images using a simple projector.Inspired by their success, large 3D point cloud-language models (3D-LLMs) alsointegrate point clouds into LLMs. However, directly aligning point clouds withLLM requires expensive training costs, typically in hundreds of GPU-hours onA100, which hinders the development of 3D-LLMs. In this paper, we introduceMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTAresults while training for only 27 hours on one RTX 3090. Specifically, wepropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, whichcan leverage the similarity between 2D and 3D visual information. We introducea novel four-stage training strategy for modality alignment in a cascaded way,and a mixture of query experts module to adaptively aggregate features withhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methodsLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, whichis up to 260x fewer than existing methods. Extensive experiments show thatMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, withsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12increase on GPT-4 evaluation score for the challenging object captioning taskcompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.We are the first to explore the efficient 3D-LLM, offering new insights to thecommunity. Code and weights are available athttps://github.com/TangYuan96/MiniGPT-3D.</description><author>Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Yixue Hao, Long Hu, Min Chen</author><pubDate>Thu, 02 May 2024 17:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01413v1</guid></item><item><title>TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation</title><link>http://arxiv.org/abs/2211.09325v3</link><description>How do we imbue robots with the ability to efficiently manipulate unseenobjects and transfer relevant skills based on demonstrations? End-to-endlearning methods often fail to generalize to novel objects or unseenconfigurations. Instead, we focus on the task-specific pose relationshipbetween relevant parts of interacting objects. We conjecture that thisrelationship is a generalizable notion of a manipulation task that can transferto new objects in the same category; examples include the relationship betweenthe pose of a pan relative to an oven or the pose of a mug relative to a mugrack. We call this task-specific pose relationship "cross-pose" and provide amathematical definition of this concept. We propose a vision-based system thatlearns to estimate the cross-pose between two objects for a given manipulationtask using learned cross-object correspondences. The estimated cross-pose isthen used to guide a downstream motion planner to manipulate the objects intothe desired pose relationship (placing a pan into the oven or the mug onto themug rack). We demonstrate our method's capability to generalize to unseenobjects, in some cases after training on only 10 demonstrations in the realworld. Results show that our system achieves state-of-the-art performance inboth simulated and real-world experiments across a number of tasks.Supplementary information and videos can be found athttps://sites.google.com/view/tax-pose/home.</description><author>Chuer Pan, Brian Okorn, Harry Zhang, Ben Eisner, David Held</author><pubDate>Thu, 02 May 2024 17:04:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09325v3</guid></item><item><title>Goal-conditioned reinforcement learning for ultrasound navigation guidance</title><link>http://arxiv.org/abs/2405.01409v1</link><description>Transesophageal echocardiography (TEE) plays a pivotal role in cardiology fordiagnostic and interventional procedures. However, using it effectivelyrequires extensive training due to the intricate nature of image acquisitionand interpretation. To enhance the efficiency of novice sonographers and reducevariability in scan acquisitions, we propose a novel ultrasound (US) navigationassistance method based on contrastive learning as goal-conditionedreinforcement learning (GCRL). We augment the previous framework using a novelcontrastive patient batching method (CPB) and a data-augmented contrastiveloss, both of which we demonstrate are essential to ensure generalization toanatomical variations across patients. The proposed framework enablesnavigation to both standard diagnostic as well as intricate interventionalviews with a single model. Our method was developed with a large dataset of 789patients and obtained an average error of 6.56 mm in position and 9.36 degreesin angle on a testing dataset of 140 patients, which is competitive or superiorto models trained on individual views. Furthermore, we quantitatively validateour method's ability to navigate to interventional views such as the LeftAtrial Appendage (LAA) view used in LAA closure. Our approach holds promise inproviding valuable guidance during transesophageal ultrasound examinations,contributing to the advancement of skill acquisition for cardiac ultrasoundpractitioners.</description><author>Abdoul Aziz Amadou, Vivek Singh, Florin C. Ghesu, Young-Ho Kim, Laura Stanciulescu, Harshitha P. Sai, Puneet Sharma, Alistair Young, Ronak Rajani, Kawal Rhode</author><pubDate>Thu, 02 May 2024 17:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01409v1</guid></item><item><title>Random Pareto front surfaces</title><link>http://arxiv.org/abs/2405.01404v1</link><description>The Pareto front of a set of vectors is the subset which is comprised solelyof all of the best trade-off points. By interpolating this subset, we obtainthe optimal trade-off surface. In this work, we prove a very useful resultwhich states that all Pareto front surfaces can be explicitly parametrisedusing polar coordinates. In particular, our polar parametrisation result tellsus that we can fully characterise any Pareto front surface using the lengthfunction, which is a scalar-valued function that returns the projected lengthalong any positive radial direction. Consequently, by exploiting thisrepresentation, we show how it is possible to generalise many useful conceptsfrom linear algebra, probability and statistics, and decision theory tofunction over the space of Pareto front surfaces. Notably, we focus ourattention on the stochastic setting where the Pareto front surface itself is astochastic process. Among other things, we showcase how it is possible todefine and estimate many statistical quantities of interest such as theexpectation, covariance and quantile of any Pareto front surface distribution.As a motivating example, we investigate how these statistics can be used withina design of experiments setting, where the goal is to both infer and use thePareto front surface distribution in order to make effective decisions. Besidesthis, we also illustrate how these Pareto front ideas can be used within thecontext of extreme value theory. Finally, as a numerical example, we appliedsome of our new methodology on a real-world air pollution data set.</description><author>Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei</author><pubDate>Thu, 02 May 2024 16:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01404v1</guid></item><item><title>Unsupervised Flow Discovery from Task-oriented Dialogues</title><link>http://arxiv.org/abs/2405.01403v1</link><description>The design of dialogue flows is a critical but time-consuming task whendeveloping task-oriented dialogue (TOD) systems. We propose an approach for theunsupervised discovery of flows from dialogue history, thus making the processapplicable to any domain for which such an history is available. Briefly,utterances are represented in a vector space and clustered according to theirsemantic similarity. Clusters, which can be seen as dialogue states, are thenused as the vertices of a transition graph for representing the flows visually.We present concrete examples of flows, discovered from MultiWOZ, a public TODdataset. We further elaborate on their significance and relevance for theunderlying conversations and introduce an automatic validation metric for theirassessment. Experimental results demonstrate the potential of the proposedapproach for extracting meaningful flows from task-oriented conversations.</description><author>Patrícia Ferreira, Daniel Martins, Ana Alves, Catarina Silva, Hugo Gonçalo Oliveira</author><pubDate>Thu, 02 May 2024 16:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01403v1</guid></item><item><title>Learning Force Control for Legged Manipulation</title><link>http://arxiv.org/abs/2405.01402v1</link><description>Controlling contact forces during interactions is critical for locomotion andmanipulation tasks. While sim-to-real reinforcement learning (RL) has succeededin many contact-rich problems, current RL methods achieve forceful interactionsimplicitly without explicitly regulating forces. We propose a method fortraining RL policies for direct force control without requiring access to forcesensing. We showcase our method on a whole-body control platform of a quadrupedrobot with an arm. Such force control enables us to perform gravitycompensation and impedance control, unlocking compliant whole-bodymanipulation. The learned whole-body controller with variable compliance makesit intuitive for humans to teleoperate the robot by only commanding themanipulator, and the robot's body adjusts automatically to achieve the desiredposition and force. Consequently, a human teleoperator can easily demonstrate awide variety of loco-manipulation tasks. To the best of our knowledge, weprovide the first deployment of learned whole-body force control in leggedmanipulators, paving the way for more versatile and adaptable legged robots.</description><author>Tifanny Portela, Gabriel B. Margolis, Yandong Ji, Pulkit Agrawal</author><pubDate>Thu, 02 May 2024 16:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01402v1</guid></item><item><title>NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural Networks</title><link>http://arxiv.org/abs/2404.08786v3</link><description>Evolutionary Algorithms (EAs) play a crucial role in the architecturalconfiguration and training of Artificial Deep Neural Networks (DNNs), a processknown as neuroevolution. However, neuroevolution is hindered by its inherentcomputational expense, requiring multiple generations, a large population, andnumerous epochs. The most computationally intensive aspect lies in evaluatingthe fitness function of a single candidate solution. To address this challenge,we employ Surrogate-assisted EAs (SAEAs). While a few SAEAs approaches havebeen proposed in neuroevolution, none have been applied to truly large DNNs dueto issues like intractable information usage. In this work, drawing inspirationfrom Genetic Programming semantics, we use phenotypic distance vectors,outputted from DNNs, alongside Kriging Partial Least Squares (KPLS), anapproach that is effective in handling these large vectors, making themsuitable for search. Our proposed approach, named Neuro-Linear GeneticProgramming surrogate model (NeuroLGP-SM), efficiently and accurately estimatesDNN fitness without the need for complete evaluations. NeuroLGP-SM demonstratescompetitive or superior results compared to 12 other methods, includingNeuroLGP without SM, convolutional neural networks, support vector machines,and autoencoders. Additionally, it is worth noting that NeuroLGP-SM is 25% moreenergy-efficient than its NeuroLGP counterpart. This efficiency advantage addsto the overall appeal of our proposed NeuroLGP-SM in optimising theconfiguration of large DNNs.</description><author>Fergal Stapleton, Edgar Galván</author><pubDate>Thu, 02 May 2024 16:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08786v3</guid></item><item><title>USC: Uncompromising Spatial Constraints for Safety-Oriented 3D Object Detectors in Autonomous Driving</title><link>http://arxiv.org/abs/2209.10368v4</link><description>We consider the safety-oriented performance of 3D object detectors inautonomous driving contexts. Specifically, despite impressive results shown bythe mass literature, developers often find it hard to ensure the safedeployment of these learning-based perception models. Attributing the challengeto the lack of safety-oriented metrics, we hereby present uncompromisingspatial constraints (USC), which characterize a simple yet importantlocalization requirement demanding the predictions to fully cover the objectswhen seen from the autonomous vehicle. The constraints, as we formulate usingthe perspective and bird's-eye views, can be naturally reflected byquantitative measures, such that having an object detector with a higher scoreimplies a lower risk of collision. Finally, beyond model evaluation, weincorporate the quantitative measures into common loss functions to enablesafety-oriented fine-tuning for existing models. With experiments using thenuScenes dataset and a closed-loop simulation, our work demonstrates suchconsiderations of safety notions at the perception level not only improve modelperformances beyond accuracy but also allow for a more direct linkage to actualsystem safety.</description><author>Brian Hsuan-Cheng Liao, Chih-Hong Cheng, Hasan Esen, Alois Knoll</author><pubDate>Thu, 02 May 2024 16:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10368v4</guid></item><item><title>Strong Priority and Determinacy in Timed CCS</title><link>http://arxiv.org/abs/2403.04618v3</link><description>Building on the standard theory of process algebra with priorities, weidentify a new scheduling mechanism, called "constructive reduction" which isdesigned to capture the essence of synchronous programming. The distinctiveproperty of this evaluation strategy is to achieve determinacy-by-constructionfor multi-cast concurrent communication with shared memory. In the technicalsetting of CCS extended by clocks and priorities, we prove for a large class of"coherent" processes a confluence property for constructive reductions. We showthat under some restrictions, called "pivotability", coherence is preserved bythe operators of prefix, summation, parallel composition, restriction andhiding. Since this permits memory and sharing, we are able to cover a strictlylarger class of processes compared to those in Milner's classical confluencetheory for CCS without priorities.</description><author>Luigi Liquori, Michael Mendler</author><pubDate>Thu, 02 May 2024 16:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04618v3</guid></item><item><title>Adaptive Federated Learning with Auto-Tuned Clients</title><link>http://arxiv.org/abs/2306.11201v3</link><description>Federated learning (FL) is a distributed machine learning framework where theglobal model of a central server is trained via multiple collaborative steps byparticipating clients without sharing their data. While being a flexibleframework, where the distribution of local data, participation rate, andcomputing power of each client can greatly vary, such flexibility gives rise tomany new challenges, especially in the hyperparameter tuning on the clientside. We propose $\Delta$-SGD, a simple step size rule for SGD that enableseach client to use its own step size by adapting to the local smoothness of thefunction each client is optimizing. We provide theoretical and empiricalresults where the benefit of the client adaptivity is shown in various FLscenarios.</description><author>Junhyung Lyle Kim, Mohammad Taha Toghani, César A. Uribe, Anastasios Kyrillidis</author><pubDate>Thu, 02 May 2024 16:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11201v3</guid></item><item><title>Operational Support Estimator Networks</title><link>http://arxiv.org/abs/2307.06065v3</link><description>In this work, we propose a novel approach called Operational SupportEstimator Networks (OSENs) for the support estimation task. Support Estimation(SE) is defined as finding the locations of non-zero elements in sparsesignals. By its very nature, the mapping between the measurement and sparsesignal is a non-linear operation. Traditional support estimators rely oncomputationally expensive iterative signal recovery techniques to achieve suchnon-linearity. Contrary to the convolutional layers, the proposed OSEN approachconsists of operational layers that can learn such complex non-linearitieswithout the need for deep networks. In this way, the performance ofnon-iterative support estimation is greatly improved. Moreover, the operationallayers comprise so-called generative super neurons with non-local kernels. Thekernel location for each neuron/feature map is optimized jointly for the SEtask during training. We evaluate the OSENs in three different applications: i.support estimation from Compressive Sensing (CS) measurements, ii.representation-based classification, and iii. learning-aided CS reconstructionwhere the output of OSENs is used as prior knowledge to the CS algorithm forenhanced reconstruction. Experimental results show that the proposed approachachieves computational efficiency and outperforms competing methods, especiallyat low measurement rates by significant margins. The software implementation isshared at https://github.com/meteahishali/OSEN.</description><author>Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Moncef Gabbouj</author><pubDate>Thu, 02 May 2024 16:41:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06065v3</guid></item><item><title>Invariant Risk Minimization Is A Total Variation Model</title><link>http://arxiv.org/abs/2405.01389v1</link><description>Invariant risk minimization (IRM) is an arising approach to generalizeinvariant features to different environments in machine learning. While mostrelated works focus on new IRM settings or new application scenarios, themathematical essence of IRM remains to be properly explained. We verify thatIRM is essentially a total variation based on $L^2$ norm (TV-$\ell_2$) of thelearning risk with respect to the classifier variable. Moreover, we propose anovel IRM framework based on the TV-$\ell_1$ model. It not only expands theclasses of functions that can be used as the learning risk, but also has robustperformance in denoising and invariant feature preservation based on the coareaformula. We also illustrate some requirements for IRM-TV-$\ell_1$ to achieveout-of-distribution generalization. Experimental results show that the proposedframework achieves competitive performance in several benchmark machinelearning scenarios.</description><author>Zhao-Rong Lai, Wei-Wen Wang</author><pubDate>Thu, 02 May 2024 16:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01389v1</guid></item><item><title>Bridging Dimensions: Confident Reachability for High-Dimensional Controllers</title><link>http://arxiv.org/abs/2311.04843v4</link><description>Autonomous systems are increasingly implemented using end-to-endlearning-based controllers. Such controllers make decisions that are executedon the real system, with images as one of the primary sensing modalities. Deepneural networks form a fundamental building block of such controllers.Unfortunately, the existing neural-network verification tools do not scale toinputs with thousands of dimensions -- especially when the individual inputs(such as pixels) are devoid of clear physical meaning. This paper takes a steptowards connecting exhaustive closed-loop verification with high-dimensionalcontrollers. Our key insight is that the behavior of a high-dimensionalcontroller can be approximated with several low-dimensional controllers. Tobalance the approximation accuracy and verifiability of our low-dimensionalcontrollers, we leverage the latest verification-aware knowledge distillation.Then, we inflate low-dimensional reachability results with statisticalapproximation errors, yielding a high-confidence reachability guarantee for thehigh-dimensional controller. We investigate two inflation techniques -- basedon trajectories and control actions -- both of which show convincingperformance in three OpenAI gym benchmarks.</description><author>Yuang Geng, Jake Brandon Baldauf, Souradeep Dutta, Chao Huang, Ivan Ruchkin</author><pubDate>Thu, 02 May 2024 16:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04843v4</guid></item><item><title>Probabilistic Feature Augmentation for AIS-Based Multi-Path Long-Term Vessel Trajectory Forecasting</title><link>http://arxiv.org/abs/2310.18948v5</link><description>Maritime transportation is paramount in achieving global economic growth,entailing concurrent ecological obligations in sustainability and safeguardingendangered marine species, most notably preserving large whale populations. Inthis regard, the Automatic Identification System (AIS) data plays a significantrole by offering real-time streaming data on vessel movement, allowing enhancedtraffic monitoring. This study explores using AIS data to preventvessel-to-whale collisions by forecasting long-term vessel trajectories fromengineered AIS data sequences. For such a task, we have developed anencoder-decoder model architecture using Bidirectional Long Short-Term MemoryNetworks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1to 3 hours of AIS data as input. We feed the model with probabilistic featuresengineered from historical AIS data that refer to each trajectory's potentialroute and destination. The model then predicts the vessel's trajectory,considering these additional features by leveraging convolutional layers forspatial feature learning and a position-aware attention mechanism thatincreases the importance of recent timesteps of a sequence during temporalfeature learning. The probabilistic features have an F1 Score of approximately85% and 75% for each feature type, respectively, demonstrating theireffectiveness in augmenting information to the neural network. We test ourmodel on the Gulf of St. Lawrence, a region known to be the habitat of NorthAtlantic Right Whales (NARW). Our model achieved a high R2 score of over 98%using various techniques and features. It stands out among other approaches asit can make complex decisions during turnings and path selection. Our studyhighlights the potential of data engineering and trajectory forecasting modelsfor marine life species preservation.</description><author>Gabriel Spadon, Jay Kumar, Derek Eden, Josh van Berkel, Tom Foster, Amilcar Soares, Ronan Fablet, Stan Matwin, Ronald Pelot</author><pubDate>Thu, 02 May 2024 16:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18948v5</guid></item><item><title>Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving</title><link>http://arxiv.org/abs/2405.01379v1</link><description>Natural language explanations have become a proxy for evaluating explainableand multi-step Natural Language Inference (NLI) models. However, assessing thevalidity of explanations for NLI is challenging as it typically involves thecrowd-sourcing of apposite datasets, a process that is time-consuming and proneto logical errors. To address existing limitations, this paper investigates theverification and refinement of natural language explanations through theintegration of Large Language Models (LLMs) and Theorem Provers (TPs).Specifically, we present a neuro-symbolic framework, named Explanation-Refiner,that augments a TP with LLMs to generate and formalise explanatory sentencesand suggest potential inference strategies for NLI. In turn, the TP is employedto provide formal guarantees on the logical validity of the explanations and togenerate feedback for subsequent improvements. We demonstrate howExplanation-Refiner can be jointly used to evaluate explanatory reasoning,autoformalisation, and error correction mechanisms of state-of-the-art LLMs aswell as to automatically enhance the quality of human-annotated explanations ofvariable complexity in different domains.</description><author>Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas</author><pubDate>Thu, 02 May 2024 16:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01379v1</guid></item><item><title>Topics in the Study of the Pragmatic Functions of Phonetic Reduction in Dialog</title><link>http://arxiv.org/abs/2405.01376v1</link><description>Reduced articulatory precision is common in speech, but for dialog itsacoustic properties and pragmatic functions have been little studied. We heretry to remedy this gap. This technical report contains content that was omittedfrom the journal article (Ward et al. 2024, submitted). Specifically, we herereport 1) lessons learned about annotating for perceived reduction, 2) thefinding that, unlike in read speech, the correlates of reduction in dialoginclude high pitch, wide pitch range, and intensity, and 3) a baseline modelfor predicting reduction in dialog, using simple acoustic/prosodic features,that achieves correlations with human perceptions of 0.24 for English, and 0.17for Spanish. We also provide examples of additional possible pragmaticfunctions of reduction in English, and various discussion, observations andspeculations</description><author>Nigel G. Ward, Carlos A. Ortega</author><pubDate>Thu, 02 May 2024 16:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01376v1</guid></item><item><title>ATOM: Attention Mixer for Efficient Dataset Distillation</title><link>http://arxiv.org/abs/2405.01373v1</link><description>Recent works in dataset distillation seek to minimize training expenses bygenerating a condensed synthetic dataset that encapsulates the informationpresent in a larger real dataset. These approaches ultimately aim to attaintest accuracy levels akin to those achieved by models trained on the entiretyof the original dataset. Previous studies in feature and distribution matchinghave achieved significant results without incurring the costs of bi-leveloptimization in the distillation process. Despite their convincing efficiency,many of these methods suffer from marginal downstream performance improvements,limited distillation of contextual information, and subpar cross-architecturegeneralization. To address these challenges in dataset distillation, we proposethe ATtentiOn Mixer (ATOM) module to efficiently distill large datasets using amixture of channel and spatial-wise attention in the feature matching process.Spatial-wise attention helps guide the learning process based on consistentlocalization of classes in their respective images, allowing for distillationfrom a broader receptive field. Meanwhile, channel-wise attention captures thecontextual information associated with the class itself, thus making thesynthetic image more informative for training. By integrating both types ofattention, our ATOM module demonstrates superior performance across variouscomputer vision datasets, including CIFAR10/100 and TinyImagenet. Notably, ourmethod significantly improves performance in scenarios with a low number ofimages per class, thereby enhancing its potential. Furthermore, we maintain theimprovement in cross-architectures and applications such as neural architecturesearch.</description><author>Samir Khaki, Ahmad Sajedi, Kai Wang, Lucy Z. Liu, Yuri A. Lawryshyn, Konstantinos N. Plataniotis</author><pubDate>Thu, 02 May 2024 16:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01373v1</guid></item><item><title>Dynamic Online Ensembles of Basis Expansions</title><link>http://arxiv.org/abs/2405.01365v1</link><description>Practical Bayesian learning often requires (1) online inference, (2) dynamicmodels, and (3) ensembling over multiple different models. Recent advances haveshown how to use random feature approximations to achieve scalable, onlineensembling of Gaussian processes with desirable theoretical properties andfruitful applications. One key to these methods' success is the inclusion of arandom walk on the model parameters, which makes models dynamic. We show thatthese methods can be generalized easily to any basis expansion model and thatusing alternative basis expansions, such as Hilbert space Gaussian processes,often results in better performance. To simplify the process of choosing aspecific basis expansion, our method's generality also allows the ensembling ofseveral entirely different models, for example, a Gaussian process andpolynomial regression. Finally, we propose a novel method to ensemble staticand dynamic models together.</description><author>Daniel Waxman, Petar M. Djurić</author><pubDate>Thu, 02 May 2024 16:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01365v1</guid></item><item><title>GAIA: A General AI Assistant for Intelligent Accelerator Operations</title><link>http://arxiv.org/abs/2405.01359v1</link><description>Large-scale machines like particle accelerators are usually run by a team ofexperienced operators. In case of a particle accelerator, these operatorspossess suitable background knowledge on both accelerator physics and thetechnology comprising the machine. Due to the complexity of the machine,particular subsystems of the machine are taken care of by experts, who theoperators can turn to. In this work the reasoning and action (ReAct) promptingparadigm is used to couple an open-weights large language model (LLM) with ahigh-level machine control system framework and other tools, e.g. theelectronic logbook or machine design documentation. By doing so, a multi-expertretrieval augmented generation (RAG) system is implemented, which assistsoperators in knowledge retrieval tasks, interacts with the machine directly ifneeded, or writes high level control system scripts. This consolidation ofexpert knowledge and machine interaction can simplify and speed up machineoperation tasks for both new and experienced human operators.</description><author>Frank Mayet</author><pubDate>Thu, 02 May 2024 16:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01359v1</guid></item><item><title>Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance</title><link>http://arxiv.org/abs/2405.01356v1</link><description>In subject-driven text-to-image synthesis, the synthesis process tends to beheavily influenced by the reference images provided by users, often overlookingcrucial attributes detailed in the text prompt. In this work, we proposeSubject-Agnostic Guidance (SAG), a simple yet effective solution to remedy theproblem. We show that through constructing a subject-agnostic condition andapplying our proposed dual classifier-free guidance, one could obtain outputsconsistent with both the given subject and input text prompts. We validate theefficacy of our approach through both optimization-based and encoder-basedmethods. Additionally, we demonstrate its applicability in second-ordercustomization methods, where an encoder-based model is fine-tuned withDreamBooth. Our approach is conceptually simple and requires only minimal codemodifications, but leads to substantial quality improvements, as evidenced byour evaluations and user studies.</description><author>Kelvin C. K. Chan, Yang Zhao, Xuhui Jia, Ming-Hsuan Yang, Huisheng Wang</author><pubDate>Thu, 02 May 2024 16:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01356v1</guid></item><item><title>StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization</title><link>http://arxiv.org/abs/2311.14495v2</link><description>In this paper, we investigate the long-term memory learning capabilities ofstate-space models (SSMs) from the perspective of parameterization. We provethat state-space models without any reparameterization exhibit a memorylimitation similar to that of traditional RNNs: the target relationships thatcan be stably approximated by state-space models must have an exponentialdecaying memory. Our analysis identifies this ``curse of memory'' as a resultof the recurrent weights converging to a stability boundary, suggesting that areparameterization technique can be effective. To this end, we introduce aclass of reparameterization techniques for SSMs that effectively lift itsmemory limitations. Besides improving approximation capabilities, we furtherillustrate that a principled choice of reparameterization scheme can alsoenhance optimization stability. We validate our findings using syntheticdatasets, language models and image classifications.</description><author>Shida Wang, Qianxiao Li</author><pubDate>Thu, 02 May 2024 16:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14495v2</guid></item><item><title>Sparse multi-view hand-object reconstruction for unseen environments</title><link>http://arxiv.org/abs/2405.01353v1</link><description>Recent works in hand-object reconstruction mainly focus on the single-viewand dense multi-view settings. On the one hand, single-view methods canleverage learned shape priors to generalise to unseen objects but are prone toinaccuracies due to occlusions. On the other hand, dense multi-view methods arevery accurate but cannot easily adapt to unseen objects without further datacollection. In contrast, sparse multi-view methods can take advantage of theadditional views to tackle occlusion, while keeping the computational cost lowcompared to dense multi-view methods. In this paper, we consider the problem ofhand-object reconstruction with unseen objects in the sparse multi-viewsetting. Given multiple RGB images of the hand and object captured at the sametime, our model SVHO combines the predictions from each view into a unifiedreconstruction without optimisation across views. We train our model on asynthetic hand-object dataset and evaluate directly on a real world recordedhand-object dataset with unseen objects. We show that while reconstruction ofunseen hands and objects from RGB is challenging, additional views can helpimprove the reconstruction quality.</description><author>Yik Lung Pang, Changjae Oh, Andrea Cavallaro</author><pubDate>Thu, 02 May 2024 16:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01353v1</guid></item><item><title>Community-Invariant Graph Contrastive Learning</title><link>http://arxiv.org/abs/2405.01350v1</link><description>Graph augmentation has received great attention in recent years for graphcontrastive learning (GCL) to learn well-generalized node/graphrepresentations. However, mainstream GCL methods often favor randomlydisrupting graphs for augmentation, which shows limited generalization andinevitably leads to the corruption of high-level graph information, i.e., thegraph community. Moreover, current knowledge-based graph augmentation methodscan only focus on either topology or node features, causing the model to lackrobustness against various types of noise. To address these limitations, thisresearch investigated the role of the graph community in graph augmentation andfigured out its crucial advantage for learnable graph augmentation. Based onour observations, we propose a community-invariant GCL framework to maintaingraph community structure during learnable graph augmentation. By maximizingthe spectral changes, this framework unifies the constraints of both topologyand feature augmentation, enhancing the model's robustness. Empirical evidenceon 21 benchmark datasets demonstrates the exclusive merits of our framework.Code is released on Github (https://github.com/ShiyinTan/CI-GCL.git).</description><author>Shiyin Tan, Dongyuan Li, Renhe Jiang, Ying Zhang, Manabu Okumura</author><pubDate>Thu, 02 May 2024 15:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01350v1</guid></item><item><title>Position Paper: Beyond Robustness Against Single Attack Types</title><link>http://arxiv.org/abs/2405.01349v1</link><description>Current research on defending against adversarial examples focuses primarilyon achieving robustness against a single attack type such as $\ell_2$ or$\ell_{\infty}$-bounded attacks. However, the space of possible perturbationsis much larger and currently cannot be modeled by a single attack type. Thediscrepancy between the focus of current defenses and the space of attacks ofinterest calls to question the practicality of existing defenses and thereliability of their evaluation. In this position paper, we argue that theresearch community should look beyond single attack robustness, and we drawattention to three potential directions involving robustness against multipleattacks: simultaneous multiattack robustness, unforeseen attack robustness, anda newly defined problem setting which we call continual adaptive robustness. Weprovide a unified framework which rigorously defines these problem settings,synthesize existing research in these fields, and outline open directions. Wehope that our position paper inspires more research in simultaneousmultiattack, unforeseen attack, and continual adaptive robustness.</description><author>Sihui Dai, Chong Xiang, Tong Wu, Prateek Mittal</author><pubDate>Thu, 02 May 2024 15:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01349v1</guid></item><item><title>ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion</title><link>http://arxiv.org/abs/2404.17230v2</link><description>We introduce ObjectAdd, a training-free diffusion modification method to adduser-expected objects into user-specified area. The motive of ObjectAdd stemsfrom: first, describing everything in one prompt can be difficult, and second,users often need to add objects into the generated image. To accommodate withreal world, our ObjectAdd maintains accurate image consistency after addingobjects with technical innovations in: (1) embedding-level concatenation toensure correct text embedding coalesce; (2) object-driven layout control withlatent and attention injection to ensure objects accessing user-specified area;(3) prompted image inpainting in an attention refocusing &amp; object expansionfashion to ensure rest of the image stays the same. With a text-prompted image,our ObjectAdd allows users to specify a box and an object, and achieves: (1)adding object inside the box area; (2) exact content outside the box area; (3)flawless fusion between the two areas</description><author>Ziyue Zhang, Mingbao Lin, Rongrong Ji</author><pubDate>Thu, 02 May 2024 15:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17230v2</guid></item><item><title>Beyond Individual Input for Deep Anomaly Detection on Tabular Data</title><link>http://arxiv.org/abs/2305.15121v6</link><description>Anomaly detection is vital in many domains, such as finance, healthcare, andcybersecurity. In this paper, we propose a novel deep anomaly detection methodfor tabular data that leverages Non-Parametric Transformers (NPTs), a modelinitially proposed for supervised tasks, to capture both feature-feature andsample-sample dependencies. In a reconstruction-based framework, we train anNPT to reconstruct masked features of normal samples. In a non-parametricfashion, we leverage the whole training set during inference and use themodel's ability to reconstruct the masked features to generate an anomalyscore. To the best of our knowledge, this is the first work to successfullycombine feature-feature and sample-sample dependencies for anomaly detection ontabular datasets. Through extensive experiments on 31 benchmark tabulardatasets, we demonstrate that our method achieves state-of-the-art performance,outperforming existing methods by 2.4% and 1.2% in terms of F1-score and AUROC,respectively. Our ablation study further proves that modeling both types ofdependencies is crucial for anomaly detection on tabular data.</description><author>Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan</author><pubDate>Thu, 02 May 2024 15:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15121v6</guid></item><item><title>MEGA-DAgger: Imitation Learning with Multiple Imperfect Experts</title><link>http://arxiv.org/abs/2303.00638v3</link><description>Imitation learning has been widely applied to various autonomous systemsthanks to recent development in interactive algorithms that address covariateshift and compounding errors induced by traditional approaches like behaviorcloning. However, existing interactive imitation learning methods assume accessto one perfect expert. Whereas in reality, it is more likely to have multipleimperfect experts instead. In this paper, we propose MEGA-DAgger, a new DAggervariant that is suitable for interactive learning with multiple imperfectexperts. First, unsafe demonstrations are filtered while aggregating thetraining data, so the imperfect demonstrations have little influence whentraining the novice policy. Next, experts are evaluated and compared onscenarios-specific metrics to resolve the conflicted labels among experts.Through experiments in autonomous racing scenarios, we demonstrate that policylearned using MEGA-DAgger can outperform both experts and policies learnedusing the state-of-the-art interactive imitation learning algorithms such asHuman-Gated DAgger. The supplementary video can be found at\url{https://youtu.be/wPCht31MHrw}.</description><author>Xiatao Sun, Shuo Yang, Mingyan Zhou, Kunpeng Liu, Rahul Mangharam</author><pubDate>Thu, 02 May 2024 15:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00638v3</guid></item><item><title>The Power of Question Translation Training in Multilingual Reasoning: Broadened Scope and Deepened Insights</title><link>http://arxiv.org/abs/2405.01345v1</link><description>Bridging the significant gap between large language model's English andnon-English performance presents a great challenge. While some previous studiesattempt to mitigate this gap with translated training data, the recentlyproposed question alignment approach leverages the model's English expertise toimprove multilingual performance with minimum usage of expensive, error-pronetranslation. In this paper, we explore how broadly this method can be appliedby examining its effects in reasoning with executable code and reasoning withcommon sense. We also explore how to apply this approach efficiently toextremely large language models using proxy-tuning. Experiment results onmultilingual reasoning benchmarks mGSM, mSVAMP and xCSQA demonstrate that thequestion alignment approach can be used to boost multilingual performanceacross diverse reasoning scenarios, model families, and sizes. For instance,when applied to the LLaMA2 models, our method brings an average accuracyimprovements of 12.2% on mGSM even with the 70B model. To understand themechanism of its success, we analyze representation space, chain-of-thought andtranslation data scales, which reveals how question translation trainingstrengthens language alignment within LLMs and shapes their working patterns.</description><author>Wenhao Zhu, Shujian Huang, Fei Yuan, Cheng Chen, Jiajun Chen, Alexandra Birch</author><pubDate>Thu, 02 May 2024 15:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01345v1</guid></item><item><title>Language Models As Semantic Indexers</title><link>http://arxiv.org/abs/2310.07815v2</link><description>Semantic identifier (ID) is an important concept in information retrievalthat aims to preserve the semantics of objects such as documents and itemsinside their IDs. Previous studies typically adopt a two-stage pipeline tolearn semantic IDs by first procuring embeddings using off-the-shelf textencoders and then deriving IDs based on the embeddings. However, each stepintroduces potential information loss, and there is usually an inherentmismatch between the distribution of embeddings within the latent spaceproduced by text encoders and the anticipated distribution required forsemantic indexing. It is non-trivial to design a method that can learn thedocument's semantic representations and its hierarchical structuresimultaneously, given that semantic IDs are discrete and sequentiallystructured, and the semantic supervision is deficient. In this paper, weintroduce LMIndexer, a self-supervised framework to learn semantic IDs with agenerative language model. We tackle the challenge of sequential discrete ID byintroducing a semantic indexer capable of generating neural sequential discreterepresentations with progressive training and contrastive learning. In responseto the semantic supervision deficiency, we propose to train the model with aself-supervised document reconstruction objective. We show the high quality ofthe learned IDs and demonstrate their effectiveness on three tasks includingrecommendation, product search, and document retrieval on five datasets fromvarious domains. Code is available athttps://github.com/PeterGriffinJin/LMIndexer.</description><author>Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, Suhang Wang, Jiawei Han, Xianfeng Tang</author><pubDate>Thu, 02 May 2024 15:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07815v2</guid></item><item><title>Multi-view Action Recognition via Directed Gromov-Wasserstein Discrepancy</title><link>http://arxiv.org/abs/2405.01337v1</link><description>Action recognition has become one of the popular research topics in computervision. There are various methods based on Convolutional Networks andself-attention mechanisms as Transformers to solve both spatial and temporaldimensions problems of action recognition tasks that achieve competitiveperformances. However, these methods lack a guarantee of the correctness of theaction subject that the models give attention to, i.e., how to ensure an actionrecognition model focuses on the proper action subject to make a reasonableaction prediction. In this paper, we propose a multi-view attention consistencymethod that computes the similarity between two attentions from two differentviews of the action videos using Directed Gromov-Wasserstein Discrepancy.Furthermore, our approach applies the idea of Neural Radiance Field toimplicitly render the features from novel views when training on single-viewdatasets. Therefore, the contributions in this work are three-fold. Firstly, weintroduce the multi-view attention consistency to solve the problem ofreasonable prediction in action recognition. Secondly, we define a new metricfor multi-view consistent attention using Directed Gromov-WassersteinDiscrepancy. Thirdly, we built an action recognition model based on VideoTransformers and Neural Radiance Fields. Compared to the recent actionrecognition methods, the proposed approach achieves state-of-the-art results onthree large-scale datasets, i.e., Jester, Something-Something V2, andKinetics-400.</description><author>Hoang-Quan Nguyen, Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 02 May 2024 15:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01337v1</guid></item><item><title>MBDP: A Model-based Approach to Achieve both Robustness and Sample Efficiency via Double Dropout Planning</title><link>http://arxiv.org/abs/2108.01295v2</link><description>Model-based reinforcement learning is a widely accepted solution for solvingexcessive sample demands. However, the predictions of the dynamics models areoften not accurate enough, and the resulting bias may incur catastrophicdecisions due to insufficient robustness. Therefore, it is highly desired toinvestigate how to improve the robustness of model-based RL algorithms whilemaintaining high sampling efficiency. In this paper, we propose Model-BasedDouble-dropout Planning (MBDP) to balance robustness and efficiency. MBDPconsists of two kinds of dropout mechanisms, where the rollout-dropout aims toimprove the robustness with a small cost of sample efficiency, while themodel-dropout is designed to compensate for the lost efficiency at a slightexpense of robustness. By combining them in a complementary way, MBDP providesa flexible control mechanism to meet different demands of robustness andefficiency by tuning two corresponding dropout ratios. The effectiveness ofMBDP is demonstrated both theoretically and experimentally.</description><author>Wanpeng Zhang, Xi Xiao, Yao Yao, Mingzhe Chen, Dijun Luo</author><pubDate>Thu, 02 May 2024 15:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.01295v2</guid></item><item><title>NeRF in Robotics: A Survey</title><link>http://arxiv.org/abs/2405.01333v1</link><description>Meticulous 3D environment representations have been a longstanding goal incomputer vision and robotics fields. The recent emergence of neural implicitrepresentations has introduced radical innovation to this field as implicitrepresentations enable numerous capabilities. Among these, the Neural RadianceField (NeRF) has sparked a trend because of the huge representationaladvantages, such as simplified mathematical models, compact environmentstorage, and continuous scene representations. Apart from computer vision, NeRFhas also shown tremendous potential in the field of robotics. Thus, we createthis survey to provide a comprehensive understanding of NeRF in the field ofrobotics. By exploring the advantages and limitations of NeRF, as well as itscurrent applications and future potential, we hope to shed light on thispromising area of research. Our survey is divided into two main sections:\textit{The Application of NeRF in Robotics} and \textit{The Advance of NeRF inRobotics}, from the perspective of how NeRF enters the field of robotics. Inthe first section, we introduce and analyze some works that have been or couldbe used in the field of robotics from the perception and interactionperspectives. In the second section, we show some works related to improvingNeRF's own properties, which are essential for deploying NeRF in the field ofrobotics. In the discussion section of the review, we summarize the existingchallenges and provide some valuable future research directions for reference.</description><author>Guangming Wang, Lei Pan, Songyou Peng, Shaohui Liu, Chenfeng Xu, Yanzi Miao, Wei Zhan, Masayoshi Tomizuka, Marc Pollefeys, Hesheng Wang</author><pubDate>Thu, 02 May 2024 15:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01333v1</guid></item><item><title>Constrained Reinforcement Learning Under Model Mismatch</title><link>http://arxiv.org/abs/2405.01327v1</link><description>Existing studies on constrained reinforcement learning (RL) may obtain awell-performing policy in the training environment. However, when deployed in areal environment, it may easily violate constraints that were originallysatisfied during training because there might be model mismatch between thetraining and real environments. To address the above challenge, we formulatethe problem as constrained RL under model uncertainty, where the goal is tolearn a good policy that optimizes the reward and at the same time satisfy theconstraint under model mismatch. We develop a Robust Constrained PolicyOptimization (RCPO) algorithm, which is the first algorithm that applies tolarge/continuous state space and has theoretical guarantees on worst-casereward improvement and constraint violation at each iteration during thetraining. We demonstrate the effectiveness of our algorithm on a set of RLtasks with constraints.</description><author>Zhongchang Sun, Sihong He, Fei Miao, Shaofeng Zou</author><pubDate>Thu, 02 May 2024 15:31:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01327v1</guid></item><item><title>Multi-modal Learnable Queries for Image Aesthetics Assessment</title><link>http://arxiv.org/abs/2405.01326v1</link><description>Image aesthetics assessment (IAA) is attracting wide interest with theprevalence of social media. The problem is challenging due to its subjectiveand ambiguous nature. Instead of directly extracting aesthetic features solelyfrom the image, user comments associated with an image could potentiallyprovide complementary knowledge that is useful for IAA. With existinglarge-scale pre-trained models demonstrating strong capabilities in extractinghigh-quality transferable visual and textual features, learnable queries areshown to be effective in extracting useful features from the pre-trained visualfeatures. Therefore, in this paper, we propose MMLQ, which utilizes multi-modallearnable queries to extract aesthetics-related features from multi-modalpre-trained features. Extensive experimental results demonstrate that MMLQachieves new state-of-the-art performance on multi-modal IAA, beating previousmethods by 7.7% and 8.3% in terms of SRCC and PLCC, respectively.</description><author>Zhiwei Xiong, Yunfan Zhang, Zhiqi Shen, Peiran Ren, Han Yu</author><pubDate>Thu, 02 May 2024 15:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01326v1</guid></item><item><title>LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked</title><link>http://arxiv.org/abs/2308.07308v4</link><description>Large language models (LLMs) are popular for high-quality text generation butcan produce harmful content, even when aligned with human values throughreinforcement learning. Adversarial prompts can bypass their safety measures.We propose LLM Self Defense, a simple approach to defend against these attacksby having an LLM screen the induced responses. Our method does not require anyfine-tuning, input preprocessing, or iterative output generation. Instead, weincorporate the generated content into a pre-defined prompt and employ anotherinstance of an LLM to analyze the text and predict whether it is harmful. Wetest LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominentLLMs against various types of attacks, such as forcefully inducing affirmativeresponses to prompts and prompt engineering attacks. Notably, LLM Self Defensesucceeds in reducing the attack success rate to virtually 0 using both GPT 3.5and Llama 2. The code is publicly available athttps://github.com/poloclub/llm-self-defense</description><author>Mansi Phute, Alec Helbling, Matthew Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, Duen Horng Chau</author><pubDate>Thu, 02 May 2024 15:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07308v4</guid></item><item><title>Data Scoping: Effectively Learning the Evolution of Generic Transport PDEs</title><link>http://arxiv.org/abs/2405.01319v1</link><description>Transport phenomena (e.g., fluid flows) are governed by time-dependentpartial differential equations (PDEs) describing mass, momentum, and energyconservation, and are ubiquitous in many engineering applications. However,deep learning architectures are fundamentally incompatible with the simulationof these PDEs. This paper clearly articulates and then solves thisincompatibility. The local-dependency of generic transport PDEs implies that itonly involves local information to predict the physical properties at alocation in the next time step. However, the deep learning architecture willinevitably increase the scope of information to make such predictions as thenumber of layers increases, which can cause sluggish convergence and compromisegeneralizability. This paper aims to solve this problem by proposing adistributed data scoping method with linear time complexity to strictly limitthe scope of information to predict the local properties. The numericalexperiments over multiple physics show that our data scoping methodsignificantly accelerates training convergence and improves thegeneralizability of benchmark models on large-scale engineering simulations.Specifically, over the geometries not included in the training data for heattransferring simulation, it can increase the accuracy of Convolutional NeuralNetworks (CNNs) by 21.7 \% and that of Fourier Neural Operators (FNOs) by 38.5\% on average.</description><author>Jiangce Chen, Wenzhuo Xu, Zeda Xu, Noelia Grande Gutiérrez, Sneha Prabha Narra, Christopher McComb</author><pubDate>Thu, 02 May 2024 15:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01319v1</guid></item><item><title>Non-iterative Optimization of Trajectory and Radio Resource for Aerial Network</title><link>http://arxiv.org/abs/2405.01314v1</link><description>We address a joint trajectory planning, user association, resourceallocation, and power control problem to maximize proportional fairness in theaerial IoT network, considering practical end-to-end quality-of-service (QoS)and communication schedules. Though the problem is rather ancient, apart fromthe fact that the previous approaches have never considered user- andtime-specific QoS, we point out a prevalent mistake in coordinate optimizationapproaches adopted by the majority of the literature. Coordinate optimizationapproaches, which repetitively optimize radio resources for a fixed trajectoryand vice versa, generally converge to local optima when all variables aredifferentiable. However, these methods often stagnate at a non-stationarypoint, significantly degrading the network utility in mixed-integer problemssuch as joint trajectory and radio resource optimization. We detour thisproblem by converting the formulated problem into the Markov decision process(MDP). Exploiting the beneficial characteristics of the MDP, we design anon-iterative framework that cooperatively optimizes trajectory and radioresources without initial trajectory choice. The proposed framework canincorporate various trajectory planning algorithms such as the geneticalgorithm, tree search, and reinforcement learning. Extensive comparisons withdiverse baselines verify that the proposed framework significantly outperformsthe state-of-the-art method, nearly achieving the global optimum. Ourimplementation code is available at https://github.com/hslyu/dbspf.</description><author>Hyeonsu Lyu, Jonggyu Jang, Harim Lee, Hyun Jong Yang</author><pubDate>Thu, 02 May 2024 15:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01314v1</guid></item><item><title>Imagine the Unseen: Occluded Pedestrian Detection via Adversarial Feature Completion</title><link>http://arxiv.org/abs/2405.01311v1</link><description>Pedestrian detection has significantly progressed in recent years, thanks tothe development of DNNs. However, detection performance at occluded scenes isstill far from satisfactory, as occlusion increases the intra-class variance ofpedestrians, hindering the model from finding an accurate classificationboundary between pedestrians and background clutters. From the perspective ofreducing intra-class variance, we propose to complete features for occludedregions so as to align the features of pedestrians across different occlusionpatterns. An important premise for feature completion is to locate occludedregions. From our analysis, channel features of different pedestrian proposalsonly show high correlation values at visible parts and thus featurecorrelations can be used to model occlusion patterns. In order to narrow downthe gap between completed features and real fully visible ones, we propose anadversarial learning method, which completes occluded features with a generatorsuch that they can hardly be distinguished by the discriminator from real fullyvisible features. We report experimental results on the CityPersons, Caltechand CrowdHuman datasets. On CityPersons, we show significant improvements overfive different baseline detectors, especially on the heavy occlusion subset.Furthermore, we show that our proposed method FeatComp++ achievesstate-of-the-art results on all the above three datasets without relying onextra cues.</description><author>Shanshan Zhang, Mingqian Ji, Yang Li, Jian Yang</author><pubDate>Thu, 02 May 2024 15:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01311v1</guid></item></channel></rss>