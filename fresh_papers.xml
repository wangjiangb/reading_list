<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 31 Aug 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Policy composition in reinforcement learning via multi-objective policy optimization</title><link>http://arxiv.org/abs/2308.15470v2</link><description>We enable reinforcement learning agents to learn successful behavior policiesby utilizing relevant pre-existing teacher policies. The teacher policies areintroduced as objectives, in addition to the task objective, in amulti-objective policy optimization setting. Using the Multi-Objective Maximuma Posteriori Policy Optimization algorithm (Abdolmaleki et al. 2020), we showthat teacher policies can help speed up learning, particularly in the absenceof shaping rewards. In two domains with continuous observation and actionspaces, our agents successfully compose teacher policies in sequence and inparallel, and are also able to further extend the policies of the teachers inorder to solve the task. Depending on the specified combination of task and teacher(s), teacher(s) maynaturally act to limit the final performance of an agent. The extent to whichagents are required to adhere to teacher policies are determined byhyperparameters which determine both the effect of teachers on learning speedand the eventual performance of the agent on the task. In the humanoid domain(Tassa et al. 2018), we also equip agents with the ability to control theselection of teachers. With this ability, agents are able to meaningfullycompose from the teacher policies to achieve a superior task reward on the walktask than in cases without access to the teacher policies. We show theresemblance of composed task policies with the corresponding teacher policiesthrough videos.</description><author>Shruti Mishra, Ankit Anand, Jordan Hoffmann, Nicolas Heess, Martin Riedmiller, Abbas Abdolmaleki, Doina Precup</author><pubDate>Wed, 30 Aug 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15470v2</guid></item><item><title>Boosting Detection in Crowd Analysis via Underutilized Output Features</title><link>http://arxiv.org/abs/2308.16187v1</link><description>Detection-based methods have been viewed unfavorably in crowd analysis due totheir poor performance in dense crowds. However, we argue that the potential ofthese methods has been underestimated, as they offer crucial information forcrowd analysis that is often ignored. Specifically, the area size andconfidence score of output proposals and bounding boxes provide insight intothe scale and density of the crowd. To leverage these underutilized features,we propose Crowd Hat, a plug-and-play module that can be easily integrated withexisting detection models. This module uses a mixed 2D-1D compression techniqueto refine the output features and obtain the spatial and numerical distributionof crowd-specific information. Based on these features, we further proposeregion-adaptive NMS thresholds and a decouple-then-align paradigm that addressthe major limitations of detection-based methods. Our extensive evaluations onvarious crowd analysis tasks, including crowd counting, localization, anddetection, demonstrate the effectiveness of utilizing output features and thepotential of detection-based methods in crowd analysis.</description><author>Shaokai Wu, Fengyu Yang</author><pubDate>Wed, 30 Aug 2023 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16187v1</guid></item><item><title>Learning Vision-based Pursuit-Evasion Robot Policies</title><link>http://arxiv.org/abs/2308.16185v1</link><description>Learning strategic robot behavior -- like that required in pursuit-evasioninteractions -- under real-world constraints is extremely challenging. Itrequires exploiting the dynamics of the interaction, and planning through bothphysical state and latent intent uncertainty. In this paper, we transform thisintractable problem into a supervised learning problem, where afully-observable robot policy generates supervision for a partially-observableone. We find that the quality of the supervision signal for thepartially-observable pursuer policy depends on two key factors: the balance ofdiversity and optimality of the evader's behavior and the strength of themodeling assumptions in the fully-observable policy. We deploy our policy on aphysical quadruped robot with an RGB-D camera on pursuit-evasion interactionsin the wild. Despite all the challenges, the sensing constraints bring aboutcreativity: the robot is pushed to gather information when uncertain, predictintent from noisy measurements, and anticipate in order to intercept. Projectwebpage: https://abajcsy.github.io/vision-based-pursuit/</description><author>Andrea Bajcsy, Antonio Loquercio, Ashish Kumar, Jitendra Malik</author><pubDate>Wed, 30 Aug 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16185v1</guid></item><item><title>SAM-Med2D</title><link>http://arxiv.org/abs/2308.16184v1</link><description>The Segment Anything Model (SAM) represents a state-of-the-art researchadvancement in natural image segmentation, achieving impressive results withinput prompts such as points and bounding boxes. However, our evaluation andrecent research indicate that directly applying the pretrained SAM to medicalimage segmentation does not yield satisfactory performance. This limitationprimarily arises from significant domain gap between natural images and medicalimages. To bridge this gap, we introduce SAM-Med2D, the most comprehensivestudies on applying SAM to medical 2D images. Specifically, we first collectand curate approximately 4.6M images and 19.7M masks from public and privatedatasets, constructing a large-scale medical image segmentation datasetencompassing various modalities and objects. Then, we comprehensively fine-tuneSAM on this dataset and turn it into SAM-Med2D. Unlike previous methods thatonly adopt bounding box or point prompts as interactive segmentation approach,we adapt SAM to medical image segmentation through more comprehensive promptsinvolving bounding boxes, points, and masks. We additionally fine-tune theencoder and decoder of the original SAM to obtain a well-performed SAM-Med2D,leading to the most comprehensive fine-tuning strategies to date. Finally, weconducted a comprehensive evaluation and analysis to investigate theperformance of SAM-Med2D in medical image segmentation across variousmodalities, anatomical structures, and organs. Concurrently, we validated thegeneralization capability of SAM-Med2D on 9 datasets from MICCAI 2023challenge. Overall, our approach demonstrated significantly superiorperformance and generalization capability compared to SAM.</description><author>Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, Yu Qiao</author><pubDate>Wed, 30 Aug 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16184v1</guid></item><item><title>GREC: Generalized Referring Expression Comprehension</title><link>http://arxiv.org/abs/2308.16182v1</link><description>The objective of Classic Referring Expression Comprehension (REC) is toproduce a bounding box corresponding to the object mentioned in a given textualdescription. Commonly, existing datasets and techniques in classic REC aretailored for expressions that pertain to a single target, meaning a soleexpression is linked to one specific object. Expressions that refer to multipletargets or involve no specific target have not been taken into account. Thisconstraint hinders the practical applicability of REC. This study introduces anew benchmark termed as Generalized Referring Expression Comprehension (GREC).This benchmark extends the classic REC by permitting expressions to describeany number of target objects. To achieve this goal, we have built the firstlarge-scale GREC dataset named gRefCOCO. This dataset encompasses a range ofexpressions: those referring to multiple targets, expressions with no specifictarget, and the single-target expressions. The design of GREC and gRefCOCOensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, aGREC method implementation code, and GREC evaluation code are available athttps://github.com/henghuiding/gRefCOCO.</description><author>Shuting He, Henghui Ding, Chang Liu, Xudong Jiang</author><pubDate>Wed, 30 Aug 2023 18:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16182v1</guid></item><item><title>Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?</title><link>http://arxiv.org/abs/2308.06032v2</link><description>Large Language Models (LLMs) could enhance access to the legal system.However, empirical research on their effectiveness in conducting legal tasks isscant. We study securities cases involving cryptocurrencies as one of numerouscontexts where AI could support the legal process, studying LLMs' legalreasoning and drafting capabilities. We examine whether a) an LLM canaccurately determine which laws are potentially being violated from a factpattern, and b) whether there is a difference in juror decision-making based oncomplaints written by a lawyer compared to an LLM. We feed fact patterns fromreal-life cases to GPT-3.5 and evaluate its ability to determine correctpotential violations from the scenario and exclude spurious violations. Second,we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5'slegal reasoning skills proved weak, though we expect improvement in futuremodels, particularly given the violations it suggested tended to be correct (itmerely missed additional, correct violations). GPT-3.5 performed better atlegal drafting, and jurors' decisions were not statistically significantlyassociated with the author of the document upon which they based theirdecisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks,they would be unable to replace lawyers at this stage. However, their draftingskills (though, perhaps, still inferior to lawyers), could provide access tojustice for more individuals by reducing the cost of legal services. Ourresearch is the first to systematically study LLMs' legal drafting andreasoning capabilities in litigation, as well as in securities law andcryptocurrency-related misconduct.</description><author>Arianna Trozze, Toby Davies, Bennett Kleinberg</author><pubDate>Wed, 30 Aug 2023 18:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06032v2</guid></item><item><title>Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment</title><link>http://arxiv.org/abs/2308.16175v1</link><description>We introduce BSDetector, a method for detecting bad and speculative answersfrom a pretrained Large Language Model by estimating a numeric confidence scorefor any output it generated. Our uncertainty quantification technique works forany LLM accessible only via a black-box API, and combines intrinsic andextrinsic assessments of confidence into a single trustworthiness estimate forany LLM response to a given prompt. Our method is extremely general and canapplied to all of the best LLMs available today (whose training data remainsunknown). By expending a bit of extra computation, users of any LLM API can nowget the same response as they would ordinarily, as well as a confidenceestimate that caution when not to trust this response. Experiments on bothclosed and open-form Question-Answer benchmarks reveal that BSDetector moreaccurately identifies incorrect LLM responses than alternative uncertaintyestimation procedures (for both GPT-3 and ChatGPT). By sampling multipleresponses from the LLM and considering the one with the highest confidencescore, we can additionally obtain more accurate responses from the same LLM,without any extra training steps.</description><author>Jiuhai Chen, Jonas Mueller</author><pubDate>Wed, 30 Aug 2023 18:53:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16175v1</guid></item><item><title>Temporal-spatial model via Trend Filtering</title><link>http://arxiv.org/abs/2308.16172v1</link><description>This research focuses on the estimation of a non-parametric regressionfunction designed for data with simultaneous time and space dependencies. Insuch a context, we study the Trend Filtering, a nonparametric estimatorintroduced by \cite{mammen1997locally} and \cite{rudin1992nonlinear}. Forunivariate settings, the signals we consider are assumed to have a kth weakderivative with bounded total variation, allowing for a general degree ofsmoothness. In the multivariate scenario, we study a $K$-Nearest Neighbor fusedlasso estimator as in \cite{padilla2018adaptive}, employing an ADMM algorithm,suitable for signals with bounded variation that adhere to a piecewiseLipschitz continuity criterion. By aligning with lower bounds, the minimaxoptimality of our estimators is validated. A unique phase transitionphenomenon, previously uncharted in Trend Filtering studies, emerges throughour analysis. Both Simulation studies and real data applications underscore thesuperior performance of our method when compared with established techniques inthe existing literature.</description><author>Carlos Misael Madrid Padilla, Oscar Hernan Madrid Padilla, Daren Wang</author><pubDate>Wed, 30 Aug 2023 18:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16172v1</guid></item><item><title>Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data</title><link>http://arxiv.org/abs/2303.17590v2</link><description>Large-scale pre-trained Vision &amp; Language (VL) models have shown remarkableperformance in many applications, enabling replacing a fixed set of supportedclasses with zero-shot open vocabulary reasoning over (almost arbitrary)natural language prompts. However, recent works have uncovered a fundamentalweakness of these models. For example, their difficulty to understand VisualLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning ofnon-object words (e.g., attributes, actions, relations, states, etc.), ordifficulty in performing compositional reasoning such as understanding thesignificance of the order of the words in a sentence. In this work, weinvestigate to which extent purely synthetic data could be leveraged to teachthese models to overcome such shortcomings without compromising their zero-shotcapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scalesynthetic dataset and data generation codebase allowing to generate additionalsuitable data to improve VLC understanding and compositional reasoning of VLmodels. Additionally, we propose a general VL finetuning strategy foreffectively leveraging SyViC towards achieving these improvements. Ourextensive experiments and ablations on VL-Checklist, Winoground, and ARObenchmarks demonstrate that it is possible to adapt strong pre-trained VLmodels with synthetic data significantly enhancing their VLC understanding(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in theirzero-shot accuracy.</description><author>Paola Cascante-Bonilla, Khaled Shehada, James Seale Smith, Sivan Doveh, Donghyun Kim, Rameswar Panda, Gül Varol, Aude Oliva, Vicente Ordonez, Rogerio Feris, Leonid Karlinsky</author><pubDate>Wed, 30 Aug 2023 18:46:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17590v2</guid></item><item><title>Algebraic, Topological, and Mereological Foundations of Existential Granules</title><link>http://arxiv.org/abs/2308.16157v1</link><description>In this research, new concepts of existential granules that determinethemselves are invented, and are characterized from algebraic, topological, andmereological perspectives. Existential granules are those that determinethemselves initially, and interact with their environment subsequently.Examples of the concept, such as those of granular balls, though inadequatelydefined, algorithmically established, and insufficiently theorized in earlierworks by others, are already used in applications of rough sets and softcomputing. It is shown that they fit into multiple theoretical frameworks(axiomatic, adaptive, and others) of granular computing. The characterizationis intended for algorithm development, application to classification problemsand possible mathematical foundations of generalizations of the approach.Additionally, many open problems are posed and directions provided.</description><author>Mani A</author><pubDate>Wed, 30 Aug 2023 18:22:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16157v1</guid></item><item><title>MMVP: Motion-Matrix-based Video Prediction</title><link>http://arxiv.org/abs/2308.16154v1</link><description>A central challenge of video prediction lies where the system has to reasonthe objects' future motions from image frames while simultaneously maintainingthe consistency of their appearances across frames. This work introduces anend-to-end trainable two-stream video prediction framework, Motion-Matrix-basedVideo Prediction (MMVP), to tackle this challenge. Unlike previous methods thatusually handle motion prediction and appearance maintenance within the same setof modules, MMVP decouples motion and appearance information by constructingappearance-agnostic motion matrices. The motion matrices represent the temporalsimilarity of each and every pair of feature patches in the input frames, andare the sole input of the motion prediction module in MMVP. This designimproves video prediction in both accuracy and efficiency, and reduces themodel size. Results of extensive experiments demonstrate that MMVP outperformsstate-of-the-art systems on public data sets by non-negligible large margins(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% thesize or smaller). Please refer tohttps://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for theofficial code and the datasets used in this paper.</description><author>Yiqi Zhong, Luming Liang, Ilya Zharkov, Ulrich Neumann</author><pubDate>Wed, 30 Aug 2023 18:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16154v1</guid></item><item><title>Walking in the Shadow: A New Perspective on Descent Directions for Constrained Minimization</title><link>http://arxiv.org/abs/2006.08426v4</link><description>Descent directions such as movement towards Descent directions, includingmovement towards Frank-Wolfe vertices, away-steps, in-face away-steps andpairwise directions, have been an important design consideration in conditionalgradient descent (CGD) variants. In this work, we attempt to demystify theimpact of the movement in these directions towards attaining constrainedminimizers. The optimal local direction of descent is the directionalderivative (i.e., shadow) of the projection of the negative gradient. We showthat this direction is the best away-step possible, and the continuous-timedynamics of moving in the shadow is equivalent to the dynamics of projectedgradient descent (PGD), although it's non-trivial to discretize. We also showthat Frank-Wolfe (FW) vertices correspond to projecting onto the polytope usingan "infinite" step in the direction of the negative gradient, thus providing anew perspective on these steps. We combine these insights into a novelShadow-CG method that uses FW and shadow steps, while enjoying linearconvergence, with a rate that depends on the number of breakpoints in itsprojection curve, rather than the pyramidal width. We provide a linear bound onthe number of breakpoints for simple polytopes and present scaling-invariantupper bounds for general polytopes based on the number of facets. We exemplifythe benefit of using Shadow-CG computationally for various applications, whileraising an open question about tightening the bound on the number ofbreakpoints for general polytopes.</description><author>Hassan Mortagy, Swati Gupta, Sebastian Pokutta</author><pubDate>Wed, 30 Aug 2023 18:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2006.08426v4</guid></item><item><title>Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI</title><link>http://arxiv.org/abs/2308.16150v1</link><description>Unsupervised anomaly segmentation aims to detect patterns that are distinctfrom any patterns processed during training, commonly called abnormal orout-of-distribution patterns, without providing any associated manualsegmentations. Since anomalies during deployment can lead to model failure,detecting the anomaly can enhance the reliability of models, which is valuablein high-risk domains like medical imaging. This paper introduces MaskedModality Cycles with Conditional Diffusion (MMCCD), a method that enablessegmentation of anomalies across diverse patterns in multimodal MRI. The methodis based on two fundamental ideas. First, we propose the use of cyclic modalitytranslation as a mechanism for enabling abnormality detection.Image-translation models learn tissue-specific modality mappings, which arecharacteristic of tissue physiology. Thus, these learned mappings fail totranslate tissues or image patterns that have never been encountered duringtraining, and the error enables their segmentation. Furthermore, we combineimage translation with a masked conditional diffusion model, which attempts to`imagine' what tissue exists under a masked area, further exposing unknownpatterns as the generative model fails to recreate them. We evaluate our methodon a proxy task by training on healthy-looking slices of BraTS2021multi-modality MRIs and testing on slices with tumors. We show that our methodcompares favorably to previous unsupervised approaches based on imagereconstruction and denoising with autoencoders and diffusion models.</description><author>Ziyun Liang, Harry Anthony, Felix Wagner, Konstantinos Kamnitsas</author><pubDate>Wed, 30 Aug 2023 18:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16150v1</guid></item><item><title>Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</title><link>http://arxiv.org/abs/2308.16149v1</link><description>We introduce Jais and Jais-chat, new state-of-the-art Arabic-centricfoundation and instruction-tuned open generative large language models (LLMs).The models are based on the GPT-3 decoder-only architecture and are pretrainedon a mixture of Arabic and English texts, including source code in variousprogramming languages. With 13 billion parameters, they demonstrate betterknowledge and reasoning capabilities in Arabic than any existing open Arabicand multilingual models by a sizable margin, based on extensive evaluation.Moreover, the models are competitive in English compared to English-centricopen models of similar size, despite being trained on much less English data.We provide a detailed description of the training, the tuning, the safetyalignment, and the evaluation of the models. We release two open versions ofthe model -- the foundation Jais model, and an instruction-tuned Jais-chatvariant -- with the aim of promoting research on Arabic LLMs. Available athttps://huggingface.co/inception-mbzuai/jais-13b-chat</description><author>Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing</author><pubDate>Wed, 30 Aug 2023 18:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16149v1</guid></item><item><title>CartiMorph: a framework for automated knee articular cartilage morphometrics</title><link>http://arxiv.org/abs/2308.01981v2</link><description>We introduce CartiMorph, a framework for automated knee articular cartilagemorphometrics. It takes an image as input and generates quantitative metricsfor cartilage subregions, including the percentage of full-thickness cartilageloss (FCL), mean thickness, surface area, and volume. CartiMorph leverages thepower of deep learning models for hierarchical image feature representation.Deep learning models were trained and validated for tissue segmentation,template construction, and template-to-image registration. We establishedmethods for surface-normal-based cartilage thickness mapping, FCL estimation,and rule-based cartilage parcellation. Our cartilage thickness map showed lesserror in thin and peripheral regions. We evaluated the effectiveness of theadopted segmentation model by comparing the quantitative metrics obtained frommodel segmentation and those from manual segmentation. The root-mean-squareddeviation of the FCL measurements was less than 8%, and strong correlationswere observed for the mean thickness (Pearson's correlation coefficient $\rho\in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in[0.89,0.98]$) measurements. We compared our FCL measurements with those from aprevious study and found that our measurements deviated less from the groundtruths. We observed superior performance of the proposed rule-based cartilageparcellation method compared with the atlas-based approach. CartiMorph has thepotential to promote imaging biomarkers discovery for knee osteoarthritis.</description><author>Yongcheng Yao, Junru Zhong, Liping Zhang, Sheheryar Khan, Weitian Chen</author><pubDate>Wed, 30 Aug 2023 18:02:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01981v2</guid></item><item><title>CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention</title><link>http://arxiv.org/abs/2308.16145v1</link><description>Both CNN-based and Transformer-based object detection with bounding boxrepresentation have been extensively studied in computer vision and medicalimage analysis, but circular object detection in medical images is stillunderexplored. Inspired by the recent anchor free CNN-based circular objectdetection method (CircleNet) for ball-shape glomeruli detection in renalpathology, in this paper, we present CircleFormer, a Transformer-based circularmedical object detection with dynamic anchor circles. Specifically, querieswith circle representation in Transformer decoder iteratively refine thecircular object detection results, and a circle cross attention module isintroduced to compute the similarity between circular queries and imagefeatures. A generalized circle IoU (gCIoU) is proposed to serve as a newregression loss of circular object detection as well. Moreover, our approach iseasy to generalize to the segmentation task by adding a simple segmentationbranch to CircleFormer. We evaluate our method in circular nuclei detection andsegmentation on the public MoNuSeg dataset, and the experimental results showthat our method achieves promising performance compared with thestate-of-the-art approaches. The effectiveness of each component is validatedvia ablation studies as well. Our code is released at:\url{https://github.com/zhanghx-iim-ahu/CircleFormer}.</description><author>Hengxu Zhang, Pengpeng Liang, Zhiyong Sun, Bo Song, Erkang Cheng</author><pubDate>Wed, 30 Aug 2023 18:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16145v1</guid></item><item><title>MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision</title><link>http://arxiv.org/abs/2308.16139v1</link><description>We present MedShapeNet, a large collection of anatomical shapes (e.g., bones,organs, vessels) and 3D surgical instrument models. Prior to the deep learningera, the broad application of statistical shape models (SSMs) in medical imageanalysis is evidence that shapes have been commonly used to describe medicaldata. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms inmedical imaging are predominantly voxel-based. In computer vision, on thecontrary, shapes (including, voxel occupancy grids, meshes, point clouds andimplicit surface models) are preferred data representations in 3D, as seen fromthe numerous shape-related publications in premier vision conferences, such asthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), aswell as the increasing popularity of ShapeNet (about 51,300 models) andPrinceton ModelNet (127,915 models) in computer vision research. MedShapeNet iscreated as an alternative to these commonly used shape benchmarks to facilitatethe translation of data-driven vision algorithms to medical applications, andit extends the opportunities to adapt SOTA vision algorithms to solve criticalmedical problems. Besides, the majority of the medical shapes in MedShapeNetare modeled directly on the imaging data of real patients, and therefore itcomplements well existing shape benchmarks comprising of computer-aided design(CAD) models. MedShapeNet currently includes more than 100,000 medical shapes,and provides annotations in the form of paired data. It is therefore also afreely available repository of 3D models for extended reality (virtual reality- VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. Thiswhite paper describes in detail the motivations behind MedShapeNet, the shapeacquisition procedures, the use cases, as well as the usage of the online shapesearch portal: https://medshapenet.ikim.nrw/</description><author>Jianning Li, Antonio Pepe, Christina Gsaxner, Gijs Luijten, Yuan Jin, Narmada Ambigapathy, Enrico Nasca, Naida Solak, Gian Marco Melito, Afaque R. Memon, Xiaojun Chen, Jan Stefan Kirschke, Ezequiel de la Rosa, Patrich Ferndinand Christ, Hongwei Bran Li, David G. Ellis, Michele R. Aizenberg, Sergios Gatidis, Thomas Kuestner, Nadya Shusharina, Nicholas Heller, Vincent Andrearczyk, Adrien Depeursinge, Mathieu Hatt, Anjany Sekuboyina, Maximilian Loeffler, Hans Liebl, Reuben Dorent, Tom Vercauteren, Jonathan Shapey, Aaron Kujawa, Stefan Cornelissen, Patrick Langenhuizen, Achraf Ben-Hamadou, Ahmed Rekik, Sergi Pujades, Edmond Boyer, Federico Bolelli, Costantino Grana, Luca Lumetti, Hamidreza Salehi, Jun Ma, Yao Zhang, Ramtin Gharleghi, Susann Beier, Eduardo A. Garza-Villarreal, Thania Balducci, </author><pubDate>Wed, 30 Aug 2023 17:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16139v1</guid></item><item><title>LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models</title><link>http://arxiv.org/abs/2308.16137v1</link><description>In recent years, there have been remarkable advancements in the performanceof Transformer-based Large Language Models (LLMs) across various domains. Asthese LLMs are deployed for increasingly complex tasks, they often face theneeds to conduct longer reasoning processes or understanding larger contexts.In these situations, the length generalization failure of LLMs on longsequences become more prominent. Most pre-training schemes truncate trainingsequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle togenerate fluent texts, let alone carry out downstream tasks, after longercontexts, even with relative positional encoding which is designed to cope withthis problem. Common solutions such as finetuning on longer corpora ofteninvolves daunting hardware and time costs and requires careful training processdesign. To more efficiently leverage the generation capacity of existing LLMs,we theoretically and empirically investigate the main out-of-distribution (OOD)factors contributing to this problem. Inspired by this diagnosis, we propose asimple yet effective solution for on-the-fly length generalization,LM-Infinite, which involves only a $\Lambda$-shaped attention mask and adistance limit while requiring no parameter updates or learning. We find itapplicable to a variety of LLMs using relative-position encoding methods.LM-Infinite is computational efficient with $O(n)$ time and space, anddemonstrates consistent fluency and generation quality to as long as 32k tokenson ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. On downstreamtask such as passkey retrieval, it continues to work on inputs much longer thantraining lengths where vanilla models fail immediately.</description><author>Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang</author><pubDate>Wed, 30 Aug 2023 17:47:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16137v1</guid></item><item><title>Distributionally Robust Statistical Verification with Imprecise Neural Networks</title><link>http://arxiv.org/abs/2308.14815v2</link><description>A particularly challenging problem in AI safety is providing guarantees onthe behavior of high-dimensional autonomous systems. Verification approachescentered around reachability analysis fail to scale, and purely statisticalapproaches are constrained by the distributional assumptions about the samplingprocess. Instead, we pose a distributionally robust version of the statisticalverification problem for black-box systems, where our performance guaranteeshold over a large family of distributions. This paper proposes a novel approachbased on a combination of active learning, uncertainty quantification, andneural network verification. A central piece of our approach is an ensembletechnique called Imprecise Neural Networks, which provides the uncertainty toguide active learning. The active learning uses an exhaustive neural-networkverification tool Sherlock to collect samples. An evaluation on multiplephysical simulators in the openAI gym Mujoco environments withreinforcement-learned controllers demonstrates that our approach can provideuseful and scalable guarantees for high-dimensional systems.</description><author>Souradeep Dutta, Michele Caprio, Vivian Lin, Matthew Cleaveland, Kuk Jin Jang, Ivan Ruchkin, Oleg Sokolsky, Insup Lee</author><pubDate>Wed, 30 Aug 2023 17:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14815v2</guid></item><item><title>On progressive sharpening, flat minima and generalisation</title><link>http://arxiv.org/abs/2305.14683v2</link><description>We present a new approach to understanding the relationship between losscurvature and input-output model behaviour in deep learning. Specifically, weuse existing empirical analyses of the spectrum of deep network loss Hessiansto ground an ansatz tying together the loss Hessian and the input-outputJacobian of a deep neural network over training samples throughout training. Wethen prove a series of theoretical results which quantify the degree to whichthe input-output Jacobian of a model approximates its Lipschitz norm over adata distribution, and deduce a novel generalisation bound in terms of theempirical Jacobian. We use our ansatz, together with our theoretical results,to give a new account of the recently observed progressive sharpeningphenomenon, as well as the generalisation properties of flat minima.Experimental evidence is provided to validate our claims.</description><author>Lachlan Ewen MacDonald, Jack Valmadre, Simon Lucey</author><pubDate>Wed, 30 Aug 2023 17:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14683v2</guid></item><item><title>Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer</title><link>http://arxiv.org/abs/2303.17783v3</link><description>Unsupervised Domain Adaptation (UDA) can effectively address domain gapissues in real-world image Super-Resolution (SR) by accessing both the sourceand target data. Considering privacy policies or transmission restrictions ofsource data in practical scenarios, we propose a SOurce-free Domain Adaptationframework for image SR (SODA-SR) to address this issue, i.e., adapt asource-trained model to a target domain with only unlabeled target data.SODA-SR leverages the source-trained model to generate refined pseudo-labelsfor teacher-student learning. To better utilize pseudo-labels, we propose anovel wavelet-based augmentation method, named Wavelet Augmentation Transformer(WAT), which can be flexibly incorporated with existing networks, to implicitlyproduce useful augmented data. WAT learns low-frequency information of varyinglevels across diverse samples, which is aggregated efficiently via deformableattention. Furthermore, an uncertainty-aware self-training mechanism isproposed to improve the accuracy of pseudo-labels, with inaccurate predictionsbeing rectified by uncertainty estimation. To acquire better SR results andavoid overfitting pseudo-labels, several regularization losses are proposed toconstrain target LR and SR images in the frequency domain. Experiments showthat without accessing source data, SODA-SR outperforms state-of-the-art UDAmethods in both synthetic$\rightarrow$real and real$\rightarrow$real adaptationsettings, and is not constrained by specific network architectures.</description><author>Yuang Ai, Xiaoqiang Zhou, Huaibo Huang, Lei Zhang, Ran He</author><pubDate>Wed, 30 Aug 2023 17:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17783v3</guid></item><item><title>Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation</title><link>http://arxiv.org/abs/2306.00914v2</link><description>Deep generative models have shown impressive results in generating realisticimages of faces. GANs managed to generate high-quality, high-fidelity imageswhen conditioned on semantic masks, but they still lack the ability todiversify their output. Diffusion models partially solve this problem and areable to generate diverse samples given the same condition. In this paper, wepropose a multi-conditioning approach for diffusion models via cross-attentionexploiting both attributes and semantic masks to generate high-quality andcontrollable face images. We also studied the impact of applyingperceptual-focused loss weighting into the latent space instead of the pixelspace. Our method extends the previous approaches by introducing conditioningon more than one set of features, guaranteeing a more fine-grained control overthe generated face images. We evaluate our approach on the CelebA-HQ dataset,and we show that it can generate realistic and diverse samples while allowingfor fine-grained control over multiple attributes and semantic regions.Additionally, we perform an ablation study to evaluate the impact of differentconditioning strategies on the quality and diversity of the generated images.</description><author>Nico Giambi, Giuseppe Lisanti</author><pubDate>Wed, 30 Aug 2023 17:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00914v2</guid></item><item><title>CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric</title><link>http://arxiv.org/abs/2308.16126v1</link><description>Detecting visually similar images is a particularly useful attribute to lookto when calculating product recommendations. Embedding similarity, whichutilizes pre-trained computer vision models to extract high-level imagefeatures, has demonstrated remarkable efficacy in identifying images withsimilar compositions. However, there is a lack of methods for evaluating theembeddings generated by these models, as conventional loss and performancemetrics do not adequately capture their performance in image similarity searchtasks. In this paper, we evaluate the viability of the image embeddings fromnumerous pre-trained computer vision models using a novel approach namedCorrEmbed. Our approach computes the correlation between distances in imageembeddings and distances in human-generated tag vectors. We extensivelyevaluate numerous pre-trained Torchvision models using this metric, revealingan intuitive relationship of linear scaling between ImageNet1k accuracy scoresand tag-correlation scores. Importantly, our method also identifies deviationsfrom this pattern, providing insights into how different models capturehigh-level image features. By offering a robust performance evaluation of these pre-trained models,CorrEmbed serves as a valuable tool for researchers and practitioners seekingto develop effective, data-driven approaches to similar item recommendations infashion retail.</description><author>Karl Audun Kagnes Borgersen, Morten Goodwin, Jivitesh Sharma, Tobias Aasmoe, Mari Leonhardsen, Gro Herredsvela Rørvik</author><pubDate>Wed, 30 Aug 2023 17:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16126v1</guid></item><item><title>Spatial Graph Coarsening: Weather and Weekday Prediction with London's Bike-Sharing Service using GNN</title><link>http://arxiv.org/abs/2308.16122v1</link><description>This study introduced the use of Graph Neural Network (GNN) for predictingthe weather and weekday of a day in London, from the dataset of SantanderCycles bike-sharing system as a graph classification task. The proposed GNNmodels newly introduced (i) a concatenation operator of graph features withtrained node embeddings and (ii) a graph coarsening operator based ongeographical contiguity, namely "Spatial Graph Coarsening". With the nodefeatures of land-use characteristics and number of households around the bikestations and graph features of temperatures in the city, our proposed modelsoutperformed the baseline model in cross-entropy loss and accuracy of thevalidation dataset.</description><author>Yuta Sato, Pak Hei Lam, Shruti Gupta, Fareesah Hussain</author><pubDate>Wed, 30 Aug 2023 17:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16122v1</guid></item><item><title>Evaluating GPT-3 Generated Explanations for Hateful Content Moderation</title><link>http://arxiv.org/abs/2305.17680v4</link><description>Recent research has focused on using large language models (LLMs) to generateexplanations for hate speech through fine-tuning or prompting. Despite thegrowing interest in this area, these generated explanations' effectiveness andpotential limitations remain poorly understood. A key concern is that theseexplanations, generated by LLMs, may lead to erroneous judgments about thenature of flagged content by both users and content moderators. For instance,an LLM-generated explanation might inaccurately convince a content moderatorthat a benign piece of content is hateful. In light of this, we propose ananalytical framework for examining hate speech explanations and conducted anextensive survey on evaluating such explanations. Specifically, we promptedGPT-3 to generate explanations for both hateful and non-hateful content, and asurvey was conducted with 2,400 unique respondents to evaluate the generatedexplanations. Our findings reveal that (1) human evaluators rated theGPT-generated explanations as high quality in terms of linguistic fluency,informativeness, persuasiveness, and logical soundness, (2) the persuasivenature of these explanations, however, varied depending on the promptingstrategy employed, and (3) this persuasiveness may result in incorrectjudgments about the hatefulness of the content. Our study underscores the needfor caution in applying LLM-generated explanations for content moderation. Codeand results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.</description><author>Han Wang, Ming Shan Hee, Md Rabiul Awal, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</author><pubDate>Wed, 30 Aug 2023 17:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17680v4</guid></item><item><title>Response: Emergent analogical reasoning in large language models</title><link>http://arxiv.org/abs/2308.16118v1</link><description>In their recent Nature Human Behaviour paper, "Emergent analogical reasoningin large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that"large language models such as GPT-3 have acquired an emergent ability to findzero-shot solutions to a broad range of analogy problems." In this response, weprovide counterexamples of the letter string analogies. In our tests, GPT-3fails to solve even the easiest variants of the problems presented in theoriginal paper. Zero-shot reasoning is an extraordinary claim that requiresextraordinary evidence. We do not see that evidence in our experiments. Tostrengthen claims of humanlike reasoning such as zero-shot reasoning, it isimportant that the field develop approaches that rule out data memorization.</description><author>Damian Hodel, Jevin West</author><pubDate>Wed, 30 Aug 2023 17:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16118v1</guid></item><item><title>survex: an R package for explaining machine learning survival models</title><link>http://arxiv.org/abs/2308.16113v1</link><description>Due to their flexibility and superior performance, machine learning modelsfrequently complement and outperform traditional statistical survival models.However, their widespread adoption is hindered by a lack of user-friendly toolsto explain their internal operations and prediction rationales. To tackle thisissue, we introduce the survex R package, which provides a cohesive frameworkfor explaining any survival model by applying explainable artificialintelligence techniques. The capabilities of the proposed software encompassunderstanding and diagnosing survival models, which can lead to theirimprovement. By revealing insights into the decision-making process, such asvariable effects and importances, survex enables the assessment of modelreliability and the detection of biases. Thus, transparency and responsibilitymay be promoted in sensitive areas, such as biomedical research and healthcareapplications.</description><author>Mikołaj Spytek, Mateusz Krzyziński, Sophie Hanna Langbein, Hubert Baniecki, Marvin N. Wright, Przemysław Biecek</author><pubDate>Wed, 30 Aug 2023 17:14:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16113v1</guid></item><item><title>Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation</title><link>http://arxiv.org/abs/2308.16110v1</link><description>Few-shot image generation, which aims to produce plausible and diverse imagesfor one category given a few images from this category, has drawn extensiveattention. Existing approaches either globally interpolate different images orfuse local representations with pre-defined coefficients. However, such anintuitive combination of images/features only exploits the most relevantinformation for generation, leading to poor diversity and coarse-grainedsemantic fusion. To remedy this, this paper proposes a novel texturalmodulation (TexMod) mechanism to inject external semantic signals into internallocal representations. Parameterized by the feedback from the discriminator,our TexMod enables more fined-grained semantic injection while maintaining thesynthesis fidelity. Moreover, a global structural discriminator (StructD) isdeveloped to explicitly guide the model to generate images with reasonablelayout and outline. Furthermore, the frequency awareness of the model isreinforced by encouraging the model to distinguish frequency signals. Togetherwith these techniques, we build a novel and effective model for few-shot imagegeneration. The effectiveness of our model is identified by extensiveexperiments on three popular datasets and various settings. Besides achievingstate-of-the-art synthesis performance on these datasets, our proposedtechniques could be seamlessly integrated into existing models for a furtherperformance boost.</description><author>Mengping Yang, Zhe Wang, Wenyi Feng, Qian Zhang, Ting Xiao</author><pubDate>Wed, 30 Aug 2023 17:10:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16110v1</guid></item><item><title>Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks</title><link>http://arxiv.org/abs/2308.14359v2</link><description>Human emotion understanding is pivotal in making conversational technologymainstream. We view speech emotion understanding as a perception task which isa more realistic setting. With varying contexts (languages, demographics, etc.)different share of people perceive the same speech segment as a non-unanimousemotion. As part of the ACM Multimedia 2023 Computational ParalinguisticsChallengE (ComParE) in the EMotion Share track, we leverage their rich datasetof multilingual speakers and multi-label regression target of 'emotion share'or perception of that emotion. We demonstrate that the training scheme ofdifferent foundation models dictates their effectiveness for tasks beyondspeech recognition, especially for non-semantic speech tasks like emotionunderstanding. This is a very complex task due to multilingual speakers,variability in the target labels, and inherent imbalance in the regressiondataset. Our results show that HuBERT-Large with a self-attention-basedlight-weight sequence model provides 4.6% improvement over the reportedbaseline.</description><author>Payal Mohapatra, Akash Pandey, Yueyuan Sui, Qi Zhu</author><pubDate>Wed, 30 Aug 2023 17:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14359v2</guid></item><item><title>What You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics</title><link>http://arxiv.org/abs/2305.11582v2</link><description>In this study, we investigate the feasibility of utilizing state-of-the-artimage perceptual metrics for evaluating audio signals by representing them asspectrograms. The encouraging outcome of the proposed approach is based on thesimilarity between the neural mechanisms in the auditory and visual pathways.Furthermore, we customise one of the metrics which has a psychoacousticallyplausible architecture to account for the peculiarities of sound signals. Weevaluate the effectiveness of our proposed metric and several baseline metricsusing a music dataset, with promising results in terms of the correlationbetween the metrics and the perceived quality of audio as rated by humanevaluators.</description><author>Tashi Namgyal, Alexander Hepburn, Raul Santos-Rodriguez, Valero Laparra, Jesus Malo</author><pubDate>Wed, 30 Aug 2023 17:06:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11582v2</guid></item><item><title>Grandma Karl is 27 years old -- research agenda for pseudonymization of research data</title><link>http://arxiv.org/abs/2308.16109v1</link><description>Accessibility of research data is critical for advances in many researchfields, but textual data often cannot be shared due to the personal andsensitive information which it contains, e.g names or political opinions.General Data Protection Regulation (GDPR) suggests pseudonymization as asolution to secure open access to research data, but we need to learn moreabout pseudonymization as an approach before adopting it for manipulation ofresearch data. This paper outlines a research agenda within pseudonymization,namely need of studies into the effects of pseudonymization on unstructureddata in relation to e.g. readability and language assessment, as well as theeffectiveness of pseudonymization as a way of protecting writer identity, whilealso exploring different ways of developing context-sensitive algorithms fordetection, labelling and replacement of personal information in unstructureddata. The recently granted project on pseudonymization Grandma Karl is 27 yearsold addresses exactly those challenges.</description><author>Elena Volodina, Simon Dobnik, Therese Lindström Tiedemann, Xuan-Son Vu</author><pubDate>Wed, 30 Aug 2023 17:04:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16109v1</guid></item><item><title>Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering</title><link>http://arxiv.org/abs/2307.15745v2</link><description>Visual question answering (VQA) has the potential to make the Internet moreaccessible in an interactive way, allowing people who cannot see images to askquestions about them. However, multiple studies have shown that people who areblind or have low-vision prefer image explanations that incorporate the contextin which an image appears, yet current VQA datasets focus on images inisolation. We argue that VQA models will not fully succeed at meeting people'sneeds unless they take context into account. To further motivate and analyzethe distinction between different contexts, we introduce Context-VQA, a VQAdataset that pairs images with contexts, specifically types of websites (e.g.,a shopping website). We find that the types of questions vary systematicallyacross contexts. For example, images presented in a travel context garner 2times more "Where?" questions, and images on social media and news garner 2.8and 1.8 times more "Who?" questions than the average. We also find that contexteffects are especially important when participants can't see the image. Theseresults demonstrate that context affects the types of questions asked and thatVQA models should be context-sensitive to better meet people's needs,especially in accessibility settings.</description><author>Nandita Naik, Christopher Potts, Elisa Kreiss</author><pubDate>Wed, 30 Aug 2023 16:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15745v2</guid></item><item><title>Cancellation-Free Regret Bounds for Lagrangian Approaches in Constrained Markov Decision Processes</title><link>http://arxiv.org/abs/2306.07001v2</link><description>Constrained Markov Decision Processes (CMDPs) are one of the common ways tomodel safe reinforcement learning problems, where constraint functions modelthe safety objectives. Lagrangian-based dual or primal-dual algorithms provideefficient methods for learning in CMDPs. For these algorithms, the currentlyknown regret bounds in the finite-horizon setting allow for a "cancellation oferrors"; one can compensate for a constraint violation in one episode with astrict constraint satisfaction in another. However, we do not consider such abehavior safe in practical applications. In this paper, we overcome thisweakness by proposing a novel model-based dual algorithm OptAug-CMDP fortabular finite-horizon CMDPs. Our algorithm is motivated by the augmentedLagrangian method and can be performed efficiently. We show that during $K$episodes of exploring the CMDP, our algorithm obtains a regret of$\tilde{O}(\sqrt{K})$ for both the objective and the constraint violation.Unlike existing Lagrangian approaches, our algorithm achieves this regretwithout the need for the cancellation of errors.</description><author>Adrian Müller, Pragnya Alatur, Giorgia Ramponi, Niao He</author><pubDate>Wed, 30 Aug 2023 16:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07001v2</guid></item><item><title>Advanced Deep Regression Models for Forecasting Time Series Oil Production</title><link>http://arxiv.org/abs/2308.16105v1</link><description>Global oil demand is rapidly increasing and is expected to reach 106.3million barrels per day by 2040. Thus, it is vital for hydrocarbon extractionindustries to forecast their production to optimize their operations and avoidlosses. Big companies have realized that exploiting the power of deep learning(DL) and the massive amount of data from various oil wells for this purpose cansave a lot of operational costs and reduce unwanted environmental impacts. Inthis direction, researchers have proposed models using conventional machinelearning (ML) techniques for oil production forecasting. However, thesetechniques are inappropriate for this problem as they can not capturehistorical patterns found in time series data, resulting in inaccuratepredictions. This research aims to overcome these issues by developing advanceddata-driven regression models using sequential convolutions and long short-termmemory (LSTM) units. Exhaustive analyses are conducted to select the optimalsequence length, model hyperparameters, and cross-well dataset formation tobuild highly generalized robust models. A comprehensive experimental study onVolve oilfield data validates the proposed models. It reveals that theLSTM-based sequence learning model can predict oil production better than the1-D convolutional neural network (CNN) with mean absolute error (MAE) and R2score of 111.16 and 0.98, respectively. It is also found that the LSTM-basedmodel performs better than all the existing state-of-the-art solutions andachieves a 37% improvement compared to a standard linear regression, which isconsidered the baseline model in this work.</description><author>Siavash Hosseini, Thangarajah Akilan</author><pubDate>Wed, 30 Aug 2023 16:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16105v1</guid></item><item><title>On the Consistency of Average Embeddings for Item Recommendation</title><link>http://arxiv.org/abs/2308.12767v2</link><description>A prevalent practice in recommender systems consists in averaging itemembeddings to represent users or higher-level concepts in the same embeddingspace. This paper investigates the relevance of such a practice. For thispurpose, we propose an expected precision score, designed to measure theconsistency of an average embedding relative to the items used for itsconstruction. We subsequently analyze the mathematical expression of this scorein a theoretical setting with specific assumptions, as well as its empiricalbehavior on real-world data from music streaming services. Our resultsemphasize that real-world averages are less consistent for recommendation,which paves the way for future research to better align real-world embeddingswith assumptions from our theoretical setting.</description><author>Walid Bendada, Guillaume Salha-Galvan, Romain Hennequin, Thomas Bouabça, Tristan Cazenave</author><pubDate>Wed, 30 Aug 2023 16:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12767v2</guid></item><item><title>Rule Generation for Classification: Scalability, Interpretability, and Fairness</title><link>http://arxiv.org/abs/2104.10751v3</link><description>We introduce a new rule-based optimization method for classification withconstraints. The proposed method leverages column generation for linearprogramming, and hence, is scalable to large datasets. The resulting pricingsubproblem is shown to be NP-Hard. We recourse to a decision tree-basedheuristic and solve a proxy pricing subproblem for acceleration. The methodreturns a set of rules along with their optimal weights indicating theimportance of each rule for learning. We address interpretability and fairnessby assigning cost coefficients to the rules and introducing additionalconstraints. In particular, we focus on local interpretability and generalizeseparation criterion in fairness to multiple sensitive attributes and classes.We test the performance of the proposed methodology on a collection of datasetsand present a case study to elaborate on its different aspects. The proposedrule-based learning method exhibits a good compromise between localinterpretability and fairness on the one side, and accuracy on the other side.</description><author>Adia C. Lumadjeng, Tabea Röber, M. Hakan Akyüz, Ş. İlker Birbil</author><pubDate>Wed, 30 Aug 2023 16:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.10751v3</guid></item><item><title>Likelihood-based inference and forecasting for trawl processes: a stochastic optimization approach</title><link>http://arxiv.org/abs/2308.16092v1</link><description>We consider trawl processes, which are stationary and infinitely divisiblestochastic processes and can describe a wide range of statistical properties,such as heavy tails and long memory. In this paper, we develop the firstlikelihood-based methodology for the inference of real-valued trawl processesand introduce novel deterministic and probabilistic forecasting methods. Beingnon-Markovian, with a highly intractable likelihood function, trawl processesrequire the use of composite likelihood functions to parsimoniously capturetheir statistical properties. We formulate the composite likelihood estimationas a stochastic optimization problem for which it is feasible to implementiterative gradient descent methods. We derive novel gradient estimators withvariances that are reduced by several orders of magnitude. We analyze both thetheoretical properties and practical implementation details of these estimatorsand release a Python library which can be used to fit a large class of trawlprocesses. In a simulation study, we demonstrate that our estimators outperformthe generalized method of moments estimators in terms of both parameterestimation error and out-of-sample forecasting error. Finally, we formalize astochastic chain rule for our gradient estimators. We apply the new theory totrawl processes and provide a unified likelihood-based methodology for theinference of both real-valued and integer-valued trawl processes.</description><author>Dan Leonte, Almut E. D. Veraart</author><pubDate>Wed, 30 Aug 2023 16:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16092v1</guid></item><item><title>Priority-Centric Human Motion Generation in Discrete Latent Space</title><link>http://arxiv.org/abs/2308.14480v2</link><description>Text-to-motion generation is a formidable task, aiming to produce humanmotions that align with the input text while also adhering to humancapabilities and physical laws. While there have been advancements in diffusionmodels, their application in discrete spaces remains underexplored. Currentmethods often overlook the varying significance of different motions, treatingthem uniformly. It is essential to recognize that not all motions hold the samerelevance to a particular textual description. Some motions, being more salientand informative, should be given precedence during generation. In response, weintroduce a Priority-Centric Motion Discrete Diffusion Model (M2DM), whichutilizes a Transformer-based VQ-VAE to derive a concise, discrete motionrepresentation, incorporating a global self-attention mechanism and aregularization term to counteract code collapse. We also present a motiondiscrete diffusion model that employs an innovative noise schedule, determinedby the significance of each motion token within the entire motion sequence.This approach retains the most salient motions during the reverse diffusionprocess, leading to more semantically rich and varied motions. Additionally, weformulate two strategies to gauge the importance of motion tokens, drawing fromboth textual and visual indicators. Comprehensive experiments on the HumanML3Dand KIT-ML datasets confirm that our model surpasses existing techniques infidelity and diversity, particularly for intricate textual descriptions.</description><author>Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, Xinchao Wang</author><pubDate>Wed, 30 Aug 2023 16:33:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14480v2</guid></item><item><title>Tensor train completion: local recovery guarantees via Riemannian optimization</title><link>http://arxiv.org/abs/2110.03975v3</link><description>In this work, we estimate the number of randomly selected elements of atensor that with high probability guarantees local convergence of Riemanniangradient descent for tensor train completion. We derive a new bound for theorthogonal projections onto the tangent spaces based on the harmonic mean ofthe unfoldings' singular values and introduce a notion of core coherence fortensor trains. We also extend the results to tensor train completion withauxiliary subspace information and obtain the corresponding local convergenceguarantees.</description><author>Stanislav Budzinskiy, Nikolai Zamarashkin</author><pubDate>Wed, 30 Aug 2023 16:32:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.03975v3</guid></item><item><title>Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces</title><link>http://arxiv.org/abs/2308.16089v1</link><description>Despite the high economic relevance of Foundation Industries, certaincomponents like Reheating furnaces within their manufacturing chain areenergy-intensive. Notable energy consumption reduction could be obtained byreducing the overall heating time in furnaces. Computer-integrated MachineLearning (ML) and Artificial Intelligence (AI) powered control systems infurnaces could be enablers in achieving the Net-Zero goals in FoundationIndustries for sustainable manufacturing. In this work, due to the infeasibility of achieving good quality data inscenarios like reheating furnaces, classical Hottel's zone method basedcomputational model has been used to generate data for ML and Deep Learning(DL) based model training via regression. It should be noted that the zonemethod provides an elegant way to model the physical phenomenon of RadiativeHeat Transfer (RHT), the dominating heat transfer mechanism in high-temperatureprocesses inside heating furnaces. Using this data, an extensive comparisonamong a wide range of state-of-the-art, representative ML and DL methods hasbeen made against their temperature prediction performances in varying furnaceenvironments. Owing to their holistic balance among inference times and modelperformance, DL stands out among its counterparts. To further enhance theOut-Of-Distribution (OOD) generalization capability of the trained DL models,we propose a Physics-Informed Neural Network (PINN) by incorporating priorphysical knowledge using a set of novel Energy-Balance regularizers. Our setupis a generic framework, is geometry-agnostic of the 3D structure of theunderlying furnace, and as such could accommodate any standard ML regressionmodel, to serve as a Digital Twin of the underlying physical processes, fortransitioning Foundation Industries towards Industry 4.0.</description><author>Ujjal Kr Dutta, Aldo Lipani, Chuan Wang, Yukun Hu</author><pubDate>Wed, 30 Aug 2023 16:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16089v1</guid></item><item><title>Quantized Low-Rank Multivariate Regression with Random Dithering</title><link>http://arxiv.org/abs/2302.11197v2</link><description>Low-rank multivariate regression (LRMR) is an important statistical learningmodel that combines highly correlated tasks as a multiresponse regressionproblem with low-rank priori on the coefficient matrix. In this paper, we studyquantized LRMR, a practical setting where the responses and/or the covariatesare discretized to finite precision. We focus on the estimation of theunderlying coefficient matrix. To make consistent estimator that could achievearbitrarily small error possible, we employ uniform quantization with randomdithering, i.e., we add appropriate random noise to the data beforequantization. Specifically, uniform dither and triangular dither are used forresponses and covariates, respectively. Based on the quantized data, we proposethe constrained Lasso and regularized Lasso estimators, and derive thenon-asymptotic error bounds. With the aid of dithering, the estimators achieveminimax optimal rate, while quantization only slightly worsens themultiplicative factor in the error rate. Moreover, we extend our results to alow-rank regression model with matrix responses. We corroborate and demonstrateour theoretical results via simulations on synthetic data or image restoration.</description><author>Junren Chen, Yueqi Wang, Michael K. Ng</author><pubDate>Wed, 30 Aug 2023 16:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11197v2</guid></item><item><title>Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-Spectral Image Fusion</title><link>http://arxiv.org/abs/2308.16083v1</link><description>The success of deep neural networks for pan-sharpening is commonly in a formof black box, lacking transparency and interpretability. To alleviate thisissue, we propose a novel model-driven deep unfolding framework with imagereasoning prior tailored for the pan-sharpening task. Different from existingunfolding solutions that deliver the proximal operator networks as theuncertain and vague priors, our framework is motivated by the content reasoningability of masked autoencoders (MAE) with insightful designs. Specifically, thepre-trained MAE with spatial masking strategy, acting as intrinsic reasoningprior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAEwith spatial-spectral masking strategy is treated as the regularization termwithin loss function to constrain the spatial-spectral consistency. Suchdesigns penetrate the image reasoning prior into deep unfolding networks whileimproving its interpretability and representation capability. The uniqueness ofour framework is that the holistic learning process is explicitly integratedwith the inherent physical mechanism underlying the pan-sharpening task.Extensive experiments on multiple satellite datasets demonstrate thesuperiority of our method over the existing state-of-the-art approaches. Codewill be released at \url{https://manman1995.github.io/}.</description><author>Man Zhou, Jie Huang, Naishan Zheng, Chongyi Li</author><pubDate>Wed, 30 Aug 2023 16:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16083v1</guid></item><item><title>SignDiff: Learning Diffusion Models for American Sign Language Production</title><link>http://arxiv.org/abs/2308.16082v1</link><description>The field of Sign Language Production (SLP) lacked a large-scale, pre-trainedmodel based on deep learning for continuous American Sign Language (ASL)production in the past decade. This limitation hampers communication for allindividuals with disabilities relying on ASL. To address this issue, weundertook the secondary development and utilization of How2Sign, one of thelargest publicly available ASL datasets. Despite its significance, priorresearchers in the field of sign language have not effectively employed thiscorpus due to the intricacies involved in American Sign Language Production(ASLP). To conduct large-scale ASLP, we propose SignDiff based on the latest work inrelated fields, which is a dual-condition diffusion pre-training model that cangenerate human sign language speakers from a skeleton pose. SignDiff has anovel Frame Reinforcement Network called FR-Net, similar to dense human poseestimation work, which enhances the correspondence between text lexical symbolsand sign language dense pose frames reduce the occurrence of multiple fingersin the diffusion model. In addition, our ASLP method proposes two new improvedmodules and a new loss function to improve the accuracy and quality of signlanguage skeletal posture and enhance the ability of the model to train onlarge-scale data. We propose the first baseline for ASL production and report the scores of17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We also evaluated ourmodel on the previous mainstream dataset called PHOENIX14T, and the mainexperiments achieved the results of SOTA. In addition, our image quality farexceeds all previous results by 10 percentage points on the SSIM indicator.Finally, we conducted ablation studies and qualitative evaluations fordiscussion.</description><author>Sen Fang, Chunyu Sui, Xuedong Zhang, Yapeng Tian</author><pubDate>Wed, 30 Aug 2023 16:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16082v1</guid></item><item><title>MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers</title><link>http://arxiv.org/abs/2305.09438v3</link><description>Message Passing Interface (MPI) plays a crucial role in distributed memoryparallelization across multiple nodes. However, parallelizing MPI codemanually, and specifically, performing domain decomposition, is a challenging,error-prone task. In this paper, we address this problem by developingMPI-RICAL, a novel data-driven, programming-assistance tool that assistsprogrammers in writing domain decomposition based distributed memoryparallelization code. Specifically, we train a supervised language model tosuggest MPI functions and their proper locations in the code on the fly. Wealso introduce MPICodeCorpus, the first publicly available corpus of MPI-basedparallel programs that is created by mining more than 15,000 open-sourcerepositories on GitHub. Experimental results have been done on MPICodeCorpusand more importantly, on a compiled benchmark of MPI-based parallel programsfor numerical computations that represent real-world scientific applications.MPI-RICAL achieves F1 scores between 0.87-0.91 on these programs, demonstratingits accuracy in suggesting correct MPI functions at appropriate codelocations.. The source code used in this work, as well as other relevantsources, are available at:https://github.com/Scientific-Computing-Lab-NRCN/MPI-rical</description><author>Nadav Schneider, Tal Kadosh, Niranjan Hasabnis, Timothy Mattson, Yuval Pinter, Gal Oren</author><pubDate>Wed, 30 Aug 2023 15:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09438v3</guid></item><item><title>LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework</title><link>http://arxiv.org/abs/2308.10390v3</link><description>While Large Language Models (LLMs) have demonstrated commendable performanceacross a myriad of domains and tasks, existing LLMs still exhibit a palpabledeficit in handling multimodal functionalities, especially for the SpokenQuestion Answering (SQA) task which necessitates precise alignment and deepinteraction between speech and text features. To address the SQA challenge onLLMs, we initially curated the free-form and open-ended LibriSQA dataset fromLibrispeech, comprising Part I with natural conversational formats and Part IIencompassing multiple-choice questions followed by answers and analyticalsegments. Both parts collectively include 107k SQA pairs that cover varioustopics. Given the evident paucity of existing speech-text LLMs, we propose alightweight, end-to-end framework to execute the SQA task on the LibriSQA,witnessing significant results. By reforming ASR into the SQA format, wefurther substantiate our framework's capability in handling ASR tasks. Ourempirical findings bolster the LLMs' aptitude for aligning and comprehendingmultimodal information, paving the way for the development of universalmultimodal LLMs. The dataset and demo can be found athttps://github.com/ZihanZhaoSJTU/LibriSQA.</description><author>Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, Yu Wang</author><pubDate>Wed, 30 Aug 2023 15:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10390v3</guid></item><item><title>Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages</title><link>http://arxiv.org/abs/2308.16075v1</link><description>The study investigates the effectiveness of utilizing multimodal informationin Neural Machine Translation (NMT). While prior research focused on usingmultimodal data in low-resource scenarios, this study examines how imagefeatures impact translation when added to a large-scale, pre-trained unimodalNMT system. Surprisingly, the study finds that images might be redundant inthis context. Additionally, the research introduces synthetic noise to assesswhether images help the model deal with textual noise. Multimodal modelsslightly outperform text-only models in noisy settings, even with randomimages. The study's experiments translate from English to Hindi, Bengali, andMalayalam, outperforming state-of-the-art benchmarks significantly.Interestingly, the effect of visual context varies with source text noise: novisual context works best for non-noisy translations, cropped image featuresare optimal for low noise, and full image features work better in high-noisescenarios. This sheds light on the role of visual context, especially in noisysettings, opening up a new research direction for Noisy Neural MachineTranslation in multimodal setups. The research emphasizes the importance ofcombining visual and textual information for improved translation in variousenvironments.</description><author>Baban Gain, Dibyanayan Bandyopadhyay, Samrat Mukherjee, Chandranath Adak, Asif Ekbal</author><pubDate>Wed, 30 Aug 2023 15:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16075v1</guid></item><item><title>Semantic Image Synthesis via Class-Adaptive Cross-Attention</title><link>http://arxiv.org/abs/2308.16071v1</link><description>In semantic image synthesis, the state of the art is dominated by methodsthat use spatially-adaptive normalization layers, which allow for excellentvisual generation quality and editing versatility. Granted their efficacy,recent research efforts have focused toward finer-grained local style controland multi-modal generation. By construction though, such layers tend tooverlook global image statistics leading to unconvincing local style editingand causing global inconsistencies such as color or illumination distributionshifts. Also, the semantic layout is required for mapping styles in thegenerator, putting a strict alignment constraint over the features. Inresponse, we designed a novel architecture where cross-attention layers areused in place of de-normalization ones for conditioning the image generation.Our model inherits the advantages of both solutions, retaining state-of-the-artreconstruction quality, as well as improved global and local style transfer.Code and models available at https://github.com/TFonta/CA2SIS.</description><author>Tomaso Fontanini, Claudio Ferrari, Giuseppe Lisanti, Massimo Bertozzi, Andrea Prati</author><pubDate>Wed, 30 Aug 2023 15:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16071v1</guid></item><item><title>Consensus of state of the art mortality prediction models: From all-cause mortality to sudden death prediction</title><link>http://arxiv.org/abs/2308.16067v1</link><description>Worldwide, many millions of people die suddenly and unexpectedly each year,either with or without a prior history of cardiovascular disease. Such eventsare sparse (once in a lifetime), many victims will not have had priorinvestigations for cardiac disease and many different definitions of suddendeath exist. Accordingly, sudden death is hard to predict. This analysis used NHS Electronic Health Records (EHRs) for people aged$\geq$50 years living in the Greater Glasgow and Clyde (GG\&amp;C) region in 2010(n = 380,000) to try to overcome these challenges. We investigated whethermedical history, blood tests, prescription of medicines, and hospitalisationsmight, in combination, predict a heightened risk of sudden death. We compared the performance of models trained to predict either sudden deathor all-cause mortality. We built six models for each outcome of interest: threetaken from state-of-the-art research (BEHRT, Deepr and Deep Patient), and threeof our own creation. We trained these using two different data representations:a language-based representation, and a sparse temporal matrix. We used global interpretability to understand the most important features ofeach model, and compare how much agreement there was amongst models using RankBiased Overlap. It is challenging to account for correlated variables withoutincreasing the complexity of the interpretability technique. We overcame thisby clustering features into groups and comparing the most important groups foreach model. We found the agreement between models to be much higher whenaccounting for correlated variables. Our analysis emphasises the challenge of predicting sudden death andemphasises the need for better understanding and interpretation of machinelearning models applied to healthcare applications.</description><author>Dr Yola Jones, Dr Fani Deligianni, Dr Jeff Dalton, Dr Pierpaolo Pellicori, Professor John G F Cleland</author><pubDate>Wed, 30 Aug 2023 15:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16067v1</guid></item><item><title>Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2301.08491v3</link><description>Practical uses of Artificial Intelligence (AI) in the real world havedemonstrated the importance of embedding moral choices into intelligent agents.They have also highlighted that defining top-down ethical constraints on AIaccording to any one type of morality is extremely challenging and can poserisks. A bottom-up learning approach may be more appropriate for studying anddeveloping ethical behavior in AI agents. In particular, we believe that aninteresting and insightful starting point is the analysis of emergent behaviorof Reinforcement Learning (RL) agents that act according to a predefined set ofmoral rewards in social dilemmas. In this work, we present a systematic analysis of the choices made byintrinsically-motivated RL agents whose rewards are based on moral theories. Weaim to design reward structures that are simplified yet representative of a setof key ethical systems. Therefore, we first define moral reward functions thatdistinguish between consequence- and norm-based agents, between morality basedon societal norms or internal virtues, and between single- and mixed-virtue(e.g., multi-objective) methodologies. Then, we evaluate our approach bymodeling repeated dyadic interactions between learning moral agents in threeiterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and StagHunt). We analyze the impact of different types of morality on the emergence ofcooperation, defection or exploitation, and the corresponding social outcomes.Finally, we discuss the implications of these findings for the development ofmoral agents in artificial and mixed human-AI societies.</description><author>Elizaveta Tennant, Stephen Hailes, Mirco Musolesi</author><pubDate>Wed, 30 Aug 2023 15:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08491v3</guid></item><item><title>NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data</title><link>http://arxiv.org/abs/2107.07752v2</link><description>Deep learning based Quantitative Susceptibility Mapping (QSM) has shown greatpotential in recent years, obtaining similar results to establishednon-learning approaches. Many current deep learning approaches are not dataconsistent, require in vivo training data or solve the QSM problem inconsecutive steps resulting in the propagation of errors. Here we aim toovercome these limitations and developed a framework to solve the QSMprocessing steps jointly. We developed a new hybrid training data generationmethod that enables the end-to-end training for solving background fieldcorrection and dipole inversion in a data-consistent fashion using avariational network that combines the QSM model term and a learned regularizer.We demonstrate that NeXtQSM overcomes the limitations of previous deep learningmethods. NeXtQSM offers a new deep learning based pipeline for computingquantitative susceptibility maps that integrates each processing step into thetraining and provides results that are robust and fast.</description><author>Francesco Cognolato, Kieran O'Brien, Jin Jin, Simon Robinson, Frederik B. Laun, Markus Barth, Steffen Bollmann</author><pubDate>Wed, 30 Aug 2023 15:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.07752v2</guid></item><item><title>Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning</title><link>http://arxiv.org/abs/2308.16061v1</link><description>Ransomware-as-a-service (RaaS) is increasing the scale and complexity ofransomware attacks. Understanding the internal operations behind RaaS has beena challenge due to the illegality of such activities. The recent chat leak ofthe Conti RaaS operator, one of the most infamous ransomware operators on theinternational scene, offers a key opportunity to better understand the innerworkings of such organizations. This paper analyzes the main topic discussionsin the Conti chat leak using machine learning techniques such as NaturalLanguage Processing (NLP) and Latent Dirichlet Allocation (LDA), as well asvisualization strategies. Five discussion topics are found: 1) Business, 2)Technical, 3) Internal tasking/Management, 4) Malware, and 5) CustomerService/Problem Solving. Moreover, the distribution of topics among Contimembers shows that only 4% of individuals have specialized discussions whilealmost all individuals (96%) are all-rounders, meaning that their discussionsrevolve around the five topics. The results also indicate that a significantproportion of Conti discussions are non-tech related. This study thushighlights that running such large RaaS operations requires a workforce skilledbeyond technical abilities, with individuals involved in various tasks, frommanagement to customer service or problem solving. The discussion topics alsoshow that the organization behind the Conti RaaS oper5086933ator sharessimilarities with a large firm. We conclude that, although RaaS represents anexample of specialization in the cybercrime industry, only a few members arespecialized in one topic, while the rest runs and coordinates the RaaSoperation.</description><author>Estelle Ruellan, Masarah Paquet-Clouston, Sebastian Garcia</author><pubDate>Wed, 30 Aug 2023 15:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16061v1</guid></item><item><title>Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap</title><link>http://arxiv.org/abs/2308.16060v1</link><description>We present Text-to-OverpassQL, a task designed to facilitate a naturallanguage interface for querying geodata from OpenStreetMap (OSM). The OverpassQuery Language (OverpassQL) allows users to formulate complex database queriesand is widely adopted in the OSM ecosystem. Generating Overpass queries fromnatural language input serves multiple use-cases. It enables novice users toutilize OverpassQL without prior knowledge, assists experienced users withcrafting advanced queries, and enables tool-augmented large language models toaccess information stored in the OSM database. In order to assess theperformance of current sequence generation models on this task, we proposeOverpassNL, a dataset of 8,352 queries with corresponding natural languageinputs. We further introduce task specific evaluation metrics and ground theevaluation of the Text-to-OverpassQL task by executing the queries against theOSM database. We establish strong baselines by finetuning sequence-to-sequencemodels and adapting large language models with in-context examples. Thedetailed evaluation reveals strengths and weaknesses of the considered learningstrategies, laying the foundations for further research into theText-to-OverpassQL task.</description><author>Michael Staniek, Raphael Schumann, Maike Züfle, Stefan Riezler</author><pubDate>Wed, 30 Aug 2023 15:33:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16060v1</guid></item><item><title>A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate</title><link>http://arxiv.org/abs/2308.16059v1</link><description>A covariance matrix estimator using two bits per entry was recently developedby Dirksen, Maly and Rauhut [Annals of Statistics, 50(6), pp. 3538-3562]. Theestimator achieves near minimax rate for general sub-Gaussian distributions,but also suffers from two downsides: theoretically, there is an essential gapon operator norm error between their estimator and sample covariance when thediagonal of the covariance matrix is dominated by only a few entries;practically, its performance heavily relies on the dithering scale, which needsto be tuned according to some unknown parameters. In this work, we propose anew 2-bit covariance matrix estimator that simultaneously addresses bothissues. Unlike the sign quantizer associated with uniform dither in Dirksen etal., we adopt a triangular dither prior to a 2-bit quantizer inspired by themulti-bit uniform quantizer. By employing dithering scales varying acrossentries, our estimator enjoys an improved operator norm error rate that dependson the effective rank of the underlying covariance matrix rather than theambient dimension, thus closing the theoretical gap. Moreover, our proposedmethod eliminates the need of any tuning parameter, as the dithering scales areentirely determined by the data. Experimental results under Gaussian samplesare provided to showcase the impressive numerical performance of our estimator.Remarkably, by halving the dithering scales, our estimator oftentimes achievesoperator norm errors less than twice of the errors of sample covariance.</description><author>Junren Chen, Michael K. Ng</author><pubDate>Wed, 30 Aug 2023 15:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16059v1</guid></item><item><title>TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</title><link>http://arxiv.org/abs/2306.08637v2</link><description>We present a novel model for Tracking Any Point (TAP) that effectively tracksany queried point on any physical surface throughout a video sequence. Ourapproach employs two stages: (1) a matching stage, which independently locatesa suitable candidate point match for the query point on every other frame, and(2) a refinement stage, which updates both the trajectory and query featuresbased on local correlations. The resulting model surpasses all baseline methodsby a significant margin on the TAP-Vid benchmark, as demonstrated by anapproximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our modelfacilitates fast inference on long and high-resolution video sequences. On amodern GPU, our implementation has the capacity to track points faster thanreal-time, and can be flexibly extended to higher-resolution videos. Given thehigh-quality trajectories extracted from a large dataset, we demonstrate aproof-of-concept diffusion model which generates trajectories from staticimages, enabling plausible animations. Visualizations, source code, andpretrained models can be found on our project webpage.</description><author>Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, Andrew Zisserman</author><pubDate>Wed, 30 Aug 2023 15:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08637v2</guid></item><item><title>Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs</title><link>http://arxiv.org/abs/2308.16056v1</link><description>Multitask learning (MTL) leverages task-relatedness to enhance performance.With the emergence of multimodal data, tasks can now be referenced by multipleindices. In this paper, we employ high-order tensors, with each modecorresponding to a task index, to naturally represent tasks referenced bymultiple indices and preserve their structural relations. Based on thisrepresentation, we propose a general framework of low-rank MTL methods withtensorized support vector machines (SVMs) and least square support vectormachines (LSSVMs), where the CP factorization is deployed over the coefficienttensor. Our approach allows to model the task relation through a linearcombination of shared factors weighted by task-specific factors and isgeneralized to both classification and regression problems. Through thealternating optimization scheme and the Lagrangian function, each subproblem istransformed into a convex problem, formulated as a quadratic programming orlinear system in the dual form. In contrast to previous MTL frameworks, ourdecision function in the dual induces a weighted kernel function with atask-coupling term characterized by the similarities of the task-specificfactors, better revealing the explicit relations across tasks in MTL.Experimental results validate the effectiveness and superiority of our proposedmethods compared to existing state-of-the-art approaches in MTL. The code ofimplementation will be available at https://github.com/liujiani0216/TSVM-MTL.</description><author>Jiani Liu, Qinghua Tao, Ce Zhu, Yipeng Liu, Xiaolin Huang, Johan A. K. Suykens</author><pubDate>Wed, 30 Aug 2023 15:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16056v1</guid></item><item><title>TriangleNet: Edge Prior Augmented Network for Semantic Segmentation through Cross-Task Consistency</title><link>http://arxiv.org/abs/2210.05152v5</link><description>This paper addresses the task of semantic segmentation in computer vision,aiming to achieve precise pixel-wise classification. We investigate the jointtraining of models for semantic edge detection and semantic segmentation, whichhas shown promise. However, implicit cross-task consistency learning inmulti-task networks is limited. To address this, we propose a novel "decoupledcross-task consistency loss" that explicitly enhances cross-task consistency.Our semantic segmentation network, TriangleNet, achieves a substantial 2.88\%improvement over the Baseline in mean Intersection over Union (mIoU) on theCityscapes test set. Notably, TriangleNet operates at 77.4\% mIoU/46.2 FPS onCityscapes, showcasing real-time inference capabilities at full resolution.With multi-scale inference, performance is further enhanced to 77.8\%.Furthermore, TriangleNet consistently outperforms the Baseline on the FloodNetdataset, demonstrating its robust generalization capabilities. The proposedmethod underscores the significance of multi-task learning and explicitcross-task consistency enhancement for advancing semantic segmentation andhighlights the potential of multitasking in real-time semantic segmentation.</description><author>Dan Zhang, Rui Zheng, Luosang Gadeng, Pei Yang</author><pubDate>Wed, 30 Aug 2023 15:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05152v5</guid></item><item><title>AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with Auxiliary Relations</title><link>http://arxiv.org/abs/2308.16055v1</link><description>Knowledge graph entity typing (KGET) is a task to predict the missing entitytypes in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried tosolve the KGET task by introducing an auxiliary relation, 'hasType', to modelthe relationship between entities and their types. However, a single auxiliaryrelation has limited expressiveness for diverse entity-type patterns. Weimprove the expressiveness of KGE methods by introducing multiple auxiliaryrelations in this work. Similar entity types are grouped to reduce the numberof auxiliary relations and improve their capability to model entity-typepatterns with different granularities. With the presence of multiple auxiliaryrelations, we propose a method adopting an Asynchronous learning scheme forEntity Typing, named AsyncET, which updates the entity and type embeddingsalternatively to keep the learned entity embedding up-to-date and informativefor entity type prediction. Experiments are conducted on two commonly used KGETdatasets to show that the performance of KGE methods on the KGET task can besubstantially improved by the proposed multiple auxiliary relations andasynchronous embedding learning. Furthermore, our method has a significantadvantage over state-of-the-art methods in model sizes and time complexity.</description><author>Yun-Cheng Wang, Xiou Ge, Bin Wang, C. -C. Jay Kuo</author><pubDate>Wed, 30 Aug 2023 15:24:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16055v1</guid></item><item><title>DREAM: Efficient Dataset Distillation by Representative Matching</title><link>http://arxiv.org/abs/2302.14416v3</link><description>Dataset distillation aims to synthesize small datasets with littleinformation loss from original large-scale ones for reducing storage andtraining costs. Recent state-of-the-art methods mainly constrain the samplesynthesis process by matching synthetic images and the original ones regardinggradients, embedding distributions, or training trajectories. Although thereare various matching objectives, currently the strategy for selecting originalimages is limited to naive random sampling. We argue that random sampling overlooks the evenness of the selected sampledistribution, which may result in noisy or biased matching targets. Besides, the sample diversity is also not constrained by random sampling.These factors together lead to optimization instability in the distillingprocess and degrade the training efficiency. Accordingly, we propose a novelmatching strategy named as \textbf{D}ataset distillation by\textbf{RE}present\textbf{A}tive \textbf{M}atching (DREAM), where onlyrepresentative original images are selected for matching. DREAM is able to beeasily plugged into popular dataset distillation frameworks and reduce thedistilling iterations by more than 8 times without performance drop. Givensufficient training time, DREAM further provides significant improvements andachieves state-of-the-art performances.</description><author>Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, Yang You</author><pubDate>Wed, 30 Aug 2023 15:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14416v3</guid></item><item><title>LAC -- Latent Action Composition for Skeleton-based Action Segmentation</title><link>http://arxiv.org/abs/2308.14500v2</link><description>Skeleton-based action segmentation requires recognizing composable actions inuntrimmed videos. Current approaches decouple this problem by first extractinglocal visual features from skeleton sequences and then processing them by atemporal model to classify frame-wise actions. However, their performancesremain limited as the visual features cannot sufficiently express composableactions. In this context, we propose Latent Action Composition (LAC), a novelself-supervised framework aiming at learning from synthesized composablemotions for skeleton-based action segmentation. LAC is composed of a novelgeneration module towards synthesizing new sequences. Specifically, we design alinear latent space in the generator to represent primitive motion. Newcomposed motions can be synthesized by simply performing arithmetic operationson latent representations of multiple input skeleton sequences. LAC leveragessuch synthesized sequences, which have large diversity and complexity, forlearning visual representations of skeletons in both sequence and frame spacesvia contrastive learning. The resulting visual encoder has a high expressivepower and can be effectively transferred onto action segmentation tasks byend-to-end fine-tuning without the need for additional temporal models. Weconduct a study focusing on transfer-learning and we show that representationslearned from pre-trained LAC outperform the state-of-the-art by a large marginon TSU, Charades, PKU-MMD datasets.</description><author>Di Yang, Yaohui Wang, Antitza Dantcheva, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond</author><pubDate>Wed, 30 Aug 2023 15:18:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14500v2</guid></item><item><title>Top-Down Network Combines Back-Propagation with Attention</title><link>http://arxiv.org/abs/2306.02415v2</link><description>Cortical processing, in vision and other domains, combines bottom-up (BU)with extensive top-down (TD) processing. Two primary goals attributed to TDprocessing are learning and directing attention. These two roles areaccomplished in current network models through distinct mechanisms. Attentionguidance is often implemented by extending the model's architecture, whilelearning is typically accomplished by an external learning algorithm such asback-propagation. In the current work, we present an integration of the twofunctions above, which appear unrelated, using a single unified mechanisminspired by the human brain. We propose a novel symmetric bottom-up top-downnetwork structure that can integrate conventional bottom-up networks with asymmetric top-down counterpart, allowing each network to recurrently guide andinfluence the other. For example, during multi-task learning, the same top-downnetwork is being used for both learning, via propagating feedback signals, andat the same time also for top-down attention, by guiding the bottom-up networkto perform a selected task. In contrast with standard models, no externalback-propagation is used for learning. Instead, we propose a 'Counter-Hebb'learning, which adjusts the weights of both the bottom-up and top-down networkssimultaneously. We show that our method achieves competitive performance onstandard multi-task learning benchmarks. Yet, unlike existing methods, we relyon single-task architectures and optimizers, without any task-specificparameters. The results, which show how attention-guided multi-tasks can becombined efficiently with internal learning in a unified TD process, suggest apossible model for combining BU and TD processing in human vision.</description><author>Roy Abel, Shimon Ullman</author><pubDate>Wed, 30 Aug 2023 15:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02415v2</guid></item><item><title>An exponentially-growing family of universal quantum circuits</title><link>http://arxiv.org/abs/2212.00736v3</link><description>Quantum machine learning has become an area of growing interest but hascertain theoretical and hardware-specific limitations. Notably, the problem ofvanishing gradients, or barren plateaus, renders the training impossible forcircuits with high qubit counts, imposing a limit on the number of qubits thatdata scientists can use for solving problems. Independently, angle-embeddedsupervised quantum neural networks were shown to produce truncated Fourierseries with a degree directly dependent on two factors: the depth of theencoding and the number of parallel qubits the encoding applied to. The degreeof the Fourier series limits the model expressivity. This work introduces twonew architectures whose Fourier degrees grow exponentially: the sequential andparallel exponential quantum machine learning architectures. This is done byefficiently using the available Hilbert space when encoding, increasing theexpressivity of the quantum encoding. Therefore, the exponential growth allowsstaying at the low-qubit limit to create highly expressive circuits avoidingbarren plateaus. Practically, parallel exponential architecture was shown tooutperform the existing linear architectures by reducing their final meansquare error value by up to 44.7% in a one-dimensional test problem.Furthermore, the feasibility of this technique was also shown on a trapped ionquantum processing unit.</description><author>Mo Kordzanganeh, Pavel Sekatski, Leonid Fedichkin, Alexey Melnikov</author><pubDate>Wed, 30 Aug 2023 15:11:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00736v3</guid></item><item><title>Exploring the Benefits of Visual Prompting in Differential Privacy</title><link>http://arxiv.org/abs/2303.12247v2</link><description>Visual Prompting (VP) is an emerging and powerful technique that allowssample-efficient adaptation to downstream tasks by engineering a well-trainedfrozen source model. In this work, we explore the benefits of VP inconstructing compelling neural network classifiers with differential privacy(DP). We explore and integrate VP into canonical DP training methods anddemonstrate its simplicity and efficiency. In particular, we discover that VPin tandem with PATE, a state-of-the-art DP training method that leverages theknowledge transfer from an ensemble of teachers, achieves the state-of-the-artprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,we conduct additional experiments on cross-domain image classification with asufficient domain gap to further unveil the advantage of VP in DP. Lastly, wealso conduct extensive ablation studies to validate the effectiveness andcontribution of VP under DP consideration. Our code is available at(https://github.com/EzzzLi/Prompt-PATE).</description><author>Yizhe Li, Yu-Lin Tsai, Xuebin Ren, Chia-Mu Yu, Pin-Yu Chen</author><pubDate>Wed, 30 Aug 2023 15:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12247v2</guid></item><item><title>DeltaNN: Assessing the Impact of Computational Environment Parameters on the Performance of Image Recognition Models</title><link>http://arxiv.org/abs/2306.06208v3</link><description>Image recognition tasks typically use deep learning and require enormousprocessing power, thus relying on hardware accelerators like GPUs and TPUs forfast, timely processing. Failure in real-time image recognition tasks can occurdue to sub-optimal mapping on hardware accelerators during model deployment,which may lead to timing uncertainty and erroneous behavior. Mapping onhardware accelerators is done using multiple software components like deeplearning frameworks, compilers, and device libraries, that we refer to as thecomputational environment. Owing to the increased use of image recognitiontasks in safety-critical applications like autonomous driving and medicalimaging, it is imperative to assess their robustness to changes in thecomputational environment, as the impact of parameters like deep learningframeworks, compiler optimizations, and hardware devices on model performanceand correctness is not yet well understood. In this paper we present a differential testing framework, DeltaNN, thatallows us to assess the impact of different computational environmentparameters on the performance of image recognition models during deployment,post training. DeltaNN generates different implementations of a given imagerecognition model for variations in environment parameters, namely, deeplearning frameworks, compiler optimizations and hardware devices and analyzesdifferences in model performance as a result. Using DeltaNN, we conduct anempirical study of robustness analysis of three popular image recognitionmodels using the ImageNet dataset. We report the impact in terms ofmisclassifications and inference time differences across different settings. Intotal, we observed up to 72% output label differences across deep learningframeworks, and up to 81% unexpected performance degradation in terms ofinference time, when applying compiler optimizations.</description><author>Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan</author><pubDate>Wed, 30 Aug 2023 15:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06208v3</guid></item><item><title>Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models</title><link>http://arxiv.org/abs/2305.08854v2</link><description>Speech-driven animation has gained significant traction in recent years, withcurrent methods achieving near-photorealistic results. However, the fieldremains underexplored regarding non-verbal communication despite evidencedemonstrating its importance in human interaction. In particular, generatinglaughter sequences presents a unique challenge due to the intricacy and nuancesof this behaviour. This paper aims to bridge this gap by proposing a novelmodel capable of generating realistic laughter sequences, given a stillportrait and an audio clip containing laughter. We highlight the failure casesof traditional facial animation methods and leverage recent advances indiffusion models to produce convincing laughter videos. We train our model on adiverse set of laughter datasets and introduce an evaluation metricspecifically designed for laughter. When compared with previous speech-drivenapproaches, our model achieves state-of-the-art performance across all metrics,even when these are re-trained for laughter generation. Our code and projectare publicly available</description><author>Antoni Bigata Casademunt, Rodrigo Mira, Nikita Drobyshev, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic</author><pubDate>Wed, 30 Aug 2023 15:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08854v2</guid></item><item><title>From Pixels to Portraits: A Comprehensive Survey of Talking Head Generation Techniques and Applications</title><link>http://arxiv.org/abs/2308.16041v1</link><description>Recent advancements in deep learning and computer vision have led to a surgeof interest in generating realistic talking heads. This paper presents acomprehensive survey of state-of-the-art methods for talking head generation.We systematically categorises them into four main approaches: image-driven,audio-driven, video-driven and others (including neural radiance fields (NeRF),and 3D-based methods). We provide an in-depth analysis of each method,highlighting their unique contributions, strengths, and limitations.Furthermore, we thoroughly compare publicly available models, evaluating themon key aspects such as inference time and human-rated quality of the generatedoutputs. Our aim is to provide a clear and concise overview of the currentlandscape in talking head generation, elucidating the relationships betweendifferent approaches and identifying promising directions for future research.This survey will serve as a valuable reference for researchers andpractitioners interested in this rapidly evolving field.</description><author>Shreyank N Gowda, Dheeraj Pandey, Shashank Narayana Gowda</author><pubDate>Wed, 30 Aug 2023 15:00:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16041v1</guid></item><item><title>Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification</title><link>http://arxiv.org/abs/2301.10611v2</link><description>In this paper, a discriminator-free adversarial-based Unsupervised DomainAdaptation (UDA) for Multi-Label Image Classification (MLIC) referred to asDDA-MLIC is proposed. Recently, some attempts have been made for introducingadversarial-based UDA methods in the context of MLIC. However, these methodswhich rely on an additional discriminator subnet present one major shortcoming.The learning of domain-invariant features may harm their task-specificdiscriminative power, since the classification and discrimination tasks aredecoupled. Herein, we propose to overcome this issue by introducing a noveladversarial critic that is directly deduced from the task-specific classifier.Specifically, a two-component Gaussian Mixture Model (GMM) is fitted on thesource and target predictions in order to distinguish between two clusters.This allows extracting a Gaussian distribution for each component. Theresulting Gaussian distributions are then used for formulating an adversarialloss based on a Frechet distance. The proposed method is evaluated on severalmulti-label image datasets covering three different types of domain shift. Theobtained results demonstrate that DDA-MLIC outperforms existingstate-of-the-art methods in terms of precision while requiring a lower numberof parameters. The code will be made publicly available online.</description><author>Indel Pal Singh, Enjie Ghorbel, Anis Kacem, Arunkumar Rathinam, Djamila Aouada</author><pubDate>Wed, 30 Aug 2023 14:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10611v2</guid></item><item><title>Fault Localization for Buggy Deep Learning Framework Conversions in Image Recognition</title><link>http://arxiv.org/abs/2306.06157v3</link><description>When deploying Deep Neural Networks (DNNs), developers often convert modelsfrom one deep learning framework to another (e.g., TensorFlow to PyTorch).However, this process is error-prone and can impact target model accuracy. Toidentify the extent of such impact, we perform and briefly present adifferential analysis against three DNNs widely used for image recognition(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deeplearning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), whichrevealed numerous model crashes and output label discrepancies of up to 72%. Tomitigate such errors, we present a novel approach towards fault localizationand repair of buggy deep learning framework conversions, focusing onpre-trained image recognition models. Our technique consists of four stages ofanalysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,and 4) graph representation. In addition, we propose various strategies towardsfault repair of the faults detected. We implement our technique on top of theApache TVM deep learning compiler, and we test it by conducting a preliminaryfault localization analysis for the conversion of InceptionV3 from TF toTFLite. Our approach detected a fault in a common DNN converter tool, whichintroduced precision errors in weights, reducing model accuracy. After ourfault localization, we repaired the issue, reducing our conversion error tozero.</description><author>Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan</author><pubDate>Wed, 30 Aug 2023 14:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06157v3</guid></item><item><title>Nonrigid Object Contact Estimation With Regional Unwrapping Transformer</title><link>http://arxiv.org/abs/2308.14074v2</link><description>Acquiring contact patterns between hands and nonrigid objects is a commonconcern in the vision and robotics community. However, existing learning-basedmethods focus more on contact with rigid ones from monocular images. Whenadopting them for nonrigid contact, a major problem is that the existingcontact representation is restricted by the geometry of the object.Consequently, contact neighborhoods are stored in an unordered manner andcontact features are difficult to align with image cues. At the core of ourapproach lies a novel hand-object contact representation called RUPs (RegionUnwrapping Profiles), which unwrap the roughly estimated hand-object surfacesas multiple high-resolution 2D regional profiles. The region grouping strategyis consistent with the hand kinematic bone division because they are theprimitive initiators for a composite contact pattern. Based on thisrepresentation, our Regional Unwrapping Transformer (RUFormer) learns thecorrelation priors across regions from monocular inputs and predictscorresponding contact and deformed transformations. Our experiments demonstratethat the proposed framework can robustly estimate the deformed degrees anddeformed transformations, which makes it suitable for both nonrigid and rigidcontact.</description><author>Wei Xie, Zimeng Zhao, Shiying Li, Binghui Zuo, Yangang Wang</author><pubDate>Wed, 30 Aug 2023 14:40:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14074v2</guid></item><item><title>How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges</title><link>http://arxiv.org/abs/2307.15016v2</link><description>Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT inthe field of conversational AI. Notably, Bard has recently been updated tohandle visual inputs alongside text prompts during conversations. Given Bard'simpressive track record in handling textual inputs, we explore its capabilitiesin understanding and interpreting visual data (images) conditioned by textquestions. This exploration holds the potential to unveil new insights andchallenges for Bard and other forthcoming multi-modal Generative models,especially in addressing complex computer vision problems that demand accuratevisual and language understanding. Specifically, in this study, we focus on 15diverse task scenarios encompassing regular, camouflaged, medical, under-waterand remote sensing data to comprehensively evaluate Bard's performance. Ourprimary finding indicates that Bard still struggles in these vision scenarios,highlighting the significant gap in vision-based understanding that needs to bebridged in future developments. We expect that this empirical study will provevaluable in advancing future models, leading to enhanced capabilities incomprehending and interpreting fine-grained visual data. Our project isreleased on https://github.com/htqin/GoogleBard-VisUnderstand</description><author>Haotong Qin, Ge-Peng Ji, Salman Khan, Deng-Ping Fan, Fahad Shahbaz Khan, Luc Van Gool</author><pubDate>Wed, 30 Aug 2023 14:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15016v2</guid></item><item><title>MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing</title><link>http://arxiv.org/abs/2308.14036v2</link><description>In recent years, Transformer networks are beginning to replace pureconvolutional neural networks (CNNs) in the field of computer vision due totheir global receptive field and adaptability to input. However, the quadraticcomputational complexity of softmax-attention limits the wide application inimage dehazing task, especially for high-resolution images. To address thisissue, we propose a new Transformer variant, which applies the Taylor expansionto approximate the softmax-attention and achieves linear computationalcomplexity. A multi-scale attention refinement module is proposed as acomplement to correct the error of the Taylor expansion. Furthermore, weintroduce a multi-branch architecture with multi-scale patch embedding to theproposed Transformer, which embeds features by overlapping deformableconvolution of different scales. The design of multi-scale patch embedding isbased on three key ideas: 1) various sizes of the receptive field; 2)multi-level semantic information; 3) flexible shapes of the receptive field.Our model, named Multi-branch Transformer expanded by Taylor formula(MB-TaylorFormer), can embed coarse to fine features more flexibly at the patchembedding stage and capture long-distance pixel interactions with limitedcomputational cost. Experimental results on several dehazing benchmarks showthat MB-TaylorFormer achieves state-of-the-art (SOTA) performance with a lightcomputational burden. The source code and pre-trained models are available athttps://github.com/FVL2020/ICCV-2023-MB-TaylorFormer.</description><author>Yuwei Qiu, Kaihao Zhang, Chenxi Wang, Wenhan Luo, Hongdong Li, Zhi Jin</author><pubDate>Wed, 30 Aug 2023 14:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14036v2</guid></item><item><title>Large Language Models are not Fair Evaluators</title><link>http://arxiv.org/abs/2305.17926v2</link><description>In this paper, we uncover a systematic bias in the evaluation paradigm ofadopting large language models~(LLMs), e.g., GPT-4, as a referee to score andcompare the quality of responses generated by candidate models. We find thatthe quality ranking of candidate responses can be easily hacked by simplyaltering their order of appearance in the context. This manipulation allows usto skew the evaluation result, making one model appear considerably superior tothe other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested querieswith ChatGPT as an evaluator. To address this issue, we propose a calibrationframework with three simple yet effective strategies: 1) Multiple EvidenceCalibration, which requires the evaluator model to generate multiple evaluationevidence before assigning ratings; 2) Balanced Position Calibration, whichaggregates results across various orders to determine the final score; 3)Human-in-the-Loop Calibration, which introduces a balanced position diversityentropy to measure the difficulty of each example and seeks human assistancewhen needed. We also manually annotate the "win/tie/lose" outcomes of responsesfrom ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, andextensive experiments demonstrate that our approach successfully mitigatesevaluation bias, resulting in closer alignment with human judgments. We releaseour code and human annotation at \url{https://github.com/i-Eval/FairEval} tofacilitate future research.</description><author>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui</author><pubDate>Wed, 30 Aug 2023 14:22:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17926v2</guid></item><item><title>PAVI: Plate-Amortized Variational Inference</title><link>http://arxiv.org/abs/2308.16022v1</link><description>Given observed data and a probabilistic generative model, Bayesian inferencesearches for the distribution of the model's parameters that could have yieldedthe data. Inference is challenging for large population studies where millionsof measurements are performed over a cohort of hundreds of subjects, resultingin a massive parameter space. This large cardinality renders off-the-shelfVariational Inference (VI) computationally impractical. In this work, we design structured VI families that efficiently tackle largepopulation studies. Our main idea is to share the parameterization and learningacross the different i.i.d. variables in a generative model, symbolized by themodel's \textit{plates}. We name this concept \textit{plate amortization}.Contrary to off-the-shelf stochastic VI, which slows down inference, plateamortization results in orders of magnitude faster to train variationaldistributions. Applied to large-scale hierarchical problems, PAVI yields expressive,parsimoniously parameterized VI with an affordable training time. This fasterconvergence effectively unlocks inference in those large regimes. We illustratethe practical utility of PAVI through a challenging Neuroimaging examplefeaturing 400 million latent parameters, demonstrating a significant steptowards scalable and expressive Variational Inference.</description><author>Louis Rouillard, Alexandre Le Bris, Thomas Moreau, Demian Wassermann</author><pubDate>Wed, 30 Aug 2023 14:22:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16022v1</guid></item><item><title>Topology-aware MLP for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v1</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, existing previous GCN-based methodshave relied excessively on elaborate human body priors and constructed complexfeature aggregation mechanisms, which limits the generalizability of networks.To solve these problems, we propose a novel Spatial Topology Gating Unit(STGU), which is an MLP-based variant without extra priors, to capture theco-occurrence topology features that encode the spatial dependency across alljoints. In STGU, to model the sample-specific and completely independentpoint-wise topology attention, a new gate-based feature interaction mechanismis introduced to activate the features point-to-point by the attention mapgenerated from the input. Based on the STGU, in this work, we propose the firsttopology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.In comparison with existing previous methods on three large-scale datasets,Ta-MLP achieves competitive performance. In addition, Ta-MLP reduces theparameters by up to 62.5% with favorable results. Compared with previousstate-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-timeaction recognition. The code will be available athttps://github.com/BUPTSJZhang/Ta-MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Wed, 30 Aug 2023 14:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v1</guid></item><item><title>Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations</title><link>http://arxiv.org/abs/2308.13081v2</link><description>This document presents adequate formal terminology for the mathematicalspecification of a subset of Agent Based Models (ABMs) in the field ofDemography. The simulation of the targeted ABMs follows a fixed-stepsingle-clocked pattern. The proposed terminology further improves the modelunderstanding and can act as a stand-alone methodology for the specificationand optionally the documentation of a significant set of (demographic) ABMs.Nevertheless, it is imaginable the this terminology probably with furtherextensions can be merged with the largely-informal widely-used modeldocumentation and communication O.D.D. protocol [Grimm and et al., 2020,Amouroux et al., 2010] to reduce many sources of ambiguity, hindering modelreplications by other modelers. A published demographic model documentation,largely simplified version of the Lone Parent Model [Gostoli and Silverman,2020] is separately published in [Elsheikh, 2023b] as illustration for theformal terminology. The model was implemented in the Julia language [Elsheikh,2023a] based on the Agents.jl julia package [Datseris et al., 2022].</description><author>Atiyah Elsheikh</author><pubDate>Wed, 30 Aug 2023 14:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13081v2</guid></item><item><title>FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions</title><link>http://arxiv.org/abs/2308.15214v2</link><description>We demonstrate an embodied conversational agent that can function as areceptionist and generate a mixture of open and closed-domain dialogue alongwith facial expressions, by using a large language model (LLM) to develop anengaging conversation. We deployed the system onto a Furhat robot, which ishighly expressive and capable of using both verbal and nonverbal cues duringinteraction. The system was designed specifically for the National Robotariumto interact with visitors through natural conversations, providing them withinformation about the facilities, research, news, upcoming events, etc. Thesystem utilises the state-of-the-art GPT-3.5 model to generate such informationalong with domain-general conversations and facial expressions based on promptengineering.</description><author>Neeraj Cherakara, Finny Varghese, Sheena Shabana, Nivan Nelson, Abhiram Karukayil, Rohith Kulothungan, Mohammed Afil Farhan, Birthe Nesset, Meriam Moujahid, Tanvi Dinkar, Verena Rieser, Oliver Lemon</author><pubDate>Wed, 30 Aug 2023 14:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15214v2</guid></item><item><title>Classifying World War II Era Ciphers with Machine Learning</title><link>http://arxiv.org/abs/2307.00501v2</link><description>We determine the accuracy with which machine learning and deep learningtechniques can classify selected World War II era ciphers when only ciphertextis available. The specific ciphers considered are Enigma, M-209, Sigaba,Purple, and Typex. We experiment with three classic machine learning models,namely, Support Vector Machines (SVM), $k$-Nearest Neighbors ($k$-NN), andRandom Forest (RF). We also experiment with four deep learning neuralnetwork-based models: Multi-Layer Perceptrons (MLP), Long Short-Term Memory(LSTM), Extreme Learning Machines (ELM), and Convolutional Neural Networks(CNN). Each model is trained on features consisting of histograms, digrams, andraw ciphertext letter sequences. Furthermore, the classification problem isconsidered under four distinct scenarios: Fixed plaintext with fixed keys,random plaintext with fixed keys, fixed plaintext with random keys, and randomplaintext with random keys. Under the most realistic scenario, given 1000characters per ciphertext, we are able to distinguish the ciphers with greaterthan 97% accuracy. In addition, we consider the accuracy of a subset of thelearning techniques as a function of the length of the ciphertext messages.Somewhat surprisingly, our classic machine learning models perform at least aswell as our deep learning models. We also find that ciphers that are moresimilar in design are somewhat more challenging to distinguish, but not asdifficult as might be expected.</description><author>Brooke Dalton, Mark Stamp</author><pubDate>Wed, 30 Aug 2023 14:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00501v2</guid></item><item><title>SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers</title><link>http://arxiv.org/abs/2306.06289v2</link><description>This paper investigates the capability of plain Vision Transformers (ViTs)for semantic segmentation using the encoder-decoder framework and introduces\textbf{SegViTv2}. In this study, we introduce a novel Attention-to-Mask (\atm)module to design a lightweight decoder effective for plain ViT. The proposedATM converts the global attention map into semantic masks for high-qualitysegmentation results. Our decoder outperforms the popular decoder UPerNet usingvarious ViT backbones while consuming only about $5\%$ of the computationalcost. For the encoder, we address the concern of the relatively highcomputational cost in the ViT-based encoders and propose a \emph{Shrunk++}structure that incorporates edge-aware query-based down-sampling (EQD) andquery-based upsampling (QU) modules. The Shrunk++ structure reduces thecomputational cost of the encoder by up to $50\%$ while maintaining competitiveperformance. Furthermore, we propose to adapt SegViT for continual semanticsegmentation, demonstrating nearly zero forgetting of previously learnedknowledge. Experiments show that our proposed SegViTv2 surpasses recentsegmentation methods on three popular benchmarks including ADE20k,COCO-Stuff-10k and PASCAL-Context datasets. The code is available through thefollowing link: \url{https://github.com/zbwxp/SegVit}.</description><author>Bowen Zhang, Liyang Liu, Minh Hieu Phan, Zhi Tian, Chunhua Shen, Yifan Liu</author><pubDate>Wed, 30 Aug 2023 14:01:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06289v2</guid></item><item><title>BinaryViT: Towards Efficient and Accurate Binary Vision Transformers</title><link>http://arxiv.org/abs/2305.14730v2</link><description>Vision Transformers (ViTs) have emerged as the fundamental architecture formost computer vision fields, but the considerable memory and computation costshinders their application on resource-limited devices. As one of the mostpowerful compression methods, binarization reduces the computation of theneural network by quantizing the weights and activation values as $\pm$1.Although existing binarization methods have demonstrated excellent performanceon Convolutional Neural Networks (CNNs), the full binarization of ViTs is stillunder-studied and suffering a significant performance drop. In this paper, wefirst argue empirically that the severe performance degradation is mainlycaused by the weight oscillation in the binarization training and theinformation distortion in the activation of ViTs. Based on these analyses, wepropose $\textbf{BinaryViT}$, an accurate full binarization scheme for ViTs,which pushes the quantization of ViTs to the limit. Specifically, we propose anovel gradient regularization scheme (GRS) for driving a bimodal distributionof the weights to reduce oscillation in binarization training. Moreover, wedesign an activation shift module (ASM) to adaptively tune the activationdistribution to reduce the information distortion caused by binarization.Extensive experiments on ImageNet dataset show that our BinaryViT consistentlysurpasses the strong baseline by 2.05% and improve the accuracy of fullybinarized ViTs to a usable level. Furthermore, our method achieves impressivesavings of 16.2$\times$ and 17.7$\times$ in model size and OPs compared to thefull-precision DeiT-S.</description><author>Junrui Xiao, Zhikai Li, Lianwei Yang, Qingyi Gu</author><pubDate>Wed, 30 Aug 2023 14:00:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14730v2</guid></item><item><title>EnsembleFollower: A Hybrid Car-Following Framework Based On Reinforcement Learning and Hierarchical Planning</title><link>http://arxiv.org/abs/2308.16008v1</link><description>Car-following models have made significant contributions to our understandingof longitudinal driving behavior. However, they often exhibit limited accuracyand flexibility, as they cannot fully capture the complexity inherent incar-following processes, or may falter in unseen scenarios due to theirreliance on confined driving skills present in training data. It is worthnoting that each car-following model possesses its own strengths and weaknessesdepending on specific driving scenarios. Therefore, we proposeEnsembleFollower, a hierarchical planning framework for achieving advancedhuman-like car-following. The EnsembleFollower framework involves a high-levelReinforcement Learning-based agent responsible for judiciously managingmultiple low-level car-following models according to the current state, eitherby selecting an appropriate low-level model to perform an action or byallocating different weights across all low-level components. Moreover, wepropose a jerk-constrained kinematic model for more convincing car-followingsimulations. We evaluate the proposed method based on real-world driving datafrom the HighD dataset. The experimental results illustrate thatEnsembleFollower yields improved accuracy of human-like behavior and achieveseffectiveness in combining hybrid models, demonstrating that our proposedframework can handle diverse car-following conditions by leveraging thestrengths of various low-level models.</description><author>Xu Han, Xianda Chen, Meixin Zhu, Pinlong Cai, Jianshan Zhou, Xiaowen Chu</author><pubDate>Wed, 30 Aug 2023 13:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16008v1</guid></item><item><title>Evaluating the Quality and Diversity of DCGAN-based Generatively Synthesized Diabetic Retinopathy Imagery</title><link>http://arxiv.org/abs/2208.05593v3</link><description>Publicly available diabetic retinopathy (DR) datasets are imbalanced,containing limited numbers of images with DR. This imbalance contributes tooverfitting when training machine learning classifiers. The impact of thisimbalance is exacerbated as the severity of the DR stage increases, affectingthe classifiers' diagnostic capacity. The imbalance can be addressed usingGenerative Adversarial Networks (GANs) to augment the datasets with syntheticimages. Generating synthetic images is advantageous if high-quality anddiversified images are produced. To evaluate the quality and diversity ofsynthetic images, several evaluation metrics, such as Multi-Scale StructuralSimilarity Index (MS-SSIM), Cosine Distance (CD), and Fr\'echet InceptionDistance (FID) are used. Understanding the effectiveness of each metric inevaluating the quality and diversity of GAN-based synthetic images is criticalto select images for augmentation. To date, there has been limited analysis ofthe appropriateness of these metrics in the context of biomedical imagery. Thiswork contributes an empirical assessment of these evaluation metrics as appliedto synthetic Proliferative DR imagery generated by a Deep Convolutional GAN(DCGAN). Furthermore, the metrics' capacity to indicate the quality anddiversity of synthetic images and a correlation with classifier performance isundertaken. This enables a quantitative selection of synthetic imagery and aninformed augmentation strategy. Results indicate that FID is suitable forevaluating the quality, while MS-SSIM and CD are suitable for evaluating thediversity of synthetic imagery. Furthermore, the superior performance ofConvolutional Neural Network (CNN) and EfficientNet classifiers, as indicatedby the F1 and AUC scores, for the augmented datasets demonstrates the efficacyof synthetic imagery to augment the imbalanced dataset.</description><author>Cristina-Madalina Dragan, Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O'Reilly</author><pubDate>Wed, 30 Aug 2023 13:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.05593v3</guid></item><item><title>DTrOCR: Decoder-only Transformer for Optical Character Recognition</title><link>http://arxiv.org/abs/2308.15996v1</link><description>Typical text recognition methods rely on an encoder-decoder structure, inwhich the encoder extracts features from an image, and the decoder producesrecognized text from these features. In this study, we propose a simpler andmore effective method for text recognition, known as the Decoder-onlyTransformer for Optical Character Recognition (DTrOCR). This method uses adecoder-only Transformer to take advantage of a generative language model thatis pre-trained on a large corpus. We examined whether a generative languagemodel that has been successful in natural language processing can also beeffective for text recognition in computer vision. Our experiments demonstratedthat DTrOCR outperforms current state-of-the-art methods by a large margin inthe recognition of printed, handwritten, and scene text in both English andChinese.</description><author>Masato Fujitake</author><pubDate>Wed, 30 Aug 2023 13:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15996v1</guid></item><item><title>CLSE: Corpus of Linguistically Significant Entities</title><link>http://arxiv.org/abs/2211.02423v2</link><description>One of the biggest challenges of natural language generation (NLG) is theproper handling of named entities. Named entities are a common source ofgrammar mistakes such as wrong prepositions, wrong article handling, orincorrect entity inflection. Without factoring linguistic representation, sucherrors are often underrepresented when evaluating on a small set of arbitrarilypicked argument values, or when translating a dataset from a linguisticallysimpler language, like English, to a linguistically complex language, likeRussian. However, for some applications, broadly precise grammaticalcorrectness is critical -- native speakers may find entity-related grammarerrors silly, jarring, or even offensive. To enable the creation of more linguistically diverse NLG datasets, werelease a Corpus of Linguistically Significant Entities (CLSE) annotated bylinguist experts. The corpus includes 34 languages and covers 74 differentsemantic types to support various applications from airline ticketing to videogames. To demonstrate one possible use of CLSE, we produce an augmented versionof the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE's entities and asmall number of human translations, we create a linguistically representativeNLG evaluation benchmark in three languages: French (high-resource), Marathi(low-resource), and Russian (highly inflected language). We establish qualitybaselines for neural, template-based, and hybrid NLG systems and discuss thestrengths and weaknesses of each approach.</description><author>Aleksandr Chuklin, Justin Zhao, Mihir Kale</author><pubDate>Wed, 30 Aug 2023 13:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02423v2</guid></item><item><title>DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving</title><link>http://arxiv.org/abs/2308.15991v1</link><description>Autonomous driving systems are always built on motion-related modules such asthe planner and the controller. An accurate and robust trajectory trackingmethod is indispensable for these motion-related modules as a primitiveroutine. Current methods often make strong assumptions about the model such asthe context and the dynamics, which are not robust enough to deal with thechanging scenarios in a real-world system. In this paper, we propose a DeepReinforcement Learning (DRL)-based trajectory tracking method for themotion-related modules in autonomous driving systems. The representationlearning ability of DL and the exploration nature of RL bring strong robustnessand improve accuracy. Meanwhile, it enhances versatility by running thetrajectory tracking in a model-free and data-driven manner. Through extensiveexperiments, we demonstrate both the efficiency and effectiveness of our methodcompared to current methods.</description><author>Yinda Xu, Lidong Yu</author><pubDate>Wed, 30 Aug 2023 13:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15991v1</guid></item><item><title>DiffuVolume: Diffusion Model for Volume based Stereo Matching</title><link>http://arxiv.org/abs/2308.15989v1</link><description>Stereo matching is a significant part in many computer vision tasks anddriving-based applications. Recently cost volume-based methods have achievedgreat success benefiting from the rich geometry information in paired images.However, the redundancy of cost volume also interferes with the model trainingand limits the performance. To construct a more precise cost volume, wepioneeringly apply the diffusion model to stereo matching. Our method, termedDiffuVolume, considers the diffusion model as a cost volume filter, which willrecurrently remove the redundant information from the cost volume. Two maindesigns make our method not trivial. Firstly, to make the diffusion model moreadaptive to stereo matching, we eschew the traditional manner of directlyadding noise into the image but embed the diffusion model into a task-specificmodule. In this way, we outperform the traditional diffusion stereo matchingmethod by 22% EPE improvement and 240 times inference acceleration. Secondly,DiffuVolume can be easily embedded into any volume-based stereo matchingnetwork with boost performance but slight parameters rise (only 2%). By addingthe DiffuVolume into well-performed methods, we outperform all the publishedmethods on Scene Flow, KITTI2012, KITTI2015 benchmarks and zero-shotgeneralization setting. It is worth mentioning that the proposed model ranks1st on KITTI 2012 leader board, 2nd on KITTI 2015 leader board since 15, July2023.</description><author>Dian Zheng, Xiao-Ming Wu, Zuhao Liu, Jingke Meng, Wei-shi Zheng</author><pubDate>Wed, 30 Aug 2023 13:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15989v1</guid></item><item><title>FPTQ: Fine-grained Post-Training Quantization for Large Language Models</title><link>http://arxiv.org/abs/2308.15987v1</link><description>In the era of large-scale language models, the substantial parameter sizeposes significant challenges for deployment. Being a prevalent compressiontechnique, quantization has emerged as the mainstream practice to tackle thisissue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights andactivations in such bit widths). In this study, we propose a novel W4A8post-training quantization method for the available open-sourced LLMs, whichcombines the advantages of both two recipes. Therefore, we can leverage thebenefit in the I/O utilization of 4-bit weight quantization and theacceleration due to 8-bit matrix computation. Nevertheless, the W4A8 facesnotorious performance degradation. As a remedy, we involve layerwise activationquantization strategies which feature a novel logarithmic equalization for mostintractable layers, and we combine them with fine-grained weight quantization.Without whistles and bells, we eliminate the necessity for further fine-tuningand obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, andLLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization isachievable for the deployment of large language models, fostering theirwide-spreading real-world applications.</description><author>Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, Yuchen Xie</author><pubDate>Wed, 30 Aug 2023 13:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15987v1</guid></item><item><title>Vision-Based Traffic Accident Detection and Anticipation: A Survey</title><link>http://arxiv.org/abs/2308.15985v1</link><description>Traffic accident detection and anticipation is an obstinate road safetyproblem and painstaking efforts have been devoted. With the rapid growth ofvideo data, Vision-based Traffic Accident Detection and Anticipation (namedVision-TAD and Vision-TAA) become the last one-mile problem for safe drivingand surveillance safety. However, the long-tailed, unbalanced, highly dynamic,complex, and uncertain properties of traffic accidents form theOut-of-Distribution (OOD) feature for Vision-TAD and Vision-TAA. Current AIdevelopment may focus on these OOD but important problems. What has been donefor Vision-TAD and Vision-TAA? What direction we should focus on in the futurefor this problem? A comprehensive survey is important. We present the firstsurvey on Vision-TAD in the deep learning era and the first-ever survey forVision-TAA. The pros and cons of each research prototype are discussed indetail during the investigation. In addition, we also provide a critical reviewof 31 publicly available benchmarks and related evaluation metrics. Throughthis survey, we want to spawn new insights and open possible trends forVision-TAD and Vision-TAA tasks.</description><author>Jianwu Fang, iahuan Qiao, Jianru Xue, Zhengguo Li</author><pubDate>Wed, 30 Aug 2023 13:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15985v1</guid></item><item><title>Learning Structure-from-Motion with Graph Attention Networks</title><link>http://arxiv.org/abs/2308.15984v1</link><description>In this paper we tackle the problem of learning Structure-from-Motion (SfM)through the use of graph attention networks. SfM is a classic computer visionproblem that is solved though iterative minimization of reprojection errors,referred to as Bundle Adjustment (BA), starting from a good initialization. Inorder to obtain a good enough initialization to BA, conventional methods relyon a sequence of sub-problems (such as pairwise pose estimation, pose averagingor triangulation) which provides an initial solution that can then be refinedusing BA. In this work we replace these sub-problems by learning a model thattakes as input the 2D keypoints detected across multiple views, and outputs thecorresponding camera poses and 3D keypoint coordinates. Our model takesadvantage of graph neural networks to learn SfM-specific primitives, and weshow that it can be used for fast inference of the reconstruction for new andunseen sequences. The experimental results show that the proposed modeloutperforms competing learning-based methods, and challenges COLMAP whilehaving lower runtime.</description><author>Lucas Brynte, José Pedro Iglesias, Carl Olsson, Fredrik Kahl</author><pubDate>Wed, 30 Aug 2023 13:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15984v1</guid></item><item><title>MerA: Merging Pretrained Adapters For Few-Shot Learning</title><link>http://arxiv.org/abs/2308.15982v1</link><description>Adapter tuning, which updates only a few parameters, has become a mainstreammethod for fine-tuning pretrained language models to downstream tasks. However,it often yields subpar results in few-shot learning. AdapterFusion, whichassembles pretrained adapters using composition layers tailored to specifictasks, is a possible solution but significantly increases trainable parametersand deployment costs. Despite this, our preliminary study reveals that evensingle adapters can outperform Adapterfusion in few-shot learning, urging us topropose \textbf{\texttt{Merging Pretrained Adapters}} (MerA) that efficientlyincorporates pretrained adapters to a single model through model fusion.Extensive experiments on two PLMs demonstrate that MerA achieves substantialimprovements compared to both single adapters and AdapterFusion. To furtherenhance the capacity of MerA, we also introduce a simple yet effectivetechnique, referred to as the "\textit{same-track}" setting, that mergesadapters from the same track of pretraining tasks. With the implementation ofthe "\textit{same-track}" setting, we observe even more impressive gains,surpassing the performance of both full fine-tuning and adapter tuning by asubstantial margin, e.g., 3.5\% in MRPC and 5.0\% in MNLI.</description><author>Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao</author><pubDate>Wed, 30 Aug 2023 13:10:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15982v1</guid></item><item><title>HypLL: The Hyperbolic Learning Library</title><link>http://arxiv.org/abs/2306.06154v2</link><description>Deep learning in hyperbolic space is quickly gaining traction in the fieldsof machine learning, multimedia, and computer vision. Deep networks commonlyoperate in Euclidean space, implicitly assuming that data lies on regulargrids. Recent advances have shown that hyperbolic geometry provides a viablealternative foundation for deep learning, especially when data is hierarchicalin nature and when working with few embedding dimensions. Currently however, noaccessible open-source library exists to build hyperbolic network modules akinto well-known deep learning libraries. We present HypLL, the HyperbolicLearning Library to bring the progress on hyperbolic deep learning together.HypLL is built on top of PyTorch, with an emphasis in its design forease-of-use, in order to attract a broad audience towards this new andopen-ended research direction. The code is available at:https://github.com/maxvanspengler/hyperbolic_learning_library.</description><author>Max van Spengler, Philipp Wirth, Pascal Mettes</author><pubDate>Wed, 30 Aug 2023 13:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06154v2</guid></item><item><title>Quantifying Process Quality: The Role of Effective Organizational Learning in Software Evolution</title><link>http://arxiv.org/abs/2305.18061v4</link><description>Real-world software applications must constantly evolve to remain relevant.This evolution occurs when developing new applications or adapting existingones to meet new requirements, make corrections, or incorporate futurefunctionality. Traditional methods of software quality control involve softwarequality models and continuous code inspection tools. These measures focus ondirectly assessing the quality of the software. However, there is a strongcorrelation and causation between the quality of the development process andthe resulting software product. Therefore, improving the development processindirectly improves the software product, too. To achieve this, effectivelearning from past processes is necessary, often embraced through post mortemorganizational learning. While qualitative evaluation of large artifacts iscommon, smaller quantitative changes captured by application lifecyclemanagement are often overlooked. In addition to software metrics, these smallerchanges can reveal complex phenomena related to project culture and management.Leveraging these changes can help detect and address such complex issues. Software evolution was previously measured by the size of changes, but thelack of consensus on a reliable and versatile quantification method preventsits use as a dependable metric. Different size classifications fail to reliablydescribe the nature of evolution. While application lifecycle management datais rich, identifying which artifacts can model detrimental managerial practicesremains uncertain. Approaches such as simulation modeling, discrete eventssimulation, or Bayesian networks have only limited ability to exploitcontinuous-time process models of such phenomena. Even worse, the accessibilityand mechanistic insight into such gray- or black-box models are typically verylow. To address these challenges, we suggest leveraging objectively [...]</description><author>Sebastian Hönel</author><pubDate>Wed, 30 Aug 2023 13:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18061v4</guid></item><item><title>RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation</title><link>http://arxiv.org/abs/2308.15975v1</link><description>For robots to be useful outside labs and specialized factories we need a wayto teach them new useful behaviors quickly. Current approaches lack either thegenerality to onboard new tasks without task-specific engineering, or else lackthe data-efficiency to do so in an amount of time that enables practical use.In this work we explore dense tracking as a representational vehicle to allowfaster and more general learning from demonstration. Our approach utilizesTrack-Any-Point (TAP) models to isolate the relevant motion in a demonstration,and parameterize a low-level controller to reproduce this motion across changesin the scene configuration. We show this results in robust robot policies thatcan solve complex object-arrangement tasks such as shape-matching, stacking,and even full path-following tasks such as applying glue and sticking objectstogether, all from demonstrations that can be collected in minutes.</description><author>Mel Vecerik, Carl Doersch, Yi Yang, Todor Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, Jon Scholz</author><pubDate>Wed, 30 Aug 2023 12:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15975v1</guid></item><item><title>Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles</title><link>http://arxiv.org/abs/2308.14521v2</link><description>Computational agents support humans in many areas of life and are thereforefound in heterogeneous contexts. This means they operate in rapidly changingenvironments and can be confronted with huge state and action spaces. In orderto perform services and carry out activities in a goal-oriented manner, agentsrequire prior knowledge and therefore have to develop and pursuecontext-dependent policies. However, prescribing policies in advance is limitedand inflexible, especially in dynamically changing environments. Moreover, thecontext of an agent determines its choice of actions. Since the environmentscan be stochastic and complex in terms of the number of states and feasibleactions, activities are usually modelled in a simplified way by Markov decisionprocesses so that, e.g., agents with reinforcement learning are able to learnpolicies, that help to capture the context and act accordingly to optimallyperform activities. However, training policies for all possible contexts usingreinforcement learning is time-consuming. A requirement and challenge foragents is to learn strategies quickly and respond immediately in cross-contextenvironments and applications, e.g., the Internet, service robotics,cyber-physical systems. In this work, we propose a novel simulation-basedapproach that enables a) the representation of heterogeneous contexts throughknowledge graphs and entity embeddings and b) the context-aware composition ofpolicies on demand by ensembles of agents running in parallel. The evaluationwe conducted with the "Virtual Home" dataset indicates that agents with a needto switch seamlessly between different contexts, can request on-demand composedpolicies that lead to the successful completion of context-appropriateactivities without having to learn these policies in lengthy training steps andepisodes, in contrast to agents that use reinforcement learning.</description><author>Nicole Merkle, Ralf Mikut</author><pubDate>Wed, 30 Aug 2023 12:56:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14521v2</guid></item><item><title>Demo: A Digital Twin of the 5G Radio Access Network for Anomaly Detection Functionality</title><link>http://arxiv.org/abs/2308.15973v1</link><description>Recently, the concept of digital twins (DTs) has received significantattention within the realm of 5G/6G. This demonstration shows an innovative DTdesign and implementation framework tailored toward integration within the 5Ginfrastructure. The proposed DT enables near real-time anomaly detectioncapability pertaining to user connectivity. It empowers the 5G system toproactively execute decisions for resource control and connection restoration.</description><author>Peizheng Li, Adnan Aijaz, Tim Farnham, Sajida Gufran, Sita Chintalapati</author><pubDate>Wed, 30 Aug 2023 12:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15973v1</guid></item><item><title>Food Classification using Joint Representation of Visual and Textual Data</title><link>http://arxiv.org/abs/2308.02562v2</link><description>Food classification is an important task in health care. In this work, wepropose a multimodal classification framework that uses the modified version ofEfficientNet with the Mish activation function for image classification, andthe traditional BERT transformer-based network is used for text classification.The proposed network and the other state-of-the-art methods are evaluated on alarge open-source dataset, UPMC Food-101. The experimental results show thatthe proposed network outperforms the other methods, a significant difference of11.57% and 6.34% in accuracy is observed for image and text classification,respectively, when compared with the second-best performing method. We alsocompared the performance in terms of accuracy, precision, and recall for textclassification using both machine learning and deep learning-based models. Thecomparative analysis from the prediction results of both images and textdemonstrated the efficiency and robustness of the proposed approach.</description><author>Prateek Mittal, Puneet Goyal, Joohi Chauhan</author><pubDate>Wed, 30 Aug 2023 12:47:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02562v2</guid></item><item><title>Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification</title><link>http://arxiv.org/abs/2308.15969v1</link><description>A well-defined reward function is crucial for successful training of anreinforcement learning (RL) agent. However, defining a suitable reward functionis a notoriously challenging task, especially in complex, multi-objectiveenvironments. Developers often have to resort to starting with an initial,potentially misspecified reward function, and iteratively adjusting itsparameters, based on observed learned behavior. In this work, we aim toautomate this process by proposing ITERS, an iterative reward shaping approachusing human feedback for mitigating the effects of a misspecified rewardfunction. Our approach allows the user to provide trajectory-level feedback onagent's behavior during training, which can be integrated as a reward shapingsignal in the following training iteration. We also allow the user to provideexplanations of their feedback, which are used to augment the feedback andreduce user effort and feedback frequency. We evaluate ITERS in threeenvironments and show that it can successfully correct misspecified rewardfunctions.</description><author>Jasmina Gajcin, James McCarthy, Rahul Nair, Radu Marinescu, Elizabeth Daly, Ivana Dusparic</author><pubDate>Wed, 30 Aug 2023 12:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15969v1</guid></item><item><title>SHARP Challenge 2023: Solving CAD History and pArameters Recovery from Point clouds and 3D scans. Overview, Datasets, Metrics, and Baselines</title><link>http://arxiv.org/abs/2308.15966v1</link><description>Recent breakthroughs in geometric Deep Learning (DL) and the availability oflarge Computer-Aided Design (CAD) datasets have advanced the research onlearning CAD modeling processes and relating them to real objects. In thiscontext, 3D reverse engineering of CAD models from 3D scans is considered to beone of the most sought-after goals for the CAD industry. However, recentefforts assume multiple simplifications limiting the applications in real-worldsettings. The SHARP Challenge 2023 aims at pushing the research a step closerto the real-world scenario of CAD reverse engineering through dedicateddatasets and tracks. In this paper, we define the proposed SHARP 2023 tracks,describe the provided datasets, and propose a set of baseline methods alongwith suitable evaluation metrics to assess the performance of the tracksolutions. All proposed datasets along with useful routines and the evaluationmetrics are publicly available.</description><author>Dimitrios Mallis, Sk Aziz Ali, Elona Dupont, Kseniya Cherenkova, Ahmet Serdar Karadeniz, Mohammad Sadil Khan, Anis Kacem, Gleb Gusev, Djamila Aouada</author><pubDate>Wed, 30 Aug 2023 12:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15966v1</guid></item><item><title>Review of Parameter Tuning Methods for Nature-Inspired Algorithms</title><link>http://arxiv.org/abs/2308.15965v1</link><description>Almost all optimization algorithms have algorithm-dependent parameters, andthe setting of such parameter values can largely influence the behaviour of thealgorithm under consideration. Thus, proper parameter tuning should be carriedout to ensure the algorithm used for optimization may perform well and can besufficiently robust for solving different types of optimization problems. Thischapter reviews some of the main methods for parameter tuning and thenhighlights the important issues concerning the latest development in parametertuning. A few open problems are also discussed with some recommendations forfuture research.</description><author>Geethu Joy, Christian Huyck, Xin-She Yang</author><pubDate>Wed, 30 Aug 2023 12:41:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15965v1</guid></item><item><title>Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting</title><link>http://arxiv.org/abs/2308.15961v1</link><description>The task of radiology reporting comprises describing and interpreting themedical findings in radiographic images, including description of theirlocation and appearance. Automated approaches to radiology reporting requirethe image to be encoded into a suitable token representation for input to thelanguage model. Previous methods commonly use convolutional neural networks toencode an image into a series of image-level feature map representations.However, the generated reports often exhibit realistic style but imperfectaccuracy. Inspired by recent works for image captioning in the general domainin which each visual token corresponds to an object detected in an image, weinvestigate whether using local tokens corresponding to anatomical structurescan improve the quality of the generated reports. We introduce a noveladaptation of Faster R-CNN in which finding detection is performed for thecandidate bounding boxes extracted during anatomical structure localisation. Weuse the resulting bounding box feature representations as our set offinding-aware anatomical tokens. This encourages the extracted anatomicaltokens to be informative about the findings they contain (required for thefinal task of radiology reporting). Evaluating on the MIMIC-CXR dataset ofchest X-Ray images, we show that task-aware anatomical tokens givestate-of-the-art performance when integrated into an automated reportingpipeline, yielding generated reports with improved clinical accuracy.</description><author>Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni, Jeffrey Dalton, Alison Q. O'Neil</author><pubDate>Wed, 30 Aug 2023 12:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15961v1</guid></item><item><title>WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model</title><link>http://arxiv.org/abs/2308.15962v1</link><description>Enabling robots to understand language instructions and react accordingly tovisual perception has been a long-standing goal in the robotics researchcommunity. Achieving this goal requires cutting-edge advances in naturallanguage processing, computer vision, and robotics engineering. Thus, thispaper mainly investigates the potential of integrating the most recent LargeLanguage Models (LLMs) and existing visual grounding and robotic graspingsystem to enhance the effectiveness of the human-robot interaction. Weintroduce the WALL-E (Embodied Robotic WAiter load lifting with Large Languagemodel) as an example of this integration. The system utilizes the LLM ofChatGPT to summarize the preference object of the users as a target instructionvia the multi-round interactive dialogue. The target instruction is thenforwarded to a visual grounding system for object pose and size estimation,following which the robot grasps the object accordingly. We deploy thisLLM-empowered system on the physical robot to provide a more user-friendlyinterface for the instruction-guided grasping task. The further experimentalresults on various real-world scenarios demonstrated the feasibility andefficacy of our proposed framework.</description><author>Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, Yanwei Fu</author><pubDate>Wed, 30 Aug 2023 12:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15962v1</guid></item><item><title>Fusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios</title><link>http://arxiv.org/abs/2308.15960v1</link><description>Advanced Driver Assistance Systems (ADAS) have made significant strides,capitalizing on computer vision to enhance perception and decision-makingcapabilities. Nonetheless, the adaptation of these systems to diverse trafficscenarios poses challenges due to shifts in data distribution stemming fromfactors such as location, weather, and road infrastructure. To tackle this, weintroduce a weakly-supervised label unification pipeline that amalgamatespseudo labels from a multitude of object detection models trained onheterogeneous datasets. Our pipeline engenders a unified label space throughthe amalgamation of labels from disparate datasets, rectifying bias andenhancing generalization. We fine-tune multiple object detection models onindividual datasets, subsequently crafting a unified dataset featuring pseudolabels, meticulously validated for precision. Following this, we retrain asolitary object detection model using the merged label space, culminating in aresilient model proficient in dynamic traffic scenarios. We put forth acomprehensive evaluation of our approach, employing diverse datasetsoriginating from varied Asian countries, effectively demonstrating its efficacyin challenging road conditions. Notably, our method yields substantialenhancements in object detection performance, culminating in a model withheightened resistance against domain shifts.</description><author>Harshith Mohan Kumar, Sean Lawrence</author><pubDate>Wed, 30 Aug 2023 12:33:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15960v1</guid></item></channel></rss>