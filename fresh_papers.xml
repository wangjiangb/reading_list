<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 10 Sep 2024 01:00:15 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Synergy and Synchrony in Couple Dances</title><link>http://arxiv.org/abs/2409.04440v1</link><description>This paper asks to what extent social interaction influences one's behavior.We study this in the setting of two dancers dancing as a couple. We firstconsider a baseline in which we predict a dancer's future moves conditionedonly on their past motion without regard to their partner. We then investigatethe advantage of taking social information into account by conditioning also onthe motion of their dancing partner. We focus our analysis on Swing, a dancegenre with tight physical coupling for which we present an in-the-wild videodataset. We demonstrate that single-person future motion prediction in thiscontext is challenging. Instead, we observe that prediction greatly benefitsfrom considering the interaction partners' behavior, resulting in surprisinglycompelling couple dance synthesis results (see supp. video). Our contributionsare a demonstration of the advantages of socially conditioned future motionprediction and an in-the-wild, couple dance video dataset to enable futureresearch in this direction. Video results are available on the project website:https://von31.github.io/synNsync</description><author>Vongani Maluleke, Lea Müller, Jathushan Rajasegaran, Georgios Pavlakos, Shiry Ginosar, Angjoo Kanazawa, Jitendra Malik</author><pubDate>Fri, 06 Sep 2024 17:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04440v1</guid></item><item><title>Modeling, Inference, and Prediction in Mobility-Based Compartmental Models for Epidemiology</title><link>http://arxiv.org/abs/2406.12002v2</link><description>Classical compartmental models in epidemiology often assume a homogeneouspopulation for simplicity, which neglects the inherent heterogeneity amongindividuals. This assumption frequently leads to inaccurate predictions whenapplied to real-world data. For example, evidence has shown that classicalmodels overestimate the final pandemic size in the H1N1-2009 and COVID-19outbreaks. To address this issue, we introduce individual mobility as a keyfactor in disease transmission and control. We characterize disease dynamicsusing mobility distribution functions for each compartment and propose amobility-based compartmental model that incorporates population heterogeneity.Our results demonstrate that, for the same basic reproduction number, ourmobility-based model predicts a smaller final pandemic size compared to theclassical models, effectively addressing the common overestimation problem.Additionally, we infer mobility distributions from the time series of theinfected population. We provide sufficient conditions for uniquely identifyingthe mobility distribution from a dataset and propose a machine-learning-basedapproach to learn mobility from both synthesized and real-world data.</description><author>Ning Jiang, Weiqi Chu, Yao Li</author><pubDate>Fri, 06 Sep 2024 17:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12002v2</guid></item><item><title>Re-evaluating Retrosynthesis Algorithms with Syntheseus</title><link>http://arxiv.org/abs/2310.19796v3</link><description>Automated Synthesis Planning has recently re-emerged as a research area atthe intersection of chemistry and machine learning. Despite the appearance ofsteady progress, we argue that imperfect benchmarks and inconsistentcomparisons mask systematic shortcomings of existing techniques, andunnecessarily hamper progress. To remedy this, we present a synthesis planninglibrary with an extensive benchmarking framework, called syntheseus, whichpromotes best practice by default, enabling consistent meaningful evaluation ofsingle-step models and multi-step planning algorithms. We demonstrate thecapabilities of syntheseus by re-evaluating several previous retrosynthesisalgorithms, and find that the ranking of state-of-the-art models changes incontrolled evaluation experiments. We end with guidance for future works inthis area, and call the community to engage in the discussion on how to improvebenchmarks for synthesis planning.</description><author>Krzysztof Maziarz, Austin Tripp, Guoqing Liu, Megan Stanley, Shufang Xie, Piotr Gaiński, Philipp Seidl, Marwin Segler</author><pubDate>Fri, 06 Sep 2024 17:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19796v3</guid></item><item><title>Accelerating Training with Neuron Interaction and Nowcasting Networks</title><link>http://arxiv.org/abs/2409.04434v1</link><description>Neural network training can be accelerated when a learnable update rule isused in lieu of classic adaptive optimizers (e.g. Adam). However, learnableupdate rules can be costly and unstable to train and use. A simpler recentlyproposed approach to accelerate training is to use Adam for most of theoptimization steps and periodically, only every few steps, nowcast (predictfuture) parameters. We improve this approach by Neuron interaction andNowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neuralnetworks to more accurately nowcast parameters by learning in a supervised wayfrom a set of training trajectories over multiple tasks. We show that in somenetworks, such as Transformers, neuron connectivity is non-trivial. Byaccurately modeling neuron connectivity, we allow NiNo to accelerate Adamtraining by up to 50\% in vision and language tasks.</description><author>Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon Lacoste-Julien</author><pubDate>Fri, 06 Sep 2024 17:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04434v1</guid></item><item><title>A Survey on Knowledge Organization Systems of Research Fields: Resources and Challenges</title><link>http://arxiv.org/abs/2409.04432v1</link><description>Knowledge Organization Systems (KOSs), such as term lists, thesauri,taxonomies, and ontologies, play a fundamental role in categorising, managing,and retrieving information. In the academic domain, KOSs are often adopted forrepresenting research areas and their relationships, primarily aiming toclassify research articles, academic courses, patents, books, scientificvenues, domain experts, grants, software, experiment materials, and severalother relevant products and agents. These structured representations ofresearch areas, widely embraced by many academic fields, have proven effectivein empowering AI-based systems to i) enhance retrievability of relevantdocuments, ii) enable advanced analytic solutions to quantify the impact ofacademic research, and iii) analyse and forecast research dynamics. This paperaims to present a comprehensive survey of the current KOS for academicdisciplines. We analysed and compared 45 KOSs according to five maindimensions: scope, structure, curation, usage, and links to other KOSs. Ourresults reveal a very heterogeneous scenario in terms of scope, scale, quality,and usage, highlighting the need for more integrated solutions for representingresearch knowledge across academic fields. We conclude by discussing the mainchallenges and the most promising future directions.</description><author>Angelo Salatino, Tanay Aggarwal, Andrea Mannocci, Francesco Osborne, Enrico Motta</author><pubDate>Fri, 06 Sep 2024 17:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04432v1</guid></item><item><title>Theory, Analysis, and Best Practices for Sigmoid Self-Attention</title><link>http://arxiv.org/abs/2409.04431v1</link><description>Attention is a key part of the transformer architecture. It is asequence-to-sequence mapping that transforms each sequence element into aweighted sum of values. The weights are typically obtained as the softmax ofdot products between keys and queries. Recent work has explored alternatives tosoftmax attention in transformers, such as ReLU and sigmoid activations. Inthis work, we revisit sigmoid attention and conduct an in-depth theoretical andempirical analysis. Theoretically, we prove that transformers with sigmoidattention are universal function approximators and benefit from improvedregularity compared to softmax attention. Through detailed empirical analysis,we identify stabilization of large initial attention norms during the earlystages of training as a crucial factor for the successful training of modelswith sigmoid attention, outperforming prior attempts. We also introduceFLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoidattention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100GPUs. Experiments across language, vision, and speech show that properlynormalized sigmoid attention matches the strong performance of softmaxattention on a wide range of domains and scales, which previous attempts atsigmoid attention were unable to fully achieve. Our work unifies prior art andestablishes best practices for sigmoid attention as a drop-in softmaxreplacement in transformers.</description><author>Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge, Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, Russ Webb</author><pubDate>Fri, 06 Sep 2024 17:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04431v1</guid></item><item><title>VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</title><link>http://arxiv.org/abs/2409.04429v1</link><description>VILA-U is a Unified foundation model that integrates Video, Image, Languageunderstanding and generation. Traditional visual language models (VLMs) useseparate modules for understanding and generating visual content, which canlead to misalignment and increased complexity. In contrast, VILA-U employs asingle autoregressive next-token prediction framework for both tasks,eliminating the need for additional components like diffusion models. Thisapproach not only simplifies the model but also achieves near state-of-the-artperformance in visual language understanding and generation. The success ofVILA-U is attributed to two main factors: the unified vision tower that alignsdiscrete visual tokens with textual inputs during pretraining, which enhancesvisual perception, and autoregressive image generation can achieve similarquality as diffusion models with high-quality dataset. This allows VILA-U toperform comparably to more complex models using a fully token-basedautoregressive framework.</description><author>Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu</author><pubDate>Fri, 06 Sep 2024 17:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04429v1</guid></item><item><title>Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces</title><link>http://arxiv.org/abs/2409.04428v1</link><description>Intra-cortical brain-machine interfaces (iBMIs) have the potential todramatically improve the lives of people with paraplegia by restoring theirability to perform daily activities. However, current iBMIs suffer fromscalability and mobility limitations due to bulky hardware and wiring. WirelessiBMIs offer a solution but are constrained by a limited data rate. To overcomethis challenge, we are investigating hybrid spiking neural networks forembedded neural decoding in wireless iBMIs. The networks consist of a temporalconvolution-based compression followed by recurrent processing and a finalinterpolation back to the original sequence length. As recurrent units, weexplore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons,and a combination of both - spiking GRUs (sGRUs) and analyze their differencesin terms of accuracy, footprint, and activation sparsity. To that end, we traindecoders on the "Nonhuman Primate Reaching with Multichannel SensorimotorCortex Electrophysiology" dataset and evaluate it using the NeuroBenchframework, targeting both tracks of the IEEE BioCAS Grand Challenge on NeuralDecoding. Our approach achieves high accuracy in predicting velocities ofprimate reaching movements from multichannel primary motor cortex recordingswhile maintaining a low number of synaptic operations, surpassing the currentbaseline models in the NeuroBench framework. This work highlights the potentialof hybrid neural networks to facilitate wireless iBMIs with high decodingprecision and a substantial increase in the number of monitored neurons, pavingthe way toward more advanced neuroprosthetic technologies.</description><author>Alexandru Vasilache, Jann Krausse, Klaus Knobloch, Juergen Becker</author><pubDate>Fri, 06 Sep 2024 17:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04428v1</guid></item><item><title>Exploring Foundation Models for Synthetic Medical Imaging: A Study on Chest X-Rays and Fine-Tuning Techniques</title><link>http://arxiv.org/abs/2409.04424v1</link><description>Machine learning has significantly advanced healthcare by aiding in diseaseprevention and treatment identification. However, accessing patient data can bechallenging due to privacy concerns and strict regulations. Generatingsynthetic, realistic data offers a potential solution for overcoming theselimitations, and recent studies suggest that fine-tuning foundation models canproduce such data effectively. In this study, we explore the potential offoundation models for generating realistic medical images, particularly chestx-rays, and assess how their performance improves with fine-tuning. We proposeusing a Latent Diffusion Model, starting with a pre-trained foundation modeland refining it through various configurations. Additionally, we performedexperiments with input from a medical professional to assess the realism of theimages produced by each trained model.</description><author>Davide Clode da Silva, Marina Musse Bernardes, Nathalia Giacomini Ceretta, Gabriel Vaz de Souza, Gabriel Fonseca Silva, Rafael Heitor Bordini, Soraia Raupp Musse</author><pubDate>Fri, 06 Sep 2024 17:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04424v1</guid></item><item><title>RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs</title><link>http://arxiv.org/abs/2409.04421v1</link><description>LLM-powered personalization agent systems employ Large Language Models (LLMs)to predict users' behavior from their past activities. However, theireffectiveness often hinges on the ability to effectively leverage extensive,long user historical data due to its inherent noise and length of such data.Existing pretrained LLMs may generate summaries that are concise but lack thenecessary context for downstream tasks, hindering their utility inpersonalization systems. To address these challenges, we introduceReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs togenerate concise, human-readable user summaries that are optimized fordownstream task performance. By maximizing the usefulness of the generatedsummaries, RLPF effectively distills extensive user history data whilepreserving essential information for downstream tasks. Our empirical evaluationdemonstrates significant improvements in both extrinsic downstream task utilityand intrinsic summary quality, surpassing baseline methods by up to 22% ondownstream task performance and achieving an up to 84.59% win rate onFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable74% reduction in context length while improving performance on 16 out of 19unseen tasks and/or datasets, showcasing its generalizability. This approachoffers a promising solution for enhancing LLM personalization by effectivelytransforming long, noisy user histories into informative and human-readablerepresentations.</description><author>Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie</author><pubDate>Fri, 06 Sep 2024 17:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04421v1</guid></item><item><title>Deep Limit Model-free Prediction in Regression</title><link>http://arxiv.org/abs/2408.09532v2</link><description>In this paper, we provide a novel Model-free approach based on Deep NeuralNetwork (DNN) to accomplish point prediction and prediction interval under ageneral regression setting. Usually, people rely on parametric ornon-parametric models to bridge dependent and independent variables (Y and X).However, this classical method relies heavily on the correct modelspecification. Even for the non-parametric approach, some additive form isoften assumed. A newly proposed Model-free prediction principle sheds light ona prediction procedure without any model assumption. Previous work regardingthis principle has shown better performance than other standard alternatives.Recently, DNN, one of the machine learning methods, has received increasingattention due to its great performance in practice. Guided by the Model-freeprediction idea, we attempt to apply a fully connected forward DNN to map X andsome appropriate reference random variable Z to Y. The targeted DNN is trainedby minimizing a specially designed loss function so that the randomness of Yconditional on X is outsourced to Z through the trained DNN. Our method is morestable and accurate compared to other DNN-based counterparts, especially foroptimal point predictions. With a specific prediction procedure, our predictioninterval can capture the estimation variability so that it can render a bettercoverage rate for finite sample cases. The superior performance of our methodis verified by simulation and empirical studies.</description><author>Kejin Wu, Dimitris N. Politis</author><pubDate>Fri, 06 Sep 2024 17:27:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09532v2</guid></item><item><title>TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design</title><link>http://arxiv.org/abs/2310.03223v6</link><description>Searching the vast chemical space for drug-like molecules that bind with aprotein pocket is a challenging task in drug discovery. Recently,structure-based generative models have been introduced which promise to be moreefficient by learning to generate molecules for any given protein structure.However, since they learn the distribution of a limited protein-ligand complexdataset, structure-based methods do not yet outperform optimization-basedmethods that generate binding molecules for just one pocket. To overcomelimitations on data while leveraging learning across protein targets, we chooseto model the reward distribution conditioned on pocket structure, instead ofthe training data distribution. We design TacoGFN, a novel GFlowNet-basedapproach for structure-based drug design, which can generate moleculesconditioned on any protein pocket structure with probabilities proportional toits affinity and property rewards. In the generative setting forCrossDocked2020 benchmark, TacoGFN attains a state-of-the-art success rate of$56.0\%$ and $-8.44$ kcal/mol in median Vina Dock score while improving thegeneration time by multiple orders of magnitude. Fine-tuning TacoGFN furtherimproves the median Vina Dock score to $-10.93$ kcal/mol and the success rateto $88.8\%$, outperforming all optimization-based methods.</description><author>Tony Shen, Seonghwan Seo, Grayson Lee, Mohit Pandey, Jason R Smith, Artem Cherkasov, Woo Youn Kim, Martin Ester</author><pubDate>Fri, 06 Sep 2024 17:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03223v6</guid></item><item><title>Improved Parallel Algorithm for Non-Monotone Submodular Maximization under Knapsack Constraint</title><link>http://arxiv.org/abs/2409.04415v1</link><description>This work proposes an efficient parallel algorithm for non-monotonesubmodular maximization under a knapsack constraint problem over the ground setof size $n$. Our algorithm improves the best approximation factor of theexisting parallel one from $8+\epsilon$ to $7+\epsilon$ with $O(\log n)$adaptive complexity. The key idea of our approach is to create a new alternate thresholdalgorithmic framework. This strategy alternately constructs two disjointcandidate solutions within a constant number of sequence rounds. Then, thealgorithm boosts solution quality without sacrificing the adaptive complexity.Extensive experimental studies on three applications, Revenue Maximization,Image Summarization, and Maximum Weighted Cut, show that our algorithm not onlysignificantly increases solution quality but also requires comparativeadaptivity to state-of-the-art algorithms.</description><author>Tan D. Tran, Canh V. Pham, Dung T. K. Ha, Phuong N. H. Pham</author><pubDate>Fri, 06 Sep 2024 17:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04415v1</guid></item><item><title>MSLIQA: Enhancing Learning Representations for Image Quality Assessment through Multi-Scale Learning</title><link>http://arxiv.org/abs/2408.16879v2</link><description>No-Reference Image Quality Assessment (NR-IQA) remains a challenging task dueto the diversity of distortions and the lack of large annotated datasets. Manystudies have attempted to tackle these challenges by developing more accurateNR-IQA models, often employing complex and computationally expensive networks,or by bridging the domain gap between various distortions to enhanceperformance on test datasets. In our work, we improve the performance of ageneric lightweight NR-IQA model by introducing a novel augmentation strategythat boosts its performance by almost 28\%. This augmentation strategy enablesthe network to better discriminate between different distortions in variousparts of the image by zooming in and out. Additionally, the inclusion oftest-time augmentation further enhances performance, making our lightweightnetwork's results comparable to the current state-of-the-art models, simplythrough the use of augmentations.</description><author>Nasim Jamshidi Avanaki, Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj</author><pubDate>Fri, 06 Sep 2024 17:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16879v2</guid></item><item><title>LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality Assessment Model</title><link>http://arxiv.org/abs/2408.17057v2</link><description>Recent advancements in the field of No-Reference Image Quality Assessment(NR-IQA) using deep learning techniques demonstrate high performance acrossmultiple open-source datasets. However, such models are typically very largeand complex making them not so suitable for real-world deployment, especiallyon resource- and battery-constrained mobile devices. To address thislimitation, we propose a compact, lightweight NR-IQA model that achievesstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validationand test datasets while being also nearly 5.7 times faster than the fastestSOTA model. Our model features a dual-branch architecture, with each branchseparately trained on synthetically and authentically distorted images whichenhances the model's generalizability across different distortion types. Toimprove robustness under diverse real-world visual conditions, we additionallyincorporate multiple color spaces during the training process. We alsodemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks(KANs) for final quality regression as compared to the conventional Multi-LayerPerceptrons (MLPs). Our evaluation considering various open-source datasetshighlights the practical, high-accuracy, and robust performance of our proposedlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.</description><author>Nasim Jamshidi Avanaki, Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj</author><pubDate>Fri, 06 Sep 2024 17:15:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.17057v2</guid></item><item><title>Approximating Metric Magnitude of Point Sets</title><link>http://arxiv.org/abs/2409.04411v1</link><description>Metric magnitude is a measure of the "size" of point clouds with manydesirable geometric properties. It has been adapted to various mathematicalcontexts and recent work suggests that it can enhance machine learning andoptimization algorithms. But its usability is limited due to the computationalcost when the dataset is large or when the computation must be carried outrepeatedly (e.g. in model training). In this paper, we study the magnitudecomputation problem, and show efficient ways of approximating it. We show thatit can be cast as a convex optimization problem, but not as a submodularoptimization. The paper describes two new algorithms - an iterativeapproximation algorithm that converges fast and is accurate, and a subsetselection method that makes the computation even faster. It has been previouslyproposed that magnitude of model sequences generated during stochastic gradientdescent is correlated to generalization gap. Extension of this result using ourmore scalable algorithms shows that longer sequences in fact bear highercorrelations. We also describe new applications of magnitude in machinelearning - as an effective regularizer for neural network training, and as anovel clustering criterion.</description><author>Rayna Andreeva, James Ward, Primoz Skraba, Jie Gao, Rik Sarkar</author><pubDate>Fri, 06 Sep 2024 17:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04411v1</guid></item><item><title>Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation</title><link>http://arxiv.org/abs/2409.04410v1</link><description>We present Open-MAGVIT2, a family of auto-regressive image generation modelsranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-sourcereplication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-largecodebook (i.e., $2^{18}$ codes), and achieves the state-of-the-artreconstruction performance (1.17 rFID) on ImageNet $256 \times 256$.Furthermore, we explore its application in plain auto-regressive models andvalidate scalability properties. To assist auto-regressive models in predictingwith a super-large vocabulary, we factorize it into two sub-vocabulary ofdifferent sizes by asymmetric token factorization, and further introduce "nextsub-token prediction" to enhance sub-token interaction for better generationquality. We release all models and codes to foster innovation and creativity inthe field of auto-regressive visual generation.</description><author>Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, Ying Shan</author><pubDate>Fri, 06 Sep 2024 17:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04410v1</guid></item><item><title>Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation</title><link>http://arxiv.org/abs/2409.04409v1</link><description>We tackle the challenging problem of source-free unsupervised domainadaptation (SFUDA) for 3D semantic segmentation. It amounts to performingdomain adaptation on an unlabeled target domain without any access to sourcedata; the available information is a model trained to achieve good performanceon the source domain. A common issue with existing SFUDA approaches is thatperformance degrades after some training time, which is a by product of anunder-constrained and ill-posed problem. We discuss two strategies to alleviatethis issue. First, we propose a sensible way to regularize the learningproblem. Second, we introduce a novel criterion based on agreement with areference model. It is used (1) to stop the training when appropriate and (2)as validator to select hyperparameters without any knowledge on the targetdomain. Our contributions are easy to implement and readily amenable for allSFUDA methods, ensuring stable improvements over all baselines. We validate ourfindings on various 3D lidar settings, achieving state-of-the-art performance.The project repository (with code) is: github.com/valeoai/TTYD.</description><author>Björn Michele, Alexandre Boulch, Tuan-Hung Vu, Gilles Puy, Renaud Marlet, Nicolas Courty</author><pubDate>Fri, 06 Sep 2024 17:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04409v1</guid></item><item><title>Exploiting the Data Gap: Utilizing Non-ignorable Missingness to Manipulate Model Learning</title><link>http://arxiv.org/abs/2409.04407v1</link><description>Missing data is commonly encountered in practice, and when the missingness isnon-ignorable, effective remediation depends on knowledge of the missingnessmechanism. Learning the underlying missingness mechanism from the data is notpossible in general, so adversaries can exploit this fact by maliciouslyengineering non-ignorable missingness mechanisms. Such Adversarial Missingness(AM) attacks have only recently been motivated and introduced, and thensuccessfully tailored to mislead causal structure learning algorithms intohiding specific cause-and-effect relationships. However, existing AM attacksassume the modeler (victim) uses full-information maximum likelihood methods tohandle the missing data, and are of limited applicability when the modeler usesdifferent remediation strategies. In this work we focus on associationallearning in the context of AM attacks. We consider (i) complete case analysis,(ii) mean imputation, and (iii) regression-based imputation as alternativestrategies used by the modeler. Instead of combinatorially searching formissing entries, we propose a novel probabilistic approximation by deriving theasymptotic forms of these methods used for handling the missing entries. Wethen formulate the learning of the adversarial missingness mechanism as abi-level optimization problem. Experiments on generalized linear models showthat AM attacks can be used to change the p-values of features from significantto insignificant in real datasets, such as the California-housing dataset,while using relatively moderate amounts of missingness (&lt;20%). Additionally, weassess the robustness of our attacks against defense strategies based on datavaluation.</description><author>Deniz Koyuncu, Alex Gittens, Bülent Yener, Moti Yung</author><pubDate>Fri, 06 Sep 2024 17:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04407v1</guid></item><item><title>Quantum Kernel Methods under Scrutiny: A Benchmarking Study</title><link>http://arxiv.org/abs/2409.04406v1</link><description>Since the entry of kernel theory in the field of quantum machine learning,quantum kernel methods (QKMs) have gained increasing attention with regard toboth probing promising applications and delivering intriguing researchinsights. Two common approaches for computing the underlying Gram matrix haveemerged: fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs).Benchmarking these methods is crucial to gain robust insights and to understandtheir practical utility. In this work, we present a comprehensive large-scalestudy examining QKMs based on FQKs and PQKs across a manifold of designchoices. Our investigation encompasses both classification and regression tasksfor five dataset families and 64 datasets, systematically comparing the use ofFQKs and PQKs quantum support vector machines and kernel ridge regression. Thisresulted in over 20,000 models that were trained and optimized using astate-of-the-art hyperparameter search to ensure robust and comprehensiveinsights. We delve into the importance of hyperparameters on model performancescores and support our findings through rigorous correlation analyses. In this,we also closely inspect two data encoding strategies. Moreover, we provide anin-depth analysis addressing the design freedom of PQKs and explore theunderlying principles responsible for learning. Our goal is not to identify thebest-performing model for a specific task but to uncover the mechanisms thatlead to effective QKMs and reveal universal patterns.</description><author>Jan Schnabel, Marco Roth</author><pubDate>Fri, 06 Sep 2024 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04406v1</guid></item><item><title>NUMOSIM: A Synthetic Mobility Dataset with Anomaly Detection Benchmarks</title><link>http://arxiv.org/abs/2409.03024v2</link><description>Collecting real-world mobility data is challenging. It is often fraught withprivacy concerns, logistical difficulties, and inherent biases. Moreover,accurately annotating anomalies in large-scale data is nearly impossible, as itdemands meticulous effort to distinguish subtle and complex patterns. Thesechallenges significantly impede progress in geospatial anomaly detectionresearch by restricting access to reliable data and complicating the rigorousevaluation, comparison, and benchmarking of methodologies. To address theselimitations, we introduce a synthetic mobility dataset, NUMOSIM, that providesa controlled, ethical, and diverse environment for benchmarking anomalydetection techniques. NUMOSIM simulates a wide array of realistic mobilityscenarios, encompassing both typical and anomalous behaviours, generatedthrough advanced deep learning models trained on real mobility data. Thisapproach allows NUMOSIM to accurately replicate the complexities of real-worldmovement patterns while strategically injecting anomalies to challenge andevaluate detection algorithms based on how effectively they capture theinterplay between demographic, geospatial, and temporal factors. Our goal is toadvance geospatial mobility analysis by offering a realistic benchmark forimproving anomaly detection and mobility modeling techniques. To support this,we provide open access to the NUMOSIM dataset, along with comprehensivedocumentation, evaluation metrics, and benchmark results.</description><author>Chris Stanford, Suman Adari, Xishun Liao, Yueshuai He, Qinhua Jiang, Chenchen Kuai, Jiaqi Ma, Emmanuel Tung, Yinlong Qian, Lingyi Zhao, Zihao Zhou, Zeeshan Rasheed, Khurram Shafique</author><pubDate>Fri, 06 Sep 2024 16:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03024v2</guid></item><item><title>HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts</title><link>http://arxiv.org/abs/2409.02919v2</link><description>The potential for higher-resolution image generation using pretraineddiffusion models is immense, yet these models often struggle with issues ofobject repetition and structural artifacts especially when scaling to 4Kresolution and higher. We figure out that the problem is caused by that, asingle prompt for the generation of multiple scales provides insufficientefficacy. In response, we propose HiPrompt, a new tuning-free solution thattackles the above problems by introducing hierarchical prompts. Thehierarchical prompts offer both global and local guidance. Specifically, theglobal guidance comes from the user input that describes the overall content,while the local guidance utilizes patch-wise descriptions from MLLMs toelaborately guide the regional structure and texture generation. Furthermore,during the inverse denoising process, the generated noise is decomposed intolow- and high-frequency spatial components. These components are conditioned onmultiple prompt levels, including detailed patch-wise descriptions and broaderimage-level prompts, facilitating prompt-guided denoising under hierarchicalsemantic guidance. It further allows the generation to focus more on localspatial regions and ensures the generated images maintain coherent local andglobal semantics, structures, and textures with high definition. Extensiveexperiments demonstrate that HiPrompt outperforms state-of-the-art works inhigher-resolution image generation, significantly reducing object repetitionand enhancing structural quality.</description><author>Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, Wenhan Luo, Qingfeng Liu, Yike Guo</author><pubDate>Fri, 06 Sep 2024 16:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02919v2</guid></item><item><title>HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LiDAR</title><link>http://arxiv.org/abs/2409.04398v1</link><description>We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capturemethod, aimed at accurately and efficiently creating a dynamic digital world,containing large-scale indoor-outdoor scenes, diverse human motions, richhuman-human interactions, and human-environment interactions. By utilizingbody-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric humanmotions in unconstrained space without the need for external devices andpre-built maps. This affords great flexibility and accessibility forhuman-centered interaction and 4D scene capturing in various environments.Taking into account that IMUs can capture human spatially unrestricted posesbut are prone to drifting for long-period using, and while LiDAR is stable forglobal localization but rough for local positions and orientations, HiSC4Demploys a joint optimization method, harmonizing all sensors and utilizingenvironment cues, yielding promising results for long-term capture in largescenes. To promote research of egocentric human interaction in large scenes andfacilitate downstream tasks, we also present a dataset, containing 8 sequencesin 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4Dhuman motions with SMPL annotations and dynamic scenes, 31k frames of croppedhuman point clouds, and scene mesh of the environment. A variety of scenarios,such as the basketball gym and commercial street, alongside challenging humanmotions, such as daily greeting, one-on-one basketball playing, and tourguiding, demonstrate the effectiveness and the generalization ability ofHiSC4D. The dataset and code will be publicated onwww.lidarhumanmotion.net/hisc4d available for research purposes.</description><author>Yudi Dai, Zhiyong Wang, Xiping Lin, Chenglu Wen, Lan Xu, Siqi Shen, Yuexin Ma, Cheng Wang</author><pubDate>Fri, 06 Sep 2024 16:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04398v1</guid></item><item><title>Future Does Matter: Boosting 3D Object Detection with Temporal Motion Estimation in Point Cloud Sequences</title><link>http://arxiv.org/abs/2409.04390v1</link><description>Accurate and robust LiDAR 3D object detection is essential for comprehensivescene understanding in autonomous driving. Despite its importance, LiDARdetection performance is limited by inherent constraints of point cloud data,particularly under conditions of extended distances and occlusions. Recently,temporal aggregation has been proven to significantly enhance detectionaccuracy by fusing multi-frame viewpoint information and enriching the spatialrepresentation of objects. In this work, we introduce a novel LiDAR 3D objectdetection framework, namely LiSTM, to facilitate spatial-temporal featurelearning with cross-frame motion forecasting information. We aim to improve thespatial-temporal interpretation capabilities of the LiDAR detector byincorporating a dynamic prior, generated from a non-learnable motion estimationmodel. Specifically, Motion-Guided Feature Aggregation (MGFA) is proposed toutilize the object trajectory from previous and future motion states to modelspatial-temporal correlations into gaussian heatmap over a driving sequence.This motion-based heatmap then guides the temporal feature fusion, enrichingthe proposed object features. Moreover, we design a Dual Correlation WeightingModule (DCWM) that effectively facilitates the interaction between past andprospective frames through scene- and channel-wise feature abstraction. In theend, a cascade cross-attention-based decoder is employed to refine the 3Dprediction. We have conducted experiments on the Waymo and nuScenes datasets todemonstrate that the proposed framework achieves superior 3D detectionperformance with effective spatial-temporal feature learning.</description><author>Rui Yu, Runkai Zhao, Cong Nie, Heng Wang, HuaiCheng Yan, Meng Wang</author><pubDate>Fri, 06 Sep 2024 16:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04390v1</guid></item><item><title>Question-Answering Dense Video Events</title><link>http://arxiv.org/abs/2409.04388v1</link><description>Multimodal Large Language Models (MLLMs) have shown excellent performance inquestion-answering of single-event videos. In this paper, we presentquestion-answering dense video events, a novel task that requires answering andgrounding the dense-event questions in long videos, thus challenging MLLMs tofaithfully comprehend and reason about multiple events occurring over extendedtime periods. To facilitate the study, we construct DeVE-QA - a datasetfeaturing 78K questions about 26K events on 10.6K long videos. We thenbenchmark and show that existing MLLMs excelling at single-event QA struggle toperform well in DeVE-QA. For improvement, we propose DeVi, a noveltraining-free MLLM approach that highlights a hierarchical captioning module, atemporal event memory module, and a self-consistency checking module torespectively detect, contextualize and memorize, and ground dense-events inlong videos for question answering. Extensive experiments show that DeVi issuperior at answering dense-event questions and grounding relevant videomoments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQArespectively.</description><author>Hangyu Qin, Junbin Xiao, Angela Yao</author><pubDate>Fri, 06 Sep 2024 16:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04388v1</guid></item><item><title>Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning</title><link>http://arxiv.org/abs/2402.15761v3</link><description>Food classification is the foundation for developing food vision tasks andplays a key role in the burgeoning field of computational nutrition. Due to thecomplexity of food requiring fine-grained classification, recent academicresearch mainly modifies Convolutional Neural Networks (CNNs) and/or VisionTransformers (ViTs) to perform food category classification. However, to learnfine-grained features, the CNN backbone needs additional structural design,whereas ViT, containing the self-attention module, has increased computationalcomplexity. In recent months, a new Sequence State Space (S4) model, through aSelection mechanism and computation with a Scan (S6), colloquially termedMamba, has demonstrated superior performance and computation efficiencycompared to the Transformer architecture. The VMamba model, which incorporatesthe Mamba mechanism into image tasks (such as classification), currentlyestablishes the state-of-the-art (SOTA) on the ImageNet dataset. In thisresearch, we introduce an academically underestimated food dataset CNFOOD-241,and pioneer the integration of a residual learning framework within the VMambamodel to concurrently harness both global and local state features inherent inthe original VMamba architectural design. The research results show that VMambasurpasses current SOTA models in fine-grained and food classification. Theproposed Res-VMamba further improves the classification accuracy to 79.54\%without pretrained weight. Our findings elucidate that our proposed methodologyestablishes a new benchmark for SOTA performance in food recognition on theCNFOOD-241 dataset. The code can be obtained on GitHub:https://github.com/ChiShengChen/ResVMamba.</description><author>Chi-Sheng Chen, Guan-Ying Chen, Dong Zhou, Di Jiang, Dai-Shi Chen</author><pubDate>Fri, 06 Sep 2024 16:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15761v3</guid></item><item><title>3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from Aerial Images</title><link>http://arxiv.org/abs/2409.00381v2</link><description>Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention.However, the unstructured nature of 3DGS poses challenges for large-scalesurface reconstruction from aerial images. To address this gap, we propose thefirst large-scale surface reconstruction method for multi-view stereo (MVS)aerial images based on 3DGS, named Aerial Gaussian Splatting (AGS). Initially,we introduce a data chunking method tailored for large-scale aerial imagery,making the modern 3DGS technology feasible for surface reconstruction overextensive scenes. Additionally, we integrate the Ray-Gaussian Intersectionmethod to obtain normal and depth information, facilitating geometricconstraints. Finally, we introduce a multi-view geometric consistencyconstraint to enhance global geometric consistency and improve reconstructionaccuracy. Our experiments on multiple datasets demonstrate for the first timethat the GS-based technique can match traditional aerial MVS methods ongeometric accuracy, and beat state-of-the-art GS-based methods on geometry andrendering quality.</description><author>YuanZheng Wu, Jin Liu, Shunping Ji</author><pubDate>Fri, 06 Sep 2024 16:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00381v2</guid></item><item><title>Empirical Bayesian image restoration by Langevin sampling with a denoising diffusion implicit prior</title><link>http://arxiv.org/abs/2409.04384v1</link><description>Score-based diffusion methods provide a powerful strategy to solve imagerestoration tasks by flexibly combining a pre-trained foundational prior modelwith a likelihood function specified during test time. Such methods arepredominantly derived from two stochastic processes: reversingOrnstein-Uhlenbeck, which underpins the celebrated denoising diffusionprobabilistic models (DDPM) and denoising diffusion implicit models (DDIM), andthe Langevin diffusion process. The solutions delivered by DDPM and DDIM areoften remarkably realistic, but they are not always consistent withmeasurements because of likelihood intractability issues and the associatedrequired approximations. Alternatively, using a Langevin process circumventsthe intractable likelihood issue, but usually leads to restoration results ofinferior quality and longer computing times. This paper presents a novel andhighly computationally efficient image restoration method that carefully embedsa foundational DDPM denoiser within an empirical Bayesian Langevin algorithm,which jointly calibrates key model hyper-parameters as it estimates the model'sposterior mean. Extensive experimental results on three canonical tasks (imagedeblurring, super-resolution, and inpainting) demonstrate that the proposedapproach improves on state-of-the-art strategies both in image estimationaccuracy and computing time.</description><author>Charlesquin Kemajou Mbakam, Jean-Francois Giovannelli, Marcelo Pereyra</author><pubDate>Fri, 06 Sep 2024 16:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04384v1</guid></item><item><title>Enhancing Skin Lesion Diagnosis with Ensemble Learning</title><link>http://arxiv.org/abs/2409.04381v1</link><description>Skin lesions are an increasingly significant medical concern, varying widelyin severity from benign to cancerous. Accurate diagnosis is essential forensuring timely and appropriate treatment. This study examines theimplementation of deep learning methods to assist in the diagnosis of skinlesions using the HAM10000 dataset, which contains seven distinct types oflesions. First, we evaluated three pre-trained models: MobileNetV2, ResNet18,and VGG11, achieving accuracies of 0.798, 0.802, and 0.805, respectively. Tofurther enhance classification accuracy, we developed ensemble models employingmax voting, average voting, and stacking, resulting in accuracies of 0.803,0.82, and 0.83. Building on the best-performing ensemble learning model,stacking, we developed our proposed model, SkinNet, which incorporates acustomized architecture and fine-tuning, achieving an accuracy of 0.867 and anAUC of 0.96. This substantial improvement over individual models demonstratesthe effectiveness of ensemble learning in improving skin lesion classification.</description><author>Xiaoyi Liu, Zhou Yu, Lianghao Tan, Yafeng Yan, Ge Shi</author><pubDate>Fri, 06 Sep 2024 16:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04381v1</guid></item><item><title>Gaussian-Mixture-Model Q-Functions for Reinforcement Learning by Riemannian Optimization</title><link>http://arxiv.org/abs/2409.04374v1</link><description>This paper establishes a novel role for Gaussian-mixture models (GMMs) asfunctional approximators of Q-function losses in reinforcement learning (RL).Unlike the existing RL literature, where GMMs play their typical role asestimates of probability density functions, GMMs approximate here Q-functionlosses. The new Q-function approximators, coined GMM-QFs, are incorporated inBellman residuals to promote a Riemannian-optimization task as a novelpolicy-evaluation step in standard policy-iteration schemes. The paperdemonstrates how the hyperparameters (means and covariance matrices) of theGaussian kernels are learned from the data, opening thus the door of RL to thepowerful toolbox of Riemannian optimization. Numerical tests show that with nouse of training data, the proposed design outperforms state-of-the-art methods,even deep Q-networks which use training data, on benchmark RL tasks.</description><author>Minh Vu, Konstantinos Slavakis</author><pubDate>Fri, 06 Sep 2024 16:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04374v1</guid></item><item><title>Are LLM-based methods good enough for detecting unfair terms of service?</title><link>http://arxiv.org/abs/2409.00077v2</link><description>Countless terms of service (ToS) are being signed everyday by users all overthe world while interacting with all kinds of apps and websites. More oftenthan not, these online contracts spanning double-digit pages are signed blindlyby users who simply want immediate access to the desired service. What wouldnormally require a consultation with a legal team, has now become a mundaneactivity consisting of a few clicks where users potentially sign away theirrights, for instance in terms of their data privacy, to countless onlineentities/companies. Large language models (LLMs) are good at parsing longtext-based documents, and could potentially be adopted to help users whendealing with dubious clauses in ToS and their underlying privacy policies. Toinvestigate the utility of existing models for this task, we first build adataset consisting of 12 questions applied individually to a set of privacypolicies crawled from popular websites. Thereafter, a series of open-source aswell as commercial chatbots such as ChatGPT, are queried over each question,with the answers being compared to a given ground truth. Our results show thatsome open-source models are able to provide a higher accuracy compared to somecommercial models. However, the best performance is recorded from a commercialchatbot (ChatGPT4). Overall, all models perform only slightly better thanrandom at this task. Consequently, their performance needs to be significantlyimproved before they can be adopted at large for this purpose.</description><author>Mirgita Frasheri, Arian Bakhtiarnia, Lukas Esterle, Alexandros Iosifidis</author><pubDate>Fri, 06 Sep 2024 16:12:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00077v2</guid></item><item><title>Evaluating Fairness in Transaction Fraud Models: Fairness Metrics, Bias Audits, and Challenges</title><link>http://arxiv.org/abs/2409.04373v1</link><description>Ensuring fairness in transaction fraud detection models is vital due to thepotential harms and legal implications of biased decision-making. Despiteextensive research on algorithmic fairness, there is a notable gap in the studyof bias in fraud detection models, mainly due to the field's unique challenges.These challenges include the need for fairness metrics that account for frauddata's imbalanced nature and the tradeoff between fraud protection and servicequality. To address this gap, we present a comprehensive fairness evaluation oftransaction fraud models using public synthetic datasets, marking the firstalgorithmic bias audit in this domain. Our findings reveal three criticalinsights: (1) Certain fairness metrics expose significant bias only afternormalization, highlighting the impact of class imbalance. (2) Bias issignificant in both service quality-related parity metrics and fraudprotection-related parity metrics. (3) The fairness through unawarenessapproach, which involved removing sensitive attributes such as gender, does notimprove bias mitigation within these datasets, likely due to the presence ofcorrelated proxies. We also discuss socio-technical fairness-related challengesin transaction fraud models. These insights underscore the need for a nuancedapproach to fairness in fraud detection, balancing protection and servicequality, and moving beyond simple bias mitigation strategies. Future work mustfocus on refining fairness metrics and developing methods tailored to theunique complexities of the transaction fraud domain.</description><author>Parameswaran Kamalaruban, Yulu Pi, Stuart Burrell, Eleanor Drage, Piotr Skalski, Jason Wong, David Sutton</author><pubDate>Fri, 06 Sep 2024 16:08:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04373v1</guid></item><item><title>The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study</title><link>http://arxiv.org/abs/2409.04368v1</link><description>Purpose: Medical images acquired using different scanners and protocols candiffer substantially in their appearance. This phenomenon, scanner domainshift, can result in a drop in the performance of deep neural networks whichare trained on data acquired by one scanner and tested on another. Thissignificant practical issue is well-acknowledged, however, no systematic studyof the issue is available across different modalities and diagnostic tasks.Materials and Methods: In this paper, we present a broad experimental studyevaluating the impact of scanner domain shift on convolutional neural networkperformance for different automated diagnostic tasks. We evaluate thisphenomenon in common radiological modalities, including X-ray, CT, and MRI.Results: We find that network performance on data from a different scanner isalmost always worse than on same-scanner data, and we quantify the degree ofperformance drop across different datasets. Notably, we find that this drop ismost severe for MRI, moderate for X-ray, and quite small for CT, on average,which we attribute to the standardized nature of CT acquisition systems whichis not present in MRI or X-ray. We also study how injecting varying amounts oftarget domain data into the training set, as well as adding noise to thetraining data, helps with generalization. Conclusion: Our results provideextensive experimental evidence and quantification of the extent of performancedrop caused by scanner domain shift in deep learning across differentmodalities, with the goal of guiding the future development of robust deeplearning models for medical image analysis.</description><author>Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski</author><pubDate>Fri, 06 Sep 2024 15:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04368v1</guid></item><item><title>Provable Hyperparameter Tuning for Structured Pfaffian Settings</title><link>http://arxiv.org/abs/2409.04367v1</link><description>Data-driven algorithm design automatically adapts algorithms to specificapplication domains, achieving better performance. In the context ofparameterized algorithms, this approach involves tuning the algorithmparameters using problem instances drawn from the problem distribution of thetarget application domain. While empirical evidence supports the effectivenessof data-driven algorithm design, providing theoretical guarantees for severalparameterized families remains challenging. This is due to the intricatebehaviors of their corresponding utility functions, which typically admitpiece-wise and discontinuity structures. In this work, we present refinedframeworks for providing learning guarantees for parameterized data-drivenalgorithm design problems in both distributional and online learning settings.For the distributional learning setting, we introduce the Pfaffian GJframework, an extension of the classical GJ framework, capable of providinglearning guarantees for function classes for which the computation involvesPfaffian functions. Unlike the GJ framework, which is limited to functionclasses with computation characterized by rational functions, our proposedframework can deal with function classes involving Pfaffian functions, whichare much more general and widely applicable. We then show that for manyparameterized algorithms of interest, their utility function possesses arefined piece-wise structure, which automatically translates to learningguarantees using our proposed framework. For the online learning setting, weprovide a new tool for verifying dispersion property of a sequence of lossfunctions. This sufficient condition allows no-regret learning for sequences ofpiece-wise structured loss functions where the piece-wise structure involvesPfaffian transition boundaries.</description><author>Maria-Florina Balcan, Anh Tuan Nguyen, Dravyansh Sharma</author><pubDate>Fri, 06 Sep 2024 15:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04367v1</guid></item><item><title>Leveraging Machine Learning for Official Statistics: A Statistical Manifesto</title><link>http://arxiv.org/abs/2409.04365v1</link><description>It is important for official statistics production to apply ML withstatistical rigor, as it presents both opportunities and challenges. Althoughmachine learning has enjoyed rapid technological advances in recent years, itsapplication does not possess the methodological robustness necessary to producehigh quality statistical results. In order to account for all sources of errorin machine learning models, the Total Machine Learning Error (TMLE) ispresented as a framework analogous to the Total Survey Error Model used insurvey methodology. As a means of ensuring that ML models are both internallyvalid as well as externally valid, the TMLE model addresses issues such asrepresentativeness and measurement errors. There are several case studiespresented, illustrating the importance of applying more rigor to theapplication of machine learning in official statistics.</description><author>Marco Puts, David Salgado, Piet Daas</author><pubDate>Fri, 06 Sep 2024 15:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04365v1</guid></item><item><title>Training-Free Condition Video Diffusion Models for single frame Spatial-Semantic Echocardiogram Synthesis</title><link>http://arxiv.org/abs/2408.03035v2</link><description>Conditional video diffusion models (CDM) have shown promising results forvideo synthesis, potentially enabling the generation of realisticechocardiograms to address the problem of data scarcity. However, current CDMsrequire a paired segmentation map and echocardiogram dataset. We present a newmethod called Free-Echo for generating realistic echocardiograms from a singleend-diastolic segmentation map without additional training data. Our method isbased on the 3D-Unet with Temporal Attention Layers model and is conditioned onthe segmentation map using a training-free conditioning method based on SDEdit.We evaluate our model on two public echocardiogram datasets, CAMUS andEchoNet-Dynamic. We show that our model can generate plausible echocardiogramsthat are spatially aligned with the input segmentation map, achievingperformance comparable to training-based CDMs. Our work opens up newpossibilities for generating echocardiograms from a single segmentation map,which can be used for data augmentation, domain adaptation, and otherapplications in medical imaging. Our code is available at\url{https://github.com/gungui98/echo-free}</description><author>Van Phi Nguyen, Tri Nhan Luong Ha, Huy Hieu Pham, Quoc Long Tran</author><pubDate>Fri, 06 Sep 2024 15:52:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03035v2</guid></item><item><title>RCNet: Deep Recurrent Collaborative Network for Multi-View Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2409.04363v1</link><description>Scene observation from multiple perspectives would bring a more comprehensivevisual experience. However, in the context of acquiring multiple views in thedark, the highly correlated views are seriously alienated, making itchallenging to improve scene understanding with auxiliary views. Recent singleimage-based enhancement methods may not be able to provide consistentlydesirable restoration performance for all views due to the ignorance ofpotential feature correspondence among different views. To alleviate thisissue, we make the first attempt to investigate multi-view low-light imageenhancement. First, we construct a new dataset called Multi-View Low-lightTriplets (MVLT), including 1,860 pairs of triple images with large illuminationranges and wide noise distribution. Each triplet is equipped with threedifferent viewpoints towards the same scene. Second, we propose a deepmulti-view enhancement framework based on the Recurrent Collaborative Network(RCNet). Specifically, in order to benefit from similar texture correspondenceacross different views, we design the recurrent feature enhancement, alignmentand fusion (ReEAF) module, in which intra-view feature enhancement (Intra-viewEN) followed by inter-view feature alignment and fusion (Inter-view AF) isperformed to model the intra-view and inter-view feature propagationsequentially via multi-view collaboration. In addition, two different modulesfrom enhancement to alignment (E2A) and from alignment to enhancement (A2E) aredeveloped to enable the interactions between Intra-view EN and Inter-view AF,which explicitly utilize attentive feature weighting and sampling forenhancement and alignment, respectively. Experimental results demonstrate thatour RCNet significantly outperforms other state-of-the-art methods. All of ourdataset, code, and model will be available at https://github.com/hluo29/RCNet.</description><author>Hao Luo, Baoliang Chen, Lingyu Zhu, Peilin Chen, Shiqi Wang</author><pubDate>Fri, 06 Sep 2024 15:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04363v1</guid></item><item><title>Connectivity-Inspired Network for Context-Aware Recognition</title><link>http://arxiv.org/abs/2409.04360v1</link><description>The aim of this paper is threefold. We inform the AI practitioner about thehuman visual system with an extensive literature review; we propose a novelbiologically motivated neural network for image classification; and, finally,we present a new plug-and-play module to model context awareness. We focus onthe effect of incorporating circuit motifs found in biological brains toaddress visual recognition. Our convolutional architecture is inspired by theconnectivity of human cortical and subcortical streams, and we implementbottom-up and top-down modulations that mimic the extensive afferent andefferent connections between visual and cognitive areas. Our ContextualAttention Block is simple and effective and can be integrated with anyfeed-forward neural network. It infers weights that multiply the feature mapsaccording to their causal influence on the scene, modeling the co-occurrence ofdifferent objects in the image. We place our module at different bottlenecks toinfuse a hierarchical context awareness into the model. We validated ourproposals through image classification experiments on benchmark data and founda consistent improvement in performance and the robustness of the producedexplanations via class activation. Our code is available athttps://github.com/gianlucarloni/CoCoReco.</description><author>Gianluca Carloni, Sara Colantonio</author><pubDate>Fri, 06 Sep 2024 15:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04360v1</guid></item><item><title>Serp-Mamba: Advancing High-Resolution Retinal Vessel Segmentation with Selective State-Space Model</title><link>http://arxiv.org/abs/2409.04356v1</link><description>Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) images capturehigh-resolution views of the retina with typically 200 spanning degrees.Accurate segmentation of vessels in UWF-SLO images is essential for detectingand diagnosing fundus disease. Recent studies have revealed that the selectiveState Space Model (SSM) in Mamba performs well in modeling long-rangedependencies, which is crucial for capturing the continuity of elongated vesselstructures. Inspired by this, we propose the first Serpentine Mamba(Serp-Mamba) network to address this challenging task. Specifically, werecognize the intricate, varied, and delicate nature of the tubular structureof vessels. Furthermore, the high-resolution of UWF-SLO images exacerbates theimbalance between the vessel and background categories. Based on the aboveobservations, we first devise a Serpentine Interwoven Adaptive (SIA) scanmechanism, which scans UWF-SLO images along curved vessel structures in asnake-like crawling manner. This approach, consistent with vascular texturetransformations, ensures the effective and continuous capture of curvedvascular structure features. Second, we propose an Ambiguity-Driven DualRecalibration (ADDR) module to address the category imbalance problemintensified by high-resolution images. Our ADDR module delineates pixels by twolearnable thresholds and refines ambiguous pixels through a dual-drivenstrategy, thereby accurately distinguishing vessels and background regions.Experiment results on three datasets demonstrate the superior performance ofour Serp-Mamba on high-resolution vessel segmentation. We also conduct a seriesof ablation studies to verify the impact of our designs. Our code shall bereleased upon publication of this work.</description><author>Hongqiu Wang, Yixian Chen, Wu Chen, Huihui Xu, Haoyu Zhao, Bin Sheng, Huazhu Fu, Guang Yang, Lei Zhu</author><pubDate>Fri, 06 Sep 2024 15:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04356v1</guid></item><item><title>A naive aggregation algorithm for improving generalization in a class of learning problems</title><link>http://arxiv.org/abs/2409.04352v1</link><description>In this brief paper, we present a naive aggregation algorithm for a typicallearning problem with expert advice setting, in which the task of improvinggeneralization, i.e., model validation, is embedded in the learning process asa sequential decision-making problem. In particular, we consider a class oflearning problem of point estimations for modeling high-dimensional nonlinearfunctions, where a group of experts update their parameter estimates using thediscrete-time version of gradient systems, with small additive noise term,guided by the corresponding subsample datasets obtained from the originaldataset. Here, our main objective is to provide conditions under which such analgorithm will sequentially determine a set of mixing distribution strategiesused for aggregating the experts' estimates that ultimately leading to anoptimal parameter estimate, i.e., as a consensus solution for all experts,which is better than any individual expert's estimate in terms of improvedgeneralization or learning performances. Finally, as part of this work, wepresent some numerical results for a typical case of nonlinear regressionproblem.</description><author>Getachew K Befekadu</author><pubDate>Fri, 06 Sep 2024 15:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04352v1</guid></item><item><title>Universal randomised signatures for generative time series modelling</title><link>http://arxiv.org/abs/2406.10214v2</link><description>Randomised signature has been proposed as a flexible and easily implementablealternative to the well-established path signature. In this article, we employrandomised signature to introduce a generative model for financial time seriesdata in the spirit of reservoir computing. Specifically, we propose a novelWasserstein-type distance based on discrete-time randomised signatures. Thismetric on the space of probability measures captures the distance between(conditional) distributions. Its use is justified by our novel universalapproximation results for randomised signatures on the space of continuousfunctions taking the underlying path as an input. We then use our metric as theloss function in a non-adversarial generator model for synthetic time seriesdata based on a reservoir neural stochastic differential equation. We comparethe results of our model to benchmarks from the existing literature.</description><author>Francesca Biagini, Lukas Gonon, Niklas Walter</author><pubDate>Fri, 06 Sep 2024 15:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10214v2</guid></item><item><title>Computer-Generated Sand Mixtures and Sand-based Images</title><link>http://arxiv.org/abs/2409.04345v1</link><description>This paper aims to verify the effectiveness of the software implementation ofthe proposed algorithm in creating computer-generated images of sand mixturesusing a photograph of sand as an input and its effectiveness in convertingdigital pictures into sand-based images out of the mixtures it generated. Themethod of this paper is to visually compare the photographed image of theactual mixtures to its computer-generated counterpart to verify if the mixturegeneration produces results as expected and compare the computer-generatedsand-based images with its source to verify image reproduction maintains sameimage content. The results of the mixture comparison shows that the actual andthe computer-generated ones have similar overall shade and color. Still, thegenerated one has a rougher texture and higher contrast due to the method ofinheriting visual features by pixel, not by individual sand particles. Thecomparison of the sand-based image and its source has demonstrated thesoftware's ability to maintain the essence of its contents during conversionwhile replacing its texture with the visual properties of the generated sandmixture. The result have shown that the software implementation of the proposedalgorithm can effectively use the images of sand to generate images of itsmixtures and use those mixture images to convert a digital picture into acomputer-generated sand-based image.</description><author>Ryan A. Subong, Alma Jean D. Subong</author><pubDate>Fri, 06 Sep 2024 15:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04345v1</guid></item><item><title>Towards Fine-Grained Webpage Fingerprinting at Scale</title><link>http://arxiv.org/abs/2409.04341v1</link><description>Website Fingerprinting (WF) attacks can effectively identify the websitesvisited by Tor clients via analyzing encrypted traffic patterns. Existingattacks focus on identifying different websites, but their accuracydramatically decreases when applied to identify fine-grained webpages,especially when distinguishing among different subpages of the same website.WebPage Fingerprinting (WPF) attacks face the challenges of highly similartraffic patterns and a much larger scale of webpages. Furthermore, clientsoften visit multiple webpages concurrently, increasing the difficulty ofextracting the traffic patterns of each webpage from the obfuscated traffic. Inthis paper, we propose Oscar, a WPF attack based on multi-label metric learningthat identifies different webpages from obfuscated traffic by transforming thefeature space. Oscar can extract the subtle differences among various webpages,even those with similar traffic patterns. In particular, Oscar combinesproxy-based and sample-based metric learning losses to extract webpage featuresfrom obfuscated traffic and identify multiple webpages. We prototype Oscar andevaluate its performance using traffic collected from 1,000 monitored webpagesand over 9,000 unmonitored webpages in the real world. Oscar demonstrates an88.6% improvement in the multi-label metric Recall@5 compared to thestate-of-the-art attacks.</description><author>Xiyuan Zhao, Xinhao Deng, Qi Li, Yunpeng Liu, Zhuotao Liu, Kun Sun, Ke Xu</author><pubDate>Fri, 06 Sep 2024 15:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04341v1</guid></item><item><title>AGR: Age Group fairness Reward for Bias Mitigation in LLMs</title><link>http://arxiv.org/abs/2409.04340v1</link><description>LLMs can exhibit age biases, resulting in unequal treatment of individualsacross age groups. While much research has addressed racial and gender biases,age bias remains little explored. The scarcity of instruction-tuning andpreference datasets for age bias hampers its detection and measurement, andexisting fine-tuning methods seldom address age-related fairness. In thispaper, we construct age bias preference datasets and instruction-tuningdatasets for RLHF. We introduce ARG, an age fairness reward to reducedifferences in the response quality of LLMs across different age groups.Extensive experiments demonstrate that this reward significantly improvesresponse accuracy and reduces performance disparities across age groups. Oursource code and datasets are available at the anonymous\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.</description><author>Shuirong Cao, Ruoxi Cheng, Zhiqiang Wang</author><pubDate>Fri, 06 Sep 2024 15:18:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04340v1</guid></item><item><title>Many-Worlds Inverse Rendering</title><link>http://arxiv.org/abs/2408.16005v3</link><description>Discontinuous visibility changes remain a major bottleneck when optimizingsurfaces within a physically-based inverse renderer. Many previous works haveproposed sophisticated algorithms and data structures to sample visibilitysilhouettes more efficiently. Our work presents another solution: instead of differentiating a tentativesurface locally, we differentiate a volumetric perturbation of a surface. Werefer this as a many-worlds representation because it models a non-interactingsuperposition of conflicting explanations (worlds) of the input dataset. Eachworld is optically isolated from others, leading to a new transport law thatdistinguishes our method from prior work based on exponential random media. The resulting Monte Carlo algorithm is simpler and more efficient than priormethods. We demonstrate that our method promotes rapid convergence, both interms of the total iteration count and the cost per iteration.</description><author>Ziyi Zhang, Nicolas Roussel, Wenzel Jakob</author><pubDate>Fri, 06 Sep 2024 15:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16005v3</guid></item><item><title>A high-accuracy multi-model mixing retrosynthetic method</title><link>http://arxiv.org/abs/2409.04335v1</link><description>The field of computer-aided synthesis planning (CASP) has seen rapidadvancements in recent years, achieving significant progress across variousalgorithmic benchmarks. However, chemists often encounter numerous infeasiblereactions when using CASP in practice. This article delves into common errorsassociated with CASP and introduces a product prediction model aimed atenhancing the accuracy of single-step models. While the product predictionmodel reduces the number of single-step reactions, it integrates multiplesingle-step models to maintain the overall reaction count and increase reactiondiversity. Based on manual analysis and large-scale testing, the productprediction model, combined with the multi-model ensemble approach, has beenproven to offer higher feasibility and greater diversity.</description><author>Shang Xiang, Lin Yao, Zhen Wang, Qifan Yu, Wentan Liu, Wentao Guo, Guolin Ke</author><pubDate>Fri, 06 Sep 2024 15:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04335v1</guid></item><item><title>Open-Vocabulary Object Detectors: Robustness Challenges under Distribution Shifts</title><link>http://arxiv.org/abs/2405.14874v4</link><description>The challenge of Out-Of-Distribution (OOD) robustness remains a criticalhurdle towards deploying deep vision models. Vision-Language Models (VLMs) haverecently achieved groundbreaking results. VLM-based open-vocabulary objectdetection extends the capabilities of traditional object detection frameworks,enabling the recognition and classification of objects beyond predefinedcategories. Investigating OOD robustness in recent open-vocabulary objectdetection is essential to increase the trustworthiness of these models. Thisstudy presents a comprehensive robustness evaluation of the zero-shotcapabilities of three recent open-vocabulary (OV) foundation object detectionmodels: OWL-ViT, YOLO World, and Grounding DINO. Experiments carried out on therobustness benchmarks COCO-O, COCO-DC, and COCO-C encompassing distributionshifts due to information loss, corruption, adversarial attacks, andgeometrical deformation, highlighting the challenges of the model's robustnessto foster the research for achieving robustness. Project page:https://prakashchhipa.github.io/projects/ovod_robustness</description><author>Prakash Chandra Chhipa, Kanjar De, Meenakshi Subhash Chippa, Rajkumar Saini, Marcus Liwicki</author><pubDate>Fri, 06 Sep 2024 15:11:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14874v4</guid></item><item><title>Amortized Bayesian Workflow (Extended Abstract)</title><link>http://arxiv.org/abs/2409.04332v1</link><description>Bayesian inference often faces a trade-off between computational speed andsampling accuracy. We propose an adaptive workflow that integrates rapidamortized inference with gold-standard MCMC techniques to achieve both speedand accuracy when performing inference on many observed datasets. Our approachuses principled diagnostics to guide the choice of inference method for eachdataset, moving along the Pareto front from fast amortized sampling to slowerbut guaranteed-accurate MCMC when necessary. By reusing computations acrosssteps, our workflow creates synergies between amortized and MCMC-basedinference. We demonstrate the effectiveness of this integrated approach on ageneralized extreme value task with 1000 observed data sets, showing 90x timeefficiency gains while maintaining high posterior quality.</description><author>Marvin Schmitt, Chengkun Li, Aki Vehtari, Luigi Acerbi, Paul-Christian Bürkner, Stefan T. Radev</author><pubDate>Fri, 06 Sep 2024 15:09:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04332v1</guid></item><item><title>The Faiss library</title><link>http://arxiv.org/abs/2401.08281v2</link><description>Vector databases typically manage large collections of embedding vectors.Currently, AI applications are growing rapidly, and so is the number ofembeddings that need to be stored and indexed. The Faiss library is dedicatedto vector similarity search, a core functionality of vector databases. Faiss isa toolkit of indexing methods and related primitives used to search, cluster,compress and transform vectors. This paper describes the trade-off space ofvector search and the design principles of Faiss in terms of structure,approach to optimization and interfacing. We benchmark key features of thelibrary and discuss a few selected applications to highlight its broadapplicability.</description><author>Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou</author><pubDate>Fri, 06 Sep 2024 15:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08281v2</guid></item><item><title>How to Identify Good Superpixels for Deforestation Detection on Tropical Rainforests</title><link>http://arxiv.org/abs/2409.04330v1</link><description>The conservation of tropical forests is a topic of significant social andecological relevance due to their crucial role in the global ecosystem.Unfortunately, deforestation and degradation impact millions of hectaresannually, requiring government or private initiatives for effective forestmonitoring. However, identifying deforested regions in satellite images ischallenging due to data imbalance, image resolution, low-contrast regions, andocclusion. Superpixel segmentation can overcome these drawbacks, reducingworkload and preserving important image boundaries. However, most works forremote sensing images do not exploit recent superpixel methods. In this work,we evaluate 16 superpixel methods in satellite images to support adeforestation detection system in tropical forests. We also assess theperformance of superpixel methods for the target task, establishing arelationship with segmentation methodological evaluation. According to ourresults, ERS, GMMSP, and DISF perform best on UE, BR, and SIRS, respectively,whereas ERS has the best trade-off with CO and Reg. In classification, SH,DISF, and ISF perform best on RGB, UMDA, and PCA compositions, respectively.According to our experiments, superpixel methods with better trade-offs betweendelineation, homogeneity, compactness, and regularity are more suitable foridentifying good superpixels for deforestation detection tasks.</description><author>Isabela Borlido, Eduardo Bouhid, Victor Sundermann, Hugo Resende, Alvaro Luiz Fazenda, Fabio Faria, Silvio Jamil F. Guimarães</author><pubDate>Fri, 06 Sep 2024 15:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04330v1</guid></item><item><title>Active learning for regression in engineering populations: A risk-informed approach</title><link>http://arxiv.org/abs/2409.04328v1</link><description>Regression is a fundamental prediction task common in data-centricengineering applications that involves learning mappings between continuousvariables. In many engineering applications (e.g.\ structural healthmonitoring), feature-label pairs used to learn such mappings are of limitedavailability which hinders the effectiveness of traditional supervised machinelearning approaches. The current paper proposes a methodology for overcomingthe issue of data scarcity by combining active learning with hierarchicalBayesian modelling. Active learning is an approach for preferentially acquiring feature-labelpairs in a resource-efficient manner. In particular, the current work adopts arisk-informed approach that leverages contextual information associated withregression-based engineering decision-making tasks (e.g.\ inspection andmaintenance). Hierarchical Bayesian modelling allow multiple related regressiontasks to be learned over a population, capturing local and global effects. Theinformation sharing facilitated by this modelling approach means thatinformation acquired for one engineering system can improve predictiveperformance across the population. The proposed methodology is demonstrated using an experimental case study.Specifically, multiple regressions are performed over a population of machiningtools, where the quantity of interest is the surface roughness of theworkpieces. An inspection and maintenance decision process is defined usingthese regression tasks which is in turn used to construct the active-learningalgorithm. The novel methodology proposed is benchmarked against an uninformedapproach to label acquisition and independent modelling of the regressiontasks. It is shown that the proposed approach has superior performance in termsof expected cost -- maintaining predictive performance while reducing thenumber of inspections required.</description><author>Daniel R. Clarkson, Lawrence A. Bull, Chandula T. Wickramarachchi, Elizabeth J. Cross, Timothy J. Rogers, Keith Worden, Nikolaos Dervilis, Aidan J. Hughes</author><pubDate>Fri, 06 Sep 2024 15:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04328v1</guid></item><item><title>Zero-Shot Video Editing through Adaptive Sliding Score Distillation</title><link>http://arxiv.org/abs/2406.04888v2</link><description>The rapidly evolving field of Text-to-Video generation (T2V) has catalyzedrenewed interest in controllable video editing research. While the applicationof editing prompts to guide diffusion model denoising has gained prominence,mirroring advancements in image editing, this noise-based inference processinherently compromises the original video's integrity, resulting in unintendedover-editing and temporal discontinuities. To address these challenges, thisstudy proposes a novel paradigm of video-based score distillation, facilitatingdirect manipulation of original video content. Specifically, distinguishing itfrom image-based score distillation, we propose an Adaptive Sliding ScoreDistillation strategy, which incorporates both global and local video guidanceto reduce the impact of editing errors. Combined with our proposed Image-basedJoint Guidance mechanism, it has the ability to mitigate the inherentinstability of the T2V model and single-step sampling. Additionally, we designa Weighted Attention Fusion module to further preserve the key features of theoriginal video and avoid over-editing. Extensive experiments demonstrate thatthese strategies effectively address existing challenges, achieving superiorperformance compared to current state-of-the-art methods.</description><author>Lianghan Zhu, Yanqi Bao, Jing Huo, Jing Wu, Yu-Kun Lai, Wenbin Li, Yang Gao</author><pubDate>Fri, 06 Sep 2024 14:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04888v2</guid></item><item><title>Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers</title><link>http://arxiv.org/abs/2409.04320v1</link><description>We consider the problem of sampling from a log-concave distribution$\pi(\theta) \propto e^{-f(\theta)}$ constrained to a polytope $K:=\{\theta \in\mathbb{R}^d: A\theta \leq b\}$, where $A\in \mathbb{R}^{m\times d}$ and $b \in\mathbb{R}^m$.The fastest-known algorithm \cite{mangoubi2022faster} for thesetting when $f$ is $O(1)$-Lipschitz or $O(1)$-smooth runs in roughly $O(md\times md^{\omega -1})$ arithmetic operations, where the $md^{\omega -1}$ termarises because each Markov chain step requires computing a matrix inversion anddeterminant (here $\omega \approx 2.37$ is the matrix multiplication constant).We present a nearly-optimal implementation of this Markov chain with per-stepcomplexity which is roughly the number of non-zero entries of $A$ while thenumber of Markov chain steps remains the same. The key technical ingredientsare 1) to show that the matrices that arise in this Dikin walk change slowly,2) to deploy efficient linear solvers that can leverage this slow change tospeed up matrix inversion by using information computed in previous steps, and3) to speed up the computation of the determinantal term in the Metropolisfilter step via a randomized Taylor series-based estimator.</description><author>Oren Mangoubi, Nisheeth K. Vishnoi</author><pubDate>Fri, 06 Sep 2024 14:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04320v1</guid></item><item><title>Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for Metadata Enrichment</title><link>http://arxiv.org/abs/2403.00884v3</link><description>Traditional dataset retrieval systems rely on metadata for indexing, ratherthan on the underlying data values. However, high-quality metadata creation andenrichment often require manual annotations, which is a labour-intensive andchallenging process to automate. In this study, we propose a method to supportmetadata enrichment using topic annotations generated by three Large LanguageModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuseson classifying column headers based on domain-specific topics from theConsortium of European Social Science Data Archives (CESSDA), a Linked Datacontrolled vocabulary. Our approach operates in a zero-shot setting,integrating the controlled topic vocabulary directly within the input prompt.This integration serves as a Large Context Windows approach, with the aim ofimproving the results of the topic classification task. We evaluated the performance of the LLMs in terms of internal consistency,inter-machine alignment, and agreement with human classification. Additionally,we investigate the impact of contextual information (i.e., dataset description)on the classification outcomes. Our findings suggest that ChatGPT andGoogleGemini outperform GoogleBard in terms of internal consistency as well asLLM-human-agreement. Interestingly, we found that contextual information had nosignificant impact on LLM performance. This work proposes a novel approach that leverages LLMs for topicclassification of column headers using a controlled vocabulary, presenting apractical application of LLMs and Large Context Windows within the Semantic Webdomain. This approach has the potential to facilitate automated metadataenrichment, thereby enhancing dataset retrieval and the Findability,Accessibility, Interoperability, and Reusability (FAIR) of research data on theWeb.</description><author>Margherita Martorana, Tobias Kuhn, Lise Stork, Jacco van Ossenbruggen</author><pubDate>Fri, 06 Sep 2024 14:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00884v3</guid></item><item><title>Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs</title><link>http://arxiv.org/abs/2409.04318v1</link><description>Generative Large Language Models (LLMs) are capable of being in-contextlearners. However, the underlying mechanism of in-context learning (ICL) isstill a major research question, and experimental research results about howmodels exploit ICL are not always consistent. In this work, we propose aframework for evaluating in-context learning mechanisms, which we claim are acombination of retrieving internal knowledge and learning from in-contextexamples by focusing on regression tasks. First, we show that LLMs can performregression on real-world datasets and then design experiments to measure theextent to which the LLM retrieves its internal knowledge versus learning fromin-context examples. We argue that this process lies on a spectrum betweenthese two extremes. We provide an in-depth analysis of the degrees to whichthese mechanisms are triggered depending on various factors, such as priorknowledge about the tasks and the type and richness of the information providedby the in-context examples. We employ three LLMs and utilize multiple datasetsto corroborate the robustness of our findings. Our results shed light on how toengineer prompts to leverage meta-learning from in-context examples and fosterknowledge retrieval depending on the problem being addressed.</description><author>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</author><pubDate>Fri, 06 Sep 2024 14:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04318v1</guid></item><item><title>UniPortrait: A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalization</title><link>http://arxiv.org/abs/2408.05939v2</link><description>This paper presents UniPortrait, an innovative human image personalizationframework that unifies single- and multi-ID customization with high facefidelity, extensive facial editability, free-form input description, anddiverse layout generation. UniPortrait consists of only two plug-and-playmodules: an ID embedding module and an ID routing module. The ID embeddingmodule extracts versatile editable facial features with a decoupling strategyfor each ID and embeds them into the context space of diffusion models. The IDrouting module then combines and distributes these embeddings adaptively totheir respective regions within the synthesized image, achieving thecustomization of single and multiple IDs. With a carefully designed two-stagetraining scheme, UniPortrait achieves superior performance in both single- andmulti-ID customization. Quantitative and qualitative experiments demonstratethe advantages of our method over existing approaches as well as its goodscalability, e.g., the universal compatibility with existing generative controltools. The project page is athttps://aigcdesigngroup.github.io/UniPortrait-Page/ .</description><author>Junjie He, Yifeng Geng, Liefeng Bo</author><pubDate>Fri, 06 Sep 2024 14:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05939v2</guid></item><item><title>Enhancing Uncertainty Quantification in Drug Discovery with Censored Regression Labels</title><link>http://arxiv.org/abs/2409.04313v1</link><description>In the early stages of drug discovery, decisions regarding which experimentsto pursue can be influenced by computational models. These decisions arecritical due to the time-consuming and expensive nature of the experiments.Therefore, it is becoming essential to accurately quantify the uncertainty inmachine learning predictions, such that resources can be used optimally andtrust in the models improves. While computational methods for drug discoveryoften suffer from limited data and sparse experimental observations, additionalinformation can exist in the form of censored labels that provide thresholdsrather than precise values of observations. However, the standard approachesthat quantify uncertainty in machine learning cannot fully utilize censoredlabels. In this work, we adapt ensemble-based, Bayesian, and Gaussian modelswith tools to learn from censored labels by using the Tobit model from survivalanalysis. Our results demonstrate that despite the partial informationavailable in censored labels, they are essential to accurately and reliablymodel the real pharmaceutical setting.</description><author>Emma Svensson, Hannah Rosa Friesacher, Susanne Winiwarter, Lewis Mervin, Adam Arany, Ola Engkvist</author><pubDate>Fri, 06 Sep 2024 14:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04313v1</guid></item><item><title>Geospecific View Generation -- Geometry-Context Aware High-resolution Ground View Inference from Satellite Views</title><link>http://arxiv.org/abs/2407.08061v3</link><description>Predicting realistic ground views from satellite imagery in urban scenes is achallenging task due to the significant view gaps between satellite andground-view images. We propose a novel pipeline to tackle this challenge, bygenerating geospecifc views that maximally respect the weak geometry andtexture from multi-view satellite images. Different from existing approachesthat hallucinate images from cues such as partial semantics or geometry fromoverhead satellite images, our method directly predicts ground-view images atgeolocation by using a comprehensive set of information from the satelliteimage, resulting in ground-level images with a resolution boost at a factor often or more. We leverage a novel building refinement method to reduce geometricdistortions in satellite data at ground level, which ensures the creation ofaccurate conditions for view synthesis using diffusion networks. Moreover, weproposed a novel geospecific prior, which prompts distribution learning ofdiffusion models to respect image samples that are closer to the geolocation ofthe predicted images. We demonstrate our pipeline is the first to generateclose-to-real and geospecific ground views merely based on satellite images.</description><author>Ningli Xu, Rongjun Qin</author><pubDate>Fri, 06 Sep 2024 14:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08061v3</guid></item><item><title>Deep learning modelling of manufacturing and build variations on multi-stage axial compressors aerodynamics</title><link>http://arxiv.org/abs/2310.04264v4</link><description>Applications of deep learning to physical simulations such as ComputationalFluid Dynamics have recently experienced a surge in interest, and theirviability has been demonstrated in different domains. However, due to thehighly complex, turbulent and three-dimensional flows, they have not yet beenproven usable for turbomachinery applications. Multi-stage axial compressorsfor gas turbine applications represent a remarkably challenging case, due tothe high-dimensionality of the regression of the flow-field from geometricaland operational variables. This paper demonstrates the development andapplication of a deep learning framework for predictions of the flow field andaerodynamic performance of multi-stage axial compressors. A physics-baseddimensionality reduction unlocks the potential for flow-field predictions, asit re-formulates the regression problem from an un-structured to a structuredone, as well as reducing the number of degrees of freedom. Compared totraditional "black-box" surrogate models, it provides explainability to thepredictions of overall performance by identifying the corresponding aerodynamicdrivers. This is applied to model the effect of manufacturing and buildvariations, as the associated performance scatter is known to have asignificant impact on $CO_2$ emissions, therefore posing a challenge of greatindustrial and environmental relevance. The proposed architecture is proven toachieve an accuracy comparable to that of the CFD benchmark, in real-time, foran industrially relevant application. The deployed model, is readily integratedwithin the manufacturing and build process of gas turbines, thus providing theopportunity to analytically assess the impact on performance with actionableand explainable data.</description><author>Giuseppe Bruni, Sepehr Maleki, Senthil K. Krishnababu</author><pubDate>Fri, 06 Sep 2024 14:35:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04264v4</guid></item><item><title>Advancing SEM Based Nano-Scale Defect Analysis in Semiconductor Manufacturing for Advanced IC Nodes</title><link>http://arxiv.org/abs/2409.04310v1</link><description>In this research, we introduce a unified end-to-end Automated DefectClassification-Detection-Segmentation (ADCDS) framework for classifying,detecting, and segmenting multiple instances of semiconductor defects foradvanced nodes. This framework consists of two modules: (a) a defect detectionmodule, followed by (b) a defect segmentation module. The defect detectionmodule employs Deformable DETR to aid in the classification and detection ofnano-scale defects, while the segmentation module utilizes BoxSnake. BoxSnakefacilitates box-supervised instance segmentation of nano-scale defects,supported by the former module. This simplifies the process by eliminating thelaborious requirement for ground-truth pixel-wise mask annotation by humanexperts, which is typically associated with training conventional segmentationmodels. We have evaluated the performance of our ADCDS framework using twodistinct process datasets from real wafers, as ADI and AEI, specificallyfocusing on Line-space patterns. We have demonstrated the applicability andsignificance of our proposed methodology, particularly in the nano-scalesegmentation and generation of binary defect masks, using the challenging ADISEM dataset where ground-truth pixelwise segmentation annotations wereunavailable. Furthermore, we have presented a comparative analysis of ourproposed framework against previous approaches to demonstrate itseffectiveness. Our proposed framework achieved an overall mAP@IoU0.5 of 72.19for detection and 78.86 for segmentation on the ADI dataset. Similarly, for theAEI dataset, these metrics were 90.38 for detection and 95.48 for segmentation.Thus, our proposed framework effectively fulfils the requirements of advanceddefect analysis while addressing significant constraints.</description><author>Bappaditya Dey, Matthias Monden, Victor Blanco, Sandip Halder, Stefan De Gendt</author><pubDate>Fri, 06 Sep 2024 14:35:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04310v1</guid></item><item><title>Safe and Efficient Path Planning under Uncertainty via Deep Collision Probability Fields</title><link>http://arxiv.org/abs/2409.04306v1</link><description>Estimating collision probabilities between robots and environmental obstaclesor other moving agents is crucial to ensure safety during path planning. Thisis an important building block of modern planning algorithms in manyapplication scenarios such as autonomous driving, where noisy sensors perceiveobstacles. While many approaches exist, they either provide too conservativeestimates of the collision probabilities or are computationally intensive dueto their sampling-based nature. To deal with these issues, we introduce DeepCollision Probability Fields, a neural-based approach for computing collisionprobabilities of arbitrary objects with arbitrary unimodal uncertaintydistributions. Our approach relegates the computationally intensive estimationof collision probabilities via sampling at the training step, allowing for fastneural network inference of the constraints during planning. In extensiveexperiments, we show that Deep Collision Probability Fields can producereasonably accurate collision probabilities (up to 10^{-3}) for planning andthat our approach can be easily plugged into standard path planning approachesto plan safe paths on 2-D maps containing uncertain static and dynamicobstacles. Additional material, code, and videos are available athttps://sites.google.com/view/ral-dcpf.</description><author>Felix Herrmann, Sebastian Zach, Jacopo Banfi, Jan Peters, Georgia Chalvatzaki, Davide Tateo</author><pubDate>Fri, 06 Sep 2024 14:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04306v1</guid></item><item><title>A Unified Approach to Inferring Chemical Compounds with the Desired Aqueous Solubility</title><link>http://arxiv.org/abs/2409.04301v1</link><description>Aqueous solubility (AS) is a key physiochemical property that plays a crucialrole in drug discovery and material design. We report a novel unified approachto predict and infer chemical compounds with the desired AS based on simpledeterministic graph-theoretic descriptors, multiple linear regression (MLR) andmixed integer linear programming (MILP). Selected descriptors based on aforward stepwise procedure enabled the simplest regression model, MLR, toachieve significantly good prediction accuracy compared to the existingapproaches, achieving the accuracy in the range [0.7191, 0.9377] for 29 diversedatasets. By simulating these descriptors and learning models as MILPs, weinferred mathematically exact and optimal compounds with the desired AS,prescribed structures, and up to 50 non-hydrogen atoms in a reasonable timerange [6, 1204] seconds. These findings indicate a strong correlation betweenthe simple graph-theoretic descriptors and the AS of compounds, potentiallyleading to a deeper understanding of their AS without relying on widely usedcomplicated chemical descriptors and complex machine learning models that arecomputationally expensive, and therefore difficult to use for inference. Animplementation of the proposed approach is available athttps://github.com/ku-dml/mol-infer/tree/master/AqSol.</description><author>Muniba Batool, Naveed Ahmed Azam, Jianshen Zhu, Kazuya Haraguchi, Liang Zhao, Tatsuya Akutsu</author><pubDate>Fri, 06 Sep 2024 14:20:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04301v1</guid></item><item><title>RAG based Question-Answering for Contextual Response Prediction System</title><link>http://arxiv.org/abs/2409.03708v2</link><description>Large Language Models (LLMs) have shown versatility in various NaturalLanguage Processing (NLP) tasks, including their potential as effectivequestion-answering systems. However, to provide precise and relevantinformation in response to specific customer queries in industry settings, LLMsrequire access to a comprehensive knowledge base to avoid hallucinations.Retrieval Augmented Generation (RAG) emerges as a promising technique toaddress this challenge. Yet, developing an accurate question-answeringframework for real-world applications using RAG entails several challenges: 1)data availability issues, 2) evaluating the quality of generated content, and3) the costly nature of human evaluation. In this paper, we introduce anend-to-end framework that employs LLMs with RAG capabilities for industry usecases. Given a customer query, the proposed system retrieves relevant knowledgedocuments and leverages them, along with previous chat history, to generateresponse suggestions for customer service agents in the contact centers of amajor retail company. Through comprehensive automated and human evaluations, weshow that this solution outperforms the current BERT-based algorithms inaccuracy and relevance. Our findings suggest that RAG-based LLMs can be anexcellent support to human customer service representatives by lightening theirworkload.</description><author>Sriram Veturi, Saurabh Vaichal, Reshma Lal Jagadheesh, Nafis Irtiza Tripto, Nian Yan</author><pubDate>Fri, 06 Sep 2024 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03708v2</guid></item><item><title>FS-MedSAM2: Exploring the Potential of SAM2 for Few-Shot Medical Image Segmentation without Fine-tuning</title><link>http://arxiv.org/abs/2409.04298v1</link><description>The Segment Anything Model 2 (SAM2) has recently demonstrated exceptionalperformance in zero-shot prompt segmentation for natural images and videos.However, it faces significant challenges when applied to medical images. Sinceits release, many attempts have been made to adapt SAM2's segmentationcapabilities to the medical imaging domain. These efforts typically involveusing a substantial amount of labeled data to fine-tune the model's weights. Inthis paper, we explore SAM2 from a different perspective via making the fulluse of its trained memory attention module and its ability of processing maskprompts. We introduce FS-MedSAM2, a simple yet effective framework that enablesSAM2 to achieve superior medical image segmentation in a few-shot setting,without the need for fine-tuning. Our framework outperforms the currentstate-of-the-arts on two publicly available medical image datasets. The code isavailable at https://github.com/DeepMed-Lab-ECNU/FS_MedSAM2.</description><author>Yunhao Bai, Qinji Yu, Boxiang Yun, Dakai Jin, Yingda Xia, Yan Wang</author><pubDate>Fri, 06 Sep 2024 14:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04298v1</guid></item><item><title>iSeg: An Iterative Refinement-based Framework for Training-free Segmentation</title><link>http://arxiv.org/abs/2409.03209v2</link><description>Stable diffusion has demonstrated strong image synthesis ability to giventext descriptions, suggesting it to contain strong semantic clue for groupingobjects. Inspired by this, researchers have explored employing stable diffusionfor trainingfree segmentation. Most existing approaches either simply employcross-attention map or refine it by self-attention map, to generatesegmentation masks. We believe that iterative refinement with self-attentionmap would lead to better results. However, we mpirically demonstrate that sucha refinement is sub-optimal likely due to the self-attention map containingirrelevant global information which hampers accurately refining cross-attentionmap with multiple iterations. To address this, we propose an iterativerefinement framework for training-free segmentation, named iSeg, having anentropy-reduced self-attention module which utilizes a gradient descent schemeto reduce the entropy of self-attention map, thereby suppressing the weakresponses corresponding to irrelevant global information. Leveraging theentropy-reduced self-attention module, our iSeg stably improves refinedcrossattention map with iterative refinement. Further, we design acategory-enhanced cross-attention module to generate accurate cross-attentionmap, providing a better initial input for iterative refinement. Extensiveexperiments across different datasets and diverse segmentation tasks reveal themerits of proposed contributions, leading to promising performance on diversesegmentation tasks. For unsupervised semantic segmentation on Cityscapes, ouriSeg achieves an absolute gain of 3.8% in terms of mIoU compared to the bestexisting training-free approach in literature. Moreover, our proposed iSeg cansupport segmentation with different kind of images and interactions.</description><author>Lin Sun, Jiale Cao, Jin Xie, Fahad Shahbaz Khan, Yanwei Pang</author><pubDate>Fri, 06 Sep 2024 14:15:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03209v2</guid></item><item><title>HSTR-Net: Reference Based Video Super-resolution with Dual Cameras</title><link>http://arxiv.org/abs/2310.12092v2</link><description>High-spatio-temporal resolution (HSTR) video recording plays a crucial rolein enhancing various imagery tasks that require fine-detailed information.State-of-the-art cameras provide this required high frame-rate and high spatialresolution together, albeit at a high cost. To alleviate this issue, this paperproposes a dual camera system for the generation of HSTR video usingreference-based super-resolution (RefSR). One camera captures high spatialresolution low frame rate (HSLF) video while the other captures low spatialresolution high frame rate (LSHF) video simultaneously for the same scene. Anovel deep learning architecture is proposed to fuse HSLF and LSHF video feedsand synthesize HSTR video frames. The proposed model combines optical flowestimation and (channel-wise and spatial) attention mechanisms to capture thefine motion and complex dependencies between frames of the two video feeds.Simulations show that the proposed model provides significant improvement overexisting reference-based SR techniques in terms of PSNR and SSIM metrics. Themethod also exhibits sufficient frames per second (FPS) for aerial monitoringwhen deployed on a power-constrained drone equipped with dual cameras.</description><author>H. Umut Suluhan, Abdullah Enes Doruk, Hasan F. Ates, Bahadir K. Gunturk</author><pubDate>Fri, 06 Sep 2024 14:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12092v2</guid></item><item><title>Standing on the shoulders of giants</title><link>http://arxiv.org/abs/2409.03151v2</link><description>Although fundamental to the advancement of Machine Learning, the classicevaluation metrics extracted from the confusion matrix, such as precision andF1, are limited. Such metrics only offer a quantitative view of the models'performance, without considering the complexity of the data or the quality ofthe hit. To overcome these limitations, recent research has introduced the useof psychometric metrics such as Item Response Theory (IRT), which allows anassessment at the level of latent characteristics of instances. This workinvestigates how IRT concepts can enrich a confusion matrix in order toidentify which model is the most appropriate among options with similarperformance. In the study carried out, IRT does not replace, but complementsclassical metrics by offering a new layer of evaluation and observation of thefine behavior of models in specific instances. It was also observed that thereis 97% confidence that the score from the IRT has different contributions from66% of the classical metrics analyzed.</description><author>Lucas Felipe Ferraro Cardoso, José de Sousa Ribeiro Filho, Vitor Cirilo Araujo Santos, Regiane Silva Kawasaki Frances, Ronnie Cley de Oliveira Alves</author><pubDate>Fri, 06 Sep 2024 14:04:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03151v2</guid></item><item><title>CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis</title><link>http://arxiv.org/abs/2409.04290v1</link><description>Survival analysis is a branch of statistics used for modeling the time untila specific event occurs and is widely used in medicine, engineering, finance,and many other fields. When choosing survival models, there is typically atrade-off between performance and interpretability, where the highestperformance is achieved by black-box models based on deep learning. This is amajor problem in fields such as medicine where practitioners are reluctant toblindly trust black-box models to make important patient decisions.Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretableand accurate alternative to multi-layer perceptrons (MLPs). We introduceCoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,high-performance survival analysis. We evaluate the proposed CoxKAN on 4synthetic datasets and 9 real medical datasets. The synthetic experimentsdemonstrate that CoxKAN accurately recovers interpretable symbolic formulae forthe hazard function, and effectively performs automatic feature selection.Evaluation on the 9 real datasets show that CoxKAN consistently outperforms theCox proportional hazards model and achieves performance that is superior orcomparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifiescomplex interactions between predictor variables that would be extremelydifficult to recognise using existing survival methods, and automatically findssymbolic formulae which uncover the precise effect of important biomarkers onpatient risk.</description><author>William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar</author><pubDate>Fri, 06 Sep 2024 13:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04290v1</guid></item><item><title>Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets</title><link>http://arxiv.org/abs/2409.04286v1</link><description>Current publicly available knowledge work data collections lack diversity,extensive annotations, and contextual information about the users and theirdocuments. These issues hinder objective and comparable data-driven evaluationsand optimizations of knowledge work assistance systems. Due to the considerableresources needed to collect such data in real-life settings and the necessityof data censorship, collecting such a dataset appears nearly impossible. Forthis reason, we propose a configurable, multi-agent knowledge work datasetgenerator. This system simulates collaborative knowledge work among agentsproducing Large Language Model-generated documents and accompanying datatraces. Additionally, the generator captures all background information, givenin its configuration or created during the simulation process, in a knowledgegraph. Finally, the resulting dataset can be utilized and shared withoutprivacy or confidentiality concerns. This paper introduces our approach's design and vision and focuses ongenerating authentic knowledge work documents using Large Language Models. Ourstudy involving human raters who assessed 53% of the generated and 74% of thereal documents as realistic demonstrates the potential of our approach.Furthermore, we analyze the authenticity criteria mentioned in theparticipants' comments and elaborate on potential improvements for identifiedcommon issues.</description><author>Desiree Heim, Christian Jilek, Adrian Ulges, Andreas Dengel</author><pubDate>Fri, 06 Sep 2024 13:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04286v1</guid></item><item><title>Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy</title><link>http://arxiv.org/abs/2406.17324v2</link><description>Rapid progress in the capabilities of machine learning approaches in naturallanguage processing has culminated in the rise of large language models overthe last two years. Recent works have shown unprecedented adoption of these foracademic writing, especially in some fields, but their pervasiveness inastronomy has not been studied sufficiently. To remedy this, we extract wordsthat ChatGPT uses more often than humans when generating academic text andsearch a total of 1 million articles for them. This way, we assess thefrequency of word occurrence in published works in astronomy tracked by theNASA Astrophysics Data System since 2000. We then perform a statisticalanalysis of the occurrences. We identify a list of words favoured by ChatGPTand find a statistically significant increase for these words against a controlgroup in 2024, which matches the trend in other disciplines. These resultssuggest a widespread adoption of these models in the writing of astronomypapers. We encourage organisations, publishers, and researchers to worktogether to identify ethical and pragmatic guidelines to maximise the benefitsof these systems while maintaining scientific rigour.</description><author>Simone Astarita, Sandor Kruk, Jan Reerink, Pablo Gómez</author><pubDate>Fri, 06 Sep 2024 13:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17324v2</guid></item><item><title>Extension of Recurrent Kernels to different Reservoir Computing topologies</title><link>http://arxiv.org/abs/2401.14557v2</link><description>Reservoir Computing (RC) has become popular in recent years due to its fastand efficient computational capabilities. Standard RC has been shown to beequivalent in the asymptotic limit to Recurrent Kernels, which helps inanalyzing its expressive power. However, many well-established RC paradigms,such as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way.This study aims to fill this gap by providing an empirical analysis of theequivalence of specific RC architectures with their corresponding RecurrentKernel formulation. We conduct a convergence study by varying the activationfunction implemented in each architecture. Our study also sheds light on therole of sparse connections in RC architectures and propose an optimal sparsitylevel that depends on the reservoir size. Furthermore, our systematic analysisshows that in Deep RC models, convergence is better achieved with successivereservoirs of decreasing sizes.</description><author>Giuseppe Alessio D'Inverno, Jonathan Dong</author><pubDate>Fri, 06 Sep 2024 13:49:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14557v2</guid></item><item><title>LMFLOSS: A Hybrid Loss For Imbalanced Medical Image Classification</title><link>http://arxiv.org/abs/2212.12741v2</link><description>With advances in digital technology, the classification of medical images hasbecome a crucial step for image-based clinical decision support systems.Automatic medical image classification represents a pivotal domain where theuse of AI holds the potential to create a significant social impact. However,several challenges act as obstacles to the development of practical andeffective solutions. One of these challenges is the prevalent class imbalanceproblem in most medical imaging datasets. As a result, existing AI techniques,particularly deep-learning-based methodologies, often underperform in suchscenarios. In this study, we propose a novel framework called Large Marginaware Focal (LMF) loss to mitigate the class imbalance problem in medicalimaging. The LMF loss represents a linear combination of two loss functionsoptimized by two hyperparameters. This framework harnesses the distinctcharacteristics of both loss functions by enforcing wider margins for minorityclasses while simultaneously emphasizing challenging samples found in thedatasets. We perform rigorous experiments on three neural network architecturesand with four medical imaging datasets. We provide empirical evidence that ourproposed framework consistently outperforms other baseline methods, showing animprovement of 2%-9% in macro-f1 scores. Through class-wise analysis of f1scores, we also demonstrate how the proposed framework can significantlyimprove performance for minority classes. The results of our experiments showthat our proposed framework can perform consistently well across differentarchitectures and datasets. Overall, our study demonstrates a simple andeffective approach to addressing the class imbalance problem in medical imagingdatasets. We hope our work will inspire new research toward a more generalizedapproach to medical image classification.</description><author>Abu Adnan Sadi, Labib Chowdhury, Nusrat Jahan, Mohammad Newaz Sharif Rafi, Radeya Chowdhury, Faisal Ahamed Khan, Nabeel Mohammed</author><pubDate>Fri, 06 Sep 2024 13:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12741v2</guid></item><item><title>AttentionX: Exploiting Consensus Discrepancy In Attention from A Distributed Optimization Perspective</title><link>http://arxiv.org/abs/2409.04275v1</link><description>In this paper, we extend the standard Attention in transformer by exploitingthe consensus discrepancy from a distributed optimization perspective, referredto as AttentionX. It is noted that %the popular distributed optimizationalgorithm \cite{Boyd11ADMM} and the primal-dual method of multipliers (PDMM)\cite{Zhang16PDMM} is designed to iteratively solve a broad class ofdistributed optimization problems over a pear-to-pear (P2P) network, whereneighbouring nodes gradually reach consensus as specified by predefined linearedge-constraints in the optimization process. In particular, at each iterationof PDMM, each node in a network first performs information-gathering fromneighbours and then performs local information-fusion. From a high-level pointof view, the $KQ$-softmax-based weighted summation of $V$-representations inAttention corresponds information-gathering from neighbours while thefeature-processing via the feed-forward network (FFN) in transformercorresponds to local information fusion. PDMM exploits the Lagrangianmultipliers to capture the historical consensus discrepancy in the form ofresidual errors of the linear edge-constraints, which plays a crucial role forthe algorithm to converge. Inspired by PDMM, we propose AttentionX toincorporate the consensus discrepancy in the output update-expression of thestandard Attention. The consensus discrepancy in AttentionX refers to thedifference between the weighted summation of $V$-representations and scaled$V$-representions themselves. Experiments on ViT and nanoGPT show promisingperformance.</description><author>Guoqiang Zhang, Richard Heusdens</author><pubDate>Fri, 06 Sep 2024 13:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04275v1</guid></item><item><title>Hybrid Semantic Search: Unveiling User Intent Beyond Keywords</title><link>http://arxiv.org/abs/2408.09236v3</link><description>This paper addresses the limitations of traditional keyword-based search inunderstanding user intent and introduces a novel hybrid search approach thatleverages the strengths of non-semantic search engines, Large Language Models(LLMs), and embedding models. The proposed system integrates keyword matching,semantic vector embeddings, and LLM-generated structured queries to deliverhighly relevant and contextually appropriate search results. By combining thesecomplementary methods, the hybrid approach effectively captures both explicitand implicit user intent.The paper further explores techniques to optimizequery execution for faster response times and demonstrates the effectiveness ofthis hybrid search model in producing comprehensive and accurate searchoutcomes.</description><author>Aman Ahluwalia, Bishwajit Sutradhar, Karishma Ghosh, Indrapal Yadav, Arpan Sheetal, Prashant Patil</author><pubDate>Fri, 06 Sep 2024 13:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09236v3</guid></item><item><title>Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT</title><link>http://arxiv.org/abs/2407.11041v2</link><description>This paper presents the design of a hardware accelerator for Transformers,optimized for on-device time-series forecasting in AIoT systems. It integratesinteger-only quantization and Quantization-Aware Training with optimizedhardware designs to realize 6-bit and 4-bit quantized Transformer models, whichachieved precision comparable to 8-bit quantized models from related research.Utilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7XC7S15), we examine the feasibility of deploying Transformer models on embeddedIoT devices. This includes a thorough analysis of achievable precision,resource utilization, timing, power, and energy consumption for on-deviceinference. Our results indicate that while sufficient performance can beattained, the optimization process is not trivial. For instance, reducing thequantization bitwidth does not consistently result in decreased latency orenergy consumption, underscoring the necessity of systematically exploringvarious optimization combinations. Compared to an 8-bit quantized Transformermodel in related studies, our 4-bit quantized Transformer model increases testloss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x lessenergy.</description><author>Tianheng Ling, Chao Qian, Gregor Schiele</author><pubDate>Fri, 06 Sep 2024 13:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11041v2</guid></item><item><title>A Black-Box Physics-Informed Estimator based on Gaussian Process Regression for Robot Inverse Dynamics Identification</title><link>http://arxiv.org/abs/2310.06585v2</link><description>Learning the inverse dynamics of robots directly from data, adopting ablack-box approach, is interesting for several real-world scenarios wherelimited knowledge about the system is available. In this paper, we propose ablack-box model based on Gaussian Process (GP) Regression for theidentification of the inverse dynamics of robotic manipulators. The proposedmodel relies on a novel multidimensional kernel, called \textit{LagrangianInspired Polynomial} (\kernelInitials{}) kernel. The \kernelInitials{} kernelis based on two main ideas. First, instead of directly modeling the inversedynamics components, we model as GPs the kinetic and potential energy of thesystem. The GP prior on the inverse dynamics components is derived from thoseon the energies by applying the properties of GPs under linear operators.Second, as regards the energy prior definition, we prove a polynomial structureof the kinetic and potential energy, and we derive a polynomial kernel thatencodes this property. As a consequence, the proposed model allows also toestimate the kinetic and potential energy without requiring any label on thesequantities. Results on simulation and on two real robotic manipulators, namelya 7 DOF Franka Emika Panda, and a 6 DOF MELFA RV4FL, show that the proposedmodel outperforms state-of-the-art black-box estimators based both on GaussianProcesses and Neural Networks in terms of accuracy, generality and dataefficiency. The experiments on the MELFA robot also demonstrate that ourapproach achieves performance comparable to fine-tuned model-based estimators,despite requiring less prior information.</description><author>Giulio Giacomuzzos, Ruggero Carli, Diego Romeres, Alberto Dalla Libera</author><pubDate>Fri, 06 Sep 2024 13:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06585v2</guid></item><item><title>Cycle Pixel Difference Network for Crisp Edge Detection</title><link>http://arxiv.org/abs/2409.04272v1</link><description>Edge detection, as a fundamental task in computer vision, has garneredincreasing attention. The advent of deep learning has significantly advancedthis field. However, recent deep learning-based methods which rely onlarge-scale pre-trained weights cannot be trained from scratch, with verylimited research addressing this issue. This paper proposes a novel cycle pixeldifference convolution (CPDC), which effectively integrates image gradientinformation with modern convolution operations. Based on the CPDC, we develop aU-shape encoder-decoder model named CPD-Net, which is a purely end-to-endnetwork. Additionally, to address the issue of edge thickness produced by mostexisting methods, we construct a multi-scale information enhancement module(MSEM) to enhance the discriminative ability of the model, thereby generatingcrisp and clean contour maps. Comprehensive experiments conducted on threestandard benchmarks demonstrate that our method achieves competitiveperformance on the BSDS500 dataset (ODS=0.813), NYUD-V2 (ODS=0.760), and BIPEDdataset (ODS=0.898). Our approach provides a novel perspective for addressingthese challenges in edge detection.</description><author>Changsong Liu, Wei Zhang, Yanyan Liu, Mingyang Li, Wenlin Li, Yimeng Fan, Xiangnan Bai, Liang Zhangd</author><pubDate>Fri, 06 Sep 2024 13:28:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04272v1</guid></item><item><title>A Unified Representation Framework for the Evaluation of Optical Music Recognition Systems</title><link>http://arxiv.org/abs/2312.12908v2</link><description>Modern-day Optical Music Recognition (OMR) is a fairly fragmented field. MostOMR approaches use datasets that are independent and incompatible between eachother, making it difficult to both combine them and compare recognition systemsbuilt upon them. In this paper we identify the need of a common musicrepresentation language and propose the Music Tree Notation (MTN) format, withthe idea to construct a common endpoint for OMR research that allowscoordination, reuse of technology and fair evaluation of community efforts.This format represents music as a set of primitives that group together intohigher-abstraction nodes, a compromise between the expression of fullygraph-based and sequential notation formats. We have also developed a specificset of OMR metrics and a typeset score dataset as a proof of concept of thisidea.</description><author>Pau Torras, Sanket Biswas, Alicia Fornés</author><pubDate>Fri, 06 Sep 2024 13:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12908v2</guid></item><item><title>Advancing Automated Knowledge Transfer in Evolutionary Multitasking via Large Language Models</title><link>http://arxiv.org/abs/2409.04270v1</link><description>Evolutionary Multi-task Optimization (EMTO) is a paradigm that leveragesknowledge transfer across simultaneously optimized tasks for enhanced searchperformance. To facilitate EMTO's performance, various knowledge transfermodels have been developed for specific optimization tasks. However, designingthese models often requires substantial expert knowledge. Recently, largelanguage models (LLMs) have achieved remarkable success in autonomousprogramming, aiming to produce effective solvers for specific problems. In thiswork, a LLM-based optimization paradigm is introduced to establish anautonomous model factory for generating knowledge transfer models, ensuringeffective and efficient knowledge transfer across various optimization tasks.To evaluate the performance of the proposed method, we conducted comprehensiveempirical studies comparing the knowledge transfer model generated by the LLMwith existing state-of-the-art knowledge transfer methods. The resultsdemonstrate that the generated model is able to achieve superior or competitiveperformance against hand-crafted knowledge transfer models in terms of bothefficiency and effectiveness.</description><author>Yuxiao Huang, Xuebin Lv, Shenghao Wu, Jibin Wu, Liang Feng, Kay Chen Tan</author><pubDate>Fri, 06 Sep 2024 13:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04270v1</guid></item><item><title>Open Language Data Initiative: Advancing Low-Resource Machine Translation for Karakalpak</title><link>http://arxiv.org/abs/2409.04269v1</link><description>This study presents several contributions for the Karakalpak language: aFLORES+ devtest dataset translated to Karakalpak, parallel corpora forUzbek-Karakalpak, Russian-Karakalpak and English-Karakalpak of 100,000 pairseach and open-sourced fine-tuned neural models for translation across theselanguages. Our experiments compare different model variants and trainingapproaches, demonstrating improvements over existing baselines. This work,conducted as part of the Open Language Data Initiative (OLDI) shared task, aimsto advance machine translation capabilities for Karakalpak and contribute toexpanding linguistic diversity in NLP technologies.</description><author>Mukhammadsaid Mamasaidov, Abror Shopulatov</author><pubDate>Fri, 06 Sep 2024 13:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04269v1</guid></item><item><title>An overview of domain-specific foundation model: key technologies, applications and challenges</title><link>http://arxiv.org/abs/2409.04267v1</link><description>The impressive performance of ChatGPT and other foundation-model-basedproducts in human language understanding has prompted both academia andindustry to explore how these models can be tailored for specific industriesand application scenarios. This process, known as the customization ofdomain-specific foundation models, addresses the limitations of general-purposemodels, which may not fully capture the unique patterns and requirements ofdomain-specific data. Despite its importance, there is a notable lack ofcomprehensive overview papers on building domain-specific foundation models,while numerous resources exist for general-purpose models. To bridge this gap,this article provides a timely and thorough overview of the methodology forcustomizing domain-specific foundation models. It introduces basic concepts,outlines the general architecture, and surveys key methods for constructingdomain-specific models. Furthermore, the article discusses various domains thatcan benefit from these specialized models and highlights the challenges ahead.Through this overview, we aim to offer valuable guidance and reference forresearchers and practitioners from diverse fields to develop their owncustomized foundation models.</description><author>Haolong Chen, Hanzhi Chen, Zijian Zhao, Kaifeng Han, Guangxu Zhu, Yichen Zhao, Ying Du, Wei Xu, Qingjiang Shi</author><pubDate>Fri, 06 Sep 2024 13:24:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04267v1</guid></item><item><title>RSF-Conv: Rotation-and-Scale Equivariant Fourier Parameterized Convolution for Retinal Vessel Segmentation</title><link>http://arxiv.org/abs/2309.15638v2</link><description>Retinal vessel segmentation is of great clinical significance for thediagnosis of many eye-related diseases, but it is still a formidable challengedue to the intricate vascular morphology. With the skillful characterization ofthe translation symmetry existing in retinal vessels, convolutional neuralnetworks (CNNs) have achieved great success in retinal vessel segmentation.However, the rotation-and-scale symmetry, as a more widespread image prior inretinal vessels, fails to be characterized by CNNs. Therefore, we propose arotation-and-scale equivariant Fourier parameterized convolution (RSF-Conv)specifically for retinal vessel segmentation, and provide the correspondingequivariance analysis. As a general module, RSF-Conv can be integrated intoexisting networks in a plug-and-play manner while significantly reducing thenumber of parameters. For instance, we replace the traditional convolutionfilters in U-Net and Iter-Net with RSF-Convs, and faithfully conductcomprehensive experiments. RSF-Conv+U-Net and RSF-Conv+Iter-Net not only haveslight advantages under in-domain evaluation, but more importantly, outperformall comparison methods by a significant margin under out-of-domain evaluation.It indicates the remarkable generalization of RSF-Conv, which holds greaterpractical clinical significance for the prevalent cross-device andcross-hospital challenges in clinical practice. To comprehensively demonstratethe effectiveness of RSF-Conv, we also apply RSF-Conv+U-Net andRSF-Conv+Iter-Net to retinal artery/vein classification and achieve promisingperformance as well, indicating its clinical application potential.</description><author>Zihong Sun, Hong Wang, Qi Xie, Yefeng Zheng, Deyu Meng</author><pubDate>Fri, 06 Sep 2024 13:21:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15638v2</guid></item><item><title>AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation</title><link>http://arxiv.org/abs/2409.00342v2</link><description>Recent studies have demonstrated the effectiveness of token-based methods forvisual content generation. As a representative work, non-autoregressiveTransformers (NATs) are able to synthesize images with decent quality in asmall number of steps. However, NATs usually necessitate configuring acomplicated generation policy comprising multiple manually-designed schedulingrules. These heuristic-driven rules are prone to sub-optimality and come withthe requirements of expert knowledge and labor-intensive efforts. Moreover,their one-size-fits-all nature cannot flexibly adapt to the diversecharacteristics of each individual sample. To address these issues, we proposeAdaNAT, a learnable approach that automatically configures a suitable policytailored for every sample to be generated. In specific, we formulate thedetermination of generation policies as a Markov decision process. Under thisframework, a lightweight policy network for generation can be learned viareinforcement learning. Importantly, we demonstrate that simple reward designssuch as FID or pre-trained reward models, may not reliably guarantee thedesired quality or diversity of generated samples. Therefore, we propose anadversarial reward design to guide the training of policy networks effectively.Comprehensive experiments on four benchmark datasets, i.e., ImageNet-256 &amp; 512,MS-COCO, and CC3M, validate the effectiveness of AdaNAT. Code and pre-trainedmodels will be released at https://github.com/LeapLabTHU/AdaNAT.</description><author>Zanlin Ni, Yulin Wang, Renping Zhou, Rui Lu, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Yuan Yao, Gao Huang</author><pubDate>Fri, 06 Sep 2024 13:00:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00342v2</guid></item><item><title>Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge Devices</title><link>http://arxiv.org/abs/2409.04249v1</link><description>The application of Transformer-based large models has achieved numeroussuccess in recent years. However, the exponential growth in the parameters oflarge models introduces formidable memory challenge for edge deployment. Priorworks to address this challenge mainly focus on optimizing the model structureand adopting memory swapping methods. However, the former reduces the inferenceaccuracy, and the latter raises the inference latency. This paper introducesPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reducesmemory usage by incorporating dynamic memory management and minimizes inferencelatency by employing parallel model loading. Based on PIPELOAD mechanism, wepresent Hermes, a framework optimized for large model inference on edgedevices. We evaluate Hermes on Transformer-based models of different sizes. Ourexperiments illustrate that Hermes achieves up to 4.24 X increase in inferencespeed and 86.7% lower memory consumption than the state-of-the-art pipelinemechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%lower memory consumption for GPT-style models.</description><author>Xueyuan Han, Zinuo Cai, Yichu Zhang, Chongxin Fan, Junhan Liu, Ruhui Ma, Rajkumar Buyya</author><pubDate>Fri, 06 Sep 2024 12:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04249v1</guid></item><item><title>Enhancing AI-based Generation of Software Exploits with Contextual Information</title><link>http://arxiv.org/abs/2408.02402v3</link><description>This practical experience report explores Neural Machine Translation (NMT)models' capability to generate offensive security code from natural language(NL) descriptions, highlighting the significance of contextual understandingand its impact on model performance. Our study employs a dataset comprisingreal shellcodes to evaluate the models across various scenarios, includingmissing information, necessary context, and unnecessary context. Theexperiments are designed to assess the models' resilience against incompletedescriptions, their proficiency in leveraging context for enhanced accuracy,and their ability to discern irrelevant information. The findings reveal thatthe introduction of contextual data significantly improves performance.However, the benefits of additional context diminish beyond a certain point,indicating an optimal level of contextual information for model training.Moreover, the models demonstrate an ability to filter out unnecessary context,maintaining high levels of accuracy in the generation of offensive securitycode. This study paves the way for future research on optimizing context use inAI-driven code generation, particularly for applications requiring a highdegree of technical precision such as the generation of offensive code.</description><author>Pietro Liguori, Cristina Improta, Roberto Natella, Bojan Cukic, Domenico Cotroneo</author><pubDate>Fri, 06 Sep 2024 12:51:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02402v3</guid></item><item><title>WarpAdam: A new Adam optimizer based on Meta-Learning approach</title><link>http://arxiv.org/abs/2409.04244v1</link><description>Optimal selection of optimization algorithms is crucial for training deeplearning models. The Adam optimizer has gained significant attention due to itsefficiency and wide applicability. However, to enhance the adaptability ofoptimizers across diverse datasets, we propose an innovative optimizationstrategy by integrating the 'warped gradient descend'concept from Meta Learninginto the Adam optimizer. In the conventional Adam optimizer, gradients areutilized to compute estimates of gradient mean and variance, subsequentlyupdating model parameters. Our approach introduces a learnable distortionmatrix, denoted as P, which is employed for linearly transforming gradients.This transformation slightly adjusts gradients during each iteration, enablingthe optimizer to better adapt to distinct dataset characteristics. By learningan appropriate distortion matrix P, our method aims to adaptively adjustgradient information across different data distributions, thereby enhancingoptimization performance. Our research showcases the potential of this novelapproach through theoretical insights and empirical evaluations. Experimentalresults across various tasks and datasets validate the superiority of ouroptimizer that integrates the 'warped gradient descend' concept in terms ofadaptability. Furthermore, we explore effective strategies for training theadaptation matrix P and identify scenarios where this method can yield optimalresults. In summary, this study introduces an innovative approach that mergesthe 'warped gradient descend' concept from Meta Learning with the Adamoptimizer. By introducing a learnable distortion matrix P within the optimizer,we aim to enhance the model's generalization capability across diverse datadistributions, thus opening up new possibilities in the field of deep learningoptimization.</description><author>Chengxi Pan, Junshang Chen, Jingrui Ye</author><pubDate>Fri, 06 Sep 2024 12:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04244v1</guid></item><item><title>Hybrid Cost Volume for Memory-Efficient Optical Flow</title><link>http://arxiv.org/abs/2409.04243v1</link><description>Current state-of-the-art flow methods are mostly based on dense all-pairscost volumes. However, as image resolution increases, the computational andspatial complexity of constructing these cost volumes grows at a quartic rate,making these methods impractical for high-resolution images. In this paper, wepropose a novel Hybrid Cost Volume for memory-efficient optical flow, namedHCV. To construct HCV, we first propose a Top-k strategy to separate the 4Dcost volume into two global 3D cost volumes. These volumes significantly reducememory usage while retaining a substantial amount of matching information. Wefurther introduce a local 4D cost volume with a local search space tosupplement the local information for HCV. Based on HCV, we design amemory-efficient optical flow network, named HCVFlow. Compared to the recurrentflow methods based the all-pairs cost volumes, our HCVFlow significantlyreduces memory consumption while ensuring high accuracy. We validate theeffectiveness and efficiency of our method on the Sintel and KITTI datasets andreal-world 4K (2160*3840) resolution images. Extensive experiments show thatour HCVFlow has very low memory usage and outperforms other memory-efficientmethods in terms of accuracy. The code is publicly available athttps://github.com/gangweiX/HCVFlow.</description><author>Yang Zhao, Gangwei Xu, Gang Wu</author><pubDate>Fri, 06 Sep 2024 12:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04243v1</guid></item><item><title>Unmasking Covert Intrusions: Detection of Fault-Masking Cyberattacks on Differential Protection Systems</title><link>http://arxiv.org/abs/2409.04242v1</link><description>Line Current Differential Relays (LCDRs) are high-speed relays progressivelyused to protect critical transmission lines. However, LCDRs are vulnerable tocyberattacks. Fault-Masking Attacks (FMAs) are stealthy cyberattacks performedby manipulating the remote measurements of the targeted LCDR to disguise faultson the protected line. Hence, they remain undetected by this LCDR. In thispaper, we propose a two-module framework to detect FMAs. The first module is aMismatch Index (MI) developed from the protected transmission line's equivalentphysical model. The MI is triggered only if there is a significant mismatch inthe LCDR's local and remote measurements while the LCDR itself is untriggered,which indicates an FMA. After the MI is triggered, the second module, a neuralnetwork-based classifier, promptly confirms that the triggering event is aphysical fault that lies on the line protected by the LCDR before declaring theoccurrence of an FMA. The proposed framework is tested using the IEEE 39-busbenchmark system. Our simulation results confirm that the proposed frameworkcan accurately detect FMAs on LCDRs and is not affected by normal systemdisturbances, variations, or measurement noise. Our experimental results usingOPAL-RT's real-time simulator confirm the proposed solution's real-timeperformance capability.</description><author>Ahmad Mohammad Saber, Amr Youssef, Davor Svetinovic, Hatem Zeineldin, Ehab F. El-Saadany</author><pubDate>Fri, 06 Sep 2024 12:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04242v1</guid></item><item><title>Calibration of Network Confidence for Unsupervised Domain Adaptation Using Estimated Accuracy</title><link>http://arxiv.org/abs/2409.04241v1</link><description>This study addresses the problem of calibrating network confidence whileadapting a model that was originally trained on a source domain to a targetdomain using unlabeled samples from the target domain. The absence of labelsfrom the target domain makes it impossible to directly calibrate the adaptednetwork on the target domain. To tackle this challenge, we introduce acalibration procedure that relies on estimating the network's accuracy on thetarget domain. The network accuracy is first computed on the labeled sourcedata and then is modified to represent the actual accuracy of the model on thetarget domain. The proposed algorithm calibrates the prediction confidencedirectly in the target domain by minimizing the disparity between the estimatedaccuracy and the computed confidence. The experimental results show that ourmethod significantly outperforms existing methods, which rely on importanceweighting, across several standard datasets.</description><author>Coby Penso, Jacob Goldberger</author><pubDate>Fri, 06 Sep 2024 12:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04241v1</guid></item><item><title>UniDet3D: Multi-dataset Indoor 3D Object Detection</title><link>http://arxiv.org/abs/2409.04234v1</link><description>Growing customer demand for smart solutions in robotics and augmented realityhas attracted considerable attention to 3D object detection from point clouds.Yet, existing indoor datasets taken individually are too small andinsufficiently diverse to train a powerful and general 3D object detectionmodel. In the meantime, more general approaches utilizing foundation models arestill inferior in quality to those based on supervised training for a specifictask. In this work, we propose \ours{}, a simple yet effective 3D objectdetection model, which is trained on a mixture of indoor datasets and iscapable of working in various indoor environments. By unifying different labelspaces, \ours{} enables learning a strong representation across multipledatasets through a supervised joint training scheme. The proposed networkarchitecture is built upon a vanilla transformer encoder, making it easy torun, customize and extend the prediction pipeline for practical use. Extensiveexperiments demonstrate that \ours{} obtains significant gains over existing 3Dobject detection methods in 6 indoor benchmarks: ScanNet (+1.1 mAP50),ARKitScenes (+19.4 mAP25), S3DIS (+9.1 mAP50), MultiScan (+9.3 mAP50), 3RScan(+3.2 mAP50), and ScanNet++ (+2.7 mAP50). Code is available athttps://github.com/filapro/unidet3d .</description><author>Maksim Kolodiazhnyi, Anna Vorontsova, Matvey Skripkin, Danila Rukhovich, Anton Konushin</author><pubDate>Fri, 06 Sep 2024 12:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04234v1</guid></item><item><title>SPACE: A Python-based Simulator for Evaluating Decentralized Multi-Robot Task Allocation Algorithms</title><link>http://arxiv.org/abs/2409.04230v1</link><description>Swarm robotics explores the coordination of multiple robots to achievecollective goals, with collective decision-making being a central focus. Thisprocess involves decentralized robots autonomously making local decisions andcommunicating them, which influences the overall emergent behavior. Testingsuch decentralized algorithms in real-world scenarios with hundreds or morerobots is often impractical, underscoring the need for effective simulationtools. We propose SPACE (Swarm Planning and Control Evaluation), a Python-basedsimulator designed to support the research, evaluation, and comparison ofdecentralized Multi-Robot Task Allocation (MRTA) algorithms. SPACE streamlinescore algorithmic development by allowing users to implement decision-makingalgorithms as Python plug-ins, easily construct agent behavior trees via anintuitive GUI, and leverage built-in support for inter-agent communication andlocal task awareness. To demonstrate its practical utility, we implement andevaluate CBBA and GRAPE within the simulator, comparing their performanceacross different metrics, particularly in scenarios with dynamically introducedtasks. This evaluation shows the usefulness of SPACE in conducting rigorous andstandardized comparisons of MRTA algorithms, helping to support future researchin the field.</description><author>Inmo Jang</author><pubDate>Fri, 06 Sep 2024 12:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04230v1</guid></item><item><title>Probabilistic Matching of Real and Generated Data Statistics in Generative Adversarial Networks</title><link>http://arxiv.org/abs/2306.10943v3</link><description>Generative adversarial networks constitute a powerful approach to generativemodeling. While generated samples often are indistinguishable from real data,there is no guarantee that they will follow the true data distribution. Forscientific applications in particular, it is essential that the truedistribution is well captured by the generated distribution. In this work, wepropose a method to ensure that the distributions of certain generated datastatistics coincide with the respective distributions of the real data. Inorder to achieve this, we add a new loss term to the generator loss function,which quantifies the difference between these distributions via suitablef-divergences. Kernel density estimation is employed to obtain representationsof the true distributions, and to estimate the corresponding generateddistributions from minibatch values at each iteration. When compared to othermethods, our approach has the advantage that the complete shapes of thedistributions are taken into account. We evaluate the method on a syntheticdataset and a real-world dataset and demonstrate improved performance of ourapproach.</description><author>Philipp Pilar, Niklas Wahlström</author><pubDate>Fri, 06 Sep 2024 12:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10943v3</guid></item><item><title>Enhancing Biomedical Knowledge Discovery for Diseases: An Open-Source Framework Applied on Rett Syndrome and Alzheimer's Disease</title><link>http://arxiv.org/abs/2407.13492v2</link><description>The ever-growing volume of biomedical publications creates a critical needfor efficient knowledge discovery. In this context, we introduce an open-sourceend-to-end framework designed to construct knowledge around specific diseasesdirectly from raw text. To facilitate research in disease-related knowledgediscovery, we create two annotated datasets focused on Rett syndrome andAlzheimer's disease, enabling the identification of semantic relations betweenbiomedical entities. Extensive benchmarking explores various ways to representrelations and entity representations, offering insights into optimal modelingstrategies for semantic relation detection and highlighting language models'competence in knowledge discovery. We also conduct probing experiments usingdifferent layer representations and attention scores to explore transformers'ability to capture semantic relations.</description><author>Christos Theodoropoulos, Andrei Catalin Coman, James Henderson, Marie-Francine Moens</author><pubDate>Fri, 06 Sep 2024 12:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13492v2</guid></item><item><title>Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework</title><link>http://arxiv.org/abs/2409.04224v1</link><description>Multi-organ diseases present significant challenges due to their simultaneousimpact on multiple organ systems, necessitating complex and adaptive treatmentstrategies. Despite recent advancements in AI-powered healthcare decisionsupport systems, existing solutions are limited to individual organ systems.They often ignore the intricate dependencies between organ system and therebyfails to provide holistic treatment recommendations that are useful inpractice. We propose a novel hierarchical multi-agent reinforcement learning(HMARL) framework to address these challenges. This framework uses dedicatedagents for each organ system, and model dynamic through explicit inter-agentcommunication channels, enabling coordinated treatment strategies acrossorgans. Furthermore, we introduce a dual-layer state representation techniqueto contextualize patient conditions at various hierarchical levels, enhancingthe treatment accuracy and relevance. Through extensive qualitative andquantitative evaluations in managing sepsis (a complex multi-organ disease),our approach demonstrates its ability to learn effective treatment policiesthat significantly improve patient survival rates. This framework marks asubstantial advancement in clinical decision support systems, pioneering acomprehensive approach for multi-organ treatment recommendations.</description><author>Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng</author><pubDate>Fri, 06 Sep 2024 12:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04224v1</guid></item><item><title>MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox Detection</title><link>http://arxiv.org/abs/2409.04218v1</link><description>Due to the lack of effective mpox detection tools, the mpox virus continuesto spread worldwide and has once again been declared a public health emergencyof international concern by the World Health Organization. Deep learning-basedmpox detection tools are crucial to alleviate mpox outbreak. However, existingmethods have difficulty in achieving a good trade-off between detectionperformance, parameter size, and model complexity, which is crucial forpractical applications and widespread deployment, especially inresource-limited scenarios. Given that the success of Mamba in modelinglong-range dependencies and its linear complexity, we proposed a lightweighthybrid architecture called MpoxMamba. MpoxMamba utilizes deep separableconvolutions to extract local feature representations in mpox skin lesions, andgreatly enhances the model's ability to model the global contextual informationby grouped Mamba modules. Experimental results on two widely recognized mpoxdatasets demonstrate that MpoxMamba outperforms existing mpox detection methodsand state-of-the-art lightweight models. We also developed a web-based onlineapplication to provide free mpox detection services to the public in theepidemic areas (http://5227i971s5.goho.co:30290). The source codes of MpoxMambaare available at https://github.com/YubiaoYue/MpoxMamba.</description><author>Yubiao Yue, Jun Xue, Haihuang Liang, Zhenzhang Li, Yufeng Wang</author><pubDate>Fri, 06 Sep 2024 12:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04218v1</guid></item><item><title>On The Expressivity of Recurrent Neural Cascades</title><link>http://arxiv.org/abs/2312.09048v2</link><description>Recurrent Neural Cascades (RNCs) are the recurrent neural networks with nocyclic dependencies among recurrent neurons. This class of recurrent networkshas received a lot of attention in practice. Besides training methods for afixed architecture such as backpropagation, the cascade architecture naturallyallows for constructive learning methods, where recurrent nodes are addedincrementally one at a time, often yielding smaller networks. Furthermore,acyclicity amounts to a structural prior that even for the same number ofneurons yields a more favourable sample complexity compared to afully-connected architecture. A central question is whether the advantages ofthe cascade architecture come at the cost of a reduced expressivity. We providenew insights into this question. We show that the regular languages captured byRNCs with sign and tanh activation with positive recurrent weights are thestar-free regular languages. In order to establish our results we developed anovel framework where capabilities of RNCs are accessed by analysing whichsemigroups and groups a single neuron is able to implement. A notableimplication of our framework is that RNCs can achieve the expressivity of allregular languages by introducing neurons that can implement groups.</description><author>Nadezda Alexandrovna Knorozova, Alessandro Ronca</author><pubDate>Fri, 06 Sep 2024 12:13:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09048v2</guid></item><item><title>The Transformation Logics</title><link>http://arxiv.org/abs/2304.09639v3</link><description>We introduce a new family of temporal logics designed to finely balance thetrade-off between expressivity and complexity. Their key feature is thepossibility of defining operators of a new kind that we call transformationoperators. Some of them subsume existing temporal operators, while others areentirely novel. Of particular interest are transformation operators based onsemigroups. They enable logics to harness the richness of semigroup theory, andwe show them to yield logics capable of creating hierarchies of increasingexpressivity and complexity which are non-trivial to characterise in existinglogics. The result is a genuinely novel and yet unexplored landscape oftemporal logics, each of them with the potential of matching the trade-offbetween expressivity and complexity required by specific applications.</description><author>Alessandro Ronca</author><pubDate>Fri, 06 Sep 2024 12:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09639v3</guid></item><item><title>Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver</title><link>http://arxiv.org/abs/2409.04214v1</link><description>Mathematical reasoning remains an ongoing challenge for AI models, especiallyfor geometry problems that require both linguistic and visual signals. As thevision encoders of most MLLMs are trained on natural scenes, they oftenstruggle to understand geometric diagrams, performing no better in geometryproblem solving than LLMs that only process text. This limitation is amplifiedby the lack of effective methods for representing geometric relationships. Toaddress these issues, we introduce the Diagram Formalization Enhanced GeometryProblem Solver (DFE-GPS), a new framework that integrates visual features,geometric formal language, and natural language representations. We propose anovel synthetic data approach and create a large-scale geometric dataset,SynthGeo228K, annotated with both formal and natural language captions,designed to enhance the vision encoder for a better understanding of geometricstructures. Our framework improves MLLMs' ability to process geometric diagramsand extends their application to open-ended tasks on the formalgeo7k dataset.</description><author>Zeren Zhang, Jo-Ku Cheng, Jingyang Deng, Lu Tian, Jinwen Ma, Ziran Qin, Xiaokai Zhang, Na Zhu, Tuo Leng</author><pubDate>Fri, 06 Sep 2024 12:11:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04214v1</guid></item><item><title>TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection</title><link>http://arxiv.org/abs/2403.08108v2</link><description>Task-oriented object detection aims to find objects suitable foraccomplishing specific tasks. As a challenging task, it requires simultaneousvisual data processing and reasoning under ambiguous semantics. Recentsolutions are mainly all-in-one models. However, the object detection backbonesare pre-trained without text supervision. Thus, to incorporate taskrequirements, their intricate models undergo extensive learning on a highlyimbalanced and scarce dataset, resulting in capped performance, laborioustraining, and poor generalizability. In contrast, we propose TaskCLIP, a morenatural two-stage design composed of general object detection and task-guidedobject selection. Particularly for the latter, we resort to the recentlysuccessful large Vision-Language Models (VLMs) as our backbone, which providesrich semantic knowledge and a uniform embedding space for images and texts.Nevertheless, the naive application of VLMs leads to sub-optimal quality, dueto the misalignment between embeddings of object images and their visualattributes, which are mainly adjective phrases. To this end, we design atransformer-based aligner after the pre-trained VLMs to re-calibrate bothembeddings. Finally, we employ a trainable score function to post-process theVLM matching results for object selection. Experimental results demonstratethat our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by3.5% and only requires a single NVIDIA RTX 4090 for both training andinference.</description><author>Hanning Chen, Wenjun Huang, Yang Ni, Sanggeon Yun, Yezi Liu, Fei Wen, Alvaro Velasquez, Hugo Latapie, Mohsen Imani</author><pubDate>Fri, 06 Sep 2024 12:10:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08108v2</guid></item><item><title>Video alignment using unsupervised learning of local and global features</title><link>http://arxiv.org/abs/2304.06841v3</link><description>In this paper, we tackle the problem of video alignment, the process ofmatching the frames of a pair of videos containing similar actions. The mainchallenge in video alignment is that accurate correspondence should beestablished despite the differences in the execution processes and appearancesbetween the two videos. We introduce an unsupervised method for alignment thatuses global and local features of the frames. In particular, we introduceeffective features for each video frame by means of three machine vision tools:person detection, pose estimation, and VGG network. Then the features areprocessed and combined to construct a multidimensional time series thatrepresent the video. The resulting time series are used to align videos of thesame actions using a novel version of dynamic time warping named DiagonalizedDynamic Time Warping(DDTW). The main advantage of our approach is that notraining is required, which makes it applicable for any new type of actionwithout any need to collect training samples for it. Additionally, our approachcan be used for framewise labeling of action phases in a dataset with only afew labeled videos. For evaluation, we considered video synchronization andphase classification tasks on the Penn action and subset of UCF101 datasets.Also, for an effective evaluation of the video synchronization task, we presenta new metric called Enclosed Area Error(EAE). The results show that our methodoutperforms previous state-of-the-art methods, such as TCC, and otherself-supervised and weakly supervised methods.</description><author>Niloufar Fakhfour, Mohammad ShahverdiKondori, Sajjad Hashembeiki, Mohammadjavad Norouzi, Hoda Mohammadzade</author><pubDate>Fri, 06 Sep 2024 12:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06841v3</guid></item></channel></rss>