<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 09 May 2023 06:00:57 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>RelPose++: Recovering 6D Poses from Sparse-view Observations</title><link>http://arxiv.org/abs/2305.04926v1</link><description>We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.</description><author>Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Mon, 08 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04926v1</guid></item><item><title>PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds</title><link>http://arxiv.org/abs/2305.04925v1</link><description>In order to deal with the sparse and unstructured raw point clouds, LiDARbased 3D object detection research mostly focuses on designing dedicated localpoint aggregators for fine-grained geometrical modeling. In this paper, werevisit the local point aggregators from the perspective of allocatingcomputational resources. We find that the simplest pillar based models performsurprisingly well considering both accuracy and latency. Additionally, we showthat minimal adaptions from the success of 2D object detection, such asenlarging receptive field, significantly boost the performance. Extensiveexperiments reveal that our pillar based networks with modernized designs interms of architecture and training render the state-of-the-art performance onthe two popular benchmarks: Waymo Open Dataset and nuScenes. Our resultschallenge the common intuition that the detailed geometry modeling is essentialto achieve high performance for 3D object detection.</description><author>Jinyu Li, Chenxu Luo, Xiaodong Yang</author><pubDate>Mon, 08 May 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04925v1</guid></item><item><title>Optimal Layout Synthesis for Quantum Circuits as Classical Planning</title><link>http://arxiv.org/abs/2304.12014v2</link><description>In Layout Synthesis, the logical qubits of a quantum circuit are mapped tothe physical qubits of a given quantum hardware platform, taking into accountthe connectivity of physical qubits. This involves inserting SWAP gates beforean operation is applied on distant qubits. Optimal Layout Synthesis is crucialfor practical Quantum Computing on current error-prone hardware: Minimizing thenumber of SWAP gates directly mitigates the error rates when running quantumcircuits. In recent years, several approaches have been proposed for minimizing therequired SWAP insertions. The proposed exact approaches can only scale to asmall number of qubits. Proving that a number of swap insertions is optimal ismuch harder than producing near optimal mappings. In this paper, we provide two encodings for Optimal Layout Synthesis as aclassical planning problem. We use optimal classical planners to synthesize theoptimal layout for a standard set of benchmarks. Our results show thescalability of our approach compared to previous leading approaches. We canoptimally map circuits with 9 qubits onto a 14 qubit platform, which could notbe handled before by exact methods.</description><author>Irfansha Shaik, Jaco van de Pol</author><pubDate>Mon, 08 May 2023 18:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12014v2</guid></item><item><title>Learning to Evaluate the Artness of AI-generated Images</title><link>http://arxiv.org/abs/2305.04923v1</link><description>Assessing the artness of AI-generated images continues to be a challengewithin the realm of image generation. Most existing metrics cannot be used toperform instance-level and reference-free artness evaluation. This paperpresents ArtScore, a metric designed to evaluate the degree to which an imageresembles authentic artworks by artists (or conversely photographs), therebyoffering a novel approach to artness assessment. We first blend pre-trainedmodels for photo and artwork generation, resulting in a series of mixed models.Subsequently, we utilize these mixed models to generate images exhibitingvarying degrees of artness with pseudo-annotations. Each photorealistic imagehas a corresponding artistic counterpart and a series of interpolated imagesthat range from realistic to artistic. This dataset is then employed to train aneural network that learns to estimate quantized artness levels of arbitraryimages. Extensive experiments reveal that the artness levels predicted byArtScore align more closely with human artistic evaluation than existingevaluation metrics, such as Gram loss and ArtFID.</description><author>Junyu Chen, Jie An, Hanjia Lyu, Jiebo Luo</author><pubDate>Mon, 08 May 2023 18:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04923v1</guid></item><item><title>Causal Reasoning and Large Language Models: Opening a New Frontier for Causality</title><link>http://arxiv.org/abs/2305.00050v2</link><description>The causal capabilities of large language models (LLMs) is a matter ofsignificant debate, with critical implications for the use of LLMs insocietally impactful domains such as medicine, science, law, and policy. Wefurther our understanding of LLMs and their causal implications, consideringthe distinctions between different types of causal reasoning tasks, as well asthe entangled threats of construct and measurement validity. LLM-based methodsestablish new state-of-the-art accuracies on multiple causal benchmarks.Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwisecausal discovery task (97%, 13 points gain), counterfactual reasoning task(92%, 20 points gain), and actual causality (86% accuracy in determiningnecessary and sufficient causes in vignettes). At the same time, LLMs exhibitunpredictable failure modes and we provide some techniques to interpret theirrobustness. Crucially, LLMs perform these causal tasks while relying on sources ofknowledge and methods distinct from and complementary to non-LLM basedapproaches. Specifically, LLMs bring capabilities so far understood to berestricted to humans, such as using collected knowledge to generate causalgraphs or identifying background causal context from natural language. Weenvision LLMs to be used alongside existing causal methods, as a proxy forhuman domain knowledge and to reduce human effort in setting up a causalanalysis, one of the biggest impediments to the widespread adoption of causalmethods. We also see existing causal methods as promising tools for LLMs toformalize, validate, and communicate their reasoning especially in high-stakesscenarios. In capturing common sense and domain knowledge about causal mechanisms andsupporting translation between natural language and formal methods, LLMs opennew frontiers for advancing the research, practice, and adoption of causality.</description><author>Emre Kıcıman, Robert Ness, Amit Sharma, Chenhao Tan</author><pubDate>Mon, 08 May 2023 18:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00050v2</guid></item><item><title>On User-Level Private Convex Optimization</title><link>http://arxiv.org/abs/2305.04912v1</link><description>We introduce a new mechanism for stochastic convex optimization (SCO) withuser-level differential privacy guarantees. The convergence rates of thismechanism are similar to those in the prior work of Levy et al. (2021);Narayanan et al. (2022), but with two important improvements. Our mechanismdoes not require any smoothness assumptions on the loss. Furthermore, ourbounds are also the first where the minimum number of users needed foruser-level privacy has no dependence on the dimension and only a logarithmicdependence on the desired excess error. The main idea underlying the newmechanism is to show that the optimizers of strongly convex losses have lowlocal deletion sensitivity, along with an output perturbation method forfunctions with low local deletion sensitivity, which could be of independentinterest.</description><author>Badih Ghazi, Pritish Kamath, Ravi Kumar, Raghu Meka, Pasin Manurangsi, Chiyuan Zhang</author><pubDate>Mon, 08 May 2023 18:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04912v1</guid></item><item><title>What Do Patients Say About Their Disease Symptoms? Deep Multilabel Text Classification With Human-in-the-Loop Curation for Automatic Labeling of Patient Self Reports of Problems</title><link>http://arxiv.org/abs/2305.04905v1</link><description>The USA Food and Drug Administration has accorded increasing importance topatient-reported problems in clinical and research settings. In this paper, weexplore one of the largest online datasets comprising 170,141 open-endedself-reported responses (called "verbatims") from patients with Parkinson's(PwPs) to questions about what bothers them about their Parkinson's Disease andhow it affects their daily functioning, also known as the Parkinson's DiseasePatient Report of Problems. Classifying such verbatims into multiple clinicallyrelevant symptom categories is an important problem and requires multiple steps- expert curation, a multi-label text classification (MLTC) approach and largeamounts of labelled training data. Further, human annotation of such largedatasets is tedious and expensive. We present a novel solution to this problemwhere we build a baseline dataset using 2,341 (of the 170,141) verbatimsannotated by nine curators including clinical experts and PwPs. We develop arules based linguistic-dictionary using NLP techniques and graph database-basedexpert phrase-query system to scale the annotation to the remaining cohortgenerating the machine annotated dataset, and finally build a Keras-Tensorflowbased MLTC model for both datasets. The machine annotated model significantlyoutperforms the baseline model with a F1-score of 95% across 65 symptomcategories on a held-out test set.</description><author>Lakshmi Arbatti, Abhishek Hosamath, Vikram Ramanarayanan, Ira Shoulson</author><pubDate>Mon, 08 May 2023 18:42:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04905v1</guid></item><item><title>Regenerative Particle Thompson Sampling</title><link>http://arxiv.org/abs/2203.08082v2</link><description>This paper proposes regenerative particle Thompson sampling (RPTS), aflexible variation of Thompson sampling. Thompson sampling itself is a Bayesianheuristic for solving stochastic bandit problems, but it is hard to implementin practice due to the intractability of maintaining a continuous posteriordistribution. Particle Thompson sampling (PTS) is an approximation of Thompsonsampling obtained by simply replacing the continuous distribution by a discretedistribution supported at a set of weighted static particles. We observe thatin PTS, the weights of all but a few fit particles converge to zero. RPTS isbased on the heuristic: delete the decaying unfit particles and regenerate newparticles in the vicinity of fit surviving particles. Empirical evidence showsuniform improvement from PTS to RPTS and flexibility and efficacy of RPTSacross a set of representative bandit problems, including an application to 5Gnetwork slicing.</description><author>Zeyu Zhou, Bruce Hajek, Nakjung Choi, Anwar Walid</author><pubDate>Mon, 08 May 2023 18:27:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.08082v2</guid></item><item><title>A Large and Diverse Arabic Corpus for Language Modeling</title><link>http://arxiv.org/abs/2201.09227v3</link><description>Language models (LMs) have introduced a major paradigm shift in NaturalLanguage Processing (NLP) modeling where large pre-trained LMs became integralto most of the NLP tasks. The LMs are intelligent enough to find useful andrelevant representations of the language without any supervision. Perhaps,these models are used to fine-tune typical NLP tasks with significantly highaccuracy as compared to the traditional approaches. Conversely, the training ofthese models requires a massively large corpus that is a good representation ofthe language. English LMs generally perform better than their other languagecounterparts, due to the availability of massive English corpora. This workelaborates on the design and development of a large Arabic corpus. It consistsof over 500 GB of Arabic cleaned text targeted at improving cross-domainknowledge and downstream generalization capability of large-scale languagemodels. Moreover, the corpus is utilized in the training of a large Arabic LM.In order to evaluate the effectiveness of the LM, a number of typical NLP tasksare fine-tuned. The tasks demonstrate a significant boost from 4.5 to 8.5% whencompared to tasks fine-tuned on multi-lingual BERT (mBERT). To the best of myknowledge, this is currently the largest clean and diverse Arabic corpus evercollected.</description><author>Abbas Raza Ali, Muhammad Ajmal Siddiqui, Rema Algunaibet, Hasan Raza Ali</author><pubDate>Mon, 08 May 2023 18:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.09227v3</guid></item><item><title>Nonlinear Isometric Manifold Learning for Injective Normalizing Flows</title><link>http://arxiv.org/abs/2203.03934v2</link><description>To model manifold data using normalizing flows, we employ isometricautoencoders to design embeddings with explicit inverses that do not distortthe probability distribution. Using isometries separates manifold learning anddensity estimation and enables training of both parts to high accuracy. Thus,model selection and tuning are simplified compared to existing injectivenormalizing flows. Applied to data sets on (approximately) flat manifolds, thecombined approach generates high-quality data.</description><author>Eike Cramer, Felix Rauh, Alexander Mitsos, Raúl Tempone, Manuel Dahmen</author><pubDate>Mon, 08 May 2023 18:22:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.03934v2</guid></item><item><title>Language Model Analysis for Ontology Subsumption Inference</title><link>http://arxiv.org/abs/2302.06761v3</link><description>Investigating whether pre-trained language models (LMs) can function asknowledge bases (KBs) has raised wide research interests recently. However,existing works focus on simple, triple-based, relational KBs, but omit moresophisticated, logic-based, conceptualised KBs such as OWL ontologies. Toinvestigate an LM's knowledge of ontologies, we propose OntoLAMA, a set ofinference-based probing tasks and datasets from ontology subsumption axiomsinvolving both atomic and complex concepts. We conduct extensive experiments onontologies of different domains and scales, and our results demonstrate thatLMs encode relatively less background knowledge of Subsumption Inference (SI)than traditional Natural Language Inference (NLI) but can improve on SIsignificantly when a small number of samples are given. We will open-source ourcode and datasets.</description><author>Yuan He, Jiaoyan Chen, Ernesto Jiménez-Ruiz, Hang Dong, Ian Horrocks</author><pubDate>Mon, 08 May 2023 18:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06761v3</guid></item><item><title>Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting</title><link>http://arxiv.org/abs/2305.04876v1</link><description>Accurate time series forecasting is a fundamental challenge in data science.It is often affected by external covariates such as weather or humanintervention, which in many applications, may be predicted with reasonableaccuracy. We refer to them as predicted future covariates. However, existingmethods that attempt to predict time series in an iterative manner withautoregressive models end up with exponential error accumulations. Otherstrategies hat consider the past and future in the encoder and decoderrespectively limit themselves by dealing with the historical and future dataseparately. To address these limitations, a novel feature representationstrategy -- shifting -- is proposed to fuse the past data and future covariatessuch that their interactions can be considered. To extract complex dynamics intime series, we develop a parallel deep learning framework composed of RNN andCNN, both of which are used hierarchically. We also utilize the skip connectiontechnique to improve the model's performance. Extensive experiments on threedatasets reveal the effectiveness of our method. Finally, we demonstrate themodel interpretability using the Grad-CAM algorithm.</description><author>Jimeng Shi, Rukmangadh Myana, Vitalii Stebliankin, Azam Shirali, Giri Narasimhan</author><pubDate>Mon, 08 May 2023 18:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04876v1</guid></item><item><title>Gaussian process deconvolution</title><link>http://arxiv.org/abs/2305.04871v1</link><description>Let us consider the deconvolution problem, that is, to recover a latentsource $x(\cdot)$ from the observations $\y = [y_1,\ldots,y_N]$ of aconvolution process $y = x\star h + \eta$, where $\eta$ is an additive noise,the observations in $\y$ might have missing parts with respect to $y$, and thefilter $h$ could be unknown. We propose a novel strategy to address this taskwhen $x$ is a continuous-time signal: we adopt a Gaussian process (GP) prior onthe source $x$, which allows for closed-form Bayesian nonparametricdeconvolution. We first analyse the direct model to establish the conditionsunder which the model is well defined. Then, we turn to the inverse problem,where we study i) some necessary conditions under which Bayesian deconvolutionis feasible, and ii) to which extent the filter $h$ can be learnt from data orapproximated for the blind deconvolution case. The proposed approach, termedGaussian process deconvolution (GPDC) is compared to other deconvolutionmethods conceptually, via illustrative examples, and using real-world datasets.</description><author>Felipe Tobar, Arnaud Robert, Jorge F. Silva</author><pubDate>Mon, 08 May 2023 18:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04871v1</guid></item><item><title>SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding</title><link>http://arxiv.org/abs/2305.04868v1</link><description>Hand gesture serves as a crucial role during the expression of sign language.Current deep learning based methods for sign language understanding (SLU) areprone to over-fitting due to insufficient sign data resource and suffer limitedinterpretability. In this paper, we propose the first self-supervisedpre-trainable SignBERT+ framework with model-aware hand prior incorporated. Inour framework, the hand pose is regarded as a visual token, which is derivedfrom an off-the-shelf detector. Each visual token is embedded with gesturestate and spatial-temporal position encoding. To take full advantage of currentsign data resource, we first perform self-supervised learning to model itsstatistics. To this end, we design multi-level masked modeling strategies(joint, frame and clip) to mimic common failure detection cases. Jointly withthese masked modeling strategies, we incorporate model-aware hand prior tobetter capture hierarchical context over the sequence. After the pre-training,we carefully design simple yet effective prediction heads for downstream tasks.To validate the effectiveness of our framework, we perform extensiveexperiments on three main SLU tasks, involving isolated and continuous signlanguage recognition (SLR), and sign language translation (SLT). Experimentalresults demonstrate the effectiveness of our method, achieving newstate-of-the-art performance with a notable gain.</description><author>Hezhen Hu, Weichao Zhao, Wengang Zhou, Houqiang Li</author><pubDate>Mon, 08 May 2023 18:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04868v1</guid></item><item><title>Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction</title><link>http://arxiv.org/abs/2301.04791v2</link><description>Max sliced Wasserstein (Max-SW) distance has been widely known as a solutionfor less discriminative projections of sliced Wasserstein (SW) distance. Inapplications that have various independent pairs of probability measures,amortized projection optimization is utilized to predict the ``max" projectingdirections given two input measures instead of using projected gradient ascentmultiple times. Despite being efficient, Max-SW and its amortized versioncannot guarantee metricity property due to the sub-optimality of the projectedgradient ascent and the amortization gap. Therefore, we propose to replaceMax-SW with distributional sliced Wasserstein distance with von Mises-Fisher(vMF) projecting distribution (v-DSW). Since v-DSW is a metric with anynon-degenerate vMF distribution, its amortized version can guarantee themetricity when performing amortization. Furthermore, current amortized modelsare not permutation invariant and symmetric. To address the issue, we designamortized models based on self-attention architecture. In particular, we adoptefficient self-attention architectures to make the computation linear in thenumber of supports. With the two improvements, we derive self-attentionamortized distributional projection optimization and show its appealingperformance in point-cloud reconstruction and its downstream applications.</description><author>Khai Nguyen, Dang Nguyen, Nhat Ho</author><pubDate>Mon, 08 May 2023 18:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04791v2</guid></item><item><title>A Frustratingly Easy Improvement for Position Embeddings via Random Padding</title><link>http://arxiv.org/abs/2305.04859v1</link><description>Position embeddings, encoding the positional relationships among tokens intext sequences, make great contributions to modeling local context features inTransformer-based pre-trained language models. However, in Extractive QuestionAnswering, position embeddings trained with instances of varied context lengthsmay not perform well as we expect. Since the embeddings of rear positions areupdated fewer times than the front position embeddings, the rear ones may notbe properly trained. In this paper, we propose a simple but effective strategy,Random Padding, without any modifications to architectures of existingpre-trained language models. We adjust the token order of input sequences whenfine-tuning, to balance the number of updating times of every positionembedding. Experiments show that Random Padding can significantly improve modelperformance on the instances whose answers are located at rear positions,especially when models are trained on short contexts but evaluated on longcontexts. Our code and data will be released for future research.</description><author>Mingxu Tao, Yansong Feng, Dongyan Zhao</author><pubDate>Mon, 08 May 2023 18:08:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04859v1</guid></item><item><title>The Current State of Summarization</title><link>http://arxiv.org/abs/2305.04853v1</link><description>With the explosive growth of textual information, summarization systems havebecome increasingly important. This work aims at indicating the current stateof the art in abstractive text summarization concisely. As part of this, weoutline the current paradigm shifts towards pre-trained encoder-decoder modelsand large autoregressive language models. Additionally, we delve further intothe challenges of evaluating summarization systems and the potential ofinstruction-tuned models for zero-shot summarization. Finally, we provide abrief overview of how summarization systems are currently being integrated intocommercial applications.</description><author>Fabian Retkowski</author><pubDate>Mon, 08 May 2023 18:00:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04853v1</guid></item><item><title>Romanian Multiword Expression Detection Using Multilingual Adversarial Training and Lateral Inhibition</title><link>http://arxiv.org/abs/2304.11350v2</link><description>Multiword expressions are a key ingredient for developing large-scale andlinguistically sound natural language processing technology. This paperdescribes our improvements in automatically identifying Romanian multiwordexpressions on the corpus released for the PARSEME v1.2 shared task. Ourapproach assumes a multilingual perspective based on the recently introducedlateral inhibition layer and adversarial training to boost the performance ofthe employed multilingual language models. With the help of these two methods,we improve the F1-score of XLM-RoBERTa by approximately 2.7% on unseenmultiword expressions, the main task of the PARSEME 1.2 edition. In addition,our results can be considered SOTA performance, as they outperform the previousresults on Romanian obtained by the participants in this competition.</description><author>Andrei-Marius Avram, Verginica Barbu Mititelu, Dumitru-Clementin Cercel</author><pubDate>Mon, 08 May 2023 17:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11350v2</guid></item><item><title>Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective</title><link>http://arxiv.org/abs/2212.10529v2</link><description>In this work, we determined whether large language models (LLMs) arepsychologically safe. We designed unbiased prompts to systematically evaluateLLMs from a psychological perspective. First, we tested three different LLMs byusing two personality tests: Short Dark Triad (SD-3) and Big Five Inventory(BFI). All models scored higher than the human average on SD-3, suggesting arelatively darker personality pattern. Despite being instruction fine-tunedwith safety metrics to reduce toxicity, InstructGPT and FLAN-T5 still showedimplicit dark personality patterns; both models scored higher thanself-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3.Then, we evaluated the LLMs in the GPT-3 series by using well-being tests tostudy the impact of fine-tuning with more training data. We observed acontinuous increase in the well-being scores of GPT-3 and InstructGPT.Following these observations, we showed that instruction fine-tuning FLAN-T5with positive answers from BFI could effectively improve the model from apsychological perspective. On the basis of the findings, we recommended theapplication of more systematic and comprehensive psychological metrics tofurther evaluate and improve the safety of LLMs.</description><author>Xingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei Huang, Lin Qiu, Lidong Bing</author><pubDate>Mon, 08 May 2023 17:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10529v2</guid></item><item><title>Deep Signature Algorithm for Multi-dimensional Path-Dependent Options</title><link>http://arxiv.org/abs/2211.11691v2</link><description>In this work, we study the deep signature algorithms for path-dependentoptions. We extend the backward scheme in [Hur\'e-Pham-Warin. Mathematics ofComputation 89, no. 324 (2020)] for state-dependent FBSDEs with reflections topath-dependent FBSDEs with reflections, by adding the signature layer to thebackward scheme. Our algorithm applies to both European and American typeoption pricing problems while the payoff function depends on the whole paths ofthe underlying forward stock process. We prove the convergence analysis of ournumerical algorithm with explicit dependence on the truncation order of thesignature and the neural network approximation errors. Numerical examples forthe algorithm are provided including: Amerasian option under the Black-Scholesmodel, American option with a path-dependent geometric mean payoff function,and the Shiryaev's optimal stopping problem.</description><author>Erhan Bayraktar, Qi Feng, Zhaoyu Zhang</author><pubDate>Mon, 08 May 2023 17:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11691v2</guid></item><item><title>Reasoning with Language Model Prompting: A Survey</title><link>http://arxiv.org/abs/2212.09597v3</link><description>Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updatedperiodically).</description><author>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen</author><pubDate>Mon, 08 May 2023 17:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09597v3</guid></item><item><title>CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter Simulation</title><link>http://arxiv.org/abs/2305.04847v1</link><description>Simulating showers of particles in highly-granular detectors is a keyfrontier in the application of machine learning to particle physics. Achievinghigh accuracy and speed with generative machine learning models would enablethem to augment traditional simulations and alleviate a major computingconstraint. This work achieves a major breakthrough in this task by, for thefirst time, directly generating a point cloud of a few thousand space pointswith energy depositions in the detector in 3D space without relying on afixed-grid structure. This is made possible by two key innovations: i) usingrecent improvements in generative modeling we apply a diffusion model togenerate ii) an initial even higher-resolution point cloud of up to 40,000so-called Geant4 steps which is subsequently down-sampled to the desired numberof up to 6,000 space points. We showcase the performance of this approach usingthe specific example of simulating photon showers in the plannedelectromagnetic calorimeter of the International Large Detector (ILD) andachieve overall good modeling of physically relevant distributions.</description><author>Erik Buhmann, Sascha Diefenbacher, Engin Eren, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William Korcari, Katja Krüger, Peter McKeown</author><pubDate>Mon, 08 May 2023 17:44:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04847v1</guid></item><item><title>Compressed Video Quality Assessment for Super-Resolution: a Benchmark and a Quality Metric</title><link>http://arxiv.org/abs/2305.04844v1</link><description>We developed a super-resolution (SR) benchmark to analyze SR's capacity toupscale compressed videos. Our dataset employed video codecs based on fivecompression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 17state-ofthe-art SR models using our benchmark and evaluated their ability topreserve scene context and their susceptibility to compression artifacts. Toget an accurate perceptual ranking of SR models, we conducted a crowd-sourcedside-by-side comparison of their outputs. The benchmark is publicly availableathttps://videoprocessing.ai/benchmarks/super-resolutionfor-video-compression.html.We also analyzed benchmark results and developed anobjective-quality-assessment metric based on the current bestperformingobjective metrics. Our metric outperforms others, according to Spearmancorrelation with subjective scores for compressed video upscaling. It ispublicly available athttps://github.com/EvgeneyBogatyrev/super-resolution-metric.</description><author>Evgeney Bogatyrev, Ivan Molodetskikh, Dmitriy Vatolin</author><pubDate>Mon, 08 May 2023 17:42:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04844v1</guid></item><item><title>On the Fusion Strategies for Federated Decision Making</title><link>http://arxiv.org/abs/2303.06109v2</link><description>We consider the problem of information aggregation in federated decisionmaking, where a group of agents collaborate to infer the underlying state ofnature without sharing their private data with the central processor or eachother. We analyze the non-Bayesian social learning strategy in which agentsincorporate their individual observations into their opinions (i.e.,soft-decisions) with Bayes rule, and the central processor aggregates theseopinions by arithmetic or geometric averaging. Building on our previous work,we establish that both pooling strategies result in asymptotic normalitycharacterization of the system, which, for instance, can be utilized to deriveapproximate expressions for the error probability. We verify the theoreticalfindings with simulations and compare both strategies.</description><author>Mert Kayaalp, Yunus Inan, Visa Koivunen, Emre Telatar, Ali H. Sayed</author><pubDate>Mon, 08 May 2023 17:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06109v2</guid></item><item><title>Reinforcement Learning for Topic Models</title><link>http://arxiv.org/abs/2305.04843v1</link><description>We apply reinforcement learning techniques to topic modeling by replacing thevariational autoencoder in ProdLDA with a continuous action space reinforcementlearning policy. We train the system with a policy gradient algorithmREINFORCE. Additionally, we introduced several modifications: modernize theneural network architecture, weight the ELBO loss, use contextual embeddings,and monitor the learning process via computing topic diversity and coherencefor each training step. Experiments are performed on 11 data sets. Ourunsupervised model outperforms all other unsupervised models and performs onpar with or better than most models using supervised labeling. Our model isoutperformed on certain data sets by a model using supervised labeling andcontrastive learning. We have also conducted an ablation study to provideempirical evidence of performance improvements from changes we made to ProdLDAand found that the reinforcement learning formulation boosts performance.</description><author>Jeremy Costello, Marek Z. Reformat</author><pubDate>Mon, 08 May 2023 17:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04843v1</guid></item><item><title>Scalable Optimal Margin Distribution Machine</title><link>http://arxiv.org/abs/2305.04837v1</link><description>Optimal margin Distribution Machine (ODM) is a newly proposed statisticallearning framework rooting in the novel margin theory, which demonstratesbetter generalization performance than the traditional large margin basedcounterparts. Nonetheless, it suffers from the ubiquitous scalability problemregarding both computation time and memory as other kernel methods. This paperproposes a scalable ODM, which can achieve nearly ten times speedup compared tothe original ODM training method. For nonlinear kernels, we propose a noveldistribution-aware partition method to make the local ODM trained on eachpartition be close and converge fast to the global one. When linear kernel isapplied, we extend a communication efficient SVRG method to accelerate thetraining further. Extensive empirical studies validate that our proposed methodis highly computational efficient and almost never worsen the generalization.</description><author>Yilin Wang, Nan Cao, Teng Zhang, Xuanhua Shi, Hai Jin</author><pubDate>Mon, 08 May 2023 17:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04837v1</guid></item><item><title>How Do In-Context Examples Affect Compositional Generalization?</title><link>http://arxiv.org/abs/2305.04835v1</link><description>Compositional generalization--understanding unseen combinations of seenprimitives--is an essential reasoning capability in human intelligence. The AIcommunity mainly studies this capability by fine-tuning neural networks on lotsof training samples, while it is still unclear whether and how in-contextlearning--the prevailing few-shot paradigm based on large languagemodels--exhibits compositional generalization. In this paper, we present CoFe,a test suite to investigate in-context compositional generalization. We findthat the compositional generalization performance can be easily affected by theselection of in-context examples, thus raising the research question what thekey factors are to make good in-context examples for compositionalgeneralization. We study three potential factors: similarity, diversity andcomplexity. Our systematic experiments indicate that in-context examples shouldbe structurally similar to the test case, diverse from each other, andindividually simple. Furthermore, two strong limitations are observed:in-context compositional generalization on fictional words is much weaker thanthat on commonly used ones; it is still critical that the in-context examplesshould cover required linguistic structures, even though the backbone model hasbeen pre-trained on large corpus. We hope our analysis would facilitate theunderstanding and utilization of in-context learning paradigm.</description><author>Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, Dongmei Zhang</author><pubDate>Mon, 08 May 2023 17:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04835v1</guid></item><item><title>Learning-Augmented Private Algorithms for Multiple Quantile Release</title><link>http://arxiv.org/abs/2210.11222v2</link><description>When applying differential privacy to sensitive data, we can often improveperformance using external information such as other sensitive data, publicdata, or human priors. We propose to use the learning-augmented algorithms (oralgorithms with predictions) framework -- previously applied largely to improvetime complexity or competitive ratios -- as a powerful way of designing andanalyzing privacy-preserving methods that can take advantage of such externalinformation to improve utility. This idea is instantiated on the important taskof multiple quantile release, for which we derive error guarantees that scalewith a natural measure of prediction quality while (almost) recoveringstate-of-the-art prediction-independent guarantees. Our analysis enjoys severaladvantages, including minimal assumptions about the data, a natural way ofadding robustness, and the provision of useful surrogate losses for two novel``meta" algorithms that learn predictions from other (potentially sensitive)data. We conclude with experiments on challenging tasks demonstrating thatlearning predictions across one or more instances can lead to large errorreductions while preserving privacy.</description><author>Mikhail Khodak, Kareem Amin, Travis Dick, Sergei Vassilvitskii</author><pubDate>Mon, 08 May 2023 17:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11222v2</guid></item><item><title>Learning Summary-Worthy Visual Representation for Abstractive Summarization in Video</title><link>http://arxiv.org/abs/2305.04824v1</link><description>Multimodal abstractive summarization for videos (MAS) requires generating aconcise textual summary to describe the highlights of a video according tomultimodal resources, in our case, the video content and its transcript.Inspired by the success of the large-scale generative pre-trained languagemodel (GPLM) in generating high-quality textual content (e.g., summary), recentMAS methods have proposed to adapt the GPLM to this task by equipping it withthe visual information, which is often obtained through a general-purposevisual feature extractor. However, the generally extracted visual features mayoverlook some summary-worthy visual information, which impedes modelperformance. In this work, we propose a novel approach to learning thesummary-worthy visual representation that facilitates abstractivesummarization. Our method exploits the summary-worthy information from both thecross-modal transcript data and the knowledge that distills from the pseudosummary. Extensive experiments on three public multimodal datasets show thatour method outperforms all competing baselines. Furthermore, with theadvantages of summary-worthy visual information, our model can have asignificant improvement on small datasets or even datasets with limitedtraining data.</description><author>Zenan Xu, Xiaojun Meng, Yasheng Wang, Qinliang Su, Zexuan Qiu, Xin Jiang, Qun Liu</author><pubDate>Mon, 08 May 2023 17:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04824v1</guid></item><item><title>Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2305.04819v1</link><description>Policy optimization methods with function approximation are widely used inmulti-agent reinforcement learning. However, it remains elusive how to designsuch algorithms with statistical guarantees. Leveraging a multi-agentperformance difference lemma that characterizes the landscape of multi-agentpolicy optimization, we find that the localized action value function serves asan ideal descent direction for each local policy. Motivated by the observation,we present a multi-agent PPO algorithm in which the local policy of each agentis updated similarly to vanilla PPO. We prove that with standard regularityconditions on the Markov game and problem-dependent quantities, our algorithmconverges to the globally optimal policy at a sublinear rate. We extend ouralgorithm to the off-policy setting and introduce pessimism to policyevaluation, which aligns with experiments. To our knowledge, this is the firstprovably convergent multi-agent PPO algorithm in cooperative Markov games.</description><author>Yulai Zhao, Zhuoran Yang, Zhaoran Wang, Jason D. Lee</author><pubDate>Mon, 08 May 2023 17:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04819v1</guid></item><item><title>Increasing Depth of Neural Networks for Life-long Learning</title><link>http://arxiv.org/abs/2202.10821v2</link><description>Purpose: We propose a novel method for continual learning based on theincreasing depth of neural networks. This work explores whether extendingneural network depth may be beneficial in a life-long learning setting. Methods: We propose a novel approach based on adding new layers on top ofexisting ones to enable the forward transfer of knowledge and adaptingpreviously learned representations. We employ a method of determining the mostsimilar tasks for selecting the best location in our network to add new nodeswith trainable parameters. This approach allows for creating a tree-like model,where each node is a set of neural network parameters dedicated to a specifictask. The Progressive Neural Network concept inspires the proposed method.Therefore, it benefits from dynamic changes in network structure. However,Progressive Neural Network allocates a lot of memory for the whole networkstructure during the learning process. The proposed method alleviates this byadding only part of a network for a new task and utilizing a subset ofpreviously trained weights. At the same time, we may retain the benefit of PNN,such as no forgetting guaranteed by design, without needing a memory buffer. Results: Experiments on Split CIFAR and Split Tiny ImageNet show that theproposed algorithm is on par with other continual learning methods. In a morechallenging setup with a single computer vision dataset as a separate task, ourmethod outperforms Experience Replay. Conclusion: It is compatible with commonly used computer vision architecturesand does not require a custom network structure. As an adaptation to changingdata distribution is made by expanding the architecture, there is no need toutilize a rehearsal buffer. For this reason, our method could be used forsensitive applications where data privacy must be considered.</description><author>Jędrzej Kozal, Michał Woźniak</author><pubDate>Mon, 08 May 2023 17:17:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.10821v2</guid></item><item><title>One-class Damage Detector Using Deeper Fully-Convolutional Data Descriptions for Civil Application</title><link>http://arxiv.org/abs/2303.01732v3</link><description>Infrastructure managers must maintain high standards to ensure usersatisfaction during the lifecycle of infrastructures. Surveillance cameras andvisual inspections have enabled progress in automating the detection ofanomalous features and assessing the occurrence of deterioration. However,collecting damage data is typically time consuming and requires repeatedinspections. The one-class damage detection approach has an advantage in thatnormal images can be used to optimize model parameters. Additionally, visualevaluation of heatmaps enables us to understand localized anomalous features.The authors highlight damage vision applications utilized in the robustproperty and localized damage explainability. First, we propose a civil-purposeapplication for automating one-class damage detection reproducing a fullyconvolutional data description (FCDD) as a baseline model. We have obtainedaccurate and explainable results demonstrating experimental studies on concretedamage and steel corrosion in civil engineering. Additionally, to develop amore robust application, we applied our method to another outdoor domain thatcontains complex and noisy backgrounds using natural disaster datasetscollected using various devices. Furthermore, we propose a valuable solution ofdeeper FCDDs focusing on other powerful backbones to improve the performance ofdamage detection and implement ablation studies on disaster datasets. The keyresults indicate that the deeper FCDDs outperformed the baseline FCDD ondatasets representing natural disaster damage caused by hurricanes, typhoons,earthquakes, and four-event disasters.</description><author>Takato Yasuno, Masahiro Okano, Junichiro Fujii</author><pubDate>Mon, 08 May 2023 17:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01732v3</guid></item><item><title>A Drop of Ink may Make a Million Think: The Spread of False Information in Large Language Models</title><link>http://arxiv.org/abs/2305.04812v1</link><description>Large language models (LLMs) like ChatGPT have gained increasing prominencein artificial intelligence, making a profound impact on society and variousindustries like business and science. However, the presence of falseinformation on the internet and in text corpus poses a significant risk to thereliability and safety of LLMs, underscoring the urgent need to understand themechanisms of how false information impacts and spreads in LLMs. In this paper,we investigate how false information spreads in LLMs and affects relatedresponses by conducting a series of experiments on the effects of sourceauthority, injection paradigm, and information relevance. Specifically, wecompare four authority levels of information sources (Twitter, web blogs, newsreports, and research papers), two common knowledge injection paradigms(in-context injection and learning-based injection), and three degrees ofinformation relevance (direct, indirect, and peripheral). The experimentalresults show that (1) False information will spread and contaminate relatedmemories in LLMs via a semantic diffusion process, i.e., false information hasglobal detrimental effects beyond its direct impact. (2) Current LLMs aresusceptible to authority bias, i.e., LLMs are more likely to follow falseinformation presented in a trustworthy style like news or research papers,which usually causes deeper and wider pollution of information. (3) CurrentLLMs are more sensitive to false information through in-context injection thanthrough learning-based injection, which severely challenges the reliability andsafety of LLMs even if all training data are trusty and correct. The abovefindings raise the need for new false information defense algorithms to addressthe global impact of false information, and new alignment algorithms tounbiasedly lead LLMs to follow internal human values rather than superficialpatterns.</description><author>Ning Bian, Peilin Liu, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, Le Sun</author><pubDate>Mon, 08 May 2023 17:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04812v1</guid></item><item><title>CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning</title><link>http://arxiv.org/abs/2305.04808v1</link><description>Commonsense reasoning, aiming at endowing machines with a human-like abilityto make situational presumptions, is extremely challenging to generalize. Forsomeone who barely knows about "meditation," while is knowledgeable about"singing," he can still infer that "meditation makes people relaxed" from theexisting knowledge that "singing makes people relaxed" by first conceptualizing"singing" as a "relaxing event" and then instantiating that event to"meditation." This process, known as conceptual induction and deduction, isfundamental to commonsense reasoning while lacking both labeled data andmethodologies to enhance commonsense modeling. To fill such a research gap, wepropose CAT (Contextualized ConceptuAlization and InsTantiation), asemi-supervised learning framework that integrates event conceptualization andinstantiation to conceptualize commonsense knowledge bases at scale. Extensiveexperiments show that our framework achieves state-of-the-art performances ontwo conceptualization tasks, and the acquired abstract commonsense knowledgecan significantly improve commonsense inference modeling. Our code, data, andfine-tuned models are publicly available athttps://github.com/HKUST-KnowComp/CAT.</description><author>Weiqi Wang, Tianqing Fang, Baixuan Xu, Chun Yi Louis Bo, Yangqiu Song, Lei Chen</author><pubDate>Mon, 08 May 2023 17:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04808v1</guid></item><item><title>Dual PatchNorm</title><link>http://arxiv.org/abs/2302.01327v3</link><description>We propose Dual PatchNorm: two Layer Normalization layers (LayerNorms),before and after the patch embedding layer in Vision Transformers. Wedemonstrate that Dual PatchNorm outperforms the result of exhaustive search foralternative LayerNorm placement strategies in the Transformer block itself. Inour experiments, incorporating this trivial modification, often leads toimproved accuracy over well-tuned Vision Transformers and never hurts.</description><author>Manoj Kumar, Mostafa Dehghani, Neil Houlsby</author><pubDate>Mon, 08 May 2023 17:06:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01327v3</guid></item><item><title>Weighted First Order Model Counting with Directed Acyclic Graph Axioms</title><link>http://arxiv.org/abs/2302.09830v2</link><description>Statistical Relational Learning (SRL) integrates First-Order Logic (FOL) andprobability theory for learning and inference over relational data.Probabilistic inference and learning in many SRL models can be reduced toWeighted First Order Model Counting (WFOMC). However, WFOMC is known to beintractable ($\mathrm{\#P_1-}$ complete). Hence, logical fragments that admitpolynomial time WFOMC are of significant interest. Such fragments are calleddomain liftable. Recent line of works have shown the two-variable fragment ofFOL, extended with counting quantifiers ($\mathrm{C^2}$) to be domain-liftable.However, many properties of real-world data can not be modelled in$\mathrm{C^2}$. In fact many ubiquitous properties of real-world data areinexressible in FOL. Acyclicity is one such property, found in citationnetworks, genealogy data, temporal data e.t.c. In this paper we aim to addressthis problem by investigating the domain liftability of directed acyclicityconstraints. We show that the fragment $\mathrm{C^2}$ with a Directed AcyclicGraph (DAG) axiom, i.e., a predicate in the language is axiomatized torepresent a DAG, is domain-liftable. We present a method based on principle ofinclusion-exclusion for WFOMC of $\mathrm{C^2}$ formulas extended with DAGaxioms.</description><author>Sagar Malhotra, Luciano Serafini</author><pubDate>Mon, 08 May 2023 17:01:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09830v2</guid></item><item><title>Weighting-Based Treatment Effect Estimation via Distribution Learning</title><link>http://arxiv.org/abs/2012.13805v4</link><description>Existing weighting methods for treatment effect estimation are often builtupon the idea of propensity scores or covariate balance. They usually imposestrong assumptions on treatment assignment or outcome model to obtain unbiasedestimation, such as linearity or specific functional forms, which easily leadsto the major drawback of model mis-specification. In this paper, we aim toalleviate these issues by developing a distribution learning-based weightingmethod. We first learn the true underlying distribution of covariatesconditioned on treatment assignment, then leverage the ratio of covariates'density in the treatment group to that of the control group as the weight forestimating treatment effects. Specifically, we propose to approximate thedistribution of covariates in both treatment and control groups throughinvertible transformations via change of variables. To demonstrate thesuperiority, robustness, and generalizability of our method, we conductextensive experiments using synthetic and real data. From the experimentresults, we find that our method for estimating average treatment effect ontreated (ATT) with observational data outperforms several cutting-edgeweighting-only benchmarking methods, and it maintains its advantage under adoubly-robust estimation framework that combines weighting with some advancedoutcome modeling methods.</description><author>Dongcheng Zhang, Kunpeng Zhang</author><pubDate>Mon, 08 May 2023 16:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.13805v4</guid></item><item><title>FS-BAN: Born-Again Networks for Domain Generalization Few-Shot Classification</title><link>http://arxiv.org/abs/2208.10930v4</link><description>Conventional Few-shot classification (FSC) aims to recognize samples fromnovel classes given limited labeled data. Recently, domain generalization FSC(DG-FSC) has been proposed with the goal to recognize novel class samples fromunseen domains. DG-FSC poses considerable challenges to many models due to thedomain shift between base classes (used in training) and novel classes(encountered in evaluation). In this work, we make two novel contributions totackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)episodic training and comprehensively investigate its effectiveness for DG-FSC.As a specific form of knowledge distillation, BAN has been shown to achieveimproved generalization in conventional supervised classification with aclosed-set setup. This improved generalization motivates us to study BAN forDG-FSC, and we show that BAN is promising to address the domain shiftencountered in DG-FSC. Building on the encouraging findings, our second (major)contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach forDG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, eachof these is specifically designed to overcome central and unique challenges inDG-FSC, namely overfitting and domain discrepancy. We analyze different designchoices of these techniques. We conduct comprehensive quantitative andqualitative analysis and evaluation over six datasets and three baselinemodels. The results suggest that our proposed FS-BAN consistently improves thegeneralization performance of baseline models and achieves state-of-the-artaccuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.</description><author>Yunqing Zhao, Ngai-Man Cheung</author><pubDate>Mon, 08 May 2023 16:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10930v4</guid></item><item><title>Geolocation Predicting of Tweets Using BERT-Based Models</title><link>http://arxiv.org/abs/2303.07865v2</link><description>This research is aimed to solve the tweet/user geolocation prediction taskand provide a flexible methodology for the geotagging of textual big data. Thesuggested approach implements neural networks for natural language processing(NLP) to estimate the location as coordinate pairs (longitude, latitude) andtwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed modelshas been finetuned on a Twitter dataset using pretrained Bidirectional EncoderRepresentations from Transformers (BERT) as base models. Performance metricsshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15km on the US-level datasets for the models trained and evaluated on textfeatures of tweets' content and metadata context. Our source code and data areavailable at https://github.com/K4TEL/geo-twitter.git</description><author>Kateryna Lutsai, Christoph H. Lampert</author><pubDate>Mon, 08 May 2023 16:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07865v2</guid></item><item><title>Mlinear: Rethink the Linear Model for Time-series Forecasting</title><link>http://arxiv.org/abs/2305.04800v1</link><description>Recently, significant advancements have been made in time-series forecastingresearch, with an increasing focus on analyzing the inherent characteristics oftime-series data, rather than solely focusing on designing forecastingmodels.In this paper, we follow this trend and carefully examine previous workto propose an efficient time series forecasting model based on linear models.The model consists of two important core components: (1) the integration ofdifferent semantics brought by single-channel and multi-channel data for jointforecasting; (2) the use of a novel loss function that replaces the traditionalMSE loss and MAE loss to achieve higher forecasting accuracy.On widely-usedbenchmark time series datasets, our model not only outperforms the currentSOTA, but is also 10 $\times$ speedup and has fewer parameters than the latestSOTA model.</description><author>Jianing Chen, Chuhao Chen, Xiangxu Meng</author><pubDate>Mon, 08 May 2023 16:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04800v1</guid></item><item><title>Global Update Tracking: A Decentralized Learning Algorithm for Heterogeneous Data</title><link>http://arxiv.org/abs/2305.04792v1</link><description>Decentralized learning enables the training of deep learning models overlarge distributed datasets generated at different locations, without the needfor a central server. However, in practical scenarios, the data distributionacross these devices can be significantly different, leading to a degradationin model performance. In this paper, we focus on designing a decentralizedlearning algorithm that is less susceptible to variations in data distributionacross devices. We propose Global Update Tracking (GUT), a novel tracking-basedmethod that aims to mitigate the impact of heterogeneous data in decentralizedlearning without introducing any communication overhead. We demonstrate theeffectiveness of the proposed technique through an exhaustive set ofexperiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, FashionMNIST, and ImageNette), model architectures, and network topologies. Ourexperiments show that the proposed method achieves state-of-the-art performancefor decentralized learning on heterogeneous data via a $1-6\%$ improvement intest accuracy compared to other existing techniques.</description><author>Sai Aparna Aketi, Abolfazl Hashemi, Kaushik Roy</author><pubDate>Mon, 08 May 2023 16:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04792v1</guid></item><item><title>MultiModal-GPT: A Vision and Language Model for Dialogue with Humans</title><link>http://arxiv.org/abs/2305.04790v1</link><description>We present a vision and language model named MultiModal-GPT to conductmulti-round dialogue with humans. MultiModal-GPT can follow variousinstructions from humans, such as generating a detailed caption, counting thenumber of interested objects, and answering general questions from users.MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, withLow-rank Adapter (LoRA) added both in the cross-attention part and theself-attention part of the language model. We first construct instructiontemplates with vision and language data for multi-modality instruction tuningto make the model understand and follow human instructions. We find the qualityof training data is vital for the dialogue performance, where few datacontaining short answers can lead the model to respond shortly to anyinstructions. To further enhance the ability to chat with humans of theMultiModal-GPT, we utilize language-only instruction-following data to trainthe MultiModal-GPT jointly. The joint training of language-only andvisual-language instructions with the \emph{same} instruction templateeffectively improves dialogue performance. Various demos show the ability ofcontinuous dialogue of MultiModal-GPT with humans. Code and demo are athttps://github.com/open-mmlab/Multimodal-GPT</description><author>Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen</author><pubDate>Mon, 08 May 2023 16:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04790v1</guid></item><item><title>Using Language Models on Low-end Hardware</title><link>http://arxiv.org/abs/2305.02350v2</link><description>This paper evaluates the viability of using fixed language models fortraining text classification networks on low-end hardware. We combine languagemodels with a CNN architecture and put together a comprehensive benchmark with8 datasets covering single-label and multi-label classification of topic,sentiment, and genre. Our observations are distilled into a list of trade-offs,concluding that there are scenarios, where not fine-tuning a language modelyields competitive effectiveness at faster training, requiring only a quarterof the memory compared to fine-tuning.</description><author>Fabian Ziegner, Janos Borst, Andreas Niekler, Martin Potthast</author><pubDate>Mon, 08 May 2023 16:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02350v2</guid></item><item><title>AvatarReX: Real-time Expressive Full-body Avatars</title><link>http://arxiv.org/abs/2305.04789v1</link><description>We present AvatarReX, a new method for learning NeRF-based full-body avatarsfrom video data. The learnt avatar not only provides expressive control of thebody, hands and the face together, but also supports real-time animation andrendering. To this end, we propose a compositional avatar representation, wherethe body, hands and the face are separately modeled in a way that thestructural prior from parametric mesh templates is properly utilized withoutcompromising representation flexibility. Furthermore, we disentangle thegeometry and appearance for each part. With these technical designs, we proposea dedicated deferred rendering pipeline, which can be executed in real-timeframerate to synthesize high-quality free-view images. The disentanglement ofgeometry and appearance also allows us to design a two-pass training strategythat combines volume rendering and surface rendering for network training. Inthis way, patch-level supervision can be applied to force the network to learnsharp appearance details on the basis of geometry estimation. Overall, ourmethod enables automatic construction of expressive full-body avatars withreal-time rendering capability, and can generate photo-realistic images withdynamic details for novel body motions and facial expressions.</description><author>Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, Yebin Liu</author><pubDate>Mon, 08 May 2023 16:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04789v1</guid></item><item><title>HistAlign: Improving Context Dependency in Language Generation by Aligning with History</title><link>http://arxiv.org/abs/2305.04782v1</link><description>Language models (LMs) can generate hallucinations and incoherent outputs,which highlights their weak context dependency. Cache-LMs, which augment LMswith a memory of recent history, can increase context dependency and have shownremarkable performance in diverse language generation tasks. However, we findthat even with training, the performance gain stemming from the cache componentof current cache-LMs is suboptimal due to the misalignment between the currenthidden states and those stored in the memory. In this work, we presentHistAlign, a new training approach to ensure good cache alignment such that themodel receives useful signals from the history. We first prove our concept on asimple and synthetic task where the memory is essential for correctpredictions, and we show that the cache component of HistAlign is betteraligned and improves overall performance. Next, we evaluate HistAlign ondiverse downstream language generation tasks, including prompt continuation,abstractive summarization, and data-to-text. We demonstrate that HistAlignimproves text coherence and faithfulness in open-ended and conditionalgeneration settings respectively. HistAlign is also generalizable acrossdifferent model families, showcasing its strength in improving contextdependency of LMs in diverse scenarios. Our code is publicly available athttps://github.com/meetdavidwan/histalign</description><author>David Wan, Shiyue Zhang, Mohit Bansal</author><pubDate>Mon, 08 May 2023 16:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04782v1</guid></item><item><title>BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning</title><link>http://arxiv.org/abs/2305.04769v1</link><description>The ability of deep neural networks to continually learn and adapt to asequence of tasks has remained challenging due to catastrophic forgetting ofpreviously learned tasks. Humans, on the other hand, have a remarkable abilityto acquire, assimilate, and transfer knowledge across tasks throughout theirlifetime without catastrophic forgetting. The versatility of the brain can beattributed to the rehearsal of abstract experiences through a complementarylearning system. However, representation rehearsal in vision transformers lacksdiversity, resulting in overfitting and consequently, performance dropssignificantly compared to raw image rehearsal. Therefore, we propose BiRT, anovel representation rehearsal-based continual learning approach using visiontransformers. Specifically, we introduce constructive noises at various stagesof the vision transformer and enforce consistency in predictions with respectto an exponential moving average of the working model. Our method providesconsistent performance gain over raw image and vanilla representation rehearsalon several challenging CL benchmarks, while being memory efficient and robustto natural and adversarial corruptions.</description><author>Kishaan Jeeveswaran, Prashant Bhat, Bahram Zonooz, Elahe Arani</author><pubDate>Mon, 08 May 2023 16:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04769v1</guid></item><item><title>White-Box Multi-Objective Adversarial Attack on Dialogue Generation</title><link>http://arxiv.org/abs/2305.03655v2</link><description>Pre-trained transformers are popular in state-of-the-art dialogue generation(DG) systems. Such language models are, however, vulnerable to variousadversarial samples as studied in traditional tasks such as textclassification, which inspires our curiosity about their robustness in DGsystems. One main challenge of attacking DG models is that perturbations on thecurrent sentence can hardly degrade the response accuracy because the unchangedchat histories are also considered for decision-making. Instead of merelypursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe thatcrafting adversarial samples to force longer generation outputs benefits attackeffectiveness -- the generated responses are typically irrelevant, lengthy, andrepetitive. To this end, we propose a white-box multi-objective attack methodcalled DGSlow. Specifically, DGSlow balances two objectives -- generationaccuracy and length, via a gradient-based multi-objective optimizer and appliesan adaptive searching mechanism to iteratively craft adversarial samples withonly a few modifications. Comprehensive experiments on four benchmark datasetsdemonstrate that DGSlow could significantly degrade state-of-the-art DG modelswith a higher success rate than traditional accuracy-based methods. Besides,our crafted sentences also exhibit strong transferability in attacking othermodels.</description><author>Yufei Li, Zexin Li, Yingfan Gao, Cong Liu</author><pubDate>Mon, 08 May 2023 16:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03655v2</guid></item><item><title>OSTA: One-shot Task-adaptive Channel Selection for Semantic Segmentation of Multichannel Images</title><link>http://arxiv.org/abs/2305.04766v1</link><description>Semantic segmentation of multichannel images is a fundamental task for manyapplications. Selecting an appropriate channel combination from the originalmultichannel image can improve the accuracy of semantic segmentation and reducethe cost of data storage, processing and future acquisition. Existing channelselection methods typically use a reasonable selection procedure to determine adesirable channel combination, and then train a semantic segmentation networkusing that combination. In this study, the concept of pruning from a supernetis used for the first time to integrate the selection of channel combinationand the training of a semantic segmentation network. Based on this concept, aOne-Shot Task-Adaptive (OSTA) channel selection method is proposed for thesemantic segmentation of multichannel images. OSTA has three stages, namely thesupernet training stage, the pruning stage and the fine-tuning stage. Theoutcomes of six groups of experiments (L7Irish3C, L7Irish2C, L8Biome3C,L8Biome2C, RIT-18 and Semantic3D) demonstrated the effectiveness and efficiencyof OSTA. OSTA achieved the highest segmentation accuracies in all tests (62.49%(mIoU), 75.40% (mIoU), 68.38% (mIoU), 87.63% (mIoU), 66.53% (mA) and 70.86%(mIoU), respectively). It even exceeded the highest accuracies of exhaustivetests (61.54% (mIoU), 74.91% (mIoU), 67.94% (mIoU), 87.32% (mIoU), 65.32% (mA)and 70.27% (mIoU), respectively), where all possible channel combinations weretested. All of this can be accomplished within a predictable and relativelyefficient timeframe, ranging from 101.71% to 298.1% times the time required totrain the segmentation network alone. In addition, there were interestingfindings that were deemed valuable for several fields.</description><author>Yuanzhi Cai, Jagannath Aryal, Yuan Fang, Hong Huang, Lei Fan</author><pubDate>Mon, 08 May 2023 16:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04766v1</guid></item><item><title>Large-scale and Efficient Texture Mapping Algorithm via Loopy Belief Propagation</title><link>http://arxiv.org/abs/2305.04763v1</link><description>Texture mapping as a fundamental task in 3D modeling has been wellestablished for well-acquired aerial assets under consistent illumination, yetit remains a challenge when it is scaled to large datasets with images undervarying views and illuminations. A well-performed texture mapping algorithmmust be able to efficiently select views, fuse and map textures from theseviews to mesh models, at the same time, achieve consistent radiometry over theentire model. Existing approaches achieve efficiency either by limiting thenumber of images to one view per face, or simplifying global inferences to onlyachieve local color consistency. In this paper, we break this tie by proposinga novel and efficient texture mapping framework that allows the use of multipleviews of texture per face, at the same time to achieve global colorconsistency. The proposed method leverages a loopy belief propagation algorithmto perform an efficient and global-level probabilistic inferences to rankcandidate views per face, which enables face-level multi-view texture fusionand blending. The texture fusion algorithm, being non-parametric, bringsanother advantage over typical parametric post color correction methods, due toits improved robustness to non-linear illumination differences. The experimentson three different types of datasets (i.e. satellite dataset, unmanned-aerialvehicle dataset and close-range dataset) show that the proposed method hasproduced visually pleasant and texturally consistent results in all scenarios,with an added advantage of consuming less running time as compared to the stateof the art methods, especially for large-scale dataset such assatellite-derived models.</description><author>Xiao ling, Rongjun Qin</author><pubDate>Mon, 08 May 2023 16:11:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04763v1</guid></item><item><title>Augmented Large Language Models with Parametric Knowledge Guiding</title><link>http://arxiv.org/abs/2305.04757v1</link><description>Large Language Models (LLMs) have significantly advanced natural languageprocessing (NLP) with their impressive language understanding and generationcapabilities. However, their performance may be suboptimal for long-tail ordomain-specific tasks due to limited exposure to domain-specific knowledge andvocabulary. Additionally, the lack of transparency of most state-of-the-art(SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuningwith custom data. Moreover, data privacy is a significant concern. To addressthese challenges, we propose the novel Parametric Knowledge Guiding (PKG)framework, which equips LLMs with a knowledge-guiding module to access relevantknowledge at runtime without altering the LLMs' parameters. Our PKG is based onopen-source "white-box" small language models, allowing offline storage of anyknowledge that LLMs require. We demonstrate that our PKG framework can enhancethe performance of "black-box" LLMs on a range of long-tail and domain-specificdownstream tasks requiring factual, tabular, medical, and multimodal knowledge.</description><author>Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang</author><pubDate>Mon, 08 May 2023 16:05:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04757v1</guid></item><item><title>Robust Pose Transfer with Dynamic Details using Neural Video Rendering</title><link>http://arxiv.org/abs/2106.14132v3</link><description>Pose transfer of human videos aims to generate a high fidelity video of atarget person imitating actions of a source person. A few studies have madegreat progress either through image translation with deep latent features orneural rendering with explicit 3D features. However, both of them rely on largeamounts of training data to generate realistic results, and the performancedegrades on more accessible internet videos due to insufficient trainingframes. In this paper, we demonstrate that the dynamic details can be preservedeven trained from short monocular videos. Overall, we propose a neural videorendering framework coupled with an image-translation-based dynamic detailsgeneration network (D2G-Net), which fully utilizes both the stability ofexplicit 3D features and the capacity of learning components. To be specific, anovel texture representation is presented to encode both the static andpose-varying appearance characteristics, which is then mapped to the imagespace and rendered as a detail-rich frame in the neural rendering stage.Moreover, we introduce a concise temporal loss in the training stage tosuppress the detail flickering that is made more visible due to high-qualitydynamic details generated by our method. Through extensive comparisons, wedemonstrate that our neural human video renderer is capable of achieving bothclearer dynamic details and more robust performance even on accessible shortvideos with only 2k - 4k frames.</description><author>Yang-tian Sun, Hao-zhi Huang, Xuan Wang, Yu-kun Lai, Wei Liu, Lin Gao</author><pubDate>Mon, 08 May 2023 15:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.14132v3</guid></item><item><title>Is AUC the best measure for practical comparison of anomaly detectors?</title><link>http://arxiv.org/abs/2305.04754v1</link><description>The area under receiver operating characteristics (AUC) is the standardmeasure for comparison of anomaly detectors. Its advantage is in providing ascalar number that allows a natural ordering and is independent on a threshold,which allows to postpone the choice. In this work, we question whether AUC is agood metric for anomaly detection, or if it gives a false sense of comfort, dueto relying on assumptions which are unlikely to hold in practice. Ourinvestigation shows that variations of AUC emphasizing accuracy at low falsepositive rate seem to be better correlated with the needs of practitioners, butalso that we can compare anomaly detectors only in the case when we haverepresentative examples of anomalous samples. This last result is disturbing,as it suggests that in many cases, we should do active or few-show learninginstead of pure anomaly detection.</description><author>Vít Škvára, Tomáš Pevný, Václav Šmídl</author><pubDate>Mon, 08 May 2023 15:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04754v1</guid></item><item><title>Sense, Imagine, Act: Multimodal Perception Improves Model-Based Reinforcement Learning for Head-to-Head Autonomous Racing</title><link>http://arxiv.org/abs/2305.04750v1</link><description>Model-based reinforcement learning (MBRL) techniques have recently yieldedpromising results for real-world autonomous racing using high-dimensionalobservations. MBRL agents, such as Dreamer, solve long-horizon tasks bybuilding a world model and planning actions by latent imagination. Thisapproach involves explicitly learning a model of the system dynamics and usingit to learn the optimal policy for continuous control over multiple timesteps.As a result, MBRL agents may converge to sub-optimal policies if the worldmodel is inaccurate. To improve state estimation for autonomous racing, thispaper proposes a self-supervised sensor fusion technique that combinesegocentric LiDAR and RGB camera observations collected from the F1TENTH Gym.The zero-shot performance of MBRL agents is empirically evaluated on unseentracks and against a dynamic obstacle. This paper illustrates that multimodalperception improves robustness of the world model without requiring additionaltraining data. The resulting multimodal Dreamer agent safely avoided collisionsand won the most races compared to other tested baselines in zero-shothead-to-head autonomous racing.</description><author>Elena Shrestha, Chetan Reddy, Hanxi Wan, Yulun Zhuang, Ram Vasudevan</author><pubDate>Mon, 08 May 2023 15:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04750v1</guid></item><item><title>Toeplitz Neural Network for Sequence Modeling</title><link>http://arxiv.org/abs/2305.04749v1</link><description>Sequence modeling has important applications in natural language processingand computer vision. Recently, the transformer-based models have shown strongperformance on various sequence modeling tasks, which rely on attention tocapture pairwise token relations, and position embedding to inject positionalinformation. While showing good performance, the transformer models areinefficient to scale to long input sequences, mainly due to the quadraticspace-time complexity of attention. To overcome this inefficiency, we proposeto model sequences with a relative position encoded Toeplitz matrix and use aToeplitz matrix-vector production trick to reduce the space-time complexity ofthe sequence modeling to log linear. A lightweight sub-network called relativeposition encoder is proposed to generate relative position coefficients with afixed budget of parameters, enabling the proposed Toeplitz neural network todeal with varying sequence lengths. In addition, despite being trained on512-token sequences, our model can extrapolate input sequence length up to 14Ktokens in inference with consistent performance. Extensive experiments onautoregressive and bidirectional language modeling, image modeling, and thechallenging Long-Range Arena benchmark show that our method achieves betterperformance than its competitors in most downstream tasks while beingsignificantly faster. The code is available athttps://github.com/OpenNLPLab/Tnn.</description><author>Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, Yiran Zhong</author><pubDate>Mon, 08 May 2023 15:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04749v1</guid></item><item><title>Understanding Noise-Augmented Training for Randomized Smoothing</title><link>http://arxiv.org/abs/2305.04746v1</link><description>Randomized smoothing is a technique for providing provable robustnessguarantees against adversarial attacks while making minimal assumptions about aclassifier. This method relies on taking a majority vote of any base classifierover multiple noise-perturbed inputs to obtain a smoothed classifier, and itremains the tool of choice to certify deep and complex neural network models.Nonetheless, non-trivial performance of such smoothed classifier cruciallydepends on the base model being trained on noise-augmented data, i.e., on asmoothed input distribution. While widely adopted in practice, it is stillunclear how this noisy training of the base classifier precisely affects therisk of the robust smoothed classifier, leading to heuristics and tricks thatare poorly understood. In this work we analyze these trade-offs theoreticallyin a binary classification setting, proving that these common observations arenot universal. We show that, without making stronger distributionalassumptions, no benefit can be expected from predictors trained withnoise-augmentation, and we further characterize distributions where suchbenefit is obtained. Our analysis has direct implications to the practicaldeployment of randomized smoothing, and we illustrate some of these viaexperiments on CIFAR-10 and MNIST, as well as on synthetic datasets.</description><author>Ambar Pal, Jeremias Sulam</author><pubDate>Mon, 08 May 2023 15:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04746v1</guid></item><item><title>Controllable Light Diffusion for Portraits</title><link>http://arxiv.org/abs/2305.04745v1</link><description>We introduce light diffusion, a novel method to improve lighting inportraits, softening harsh shadows and specular highlights while preservingoverall scene illumination. Inspired by professional photographers' diffusersand scrims, our method softens lighting given only a single portrait photo.Previous portrait relighting approaches focus on changing the entire lightingenvironment, removing shadows (ignoring strong specular highlights), orremoving shading entirely. In contrast, we propose a learning based method thatallows us to control the amount of light diffusion and apply it on in-the-wildportraits. Additionally, we design a method to synthetically generate plausibleexternal shadows with sub-surface scattering effects while conforming to theshape of the subject's face. Finally, we show how our approach can increase therobustness of higher level vision applications, such as albedo estimation,geometry estimation and semantic segmentation.</description><author>David Futschik, Kelvin Ritland, James Vecore, Sean Fanello, Sergio Orts-Escolano, Brian Curless, Daniel Sýkora, Rohit Pandey</author><pubDate>Mon, 08 May 2023 15:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04745v1</guid></item><item><title>SkillQG: Learning to Generate Question for Reading Comprehension Assessment</title><link>http://arxiv.org/abs/2305.04737v1</link><description>We present $\textbf{$\texttt{SkillQG}$}$: a question generation frameworkwith controllable comprehension types for assessing and improving machinereading comprehension models. Existing question generation systems widelydifferentiate questions by $\textit{literal}$ information such as questionwords and answer types to generate semantically relevant questions for a givencontext. However, they rarely consider the $\textit{comprehension}$ nature ofquestions, i.e. the different comprehension capabilities embodied by differentquestions. In comparison, our $\texttt{SkillQG}$ is able to tailor afine-grained assessment and improvement to the capabilities of questionanswering models built on it. Specifically, we first frame the comprehensiontype of questions based on a hierarchical skill-based schema, then formulate$\texttt{SkillQG}$ as a skill-conditioned question generator. Furthermore, toimprove the controllability of generation, we augment the input text withquestion focus and skill-specific knowledge, which are constructed byiteratively prompting the pre-trained language models. Empirical resultsdemonstrate that $\texttt{SkillQG}$ outperforms baselines in terms of quality,relevance, and skill-controllability while showing a promising performanceboost in downstream question answering task.</description><author>Xiaoqiang Wang, Bang Liu, Siliang Tang, Lingfei Wu</author><pubDate>Mon, 08 May 2023 15:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04737v1</guid></item><item><title>SemEval-2023 Task 10: Explainable Detection of Online Sexism</title><link>http://arxiv.org/abs/2303.04222v2</link><description>Online sexism is a widespread and harmful phenomenon. Automated tools canassist the detection of sexism at scale. Binary detection, however, disregardsthe diversity of sexist content, and fails to provide clear explanations forwhy something is sexist. To address this issue, we introduce SemEval Task 10 onthe Explainable Detection of Online Sexism (EDOS). We make three maincontributions: i) a novel hierarchical taxonomy of sexist content, whichincludes granular vectors of sexism to aid explainability; ii) a new dataset of20,000 social media comments with fine-grained labels, along with largerunlabelled datasets for model adaptation; and iii) baseline models as well asan analysis of the methods, results and errors for participant submissions toour task.</description><author>Hannah Rose Kirk, Wenjie Yin, Bertie Vidgen, Paul Röttger</author><pubDate>Mon, 08 May 2023 15:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04222v2</guid></item><item><title>DEFENDER: DTW-Based Episode Filtering Using Demonstrations for Enhancing RL Safety</title><link>http://arxiv.org/abs/2305.04727v1</link><description>Deploying reinforcement learning agents in the real world can be challengingdue to the risks associated with learning through trial and error. We propose atask-agnostic method that leverages small sets of safe and unsafedemonstrations to improve the safety of RL agents during learning. The methodcompares the current trajectory of the agent with both sets of demonstrationsat every step, and filters the trajectory if it resembles the unsafedemonstrations. We perform ablation studies on different filtering strategiesand investigate the impact of the number of demonstrations on performance. Ourmethod is compatible with any stand-alone RL algorithm and can be applied toany task. We evaluate our method on three tasks from OpenAI Gym's Mujocobenchmark and two state-of-the-art RL algorithms. The results demonstrate thatour method significantly reduces the crash rate of the agent while convergingto, and in most cases even improving, the performance of the stand-alone agent.</description><author>André Correia, Luís Alexandre</author><pubDate>Mon, 08 May 2023 15:23:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04727v1</guid></item><item><title>Strategy for Rapid Diabetic Retinopathy Exposure Based on Enhanced Feature Extraction Processing</title><link>http://arxiv.org/abs/2305.04724v1</link><description>In the modern world, one of the most severe eye infections brought on bydiabetes is known as diabetic retinopathy, which will result in retinal damage,and, thus, lead to blindness. Diabetic retinopathy can be well treated withearly diagnosis. Retinal fundus images of humans are used to screen for lesionsin the retina. However, detecting DR in the early stages is challenging due tothe minimal symptoms. Furthermore, the occurrence of diseases linked tovascular anomalies brought on by DR aids in diagnosing the condition.Nevertheless, the resources required for manually identifying the lesions arehigh. Similarly, training for Convolutional Neural Networks is moretime-consuming. This proposed research aims to improve diabetic retinopathydiagnosis by developing an enhanced deep learning model for timely DRidentification that is potentially more accurate than existing CNN-basedmodels. The proposed model will detect various lesions from retinal images inthe early stages. First, characteristics are retrieved from the retinal funduspicture and put into the EDLM for classification. For dimensionality reduction,EDLM is used. Additionally, the classification and feature extraction processesare optimized using the stochastic gradient descent optimizer. The EDLMeffectiveness is assessed on the KAG GLE dataset with 3459 retinal images, andresults are compared over VGG16, VGG19, RESNET18, RESNET34, and RESNET50.</description><author>V. Banupriya, S. Anusuya</author><pubDate>Mon, 08 May 2023 15:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04724v1</guid></item><item><title>Understanding Gaussian Attention Bias of Vision Transformers Using Effective Receptive Fields</title><link>http://arxiv.org/abs/2305.04722v1</link><description>Vision transformers (ViTs) that model an image as a sequence of partitionedpatches have shown notable performance in diverse vision tasks. Becausepartitioning patches eliminates the image structure, to reflect the order ofpatches, ViTs utilize an explicit component called positional embedding.However, we claim that the use of positional embedding does not simplyguarantee the order-awareness of ViT. To support this claim, we analyze theactual behavior of ViTs using an effective receptive field. We demonstrate thatduring training, ViT acquires an understanding of patch order from thepositional embedding that is trained to be a specific pattern. Based on thisobservation, we propose explicitly adding a Gaussian attention bias that guidesthe positional embedding to have the corresponding pattern from the beginningof training. We evaluated the influence of Gaussian attention bias on theperformance of ViTs in several image classification, object detection, andsemantic segmentation experiments. The results showed that proposed method notonly facilitates ViTs to understand images but also boosts their performance onvarious datasets, including ImageNet, COCO 2017, and ADE20K.</description><author>Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Sang Woo Kim</author><pubDate>Mon, 08 May 2023 15:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04722v1</guid></item><item><title>DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation</title><link>http://arxiv.org/abs/2305.04720v1</link><description>Despite the recent advances in open-domain dialogue systems, building areliable evaluation metric is still a challenging problem. Recent studiesproposed learnable metrics based on classification models trained todistinguish the correct response. However, neural classifiers are known to makeoverly confident predictions for examples from unseen distributions. We proposeDEnsity, which evaluates a response by utilizing density estimation on thefeature space derived from a neural classifier. Our metric measures how likelya response would appear in the distribution of human conversations. Moreover,to improve the performance of DEnsity, we utilize contrastive learning tofurther compress the feature space. Experiments on multiple response evaluationdatasets show that DEnsity correlates better with human evaluations than theexisting metrics. Our code is available at https://github.com/ddehun/DEnsity.</description><author>ChaeHun Park, Seungil Lee, Daniel Rim, Jaegul Choo</author><pubDate>Mon, 08 May 2023 15:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04720v1</guid></item><item><title>Learning to Generate Poetic Chinese Landscape Painting with Calligraphy</title><link>http://arxiv.org/abs/2305.04719v1</link><description>In this paper, we present a novel system (denoted as Polaca) to generatepoetic Chinese landscape painting with calligraphy. Unlike previous singleimage-to-image painting generation, Polaca takes the classic poetry as inputand outputs the artistic landscape painting image with the correspondingcalligraphy. It is equipped with three different modules to complete the wholepiece of landscape painting artwork: the first one is a text-to-image module togenerate landscape painting image, the second one is an image-to-image moduleto generate stylistic calligraphy image, and the third one is an image fusionmodule to fuse the two images into a whole piece of aesthetic artwork.</description><author>Shaozu Yuan, Aijun Dai, Zhiling Yan, Ruixue Liu, Meng Chen, Baoyang Chen, Zhijie Qiu, Xiaodong He</author><pubDate>Mon, 08 May 2023 15:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04719v1</guid></item><item><title>The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation</title><link>http://arxiv.org/abs/2305.04718v1</link><description>In policy learning for robotic manipulation, sample efficiency is ofparamount importance. Thus, learning and extracting more compactrepresentations from camera observations is a promising avenue. However,current methods often assume full observability of the scene and struggle withscale invariance. In many tasks and settings, this assumption does not hold asobjects in the scene are often occluded or lie outside the field of view of thecamera, rendering the camera observation ambiguous with regard to theirlocation. To tackle this problem, we present BASK, a Bayesian approach totracking scale-invariant keypoints over time. Our approach successfullyresolves inherent ambiguities in images, enabling keypoint tracking onsymmetrical objects and occluded and out-of-view objects. We employ our methodto learn challenging multi-object robot manipulation tasks from wrist cameraobservations and demonstrate superior utility for policy learning compared toother representation learning techniques. Furthermore, we show outstandingrobustness towards disturbances such as clutter, occlusions, and noisy depthmeasurements, as well as generalization to unseen objects both in simulationand real-world robotic experiments.</description><author>Jan Ole von Hartz, Eugenio Chisari, Tim Welschehold, Wolfram Burgard, Joschka Boedecker, Abhinav Valada</author><pubDate>Mon, 08 May 2023 15:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04718v1</guid></item><item><title>On Preimage Approximation for Neural Networks</title><link>http://arxiv.org/abs/2305.03686v2</link><description>Neural network verification mainly focuses on local robustness properties.However, often it is important to know whether a given property holds globallyfor the whole input domain, and if not then for what proportion of the inputthe property is true. While exact preimage generation can construct anequivalent representation of neural networks that can aid such (quantitative)global robustness verification, it is intractable at scale. In this work, wepropose an efficient and practical anytime algorithm for generating symbolicunder-approximations of the preimage of neural networks based on linearrelaxation. Our algorithm iteratively minimizes the volume approximation errorby partitioning the input region into subregions, where the neural networkrelaxation bounds become tighter. We further employ sampling and differentiableapproximations to the volume in order to prioritize regions to split andoptimize the parameters of the relaxation, leading to faster improvement andmore compact under-approximations. Evaluation results demonstrate that ourapproach is able to generate preimage approximations significantly faster thanexact methods and scales to neural network controllers for which exact preimagegeneration is intractable. We also demonstrate an application of our approachto quantitative global verification.</description><author>Xiyue Zhang, Benjie Wang, Marta Kwiatkowska</author><pubDate>Mon, 08 May 2023 15:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03686v2</guid></item><item><title>Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200</title><link>http://arxiv.org/abs/2303.06311v2</link><description>Generative Adversarial Networks trained on samples of simulated or actualevents have been proposed as a way of generating large simulated datasets at areduced computational cost. In this work, a novel approach to perform thesimulation of photodetector signals from the time projection chamber of theEXO-200 experiment is demonstrated. The method is based on a WassersteinGenerative Adversarial Network - a deep learning technique allowing forimplicit non-parametric estimation of the population distribution for a givenset of objects. Our network is trained on real calibration data using rawscintillation waveforms as input. We find that it is able to producehigh-quality simulated waveforms an order of magnitude faster than thetraditional simulation approach and, importantly, generalize from the trainingsample and discern salient high-level features of the data. In particular, thenetwork correctly deduces position dependency of scintillation light responsein the detector and correctly recognizes dead photodetector channels. Thenetwork output is then integrated into the EXO-200 analysis framework to showthat the standard EXO-200 reconstruction routine processes the simulatedwaveforms to produce energy distributions comparable to that of real waveforms.Finally, the remaining discrepancies and potential ways to improve the approachfurther are highlighted.</description><author>S. Li, I. Ostrovskiy, Z. Li, L. Yang, S. Al Kharusi, G. Anton, I. Badhrees, P. S. Barbeau, D. Beck, V. Belov, T. Bhatta, M. Breidenbach, T. Brunner, G. F. Cao, W. R. Cen, C. Chambers, B. Cleveland, M. Coon, A. Craycraft, T. Daniels, L. Darroch, S. J. Daugherty, J. Davis, S. Delaquis, A. Der Mesrobian-Kabakian, R. DeVoe, J. Dilling, A. Dolgolenko, M. J. Dolinski, J. Echevers, W. Fairbank Jr., D. Fairbank, J. Farine, S. Feyzbakhsh, P. Fierlinger, Y. S. Fu, D. Fudenberg, P. Gautam, R. Gornea, G. Gratta, C. Hall, E. V. Hansen, J. Hoessl, P. Hufschmidt, M. Hughes, A. Iverson, A. Jamil, C. Jessiman, M. J. Jewell, A. Johnson, A. Karelin, L. J. Kaufman, T. Koffas, R. Krücken, A. Kuchenkov, K. S. Kumar, Y. Lan, A. Larson, B. G. Lenardo, D. S. Leonard, G. S. Li, C. Licciardi, Y. H. Lin, R. MacLellan</author><pubDate>Mon, 08 May 2023 14:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06311v2</guid></item><item><title>Robust online active learning</title><link>http://arxiv.org/abs/2302.00422v3</link><description>In many industrial applications, obtaining labeled observations is notstraightforward as it often requires the intervention of human experts or theuse of expensive testing equipment. In these circumstances, active learning canbe highly beneficial in suggesting the most informative data points to be usedwhen fitting a model. Reducing the number of observations needed for modeldevelopment alleviates both the computational burden required for training andthe operational expenses related to labeling. Online active learning, inparticular, is useful in high-volume production processes where the decisionabout the acquisition of the label for a data point needs to be taken within anextremely short time frame. However, despite the recent efforts to developonline active learning strategies, the behavior of these methods in thepresence of outliers has not been thoroughly examined. In this work, weinvestigate the performance of online active linear regression in contaminateddata streams. Our study shows that the currently available query strategies areprone to sample outliers, whose inclusion in the training set eventuallydegrades the predictive performance of the models. To address this issue, wepropose a solution that bounds the search area of a conditional D-optimalalgorithm and uses a robust estimator. Our approach strikes a balance betweenexploring unseen regions of the input space and protecting against outliers.Through numerical simulations, we show that the proposed method is effective inimproving the performance of online active learning in the presence ofoutliers, thus expanding the potential applications of this powerful tool.</description><author>Davide Cacciarelli, Murat Kulahci, John Sølve Tyssedal</author><pubDate>Mon, 08 May 2023 14:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00422v3</guid></item><item><title>Current State of Community-Driven Radiological AI Deployment in Medical Imaging</title><link>http://arxiv.org/abs/2212.14177v2</link><description>Artificial Intelligence (AI) has become commonplace to solve routine everydaytasks. Because of the exponential growth in medical imaging data volume andcomplexity, the workload on radiologists is steadily increasing. We projectthat the gap between the number of imaging exams and the number of expertradiologist readers required to cover this increase will continue to expand,consequently introducing a demand for AI-based tools that improve theefficiency with which radiologists can comfortably interpret these exams. AIhas been shown to improve efficiency in medical-image generation, processing,and interpretation, and a variety of such AI models have been developed acrossresearch labs worldwide. However, very few of these, if any, find their wayinto routine clinical use, a discrepancy that reflects the divide between AIresearch and successful AI translation. To address the barrier to clinicaldeployment, we have formed MONAI Consortium, an open-source community which isbuilding standards for AI deployment in healthcare institutions, and developingtools and infrastructure to facilitate their implementation. This reportrepresents several years of weekly discussions and hands-on problem solvingexperience by groups of industry experts and clinicians in the MONAIConsortium. We identify barriers between AI-model development in research labsand subsequent clinical deployment and propose solutions. Our report providesguidance on processes which take an imaging AI model from development toclinical implementation in a healthcare institution. We discuss various AIintegration points in a clinical Radiology workflow. We also present a taxonomyof Radiology AI use-cases. Through this report, we intend to educate thestakeholders in healthcare and AI (AI researchers, radiologists, imaginginformaticists, and regulators) about cross-disciplinary challenges andpossible solutions.</description><author>Vikash Gupta, Barbaros Selnur Erdal, Carolina Ramirez, Ralf Floca, Laurence Jackson, Brad Genereaux, Sidney Bryson, Christopher P Bridge, Jens Kleesiek, Felix Nensa, Rickmer Braren, Khaled Younis, Tobias Penzkofer, Andreas Michael Bucher, Ming Melvin Qin, Gigon Bae, Hyeonhoon Lee, M. Jorge Cardoso, Sebastien Ourselin, Eric Kerfoot, Rahul Choudhury, Richard D. White, Tessa Cook, David Bericat, Matthew Lungren, Risto Haukioja, Haris Shuaib</author><pubDate>Mon, 08 May 2023 14:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14177v2</guid></item><item><title>High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction</title><link>http://arxiv.org/abs/2305.04712v1</link><description>We study the problem of overcoming exponential sample complexity indifferential entropy estimation under Gaussian convolutions. Specifically, weconsider the estimation of the differential entropy $h(X+Z)$ via $n$independently and identically distributed samples of $X$, where $X$ and $Z$ areindependent $D$-dimensional random variables with $X$ subgaussian with boundedsecond moment and $Z\sim\mathcal{N}(0,\sigma^2I_D)$. Under the absolute-errorloss, the above problem has a parametric estimation rate of$\frac{c^D}{\sqrt{n}}$, which is exponential in data dimension $D$ and oftenproblematic for applications. We overcome this exponential sample complexity byprojecting $X$ to a low-dimensional space via principal component analysis(PCA) before the entropy estimation, and show that the asymptotic erroroverhead vanishes as the unexplained variance of the PCA vanishes. This impliesnear-optimal performance for inherently low-dimensional structures embedded inhigh-dimensional spaces, including hidden-layer outputs of deep neural networks(DNN), which can be used to estimate mutual information (MI) in DNNs. Weprovide numerical results verifying the performance of our PCA approach onGaussian and spiral data. We also apply our method to analysis of informationflow through neural network layers (c.f. information bottleneck), with resultsmeasuring mutual information in a noisy fully connected network and a noisyconvolutional neural network (CNN) for MNIST classification.</description><author>Kristjan Greenewald, Brian Kingsbury, Yuancheng Yu</author><pubDate>Mon, 08 May 2023 14:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04712v1</guid></item><item><title>ElasticHash: Semantic Image Similarity Search by Deep Hashing with Elasticsearch</title><link>http://arxiv.org/abs/2305.04710v1</link><description>We present ElasticHash, a novel approach for high-quality, efficient, andlarge-scale semantic image similarity search. It is based on a deep hashingmodel to learn hash codes for fine-grained image similarity search in naturalimages and a two-stage method for efficiently searching binary hash codes usingElasticsearch (ES). In the first stage, a coarse search based on short hashcodes is performed using multi-index hashing and ES terms lookup of neighboringhash codes. In the second stage, the list of results is re-ranked by computingthe Hamming distance on long hash codes. We evaluate the retrieval performanceof \textit{ElasticHash} for more than 120,000 query images on about 6.9 milliondatabase images of the OpenImages data set. The results show that our approachachieves high-quality retrieval results and low search latencies.</description><author>Nikolaus Korfhage, Markus Mühling, Bernd Freisleben</author><pubDate>Mon, 08 May 2023 14:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04710v1</guid></item><item><title>Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer</title><link>http://arxiv.org/abs/2208.07282v5</link><description>In this paper, we propose a differentiable WORLD synthesizer and demonstrateits use in end-to-end audio style transfer tasks such as (singing) voiceconversion and the DDSP timbre transfer task. Accordingly, our baselinedifferentiable synthesizer has no model parameters, yet it yields adequatesynthesis quality. We can extend the baseline synthesizer by appendinglightweight black-box postnets which apply further processing to the baselineoutput in order to improve fidelity. An alternative differentiable approachconsiders extraction of the source excitation spectrum directly, which canimprove naturalness albeit for a narrower class of style transfer applications.The acoustic feature parameterization used by our approaches has the addedbenefit that it naturally disentangles pitch and timbral information so thatthey can be modeled separately. Moreover, as there exists a robust means ofestimating these acoustic features from monophonic audio sources, it allows forparameter loss terms to be added to an end-to-end objective function, which canhelp convergence and/or further stabilize (adversarial) training.</description><author>Shahan Nercessian</author><pubDate>Mon, 08 May 2023 14:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07282v5</guid></item><item><title>Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video</title><link>http://arxiv.org/abs/2303.08475v2</link><description>Temporal modeling is crucial for multi-frame human pose estimation. Mostexisting methods directly employ optical flow or deformable convolution topredict full-spectrum motion fields, which might incur numerous irrelevantcues, such as a nearby person or background. Without further efforts toexcavate meaningful motion priors, their results are suboptimal, especially incomplicated spatiotemporal interactions. On the other hand, the temporaldifference has the ability to encode representative motion information whichcan potentially be valuable for pose estimation but has not been fullyexploited. In this paper, we present a novel multi-frame human pose estimationframework, which employs temporal differences across frames to model dynamiccontexts and engages mutual information objectively to facilitate useful motioninformation disentanglement. To be specific, we design a multi-stage TemporalDifference Encoder that performs incremental cascaded learning conditioned onmulti-stage feature difference sequences to derive informative motionrepresentation. We further propose a Representation Disentanglement module fromthe mutual information perspective, which can grasp discriminativetask-relevant motion signals by explicitly defining useful and noisyconstituents of the raw motion features and minimizing their mutualinformation. These place us to rank No.1 in the Crowd Pose Estimation inComplex Events Challenge on benchmark dataset HiEve, and achievestate-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018,and PoseTrack21.</description><author>Runyang Feng, Yixing Gao, Xueqing Ma, Tze Ho Elden Tse, Hyung Jin Chang</author><pubDate>Mon, 08 May 2023 14:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08475v2</guid></item><item><title>Differentially Private Attention Computation</title><link>http://arxiv.org/abs/2305.04701v1</link><description>Large language models (LLMs) have had a profound impact on numerous aspectsof daily life including natural language processing, content generation,research methodologies and so on. However, one crucial issue concerning theinference results of large language models is security and privacy. In manyscenarios, the results generated by LLMs could possibly leak many confidentialor copyright information. A recent beautiful and breakthrough work [Vyas,Kakade and Barak 2023] focus on such privacy issue of the LLMs from theoreticalperspective. It is well-known that computing the attention matrix is one of themajor task during the LLMs computation. Thus, how to give a provable privatelyguarantees of computing the attention matrix is an important researchdirection. Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposedprovable tight result for fast computation of attention without consideringprivacy concerns. One natural mathematical formulation to quantity the privacyin theoretical computer science graduate school textbook is differentialprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide aprovable result for showing how to differentially private approximate theattention matrix. From technique perspective, our result replies on a pioneering work in thearea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang2022].</description><author>Yeqi Gao, Zhao Song, Xin Yang</author><pubDate>Mon, 08 May 2023 14:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04701v1</guid></item><item><title>Runtime Monitoring of Dynamic Fairness Properties</title><link>http://arxiv.org/abs/2305.04699v1</link><description>A machine-learned system that is fair in static decision-making tasks mayhave biased societal impacts in the long-run. This may happen when the systeminteracts with humans and feedback patterns emerge, reinforcing old biases inthe system and creating new biases. While existing works try to identify andmitigate long-run biases through smart system design, we introduce techniquesfor monitoring fairness in real time. Our goal is to build and deploy a monitorthat will continuously observe a long sequence of events generated by thesystem in the wild, and will output, with each event, a verdict on how fair thesystem is at the current point in time. The advantages of monitoring aretwo-fold. Firstly, fairness is evaluated at run-time, which is importantbecause unfair behaviors may not be eliminated a priori, at design-time, due topartial knowledge about the system and the environment, as well asuncertainties and dynamic changes in the system and the environment, such asthe unpredictability of human behavior. Secondly, monitors are by designoblivious to how the monitored system is constructed, which makes them suitableto be used as trusted third-party fairness watchdogs. They function ascomputationally lightweight statistical estimators, and their correctnessproofs rely on the rigorous analysis of the stochastic process that models theassumptions about the underlying dynamics of the system. We show, both intheory and experiments, how monitors can warn us (1) if a bank's credit policyover time has created an unfair distribution of credit scores among thepopulation, and (2) if a resource allocator's allocation policy over time hasmade unfair allocations. Our experiments demonstrate that the monitorsintroduce very low overhead. We believe that runtime monitoring is an importantand mathematically rigorous new addition to the fairness toolbox.</description><author>Thomas A. Henzinger, Mahyar Karimi, Konstantin Kueffner, Kaushik Mallik</author><pubDate>Mon, 08 May 2023 14:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04699v1</guid></item><item><title>Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning</title><link>http://arxiv.org/abs/2212.03760v4</link><description>Recent studies have proposed unified user modeling frameworks that leverageuser behavior data from various applications. Many of them benefit fromutilizing users' behavior sequences as plain texts, representing richinformation in any domain or system without losing generality. Hence, aquestion arises: Can language modeling for user history corpus help improverecommender systems? While its versatile usability has been widely investigatedin many domains, its applications to recommender systems still remainunderexplored. We show that language modeling applied directly to task-specificuser histories achieves excellent results on diverse recommendation tasks.Also, leveraging additional task-agnostic user histories delivers significantperformance benefits. We further demonstrate that our approach can providepromising transfer learning capabilities for a broad spectrum of real-worldrecommender systems, even on unseen domains and services.</description><author>Kyuyong Shin, Hanock Kwak, Wonjae Kim, Jisu Jeong, Seungjae Jung, Kyung-Min Kim, Jung-Woo Ha, Sang-Woo Lee</author><pubDate>Mon, 08 May 2023 14:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03760v4</guid></item><item><title>Anticipatory Planning: Improving Long-Lived Planning by Estimating Expected Cost of Future Tasks</title><link>http://arxiv.org/abs/2305.04692v1</link><description>We consider a service robot in a household environment given a sequence ofhigh-level tasks one at a time. Most existing task planners, lacking knowledgeof what they may be asked to do next, solve each task in isolation and so mayunwittingly introduce side effects that make subsequent tasks more costly. Inorder to reduce the overall cost of completing all tasks, we consider that therobot must anticipate the impact its actions could have on future tasks. Thus,we propose anticipatory planning: an approach in which estimates of theexpected future cost, from a graph neural network, augment model-based taskplanning. Our approach guides the robot towards behaviors that encouragepreparation and organization, reducing overall costs in long-lived planningscenarios. We evaluate our method on blockworld environments and show that ourapproach reduces the overall planning costs by 5% as compared to planningwithout anticipatory planning. Additionally, if given an opportunity to preparethe environment in advance (a special case of anticipatory planning), ourplanner improves overall cost by 11%.</description><author>Roshan Dhakal, Md Ridwan Hossain Talukder, Gregory J. Stein</author><pubDate>Mon, 08 May 2023 14:22:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04692v1</guid></item><item><title>Self-supervised Learning for Pre-Training 3D Point Clouds: A Survey</title><link>http://arxiv.org/abs/2305.04691v1</link><description>Point cloud data has been extensively studied due to its compact form andflexibility in representing complex 3D structures. The ability of point clouddata to accurately capture and represent intricate 3D geometry makes it anideal choice for a wide range of applications, including computer vision,robotics, and autonomous driving, all of which require an understanding of theunderlying spatial structures. Given the challenges associated with annotatinglarge-scale point clouds, self-supervised point cloud representation learninghas attracted increasing attention in recent years. This approach aims to learngeneric and useful point cloud representations from unlabeled data,circumventing the need for extensive manual annotations. In this paper, wepresent a comprehensive survey of self-supervised point cloud representationlearning using DNNs. We begin by presenting the motivation and general trendsin recent research. We then briefly introduce the commonly used datasets andevaluation metrics. Following that, we delve into an extensive exploration ofself-supervised point cloud representation learning methods based on thesetechniques. Finally, we share our thoughts on some of the challenges andpotential issues that future research in self-supervised learning forpre-training 3D point clouds may encounter.</description><author>Ben Fei, Weidong Yang, Liwen Liu, Tianyue Luo, Rui Zhang, Yixuan Li, Ying He</author><pubDate>Mon, 08 May 2023 14:20:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04691v1</guid></item><item><title>SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference</title><link>http://arxiv.org/abs/2303.09266v2</link><description>Dynamic early exiting has been proven to improve the inference speed of thepre-trained language model like BERT. However, all samples must go through allconsecutive layers before early exiting and more complex samples usually gothrough more layers, which still exists redundant computation. In this paper,we propose a novel dynamic early exiting combined with layer skipping for BERTinference named SmartBERT, which adds a skipping gate and an exiting operatorinto each layer of BERT. SmartBERT can adaptively skip some layers andadaptively choose whether to exit. Besides, we propose cross-layer contrastivelearning and combine it into our training phases to boost the intermediatelayers and classifiers which would be beneficial for early exiting. To keep theconsistent usage of skipping gates between training and inference phases, wepropose a hard weight mechanism during training phase. We conduct experimentson eight classification datasets of the GLUE benchmark. Experimental resultsshow that SmartBERT achieves 2-3x computation reduction with minimal accuracydrops compared with BERT and our method outperforms previous methods in bothefficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI,we prove that the early exiting based on entropy hardly works, and the skippingmechanism is essential for reducing computation.</description><author>Boren Hu, Yun Zhu, Jiacheng Li, Siliang Tang</author><pubDate>Mon, 08 May 2023 14:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09266v2</guid></item><item><title>ASDL: A Unified Interface for Gradient Preconditioning in PyTorch</title><link>http://arxiv.org/abs/2305.04684v1</link><description>Gradient preconditioning is a key technique to integrate the second-orderinformation into gradients for improving and extending gradient-based learningalgorithms. In deep learning, stochasticity, nonconvexity, and highdimensionality lead to a wide variety of gradient preconditioning methods, withimplementation complexity and inconsistent performance and feasibility. Wepropose the Automatic Second-order Differentiation Library (ASDL), an extensionlibrary for PyTorch, which offers various implementations and a plug-and-playunified interface for gradient preconditioning. ASDL enables the study andstructured comparison of a range of gradient preconditioning methods.</description><author>Kazuki Osawa, Satoki Ishikawa, Rio Yokota, Shigang Li, Torsten Hoefler</author><pubDate>Mon, 08 May 2023 13:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04684v1</guid></item><item><title>Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</title><link>http://arxiv.org/abs/2304.08014v3</link><description>Most invariance-based self-supervised methods rely on single object-centricimages (e.g., ImageNet images) for pretraining, learning invariantrepresentations from geometric transformations. However, when images are notobject-centric, the semantics of the image can be significantly altered due tocropping. Furthermore, as the model becomes insensitive to geometrictransformations, it may struggle to capture location information. For thisreason, we propose a Geometric Transformation Sensitive Architecture designedto learn features that are sensitive to geometric transformations, specificallyfocusing on four-fold rotation, random crop, and multi-crop. Our methodencourages the student to be sensitive by using targets that are sensitive tothose transforms via pooling and rotating of the teacher feature map andpredicting rotation. Additionally, as training insensitively to multi-cropencourages local-to-global correspondence, the model can capture long-termdependencies. We use patch correspondence loss to encourage correspondencebetween patches with similar features, instead of enforcing correspondencebetween views of the image. This approach allows us to capture long-termdependencies in a more appropriate way. Our approach demonstrates improvedperformance when using non-object-centric images as pretraining data comparedto other methods that learn geometric transformation-insensitiverepresentations. We surpass the DINO baseline in tasks including imageclassification, semantic segmentation, detection, and instance segmentationwith improvements of 4.9 $Top-1 Acc$, 3.3 $mIoU$, 3.4 $AP^b$, and 2.7 $AP^m$.Code and pretrained models are publicly available at:https://github.com/bok3948/GTSA</description><author>Taeho Kim, Jong-Min Lee</author><pubDate>Mon, 08 May 2023 13:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08014v3</guid></item><item><title>Enhancing Knowledge Graph Construction Using Large Language Models</title><link>http://arxiv.org/abs/2305.04676v1</link><description>The growing trend of Large Language Models (LLM) development has attractedsignificant attention, with models for various applications emergingconsistently. However, the combined application of Large Language Models withsemantic technologies for reasoning and inference is still a challenging task.This paper analyzes how the current advances in foundational LLM, like ChatGPT,can be compared with the specialized pretrained models, like REBEL, for jointentity and relation extraction. To evaluate this approach, we conducted severalexperiments using sustainability-related text as our use case. We createdpipelines for the automatic creation of Knowledge Graphs from raw texts, andour findings indicate that using advanced LLM models can improve the accuracyof the process of creating these graphs from unstructured text. Furthermore, weexplored the potential of automatic ontology creation using foundation LLMmodels, which resulted in even more relevant and accurate knowledge graphs.</description><author>Milena Trajanoska, Riste Stojanov, Dimitar Trajanov</author><pubDate>Mon, 08 May 2023 13:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04676v1</guid></item><item><title>Predicting nuclear masses with product-unit networks</title><link>http://arxiv.org/abs/2305.04675v1</link><description>Accurate estimation of nuclear masses and their prediction beyond theexperimentally explored domains of the nuclear landscape are crucial to anunderstanding of the fundamental origin of nuclear properties and to manyapplications of nuclear science, most notably in quantifying the $r$-process ofstellar nucleosynthesis. Neural networks have been applied with some success tothe prediction of nuclear masses, but they are known to have shortcomings inapplication to extrapolation tasks. In this work, we propose and explore anovel type of neural network for mass prediction in which the usual neuron-likeprocessing units are replaced by complex-valued product units that permitmultiplicative couplings of inputs to be learned from the input data. Thisgeneralized network model is tested on both interpolation and extrapolationdata sets drawn from the Atomic Mass Evaluation. Its performance is comparedwith that of several neural-network architectures, substantiating itssuitability for nuclear mass prediction. Additionally, a prediction-uncertaintymeasure for such complex-valued networks is proposed that serves to identifyregions of expected low prediction error.</description><author>Babette Dellen, Uwe Jaekel, Paulo S. A. Freitas, John W. Clark</author><pubDate>Mon, 08 May 2023 13:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04675v1</guid></item><item><title>PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models</title><link>http://arxiv.org/abs/2305.04673v1</link><description>Pre-trained Language Models such as BERT are impressive machines with theability to memorize, possibly generalized learning examples. We present here asmall, focused contribution to the analysis of the interplay betweenmemorization and performance of BERT in downstream tasks. We propose PreCog, ameasure for evaluating memorization from pre-training, and we analyze itscorrelation with the BERT's performance. Our experiments show that highlymemorized examples are better classified, suggesting memorization is anessential key to success for BERT.</description><author>Leonardo Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto</author><pubDate>Mon, 08 May 2023 13:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04673v1</guid></item><item><title>CACTO: Continuous Actor-Critic with Trajectory Optimization -- Towards global optimality</title><link>http://arxiv.org/abs/2211.06625v3</link><description>This paper presents a novel algorithm for the continuous control of dynamicalsystems that combines Trajectory Optimization (TO) and Reinforcement Learning(RL) in a single framework. The motivations behind this algorithm are the twomain limitations of TO and RL when applied to continuous nonlinear systems tominimize a non-convex cost function. Specifically, TO can get stuck in poorlocal minima when the search is not initialized close to a "good" minimum. Onthe other hand, when dealing with continuous state and control spaces, the RLtraining process may be excessively long and strongly dependent on theexploration strategy. Thus, our algorithm learns a "good" control policy viaTO-guided RL policy search that, when used as initial guess provider for TO,makes the trajectory optimization process less prone to converge to poor localoptima. Our method is validated on several reaching problems featuringnon-convex obstacle avoidance with different dynamical systems, including a carmodel with 6D state, and a 3-joint planar manipulator. Our results show thegreat capabilities of CACTO in escaping local minima, while being morecomputationally efficient than the Deep Deterministic Policy Gradient (DDPG)and Proximal Policy Optimization (PPO) RL algorithms.</description><author>Gianluigi Grandesso, Elisa Alboni, Gastone P. Rosati Papini, Patrick M. Wensing, Andrea Del Prete</author><pubDate>Mon, 08 May 2023 13:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06625v3</guid></item><item><title>Analysis of Numerical Integration in RNN-Based Residuals for Fault Diagnosis of Dynamic Systems</title><link>http://arxiv.org/abs/2305.04670v1</link><description>Data-driven modeling and machine learning are widely used to model thebehavior of dynamic systems. One application is the residual evaluation oftechnical systems where model predictions are compared with measurement data tocreate residuals for fault diagnosis applications. While recurrent neuralnetwork models have been shown capable of modeling complex non-linear dynamicsystems, they are limited to fixed steps discrete-time simulation. Modelingusing neural ordinary differential equations, however, make it possible toevaluate the state variables at specific times, compute gradients when trainingthe model and use standard numerical solvers to explicitly model the underlyingdynamic of the time-series data. Here, the effect of solver selection on theperformance of neural ordinary differential equation residuals during trainingand evaluation is investigated. The paper includes a case study of a heavy-dutytruck's after-treatment system to highlight the potential of these techniquesfor improving fault diagnosis performance.</description><author>Arman Mohammadi, Theodor Westny, Daniel Jung, Mattias Krysander</author><pubDate>Mon, 08 May 2023 13:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04670v1</guid></item><item><title>Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation</title><link>http://arxiv.org/abs/2305.03259v2</link><description>Clothes grasping and unfolding is a core step in robotic-assisted dressing.Most existing works leverage depth images of clothes to train a deeplearning-based model to recognize suitable grasping points. These methods oftenutilize physics engines to synthesize depth images to reduce the cost of reallabeled data collection. However, the natural domain gap between synthetic andreal images often leads to poor performance of these methods on real data.Furthermore, these approaches often struggle in scenarios where grasping pointsare occluded by the clothing item itself. To address the above challenges, wepropose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) forsemantic segmentation, enabling recognition of graspable regions in order toprovide more possibilities for grasping. Instead of using depth images only, wealso utilize RGB images with rich color features as input to our network inwhich the Fractal Cross Fusion (FCF) module fuses RGB and depth data byconsidering global complex features based on fractal geometry. To reduce thecost of real data collection, we further propose a data augmentation methodbased on an adversarial strategy, in which the color and geometrictransformations simultaneously process RGB and depth data while maintaining thelabel correspondence. Finally, we present a pipeline for clothes grasping andunfolding from the perspective of semantic segmentation, through the additionof a strategy for grasp point selection from segmentation regions based onclothing flatness measures, while taking into account the grasping direction.We evaluate our BiFCNet on the public dataset NYUDv2 and obtained comparableperformance to current state-of-the-art models. We also deploy our model on aBaxter robot, running extensive grasping and unfolding experiments as part ofour ablation studies, achieving an 84% success rate.</description><author>Xingyu Zhu, Xin Wang, Jonathan Freer, Hyung Jin Chang, Yixing Gao</author><pubDate>Mon, 08 May 2023 13:44:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03259v2</guid></item><item><title>Riesz networks: scale invariant neural networks in a single forward pass</title><link>http://arxiv.org/abs/2305.04665v1</link><description>Scale invariance of an algorithm refers to its ability to treat objectsequally independently of their size. For neural networks, scale invariance istypically achieved by data augmentation. However, when presented with a scalefar outside the range covered by the training set, neural networks may fail togeneralize. Here, we introduce the Riesz network, a novel scale invariant neural network.Instead of standard 2d or 3d convolutions for combining spatial information,the Riesz network is based on the Riesz transform which is a scale equivariantoperation. As a consequence, this network naturally generalizes to unseen oreven arbitrary scales in a single forward pass. As an application example, weconsider detecting and segmenting cracks in tomographic images of concrete. Inthis context, 'scale' refers to the crack thickness which may vary stronglyeven within the same sample. To prove its scale invariance, the Riesz networkis trained on one fixed crack width. We then validate its performance insegmenting simulated and real tomographic images featuring a wide range ofcrack widths. An additional experiment is carried out on the MNIST Large Scaledata set.</description><author>Tin Barisin, Katja Schladitz, Claudia Redenbach</author><pubDate>Mon, 08 May 2023 13:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04665v1</guid></item><item><title>Towards Self-adaptive Mutation in Evolutionary Multi-Objective Algorithms</title><link>http://arxiv.org/abs/2303.04611v2</link><description>Parameter control has succeeded in accelerating the convergence process ofevolutionary algorithms. While empirical and theoretical studies have shedlight on the behavior of algorithms for single-objective optimization, littleis known about how self-adaptation influences multi-objective evolutionaryalgorithms. In this work, we contribute (1) extensive experimental analysis ofthe Global Simple Evolutionary Multi-objective Algorithm (GSEMO) variants onclassic problems, such as OneMinMax, LOTZ, COCZ, and (2) a novel version ofGSEMO with self-adaptive mutation. To enable self-adaptation in GSEMO, we explore three self-adaptive mutationtechniques from single-objective optimization and use various performancemetrics, such as hypervolume and inverted generational distance, to guide theadaptation. Our experiments show that adapting the mutation rate based onsingle-objective optimization and hypervolume can speed up the convergence ofGSEMO. Moreover, we propose a GSEMO with self-adaptive mutation, whichconsiders optimizing for single objectives and adjusts the mutation rate foreach solution individually. Our results demonstrate that the proposed methodoutperforms the GSEMO with static mutation rates across all the testedproblems. This work provides a comprehensive benchmarking study for MOEAs andcomplements existing theoretical runtime analysis. Our proposed algorithmaddresses interesting issues for designing MOEAs for future practicalapplications.</description><author>Furong Ye, Frank Neumann, Jacob de Nobel, Aneta Neumann, Thomas Bäck</author><pubDate>Mon, 08 May 2023 13:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04611v2</guid></item><item><title>Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation</title><link>http://arxiv.org/abs/2103.06615v5</link><description>Over the last years, significant advances have been made in roboticmanipulation, but still, the handling of non-rigid objects, such as clothgarments, is an open problem. Physical interaction with non-rigid objects isuncertain and complex to model. Thus, extracting useful information from sampledata can considerably improve modeling performance. However, the training ofsuch models is a challenging task due to the high-dimensionality of the staterepresentation. In this paper, we propose Controlled Gaussian Process DynamicalModel (CGPDM) for learning high-dimensional, nonlinear dynamics by embedding itin a low-dimensional manifold. A CGPDM is constituted by a low-dimensionallatent space, with an associated dynamics where external control variables canact and a mapping to the observation space. The parameters of both maps aremarginalized out by considering Gaussian Process (GP) priors. Hence, a CGPDMprojects a high-dimensional state space into a smaller dimension latent space,in which it is feasible to learn the system dynamics from training data. Themodeling capacity of CGPDM has been tested in both a simulated and a realscenario, where it proved to be capable of generalizing over a wide range ofmovements and confidently predicting the cloth motions obtained by previouslyunseen sequences of control actions.</description><author>Fabio Amadio, Juan Antonio Delgado-Guerrero, Adrià Colomé, Carme Torras</author><pubDate>Mon, 08 May 2023 13:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.06615v5</guid></item><item><title>MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model</title><link>http://arxiv.org/abs/2302.08751v2</link><description>One of the major challenges in multi-person pose estimation is instance-awarekeypoint estimation. Previous methods address this problem by leveraging anoff-the-shelf detector, heuristic post-grouping process or explicit instanceidentification process, hindering further improvements in the inference speedwhich is an important factor for practical applications. From the statisticalpoint of view, those additional processes for identifying instances arenecessary to bypass learning the high-dimensional joint distribution of humankeypoints, which is a critical factor for another major challenge, theocclusion scenario. In this work, we propose a novel framework of single-stageinstance-aware pose estimation by modeling the joint distribution of humankeypoints with a mixture density model, termed as MDPose. Our MDPose estimatesthe distribution of human keypoints' coordinates using a mixture density modelwith an instance-aware keypoint head consisting simply of 8 convolutionallayers. It is trained by minimizing the negative log-likelihood of the groundtruth keypoints. Also, we propose a simple yet effective training strategy,Random Keypoint Grouping (RKG), which significantly alleviates the underflowproblem leading to successful learning of relations between keypoints. OnOCHuman dataset, which consists of images with highly occluded people, ourMDPose achieves state-of-the-art performance by successfully learning thehigh-dimensional joint distribution of human keypoints. Furthermore, our MDPoseshows significant improvement in inference speed with a competitive accuracy onMS COCO, a widely-used human keypoint dataset, thanks to the proposed muchsimpler single-stage pipeline.</description><author>Seunghyeon Seo, Jaeyoung Yoo, Jihye Hwang, Nojun Kwak</author><pubDate>Mon, 08 May 2023 13:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08751v2</guid></item><item><title>CSGCL: Community-Strength-Enhanced Graph Contrastive Learning</title><link>http://arxiv.org/abs/2305.04658v1</link><description>Graph Contrastive Learning (GCL) is an effective way to learn generalizedgraph representations in a self-supervised manner, and has grown rapidly inrecent years. However, the underlying community semantics has not been wellexplored by most previous GCL methods. Research that attempts to leveragecommunities in GCL regards them as having the same influence on the graph,leading to extra representation errors. To tackle this issue, we define''community strength'' to measure the difference of influence amongcommunities. Under this premise, we propose a Community-Strength-enhanced GraphContrastive Learning (CSGCL) framework to preserve community strengththroughout the learning process. Firstly, we present two novel graphaugmentation methods, Communal Attribute Voting (CAV) and Communal EdgeDropping (CED), where the perturbations of node attributes and edges are guidedby community strength. Secondly, we propose a dynamic ''Team-up'' contrastivelearning scheme, where community strength is used to progressively fine-tunethe contrastive objective. We report extensive experiment results on threedownstream tasks: node classification, node clustering, and link prediction.CSGCL achieves state-of-the-art performance compared with other GCL methods,validating that community strength brings effectiveness and generality to graphrepresentations. Our code is available athttps://github.com/HanChen-HUST/CSGCL.</description><author>Han Chen, Ziwen Zhao, Yuhua Li, Yixiong Zou, Ruixuan Li, Rui Zhang</author><pubDate>Mon, 08 May 2023 13:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04658v1</guid></item><item><title>Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning</title><link>http://arxiv.org/abs/2305.00312v3</link><description>Conventionally, federated learning aims to optimize a single objective,typically the utility. However, for a federated learning system to betrustworthy, it needs to simultaneously satisfy multiple/many objectives, suchas maximizing model performance, minimizing privacy leakage and training cost,and being robust to malicious attacks. Multi-Objective Optimization (MOO)aiming to optimize multiple conflicting objectives at the same time is quitesuitable for solving the optimization problem of Trustworthy Federated Learning(TFL). In this paper, we unify MOO and TFL by formulating the problem ofconstrained multi-objective federated learning (CMOFL). Under this formulation,existing MOO algorithms can be adapted to TFL straightforwardly. Different fromexisting CMOFL works focusing on utility, efficiency, fairness, and robustness,we consider optimizing privacy leakage along with utility loss and trainingcost, the three primary objectives of a TFL system. We develop two improvedCMOFL algorithms based on NSGA-II and PSL, respectively, for effectively andefficiently finding Pareto optimal solutions, and we provide theoreticalanalysis on their convergence. We design specific measurements of privacyleakage, utility loss, and training cost for three privacy protectionmechanisms: Randomization, BatchCrypt (An efficient version of homomorphicencryption), and Sparsification. Empirical experiments conducted under each ofthe three protection mechanisms demonstrate the effectiveness of our proposedalgorithms.</description><author>Yan Kang, Hanlin Gu, Xingxing Tang, Yuanqin He, Yuzhu Zhang, Jinnan He, Yuxing Han, Lixin Fan, Qiang Yang</author><pubDate>Mon, 08 May 2023 13:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00312v3</guid></item><item><title>Reuse your features: unifying retrieval and feature-metric alignment</title><link>http://arxiv.org/abs/2204.06292v2</link><description>We propose a compact pipeline to unify all the steps of Visual Localization:image retrieval, candidate re-ranking and initial pose estimation, and camerapose refinement. Our key assumption is that the deep features used for theseindividual tasks share common characteristics, so we should reuse them in allthe procedures of the pipeline. Our DRAN (Deep Retrieval and image AlignmentNetwork) is able to extract global descriptors for efficient image retrieval,use intermediate hierarchical features to re-rank the retrieval list andproduce an initial pose guess, which is finally refined by means of afeature-metric optimization based on learned deep multi-scale dense features.DRAN is the first single network able to produce the features for the threesteps of visual localization. DRAN achieves competitive performance in terms ofrobustness and accuracy under challenging conditions in public benchmarks,outperforming other unified approaches and consuming lower computational andmemory cost than its counterparts using multiple networks. Code and models willbe publicly available at https://github.com/jmorlana/DRAN.</description><author>Javier Morlana, J. M. M. Montiel</author><pubDate>Mon, 08 May 2023 13:10:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06292v2</guid></item><item><title>ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation</title><link>http://arxiv.org/abs/2305.04651v1</link><description>Large-scale text-to-image models have demonstrated amazing ability tosynthesize diverse and high-fidelity images. However, these models are oftenviolated by several limitations. Firstly, they require the user to provideprecise and contextually relevant descriptions for the desired imagemodifications. Secondly, current models can impose significant changes to theoriginal image content during the editing process. In this paper, we exploreReGeneration learning in an image-to-image Diffusion model (ReDiffuser), thatpreserves the content of the original image without human prompting and therequisite editing direction is automatically discovered within the textembedding space. To ensure consistent preservation of the shape during imageediting, we propose cross-attention guidance based on regeneration learning.This novel approach allows for enhanced expression of the target domainfeatures while preserving the original shape of the image. In addition, weintroduce a cooperative update strategy, which allows for efficientpreservation of the original shape of an image, thereby improving the qualityand consistency of shape preservation throughout the editing process. Ourproposed method leverages an existing pre-trained text-image diffusion modelwithout any additional training. Extensive experiments show that the proposedmethod outperforms existing work in both real and synthetic image editing.</description><author>Yupei Lin, Sen Zhang, Xiaojun Yang, Xiao Wang, Yukai Shi</author><pubDate>Mon, 08 May 2023 13:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04651v1</guid></item><item><title>SImProv: Scalable Image Provenance Framework for Robust Content Attribution</title><link>http://arxiv.org/abs/2206.14245v2</link><description>We present SImProv - a scalable image provenance framework to match a queryimage back to a trusted database of originals and identify possiblemanipulations on the query. SImProv consists of three stages: a scalable searchstage for retrieving top-k most similar images; a re-ranking andnear-duplicated detection stage for identifying the original among thecandidates; and finally a manipulation detection and visualization stage forlocalizing regions within the query that may have been manipulated to differfrom the original. SImProv is robust to benign image transformations thatcommonly occur during online redistribution, such as artifacts due to noise andrecompression degradation, as well as out-of-place transformations due to imagepadding, warping, and changes in size and shape. Robustness towardsout-of-place transformations is achieved via the end-to-end training of adifferentiable warping module within the comparator architecture. Wedemonstrate effective retrieval and manipulation detection over a dataset of100 million images.</description><author>Alexander Black, Tu Bui, Simon Jenni, Zhifei Zhang, Viswanathan Swaminanthan, John Collomosse</author><pubDate>Mon, 08 May 2023 13:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.14245v2</guid></item><item><title>AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis</title><link>http://arxiv.org/abs/2305.01241v2</link><description>The generation of realistic and contextually relevant co-speech gestures is achallenging yet increasingly important task in the creation of multimodalartificial agents. Prior methods focused on learning a direct correspondencebetween co-speech gesture representations and produced motions, which createdseemingly natural but often unconvincing gestures during human assessment. Wepresent an approach to pre-train partial gesture sequences using a generativeadversarial network with a quantization pipeline. The resulting codebookvectors serve as both input and output in our framework, forming the basis forthe generation and reconstruction of gestures. By learning the mapping of alatent space representation as opposed to directly mapping it to a vectorrepresentation, this framework facilitates the generation of highly realisticand expressive gestures that closely replicate human movement and behavior,while simultaneously avoiding artifacts in the generation process. We evaluateour approach by comparing it with established methods for generating co-speechgestures as well as with existing datasets of human behavior. We also performan ablation study to assess our findings. The results show that our approachoutperforms the current state of the art by a clear margin and is partiallyindistinguishable from human gesturing. We make our data pipeline and thegeneration framework publicly available.</description><author>Hendric Voß, Stefan Kopp</author><pubDate>Mon, 08 May 2023 12:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01241v2</guid></item><item><title>CURTAINs Flows For Flows: Constructing Unobserved Regions with Maximum Likelihood Estimation</title><link>http://arxiv.org/abs/2305.04646v1</link><description>Model independent techniques for constructing background data templates usinggenerative models have shown great promise for use in searches for new physicsprocesses at the LHC. We introduce a major improvement to the CURTAINs methodby training the conditional normalizing flow between two side-band regionsusing maximum likelihood estimation instead of an optimal transport loss. Thenew training objective improves the robustness and fidelity of the transformeddata and is much faster and easier to train. We compare the performance against the previous approach and the currentstate of the art using the LHC Olympics anomaly detection dataset, where we seea significant improvement in sensitivity over the original CURTAINs method.Furthermore, CURTAINsF4F requires substantially less computational resources tocover a large number of signal regions than other fully data driven approaches.When using an efficient configuration, an order of magnitude more models can betrained in the same time required for ten signal regions, without a significantdrop in performance.</description><author>Debajyoti Sengupta, Samuel Klein, John Andrew Raine, Tobias Golling</author><pubDate>Mon, 08 May 2023 12:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04646v1</guid></item><item><title>StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning</title><link>http://arxiv.org/abs/2302.09309v2</link><description>Cross-Domain Few-Shot Learning (CD-FSL) is a recently emerging task thattackles few-shot learning across different domains. It aims at transferringprior knowledge learned on the source dataset to novel target datasets. TheCD-FSL task is especially challenged by the huge domain gap between differentdatasets. Critically, such a domain gap actually comes from the changes ofvisual styles, and wave-SAN empirically shows that spanning the styledistribution of the source data helps alleviate this issue. However, wave-SANsimply swaps styles of two images. Such a vanilla operation makes the generatedstyles ``real'' and ``easy'', which still fall into the original set of thesource styles. Thus, inspired by vanilla adversarial learning, a novelmodel-agnostic meta Style Adversarial training (StyleAdv) method together witha novel style adversarial attack method is proposed for CD-FSL. Particularly,our style attack method synthesizes both ``virtual'' and ``hard'' adversarialstyles for model training. This is achieved by perturbing the original stylewith the signed style gradients. By continually attacking styles and forcingthe model to recognize these challenging adversarial styles, our model isgradually robust to the visual styles, thus boosting the generalization abilityfor novel target datasets. Besides the typical CNN-based backbone, we alsoemploy our StyleAdv method on large-scale pretrained vision transformer.Extensive experiments conducted on eight various target datasets show theeffectiveness of our method. Whether built upon ResNet or ViT, we achieve thenew state of the art for CD-FSL. Code is available athttps://github.com/lovelyqian/StyleAdv-CDFSL.</description><author>Yuqian Fu, Yu Xie, Yanwei Fu, Yu-Gang Jiang</author><pubDate>Mon, 08 May 2023 12:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09309v2</guid></item><item><title>SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT Reconstruction</title><link>http://arxiv.org/abs/2303.15748v2</link><description>The deep image prior (DIP) is a well-established unsupervised deep learningmethod for image reconstruction; yet it is far from being flawless. The DIPoverfits to noise if not early stopped, or optimized via a regularizedobjective. We build on the regularized fine-tuning of a pretrained DIP, byadopting a novel strategy that restricts the learning to the adaptation ofsingular values. The proposed SVD-DIP uses ad hoc convolutional layers whosepretrained parameters are decomposed via the singular value decomposition.Optimizing the DIP then solely consists in the fine-tuning of the singularvalues, while keeping the left and right singular vectors fixed. We thoroughlyvalidate the proposed method on real-measured $\mu$CT data of a lotus root aswell as two medical datasets (LoDoPaB and Mayo). We report significantlyimproved stability of the DIP optimization, by overcoming the overfitting tonoise.</description><author>Marco Nittscher, Michael Lameter, Riccardo Barbano, Johannes Leuschner, Bangti Jin, Peter Maass</author><pubDate>Mon, 08 May 2023 12:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15748v2</guid></item><item><title>APR: Online Distant Point Cloud Registration Through Aggregated Point Cloud Reconstruction</title><link>http://arxiv.org/abs/2305.02893v2</link><description>For many driving safety applications, it is of great importance to accuratelyregister LiDAR point clouds generated on distant moving vehicles. However, suchpoint clouds have extremely different point density and sensor perspective onthe same object, making registration on such point clouds very hard. In thispaper, we propose a novel feature extraction framework, called APR, for onlinedistant point cloud registration. Specifically, APR leverages an autoencoderdesign, where the autoencoder reconstructs a denser aggregated point cloud withseveral frames instead of the original single input point cloud. Our designforces the encoder to extract features with rich local geometry informationbased on one single input point cloud. Such features are then used for onlinedistant point cloud registration. We conduct extensive experiments againststate-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets.Results show that APR outperforms all other extractors by a large margin,increasing average registration recall of SOTA extractors by 7.1% on LoKITTIand 4.6% on LoNuScenes. Code is available at https://github.com/liuQuan98/APR.</description><author>Quan Liu, Yunsong Zhou, Hongzi Zhu, Shan Chang, Minyi Guo</author><pubDate>Mon, 08 May 2023 12:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02893v2</guid></item></channel></rss>