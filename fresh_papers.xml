<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 12 Oct 2023 06:00:29 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection</title><link>http://arxiv.org/abs/2310.07716v1</link><description>Object anomaly detection is an important problem in the field of machinevision and has seen remarkable progress recently. However, two significantchallenges hinder its research and application. First, existing datasets lackcomprehensive visual information from various pose angles. They usually have anunrealistic assumption that the anomaly-free training dataset is pose-aligned,and the testing samples have the same pose as the training data. However, inpractice, anomaly may exist in any regions on a object, the training and querysamples may have different poses, calling for the study on pose-agnosticanomaly detection. Second, the absence of a consensus on experimental protocolsfor pose-agnostic anomaly detection leads to unfair comparisons of differentmethods, hindering the research on pose-agnostic anomaly detection. To addressthese issues, we develop Multi-pose Anomaly Detection (MAD) dataset andPose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step toaddress the pose-agnostic anomaly detection problem. Specifically, we build MADusing 20 complex-shaped LEGO toys including 4K views with various poses, andhigh-quality and diverse 3D anomalies in both simulated and real environments.Additionally, we propose a novel method OmniposeAD, trained using MAD,specifically designed for pose-agnostic anomaly detection. Throughcomprehensive evaluations, we demonstrate the relevance of our dataset andmethod. Furthermore, we provide an open-source benchmark library, includingdataset and baseline methods that cover 8 anomaly detection paradigms, tofacilitate future research and application in this domain. Code, data, andmodels are publicly available at https://github.com/EricLee0224/PAD.</description><author>Qiang Zhou, Weize Li, Lihan Jiang, Guoliang Wang, Guyue Zhou, Shanghang Zhang, Hao Zhao</author><pubDate>Wed, 11 Oct 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07716v1</guid></item><item><title>TidyBot: Personalized Robot Assistance with Large Language Models</title><link>http://arxiv.org/abs/2305.05658v2</link><description>For a robot to personalize physical assistance effectively, it must learnuser preferences that can be generally reapplied to future scenarios. In thiswork, we investigate personalization of household cleanup with robots that cantidy up rooms by picking up objects and putting them away. A key challenge isdetermining the proper place to put each object, as people's preferences canvary greatly depending on personal taste or cultural background. For instance,one person may prefer storing shirts in the drawer, while another may preferthem on the shelf. We aim to build systems that can learn such preferences fromjust a handful of examples via prior interactions with a particular person. Weshow that robots can combine language-based planning and perception with thefew-shot summarization capabilities of large language models (LLMs) to infergeneralized user preferences that are broadly applicable to futureinteractions. This approach enables fast adaptation and achieves 91.2% accuracyon unseen objects in our benchmark dataset. We also demonstrate our approach ona real-world mobile manipulator called TidyBot, which successfully puts away85.0% of objects in real-world test scenarios.</description><author>Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser</author><pubDate>Wed, 11 Oct 2023 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05658v2</guid></item><item><title>To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing</title><link>http://arxiv.org/abs/2310.07715v1</link><description>NLP is in a period of disruptive change that is impacting our methodologies,funding sources, and public perception. In this work, we seek to understand howto shape our future by better understanding our past. We study factors thatshape NLP as a field, including culture, incentives, and infrastructure byconducting long-form interviews with 26 NLP researchers of varying seniority,research area, institution, and social identity. Our interviewees identifycyclical patterns in the field, as well as new shifts without historicalparallel, including changes in benchmark culture and software infrastructure.We complement this discussion with quantitative analysis of citation,authorship, and language use in the ACL Anthology over time. We conclude bydiscussing shared visions, concerns, and hopes for the future of NLP. We hopethat this study of our field's past and present can prompt informed discussionof our community's implicit norms and more deliberate action to consciouslyshape the future.</description><author>Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell</author><pubDate>Wed, 11 Oct 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07715v1</guid></item><item><title>InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining</title><link>http://arxiv.org/abs/2310.07713v1</link><description>Pretraining auto-regressive large language models (LLMs) with retrievaldemonstrates better perplexity and factual accuracy by leveraging externaldatabases. However, the size of existing pretrained retrieval-augmented LLM isstill limited (e.g., Retro has 7.5B parameters), which limits the effectivenessof instruction tuning and zero-shot generalization. In this work, we introduceRetro 48B, the largest LLM pretrained with retrieval before instruction tuning.Specifically, we continue to pretrain the 43B GPT model on additional 100billion tokens using the Retro augmentation method by retrieving from 1.2trillion tokens. The obtained foundation model, Retro 48B, largely outperformsthe original 43B GPT in terms of perplexity. After instruction tuning on Retro,InstructRetro demonstrates significant improvement over the instruction tunedGPT on zero-shot question answering (QA) tasks. Specifically, the averageimprovement of InstructRetro is 7% over its GPT counterpart across 8 short-formQA tasks, and 10% over GPT across 4 challenging long-form QA tasks.Surprisingly, we find that one can ablate the encoder from InstructRetroarchitecture and directly use its decoder backbone, while achieving comparableresults. We hypothesize that pretraining with retrieval makes its decoder goodat incorporating context for QA. Our results highlights the promising directionto obtain a better GPT decoder for QA through continued pretraining withretrieval before instruction tuning.</description><author>Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro</author><pubDate>Wed, 11 Oct 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07713v1</guid></item><item><title>Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models</title><link>http://arxiv.org/abs/2310.07712v1</link><description>Large language models (LLMs) exhibit positional bias in how they use context,which especially complicates listwise ranking. To address this, we proposepermutation self-consistency, a form of self-consistency over ranking listoutputs of black-box LLMs. Our key idea is to marginalize out different listorders in the prompt to produce an order-independent ranking with lesspositional bias. First, given some input prompt, we repeatedly shuffle the listin the prompt and pass it through the LLM while holding the instructions thesame. Next, we aggregate the resulting sample of rankings by computing thecentral ranking closest in distance to all of them, marginalizing out promptorder biases in the process. Theoretically, we prove the robustness of ourmethod, showing convergence to the true ranking in the presence of randomperturbations. Empirically, on five list-ranking datasets in sorting andpassage reranking, our approach improves scores from conventional inference byup to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previousstate of the art in passage reranking. Our code is athttps://github.com/castorini/perm-sc.</description><author>Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, Ferhan Ture</author><pubDate>Wed, 11 Oct 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07712v1</guid></item><item><title>Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks</title><link>http://arxiv.org/abs/2310.07711v1</link><description>Recurrent neural networks (RNNs) trained on compositional tasks can exhibitfunctional modularity, in which neurons can be clustered by activity similarityand participation in shared computational subtasks. Unlike brains, these RNNsdo not exhibit anatomical modularity, in which functional clustering iscorrelated with strong recurrent coupling and spatial localization offunctional clusters. Contrasting with functional modularity, which can beephemerally dependent on the input, anatomically modular networks form a robustsubstrate for solving the same subtasks in the future. To examine whether it ispossible to grow brain-like anatomical modularity, we apply a recent machinelearning method, brain-inspired modular training (BIMT), to a network beingtrained to solve a set of compositional cognitive tasks. We find thatfunctional and anatomical clustering emerge together, such that functionallysimilar neurons also become spatially localized and interconnected. Moreover,compared to standard $L_1$ or no regularization settings, the model exhibitssuperior performance by optimally balancing task performance and networksparsity. In addition to achieving brain-like organization in RNNs, ourfindings also suggest that BIMT holds promise for applications in neuromorphiccomputing and enhancing the interpretability of neural network architectures.</description><author>Ziming Liu, Mikail Khona, Ila R. Fiete, Max Tegmark</author><pubDate>Wed, 11 Oct 2023 18:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07711v1</guid></item><item><title>DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models</title><link>http://arxiv.org/abs/2310.07710v1</link><description>Watermarking techniques offer a promising way to secure data via embeddingcovert information into the data. A paramount challenge in the domain lies inpreserving the distribution of original data during watermarking. Our researchextends and refines existing watermarking framework, placing emphasis on theimportance of a distribution-preserving (DiP) watermark. Contrary to thecurrent strategies, our proposed DiPmark preserves the original tokendistribution during watermarking (stealthy), is detectable without access tothe language model API or weights (efficient), and is robust to moderatechanges of tokens (resilient). This is achieved by incorporating a novelreweight strategy, combined with a hash function that assigns unique\textit{i.i.d.} ciphers based on the context. The empirical benchmarks of ourapproach underscore its stealthiness, efficiency, and resilience, making it arobust solution for watermarking tasks that demand impeccable qualitypreservation.</description><author>Yihan Wu, Zhengmian Hu, Hongyang Zhang, Heng Huang</author><pubDate>Wed, 11 Oct 2023 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07710v1</guid></item><item><title>MatFormer: Nested Transformer for Elastic Inference</title><link>http://arxiv.org/abs/2310.07707v1</link><description>Transformer models are deployed in a wide range of settings, frommulti-accelerator clusters to standalone mobile phones. The diverse inferenceconstraints in these scenarios necessitate practitioners to train foundationmodels such as PaLM 2, Llama, &amp; ViTs as a series of models of varying sizes.Due to significant training costs, only a select few model sizes are trainedand supported, limiting more fine-grained control over relevant tradeoffs,including latency, cost, and accuracy. This work introduces MatFormer, a nestedTransformer architecture designed to offer elasticity in a variety ofdeployment constraints. Each Feed Forward Network (FFN) block of a MatFormermodel is jointly optimized with a few nested smaller FFN blocks. This trainingprocedure allows for the Mix'n'Match of model granularities across layers --i.e., a trained universal MatFormer model enables extraction of hundreds ofaccurate smaller models, which were never explicitly optimized. We empiricallydemonstrate MatFormer's effectiveness across different model classes (decoders&amp; encoders), modalities (language &amp; vision), and scales (up to 2.6Bparameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibitingcomparable validation loss and one-shot downstream evaluations to theirindependently trained counterparts. Furthermore, we observe that smallerencoders extracted from a universal MatFormer-based ViT (MatViT) encoderpreserve the metric-space structure for adaptive large-scale retrieval.Finally, we showcase that speculative decoding with the accurate and consistentsubmodels extracted from MatFormer can further reduce inference latency.</description><author>Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain</author><pubDate>Wed, 11 Oct 2023 18:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07707v1</guid></item><item><title>Pixel State Value Network for Combined Prediction and Planning in Interactive Environments</title><link>http://arxiv.org/abs/2310.07706v1</link><description>Automated vehicles operating in urban environments have to reliably interactwith other traffic participants. Planning algorithms often utilize separateprediction modules forecasting probabilistic, multi-modal, and interactivebehaviors of objects. Designing prediction and planning as two separate modulesintroduces significant challenges, particularly due to the interdependence ofthese modules. This work proposes a deep learning methodology to combineprediction and planning. A conditional GAN with the U-Net architecture istrained to predict two high-resolution image sequences. The sequences representexplicit motion predictions, mainly used to train context understanding, andpixel state values suitable for planning encoding kinematic reachability,object dynamics, safety, and driving comfort. The model can be trained offlineon target images rendered by a sampling-based model-predictive planner,leveraging real-world driving data. Our results demonstrate intuitive behaviorin complex situations, such as lane changes amidst conflicting objectives.</description><author>Sascha Rosbach, Stefan M. Leupold, Simon Großjohann, Stefan Roth</author><pubDate>Wed, 11 Oct 2023 18:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07706v1</guid></item><item><title>Ferret: Refer and Ground Anything Anywhere at Any Granularity</title><link>http://arxiv.org/abs/2310.07704v1</link><description>We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable ofunderstanding spatial referring of any shape or granularity within an image andaccurately grounding open-vocabulary descriptions. To unify referring andgrounding in the LLM paradigm, Ferret employs a novel and powerful hybridregion representation that integrates discrete coordinates and continuousfeatures jointly to represent a region in the image. To extract the continuousfeatures of versatile regions, we propose a spatial-aware visual sampler, adeptat handling varying sparsity across different shapes. Consequently, Ferret canaccept diverse region inputs, such as points, bounding boxes, and free-formshapes. To bolster the desired capability of Ferret, we curate GRIT, acomprehensive refer-and-ground instruction tuning dataset including 1.1Msamples that contain rich hierarchical spatial knowledge, with 95K hardnegative data to promote model robustness. The resulting model not onlyachieves superior performance in classical referring and grounding tasks, butalso greatly outperforms existing MLLMs in region-based andlocalization-demanded multimodal chatting. Our evaluations also reveal asignificantly improved capability of describing image details and a remarkablealleviation in object hallucination. Code and data will be available athttps://github.com/apple/ml-ferret</description><author>Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang</author><pubDate>Wed, 11 Oct 2023 18:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07704v1</guid></item><item><title>ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models</title><link>http://arxiv.org/abs/2310.07702v1</link><description>In this work, we investigate the capability of generating images frompre-trained diffusion models at much higher resolutions than the training imagesizes. In addition, the generated images should have arbitrary image aspectratios. When generating images directly at a higher resolution, 1024 x 1024,with the pre-trained Stable Diffusion using training images of resolution 512 x512, we observe persistent problems of object repetition and unreasonableobject structures. Existing works for higher-resolution generation, such asattention-based and joint-diffusion approaches, cannot well address theseissues. As a new perspective, we examine the structural components of the U-Netin diffusion models and identify the crucial cause as the limited perceptionfield of convolutional kernels. Based on this key observation, we propose asimple yet effective re-dilation that can dynamically adjust the convolutionalperception field during inference. We further propose the dispersed convolutionand noise-damped classifier-free guidance, which can enableultra-high-resolution image generation (e.g., 4096 x 4096). Notably, ourapproach does not require any training or optimization. Extensive experimentsdemonstrate that our approach can address the repetition issue well and achievestate-of-the-art performance on higher-resolution image synthesis, especiallyin texture details. Our work also suggests that a pre-trained diffusion modeltrained on low-resolution images can be directly used for high-resolutionvisual generation without further tuning, which may provide insights for futureresearch on ultra-high-resolution image and video synthesis.</description><author>Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan</author><pubDate>Wed, 11 Oct 2023 18:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07702v1</guid></item><item><title>Knowledge-enhanced Memory Model for Emotional Support Conversation</title><link>http://arxiv.org/abs/2310.07700v1</link><description>The prevalence of mental disorders has become a significant issue, leading tothe increased focus on Emotional Support Conversation as an effectivesupplement for mental health support. Existing methods have achieved compellingresults, however, they still face three challenges: 1) variability of emotions,2) practicality of the response, and 3) intricate strategy modeling. To addressthese challenges, we propose a novel knowledge-enhanced Memory mODEl foremotional suppoRt coNversation (MODERN). Specifically, we first devise aknowledge-enriched dialogue context encoding to perceive the dynamic emotionchange of different periods of the conversation for coherent user statemodeling and select context-related concepts from ConceptNet for practicalresponse generation. Thereafter, we implement a novel memory-enhanced strategymodeling module to model the semantic patterns behind the strategy categories.Extensive experiments on a widely used large-scale dataset verify thesuperiority of our model over cutting-edge baselines.</description><author>Mengzhao Jia, Qianglong Chen, Liqiang Jing, Dawei Fu, Renyu Li</author><pubDate>Wed, 11 Oct 2023 18:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07700v1</guid></item><item><title>From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions</title><link>http://arxiv.org/abs/2310.07699v1</link><description>Web-crawled datasets are pivotal to the success of pre-trainingvision-language models, exemplified by CLIP. However, web-crawled AltTexts canbe noisy and potentially irrelevant to images, thereby undermining the crucialimage-text alignment. Existing methods for rewriting captions using largelanguage models (LLMs) have shown promise on small, curated datasets like CC3Mand CC12M. Nevertheless, their efficacy on massive web-captured captions isconstrained by the inherent noise and randomness in such data. In this study,we address this limitation by focusing on two key aspects: data quality anddata variety. Unlike recent LLM rewriting techniques, we emphasize exploitingvisual concepts and their integration into the captions to improve dataquality. For data variety, we propose a novel mixed training scheme thatoptimally leverages AltTexts alongside newly generated Visual-enriched Captions(VeC). We use CLIP as one example and adapt the method for CLIP training onlarge-scale web-crawled datasets, named VeCLIP. We conduct a comprehensiveevaluation of VeCLIP across small, medium, and large scales of raw data. Ourresults show significant advantages in image-text alignment and overall modelperformance, underscoring the effectiveness of VeCLIP in improving CLIPtraining. For example, VeCLIP achieves a remarkable over 20% improvement inCOCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,we also achieve a notable over 3% improvement while using only 14% of the dataemployed in the vanilla CLIP and 11% in ALIGN.</description><author>Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, Meng Cao</author><pubDate>Wed, 11 Oct 2023 18:49:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07699v1</guid></item><item><title>SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation</title><link>http://arxiv.org/abs/2310.07698v1</link><description>Explainable AI seeks to bring light to the decision-making processes ofblack-box models. Traditional saliency-based methods, while highlightinginfluential data segments, often lack semantic understanding. Recentadvancements, such as Concept Activation Vectors (CAVs) and Concept BottleneckModels (CBMs), offer concept-based explanations but necessitate human-definedconcepts. However, human-annotated concepts are expensive to attain. This paperintroduces the Concept Bottleneck Surrogate Models (SurroCBM), a novelframework that aims to explain the black-box models with automaticallydiscovered concepts. SurroCBM identifies shared and unique concepts acrossvarious black-box models and employs an explainable surrogate model forpost-hoc explanations. An effective training strategy using self-generated datais proposed to enhance explanation quality continuously. Through extensiveexperiments, we demonstrate the efficacy of SurroCBM in concept discovery andexplanation, underscoring its potential in advancing the field of explainableAI.</description><author>Bo Pan, Zhenke Liu, Yifei Zhang, Liang Zhao</author><pubDate>Wed, 11 Oct 2023 18:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07698v1</guid></item><item><title>ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation</title><link>http://arxiv.org/abs/2310.07697v1</link><description>Recent works have successfully extended large-scale text-to-image models tothe video domain, producing promising results but at a high computational costand requiring a large amount of video data. In this work, we introduceConditionVideo, a training-free approach to text-to-video generation based onthe provided condition, video, and input text, by leveraging the power ofoff-the-shelf text-to-image generation methods (e.g., Stable Diffusion).ConditionVideo generates realistic dynamic videos from random noise or givenscene videos. Our method explicitly disentangles the motion representation intocondition-guided and scenery motion components. To this end, the ConditionVideomodel is designed with a UNet branch and a control branch. To improve temporalcoherence, we introduce sparse bi-directional spatial-temporal attention(sBiST-Attn). The 3D control network extends the conventional 2D controlnetmodel, aiming to strengthen conditional generation accuracy by additionallyleveraging the bi-directional frames in the temporal domain. Our methodexhibits superior performance in terms of frame consistency, clip score, andconditional accuracy, outperforming other compared methods.</description><author>Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, Yu Qiao</author><pubDate>Wed, 11 Oct 2023 18:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07697v1</guid></item><item><title>FateZero: Fusing Attentions for Zero-shot Text-based Video Editing</title><link>http://arxiv.org/abs/2303.09535v3</link><description>The diffusion-based generative models have achieved remarkable success intext-based image generation. However, since it contains enormous randomness ingeneration progress, it is still challenging to apply such models forreal-world visual content editing, especially in videos. In this paper, wepropose FateZero, a zero-shot text-based editing method on real-world videoswithout per-prompt training or use-specific mask. To edit videos consistently,we propose several techniques based on the pre-trained models. Firstly, incontrast to the straightforward DDIM inversion technique, our approach capturesintermediate attention maps during inversion, which effectively retain bothstructural and motion information. These maps are directly fused in the editingprocess rather than generated during denoising. To further minimize semanticleakage of the source video, we then fuse self-attentions with a blending maskobtained by cross-attention features from the source prompt. Furthermore, wehave implemented a reform of the self-attention mechanism in denoising UNet byintroducing spatial-temporal attention to ensure frame consistency. Yetsuccinct, our method is the first one to show the ability of zero-shottext-driven video style and local attribute editing from the trainedtext-to-image model. We also have a better zero-shot shape-aware editingability based on the text-to-video model. Extensive experiments demonstrate oursuperior temporal consistency and editing capability than previous works.</description><author>Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen</author><pubDate>Wed, 11 Oct 2023 18:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09535v3</guid></item><item><title>Efficient Transformer-based 3D Object Detection with Dynamic Token Halting</title><link>http://arxiv.org/abs/2303.05078v2</link><description>Balancing efficiency and accuracy is a long-standing problem for deployingdeep learning models. The trade-off is even more important for real-timesafety-critical systems like autonomous vehicles. In this paper, we propose aneffective approach for accelerating transformer-based 3D object detectors bydynamically halting tokens at different layers depending on their contributionto the detection task. Although halting a token is a non-differentiableoperation, our method allows for differentiable end-to-end learning byleveraging an equivalent differentiable forward-pass. Furthermore, ourframework allows halted tokens to be reused to inform the model's predictionsthrough a straightforward token recycling mechanism. Our method significantlyimproves the Pareto frontier of efficiency versus accuracy when compared withthe existing approaches. By halting tokens and increasing model capacity, weare able to improve the baseline model's performance without increasing themodel's latency on the Waymo Open Dataset.</description><author>Mao Ye, Gregory P. Meyer, Yuning Chai, Qiang Liu</author><pubDate>Wed, 11 Oct 2023 18:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05078v2</guid></item><item><title>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</title><link>http://arxiv.org/abs/2303.08896v3</link><description>Generative Large Language Models (LLMs) such as GPT-3 are capable ofgenerating highly fluent responses to a wide variety of user prompts. However,LLMs are known to hallucinate facts and make non-factual statements which canundermine trust in their output. Existing fact-checking approaches eitherrequire access to the output probability distribution (which may not beavailable for systems such as ChatGPT) or external databases that areinterfaced via separate, often complex, modules. In this work, we propose"SelfCheckGPT", a simple sampling-based approach that can be used to fact-checkthe responses of black-box models in a zero-resource fashion, i.e. without anexternal database. SelfCheckGPT leverages the simple idea that if an LLM hasknowledge of a given concept, sampled responses are likely to be similar andcontain consistent facts. However, for hallucinated facts, stochasticallysampled responses are likely to diverge and contradict one another. Weinvestigate this approach by using GPT-3 to generate passages about individualsfrom the WikiBio dataset, and manually annotate the factuality of the generatedpassages. We demonstrate that SelfCheckGPT can: i) detect non-factual andfactual sentences; and ii) rank passages in terms of factuality. We compare ourapproach to several baselines and show that our approach has considerablyhigher AUC-PR scores in sentence-level hallucination detection and highercorrelation scores in passage-level factuality assessment compared to grey-boxmethods.</description><author>Potsawee Manakul, Adian Liusie, Mark J. F. Gales</author><pubDate>Wed, 11 Oct 2023 18:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08896v3</guid></item><item><title>Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series</title><link>http://arxiv.org/abs/2203.11196v2</link><description>Deep Learning and transfer learning models are being used to generate timeseries forecasts; however, there is scarce evidence about their performanceprediction that it is more evident for monthly time series. The purpose of thispaper is to compare Deep Learning models with transfer learning and withouttransfer learning and other traditional methods used for monthly forecasts toanswer three questions about the suitability of Deep Learning and TransferLearning to generate predictions of time series. Time series of M4 and M3competitions were used for the experiments. The results suggest that deeplearning models based on TCN, LSTM, and CNN with transfer learning tend tosurpass the performance prediction of other traditional methods. On the otherhand, TCN and LSTM, trained directly on the target time series, got similar orbetter performance than traditional methods for some forecast horizons.</description><author>Martín Solís, Luis-Alexander Calvo-Valverde</author><pubDate>Wed, 11 Oct 2023 18:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11196v2</guid></item><item><title>Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole</title><link>http://arxiv.org/abs/2310.07687v1</link><description>The interaction between the supermassive black hole at the center of theMilky Way, Sagittarius A$^*$, and its accretion disk, occasionally produceshigh energy flares seen in X-ray, infrared and radio. One mechanism forobserved flares is the formation of compact bright regions that appear withinthe accretion disk and close to the event horizon. Understanding these flarescan provide a window into black hole accretion processes. Althoughsophisticated simulations predict the formation of these flares, theirstructure has yet to be recovered by observations. Here we show the firstthree-dimensional (3D) reconstruction of an emission flare in orbit recoveredfrom ALMA light curves observed on April 11, 2017. Our recovery results showcompact bright regions at a distance of roughly 6 times the event horizon.Moreover, our recovery suggests a clockwise rotation in a low-inclinationorbital plane, a result consistent with prior studies by EHT and GRAVITYcollaborations. To recover this emission structure we solve a highly ill-posedtomography problem by integrating a neural 3D representation (an emergentartificial intelligence approach for 3D reconstruction) with a gravitationalmodel for black holes. Although the recovered 3D structure is subject, andsometimes sensitive, to the model assumptions, under physically motivatedchoices we find that our results are stable and our approach is successful onsimulated data. We anticipate that in the future, this approach could be usedto analyze a richer collection of time-series data that could shed light on themechanisms governing black hole and plasma dynamics.</description><author>Aviad Levis, Andrew A. Chael, Katherine L. Bouman, Maciek Wielgus, Pratul P. Srinivasan</author><pubDate>Wed, 11 Oct 2023 18:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07687v1</guid></item><item><title>Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications</title><link>http://arxiv.org/abs/2310.04381v2</link><description>In this paper, we present Hermes, an end-to-end framework to automaticallygenerate formal representations from natural language cellular specifications.We first develop a neural constituency parser, NEUTREX, to processtransition-relevant texts and extract transition components (i.e., states,conditions, and actions). We also design a domain-specific language totranslate these transition components to logical formulas by leveragingdependency parse trees. Finally, we compile these logical formulas to generatetransitions and create the formal model as finite state machines. Todemonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and5G RRC specifications and obtain an overall accuracy of 81-87%, which is asubstantial improvement over the state-of-the-art. Our security analysis of theextracted models uncovers 3 new vulnerabilities and identifies 19 previousattacks in 4G and 5G specifications, and 7 deviations in commercial 4Gbasebands.</description><author>Abdullah Al Ishtiaq, Sarkar Snigdha Sarathi Das, Syed Md Mukit Rashid, Ali Ranjbar, Kai Tu, Tianwei Wu, Zhezheng Song, Weixuan Wang, Mujtahid Akon, Rui Zhang, Syed Rafiul Hussain</author><pubDate>Wed, 11 Oct 2023 18:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04381v2</guid></item><item><title>Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design</title><link>http://arxiv.org/abs/2310.07684v1</link><description>Most of the current hypergraph learning methodologies and benchmarkingdatasets in the hypergraph realm are obtained by lifting procedures from theirgraph analogs, simultaneously leading to overshadowing hypergraph networkfoundations. This paper attempts to confront some pending questions in thatregard: Can the concept of homophily play a crucial role in Hypergraph NeuralNetworks (HGNNs), similar to its significance in graph-based research? Is thereroom for improving current hypergraph architectures and methodologies? (e.g. bycarefully addressing the specific characteristics of higher-order networks) Doexisting datasets provide a meaningful benchmark for HGNNs? Diving into thedetails, this paper proposes a novel conceptualization of homophily inhigher-order networks based on a message passing scheme; this approachharmonizes the analytical frameworks of datasets and architectures, offering aunified perspective for exploring and interpreting complex, higher-ordernetwork structures and dynamics. Further, we propose MultiSet, a novel messagepassing framework that redefines HGNNs by allowing hyperedge-dependent noderepresentations, as well as introduce a novel architecture MultiSetMixer thatleverages a new hyperedge sampling strategy. Finally, we provide an extensiveset of experiments that contextualize our proposals and lead to valuableinsights in hypergraph representation learning.</description><author>Lev Telyatnikov, Maria Sofia Bucarelli, Guillermo Bernardez, Olga Zaghen, Simone Scardapane, Pietro Lio</author><pubDate>Wed, 11 Oct 2023 18:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07684v1</guid></item><item><title>Controllable Data Generation Via Iterative Data-Property Mutual Mappings</title><link>http://arxiv.org/abs/2310.07683v1</link><description>Deep generative models have been widely used for their ability to generaterealistic data samples in various areas, such as images, molecules, text, andspeech. One major goal of data generation is controllability, namely togenerate new data with desired properties. Despite growing interest in the areaof controllable generation, significant challenges still remain, including 1)disentangling desired properties with unrelated latent variables, 2)out-of-distribution property control, and 3) objective optimization forout-of-distribution property control. To address these challenges, in thispaper, we propose a general framework to enhance VAE-based data generators withproperty controllability and ensure disentanglement. Our proposed objective canbe optimized on both data seen and unseen in the training set. We propose atraining procedure to train the objective in a semi-supervised manner byiteratively conducting mutual mappings between the data and properties. Theproposed framework is implemented on four VAE-based controllable generators toevaluate its performance on property error, disentanglement, generationquality, and training time. The results indicate that our proposed frameworkenables more precise control over the properties of generated samples in ashort training time, ensuring the disentanglement and keeping the validity ofthe generated samples.</description><author>Bo Pan, Muran Qin, Shiyu Wang, Yifei Zhang, Liang Zhao</author><pubDate>Wed, 11 Oct 2023 18:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07683v1</guid></item><item><title>VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution</title><link>http://arxiv.org/abs/2306.12424v2</link><description>We introduce VisoGender, a novel dataset for benchmarking gender bias invision-language models. We focus on occupation-related biases within ahegemonic system of binary gender, inspired by Winograd and Winogender schemas,where each image is associated with a caption containing a pronoun relationshipof subjects and objects in the scene. VisoGender is balanced by genderrepresentation in professional roles, supporting bias evaluation in two ways:i) resolution bias, where we evaluate the difference between pronoun resolutionaccuracies for image subjects with gender presentations perceived as masculineversus feminine by human annotators and ii) retrieval bias, where we compareratios of professionals perceived to have masculine and feminine genderpresentations retrieved for a gender-neutral search query. We benchmark severalstate-of-the-art vision-language models and find that they demonstrate bias inresolving binary gender in complex scenes. While the direction and magnitude ofgender bias depends on the task and the model being evaluated, captioningmodels are generally less biased than Vision-Language Encoders. Dataset andcode are available at https://github.com/oxai/visogender</description><author>Siobhan Mackenzie Hall, Fernanda Gonçalves Abrantes, Hanwen Zhu, Grace Sodunke, Aleksandar Shtedritski, Hannah Rose Kirk</author><pubDate>Wed, 11 Oct 2023 18:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12424v2</guid></item><item><title>Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images</title><link>http://arxiv.org/abs/2310.07682v1</link><description>MET protein overexpression is a targetable event in non-small cell lungcancer (NSCLC) and is the subject of active drug development. Challenges inidentifying patients for these therapies include lack of access to validatedtesting, such as standardized immunohistochemistry (IHC) assessment, andconsumption of valuable tissue for a single gene/protein assay. Development ofpre-screening algorithms using routinely available digitized hematoxylin andeosin (H&amp;E)-stained slides to predict MET overexpression could promote testingfor those who will benefit most. While assessment of MET expression using IHCis currently not routinely performed in NSCLC, next-generation sequencing iscommon and in some cases includes RNA expression panel testing. In this work,we leveraged a large database of matched H&amp;E slides and RNA expression data totrain a weakly supervised model to predict MET RNA overexpression directly fromH&amp;E images. This model was evaluated on an independent holdout test set of 300over-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95thpercentile interval: 0.66 - 0.74) with stable performance characteristicsacross different patient clinical variables and robust to synthetic noise onthe test set. These results suggest that H&amp;E-based predictive models could beuseful to prioritize patients for confirmatory testing of MET protein or METgene expression status.</description><author>Kshitij Ingale, Sun Hae Hong, Josh S. K. Bell, Abbas Rizvi, Amy Welch, Lingdao Sha, Irvin Ho, Kunal Nagpal, Aicha BenTaieb, Rohan P Joshi, Martin C Stumpe</author><pubDate>Wed, 11 Oct 2023 18:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07682v1</guid></item><item><title>Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM</title><link>http://arxiv.org/abs/2310.07678v1</link><description>With the proliferation of image-based applications in various domains, theneed for accurate and interpretable image similarity measures has becomeincreasingly critical. Existing image similarity models often lacktransparency, making it challenging to understand the reasons why two imagesare considered similar. In this paper, we propose the concept of explainableimage similarity, where the goal is the development of an approach, which iscapable of providing similarity scores along with visual factual andcounterfactual explanations. Along this line, we present a new framework, whichintegrates Siamese Networks and Grad-CAM for providing explainable imagesimilarity and discuss the potential benefits and challenges of adopting thisapproach. In addition, we provide a comprehensive discussion about factual andcounterfactual explanations provided by the proposed framework for assistingdecision making. The proposed approach has the potential to enhance theinterpretability, trustworthiness and user acceptance of image-based systems inreal-world image similarity applications. The implementation code can be foundin https://github.com/ioannislivieris/Grad_CAM_Siamese.git.</description><author>Ioannis E. Livieris, Emmanuel Pintelas, Niki Kiriakidou, Panagiotis Pintelas</author><pubDate>Wed, 11 Oct 2023 18:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07678v1</guid></item><item><title>Composite Backdoor Attacks Against Large Language Models</title><link>http://arxiv.org/abs/2310.07676v1</link><description>Large language models (LLMs) have demonstrated superior performance comparedto previous methods on various tasks, and often serve as the foundation modelsfor many researches and services. However, the untrustworthy third-party LLMsmay covertly introduce vulnerabilities for downstream tasks. In this paper, weexplore the vulnerability of LLMs through the lens of backdoor attacks.Different from existing backdoor attacks against LLMs, ours scatters multipletrigger keys in different prompt components. Such a Composite Backdoor Attack(CBA) is shown to be stealthier than implanting the same multiple trigger keysin only a single component. CBA ensures that the backdoor is activated onlywhen all trigger keys appear. Our experiments demonstrate that CBA is effectivein both natural language processing (NLP) and multimodal tasks. For instance,with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,our attack achieves a $100\%$ Attack Success Rate (ASR) with a False TriggeredRate (FTR) below $2.06\%$ and negligible model accuracy degradation. The uniquecharacteristics of our CBA can be tailored for various practical scenarios,e.g., targeting specific user groups. Our work highlights the necessity ofincreased security research on the trustworthiness of foundation LLMs.</description><author>Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</author><pubDate>Wed, 11 Oct 2023 18:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07676v1</guid></item><item><title>Stabilizing Estimates of Shapley Values with Control Variates</title><link>http://arxiv.org/abs/2310.07672v1</link><description>Shapley values are among the most popular tools for explaining predictions ofblackbox machine learning models. However, their high computational costmotivates the use of sampling approximations, inducing a considerable degree ofuncertainty. To stabilize these model explanations, we propose ControlSHAP, anapproach based on the Monte Carlo technique of control variates. Ourmethodology is applicable to any machine learning model and requires virtuallyno extra computation or modeling effort. On several high-dimensional datasets,we find it can produce dramatic reductions in the Monte Carlo variability ofShapley estimates.</description><author>Jeremy Goldwasser, Giles Hooker</author><pubDate>Wed, 11 Oct 2023 18:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07672v1</guid></item><item><title>HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D Semantic Segmentation</title><link>http://arxiv.org/abs/2310.07669v1</link><description>Signals from different modalities each have their own combination algebrawhich affects their sampling processing. RGB is mostly linear; depth is ageometric signal following the operations of mathematical morphology. If anetwork obtaining RGB-D input has both kinds of operators available in itslayers, it should be able to give effective output with fewer parameters. Inthis paper, morphological elements in conjunction with more familiar linearmodules are used to construct a mixed linear-morphological network calledHaarNet. This is the first large-scale linear-morphological hybrid, evaluatedon a set of sizeable real-world datasets. In the network, morphological Haarsampling is applied to both feature channels in several layers, which splitsextreme values and high-frequency information such that both can be processedto improve both modalities. Moreover, morphologically parameterised ReLU isused, and morphologically-sound up-sampling is applied to obtain afull-resolution output. Experiments show that HaarNet is competitive with astate-of-the-art CNN, implying that morphological networks are a promisingresearch direction for geometry-based learning tasks.</description><author>Rick Groenendijk, Leo Dorst, Theo Gevers</author><pubDate>Wed, 11 Oct 2023 18:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07669v1</guid></item><item><title>GRaMuFeN: Graph-based Multi-modal Fake News Detection in Social Media</title><link>http://arxiv.org/abs/2310.07668v1</link><description>The proliferation of social media platforms such as Twitter, Instagram, andWeibo has significantly enhanced the dissemination of false information. Thisphenomenon grants both individuals and governmental entities the ability toshape public opinions, highlighting the need for deploying effective detectionmethods. In this paper, we propose GraMuFeN, a model designed to detect fakecontent by analyzing both the textual and image content of news. GraMuFeNcomprises two primary components: a text encoder and an image encoder. Fortextual analysis, GraMuFeN treats each text as a graph and employs a GraphConvolutional Neural Network (GCN) as the text encoder. Additionally, thepre-trained ResNet-152, as a Convolutional Neural Network (CNN), has beenutilized as the image encoder. By integrating the outputs from these twoencoders and implementing a contrastive similarity loss function, GraMuFeNachieves remarkable results. Extensive evaluations conducted on two publiclyavailable benchmark datasets for social media news indicate a 10 % increase inmicro F1-Score, signifying improvement over existing state-of-the-art models.These findings underscore the effectiveness of combining GCN and CNN models fordetecting fake news in multi-modal data, all while minimizing the additionalcomputational burden imposed by model parameters.</description><author>Makan Kananian, Fatima Badiei, S. AmirAli Gh. Ghahramani</author><pubDate>Wed, 11 Oct 2023 18:17:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07668v1</guid></item><item><title>Global Minima, Recoverability Thresholds, and Higher-Order Structure in GNNS</title><link>http://arxiv.org/abs/2310.07667v1</link><description>We analyze the performance of graph neural network (GNN) architectures fromthe perspective of random graph theory. Our approach promises to complementexisting lenses on GNN analysis, such as combinatorial expressive power andworst-case adversarial analysis, by connecting the performance of GNNs totypical-case properties of the training data. First, we theoreticallycharacterize the nodewise accuracy of one- and two-layer GCNs relative to thecontextual stochastic block model (cSBM) and related models. We additionallyprove that GCNs cannot beat linear models under certain circumstances. Second,we numerically map the recoverability thresholds, in terms of accuracy, of fourdiverse GNN architectures (GCN, GAT, SAGE, and Graph Transformer) under avariety of assumptions about the data. Sample results of this second analysisinclude: heavy-tailed degree distributions enhance GNN performance, GNNs canwork well on strongly heterophilous graphs, and SAGE and Graph Transformer canperform well on arbitrarily noisy edge data, but no architecture handledsufficiently noisy feature data well. Finally, we show how both specifichigher-order structures in synthetic data and the mix of empirical structuresin real data have dramatic effects (usually negative) on GNN performance.</description><author>Drake Brown, Trevor Garrity, Kaden Parker, Jason Oliphant, Stone Carson, Cole Hanson, Zachary Boyd</author><pubDate>Wed, 11 Oct 2023 18:16:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07667v1</guid></item><item><title>High-dimensional and universally consistent k-sample tests</title><link>http://arxiv.org/abs/1910.08883v4</link><description>The k-sample testing problem involves determining whether $k$ groups of datapoints are each drawn from the same distribution. The standard method fork-sample testing in biomedicine is Multivariate analysis of variance (MANOVA),despite that it depends on strong, and often unsuitable, parametricassumptions. Moreover, independence testing and k-sample testing are closelyrelated, and several universally consistent high-dimensional independence testssuch as distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion(Hsic) enjoy solid theoretical and empirical properties. In this paper, weprove that independence tests achieve universally consistent k-sample testingand that k-sample statistics such as Energy and Maximum Mean Discrepancy (MMD)are precisely equivalent to Dcorr. An empirical evaluation of nonparametricindependence tests showed that they generally perform better than the popularMANOVA test, even in Gaussian distributed scenarios. The evaluation includedseveral popular independence statistics and covered a comprehensive set ofsimulations. Additionally, the testing approach was extended to performmultiway and multilevel tests, which were demonstrated in a simulated study aswell as a real-world fMRI brain scans with a set of attributes.</description><author>Sambit Panda, Cencheng Shen, Ronan Perry, Jelle Zorn, Antoine Lutz, Carey E. Priebe, Joshua T. Vogelstein</author><pubDate>Wed, 11 Oct 2023 18:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.08883v4</guid></item><item><title>Deep Backtracking Counterfactuals for Causally Compliant Explanations</title><link>http://arxiv.org/abs/2310.07665v1</link><description>Counterfactuals can offer valuable insights by answering what would have beenobserved under altered circumstances, conditional on a factual observation.Whereas the classical interventional interpretation of counterfactuals has beenstudied extensively, backtracking constitutes a less studied alternative thebacktracking principle has emerged as an alternative philosophy where allcausal laws are kept intact. In the present work, we introduce a practicalmethod for computing backtracking counterfactuals in structural causal modelsthat consist of deep generative components. To this end, we impose conditionson the structural assignments that enable the generation of counterfactuals bysolving a tractable constrained optimization problem in the structured latentspace of a causal model. Our formulation also facilitates a comparison withmethods in the field of counterfactual explanations. Compared to these, ourmethod represents a versatile, modular and causally compliant alternative. Wedemonstrate these properties experimentally on a modified version of MNIST andCelebA.</description><author>Klaus-Rudolf Kladny, Julius von Kügelgen, Bernhard Schölkopf, Michael Muehlebach</author><pubDate>Wed, 11 Oct 2023 18:11:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07665v1</guid></item><item><title>Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health</title><link>http://arxiv.org/abs/2301.06577v2</link><description>When data is scarce, software analytics can make many mistakes. For example,consider learning predictors for open source project health (e.g. the number ofclosed pull requests in twelve months time). The training data for this taskmay be very small (e.g. five years of data, collected every month means just 60rows of training data). The models generated from such tiny data sets can makemany prediction errors. Those errors can be tamed by a {\em landscape analysis} that selects betterlearner control parameters. Our niSNEAK tool (a)~clusters the data to find thegeneral landscape of the hyperparameters; then (b)~explores a fewrepresentatives from each part of that landscape. niSNEAK is both faster andmore effective than prior state-of-the-art hyperparameter optimizationalgorithms (e.g. FLASH, HYPEROPT, OPTUNA). The configurations found by niSNEAK have far less error than other methods.For example, for project health indicators such as $C$= number of commits;$I$=number of closed issues, and $R$=number of closed pull requests, niSNEAK's12 month prediction errors are \{I=0\%, R=33\%\,C=47\%\} Based on the above, we recommend landscape analytics (e.g. niSNEAK)especially when learning from very small data sets. This paper only exploresthe application of niSNEAK to project health. That said, we see nothing inprinciple that prevents the application of this technique to a wider range ofproblems. To assist other researchers in repeating, improving, or even refuting ourresults, all our scripts and data are available on GitHub athttps://github.com/zxcv123456qwe/niSneak</description><author>Andre Lustosa, Tim Menzies</author><pubDate>Wed, 11 Oct 2023 18:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06577v2</guid></item><item><title>Accelerating Vision Transformers Based on Heterogeneous Attention Patterns</title><link>http://arxiv.org/abs/2310.07664v1</link><description>Recently, Vision Transformers (ViTs) have attracted a lot of attention in thefield of computer vision. Generally, the powerful representative capacity ofViTs mainly benefits from the self-attention mechanism, which has a highcomputation complexity. To accelerate ViTs, we propose an integratedcompression pipeline based on observed heterogeneous attention patterns acrosslayers. On one hand, different images share more similar attention patterns inearly layers than later layers, indicating that the dynamic query-by-keyself-attention matrix may be replaced with a static self-attention matrix inearly layers. Then, we propose a dynamic-guided static self-attention (DGSSA)method where the matrix inherits self-attention information from the replaceddynamic self-attention to effectively improve the feature representationability of ViTs. On the other hand, the attention maps have more low-rankpatterns, which reflect token redundancy, in later layers than early layers. Ina view of linear dimension reduction, we further propose a method of globalaggregation pyramid (GLAD) to reduce the number of tokens in later layers ofViTs, such as Deit. Experimentally, the integrated compression pipeline ofDGSSA and GLAD can accelerate up to 121% run-time throughput compared withDeiT, which surpasses all SOTA approaches.</description><author>Deli Yu, Teng Xi, Jianwei Li, Baopu Li, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang</author><pubDate>Wed, 11 Oct 2023 18:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07664v1</guid></item><item><title>Deep Video Inpainting Guided by Audio-Visual Self-Supervision</title><link>http://arxiv.org/abs/2310.07663v1</link><description>Humans can easily imagine a scene from auditory information based on theirprior knowledge of audio-visual events. In this paper, we mimic this innatehuman ability in deep learning models to improve the quality of videoinpainting. To implement the prior knowledge, we first train the audio-visualnetwork, which learns the correspondence between auditory and visualinformation. Then, the audio-visual network is employed as a guider thatconveys the prior knowledge of audio-visual correspondence to the videoinpainting network. This prior knowledge is transferred through our proposedtwo novel losses: audio-visual attention loss and audio-visual pseudo-classconsistency loss. These two losses further improve the performance of the videoinpainting by encouraging the inpainting result to have a high correspondenceto its synchronized audio. Experimental results demonstrate that our proposedmethod can restore a wider domain of video scenes and is particularly effectivewhen the sounding object in the scene is partially blinded.</description><author>Kyuyeon Kim, Junsik Jung, Woo Jae Kim, Sung-Eui Yoon</author><pubDate>Wed, 11 Oct 2023 18:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07663v1</guid></item><item><title>Schema-adaptable Knowledge Graph Construction</title><link>http://arxiv.org/abs/2305.08703v3</link><description>Conventional Knowledge Graph Construction (KGC) approaches typically followthe static information extraction paradigm with a closed set of pre-definedschema. As a result, such approaches fall short when applied to dynamicscenarios or domains, whereas a new type of knowledge emerges. Thisnecessitates a system that can handle evolving schema automatically to extractinformation for KGC. To address this need, we propose a new task calledschema-adaptable KGC, which aims to continually extract entity, relation, andevent based on a dynamically changing schema graph without re-training. Wefirst split and convert existing datasets based on three principles to build abenchmark, i.e., horizontal schema expansion, vertical schema expansion, andhybrid schema expansion; then investigate the schema-adaptable performance ofseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. Wefurther propose a simple yet effective baseline dubbed \textsc{AdaKGC}, whichcontains schema-enriched prefix instructor and schema-conditioned dynamicdecoding to better handle evolving schema. Comprehensive experimental resultsillustrate that AdaKGC can outperform baselines but still have room forimprovement. We hope the proposed work can deliver benefits to the community.Code and datasets available at https://github.com/zjunlp/AdaKGC.</description><author>Hongbin Ye, Honghao Gui, Xin Xu, Huajun Chen, Ningyu Zhang</author><pubDate>Wed, 11 Oct 2023 18:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08703v3</guid></item><item><title>Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue</title><link>http://arxiv.org/abs/2310.07659v1</link><description>Accurate knowledge selection is critical in knowledge-grounded dialoguesystems. Towards a closer look at it, we offer a novel perspective to organizeexisting literature, i.e., knowledge selection coupled with, after, and beforegeneration. We focus on the third under-explored category of study, which cannot only select knowledge accurately in advance, but has the advantage toreduce the learning, adjustment, and interpretation burden of subsequentresponse generation models, especially LLMs. We propose GATE, agenerator-agnostic knowledge selection method, to prepare knowledge forsubsequent response generation models by selecting context-related knowledgeamong different knowledge structures and variable knowledge requirements.Experimental results demonstrate the superiority of GATE, and indicate thatknowledge selection before generation is a lightweight yet effective way tofacilitate LLMs (e.g., ChatGPT) to generate more informative responses.</description><author>Qin Lang, Zhang Yao, Liang Hongru, Wang jun, Yang Zhenglu</author><pubDate>Wed, 11 Oct 2023 18:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07659v1</guid></item><item><title>The First Pathloss Radio Map Prediction Challenge</title><link>http://arxiv.org/abs/2310.07658v1</link><description>To foster research and facilitate fair comparisons among recently proposedpathloss radio map prediction methods, we have launched the ICASSP 2023 FirstPathloss Radio Map Prediction Challenge. In this short overview paper, webriefly describe the pathloss prediction problem, the provided datasets, thechallenge task and the challenge evaluation methodology. Finally, we presentthe results of the challenge.</description><author>Çağkan Yapar, Fabian Jaensch, Ron Levie, Gitta Kutyniok, Giuseppe Caire</author><pubDate>Wed, 11 Oct 2023 18:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07658v1</guid></item><item><title>Audio-Visual Neural Syntax Acquisition</title><link>http://arxiv.org/abs/2310.07654v1</link><description>We study phrase structure induction from visually-grounded speech. The coreidea is to first segment the speech waveform into sequences of word segments,and subsequently induce phrase structure using the inferred segment-levelcontinuous representations. We present the Audio-Visual Neural Syntax Learner(AV-NSL) that learns phrase structure by listening to audio and looking atimages, without ever being exposed to text. By training on paired images andspoken captions, AV-NSL exhibits the capability to infer meaningful phrasestructures that are comparable to those derived by naturally-supervised textparsers, for both English and German. Our findings extend prior work inunsupervised language acquisition from speech and grounded grammar induction,and present one approach to bridge the gap between the two topics.</description><author>Cheng-I Jeff Lai, Freda Shi, Puyuan Peng, Yoon Kim, Kevin Gimpel, Shiyu Chang, Yung-Sung Chuang, Saurabhchand Bhati, David Cox, David Harwath, Yang Zhang, Karen Livescu, James Glass</author><pubDate>Wed, 11 Oct 2023 17:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07654v1</guid></item><item><title>Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models</title><link>http://arxiv.org/abs/2310.07653v1</link><description>The revolution of artificial intelligence content generation has been rapidlyaccelerated with the booming text-to-image (T2I) diffusion models. Within justtwo years of development, it was unprecedentedly of high-quality, diversity,and creativity that the state-of-the-art models could generate. However, aprevalent limitation persists in the effective communication with these popularT2I models, such as Stable Diffusion, using natural language descriptions. Thistypically makes an engaging image hard to obtain without expertise in promptengineering with complex word compositions, magic tags, and annotations.Inspired by the recently released DALLE3 - a T2I model directly built-inChatGPT that talks human language, we revisit the existing T2I systemsendeavoring to align human intent and introduce a new task - interactive textto image (iT2I), where people can interact with LLM for interleavedhigh-quality image generation/edit/refinement and question answering withstronger images and text correspondences using natural language. In addressingthe iT2I problem, we present a simple approach that augments LLMs for iT2I withprompting techniques and off-the-shelf T2I models. We evaluate our approach foriT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT,LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be aconvenient and low-cost way to introduce the iT2I ability for any existing LLMsand any text-to-image models without any training while bringing littledegradation on LLMs' inherent capabilities in, e.g., question answering andcode generation. We hope this work could draw broader attention and provideinspiration for boosting user experience in human-machine interactionsalongside the image quality of the next-generation T2I systems.</description><author>Lai Zeqiang, Zhu Xizhou, Dai Jifeng, Qiao Yu, Wang Wenhai</author><pubDate>Wed, 11 Oct 2023 17:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07653v1</guid></item><item><title>Editing Large Language Models: Problems, Methods, and Opportunities</title><link>http://arxiv.org/abs/2305.13172v2</link><description>Despite the ability to train capable LLMs, the methodology for maintainingtheir relevancy and rectifying errors remains elusive. To this end, the pastfew years have witnessed a surge in techniques for editing LLMs, the objectiveof which is to efficiently alter the behavior of LLMs within a specific domainwithout negatively impacting performance across other inputs. This paperembarks on a deep exploration of the problems, methods, and opportunitiesrelated to model editing for LLMs. In particular, we provide an exhaustiveoverview of the task definition and challenges associated with model editing,along with an in-depth empirical analysis of the most progressive methodscurrently at our disposal. We also build a new benchmark dataset to facilitatea more robust evaluation and pinpoint enduring issues intrinsic to existingtechniques. Our objective is to provide valuable insights into theeffectiveness and feasibility of each editing technique, thereby assisting thecommunity in making informed decisions on the selection of the most appropriatemethod for a specific task or context. Code and datasets are available athttps://github.com/zjunlp/EasyEdit.</description><author>Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang</author><pubDate>Wed, 11 Oct 2023 17:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13172v2</guid></item><item><title>LLM4Vis: Explainable Visualization Recommendation using ChatGPT</title><link>http://arxiv.org/abs/2310.07652v1</link><description>Data visualization is a powerful tool for exploring and communicatinginsights in various domains. To automate visualization choice for datasets, atask known as visualization recommendation has been proposed. Variousmachine-learning-based approaches have been developed for this purpose, butthey often require a large corpus of dataset-visualization pairs for trainingand lack natural explanations for their results. To address this research gap,we propose LLM4Vis, a novel ChatGPT-based prompting approach to performvisualization recommendation and return human-like explanations using very fewdemonstration examples. Our approach involves feature description,demonstration example selection, explanation generation, demonstration exampleconstruction, and inference steps. To obtain demonstration examples withhigh-quality explanations, we propose a new explanation generationbootstrapping to iteratively refine generated explanations by considering theprevious generation and template-based hint. Evaluations on the VizML datasetshow that LLM4Vis outperforms or performs similarly to supervised learningmodels like Random Forest, Decision Tree, and MLP in both few-shot andzero-shot settings. The qualitative evaluation also shows the effectiveness ofexplanations generated by LLM4Vis. We make our code publicly available at\href{https://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}.</description><author>Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, Yong Wang</author><pubDate>Wed, 11 Oct 2023 17:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07652v1</guid></item><item><title>CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools</title><link>http://arxiv.org/abs/2307.15770v2</link><description>In the face of climate change, are companies really taking substantial stepstoward more sustainable operations? A comprehensive answer lies in the dense,information-rich landscape of corporate sustainability reports. However, thesheer volume and complexity of these reports make human analysis very costly.Therefore, only a few entities worldwide have the resources to analyze thesereports at scale, which leads to a lack of transparency in sustainabilityreporting. Empowering stakeholders with LLM-based automatic analysis tools canbe a promising way to democratize sustainability report analysis. However,developing such tools is challenging due to (1) the hallucination of LLMs and(2) the inefficiency of bringing domain experts into the AI development loop.In this paper, we ChatReport, a novel LLM-based system to automate the analysisof corporate sustainability reports, addressing existing challenges by (1)making the answers traceable to reduce the harm of hallucination and (2)actively involving domain experts in the development loop. We make ourmethodology, annotated datasets, and generated analyses of 1015 reportspublicly available.</description><author>Jingwei Ni, Julia Bingler, Chiara Colesanti-Senni, Mathias Kraus, Glen Gostlow, Tobias Schimanski, Dominik Stammbach, Saeid Ashraf Vaghefi, Qian Wang, Nicolas Webersinke, Tobias Wekhof, Tingyu Yu, Markus Leippold</author><pubDate>Wed, 11 Oct 2023 17:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15770v2</guid></item><item><title>Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals</title><link>http://arxiv.org/abs/2310.07648v1</link><description>Multimodal emotion recognition from physiological signals is receiving anincreasing amount of attention due to the impossibility to control them at willunlike behavioral reactions, thus providing more reliable information. Existingdeep learning-based methods still rely on extracted handcrafted features, nottaking full advantage of the learning ability of neural networks, and oftenadopt a single-modality approach, while human emotions are inherently expressedin a multimodal way. In this paper, we propose a hypercomplex multimodalnetwork equipped with a novel fusion module comprising parameterizedhypercomplex multiplications. Indeed, by operating in a hypercomplex domain theoperations follow algebraic rules which allow to model latent relations amonglearned feature dimensions for a more effective fusion step. We performclassification of valence and arousal from electroencephalogram (EEG) andperipheral physiological signals, employing the publicly available databaseMAHNOB-HCI surpassing a multimodal state-of-the-art network. The code of ourwork is freely available at https://github.com/ispamm/MHyEEG.</description><author>Eleonora Lopez, Eleonora Chiarantano, Eleonora Grassucci, Danilo Comminiello</author><pubDate>Wed, 11 Oct 2023 17:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07648v1</guid></item><item><title>Rethinking the BERT-like Pretraining for DNA Sequences</title><link>http://arxiv.org/abs/2310.07644v1</link><description>With the success of large-scale pretraining in NLP, there is an increasingtrend of applying it to the domain of life sciences. In particular, pretrainingmethods based on DNA sequences have garnered growing attention due to theirpotential to capture generic information about genes. However, existingpretraining methods for DNA sequences largely rely on direct adoptions of BERTpretraining from NLP, lacking a comprehensive understanding and a specificallytailored approach. To address this research gap, we first conducted a series ofexploratory experiments and gained several insightful observations: 1) In thefine-tuning phase of downstream tasks, when using K-mer overlappingtokenization instead of K-mer non-overlapping tokenization, both overlappingand non-overlapping pretraining weights show consistent performanceimprovement.2) During the pre-training process, using K-mer overlappingtokenization quickly produces clear K-mer embeddings and reduces the loss to avery low level, while using K-mer non-overlapping tokenization results in lessdistinct embeddings and continuously decreases the loss. 3) Using overlappingtokenization causes the self-attention in the intermediate layers ofpre-trained models to tend to overly focus on certain tokens, reflecting thatthese layers are not adequately optimized. In summary, overlapping tokenizationcan benefit the fine-tuning of downstream tasks but leads to inadequatepretraining with fast convergence. To unleash the pretraining potential, weintroduce a novel approach called RandomMask, which gradually increases thetask difficulty of BERT-like pretraining by continuously expanding its maskboundary, forcing the model to learn more knowledge. RandomMask is simple buteffective, achieving top-tier performance across 26 datasets of 28 datasetsspanning 7 downstream tasks.</description><author>Chaoqi Liang, Weiqiang Bai, Lifeng Qiao, Yuchen Ren, Jianle Sun, Peng Ye, Hongliang Yan, Xinzhu Ma, Wangmeng Zuo, Wanli Ouyang</author><pubDate>Wed, 11 Oct 2023 17:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07644v1</guid></item><item><title>Evaluating Large Language Models at Evaluating Instruction Following</title><link>http://arxiv.org/abs/2310.07641v1</link><description>As research in large language models (LLMs) continues to accelerate,LLM-based evaluation has emerged as a scalable and cost-effective alternativeto human evaluations for comparing the ever increasing list of models. Thispaper investigates the efficacy of these "LLM evaluators", particularly inusing them to assess instruction following, a metric that gauges how closelygenerated text adheres to the given instruction. We introduce a challengingmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLMevaluator in discerning instruction-following outputs. The authors manuallycurated 419 pairs of outputs, one adhering to instructions while the otherdiverging, yet may possess deceptive qualities that mislead an LLM evaluator,e.g., a more engaging tone. Contrary to existing meta-evaluation, we discoverthat different evaluators (i.e., combinations of LLMs and prompts) exhibitdistinct performance on LLMBar and even the highest-scoring ones havesubstantial room for improvement. We also present a novel suite of promptingstrategies that further close the gap between LLM and human evaluators. WithLLMBar, we hope to offer more insight into LLM evaluators and foster futureresearch in developing better instruction-following models.</description><author>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen</author><pubDate>Wed, 11 Oct 2023 17:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07641v1</guid></item><item><title>Analysis of the Cambridge Multiple-Choice Questions Reading Dataset with a Focus on Candidate Response Distribution</title><link>http://arxiv.org/abs/2306.13047v3</link><description>Multiple choice exams are widely used to assess candidates across a diverserange of domains and tasks. To moderate question quality, newly proposedquestions often pass through pre-test evaluation stages before being deployedinto real-world exams. Currently, this evaluation process is manuallyintensive, which can lead to time lags in the question development cycle.Streamlining this process via automation can significantly enhance efficiency,however, there's a current lack of datasets with adequate pre-test analysisinformation. In this paper we analyse the Cambridge Multiple-Choice QuestionsReading Dataset; a multiple-choice comprehension dataset of questions atdifferent target levels, with corresponding candidate selection distributions.We introduce the task of candidate distribution matching, propose severalevaluation metrics for the task, and demonstrate that automatic systems trainedon RACE++ can be leveraged as baselines for our task. We further demonstratethat these automatic systems can be used for practical pre-test evaluationtasks such as detecting underperforming distractors, where our detectionsystems can automatically identify poor distractors that few candidates select.</description><author>Adian Liusie, Vatsal Raina, Andrew Mullooly, Kate Knill, Mark J. F. Gales</author><pubDate>Wed, 11 Oct 2023 17:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13047v3</guid></item><item><title>Context-Enhanced Detector For Building Detection From Remote Sensing Images</title><link>http://arxiv.org/abs/2310.07638v1</link><description>The field of building detection from remote sensing images has madesignificant progress, but faces challenges in achieving high-accuracy detectiondue to the diversity in building appearances and the complexity of vast scenes.To address these challenges, we propose a novel approach calledContext-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascadestructure to enhance the extraction of contextual information and improvebuilding detection accuracy. Specifically, we introduce two modules: theSemantic Guided Contextual Mining (SGCM) module, which aggregates multi-scalecontexts and incorporates an attention mechanism to capture long-rangeinteractions, and the Instance Context Mining Module (ICMM), which capturesinstance-level relationship context by constructing a spatial relationshipgraph and aggregating instance features. Additionally, we introduce a semanticsegmentation loss based on pseudo-masks to guide contextual informationextraction. Our method achieves state-of-the-art performance on three buildingdetection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.</description><author>Ziyue Huang, Mingming Zhang, Qingjie Liu, Wei Wang, Zhe Dong, Yunhong Wang</author><pubDate>Wed, 11 Oct 2023 17:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07638v1</guid></item><item><title>OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models</title><link>http://arxiv.org/abs/2310.07637v1</link><description>Large language models (LLMs) have exhibited remarkable capabilities inNLP-related tasks such as translation, summarizing, and generation. Theapplication of LLMs in specific areas, notably AIOps (Artificial Intelligencefor IT Operations), holds great potential due to their advanced abilities ininformation summarizing, report analyzing, and ability of API calling.Nevertheless, the performance of current LLMs in AIOps tasks is yet to bedetermined. Furthermore, a comprehensive benchmark is required to steer theoptimization of LLMs tailored for AIOps. Compared with existing benchmarks thatfocus on evaluating specific fields like network configuration, in this paper,we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmarkdesigned for LLMs. For the first time, OpsEval assesses LLMs' proficiency inthree crucial scenarios (Wired Network Operation, 5G Communication Operation,and Database Operation) at various ability levels (knowledge recall, analyticalthinking, and practical application). The benchmark includes 7,200 questions inboth multiple-choice and question-answer (QA) formats, available in English andChinese. With quantitative and qualitative results, we show how various LLMtricks can affect the performance of AIOps, including zero-shot,chain-of-thought, and few-shot in-context learning. We find that GPT4-score ismore consistent with experts than widely used Bleu and Rouge, which can be usedto replace automatic metrics for large-scale qualitative evaluations.</description><author>Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidaoo Wen, Xiaohui Nie, Dan Pei</author><pubDate>Wed, 11 Oct 2023 17:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07637v1</guid></item><item><title>DataPerf: Benchmarks for Data-Centric AI Development</title><link>http://arxiv.org/abs/2207.10062v3</link><description>Machine learning research has long focused on models rather than datasets,and prominent datasets are used for common ML tasks without regard to thebreadth, difficulty, and faithfulness of the underlying problems. Neglectingthe fundamental importance of data has given rise to inaccuracy, bias, andfragility in real-world applications, and research is hindered by saturationacross existing dataset benchmarks. In response, we present DataPerf, acommunity-led benchmark suite for evaluating ML datasets and data-centricalgorithms. We aim to foster innovation in data-centric AI through competition,comparability, and reproducibility. We enable the ML community to iterate ondatasets, instead of just architectures, and we provide an open, onlineplatform with multiple rounds of challenges to support this iterativedevelopment. The first iteration of DataPerf contains five benchmarks coveringa wide spectrum of data-centric techniques, tasks, and modalities in vision,speech, acquisition, debugging, and diffusion prompting, and we support hostingnew contributed benchmarks from the community. The benchmarks, onlineevaluation platform, and baseline implementations are open source, and theMLCommons Association will maintain DataPerf to ensure long-term benefits toacademia and industry.</description><author>Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karlaš, William Gaviria Rojas, Sudnya Diamos, Greg Diamos, Lynn He, Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera, Juan Ciro, Lora Aroyo, Bilge Acun, Lingjiao Chen, Mehul Smriti Raje, Max Bartolo, Sabri Eyuboglu, Amirata Ghorbani, Emmett Goodman, Oana Inel, Tariq Kane, Christine R. Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin Vanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen Paritosh, Lilith Bath-Leah, Ce Zhang, James Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson, Vijay Janapa Reddi</author><pubDate>Wed, 11 Oct 2023 17:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.10062v3</guid></item><item><title>Attention-Map Augmentation for Hypercomplex Breast Cancer Classification</title><link>http://arxiv.org/abs/2310.07633v1</link><description>Breast cancer is the most widespread neoplasm among women and early detectionof this disease is critical. Deep learning techniques have become of greatinterest to improve diagnostic performance. Nonetheless, discriminating betweenmalignant and benign masses from whole mammograms remains challenging due tothem being almost identical to an untrained eye and the region of interest(ROI) occupying a minuscule portion of the entire image. In this paper, wepropose a framework, parameterized hypercomplex attention maps (PHAM), toovercome these problems. Specifically, we deploy an augmentation step based oncomputing attention maps. Then, the attention maps are used to condition theclassification step by constructing a multi-dimensional input comprised of theoriginal breast cancer image and the corresponding attention map. In this step,a parameterized hypercomplex neural network (PHNN) is employed to performbreast cancer classification. The framework offers two main advantages. First,attention maps provide critical information regarding the ROI and allow theneural model to concentrate on it. Second, the hypercomplex architecture hasthe ability to model local relations between input dimensions thanks tohypercomplex algebra rules, thus properly exploiting the information providedby the attention map. We demonstrate the efficacy of the proposed framework onboth mammography images as well as histopathological ones, surpassingattention-based state-of-the-art networks and the real-valued counterpart ofour method. The code of our work is available athttps://github.com/elelo22/AttentionBCS.</description><author>Eleonora Lopez, Filippo Betello, Federico Carmignani, Eleonora Grassucci, Danilo Comminiello</author><pubDate>Wed, 11 Oct 2023 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07633v1</guid></item><item><title>Prompt Backdoors in Visual Prompt Learning</title><link>http://arxiv.org/abs/2310.07632v1</link><description>Fine-tuning large pre-trained computer vision models is infeasible forresource-limited users. Visual prompt learning (VPL) has thus emerged toprovide an efficient and flexible alternative to model fine-tuning throughVisual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provideroptimizes a visual prompt given downstream data, and downstream users can usethis prompt together with the large pre-trained model for prediction. However,this new learning paradigm may also pose security risks when the VPPTaaSprovider instead provides a malicious visual prompt. In this paper, we take thefirst step to explore such risks through the lens of backdoor attacks.Specifically, we propose BadVisualPrompt, a simple yet effective backdoorattack against VPL. For example, poisoning $5\%$ CIFAR10 training data leads toabove $99\%$ attack success rates with only negligible model accuracy drop by$1.5\%$. In particular, we identify and then address a new technical challengerelated to interactions between the backdoor trigger and visual prompt, whichdoes not exist in conventional, model-level backdoors. Moreover, we providein-depth analyses of seven backdoor defenses from model, prompt, and inputlevels. Overall, all these defenses are either ineffective or impractical tomitigate our BadVisualPrompt, implying the critical vulnerability of VPL.</description><author>Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</author><pubDate>Wed, 11 Oct 2023 17:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07632v1</guid></item><item><title>Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates</title><link>http://arxiv.org/abs/2310.07631v1</link><description>Floods can be very destructive causing heavy damage to life, property, andlivelihoods. Global climate change and the consequent sea-level rise haveincreased the occurrence of extreme weather events, resulting in elevated andfrequent flood risk. Therefore, accurate and timely flood forecasting incoastal river systems is critical to facilitate good flood management. However,the computational tools currently used are either slow or inaccurate. In thispaper, we propose a Flood prediction tool using Graph Transformer Network(FloodGTN) for river systems. More specifically, FloodGTN learns thespatio-temporal dependencies of water levels at different monitoring stationsusing Graph Neural Networks (GNNs) and an LSTM. It is currently implemented toconsider external covariates such as rainfall, tide, and the settings ofhydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along theriver. We use a Transformer to learn the attention given to external covariatesin computing water levels. We apply the FloodGTN tool to data from the SouthFlorida Water Management District, which manages a coastal area prone tofrequent storms and hurricanes. Experimental results show that FloodGTNoutperforms the physics-based model (HEC-RAS) by achieving higher accuracy with70% improvement while speeding up run times by at least 500x.</description><author>Jimeng Shi, Vitalii Stebliankin, Zhaonan Wang, Shaowen Wang, Giri Narasimhan</author><pubDate>Wed, 11 Oct 2023 17:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07631v1</guid></item><item><title>Differentiable Euler Characteristic Transforms for Shape Classification</title><link>http://arxiv.org/abs/2310.07630v1</link><description>The Euler Characteristic Transform (ECT) has proven to be a powerfulrepresentation, combining geometrical and topological characteristics of shapesand graphs. However, the ECT was hitherto unable to learn task-specificrepresentations. We overcome this issue and develop a novel computational layerthat enables learning the ECT in an end-to-end fashion. Our method DECT is fastand computationally efficient, while exhibiting performance on a par with morecomplex models in both graph and point cloud classification tasks. Moreover, weshow that this seemingly unexpressive statistic still provides the sametopological expressivity as more complex topological deep learning layersprovide.</description><author>Ernst Roell, Bastian Rieck</author><pubDate>Wed, 11 Oct 2023 17:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07630v1</guid></item><item><title>The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values</title><link>http://arxiv.org/abs/2310.07629v1</link><description>Human feedback is increasingly used to steer the behaviours of Large LanguageModels (LLMs). However, it is unclear how to collect and incorporate feedbackin a way that is efficient, effective and unbiased, especially for highlysubjective human preferences and values. In this paper, we survey existingapproaches for learning from human feedback, drawing on 95 papers primarilyfrom the ACL and arXiv repositories.First, we summarise the past, pre-LLMtrends for integrating human feedback into language models. Second, we give anoverview of present techniques and practices, as well as the motivations forusing feedback; conceptual frameworks for defining values and preferences; andhow feedback is collected and from whom. Finally, we encourage a better futureof feedback learning in LLMs by raising five unresolved conceptual andpractical challenges.</description><author>Hannah Rose Kirk, Andrew M. Bean, Bertie Vidgen, Paul Röttger, Scott A. Hale</author><pubDate>Wed, 11 Oct 2023 17:18:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07629v1</guid></item><item><title>Chat with the Environment: Interactive Multimodal Perception Using Large Language Models</title><link>http://arxiv.org/abs/2303.08268v3</link><description>Programming robot behavior in a complex world faces challenges on multiplelevels, from dextrous low-level skills to high-level planning and reasoning.Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoningability in few-shot robotic planning. However, it remains challenging to groundLLMs in multimodal sensory input and continuous action output, while enabling arobot to interact with its environment and acquire novel information as itspolicies unfold. We develop a robot interaction scenario with a partiallyobservable state, which necessitates a robot to decide on a range of epistemicactions in order to sample sensory information among multiple modalities,before being able to execute the task correctly. Matcha (Multimodal environmentchatting) agent, an interactive perception framework, is therefore proposedwith an LLM as its backbone, whose ability is exploited to instruct epistemicactions and to reason over the resulting multimodal sensations (vision, sound,haptics, proprioception), as well as to plan an entire task execution based onthe interactively acquired information. Our study demonstrates that LLMs canprovide high-level planning and reasoning skills and control interactive robotbehavior in a multimodal environment, while multimodal modules with the contextof the environmental state help ground the LLMs and extend their processingability. The project website can be found at https://matcha-agent.github.io.</description><author>Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter</author><pubDate>Wed, 11 Oct 2023 17:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08268v3</guid></item><item><title>Clifford Group Equivariant Neural Networks</title><link>http://arxiv.org/abs/2305.11141v3</link><description>We introduce Clifford Group Equivariant Neural Networks: a novel approach forconstructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. Weidentify and study the $\textit{Clifford group}$, a subgroup inside theClifford algebra whose definition we adjust to achieve several favorableproperties. Primarily, the group's action forms an orthogonal automorphism thatextends beyond the typical vector space to the entire Clifford algebra whilerespecting the multivector grading. This leads to several non-equivalentsubrepresentations corresponding to the multivector decomposition. Furthermore,we prove that the action respects not just the vector space structure of theClifford algebra but also its multiplicative structure, i.e., the geometricproduct. These findings imply that every polynomial in multivectors, Anadvantage worth mentioning is that we obtain expressive layers that canelegantly generalize to inner-product spaces of any dimension. We demonstrate,notably from a single core implementation, state-of-the-art performance onseveral distinct tasks, including a three-dimensional $n$-body experiment, afour-dimensional Lorentz-equivariant high-energy physics experiment, and afive-dimensional convex hull experiment.</description><author>David Ruhe, Johannes Brandstetter, Patrick Forré</author><pubDate>Wed, 11 Oct 2023 17:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11141v3</guid></item><item><title>Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations</title><link>http://arxiv.org/abs/2310.07626v1</link><description>Satellite-based remote sensing missions have revolutionized our understandingof the Ocean state and dynamics. Among them, spaceborne altimetry providesvaluable measurements of Sea Surface Height (SSH), which is used to estimatesurface geostrophic currents. However, due to the sensor technology employed,important gaps occur in SSH observations. Complete SSH maps are produced by thealtimetry community using linear Optimal Interpolations (OI) such as thewidely-used Data Unification and Altimeter Combination System (DUACS). However,OI is known for producing overly smooth fields and thus misses somemesostructures and eddies. On the other hand, Sea Surface Temperature (SST)products have much higher data coverage and SST is physically linked togeostrophic currents through advection. We design a realistic twin experimentto emulate the satellite observations of SSH and SST to evaluate interpolationmethods. We introduce a deep learning network able to use SST information, anda trainable in two settings: one where we have no access to ground truth duringtraining and one where it is accessible. Our investigation involves acomparative analysis of the aforementioned network when trained using eithersupervised or unsupervised loss functions. We assess the quality of SSHreconstructions and further evaluate the network's performance in terms of eddydetection and physical properties. We find that it is possible, even in anunsupervised setting to use SST to improve reconstruction performance comparedto SST-agnostic interpolations. We compare our reconstructions to DUACS's andreport a decrease of 41\% in terms of root mean squared error.</description><author>Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria</author><pubDate>Wed, 11 Oct 2023 17:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07626v1</guid></item><item><title>Dual Quaternion Rotational and Translational Equivariance in 3D Rigid Motion Modelling</title><link>http://arxiv.org/abs/2310.07623v1</link><description>Objects' rigid motions in 3D space are described by rotations andtranslations of a highly-correlated set of points, each with associated $x,y,z$coordinates that real-valued networks consider as separate entities, losinginformation. Previous works exploit quaternion algebra and their ability tomodel rotations in 3D space. However, these algebras do not properly encodetranslations, leading to sub-optimal performance in 3D learning tasks. Toovercome these limitations, we employ a dual quaternion representation of rigidmotions in the 3D space that jointly describes rotations and translations ofpoint sets, processing each of the points as a single entity. Our approach istranslation and rotation equivariant, so it does not suffer from shifts in thedata and better learns object trajectories, as we validate in the experimentalevaluations. Models endowed with this formulation outperform previousapproaches in a human pose forecasting application, attesting to theeffectiveness of the proposed dual quaternion formulation for rigid motions in3D space.</description><author>Guilherme Vieira, Eleonora Grassucci, Marcos Eduardo Valle, Danilo Comminiello</author><pubDate>Wed, 11 Oct 2023 17:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07623v1</guid></item><item><title>BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer</title><link>http://arxiv.org/abs/2305.12534v2</link><description>We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL)based fuzzer aimed at finding security vulnerabilities for Web applications.BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performsgrammar-adhering and attack-provoking mutation operations on them to generatecandidate attack vectors. The key insight of BertRLFuzzer is the use of RL witha BERT model as an agent to guide the fuzzer to efficiently learngrammar-adhering and attack-provoking mutation operators. In order to establishthe efficacy of BertRLFuzzer we compare it against a total of 13 black box andwhite box fuzzers over a benchmark of 9 victim websites with over 16K LOC. Weobserved a significant improvement, relative to the nearest competing tool, interms of time to first attack (54% less), new vulnerabilities found (17 newvulnerabilities), and attack rate (4.4% more attack vectors generated).</description><author>Piyush Jha, Joseph Scott, Jaya Sriram Ganeshna, Mudit Singh, Vijay Ganesh</author><pubDate>Wed, 11 Oct 2023 17:05:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12534v2</guid></item><item><title>Reinforcement Learning-based Knowledge Graph Reasoning for Explainable Fact-checking</title><link>http://arxiv.org/abs/2310.07613v1</link><description>Fact-checking is a crucial task as it ensures the prevention ofmisinformation. However, manual fact-checking cannot keep up with the rate atwhich false information is generated and disseminated online. Automatedfact-checking by machines is significantly quicker than by humans. But forbetter trust and transparency of these automated systems, explainability in thefact-checking process is necessary. Fact-checking often entails contrasting afactual assertion with a body of knowledge for such explanations. An effectiveway of representing knowledge is the Knowledge Graph (KG). There have beensufficient works proposed related to fact-checking with the usage of KG but notmuch focus is given to the application of reinforcement learning (RL) in suchcases. To mitigate this gap, we propose an RL-based KG reasoning approach forexplainable fact-checking. Extensive experiments on FB15K-277 and NELL-995datasets reveal that reasoning over a KG is an effective way of producinghuman-readable explanations in the form of paths and classifications for factclaims. The RL reasoning agent computes a path that either proves or disprovesa factual claim, but does not provide a verdict itself. A verdict is reached bya voting mechanism that utilizes paths produced by the agent. These paths canbe presented to human readers so that they themselves can decide whether or notthe provided evidence is convincing or not. This work will encourage works inthis direction for incorporating RL for explainable fact-checking as itincreases trustworthiness by providing a human-in-the-loop approach.</description><author>Gustav Nikopensius, Mohit Mayank, Orchid Chetia Phukan, Rajesh Sharma</author><pubDate>Wed, 11 Oct 2023 16:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07613v1</guid></item><item><title>PHYDI: Initializing Parameterized Hypercomplex Neural Networks as Identity Functions</title><link>http://arxiv.org/abs/2310.07612v1</link><description>Neural models based on hypercomplex algebra systems are growing andprolificating for a plethora of applications, ranging from computer vision tonatural language processing. Hand in hand with their adoption, parameterizedhypercomplex neural networks (PHNNs) are growing in size and no techniques havebeen adopted so far to control their convergence at a large scale. In thispaper, we study PHNNs convergence and propose parameterized hypercomplexidentity initialization (PHYDI), a method to improve their convergence atdifferent scales, leading to more robust performance when the number of layersscales up, while also reaching the same performance with fewer iterations. Weshow the effectiveness of this approach in different benchmarks and with commonPHNNs with ResNets- and Transformer-based architecture. The code is availableat https://github.com/ispamm/PHYDI.</description><author>Matteo Mancanelli, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</author><pubDate>Wed, 11 Oct 2023 16:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07612v1</guid></item><item><title>Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models</title><link>http://arxiv.org/abs/2310.07611v1</link><description>The dominance of proprietary LLMs has led to restricted access and raisedinformation privacy concerns. High-performing open-source alternatives arecrucial for information-sensitive and high-volume applications but often lagbehind in performance. To address this gap, we propose (1) A untargeted variantof iterative self-critique and self-refinement devoid of external influence.(2) A novel ranking metric - Performance, Refinement, and Inference Cost Score(PeRFICS) - to find the optimal model for a given task considering refinedperformance and cost. Our experiments show that SoTA open source models ofvarying sizes from 7B - 65B, on average, improve 8.2% from their baselineperformance. Strikingly, even models with extremely small memory footprints,such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39%improvement in high-creativity, open ended tasks on the Vicuna benchmark.Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement.This work has profound implications for resource-constrained andinformation-sensitive environments seeking to leverage LLMs without incurringprohibitive costs, compromising on performance and privacy. The domain-agnosticself-refinement process coupled with our novel ranking metric facilitatesinformed decision-making in model selection, thereby reducing costs anddemocratizing access to high-performing language models, as evidenced by casestudies.</description><author>Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang, Heng Ji</author><pubDate>Wed, 11 Oct 2023 16:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07611v1</guid></item><item><title>QACHECK: A Demonstration System for Question-Guided Multi-Hop Fact-Checking</title><link>http://arxiv.org/abs/2310.07609v1</link><description>Fact-checking real-world claims often requires complex, multi-step reasoningdue to the absence of direct evidence to support or refute them. However,existing fact-checking systems often lack transparency in theirdecision-making, making it challenging for users to comprehend their reasoningprocess. To address this, we propose the Question-guided Multi-hopFact-Checking (QACHECK) system, which guides the model's reasoning process byasking a series of questions critical for verifying a claim. QACHECK has fivekey modules: a claim verifier, a question generator, a question-answeringmodule, a QA validator, and a reasoner. Users can input a claim into QACHECK,which then predicts its veracity and provides a comprehensive report detailingits reasoning process, guided by a sequence of (question, answer) pairs.QACHECK also provides the source of evidence supporting each question,fostering a transparent, explainable, and user-friendly fact-checking process.A recorded video of QACHECK is at https://www.youtube.com/watch?v=ju8kxSldM64</description><author>Liangming Pan, Xinyuan Lu, Min-Yen Kan, Preslav Nakov</author><pubDate>Wed, 11 Oct 2023 16:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07609v1</guid></item><item><title>Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse</title><link>http://arxiv.org/abs/2203.06768v4</link><description>As machine learning models are increasingly being employed to makeconsequential decisions in real-world settings, it becomes critical to ensurethat individuals who are adversely impacted (e.g., loan denied) by thepredictions of these models are provided with a means for recourse. Whileseveral approaches have been proposed to construct recourses for affectedindividuals, the recourses output by these methods either achieve low costs(i.e., ease-of-implementation) or robustness to small perturbations (i.e.,noisy implementations of recourses), but not both due to the inherenttrade-offs between the recourse costs and robustness. Furthermore, priorapproaches do not provide end users with any agency over navigating theaforementioned trade-offs. In this work, we address the above challenges byproposing the first algorithmic framework which enables users to effectivelymanage the recourse cost vs. robustness trade-offs. More specifically, ourframework Probabilistically ROBust rEcourse (\texttt{PROBE}) lets users choosethe probability with which a recourse could get invalidated (recourseinvalidation rate) if small changes are made to the recourse i.e., the recourseis implemented somewhat noisily. To this end, we propose a novel objectivefunction which simultaneously minimizes the gap between the achieved(resulting) and desired recourse invalidation rates, minimizes recourse costs,and also ensures that the resulting recourse achieves a positive modelprediction. We develop novel theoretical results to characterize the recourseinvalidation rates corresponding to any given instance w.r.t. different classesof underlying models (e.g., linear models, tree based models etc.), andleverage these results to efficiently optimize the proposed objective.Experimental evaluation with multiple real world datasets demonstrates theefficacy of the proposed framework.</description><author>Martin Pawelczyk, Teresa Datta, Johannes van-den-Heuvel, Gjergji Kasneci, Himabindu Lakkaraju</author><pubDate>Wed, 11 Oct 2023 16:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.06768v4</guid></item><item><title>Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous Driving</title><link>http://arxiv.org/abs/2310.07602v1</link><description>Radar has stronger adaptability in adverse scenarios for autonomous drivingenvironmental perception compared to widely adopted cameras and LiDARs.Compared with commonly used 3D radars, latest 4D radars have precise verticalresolution and higher point cloud density, making it a highly promising sensorfor autonomous driving in complex environmental perception. However, due to themuch higher noise than LiDAR, manufacturers choose different filteringstrategies, resulting in an inverse ratio between noise level and point clouddensity. There is still a lack of comparative analysis on which method isbeneficial for deep learning-based perception algorithms in autonomous driving.One of the main reasons is that current datasets only adopt one type of 4Dradar, making it difficult to compare different 4D radars in the same scene.Therefore, in this paper, we introduce a novel large-scale multi-modal datasetfeaturing, for the first time, two types of 4D radars captured simultaneously.This dataset enables further research into effective 4D radar perceptionalgorithms.Our dataset consists of 151 consecutive series, most of which last20 seconds and contain 10,007 meticulously synchronized and annotated frames.Moreover, our dataset captures a variety of challenging driving scenarios,including many road conditions, weather conditions, nighttime and daytime withdifferent lighting intensities and periods. Our dataset annotates consecutiveframes, which can be applied to 3D object detection and tracking, and alsosupports the study of multi-modal tasks. We experimentally validate ourdataset, providing valuable results for studying different types of 4D radars.This dataset is released on https://github.com/adept-thu/Dual-Radar.</description><author>Xinyu Zhang, Li Wang, Jian Chen, Cheng Fang, Lei Yang, Ziying Song, Guangqi Yang, Yichen Wang, Xiaofei Zhang, Jun Li</author><pubDate>Wed, 11 Oct 2023 16:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07602v1</guid></item><item><title>Survey on Imbalanced Data, Representation Learning and SEP Forecasting</title><link>http://arxiv.org/abs/2310.07598v1</link><description>Deep Learning methods have significantly advanced various data-driven taskssuch as regression, classification, and forecasting. However, much of thisprogress has been predicated on the strong but often unrealistic assumptionthat training datasets are balanced with respect to the targets they contain.This misalignment with real-world conditions, where data is frequentlyimbalanced, hampers the effectiveness of such models in practical applications.Methods that reconsider that assumption and tackle real-world imbalances havebegun to emerge and explore avenues to address this challenge. One suchpromising avenue is representation learning, which enables models to capturecomplex data characteristics and generalize better to minority classes. Byfocusing on a richer representation of the feature space, these techniques holdthe potential to mitigate the impact of data imbalance. In this survey, wepresent deep learning works that step away from the balanced-data assumption,employing strategies like representation learning to better approximatereal-world imbalances. We also highlight a critical application in SEPforecasting where addressing data imbalance is paramount for success.</description><author>Josias Moukpe</author><pubDate>Wed, 11 Oct 2023 16:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07598v1</guid></item><item><title>Prospective Side Information for Latent MDPs</title><link>http://arxiv.org/abs/2310.07596v1</link><description>In many interactive decision-making settings, there is latent and unobservedinformation that remains fixed. Consider, for example, a dialogue system, wherecomplete information about a user, such as the user's preferences, is notgiven. In such an environment, the latent information remains fixed throughouteach episode, since the identity of the user does not change during aninteraction. This type of environment can be modeled as a Latent MarkovDecision Process (LMDP), a special instance of Partially Observed MarkovDecision Processes (POMDPs). Previous work established exponential lower boundsin the number of latent contexts for the LMDP class. This puts forward aquestion: under which natural assumptions a near-optimal policy of an LMDP canbe efficiently learned? In this work, we study the class of LMDPs with {\emprospective side information}, when an agent receives additional, weaklyrevealing, information on the latent context at the beginning of each episode.We show that, surprisingly, this problem is not captured by contemporarysettings and algorithms designed for partially observed environments. We thenestablish that any sample efficient algorithm must suffer at least$\Omega(K^{2/3})$-regret, as opposed to standard $\Omega(\sqrt{K})$ lowerbounds, and design an algorithm with a matching upper bound.</description><author>Jeongyeol Kwon, Yonathan Efroni, Shie Mannor, Constantine Caramanis</author><pubDate>Wed, 11 Oct 2023 16:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07596v1</guid></item><item><title>Transformers for Green Semantic Communication: Less Energy, More Semantics</title><link>http://arxiv.org/abs/2310.07592v1</link><description>Semantic communication aims to transmit meaningful and effective informationrather than focusing on individual symbols or bits, resulting in benefits likereduced latency, bandwidth usage, and higher throughput compared to traditionalcommunication. However, semantic communication poses significant challenges dueto the need for universal metrics for benchmarking the joint effects ofsemantic information loss and practical energy consumption. This researchpresents a novel multi-objective loss function named "Energy-Optimized SemanticLoss" (EOSL), addressing the challenge of balancing semantic information lossand energy consumption. Through comprehensive experiments on transformermodels, including CPU and GPU energy usage, it is demonstrated that EOSL-basedencoder model selection can save up to 90\% of energy while achieving a 44\%improvement in semantic similarity performance during inference in thisexperiment. This work paves the way for energy-efficient neural networkselection and the development of greener semantic communication architectures.</description><author>Shubhabrata Mukherjee, Cory Beard, Sejun Song</author><pubDate>Wed, 11 Oct 2023 16:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07592v1</guid></item><item><title>On the Trade-Off between Actionable Explanations and the Right to be Forgotten</title><link>http://arxiv.org/abs/2208.14137v3</link><description>As machine learning (ML) models are increasingly being deployed inhigh-stakes applications, policymakers have suggested tighter data protectionregulations (e.g., GDPR, CCPA). One key principle is the "right to beforgotten" which gives users the right to have their data deleted. Another keyprinciple is the right to an actionable explanation, also known as algorithmicrecourse, allowing users to reverse unfavorable decisions. To date, it isunknown whether these two principles can be operationalized simultaneously.Therefore, we introduce and study the problem of recourse invalidation in thecontext of data deletion requests. More specifically, we theoretically andempirically analyze the behavior of popular state-of-the-art algorithms anddemonstrate that the recourses generated by these algorithms are likely to beinvalidated if a small number of data deletion requests (e.g., 1 or 2) warrantupdates of the predictive model. For the setting of differentiable models, wesuggest a framework to identify a minimal subset of critical training pointswhich, when removed, maximize the fraction of invalidated recourses. Using ourframework, we empirically show that the removal of as little as 2 datainstances from the training set can invalidate up to 95 percent of allrecourses output by popular state-of-the-art algorithms. Thus, our work raisesfundamental questions about the compatibility of "the right to an actionableexplanation" in the context of the "right to be forgotten", while alsoproviding constructive insights on the determining factors of recourserobustness.</description><author>Martin Pawelczyk, Tobias Leemann, Asia Biega, Gjergji Kasneci</author><pubDate>Wed, 11 Oct 2023 16:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.14137v3</guid></item><item><title>PeP: a Point enhanced Painting method for unified point cloud tasks</title><link>http://arxiv.org/abs/2310.07591v1</link><description>Point encoder is of vital importance for point cloud recognition. As the verybeginning step of whole model pipeline, adding features from diverse sourcesand providing stronger feature encoding mechanism would provide better inputfor downstream modules. In our work, we proposed a novel PeP module to tackleabove issue. PeP contains two main parts, a refined point painting method and aLM-based point encoder. Experiments results on the nuScenes and KITTI datasetsvalidate the superior performance of our PeP. The advantages leads to strongperformance on both semantic segmentation and object detection, in both lidarand multi-modal settings. Notably, our PeP module is model agnostic andplug-and-play. Our code will be publicly available soon.</description><author>Zichao Dong, Hang Ji, Xufeng Huang, Weikun Zhang, Xin Zhan, Junbo Chen</author><pubDate>Wed, 11 Oct 2023 16:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07591v1</guid></item><item><title>Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models</title><link>http://arxiv.org/abs/2310.07589v1</link><description>Considerable effort has been dedicated to mitigating toxicity, but existingmethods often require drastic modifications to model parameters or the use ofcomputationally intensive auxiliary models. Furthermore, previous approacheshave often neglected the crucial factor of language's evolving nature overtime. In this work, we present a comprehensive perspective on toxicitymitigation that takes into account its changing nature. We introduceGoodtriever, a flexible methodology that matches the current state-of-the-arttoxicity mitigation while achieving 43% relative latency reduction duringinference and being more computationally efficient. By incorporating aretrieval-based approach at decoding time, Goodtriever enablestoxicity-controlled text generation. Our research advocates for an increasedfocus on adaptable mitigation techniques, which better reflect the data driftmodels face when deployed in the wild. Code and data are available athttps://github.com/for-ai/goodtriever.</description><author>Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</author><pubDate>Wed, 11 Oct 2023 16:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07589v1</guid></item><item><title>Accurate Use of Label Dependency in Multi-Label Text Classification Through the Lens of Causality</title><link>http://arxiv.org/abs/2310.07588v1</link><description>Multi-Label Text Classification (MLTC) aims to assign the most relevantlabels to each given text. Existing methods demonstrate that label dependencycan help to improve the model's performance. However, the introduction of labeldependency may cause the model to suffer from unwanted prediction bias. In thisstudy, we attribute the bias to the model's misuse of label dependency, i.e.,the model tends to utilize the correlation shortcut in label dependency ratherthan fusing text information and label dependency for prediction. Motivated bycausal inference, we propose a CounterFactual Text Classifier (CFTC) toeliminate the correlation bias, and make causality-based predictions.Specifically, our CFTC first adopts the predict-then-modify backbone to extractprecise label information embedded in label dependency, then blocks thecorrelation shortcut through the counterfactual de-bias technique with the helpof the human causal graph. Experimental results on three datasets demonstratethat our CFTC significantly outperforms the baselines and effectivelyeliminates the correlation bias in datasets.</description><author>Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin</author><pubDate>Wed, 11 Oct 2023 16:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07588v1</guid></item><item><title>Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer</title><link>http://arxiv.org/abs/2310.07587v1</link><description>Data privacy and long-tailed distribution are the norms rather than theexception in many real-world tasks. This paper investigates a federatedlong-tailed learning (Fed-LT) task in which each client holds a locallyheterogeneous dataset; if the datasets can be globally aggregated, they jointlyexhibit a long-tailed distribution. Under such a setting, existing federatedoptimization and/or centralized long-tailed learning methods hardly apply dueto challenges in (a) characterizing the global long-tailed distribution underprivacy constraints and (b) adjusting the local learning strategy to cope withthe head-tail imbalance. In response, we propose a method termed$\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB)module that re-weights clients' gradients in a closed-loop manner, based on thefeedback of global long-tailed distribution evaluated by a Direct PriorAnalyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectivelyalleviate the distribution drift caused by data heterogeneity during the modeltraining process and obtain a global model with better performance on theminority classes while maintaining the performance of the majority classes.Extensive experiments demonstrate that $\texttt{Fed-GraB}$ achievesstate-of-the-art performance on representative datasets such as CIFAR-10-LT,CIFAR-100-LT, ImageNet-LT, and iNaturalist.</description><author>Zikai Xiao, Zihan Chen, Songshang Liu, Hualiang Wang, Yang Feng, Jin Hao, Joey Tianyi Zhou, Jian Wu, Howard Hao Yang, Zuozhu Liu</author><pubDate>Wed, 11 Oct 2023 16:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07587v1</guid></item><item><title>A Discrepancy Aware Framework for Robust Anomaly Detection</title><link>http://arxiv.org/abs/2310.07585v1</link><description>Defect detection is a critical research area in artificial intelligence.Recently, synthetic data-based self-supervised learning has shown greatpotential on this task. Although many sophisticated synthesizing strategiesexist, little research has been done to investigate the robustness of modelswhen faced with different strategies. In this paper, we focus on this issue andfind that existing methods are highly sensitive to them. To alleviate thisissue, we present a Discrepancy Aware Framework (DAF), which demonstratesrobust performance consistently with simple and cheap strategies acrossdifferent anomaly detection benchmarks. We hypothesize that the highsensitivity to synthetic data of existing self-supervised methods arises fromtheir heavy reliance on the visual appearance of synthetic data duringdecoding. In contrast, our method leverages an appearance-agnostic cue to guidethe decoder in identifying defects, thereby alleviating its reliance onsynthetic appearance. To this end, inspired by existing knowledge distillationmethods, we employ a teacher-student network, which is trained based onsynthesized outliers, to compute the discrepancy map as the cue. Extensiveexperiments on two challenging datasets prove the robustness of our method.Under the simple synthesis strategies, it outperforms existing methods by alarge margin. Furthermore, it also achieves the state-of-the-art localizationperformance. Code is available at: https://github.com/caiyuxuan1120/DAF.</description><author>Yuxuan Cai, Dingkang Liang, Dongliang Luo, Xinwei He, Xin Yang, Xiang Bai</author><pubDate>Wed, 11 Oct 2023 16:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07585v1</guid></item><item><title>On the Probability of Immunity</title><link>http://arxiv.org/abs/2309.11942v2</link><description>This work is devoted to the study of the probability of immunity, i.e. theeffect occurs whether exposed or not. We derive necessary and sufficientconditions for non-immunity and $\epsilon$-bounded immunity, i.e. theprobability of immunity is zero and $\epsilon$-bounded, respectively. Theformer allows us to estimate the probability of benefit (i.e., the effectoccurs if and only if exposed) from a randomized controlled trial, and thelatter allows us to produce bounds of the probability of benefit that aretighter than the existing ones. We also introduce the concept of indirectimmunity (i.e., through a mediator) and repeat our previous analysis for it.Finally, we propose a method for sensitivity analysis of the probability ofimmunity under unmeasured confounding.</description><author>Jose M. Peña</author><pubDate>Wed, 11 Oct 2023 16:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11942v2</guid></item><item><title>Centrality of the Fingerprint Core Location</title><link>http://arxiv.org/abs/2310.07584v1</link><description>Fingerprints have long been recognized as a unique and reliable means ofpersonal identification. Central to the analysis and enhancement offingerprints is the concept of the fingerprint core. Although the location ofthe core is used in many applications, to the best of our knowledge, this studyis the first to investigate the empirical distribution of the core over alarge, combined dataset of rolled, as well as plain fingerprint recordings. Weidentify and investigate the extent of incomplete rolling during the rolledfingerprint acquisition and investigate the centrality of the core. Aftercorrecting for the incomplete rolling, we find that the core deviates from thefingerprint center by 5.7% $\pm$ 5.2% to 7.6% $\pm$ 6.9%, depending on thefinger. Additionally, we find that the assumption of normal distribution of thecore position of plain fingerprint recordings cannot be rejected, but forrolled ones it can. Therefore, we use a multi-step process to find thedistribution of the rolled fingerprint recordings. The process consists of anAnderson-Darling normality test, the Bayesian Information Criterion to reducethe number of possible candidate distributions and finally a Generalized MonteCarlo goodness-of-fit procedure to find the best fitting distribution. We findthe non-central Fischer distribution best describes the cores' horizontalpositions. Finally, we investigate the correlation between mean core positionoffset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprintrecordings where the core sits slightly below the fingerprint center.</description><author>Laurenz Ruzicka, Bernhard Strobl, Bernhard Kohn, Clemens Heitzinger</author><pubDate>Wed, 11 Oct 2023 16:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07584v1</guid></item><item><title>Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT</title><link>http://arxiv.org/abs/2310.07582v1</link><description>Foundation models exhibit significant capabilities in decision-making andlogical deductions. Nonetheless, a continuing discourse persists regardingtheir genuine understanding of the world as opposed to mere stochastic mimicry.This paper meticulously examines a simple transformer trained for Othello,extending prior research to enhance comprehension of the emergent world modelof Othello-GPT. The investigation reveals that Othello-GPT encapsulates alinear representation of opposing pieces, a factor that causally steers itsdecision-making process. This paper further elucidates the interplay betweenthe linear world representation and causal decision-making, and theirdependence on layer depth and model complexity. We have made the code public.</description><author>Dean S. Hazineh, Zechen Zhang, Jeffery Chiu</author><pubDate>Wed, 11 Oct 2023 16:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07582v1</guid></item><item><title>In-Context Unlearning: Language Models as Few Shot Unlearners</title><link>http://arxiv.org/abs/2310.07579v1</link><description>Machine unlearning, the study of efficiently removing the impact of specifictraining points on the trained model, has garnered increased attention of late,driven by the need to comply with privacy regulations like the \emph{Right tobe Forgotten}. Although unlearning is particularly relevant for LLMs in lightof the copyright issues they raise, achieving precise unlearning iscomputationally infeasible for very large models. To this end, recent work hasproposed several algorithms which approximate the removal of training datawithout retraining the model. These algorithms crucially rely on access to themodel parameters in order to update them, an assumption that may not hold inpractice due to computational constraints or when the LLM is accessed via API.In this work, we propose a new class of unlearning methods for LLMs we call``In-Context Unlearning'', providing inputs in context and without having toupdate model parameters. To unlearn a particular training instance, we providethe instance alongside a flipped label and additional correctly labelledinstances which are prepended as inputs to the LLM at inference time. Ourexperimental results demonstrate that these contexts effectively removespecific information from the training set while maintaining performance levelsthat are competitive with (or in some cases exceed) state-of-the-art unlearningmethods that require access to the LLM parameters.</description><author>Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju</author><pubDate>Wed, 11 Oct 2023 16:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07579v1</guid></item><item><title>Analyzing Trendy Twitter Hashtags in the 2022 French Election</title><link>http://arxiv.org/abs/2310.07576v1</link><description>Regressions trained to predict the future activity of social media users needrich features for accurate predictions. Many advanced models exist to generatesuch features; however, the time complexities of their computations are oftenprohibitive when they run on enormous data-sets. Some studies have shown thatsimple semantic network features can be rich enough to use for regressionswithout requiring complex computations. We propose a method for using semanticnetworks as user-level features for machine learning tasks. We conducted anexperiment using a semantic network of 1037 Twitter hashtags from a corpus of3.7 million tweets related to the 2022 French presidential election. Abipartite graph is formed where hashtags are nodes and weighted edges connectthe hashtags reflecting the number of Twitter users that interacted with bothhashtags. The graph is then transformed into a maximum-spanning tree with themost popular hashtag as its root node to construct a hierarchy amongst thehashtags. We then provide a vector feature for each user based on this tree. Tovalidate the usefulness of our semantic feature we performed a regressionexperiment to predict the response rate of each user with six emotions likeanger, enjoyment, or disgust. Our semantic feature performs well with theregression with most emotions having $R^2$ above 0.5. These results suggestthat our semantic feature could be considered for use in further experimentspredicting social media response on big data-sets.</description><author>Aamir Mandviwalla, Lake Yin, Boleslaw K. Szymanski</author><pubDate>Wed, 11 Oct 2023 16:17:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07576v1</guid></item><item><title>Relational Prior Knowledge Graphs for Detection and Instance Segmentation</title><link>http://arxiv.org/abs/2310.07573v1</link><description>Humans have a remarkable ability to perceive and reason about the worldaround them by understanding the relationships between objects. In this paper,we investigate the effectiveness of using such relationships for objectdetection and instance segmentation. To this end, we propose a RelationalPrior-based Feature Enhancement Model (RP-FEM), a graph transformer thatenhances object proposal features using relational priors. The proposedarchitecture operates on top of scene graphs obtained from initial proposalsand aims to concurrently learn relational context modeling for object detectionand instance segmentation. Experimental evaluations on COCO show that theutilization of scene graphs, augmented with relational priors, offer benefitsfor object detection and instance segmentation. RP-FEM demonstrates itscapacity to suppress improbable class predictions within the image while alsopreventing the model from generating duplicate predictions, leading toimprovements over the baseline model on which it is built.</description><author>Osman Ülger, Yu Wang, Ysbrand Galama, Sezer Karaoglu, Theo Gevers, Martin R. Oswald</author><pubDate>Wed, 11 Oct 2023 16:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07573v1</guid></item><item><title>Impact of Label Types on Training SWIN Models with Overhead Imagery</title><link>http://arxiv.org/abs/2310.07572v1</link><description>Understanding the impact of data set design on model training and performancecan help alleviate the costs associated with generating remote sensing andoverhead labeled data. This work examined the impact of training shifted windowtransformers using bounding boxes and segmentation labels, where the latter aremore expensive to produce. We examined classification tasks by comparing modelstrained with both target and backgrounds against models trained with onlytarget pixels, extracted by segmentation labels. For object detection models,we compared performance using either label type when training. We found thatthe models trained on only target pixels do not show performance improvementfor classification tasks, appearing to conflate background pixels in theevaluation set with target pixels. For object detection, we found that modelstrained with either label type showed equivalent performance across testing. Wefound that bounding boxes appeared to be sufficient for tasks that did notrequire more complex labels, such as object segmentation. Continuing work todetermine consistency of this result across data types and model architecturescould potentially result in substantial savings in generating remote sensingdata sets for deep learning.</description><author>Ryan Ford, Kenneth Hutchison, Nicholas Felts, Benjamin Cheng, Jesse Lew, Kyle Jackson</author><pubDate>Wed, 11 Oct 2023 16:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07572v1</guid></item><item><title>Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques</title><link>http://arxiv.org/abs/2211.15751v3</link><description>Video, as a key driver in the global explosion of digital information, cancreate tremendous benefits for human society. Governments and enterprises aredeploying innumerable cameras for a variety of applications, e.g., lawenforcement, emergency management, traffic control, and security surveillance,all facilitated by video analytics (VA). This trend is spurred by the rapidadvancement of deep learning (DL), which enables more precise models for objectclassification, detection, and tracking. Meanwhile, with the proliferation ofInternet-connected devices, massive amounts of data are generated daily,overwhelming the cloud. Edge computing, an emerging paradigm that movesworkloads and services from the network core to the network edge, has beenwidely recognized as a promising solution. The resulting new intersection, edgevideo analytics (EVA), begins to attract widespread attention. Nevertheless,only a few loosely-related surveys exist on this topic. The basic concepts ofEVA (e.g., definition, architectures) were not fully elucidated due to therapid development of this domain. To fill these gaps, we provide acomprehensive survey of the recent efforts on EVA. In this paper, we firstreview the fundamentals of edge computing, followed by an overview of VA. EVAsystems and their enabling techniques are discussed next. In addition, weintroduce prevalent frameworks and datasets to aid future researchers in thedevelopment of EVA systems. Finally, we discuss existing challenges and foreseefuture research directions. We believe this survey will help readers comprehendthe relationship between VA and edge computing, and spark new ideas on EVA.</description><author>Renjie Xu, Saiedeh Razavi, Rong Zheng</author><pubDate>Wed, 11 Oct 2023 16:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15751v3</guid></item><item><title>ChatGPT for Computational Topology</title><link>http://arxiv.org/abs/2310.07570v1</link><description>ChatGPT represents a significant milestone in the field of artificialintelligence (AI), finding widespread applications across diverse domains.However, its effectiveness in mathematical contexts has been somewhatconstrained by its susceptibility to conceptual errors. Concurrently,topological data analysis (TDA), a relatively new discipline, has garneredsubstantial interest in recent years. Nonetheless, the advancement of TDA isimpeded by the limited understanding of computational algorithms and codingproficiency among theoreticians. This work endeavors to bridge the gap betweentheoretical topological concepts and their practical implementation incomputational topology through the utilization of ChatGPT. We showcase how apure theoretician, devoid of computational experience and coding skills, caneffectively transform mathematical formulations and concepts into functionalcode for computational topology with the assistance of ChatGPT. Our strategyoutlines a productive process wherein a mathematician trains ChatGPT on puremathematical concepts, steers ChatGPT towards generating computational topologycode, and subsequently validates the generated code using established examples.Our specific case studies encompass the computation of Betti numbers, Laplacianmatrices, and Dirac matrices for simplicial complexes, as well as thepersistence of various homologies and Laplacians. Furthermore, we explore theapplication of ChatGPT in computing recently developed topological theories forhypergraphs and digraphs. This work serves as an initial step towardseffectively transforming pure mathematical theories into practicalcomputational tools, with the ultimate goal of enabling real applicationsacross diverse fields.</description><author>Jian Liu, Li Shen, Guo-Wei Wei</author><pubDate>Wed, 11 Oct 2023 16:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07570v1</guid></item><item><title>Multi-kernel Correntropy-based Orientation Estimation of IMUs: Gradient Descent Methods</title><link>http://arxiv.org/abs/2304.06548v2</link><description>This paper presents two computationally efficient algorithms for theorientation estimation of inertial measurement units (IMUs): thecorrentropy-based gradient descent (CGD) and the correntropy-based decoupledorientation estimation (CDOE). Traditional methods, such as gradient descent(GD) and decoupled orientation estimation (DOE), rely on the mean squared error(MSE) criterion, making them vulnerable to external acceleration and magneticinterference. To address this issue, we demonstrate that the multi-kernelcorrentropy loss (MKCL) is an optimal objective function for maximum likelihoodestimation (MLE) when the noise follows a type of heavy-tailed distribution. Incertain situations, the estimation error of the MKCL is bounded even in thepresence of arbitrarily large outliers. By replacing the standard MSE costfunction with MKCL, we develop the CGD and CDOE algorithms. We evaluate theeffectiveness of our proposed methods by comparing them with existingalgorithms in various situations. Experimental results indicate that ourproposed methods (CGD and CDOE) outperform their conventional counterparts (GDand DOE), especially when faced with external acceleration and magneticdisturbances. Furthermore, the new algorithms demonstrate significantly lowercomputational complexity than Kalman filter-based approaches, making themsuitable for applications with low-cost microprocessors.</description><author>Shilei Li, Lijing Li, Dawei Shi, Yunjiang Lou, Ling Shi</author><pubDate>Wed, 11 Oct 2023 16:09:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06548v2</guid></item><item><title>An Empirical Study of Multimodal Model Merging</title><link>http://arxiv.org/abs/2304.14933v2</link><description>Model merging (e.g., via interpolation or task arithmetic) fuses multiplemodels trained on different tasks to generate a multi-task solution. Thetechnique has been proven successful in previous studies, where the models aretrained on similar tasks and with the same initialization. In this paper, weexpand on this concept to a multimodal setup by merging transformers trained ondifferent modalities. Furthermore, we conduct our study for a novel goal wherewe can merge vision, language, and cross-modal transformers of amodality-specific architecture to create a parameter-efficientmodality-agnostic architecture. Through comprehensive experiments, wesystematically investigate the key factors impacting model performance aftermerging, including initialization, merging mechanisms, and model architectures.We also propose two metrics that assess the distance between weights to bemerged and can serve as an indicator of the merging outcomes. Our analysisleads to an effective training recipe for matching the performance of themodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.Our method also outperforms naive merging significantly on various tasks, withimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30kand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging</description><author>Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, Lijuan Wang</author><pubDate>Wed, 11 Oct 2023 16:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14933v2</guid></item><item><title>Vibroacoustic Frequency Response Prediction with Query-based Operator Networks</title><link>http://arxiv.org/abs/2310.05469v2</link><description>Understanding vibroacoustic wave propagation in mechanical structures likeairplanes, cars and houses is crucial to ensure health and comfort of theirusers. To analyze such systems, designers and engineers primarily consider thedynamic response in the frequency domain, which is computed through expensivenumerical simulations like the finite element method. In contrast, data-drivensurrogate models offer the promise of speeding up these simulations, therebyfacilitating tasks like design optimization, uncertainty quantification, anddesign space exploration. We present a structured benchmark for arepresentative vibroacoustic problem: Predicting the frequency response forvibrating plates with varying forms of beadings. The benchmark features a totalof 12,000 plate geometries with an associated numerical solution and introducesevaluation metrics to quantify the prediction quality. To address the frequencyresponse prediction task, we propose a novel frequency query operator model,which is trained to map plate geometries to frequency response functions. Byintegrating principles from operator learning and implicit models for shapeencoding, our approach effectively addresses the prediction of resonance peaksof frequency responses. We evaluate the method on our vibrating-platesbenchmark and find that it outperforms DeepONets, Fourier Neural Operators andmore traditional neural network architectures. The code and dataset areavailable from https://eckerlab.org/code/delden2023_plate.</description><author>Jan van Delden, Julius Schultz, Christopher Blech, Sabine C. Langer, Timo Lüddecke</author><pubDate>Wed, 11 Oct 2023 16:07:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05469v2</guid></item><item><title>ROMO: Retrieval-enhanced Offline Model-based Optimization</title><link>http://arxiv.org/abs/2310.07560v1</link><description>Data-driven black-box model-based optimization (MBO) problems arise in agreat number of practical application scenarios, where the goal is to find adesign over the whole space maximizing a black-box target function based on astatic offline dataset. In this work, we consider a more general butchallenging MBO setting, named constrained MBO (CoMBO), where only part of thedesign space can be optimized while the rest is constrained by the environment.A new challenge arising from CoMBO is that most observed designs that satisfythe constraints are mediocre in evaluation. Therefore, we focus on optimizingthese mediocre designs in the offline dataset while maintaining the givenconstraints rather than further boosting the best observed design in thetraditional MBO setting. We propose retrieval-enhanced offline model-basedoptimization (ROMO), a new derivable forward approach that retrieves theoffline dataset and aggregates relevant samples to provide a trustedprediction, and use it for gradient-based optimization. ROMO is simple toimplement and outperforms state-of-the-art approaches in the CoMBO setting.Empirically, we conduct experiments on a synthetic Hartmann (3D) functiondataset, an industrial CIO dataset, and a suite of modified tasks in theDesign-Bench benchmark. Results show that ROMO performs well in a wide range ofconstrained optimization tasks.</description><author>Mingcheng Chen, Haoran Zhao, Yuxiang Zhao, Hulei Fan, Hongqiao Gao, Yong Yu, Zheng Tian</author><pubDate>Wed, 11 Oct 2023 16:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07560v1</guid></item><item><title>Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning</title><link>http://arxiv.org/abs/2310.07558v1</link><description>We study the dynamic pricing problem where the demand function isnonparametric and H\"older smooth, and we focus on adaptivity to the unknownH\"older smoothness parameter $\beta$ of the demand function. Traditionally theoptimal dynamic pricing algorithm heavily relies on the knowledge of $\beta$ toachieve a minimax optimal regret of$\widetilde{O}(T^{\frac{\beta+1}{2\beta+1}})$. However, we highlight thechallenge of adaptivity in this dynamic pricing problem by proving that nopricing policy can adaptively achieve this minimax optimal regret withoutknowledge of $\beta$. Motivated by the impossibility result, we propose aself-similarity condition to enable adaptivity. Importantly, we show that theself-similarity condition does not compromise the problem's inherent complexitysince it preserves the regret lower bound$\Omega(T^{\frac{\beta+1}{2\beta+1}})$. Furthermore, we develop asmoothness-adaptive dynamic pricing algorithm and theoretically prove that thealgorithm achieves this minimax optimal regret bound without the priorknowledge $\beta$.</description><author>Zeqi Ye, Hansheng Jiang</author><pubDate>Wed, 11 Oct 2023 16:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07558v1</guid></item><item><title>Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape Bias by Distorted Shape</title><link>http://arxiv.org/abs/2310.07555v1</link><description>Deep learning models are known to exhibit a strong texture bias, while humantends to rely heavily on global shape for object recognition. The currentbenchmark for evaluating a model's shape bias is a set of style-transferredimages with the assumption that resistance to the attack of style transfer isrelated to the development of shape sensitivity in the model. In this work, weshow that networks trained with style-transfer images indeed learn to ignorestyle, but its shape bias arises primarily from local shapes. We provide aDistorted Shape Testbench (DiST) as an alternative measurement of global shapesensitivity. Our test includes 2400 original images from ImageNet-1K, each ofwhich is accompanied by two images with the global shapes of the original imagedistorted while preserving its texture via the texture synthesis program. Wefound that (1) models that performed well on the previous shape bias evaluationdo not fare well in the proposed DiST; (2) the widely adopted ViT models do notshow significant advantages over Convolutional Neural Networks (CNNs) on thisbenchmark despite that ViTs rank higher on the previous shape bias tests. (3)training with DiST images bridges the significant gap between human andexisting SOTA models' performance while preserving the models' accuracy onstandard image classification tasks; training with DiST images andstyle-transferred images are complementary, and can be combined to trainnetwork together to enhance both the global and local shape sensitivity of thenetwork. Our code will be host at: https://github.com/leelabcnbc/DiST</description><author>Ziqi Wen, Tianqin Li, Tai Sing Lee</author><pubDate>Wed, 11 Oct 2023 16:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07555v1</guid></item><item><title>Sequential composition of propositional logic programs</title><link>http://arxiv.org/abs/2009.05774v7</link><description>This paper introduces and studies the sequential composition anddecomposition of propositional logic programs. We show that acyclic programscan be decomposed into single-rule programs and provide a general decompositionresult for arbitrary programs. We show that the immediate consequence operatorof a program can be represented via composition which allows us to compute itsleast model without any explicit reference to operators. This bridges theconceptual gap between the syntax and semantics of a propositional logicprogram in a mathematically satisfactory way.</description><author>Christian Antic</author><pubDate>Wed, 11 Oct 2023 16:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.05774v7</guid></item><item><title>Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots</title><link>http://arxiv.org/abs/2306.12898v3</link><description>Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuablefor developing various optoelectronic devices such as QD lasers and singlephoton sources. The applications strongly rely on the density and quality ofthese dots, which has motivated studies of the growth process control torealize high-quality epi-wafers and devices. Establishing the processparameters in molecular beam epitaxy (MBE) for a specific density of QDs is amultidimensional optimization challenge, usually addressed throughtime-consuming and iterative trial-and-error. Here, we report a real-timefeedback control method to realize the growth of QDs with arbitrary density,which is fully automated and intelligent. We developed a machine learning (ML)model named 3D ResNet 50 trained using reflection high-energy electrondiffraction (RHEED) videos as input instead of static images and providingreal-time feedback on surface morphologies for process control. As a result, wedemonstrated that ML from previous growth could predict the post-growth densityof QDs, by successfully tuning the QD densities in near-real time from 1.5E10cm-2 down to 3.8E8 cm-2 or up to 1.4E11 cm-2. Compared to traditional methods,our approach, with in situ tuning capabilities and excellent reliability, candramatically expedite the material optimization process and improve thereproducibility of MBE, constituting significant progress for thin film growthtechniques. The concepts and methodologies proved feasible in this work arepromising to be applied to a variety of material growth processes, which willrevolutionize semiconductor manufacturing for optoelectronic andmicroelectronic industries.</description><author>Chao Shen, Wenkang Zhan, Kaiyao Xin, Manyang Li, Zhenyu Sun, Hui Cong, Chi Xu, Jian Tang, Zhaofeng Wu, Bo Xu, Zhongming Wei, Chunlai Xue, Chao Zhao, Zhanguo Wang</author><pubDate>Wed, 11 Oct 2023 15:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12898v3</guid></item><item><title>Proportional algebras</title><link>http://arxiv.org/abs/2210.01751v4</link><description>Analogical proportions are expressions of the form "$a$ is to $b$ what $c$ isto $d$" at the core of analogical reasoning which itself is at the core ofartificial intelligence. This paper introduces proportional algebras asalgebras endowed with a 4-ary analogical proportion relation $a:b::c:d$satisfying a suitable set of axioms. Functions preserving analogicalproportions have already proven to be of practical interest in artificialintelligence and studying their mathematical properties is essential forunderstanding proportions. We therefore introduce proportional homomorphismsand their associated congruences and proportional functors, and show that theyare closely related notions. In a broader sense, this paper is a further steptowards a mathematical theory of analogical proportions and analogicalreasoning in general.</description><author>Christian Antić</author><pubDate>Wed, 11 Oct 2023 15:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01751v4</guid></item><item><title>ProtoHPE: Prototype-guided High-frequency Patch Enhancement for Visible-Infrared Person Re-identification</title><link>http://arxiv.org/abs/2310.07552v1</link><description>Visible-infrared person re-identification is challenging due to the largemodality gap. To bridge the gap, most studies heavily rely on the correlationof visible-infrared holistic person images, which may perform poorly undersevere distribution shifts. In contrast, we find that some cross-modalcorrelated high-frequency components contain discriminative visual patterns andare less affected by variations such as wavelength, pose, and backgroundclutter than holistic images. Therefore, we are motivated to bridge themodality gap based on such high-frequency components, and propose\textbf{Proto}type-guided \textbf{H}igh-frequency \textbf{P}atch\textbf{E}nhancement (ProtoHPE) with two core designs. \textbf{First}, toenhance the representation ability of cross-modal correlated high-frequencycomponents, we split patches with such components by Wavelet Transform andexponential moving average Vision Transformer (ViT), then empower ViT to takethe split patches as auxiliary input. \textbf{Second}, to obtain semanticallycompact and discriminative high-frequency representations of the same identity,we propose Multimodal Prototypical Contrast. To be specific, it hierarchicallycaptures the comprehensive semantics of different modal instances, facilitatingthe aggregation of high-frequency representations belonging to the sameidentity. With it, ViT can capture key high-frequency components duringinference without relying on ProtoHPE, thus bringing no extra complexity.Extensive experiments validate the effectiveness of ProtoHPE.</description><author>Guiwei Zhang, Yongfei Zhang, Zichang Tan</author><pubDate>Wed, 11 Oct 2023 15:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07552v1</guid></item><item><title>Analogical proportions in monounary algebras</title><link>http://arxiv.org/abs/2208.06829v3</link><description>This paper studies analogical proportions in monounary algebras consistingonly of a universe and a single unary function. We show that the analogicalproportion relation is characterized in the infinite monounary algebra formedby the natural numbers together with the successor function via differenceproportions.</description><author>Christian Antić</author><pubDate>Wed, 11 Oct 2023 15:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.06829v3</guid></item><item><title>Attribute Localization and Revision Network for Zero-Shot Learning</title><link>http://arxiv.org/abs/2310.07548v1</link><description>Zero-shot learning enables the model to recognize unseen categories with theaid of auxiliary semantic information such as attributes. Current worksproposed to detect attributes from local image regions and align extractedfeatures with class-level semantics. In this paper, we find that the choicebetween local and global features is not a zero-sum game, global features canalso contribute to the understanding of attributes. In addition, aligningattribute features with class-level semantics ignores potential intra-classattribute variation. To mitigate these disadvantages, we present AttributeLocalization and Revision Network in this paper. First, we design AttributeLocalization Module (ALM) to capture both local and global features from imageregions, a novel module called Scale Control Unit is incorporated to fuseglobal and local representations. Second, we propose Attribute Revision Module(ARM), which generates image-level semantics by revising the ground-truth valueof each attribute, compensating for performance degradation caused by ignoringintra-class variation. Finally, the output of ALM will be aligned with revisedsemantics produced by ARM to achieve the training process. Comprehensiveexperimental results on three widely used benchmarks demonstrate theeffectiveness of our model in the zero-shot prediction task.</description><author>Junzhe Xu, Suling Duan, Chenwei Tang, Zhenan He, Jiancheng Lv</author><pubDate>Wed, 11 Oct 2023 15:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07548v1</guid></item><item><title>Generalization-based similarity</title><link>http://arxiv.org/abs/2302.10096v3</link><description>Detecting and exploiting similarities between seemingly distant objects is atthe core of analogical reasoning which itself is at the core of artificialintelligence. This paper develops {\em from the ground up} an abstractalgebraic and {\em qualitative} notion of similarity based on the observationthat sets of generalizations encode important properties of elements. We showthat similarity defined in this way has appealing mathematical properties. Aswe construct our notion of similarity from first principles using onlyelementary concepts of universal algebra, to convince the reader of itsplausibility, we show that it can be naturally embedded into first-order logicvia model-theoretic types.</description><author>Christian Antić</author><pubDate>Wed, 11 Oct 2023 15:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10096v3</guid></item><item><title>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</title><link>http://arxiv.org/abs/2308.12067v2</link><description>Multimodal large language models are typically trained in two stages: firstpre-training on image-text pairs, and then fine-tuning using supervisedvision-language instruction data. Recent studies have shown that large languagemodels can achieve satisfactory results even with a limited amount ofhigh-quality instruction-following data. In this paper, we introduceInstructionGPT-4, which is fine-tuned on a small dataset comprising only 200examples, amounting to approximately 6\% of the instruction-following data usedin the alignment dataset for MiniGPT-4. To achieve this, we first proposeseveral metrics to access the quality of multimodal instruction data. Based onthese metrics, we present an effective and trainable data selector toautomatically identify and filter low-quality vision-language data. Byemploying this method, InstructionGPT-4 outperforms the original MiniGPT-4 onvarious evaluations. Overall, our findings demonstrate that less buthigh-quality instruction tuning data is efficient in enabling multimodal largelanguage models to generate better output. Our code is available athttps://github.com/waltonfuture/InstructionGPT-4.</description><author>Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</author><pubDate>Wed, 11 Oct 2023 15:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12067v2</guid></item><item><title>Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift</title><link>http://arxiv.org/abs/2310.07535v1</link><description>Covariate shift in the test data can significantly downgrade both theaccuracy and the fairness performance of the model. Ensuring fairness acrossdifferent sensitive groups in such settings is of paramount importance due tosocietal implications like criminal justice. We operate under the unsupervisedregime where only a small set of unlabeled test samples along with a labeledtraining set is available. Towards this problem, we make three contributions.First is a novel composite weighted entropy based objective for predictionaccuracy which is optimized along with a representation matching loss forfairness. We experimentally verify that optimizing with our loss formulationoutperforms a number of state-of-the-art baselines in the pareto sense withrespect to the fairness-accuracy tradeoff on several standard datasets. Oursecond contribution is a new setting we term Asymmetric Covariate Shift that,to the best of our knowledge, has not been studied before. Asymmetric covariateshift occurs when distribution of covariates of one group shifts significantlycompared to the other groups and this happens when a dominant group isover-represented. While this setting is extremely challenging for currentbaselines, We show that our proposed method significantly outperforms them. Ourthird contribution is theoretical, where we show that our weighted entropy termalong with prediction loss on the training set approximates test loss undercovariate shift. Empirically and through formal sample complexity bounds, weshow that this approximation to the unseen test loss does not depend onimportance sampling variance which affects many other baselines.</description><author>Shreyas Havaldar, Jatin Chauhan, Karthikeyan Shanmugam, Jay Nandy, Aravindan Raghuveer</author><pubDate>Wed, 11 Oct 2023 15:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07535v1</guid></item></channel></rss>