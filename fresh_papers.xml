<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 08 Jun 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ModuleFormer: Learning Modular Large Language Models From Uncurated Data</title><link>http://arxiv.org/abs/2306.04640v1</link><description>Large Language Models (LLMs) have achieved remarkable results. But existingmodels are expensive to train and deploy, and it is also difficult to expandtheir knowledge beyond pre-training data without forgetting previous knowledge.This paper proposes a new neural network architecture, ModuleFormer, thatleverages modularity to improve the efficiency and flexibility of largelanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).Unlike the previous SMoE-based modular language model [Gururangan et al.,2021], which requires domain-labeled data to learn domain-specific experts,ModuleFormer can induce modularity from uncurated data with its new loadbalancing and load concentration losses. ModuleFormer is a modular architecturethat includes two different types of modules, new stick-breaking attentionheads, and feedforward experts. Different modules are sparsely activatedconditions on the input token during training and inference. In our experiment,we found that the modular architecture enables three important abilities forlarge pre-trained language models: 1) Efficiency, since ModuleFormer onlyactivates a subset of its modules for each input token, thus it could achievethe same performance as dense LLMs with more than two times throughput; 2)Extendability, ModuleFormer is more immune to catastrophic forgetting thandense LLMs and can be easily extended with new modules to learn new knowledgethat is not included in the training data; 3) Specialisation, finetuningModuleFormer could specialize a subset of modules to the finetuning task, andthe task-unrelated modules could be easily pruned for a lightweight deployment.</description><author>Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, Chuang Gan</author><pubDate>Wed, 07 Jun 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04640v1</guid></item><item><title>Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection</title><link>http://arxiv.org/abs/2306.04637v1</link><description>Neural sequence models based on the transformer architecture havedemonstrated remarkable \emph{in-context learning} (ICL) abilities, where theycan perform new tasks when prompted with training and test examples, withoutany parameter update to the model. This work first provides a comprehensivestatistical theory for transformers to perform ICL. Concretely, we show thattransformers can implement a broad class of standard machine learningalgorithms in context, such as least squares, ridge regression, Lasso, learninggeneralized linear models, and gradient descent on two-layer neural networks,with near-optimal predictive power on various in-context data distributions.Using an efficient implementation of in-context gradient descent as theunderlying mechanism, our transformer constructions admit mild size bounds, andcan be learned with polynomially many pretraining sequences. Building on these ``base'' ICL algorithms, intriguingly, we show thattransformers can implement more complex ICL procedures involving\emph{in-context algorithm selection}, akin to what a statistician can do inreal life -- A \emph{single} transformer can adaptively select different baseICL algorithms -- or even perform qualitatively different tasks -- on differentinput sequences, without any explicit prompting of the right algorithm or task.We both establish this in theory by explicit constructions, and also observethis phenomenon experimentally. In theory, we construct two general mechanismsfor algorithm selection with concrete examples: pre-ICL testing, and post-ICLvalidation. As an example, we use the post-ICL validation mechanism toconstruct a transformer that can perform nearly Bayes-optimal ICL on achallenging task -- noisy linear models with mixed noise levels.Experimentally, we demonstrate the strong in-context algorithm selectioncapabilities of standard transformer architectures.</description><author>Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei</author><pubDate>Wed, 07 Jun 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04637v1</guid></item><item><title>GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation</title><link>http://arxiv.org/abs/2306.04636v1</link><description>Recent advances in deep learning have witnessed many successful unsupervisedimage-to-image translation models that learn correspondences between two visualdomains without paired data. However, it is still a great challenge to buildrobust mappings between various domains especially for those with drasticvisual discrepancies. In this paper, we introduce a novel versatile framework,Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), thatimproves the quality, applicability and controllability of the existingtranslation models. The key idea of GP-UNIT is to distill the generative priorfrom pre-trained class-conditional GANs to build coarse-level cross-domaincorrespondences, and to apply the learned prior to adversarial translations toexcavate fine-level correspondences. With the learned multi-level contentcorrespondences, GP-UNIT is able to perform valid translations between bothclose domains and distant domains. For close domains, GP-UNIT can beconditioned on a parameter to determine the intensity of the contentcorrespondences during translation, allowing users to balance between contentand style consistency. For distant domains, semi-supervised learning isexplored to guide GP-UNIT to discover accurate semantic correspondences thatare hard to learn solely from the appearance. We validate the superiority ofGP-UNIT over state-of-the-art translation models in robust, high-quality anddiversified translations between various domains through extensive experiments.</description><author>Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy</author><pubDate>Wed, 07 Jun 2023 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04636v1</guid></item><item><title>On the Reliability of Watermarks for Large Language Models</title><link>http://arxiv.org/abs/2306.04634v1</link><description>Large language models (LLMs) are now deployed to everyday use and positionedto produce large quantities of text in the coming decade. Machine-generatedtext may displace human-written text on the internet and has the potential tobe used for malicious purposes, such as spearphishing attacks and social mediabots. Watermarking is a simple and effective strategy for mitigating such harmsby enabling the detection and documentation of LLM-generated text. Yet, acrucial question remains: How reliable is watermarking in realistic settings inthe wild? There, watermarked text might be mixed with other text sources,paraphrased by human writers or other language models, and used forapplications in a broad number of domains, both social and technical. In thispaper, we explore different detection schemes, quantify their power atdetecting watermarks, and determine how much machine-generated text needs to beobserved in each scenario to reliably detect the watermark. We especiallyhighlight our human study, where we investigate the reliability of watermarkingwhen faced with human paraphrasing. We compare watermark-based detection toother detection strategies, finding overall that watermarking is a reliablesolution, especially because of its sample complexity - for all attacks weconsider, the watermark evidence compounds the more examples are given, and thewatermark is eventually detected.</description><author>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein</author><pubDate>Wed, 07 Jun 2023 18:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04634v1</guid></item><item><title>Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion</title><link>http://arxiv.org/abs/2306.04633v1</link><description>Instance segmentation in 3D is a challenging task due to the lack oflarge-scale annotated datasets. In this paper, we show that this task can beaddressed effectively by leveraging instead 2D pre-trained models for instancesegmentation. We propose a novel approach to lift 2D segments to 3D and fusethem by means of a neural field representation, which encourages multi-viewconsistency across frames. The core of our approach is a slow-fast clusteringobjective function, which is scalable and well-suited for scenes with a largenumber of objects. Unlike previous approaches, our method does not require anupper bound on the number of objects or object tracking across frames. Todemonstrate the scalability of the slow-fast clustering, we create a newsemi-realistic dataset called the Messy Rooms dataset, which features sceneswith up to 500 objects per scene. Our approach outperforms the state-of-the-arton challenging scenes from the ScanNet, Hypersim, and Replica datasets, as wellas on our newly created Messy Rooms dataset, demonstrating the effectivenessand scalability of our slow-fast clustering method.</description><author>Yash Bhalgat, Iro Laina, Jo√£o F. Henriques, Andrew Zisserman, Andrea Vedaldi</author><pubDate>Wed, 07 Jun 2023 18:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04633v1</guid></item><item><title>Designing a Better Asymmetric VQGAN for StableDiffusion</title><link>http://arxiv.org/abs/2306.04632v1</link><description>StableDiffusion is a revolutionary text-to-image generator that is causing astir in the world of image generation and editing. Unlike traditional methodsthat learn a diffusion model in pixel space, StableDiffusion learns a diffusionmodel in the latent space via a VQGAN, ensuring both efficiency and quality. Itnot only supports image generation tasks, but also enables image editing forreal images, such as image inpainting and local editing. However, we haveobserved that the vanilla VQGAN used in StableDiffusion leads to significantinformation loss, causing distortion artifacts even in non-edited imageregions. To this end, we propose a new asymmetric VQGAN with two simpledesigns. Firstly, in addition to the input from the encoder, the decodercontains a conditional branch that incorporates information from task-specificpriors, such as the unmasked image region in inpainting. Secondly, the decoderis much heavier than the encoder, allowing for more detailed recovery whileonly slightly increasing the total inference cost. The training cost of ourasymmetric VQGAN is cheap, and we only need to retrain a new asymmetric decoderwhile keeping the vanilla VQGAN encoder and StableDiffusion unchanged. Ourasymmetric VQGAN can be widely used in StableDiffusion-based inpainting andlocal editing methods. Extensive experiments demonstrate that it cansignificantly improve the inpainting and editing performance, while maintainingthe original text-to-image capability. The code is available at\url{https://github.com/buxiangzhiren/Asymmetric_VQGAN}.</description><author>Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, Gang Hua</author><pubDate>Wed, 07 Jun 2023 18:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04632v1</guid></item><item><title>PALR: Personalization Aware LLMs for Recommendation</title><link>http://arxiv.org/abs/2305.07622v3</link><description>Large language models (LLMs) have recently received significant attention fortheir exceptional capabilities. Despite extensive efforts in developinggeneral-purpose LLMs that can be utilized in various natural languageprocessing (NLP) tasks, there has been less research exploring their potentialin recommender systems. In this paper, we propose a novel framework, namedPALR, which aiming to combine user history behaviors (such as clicks,purchases, ratings, etc.) with LLMs to generate user preferred items.Specifically, we first use user/item interactions as guidance for candidateretrieval. Then we adopt a LLM-based ranking model to generate recommendeditems. Unlike existing approaches that typically adopt general-purpose LLMs forzero/few-shot recommendation testing or training on small-sized language models(with less than 1 billion parameters), which cannot fully elicit LLMs'reasoning abilities and leverage rich item side parametric knowledge, wefine-tune a 7 billion parameters LLM for the ranking purpose. This model takesretrieval candidates in natural language format as input, with instructionwhich explicitly asking to select results from input candidates duringinference. Our experimental results demonstrate that our solution outperformsstate-of-the-art models on various sequential recommendation tasks.</description><author>Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, Yanbin Lu</author><pubDate>Wed, 07 Jun 2023 18:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07622v3</guid></item><item><title>Yet Another Algorithm for Supervised Principal Component Analysis: Supervised Linear Centroid-Encoder</title><link>http://arxiv.org/abs/2306.04622v1</link><description>We propose a new supervised dimensionality reduction technique calledSupervised Linear Centroid-Encoder (SLCE), a linear counterpart of thenonlinear Centroid-Encoder (CE) \citep{ghosh2022supervised}. SLCE works bymapping the samples of a class to its class centroid using a lineartransformation. The transformation is a projection that reconstructs a pointsuch that its distance from the corresponding class centroid, i.e.,centroid-reconstruction loss, is minimized in the ambient space. We derive aclosed-form solution using an eigendecomposition of a symmetric matrix. We dida detailed analysis and presented some crucial mathematical properties of theproposed approach. %We also provide an iterative solution approach basedsolving the optimization problem using a descent method. We establish aconnection between the eigenvalues and the centroid-reconstruction loss. Incontrast to Principal Component Analysis (PCA) which reconstructs a sample inthe ambient space, the transformation of SLCE uses the instances of a class torebuild the corresponding class centroid. Therefore the proposed method can beconsidered a form of supervised PCA. Experimental results show the performanceadvantage of SLCE over other supervised methods.</description><author>Tomojit Ghosh, Michael Kirby</author><pubDate>Wed, 07 Jun 2023 18:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04622v1</guid></item><item><title>Balanced Product of Calibrated Experts for Long-Tailed Recognition</title><link>http://arxiv.org/abs/2206.05260v3</link><description>Many real-world recognition problems are characterized by long-tailed labeldistributions. These distributions make representation learning highlychallenging due to limited generalization over the tail classes. If the testdistribution differs from the training distribution, e.g. uniform versuslong-tailed, the problem of the distribution shift needs to be addressed. Arecent line of work proposes learning multiple diverse experts to tackle thisissue. Ensemble diversity is encouraged by various techniques, e.g. byspecializing different experts in the head and the tail classes. In this work,we take an analytical approach and extend the notion of logit adjustment toensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines afamily of experts with different test-time target distributions, generalizingseveral previous approaches. We show how to properly define these distributionsand combine the experts in order to achieve unbiased predictions, by provingthat the ensemble is Fisher-consistent for minimizing the balanced error. Ourtheoretical analysis shows that our balanced ensemble requires calibratedexperts, which we achieve in practice using mixup. We conduct extensiveexperiments and our method obtains new state-of-the-art results on threelong-tailed datasets: CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018. Our codeis available at https://github.com/emasa/BalPoE-CalibratedLT.</description><author>Emanuel Sanchez Aimar, Arvi Jonnarth, Michael Felsberg, Marco Kuhlmann</author><pubDate>Wed, 07 Jun 2023 18:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.05260v3</guid></item><item><title>Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning</title><link>http://arxiv.org/abs/2306.04621v1</link><description>Addressing the class imbalance in long-tailed semi-supervised learning (SSL)poses a few significant challenges stemming from differences between themarginal distributions of unlabeled data and the labeled data, as the former isoften unknown and potentially distinct from the latter. The first challenge isto avoid biasing the pseudo-labels towards an incorrect distribution, such asthat of the labeled data or a balanced distribution, during training. However,we still wish to ensure a balanced unlabeled distribution during inference,which is the second challenge. To address both of these challenges, we proposea three-faceted solution: a flexible distribution alignment that progressivelyaligns the classifier from a dynamically estimated unlabeled prior towards abalanced distribution, a soft consistency regularization that exploitsunderconfident pseudo-labels discarded by threshold-based methods, and a schemafor expanding the unlabeled set with input data from the labeled partition.This last facet comes in as a response to the commonly-overlooked fact thatdisjoint partitions of labeled and unlabeled data prevent the benefits ofstrong data augmentation on the labeled set. Our overall framework requires noadditional training cycles, so it will align, distill, and augment everythingall at once (ADALLO). Our extensive evaluations of ADALLO on imbalanced SSLbenchmark datasets, including CIFAR10-LT, CIFAR100-LT, and STL10-LT withvarying degrees of class imbalance, amount of labeled data, and distributionmismatch, demonstrate significant improvements in the performance of imbalancedSSL under large distribution mismatch, as well as competitiveness withstate-of-the-art methods when the labeled and unlabeled data follow the samemarginal distribution. Our code will be released upon paper acceptance.</description><author>Emanuel Sanchez Aimar, Hannah Helgesen, Michael Felsberg, Marco Kuhlmann</author><pubDate>Wed, 07 Jun 2023 18:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04621v1</guid></item><item><title>Language Models can Solve Computer Tasks</title><link>http://arxiv.org/abs/2303.17491v2</link><description>Agents capable of carrying out general tasks on a computer can improveefficiency and productivity by automating repetitive tasks and assisting incomplex problem-solving. Ideally, such agents should be able to solve newcomputer tasks presented to them through natural language commands. However,previous approaches to this problem require large amounts of expertdemonstrations and task-specific reward functions, both of which areimpractical for new tasks. In this work, we show that a pre-trained largelanguage model (LLM) agent can execute computer tasks guided by naturallanguage using a simple prompting scheme where the agent Recursively Criticizesand Improves its output (RCI). The RCI approach significantly outperformsexisting LLM methods for automating computer tasks and surpasses supervisedlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++benchmark. We compare multiple LLMs and find that RCI with theInstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handfulof demonstrations per task rather than tens of thousands, and without atask-specific reward function. Furthermore, we demonstrate RCI prompting'seffectiveness in enhancing LLMs' reasoning abilities on a suite of naturallanguage reasoning tasks, outperforming chain of thought (CoT) prompting. Wefind that RCI combined with CoT performs better than either separately. Ourcode can be found here: https://github.com/posgnu/rci-agent.</description><author>Geunwoo Kim, Pierre Baldi, Stephen McAleer</author><pubDate>Wed, 07 Jun 2023 18:50:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17491v2</guid></item><item><title>Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular Design</title><link>http://arxiv.org/abs/2306.04620v1</link><description>In recent years, in-silico molecular design has received much attention fromthe machine learning community. When designing a new compound forpharmaceutical applications, there are usually multiple properties of suchmolecules that need to be optimised: binding energy to the target,synthesizability, toxicity, EC50, and so on. While previous approaches haveemployed a scalarization scheme to turn the multi-objective problem into apreference-conditioned single objective, it has been established that this kindof reduction may produce solutions that tend to slide towards the extremepoints of the objective space when presented with a problem that exhibits aconcave Pareto front. In this work we experiment with an alternativeformulation of goal-conditioned molecular generation to obtain a morecontrollable conditional model that can uniformly explore solutions along theentire Pareto front.</description><author>Julien Roy, Pierre-Luc Bacon, Christopher Pal, Emmanuel Bengio</author><pubDate>Wed, 07 Jun 2023 18:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04620v1</guid></item><item><title>Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations</title><link>http://arxiv.org/abs/2306.04618v1</link><description>This paper reexamines the research on out-of-distribution (OOD) robustness inthe field of NLP. We find that the distribution shift settings in previousstudies commonly lack adequate challenges, hindering the accurate evaluation ofOOD robustness. To address these issues, we propose a benchmark constructionprotocol that ensures clear differentiation and challenging distributionshifts. Then we introduce BOSS, a Benchmark suite for Out-of-distributionrobustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, weconduct a series of experiments on pre-trained language models for analysis andevaluation of OOD robustness. First, for vanilla fine-tuning, we examine therelationship between in-distribution (ID) and OOD performance. We identifythree typical types that unveil the inner learning mechanism, which couldpotentially facilitate the forecasting of OOD robustness, correlating with theadvancements on ID datasets. Then, we evaluate 5 classic methods on BOSS andfind that, despite exhibiting some effectiveness in specific cases, they do notoffer significant improvement compared to vanilla fine-tuning. Further, weevaluate 5 LLMs with various adaptation paradigms and find that when sufficientID data is available, fine-tuning domain-specific models outperform LLMs on IDexamples significantly. However, in the case of OOD instances, prioritizingLLMs with in-context learning yields better results. We identify that bothfine-tuned small models and LLMs face challenges in effectively addressingdownstream tasks. The code is public at\url{https://github.com/lifan-yuan/OOD_NLP}.</description><author>Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, Maosong Sun</author><pubDate>Wed, 07 Jun 2023 18:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04618v1</guid></item><item><title>ICON$^2$: Reliably Benchmarking Predictive Inequity in Object Detection</title><link>http://arxiv.org/abs/2306.04482v1</link><description>As computer vision systems are being increasingly deployed at scale inhigh-stakes applications like autonomous driving, concerns about social bias inthese systems are rising. Analysis of fairness in real-world vision systems,such as object detection in driving scenes, has been limited to observingpredictive inequity across attributes such as pedestrian skin tone, and lacks aconsistent methodology to disentangle the role of confounding variables e.g.does my model perform worse for a certain skin tone, or are such scenes in mydataset more challenging due to occlusion and crowds? In this work, weintroduce ICON$^2$, a framework for robustly answering this question. ICON$^2$leverages prior knowledge on the deficiencies of object detection systems toidentify performance discrepancies across sub-populations, compute correlationsbetween these potential confounders and a given sensitive attribute, andcontrol for the most likely confounders to obtain a more reliable estimate ofmodel bias. Using our approach, we conduct an in-depth study on the performanceof object detection with respect to income from the BDD100K driving dataset,revealing useful insights.</description><author>Sruthi Sudhakar, Viraj Prabhu, Olga Russakovsky, Judy Hoffman</author><pubDate>Wed, 07 Jun 2023 18:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04482v1</guid></item><item><title>Smooth Non-Stationary Bandits</title><link>http://arxiv.org/abs/2301.12366v2</link><description>In many applications of online decision making, the environment isnon-stationary and it is therefore crucial to use bandit algorithms that handlechanges. Most existing approaches are designed to protect against non-smoothchanges, constrained only by total variation or Lipschitzness over time, wherethey guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practiceenvironments are often changing {\bf smoothly}, so such algorithms may incurhigher-than-necessary regret in these settings and do not leverage informationon the rate of change. We study a non-stationary two-armed bandits problemwhere we assume that an arm's mean reward is a $\beta$-H\"older function over(normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuouslydifferentiable. We show the first separation between the smooth and non-smoothregimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$.We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower boundfor any integer $\beta\ge 1$, which matches our upper bound for $\beta=2$.</description><author>Su Jia, Qian Xie, Nathan Kallus, Peter I. Frazier</author><pubDate>Wed, 07 Jun 2023 18:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12366v2</guid></item><item><title>Progressive Update Guided Interdependent Networks for Single Image Dehazing</title><link>http://arxiv.org/abs/2008.01701v4</link><description>Images with haze of different varieties often pose a significant challenge todehazing. Therefore, guidance by estimates of haze parameters related to thevariety would be beneficial, and their progressive update jointly with hazereduction will allow effective dehazing. To this end, we propose amulti-network dehazing framework containing novel interdependent dehazing andhaze parameter updater networks that operate in a progressive manner. The hazeparameters, transmission map and atmospheric light, are first estimated usingdedicated convolutional networks that allow color-cast handling. The estimatedparameters are then used to guide our dehazing module, where the estimates areprogressively updated by novel convolutional networks. The updating takes placejointly with progressive dehazing using a network that invokes inter-stepdependencies. The joint progressive updating and dehazing gradually modify thehaze parameter values toward achieving effective dehazing. Through differentstudies, our dehazing framework is shown to be more effective thanimage-to-image mapping and predefined haze formation model based dehazing. Theframework is also found capable of handling a wide variety of hazy conditionswtih different types and amounts of haze and color casts. Our dehazingframework is qualitatively and quantitatively found to outperform thestate-of-the-art on synthetic and real-world hazy images of multiple datasetswith varied haze conditions.</description><author>Aupendu Kar, Sobhan Kanti Dhara, Debashis Sen, Prabir Kumar Biswas</author><pubDate>Wed, 07 Jun 2023 18:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.01701v4</guid></item><item><title>Mixed Autoencoder for Self-supervised Visual Representation Learning</title><link>http://arxiv.org/abs/2303.17152v2</link><description>Masked Autoencoder (MAE) has demonstrated superior performance on variousvision tasks via randomly masking image patches and reconstruction. However,effective data augmentation strategies for MAE still remain open questions,different from those in contrastive learning that serve as the most importantpart. This paper studies the prevailing mixing augmentation for MAE. We firstdemonstrate that naive mixing will in contrast degenerate model performance dueto the increase of mutual information (MI). To address, we propose homologousrecognition, an auxiliary pretext task, not only to alleviate the MIincreasement by explicitly requiring each patch to recognize homologouspatches, but also to perform object-aware self-supervised pre-training forbetter downstream dense perception performance. With extensive experiments, wedemonstrate that our proposed Mixed Autoencoder (MixedAE) achieves thestate-of-the-art transfer results among masked image modeling (MIM)augmentations on different downstream tasks with significant efficiency.Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instancediscrimination, while accelerating training by 2x. To our best knowledge, thisis the very first work to consider mixing for MIM from the perspective ofpretext task design. Code will be made available.</description><author>Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung</author><pubDate>Wed, 07 Jun 2023 18:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17152v2</guid></item><item><title>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</title><link>http://arxiv.org/abs/2306.03901v2</link><description>Large language models (LLMs) with memory are computationally universal.However, mainstream LLMs are not taking full advantage of memory, and thedesigns are heavily influenced by biological brains. Due to their approximatenature and proneness to the accumulation of errors, conventional neural memorymechanisms cannot support LLMs to simulate complex reasoning. In this paper, weseek inspiration from modern computer architectures to augment LLMs withsymbolic memory for complex multi-hop reasoning. Such a symbolic memoryframework is instantiated as an LLM and a set of SQL databases, where the LLMgenerates SQL instructions to manipulate the SQL databases. We validate theeffectiveness of the proposed memory framework on a synthetic dataset requiringcomplex reasoning. The project website is available athttps://chatdatabase.github.io/ .</description><author>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao</author><pubDate>Wed, 07 Jun 2023 18:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03901v2</guid></item><item><title>The Two Word Test: A Semantic Benchmark for Large Language Models</title><link>http://arxiv.org/abs/2306.04610v1</link><description>Large Language Models (LLMs) have shown remarkable abilities recently,including passing advanced professional exams and demanding benchmark tests.This performance has led many to suggest that they are close to achievinghumanlike or 'true' understanding of language, and even Artificial GeneralIntelligence (AGI). Here, we provide a new open-source benchmark that canassess semantic abilities of LLMs using two-word phrases using a task that canbe performed relatively easily by humans without advanced training. Combiningmultiple words into a single concept is a fundamental aspect of human languageand intelligence. The test requires meaningfulness judgments of 1768 noun-nouncombinations that have been rated as meaningful (e.g., baby boy) or notmeaningful (e.g., goat sky). by 150 human raters. We provide versions of thetask that probe meaningfulness ratings on a 0-4 scale as well as binaryjudgments. We conducted a series of experiments using the TWT on GPT-4,GPT-3.5, and Bard, with both versions. Results demonstrated that, compared tohumans, all models perform poorly at rating meaningfulness of these phrases.GPT-3.5 and Bard are also unable to make binary discriminations betweensensible and nonsense phrases as making sense. GPT-4 makes a substantialimprovement in binary discrimination of combinatorial phrases but is stillsignificantly worse than human performance. The TWT can be used to understandthe limitations and weaknesses of current LLMs, and potentially improve them.The test also reminds us that caution is warranted in attributing 'trueunderstanding' or AGI to LLMs. TWT is available at:https://github.com/NickRiccardi/two-word-test</description><author>Nicholas Riccardi, Rutvik H. Desai</author><pubDate>Wed, 07 Jun 2023 18:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04610v1</guid></item><item><title>Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt</title><link>http://arxiv.org/abs/2306.04607v1</link><description>Diffusion models have attracted significant attention due to their remarkableability to create content and generate data for tasks such as imageclassification. However, the usage of diffusion models to generate high-qualityobject detection data remains an underexplored area, where not only theimage-level perceptual quality but also geometric conditions such as boundingboxes and camera views are essential. Previous studies have utilized eithercopy-paste synthesis or layout-to-image (L2I) generation with specificallydesigned modules to encode semantic layouts. In this paper, we proposeGeoDiffusion, a simple framework that can flexibly translate various geometricconditions into text prompts and empower the pre-trained text-to-image (T2I)diffusion models for high-quality detection data generation. Unlike previousL2I methods, our GeoDiffusion is able to encode not only bounding boxes butalso extra geometric conditions such as camera views in self-driving scenes.Extensive experiments demonstrate GeoDiffusion outperforms previous L2I methodswhile maintaining 4x training time faster. To the best of our knowledge, thisis the first work to adopt diffusion models for layout-to-image generation withgeometric conditions and demonstrate that L2I-generated images can bebeneficial for improving the performance of object detectors.</description><author>Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung</author><pubDate>Wed, 07 Jun 2023 18:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04607v1</guid></item><item><title>Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization</title><link>http://arxiv.org/abs/2208.09770v2</link><description>This paper presents Z-Code++, a new pre-trained language model optimized forabstractive text summarization. The model extends the state of the artencoder-decoder model using three techniques. First, we use a two-phasepre-training process to improve model's performance on low-resourcesummarization tasks. The model is first pre-trained using text corpora forlanguage understanding, and then is continually pre-trained on summarizationcorpora for grounded text generation. Second, we replace self-attention layersin the encoder with disentangled attention layers, where each word isrepresented using two vectors that encode its content and position,respectively. Third, we use fusion-in-encoder, a simple yet effective method ofencoding long sequences in a hierarchical manner. Z-Code++ creates new state ofthe art on 9 out of 13 text summarization tasks across 5 languages. Our modelis parameter-efficient in that it outperforms the 600x larger PaLM-540B onXSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot andfew-shot settings, our model substantially outperforms the competing models.</description><author>Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao, Xuedong Huang</author><pubDate>Wed, 07 Jun 2023 18:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09770v2</guid></item><item><title>Uncovering solutions from data corrupted by systematic errors: A physics-constrained convolutional neural network approach</title><link>http://arxiv.org/abs/2306.04600v1</link><description>Information on natural phenomena and engineering systems is typicallycontained in data. Data can be corrupted by systematic errors in models andexperiments. In this paper, we propose a tool to uncover the spatiotemporalsolution of the underlying physical system by removing the systematic errorsfrom data. The tool is the physics-constrained convolutional neural network(PC-CNN), which combines information from both the systems governing equationsand data. We focus on fundamental phenomena that are modelled by partialdifferential equations, such as linear convection, Burgers equation, andtwo-dimensional turbulence. First, we formulate the problem, describe thephysics-constrained convolutional neural network, and parameterise thesystematic error. Second, we uncover the solutions from data corrupted by largemultimodal systematic errors. Third, we perform a parametric study fordifferent systematic errors. We show that the method is robust. Fourth, weanalyse the physical properties of the uncovered solutions. We show that thesolutions inferred from the PC-CNN are physical, in contrast to the datacorrupted by systematic errors that does not fulfil the governing equations.This work opens opportunities for removing epistemic errors from models, andsystematic errors from measurements.</description><author>Daniel Kelshaw, Luca Magri</author><pubDate>Wed, 07 Jun 2023 18:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04600v1</guid></item><item><title>Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions</title><link>http://arxiv.org/abs/2306.04597v1</link><description>Societal biases present in pre-trained large language models are a criticalissue as these models have been shown to propagate biases in countlessdownstream applications, rendering them unfair towards specific groups ofpeople. Since large-scale retraining of these models from scratch is both timeand compute-expensive, a variety of approaches have been previously proposedthat de-bias a pre-trained model. While the majority of currentstate-of-the-art debiasing methods focus on changes to the training regime, inthis paper, we propose data intervention strategies as a powerful yet simpletechnique to reduce gender bias in pre-trained models. Specifically, weempirically show that by fine-tuning a pre-trained model on only 10 de-biased(intervened) training examples, the tendency to favor any gender issignificantly reduced. Since our proposed method only needs a few trainingexamples, our few-shot debiasing approach is highly feasible and practical.Through extensive experimentation, we show that our debiasing techniqueperforms better than competitive state-of-the-art baselines with minimal lossin language modeling ability.</description><author>Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency</author><pubDate>Wed, 07 Jun 2023 17:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04597v1</guid></item><item><title>Generalization Across Observation Shifts in Reinforcement Learning</title><link>http://arxiv.org/abs/2306.04595v1</link><description>Learning policies which are robust to changes in the environment are criticalfor real world deployment of Reinforcement Learning agents. They are alsonecessary for achieving good generalization across environment shifts. We focuson bisimulation metrics, which provide a powerful means for abstracting taskrelevant components of the observation and learning a succinct representationspace for training the agent using reinforcement learning. In this work, weextend the bisimulation framework to also account for context dependentobservation shifts. Specifically, we focus on the simulator based learningsetting and use alternate observations to learn a representation space which isinvariant to observation shifts using a novel bisimulation based objective.This allows us to deploy the agent to varying observation settings during testtime and generalize to unseen scenarios. We further provide novel theoreticalbounds for simulator fidelity and performance transfer guarantees for using alearnt policy to unseen shifts. Empirical analysis on the high-dimensionalimage based control domains demonstrates the efficacy of our method.</description><author>Anuj Mahajan, Amy Zhang</author><pubDate>Wed, 07 Jun 2023 17:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04595v1</guid></item><item><title>MarineVRS: Marine Video Retrieval System with Explainability via Semantic Understanding</title><link>http://arxiv.org/abs/2306.04593v1</link><description>Building a video retrieval system that is robust and reliable, especially forthe marine environment, is a challenging task due to several factors such asdealing with massive amounts of dense and repetitive data, occlusion,blurriness, low lighting conditions, and abstract queries. To address thesechallenges, we present MarineVRS, a novel and flexible video retrieval systemdesigned explicitly for the marine domain. MarineVRS integratesstate-of-the-art methods for visual and linguistic object representation toenable efficient and accurate search and analysis of vast volumes of underwatervideo data. In addition, unlike the conventional video retrieval system, whichonly permits users to index a collection of images or videos and search using afree-form natural language sentence, our retrieval system includes anadditional Explainability module that outputs the segmentation masks of theobjects that the input query referred to. This feature allows users to identifyand isolate specific objects in the video footage, leading to more detailedanalysis and understanding of their behavior and movements. Finally, with itsadaptability, explainability, accuracy, and scalability, MarineVRS is apowerful tool for marine researchers and scientists to efficiently andaccurately process vast amounts of data and gain deeper insights into thebehavior and movements of marine species.</description><author>Tan-Sang Ha, Hai Nguyen-Truong, Tuan-Anh Vu, Sai-Kit Yeung</author><pubDate>Wed, 07 Jun 2023 17:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04593v1</guid></item><item><title>Proximity-Informed Calibration for Deep Neural Networks</title><link>http://arxiv.org/abs/2306.04590v1</link><description>Confidence calibration is central to providing accurate and interpretableuncertainty estimates, especially under safety-critical scenarios. However, wefind that existing calibration algorithms often overlook the issue of proximitybias, a phenomenon where models tend to be more overconfident in low proximitydata (i.e., lying in the sparse region of the data distribution) compared tohigh proximity samples, and thus suffer from inconsistent miscalibration acrossdifferent proximity samples. We examine the problem over pretrained ImageNetmodels and observe that: 1) Proximity bias exists across a wide variety ofmodel architectures and sizes; 2) Transformer-based models are more susceptibleto proximity bias than CNN-based models; 3) Proximity bias persists even afterperforming popular calibration algorithms like temperature scaling; 4) Modelstend to overfit more heavily on low proximity samples than on high proximitysamples. Motivated by the empirical findings, we propose ProCal, aplug-and-play algorithm with a theoretical guarantee to adjust sampleconfidence based on proximity. To further quantify the effectiveness ofcalibration algorithms in mitigating proximity bias, we introduceproximity-informed expected calibration error (PIECE) with theoreticalanalysis. We show that ProCal is effective in addressing proximity bias andimproving calibration on balanced, long-tail, and distribution-shift settingsunder four metrics over various model architectures.</description><author>Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, Bryan Hooi</author><pubDate>Wed, 07 Jun 2023 17:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04590v1</guid></item><item><title>Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale</title><link>http://arxiv.org/abs/2211.03759v2</link><description>Machine learning models that convert user-written text descriptions intoimages are now widely available online and used by millions of users togenerate millions of images a day. We investigate the potential for thesemodels to amplify dangerous and complex stereotypes. We find a broad range ofordinary prompts produce stereotypes, including prompts simply mentioningtraits, descriptors, occupations, or objects. For example, we find cases ofprompting for basic traits or social roles resulting in images reinforcingwhiteness as ideal, prompting for occupations resulting in amplification ofracial and gender disparities, and prompting for objects resulting inreification of American norms. Stereotypes are present regardless of whetherprompts explicitly mention identity and demographic language or avoid suchlanguage. Moreover, stereotypes persist despite mitigation strategies; neitheruser attempts to counter stereotypes by requesting images with specificcounter-stereotypes nor institutional attempts to add system ``guardrails''have prevented the perpetuation of stereotypes. Our analysis justifies concernsregarding the impacts of today's models, presenting striking exemplars, andconnecting these findings with deep insights into harms drawn from socialscientific and humanist disciplines. This work contributes to the effort toshed light on the uniquely complex biases in language-vision models anddemonstrates the ways that the mass deployment of text-to-image generationmodels results in mass dissemination of stereotypes and resulting harms.</description><author>Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, Aylin Caliskan</author><pubDate>Wed, 07 Jun 2023 17:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03759v2</guid></item><item><title>Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations</title><link>http://arxiv.org/abs/2306.04581v1</link><description>We consider the problem of learning to perform a task from demonstrationsgiven by teachers or experts, when some of the experts' demonstrations might beadversarial and demonstrate an incorrect way to perform the task. We propose anovel technique that can identify parts of demonstrated trajectories that havenot been significantly modified by the adversary and utilize them for learning,using temporally extended policies or options. We first define a trajectorydivergence measure based on the spatial and temporal features of demonstratedtrajectories to detect and discard parts of the trajectories that have beensignificantly modified by an adversarial expert, and, could degrade thelearner's performance, if used for learning, We then use an options-basedalgorithm that partitions trajectories and learns only from the parts oftrajectories that have been determined as admissible. We provide theoreticalresults of our technique to show that repairing partial trajectories improvesthe sample efficiency of the demonstrations without degrading the learner'sperformance. We then evaluate the proposed algorithm for learning to play anAtari-like, computer-based game called LunarLander in the presence of differenttypes and degrees of adversarial attacks of demonstrated trajectories. Ourexperimental results show that our technique can identify adversariallymodified parts of the demonstrated trajectories and successfully prevent thelearning performance from degrading due to adversarial demonstrations.</description><author>Prithviraj Dasgupta</author><pubDate>Wed, 07 Jun 2023 17:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04581v1</guid></item><item><title>A Dataset for Deep Learning-based Bone Structure Analyses in Total Hip Arthroplasty</title><link>http://arxiv.org/abs/2306.04579v1</link><description>Total hip arthroplasty (THA) is a widely used surgical procedure inorthopedics. For THA, it is of clinical significance to analyze the bonestructure from the CT images, especially to observe the structure of theacetabulum and femoral head, before the surgical procedure. For such bonestructure analyses, deep learning technologies are promising but requirehigh-quality labeled data for the learning, while the data labeling is costly.We address this issue and propose an efficient data annotation pipeline forproducing a deep learning-oriented dataset. Our pipeline consists ofnon-learning-based bone extraction (BE) and acetabulum and femoral headsegmentation (AFS) and active-learning-based annotation refinement (AAR). ForBE we use the classic graph-cut algorithm. For AFS we propose an improvedalgorithm, including femoral head boundary localization using first-order andsecond-order gradient regularization, line-based non-maximum suppression, andanatomy prior-based femoral head extraction. For AAR, we refine thealgorithm-produced pseudo labels with the help of trained deep models: wemeasure the uncertainty based on the disagreement between the original pseudolabels and the deep model predictions, and then find out the samples with thelargest uncertainty to ask for manual labeling. Using the proposed pipeline, weconstruct a large-scale bone structure analyses dataset from more than 300clinical and diverse CT scans. We perform careful manual labeling for the testset of our data. We then benchmark multiple state-of-the art deeplearning-based methods of medical image segmentation using the training andtest sets of our data. The extensive experimental results validate the efficacyof the proposed data annotation pipeline. The dataset, related codes and modelswill be publicly available at https://github.com/hitachinsk/THA.</description><author>Kaidong Zhang, Ziyang Gan, Dong Liu, Xifu Shang</author><pubDate>Wed, 07 Jun 2023 17:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04579v1</guid></item><item><title>Geo-Tiles for Semantic Segmentation of Earth Observation Imagery</title><link>http://arxiv.org/abs/2306.00823v2</link><description>To cope with the high requirements during the computation of semanticsegmentations of earth observation imagery, current state-of-the-art pipelinesdivide the corresponding data into smaller images. Existing methods andbenchmark datasets oftentimes rely on pixel-based tiling schemes or ongeo-tiling schemes employed by web mapping applications. The selection ofsubimages (comprising size, location and orientation) is crucial. It affectsthe available context information of each pixel, defines the number of tilesduring training, and influences the degree of information degradation whiledown- and up-sampling the tile contents to the size required by thesegmentation model. We propose a new segmentation pipeline for earthobservation imagery relying on a tiling scheme that creates geo-tiles based onthe geo-information of the raster data. This approach exhibits severalbeneficial properties compared to pixel-based or common web mapping approaches.The proposed tiling scheme shows flexible customization properties regardingtile granularity, tile stride and image boundary alignment. This allows us toperform a tile specific data augmentation during training and a substitution ofpixel predictions with limited context information using data of overlappingtiles during inference. The generated tiles show a consistent spatial tileextent w.r.t. heterogeneous sensors, varying recording distances and differentlatitudes. We demonstrate how the proposed tiling system allows to improve theresults of current state-of-the-art semantic segmentation models. To fosterfuture research we make the source code publicly available.</description><author>Sebastian Bullinger, Florian Fervers, Christoph Bodensteiner, Michael Arens</author><pubDate>Wed, 07 Jun 2023 17:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00823v2</guid></item><item><title>Gender, names and other mysteries: Towards the ambiguous for gender-inclusive translation</title><link>http://arxiv.org/abs/2306.04573v1</link><description>The vast majority of work on gender in MT focuses on 'unambiguous' inputs,where gender markers in the source language are expected to be resolved in theoutput. Conversely, this paper explores the widespread case where the sourcesentence lacks explicit gender markers, but the target sentence contains themdue to richer grammatical gender. We particularly focus on inputs containingperson names. Investigating such sentence pairs casts a new light on research into MTgender bias and its mitigation. We find that many name-gender co-occurrences inMT data are not resolvable with 'unambiguous gender' in the source language,and that gender-ambiguous examples can make up a large proportion of trainingexamples. From this, we discuss potential steps toward gender-inclusivetranslation which accepts the ambiguity in both gender and translation.</description><author>Danielle Saunders, Katrina Olsen</author><pubDate>Wed, 07 Jun 2023 17:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04573v1</guid></item><item><title>Recent applications of machine learning, remote sensing, and iot approaches in yield prediction: a critical review</title><link>http://arxiv.org/abs/2306.04566v1</link><description>The integration of remote sensing and machine learning in agriculture istransforming the industry by providing insights and predictions through dataanalysis. This combination leads to improved yield prediction and watermanagement, resulting in increased efficiency, better yields, and moresustainable agricultural practices. Achieving the United Nations' SustainableDevelopment Goals, especially "zero hunger," requires the investigation of cropyield and precipitation gaps, which can be accomplished through, the usage ofartificial intelligence (AI), machine learning (ML), remote sensing (RS), andthe internet of things (IoT). By integrating these technologies, a robustagricultural mobile or web application can be developed, providing farmers anddecision-makers with valuable information and tools for improving cropmanagement and increasing efficiency. Several studies have investigated thesenew technologies and their potential for diverse tasks such as crop monitoring,yield prediction, irrigation management, etc. Through a critical review, thispaper reviews relevant articles that have used RS, ML, cloud computing, and IoTin crop yield prediction. It reviews the current state-of-the-art in this fieldby critically evaluating different machine-learning approaches proposed in theliterature for crop yield prediction and water management. It provides insightsinto how these methods can improve decision-making in agricultural productionsystems. This work will serve as a compendium for those interested in yieldprediction in terms of primary literature but, most importantly, whatapproaches can be used for real-time and robust prediction.</description><author>Fatima Zahra Bassine, Terence Epule Epule, Ayoub Kechchour, Abdelghani Chehbouni</author><pubDate>Wed, 07 Jun 2023 17:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04566v1</guid></item><item><title>Gradient boosting for convex cone predict and optimize problems</title><link>http://arxiv.org/abs/2204.06895v2</link><description>Prediction models are typically optimized independently from decisionoptimization. A smart predict then optimize (SPO) framework optimizesprediction models to minimize downstream decision regret. In this paper wepresent dboost, the first general purpose implementation of smart gradientboosting for `predict, then optimize' problems. The framework supports convexquadratic cone programming and gradient boosting is performed by implicitdifferentiation of a custom fixed-point mapping. Experiments comparing withstate-of-the-art SPO methods show that dboost can further reduce out-of-sampledecision regret.</description><author>Andrew Butler, Roy H. Kwon</author><pubDate>Wed, 07 Jun 2023 17:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06895v2</guid></item><item><title>GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction</title><link>http://arxiv.org/abs/2306.01951v3</link><description>Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodeswithin graphs, finding applications in network security, fraud detection,social media spam detection, and various other domains. A common method for GADis Graph Auto-Encoders (GAEs), which encode graph data into noderepresentations and identify anomalies by assessing the reconstruction qualityof the graphs based on these representations. However, existing GAE models areprimarily optimized for direct link reconstruction, resulting in nodesconnected in the graph being clustered in the latent space. As a result, theyexcel at detecting cluster-type structural anomalies but struggle with morecomplex structural anomalies that do not conform to clusters. To address thislimitation, we propose a novel solution called GAD-NR, a new variant of GAEthat incorporates neighborhood reconstruction for graph anomaly detection.GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing thelocal structure, self-attributes, and neighbor attributes, based on thecorresponding node representation. By comparing the neighborhood reconstructionloss between anomalous nodes and normal nodes, GAD-NR can effectively detectany anomalies. Extensive experimentation conducted on six real-world datasetsvalidates the effectiveness of GAD-NR, showcasing significant improvements (byup to 30% in AUC) over state-of-the-art competitors. The source code for GAD-NRis openly available. Importantly, the comparative analysis reveals that theexisting methods perform well only in detecting one or two types of anomaliesout of the three types studied. In contrast, GAD-NR excels at detecting allthree types of anomalies across the datasets, demonstrating its comprehensiveanomaly detection capabilities.</description><author>Amit Roy, Juan Shu, Jia Li, Carl Yang, Olivier Elshocht, Jeroen Smeets, Pan Li</author><pubDate>Wed, 07 Jun 2023 17:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01951v3</guid></item><item><title>ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models</title><link>http://arxiv.org/abs/2306.04563v1</link><description>Humor is a central aspect of human communication that has not been solved forartificial agents so far. Large language models (LLMs) are increasingly able tocapture implicit and contextual information. Especially, OpenAI's ChatGPTrecently gained immense public attention. The GPT3-based model almost seems tocommunicate on a human level and can even tell jokes. Humor is an essentialcomponent of human communication. But is ChatGPT really funny? We put ChatGPT'ssense of humor to the test. In a series of exploratory experiments aroundjokes, i.e., generation, explanation, and detection, we seek to understandChatGPT's capability to grasp and reproduce human humor. Since the model itselfis not accessible, we applied prompt-based experiments. Our empirical evidenceindicates that jokes are not hard-coded but mostly also not newly generated bythe model. Over 90% of 1008 generated jokes were the same 25 Jokes. The systemaccurately explains valid jokes but also comes up with fictional explanationsfor invalid jokes. Joke-typical characteristics can mislead ChatGPT in theclassification of jokes. ChatGPT has not solved computational humor yet but itcan be a big leap toward "funny" machines.</description><author>Sophie Jentzsch, Kristian Kersting</author><pubDate>Wed, 07 Jun 2023 17:10:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04563v1</guid></item><item><title>Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement</title><link>http://arxiv.org/abs/2211.13585v2</link><description>Optimizing user engagement is a key goal for modern recommendation systems,but blindly pushing users towards increased consumption risks burn-out, churn,or even addictive habits. To promote digital well-being, most platforms nowoffer a service that periodically prompts users to take breaks. These, however,must be set up manually, and so may be suboptimal for both users and thesystem. In this paper, we study the role of breaks in recommendation, andpropose a framework for learning optimal breaking policies that promote andsustain long-term engagement. Based on the notion that recommendation dynamicsare susceptible to both positive and negative feedback, we cast recommendationas a Lotka-Volterra dynamical system, where breaking reduces to a problem ofoptimal control. We then give an efficient learning algorithm, providetheoretical guarantees, and empirically demonstrate the utility of our approachon semi-synthetic data.</description><author>Eden Saig, Nir Rosenfeld</author><pubDate>Wed, 07 Jun 2023 17:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13585v2</guid></item><item><title>Self-supervised learning-based cervical cytology for the triage of HPV-positive women in resource-limited settings and low-data regime</title><link>http://arxiv.org/abs/2302.05195v2</link><description>Screening Papanicolaou test samples has proven to be highly effective inreducing cervical cancer-related mortality. However, the lack of trainedcytopathologists hinders its widespread implementation in low-resourcesettings. Deep learning-based telecytology diagnosis emerges as an appealingalternative, but it requires the collection of large annotated trainingdatasets, which is costly and time-consuming. In this paper, we demonstratethat the abundance of unlabeled images that can be extracted from Pap smeartest whole slide images presents a fertile ground for self-supervised learningmethods, yielding performance improvements relative to readily availablepre-trained models for various downstream tasks. In particular, we propose\textbf{C}ervical \textbf{C}ell \textbf{C}opy-\textbf{P}asting($\texttt{C}^{3}\texttt{P}$) as an effective augmentation method, which enablesknowledge transfer from open-source and labeled single-cell datasets tounlabeled tiles. Not only does $\texttt{C}^{3}\texttt{P}$ outperforms naivetransfer from single-cell images, but we also demonstrate its advantageousintegration into multiple instance learning methods. Importantly, all ourexperiments are conducted on our introduced \textit{in-house} datasetcomprising liquid-based cytology Pap smear images obtained using low-costtechnologies. This aligns with our objective of leveraging deep learning-basedtelecytology for diagnosis in low-resource settings.</description><author>Thomas Stegm√ºller, Christian Abbet, Behzad Bozorgtabar, Holly Clarke, Patrick Petignat, Pierre Vassilakos, Jean-Philippe Thiran</author><pubDate>Wed, 07 Jun 2023 17:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05195v2</guid></item><item><title>SpokenWOZ: A Large-Scale Speech-Text Dataset for Spoken Task-Oriented Dialogue in Multiple Domains</title><link>http://arxiv.org/abs/2305.13040v2</link><description>Task-oriented dialogue (TOD) models have made significant progress in recentyears. However, previous studies primarily focus on datasets written byannotators, which has resulted in a gap between academic research andreal-world spoken conversation scenarios. While several small-scale spoken TODdatasets are proposed to address robustness issues such as ASR errors, theyignore the unique challenges in spoken conversation. To tackle the limitations,we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios fromhuman-to-human spoken conversations. SpokenWOZ further incorporates commonspoken characteristics such as word-by-word processing and reasoning in spokenlanguage. Based on these characteristics, we present cross-turn slot andreasoning slot detection as new challenges. We conduct experiments on variousbaselines, including text-modal models, newly proposed dual-modal models, andLLMs, e.g., ChatGPT. The results show that the current models still havesubstantial room for improvement in spoken conversation, where the mostadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy andthe SOTA end-to-end model only correctly completes the user request in 52.1% ofdialogues. The dataset, code, and leaderboard are available:https://spokenwoz.github.io/SpokenWOZ-github.io/.</description><author>Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li</author><pubDate>Wed, 07 Jun 2023 17:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13040v2</guid></item><item><title>Efficient Video Action Detection with Token Dropout and Context Refinement</title><link>http://arxiv.org/abs/2304.08451v2</link><description>Streaming video clips with large-scale video tokens impede visiontransformers (ViTs) for efficient recognition, especially in video actiondetection where sufficient spatiotemporal representations are required forprecise actor identification. In this work, we propose an end-to-end frameworkfor efficient video action detection (EVAD) based on vanilla ViTs. Our EVADconsists of two specialized designs for video action detection. First, wepropose a spatiotemporal token dropout from a keyframe-centric perspective. Ina video clip, we maintain all tokens from its keyframe, preserve tokensrelevant to actor motions from other frames, and drop out the remaining tokensin this clip. Second, we refine scene context by leveraging remaining tokensfor better recognizing actor identities. The region of interest (RoI) in ouraction detector is expanded into temporal domain. The captured spatiotemporalactor identity representations are refined via scene context in a decoder withthe attention mechanism. These two designs make our EVAD efficient whilemaintaining accuracy, which is validated on three benchmark datasets (i.e.,AVA, UCF101-24, JHMDB). Compared to the vanilla ViT backbone, our EVAD reducesthe overall GFLOPs by 43% and improves real-time inference speed by 40% with noperformance degradation. Moreover, even at similar computational costs, ourEVAD can improve the performance by 1.1 mAP with higher resolution inputs. Codeis available at https://github.com/MCG-NJU/EVAD.</description><author>Lei Chen, Zhan Tong, Yibing Song, Gangshan Wu, Limin Wang</author><pubDate>Wed, 07 Jun 2023 17:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08451v2</guid></item><item><title>PhenoBench -- A Large Dataset and Benchmarks for Semantic Image Interpretation in the Agricultural Domain</title><link>http://arxiv.org/abs/2306.04557v1</link><description>The production of food, feed, fiber, and fuel is a key task of agriculture.Especially crop production has to cope with a multitude of challenges in theupcoming decades caused by a growing world population, climate change, the needfor sustainable production, lack of skilled workers, and generally the limitedavailability of arable land. Vision systems could help cope with thesechallenges by offering tools to make better and more sustainable fieldmanagement decisions and support the breeding of new varieties of crops byallowing temporally dense and reproducible measurements. Recently, tacklingperception tasks in the agricultural domain got increasing interest in thecomputer vision and robotics community since agricultural robotics are onepromising solution for coping with the lack of workers and enable a moresustainable agricultural production at the same time. While large datasets andbenchmarks in other domains are readily available and have enabled significantprogress toward more reliable vision systems, agricultural datasets andbenchmarks are comparably rare. In this paper, we present a large dataset andbenchmarks for the semantic interpretation of images of real agriculturalfields. Our dataset recorded with a UAV provides high-quality, denseannotations of crops and weeds, but also fine-grained labels of crop leaves atthe same time, which enable the development of novel algorithms for visualperception in the agricultural domain. Together with the labeled data, weprovide novel benchmarks for evaluating different visual perception tasks on ahidden test set comprised of different fields: known fields covered by thetraining data and a completely unseen field. The tasks cover semanticsegmentation, panoptic segmentation of plants, leaf instance segmentation,detection of plants and leaves, and hierarchical panoptic segmentation forjointly identifying plants and leaves.</description><author>Jan Weyler, Federico Magistri, Elias Marks, Yue Linn Chong, Matteo Sodano, Gianmarco Roggiolani, Nived Chebrolu, Cyrill Stachniss, Jens Behley</author><pubDate>Wed, 07 Jun 2023 17:04:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04557v1</guid></item><item><title>StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code</title><link>http://arxiv.org/abs/2306.04556v1</link><description>Code LLMs are being rapidly deployed and there is evidence that they can makeprofessional programmers more productive. Current benchmarks for codegeneration measure whether models generate correct programs given an expertprompt. In this paper, we present a new benchmark containing multiple promptsper problem, written by a specific population of non-expert prompters:beginning programmers. StudentEval contains 1,749 prompts for 48 problems,written by 80 students who have only completed one semester of Pythonprogramming. Our students wrote these prompts while working interactively witha Code LLM, and we observed very mixed success rates. We use StudentEval toevaluate 5 Code LLMs and find that StudentEval is a better discriminator ofmodel performance than existing benchmarks. We analyze the prompts and findsignificant variation in students' prompting techniques. We also find thatnondeterministic LLM sampling could mislead students into thinking that theirprompts are more (or less) effective than they actually are, which hasimplications for how to teach with Code LLMs.</description><author>Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, Carolyn Jane Anderson</author><pubDate>Wed, 07 Jun 2023 17:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04556v1</guid></item><item><title>Meta-learning Control Variates: Variance Reduction with Limited Data</title><link>http://arxiv.org/abs/2303.04756v3</link><description>Control variates can be a powerful tool to reduce the variance of Monte Carloestimators, but constructing effective control variates can be challenging whenthe number of samples is small. In this paper, we show that when a large numberof related integrals need to be computed, it is possible to leverage thesimilarity between these integration tasks to improve performance even when thenumber of samples per task is very small. Our approach, called meta learningCVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Ourempirical assessment indicates that Meta-CVs can lead to significant variancereduction in such settings, and our theoretical analysis establishes generalconditions under which Meta-CVs can be successfully trained.</description><author>Zhuo Sun, Chris J. Oates, Fran√ßois-Xavier Briol</author><pubDate>Wed, 07 Jun 2023 16:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04756v3</guid></item><item><title>Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning</title><link>http://arxiv.org/abs/2306.04551v1</link><description>Generative artificial intelligence (AI) is a promising direction foraugmenting clinical diagnostic decision support and reducing diagnostic errors,a leading contributor to medical errors. To further the development of clinicalAI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as acomprehensive generative AI framework, comprised of six tasks representing keycomponents in clinical reasoning. We present a comparative analysis ofin-domain versus out-of-domain language models as well as multi-task versussingle task training with a focus on the problem summarization task in DR.BENCH(Gao et al., 2023). We demonstrate that a multi-task, clinically trainedlanguage model outperforms its general domain counterpart by a large margin,establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.This research underscores the value of domain-specific training for optimizingclinical diagnostic reasoning tasks.</description><author>Brihat Sharma, Yanjun Gao, Timothy Miller, Matthew M. Churpek, Majid Afshar, Dmitriy Dligach</author><pubDate>Wed, 07 Jun 2023 16:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04551v1</guid></item><item><title>Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction</title><link>http://arxiv.org/abs/2301.08951v3</link><description>When perceiving the world from multiple viewpoints, humans have the abilityto reason about the complete objects in a compositional manner even when anobject is completely occluded from certain viewpoints. Meanwhile, humans areable to imagine novel views after observing multiple viewpoints. Recentremarkable advances in multi-view object-centric learning still leaves someunresolved problems: 1) The shapes of partially or completely occluded objectscan not be well reconstructed. 2) The novel viewpoint prediction depends onexpensive viewpoint annotations rather than implicit rules in viewrepresentations. In this paper, we introduce a time-conditioned generativemodel for videos. To reconstruct the complete shape of an object accurately, weenhance the disentanglement between the latent representations of objects andviews, where the latent representations of time-conditioned views are jointlyinferred with a Transformer and then are input to a sequential extension ofSlot Attention to learn object-centric representations. In addition, Gaussianprocesses are employed as priors of view latent variables for video generationand novel-view prediction without viewpoint annotations. Experiments onmultiple datasets demonstrate that the proposed model can make object-centricvideo decomposition, reconstruct the complete shapes of occluded objects, andmake novel-view predictions.</description><author>Chengmin Gao, Bin Li</author><pubDate>Wed, 07 Jun 2023 16:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08951v3</guid></item><item><title>Convergence of SARSA with linear function approximation: The random horizon case</title><link>http://arxiv.org/abs/2306.04548v1</link><description>The reinforcement learning algorithm SARSA combined with linear functionapproximation has been shown to converge for infinite horizon discounted Markovdecision problems (MDPs). In this paper, we investigate the convergence of thealgorithm for random horizon MDPs, which has not previously been shown. Weshow, similar to earlier results for infinite horizon discounted MDPs, that ifthe behaviour policy is $\varepsilon$-soft and Lipschitz continuous withrespect to the weight vector of the linear function approximation, with smallenough Lipschitz constant, then the algorithm will converge with probabilityone when considering a random horizon MDP.</description><author>Lina Palmborg</author><pubDate>Wed, 07 Jun 2023 16:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04548v1</guid></item><item><title>Querying Circumscribed Description Logic Knowledge Bases</title><link>http://arxiv.org/abs/2306.04546v1</link><description>Circumscription is one of the main approaches for defining non-monotonicdescription logics (DLs). While the decidability and complexity of traditionalreasoning tasks such as satisfiability of circumscribed DL knowledge bases(KBs) is well understood, for evaluating conjunctive queries (CQs) and unionsthereof (UCQs), not even decidability had been established. In this paper, weprove decidability of (U)CQ evaluation on circumscribed DL KBs and obtain arather complete picture of both the combined complexity and the datacomplexity, for DLs ranging from ALCHIO via EL to various versions of DL-Lite.We also study the much simpler atomic queries (AQs).</description><author>Carsten Lutz, Quentin Mani√®re, Robin Nolte</author><pubDate>Wed, 07 Jun 2023 16:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04546v1</guid></item><item><title>Contrastive Bootstrapping for Label Refinement</title><link>http://arxiv.org/abs/2306.04544v1</link><description>Traditional text classification typically categorizes texts into pre-definedcoarse-grained classes, from which the produced models cannot handle thereal-world scenario where finer categories emerge periodically for accurateservices. In this work, we investigate the setting where fine-grainedclassification is done only using the annotation of coarse-grained categoriesand the coarse-to-fine mapping. We propose a lightweight contrastiveclustering-based bootstrapping method to iteratively refine the labels ofpassages. During clustering, it pulls away negative passage-prototype pairsunder the guidance of the mapping from both global and local perspectives.Experiments on NYT and 20News show that our method outperforms thestate-of-the-art methods by a large margin.</description><author>Shudi Hou, Yu Xia, Muhao Chen, Sujian Li</author><pubDate>Wed, 07 Jun 2023 16:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04544v1</guid></item><item><title>On the Design Fundamentals of Diffusion Models: A Survey</title><link>http://arxiv.org/abs/2306.04542v1</link><description>Diffusion models are generative models, which gradually add and remove noiseto learn the underlying distribution of training data for data generation. Thecomponents of diffusion models have gained significant attention with manydesign choices proposed. Existing reviews have primarily focused onhigher-level solutions, thereby covering less on the design fundamentals ofcomponents. This study seeks to address this gap by providing a comprehensiveand coherent review on component-wise design choices in diffusion models.Specifically, we organize this review according to their three key components,namely the forward process, the reverse process, and the sampling procedure.This allows us to provide a fine-grained perspective of diffusion models,benefiting future studies in the analysis of individual components, theapplicability of design choices, and the implementation of diffusion models.</description><author>Ziyi Chang, George A. Koulieris, Hubert P. H. Shum</author><pubDate>Wed, 07 Jun 2023 16:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04542v1</guid></item><item><title>Top-Down Knowledge Compilation for Counting Modulo Theories</title><link>http://arxiv.org/abs/2306.04541v1</link><description>Propositional model counting (#SAT) can be solved efficiently when the inputformula is in deterministic decomposable negation normal form (d-DNNF).Translating an arbitrary formula into a representation that allows inferencetasks, such as counting, to be performed efficiently, is called knowledgecompilation. Top-down knowledge compilation is a state-of-the-art technique forsolving #SAT problems that leverages the traces of exhaustive DPLL search toobtain d-DNNF representations. While knowledge compilation is well studied forpropositional approaches, knowledge compilation for the (quantifier free)counting modulo theory setting (#SMT) has been studied to a much lesser degree.In this paper, we discuss compilation strategies for #SMT. We specificallyadvocate for a top-down compiler based on the traces of exhaustive DPLL(T)search.</description><author>Vincent Derkinderen, Pedro Zuidberg Dos Martires, Samuel Kolb, Paolo Morettin</author><pubDate>Wed, 07 Jun 2023 16:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04541v1</guid></item><item><title>NeMO: Neural Map Growing System for Spatiotemporal Fusion in Bird's-Eye-View and BDD-Map Benchmark</title><link>http://arxiv.org/abs/2306.04540v1</link><description>Vision-centric Bird's-Eye View (BEV) representation is essential forautonomous driving systems (ADS). Multi-frame temporal fusion which leverageshistorical information has been demonstrated to provide more comprehensiveperception results. While most research focuses on ego-centric maps of fixedsettings, long-range local map generation remains less explored. This workoutlines a new paradigm, named NeMO, for generating local maps through theutilization of a readable and writable big map, a learning-based fusion module,and an interaction mechanism between the two. With an assumption that thefeature distribution of all BEV grids follows an identical pattern, we adopt ashared-weight neural network for all grids to update the big map. This paradigmsupports the fusion of longer time series and the generation of long-range BEVlocal maps. Furthermore, we release BDD-Map, a BDD100K-based datasetincorporating map element annotations, including lane lines, boundaries, andpedestrian crossing. Experiments on the NuScenes and BDD-Map datasetsdemonstrate that NeMO outperforms state-of-the-art map segmentation methods. Wealso provide a new scene-level BEV map evaluation setting along with thecorresponding baseline for a more comprehensive comparison.</description><author>Xi Zhu, Xiya Cao, Zhiwei Dong, Caifa Zhou, Qiangbo Liu, Wei Li, Yongliang Wang</author><pubDate>Wed, 07 Jun 2023 16:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04540v1</guid></item><item><title>A Theory of Link Prediction via Relational Weisfeiler-Leman</title><link>http://arxiv.org/abs/2302.02209v3</link><description>Graph neural networks are prominent models for representation learning overgraph-structured data. While the capabilities and limitations of these modelsare well-understood for simple graphs, our understanding remains incomplete inthe context of knowledge graphs. Our goal is to provide a systematicunderstanding of the landscape of graph neural networks for knowledge graphspertaining to the prominent task of link prediction. Our analysis entails aunifying perspective on seemingly unrelated models and unlocks a series ofother models. The expressive power of various models is characterized via acorresponding relational Weisfeiler-Leman algorithm. This analysis is extendedto provide a precise logical characterization of the class of functionscaptured by a class of graph neural networks. The theoretical findingspresented in this paper explain the benefits of some widely employed practicaldesign choices, which are validated empirically.</description><author>Xingyue Huang, Miguel Romero Orth, ƒ∞smail ƒ∞lkan Ceylan, Pablo Barcel√≥</author><pubDate>Wed, 07 Jun 2023 16:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02209v3</guid></item><item><title>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</title><link>http://arxiv.org/abs/2306.04539v1</link><description>In many machine learning systems that jointly learn from multiple modalities,a core research question is to understand the nature of multimodalinteractions: the emergence of new task-relevant information during learningfrom both modalities that was not present in either alone. We study thischallenge of interaction quantification in a semi-supervised setting with onlylabeled unimodal data and naturally co-occurring multimodal data (e.g.,unlabeled images and captions, video and corresponding audio) but when labelingthem is time-consuming. Using a precise information-theoretic definition ofinteractions, our key contributions are the derivations of lower and upperbounds to quantify the amount of multimodal interactions in thissemi-supervised setting. We propose two lower bounds based on the amount ofshared information between modalities and the disagreement between separatelytrained unimodal classifiers, and derive an upper bound through connections toapproximate algorithms for min-entropy couplings. We validate these estimatedbounds and show how they accurately track true interactions. Finally, twosemi-supervised multimodal applications are explored based on these theoreticalresults: (1) analyzing the relationship between multimodal performance andestimated interactions, and (2) self-supervised learning that embracesdisagreement between modalities beyond agreement as is typically done.</description><author>Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov</author><pubDate>Wed, 07 Jun 2023 16:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04539v1</guid></item><item><title>Long-form analogies generated by chatGPT lack human-like psycholinguistic properties</title><link>http://arxiv.org/abs/2306.04537v1</link><description>Psycholinguistic analyses provide a means of evaluating large language model(LLM) output and making systematic comparisons to human-generated text. Thesemethods can be used to characterize the psycholinguistic properties of LLMoutput and illustrate areas where LLMs fall short in comparison tohuman-generated text. In this work, we apply psycholinguistic methods toevaluate individual sentences from long-form analogies about biochemicalconcepts. We compare analogies generated by human subjects enrolled inintroductory biochemistry courses to analogies generated by chatGPT. We performa supervised classification analysis using 78 features extracted fromCoh-metrix that analyze text cohesion, language, and readability (Graesser et.al., 2004). Results illustrate high performance for classifyingstudent-generated and chatGPT-generated analogies. To evaluate which featurescontribute most to model performance, we use a hierarchical clusteringapproach. Results from this analysis illustrate several linguistic differencesbetween the two sources.</description><author>S. M. Seals, Valerie L. Shalin</author><pubDate>Wed, 07 Jun 2023 16:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04537v1</guid></item><item><title>PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts</title><link>http://arxiv.org/abs/2306.04535v1</link><description>A key component of modern conversational systems is the Dialogue StateTracker (or DST), which models a user's goals and needs. Toward building morerobust and reliable DSTs, we introduce a prompt-based learning approach toautomatically generate effective adversarial examples to probe DST models. Twokey characteristics of this approach are: (i) it only needs the output of theDST with no need for model parameters, and (ii) it can learn to generatenatural language utterances that can target any DST. Through experiments overstate-of-the-art DSTs, the proposed framework leads to the greatest reductionin accuracy and the best attack success rate while maintaining good fluency anda low perturbation ratio. We also show how much the generated adversarialexamples can bolster a DST through adversarial training. These results indicatethe strength of prompt-based attacks on DSTs and leave open avenues forcontinued refinement.</description><author>Xiangjue Dong, Yun He, Ziwei Zhu, James Caverlee</author><pubDate>Wed, 07 Jun 2023 16:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04535v1</guid></item><item><title>Long Sequence Hopfield Memory</title><link>http://arxiv.org/abs/2306.04532v1</link><description>Sequence memory is an essential attribute of natural and artificialintelligence that enables agents to encode, store, and retrieve complexsequences of stimuli and actions. Computational models of sequence memory havebeen proposed where recurrent Hopfield-like neural networks are trained withtemporally asymmetric Hebbian rules. However, these networks suffer fromlimited sequence capacity (maximal length of the stored sequence) due tointerference between the memories. Inspired by recent work on Dense AssociativeMemories, we expand the sequence capacity of these models by introducing anonlinear interaction term, enhancing separation between the patterns. Wederive novel scaling laws for sequence capacity with respect to network size,significantly outperforming existing scaling laws for models based ontraditional Hopfield networks, and verify these theoretical results withnumerical simulation. Moreover, we introduce a generalized pseudoinverse ruleto recall sequences of highly correlated patterns. Finally, we extend thismodel to store sequences with variable timing between states' transitions anddescribe a biologically-plausible implementation, with connections to motorneuroscience.</description><author>Hamza Tahir Chaudhry, Jacob A. Zavatone-Veth, Dmitry Krotov, Cengiz Pehlevan</author><pubDate>Wed, 07 Jun 2023 16:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04532v1</guid></item><item><title>Lenient Evaluation of Japanese Speech Recognition: Modeling Naturally Occurring Spelling Inconsistency</title><link>http://arxiv.org/abs/2306.04530v1</link><description>Word error rate (WER) and character error rate (CER) are standard metrics inSpeech Recognition (ASR), but one problem has always been alternativespellings: If one's system transcribes adviser whereas the ground truth hasadvisor, this will count as an error even though the two spellings reallyrepresent the same word. Japanese is notorious for ``lacking orthography'': most words can be spelledin multiple ways, presenting a problem for accurate ASR evaluation. In thispaper we propose a new lenient evaluation metric as a more defensible CERmeasure for Japanese ASR. We create a lattice of plausible respellings of thereference transcription, using a combination of lexical resources, a Japanesetext-processing system, and a neural machine translation model forreconstructing kanji from hiragana or katakana. In a manual evaluation, ratersrated 95.4% of the proposed spelling variants as plausible. ASR results showthat our method, which does not penalize the system for choosing a validalternate spelling of a word, affords a 2.4%-3.1% absolute reduction in CERdepending on the task.</description><author>Shigeki Karita, Richard Sproat, Haruko Ishikawa</author><pubDate>Wed, 07 Jun 2023 16:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04530v1</guid></item><item><title>Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models</title><link>http://arxiv.org/abs/2306.04529v1</link><description>Currently, most machine learning models are trained by centralized teams andare rarely updated. In contrast, open-source software development involves theiterative development of a shared artifact through distributed collaborationusing a version control system. In the interest of enabling collaborative andcontinual improvement of machine learning models, we introduce Git-Theta, aversion control system for machine learning models. Git-Theta is an extensionto Git, the most widely used version control software, that allows fine-grainedtracking of changes to model parameters alongside code and other artifacts.Unlike existing version control systems that treat a model checkpoint as a blobof data, Git-Theta leverages the structure of checkpoints to supportcommunication-efficient updates, automatic model merges, and meaningfulreporting about the difference between two versions of a model. In addition,Git-Theta includes a plug-in system that enables users to easily add supportfor new functionality. In this paper, we introduce Git-Theta's design andfeatures and include an example use-case of Git-Theta where a pre-trained modelis continually adapted and modified. We publicly release Git-Theta in hopes ofkickstarting a new era of collaborative model development.</description><author>Nikhil Kandpal, Brian Lester, Mohammed Muqeeth, Anisha Mascarenhas, Monty Evans, Vishal Baskaran, Tenghao Huang, Haokun Liu, Colin Raffel</author><pubDate>Wed, 07 Jun 2023 16:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04529v1</guid></item><item><title>PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</title><link>http://arxiv.org/abs/2306.04528v1</link><description>The increasing reliance on Large Language Models (LLMs) across academia andindustry necessitates a comprehensive understanding of their robustness toprompts. In response to this vital need, we introduce PromptBench, a robustnessbenchmark designed to measure LLMs' resilience to adversarial prompts. Thisstudy uses a plethora of adversarial textual attacks targeting prompts acrossmultiple levels: character, word, sentence, and semantic. These prompts arethen employed in diverse tasks, such as sentiment analysis, natural languageinference, reading comprehension, machine translation, and mathproblem-solving. Our study generates 4,032 adversarial prompts, meticulouslyevaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Ourfindings demonstrate that contemporary LLMs are vulnerable to adversarialprompts. Furthermore, we present comprehensive analysis to understand themystery behind prompt robustness and its transferability. We then offerinsightful robustness analysis and pragmatic recommendations for promptcomposition, beneficial to both researchers and everyday users. We make ourcode, prompts, and methodologies to generate adversarial prompts publiclyaccessible, thereby enabling and encouraging collaborative exploration in thispivotal field: https://github.com/microsoft/promptbench.</description><author>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie</author><pubDate>Wed, 07 Jun 2023 16:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04528v1</guid></item><item><title>ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis</title><link>http://arxiv.org/abs/2306.04527v1</link><description>Domain generalization is critical for real-world applications of machinelearning models to microscopy images, including histopathology and fluorescenceimaging. Artifacts in histopathology arise through a complex combination offactors relating to tissue collection and laboratory processing, as well asfactors intrinsic to patient samples. In fluorescence imaging, these artifactsstem from variations across experimental batches. The complexity and subtletyof these artifacts make the enumeration of data domains intractable. Therefore,augmentation-based methods of domain generalization that require domainidentifiers and manual fine-tuning are inadequate in this setting. To overcomethis challenge, we introduce ContriMix, a domain generalization technique thatlearns to generate synthetic images by disentangling and permuting thebiological content ("content") and technical variations ("attributes") inmicroscopy images. ContriMix does not rely on domain identifiers or handcraftedaugmentations and makes no assumptions about the input characteristics ofimages. We assess the performance of ContriMix on two pathology datasets(Camelyon17-WILDS and a prostate cell classification dataset) and onefluorescence microscopy dataset (RxRx1-WILDS). ContriMix outperforms currentstate-of-the-art methods in all datasets, motivating its usage for microscopyimage analysis in real-world settings where domain information is hard to comeby.</description><author>Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah, Chintan Shah, Sai Chowdary Gullapally, Michael Griffin, Anand Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner</author><pubDate>Wed, 07 Jun 2023 16:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04527v1</guid></item><item><title>Extrapolative Controlled Sequence Generation via Iterative Refinement</title><link>http://arxiv.org/abs/2303.04562v3</link><description>We study the problem of extrapolative controlled generation, i.e., generatingsequences with attribute values beyond the range seen in training. This task isof significant importance in automated design, especially drug discovery, wherethe goal is to design novel proteins that are \textit{better} (e.g., morestable) than existing sequences. Thus, by definition, the target sequences andtheir attribute values are out of the training distribution, posing challengesto existing methods that aim to directly generate the target sequence. Instead,in this work, we propose Iterative Controlled Extrapolation (ICE) whichiteratively makes local edits to a sequence to enable extrapolation. We trainthe model on synthetically generated sequence pairs that demonstrate smallimprovement in the attribute value. Results on one natural language task(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAVfitness) show that ICE considerably outperforms state-of-the-art approachesdespite its simplicity. Our code and models are available at:https://github.com/vishakhpk/iter-extrapolation.</description><author>Vishakh Padmakumar, Richard Yuanzhe Pang, He He, Ankur P. Parikh</author><pubDate>Wed, 07 Jun 2023 16:34:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04562v3</guid></item><item><title>Analysing the Robustness of NSGA-II under Noise</title><link>http://arxiv.org/abs/2306.04525v1</link><description>Runtime analysis has produced many results on the efficiency of simpleevolutionary algorithms like the (1+1) EA, and its analogue called GSEMO inevolutionary multiobjective optimisation (EMO). Recently, the first runtimeanalyses of the famous and highly cited EMO algorithm NSGA-II have emerged,demonstrating that practical algorithms with thousands of applications can berigorously analysed. However, these results only show that NSGA-II has the sameperformance guarantees as GSEMO and it is unclear how and when NSGA-II canoutperform GSEMO. We study this question in noisy optimisation and consider anoise model that adds large amounts of posterior noise to all objectives withsome constant probability $p$ per evaluation. We show that GSEMO fails badly onevery noisy fitness function as it tends to remove large parts of thepopulation indiscriminately. In contrast, NSGA-II is able to handle the noiseefficiently on \textsc{LeadingOnesTrailingZeroes} when $p&lt;1/2$, as thealgorithm is able to preserve useful search points even in the presence ofnoise. We identify a phase transition at $p=1/2$ where the expected time tocover the Pareto front changes from polynomial to exponential. To ourknowledge, this is the first proof that NSGA-II can outperform GSEMO and thefirst runtime analysis of NSGA-II in noisy optimisation.</description><author>Duc-Cuong Dang, Andre Opris, Bahare Salehi, Dirk Sudholt</author><pubDate>Wed, 07 Jun 2023 16:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04525v1</guid></item><item><title>Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs</title><link>http://arxiv.org/abs/2306.04523v1</link><description>Compared to English, German word order is freer and therefore posesadditional challenges for natural language inference (NLI). We create WOGLI(Word Order in German Language Inference), the first adversarial NLI datasetfor German word order that has the following properties: (i) each premise hasan entailed and a non-entailed hypothesis; (ii) premise and hypotheses differonly in word order and necessary morphological changes to mark case and number.In particular, each premise andits two hypotheses contain exactly the samelemmata. Our adversarial examples require the model to use morphologicalmarkers in order to recognise or reject entailment. We show that current Germanautoencoding models fine-tuned on translated NLI data can struggle on thischallenge set, reflecting the fact that translated NLI datasets will not mirrorall necessary language phenomena in the target language. We also examineperformance after data augmentation as well as on related word order phenomenaderived from WOGLI. Our datasets are publically available athttps://github.com/ireinig/wogli.</description><author>Ines Reinig, Katja Markert</author><pubDate>Wed, 07 Jun 2023 16:33:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04523v1</guid></item><item><title>Estimating Koopman operators with sketching to provably learn large scale dynamical systems</title><link>http://arxiv.org/abs/2306.04520v1</link><description>The theory of Koopman operators allows to deploy non-parametric machinelearning algorithms to predict and analyze complex dynamical systems.Estimators such as principal component regression (PCR) or reduced rankregression (RRR) in kernel spaces can be shown to provably learn Koopmanoperators from finite empirical observations of the system's time evolution.Scaling these approaches to very long trajectories is a challenge and requiresintroducing suitable approximations to make computations feasible. In thispaper, we boost the efficiency of different kernel-based Koopman operatorestimators using random projections (sketching). We derive, implement and testthe new "sketched" estimators with extensive experiments on synthetic andlarge-scale molecular dynamics datasets. Further, we establish non asymptoticerror bounds giving a sharp characterization of the trade-offs betweenstatistical learning rates and computational efficiency. Our empirical andtheoretical analysis shows that the proposed estimators provide a sound andefficient way to learn large scale dynamical systems. In particular ourexperiments indicate that the proposed estimators retain the same accuracy ofPCR or RRR, while being much faster.</description><author>Giacomo Meanti, Antoine Chatalic, Vladimir R. Kostic, Pietro Novelli, Massimiliano Pontil, Lorenzo Rosasco</author><pubDate>Wed, 07 Jun 2023 16:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04520v1</guid></item><item><title>Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks</title><link>http://arxiv.org/abs/2306.04519v1</link><description>Multi-task learning (MTL) can improve the generalization performance ofneural networks by sharing representations with related tasks. Nonetheless, MTLcan also degrade performance through harmful interference between tasks. Recentwork has pursued task-specific loss weighting as a solution for thisinterference. However, existing algorithms treat tasks as atomic, lacking theability to explicitly separate harmful and helpful signals beyond the tasklevel. To this end, we propose SLGrad, a sample-level weighting algorithm formulti-task learning with auxiliary tasks. Through sample-specific task weights,SLGrad reshapes the task distributions during training to eliminate harmfulauxiliary signals and augment useful task signals. Substantial generalizationperformance gains are observed on (semi-) synthetic datasets and commonsupervised multi-task problems.</description><author>Emilie Gr√©goire, Hafeez Chaudhary, Sam Verboven</author><pubDate>Wed, 07 Jun 2023 16:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04519v1</guid></item><item><title>Optimal sensor placement for reconstructing wind pressure field around buildings using compressed sensing</title><link>http://arxiv.org/abs/2306.04518v1</link><description>Deciding how to optimally deploy sensors in a large, complex, and spatiallyextended structure is critical to ensure that the surface pressure field isaccurately captured for subsequent analysis and design. In some cases,reconstruction of missing data is required in downstream tasks such as thedevelopment of digital twins. This paper presents a data-driven sparse sensorselection algorithm, aiming to provide the most information contents forreconstructing aerodynamic characteristics of wind pressures over tall buildingstructures parsimoniously. The algorithm first fits a set of basis functions tothe training data, then applies a computationally efficient QR algorithm thatranks existing pressure sensors in order of importance based on the statereconstruction to this tailored basis. The findings of this study show that theproposed algorithm successfully reconstructs the aerodynamic characteristics oftall buildings from sparse measurement locations, generating stable and optimalsolutions across a range of conditions. As a result, this study serves as apromising first step toward leveraging the success of data-driven and machinelearning algorithms to supplement traditional genetic algorithms currently usedin wind engineering.</description><author>Xihaier Luo, Ahsan Kareem, Shinjae Yoo</author><pubDate>Wed, 07 Jun 2023 16:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04518v1</guid></item><item><title>Cross-attention learning enables real-time nonuniform rotational distortion correction in OCT</title><link>http://arxiv.org/abs/2306.04512v1</link><description>Nonuniform rotational distortion (NURD) correction is vital for endoscopicoptical coherence tomography (OCT) imaging and its functional extensions, suchas angiography and elastography. Current NURD correction methods requiretime-consuming feature tracking or cross-correlation calculations and thussacrifice temporal resolution. Here we propose a cross-attention learningmethod for the NURD correction in OCT. Our method is inspired by the recentsuccess of the self-attention mechanism in natural language processing andcomputer vision. By leveraging its ability to model long-range dependencies, wecan directly obtain the correlation between OCT A-lines at any distance, thusaccelerating the NURD correction. We develop an end-to-end stackedcross-attention network and design three types of optimization constraints. Wecompare our method with two traditional feature-based methods and a CNN-basedmethod, on two publicly-available endoscopic OCT datasets and a private datasetcollected on our home-built endoscopic OCT system. Our method achieved a$\sim3\times$ speedup to real time ($26\pm 3$ fps), and superior correctionperformance.</description><author>Haoran Zhang, Jianlong Yang, Jingqian Zhang, Shiqing Zhao, Aili Zhang</author><pubDate>Wed, 07 Jun 2023 16:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04512v1</guid></item><item><title>Unified Model for Crystalline Material Generation</title><link>http://arxiv.org/abs/2306.04510v1</link><description>One of the greatest challenges facing our society is the discovery of newinnovative crystal materials with specific properties. Recently, the problem ofgenerating crystal materials has received increasing attention, however, itremains unclear to what extent, or in what way, we can develop generativemodels that consider both the periodicity and equivalence geometric of crystalstructures. To alleviate this issue, we propose two unified models that act atthe same time on crystal lattice and atomic positions using periodicequivariant architectures. Our models are capable to learn any arbitrarycrystal lattice deformation by lowering the total energy to reach thermodynamicstability. Code and data are available at https://github.com/aklipf/GemsNet.</description><author>Astrid Klipfel, Ya√´l Fr√©gier, Adlane Sayede, Zied Bouraoui</author><pubDate>Wed, 07 Jun 2023 16:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04510v1</guid></item><item><title>Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering</title><link>http://arxiv.org/abs/2306.04508v1</link><description>Whereas the recent emergence of large language models (LLMs) like ChatGPT hasexhibited impressive general performance, it still has a large gap withfully-supervised models on specific tasks such as multi-span questionanswering. Previous researches found that in-context learning is an effectiveapproach to exploiting LLM, by using a few task-related labeled data asdemonstration examples to construct a few-shot prompt for answering newquestions. A popular implementation is to concatenate a few questions and theircorrect answers through simple templates, informing LLM of the desired output.In this paper, we propose a novel way of employing labeled data such that italso informs LLM of some undesired output, by extending demonstration exampleswith feedback about answers predicted by an off-the-shelf model, e.g., correct,incorrect, or incomplete. Experiments on three multi-span question answeringdatasets as well as a keyphrase extraction dataset show that our new promptingstrategy consistently improves LLM's in-context learning performance.</description><author>Zixian Huang, Jiaying Zhou, Gengyang Xiao, Gong Cheng</author><pubDate>Wed, 07 Jun 2023 16:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04508v1</guid></item><item><title>Improving neural network representations using human similarity judgments</title><link>http://arxiv.org/abs/2306.04507v1</link><description>Deep neural networks have reached human-level performance on many computervision tasks. However, the objectives used to train these networks enforce onlythat similar images are embedded at similar locations in the representationspace, and do not directly constrain the global structure of the resultingspace. Here, we explore the impact of supervising this global structure bylinearly aligning it with human similarity judgments. We find that a naiveapproach leads to large changes in local representational structure that harmdownstream performance. Thus, we propose a novel method that aligns the globalstructure of representations while preserving their local structure. Thisglobal-local transform considerably improves accuracy across a variety offew-shot learning and anomaly detection tasks. Our results indicate that humanvisual representations are globally organized in a way that facilitateslearning from few examples, and incorporating this global structure into neuralnetwork representations improves performance on downstream tasks.</description><author>Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, Andrew K. Lampinen, Simon Kornblith</author><pubDate>Wed, 07 Jun 2023 16:17:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04507v1</guid></item><item><title>Defocus to focus: Photo-realistic bokeh rendering by fusing defocus and radiance priors</title><link>http://arxiv.org/abs/2306.04506v1</link><description>We consider the problem of realistic bokeh rendering from a singleall-in-focus image. Bokeh rendering mimics aesthetic shallow depth-of-field(DoF) in professional photography, but these visual effects generated byexisting methods suffer from simple flat background blur and blurred in-focusregions, giving rise to unrealistic rendered results. In this work, we arguethat realistic bokeh rendering should (i) model depth relations and distinguishin-focus regions, (ii) sustain sharp in-focus regions, and (iii) renderphysically accurate Circle of Confusion (CoC). To this end, we present aDefocus to Focus (D2F) framework to learn realistic bokeh rendering by fusingdefocus priors with the all-in-focus image and by implementing radiance priorsin layered fusion. Since no depth map is provided, we introduce defocushallucination to integrate depth by learning to focus. The predicted defocusmap implies the blur amount of bokeh and is used to guide weighted layeredrendering. In layered rendering, we fuse images blurred by different kernelsbased on the defocus map. To increase the reality of the bokeh, we adoptradiance virtualization to simulate scene radiance. The scene radiance used inweighted layered rendering reassigns weights in the soft disk kernel to producethe CoC. To ensure the sharpness of in-focus regions, we propose to fuseupsampled bokeh images and original images. We predict the initial fusion maskfrom our defocus map and refine the mask with a deep network. We evaluate ourmodel on a large-scale bokeh dataset. Extensive experiments show that ourapproach is capable of rendering visually pleasing bokeh effects in complexscenes. In particular, our solution receives the runner-up award in the AIM2020 Rendering Realistic Bokeh Challenge.</description><author>Xianrui Luo, Juewen Peng, Ke Xian, Zijin Wu, Zhiguo Cao</author><pubDate>Wed, 07 Jun 2023 16:15:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04506v1</guid></item><item><title>A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs</title><link>http://arxiv.org/abs/2211.02918v2</link><description>Epistemic graphs are a generalization of the epistemic approach toprobabilistic argumentation. Hunter proposed a 2-way generalization frameworkto learn epistemic constraints from crowd-sourcing data. However, the learntepistemic constraints only reflect users' beliefs from data, withoutconsidering the rationality encoded in epistemic graphs. Meanwhile, the currentframework can only generate epistemic constraints that reflect whether an agentbelieves an argument, but not the degree to which it believes in it. The majorchallenge to achieving this effect is that the computational complexity willincrease sharply when expanding the variety of constraints, which may lead tounacceptable time performance. To address these problems, we propose afiltering-based approach using a multiple-way generalization step to generate aset of rational rules which are consistent with their epistemic graphs from adataset. This approach is able to learn a wider variety of rational rules thatreflect information in both the domain model and the user model. Moreover, toimprove computational efficiency, we introduce a new function to excludemeaningless rules. The empirical results show that our approach significantlyoutperforms the existing framework when expanding the variety of rules.</description><author>Xiao Chi</author><pubDate>Wed, 07 Jun 2023 16:13:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02918v2</guid></item><item><title>Hardness of Deceptive Certificate Selection</title><link>http://arxiv.org/abs/2306.04505v1</link><description>Recent progress towards theoretical interpretability guarantees for AI hasbeen made with classifiers that are based on interactive proof systems. Aprover selects a certificate from the datapoint and sends it to a verifier whodecides the class. In the context of machine learning, such a certificate canbe a feature that is informative of the class. For a setup with high soundnessand completeness, the exchanged certificates must have a high mutualinformation with the true class of the datapoint. However, this guaranteerelies on a bound on the Asymmetric Feature Correlation of the dataset, aproperty that so far is difficult to estimate for high-dimensional data. It wasconjectured in W\"aldchen et al. that it is computationally hard to exploit theAFC, which is what we prove here. We consider a malicious prover-verifier duo that aims to exploit the AFC toachieve high completeness and soundness while using uninformative certificates.We show that this task is $\mathsf{NP}$-hard and cannot be approximated betterthan $\mathcal{O}(m^{1/8 - \epsilon})$, where $m$ is the number of possiblecertificates, for $\epsilon&gt;0$ under the Dense-vs-Random conjecture. This issome evidence that AFC should not prevent the use of interactive classificationfor real-world tasks, as it is computationally hard to be exploited.</description><author>Stephan W√§ldchen</author><pubDate>Wed, 07 Jun 2023 16:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04505v1</guid></item><item><title>Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers</title><link>http://arxiv.org/abs/2306.04504v1</link><description>ChatGPT is a large language model developed by OpenAI. Despite its impressiveperformance across various tasks, no prior work has investigated its capabilityin the biomedical domain yet. To this end, this paper aims to evaluate theperformance of ChatGPT on various benchmark biomedical tasks, such as relationextraction, document classification, question answering, and summarization. Tothe best of our knowledge, this is the first work that conducts an extensiveevaluation of ChatGPT in the biomedical domain. Interestingly, we find based onour evaluation that in biomedical datasets that have smaller training sets,zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generativetransformer models, such as BioGPT and BioBART. This suggests that ChatGPT'spre-training on large text corpora makes it quite specialized even in thebiomedical domain. Our findings demonstrate that ChatGPT has the potential tobe a valuable tool for various tasks in the biomedical domain that lack largeannotated data.</description><author>Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang</author><pubDate>Wed, 07 Jun 2023 16:11:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04504v1</guid></item><item><title>Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal</title><link>http://arxiv.org/abs/2306.04502v1</link><description>An accurate and substantial dataset is necessary to train a reliable andwell-performing model. However, even manually labeled datasets contain errors,not to mention automatically labeled ones. The problem of data denoising wasaddressed in different existing research, most of which focuses on thedetection of outliers and their permanent removal - a process that is likely toover- or underfilter the dataset. In this work, we propose AGRA: a new methodfor Adaptive GRAdient-based outlier removal. Instead of cleaning the datasetprior to model training, the dataset is adjusted during the training process.By comparing the aggregated gradient of a batch of samples and an individualexample gradient, our method dynamically decides whether a correspondingexample is helpful for the model at this point or is counter-productive andshould be left out for the current update. Extensive evaluation on severaldatasets demonstrates the AGRA effectiveness, while comprehensive resultsanalysis supports our initial hypothesis: permanent hard outlier removal is notalways what model benefits the most from.</description><author>Anastasiia Sedova, Lena Zellinger, Benjamin Roth</author><pubDate>Wed, 07 Jun 2023 16:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04502v1</guid></item><item><title>A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel</title><link>http://arxiv.org/abs/2206.12543v3</link><description>Empirical neural tangent kernels (eNTKs) can provide a good understanding ofa given network's representation: they are often far less expensive to computeand applicable more broadly than infinite width NTKs. For networks with Ooutput units (e.g. an O-class classifier), however, the eNTK on N inputs is ofsize $NO \times NO$, taking $O((NO)^2)$ memory and up to $O((NO)^3)$computation. Most existing applications have therefore used one of a handful ofapproximations yielding $N \times N$ kernel matrices, saving orders ofmagnitude of computation, but with limited to no justification. We prove thatone such approximation, which we call "sum of logits", converges to the trueeNTK at initialization for any network with a wide final "readout" layer. Ourexperiments demonstrate the quality of this approximation for various usesacross a range of settings.</description><author>Mohamad Amin Mohamadi, Wonho Bae, Danica J. Sutherland</author><pubDate>Wed, 07 Jun 2023 16:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12543v3</guid></item><item><title>On the Correspondence Between Monotonic Max-Sum GNNs and Datalog</title><link>http://arxiv.org/abs/2305.18015v2</link><description>Although there has been significant interest in applying machine learningtechniques to structured data, the expressivity (i.e., a description of whatcan be learned) of such techniques is still poorly understood. In this paper,we study data transformations based on graph neural networks (GNNs). First, wenote that the choice of how a dataset is encoded into a numeric formprocessable by a GNN can obscure the characterisation of a model'sexpressivity, and we argue that a canonical encoding provides an appropriatebasis. Second, we study the expressivity of monotonic max-sum GNNs, which covera subclass of GNNs with max and sum aggregation functions. We show that, foreach such GNN, one can compute a Datalog program such that applying the GNN toany dataset produces the same facts as a single round of application of theprogram's rules to the dataset. Monotonic max-sum GNNs can sum an unboundednumber of feature vectors which can result in arbitrarily large feature values,whereas rule application requires only a bounded number of constants. Hence,our result shows that the unbounded summation of monotonic max-sum GNNs doesnot increase their expressive power. Third, we sharpen our result to thesubclass of monotonic max GNNs, which use only the max aggregation function,and identify a corresponding class of Datalog programs.</description><author>David Tena Cucala, Bernardo Cuenca Grau, Boris Motik, Egor V. Kostylev</author><pubDate>Wed, 07 Jun 2023 16:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18015v2</guid></item><item><title>Optimal Fair Multi-Agent Bandits</title><link>http://arxiv.org/abs/2306.04498v1</link><description>In this paper, we study the problem of fair multi-agent multi-arm banditlearning when agents do not communicate with each other, except collisioninformation, provided to agents accessing the same arm simultaneously. Weprovide an algorithm with regret $O\left(N^3 \log N \log T \right)$ (assumingbounded rewards, with unknown bound). This significantly improves previousresults which had regret of order $O(\log T \log\log T)$ and exponentialdependence on the number of agents. The result is attained by using adistributed auction algorithm to learn the sample-optimal matching, a new typeof exploitation phase whose length is derived from the observed samples, and anovel order-statistics-based regret analysis. Simulation results present thedependence of the regret on $\log T$.</description><author>Amir Leshem</author><pubDate>Wed, 07 Jun 2023 16:05:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04498v1</guid></item><item><title>Limits, approximation and size transferability for GNNs on sparse graphs via graphops</title><link>http://arxiv.org/abs/2306.04495v1</link><description>Can graph neural networks generalize to graphs that are different from thegraphs they were trained on, e.g., in size? In this work, we study thisquestion from a theoretical perspective. While recent work established suchtransferability and approximation results via graph limits, e.g., via graphons,these only apply non-trivially to dense graphs. To include frequentlyencountered sparse graphs such as bounded-degree or power law graphs, we take aperspective of taking limits of operators derived from graphs, such as theaggregation operation that makes up GNNs. This leads to the recently introducedlimit notion of graphops (Backhausz and Szegedy, 2022). We demonstrate how theoperator perspective allows us to develop quantitative bounds on the distancebetween a finite GNN and its limit on an infinite graph, as well as thedistance between the GNN on graphs of different sizes that share structuralproperties, under a regularity assumption verified for various graph sequences.Our results hold for dense and sparse graphs, and various notions of graphlimits.</description><author>Thien Le, Stefanie Jegelka</author><pubDate>Wed, 07 Jun 2023 16:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04495v1</guid></item><item><title>Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches</title><link>http://arxiv.org/abs/2302.08950v3</link><description>Wake word detection exists in most intelligent homes and portable devices. Itoffers these devices the ability to "wake up" when summoned at a low cost ofpower and computing. This paper focuses on understanding alignment's role indeveloping a wake-word system that answers a generic phrase. We discuss threeapproaches. The first is alignment-based, where the model is trained withframe-wise cross-entropy. The second is alignment-free, where the model istrained with CTC. The third, proposed by us, is a hybrid solution in which themodel is trained with a small set of aligned data and then tuned with asizeable unaligned dataset. We compare the three approaches and evaluate theimpact of the different aligned-to-unaligned ratios for hybrid training. Ourresults show that the alignment-free system performs better than thealignment-based for the target operating point, and with a small fraction ofthe data (20%), we can train a model that complies with our initialconstraints.</description><author>Vinicius Ribeiro, Yiteng Huang, Yuan Shangguan, Zhaojun Yang, Li Wan, Ming Sun</author><pubDate>Wed, 07 Jun 2023 16:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08950v3</guid></item><item><title>Fair Column Subset Selection</title><link>http://arxiv.org/abs/2306.04489v1</link><description>We consider the problem of fair column subset selection. In particular, weassume that two groups are present in the data, and the chosen column subsetmust provide a good approximation for both, relative to their respective bestrank-k approximations. We show that this fair setting introduces significantchallenges: in order to extend known results, one cannot do better than thetrivial solution of simply picking twice as many columns as the originalmethods. We adopt a known approach based on deterministic leverage-scoresampling, and show that merely sampling a subset of appropriate size becomesNP-hard in the presence of two groups. Whereas finding a subset of two timesthe desired size is trivial, we provide an efficient algorithm that achievesthe same guarantees with essentially 1.5 times that size. We validate ourmethods through an extensive set of experiments on real-world data.</description><author>Antonis Matakos, Bruno Ordozgoiti, Suhas Thejaswi</author><pubDate>Wed, 07 Jun 2023 16:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04489v1</guid></item><item><title>Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</title><link>http://arxiv.org/abs/2306.04488v1</link><description>Foundation models are first pre-trained on vast unsupervised datasets andthen fine-tuned on labeled data. Reinforcement learning, notably from humanfeedback (RLHF), can further align the network with the intended usage. Yet theimperfections in the proxy reward may hinder the training and lead tosuboptimal results; the diversity of objectives in real-world tasks and humanopinions exacerbate the issue. This paper proposes embracing the heterogeneityof diverse rewards by following a multi-policy strategy. Rather than focusingon a single a priori reward, we aim for Pareto-optimal generalization acrossthe entire space of preferences. To this end, we propose rewarded soup, firstspecializing multiple networks independently (one for each proxy reward) andthen interpolating their weights linearly. This succeeds empirically because weshow that the weights remain linearly connected when fine-tuned on diverserewards from a shared pre-trained initialization. We demonstrate theeffectiveness of our approach for text-to-text (summarization, Q&amp;A, helpfulassistant, review), text-image (image captioning, text-to-image generation,visual grounding, VQA), and control (locomotion) tasks. We hope to enhance thealignment of deep models, and how they interact with the world in all itsdiversity.</description><author>Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord</author><pubDate>Wed, 07 Jun 2023 15:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04488v1</guid></item><item><title>FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features</title><link>http://arxiv.org/abs/2303.07852v2</link><description>Ultrasound imaging is one of the most prominent technologies to evaluate thegrowth, progression, and overall health of a fetus during its gestation.However, the interpretation of the data obtained from such studies is best leftto expert physicians and technicians who are trained and well-versed inanalyzing such images. To improve the clinical workflow and potentially developan at-home ultrasound-based fetal monitoring platform, we present a novel fetusphantom ultrasound dataset, FPUS23, which can be used to identify (1) thecorrect diagnostic planes for estimating fetal biometric values, (2) fetusorientation, (3) their anatomical features, and (4) bounding boxes of the fetusphantom anatomies at 23 weeks gestation. The entire dataset is composed of15,728 images, which are used to train four different Deep Neural Networkmodels, built upon a ResNet34 backbone, for detecting aforementioned fetusfeatures and use-cases. We have also evaluated the models trained using ourFPUS23 dataset, to show that the information learned by these models can beused to substantially increase the accuracy on real-world ultrasound fetusdatasets. We make the FPUS23 dataset and the pre-trained models publiclyaccessible at https://github.com/bharathprabakaran/FPUS23, which will furtherfacilitate future research on fetal ultrasound imaging and analysis.</description><author>Bharath Srinivas Prabakaran, Paul Hamelmann, Erik Ostrowski, Muhammad Shafique</author><pubDate>Wed, 07 Jun 2023 15:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07852v2</guid></item><item><title>Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models</title><link>http://arxiv.org/abs/2304.13835v2</link><description>Current dialogue research primarily studies pairwise (two-party)conversations, and does not address the everyday setting where more than twospeakers converse together. In this work, we both collect and evaluatemulti-party conversations to study this more general case. We use the LIGHTenvironment to construct grounded conversations, where each participant has anassigned character to role-play. We thus evaluate the ability of languagemodels to act as one or more characters in such conversations. Models requiretwo skills that pairwise-trained models appear to lack: (1) being able todecide when to talk; (2) producing coherent utterances grounded on multiplecharacters. We compare models trained on our new dataset to existingpairwise-trained dialogue models, as well as large language models withfew-shot prompting. We find that our new dataset, MultiLIGHT, which we willpublicly release, can help bring significant improvements in the group setting.</description><author>Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, Mojtaba Komeili</author><pubDate>Wed, 07 Jun 2023 15:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13835v2</guid></item><item><title>Artificial Intelligence can facilitate selfish decisions by altering the appearance of interaction partners</title><link>http://arxiv.org/abs/2306.04484v1</link><description>The increasing prevalence of image-altering filters on social media and videoconferencing technologies has raised concerns about the ethical andpsychological implications of using Artificial Intelligence (AI) to manipulateour perception of others. In this study, we specifically investigate thepotential impact of blur filters, a type of appearance-altering technology, onindividuals' behavior towards others. Our findings consistently demonstrate asignificant increase in selfish behavior directed towards individuals whoseappearance is blurred, suggesting that blur filters can facilitate moraldisengagement through depersonalization. These results emphasize the need forbroader ethical discussions surrounding AI technologies that modify ourperception of others, including issues of transparency, consent, and theawareness of being subject to appearance manipulation by others. We alsoemphasize the importance of anticipatory experiments in informing thedevelopment of responsible guidelines and policies prior to the widespreadadoption of such technologies.</description><author>Nils K√∂bis, Philipp Lorenz-Spreen, Tamer Ajaj, Jean-Francois Bonnefon, Ralph Hertwig, Iyad Rahwan</author><pubDate>Wed, 07 Jun 2023 15:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04484v1</guid></item><item><title>FoSp: Focus and Separation Network for Early Smoke Segmentation</title><link>http://arxiv.org/abs/2306.04474v1</link><description>Early smoke segmentation (ESS) enables the accurate identification of smokesources, facilitating the prompt extinguishing of fires and preventinglarge-scale gas leaks. But ESS poses greater challenges than conventionalobject and regular smoke segmentation due to its small scale and transparentappearance, which can result in high miss detection rate and low precision. Toaddress these issues, a Focus and Separation Network (FoSp) is proposed. Wefirst introduce a Focus module employing bidirectional cascade which guideslow-resolution and high-resolution features towards mid-resolution to locateand determine the scope of smoke, reducing the miss detection rate. Next, wepropose a Separation module that separates smoke images into a pure smokeforeground and a smoke-free background, enhancing the contrast between smokeand background fundamentally, improving segmentation precision. Finally, aDomain Fusion module is developed to integrate the distinctive features of thetwo modules which can balance recall and precision to achieve high F_beta.Futhermore, to promote the development of ESS, we introduce a high-qualityreal-world dataset called SmokeSeg, which contains more small and transparentsmoke than the existing datasets. Experimental results show that our modelachieves the best performance on three available datasets: SYN70K (mIoU:83.00%), SMOKE5K (F_beta: 81.6%) and SmokeSeg (F_beta: 72.05%). Especially, ourFoSp outperforms SegFormer by 7.71% (F_beta) for early smoke segmentation onSmokeSeg.</description><author>Lujian Yao, Haitao Zhao, Jingchao Peng, Zhongze Wang, Kaijie Zhao</author><pubDate>Wed, 07 Jun 2023 15:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04474v1</guid></item><item><title>Introduction to Medical Imaging Informatics</title><link>http://arxiv.org/abs/2306.00421v2</link><description>Medical imaging informatics is a rapidly growing field that combines theprinciples of medical imaging and informatics to improve the acquisition,management, and interpretation of medical images. This chapter introduces thebasic concepts of medical imaging informatics, including image processing,feature engineering, and machine learning. It also discusses the recentadvancements in computer vision and deep learning technologies and how they areused to develop new quantitative image markers and prediction models fordisease detection, diagnosis, and prognosis prediction. By covering the basicknowledge of medical imaging informatics, this chapter provides a foundationfor understanding the role of informatics in medicine and its potential impacton patient care.</description><author>Md. Zihad Bin Jahangir, Ruksat Hossain, Riadul Islam, MD Abdullah Al Nasim, Md. Mahim Anjum Haque, Md Jahangir Alam, Sajedul Talukder</author><pubDate>Wed, 07 Jun 2023 15:38:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00421v2</guid></item><item><title>Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic Knowledge Graph and Text Mining Approach</title><link>http://arxiv.org/abs/2306.03833v2</link><description>Virtual health has been acclaimed as a transformative force in healthcaredelivery. Yet, its dropout issue is critical that leads to poor healthoutcomes, increased health, societal, and economic costs. Timely prediction ofpatient dropout enables stakeholders to take proactive steps to addresspatients' concerns, potentially improving retention rates. In virtual health,the information asymmetries inherent in its delivery format, between differentstakeholders, and across different healthcare delivery systems hinder theperformance of existing predictive methods. To resolve those informationasymmetries, we propose a Multimodal Dynamic Knowledge-driven DropoutPrediction (MDKDP) framework that learns implicit and explicit knowledge fromdoctor-patient dialogues and the dynamic and complex networks of variousstakeholders in both online and offline healthcare delivery systems. Weevaluate MDKDP by partnering with one of the largest virtual health platformsin China. MDKDP improves the F1-score by 3.26 percentage points relative to thebest benchmark. Comprehensive robustness analyses show that integratingstakeholder attributes, knowledge dynamics, and compact bilinear poolingsignificantly improves the performance. Our work provides significantimplications for healthcare IT by revealing the value of mining relations andknowledge across different service modalities. Practically, MDKDP offers anovel design artifact for virtual health platforms in patient dropoutmanagement.</description><author>Shuang Geng, Wenli Zhang, Jiaheng Xie, Gemin Liang, Ben Niu</author><pubDate>Wed, 07 Jun 2023 15:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03833v2</guid></item><item><title>Training-Free Neural Active Learning with Initialization-Robustness Guarantees</title><link>http://arxiv.org/abs/2306.04454v1</link><description>Existing neural active learning algorithms have aimed to optimize thepredictive performance of neural networks (NNs) by selecting data forlabelling. However, other than a good predictive performance, being robustagainst random parameter initializations is also a crucial requirement insafety-critical applications. To this end, we introduce our expected variancewith Gaussian processes (EV-GP) criterion for neural active learning, which istheoretically guaranteed to select data points which lead to trained NNs withboth (a) good predictive performances and (b) initialization robustness.Importantly, our EV-GP criterion is training-free, i.e., it does not requireany training of the NN during data selection, which makes it computationallyefficient. We empirically demonstrate that our EV-GP criterion is highlycorrelated with both initialization robustness and generalization performance,and show that it consistently outperforms baseline methods in terms of bothdesiderata, especially in situations with limited initial data or large batchsizes.</description><author>Apivich Hemachandra, Zhongxiang Dai, Jasraj Singh, See-Kiong Ng, Bryan Kian Hsiang Low</author><pubDate>Wed, 07 Jun 2023 15:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04454v1</guid></item><item><title>PyTorch Hyperparameter Tuning - A Tutorial for spotPython</title><link>http://arxiv.org/abs/2305.11930v2</link><description>The goal of hyperparameter tuning (or hyperparameter optimization) is tooptimize the hyperparameters to improve the performance of the machine or deeplearning model. spotPython (``Sequential Parameter Optimization Toolbox inPython'') is the Python version of the well-known hyperparameter tuner SPOT,which has been developed in the R programming environment for statisticalanalysis for over a decade. PyTorch is an optimized tensor library for deeplearning using GPUs and CPUs. This document shows how to integrate thespotPython hyperparameter tuner into the PyTorch training workflow. As anexample, the results of the CIFAR10 image classifier are used. In addition toan introduction to spotPython, this tutorial also includes a brief comparisonwith Ray Tune, a Python library for running experiments and tuninghyperparameters. This comparison is based on the PyTorch hyperparameter tuningtutorial. The advantages and disadvantages of both approaches are discussed. Weshow that spotPython achieves similar or even better results while being moreflexible and transparent than Ray Tune.</description><author>Thomas Bartz-Beielstein</author><pubDate>Wed, 07 Jun 2023 15:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11930v2</guid></item><item><title>Multi-modal Latent Diffusion</title><link>http://arxiv.org/abs/2306.04445v1</link><description>Multi-modal data-sets are ubiquitous in modern applications, and multi-modalVariational Autoencoders are a popular family of models that aim to learn ajoint representation of the different modalities. However, existing approachessuffer from a coherence-quality tradeoff, where models with good generationquality lack generative coherence across modalities, and vice versa. We discussthe limitations underlying the unsatisfactory performance of existing methods,to motivate the need for a different approach. We propose a novel method thatuses a set of independently trained, uni-modal, deterministic autoencoders.Individual latent variables are concatenated into a common latent space, whichis fed to a masked diffusion model to enable generative modeling. We alsointroduce a new multi-time training method to learn the conditional scorenetwork for multi-modal diffusion. Our methodology substantially outperformscompetitors in both generation quality and coherence, as shown through anextensive experimental campaign.</description><author>Mustapha Bounoua, Giulio Franzese, Pietro Michiardi</author><pubDate>Wed, 07 Jun 2023 15:16:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04445v1</guid></item><item><title>Fast Optimal Locally Private Mean Estimation via Random Projections</title><link>http://arxiv.org/abs/2306.04444v1</link><description>We study the problem of locally private mean estimation of high-dimensionalvectors in the Euclidean ball. Existing algorithms for this problem eitherincur sub-optimal error or have high communication and/or run-time complexity.We propose a new algorithmic framework, ProjUnit, for private mean estimationthat yields algorithms that are computationally efficient, have lowcommunication complexity, and incur optimal error up to a $1+o(1)$-factor. Ourframework is deceptively simple: each randomizer projects its input to a randomlow-dimensional subspace, normalizes the result, and then runs an optimalalgorithm such as PrivUnitG in the lower-dimensional space. In addition, weshow that, by appropriately correlating the random projection matrices acrossdevices, we can achieve fast server run-time. We mathematically analyze theerror of the algorithm in terms of properties of the random projections, andstudy two instantiations. Lastly, our experiments for private mean estimationand private federated learning demonstrate that our algorithms empiricallyobtain nearly the same utility as optimal ones while having significantly lowercommunication and computational cost.</description><author>Hilal Asi, Vitaly Feldman, Jelani Nelson, Huy L. Nguyen, Kunal Talwar</author><pubDate>Wed, 07 Jun 2023 15:07:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04444v1</guid></item><item><title>GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?</title><link>http://arxiv.org/abs/2306.00693v2</link><description>The recent upsurge in pre-trained large models (e.g. GPT-4) has swept acrossthe entire deep learning community. Such powerful large language models (LLMs)demonstrate advanced generative ability and multimodal understandingcapability, which quickly achieve new state-of-the-art performances on avariety of benchmarks. The pre-trained LLM usually plays the role as auniversal AI model that can conduct various tasks, including context reasoning,article analysis and image content comprehension. However, considering theprohibitively high memory and computational cost for implementing such a largemodel, the conventional models (such as CNN and ViT), are still essential formany visual perception tasks. In this paper, we propose to enhance therepresentation ability of ordinary vision models for perception tasks (e.g.image classification) by taking advantage of large pre-trained models. Wepresent a new learning paradigm in which the knowledge extracted from largepre-trained models are utilized to help models like CNN and ViT learn enhancedrepresentations and achieve better performance. Firstly, we curate a highquality description set by prompting a multimodal LLM to generate descriptivetext for all training images. Furthermore, we feed these detailed descriptionsinto a pre-trained encoder to extract text embeddings with rich semanticinformation that encodes the content of images. During training, textembeddings will serve as extra supervising signals and be aligned with imagerepresentations learned by vision models. The alignment process helps visionmodels learn better and achieve higher accuracy with the assistance ofpre-trained LLMs. We conduct extensive experiments to verify that the proposedalgorithm consistently improves the performance for various vision models withheterogeneous architectures.</description><author>Ning Ding, Yehui Tang, Zhongqian Fu, Chao Xu, Kai Han, Yunhe Wang</author><pubDate>Wed, 07 Jun 2023 14:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00693v2</guid></item><item><title>STEPS: A Benchmark for Order Reasoning in Sequential Tasks</title><link>http://arxiv.org/abs/2306.04441v1</link><description>Various human activities can be abstracted into a sequence of actions innatural text, i.e. cooking, repairing, manufacturing, etc. Such actionsequences heavily depend on the executing order, while disorder in actionsequences leads to failure of further task execution by robots or AI agents.Therefore, to verify the order reasoning capability of current neural models insequential tasks, we propose a challenging benchmark , named STEPS. STEPSinvolves two subtask settings, focusing on determining the rationality of givennext step in recipes and selecting the reasonable step from the multi-choicequestion, respectively. We describe the data construction and taskformulations, and benchmark most of significant Large Language Models (LLMs).The experimental results demonstrate 1) The commonsense reasoning of actionorders in sequential tasks are challenging to resolve via zero-shot promptingor few-shot in-context learning for LLMs; 2) Prompting method stillsignificantly lags behind tuning-based method on STEPS.</description><author>Weizhi Wang, Hong Wang, Xifeng Yan</author><pubDate>Wed, 07 Jun 2023 14:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04441v1</guid></item><item><title>Dual policy as self-model for planning</title><link>http://arxiv.org/abs/2306.04440v1</link><description>Planning is a data efficient decision-making strategy where an agent selectscandidate actions by exploring possible future states. To simulate futurestates when there is a high-dimensional action space, the knowledge of one'sdecision making strategy must be used to limit the number of actions to beexplored. We refer to the model used to simulate one's decisions as the agent'sself-model. While self-models are implicitly used widely in conjunction withworld models to plan actions, it remains unclear how self-models should bedesigned. Inspired by current reinforcement learning approaches andneuroscience, we explore the benefits and limitations of using a distilledpolicy network as the self-model. In such dual-policy agents, a model-freepolicy and a distilled policy are used for model-free actions and plannedactions, respectively. Our results on a ecologically relevant, parametricenvironment indicate that distilled policy network for self-model stabilizestraining, has faster inference than using model-free policy, promotes betterexploration, and could learn a comprehensive understanding of its ownbehaviors, at the cost of distilling a new network apart from the model-freepolicy.</description><author>Jaesung Yoo, Fernanda de la Torre, Robert Guangyu Yang</author><pubDate>Wed, 07 Jun 2023 14:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04440v1</guid></item><item><title>Convergence Analysis of Sequencial Split Learning on Heterogeneous Data</title><link>http://arxiv.org/abs/2302.01633v2</link><description>Federated Learning (FL) and Split Learning (SL) are two popular paradigms ofdistributed machine learning. By offloading the computation-intensive portionsto the server, SL is promising for deep model training on resource-constraineddevices, yet still lacking of rigorous convergence analysis. In this paper, wederive the convergence guarantees of Sequential SL (SSL, the vanilla case of SLthat conducts the model training in sequence) for strongly/general/non-convexobjectives on heterogeneous data. Notably, the derived guarantees suggest thatSSL is better than Federated Averaging (FedAvg, the most popular algorithm inFL) on heterogeneous data. We validate the counterintuitive analysis resultempirically on extremely heterogeneous data.</description><author>Yipeng Li, Xinchen Lyu</author><pubDate>Wed, 07 Jun 2023 14:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01633v2</guid></item><item><title>Cross-Database and Cross-Channel ECG Arrhythmia Heartbeat Classification Based on Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2306.04433v1</link><description>The classification of electrocardiogram (ECG) plays a crucial role in thedevelopment of an automatic cardiovascular diagnostic system. However,considerable variances in ECG signals between individuals is a significantchallenge. Changes in data distribution limit cross-domain utilization of amodel. In this study, we propose a solution to classify ECG in an unlabeleddataset by leveraging knowledge obtained from labeled source domain. We presenta domain-adaptive deep network based on cross-domain feature discrepancyoptimization. Our method comprises three stages: pre-training, cluster-centroidcomputing, and adaptation. In pre-training, we employ a Distributionally RobustOptimization (DRO) technique to deal with the vanishing worst-case trainingloss. To enhance the richness of the features, we concatenate three temporalfeatures with the deep learning features. The cluster computing stage involvescomputing centroids of distinctly separable clusters for the source using truelabels, and for the target using confident predictions. We propose a noveltechnique to select confident predictions in the target domain. In theadaptation stage, we minimize compacting loss within the same cluster,separating loss across different clusters, inter-domain cluster discrepancyloss, and running combined loss to produce a domain-robust model. Experimentsconducted in both cross-domain and cross-channel paradigms show the efficacy ofthe proposed method. Our method achieves superior performance compared to otherstate-of-the-art approaches in detecting ventricular ectopic beats (V),supraventricular ectopic beats (S), and fusion beats (F). Our method achievesan average improvement of 11.78% in overall accuracy over thenon-domain-adaptive baseline method on the three test datasets.</description><author>Md Niaz Imtiaz, Naimul Khan</author><pubDate>Wed, 07 Jun 2023 14:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04433v1</guid></item><item><title>ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models</title><link>http://arxiv.org/abs/2305.19926v2</link><description>Large Language Models (LLMs) have made remarkable advancements in the fieldof artificial intelligence, significantly reshaping the human-computerinteraction. We not only focus on the performance of LLMs, but also exploretheir features from a psychological perspective, acknowledging the importanceof understanding their behavioral characteristics. Our study examines thebehavioral patterns displayed by LLMs by employing trait theory, apsychological framework. We first focus on evaluating the consistency ofpersonality types exhibited by ChatGPT. Furthermore, experiments includecross-lingual effects on seven additional languages, and the investigation ofsix other LLMs. Moreover, the study investigates whether ChatGPT can exhibitpersonality changes in response to instructions or contextual cues. Thefindings show that ChatGPT consistently maintains its ENFJ personalityregardless of instructions or contexts. By shedding light on thepersonalization of LLMs, we anticipate that our study will serve as a catalystfor further research in this field.</description><author>Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, Michael R. Lyu</author><pubDate>Wed, 07 Jun 2023 14:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19926v2</guid></item><item><title>Faithful Knowledge Distillation</title><link>http://arxiv.org/abs/2306.04431v1</link><description>Knowledge distillation (KD) has received much attention due to its success incompressing networks to allow for their deployment in resource-constrainedsystems. While the problem of adversarial robustness has been studied before inthe KD setting, previous works overlook what we term the relative calibrationof the student network with respect to its teacher in terms of softconfidences. In particular, we focus on two crucial questions with regard to ateacher-student pair: (i) do the teacher and student disagree at points closeto correctly classified dataset examples, and (ii) is the distilled student asconfident as the teacher around dataset examples? These are critical questionswhen considering the deployment of a smaller student network trained from arobust teacher within a safety-critical setting. To address these questions, weintroduce a faithful imitation framework to discuss the relative calibration ofconfidences, as well as provide empirical and certified methods to evaluate therelative calibration of a student w.r.t. its teacher. Further, to verifiablyalign the relative calibration incentives of the student to those of itsteacher, we introduce faithful distillation. Our experiments on the MNIST andFashion-MNIST datasets demonstrate the need for such an analysis and theadvantages of the increased verifiability of faithful distillation overalternative adversarial distillation methods.</description><author>Tom A. Lamb, Rudy Brunel, Krishnamurthy, Dvijotham, M. Pawan Kumar, Philip H. S. Torr, Francisco Eiras</author><pubDate>Wed, 07 Jun 2023 14:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04431v1</guid></item><item><title>Balancing of competitive two-player Game Levels with Reinforcement Learning</title><link>http://arxiv.org/abs/2306.04429v1</link><description>The balancing process for game levels in a competitive two-player contextinvolves a lot of manual work and testing, particularly in non-symmetrical gamelevels. In this paper, we propose an architecture for automated balancing oftile-based levels within the recently introduced PCGRL framework (proceduralcontent generation via reinforcement learning). Our architecture is dividedinto three parts: (1) a level generator, (2) a balancing agent and, (3) areward modeling simulation. By playing the level in a simulation repeatedly,the balancing agent is rewarded for modifying it towards the same win rates forall players. To this end, we introduce a novel family of swap-basedrepresentations to increase robustness towards playability. We show that thisapproach is capable to teach an agent how to alter a level for balancing betterand faster than plain PCGRL. In addition, by analyzing the agent's swappingbehavior, we can draw conclusions about which tile types influence thebalancing most. We test and show our results using the Neural MMO (NMMO)environment in a competitive two-player setting.</description><author>Florian Rupp, Manuel Eberhardinger, Kai Eckert</author><pubDate>Wed, 07 Jun 2023 14:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04429v1</guid></item></channel></rss>