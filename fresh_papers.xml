<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 04 Apr 2024 06:00:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title><link>http://arxiv.org/abs/2404.02905v1</link><description>We present Visual AutoRegressive modeling (VAR), a new generation paradigmthat redefines the autoregressive learning on images as coarse-to-fine"next-scale prediction" or "next-resolution prediction", diverging from thestandard raster-scan "next-token prediction". This simple, intuitivemethodology allows autoregressive (AR) transformers to learn visualdistributions fast and generalize well: VAR, for the first time, makes ARmodels surpass diffusion transformers in image generation. On ImageNet 256x256benchmark, VAR significantly improve AR baseline by improving Frechet inceptiondistance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,with around 20x faster inference speed. It is also empirically verified thatVAR outperforms the Diffusion Transformer (DiT) in multiple dimensionsincluding image quality, inference speed, data efficiency, and scalability.Scaling up VAR models exhibits clear power-law scaling laws similar to thoseobserved in LLMs, with linear correlation coefficients near -0.998 as solidevidence. VAR further showcases zero-shot generalization ability in downstreamtasks including image in-painting, out-painting, and editing. These resultssuggest VAR has initially emulated the two important properties of LLMs:Scaling Laws and zero-shot task generalization. We have released all models andcodes to promote the exploration of AR/VAR models for visual generation andunified learning.</description><author>Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang</author><pubDate>Wed, 03 Apr 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02905v1</guid></item><item><title>ALOHa: A New Measure for Hallucination in Captioning Models</title><link>http://arxiv.org/abs/2404.02904v1</link><description>Despite recent advances in multimodal pre-training for visual description,state-of-the-art models still produce captions containing errors, such ashallucinating objects not present in a scene. The existing prominent metric forobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects andsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,which leverages large language models (LLMs) to measure object hallucinations.Specifically, we use an LLM to extract groundable objects from a candidatecaption, measure their semantic similarity to reference objects from captionsand object detections, and use Hungarian matching to produce a finalhallucination score. We show that ALOHa correctly identifies 13.6% morehallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCOCaptions annotated for hallucinations, and 30.8% more on nocaps, where objectsextend beyond MS COCO categories. Our code is available athttps://davidmchan.github.io/aloha/.</description><author>Suzanne Petryk, David M. Chan, Anish Kachinthaya, Haodi Zou, John Canny, Joseph E. Gonzalez, Trevor Darrell</author><pubDate>Wed, 03 Apr 2024 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02904v1</guid></item><item><title>LidarDM: Generative LiDAR Simulation in a Generated World</title><link>http://arxiv.org/abs/2404.02903v1</link><description>We present LidarDM, a novel LiDAR generative model capable of producingrealistic, layout-aware, physically plausible, and temporally coherent LiDARvideos. LidarDM stands out with two unprecedented capabilities in LiDARgenerative modeling: (i) LiDAR generation guided by driving scenarios, offeringsignificant potential for autonomous driving simulations, and (ii) 4D LiDARpoint cloud generation, enabling the creation of realistic and temporallycoherent sequences. At the heart of our model is a novel integrated 4D worldgeneration framework. Specifically, we employ latent diffusion models togenerate the 3D scene, combine it with dynamic actors to form the underlying 4Dworld, and subsequently produce realistic sensory observations within thisvirtual environment. Our experiments indicate that our approach outperformscompeting algorithms in realism, temporal coherency, and layout consistency. Weadditionally show that LidarDM can be used as a generative world modelsimulator for training and testing perception models.</description><author>Vlas Zyrianov, Henry Che, Zhijian Liu, Shenlong Wang</author><pubDate>Wed, 03 Apr 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02903v1</guid></item><item><title>DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets</title><link>http://arxiv.org/abs/2404.02900v1</link><description>Vision Transformer (ViT) has emerged as a prominent architecture for variouscomputer vision tasks. In ViT, we divide the input image into patch tokens andprocess them through a stack of self attention blocks. However, unlikeConvolutional Neural Networks (CNN), ViTs simple architecture has noinformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires alarge amount of data for pre-training. Various data efficient approaches (DeiT)have been proposed to train ViT on balanced datasets effectively. However,limited literature discusses the use of ViT for datasets with long-tailedimbalances. In this work, we introduce DeiT-LT to tackle the problem oftraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce anefficient and effective way of distillation from CNN via distillation DISTtoken by using out-of-distribution images and re-weighting the distillationloss to enhance focus on tail classes. This leads to the learning of localCNN-like features in early ViT blocks, improving generalization for tailclasses. Further, to mitigate overfitting, we propose distilling from a flatCNN teacher, which leads to learning low-rank generalizable features for DISTtokens across all ViT blocks. With the proposed DeiT-LT scheme, thedistillation DIST token becomes an expert on the tail classes, and theclassifier CLS token becomes an expert on the head classes. The experts help toeffectively learn features corresponding to both the majority and minorityclasses using a distinct set of tokens within the same ViT architecture. Weshow the effectiveness of DeiT-LT for training ViT from scratch on datasetsranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.</description><author>Harsh Rangwani, Pradipto Mondal, Mayank Mishra, Ashish Ramayee Asokan, R. Venkatesh Babu</author><pubDate>Wed, 03 Apr 2024 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02900v1</guid></item><item><title>FreeZe: Training-free zero-shot 6D pose estimation with geometric and vision foundation models</title><link>http://arxiv.org/abs/2312.00947v2</link><description>Estimating the 6D pose of objects unseen during training is highly desirableyet challenging. Zero-shot object 6D pose estimation methods address thischallenge by leveraging additional task-specific supervision provided bylarge-scale, photo-realistic synthetic datasets. However, their performanceheavily depends on the quality and diversity of rendered data and they requireextensive training. In this work, we show how to tackle the same task butwithout training on specific data. We propose FreeZe, a novel solution thatharnesses the capabilities of pre-trained geometric and vision foundationmodels. FreeZe leverages 3D geometric descriptors learned from unrelated 3Dpoint clouds and 2D visual features learned from web-scale 2D images togenerate discriminative 3D point-level descriptors. We then estimate the 6Dpose of unseen objects by 3D registration based on RANSAC. We also introduce anovel algorithm to solve ambiguous cases due to geometrically symmetric objectsthat is based on visual features. We comprehensively evaluate FreeZe across theseven core datasets of the BOP Benchmark, which include over a hundred 3Dobjects and 20,000 images captured in various scenarios. FreeZe consistentlyoutperforms all state-of-the-art approaches, including competitors extensivelytrained on synthetic 6D pose estimation data. Code will be publicly availableat https://andreacaraffa.github.io/freeze.</description><author>Andrea Caraffa, Davide Boscaini, Amir Hamza, Fabio Poiesi</author><pubDate>Wed, 03 Apr 2024 18:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00947v2</guid></item><item><title>MatAtlas: Text-driven Consistent Geometry Texturing and Material Assignment</title><link>http://arxiv.org/abs/2404.02899v1</link><description>We present MatAtlas, a method for consistent text-guided 3D model texturing.Following recent progress we leverage a large scale text-to-image generationmodel (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefullydesign an RGB texturing pipeline that leverages a grid pattern diffusion,driven by depth and edges. By proposing a multi-step texture refinementprocess, we significantly improve the quality and 3D consistency of thetexturing output. To further address the problem of baked-in lighting, we movebeyond RGB colors and pursue assigning parametric materials to the assets.Given the high-quality initial RGB texture, we propose a novel materialretrieval method capitalized on Large Language Models (LLM), enablingeditabiliy and relightability. We evaluate our method on a wide variety ofgeometries and show that our method significantly outperform prior arts. Wealso analyze the role of each component through a detailed ablation study.</description><author>Duygu Ceylan, Valentin Deschaintre, Thibault Groueix, Rosalie Martin, Chun-Hao Huang, Romain Rouffet, Vladimir Kim, Gaëtan Lassagne</author><pubDate>Wed, 03 Apr 2024 18:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02899v1</guid></item><item><title>Deep Image Composition Meets Image Forgery</title><link>http://arxiv.org/abs/2404.02897v1</link><description>Image forgery is a topic that has been studied for many years. Before thebreakthrough of deep learning, forged images were detected using handcraftedfeatures that did not require training. These traditional methods failed toperform satisfactorily even on datasets much worse in quality than real-lifeimage manipulations. Advances in deep learning have impacted image forgerydetection as much as they have impacted other areas of computer vision and haveimproved the state of the art. Deep learning models require large amounts oflabeled data for training. In the case of image forgery, labeled data at thepixel level is a very important factor for the models to learn. None of theexisting datasets have sufficient size, realism and pixel-level labeling at thesame time. This is due to the high cost of producing and labeling qualityimages. It can take hours for an image editing expert to manipulate just oneimage. To bridge this gap, we automate data generation using image compositiontechniques that are very related to image forgery. Unlike other automated datageneration frameworks, we use state of the art image composition deep learningmodels to generate spliced images close to the quality of real-lifemanipulations. Finally, we test the generated dataset on the SOTA imagemanipulation detection model and show that its prediction performance is lowercompared to existing datasets, i.e. we produce realistic images that are moredifficult to detect. Dataset will be available athttps://github.com/99eren99/DIS25k .</description><author>Eren Tahir, Mert Bal</author><pubDate>Wed, 03 Apr 2024 18:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02897v1</guid></item><item><title>Comment on "Machine learning conservation laws from differential equations"</title><link>http://arxiv.org/abs/2404.02896v1</link><description>In lieu of abstract, first paragraph reads: Six months after the authorderived a constant of motion for a 1D damped harmonic oscillator [1], a similarresult appeared by Liu, Madhavan, and Tegmark [2, 3], without citing theauthor. However, their derivation contained six serious errors, causing boththeir method and result to be incorrect. In this Comment, those errors arereviewed.</description><author>Michael F. Zimmer</author><pubDate>Wed, 03 Apr 2024 18:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02896v1</guid></item><item><title>ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline</title><link>http://arxiv.org/abs/2404.02893v1</link><description>Large language models (LLMs) have shown excellent mastering of humanlanguage, but still struggle in real-world applications that requiremathematical problem-solving. While many strategies and datasets to enhanceLLMs' mathematics are developed, it remains a challenge to simultaneouslymaintain and improve both language and mathematical capabilities in deployedLLM systems.In this work, we tailor the Self-Critique pipeline, which addressesthe challenge in the feedback learning stage of LLM alignment. We first train ageneral Math-Critique model from the LLM itself to provide feedback signals.Then, we sequentially employ rejective fine-tuning and direct preferenceoptimization over the LLM's own generations for data collection. Based onChatGLM3-32B, we conduct a series of experiments on both academic and our newlycreated challenging dataset, MathUserEval. Results show that our pipelinesignificantly enhances the LLM's mathematical problem-solving while stillimproving its language ability, outperforming LLMs that could be two timeslarger. Related techniques have been deployed toChatGLM\footnote{\url{https://chatglm.cn}}, an online serving LLM. Relatedevaluation dataset and scripts are released at\url{https://github.com/THUDM/ChatGLM-Math}.</description><author>Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, Yuxiao Dong</author><pubDate>Wed, 03 Apr 2024 18:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02893v1</guid></item><item><title>MODNO: Multi Operator Learning With Distributed Neural Operators</title><link>http://arxiv.org/abs/2404.02892v1</link><description>The study of operator learning involves the utilization of neural networks toapproximate operators. Traditionally, the focus has been on single-operatorlearning (SOL). However, recent advances have rapidly expanded this to includethe approximation of multiple operators using foundation models equipped withmillions or billions of trainable parameters, leading to the research ofmulti-operator learning (MOL). In this paper, we present a novel distributedtraining approach aimed at enabling a single neural operator with significantlyfewer parameters to effectively tackle multi-operator learning challenges, allwithout incurring additional average costs. Our method is applicable to variousChen-Chen-type neural operators, such as Deep Operator Neural Networks (DON).The core idea is to independently learn the output basis functions for eachoperator using its dedicated data, while simultaneously centralizing thelearning of the input function encoding shared by all operators using theentire dataset. Through a systematic study of five numerical examples, wecompare the accuracy and cost of training a single neural operator for eachoperator independently versus training a MOL model using our proposed method.Our results demonstrate enhanced efficiency and satisfactory accuracy.Moreover, our approach illustrates that some operators with limited data can bemore effectively constructed with the aid of data from analogous operatorsthrough MOL learning. This highlights another MOL's potential to bolsteroperator learning.</description><author>Zecheng Zhang</author><pubDate>Wed, 03 Apr 2024 18:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02892v1</guid></item><item><title>Steganographic Passport: An Owner and User Verifiable Credential for Deep Model IP Protection Without Retraining</title><link>http://arxiv.org/abs/2404.02889v1</link><description>Ensuring the legal usage of deep models is crucial to promoting trustable,accountable, and responsible artificial intelligence innovation. Currentpassport-based methods that obfuscate model functionality for license-to-useand ownership verifications suffer from capacity and quality constraints, asthey require retraining the owner model for new users. They are also vulnerableto advanced Expanded Residual Block ambiguity attacks. We proposeSteganographic Passport, which uses an invertible steganographic network todecouple license-to-use from ownership verification by hiding the user'sidentity images into the owner-side passport and recovering them from theirrespective user-side passports. An irreversible and collision-resistant hashfunction is used to avoid exposing the owner-side passport from the deriveduser-side passports and increase the uniqueness of the model signature. Tosafeguard both the passport and model's weights against advanced ambiguityattacks, an activation-level obfuscation is proposed for the verificationbranch of the owner's model. By jointly training the verification anddeployment branches, their weights become tightly coupled. The proposed methodsupports agile licensing of deep models by providing a strong ownership proofand license accountability without requiring a separate model retraining forthe admission of every new user. Experiment results show that ourSteganographic Passport outperforms other passport-based deep model protectionmethods in robustness against various known attacks.</description><author>Qi Cui, Ruohan Meng, Chaohui Xu, Chip-Hong Chang</author><pubDate>Wed, 03 Apr 2024 18:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02889v1</guid></item><item><title>Total Selfie: Generating Full-Body Selfies</title><link>http://arxiv.org/abs/2308.14740v2</link><description>We present a method to generate full-body selfies from photographs originallytaken at arms length. Because self-captured photos are typically taken closeup, they have limited field of view and exaggerated perspective that distortsfacial shapes. We instead seek to generate the photo some one else would takeof you from a few feet away. Our approach takes as input four selfies of yourface and body, a background image, and generates a full-body selfie in adesired target pose. We introduce a novel diffusion-based approach to combineall of this information into high-quality, well-composed photos of you with thedesired pose and background.</description><author>Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steven M. Seitz</author><pubDate>Wed, 03 Apr 2024 18:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14740v2</guid></item><item><title>G3DR: Generative 3D Reconstruction in ImageNet</title><link>http://arxiv.org/abs/2403.00939v3</link><description>We introduce a novel 3D generative method, Generative 3D Reconstruction(G3DR) in ImageNet, capable of generating diverse and high-quality 3D objectsfrom single images, addressing the limitations of existing methods. At theheart of our framework is a novel depth regularization technique that enablesthe generation of scenes with high-geometric fidelity. G3DR also leverages apretrained language-vision model, such as CLIP, to enable reconstruction innovel views and improve the visual realism of generations. Additionally, G3DRdesigns a simple but effective sampling procedure to further improve thequality of generations. G3DR offers diverse and efficient 3D asset generationbased on class or text conditioning. Despite its simplicity, G3DR is able tobeat state-of-theart methods, improving over them by up to 22% in perceptualmetrics and 90% in geometry scores, while needing only half of the trainingtime. Code is available at https://github.com/preddy5/G3DR</description><author>Pradyumna Reddy, Ismail Elezi, Jiankang Deng</author><pubDate>Wed, 03 Apr 2024 18:42:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00939v3</guid></item><item><title>Octopus v2: On-device language model for super agent</title><link>http://arxiv.org/abs/2404.01744v2</link><description>Language models have shown effectiveness in a variety of softwareapplications, particularly in tasks related to automatic workflow. These modelspossess the crucial ability to call functions, which is essential in creatingAI agents. Despite the high performance of large-scale language models in cloudenvironments, they are often associated with concerns over privacy and cost.Current on-device models for function calling face issues with latency andaccuracy. Our research presents a new method that empowers an on-device modelwith 2 billion parameters to surpass the performance of GPT-4 in both accuracyand latency, and decrease the context length by 95\%. When compared to Llama-7Bwith a RAG-based function calling mechanism, our method enhances latency by35-fold. This method reduces the latency to levels deemed suitable fordeployment across a variety of edge devices in production environments,aligning with the performance requisites for real-world applications.</description><author>Wei Chen, Zhiyuan Li</author><pubDate>Wed, 03 Apr 2024 18:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01744v2</guid></item><item><title>PoCo: Point Context Cluster for RGBD Indoor Place Recognition</title><link>http://arxiv.org/abs/2404.02885v1</link><description>We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D placerecognition task, aimed at identifying the most likely match for a given queryframe within a reference database. The task presents inherent challengesattributed to the constrained field of view and limited range of perceptionsensors. We propose a new network architecture, which generalizes the recentContext of Clusters (CoCs) to extract global descriptors directly from thenoisy point clouds through end-to-end learning. Moreover, we develop thearchitecture by integrating both color and geometric modalities into the pointfeatures to enhance the global descriptor representation. We conductedevaluations on public datasets ScanNet-PR and ARKit with 807 and 5047scenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, weachieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis(61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from thebest-published result CGis (39.82%). In addition, PoCo shows higher efficiencythan CGis in inference time (1.75X-faster), and we demonstrate theeffectiveness of PoCo in recognizing places within a real-world laboratoryenvironment.</description><author>Jing Liang, Zhuo Deng, Zheming Zhou, Omid Ghasemalizadeh, Dinesh Manocha, Min Sun, Cheng-Hao Kuo, Arnie Sen</author><pubDate>Wed, 03 Apr 2024 18:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02885v1</guid></item><item><title>Structured Packing in LLM Training Improves Long Context Utilization</title><link>http://arxiv.org/abs/2312.17296v4</link><description>Recent developments in long-context large language models have attractedconsiderable attention. Yet, their real-world applications are often hinderedby ineffective context information use. This work shows that structuringtraining data to increase semantic interdependence is an effective strategy foroptimizing context utilization. To this end, we introduce Structured Packingfor Long Context (SPLiCe), a method for creating training examples by usinginformation retrieval methods to collate mutually relevant documents into asingle training context. We empirically validate SPLiCe on large $3$B and $7$Bmodels, showing perplexity improvements and better long-context utilization ondownstream tasks. Remarkably, already relatively short fine-tuning with SPLiCeis enough to attain these benefits. Additionally, the comprehensive study ofSPLiCe reveals intriguing transfer effects such as training on code dataleading to perplexity improvements on text data.</description><author>Konrad Staniszewski, Szymon Tworkowski, Yu Zhao, Sebastian Jaszczur, Henryk Michalewski, Łukasz Kuciński, Piotr Miłoś</author><pubDate>Wed, 03 Apr 2024 18:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17296v4</guid></item><item><title>On the Scalability of Diffusion-based Text-to-Image Generation</title><link>http://arxiv.org/abs/2404.02883v1</link><description>Scaling up model and data size has been quite successful for the evolution ofLLMs. However, the scaling law for the diffusion based text-to-image (T2I)models is not fully explored. It is also unclear how to efficiently scale themodel for better performance at reduced cost. The different training settingsand expensive training cost make a fair model comparison extremely difficult.In this work, we empirically study the scaling properties of diffusion basedT2I models by performing extensive and rigours ablations on scaling bothdenoising backbones and training set, including training scaled UNet andTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600Mimages. For model scaling, we find the location and amount of cross attentiondistinguishes the performance of existing UNet designs. And increasing thetransformer blocks is more parameter-efficient for improving text-imagealignment than increasing channel numbers. We then identify an efficient UNetvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the datascaling side, we show the quality and diversity of the training set mattersmore than simply dataset size. Increasing caption density and diversityimproves text-image alignment performance and the learning efficiency. Finally,we provide scaling functions to predict the text-image alignment performance asfunctions of the scale of model size, compute and dataset size.</description><author>Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R. Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, Stefano Soatto</author><pubDate>Wed, 03 Apr 2024 18:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02883v1</guid></item><item><title>Linear Attention Sequence Parallelism</title><link>http://arxiv.org/abs/2404.02882v1</link><description>Sequence Parallel (SP) serves as a prevalent strategy to handle longsequences that exceed the memory limit of a single GPU. However, existing SPmethods do not take advantage of linear attention features, resulting insub-optimal parallelism efficiency and usability for linear attention-basedlanguage models. In this paper, we introduce Linear Attention Sequence Parallel(LASP), an efficient SP method tailored to linear attention-based languagemodels. Specifically, we design an efficient point-to-point communicationmechanism to leverage the right-product kernel trick of linear attention, whichsharply decreases the communication overhead of SP. We also enhance thepractical efficiency of LASP by performing kernel fusion and intermediate statecaching, making the implementation of LASP hardware-friendly on GPU clusters.Furthermore, we meticulously ensure the compatibility of sequence-level LASPwith all types of batch-level data parallel methods, which is vital fordistributed training on large clusters with long sequences and large batches.We conduct extensive experiments on two linear attention-based models withvarying sequence lengths and GPU cluster sizes. LASP scales sequence length upto 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer thanexisting SP methods while being significantly faster. The code is available athttps://github.com/OpenNLPLab/LASP.</description><author>Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong</author><pubDate>Wed, 03 Apr 2024 18:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02882v1</guid></item><item><title>Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits</title><link>http://arxiv.org/abs/2404.00267v2</link><description>Prior research has established associations between individuals' languageusage and their personal traits; our linguistic patterns reveal informationabout our personalities, emotional states, and beliefs. However, with theincreasing adoption of Large Language Models (LLMs) as writing assistants ineveryday writing, a critical question emerges: are authors' linguistic patternsstill predictive of their personal traits when LLMs are involved in the writingprocess? We investigate the impact of LLMs on the linguistic markers ofdemographic and psychological traits, specifically examining three LLMs -GPT3.5, Llama 2, and Gemini - across six different traits: gender, age,political affiliation, personality, empathy, and morality. Our findingsindicate that although the use of LLMs slightly reduces the predictive power oflinguistic patterns over authors' personal traits, the significant changes areinfrequent, and the use of LLMs does not fully diminish the predictive power ofauthors' linguistic patterns over their personal traits. We also note that sometheoretically established lexical-based linguistic markers lose theirreliability as predictors when LLMs are used in the writing process. Ourfindings have important implications for the study of linguistic markers ofpersonal traits in the age of LLMs.</description><author>Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan Wen, Ala Tak, Fred Morstatter, Morteza Dehghani</author><pubDate>Wed, 03 Apr 2024 18:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00267v2</guid></item><item><title>FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery</title><link>http://arxiv.org/abs/2404.02877v1</link><description>Object detection in remotely sensed satellite pictures is fundamental in manyfields such as biophysical, and environmental monitoring. While deep learningalgorithms are constantly evolving, they have been mostly implemented andtested on popular ground-based taken photos. This paper critically evaluatesand compares a suite of advanced object detection algorithms customized for thetask of identifying aircraft within satellite imagery. Using the largeHRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,this research encompasses an array of methodologies including YOLO versions 5and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained fromscratch. This exhaustive training and validation study reveal YOLOv5 as thepreeminent model for the specific case of identifying airplanes from remotesensing data, showcasing high precision and adaptability across diverse imagingconditions. This research highlight the nuanced performance landscapes of thesealgorithms, with YOLOv5 emerging as a robust solution for aerial objectdetection, underlining its importance through superior mean average precision,Recall, and Intersection over Union scores. The findings described hereunderscore the fundamental role of algorithm selection aligned with thespecific demands of satellite imagery analysis and extend a comprehensiveframework to evaluate model efficacy. The benchmark toolkit and codes,available via https://github.com/toelt-llc/FlightScope_Bench, aims to furtherexploration and innovation in the realm of remote sensing object detection,paving the way for improved analytical methodologies in satellite imageryapplications.</description><author>Safouane El Ghazouali, Arnaud Gucciardi, Nicola Venturi, Michael Rueegsegger, Umberto Michelucci</author><pubDate>Wed, 03 Apr 2024 18:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02877v1</guid></item><item><title>Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective</title><link>http://arxiv.org/abs/2403.18346v3</link><description>Recent advancements in Large Language Models (LLMs) have facilitated thedevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,MLLMs often suffer from an over-reliance on unimodal biases (e.g., languagebias and vision bias), leading to incorrect answers in complex multimodaltasks. To investigate this issue, we propose a causal framework to interpretthe biases in Visual Question Answering (VQA) problems. Within our framework,we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,and assess the causal effect of biases through an in-depth causal analysis.Motivated by the causal graph, we introduce a novel MORE dataset, consisting of12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,necessitating multi-hop reasoning and the surmounting of unimodal biases.Furthermore, we propose two strategies to mitigate unimodal biases and enhanceMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)framework for limited-access MLLMs and the refinement of open-source MLLMsthrough fine-tuning. Extensive quantitative and qualitative experiments offervaluable insights for future research. Our project page is athttps://opencausalab.github.io/MORE.</description><author>Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu</author><pubDate>Wed, 03 Apr 2024 18:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18346v3</guid></item><item><title>tsGT: Stochastic Time Series Modeling With Transformer</title><link>http://arxiv.org/abs/2403.05713v3</link><description>Time series methods are of fundamental importance in virtually any field ofscience that deals with temporally structured data. Recently, there has been asurge of deterministic transformer models with time series-specificarchitectural biases. In this paper, we go in a different direction byintroducing tsGT, a stochastic time series model built on a general-purposetransformer architecture. We focus on using a well-known and theoreticallyjustified rolling window backtesting and evaluation protocol. We show that tsGToutperforms the state-of-the-art models on MAD and RMSE, and surpasses itsstochastic peers on QL and CRPS, on four commonly used datasets. We complementthese results with a detailed analysis of tsGT's ability to model the datadistribution and predict marginal quantile values.</description><author>Łukasz Kuciński, Witold Drzewakowski, Mateusz Olko, Piotr Kozakowski, Łukasz Maziarka, Marta Emilia Nowakowska, Łukasz Kaiser, Piotr Miłoś</author><pubDate>Wed, 03 Apr 2024 18:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05713v3</guid></item><item><title>Off-Policy Correction For Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2111.11229v3</link><description>Multi-agent reinforcement learning (MARL) provides a framework for problemsinvolving multiple interacting agents. Despite apparent similarity to thesingle-agent case, multi-agent problems are often harder to train and analyzetheoretically. In this work, we propose MA-Trace, a new on-policy actor-criticalgorithm, which extends V-Trace to the MARL setting. The key advantage of ouralgorithm is its high scalability in a multi-worker setting. To this end,MA-Trace utilizes importance sampling as an off-policy correction method, whichallows distributing the computations with no impact on the quality of training.Furthermore, our algorithm is theoretically grounded - we prove a fixed-pointtheorem that guarantees convergence. We evaluate the algorithm extensively onthe StarCraft Multi-Agent Challenge, a standard benchmark for multi-agentalgorithms. MA-Trace achieves high performance on all its tasks and exceedsstate-of-the-art results on some of them.</description><author>Michał Zawalski, Błażej Osiński, Henryk Michalewski, Piotr Miłoś</author><pubDate>Wed, 03 Apr 2024 18:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.11229v3</guid></item><item><title>Gaussian Process Regression with Soft Inequality and Monotonicity Constraints</title><link>http://arxiv.org/abs/2404.02873v1</link><description>Gaussian process (GP) regression is a non-parametric, Bayesian framework toapproximate complex models. Standard GP regression can lead to an unboundedmodel in which some points can take infeasible values. We introduce a new GPmethod that enforces the physical constraints in a probabilistic manner. ThisGP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC).QHMC is an efficient way to sample from a broad class of distributions. Unlikethe standard Hamiltonian Monte Carlo algorithm in which a particle has a fixedmass, QHMC allows a particle to have a random mass matrix with a probabilitydistribution. Introducing the QHMC method to the inequality and monotonicityconstrained GP regression in the probabilistic sense, our approach improves theaccuracy and reduces the variance in the resulting GP model. According to ourexperiments on several datasets, the proposed approach serves as an efficientmethod as it accelerates the sampling process while maintaining the accuracy,and it is applicable to high dimensional problems.</description><author>Didem Kochan, Xiu Yang</author><pubDate>Wed, 03 Apr 2024 18:09:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02873v1</guid></item><item><title>Integrating Explanations in Learning LTL Specifications from Demonstrations</title><link>http://arxiv.org/abs/2404.02872v1</link><description>This paper investigates whether recent advances in Large Language Models(LLMs) can assist in translating human explanations into a format that canrobustly support learning Linear Temporal Logic (LTL) from demonstrations. BothLLMs and optimization-based methods can extract LTL specifications fromdemonstrations; however, they have distinct limitations. LLMs can quicklygenerate solutions and incorporate human explanations, but their lack ofconsistency and reliability hampers their applicability in safety-criticaldomains. On the other hand, optimization-based methods do provide formalguarantees but cannot process natural language explanations and facescalability challenges. We present a principled approach to combining LLMs andoptimization-based methods to faithfully translate human explanations anddemonstrations into LTL specifications. We have implemented a tool calledJanaka based on our approach. Our experiments demonstrate the effectiveness ofcombining explanations with demonstrations in learning LTL specificationsthrough several case studies.</description><author>Ashutosh Gupta, John Komp, Abhay Singh Rajput, Krishna Shankaranarayanan, Ashutosh Trivedi, Namrita Varshney</author><pubDate>Wed, 03 Apr 2024 18:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02872v1</guid></item><item><title>Human Activity Recognition using Smartphones</title><link>http://arxiv.org/abs/2404.02869v1</link><description>Human Activity Recognition is a subject of great research today and has itsapplications in remote healthcare, activity tracking of the elderly or thedisables, calories burnt tracking etc. In our project, we have created anAndroid application that recognizes the daily human activities and calculatethe calories burnt in real time. We first captured labeled triaxialacceleration readings for different daily human activities from thesmartphone's embedded accelerometer. These readings were preprocessed using amedian filter. 42 features were extracted using various methods. We then testedvarious machine learning algorithms along with dimensionality reduction.Finally, in our Android application, we used the machine learning algorithm anda subset of features that provided maximum accuracy and minimum model buildingtime. This is used for real-time activity recognition and calculation ofcalories burnt using a formula based on Metabolic Equivalent.</description><author>Mayur Sonawane, Sahil Rajesh Dhayalkar, Siddesh Waje, Soyal Markhelkar, Akshay Wattamwar, Seema C. Shrawne</author><pubDate>Wed, 03 Apr 2024 18:05:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02869v1</guid></item><item><title>ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback</title><link>http://arxiv.org/abs/2404.00934v2</link><description>ChatGLM is a free-to-use AI service powered by the ChatGLM family of largelanguage models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline --a reinforcement learning from human feedback (RLHF) system -- designed toenhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompassesthree major components: the collection of human preference data, the trainingof the reward model, and the optimization of policies. Throughout the processof integrating ChatGLM-RLHF into production, we encountered and addressedseveral unprecedented challenges. We introduce the strategies to mitigatereward variance for stabilized large-scale training, implement modelparallelism with fused gradient-descent, and design regularization constraintsto avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHFbrings significant improvements in alignment tasks compared to the supervisedfine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\%more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents ourpractices of aligning LLMs with human preferences, offering insights into thechallenges and solutions in RLHF implementations.</description><author>Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong</author><pubDate>Wed, 03 Apr 2024 18:04:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00934v2</guid></item><item><title>Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation</title><link>http://arxiv.org/abs/2311.09467v2</link><description>Knowledge-to-text generators often struggle to faithfully generatedescriptions for the input facts: they may produce hallucinations thatcontradict the input, or describe facts not present in the input. To reducehallucinations, we propose a decoding-only method, TWEAK (Think WhileEffectively Articulating Knowledge), which can be integrated with any generatorwithout retraining. TWEAK treats the generated sequences at each decoding stepand its future sequences as hypotheses, and ranks each generation candidatebased on the extent to which their hypotheses are supported by the input factsusing a Hypothesis Verification Model (HVM). We first demonstrate theeffectiveness of TWEAK by using a Natural Language Inference (NLI) model as theHVM and report improved faithfulness with a minimal impact on the quality. Wethen replace the NLI model with a task-specific HVM trained with afirst-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairsinput facts with their original and perturbed descriptions. We test TWEAK withtwo generators, and the best TWEAK variants improve on average for the twomodels by 2.24/7.17 points in faithfulness (FactKB) in in/out-of-distributionevaluations, respectively, and with only a 0.14/0.32-point decline in quality(BERTScore).</description><author>Yifu Qiu, Varun Embar, Shay B. Cohen, Benjamin Han</author><pubDate>Wed, 03 Apr 2024 18:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09467v2</guid></item><item><title>Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds</title><link>http://arxiv.org/abs/2404.02866v1</link><description>Protecting privacy during inference with deep neural networks is possible byadding noise to the activations in the last layers prior to the finalclassifiers or other task-specific layers. The activations in such layers areknown as "features" (or, less commonly, as "embeddings" or "featureembeddings"). The added noise helps prevent reconstruction of the inputs fromthe noisy features. Lower bounding the variance of every possible unbiasedestimator of the inputs quantifies the confidentiality arising from such addednoise. Convenient, computationally tractable bounds are available from classicinequalities of Hammersley and of Chapman and Robbins -- the HCR bounds.Numerical experiments indicate that the HCR bounds are on the precipice ofbeing effectual for small neural nets with the data sets, "MNIST" and"CIFAR-10," which contain 10 classes each for image classification. The HCRbounds appear to be insufficient on their own to guarantee confidentiality ofthe inputs to inference with standard deep neural nets, "ResNet-18" and"Swin-T," pre-trained on the data set, "ImageNet-1000," which contains 1000classes. Supplementing the addition of noise to features with other methods forproviding confidentiality may be warranted in the case of ImageNet. In allcases, the results reported here limit consideration to amounts of added noisethat incur little degradation in the accuracy of classification from the noisyfeatures. Thus, the added noise enhances confidentiality without much reductionin the accuracy on the task of image classification.</description><author>Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed Mahloujifar, Mark Tygert</author><pubDate>Wed, 03 Apr 2024 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02866v1</guid></item><item><title>Learning Object State Changes in Videos: An Open-World Perspective</title><link>http://arxiv.org/abs/2312.11782v2</link><description>Object State Changes (OSCs) are pivotal for video understanding. While humanscan effortlessly generalize OSC understanding from familiar to unknown objects,current approaches are confined to a closed vocabulary. Addressing this gap, weintroduce a novel open-world formulation for the video OSC problem. The goal isto temporally localize the three stages of an OSC -- the object's initialstate, its transitioning state, and its end state -- whether or not the objecthas been observed during training. Towards this end, we develop VidOSC, aholistic learning approach that: (1) leverages text and vision-language modelsfor supervisory signals to obviate manually labeling OSC training data, and (2)abstracts fine-grained shared state representations from objects to enhancegeneralization. Furthermore, we present HowToChange, the first open-worldbenchmark for video OSC localization, which offers an order of magnitudeincrease in the label space and annotation volume compared to the best existingbenchmark. Experimental results demonstrate the efficacy of our approach, inboth traditional closed-world and open-world scenarios.</description><author>Zihui Xue, Kumar Ashutosh, Kristen Grauman</author><pubDate>Wed, 03 Apr 2024 17:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11782v2</guid></item><item><title>End-To-End Self-tuning Self-supervised Time Series Anomaly Detection</title><link>http://arxiv.org/abs/2404.02865v1</link><description>Time series anomaly detection (TSAD) finds many applications such asmonitoring environmental sensors, industry KPIs, patient biomarkers, etc. Atwo-fold challenge for TSAD is a versatile and unsupervised model that candetect various different types of time series anomalies (spikes,discontinuities, trend shifts, etc.) without any labeled data. Modern neuralnetworks have outstanding ability in modeling complex time series.Self-supervised models in particular tackle unsupervised TSAD by transformingthe input via various augmentations to create pseudo anomalies for training.However, their performance is sensitive to the choice of augmentation, which ishard to choose in practice, while there exists no effort in the literature ondata augmentation tuning for TSAD without labels. Our work aims to fill thisgap. We introduce TSAP for TSA "on autoPilot", which can (self-)tuneaugmentation hyperparameters end-to-end. It stands on two key components: adifferentiable augmentation architecture and an unsupervised validation loss toeffectively assess the alignment between augmentation type and anomaly type.Case studies show TSAP's ability to effectively select the (discrete)augmentation type and associated (continuous) hyperparameters. In turn, itoutperforms established baselines, including SOTA self-supervised models, ondiverse TSAD tasks exhibiting different anomaly types.</description><author>Boje Deforce, Meng-Chieh Lee, Bart Baesens, Estefanía Serral Asensio, Jaemin Yoo, Leman Akoglu</author><pubDate>Wed, 03 Apr 2024 17:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02865v1</guid></item><item><title>AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation</title><link>http://arxiv.org/abs/2404.01717v2</link><description>Blind super-resolution methods based on stable diffusion showcase formidablegenerative capabilities in reconstructing clear high-resolution images withintricate details from low-resolution inputs. However, their practicalapplicability is often hampered by poor efficiency, stemming from therequirement of thousands or hundreds of sampling steps. Inspired by theefficient text-to-image approach adversarial diffusion distillation (ADD), wedesign AddSR to address this issue by incorporating the ideas of bothdistillation and ControlNet. Specifically, we first propose a prediction-basedself-refinement strategy to provide high-frequency information in the studentmodel output with marginal additional time cost. Furthermore, we refine thetraining process by employing HR images, rather than LR images, to regulate theteacher model, providing a more robust constraint for distillation. Second, weintroduce a timestep-adapting loss to address the perception-distortionimbalance problem introduced by ADD. Extensive experiments demonstrate ourAddSR generates better restoration results, while achieving faster speed thanprevious SD-based state-of-the-art models (e.g., 7x faster than SeeSR).</description><author>Rui Xie, Ying Tai, Kai Zhang, Zhenyu Zhang, Jun Zhou, Jian Yang</author><pubDate>Wed, 03 Apr 2024 17:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01717v2</guid></item><item><title>Toward Inference-optimal Mixture-of-Expert Large Language Models</title><link>http://arxiv.org/abs/2404.02852v1</link><description>Mixture-of-Expert (MoE) based large language models (LLMs), such as therecent Mixtral and DeepSeek-MoE, have shown great promise in scaling model sizewithout suffering from the quadratic growth of training cost of densetransformers. Like dense models, training MoEs requires answering the samequestion: given a training budget, what is the optimal allocation on the modelsize and number of tokens? We study the scaling law of MoE-based LLMs regardingthe relations between the model performance, model size, dataset size, and theexpert degree. Echoing previous research studying MoE in different contexts, weobserve the diminishing return of increasing the number of experts, but thisseems to suggest we should scale the number of experts until saturation, as thetraining cost would remain constant, which is problematic during inferencetime. We propose to amend the scaling law of MoE by introducing inferenceefficiency as another metric besides the validation loss. We find that MoEswith a few (4/8) experts are the most serving efficient solution under the sameperformance, but costs 2.5-3.5x more in training. On the other hand, training a(16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, butwith a larger training dataset is a promising setup under a training budget.</description><author>Longfei Yun, Yonghao Zhuang, Yao Fu, Eric P Xing, Hao Zhang</author><pubDate>Wed, 03 Apr 2024 17:33:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02852v1</guid></item><item><title>Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification</title><link>http://arxiv.org/abs/2310.19558v2</link><description>Federated learning (FL) has been recognized as a rapidly growing researcharea, where the model is trained over massively distributed clients under theorchestration of a parameter server (PS) without sharing clients' data. Thispaper delves into a class of federated problems characterized by non-convex andnon-smooth loss functions, that are prevalent in FL applications butchallenging to handle due to their intricate non-convexity and non-smoothnessnature and the conflicting requirements on communication efficiency and privacyprotection. In this paper, we propose a novel federated primal-dual algorithmwith bidirectional model sparsification tailored for non-convex and non-smoothFL problems, and differential privacy is applied for privacy guarantee. Itsunique insightful properties and some privacy and convergence analyses are alsopresented as the FL algorithm design guidelines. Extensive experiments onreal-world data are conducted to demonstrate the effectiveness of the proposedalgorithm and much superior performance than some state-of-the-art FLalgorithms, together with the validation of all the analytical results andproperties.</description><author>Yiwei Li, Chien-Wei Huang, Shuai Wang, Chong-Yung Chi, Tony Q. S. Quek</author><pubDate>Wed, 03 Apr 2024 17:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19558v2</guid></item><item><title>Proper Laplacian Representation Learning</title><link>http://arxiv.org/abs/2310.10833v2</link><description>The ability to learn good representations of states is essential for solvinglarge reinforcement learning problems, where exploration, generalization, andtransfer are particularly challenging. The Laplacian representation is apromising approach to address these problems by inducing informative stateencoding and intrinsic rewards for temporally-extended action discovery andreward shaping. To obtain the Laplacian representation one needs to compute theeigensystem of the graph Laplacian, which is often approximated throughoptimization objectives compatible with deep learning approaches. Theseapproximations, however, depend on hyperparameters that are impossible to tuneefficiently, converge to arbitrary rotations of the desired eigenvectors, andare unable to accurately recover the corresponding eigenvalues. In this paperwe introduce a theoretically sound objective and corresponding optimizationalgorithm for approximating the Laplacian representation. Our approachnaturally recovers both the true eigenvectors and eigenvalues while eliminatingthe hyperparameter dependence of previous approximations. We providetheoretical guarantees for our method and we show that those results translateempirically into robust learning across multiple environments.</description><author>Diego Gomez, Michael Bowling, Marlos C. Machado</author><pubDate>Wed, 03 Apr 2024 17:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10833v2</guid></item><item><title>Language Models Implement Simple Word2Vec-style Vector Arithmetic</title><link>http://arxiv.org/abs/2305.16130v3</link><description>A primary criticism towards language models (LMs) is their inscrutability.This paper presents evidence that, despite their size and complexity, LMssometimes exploit a simple vector arithmetic style mechanism to solve somerelational tasks using regularities encoded in the hidden space of the model(e.g., Poland:Warsaw::China:Beijing). We investigate a range of language modelsizes (from 124M parameters to 176B parameters) in an in-context learningsetting, and find that for a variety of tasks (involving capital cities,uppercasing, and past-tensing) a key part of the mechanism reduces to a simpleadditive update typically applied by the feedforward (FFN) networks. We furthershow that this mechanism is specific to tasks that require retrieval frompretraining memory, rather than retrieval from local context. Our resultscontribute to a growing body of work on the interpretability of LMs, and offerreason to be optimistic that, despite the massive and non-linear nature of themodels, the strategies they ultimately use to solve tasks can sometimes reduceto familiar and even intuitive algorithms.</description><author>Jack Merullo, Carsten Eickhoff, Ellie Pavlick</author><pubDate>Wed, 03 Apr 2024 17:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16130v3</guid></item><item><title>Taming the Interacting Particle Langevin Algorithm -- the superlinear case</title><link>http://arxiv.org/abs/2403.19587v2</link><description>Recent advances in stochastic optimization have yielded the interactiveparticle Langevin algorithm (IPLA), which leverages the notion of interactingparticle systems (IPS) to efficiently sample from approximate posteriordensities. This becomes particularly crucial within the framework ofExpectation-Maximization (EM), where the E-step is computationally challengingor even intractable. Although prior research has focused on scenarios involvingconvex cases with gradients of log densities that grow at most linearly, ourwork extends this framework to include polynomial growth. Taming techniques areemployed to produce an explicit discretization scheme that yields a new classof stable, under such non-linearities, algorithms which are called tamedinteractive particle Langevin algorithms (tIPLA). We obtain non-asymptoticconvergence error estimates in Wasserstein-2 distance for the new class underan optimal rate.</description><author>Tim Johnston, Nikolaos Makras, Sotirios Sabanis</author><pubDate>Wed, 03 Apr 2024 17:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19587v2</guid></item><item><title>Cross-Modal Conditioned Reconstruction for Language-guided Medical Image Segmentation</title><link>http://arxiv.org/abs/2404.02845v1</link><description>Recent developments underscore the potential of textual information inenhancing learning models for a deeper understanding of medical visualsemantics. However, language-guided medical image segmentation still faces achallenging issue. Previous works employ implicit and ambiguous architecturesto embed textual information. This leads to segmentation results that areinconsistent with the semantics represented by the language, sometimes evendiverging significantly. To this end, we propose a novel cross-modalconditioned Reconstruction for Language-guided Medical Image Segmentation(RecLMIS) to explicitly capture cross-modal interactions, which assumes thatwell-aligned medical visual features and medical notes can effectivelyreconstruct each other. We introduce conditioned interaction to adaptivelypredict patches and words of interest. Subsequently, they are utilized asconditioning factors for mutual reconstruction to align with regions describedin the medical notes. Extensive experiments demonstrate the superiority of ourRecLMIS, surpassing LViT by 3.74% mIoU on the publicly available MosMedData+dataset and achieving an average increase of 1.89% mIoU for cross-domain testson our QATA-CoV19 dataset. Simultaneously, we achieve a relative reduction of20.2% in parameter count and a 55.5% decrease in computational load. The codewill be available at https://github.com/ShashankHuang/RecLMIS.</description><author>Xiaoshuang Huang, Hongxiang Li, Meng Cao, Long Chen, Chenyu You, Dong An</author><pubDate>Wed, 03 Apr 2024 17:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02845v1</guid></item><item><title>LeanVec: Searching vectors faster by making them fit</title><link>http://arxiv.org/abs/2312.16335v2</link><description>Modern deep learning models have the ability to generate high-dimensionalvectors whose similarity reflects semantic resemblance. Thus, similaritysearch, i.e., the operation of retrieving those vectors in a large collectionthat are similar to a given query, has become a critical component of a widerange of applications that demand highly accurate and timely answers. In thissetting, the high vector dimensionality puts similarity search systems undercompute and memory pressure, leading to subpar performance. Additionally,cross-modal retrieval tasks have become increasingly common, e.g., where a userinputs a text query to find the most relevant images for that query. However,these queries often have different distributions than the database embeddings,making it challenging to achieve high accuracy. In this work, we presentLeanVec, a framework that combines linear dimensionality reduction with vectorquantization to accelerate similarity search on high-dimensional vectors whilemaintaining accuracy. We present LeanVec variants for in-distribution (ID) andout-of-distribution (OOD) queries. LeanVec-ID yields accuracies on par withthose from recently introduced deep learning alternatives whose computationaloverhead precludes their usage in practice. LeanVec-OOD uses two noveltechniques for dimensionality reduction that consider the query and databasedistributions to simultaneously boost the accuracy and the performance of theframework even further (even presenting competitive results when the query anddatabase distributions match). All in all, our extensive and variedexperimental results show that LeanVec produces state-of-the-art results, withup to 3.7x improvement in search throughput and up to 4.9x faster index buildtime over the state of the art.</description><author>Mariano Tepper, Ishwar Singh Bhati, Cecilia Aguerrebere, Mark Hildebrand, Ted Willke</author><pubDate>Wed, 03 Apr 2024 17:18:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16335v2</guid></item><item><title>I-Design: Personalized LLM Interior Designer</title><link>http://arxiv.org/abs/2404.02838v1</link><description>Interior design allows us to be who we are and live how we want - each designis as unique as our distinct personality. However, it is not trivial fornon-professionals to express and materialize this since it requires aligningfunctional and visual expectations with the constraints of physical space; thisrenders interior design a luxury. To make it more accessible, we presentI-Design, a personalized interior designer that allows users to generate andvisualize their design goals through natural language communication. I-Designstarts with a team of large language model agents that engage in dialogues andlogical reasoning with one another, transforming textual user input intofeasible scene graph designs with relative object relationships. Subsequently,an effective placement algorithm determines optimal locations for each objectwithin the scene. The final design is then constructed in 3D by retrieving andintegrating assets from an existing object database. Additionally, we propose anew evaluation protocol that utilizes a vision-language model and complementsthe design pipeline. Extensive quantitative and qualitative experiments showthat I-Design outperforms existing methods in delivering high-quality 3D designsolutions and aligning with abstract concepts that match user input, showcasingits advantages across detailed 3D arrangement and conceptual fidelity.</description><author>Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, Xi Wang</author><pubDate>Wed, 03 Apr 2024 17:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02838v1</guid></item><item><title>Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models</title><link>http://arxiv.org/abs/2404.02837v1</link><description>This paper reveals the phenomenon of parameter heterogeneity in largelanguage models (LLMs). We find that a small subset of ``cherry'' parametersexhibit a disproportionately large influence on model performance, while thevast majority of parameters have minimal impact. This heterogeneity is found tobe prevalent across different model families, scales, and types. Motivated bythis observation, we propose CherryQ, a novel quantization method that unifiesthe optimization of mixed-precision parameters. CherryQ identifies andpreserves the critical cherry parameters in high precision while aggressivelyquantizing the remaining parameters to low precision. Extensive experimentsdemonstrate the effectiveness of CherryQ. CherryQ outperforms existingquantization approaches in terms of perplexity and downstream task performance.Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performancecompared to their 16-bit counterparts. These findings highlight the potentialof CherryQ for enabling efficient deployment of LLMs by taking advantage ofparameter heterogeneity.</description><author>Wanyun Cui, Qianle Wang</author><pubDate>Wed, 03 Apr 2024 17:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02837v1</guid></item><item><title>Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison</title><link>http://arxiv.org/abs/2404.02835v1</link><description>Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieveexamples from memory to guide the generation process. While most works in thistrend explore new ways to exploit the retrieved examples, the upstreamretrieval step is mostly unexplored. In this paper, we study the effect ofvarying retrieval methods for several translation architectures, to betterunderstand the interplay between these two processes. We conduct experiments intwo language pairs in a multi-domain setting and consider several downstreamarchitectures based on a standard autoregressive model, an edit-based model,and a large language model with in-context learning. Our experiments show thatthe choice of the retrieval technique impacts the translation scores, withvariance across architectures. We also discuss the effects of increasing thenumber and diversity of examples, which are mostly positive across the board.</description><author>Maxime Bouthors, Josep Crego, Francois Yvon</author><pubDate>Wed, 03 Apr 2024 17:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02835v1</guid></item><item><title>EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling</title><link>http://arxiv.org/abs/2403.14541v2</link><description>Recently, Large Language Models (LLMs) have demonstrated outstandingperformance across a wide range of downstream language tasks. Temperaturesampling is a commonly used decoding strategy for LLMs' generation process.However, a fixed temperature parameter is used in most cases, which may notalways be an optimal choice for balancing generation quality and diversity. Inthis paper, we propose an effective Entropy-based Dynamic Temperature (EDT)Sampling method, to achieve a more balanced performance in terms of bothgeneration quality and diversity by dynamically selecting the temperatureparameter. Additionally, we also show model performance and comprehensiveanalyses for 4 different generation benchmarks. Our experiments show that EDTsignificantly outperforms the existing strategies across different tasks.</description><author>Shimao Zhang, Yu Bao, Shujian Huang</author><pubDate>Wed, 03 Apr 2024 17:09:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14541v2</guid></item><item><title>Empowering Biomedical Discovery with AI Agents</title><link>http://arxiv.org/abs/2404.02831v1</link><description>We envision 'AI scientists' as systems capable of skeptical learning andreasoning that empower biomedical research through collaborative agents thatintegrate machine learning tools with experimental platforms. Rather thantaking humans out of the discovery process, biomedical AI agents combine humancreativity and expertise with AI's ability to analyze large datasets, navigatehypothesis spaces, and execute repetitive tasks. AI agents are proficient in avariety of tasks, including self-assessment and planning of discoveryworkflows. These agents use large language models and generative models tofeature structured memory for continual learning and use machine learning toolsto incorporate scientific knowledge, biological principles, and theories. AIagents can impact areas ranging from hybrid cell simulation, programmablecontrol of phenotypes, and the design of cellular circuits to the developmentof new therapies.</description><author>Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik</author><pubDate>Wed, 03 Apr 2024 17:08:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02831v1</guid></item><item><title>Privacy-Aware Semantic Cache for Large Language Models</title><link>http://arxiv.org/abs/2403.02694v2</link><description>Large Language Models (LLMs) like ChatGPT and Llama2 have revolutionizednatural language processing and search engine dynamics. However, these modelsincur exceptionally high computational costs. For instance, GPT-3 consists of175 billion parameters where inference demands billions of floating-pointoperations. Caching is a natural solution to reduce LLM inference costs onrepeated queries which constitute about 31% of the total queries. However,existing caching methods are incapable of finding semantic similarities amongLLM queries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a user-centric semantic cache for LLMs thatidentifies semantically similar queries to determine cache hit or miss. UsingMeanCache, the response to a user's semantically similar query can be retrievedfrom a local cache rather than re-querying the LLM, thus reducing costs,service provider load, and environmental impact. Existing caching solutions forLLMs raise privacy and scalability concerns and perform wasteful queryrequests. MeanCache leverages Federated Learning (FL) to collaboratively traina query similarity model across LLM users without violating privacy. By placinga local cache in each user's device and using FL, MeanCache reduces the latencyand costs and enhances model performance, resulting in lower false hit rates.MeanCache compresses the embedding dimensions to minimize cache storage andalso finds the optimal cosine similarity threshold. Our experiments benchmarkedagainst the state-of-the-art caching method, reveal that MeanCache attains anapproximately 17% higher F-score and a 20% increase in precision duringsemantic cache hit-and-miss decisions. It also reduces the storage requirementby 83% and accelerates semantic cache hit-and-miss decisions by 11%.</description><author>Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ali Anwar, Muhammad Ali Gulzar</author><pubDate>Wed, 03 Apr 2024 17:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02694v2</guid></item><item><title>Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes</title><link>http://arxiv.org/abs/2404.02830v1</link><description>Vertebral fracture grading classifies the severity of vertebral fractures,which is a challenging task in medical imaging and has recently attracted DeepLearning (DL) models. Only a few works attempted to make such modelshuman-interpretable despite the need for transparency and trustworthiness incritical use cases like DL-assisted medical diagnosis. Moreover, such modelseither rely on post-hoc methods or additional annotations. In this work, wepropose a novel interpretable-by-design method, ProtoVerse, to find relevantsub-parts of vertebral fractures (prototypes) that reliably explain the model'sdecision in a human-understandable way. Specifically, we introduce a noveldiversity-promoting loss to mitigate prototype repetitions in small datasetswith intricate semantics. We have experimented with the VerSe'19 dataset andoutperformed the existing prototype-based method. Further, our model providessuperior interpretability against the post-hoc method. Importantly, expertradiologists validated the visual interpretability of our results, showingclinical applicability.</description><author>Poulami Sinhamahapatra, Suprosanna Shit, Anjany Sekuboyina, Malek Husseini, David Schinz, Nicolas Lenhart, Joern Menze, Jan Kirschke, Karsten Roscher, Stephan Guennemann</author><pubDate>Wed, 03 Apr 2024 17:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02830v1</guid></item><item><title>Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery</title><link>http://arxiv.org/abs/2211.13715v5</link><description>Inferring causal structure from data is a challenging task of fundamentalimportance in science. Observational data are often insufficient to identify asystem's causal structure uniquely. While conducting interventions (i.e.,experiments) can improve the identifiability, such samples are usuallychallenging and expensive to obtain. Hence, experimental design approaches forcausal discovery aim to minimize the number of interventions by estimating themost informative intervention target. In this work, we propose a novelGradient-based Intervention Targeting method, abbreviated GIT, that 'trusts'the gradient estimator of a gradient-based causal discovery framework toprovide signals for the intervention acquisition function. We provide extensiveexperiments in simulated and real-world datasets and demonstrate that GITperforms on par with competitive baselines, surpassing them in the low-dataregime.</description><author>Mateusz Olko, Michał Zając, Aleksandra Nowak, Nino Scherrer, Yashas Annadani, Stefan Bauer, Łukasz Kuciński, Piotr Miłoś</author><pubDate>Wed, 03 Apr 2024 17:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13715v5</guid></item><item><title>Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models</title><link>http://arxiv.org/abs/2312.10835v3</link><description>Knowledge distillation methods have recently shown to be a promisingdirection to speedup the synthesis of large-scale diffusion models by requiringonly a few inference steps. While several powerful distillation methods wererecently proposed, the overall quality of student samples is typically lowercompared to the teacher ones, which hinders their practical usage. In thiswork, we investigate the relative quality of samples produced by the teachertext-to-image diffusion model and its distilled student version. As our mainempirical finding, we discover that a noticeable portion of student samplesexhibit superior fidelity compared to the teacher ones, despite the"approximate" nature of the student. Based on this finding, we propose anadaptive collaboration between student and teacher diffusion models foreffective text-to-image synthesis. Specifically, the distilled model producesthe initial sample, and then an oracle decides whether it needs furtherimprovements with a slow teacher model. Extensive experiments demonstrate thatthe designed pipeline surpasses state-of-the-art text-to-image alternatives forvarious inference budgets in terms of human preference. Furthermore, theproposed approach can be naturally used in popular applications such astext-guided image editing and controllable generation.</description><author>Nikita Starodubcev, Artem Fedorov, Artem Babenko, Dmitry Baranchuk</author><pubDate>Wed, 03 Apr 2024 17:00:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10835v3</guid></item><item><title>BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models</title><link>http://arxiv.org/abs/2404.02827v1</link><description>This work presents BAdam, an optimizer that leverages the block coordinateoptimization framework with Adam as the inner solver. BAdam offers a memoryefficient approach to the full parameter finetuning of large language modelsand reduces running time of the backward process thanks to the chain ruleproperty. Experimentally, we apply BAdam to instruction-tune the Llama 2-7Bmodel on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The resultsindicate that BAdam exhibits superior convergence behavior in comparison toLoRA and LOMO. Furthermore, our downstream performance evaluation of theinstruction-tuned models using the MT-bench shows that BAdam modestly surpassesLoRA and more substantially outperforms LOMO. Finally, we compare BAdam withAdam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUEbenchmark. The results demonstrate that BAdam is capable of narrowing theperformance gap with Adam. Our code is available athttps://github.com/Ledzy/BAdam.</description><author>Qijun Luo, Hengxu Yu, Xiao Li</author><pubDate>Wed, 03 Apr 2024 16:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02827v1</guid></item><item><title>Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models</title><link>http://arxiv.org/abs/2404.02823v1</link><description>The ability of large language models (LLMs) to follow instructions is crucialto real-world applications. Despite recent advances, several studies havehighlighted that LLMs struggle when faced with challenging instructions,especially those that include complex constraints, hindering theireffectiveness in various tasks. To address this challenge, we introduceConifer, a novel instruction tuning dataset, designed to enhance LLMs to followmulti-level instructions with complex constraints. Utilizing GPT-4, we curatethe dataset by a series of LLM-driven refinement processes to ensure highquality. We also propose a progressive learning scheme that emphasizes aneasy-to-hard progression, and learning from process feedback. Models trainedwith Conifer exhibit remarkable improvements in instruction-followingabilities, especially for instructions with complex constraints. On severalinstruction-following benchmarks, our 7B model outperforms the state-of-the-artopen-source 7B models, even exceeds the performance of models 10 times largeron certain metrics. All the code and Conifer dataset are available athttps://www.github.com/ConiferLM/Conifer.</description><author>Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, Ruohui Huang</author><pubDate>Wed, 03 Apr 2024 16:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02823v1</guid></item><item><title>Identifying Climate Targets in National Laws and Policies using Machine Learning</title><link>http://arxiv.org/abs/2404.02822v1</link><description>Quantified policy targets are a fundamental element of climate policy,typically characterised by domain-specific and technical language. Currentmethods for curating comprehensive views of global climate policy targetsentail significant manual effort. At present there are few scalable methods forextracting climate targets from national laws or policies, which limitspolicymakers' and researchers' ability to (1) assess private and public sectoralignment with global goals and (2) inform policy decisions. In this paper wepresent an approach for extracting mentions of climate targets from nationallaws and policies. We create an expert-annotated dataset identifying threecategories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewableenergy targets)) and train a classifier to reliably identify them in text. Weinvestigate bias and equity impacts related to our model and identify specificyears and country names as problematic features. Finally, we investigate thecharacteristics of the dataset produced by running this classifier on theClimate Policy Radar (CPR) dataset of global national climate laws and policiesand UNFCCC submissions, highlighting the potential of automated and scalabledata collection for existing climate policy databases and supporting furtherresearch. Our work represents a significant upgrade in the accessibility ofthese key climate policy elements for policymakers and researchers. We publishour model at\url{https://huggingface.co/ClimatePolicyRadar/national-climate-targets} andrelated dataset at\url{https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets}.</description><author>Matyas Juhasz, Tina Marchand, Roshan Melwani, Kalyan Dutia, Sarah Goodenough, Harrison Pim, Henry Franks</author><pubDate>Wed, 03 Apr 2024 16:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02822v1</guid></item><item><title>ElasticLaneNet: An Efficient Geometry-Flexible Approach for Lane Detection</title><link>http://arxiv.org/abs/2312.10389v2</link><description>The task of lane detection involves identifying the boundaries of drivingareas in real-time. Recognizing lanes with variable and complex geometricstructures remains a challenge. In this paper, we explore a novel and flexibleway of implicit lanes representation named \textit{Elastic Lane map (ELM)}, andintroduce an efficient physics-informed end-to-end lane detection framework,namely, ElasticLaneNet (Elastic interaction energy-informed Lane detectionNetwork). The approach considers predicted lanes as moving zero-contours on theflexibly shaped \textit{ELM} that are attracted to the ground truth guided byan elastic interaction energy-loss function (EIE loss). Our framework wellintegrates the global information and low-level features. The method performswell in complex lane scenarios, including those with large curvature, weakgeometry features at intersections, complicated cross lanes, Y-shapes lanes,dense lanes, etc. We apply our approach on three datasets: SDLane, CULane, andTuSimple. The results demonstrate exceptional performance of our method, withthe state-of-the-art results on the structurally diverse SDLane, achievingF1-score of 89.51, Recall rate of 87.50, and Precision of 91.61 with fastinference speed.</description><author>Yaxin Feng, Yuan Lan, Luchan Zhang, Yang Xiang</author><pubDate>Wed, 03 Apr 2024 16:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10389v2</guid></item><item><title>Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References</title><link>http://arxiv.org/abs/2305.15067v2</link><description>Most research about natural language generation (NLG) relies on evaluationbenchmarks with limited references for a sample, which may result in poorcorrelations with human judgements. The underlying reason is that one semanticmeaning can actually be expressed in different forms, and the evaluation with asingle or few references may not accurately reflect the quality of the model'shypotheses. To address this issue, this paper presents a simple and effectivemethod, named Div-Ref, to enhance existing evaluation benchmarks by enrichingthe number of references. We leverage large language models (LLMs) to diversifythe expression of a single reference into multiple high-quality ones to coverthe semantic space of the reference sentence as much as possible. We conductcomprehensive experiments to empirically demonstrate that diversifying theexpression of reference can significantly enhance the correlation betweenautomatic evaluation and human evaluation. This idea is compatible with recentLLM-based evaluation which can similarly derive advantages from incorporatingmultiple references. We strongly encourage future generation benchmarks toinclude more references, even if they are generated by LLMs, which is once forall. We release all the code and data at https://github.com/RUCAIBox/Div-Ref tofacilitate research.</description><author>Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, Tom Kocmi, Furu Wei</author><pubDate>Wed, 03 Apr 2024 16:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15067v2</guid></item><item><title>Structure-reinforced Transformer for Dynamic Graph Representation Learning with Edge Temporal States</title><link>http://arxiv.org/abs/2304.10079v2</link><description>The burgeoning field of dynamic graph representation learning, fuelled by theincreasing demand for graph data analysis in real-world applications, posesboth enticing opportunities and formidable challenges. Despite the promisingresults achieved by recent research leveraging recurrent neural networks (RNNs)and graph neural networks (GNNs), these approaches often fail to adequatelyconsider the impact of the edge temporal states on the strength of inter-noderelationships across different time slices, further overlooking the dynamicchanges in node features induced by fluctuations in relationship strength.Furthermore, the extraction of global structural features is hindered by theinherent over-smoothing drawback of GNNs, which in turn limits their overallperformance. In this paper, we introduce a novel dynamic graph representationlearning framework namely Recurrent Structure-reinforced Graph Transformer(RSGT), which initially models the temporal status of edges explicitly byutilizing different edge types and weights based on the differences between anytwo consecutive snapshots. In this manner, the varying edge temporal states aremapped as a part of the topological structure of the graph. Subsequently, astructure-reinforced graph transformer is proposed to capture temporal noderepresentations that encoding both the graph topological structure and evolvingdynamics,through a recurrent learning paradigm. Our experimental evaluations,conducted on four real-world datasets, underscore the superior performance ofthe RSGT in the realm of discrete dynamic graph representation learning. Theresults reveal that RSGT consistently surpasses competing methods in dynamiclink prediction tasks.</description><author>Shengxiang Hu, Guobing Zou, Song Yang, Shiyi Lin, Bofeng Zhang, Yixin Chen</author><pubDate>Wed, 03 Apr 2024 16:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10079v2</guid></item><item><title>Psychometric Predictive Power of Large Language Models</title><link>http://arxiv.org/abs/2311.07484v2</link><description>Instruction tuning aligns the response of large language models (LLMs) withhuman preferences. Despite such efforts in human--LLM alignment, we reportthat, interestingly, instruction tuning does not always make LLMs human-likefrom a cognitive modeling perspective. More specifically, next-wordprobabilities estimated by instruction-tuned LLMs are often worse at simulatinghuman reading behavior than those estimated by base LLMs. In addition, weexplore prompting methodologies in simulating human reading behavior with LLMs.Our results show that prompts reflecting a particular linguistic hypothesisimprove PPP but are still inferior to PPP from small base models. Thesefindings highlight that recent advancements in LLMs, i.e., instruction tuningand prompting, do not offer better estimates than direct probabilitymeasurements from base LLMs in cognitive modeling. In other words, ourexperiments highlight that pure next-word probability remains a strongpredictor for human reading behavior, even in the age of LLMs.</description><author>Tatsuki Kuribayashi, Yohei Oseki, Timothy Baldwin</author><pubDate>Wed, 03 Apr 2024 16:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07484v2</guid></item><item><title>Dynamic LiDAR Re-simulation using Compositional Neural Fields</title><link>http://arxiv.org/abs/2312.05247v2</link><description>We introduce DyNFL, a novel neural field-based approach for high-fidelityre-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDARmeasurements from dynamic environments, accompanied by bounding boxes of movingobjects, to construct an editable neural field. This field, comprisingseparately reconstructed static background and dynamic objects, allows users tomodify viewpoints, adjust object positions, and seamlessly add or removeobjects in the re-simulated scene. A key innovation of our method is the neuralfield composition technique, which effectively integrates reconstructed neuralassets from various scenes through a ray drop test, accounting for occlusionsand transparent surfaces. Our evaluation with both synthetic and real-worldenvironments demonstrates that DyNFL substantially improves dynamic scene LiDARsimulation, offering a combination of physical fidelity and flexible editingcapabilities.</description><author>Hanfeng Wu, Xingxing Zuo, Stefan Leutenegger, Or Litany, Konrad Schindler, Shengyu Huang</author><pubDate>Wed, 03 Apr 2024 16:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05247v2</guid></item><item><title>Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication</title><link>http://arxiv.org/abs/2111.06464v2</link><description>Communication is compositional if complex signals can be represented as acombination of simpler subparts. In this paper, we theoretically show thatinductive biases on both the training framework and the data are needed todevelop a compositional communication. Moreover, we prove that compositionalityspontaneously arises in the signaling games, where agents communicate over anoisy channel. We experimentally confirm that a range of noise levels, whichdepends on the model and the data, indeed promotes compositionality. Finally,we provide a comprehensive study of this dependence and report results in termsof recently studied compositionality metrics: topographical similarity,conflict count, and context independence.</description><author>Łukasz Kuciński, Tomasz Korbak, Paweł Kołodziej, Piotr Miłoś</author><pubDate>Wed, 03 Apr 2024 16:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.06464v2</guid></item><item><title>A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches</title><link>http://arxiv.org/abs/2404.02817v1</link><description>Task and Motion Planning (TAMP) integrates high-level task planning andlow-level motion planning to equip robots with the autonomy to effectivelyreason over long-horizon, dynamic tasks. Optimization-based TAMP focuses onhybrid optimization approaches that define goal conditions via objectivefunctions and are capable of handling open-ended goals, robotic dynamics, andphysical interaction between the robot and the environment. Therefore,optimization-based TAMP is particularly suited to solve highly complex,contact-rich locomotion and manipulation problems. This survey provides acomprehensive review on optimization-based TAMP, covering (i) planning domainrepresentations, including action description languages and temporal logic,(ii) individual solution strategies for components of TAMP, including AIplanning and trajectory optimization (TO), and (iii) the dynamic interplaybetween logic-based task planning and model-based TO. A particular focus ofthis survey is to highlight the algorithm structures to efficiently solve TAMP,especially hierarchical and distributed approaches. Additionally, the surveyemphasizes the synergy between the classical methods and contemporarylearning-based innovations such as large language models. Furthermore, thefuture research directions for TAMP is discussed in this survey, highlightingboth algorithmic and application-specific challenges.</description><author>Zhigen Zhao, Shuo Chen, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao</author><pubDate>Wed, 03 Apr 2024 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02817v1</guid></item><item><title>Three Heads Are Better Than One: Complementary Experts for Long-Tailed Semi-supervised Learning</title><link>http://arxiv.org/abs/2312.15702v2</link><description>We address the challenging problem of Long-Tailed Semi-Supervised Learning(LTSSL) where labeled data exhibit imbalanced class distribution and unlabeleddata follow an unknown distribution. Unlike in balanced SSL, the generatedpseudo-labels are skewed towards head classes, intensifying the training bias.Such a phenomenon is even amplified as more unlabeled data will be mislabeledas head classes when the class distribution of labeled and unlabeled datasetsare mismatched. To solve this problem, we propose a novel method namedComPlementary Experts (CPE). Specifically, we train multiple experts to modelvarious class distributions, each of them yielding high-quality pseudo-labelswithin one form of class distribution. Besides, we introduce Classwise BatchNormalization for CPE to avoid performance degradation caused by featuredistribution mismatch between head and non-head classes. CPE achievesstate-of-the-art performances on CIFAR-10-LT, CIFAR-100-LT, and STL-10-LTdataset benchmarks. For instance, on CIFAR-10-LT, CPE improves test accuracy byover 2.22% compared to baselines. Code is available athttps://github.com/machengcheng2016/CPE-LTSSL.</description><author>Chengcheng Ma, Ismail Elezi, Jiankang Deng, Weiming Dong, Changsheng Xu</author><pubDate>Wed, 03 Apr 2024 16:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15702v2</guid></item><item><title>GPU-Accelerated RSF Level Set Evolution for Large-Scale Microvascular Segmentation</title><link>http://arxiv.org/abs/2404.02813v1</link><description>Microvascular networks are challenging to model because these structures arecurrently near the diffraction limit for most advanced three-dimensionalimaging modalities, including confocal and light sheet microscopy. This makessemantic segmentation difficult, because individual components of thesenetworks fluctuate within the confines of individual pixels. Level set methodsare ideally suited to solve this problem by providing surface and topologicalconstraints on the resulting model, however these active contour techniques areextremely time intensive and impractical for terabyte-scale images. We proposea reformulation and implementation of the region-scalable fitting (RSF) levelset model that makes it amenable to three-dimensional evaluation using bothsingle-instruction multiple data (SIMD) and single-program multiple-data (SPMD)parallel processing. This enables evaluation of the level set equation onindependent regions of the data set using graphics processing units (GPUs),making large-scale segmentation of high-resolution networks practical andinexpensive. We tested this 3D parallel RSF approach on multiple data sets acquired usingstate-of-the-art imaging techniques to acquire microvascular data, includingmicro-CT, light sheet fluorescence microscopy (LSFM) and milling microscopy. Toassess the performance and accuracy of the RSF model, we conducted aMonte-Carlo-based validation technique to compare results to other segmentationmethods. We also provide a rigorous profiling to show the gains in processingspeed leveraging parallel hardware. This study showcases the practicalapplication of the RSF model, emphasizing its utility in the challenging domainof segmenting large-scale high-topology network structures with a particularfocus on building microvascular models.</description><author>Meher Niger, Helya Goharbavang, Taeyong Ahn, Emily K. Alley, Joshua D. Wythe, Guoning Chen, David Mayerich</author><pubDate>Wed, 03 Apr 2024 16:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02813v1</guid></item><item><title>Subgoal Search For Complex Reasoning Tasks</title><link>http://arxiv.org/abs/2108.11204v3</link><description>Humans excel in solving complex reasoning tasks through a mental process ofmoving from one idea to a related one. Inspired by this, we propose SubgoalSearch (kSubS) method. Its key component is a learned subgoal generator thatproduces a diversity of subgoals that are both achievable and closer to thesolution. Using subgoals reduces the search space and induces a high-levelsearch graph suitable for efficient planning. In this paper, we implement kSubSusing a transformer-based subgoal module coupled with the classical best-firstsearch framework. We show that a simple approach of generating $k$-th stepahead subgoals is surprisingly efficient on three challenging domains: twopopular puzzle games, Sokoban and the Rubik's Cube, and an inequality provingbenchmark INT. kSubS achieves strong results including state-of-the-art on INTwithin a modest computational budget.</description><author>Konrad Czechowski, Tomasz Odrzygóźdź, Marek Zbysiński, Michał Zawalski, Krzysztof Olejnik, Yuhuai Wu, Łukasz Kuciński, Piotr Miłoś</author><pubDate>Wed, 03 Apr 2024 16:35:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.11204v3</guid></item><item><title>Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion</title><link>http://arxiv.org/abs/2402.14227v2</link><description>We develop a robust quaternion recurrent neural network (QRNN) for real-timeprocessing of 3D and 4D data with outliers. This is achieved by combining thereal-time recurrent learning (RTRL) algorithm and the maximum correntropycriterion (MCC) as a loss function. While both the mean square error andmaximum correntropy criterion are viable cost functions, it is shown that thenon-quadratic maximum correntropy loss function is less sensitive to outliers,making it suitable for applications with multidimensional noisy or uncertaindata. Both algorithms are derived based on the novel generalised HR (GHR)calculus, which allows for the differentiation of real functions of quaternionvariables and offers the product and chain rules, thus enabling elegant andcompact derivations. Simulation results in the context of motion prediction ofchest internal markers for lung cancer radiotherapy, which includes regular andirregular breathing sequences, support the analysis.</description><author>Pauline Bourigault, Dongpo Xu, Danilo P. Mandic</author><pubDate>Wed, 03 Apr 2024 16:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14227v2</guid></item><item><title>Elastic Interaction Energy-Informed Real-Time Traffic Scene Perception</title><link>http://arxiv.org/abs/2310.01449v2</link><description>Urban segmentation and lane detection are two important tasks for trafficscene perception. Accuracy and fast inference speed of visual perception arecrucial for autonomous driving safety. Fine and complex geometric objects arethe most challenging but important recognition targets in traffic scene, suchas pedestrians, traffic signs and lanes. In this paper, a simple and efficienttopology-aware energy loss function-based network training strategy namedEIEGSeg is proposed. EIEGSeg is designed for multi-class segmentation onreal-time traffic scene perception. To be specific, the convolutional neuralnetwork (CNN) extracts image features and produces multiple outputs, and theelastic interaction energy loss function (EIEL) drives the predictions movingtoward the ground truth until they are completely overlapped. Our strategyperforms well especially on fine-scale structure, \textit{i.e.} small orirregularly shaped objects can be identified more accurately, and discontinuityissues on slender objects can be improved. We quantitatively and qualitativelyanalyze our method on three traffic datasets, including urban scenesegmentation data Cityscapes and lane detection data TuSimple and CULane. Ourresults demonstrate that EIEGSeg consistently improves the performance,especially on real-time, lightweight networks that are better suited forautonomous driving.</description><author>Yaxin Feng, Yuan Lan, Luchan Zhang, Guoqing Liu, Yang Xiang</author><pubDate>Wed, 03 Apr 2024 16:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01449v2</guid></item><item><title>Generative-Contrastive Heterogeneous Graph Neural Network</title><link>http://arxiv.org/abs/2404.02810v1</link><description>Heterogeneous Graphs (HGs) can effectively model complex relationships in thereal world by multi-type nodes and edges. In recent years, inspired byself-supervised learning, contrastive Heterogeneous Graphs Neural Networks(HGNNs) have shown great potential by utilizing data augmentation anddiscriminators for downstream tasks. However, data augmentation is stilllimited due to the discrete and abstract nature of graphs. To tackle the abovelimitations, we propose a novel \textit{Generative-Contrastive HeterogeneousGraph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneousgraph generative learning enhanced contrastive paradigm. This paradigmincludes: 1) A contrastive view augmentation strategy by using maskedautoencoder. 2) Position-aware and semantics-aware positive sample samplingstrategy for generate hard negative samples. 3) A hierarchical contrastivelearning strategy for capturing local and global information. Furthermore, thehierarchical contrastive learning and sampling strategies aim to constitute anenhanced discriminator under the generative-contrastive perspective. Finally,we compare our model with seventeen baselines on eight real-world datasets. Ourmodel outperforms the latest contrastive and generative baselines on nodeclassification and link prediction tasks. To reproduce our work, we haveopen-sourced our code at https://github.com/xxx.</description><author>Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang</author><pubDate>Wed, 03 Apr 2024 16:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02810v1</guid></item><item><title>Understanding the Learning Dynamics of Alignment with Human Feedback</title><link>http://arxiv.org/abs/2403.18742v2</link><description>Aligning large language models (LLMs) with human intentions has become acritical task for safely deploying models in real-world systems. While existingalignment approaches have seen empirical success, theoretically understandinghow these methods affect model behavior remains an open question. Our workprovides an initial attempt to theoretically analyze the learning dynamics ofhuman preference alignment. We formally show how the distribution of preferencedatasets influences the rate of model updates and provide rigorous guaranteeson the training accuracy. Our theory also reveals an intricate phenomenon wherethe optimization is prone to prioritizing certain behaviors with higherpreference distinguishability. We empirically validate our findings oncontemporary LLMs and alignment tasks, reinforcing our theoretical insights andshedding light on considerations for future alignment approaches. Disclaimer:This paper contains potentially offensive text; reader discretion is advised.</description><author>Shawn Im, Yixuan Li</author><pubDate>Wed, 03 Apr 2024 16:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18742v2</guid></item><item><title>An Optimization Framework to Personalize Passive Cardiac Mechanics</title><link>http://arxiv.org/abs/2404.02807v1</link><description>Personalized cardiac mechanics modeling is a powerful tool for understandingthe biomechanics of cardiac function in health and disease and assisting intreatment planning. However, current models are limited to using medical imagesacquired at a single cardiac phase, often limiting their applicability forprocessing dynamic image acquisitions. This study introduces an inverse finiteelement analysis (iFEA) framework to estimate the passive mechanical propertiesof cardiac tissue using time-dependent medical image data. The iFEA frameworkrelies on a novel nested optimization scheme, in which the outer iterationsutilize a traditional optimization method to best approximate materialparameters that fit image data, while the inner iterations employ an augmentedSellier's algorithm to estimate the stress-free reference configuration. With afocus on characterizing the passive mechanical behavior, the framework employsstructurally based anisotropic hyperelastic constitutive models andphysiologically relevant boundary conditions to simulate myocardial mechanics.We use a stabilized variational multiscale formulation for solving thegoverning nonlinear elastodynamics equations, verified for cardiac mechanicsapplications. The framework is tested in myocardium models of biventricle andleft atrium derived from cardiac phase-resolved computed tomographic (CT)images of a healthy subject and three patients with hypertrophic obstructivecardiomyopathy (HOCM). The impact of the choice of optimization methods andother numerical settings, including fiber direction parameters, mesh size,initial parameters for optimization, and perturbations to optimal materialparameters, is assessed using a rigorous sensitivity analysis. The performanceof the current iFEA is compared against an assumed power-law-basedpressure-volume relation, typically used for single-phase image acquisition.</description><author>Lei Shi, Ian Chen, Hiroo Takayama, Vijay Vedula</author><pubDate>Wed, 03 Apr 2024 16:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02807v1</guid></item><item><title>MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning</title><link>http://arxiv.org/abs/2402.17231v3</link><description>Tool-augmented Large Language Models (TALMs) are known to enhance theskillset of large language models (LLMs), thereby, leading to their improvedreasoning abilities across many tasks. While, TALMs have been successfullyemployed in different question-answering benchmarks, their efficacy on complexmathematical reasoning benchmarks, and the potential complementary benefitsoffered by tools for knowledge retrieval and mathematical equation solving areopen research questions. In this work, we present MathSensei, a tool-augmentedlarge language model for mathematical reasoning. We study the complementarybenefits of the tools - knowledge retriever (Bing Web Search), programgenerator + executor (Python), and symbolic equation solver (Wolfram-Alpha API)through evaluations on mathematical reasoning datasets. We perform exhaustiveablations on MATH, a popular dataset for evaluating mathematical reasoning ondiverse mathematical disciplines. We also conduct experiments involvingwell-known tool planners to study the impact of tool sequencing on the modelperformance. MathSensei achieves 13.5% better accuracy over gpt-3.5-turbo withChain-of-Thought on the MATH dataset. We further observe that TALMs are not aseffective for simpler math word problems (in GSM-8K), and the benefit increasesas the complexity and required knowledge increases (progressively over AQuA,MMLU-Math, and higher level complex questions in MATH). The code and data areavailable at https://github.com/Debrup-61/MathSensei.</description><author>Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni</author><pubDate>Wed, 03 Apr 2024 16:22:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17231v3</guid></item><item><title>Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization</title><link>http://arxiv.org/abs/2403.08730v2</link><description>Multimodal Large Language Models (MLLMs) excel in generating responses basedon visual inputs. However, they often suffer from a bias towards generatingresponses similar to their pretraining corpus, overshadowing the importance ofvisual information. We treat this bias as a "preference" for pretrainingstatistics, which hinders the model's grounding in visual input. To mitigatethis issue, we propose Bootstrapped Preference Optimization (BPO), whichconducts preference learning with datasets containing negative responsesbootstrapped from the model itself. Specifically, we propose the following twostrategies: 1) using distorted image inputs to the MLLM for eliciting responsesthat contain signified pretraining bias; 2) leveraging text-based LLM toexplicitly inject erroneous but common elements into the original response.Those undesirable responses are paired with original annotated responses fromthe datasets to construct the preference dataset, which is subsequentlyutilized to perform preference learning. Our approach effectively suppressespretrained LLM bias, enabling enhanced grounding in visual inputs. Extensiveexperimentation demonstrates significant performance improvements acrossmultiple benchmarks, advancing the state-of-the-art in multimodalconversational systems.</description><author>Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, Tong Zhang</author><pubDate>Wed, 03 Apr 2024 16:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08730v2</guid></item><item><title>The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers</title><link>http://arxiv.org/abs/2404.02806v1</link><description>Evaluation of large language models (LLMs) for code has primarily relied onstatic benchmarks, including HumanEval (Chen et al., 2021), which measure theability of LLMs to generate complete code that passes unit tests. As LLMs areincreasingly used as programmer assistants, we study whether gains on existingbenchmarks translate to gains in programmer productivity when coding with LLMs,including time spent coding. In addition to static benchmarks, we investigatethe utility of preference metrics that might be used as proxies to measure LLMhelpfulness, such as code acceptance or copy rates. To do so, we introduceRealHumanEval, a web interface to measure the ability of LLMs to assistprogrammers, through either autocomplete or chat support. We conducted a userstudy (N=213) using RealHumanEval in which users interacted with six LLMs ofvarying base model performance. Despite static benchmarks not incorporatinghumans-in-the-loop, we find that improvements in benchmark performance lead toincreased programmer productivity; however gaps in benchmark versus humanperformance are not proportional -- a trend that holds across both forms of LLMsupport. In contrast, we find that programmer preferences do not correlate withtheir actual performance, motivating the need for better, human-centric proxysignals. We also open-source RealHumanEval to enable human-centric evaluationof new models and the study data to facilitate efforts to improve code models.</description><author>Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, David Sontag</author><pubDate>Wed, 03 Apr 2024 16:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02806v1</guid></item><item><title>Isometric Multi-Shape Matching</title><link>http://arxiv.org/abs/2012.02689v2</link><description>Finding correspondences between shapes is a fundamental problem in computervision and graphics, which is relevant for many applications, including 3Dreconstruction, object tracking, and style transfer. The vast majority ofcorrespondence methods aim to find a solution between pairs of shapes, even ifmultiple instances of the same class are available. While isometries are oftenstudied in shape correspondence problems, they have not been consideredexplicitly in the multi-matching setting. This paper closes this gap byproposing a novel optimisation formulation for isometric multi-shape matching.We present a suitable optimisation algorithm for solving our formulation andprovide a convergence and complexity analysis. Our algorithm obtainsmulti-matchings that are by construction provably cycle-consistent. Wedemonstrate the superior performance of our method on various datasets and setthe new state-of-the-art in isometric multi-shape matching.</description><author>Maolin Gao, Zorah Lähner, Johan Thunberg, Daniel Cremers, Florian Bernard</author><pubDate>Wed, 03 Apr 2024 16:18:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.02689v2</guid></item><item><title>On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension</title><link>http://arxiv.org/abs/2404.02800v1</link><description>Question Generation aims to automatically generate questions based on a giveninput provided as context. A controllable question generation scheme focuses ongenerating questions with specific attributes, allowing better control. In thisstudy, we propose a few-shot prompting strategy for controlling the generationof question-answer pairs from children's narrative texts. We aim to control twoattributes: the question's explicitness and underlying narrative elements. Withempirical evaluation, we show the effectiveness of controlling the generationprocess by employing few-shot prompting side by side with a reference model.Our experiments highlight instances where the few-shot strategy surpasses thereference model, particularly in scenarios such as semantic closenessevaluation and the diversity and coherency of question-answer pairs. However,these improvements are not always statistically significant. The code ispublicly available at github.com/bernardoleite/few-shot-prompting-qg-control.</description><author>Bernardo Leite, Henrique Lopes Cardoso</author><pubDate>Wed, 03 Apr 2024 16:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02800v1</guid></item><item><title>Global and Local Prompts Cooperation via Optimal Transport for Federated Learning</title><link>http://arxiv.org/abs/2403.00041v2</link><description>Prompt learning in pretrained visual-language models has shown remarkableflexibility across various downstream tasks. Leveraging its inherentlightweight nature, recent research attempted to integrate the powerfulpretrained models into federated learning frameworks to simultaneously reducecommunication costs and promote local training on insufficient data. Despitethese efforts, current federated prompt learning methods lack specializeddesigns to systematically address severe data heterogeneities, e.g., datadistribution with both label and feature shifts involved. To address thischallenge, we present Federated Prompts Cooperation via Optimal Transport(FedOTP), which introduces efficient collaborative prompt learning strategiesto capture diverse category traits on a per-client basis. Specifically, foreach client, we learn a global prompt to extract consensus knowledge amongclients, and a local prompt to capture client-specific categorycharacteristics. Unbalanced Optimal Transport is then employed to align localvisual features with these prompts, striking a balance between global consensusand local personalization. By relaxing one of the equality constraints, FedOTPenables prompts to focus solely on the core regions of image patches. Extensiveexperiments on datasets with various types of heterogeneities have demonstratedthat our FedOTP outperforms the state-of-the-art methods.</description><author>Hongxia Li, Wei Huang, Jingya Wang, Ye Shi</author><pubDate>Wed, 03 Apr 2024 16:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00041v2</guid></item><item><title>Semi-supervised Active Learning for Video Action Detection</title><link>http://arxiv.org/abs/2312.07169v3</link><description>In this work, we focus on label efficient learning for video actiondetection. We develop a novel semi-supervised active learning approach whichutilizes both labeled as well as unlabeled data along with informative sampleselection for action detection. Video action detection requires spatio-temporallocalization along with classification, which poses several challenges for bothactive learning informative sample selection as well as semi-supervisedlearning pseudo label generation. First, we propose NoiseAug, a simpleaugmentation strategy which effectively selects informative samples for videoaction detection. Next, we propose fft-attention, a novel technique based onhigh-pass filtering which enables effective utilization of pseudo label for SSLin video action detection by emphasizing on relevant activity region within avideo. We evaluate the proposed approach on three different benchmark datasets,UCF-101-24, JHMDB-21, and Youtube-VOS. First, we demonstrate its effectivenesson video action detection where the proposed approach outperforms prior worksin semi-supervised and weakly-supervised learning along with several baselineapproaches in both UCF101-24 and JHMDB-21. Next, we also show its effectivenesson Youtube-VOS for video object segmentation demonstrating its generalizationcapability for other dense prediction tasks in videos. The code and models ispublicly available at:\url{https://github.com/AKASH2907/semi-sup-active-learning}.</description><author>Ayush Singh, Aayush J Rana, Akash Kumar, Shruti Vyas, Yogesh Singh Rawat</author><pubDate>Wed, 03 Apr 2024 16:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07169v3</guid></item><item><title>Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness</title><link>http://arxiv.org/abs/2311.09694v2</link><description>Do larger and more performant models resolve NLP's longstanding robustnessissues? We investigate this question using over 20 models of different sizesspanning different architectural choices and pretraining objectives. We conductevaluations using (a) out-of-domain and challenge test sets, (b) behavioraltesting with CheckLists, (c) contrast sets, and (d) adversarial inputs. Ouranalysis reveals that not all out-of-domain tests provide insight intorobustness. Evaluating with CheckLists and contrast sets shows significant gapsin model performance; merely scaling models does not make them adequatelyrobust. Finally, we point out that current approaches for adversarialevaluations of models are themselves problematic: they can be easily thwarted,and in their current forms, do not represent a sufficiently deep probe of modelrobustness. We conclude that not only is the question of robustness in NLP asyet unresolved, but even some of the approaches to measure robustness need tobe reassessed.</description><author>Ashim Gupta, Rishanth Rajendhran, Nathan Stringham, Vivek Srikumar, Ana Marasović</author><pubDate>Wed, 03 Apr 2024 16:07:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09694v2</guid></item><item><title>Text-Driven Image Editing via Learnable Regions</title><link>http://arxiv.org/abs/2311.16432v2</link><description>Language has emerged as a natural interface for image editing. In this paper,we introduce a method for region-based image editing driven by textual prompts,without the need for user-provided masks or sketches. Specifically, ourapproach leverages an existing pre-trained text-to-image model and introduces abounding box generator to identify the editing regions that are aligned withthe textual prompts. We show that this simple approach enables flexible editingthat is compatible with current image generation models, and is able to handlecomplex prompts featuring multiple objects, complex sentences, or lengthyparagraphs. We conduct an extensive user study to compare our method againststate-of-the-art methods. The experiments demonstrate the competitiveperformance of our method in manipulating images with high fidelity and realismthat correspond to the provided language descriptions. Our project webpage canbe found at: https://yuanze-lin.me/LearnableRegions_page.</description><author>Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, Ming-Hsuan Yang</author><pubDate>Wed, 03 Apr 2024 16:05:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16432v2</guid></item><item><title>SIGMA: Scale-Invariant Global Sparse Shape Matching</title><link>http://arxiv.org/abs/2308.08393v2</link><description>We propose a novel mixed-integer programming (MIP) formulation for generatingprecise sparse correspondences for highly non-rigid shapes. To this end, weintroduce a projected Laplace-Beltrami operator (PLBO) which combines intrinsicand extrinsic geometric information to measure the deformation quality inducedby predicted correspondences. We integrate the PLBO, together with anorientation-aware regulariser, into a novel MIP formulation that can be solvedto global optimality for many practical problems. In contrast to previousmethods, our approach is provably invariant to rigid transformations and globalscaling, initialisation-free, has optimality guarantees, and scales to highresolution meshes with (empirically observed) linear time. We showstate-of-the-art results for sparse non-rigid matching on several challenging3D datasets, including data with inconsistent meshing, as well as applicationsin mesh-to-point-cloud matching.</description><author>Maolin Gao, Paul Roetzer, Marvin Eisenberger, Zorah Lähner, Michael Moeller, Daniel Cremers, Florian Bernard</author><pubDate>Wed, 03 Apr 2024 16:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08393v2</guid></item><item><title>Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning</title><link>http://arxiv.org/abs/2210.01708v3</link><description>Federated learning (FL) has emerged as a promising paradigm for enabling thecollaborative training of models without centralized access to the raw data onlocal devices. In the typical FL paradigm (e.g., FedAvg), model weights aresent to and from the server each round to participating clients. Recently, theuse of small pre-trained models has been shown effective in federated learningoptimization and improving convergence. However, recent state-of-the-artpre-trained models are getting more capable but also have more parameters. Inconventional FL, sharing the enormous model weights can quickly put a massivecommunication burden on the system, especially if more capable models areemployed. Can we find a solution to enable those strong and readily-availablepre-trained models in FL to achieve excellent performance while simultaneouslyreducing the communication burden? To this end, we investigate the use ofparameter-efficient fine-tuning in federated learning and thus introduce a newframework: FedPEFT. Specifically, we systemically evaluate the performance ofFedPEFT across a variety of client stability, data distribution, anddifferential privacy settings. By only locally tuning and globally sharing asmall portion of the model weights, significant reductions in the totalcommunication overhead can be achieved while maintaining competitive or evenbetter performance in a wide range of federated learning scenarios, providinginsight into a new paradigm for practical and effective federated systems.</description><author>Guangyu Sun, Umar Khalid, Matias Mendieta, Taojiannan Yang, Chen Chen</author><pubDate>Wed, 03 Apr 2024 16:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01708v3</guid></item><item><title>Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</title><link>http://arxiv.org/abs/2402.14505v3</link><description>Recent studies show that vision models pre-trained in generic visual learningtasks with large-scale data can provide useful feature representations for awide range of visual perception problems. However, few attempts have been madeto exploit pre-trained foundation models in visual place recognition (VPR). Dueto the inherent difference in training objectives and data between the tasks ofmodel pre-training and VPR, how to bridge the gap and fully unleash thecapability of pre-trained models for VPR is still a key issue to address. Tothis end, we propose a novel method to realize seamless adaptation ofpre-trained models for VPR. Specifically, to obtain both global and localfeatures that focus on salient landmarks for discriminating places, we design ahybrid adaptation method to achieve both global and local adaptationefficiently, in which only lightweight adapters are tuned without adjusting thepre-trained model. Besides, to guide effective adaptation, we propose a mutualnearest neighbor local feature loss, which ensures proper dense local featuresare produced for local matching and avoids time-consuming spatial verificationin re-ranking. Experimental results show that our method outperforms thestate-of-the-art methods with less training data and training time, and usesabout only 3% retrieval runtime of the two-stage VPR methods with RANSAC-basedspatial verification. It ranks 1st on the MSLS challenge leaderboard (at thetime of submission). The code is released athttps://github.com/Lu-Feng/SelaVPR.</description><author>Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun Yuan</author><pubDate>Wed, 03 Apr 2024 15:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14505v3</guid></item><item><title>MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation</title><link>http://arxiv.org/abs/2404.02790v1</link><description>Text-to-image generation has achieved astonishing results, yet precisespatial controllability and prompt fidelity remain highly challenging. Thislimitation is typically addressed through cumbersome prompt engineering, scenelayout conditioning, or image editing techniques which often require hand drawnmasks. Nonetheless, pre-existing works struggle to take advantage of thenatural instance-level compositionality of scenes due to the typically flatnature of rasterized RGB output images. Towards adressing this challenge, weintroduce MuLAn: a novel dataset comprising over 44K MUlti-Layer ANnotations ofRGB images as multilayer, instance-wise RGBA decompositions, and over 100Kinstance images. To build MuLAn, we developed a training free pipeline whichdecomposes a monocular RGB image into a stack of RGBA layers comprising ofbackground and isolated instances. We achieve this through the use ofpretrained general-purpose models, and by developing three modules: imagedecomposition for instance discovery and extraction, instance completion toreconstruct occluded areas, and image re-assembly. We use our pipeline tocreate MuLAn-COCO and MuLAn-LAION datasets, which contain a variety of imagedecompositions in terms of style, composition and complexity. With MuLAn, weprovide the first photorealistic resource providing instance decomposition andocclusion information for high quality images, opening up new avenues fortext-to-image generative AI research. With this, we aim to encourage thedevelopment of novel generation and editing technology, in particularlayer-wise solutions. MuLAn data resources are available athttps://MuLAn-dataset.github.io/.</description><author>Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, Sarah Parisot</author><pubDate>Wed, 03 Apr 2024 15:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02790v1</guid></item><item><title>Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map based Complex-valued Precoding Network Approach</title><link>http://arxiv.org/abs/2312.02184v2</link><description>As the demand for high-quality services proliferates, an innovative networkarchitecture, the fully-decoupled RAN (FD-RAN), has emerged for more flexiblespectrum resource utilization and lower network costs. However, with thedecoupling of uplink base stations and downlink base stations in FD-RAN, thetraditional transmission mechanism, which relies on real-time channel feedback,is not suitable as the receiver is not able to feedback accurate and timelychannel state information to the transmitter. This paper proposes a noveltransmission scheme without relying on physical layer channel feedback.Specifically, we design a radio map based complex-valued precodingnetwork~(RMCPNet) model, which outputs the base station precoding based on userlocation. RMCPNet comprises multiple subnets, with each subnet responsible forextracting unique modal features from diverse input modalities. Furthermore,the multi-modal embeddings derived from these distinct subnets are integratedwithin the information fusion layer, culminating in a unified representation.We also develop a specific RMCPNet training algorithm that employs the negativespectral efficiency as the loss function. We evaluate the performance of theproposed scheme on the public DeepMIMO dataset and show that RMCPNet canachieve 16\% and 76\% performance improvements over the conventionalreal-valued neural network and statistical codebook approach, respectively.</description><author>Jiwei Zhao, Jiacheng Chen, Zeyu Sun, Yuhang Shi, Haibo Zhou, Xuemin, Shen</author><pubDate>Wed, 03 Apr 2024 15:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02184v2</guid></item><item><title>GenN2N: Generative NeRF2NeRF Translation</title><link>http://arxiv.org/abs/2404.02788v1</link><description>We present GenN2N, a unified NeRF-to-NeRF translation framework for variousNeRF translation tasks such as text-driven NeRF editing, colorization,super-resolution, inpainting, etc. Unlike previous methods designed forindividual translation tasks with task-specific schemes, GenN2N achieves allthese NeRF editing tasks by employing a plug-and-play image-to-image translatorto perform editing in the 2D domain and lifting 2D edits into the 3D NeRFspace. Since the 3D consistency of 2D edits may not be assured, we propose tomodel the distribution of the underlying 3D edits through a generative modelthat can cover all possible edited NeRFs. To model the distribution of 3Dedited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodesimages while decoding NeRFs. The latent space is trained to align with aGaussian distribution and the NeRFs are supervised through an adversarial losson its renderings. To ensure the latent code does not depend on 2D viewpointsbut truly reflects the 3D edits, we also regularize the latent code through acontrastive learning scheme. Extensive experiments on various editing tasksshow GenN2N, as a universal framework, performs as well or better thantask-specific specialists while possessing flexible generative power. Moreresults on our project page: https://xiangyueliu.github.io/GenN2N/</description><author>Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan, Li Yi</author><pubDate>Wed, 03 Apr 2024 15:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02788v1</guid></item><item><title>Domain Generalization through Meta-Learning: A Survey</title><link>http://arxiv.org/abs/2404.02785v1</link><description>Deep neural networks (DNNs) have revolutionized artificial intelligence butoften lack performance when faced with out-of-distribution (OOD) data, a commonscenario due to the inevitable domain shifts in real-world applications. Thislimitation stems from the common assumption that training and testing datashare the same distribution-an assumption frequently violated in practice.Despite their effectiveness with large amounts of data and computational power,DNNs struggle with distributional shifts and limited labeled data, leading tooverfitting and poor generalization across various tasks and domains.Meta-learning presents a promising approach by employing algorithms thatacquire transferable knowledge across various tasks for fast adaptation,eliminating the need to learn each task from scratch. This survey paper delvesinto the realm of meta-learning with a focus on its contribution to domaingeneralization. We first clarify the concept of meta-learning for domaingeneralization and introduce a novel taxonomy based on the feature extractionstrategy and the classifier learning methodology, offering a granular view ofmethodologies. Through an exhaustive review of existing methods and underlyingtheories, we map out the fundamentals of the field. Our survey providespractical insights and an informed discussion on promising research directions,paving the way for future innovation in meta-learning for domaingeneralization.</description><author>Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt</author><pubDate>Wed, 03 Apr 2024 15:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02785v1</guid></item><item><title>Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search</title><link>http://arxiv.org/abs/2206.00702v9</link><description>Complex reasoning problems contain states that vary in the computational costrequired to determine a good action plan. Taking advantage of this property, wepropose Adaptive Subgoal Search (AdaSubS), a search method that adaptivelyadjusts the planning horizon. To this end, AdaSubS generates diverse sets ofsubgoals at different distances. A verification mechanism is employed to filterout unreachable subgoals swiftly, allowing to focus on feasible furthersubgoals. In this way, AdaSubS benefits from the efficiency of planning withlonger subgoals and the fine control with the shorter ones, and thus scaleswell to difficult planning problems. We show that AdaSubS significantlysurpasses hierarchical planning algorithms on three complex reasoning tasks:Sokoban, the Rubik's Cube, and inequality proving benchmark INT.</description><author>Michał Zawalski, Michał Tyrolski, Konrad Czechowski, Tomasz Odrzygóźdź, Damian Stachura, Piotr Piękos, Yuhuai Wu, Łukasz Kuciński, Piotr Miłoś</author><pubDate>Wed, 03 Apr 2024 15:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.00702v9</guid></item><item><title>Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation</title><link>http://arxiv.org/abs/2310.17463v2</link><description>Treatment effect estimation in continuous time is crucial for personalizedmedicine. However, existing methods for this task are limited to pointestimates of the potential outcomes, whereas uncertainty estimates have beenignored. Needless to say, uncertainty quantification is crucial for reliabledecision-making in medical applications. To fill this gap, we propose a novelBayesian neural controlled differential equation (BNCDE) for treatment effectestimation in continuous time. In our BNCDE, the time dimension is modeledthrough a coupled system of neural controlled differential equations and neuralstochastic differential equations, where the neural stochastic differentialequations allow for tractable variational Bayesian inference. Thereby, for anassigned sequence of treatments, our BNCDE provides meaningful posteriorpredictive distributions of the potential outcomes. To the best of ourknowledge, ours is the first tailored neural method to provide uncertaintyestimates of treatment effects in continuous time. As such, our method is ofdirect practical value for promoting reliable decision-making in medicine.</description><author>Konstantin Hess, Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</author><pubDate>Wed, 03 Apr 2024 15:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17463v2</guid></item><item><title>CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective Models on French Biomedical Data</title><link>http://arxiv.org/abs/2306.15550v3</link><description>Clinical data in hospitals are increasingly accessible for research throughclinical data warehouses. However these documents are unstructured and it istherefore necessary to extract information from medical reports to conductclinical studies. Transfer learning with BERT-like models such as CamemBERT hasallowed major advances for French, especially for named entity recognition.However, these models are trained for plain language and are less efficient onbiomedical data. Addressing this gap, we introduce CamemBERT-bio, a dedicatedFrench biomedical model derived from a new public French biomedical dataset.Through continual pre-training of the original CamemBERT, CamemBERT-bioachieves an improvement of 2.54 points of F1-score on average across variousbiomedical named entity recognition tasks, reinforcing the potential ofcontinual pre-training as an equally proficient yet less computationallyintensive alternative to training from scratch. Additionally, we highlight theimportance of using a standard evaluation protocol that provides a clear viewof the current state-of-the-art for French biomedical models.</description><author>Rian Touchent, Laurent Romary, Eric de la Clergerie</author><pubDate>Wed, 03 Apr 2024 15:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15550v3</guid></item><item><title>Federated Computing -- Survey on Building Blocks, Extensions and Systems</title><link>http://arxiv.org/abs/2404.02779v1</link><description>In response to the increasing volume and sensitivity of data, traditionalcentralized computing models face challenges, such as data security breachesand regulatory hurdles. Federated Computing (FC) addresses these concerns byenabling collaborative processing without compromising individual data privacy.This is achieved through a decentralized network of devices, each retainingcontrol over its data, while participating in collective computations. Themotivation behind FC extends beyond technical considerations to encompasssocietal implications. As the need for responsible AI and ethical datapractices intensifies, FC aligns with the principles of user empowerment anddata sovereignty. FC comprises of Federated Learning (FL) and FederatedAnalytics (FA). FC systems became more complex over time and they currentlylack a clear definition and taxonomy describing its moving pieces. Currentsurveys capture domain-specific FL use cases, describe individual components inan FC pipeline individually or decoupled from each other, or provide aquantitative overview of the number of published papers. This work surveys morethan 150 papers to distill the underlying structure of FC systems with theirbasic building blocks, extensions, architecture, environment, and motivation.We capture FL and FA systems individually and point out unique differencebetween those two.</description><author>René Schwermer, Ruben Mayer, Hans-Arno Jacobsen</author><pubDate>Wed, 03 Apr 2024 15:47:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02779v1</guid></item><item><title>NEAT: Distilling 3D Wireframes from Neural Attraction Fields</title><link>http://arxiv.org/abs/2307.10206v2</link><description>This paper studies the problem of structured 3D reconstruction usingwireframes that consist of line segments and junctions, focusing on thecomputation of structured boundary geometries of scenes. Instead of leveragingmatching-based solutions from 2D wireframes (or line segments) for 3D wireframereconstruction as done in prior arts, we present NEAT, a rendering-distillingformulation using neural fields to represent 3D line segments with 2Dobservations, and bipartite matching for perceiving and distilling of a sparseset of 3D global junctions. The proposed {NEAT} enjoys the joint optimizationof the neural fields and the global junctions from scratch, usingview-dependent 2D observations without precomputed cross-view feature matching.Comprehensive experiments on the DTU and BlendedMVS datasets demonstrate ourNEAT's superiority over state-of-the-art alternatives for 3D wireframereconstruction. Moreover, the distilled 3D global junctions by NEAT, are abetter initialization than SfM points, for the recently-emerged 3D GaussianSplatting for high-fidelity novel view synthesis using about 20 times fewerinitial 3D points. Project page: \url{https://xuenan.net/neat}.</description><author>Nan Xue, Bin Tan, Yuxi Xiao, Liang Dong, Gui-Song Xia, Tianfu Wu, Yujun Shen</author><pubDate>Wed, 03 Apr 2024 15:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10206v2</guid></item><item><title>FPT: Feature Prompt Tuning for Few-shot Readability Assessment</title><link>http://arxiv.org/abs/2404.02772v1</link><description>Prompt-based methods have achieved promising results in most few-shot textclassification tasks. However, for readability assessment tasks, traditionalprompt methods lackcrucial linguistic knowledge, which has already been provento be essential. Moreover, previous studies on utilizing linguistic featureshave shown non-robust performance in few-shot settings and may even impairmodel performance.To address these issues, we propose a novel prompt-basedtuning framework that incorporates rich linguistic knowledge, called FeaturePrompt Tuning (FPT). Specifically, we extract linguistic features from the textand embed them into trainable soft prompts. Further, we devise a new lossfunction to calibrate the similarity ranking order between categories.Experimental results demonstrate that our proposed method FTP not only exhibitsa significant performance improvement over the prior best prompt-based tuningapproaches, but also surpasses the previous leading methods that incorporatelinguistic features. Also, our proposed model significantly outperforms thelarge language model gpt-3.5-turbo-16k in most cases. Our proposed methodestablishes a new architecture for prompt tuning that sheds light on howlinguistic features can be easily adapted to linguistic-related tasks.</description><author>Ziyang Wang, Sanwoo Lee, Hsiu-Yuan Huang, Yunfang Wu</author><pubDate>Wed, 03 Apr 2024 15:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02772v1</guid></item><item><title>RadEdit: stress-testing biomedical vision models via diffusion image editing</title><link>http://arxiv.org/abs/2312.12865v3</link><description>Biomedical imaging datasets are often small and biased, meaning thatreal-world performance of predictive models can be substantially lower thanexpected from internal testing. This work proposes using generative imageediting to simulate dataset shifts and diagnose failure modes of biomedicalvision models; this can be used in advance of deployment to assess readiness,potentially reducing cost and patient harm. Existing editing methods canproduce undesirable changes, with spurious correlations learned due to theco-occurrence of disease and treatment interventions, limiting practicalapplicability. To address this, we train a text-to-image diffusion model onmultiple chest X-ray datasets and introduce a new editing method RadEdit thatuses multiple masks, if present, to constrain changes and ensure consistency inthe edited images. We consider three types of dataset shifts: acquisitionshift, manifestation shift, and population shift, and demonstrate that ourapproach can diagnose failures and quantify model robustness without additionaldata collection, complementing more qualitative tools for explainable AI.</description><author>Fernando Pérez-García, Sam Bond-Taylor, Pedro P. Sanchez, Boris van Breugel, Daniel C. Castro, Harshita Sharma, Valentina Salvatelli, Maria T. A. Wetscherek, Hannah Richardson, Matthew P. Lungren, Aditya Nori, Javier Alvarez-Valle, Ozan Oktay, Maximilian Ilse</author><pubDate>Wed, 03 Apr 2024 15:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12865v3</guid></item><item><title>Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning</title><link>http://arxiv.org/abs/2404.00686v2</link><description>Accounting for the uncertainty of value functions boosts exploration inReinforcement Learning (RL). Our work introduces Maximum Mean DiscrepancyQ-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertaintypropagation during Temporal Difference (TD) updates. MMD-QL uses the MMDbarycenter for this purpose, as MMD provides a tighter estimate of closenessbetween probability measures than the Wasserstein distance. Firstly, weestablish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) underthe average loss metric. Concerning the accumulated rewards, experiments ontabular environments show that MMD-QL outperforms WQL and other algorithms.Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network(MMD-QN). Making reasonable assumptions, we analyze the convergence rates ofMMD-QN using function approximation. Empirical results on challenging Atarigames demonstrate that MMD-QN performs well compared to benchmark deep RLalgorithms, highlighting its effectiveness in handling large state-actionspaces.</description><author>Srinjoy Roy, Swagatam Das</author><pubDate>Wed, 03 Apr 2024 15:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00686v2</guid></item><item><title>DriftRec: Adapting diffusion models to blind JPEG restoration</title><link>http://arxiv.org/abs/2211.06757v3</link><description>In this work, we utilize the high-fidelity generation abilities of diffusionmodels to solve blind JPEG restoration at high compression levels. We proposean elegant modification of the forward stochastic differential equation ofdiffusion models to adapt them to this restoration task and name our methodDriftRec. Comparing DriftRec against an $L_2$ regression baseline with the samenetwork architecture and state-of-the-art techniques for JPEG restoration, weshow that our approach can escape the tendency of other methods to generateblurry images, and recovers the distribution of clean images significantly morefaithfully. For this, only a dataset of clean/corrupted image pairs and noknowledge about the corruption operation is required, enabling widerapplicability to other restoration tasks. In contrast to other conditional andunconditional diffusion models, we utilize the idea that the distributions ofclean and corrupted images are much closer to each other than each is to theusual Gaussian prior of the reverse process in diffusion models. Our approachtherefore requires only low levels of added noise and needs comparatively fewsampling steps even without further optimizations. We show that DriftRecnaturally generalizes to realistic and difficult scenarios such as unaligneddouble JPEG compression and blind restoration of JPEGs found online, withouthaving encountered such examples during training.</description><author>Simon Welker, Henry N. Chapman, Timo Gerkmann</author><pubDate>Wed, 03 Apr 2024 15:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06757v3</guid></item><item><title>Breeze-7B Technical Report</title><link>http://arxiv.org/abs/2403.02712v2</link><description>Breeze-7B is an open-source language model based on Mistral-7B, designed toaddress the need for improved language comprehension and chatbot-orientedcapabilities in Traditional Chinese. This technical report provides an overviewof the additional pretraining, finetuning, and evaluation stages for theBreeze-7B model. The Breeze-7B family of base and chat models exhibits goodperformance on language comprehension and chatbot-oriented tasks, reaching thetop in several benchmarks among models comparable in its complexity class.</description><author>Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang Chen, Da-Shan Shiu</author><pubDate>Wed, 03 Apr 2024 15:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02712v2</guid></item><item><title>Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck</title><link>http://arxiv.org/abs/2310.19660v2</link><description>Black-box deep neural networks excel in text classification, yet theirapplication in high-stakes domains is hindered by their lack ofinterpretability. To address this, we propose Text Bottleneck Models (TBM), anintrinsically interpretable text classification framework that offers bothglobal and local explanations. Rather than directly predicting the outputlabel, TBM predicts categorical values for a sparse set of salient concepts anduses a linear layer over those concept values to produce the final prediction.These concepts can be automatically discovered and measured by a Large LanguageModel (LLM) without the need for human curation. Experiments on 12 diverse textunderstanding datasets demonstrate that TBM can rival the performance ofblack-box baselines such as few-shot GPT-4 and finetuned DeBERTa while fallingshort against finetuned GPT-3.5. Comprehensive human evaluation validates thatTBM can generate high-quality concepts relevant to the task, and the conceptmeasurement aligns well with human judgments, suggesting that the predictionsmade by TBMs are interpretable. Overall, our findings suggest that TBM is apromising new framework that enhances interpretability with minimal performancetradeoffs.</description><author>Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, Chris Callison-Burch</author><pubDate>Wed, 03 Apr 2024 15:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19660v2</guid></item><item><title>Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates</title><link>http://arxiv.org/abs/2311.13447v2</link><description>We study private empirical risk minimization (ERM) problem for lossessatisfying the $(\gamma,\kappa)$-Kurdyka-{\L}ojasiewicz (KL) condition. ThePolyak-{\L}ojasiewicz (PL) condition is a special case of this condition when$\kappa=2$. Specifically, we study this problem under the constraint of $\rho$zero-concentrated differential privacy (zCDP). When $\kappa\in[1,2]$ and theloss function is Lipschitz and smooth over a sufficiently large region, weprovide a new algorithm based on variance reduced gradient descent thatachieves the rate$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ on theexcess empirical risk, where $n$ is the dataset size and $d$ is the dimension.We further show that this rate is nearly optimal. When $\kappa \geq 2$ and theloss is instead Lipschitz and weakly convex, we show it is possible to achievethe rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$with a private implementation of the proximal point method. When the KLparameters are unknown, we provide a novel modification and analysis of thenoisy gradient descent algorithm and show that this algorithm achieves a rateof$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{\frac{2\kappa}{4-\kappa}}\big)$adaptively, which is nearly optimal when $\kappa = 2$. We further show that,without assuming the KL condition, the same gradient descent algorithm canachieve fast convergence to a stationary point when the gradient stayssufficiently large during the run of the algorithm. Specifically, we show thatthis algorithm can approximate stationary points of Lipschitz, smooth (andpossibly nonconvex) objectives with rate as fast as$\tilde{O}\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)$ and never worse than$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{1/2}\big)$. The latterrate matches the best known rate for methods that do not rely on variancereduction.</description><author>Michael Menart, Enayat Ullah, Raman Arora, Raef Bassily, Cristóbal Guzmán</author><pubDate>Wed, 03 Apr 2024 15:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13447v2</guid></item><item><title>"Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students</title><link>http://arxiv.org/abs/2402.01687v2</link><description>This study evaluates the effectiveness of various large language models(LLMs) in performing tasks common among undergraduate computer sciencestudents. Although a number of research studies in the computing educationcommunity have explored the possibility of using LLMs for a variety of tasks,there is a lack of comprehensive research comparing different LLMs andevaluating which LLMs are most effective for different tasks. Our researchsystematically assesses some of the publicly available LLMs such as GoogleBard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diversetasks commonly encountered by undergraduate computer science students in India.These tasks include code explanation and documentation, solving classassignments, technical interview preparation, learning new concepts andframeworks, and email writing. Evaluation for these tasks was carried out bypre-final year and final year undergraduate computer science students andprovides insights into the models' strengths and limitations. This study aimsto guide students as well as instructors in selecting suitable LLMs for anyspecific task and offers valuable insights on how LLMs can be usedconstructively by students and instructors.</description><author>Vibhor Agarwal, Madhav Krishan Garg, Sahiti Dharmavaram, Dhruv Kumar</author><pubDate>Wed, 03 Apr 2024 15:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01687v2</guid></item><item><title>Nested Event Extraction upon Pivot Element Recogniton</title><link>http://arxiv.org/abs/2309.12960v2</link><description>Nested Event Extraction (NEE) aims to extract complex event structures wherean event contains other events as its arguments recursively. Nested eventsinvolve a kind of Pivot Elements (PEs) that simultaneously act as arguments ofouter-nest events and as triggers of inner-nest events, and thus connect theminto nested structures. This special characteristic of PEs brings challenges toexisting NEE methods, as they cannot well cope with the dual identities of PEs.Therefore, this paper proposes a new model, called PerNee, which extractsnested events mainly based on recognizing PEs. Specifically, PerNee firstrecognizes the triggers of both inner-nest and outer-nest events and furtherrecognizes the PEs via classifying the relation type between trigger pairs. Themodel uses prompt learning to incorporate information from both event types andargument roles for better trigger and argument representations to improve NEEperformance. Since existing NEE datasets (e.g., Genia11) are limited tospecific domains and contain a narrow range of event types with nestedstructures, we systematically categorize nested events in the generic domainand construct a new NEE dataset, called ACE2005-Nest. Experimental resultsdemonstrate that PerNee consistently achieves state-of-the-art performance onACE2005-Nest, Genia11, and Genia13. The ACE2005-Nest dataset and the code ofthe PerNee model are available at https://github.com/waysonren/PerNee.</description><author>Weicheng Ren, Zixuan Li, Xiaolong Jin, Long Bai, Miao Su, Yantao Liu, Saiping Guan, Jiafeng Guo, Xueqi Cheng</author><pubDate>Wed, 03 Apr 2024 15:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12960v2</guid></item><item><title>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2312.02145v2</link><description>Monocular depth estimation is a fundamental computer vision task. Recovering3D depth from a single image is geometrically ill-posed and requires sceneunderstanding, so it is not surprising that the rise of deep learning has ledto a breakthrough. The impressive progress of monocular depth estimators hasmirrored the growth in model capacity, from relatively modest CNNs to largeTransformer architectures. Still, monocular depth estimators tend to strugglewhen presented with images with unfamiliar content and layout, since theirknowledge of the visual world is restricted by the data seen during training,and challenged by zero-shot generalization to new domains. This motivates us toexplore whether the extensive priors captured in recent generative diffusionmodels can enable better, more generalizable depth estimation. We introduceMarigold, a method for affine-invariant monocular depth estimation that isderived from Stable Diffusion and retains its rich prior knowledge. Theestimator can be fine-tuned in a couple of days on a single GPU using onlysynthetic training data. It delivers state-of-the-art performance across a widerange of datasets, including over 20% performance gains in specific cases.Project page: https://marigoldmonodepth.github.io.</description><author>Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler</author><pubDate>Wed, 03 Apr 2024 15:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02145v2</guid></item><item><title>Price-Discrimination Game for Distributed Resource Management in Federated Learning</title><link>http://arxiv.org/abs/2308.13838v6</link><description>In vanilla federated learning (FL) such as FedAvg, the parameter server (PS)and multiple distributed clients can form a typical buyer's market, where thenumber of PS/buyers of FL services is far less than the number ofclients/sellers. In order to improve the performance of FL and reduce the costof motivating clients to participate in FL, this paper proposes todifferentiate the pricing for services provided by different clients ratherthan simply providing the same service pricing for different clients. The priceis differentiated based on the performance improvements brought to FL and theirheterogeneity in computing and communication capabilities. To this end, aprice-discrimination game (PDG) is formulated to comprehensively address thedistributed resource management problems in FL, including multi-objectivetrade-off, client selection, and incentive mechanism. As the PDG is amixed-integer nonlinear programming (MINLP) problem, a distributedsemi-heuristic algorithm with low computational complexity and lowcommunication overhead is designed to solve it. The simulation result verifiesthe effectiveness of the proposed approach.</description><author>Han Zhang, Halvin Yang, Guopeng Zhang</author><pubDate>Wed, 03 Apr 2024 15:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13838v6</guid></item><item><title>UniverSLU: Universal Spoken Language Understanding for Diverse Tasks with Natural Language Instructions</title><link>http://arxiv.org/abs/2310.02973v2</link><description>Recent studies leverage large language models with multi-taskingcapabilities, using natural language prompts to guide the model's behavior andsurpassing performance of task-specific models. Motivated by this, we ask: canwe build a single model that jointly performs various spoken languageunderstanding (SLU) tasks? We start by adapting a pre-trained automatic speechrecognition model to additional tasks using single-token task specifiers. Weenhance this approach through instruction tuning, i.e., finetuning bydescribing the task using natural language instructions followed by the list oflabel options. Our approach can generalize to new task descriptions for theseen tasks during inference, thereby enhancing its user-friendliness. Wedemonstrate the efficacy of our single multi-task learning model "UniverSLU"for 12 speech classification and sequence generation task types spanning 17datasets and 9 languages. On most tasks, UniverSLU achieves competitiveperformance and often even surpasses task-specific models. Additionally, weassess the zero-shot capabilities, finding that the model generalizes to newdatasets and languages for seen task types.</description><author>Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Karen Livescu, Shinji Watanabe</author><pubDate>Wed, 03 Apr 2024 15:12:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02973v2</guid></item><item><title>Learnable Weight Initialization for Volumetric Medical Image Segmentation</title><link>http://arxiv.org/abs/2306.09320v4</link><description>Hybrid volumetric medical image segmentation models, combining the advantagesof local convolution and global attention, have recently received considerableattention. While mainly focusing on architectural modifications, most existinghybrid approaches still use conventional data-independent weight initializationschemes which restrict their performance due to ignoring the inherentvolumetric nature of the medical data. To address this issue, we propose alearnable weight initialization approach that utilizes the available medicaltraining data to effectively learn the contextual and structural cues via theproposed self-supervised objectives. Our approach is easy to integrate into anyhybrid model and requires no external training data. Experiments on multi-organand lung cancer segmentation tasks demonstrate the effectiveness of ourapproach, leading to state-of-the-art segmentation performance. Our proposeddata-dependent initialization approach performs favorably as compared to theSwin-UNETR model pretrained using large-scale datasets on multi-organsegmentation task. Our source code and models are available at:https://github.com/ShahinaKK/LWI-VMS.</description><author>Shahina Kunhimon, Abdelrahman Shaker, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Wed, 03 Apr 2024 15:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09320v4</guid></item></channel></rss>