<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 30 Sep 2024 01:01:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>BeanCounter: A low-toxicity, large-scale, and open dataset of business-oriented text</title><link>http://arxiv.org/abs/2409.17827v2</link><description>Many of the recent breakthroughs in language modeling have resulted fromscaling effectively the same model architecture to larger datasets. In thisvein, recent work has highlighted performance gains from increasing trainingdataset size and quality, suggesting a need for novel sources of large-scaledatasets. In this work, we introduce BeanCounter, a public dataset consistingof more than 159B tokens extracted from businesses' disclosures. We show thatthis data is indeed novel: less than 0.1% of BeanCounter appears in CommonCrawl-based datasets and it is an order of magnitude larger than datasetsrelying on similar sources. Given the data's provenance, we hypothesize thatBeanCounter is comparatively more factual and less toxic than web-baseddatasets. Exploring this hypothesis, we find that many demographic identitiesoccur with similar prevalence in BeanCounter but with significantly less toxiccontext relative to other datasets. To demonstrate the utility of BeanCounter,we evaluate and compare two LLMs continually pre-trained on BeanCounter withtheir base models. We find an 18-33% reduction in toxic generation and improvedperformance within the finance domain for the continually pretrained models.Collectively, our work suggests that BeanCounter is a novel source oflow-toxicity and high-quality domain-specific data with sufficient scale totrain multi-billion parameter LLMs.</description><author>Siyan Wang, Bradford Levy</author><pubDate>Fri, 27 Sep 2024 16:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17827v2</guid></item><item><title>A New Dataset for Monocular Depth Estimation Under Viewpoint Shifts</title><link>http://arxiv.org/abs/2409.17851v2</link><description>Monocular depth estimation is a critical task for autonomous driving and manyother computer vision applications. While significant progress has been made inthis field, the effects of viewpoint shifts on depth estimation models remainlargely underexplored. This paper introduces a novel dataset and evaluationmethodology to quantify the impact of different camera positions andorientations on monocular depth estimation performance. We propose a groundtruth strategy based on homography estimation and object detection, eliminatingthe need for expensive lidar sensors. We collect a diverse dataset of roadscenes from multiple viewpoints and use it to assess the robustness of a moderndepth estimation model to geometric shifts. After assessing the validity of ourstrategy on a public dataset, we provide valuable insights into the limitationsof current models and highlight the importance of considering viewpointvariations in real-world applications.</description><author>Aurel Pjetri, Stefano Caprasecca, Leonardo Taccari, Matteo Simoncini, Henrique Pi√±eiro Monteagudo, Walter Wallace, Douglas Coimbra de Andrade, Francesco Sambo, Andrew David Bagdanov</author><pubDate>Fri, 27 Sep 2024 15:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17851v2</guid></item><item><title>VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection</title><link>http://arxiv.org/abs/2409.16225v3</link><description>Video anomaly detection (VAD) is a crucial task in video analysis andsurveillance within computer vision. Currently, VAD is gaining attention withmemory techniques that store the features of normal frames. The stored featuresare utilized for frame reconstruction, identifying an abnormality when asignificant difference exists between the reconstructed and input frames.However, this approach faces several challenges due to the simultaneousoptimization required for both the memory and encoder-decoder model. Thesechallenges include increased optimization difficulty, complexity ofimplementation, and performance variability depending on the memory size. Toaddress these challenges,we propose an effective memory method for VAD, calledVideoPatchCore. Inspired by PatchCore, our approach introduces a structure thatprioritizes memory optimization and configures three types of memory tailoredto the characteristics of video data. This method effectively addresses thelimitations of existing memory-based methods, achieving good performancecomparable to state-of-the-art methods. Furthermore, our method requires notraining and is straightforward to implement, making VAD tasks more accessible.Our code is available online at github.com/SkiddieAhn/Paper-VideoPatchCore.</description><author>Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sanghyun Park</author><pubDate>Fri, 27 Sep 2024 14:04:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16225v3</guid></item><item><title>Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</title><link>http://arxiv.org/abs/2403.01432v4</link><description>Language Models (LMs) memorize a vast amount of factual knowledge, exhibitingstrong performance across diverse tasks and domains. However, it has beenobserved that the performance diminishes when dealing with less-popular orlow-frequency concepts and entities, for example in domain specificapplications. The two prominent approaches to enhance the performance of LMs onlow-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning(FT) over synthetic data. This paper explores and evaluates the impact of RAGand FT on customizing LMs in handling low-frequency entities on questionanswering tasks. We conduct extensive experiments on twelve LMs of varying sizeand type and different fine tuning, data augmentation, and retrieval models.Our findings indicate that while FT boosts the performance across entities ofvarying popularity, RAG surpasses FT by a large margin particularly for leastpopular factual knowledge. Additionally, the success of both RAG and FTapproaches is amplified by improving retrieval and data augmentationtechniques. Fine tuning, while beneficial for small LMs, requires extensiveresources. To address this issue, we propose the new Stimulus RAG approach thatsurpasses the effectiveness of fine tuning based approaches, therebyeliminating the need for the costly data augmentation and fine tuning step forenriching LMs with less popular factual knowledge.</description><author>Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi</author><pubDate>Fri, 27 Sep 2024 13:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01432v4</guid></item><item><title>Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions</title><link>http://arxiv.org/abs/2405.08027v3</link><description>As machine learning (ML) models are increasingly used in social domains tomake consequential decisions about humans, they often have the power to reshapedata distributions. Humans, as strategic agents, continuously adapt theirbehaviors in response to the learning system. As populations changedynamically, ML systems may need frequent updates to ensure high performance.However, acquiring high-quality human-annotated samples can be highlychallenging and even infeasible in social domains. A common practice to addressthis issue is using the model itself to annotate unlabeled data samples. Thispaper investigates the long-term impacts when ML models are retrained withmodel-annotated samples when they incorporate human strategic responses. Wefirst formalize the interactions between strategic agents and the model andthen analyze how they evolve under such dynamic interactions. We find thatagents are increasingly likely to receive positive decisions as the model getsretrained, whereas the proportion of agents with positive labels may decreaseover time. We thus propose a refined retraining process to stabilize thedynamics. Last, we examine how algorithmic fairness can be affected by theseretraining processes and find that enforcing common fairness constraints atevery round may not benefit the disadvantaged group in the long run.Experiments on (semi-)synthetic and real data validate the theoreticalfindings.</description><author>Tian Xie, Xueru Zhang</author><pubDate>Fri, 27 Sep 2024 12:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08027v3</guid></item><item><title>Plurals: A System for Guiding LLMs Via Simulated Social Ensembles</title><link>http://arxiv.org/abs/2409.17213v2</link><description>Recent debates raised concerns that language models may favor certainviewpoints. But what if the solution is not to aim for a 'view from nowhere'but rather to leverage different viewpoints? We introduce Plurals, a system andPython library for pluralistic AI deliberation. Plurals consists of Agents(LLMs, optionally with personas) which deliberate within customizableStructures, with Moderators overseeing deliberation. Plurals is a generator ofsimulated social ensembles. Plurals integrates with government datasets tocreate nationally representative personas, includes deliberation templatesinspired by democratic deliberation theory, and allows users to customize bothinformation-sharing structures and deliberation behavior within Structures. Sixcase studies demonstrate fidelity to theoretical constructs and efficacy. Threerandomized experiments show simulated focus groups produced output resonantwith an online sample of the relevant audiences (chosen over zero-shotgeneration in 75% of trials). Plurals is both a paradigm and a concrete systemfor pluralistic AI. The Plurals library is available athttps://github.com/josh-ashkinaze/plurals and will be continually updated.</description><author>Joshua Ashkinaze, Emily Fry, Narendra Edara, Eric Gilbert, Ceren Budak</author><pubDate>Fri, 27 Sep 2024 12:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17213v2</guid></item><item><title>Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling</title><link>http://arxiv.org/abs/2409.16937v2</link><description>The lack of labeled data is a common challenge in speech classificationtasks, particularly those requiring extensive subjective assessment, such ascognitive state classification. In this work, we propose a Semi-SupervisedLearning (SSL) framework, introducing a novel multi-view pseudo-labeling methodthat leverages both acoustic and linguistic characteristics to select the mostconfident data for training the classification model. Acoustically, unlabeleddata are compared to labeled data using the Frechet audio distance, calculatedfrom embeddings generated by multiple audio encoders. Linguistically, largelanguage models are prompted to revise automatic speech recognitiontranscriptions and predict labels based on our proposed task-specificknowledge. High-confidence data are identified when pseudo-labels from bothsources align, while mismatches are treated as low-confidence data. A bimodalclassifier is then trained to iteratively label the low-confidence data until apredefined criterion is met. We evaluate our SSL framework on emotionrecognition and dementia detection tasks. Experimental results demonstrate thatour method achieves competitive performance compared to fully supervisedlearning using only 30% of the labeled data and significantly outperforms twoselected baselines.</description><author>Yuanchao Li, Zixing Zhang, Jing Han, Peter Bell, Catherine Lai</author><pubDate>Fri, 27 Sep 2024 11:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16937v2</guid></item><item><title>MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks</title><link>http://arxiv.org/abs/2409.17699v2</link><description>The proliferation of Large Language Models (LLMs) in diverse applicationsunderscores the pressing need for robust security measures to thwart potentialjailbreak attacks. These attacks exploit vulnerabilities within LLMs, endangerdata integrity and user privacy. Guardrails serve as crucial protectivemechanisms against such threats, but existing models often fall short in termsof both detection accuracy, and computational efficiency. This paper advocatesfor the significance of jailbreak attack prevention on LLMs, and emphasises therole of input guardrails in safeguarding these models. We introduce MoJE(Mixture of Jailbreak Expert), a novel guardrail architecture designed tosurpass current limitations in existing state-of-the-art guardrails. Byemploying simple linguistic statistical techniques, MoJE excels in detectingjailbreak attacks while maintaining minimal computational overhead during modelinference. Through rigorous experimentation, MoJE demonstrates superiorperformance capable of detecting 90% of the attacks without compromising benignprompts, enhancing LLMs security against jailbreak attacks.</description><author>Giandomenico Cornacchia, Giulio Zizzo, Kieran Fraser, Muhammad Zaid Hamed, Ambrish Rawat, Mark Purcell</author><pubDate>Fri, 27 Sep 2024 10:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17699v2</guid></item><item><title>Leveraging Anthropometric Measurements to Improve Human Mesh Estimation and Ensure Consistent Body Shapes</title><link>http://arxiv.org/abs/2409.17671v2</link><description>The basic body shape of a person does not change within a single video.However, most SOTA human mesh estimation (HME) models output a slightlydifferent body shape for each video frame, which results in inconsistent bodyshapes for the same person. In contrast, we leverage anthropometricmeasurements like tailors are already obtaining from humans for centuries. Wecreate a model called A2B that converts such anthropometric measurements tobody shape parameters of human mesh models. Moreover, we find that finetunedSOTA 3D human pose estimation (HPE) models outperform HME models regarding theprecision of the estimated keypoints. We show that applying inverse kinematics(IK) to the results of such a 3D HPE model and combining the resulting bodypose with the A2B body shape leads to superior and consistent human meshes forchallenging datasets like ASPset or fit3D, where we can lower the MPJPE by over30 mm compared to SOTA HME models. Further, replacing HME models estimates ofthe body shape parameters with A2B model results not only increases theperformance of these HME models, but also leads to consistent body shapes.</description><author>Katja Ludwig, Julian Lorenz, Daniel Kienzle, Tuan Bui, Rainer Lienhart</author><pubDate>Fri, 27 Sep 2024 10:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17671v2</guid></item><item><title>Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval Model</title><link>http://arxiv.org/abs/2409.17745v2</link><description>A supervised ranking model, despite its advantage of being effective, usuallyinvolves complex processing - typically multiple stages of task-specificpre-training and fine-tuning. This has motivated researchers to explore simplerpipelines leveraging large language models (LLMs) that are capable of workingin a zero-shot manner. However, since zero-shot inference does not make use ofa training set of pairs of queries and their relevant documents, itsperformance is mostly worse than that of supervised models, which are trainedon such example pairs. Motivated by the existing findings that trainingexamples generally improve zero-shot performance, in our work, we explore ifthis also applies to ranking models. More specifically, given a query and apair of documents, the preference prediction task is improved by augmentingexamples of preferences for similar queries from a training set. Our proposedpairwise few-shot ranker demonstrates consistent improvements over thezero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)retrieval benchmarks. Our method also achieves a close performance to that of asupervised model without requiring any complex training pipeline.</description><author>Nilanjan Sinhababu, Andrew Parry, Debasis Ganguly, Debasis Samanta, Pabitra Mitra</author><pubDate>Fri, 27 Sep 2024 08:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17745v2</guid></item><item><title>Confidence intervals uncovered: Are we ready for real-world medical imaging AI?</title><link>http://arxiv.org/abs/2409.17763v2</link><description>Medical imaging is spearheading the AI transformation of healthcare.Performance reporting is key to determine which methods should be translatedinto clinical practice. Frequently, broad conclusions are simply derived frommean performance values. In this paper, we argue that this common practice isoften a misleading simplification as it ignores performance variability. Ourcontribution is threefold. (1) Analyzing all MICCAI segmentation papers (n =221) published in 2023, we first observe that more than 50% of papers do notassess performance variability at all. Moreover, only one (0.5%) paper reportedconfidence intervals (CIs) for model performance. (2) To address the reportingbottleneck, we show that the unreported standard deviation (SD) in segmentationpapers can be approximated by a second-order polynomial function of the meanDice similarity coefficient (DSC). Based on external validation data from 56previous MICCAI challenges, we demonstrate that this approximation canaccurately reconstruct the CI of a method using information provided inpublications. (3) Finally, we reconstructed 95% CIs around the mean DSC ofMICCAI 2023 segmentation papers. The median CI width was 0.03 which is threetimes larger than the median performance gap between the first and secondranked method. For more than 60% of papers, the mean performance of thesecond-ranked method was within the CI of the first-ranked method. We concludethat current publications typically do not provide sufficient evidence tosupport which models could potentially be translated into clinical practice.</description><author>Evangelia Christodoulou, Annika Reinke, Rola Houhou, Piotr Kalinowski, Selen Erkan, Carole H. Sudre, Ninon Burgos, Sofi√®ne Boutaj, Sophie Loizillon, Ma√´lys Solal, Nicola Rieke, Veronika Cheplygina, Michela Antonelli, Leon D. Mayer, Minu D. Tizabi, M. Jorge Cardoso, Amber Simpson, Paul F. J√§ger, Annette Kopp-Schneider, Ga√´l Varoquaux, Olivier Colliot, Lena Maier-Hein</author><pubDate>Fri, 27 Sep 2024 06:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17763v2</guid></item><item><title>Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult</title><link>http://arxiv.org/abs/2409.17545v2</link><description>Preference optimization methods typically begin training with a well-trainedSFT model as a reference model. In RLHF and DPO, a regularization term is usedduring the preference optimization process to prevent the policy model fromdeviating too far from the reference model's distribution, thereby avoiding thegeneration of anomalous responses. When the reference model is alreadywell-aligned with the given data or only requires slight adjustments, thisapproach can produce a well-aligned model. However, if the reference model isnot aligned with the given data and requires significant deviation from itscurrent state, a regularization term may actually hinder the model alignment.In this study, we propose \textbf{Modulated Intervention PreferenceOptimization (MIPO)} to address this issue. MIPO modulates the degree ofintervention from the reference model based on how well the given data isaligned with it. If the data is well-aligned, the intervention is increased toprevent the policy model from diverging significantly from reference model.Conversely, if the alignment is poor, the interference is reduced to facilitatemore extensive training. We compare the performance of MIPO and DPO usingMistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimentalresults demonstrate that MIPO consistently outperforms DPO across variousevaluation scenarios.</description><author>Cheolhun Jang</author><pubDate>Fri, 27 Sep 2024 06:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17545v2</guid></item><item><title>Summarizing Radiology Reports Findings into Impressions</title><link>http://arxiv.org/abs/2405.06802v3</link><description>Patient hand-off and triage are two fundamental problems in health care.Often doctors must painstakingly summarize complex findings to efficientlycommunicate with specialists and quickly make decisions on which patients havethe most urgent cases. In pursuit of these challenges, we present (1) a modelwith state-of-art radiology report summarization performance using (2) a novelmethod for augmenting medical data, and (3) an analysis of the modellimitations and radiology knowledge gain. We also provide a data processingpipeline for future models developed on the the MIMIC CXR dataset. Our bestperforming model was a fine-tuned BERT-to-BERT encoder-decoder with 58.75/100ROUGE-L F1, which outperformed specialized checkpoints with more sophisticatedattention mechanisms. We investigate these aspects in this work.</description><author>Raul Salles de Padua, Imran Qureshi</author><pubDate>Fri, 27 Sep 2024 06:13:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06802v3</guid></item><item><title>CRoP: Context-wise Robust Static Human-Sensing Personalization</title><link>http://arxiv.org/abs/2409.17994v2</link><description>The advancement in deep learning and internet-of-things have led to diversehuman sensing applications. However, distinct patterns in human sensing,influenced by various factors or contexts, challenge generic neural networkmodel's performance due to natural distribution shifts. To address this,personalization tailors models to individual users. Yet most personalizationstudies overlook intra-user heterogeneity across contexts in sensory data,limiting intra-user generalizability. This limitation is especially critical inclinical applications, where limited data availability hampers bothgeneralizability and personalization. Notably, intra-user sensing attributesare expected to change due to external factors such as treatment progression,further complicating the challenges. This work introduces CRoP, a novel staticpersonalization approach using an off-the-shelf pre-trained model and pruningto optimize personalization and generalization. CRoP shows superiorpersonalization effectiveness and intra-user robustness across fourhuman-sensing datasets, including two from real-world health domains,highlighting its practical and social impact. Additionally, to support CRoP'sgeneralization ability and design choices, we provide empirical justificationthrough gradient inner product analysis, ablation studies, and comparisonsagainst state-of-the-art baselines.</description><author>Sawinder Kaur, Avery Gump, Jingyu Xin, Yi Xiao, Harshit Sharma, Nina R Benway, Jonathan L Preston, Asif Salekin</author><pubDate>Fri, 27 Sep 2024 04:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17994v2</guid></item><item><title>InterNet: Unsupervised Cross-modal Homography Estimation Based on Interleaved Modality Transfer and Self-supervised Homography Prediction</title><link>http://arxiv.org/abs/2409.17993v2</link><description>We propose a novel unsupervised cross-modal homography estimation framework,based on interleaved modality transfer and self-supervised homographyprediction, named InterNet. InterNet integrates modality transfer andself-supervised homography estimation, introducing an innovative interleavedoptimization framework to alternately promote both components. The modalitytransfer gradually narrows the modality gaps, facilitating the self-supervisedhomography estimation to fully leverage the synthetic intra-modal data. Theself-supervised homography estimation progressively achieves reliablepredictions, thereby providing robust cross-modal supervision for the modalitytransfer. To further boost the estimation accuracy, we also formulate afine-grained homography feature loss to improve the connection between twocomponents. Furthermore, we employ a simple yet effective distillation trainingtechnique to reduce model parameters and improve cross-domain generalizationability while maintaining comparable performance. Experiments reveal thatInterNet achieves the state-of-the-art (SOTA) performance among unsupervisedmethods, and even outperforms many supervised methods such as MHN andLocalTrans.</description><author>Junchen Yu, Si-Yuan Cao, Runmin Zhang, Chenghao Zhang, Jianxin Hu, Zhu Yu, Beinan Yu, Hui-liang Shen</author><pubDate>Fri, 27 Sep 2024 02:35:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17993v2</guid></item><item><title>Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia</title><link>http://arxiv.org/abs/2409.17391v2</link><description>Though Large Language Models (LLMs) have shown remarkable abilities inmathematics reasoning, they are still struggling with performing numericoperations accurately, such as addition and multiplication. Numbers can betokenized into tokens in various ways by different LLMs and affect the numericoperations performance. Currently, there are two representatives: 1) Tokenizeinto $1$-digit, and 2) Tokenize into $1\sim 3$ digit. The difference is roughlyequivalent to using different numeral systems (namely base $10$ or base$10^{3}$). In light of this, we study the scaling behavior of different numeralsystems in the context of transformer-based large language models. Weempirically show that a base $10$ system is consistently more data-efficientthan a base $10^{2}$ or $10^{3}$ system across training data scale, model sizesunder from-scratch training settings, while different number systems have verysimilar fine-tuning performances. We attribute this to higher token frequenciesof a base $10$ system. Additionally, we reveal extrapolation behavior patternson addition and multiplication. We identify that base $100$ and base $1000$systems struggle on token-level discernment and token-level operations. We alsosheds light on the mechanism learnt by the models.</description><author>Zhejian Zhou, Jiayu Wang, Dahua Lin, Kai Chen</author><pubDate>Fri, 27 Sep 2024 02:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17391v2</guid></item><item><title>Explaining Explaining</title><link>http://arxiv.org/abs/2409.18052v2</link><description>Explanation is key to people having confidence in high-stakes AI systems.However, machine-learning-based systems -- which account for almost all currentAI -- can't explain because they are usually black boxes. The explainable AI(XAI) movement hedges this problem by redefining "explanation". Thehuman-centered explainable AI (HCXAI) movement identifies theexplanation-oriented needs of users but can't fulfill them because of itscommitment to machine learning. In order to achieve the kinds of explanationsneeded by real people operating in critical domains, we must rethink how toapproach AI. We describe a hybrid approach to developing cognitive agents thatuses a knowledge-based infrastructure supplemented by data obtained throughmachine learning when applicable. These agents will serve as assistants tohumans who will bear ultimate responsibility for the decisions and actions ofthe human-robot team. We illustrate the explanatory potential of such agentsusing the under-the-hood panels of a demonstration system in which a team ofsimulated robots collaborate on a search task assigned by a human.</description><author>Sergei Nirenburg, Marjorie McShane, Kenneth W. Goodman, Sanjay Oruganti</author><pubDate>Fri, 27 Sep 2024 02:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18052v2</guid></item><item><title>Hypergame Theory for Decentralized Resource Allocation in Multi-user Semantic Communications</title><link>http://arxiv.org/abs/2409.17985v2</link><description>Semantic communications (SC) is an emerging communication paradigm in whichwireless devices can send only relevant information from a source of data whilerelying on computing resources to regenerate missing data points. However, thedesign of a multi-user SC system becomes more challenging because of thecomputing and communication overhead required for coordination. Existingsolutions for learning the semantic language and performing resource allocationoften fail to capture the computing and communication tradeoffs involved inmultiuser SC. To address this gap, a novel framework for decentralizedcomputing and communication resource allocation in multiuser SC systems isproposed. The challenge of efficiently allocating communication and computingresources (for reasoning) in a decentralized manner to maximize the quality oftask experience for the end users is addressed through the application ofStackelberg hyper game theory. Leveraging the concept of second-level hypergames, novel analytical formulations are developed to model misperceptions ofthe users about each other's communication and control strategies. Further,equilibrium analysis of the learned resource allocation protocols examines theconvergence of the computing and communication strategies to a localStackelberg equilibria, considering misperceptions. Simulation results showthat the proposed Stackelberg hyper game results in efficient usage ofcommunication and computing resources while maintaining a high quality ofexperience for the users compared to state-of-the-art that does not account forthe misperceptions.</description><author>Christo Kurisummoottil Thomas, Walid Saad</author><pubDate>Fri, 27 Sep 2024 02:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17985v2</guid></item><item><title>Internalizing ASR with Implicit Chain of Thought for Efficient Speech-to-Speech Conversational LLM</title><link>http://arxiv.org/abs/2409.17353v2</link><description>Current speech-based LLMs are predominantly trained on extensive ASR and TTSdatasets, excelling in tasks related to these domains. However, their abilityto handle direct speech-to-speech conversations remains notably constrained.These models often rely on an ASR-to-TTS chain-of-thought pipeline, convertingspeech into text for processing before generating audio responses, whichintroduces latency and loses audio features. We propose a method thatimplicitly internalizes ASR chain of thought into a speech LLM, enhancing itsnative speech understanding capabilities. Our approach reduces latency andimproves the model's native understanding of speech, paving the way for moreefficient and natural real-time audio interactions. We also release alarge-scale synthetic conversational dataset to facilitate further research.</description><author>Robin Shing-Hei Yuen, Timothy Tin-Long Tse, Jian Zhu</author><pubDate>Fri, 27 Sep 2024 01:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17353v2</guid></item><item><title>Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition</title><link>http://arxiv.org/abs/2409.17073v2</link><description>Accurately attributing answer text to its source document is crucial fordeveloping a reliable question-answering system. However, attribution for longdocuments remains largely unexplored. Post-hoc attribution systems are designedto map answer text back to the source document, yet the granularity of thismapping has not been addressed. Furthermore, a critical question arises: Whatexactly should be attributed? This involves identifying the specificinformation units within an answer that require grounding. In this paper, wepropose and investigate a novel approach to the factual decomposition ofgenerated answers for attribution, employing template-based in-contextlearning. To accomplish this, we utilize the question and integrate negativesampling during few-shot in-context learning for decomposition. This approachenhances the semantic understanding of both abstractive and extractive answers.We examine the impact of answer decomposition by providing a thoroughexamination of various attribution approaches, ranging from retrieval-basedtechniques to LLM-based attributors.</description><author>Pritika Ramu, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivavsan</author><pubDate>Thu, 26 Sep 2024 20:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17073v2</guid></item><item><title>FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner</title><link>http://arxiv.org/abs/2409.18128v1</link><description>Building on the success of diffusion models in visual generation, flow-basedmodels reemerge as another prominent family of generative models that haveachieved competitive or better performance in terms of both visual quality andinference speed. By learning the velocity field through flow-matching,flow-based models tend to produce a straighter sampling trajectory, which isadvantageous during the sampling process. However, unlike diffusion models forwhich fast samplers are well-developed, efficient sampling of flow-basedgenerative models has been rarely explored. In this paper, we propose aframework called FlowTurbo to accelerate the sampling of flow-based modelswhile still enhancing the sampling quality. Our primary observation is that thevelocity predictor's outputs in the flow-based models will become stable duringthe sampling, enabling the estimation of velocity via a lightweight velocityrefiner. Additionally, we introduce several techniques including a pseudocorrector and sample-aware compilation to further reduce inference time. SinceFlowTurbo does not change the multi-step sampling paradigm, it can beeffectively applied for various tasks such as image editing, inpainting, etc.By integrating FlowTurbo into different flow-based models, we obtain anacceleration ratio of 53.1%$\sim$58.3% on class-conditional generation and29.8%$\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FIDof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),achieving the real-time image generation and establishing the newstate-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.</description><author>Wenliang Zhao, Minglei Shi, Xumin Yu, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 26 Sep 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18128v1</guid></item><item><title>EgoLM: Multi-Modal Language Model of Egocentric Motions</title><link>http://arxiv.org/abs/2409.18127v1</link><description>As the prevalence of wearable devices, learning egocentric motions becomesessential to develop contextual AI. In this work, we present EgoLM, a versatileframework that tracks and understands egocentric motions from multi-modalinputs, e.g., egocentric videos and motion sensors. EgoLM exploits richcontexts for the disambiguation of egomotion tracking and understanding, whichare ill-posed under single modality conditions. To facilitate the versatile andmulti-modal framework, our key insight is to model the joint distribution ofegocentric motions and natural languages using large language models (LLM).Multi-modal sensor inputs are encoded and projected to the joint latent spaceof language models, and used to prompt motion generation or text generation foregomotion tracking or understanding, respectively. Extensive experiments onlarge-scale multi-modal human motion dataset validate the effectiveness ofEgoLM as a generalist model for universal egocentric learning.</description><author>Fangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei Liu, Lingni Ma</author><pubDate>Thu, 26 Sep 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18127v1</guid></item><item><title>LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness</title><link>http://arxiv.org/abs/2409.18125v1</link><description>Recent advancements in Large Multimodal Models (LMMs) have greatly enhancedtheir proficiency in 2D visual understanding tasks, enabling them toeffectively process and understand images and videos. However, the developmentof LMMs with 3D-awareness for 3D scene understanding has been hindered by thelack of large-scale 3D vision-language datasets and powerful 3D encoders. Inthis paper, we introduce a simple yet effective framework called LLaVA-3D.Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3Defficiently adapts LLaVA for 3D scene understanding without compromising 2Dunderstanding capabilities. To achieve this, we employ a simple yet effectiverepresentation, 3D Patch, which connects 2D CLIP patch features with theircorresponding positions in 3D space. By integrating the 3D Patches into 2D LMMsand employing joint 2D and 3D vision-language instruction tuning, we establisha unified architecture for both 2D image understanding and 3D sceneunderstanding. Experimental results show that LLaVA-3D converges 3.5x fasterthan existing 3D LMMs when trained on 3D vision-language datasets. Moreover,LLaVA-3D not only achieves state-of-the-art performance across various 3D tasksbut also maintains comparable 2D image understanding and vision-languageconversation capabilities with LLaVA.</description><author>Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, Xihui Liu</author><pubDate>Thu, 26 Sep 2024 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18125v1</guid></item><item><title>Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction</title><link>http://arxiv.org/abs/2409.18124v1</link><description>Leveraging the visual priors of pre-trained text-to-image diffusion modelsoffers a promising solution to enhance zero-shot generalization in denseprediction tasks. However, existing methods often uncritically use the originaldiffusion formulation, which may not be optimal due to the fundamentaldifferences between dense prediction and image generation. In this paper, weprovide a systemic analysis of the diffusion formulation for the denseprediction, focusing on both quality and efficiency. And we find that theoriginal parameterization type for image generation, which learns to predictnoise, is harmful for dense prediction; the multi-step noising/denoisingdiffusion process is also unnecessary and challenging to optimize. Based onthese insights, we introduce Lotus, a diffusion-based visual foundation modelwith a simple yet effective adaptation protocol for dense prediction.Specifically, Lotus is trained to directly predict annotations instead ofnoise, thereby avoiding harmful variance. We also reformulate the diffusionprocess into a single-step procedure, simplifying optimization andsignificantly boosting inference speed. Additionally, we introduce a noveltuning strategy called detail preserver, which achieves more accurate andfine-grained predictions. Without scaling up the training data or modelcapacity, Lotus achieves SoTA performance in zero-shot depth and normalestimation across various datasets. It also significantly enhances efficiency,being hundreds of times faster than most existing diffusion-based methods.</description><author>Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Liu, Bingbing Liu, Ying-Cong Chen</author><pubDate>Thu, 26 Sep 2024 17:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18124v1</guid></item><item><title>Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction</title><link>http://arxiv.org/abs/2409.18121v1</link><description>Humans can learn to manipulate new objects by simply watching others;providing robots with the ability to learn from such demonstrations wouldenable a natural interface specifying new behaviors. This work develops RobotSee Robot Do (RSRD), a method for imitating articulated object manipulationfrom a single monocular RGB human demonstration given a single staticmulti-view object scan. We first propose 4D Differentiable Part Models(4D-DPM), a method for recovering 3D part motion from a monocular video withdifferentiable rendering. This analysis-by-synthesis approach uses part-centricfeature fields in an iterative optimization which enables the use of geometricregularizers to recover 3D motions from only a single video. Given this 4Dreconstruction, the robot replicates object trajectories by planning bimanualarm motions that induce the demonstrated object part motion. By representingdemonstrations as part-centric trajectories, RSRD focuses on replicating thedemonstration's intended behavior while considering the robot's ownmorphological limits, rather than attempting to reproduce the hand's motion. Weevaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D parttrajectories and RSRD's physical execution performance on 9 objects across 10trials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of87% success rate, for a total end-to-end success rate of 60% across 90 trials.Notably, this is accomplished using only feature fields distilled from largepretrained vision models -- without any task-specific training, fine-tuning,dataset collection, or annotation. Project page:https://robot-see-robot-do.github.io</description><author>Justin Kerr, Chung Min Kim, Mingxuan Wu, Brent Yi, Qianqian Wang, Ken Goldberg, Angjoo Kanazawa</author><pubDate>Thu, 26 Sep 2024 17:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18121v1</guid></item><item><title>EvMAPPER: High Altitude Orthomapping with Event Cameras</title><link>http://arxiv.org/abs/2409.18120v1</link><description>Traditionally, unmanned aerial vehicles (UAVs) rely on CMOS-based cameras tocollect images about the world below. One of the most successful applicationsof UAVs is to generate orthomosaics or orthomaps, in which a series of imagesare integrated together to develop a larger map. However, the use of CMOS-basedcameras with global or rolling shutters mean that orthomaps are vulnerable tochallenging light conditions, motion blur, and high-speed motion ofindependently moving objects under the camera. Event cameras are less sensitiveto these issues, as their pixels are able to trigger asynchronously onbrightness changes. This work introduces the first orthomosaic approach usingevent cameras. In contrast to existing methods relying only on CMOS cameras,our approach enables map generation even in challenging light conditions,including direct sunlight and after sunset.</description><author>Fernando Cladera, Kenneth Chaney, M. Ani Hsieh, Camillo J. Taylor, Vijay Kumar</author><pubDate>Thu, 26 Sep 2024 17:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18120v1</guid></item><item><title>Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography</title><link>http://arxiv.org/abs/2409.18119v1</link><description>Contrastive Language-Image Pre-training (CLIP) shows promise in medical imageanalysis but requires substantial data and computational resources. Due tothese restrictions, existing CLIP applications in medical imaging focus mainlyon modalities like chest X-rays that have abundant image-report data available,leaving many other important modalities under-explored. Here, we propose thefirst adaptation of the full CLIP model to mammography, which presentssignificant challenges due to labeled data scarcity, high-resolution imageswith small regions of interest, and data imbalance. We first develop aspecialized supervision framework for mammography that leverages its multi-viewnature. Furthermore, we design a symmetric local alignment module to betterfocus on detailed features in high-resolution images. Lastly, we incorporate aparameter-efficient fine-tuning approach for large language models pre-trainedwith medical knowledge to address data limitations. Our multi-view andmulti-scale alignment (MaMA) method outperforms state-of-the-art baselines forthree different tasks on two large real-world mammography datasets, EMBED andRSNA-Mammo, with only 52% model size compared with the largest baseline.</description><author>Yuexi Du, John Onofrey, Nicha C. Dvornek</author><pubDate>Thu, 26 Sep 2024 17:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18119v1</guid></item><item><title>Assumption violations in causal discovery and the robustness of score matching</title><link>http://arxiv.org/abs/2310.13387v2</link><description>When domain knowledge is limited and experimentation is restricted byethical, financial, or time constraints, practitioners turn to observationalcausal discovery methods to recover the causal structure, exploiting thestatistical properties of their data. Because causal discovery without furtherassumptions is an ill-posed problem, each algorithm comes with its own set ofusually untestable assumptions, some of which are hard to meet in realdatasets. Motivated by these considerations, this paper extensively benchmarksthe empirical performance of recent causal discovery methods on observationali.i.d. data generated under different background conditions, allowing forviolations of the critical assumptions required by each selected approach. Ourexperimental findings show that score matching-based methods demonstratesurprising performance in the false positive and false negative rate of theinferred graph in these challenging scenarios, and we provide theoreticalinsights into their performance. This work is also the first effort tobenchmark the stability of causal discovery algorithms with respect to thevalues of their hyperparameters. Finally, we hope this paper will set a newstandard for the evaluation of causal discovery methods and can serve as anaccessible entry point for practitioners interested in the field, highlightingthe empirical implications of different algorithm choices.</description><author>Francesco Montagna, Atalanti A. Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco, Dominik Janzing, Bryon Aragam, Francesco Locatello</author><pubDate>Thu, 26 Sep 2024 17:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13387v2</guid></item><item><title>UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems</title><link>http://arxiv.org/abs/2407.00312v2</link><description>Single-stage neural combinatorial optimization solvers have achievednear-optimal results on various small-scale combinatorial optimization (CO)problems without needing expert knowledge. However, these solvers exhibitsignificant performance degradation when applied to large-scale CO problems.Recently, two-stage neural methods with divide-and-conquer strategies haveshown efficiency in addressing large-scale CO problems. Nevertheless, theperformance of these methods highly relies on problem-specific heuristics ineither the divide or the conquer procedure, which limits their applicability togeneral CO problems. Moreover, these methods employ separate training schemesand ignore the interdependencies between the dividing and conqueringstrategies, which often leads to sub-optimal solutions. To tackle thesedrawbacks, this article develops a unified neural divide-and-conquer framework(i.e., UDC) for solving general large-scale CO problems. UDC offers aDivide-Conquer-Reunion (DCR) training method to eliminate the negative impactof a sub-optimal dividing policy. Employing a high-efficiency Graph NeuralNetwork (GNN) for global instance dividing and a fixed-length sub-path solverfor conquering divided sub-problems, the proposed UDC framework demonstratesextensive applicability, achieving superior performance in 10 representativelarge-scale CO problems. The code is available athttps://github.com/CIAM-Group/NCO_code/tree/main/single_objective/UDC-Large-scale-CO-master.</description><author>Zhi Zheng, Changliang Zhou, Tong Xialiang, Mingxuan Yuan, Zhenkun Wang</author><pubDate>Thu, 26 Sep 2024 17:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00312v2</guid></item><item><title>EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation</title><link>http://arxiv.org/abs/2409.18114v1</link><description>Current auto-regressive mesh generation methods suffer from issues such asincompleteness, insufficient detail, and poor generalization. In this paper, wepropose an Auto-regressive Auto-encoder (ArAE) model capable of generatinghigh-quality 3D meshes with up to 4,000 faces at a spatial resolution of$512^3$. We introduce a novel mesh tokenization algorithm that efficientlycompresses triangular meshes into 1D token sequences, significantly enhancingtraining efficiency. Furthermore, our model compresses variable-lengthtriangular meshes into a fixed-length latent space, enabling training latentdiffusion models for better generalization. Extensive experiments demonstratethe superior quality, diversity, and generalization capabilities of our modelin both point cloud and image-conditioned mesh generation tasks.</description><author>Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, Qinsheng Zhang</author><pubDate>Thu, 26 Sep 2024 17:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18114v1</guid></item><item><title>E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding</title><link>http://arxiv.org/abs/2409.18111v1</link><description>Recent advances in Video Large Language Models (Video-LLMs) have demonstratedtheir great potential in general-purpose video understanding. To verify thesignificance of these models, a number of benchmarks have been proposed todiagnose their capabilities in different scenarios. However, existingbenchmarks merely evaluate models through video-level question-answering,lacking fine-grained event-level assessment and task diversity. To fill thisgap, we introduce E.T. Bench (Event-Level &amp; Time-Sensitive Video UnderstandingBenchmark), a large-scale and high-quality benchmark for open-ended event-levelvideo understanding. Categorized within a 3-level task taxonomy, E.T. Benchencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)under 8 domains, providing comprehensive evaluations. We extensively evaluated8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal thatstate-of-the-art models for coarse-level (video-level) understanding struggleto solve our fine-grained tasks, e.g., grounding event-of-interests withinvideos, largely due to the short video context length, improper timerepresentations, and lack of multi-event training data. Focusing on theseissues, we further propose a strong baseline model, E.T. Chat, together with aninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grainedevent-level understanding. Our simple but effective solution demonstratessuperior performance in multiple scenarios.</description><author>Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen</author><pubDate>Thu, 26 Sep 2024 17:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18111v1</guid></item><item><title>Open-World Evaluation for Retrieving Diverse Perspectives</title><link>http://arxiv.org/abs/2409.18110v1</link><description>We study retrieving a set of documents that covers various perspectives on acomplex and contentious question (e.g., will ChatGPT do more harm than good?).We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS),where each example consists of a question and diverse perspectives associatedwith the question, sourced from survey questions and debate websites. On thisdata, retrievers paired with a corpus are evaluated to surface a document setthat contains diverse perspectives. Our framing diverges from most retrievaltasks in that document relevancy cannot be decided by simple string matches toreferences. Instead, we build a language model based automatic evaluator thatdecides whether each retrieved document contains a perspective. This allows usto evaluate the performance of three different types of corpus (Wikipedia, websnapshot, and corpus constructed on the fly with retrieved pages from thesearch engine) paired with retrievers. Retrieving diverse documents remainschallenging, with the outputs from existing retrievers covering allperspectives on only 33.74% of the examples. We further study the impact ofquery expansion and diversity-focused reranking approaches and analyzeretriever sycophancy. Together, we lay the foundation for future studies inretrieval diversity handling complex queries.</description><author>Hung-Ting Chen, Eunsol Choi</author><pubDate>Thu, 26 Sep 2024 17:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18110v1</guid></item><item><title>Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats</title><link>http://arxiv.org/abs/2409.18104v1</link><description>Much of Earth's charismatic megafauna is endangered by human activities,particularly the rhino, which is at risk of extinction due to the poachingcrisis in Africa. Monitoring rhinos' movement is crucial to their protectionbut has unfortunately proven difficult because rhinos are elusive. Therefore,instead of tracking rhinos, we propose the novel approach of mapping communaldefecation sites, called middens, which give information about rhinos' spatialbehavior valuable to anti-poaching, management, and reintroduction efforts.This paper provides the first-ever mapping of rhino midden locations bybuilding classifiers to detect them using remotely sensed thermal, RGB, andLiDAR imagery in passive and active learning settings. As existing activelearning methods perform poorly due to the extreme class imbalance in ourdataset, we design MultimodAL, an active learning system employing a rankingtechnique and multimodality to achieve competitive performance with passivelearning models with 94% fewer labels. Our methods could therefore save over 76hours in labeling time when used on a similarly-sized dataset. Unexpectedly,our midden map reveals that rhino middens are not randomly distributedthroughout the landscape; rather, they are clustered. Consequently, rangersshould be targeted at areas with high midden densities to strengthenanti-poaching efforts, in line with UN Target 15.7.</description><author>Lucia Gordon, Nikhil Behari, Samuel Collier, Elizabeth Bondi-Kelly, Jackson A. Killian, Catherine Ressijac, Peter Boucher, Andrew Davies, Milind Tambe</author><pubDate>Thu, 26 Sep 2024 17:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18104v1</guid></item><item><title>MALPOLON: A Framework for Deep Species Distribution Modeling</title><link>http://arxiv.org/abs/2409.18102v1</link><description>This paper describes a deep-SDM framework, MALPOLON. Written in Python andbuilt upon the PyTorch library, this framework aims to facilitate training andinferences of deep species distribution models (deep-SDM) and sharing for userswith only general Python language skills (e.g., modeling ecologists) who areinterested in testing deep learning approaches to build new SDMs. More advancedusers can also benefit from the framework's modularity to run more specificexperiments by overriding existing classes while taking advantage ofpress-button examples to train neural networks on multiple classification tasksusing custom or provided raw and pre-processed datasets. The framework isopen-sourced on GitHub and PyPi along with extensive documentation and examplesof use in various scenarios. MALPOLON offers straightforward installation,YAML-based configuration, parallel computing, multi-GPU utilization, baselineand foundational models for benchmarking, and extensivetutorials/documentation, aiming to enhance accessibility and performancescalability for ecologists and researchers.</description><author>Theo Larcher, Lukas Picek, Benjamin Deneu, Titouan Lorieul, Maximilien Servajean, Alexis Joly</author><pubDate>Thu, 26 Sep 2024 17:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18102v1</guid></item><item><title>AI-Powered Augmented Reality for Satellite Assembly, Integration and Test</title><link>http://arxiv.org/abs/2409.18101v1</link><description>The integration of Artificial Intelligence (AI) and Augmented Reality (AR) isset to transform satellite Assembly, Integration, and Testing (AIT) processesby enhancing precision, minimizing human error, and improving operationalefficiency in cleanroom environments. This paper presents a technicaldescription of the European Space Agency's (ESA) project "AI for AR inSatellite AIT," which combines real-time computer vision and AR systems toassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2as the AR interface, the system delivers context-aware instructions andreal-time feedback, tackling the complexities of object recognition and 6D poseestimation in AIT workflows. All AI models demonstrated over 70% accuracy, withthe detection model exceeding 95% accuracy, indicating a high level ofperformance and reliability. A key contribution of this work lies in theeffective use of synthetic data for training AI models in AR applications,addressing the significant challenges of obtaining real-world datasets inhighly dynamic satellite environments, as well as the creation of the SegmentedAnything Model for Automatic Labelling (SAMAL), which facilitates the automaticannotation of real data, achieving speeds up to 20 times faster than manualhuman annotation. The findings demonstrate the efficacy of AI-driven AR systemsin automating critical satellite assembly tasks, setting a foundation forfuture innovations in the space industry.</description><author>Alvaro Patricio, Joao Valente, Atabak Dehban, Ines Cadilha, Daniel Reis, Rodrigo Ventura</author><pubDate>Thu, 26 Sep 2024 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18101v1</guid></item><item><title>Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation</title><link>http://arxiv.org/abs/2409.18100v1</link><description>Self-supervised pretraining (SSP) has shown promising results in learningfrom large unlabeled datasets and, thus, could be useful for automatedcardiovascular magnetic resonance (CMR) short-axis cine segmentation. However,inconsistent reports of the benefits of SSP for segmentation have made itdifficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSPmethods for CMR cine segmentation. To this end, short-axis cine stacks of 296 subjects (90618 2D slices) wereused for unlabeled pretraining with four SSP methods; SimCLR, positionalcontrastive learning, DINO, and masked image modeling (MIM). Subsets of varyingnumbers of subjects were used for supervised fine-tuning of 2D models for eachSSP method, as well as to train a 2D baseline model from scratch. Thefine-tuned models were compared to the baseline using the 3D Dice similaritycoefficient (DSC) in a test dataset of 140 subjects. The SSP methods showed no performance gains with the largest supervisedfine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects(231 2D slices) are available for supervised training, SSP using MIM (DSC =0.86) improves over training from scratch (DSC = 0.82). This study found that SSP is valuable for CMR cine segmentation when labeledtraining data is scarce, but does not aid state-of-the-art deep learningmethods when ample labeled data is available. Moreover, the choice of SSPmethod is important. The code is publicly available at:https://github.com/q-cardIA/ssp-cmr-cine-segmentation</description><author>Rob A. J. de Mooij, Josien P. W. Pluim, Cian M. Scannell</author><pubDate>Thu, 26 Sep 2024 17:44:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18100v1</guid></item><item><title>EfficientCrackNet: A Lightweight Model for Crack Segmentation</title><link>http://arxiv.org/abs/2409.18099v1</link><description>Crack detection, particularly from pavement images, presents a formidablechallenge in the domain of computer vision due to several inherent complexitiessuch as intensity inhomogeneity, intricate topologies, low contrast, and noisybackgrounds. Automated crack detection is crucial for maintaining thestructural integrity of essential infrastructures, including buildings,pavements, and bridges. Existing lightweight methods often face challengesincluding computational inefficiency, complex crack patterns, and difficultbackgrounds, leading to inaccurate detection and impracticality for real-worldapplications. To address these limitations, we propose EfficientCrackNet, alightweight hybrid model combining Convolutional Neural Networks (CNNs) andtransformers for precise crack segmentation. EfficientCrackNet integratesdepthwise separable convolutions (DSC) layers and MobileViT block to captureboth global and local features. The model employs an Edge Extraction Method(EEM) and for efficient crack edge detection without pretraining, andUltra-Lightweight Subspace Attention Module (ULSAM) to enhance featureextraction. Extensive experiments on three benchmark datasets Crack500,DeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superiorperformance compared to existing lightweight models, while requiring only 0.26Mparameters, and 0.483 FLOPs (G). The proposed model offers an optimal balancebetween accuracy and computational efficiency, outperforming state-of-the-artlightweight models, and providing a robust and adaptable solution forreal-world crack segmentation.</description><author>Abid Hasan Zim, Aquib Iqbal, Zaid Al-Huda, Asad Malik, Minoru Kuribayash</author><pubDate>Thu, 26 Sep 2024 17:44:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18099v1</guid></item><item><title>Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?</title><link>http://arxiv.org/abs/2406.12822v3</link><description>Multilingual large language models are designed, claimed, and expected tocater to speakers of varied languages. We hypothesise that the currentpractices of fine-tuning and evaluating these models may not perfectly alignwith this objective owing to a heavy reliance on translation, which cannotcover language-specific knowledge but can introduce translation defects. Itremains unknown whether the nature of the instruction data has an impact on themodel output; conversely, it is questionable whether translated test sets cancapture such nuances. Due to the often coupled practices of using translateddata in both stages, such imperfections could have been overlooked. This workinvestigates these issues using controlled native or translated data during theinstruction tuning and evaluation stages. We show that native or generationbenchmarks reveal a notable difference between native and translatedinstruction data especially when model performance is high, whereas other typesof test sets cannot. The comparison between round-trip and single-passtranslations reflects the importance of knowledge from language-nativeresources. Finally, we demonstrate that regularization is beneficial tobridging this gap on structured but not generative tasks.</description><author>Pinzhen Chen, Simon Yu, Zhicheng Guo, Barry Haddow</author><pubDate>Thu, 26 Sep 2024 17:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12822v3</guid></item><item><title>DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2409.18092v1</link><description>Perception systems play a crucial role in autonomous driving, incorporatingmultiple sensors and corresponding computer vision algorithms. 3D LiDAR sensorsare widely used to capture sparse point clouds of the vehicle's surroundings.However, such systems struggle to perceive occluded areas and gaps in the scenedue to the sparsity of these point clouds and their lack of semantics. Toaddress these challenges, Semantic Scene Completion (SSC) jointly predictsunobserved geometry and semantics in the scene given raw LiDAR measurements,aiming for a more complete scene representation. Building on promising resultsof diffusion models in image generation and super-resolution tasks, we proposetheir extension to SSC by implementing the noising and denoising diffusionprocesses in the point and semantic spaces individually. To control thegeneration, we employ semantic LiDAR point clouds as conditional input anddesign local and global regularization losses to stabilize the denoisingprocess. We evaluate our approach on autonomous driving datasets and ourapproach outperforms the state-of-the-art for SSC.</description><author>Helin Cao, Sven Behnke</author><pubDate>Thu, 26 Sep 2024 17:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18092v1</guid></item><item><title>Quantum Kernel Methods under Scrutiny: A Benchmarking Study</title><link>http://arxiv.org/abs/2409.04406v2</link><description>Since the entry of kernel theory in the field of quantum machine learning,quantum kernel methods (QKMs) have gained increasing attention with regard toboth probing promising applications and delivering intriguing researchinsights. Two common approaches for computing the underlying Gram matrix haveemerged: fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs).Benchmarking these methods is crucial to gain robust insights and to understandtheir practical utility. In this work, we present a comprehensive large-scalestudy examining QKMs based on FQKs and PQKs across a manifold of designchoices. Our investigation encompasses both classification and regression tasksfor five dataset families and 64 datasets, systematically comparing the use ofFQKs and PQKs quantum support vector machines and kernel ridge regression. Thisresulted in over 20,000 models that were trained and optimized using astate-of-the-art hyperparameter search to ensure robust and comprehensiveinsights. We delve into the importance of hyperparameters on model performancescores and support our findings through rigorous correlation analyses. In this,we also closely inspect two data encoding strategies. Moreover, we provide anin-depth analysis addressing the design freedom of PQKs and explore theunderlying principles responsible for learning. Our goal is not to identify thebest-performing model for a specific task but to uncover the mechanisms thatlead to effective QKMs and reveal universal patterns.</description><author>Jan Schnabel, Marco Roth</author><pubDate>Thu, 26 Sep 2024 17:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04406v2</guid></item><item><title>AI-driven View Guidance System in Intra-cardiac Echocardiography Imaging</title><link>http://arxiv.org/abs/2409.16898v2</link><description>Intra-cardiac Echocardiography (ICE) is a crucial imaging modality used inelectrophysiology (EP) and structural heart disease (SHD) interventions,providing real-time, high-resolution views from within the heart. Despite itsadvantages, effective manipulation of the ICE catheter requires significantexpertise, which can lead to inconsistent outcomes, particularly among lessexperienced operators. To address this challenge, we propose an AI-drivenclosed-loop view guidance system with human-in-the-loop feedback, designed toassist users in navigating ICE imaging without requiring specialized knowledge.Our method models the relative position and orientation vectors betweenarbitrary views and clinically defined ICE views in a spatial coordinatesystem, guiding users on how to manipulate the ICE catheter to transition fromthe current view to the desired view over time. Operating in a closed-loopconfiguration, the system continuously predicts and updates the necessarycatheter manipulations, ensuring seamless integration into existing clinicalworkflows. The effectiveness of the proposed system is demonstrated through asimulation-based evaluation, achieving an 89% success rate with the 6532 testdataset, highlighting its potential to improve the accuracy and efficiency ofICE imaging procedures.</description><author>Jaeyoung Huh, Paul Klein, Gareth Funka-Lea, Puneet Sharma, Ankur Kapoor, Young-Ho Kim</author><pubDate>Thu, 26 Sep 2024 17:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16898v2</guid></item><item><title>Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</title><link>http://arxiv.org/abs/2409.16147v2</link><description>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significantpotential for modeling 3D head avatars, providing greater flexibility thanmesh-based methods and more efficient rendering compared to NeRF-basedapproaches. Despite these advancements, the creation of controllable 3DGS-basedhead avatars remains time-intensive, often requiring tens of minutes to hours.To expedite this process, we here introduce the ``Gaussian D\'ej\`a-vu"framework, which first obtains a generalized model of the head avatar and thenpersonalizes the result. The generalized model is trained on large 2D(synthetic and real) image datasets. This model provides a well-initialized 3DGaussian head that is further refined using a monocular video to achieve thepersonalized head avatar. For personalizing, we propose learnableexpression-aware rectification blendmaps to correct the initial 3D Gaussians,ensuring rapid convergence without the reliance on neural networks. Experimentsdemonstrate that the proposed method meets its objectives. It outperformsstate-of-the-art 3D Gaussian head avatars in terms of photorealistic quality aswell as reduces training time consumption to at least a quarter of the existingmethods, producing the avatar in minutes.</description><author>Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</author><pubDate>Thu, 26 Sep 2024 17:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16147v2</guid></item><item><title>GSON: A Group-based Social Navigation Framework with Large Multimodal Model</title><link>http://arxiv.org/abs/2409.18084v1</link><description>As the number of service robots and autonomous vehicles in human-centeredenvironments grows, their requirements go beyond simply navigating to adestination. They must also take into account dynamic social contexts andensure respect and comfort for others in shared spaces, which poses significantchallenges for perception and planning. In this paper, we present a group-basedsocial navigation framework GSON to enable mobile robots to perceive andexploit the social group of their surroundings by leveling the visual reasoningcapability of the Large Multimodal Model (LMM). For perception, we apply visualprompting techniques to zero-shot extract the social relationship amongpedestrians and combine the result with a robust pedestrian detection andtracking pipeline to alleviate the problem of low inference speed of the LMM.Given the perception result, the planning system is designed to avoiddisrupting the current social structure. We adopt a social structure-basedmid-level planner as a bridge between global path planning and local motionplanning to preserve the global context and reactive response. The proposedmethod is validated on real-world mobile robot navigation tasks involvingcomplex social structure understanding and reasoning. Experimental resultsdemonstrate the effectiveness of the system in these scenarios compared withseveral baselines.</description><author>Shangyi Luo, Ji Zhu, Peng Sun, Yuhong Deng, Cunjun Yu, Anxing Xiao, Xueqian Wang</author><pubDate>Thu, 26 Sep 2024 17:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18084v1</guid></item><item><title>Stable Video Portraits</title><link>http://arxiv.org/abs/2409.18083v1</link><description>Rapid advances in the field of generative AI and text-to-image methods inparticular have transformed the way we interact with and perceivecomputer-generated imagery today. In parallel, much progress has been made in3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, wepresent SVP, a novel hybrid 2D/3D generation method that outputs photorealisticvideos of talking faces leveraging a large pre-trained text-to-image prior(2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specificfine-tuning of a general 2D stable diffusion model which we lift to a videomodel by providing temporal 3DMM sequences as conditioning and by introducing atemporal denoising procedure. As an output, this model generates temporallysmooth imagery of a person with 3DMM-based controls, i.e., a person-specificavatar. The facial appearance of this person-specific avatar can be edited andmorphed to text-defined celebrities, without any fine-tuning at test time. Themethod is analyzed quantitatively and qualitatively, and we show that ourmethod outperforms state-of-the-art monocular head avatar methods.</description><author>Mirela Ostrek, Justus Thies</author><pubDate>Thu, 26 Sep 2024 17:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18083v1</guid></item><item><title>SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation</title><link>http://arxiv.org/abs/2409.18082v1</link><description>Automating garment manipulation poses a significant challenge for assistiverobotics due to the diverse and deformable nature of garments. Traditionalapproaches typically require separate models for each garment type, whichlimits scalability and adaptability. In contrast, this paper presents a unifiedapproach using vision-language models (VLMs) to improve keypoint predictionacross various garment categories. By interpreting both visual and semanticinformation, our model enables robots to manage different garment states with asingle model. We created a large-scale synthetic dataset using advancedsimulation techniques, allowing scalable training without extensive real-worlddata. Experimental results indicate that the VLM-based method significantlyenhances keypoint detection accuracy and task success rates, providing a moreflexible and general solution for robotic garment manipulation. In addition,this research also underscores the potential of VLMs to unify various garmentmanipulation tasks within a single framework, paving the way for broaderapplications in home automation and assistive robotics for future.</description><author>Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu</author><pubDate>Thu, 26 Sep 2024 17:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18082v1</guid></item><item><title>Infer Human's Intentions Before Following Natural Language Instructions</title><link>http://arxiv.org/abs/2409.18073v1</link><description>For AI agents to be helpful to humans, they should be able to follow naturallanguage instructions to complete everyday cooperative tasks in humanenvironments. However, real human instructions inherently possess ambiguity,because the human speakers assume sufficient prior knowledge about their hiddengoals and intentions. Standard language grounding and planning methods fail toaddress such ambiguities because they do not model human internal goals asadditional partially observable factors in the environment. We propose a newframework, Follow Instructions with Social and Embodied Reasoning (FISER),aiming for better natural language instruction following in collaborativeembodied tasks. Our framework makes explicit inferences about human goals andintentions as intermediate reasoning steps. We implement a set ofTransformer-based models and evaluate them over a challenging benchmark,HandMeThat. We empirically demonstrate that using social reasoning toexplicitly infer human intentions before making action plans surpasses purelyend-to-end approaches. We also compare our implementation with strongbaselines, including Chain of Thought prompting on the largest availablepre-trained language models, and find that FISER provides better performance onthe embodied social reasoning tasks under investigation, reaching thestate-of-the-art on HandMeThat.</description><author>Yanming Wan, Yue Wu, Yiping Wang, Jiayuan Mao, Natasha Jaques</author><pubDate>Thu, 26 Sep 2024 17:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18073v1</guid></item><item><title>FreeEdit: Mask-free Reference-based Image Editing with Multi-modal Instruction</title><link>http://arxiv.org/abs/2409.18071v1</link><description>Introducing user-specified visual concepts in image editing is highlypractical as these concepts convey the user's intent more precisely thantext-based descriptions. We propose FreeEdit, a novel approach for achievingsuch reference-based image editing, which can accurately reproduce the visualconcept from the reference image based on user-friendly language instructions.Our approach leverages the multi-modal instruction encoder to encode languageinstructions to guide the editing process. This implicit way of locating theediting area eliminates the need for manual editing masks. To enhance thereconstruction of reference details, we introduce the Decoupled ResidualReferAttention (DRRA) module. This module is designed to integrate fine-grainedreference features extracted by a detail extractor into the image editingprocess in a residual way without interfering with the original self-attention.Given that existing datasets are unsuitable for reference-based image editingtasks, particularly due to the difficulty in constructing image triplets thatinclude a reference image, we curate a high-quality dataset, FreeBench, using anewly developed twice-repainting scheme. FreeBench comprises the images beforeand after editing, detailed editing instructions, as well as a reference imagethat maintains the identity of the edited object, encompassing tasks such asobject addition, replacement, and deletion. By conducting phased training onFreeBench followed by quality tuning, FreeEdit achieves high-quality zero-shotediting through convenient language instructions. We conduct extensiveexperiments to evaluate the effectiveness of FreeEdit across multiple tasktypes, demonstrating its superiority over existing methods. The code will beavailable at: https://freeedit.github.io/.</description><author>Runze He, Kai Ma, Linjiang Huang, Shaofei Huang, Jialin Gao, Xiaoming Wei, Jiao Dai, Jizhong Han, Si Liu</author><pubDate>Thu, 26 Sep 2024 17:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18071v1</guid></item><item><title>Learning Interactive Real-World Simulators</title><link>http://arxiv.org/abs/2310.06114v3</link><description>Generative models trained on internet data have revolutionized how text,image, and video content can be created. Perhaps the next milestone forgenerative models is to simulate realistic experience in response to actionstaken by humans, robots, and other interactive agents. Applications of areal-world simulator range from controllable content creation in games andmovies, to training embodied agents purely in simulation that can be directlydeployed in the real world. We explore the possibility of learning a universalsimulator (UniSim) of real-world interaction through generative modeling. Wefirst make the important observation that natural datasets available forlearning a real-world simulator are often rich along different dimensions(e.g., abundant objects in image data, densely sampled actions in roboticsdata, and diverse movements in navigation data). With careful orchestration ofdiverse datasets, each providing a different aspect of the overall experience,we can simulate the visual outcome of both high-level instructions such as"open the drawer" and low-level controls from otherwise static scenes andobjects. We use the simulator to train both high-level vision-language policiesand low-level reinforcement learning policies, each of which can be deployed inthe real world in zero shot after training purely in simulation. We also showthat other types of intelligence such as video captioning models can benefitfrom training with simulated experience, opening up even wider applications.Video demos can be found at https://universal-simulator.github.io.</description><author>Sherry Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Leslie Kaelbling, Dale Schuurmans, Pieter Abbeel</author><pubDate>Thu, 26 Sep 2024 17:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06114v3</guid></item><item><title>Optimal Protocols for Continual Learning via Statistical Physics and Control Theory</title><link>http://arxiv.org/abs/2409.18061v1</link><description>Artificial neural networks often struggle with catastrophic forgetting whenlearning multiple tasks sequentially, as training on new tasks degrades theperformance on previously learned ones. Recent theoretical work has addressedthis issue by analysing learning curves in synthetic frameworks underpredefined training protocols. However, these protocols relied on heuristicsand lacked a solid theoretical foundation assessing their optimality. In thispaper, we fill this gap combining exact equations for training dynamics,derived using statistical physics techniques, with optimal control methods. Weapply this approach to teacher-student models for continual learning andmulti-task problems, obtaining a theory for task-selection protocols maximisingperformance while minimising forgetting. Our theoretical analysis offersnon-trivial yet interpretable strategies for mitigating catastrophicforgetting, shedding light on how optimal learning protocols can modulateestablished effects, such as the influence of task similarity on forgetting.Finally, we validate our theoretical findings on real-world data.</description><author>Francesco Mori, Stefano Sarao Mannelli, Francesca Mignacco</author><pubDate>Thu, 26 Sep 2024 17:01:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18061v1</guid></item><item><title>LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</title><link>http://arxiv.org/abs/2409.18057v1</link><description>Recent works have shown that neural radiance fields (NeRFs) on top ofparametric models have reached SOTA quality to build photorealistic headavatars from a monocular video. However, one major limitation of the NeRF-basedavatars is the slow rendering speed due to the dense point sampling of NeRF,preventing them from broader utility on resource-constrained devices. Weintroduce LightAvatar, the first head avatar model based on neural light fields(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera posevia a single network forward pass, without using mesh or volume rendering. Theproposed approach, while being conceptually appealing, poses a significantchallenge towards real-time efficiency and training stability. To resolve them,we introduce dedicated network designs to obtain proper representations for theNeLF model and maintain a low FLOPs budget. Meanwhile, we tap into adistillation-based training strategy that uses a pretrained avatar model asteacher to synthesize abundant pseudo data for training. A warping fieldnetwork is introduced to correct the fitting error in the real data so that themodel can learn better. Extensive experiments suggest that our method canachieve new SOTA image quality quantitatively or qualitatively, while beingsignificantly faster than the counterparts, reporting 174.1 FPS (512x512resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.</description><author>Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu</author><pubDate>Thu, 26 Sep 2024 17:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18057v1</guid></item><item><title>Visual Data Diagnosis and Debiasing with Concept Graphs</title><link>http://arxiv.org/abs/2409.18055v1</link><description>The widespread success of deep learning models today is owed to the curationof extensive datasets significant in size and complexity. However, such modelsfrequently pick up inherent biases in the data during the training process,leading to unreliable predictions. Diagnosing and debiasing datasets is thus anecessity to ensure reliable model performance. In this paper, we presentCONBIAS, a novel framework for diagnosing and mitigating Concept co-occurrenceBiases in visual datasets. CONBIAS represents visual datasets as knowledgegraphs of concepts, enabling meticulous analysis of spurious conceptco-occurrences to uncover concept imbalances across the whole dataset.Moreover, we show that by employing a novel clique-based concept balancingstrategy, we can mitigate these imbalances, leading to enhanced performance ondownstream tasks. Extensive experiments show that data augmentation based on abalanced concept distribution augmented by CONBIAS improves generalizationperformance across multiple datasets compared to state-of-the-art methods. Wewill make our code and data publicly available.</description><author>Rwiddhi Chakraborty, Yinong Wang, Jialu Gao, Runkai Zheng, Cheng Zhang, Fernando De la Torre</author><pubDate>Thu, 26 Sep 2024 16:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18055v1</guid></item><item><title>DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving</title><link>http://arxiv.org/abs/2409.18053v1</link><description>We present a novel autonomous driving framework, DualAD, designed to imitatehuman reasoning during driving. DualAD comprises two layers: a rule-basedmotion planner at the bottom layer that handles routine driving tasks requiringminimal reasoning, and an upper layer featuring a rule-based text encoder thatconverts driving scenarios from absolute states into text description. Thistext is then processed by a large language model (LLM) to make drivingdecisions. The upper layer intervenes in the bottom layer's decisions whenpotential danger is detected, mimicking human reasoning in critical situations.Closed-loop experiments demonstrate that DualAD, using a zero-shot pre-trainedmodel, significantly outperforms rule-based motion planners that lack reasoningabilities. Our experiments also highlight the effectiveness of the textencoder, which considerably enhances the model's scenario understanding.Additionally, the integrated DualAD model improves with stronger LLMs,indicating the framework's potential for further enhancement. We make code andbenchmarks publicly available.</description><author>Dingrui Wang, Marc Kaufeld, Johannes Betz</author><pubDate>Thu, 26 Sep 2024 16:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18053v1</guid></item><item><title>Explaining Explaining</title><link>http://arxiv.org/abs/2409.18052v1</link><description>Explanation is key to people having confidence in high-stakes AI systems.However, machine-learning-based systems - which account for almost all currentAI - can't explain because they are usually black boxes. The explainable AI(XAI) movement hedges this problem by redefining "explanation". Thehuman-centered explainable AI (HCXAI) movement identifies theexplanation-oriented needs of users but can't fulfill them because of itscommitment to machine learning. In order to achieve the kinds of explanationsneeded by real people operating in critical domains, we must rethink how toapproach AI. We describe a hybrid approach to developing cognitive agents thatuses a knowledge-based infrastructure supplemented by data obtained throughmachine learning when applicable. These agents will serve as assistants tohumans who will bear ultimate responsibility for the decisions and actions ofthe human-robot team. We illustrate the explanatory potential of such agentsusing the under-the-hood panels of a demonstration system in which a team ofsimulated robots collaborates on a search task assigned by a human.</description><author>Sergei Nirenburg, Marjorie McShane, Kenneth W. Goodman, Sanjay Oruganti</author><pubDate>Thu, 26 Sep 2024 16:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18052v1</guid></item><item><title>Inverse Reinforcement Learning with Multiple Planning Horizons</title><link>http://arxiv.org/abs/2409.18051v1</link><description>In this work, we study an inverse reinforcement learning (IRL) problem wherethe experts are planning under a shared reward function but with different,unknown planning horizons. Without the knowledge of discount factors, thereward function has a larger feasible solution set, which makes it harder forexisting IRL approaches to identify a reward function. To overcome thischallenge, we develop algorithms that can learn a global multi-agent rewardfunction with agent-specific discount factors that reconstruct the expertpolicies. We characterize the feasible solution space of the reward functionand discount factors for both algorithms and demonstrate the generalizabilityof the learned reward function across multiple domains.</description><author>Jiayu Yao, Weiwei Pan, Finale Doshi-Velez, Barbara E Engelhardt</author><pubDate>Thu, 26 Sep 2024 16:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18051v1</guid></item><item><title>Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers</title><link>http://arxiv.org/abs/2312.08168v3</link><description>Recent advancements in 3D Large Language Models (LLMs) have demonstratedpromising capabilities for 3D scene understanding. However, previous methodsexhibit deficiencies in general referencing and grounding capabilities forintricate scene comprehension. In this paper, we introduce the use of objectidentifiers and object-centric representations to interact with scenes at theobject level. Specifically, we decompose the input 3D scene into a set ofobject proposals, each assigned a unique identifier token, which enablesefficient object referencing and grounding during user-assistant interactions.Given the scarcity of scene-language data, we model the scene embeddings as asequence of explicit object-level embeddings, derived from semantic-rich 2D or3D representations. By employing object identifiers, we transform diverse 3Dscene-language tasks into a unified question-answering format, facilitatingjoint training without the need for additional task-specific heads. Withminimal fine-tuning on all downstream tasks, our model significantlyoutperforms existing methods on benchmarks including ScanRefer, Multi3DRefer,Scan2Cap, ScanQA, and SQA3D.</description><author>Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, Zhou Zhao</author><pubDate>Thu, 26 Sep 2024 16:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08168v3</guid></item><item><title>Revisit Anything: Visual Place Recognition via Image Segment Retrieval</title><link>http://arxiv.org/abs/2409.18049v1</link><description>Accurately recognizing a revisited place is crucial for embodied agents tolocalize and navigate. This requires visual representations to be distinct,despite strong variations in camera viewpoint and scene appearance. Existingvisual place recognition pipelines encode the "whole" image and search formatches. This poses a fundamental challenge in matching two images of the sameplace captured from different camera viewpoints: "the similarity of whatoverlaps can be dominated by the dissimilarity of what does not overlap". Weaddress this by encoding and searching for "image segments" instead of thewhole images. We propose to use open-set image segmentation to decompose animage into `meaningful' entities (i.e., things and stuff). This enables us tocreate a novel image representation as a collection of multiple overlappingsubgraphs connecting a segment with its neighboring segments, dubbedSuperSegment. Furthermore, to efficiently encode these SuperSegments intocompact vector representations, we propose a novel factorized representation offeature aggregation. We show that retrieving these partial representationsleads to significantly higher recognition recall than the typical whole imagebased retrieval. Our segments-based approach, dubbed SegVLAD, sets a newstate-of-the-art in place recognition on a diverse selection of benchmarkdatasets, while being applicable to both generic and task-specialized imageencoders. Finally, we demonstrate the potential of our method to ``revisitanything'' by evaluating our method on an object instance retrieval task, whichbridges the two disparate areas of research: visual place recognition andobject-goal navigation, through their common aim of recognizing goal objectsspecific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything.</description><author>Kartik Garg, Sai Shubodh Puligilla, Shishir Kolathaya, Madhava Krishna, Sourav Garg</author><pubDate>Thu, 26 Sep 2024 16:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18049v1</guid></item><item><title>Two-Timescale Gradient Descent Ascent Algorithms for Nonconvex Minimax Optimization</title><link>http://arxiv.org/abs/2408.11974v2</link><description>We provide a unified analysis of two-timescale gradient descent ascent(TTGDA) for solving structured nonconvex minimax optimization problems in theform of $\min_\textbf{x} \max_{\textbf{y} \in Y} f(\textbf{x}, \textbf{y})$,where the objective function $f(\textbf{x}, \textbf{y})$ is nonconvex in$\textbf{x}$ and concave in $\textbf{y}$, and the constraint set $Y \subseteq\mathbb{R}^n$ is convex and bounded. In the convex-concave setting, thesingle-timescale gradient descent ascent (GDA) algorithm is widely used inapplications and has been shown to have strong convergence guarantees. In moregeneral settings, however, it can fail to converge. Our contribution is todesign TTGDA algorithms that are effective beyond the convex-concave setting,efficiently finding a stationary point of the function $\Phi(\cdot) :=\max_{\textbf{y} \in Y} f(\cdot, \textbf{y})$. We also establish theoreticalbounds on the complexity of solving both smooth and nonsmooth nonconvex-concaveminimax optimization problems. To the best of our knowledge, this is the firstsystematic analysis of TTGDA for nonconvex minimax optimization, shedding lighton its superior performance in training generative adversarial networks (GANs)and in other real-world application problems.</description><author>Tianyi Lin, Chi Jin, Michael. I. Jordan</author><pubDate>Thu, 26 Sep 2024 16:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11974v2</guid></item><item><title>HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams</title><link>http://arxiv.org/abs/2409.18047v1</link><description>This paper presents a novel approach to multi-robot planning andcollaboration. We demonstrate a cognitive strategy for robots in human-robotteams that incorporates metacognition, natural language communication, andexplainability. The system is embodied using the HARMONIC architecture thatflexibly integrates cognitive and control capabilities across the team. Weevaluate our approach through simulation experiments involving a joint searchtask by a team of heterogeneous robots (a UGV and a drone) and a human. Wedetail the system's handling of complex, real-world scenarios, effective actioncoordination between robots with different capabilities, and naturalhuman-robot communication. This work demonstrates that the robots' ability toreason about plans, goals, and attitudes, and to provide explanations foractions and decisions are essential prerequisites for realistic human-robotteaming.</description><author>Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt</author><pubDate>Thu, 26 Sep 2024 16:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18047v1</guid></item><item><title>IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning</title><link>http://arxiv.org/abs/2409.18046v1</link><description>Recent advancements in image captioning have explored text-only trainingmethods to overcome the limitations of paired image-text data. However,existing text-only training methods often overlook the modality gap betweenusing text data during training and employing images during inference. Toaddress this issue, we propose a novel approach called Image-like Retrieval,which aligns text features with visually relevant features to mitigate themodality gap. Our method further enhances the accuracy of generated captions bydesigning a Fusion Module that integrates retrieved captions with inputfeatures. Additionally, we introduce a Frequency-based Entity Filteringtechnique that significantly improves caption quality. We integrate thesemethods into a unified framework, which we refer to as IFCap($\textbf{I}$mage-like Retrieval and $\textbf{F}$requency-based EntityFiltering for Zero-shot $\textbf{Cap}$tioning). Through extensiveexperimentation, our straightforward yet powerful approach has demonstrated itsefficacy, outperforming the state-of-the-art methods by a significant margin inboth image captioning and video captioning compared to zero-shot captioningbased on text-only training.</description><author>Soeun Lee, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim</author><pubDate>Thu, 26 Sep 2024 16:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18046v1</guid></item><item><title>Unveiling the Role of Pretraining in Direct Speech Translation</title><link>http://arxiv.org/abs/2409.18044v1</link><description>Direct speech-to-text translation systems encounter an important drawback indata scarcity. A common solution consists on pretraining the encoder onautomatic speech recognition, hence losing efficiency in the training process.In this study, we compare the training dynamics of a system using a pretrainedencoder, the conventional approach, and one trained from scratch. We observethat, throughout the training, the randomly initialized model struggles toincorporate information from the speech inputs for its predictions. Hence, wehypothesize that this issue stems from the difficulty of effectively trainingan encoder for direct speech translation. While a model trained from scratchneeds to learn acoustic and semantic modeling simultaneously, a pretrained onecan just focus on the latter. Based on these findings, we propose a subtlechange in the decoder cross-attention to integrate source information fromearlier steps in training. We show that with this change, the model trainedfrom scratch can achieve comparable performance to the pretrained one, whilereducing the training time.</description><author>Belen Alastruey, Gerard I. G√°llego, Marta R. Costa-juss√†</author><pubDate>Thu, 26 Sep 2024 16:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18044v1</guid></item><item><title>EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions</title><link>http://arxiv.org/abs/2409.18042v1</link><description>GPT-4o, an omni-modal model that enables vocal conversations with diverseemotions and tones, marks a milestone for omni-modal foundation models.However, empowering Large Language Models to perceive and generate images,texts, and speeches end-to-end with publicly available data remains challengingin the open-source community. Existing vision-language models rely on externaltools for the speech processing, while speech-language models still suffer fromlimited or even without vision-understanding abilities. To address this gap, wepropose EMOVA (EMotionally Omni-present Voice Assistant), to enable LargeLanguage Models with end-to-end speech capabilities while maintaining theleading vision-language performance. With a semantic-acoustic disentangledspeech tokenizer, we notice surprisingly that omni-modal alignment can furtherenhance vision-language and speech abilities compared with the correspondingbi-modal aligned counterparts. Moreover, a lightweight style module is proposedfor flexible speech style controls (e.g., emotions and pitches). For the firsttime, EMOVA achieves state-of-the-art performance on both the vision-languageand speech benchmarks, and meanwhile, supporting omni-modal spoken dialoguewith vivid emotions.</description><author>Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Lanqing Hong, Lu Hou, Hang Xu</author><pubDate>Thu, 26 Sep 2024 16:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18042v1</guid></item><item><title>HARMONIC: A Framework for Explanatory Cognitive Robots</title><link>http://arxiv.org/abs/2409.18037v1</link><description>We present HARMONIC, a framework for implementing cognitive robots thattransforms general-purpose robots into trusted teammates capable of complexdecision-making, natural communication and human-level explanation. Theframework supports interoperability between a strategic (cognitive) layer forhigh-level decision-making and a tactical (robot) layer for low-level controland execution. We describe the core features of the framework and our initialimplementation, in which HARMONIC was deployed on a simulated UGV and droneinvolved in a multi-robot search and retrieval task.</description><author>Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt</author><pubDate>Thu, 26 Sep 2024 16:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18037v1</guid></item><item><title>Ascend HiFloat8 Format for Deep Learning</title><link>http://arxiv.org/abs/2409.16626v2</link><description>This preliminary white paper proposes a novel 8-bit floating-point dataformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features taperedprecision. For normal value encoding, it provides 7 exponent values with 3-bitmantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with1-bit mantissa. For denormal value encoding, it extends the dynamic range by 7extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades).Meanwhile, HiF8 encodes all the special values except that positive zero andnegative zero are represented by only one bit-pattern. Thanks to the betterbalance between precision and dynamic range, HiF8 can be simultaneously used inboth forward and backward passes of AI training. In this paper, we willdescribe the definition and rounding methods of HiF8, as well as the tentativetraining and inference solutions. To demonstrate the efficacy of HiF8, massivesimulation results on various neural networks, including traditional neuralnetworks and large language models (LLMs), will also be presented.</description><author>Yuanyong Luo, Zhongxing Zhang, Richard Wu, Hu Liu, Ying Jin, Kai Zheng, Minmin Wang, Zhanying He, Guipeng Hu, Luyao Chen, Tianchi Hu, Junsong Wang, Minqi Chen, Mikhaylov Dmitry, Korviakov Vladimir, Bobrin Maxim, Yuhao Hu, Guanfu Chen, Zeyi Huang</author><pubDate>Thu, 26 Sep 2024 16:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16626v2</guid></item><item><title>Automated Detection and Analysis of Power Words in Persuasive Text Using Natural Language Processing</title><link>http://arxiv.org/abs/2409.18033v1</link><description>Power words are terms that evoke strong emotional responses and significantlyinfluence readers' behavior, playing a crucial role in fields like marketing,politics, and motivational writing. This study proposes a methodology for theautomated detection and analysis of power words in persuasive text using acustom lexicon and the TextBlob library in Python. By identifying the presenceand frequency of power words within a given text, we aim to classify andanalyze their impact on sentiment and reader engagement. This research examinesdiverse datasets across various domains to provide insights into theeffectiveness of power words, offering practical applications for contentcreators, advertisers, and policymakers.</description><author>Sahil Garje</author><pubDate>Thu, 26 Sep 2024 16:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18033v1</guid></item><item><title>FlowBench: A Large Scale Benchmark for Flow Simulation over Complex Geometries</title><link>http://arxiv.org/abs/2409.18032v1</link><description>Simulating fluid flow around arbitrary shapes is key to solving variousengineering problems. However, simulating flow physics across complexgeometries remains numerically challenging and computationallyresource-intensive, particularly when using conventional PDE solvers. Machinelearning methods offer attractive opportunities to create fast and adaptablePDE solvers. However, benchmark datasets to measure the performance of suchmethods are scarce, especially for flow physics across complex geometries. Weintroduce FlowBench, a dataset for neural simulators with over 10K samples,which is currently larger than any publicly available flow physics dataset.FlowBench contains flow simulation data across complex geometries(\textit{parametric vs. non-parametric}), spanning a range of flow conditions(\textit{Reynolds number and Grashoff number}), capturing a diverse array offlow phenomena (\textit{steady vs. transient; forced vs. free convection}), andfor both 2D and 3D. FlowBench contains over 10K data samples, with each samplethe outcome of a fully resolved, direct numerical simulation using awell-validated simulator framework designed for modeling transport phenomena incomplex geometries. For each sample, we include velocity, pressure, andtemperature field data at 3 different resolutions and several summarystatistics features of engineering relevance (such as coefficients of lift anddrag, and Nusselt numbers). %Additionally, we include masks and signed distancefields for each shape. We envision that FlowBench will enable evaluating theinterplay between complex geometry, coupled flow phenomena, and datasufficiency on the performance of current, and future, neural PDE solvers. Weenumerate several evaluation metrics to help rank order the performance ofneural PDE solvers. We benchmark the performance of several baseline methodsincluding FNO, CNO, WNO, and DeepONet.</description><author>Ronak Tali, Ali Rabeh, Cheng-Hau Yang, Mehdi Shadkhah, Samundra Karki, Abhisek Upadhyaya, Suriya Dhakshinamoorthy, Marjan Saadati, Soumik Sarkar, Adarsh Krishnamurthy, Chinmay Hegde, Aditya Balu, Baskar Ganapathysubramanian</author><pubDate>Thu, 26 Sep 2024 16:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18032v1</guid></item><item><title>Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of Peptides</title><link>http://arxiv.org/abs/2408.15126v5</link><description>Molecular Dynamics (MD) is crucial in various fields such as materialsscience, chemistry, and pharmacology to name a few. Conventional MD softwarestruggles with the balance between time cost and prediction accuracy, whichrestricts its wider application. Recently, data-driven approaches based on deepgenerative models have been devised for time-coarsened dynamics, which aim atlearning dynamics of diverse molecular systems over a long timestep, enjoyingboth universality and efficiency. Nevertheless, most current methods aredesigned solely to learn from the data distribution regardless of theunderlying Boltzmann distribution, and the physics priors such as energies andforces are constantly overlooked. In this work, we propose a conditionalgenerative model called Force-guided Bridge Matching (FBM), which learnsfull-atom time-coarsened dynamics and targets the Boltzmann-constraineddistribution. With the guidance of our delicately-designed intermediate forcefield, FBM leverages favourable physics priors into the generation process,giving rise to enhanced simulations. Experiments on two datasets consisting ofpeptides verify our superiority in terms of comprehensive metrics anddemonstrate transferability to unseen systems.</description><author>Ziyang Yu, Wenbing Huang, Yang Liu</author><pubDate>Thu, 26 Sep 2024 16:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15126v5</guid></item><item><title>Exploring Event-based Human Pose Estimation with 3D Event Representations</title><link>http://arxiv.org/abs/2311.04591v4</link><description>Human pose estimation is a fundamental and appealing task in computer vision.Although traditional cameras are commonly applied, their reliability decreasesin scenarios under high dynamic range or heavy motion blur, where event camerasoffer a robust solution. Predominant event-based methods accumulate events intoframes, ignoring the asynchronous and high temporal resolution that is crucialfor distinguishing distinct actions. To address this issue and to unlock the 3Dpotential of event information, we introduce two 3D event representations: theRasterized Event Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). TheRasEPC aggregates events within concise temporal slices at identical positions,preserving their 3D attributes along with statistical information, therebysignificantly reducing memory and computational demands. Meanwhile, the DEVrepresentation discretizes events into voxels and projects them across threeorthogonal planes, utilizing decoupled event attention to retrieve 3D cues fromthe 2D planes. Furthermore, we develop and release EV-3DPW, a syntheticevent-based dataset crafted to facilitate training and quantitative analysis inoutdoor scenes. Our methods are tested on the DHP19 public dataset, MMHPSDdataset, and our EV-3DPW dataset, with further qualitative validation via aderived driving scene dataset EV-JAAD and an outdoor collection vehicle. Ourcode and dataset have been made publicly available athttps://github.com/MasterHow/EventPointPose.</description><author>Xiaoting Yin, Hao Shi, Jiaan Chen, Ze Wang, Yaozu Ye, Kailun Yang, Kaiwei Wang</author><pubDate>Thu, 26 Sep 2024 16:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04591v4</guid></item><item><title>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title><link>http://arxiv.org/abs/2409.13731v3</link><description>The recently developed retrieval-augmented generation (RAG) technology hasenabled the efficient construction of domain-specific applications. However, italso has limitations, including the gap between vector similarity and therelevance of knowledge reasoning, as well as insensitivity to knowledge logic,such as numerical values, temporal relations, expert rules, and others, whichhinder the effectiveness of professional knowledge services. In this work, weintroduce a professional domain knowledge service framework called KnowledgeAugmented Generation (KAG). KAG is designed to address the aforementionedchallenges with the motivation of making full use of the advantages ofknowledge graph(KG) and vector retrieval, and to improve generation andreasoning performance by bidirectionally enhancing large language models (LLMs)and KGs through five key aspects: (1) LLM-friendly knowledge representation,(2) mutual-indexing between knowledge graphs and original chunks, (3)logical-form-guided hybrid reasoning engine, (4) knowledge alignment withsemantic reasoning, and (5) model capability enhancement for KAG. We comparedKAG with existing RAG methods in multihop question answering and found that itsignificantly outperforms state-of-theart methods, achieving a relativeimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. Wehave successfully applied KAG to two professional knowledge Q&amp;A tasks of AntGroup, including E-Government Q&amp;A and E-Health Q&amp;A, achieving significantimprovement in professionalism compared to RAG methods.</description><author>Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou</author><pubDate>Thu, 26 Sep 2024 16:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13731v3</guid></item><item><title>Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective</title><link>http://arxiv.org/abs/2409.18028v1</link><description>A common practice in large language model (LLM) usage for complex analyticaltasks such as code generation, is to sample a solution for the entire taskwithin the model's context window. Previous works have shown that subtaskdecomposition within the model's context (chain of thought), is beneficial forsolving such tasks. In this work, we point a limitation of LLMs' ability toperform several sub-tasks within the same context window - an in-contexthardness of composition, pointing to an advantage for distributing a decomposedproblem in a multi-agent system of LLMs. The hardness of composition isquantified by a generation complexity metric, i.e., the number of LLMgenerations required to sample at least one correct solution. We find a gapbetween the generation complexity of solving a compositional problem within thesame context relative to distributing it among multiple agents, that increasesexponentially with the solution's length. We prove our results theoreticallyand demonstrate them empirically.</description><author>Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua</author><pubDate>Thu, 26 Sep 2024 16:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18028v1</guid></item><item><title>ReliOcc: Towards Reliable Semantic Occupancy Prediction via Uncertainty Learning</title><link>http://arxiv.org/abs/2409.18026v1</link><description>Vision-centric semantic occupancy prediction plays a crucial role inautonomous driving, which requires accurate and reliable predictions fromlow-cost sensors. Although having notably narrowed the accuracy gap with LiDAR,there is still few research effort to explore the reliability in predictingsemantic occupancy from camera. In this paper, we conduct a comprehensiveevaluation of existing semantic occupancy prediction models from a reliabilityperspective for the first time. Despite the gradual alignment of camera-basedmodels with LiDAR in term of accuracy, a significant reliability gap persists.To addresses this concern, we propose ReliOcc, a method designed to enhance thereliability of camera-based occupancy networks. ReliOcc provides aplug-and-play scheme for existing models, which integrates hybrid uncertaintyfrom individual voxels with sampling-based noise and relative voxels throughmix-up learning. Besides, an uncertainty-aware calibration strategy is devisedto further enhance model reliability in offline mode. Extensive experimentsunder various settings demonstrate that ReliOcc significantly enhances modelreliability while maintaining the accuracy of both geometric and semanticpredictions. Importantly, our proposed approach exhibits robustness to sensorfailures and out of domain noises during inference.</description><author>Song Wang, Zhongdao Wang, Jiawei Yu, Wentong Li, Bailan Feng, Junbo Chen, Jianke Zhu</author><pubDate>Thu, 26 Sep 2024 16:33:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18026v1</guid></item><item><title>An Adversarial Perspective on Machine Unlearning for AI Safety</title><link>http://arxiv.org/abs/2409.18025v1</link><description>Large language models are finetuned to refuse questions about hazardousknowledge, but these protections can often be bypassed. Unlearning methods aimat completely removing hazardous capabilities from models and make theminaccessible to adversaries. This work challenges the fundamental differencesbetween unlearning and traditional safety post-training from an adversarialperspective. We demonstrate that existing jailbreak methods, previouslyreported as ineffective against unlearning, can be successful when appliedcarefully. Furthermore, we develop a variety of adaptive methods that recovermost supposedly unlearned capabilities. For instance, we show that finetuningon 10 unrelated examples or removing specific directions in the activationspace can recover most hazardous capabilities for models edited with RMU, astate-of-the-art unlearning method. Our findings challenge the robustness ofcurrent unlearning approaches and question their advantages over safetytraining.</description><author>Jakub ≈Åucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tram√®r, Javier Rando</author><pubDate>Thu, 26 Sep 2024 16:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18025v1</guid></item><item><title>DARE: Diverse Visual Question Answering with Robustness Evaluation</title><link>http://arxiv.org/abs/2409.18023v1</link><description>Vision Language Models (VLMs) extend remarkable capabilities of text-onlylarge language models and vision-only models, and are able to learn from andprocess multi-modal vision-text input. While modern VLMs perform well on anumber of standard image classification and image-text matching tasks, theystill struggle with a number of crucial vision-language (VL) reasoningabilities such as counting and spatial reasoning. Moreover, while they might bevery brittle to small variations in instructions and/or evaluation protocols,existing benchmarks fail to evaluate their robustness (or rather the lack ofit). In order to couple challenging VL scenarios with comprehensive robustnessevaluation, we introduce DARE, Diverse Visual Question Answering withRobustness Evaluation, a carefully created and curated multiple-choice VQAbenchmark. DARE evaluates VLM performance on five diverse categories andincludes four robustness-oriented evaluations based on the variations of:prompts, the subsets of answer options, the output format and the number ofcorrect answers. Among a spectrum of other findings, we report thatstate-of-the-art VLMs still struggle with questions in most categories and areunable to consistently deliver their peak performance across the testedrobustness evaluations. The worst case performance across the subsets ofoptions is up to 34% below the performance in the standard case. The robustnessof the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match theclosed-source models such as GPT-4 and Gemini, but even the latter remain verybrittle to different variations.</description><author>Hannah Sterz, Jonas Pfeiffer, Ivan Vuliƒá</author><pubDate>Thu, 26 Sep 2024 16:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18023v1</guid></item><item><title>Transferring disentangled representations: bridging the gap between synthetic and real images</title><link>http://arxiv.org/abs/2409.18017v1</link><description>Developing meaningful and efficient representations that separate thefundamental structure of the data generation mechanism is crucial inrepresentation learning. However, Disentangled Representation Learning has notfully shown its potential on real images, because of correlated generativefactors, their resolution and limited access to ground truth labels.Specifically on the latter, we investigate the possibility of leveragingsynthetic data to learn general-purpose disentangled representations applicableto real data, discussing the effect of fine-tuning and what properties ofdisentanglement are preserved after the transfer. We provide an extensiveempirical study to address these issues. In addition, we propose a newinterpretable intervention-based metric, to measure the quality of factorsencoding in the representation. Our results indicate that some level ofdisentanglement, transferring a representation from synthetic to real data, ispossible and effective.</description><author>Jacopo Dapueto, Nicoletta Noceti, Francesca Odone</author><pubDate>Thu, 26 Sep 2024 16:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18017v1</guid></item><item><title>Synthesizing Environment-Specific People in Photographs</title><link>http://arxiv.org/abs/2312.14579v2</link><description>We present ESP, a novel method for context-aware full-body generation, thatenables photo-realistic synthesis and inpainting of people wearing clothingthat is semantically appropriate for the scene depicted in an input photograph.ESP is conditioned on a 2D pose and contextual cues that are extracted from thephotograph of the scene and integrated into the generation process, where theclothing is modeled explicitly with human parsing masks (HPM). Generated HPMsare used as tight guiding masks for inpainting, such that no changes are madeto the original background. Our models are trained on a dataset containing aset of in-the-wild photographs of people covering a wide range of differentenvironments. The method is analyzed quantitatively and qualitatively, and weshow that ESP outperforms the state-of-the-art on the task of contextualfull-body generation.</description><author>Mirela Ostrek, Carol O'Sullivan, Michael J. Black, Justus Thies</author><pubDate>Thu, 26 Sep 2024 16:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14579v2</guid></item><item><title>Relating Superconducting Optoelectronic Networks to Classical Neurodynamics</title><link>http://arxiv.org/abs/2409.18016v1</link><description>The circuits comprising superconducting optoelectronic synapses, dendrites,and neurons are described by numerically cumbersome and formally opaque coupleddifferential equations. Reference 1 showed that a phenomenological model ofsuperconducting loop neurons eliminates the need to solve the Josephson circuitequations that describe synapses and dendrites. The initial goal of the modelwas to decrease the time required for simulations, yet an additional benefit ofthe model was increased transparency of the underlying neural circuitoperations and conceptual clarity regarding the connection of loop neurons toother physical systems. Whereas the original model simplified the treatment ofthe Josephson-junction dynamics, essentially by only considering low-passversions of the dendritic outputs, the model resorted to an awkward treatmentof spikes generated by semiconductor transmitter circuits that requiredexplicitly checking for threshold crossings and distinct treatment of timesteps wherein somatic threshold is reached. Here we extend that model tosimplify the treatment of spikes coming from somas, again making use of thefact that in neural systems the downstream recipients of spike events almostalways perform low-pass filtering. We provide comparisons between the first andsecond phenomenological models, quantifying the accuracy of the additionalapproximations. We identify regions of circuit parameter space in which theextended model works well and regions where it works poorly. For some circuitparameters it is possible to represent the downstream dendritic response to asingle spike as well as coincidences or sequences of spikes, indicating themodel is not simply a reduction to rate coding. The governing equations areshown to be nearly identical to those ubiquitous in the neuroscience literaturefor modeling leaky-integrator dendrites and neurons.</description><author>Jeffrey M. Shainline, Bryce A. Primavera, Ryan O'Loughlin</author><pubDate>Thu, 26 Sep 2024 16:23:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18016v1</guid></item><item><title>Role-RL: Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles</title><link>http://arxiv.org/abs/2409.18014v1</link><description>Large language models (LLMs) with long-context processing are stillchallenging because of their implementation complexity, training efficiency anddata sparsity. To address this issue, a new paradigm named Online Long-contextProcessing (OLP) is proposed when we process a document of unlimited length,which typically occurs in the information reception and organization of diversestreaming media such as automated news reporting, live e-commerce, and viralshort videos. Moreover, a dilemma was often encountered when we tried to selectthe most suitable LLM from a large number of LLMs amidst explosive growthaiming for outstanding performance, affordable prices, and short responsedelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)to automatically deploy different LLMs in their respective roles within the OLPpipeline according to their actual performance. Extensive experiments areconducted on our OLP-MINI dataset and it is found that OLP with Role-RLframework achieves OLP benchmark with an average recall rate of 93.2% and theLLM cost saved by 79.4%. The code and dataset are publicly available at:https://anonymous.4open.science/r/Role-RL.</description><author>Lewei He, Tianyu Shi, Pengran Huang, Bingzhi Chen, Qianglong Chen, Jiahui Pan</author><pubDate>Thu, 26 Sep 2024 16:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18014v1</guid></item><item><title>Spatiotemporal Learning on Cell-embedded Graphs</title><link>http://arxiv.org/abs/2409.18013v1</link><description>Data-driven simulation of physical systems has recently kindled significantattention, where many neural models have been developed. In particular,mesh-based graph neural networks (GNNs) have demonstrated significant potentialin predicting spatiotemporal dynamics across arbitrary geometric domains.However, the existing node-edge message passing mechanism in GNNs limits themodel's representation learning ability. In this paper, we proposed acell-embedded GNN model (aka CeGNN) to learn spatiotemporal dynamics withlifted performance. Specifically, we introduce a learnable cell attribution tothe node-edge message passing process, which better captures the spatialdependency of regional features. Such a strategy essentially upgrades the localaggregation scheme from the first order (e.g., from edge to node) to a higherorder (e.g., from volume to edge and then to node), which takes advantage ofvolumetric information in message passing. Meanwhile, a novel feature-enhancedblock is designed to further improve the performance of CeGNN and relieve theover-smoothness problem, via treating the latent features as basis functions.The extensive experiments on various PDE systems and one real-world datasetdemonstrate that CeGNN achieves superior performance compared with otherbaseline models, particularly reducing the prediction error with up to 1 ordersof magnitude on several PDE systems.</description><author>Yuan Mi, Hao Sun</author><pubDate>Thu, 26 Sep 2024 16:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18013v1</guid></item><item><title>End-to-end guarantees for indirect data-driven control of bilinear systems with finite stochastic data</title><link>http://arxiv.org/abs/2409.18010v1</link><description>In this paper we propose an end-to-end algorithm for indirect data-drivencontrol for bilinear systems with stability guarantees. We consider the casewhere the collected i.i.d. data is affected by probabilistic noise withpossibly unbounded support and leverage tools from statistical learning theoryto derive finite sample identification error bounds. To this end, we solve thebilinear identification problem by solving a set of linear and affineidentification problems, by a particular choice of a control input during thedata collection phase. We provide a priori as well as data-dependent finitesample identification error bounds on the individual matrices as well asellipsoidal bounds, both of which are structurally suitable for control.Further, we integrate the structure of the derived identification error boundsin a robust controller design to obtain an exponentially stable closed-loop. Bymeans of an extensive numerical study we showcase the interplay between thecontroller design and the derived identification error bounds. Moreover, wenote appealing connections of our results to indirect data-driven control ofgeneral nonlinear systems through Koopman operator theory and discuss how ourresults may be applied in this setup.</description><author>Nicolas Chatzikiriakos, Robin Str√§sser, Frank Allg√∂wer, Andrea Iannelli</author><pubDate>Thu, 26 Sep 2024 16:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18010v1</guid></item><item><title>Control Industrial Automation System with Large Language Models</title><link>http://arxiv.org/abs/2409.18009v1</link><description>Traditional industrial automation systems require specialized expertise tooperate and complex reprogramming to adapt to new processes. Large languagemodels offer the intelligence to make them more flexible and easier to use.However, LLMs' application in industrial settings is underexplored. This paperintroduces a framework for integrating LLMs to achieve end-to-end control ofindustrial automation systems. At the core of the framework are an agent systemdesigned for industrial tasks, a structured prompting method, and anevent-driven information modeling mechanism that provides real-time data forLLM inference. The framework supplies LLMs with real-time events on differentcontext semantic levels, allowing them to interpret the information, generateproduction plans, and control operations on the automation system. It alsosupports structured dataset creation for fine-tuning on this downstreamapplication of LLMs. Our contribution includes a formal system design,proof-of-concept implementation, and a method for generating task-specificdatasets for LLM fine-tuning and testing. This approach enables a more adaptiveautomation system that can respond to spontaneous events, while allowing easieroperation and configuration through natural language for more intuitivehuman-machine interaction. We provide demo videos and detailed data on GitHub:https://github.com/YuchenXia/LLM4IAS</description><author>Yuchen Xia, Nasser Jazdi, Jize Zhang, Chaitanya Shah, Michael Weyrich</author><pubDate>Thu, 26 Sep 2024 16:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18009v1</guid></item><item><title>Multilingual Evaluation of Long Context Retrieval and Reasoning</title><link>http://arxiv.org/abs/2409.18006v1</link><description>Recent large language models (LLMs) demonstrate impressive capabilities inhandling long contexts, some exhibiting near-perfect recall on syntheticretrieval tasks. However, these evaluations have mainly focused on English textand involved a single target sentence within lengthy contexts. Our workinvestigates how LLM performance generalizes to multilingual settings withmultiple hidden target sentences. We comprehensively evaluate severallong-context LLMs on retrieval and reasoning tasks across five languages:English, Vietnamese, Indonesian, Swahili, and Somali. These languages share theLatin script but belong to distinct language families and resource levels. Ouranalysis reveals a significant performance gap between languages. Thebest-performing models such as Gemini-1.5 and GPT-4o, achieve around 96%accuracy in English to around 36% in Somali with a single target sentence.However, this accuracy drops to 40% in English and 0% in Somali when dealingwith three target sentences. Our findings highlight the challenges long-contextLLMs face when processing longer contexts, an increase in the number of targetsentences, or languages of lower resource levels.</description><author>Ameeta Agrawal, Andy Dang, Sina Bagheri Nezhad, Rhitabrat Pokharel, Russell Scheinberg</author><pubDate>Thu, 26 Sep 2024 16:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18006v1</guid></item><item><title>Valeo4Cast: A Modular Approach to End-to-End Forecasting</title><link>http://arxiv.org/abs/2406.08113v3</link><description>Motion forecasting is crucial in autonomous driving systems to anticipate thefuture trajectories of surrounding agents such as pedestrians, vehicles, andtraffic signals. In end-to-end forecasting, the model must jointly detect andtrack from sensor data (cameras or LiDARs) the past trajectories of thedifferent elements of the scene and predict their future locations. We departfrom the current trend of tackling this task via end-to-end training fromperception to forecasting, and instead use a modular approach. We individuallybuild and train detection, tracking and forecasting modules. We then only useconsecutive finetuning steps to integrate the modules better and alleviatecompounding errors. We conduct an in-depth study on the finetuning strategiesand it reveals that our simple yet effective approach significantly improvesperformance on the end-to-end forecasting benchmark. Consequently, our solutionranks first in the Argoverse 2 End-to-end Forecasting Challenge, with 63.82mAPf. We surpass forecasting results by +17.1 points over last year's winnerand by +13.3 points over this year's runner-up. This remarkable performance inforecasting can be explained by our modular paradigm, which integratesfinetuning strategies and significantly outperforms the end-to-end-trainedcounterparts. The code, model weights and results are made availablehttps://github.com/valeoai/valeo4cast.</description><author>Yihong Xu, √âloi Zablocki, Alexandre Boulch, Gilles Puy, Mickael Chen, Florent Bartoccioni, Nermin Samet, Oriane Sim√©oni, Spyros Gidaris, Tuan-Hung Vu, Andrei Bursuc, Eduardo Valle, Renaud Marlet, Matthieu Cord</author><pubDate>Thu, 26 Sep 2024 16:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08113v3</guid></item><item><title>Disentangled Clothed Avatar Generation from Text Descriptions</title><link>http://arxiv.org/abs/2312.05295v2</link><description>In this paper, we introduce a novel text-to-avatar generation method thatseparately generates the human body and the clothes and allows high-qualityanimation on the generated avatar. While recent advancements in text-to-avatargeneration have yielded diverse human avatars from text prompts, these methodstypically combine all elements-clothes, hair, and body-into a single 3Drepresentation. Such an entangled approach poses challenges for downstreamtasks like editing or animation. To overcome these limitations, we propose anovel disentangled 3D avatar representation named Sequentially Offset-SMPL(SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body andclothes with two separate meshes but associates them with offsets to ensure thephysical alignment between the body and the clothes. Then, we design a ScoreDistillation Sampling (SDS)-based distillation framework to generate theproposed SO-SMPL representation from text prompts. Our approach not onlyachieves higher texture and geometry quality and better semantic alignment withtext prompts, but also significantly improves the visual quality of characteranimation, virtual try-on, and avatar editing. Project page:https://shanemankiw.github.io/SO-SMPL/.</description><author>Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Cheng Lin, Xin Li, Wenping Wang, Rong Xie, Li Song</author><pubDate>Thu, 26 Sep 2024 16:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05295v2</guid></item><item><title>Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel</title><link>http://arxiv.org/abs/2409.18000v1</link><description>Ensuring safety is a key aspect in sequential decision making problems, suchas robotics or process control. The complexity of the underlying systems oftenmakes finding the optimal decision challenging, especially when thesafety-critical system is time-varying. Overcoming the problem of optimizing anunknown time-varying reward subject to unknown time-varying safety constraints,we propose TVSafeOpt, a new algorithm built on Bayesian optimization with aspatio-temporal kernel. The algorithm is capable of safely tracking atime-varying safe region without the need for explicit change detection.Optimality guarantees are also provided for the algorithm when the optimizationproblem becomes stationary. We show that TVSafeOpt compares favorably againstSafeOpt on synthetic data, both regarding safety and optimality. Evaluation ona realistic case study with gas compressors confirms that TVSafeOpt ensuressafety when solving time-varying optimization problems with unknown reward andsafety functions.</description><author>Jialin Li, Marta Zagorowska, Giulia De Pasquale, Alisa Rupenyan, John Lygeros</author><pubDate>Thu, 26 Sep 2024 16:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18000v1</guid></item><item><title>PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging</title><link>http://arxiv.org/abs/2409.17996v1</link><description>Lensless cameras offer significant advantages in size, weight, and costcompared to traditional lens-based systems. Without a focusing lens, lenslesscameras rely on computational algorithms to recover the scenes from multiplexedmeasurements. However, current algorithms struggle with inaccurate forwardimaging models and insufficient priors to reconstruct high-quality images. Toovercome these limitations, we introduce a novel two-stage approach forconsistent and photorealistic lensless image reconstruction. The first stage ofour approach ensures data consistency by focusing on accurately reconstructingthe low-frequency content with a spatially varying deconvolution method thatadjusts to changes in the Point Spread Function (PSF) across the camera's fieldof view. The second stage enhances photorealism by incorporating a generativeprior from pre-trained diffusion models. By conditioning on the low-frequencycontent retrieved in the first stage, the diffusion model effectivelyreconstructs the high-frequency details that are typically lost in the lenslessimaging process, while also maintaining image fidelity. Our method achieves asuperior balance between data fidelity and visual quality compared to existingmethods, as demonstrated with two popular lensless systems, PhlatCam andDiffuserCam. Project website: https://phocolens.github.io/.</description><author>Xin Cai, Zhiyuan You, Hailong Zhang, Wentao Liu, Jinwei Gu, Tianfan Xue</author><pubDate>Thu, 26 Sep 2024 16:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17996v1</guid></item><item><title>Joint Localization and Planning using Diffusion</title><link>http://arxiv.org/abs/2409.17995v1</link><description>Diffusion models have been successfully applied to robotics problems such asmanipulation and vehicle path planning. In this work, we explore theirapplication to end-to-end navigation -- including both perception and planning-- by considering the problem of jointly performing global localization andpath planning in known but arbitrary 2D environments. In particular, weintroduce a diffusion model which produces collision-free paths in a globalreference frame given an egocentric LIDAR scan, an arbitrary map, and a desiredgoal position. To this end, we implement diffusion in the space of paths inSE(2), and describe how to condition the denoising process on both obstaclesand sensor observations. In our evaluation, we show that the proposedconditioning techniques enable generalization to realistic maps of considerablydifferent appearance than the training environment, demonstrate our model'sability to accurately describe ambiguous solutions, and run extensivesimulation experiments showcasing our model's use as a real-time, end-to-endlocalization and planning stack.</description><author>L. Lao Beyer, S. Karaman</author><pubDate>Thu, 26 Sep 2024 16:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17995v1</guid></item><item><title>CRoP: Context-wise Robust Static Human-Sensing Personalization</title><link>http://arxiv.org/abs/2409.17994v1</link><description>The advancement in deep learning and internet-of-things have led to diversehuman sensing applications. However, distinct patterns in human sensing,influenced by various factors or contexts, challenge generic neural networkmodel's performance due to natural distribution shifts. To address this,personalization tailors models to individual users. Yet most personalizationstudies overlook intra-user heterogeneity across contexts in sensory data,limiting intra-user generalizability. This limitation is especially critical inclinical applications, where limited data availability hampers bothgeneralizability and personalization. Notably, intra-user sensing attributesare expected to change due to external factors such as treatment progression,further complicating the challenges.This work introduces CRoP, a novel staticpersonalization approach using an off-the-shelf pre-trained model and pruningto optimize personalization and generalization. CRoP shows superiorpersonalization effectiveness and intra-user robustness across fourhuman-sensing datasets, including two from real-world health domains,highlighting its practical and social impact. Additionally, to support CRoP'sgeneralization ability and design choices, we provide empirical justificationthrough gradient inner product analysis, ablation studies, and comparisonsagainst state-of-the-art baselines.</description><author>Sawinder Kaur, Avery Gump, Jingyu Xin, Yi Xiao, Harshit Sharma, Nina R Benway, Jonathan L Preston, Asif Salekin</author><pubDate>Thu, 26 Sep 2024 16:06:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17994v1</guid></item><item><title>MLPs Learn In-Context on Regression and Classification Tasks</title><link>http://arxiv.org/abs/2405.15618v2</link><description>In-context learning (ICL), the remarkable ability to solve a task from onlyinput exemplars, is often assumed to be a unique hallmark of Transformermodels. By examining commonly employed synthetic ICL tasks, we demonstrate thatmulti-layer perceptrons (MLPs) can also learn in-context. Moreover, MLPs, andthe closely related MLP-Mixer models, learn in-context competitively withTransformers given the same compute budget in this setting. We further showthat MLPs outperform Transformers on a series of classical tasks frompsychology designed to test relational reasoning, which are closely related toin-context classification. These results underscore a need for studyingin-context learning beyond attention-based architectures, while alsochallenging strong prior arguments about MLPs' limited ability to solverelational tasks. Altogether, our results highlight the unexpected competenceof MLPs, and support the growing interest in all-MLP alternatives totask-specific architectures.</description><author>William L. Tong, Cengiz Pehlevan</author><pubDate>Thu, 26 Sep 2024 16:05:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15618v2</guid></item><item><title>InterNet: Unsupervised Cross-modal Homography Estimation Based on Interleaved Modality Transfer and Self-supervised Homography Prediction</title><link>http://arxiv.org/abs/2409.17993v1</link><description>We propose a novel unsupervised cross-modal homography estimation framework,based on interleaved modality transfer and self-supervised homographyprediction, named InterNet. InterNet integrates modality transfer andself-supervised homography estimation, introducing an innovative interleavedoptimization framework to alternately promote both components. The modalitytransfer gradually narrows the modality gaps, facilitating the self-supervisedhomography estimation to fully leverage the synthetic intra-modal data. Theself-supervised homography estimation progressively achieves reliablepredictions, thereby providing robust cross-modal supervision for the modalitytransfer. To further boost the estimation accuracy, we also formulate afine-grained homography feature loss to improve the connection between twocomponents. Furthermore, we employ a simple yet effective distillation trainingtechnique to reduce model parameters and improve cross-domain generalizationability while maintaining comparable performance. Experiments reveal thatInterNet achieves the state-of-the-art (SOTA) performance among unsupervisedmethods, and even outperforms many supervised methods such as MHN andLocalTrans.</description><author>Junchen Yu, Si-Yuan Cao, Runmin Zhang, Chenghao Zhang, Jianxin Hu, Zhu Yu, Hui-liang Shen</author><pubDate>Thu, 26 Sep 2024 16:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17993v1</guid></item><item><title>LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots</title><link>http://arxiv.org/abs/2409.17992v1</link><description>Reinforcement Learning (RL) has shown its remarkable and generalizablecapability in legged locomotion through sim-to-real transfer. However, whileadaptive methods like domain randomization are expected to make policy morerobust to diverse environments, such comprehensiveness potentially detractsfrom the policy's performance in any specific environment according to the NoFree Lunch theorem, leading to a suboptimal solution once deployed in the realworld. To address this issue, we propose a lifelong policy adaptation frameworknamed LoopSR, which utilizes a transformer-based encoder to project real-worldtrajectories into a latent space, and accordingly reconstruct the real-worldenvironments back in simulation for further improvement. Autoencoderarchitecture and contrastive learning methods are adopted to better extract thecharacteristics of real-world dynamics. The simulation parameters for continualtraining are derived by combining predicted parameters from the decoder withretrieved parameters from the simulation trajectory dataset. By leveraging thecontinual training, LoopSR achieves superior data efficiency compared withstrong baselines, with only a limited amount of data to yield eminentperformance in both sim-to-sim and sim-to-real experiments.</description><author>Peilin Wu, Weiji Xie, Jiahang Cao, Hang Lai, Weinan Zhang</author><pubDate>Thu, 26 Sep 2024 16:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17992v1</guid></item><item><title>Dimension-independent learning rates for high-dimensional classification problems</title><link>http://arxiv.org/abs/2409.17991v1</link><description>We study the problem of approximating and estimating classification functionsthat have their decision boundary in the $RBV^2$ space. Functions of $RBV^2$type arise naturally as solutions of regularized neural network learningproblems and neural networks can approximate these functions without the curseof dimensionality. We modify existing results to show that every $RBV^2$function can be approximated by a neural network with bounded weights.Thereafter, we prove the existence of a neural network with bounded weightsapproximating a classification function. And we leverage these bounds toquantify the estimation rates. Finally, we present a numerical study thatanalyzes the effect of different regularity conditions on the decisionboundaries.</description><author>Andres Felipe Lerma-Pineda, Philipp Petersen, Simon Frieder, Thomas Lukasiewicz</author><pubDate>Thu, 26 Sep 2024 16:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17991v1</guid></item><item><title>Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models</title><link>http://arxiv.org/abs/2409.17990v1</link><description>This paper proposes temporally aligned Large Language Models (LLMs) as a toolfor longitudinal analysis of social media data. We fine-tune Temporal Adaptersfor Llama 3 8B on full timelines from a panel of British Twitter users, andextract longitudinal aggregates of emotions and attitudes with establishedquestionnaires. We validate our estimates against representative British surveydata and find strong positive, significant correlations for several collectiveemotions. The obtained estimates are robust across multiple training seeds andprompt formulations, and in line with collective emotions extracted using atraditional classification model trained on labeled data. To the best of ourknowledge, this is the first work to extend the analysis of affect in LLMs to alongitudinal setting through Temporal Adapters. Our work enables new approachestowards the longitudinal analysis of social media data.</description><author>Georg Ahnert, Max Pellert, David Garcia, Markus Strohmaier</author><pubDate>Thu, 26 Sep 2024 16:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17990v1</guid></item><item><title>Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions</title><link>http://arxiv.org/abs/2409.17988v1</link><description>The stark contrast in the design philosophy of an event camera makes itparticularly ideal for operating under high-speed, high dynamic range andlow-light conditions, where standard cameras underperform. Nonetheless, eventcameras still suffer from some amount of motion blur, especially under thesechallenging conditions, in contrary to what most think. This is attributed tothe limited bandwidth of the event sensor pixel, which is mostly proportionalto the light intensity. Thus, to ensure that event cameras can truly excel insuch conditions where it has an edge over standard cameras, it is crucial toaccount for event motion blur in downstream applications, especiallyreconstruction. However, none of the recent works on reconstructing NeuralRadiance Fields (NeRFs) from events, nor event simulators, have considered thefull effects of event motion blur. To this end, we propose, Deblur e-NeRF, anovel method to directly and effectively reconstruct blur-minimal NeRFs frommotion-blurred events generated under high-speed motion or low-lightconditions. The core component of this work is a physically-accurate pixelbandwidth model proposed to account for event motion blur under arbitrary speedand lighting conditions. We also introduce a novel threshold-normalized totalvariation loss to improve the regularization of large textureless patches.Experiments on real and novel realistically simulated sequences verify oureffectiveness. Our code, event simulator and synthetic event dataset will beopen-sourced.</description><author>Weng Fei Low, Gim Hee Lee</author><pubDate>Thu, 26 Sep 2024 15:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17988v1</guid></item><item><title>LLM4Brain: Training a Large Language Model for Brain Video Understanding</title><link>http://arxiv.org/abs/2409.17987v1</link><description>Decoding visual-semantic information from brain signals, such as functionalMRI (fMRI), across different subjects poses significant challenges, includinglow signal-to-noise ratio, limited data availability, and cross-subjectvariability. Recent advancements in large language models (LLMs) showremarkable effectiveness in processing multimodal information. In this study,we introduce an LLM-based approach for reconstructing visual-semanticinformation from fMRI signals elicited by video stimuli. Specifically, weemploy fine-tuning techniques on an fMRI encoder equipped with adaptors totransform brain responses into latent representations aligned with the videostimuli. Subsequently, these representations are mapped to textual modality byLLM. In particular, we integrate self-supervised domain adaptation methods toenhance the alignment between visual-semantic information and brain responses.Our proposed method achieves good results using various quantitative semanticmetrics, while yielding similarity with ground-truth information.</description><author>Ruizhe Zheng, Lichao Sun</author><pubDate>Thu, 26 Sep 2024 15:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17987v1</guid></item><item><title>Supra-Laplacian Encoding for Transformer on Dynamic Graphs</title><link>http://arxiv.org/abs/2409.17986v1</link><description>Fully connected Graph Transformers (GT) have rapidly become prominent in thestatic graph community as an alternative to Message-Passing models, whichsuffer from a lack of expressivity, oversquashing, and under-reaching. However,in a dynamic context, by interconnecting all nodes at multiple snapshots withself-attention, GT loose both structural and temporal information. In thiswork, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs(SLATE), a new spatio-temporal encoding to leverage the GT architecture whilekeeping spatio-temporal information. Specifically, we transform Discrete TimeDynamic Graphs into multi-layer graphs and take advantage of the spectralproperties of their associated supra-Laplacian matrix. Our second contributionexplicitly model nodes' pairwise relationships with a cross-attentionmechanism, providing an accurate edge representation for dynamic linkprediction. SLATE outperforms numerous state-of-the-art methods based onMessage-Passing Graph Neural Networks combined with recurrent models (e.gLSTM), and Dynamic Graph Transformers, on 9 datasets. Code and instructions toreproduce our results will be open-sourced.</description><author>Yannis Karmim, Marc Lafon, Rapha√´l Fournier S'niehotta, Nicolas Thome</author><pubDate>Thu, 26 Sep 2024 15:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17986v1</guid></item><item><title>A Stochastic Quasi-Newton Method for Non-convex Optimization with Non-uniform Smoothness</title><link>http://arxiv.org/abs/2403.15244v2</link><description>Classical convergence analyses for optimization algorithms rely on thewidely-adopted uniform smoothness assumption. However, recent experimentalstudies have demonstrated that many machine learning problems exhibitnon-uniform smoothness, meaning the smoothness factor is a function of themodel parameter instead of a universal constant. In particular, it has beenobserved that the smoothness grows with respect to the gradient norm along thetraining trajectory. Motivated by this phenomenon, the recently introduced$(L_0, L_1)$-smoothness is a more general notion, compared to traditional$L$-smoothness, that captures such positive relationship between smoothness andgradient norm. Under this type of non-uniform smoothness, existing literaturehas designed stochastic first-order algorithms by utilizing gradient clippingtechniques to obtain the optimal $\mathcal{O}(\epsilon^{-3})$ sample complexityfor finding an $\epsilon$-approximate first-order stationary solution.Nevertheless, the studies of quasi-Newton methods are still lacking.Considering higher accuracy and more robustness for quasi-Newton methods, inthis paper we propose a fast stochastic quasi-Newton method when there existsnon-uniformity in smoothness. Leveraging gradient clipping and variancereduction, our algorithm can achieve the best-known$\mathcal{O}(\epsilon^{-3})$ sample complexity and enjoys convergence speedupwith simple hyperparameter tuning. Our numerical experiments show that ourproposed algorithm outperforms the state-of-the-art approaches.</description><author>Zhenyu Sun, Ermin Wei</author><pubDate>Thu, 26 Sep 2024 15:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15244v2</guid></item><item><title>HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI Interactions</title><link>http://arxiv.org/abs/2409.16427v2</link><description>AI agents are increasingly autonomous in their interactions with human usersand tools, leading to increased interactional safety risks. We presentHAICOSYSTEM, a framework examining AI agent safety within diverse and complexsocial interactions. HAICOSYSTEM features a modular sandbox environment thatsimulates multi-turn interactions between human users and AI agents, where theAI agents are equipped with a variety of tools (e.g., patient managementplatforms) to navigate diverse scenarios (e.g., a user attempting to accessother patients' profiles). To examine the safety of AI agents in theseinteractions, we develop a comprehensive multi-dimensional evaluation frameworkthat uses metrics covering operational, content-related, societal, and legalrisks. Through running 1840 simulations based on 92 scenarios across sevendomains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEMcan emulate realistic user-AI interactions and complex tool use by AI agents.Our experiments show that state-of-the-art LLMs, both proprietary andopen-sourced, exhibit safety risks in over 50\% cases, with models generallyshowing higher risks when interacting with simulated malicious users. Ourfindings highlight the ongoing challenge of building agents that can safelynavigate complex interactions, particularly when faced with malicious users. Tofoster the AI agent safety ecosystem, we release a code platform that allowspractitioners to create custom scenarios, simulate interactions, and evaluatethe safety and performance of their agents.</description><author>Xuhui Zhou, Hyunwoo Kim, Faeze Brahman, Liwei Jiang, Hao Zhu, Ximing Lu, Frank Xu, Bill Yuchen Lin, Yejin Choi, Niloofar Mireshghallah, Ronan Le Bras, Maarten Sap</author><pubDate>Thu, 26 Sep 2024 15:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16427v2</guid></item><item><title>Hypergame Theory for Decentralized Resource Allocation in Multi-user Semantic Communications</title><link>http://arxiv.org/abs/2409.17985v1</link><description>Semantic communications (SC) is an emerging communication paradigm in whichwireless devices can send only relevant information from a source of data whilerelying on computing resources to regenerate missing data points. However, thedesign of a multi-user SC system becomes more challenging because of thecomputing and communication overhead required for coordination. Existingsolutions for learning the semantic language and performing resource allocationoften fail to capture the computing and communication tradeoffs involved inmultiuser SC. To address this gap, a novel framework for decentralizedcomputing and communication resource allocation in multiuser SC systems isproposed. The challenge of efficiently allocating communication and computingresources (for reasoning) in a decentralized manner to maximize the quality oftask experience for the end users is addressed through the application ofStackelberg hyper game theory. Leveraging the concept of second-level hypergames, novel analytical formulations are developed to model misperceptions ofthe users about each other's communication and control strategies. Further,equilibrium analysis of the learned resource allocation protocols examines theconvergence of the computing and communication strategies to a localStackelberg equilibria, considering misperceptions. Simulation results showthat the proposed Stackelberg hyper game results in efficient usage ofcommunication and computing resources while maintaining a high quality ofexperience for the users compared to state-of-the-art that does not account forthe misperceptions.</description><author>Christo Kurisummoottil Thomas, Walid Saad</author><pubDate>Thu, 26 Sep 2024 15:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17985v1</guid></item><item><title>BlinkTrack: Feature Tracking over 100 FPS via Events and Images</title><link>http://arxiv.org/abs/2409.17981v1</link><description>Feature tracking is crucial for, structure from motion (SFM), simultaneouslocalization and mapping (SLAM), object tracking and various computer visiontasks. Event cameras, known for their high temporal resolution and ability tocapture asynchronous changes, have gained significant attention for theirpotential in feature tracking, especially in challenging conditions. However,event cameras lack the fine-grained texture information that conventionalcameras provide, leading to error accumulation in tracking. To address this, wepropose a novel framework, BlinkTrack, which integrates event data with RGBimages for high-frequency feature tracking. Our method extends the traditionalKalman filter into a learning-based framework, utilizing differentiable Kalmanfilters in both event and image branches. This approach improvessingle-modality tracking, resolves ambiguities, and supports asynchronous datafusion. We also introduce new synthetic and augmented datasets to betterevaluate our model. Experimental results indicate that BlinkTrack significantlyoutperforms existing event-based methods, exceeding 100 FPS with preprocessedevent data and 80 FPS with multi-modality data.</description><author>Yichen Shen, Yijin Li, Shuo Chen, Guanglin Li, Zhaoyang Huang, Hujun Bao, Zhaopeng Cui, Guofeng Zhang</author><pubDate>Thu, 26 Sep 2024 15:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17981v1</guid></item><item><title>Message-Passing Monte Carlo: Generating low-discrepancy point sets via Graph Neural Networks</title><link>http://arxiv.org/abs/2405.15059v2</link><description>Discrepancy is a well-known measure for the irregularity of the distributionof a point set. Point sets with small discrepancy are called low-discrepancyand are known to efficiently fill the space in a uniform manner.Low-discrepancy points play a central role in many problems in science andengineering, including numerical integration, computer vision, machineperception, computer graphics, machine learning, and simulation. In this work,we present the first machine learning approach to generate a new class oflow-discrepancy point sets named Message-Passing Monte Carlo (MPMC) points.Motivated by the geometric nature of generating low-discrepancy point sets, weleverage tools from Geometric Deep Learning and base our model on Graph NeuralNetworks. We further provide an extension of our framework to higherdimensions, which flexibly allows the generation of custom-made points thatemphasize the uniformity in specific dimensions that are primarily importantfor the particular problem at hand. Finally, we demonstrate that our proposedmodel achieves state-of-the-art performance superior to previous methods by asignificant margin. In fact, MPMC points are empirically shown to be eitheroptimal or near-optimal with respect to the discrepancy for low dimension andsmall number of points, i.e., for which the optimal discrepancy can bedetermined. Code for generating MPMC points can be found athttps://github.com/tk-rusch/MPMC.</description><author>T. Konstantin Rusch, Nathan Kirk, Michael M. Bronstein, Christiane Lemieux, Daniela Rus</author><pubDate>Thu, 26 Sep 2024 15:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15059v2</guid></item><item><title>HydraViT: Stacking Heads for a Scalable ViT</title><link>http://arxiv.org/abs/2409.17978v1</link><description>The architecture of Vision Transformers (ViTs), particularly the Multi-headAttention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTson devices with varying constraints, such as mobile phones, requires multiplemodels of different sizes. However, this approach has limitations, such astraining and storing each required model separately. This paper introducesHydraViT, a novel approach that addresses these limitations by stackingattention heads to achieve a scalable ViT. By repeatedly changing the size ofthe embedded dimensions throughout each layer and their corresponding number ofattention heads in MHA during training, HydraViT induces multiple subnetworks.Thereby, HydraViT achieves adaptability across a wide spectrum of hardwareenvironments while maintaining performance. Our experimental resultsdemonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10subnetworks, covering a wide range of resource constraints. HydraViT achievesup to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracywith the same throughput on ImageNet-1K compared to the baselines, making it aneffective solution for scenarios where hardware availability is diverse orvaries over time. Source code available at https://github.com/ds-kiel/HydraViT.</description><author>Janek Haberer, Ali Hojjat, Olaf Landsiedel</author><pubDate>Thu, 26 Sep 2024 15:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17978v1</guid></item></channel></rss>