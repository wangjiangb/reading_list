<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 07 Nov 2024 13:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Community Forensics: Using Thousands of Generators to Train Fake Image Detectors</title><link>http://arxiv.org/abs/2411.04125v1</link><description>One of the key challenges of detecting AI-generated images is spotting imagesthat have been created by previously unseen generative models. We argue thatthe limited diversity of the training data is a major obstacle to addressingthis problem, and we propose a new dataset that is significantly larger andmore diverse than prior work. As part of creating this dataset, wesystematically download thousands of text-to-image latent diffusion models andsample images from them. We also collect images from dozens of popular opensource and commercial models. The resulting dataset contains 2.7M images thathave been sampled from 4803 different models. These images collectively capturea wide range of scene content, generator architectures, and image processingsettings. Using this dataset, we study the generalization abilities of fakeimage detectors. Our experiments suggest that detection performance improves asthe number of models in the training set increases, even when these models havesimilar architectures. We also find that detection performance improves as thediversity of the models increases, and that our trained detectors generalizebetter than those trained on other datasets.</description><author>Jeongsoo Park, Andrew Owens</author><pubDate>Wed, 06 Nov 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04125v1</guid></item><item><title>No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations</title><link>http://arxiv.org/abs/2407.10964v2</link><description>This paper introduces FUNGI, Features from UNsupervised GradIents, a methodto enhance the features of transformer encoders by leveraging self-supervisedgradients. Our method is simple: given any pretrained model, we first computegradients from various self-supervised objectives for each input. Thesegradients are projected to a lower dimension and then concatenated with themodel's output embedding. The resulting features are evaluated on k-nearestneighbor classification over 11 datasets from vision, 5 from natural languageprocessing, and 2 from audio. Across backbones spanning various sizes andpretraining strategies, FUNGI features provide consistent performanceimprovements over the embeddings. We also show that using FUNGI features canbenefit linear classification, clustering and image retrieval, and that theysignificantly improve the retrieval-based in-context scene understandingabilities of pretrained models, for example improving upon DINO by +17% forsemantic segmentation - without any training.</description><author>Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano</author><pubDate>Wed, 06 Nov 2024 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10964v2</guid></item><item><title>Engineering Trustworthy AI: A Developer Guide for Empirical Risk Minimization</title><link>http://arxiv.org/abs/2410.19361v2</link><description>AI systems increasingly shape critical decisions across personal and societaldomains. While empirical risk minimization (ERM) drives much of the AI success,it typically prioritizes accuracy over trustworthiness, often resulting inbiases, opacity, and other adverse effects. This paper discusses how keyrequirements for trustworthy AI can be translated into design choices for thecomponents of ERM. We hope to provide actionable guidance for building AIsystems that meet emerging standards for trustworthiness of AI.</description><author>Diana Pfau, Alexander Jung</author><pubDate>Wed, 06 Nov 2024 18:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19361v2</guid></item><item><title>Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?</title><link>http://arxiv.org/abs/2411.04118v1</link><description>Several recent works seek to develop foundation models specifically formedical applications, adapting general-purpose large language models (LLMs) andvision-language models (VLMs) via continued pretraining on publicly availablebiomedical corpora. These works typically claim that such domain-adaptivepretraining (DAPT) improves performance on downstream medical tasks, such asanswering medical licensing exam questions. In this paper, we compare sevenpublic "medical" LLMs and two VLMs against their corresponding base models,arriving at a different conclusion: all medical VLMs and nearly all medicalLLMs fail to consistently improve over their base models in the zero-/few-shotprompting regime for medical question-answering (QA) tasks. For instance,across the tasks and model pairs we consider in the 3-shot setting, medicalLLMs only outperform their base models in 12.1% of cases, reach a (statistical)tie in 49.8% of cases, and are significantly worse than their base models inthe remaining 38.2% of cases. Our conclusions are based on (i) comparing eachmedical model head-to-head, directly against the corresponding base model; (ii)optimizing the prompts for each model separately; and (iii) accounting forstatistical uncertainty in comparisons. While these basic practices are notconsistently adopted in the literature, our ablations show that theysubstantially impact conclusions. Our findings suggest that state-of-the-artgeneral-domain models may already exhibit strong medical knowledge andreasoning capabilities, and offer recommendations to strengthen the conclusionsof future studies.</description><author>Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst</author><pubDate>Wed, 06 Nov 2024 18:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04118v1</guid></item><item><title>Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation</title><link>http://arxiv.org/abs/2411.04112v1</link><description>Centralized learning requires data to be aggregated at a central server,which poses significant challenges in terms of data privacy and bandwidthconsumption. Federated learning presents a compelling alternative, however,vanilla federated learning methods deployed in robotics aim to learn a singleglobal model across robots that works ideally for all. But in practice onemodel may not be well suited for robots deployed in various environments. Thispaper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federatedlearning framework that is deployed with vision based autonomous robotnavigation in diverse outdoor environments. The framework addresses the keyfederated learning challenge of deteriorating model performance of a singleglobal model due to the presence of non-IID data across real-world robots.Extensive real-world experiments validate that Fed-EC reduces the communicationsize by 23x for each robot while matching the performance of centralizedlearning for goal-oriented navigation and outperforms local learning. Fed-ECcan transfer previously learnt models to new robots that join the cluster.</description><author>Shreya Gummadi, Mateus V. Gasparino, Deepak Vasisht, Girish Chowdhary</author><pubDate>Wed, 06 Nov 2024 18:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04112v1</guid></item><item><title>Self-Consistency Preference Optimization</title><link>http://arxiv.org/abs/2411.04109v1</link><description>Self-alignment, whereby models learn to improve themselves without humanannotation, is a rapidly growing research area. However, existing techniquesoften fail to improve complex reasoning tasks due to the difficulty ofassigning correct rewards. An orthogonal approach that is known to improvecorrectness is self-consistency, a method applied at inference time based onmultiple sampling in order to find the most consistent answer. In this work, weextend the self-consistency concept to help train models. We thus introduceself-consistency preference optimization (ScPO), which iteratively trainsconsistent answers to be preferred over inconsistent ones on unsupervised newproblems. We show ScPO leads to large improvements over conventional rewardmodel training on reasoning tasks such as GSM8K and MATH, closing the gap withsupervised training with gold answers or preferences, and that combining ScPOwith standard supervised learning improves results even further. On ZebraLogic,ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, andClaude-3 Haiku.</description><author>Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu</author><pubDate>Wed, 06 Nov 2024 18:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04109v1</guid></item><item><title>Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains</title><link>http://arxiv.org/abs/2411.04108v1</link><description>In this work, we consider the approximation capabilities of shallow neuralnetworks in weighted Sobolev spaces for functions in the spectral Barron space.The existing literature already covers several cases, in which the spectralBarron space can be approximated well, i.e., without curse of dimensionality,by shallow networks and several different classes of activation function. Thelimitations of the existing results are mostly on the error measures that wereconsidered, in which the results are restricted to Sobolev spaces over abounded domain. We will here treat two cases that extend upon the existingresults. Namely, we treat the case with bounded domain and Muckenhoupt weightsand the case, where the domain is allowed to be unbounded and the weights arerequired to decay. We first present embedding results for the more generalweighted Fourier-Lebesgue spaces in the weighted Sobolev spaces and then weestablish asymptotic approximation rates for shallow neural networks that comewithout curse of dimensionality.</description><author>Ahmed Abdeljawad, Thomas Dittrich</author><pubDate>Wed, 06 Nov 2024 18:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04108v1</guid></item><item><title>From Federated Learning to Quantum Federated Learning for Space-Air-Ground Integrated Networks</title><link>http://arxiv.org/abs/2411.01312v2</link><description>6G wireless networks are expected to provide seamless and data-basedconnections that cover space-air-ground and underwater networks. As a corepartition of future 6G networks, Space-Air-Ground Integrated Networks (SAGIN)have been envisioned to provide countless real-time intelligent applications.To realize this, promoting AI techniques into SAGIN is an inevitable trend. Dueto the distributed and heterogeneous architecture of SAGIN, federated learning(FL) and then quantum FL are emerging AI model training techniques for enablingfuture privacy-enhanced and computation-efficient SAGINs. In this work, weexplore the vision of using FL/QFL in SAGINs. We present a few representativeapplications enabled by the integration of FL and QFL in SAGINs. A case studyof QFL over UAV networks is also given, showing the merit of quantum-enabledtraining approach over the conventional FL benchmark. Research challenges alongwith standardization for QFL adoption in future SAGINs are also highlighted.</description><author>Vu Khanh Quy, Nguyen Minh Quy, Tran Thi Hoai, Shaba Shaon, Md Raihan Uddin, Tien Nguyen, Dinh C. Nguyen, Aryan Kaushik, Periklis Chatzimisios</author><pubDate>Wed, 06 Nov 2024 18:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01312v2</guid></item><item><title>A Comparative Study of Deep Reinforcement Learning for Crop Production Management</title><link>http://arxiv.org/abs/2411.04106v1</link><description>Crop production management is essential for optimizing yield and minimizing afield's environmental impact to crop fields, yet it remains challenging due tothe complex and stochastic processes involved. Recently, researchers haveturned to machine learning to address these complexities. Specifically,reinforcement learning (RL), a cutting-edge approach designed to learn optimaldecision-making strategies through trial and error in dynamic environments, hasemerged as a promising tool for developing adaptive crop management policies.RL models aim to optimize long-term rewards by continuously interacting withthe environment, making them well-suited for tackling the uncertainties andvariability inherent in crop management. Studies have shown that RL cangenerate crop management policies that compete with, and even outperform,expert-designed policies within simulation-based crop models. In the gym-DSSATcrop model environment, one of the most widely used simulators for cropmanagement, proximal policy optimization (PPO) and deep Q-networks (DQN) haveshown promising results. However, these methods have not yet beensystematically evaluated under identical conditions. In this study, weevaluated PPO and DQN against static baseline policies across three differentRL tasks, fertilization, irrigation, and mixed management, provided by thegym-DSSAT environment. To ensure a fair comparison, we used consistent defaultparameters, identical reward functions, and the same environment settings. Ourresults indicate that PPO outperforms DQN in fertilization and irrigationtasks, while DQN excels in the mixed management task. This comparative analysisprovides critical insights into the strengths and limitations of each approach,advancing the development of more effective RL-based crop managementstrategies.</description><author>Joseph Balderas, Dong Chen, Yanbo Huang, Li Wang, Ren-Cang Li</author><pubDate>Wed, 06 Nov 2024 18:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04106v1</guid></item><item><title>How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis</title><link>http://arxiv.org/abs/2411.04105v1</link><description>Large language models (LLMs) have shown amazing performance on tasks thatrequire planning and reasoning. Motivated by this, we investigate the internalmechanisms that underpin a network's ability to perform complex logicalreasoning. We first construct a synthetic propositional logic problem thatserves as a concrete test-bed for network training and evaluation. Crucially,this problem demands nontrivial planning to solve, but we can train a smalltransformer to achieve perfect accuracy. Building on our set-up, we then pursuean understanding of precisely how a three-layer transformer, trained fromscratch, solves this problem. We are able to identify certain "planning" and"reasoning" circuits in the network that necessitate cooperation between theattention blocks to implement the desired logic. To expand our findings, wethen study a larger model, Mistral 7B. Using activation patching, wecharacterize internal components that are critical in solving our logicproblem. Overall, our work systemically uncovers novel aspects of small andlarge transformers, and continues the study of how they plan and reason.</description><author>Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Rina Panigrahy</author><pubDate>Wed, 06 Nov 2024 18:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04105v1</guid></item><item><title>False Data Injection Attack Detection in Edge-based Smart Metering Networks with Federated Learning</title><link>http://arxiv.org/abs/2411.01313v2</link><description>Smart metering networks are increasingly susceptible to cyber threats, wherefalse data injection (FDI) appears as a critical attack. Data-driven-basedmachine learning (ML) methods have shown immense benefits in detecting FDIattacks via data learning and prediction abilities. Literature works havemostly focused on centralized learning and deploying FDI attack detectionmodels at the control center, which requires data collection from localutilities like meters and transformers. However, this data sharing may raiseprivacy concerns due to the potential disclosure of household information likeenergy usage patterns. This paper proposes a new privacy-preserved FDI attackdetection by developing an efficient federated learning (FL) framework in thesmart meter network with edge computing. Distributed edge servers located atthe network edge run an ML-based FDI attack detection model and share thetrained model with the grid operator, aiming to build a strong FDI attackdetection model without data sharing. Simulation results demonstrate theefficiency of our proposed FL method over the conventional method withoutcollaboration.</description><author>Md Raihan Uddin, Ratun Rahman, Dinh C. Nguyen</author><pubDate>Wed, 06 Nov 2024 18:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01313v2</guid></item><item><title>DeNetDM: Debiasing by Network Depth Modulation</title><link>http://arxiv.org/abs/2403.19863v4</link><description>Neural networks trained on biased datasets tend to inadvertently learnspurious correlations, hindering generalization. We formally prove that (1)samples that exhibit spurious correlations lie on a lower rank manifoldrelative to the ones that do not; and (2) the depth of a network acts as animplicit regularizer on the rank of the attribute subspace that is encoded inits representations. Leveraging these insights, we present DeNetDM, a noveldebiasing method that uses network depth modulation as a way of developingrobustness to spurious correlations. Using a training paradigm derived fromProduct of Experts, we create both biased and debiased branches with deep andshallow architectures and then distill knowledge to produce the target debiasedmodel. Our method requires no bias annotations or explicit data augmentationwhile performing on par with approaches that require either or both. Wedemonstrate that DeNetDM outperforms existing debiasing techniques on bothsynthetic and real-world datasets by 5\%. The project page is available athttps://vssilpa.github.io/denetdm/.</description><author>Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Abhra Chaudhuri, Anjan Dutta</author><pubDate>Wed, 06 Nov 2024 18:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19863v4</guid></item><item><title>Interpretable and Efficient Data-driven Discovery and Control of Distributed Systems</title><link>http://arxiv.org/abs/2411.04098v1</link><description>Effectively controlling systems governed by Partial Differential Equations(PDEs) is crucial in several fields of Applied Sciences and Engineering. Thesesystems usually yield significant challenges to conventional control schemesdue to their nonlinear dynamics, partial observability, high-dimensionalityonce discretized, distributed nature, and the requirement for low-latencyfeedback control. Reinforcement Learning (RL), particularly Deep RL (DRL), hasrecently emerged as a promising control paradigm for such systems,demonstrating exceptional capabilities in managing high-dimensional, nonlineardynamics. However, DRL faces challenges including sample inefficiency,robustness issues, and an overall lack of interpretability. To address theseissues, we propose a data-efficient, interpretable, and scalable Dyna-styleModel-Based RL framework for PDE control, combining the Sparse Identificationof Nonlinear Dynamics with Control (SINDy-C) algorithm and an autoencoder (AE)framework for the sake of dimensionality reduction of PDE states and actions.This novel approach enables fast rollouts, reducing the need for extensiveenvironment interactions, and provides an interpretable latent spacerepresentation of the PDE forward dynamics. We validate our method on two PDEproblems describing fluid flows - namely, the 1D Burgers equation and 2DNavier-Stokes equations - comparing it against a model-free baseline, andcarrying out an extensive analysis of the learned dynamics.</description><author>Florian Wolf, Nicolò Botteghi, Urban Fasel, Andrea Manzoni</author><pubDate>Wed, 06 Nov 2024 18:26:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04098v1</guid></item><item><title>RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models</title><link>http://arxiv.org/abs/2411.04097v1</link><description>Fine-tuned vision-language models (VLMs) often capture spurious correlationsbetween image features and textual attributes, resulting in degraded zero-shotperformance at test time. Existing approaches for addressing spuriouscorrelations (i) primarily operate at the global image-level rather thanintervening directly on fine-grained image features and (ii) are predominantlydesigned for unimodal settings. In this work, we present RaVL, which takes afine-grained perspective on VLM robustness by discovering and mitigatingspurious correlations using local image features rather than operating at theglobal image level. Given a fine-tuned VLM, RaVL first discovers spuriouscorrelations by leveraging a region-level clustering approach to identifyprecise image features contributing to zero-shot classification errors. Then,RaVL mitigates the identified spurious correlation with a novel region-awareloss function that enables the VLM to focus on relevant regions and ignorespurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs withvarious model architectures, data domains, and learned spurious correlations.Our results show that RaVL accurately discovers (191% improvement over theclosest baseline) and mitigates (8.2% improvement on worst-group imageclassification accuracy) spurious correlations. Qualitative evaluations ongeneral-domain and medical-domain VLMs confirm our findings.</description><author>Maya Varma, Jean-Benoit Delbrouck, Zhihong Chen, Akshay Chaudhari, Curtis Langlotz</author><pubDate>Wed, 06 Nov 2024 18:25:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04097v1</guid></item><item><title>Summarization of Opinionated Political Documents with Varied Perspectives</title><link>http://arxiv.org/abs/2411.04093v1</link><description>Global partisan hostility and polarization has increased, and thispolarization is heightened around presidential elections. Models capable ofgenerating accurate summaries of diverse perspectives can help reduce suchpolarization by exposing users to alternative perspectives. In this work, weintroduce a novel dataset and task for independently summarizing each politicalperspective in a set of passages from opinionated news articles. For this task,we propose a framework for evaluating different dimensions of perspectivesummary performance. We benchmark 10 models of varying sizes and architecturesthrough both automatic and human evaluation. While recent models like GPT-4operform well on this task, we find that all models struggle to generatesummaries faithful to the intended perspective. Our analysis of summariesfocuses on how extraction behavior depends on the features of the inputdocuments.</description><author>Nicholas Deas, Kathleen McKeown</author><pubDate>Wed, 06 Nov 2024 18:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04093v1</guid></item><item><title>A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement</title><link>http://arxiv.org/abs/2411.04090v1</link><description>Content moderation typically combines the efforts of human moderators andmachine learning models.However, these systems often rely on data wheresignificant disagreement occurs during moderation, reflecting the subjectivenature of toxicity perception.Rather than dismissing this disagreement asnoise, we interpret it as a valuable signal that highlights the inherentambiguity of the content,an insight missed when only the majority label isconsidered.In this work, we introduce a novel content moderation framework thatemphasizes the importance of capturing annotation disagreement. Our approachuses multitask learning, where toxicity classification serves as the primarytask and annotation disagreement is addressed as an auxiliarytask.Additionally, we leverage uncertainty estimation techniques, specificallyConformal Prediction, to account for both the ambiguity in comment annotationsand the model's inherent uncertainty in predicting toxicity anddisagreement.The framework also allows moderators to adjust thresholds forannotation disagreement, offering flexibility in determining when ambiguityshould trigger a review.We demonstrate that our joint approach enhances modelperformance, calibration, and uncertainty estimation, while offering greaterparameter efficiency and improving the review process in comparison tosingle-task methods.</description><author>Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz</author><pubDate>Wed, 06 Nov 2024 18:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04090v1</guid></item><item><title>Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</title><link>http://arxiv.org/abs/2409.16147v3</link><description>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significantpotential for modeling 3D head avatars, providing greater flexibility thanmesh-based methods and more efficient rendering compared to NeRF-basedapproaches. Despite these advancements, the creation of controllable 3DGS-basedhead avatars remains time-intensive, often requiring tens of minutes to hours.To expedite this process, we here introduce the "Gaussian Deja-vu" framework,which first obtains a generalized model of the head avatar and thenpersonalizes the result. The generalized model is trained on large 2D(synthetic and real) image datasets. This model provides a well-initialized 3DGaussian head that is further refined using a monocular video to achieve thepersonalized head avatar. For personalizing, we propose learnableexpression-aware rectification blendmaps to correct the initial 3D Gaussians,ensuring rapid convergence without the reliance on neural networks. Experimentsdemonstrate that the proposed method meets its objectives. It outperformsstate-of-the-art 3D Gaussian head avatars in terms of photorealistic quality aswell as reduces training time consumption to at least a quarter of the existingmethods, producing the avatar in minutes.</description><author>Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</author><pubDate>Wed, 06 Nov 2024 18:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16147v3</guid></item><item><title>OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs</title><link>http://arxiv.org/abs/2408.11832v2</link><description>The increased use of large language models (LLMs) across a variety ofreal-world applications calls for automatic tools to check the factual accuracyof their outputs, as LLMs often hallucinate. This is difficult as it requiresassessing the factuality of free-form open-domain responses. While there hasbeen a lot of research on this topic, different papers use different evaluationbenchmarks and measures, which makes them hard to compare and hampers futureprogress. To mitigate these issues, we developed OpenFactCheck, a unifiedframework, with three modules: (i) RESPONSEEVAL, which allows users to easilycustomize an automatic fact-checking system and to assess the factuality of allclaims in an input document using that system, (ii) LLMEVAL, which assesses theoverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluateautomatic fact-checking systems. OpenFactCheck is open-sourced(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Pythonlibrary (https://pypi.org/project/openfactcheck/) and also as a web service(http://app.openfactcheck.com). A video describing the system is available athttps://youtu.be/-i9VKL0HleI.</description><author>Hasan Iqbal, Yuxia Wang, Minghan Wang, Georgi Georgiev, Jiahui Geng, Iryna Gurevych, Preslav Nakov</author><pubDate>Wed, 06 Nov 2024 18:07:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11832v2</guid></item><item><title>SynCode: LLM Generation with Grammar Augmentation</title><link>http://arxiv.org/abs/2403.01632v4</link><description>LLMs are widely used in complex AI applications. These applicationsunderscore the need for LLM outputs to adhere to a specific format, for theirintegration with other components in the systems. Typically the format rulese.g., for data serialization formats such as JSON, YAML, or Code in ProgrammingLanguage are expressed as context-free grammar (CFG). Due to the hallucinationsand unreliability of LLMs, instructing LLMs to adhere to specified syntaxbecomes an increasingly important challenge. We present SynCode, a novel framework for efficient and general syntacticaldecoding with LLMs, to address this challenge. SynCode ensures soundness andcompleteness with respect to the CFG of a formal language, effectivelyretaining valid tokens while filtering out invalid ones. SynCode uses anoffline-constructed, efficient lookup table, the DFA mask store, derived fromthe DFA of the language's grammar for efficient generation. SynCode seamlesslyintegrates with any language defined by CFG, as evidenced by experimentsfocusing on generating JSON, Python, and Go outputs. Our experiments evaluatingthe effectiveness of SynCode for JSON generation demonstrate that SynCodeeliminates all syntax errors and significantly outperforms state-of-the-artbaselines. Furthermore, our results underscore how SynCode significantlyreduces 96.07% of syntax errors in generated Python and Go code, showcasing itssubstantial impact on enhancing syntactical precision in LLM generation. Ourcode is available at https://github.com/uiuc-focal-lab/syncode</description><author>Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, Gagandeep Singh</author><pubDate>Wed, 06 Nov 2024 18:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01632v4</guid></item><item><title>Textual Decomposition Then Sub-motion-space Scattering for Open-Vocabulary Motion Generation</title><link>http://arxiv.org/abs/2411.04079v1</link><description>Text-to-motion generation is a crucial task in computer vision, whichgenerates the target 3D motion by the given text. The existing annotateddatasets are limited in scale, resulting in most existing methods overfittingto the small datasets and unable to generalize to the motions of the opendomain. Some methods attempt to solve the open-vocabulary motion generationproblem by aligning to the CLIP space or using the Pretrain-then-Finetuningparadigm. However, the current annotated dataset's limited scale only allowsthem to achieve mapping from sub-text-space to sub-motion-space, instead ofmapping between full-text-space and full-motion-space (full mapping), which isthe key to attaining open-vocabulary motion generation. To this end, this paperproposes to leverage the atomic motion (simple body part motions over a shorttime period) as an intermediate representation, and leverage two orderlycoupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, toaddress the full mapping problem. For Textual Decomposition, we design afine-grained description conversion algorithm, and combine it with thegeneralization ability of a large language model to convert any given motiontext into atomic texts. Sub-motion-space Scattering learns the compositionalprocess from atomic motions to the target motions, to make the learnedsub-motion-space scattered to form the full-motion-space. For a given motion ofthe open domain, it transforms the extrapolation into interpolation and therebysignificantly improves generalization. Our network, $DSO$-Net, combines textual$d$ecomposition and sub-motion-space $s$cattering to solve the$o$pen-vocabulary motion generation. Extensive experiments demonstrate that ourDSO-Net achieves significant improvements over the state-of-the-art methods onopen-vocabulary motion generation. Code is available athttps://vankouf.github.io/DSONet/.</description><author>Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, Yabiao Wang, Yating Wang, Xin Tan, Chengjie Wang, Lizhuang Ma</author><pubDate>Wed, 06 Nov 2024 17:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04079v1</guid></item><item><title>H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations in Large Vision-Language Models</title><link>http://arxiv.org/abs/2411.04077v1</link><description>By leveraging both texts and images, large vision language models (LVLMs)have shown significant progress in various multi-modal tasks. Nevertheless,these models often suffer from hallucinations, e.g., they exhibitinconsistencies between the visual input and the textual output. To addressthis, we propose H-POPE, a coarse-to-fine-grained benchmark that systematicallyassesses hallucination in object existence and attributes. Our evaluation showsthat models are prone to hallucinations on object existence, and even more soon fine-grained attributes. We further investigate whether these models rely onvisual input to formulate the output texts.</description><author>Nhi Pham, Michael Schott</author><pubDate>Wed, 06 Nov 2024 17:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04077v1</guid></item><item><title>Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning</title><link>http://arxiv.org/abs/2411.03294v2</link><description>We propose an object-centric recovery policy framework to address thechallenges of out-of-distribution (OOD) scenarios in visuomotor policylearning. Previous behavior cloning (BC) methods rely heavily on a large amountof labeled data coverage, failing in unfamiliar spatial states. Without relyingon extra data collection, our approach learns a recovery policy constructed byan inverse policy inferred from object keypoint manifold gradient in theoriginal training data. The recovery policy serves as a simple add-on to anybase visuomotor BC policy, agnostic to a specific method, guiding the systemback towards the training distribution to ensure task success even in OODsituations. We demonstrate the effectiveness of our object-centric framework inboth simulation and real robot experiments, achieving an improvement of 77.7%over the base policy in OOD. Project Website:https://sites.google.com/view/ocr-penn</description><author>George Jiayuan Gao, Tianyu Li, Nadia Figueroa</author><pubDate>Wed, 06 Nov 2024 17:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03294v2</guid></item><item><title>Universal Sound Separation with Self-Supervised Audio Masked Autoencoder</title><link>http://arxiv.org/abs/2407.11745v2</link><description>Universal sound separation (USS) is a task of separating mixtures ofarbitrary sound sources. Typically, universal separation models are trainedfrom scratch in a supervised manner, using labeled data. Self-supervisedlearning (SSL) is an emerging deep learning approach that leverages unlabeleddata to obtain task-agnostic representations, which can benefit many downstreamtasks. In this paper, we propose integrating a self-supervised pre-trainedmodel, namely the audio masked autoencoder (A-MAE), into a universal soundseparation system to enhance its separation performance. We employ twostrategies to utilize SSL embeddings: freezing or updating the parameters ofA-MAE during fine-tuning. The SSL embeddings are concatenated with theshort-time Fourier transform (STFT) to serve as input features for theseparation model. We evaluate our methods on the AudioSet dataset, and theexperimental results indicate that the proposed methods successfully enhancethe separation performance of a state-of-the-art ResUNet-based USS model.</description><author>Junqi Zhao, Xubo Liu, Jinzheng Zhao, Yi Yuan, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang</author><pubDate>Wed, 06 Nov 2024 17:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11745v2</guid></item><item><title>M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models</title><link>http://arxiv.org/abs/2411.04075v1</link><description>Existing benchmarks for evaluating foundation models mainly focus onsingle-document, text-only tasks. However, they often fail to fully capture thecomplexity of research workflows, which typically involve interpretingnon-textual data and gathering information across multiple documents. Toaddress this gap, we introduce M3SciQA, a multi-modal, multi-documentscientific question answering benchmark designed for a more comprehensiveevaluation of foundation models. M3SciQA consists of 1,452 expert-annotatedquestions spanning 70 natural language processing paper clusters, where eachcluster represents a primary paper along with all its cited documents,mirroring the workflow of comprehending a single paper by requiring multi-modaland multi-document data. With M3SciQA, we conduct a comprehensive evaluation of18 foundation models. Our results indicate that current foundation models stillsignificantly underperform compared to human experts in multi-modal informationretrieval and in reasoning across multiple scientific documents. Additionally,we explore the implications of these findings for the future advancement ofapplying foundation models in multi-modal scientific literature analysis.</description><author>Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, Arman Cohan</author><pubDate>Wed, 06 Nov 2024 17:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04075v1</guid></item><item><title>The Function-Representation Model of Computation</title><link>http://arxiv.org/abs/2410.07928v3</link><description>Cognitive Architectures are the forefront of the research into developing anartificial cognition. However, they approach the problem from a separatedmemory and program model of computation. This model of computation poses afundamental problem: the knowledge retrieval heuristic. In this paper wepropose to solve this problem by using a novel model of computation, one wherememory and program are merged: the Function-Representation. This model ofcomputation involves defining a generic Function-Representation andinstantiating multiple instances of it. In this paper we explore the potentialof this novel model of computation through mathematical definitions and proofs.We also explore the kind of functions a Function-Representation can implement,and present different ways to organise multiple instances of aFunction-Representation.</description><author>Alfredo Ibias, Hector Antona, Guillem Ramirez-Miranda, Enric Guinovart, Eduard Alarcon</author><pubDate>Wed, 06 Nov 2024 17:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07928v3</guid></item><item><title>Counterfactual Token Generation in Large Language Models</title><link>http://arxiv.org/abs/2409.17027v2</link><description>"Sure, I am happy to generate a story for you: Captain Lyra stood at the helmof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]Lyra's eyes welled up with tears as she realized the bitter truth - she hadsacrificed everything for fleeting riches, and lost the love of her crew, herfamily, and herself." Although this story, generated by a large language model,is captivating, one may wonder -- how would the story have unfolded if themodel had chosen "Captain Maeve" as the protagonist instead? We cannot know.State-of-the-art large language models are stateless -- they maintain nointernal memory or state. Given a prompt, they generate a sequence of tokens asan output using an autoregressive process. As a consequence, they cannot reasonabout counterfactual alternatives to tokens they have generated in the past. Inthis work, our goal is to enhance them with this functionality. To this end, wedevelop a causal model of token generation that builds upon the Gumbel-Maxstructural causal model. Our model allows any large language model to performcounterfactual token generation at almost no cost in comparison with vanillatoken generation, it is embarrassingly simple to implement, and it does notrequire any fine-tuning nor prompt engineering. We implement our model on Llama3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and aquantitative analysis of counterfactually generated text. We conclude with ademonstrative application of counterfactual token generation for biasdetection, unveiling interesting insights about the model of the worldconstructed by large language models.</description><author>Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez</author><pubDate>Wed, 06 Nov 2024 17:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17027v2</guid></item><item><title>A Probabilistic Perspective on Unlearning and Alignment for Large Language Models</title><link>http://arxiv.org/abs/2410.03523v3</link><description>Comprehensive evaluation of Large Language Models (LLMs) is an open researchproblem. Existing evaluations rely on deterministic point estimates generatedvia greedy decoding. However, we find that deterministic evaluations fail tocapture the whole output distribution of a model, yielding inaccurateestimations of model capabilities. This is particularly problematic in criticalcontexts such as unlearning and alignment, where precise model evaluations arecrucial. To remedy this, we introduce the first formal probabilistic evaluationframework in LLMs. Namely, we derive novel metrics with high-probabilityguarantees concerning the output distribution of a model. Our metrics areapplication-independent and allow practitioners to make more reliable estimatesabout model capabilities before deployment. Through a case study focused onunlearning, we reveal that deterministic evaluations falsely indicatesuccessful unlearning, whereas our probabilistic evaluations demonstrate thatmost if not all of the supposedly unlearned information remains accessible inthese models. Additionally, we propose a novel unlearning loss based on entropyoptimization and adaptive temperature scaling, which significantly improvesunlearning in probabilistic settings on recent benchmarks. Our proposed shiftfrom point estimates to probabilistic evaluations of output distributionsrepresents an important step toward comprehensive evaluations of LLMs. Codeavailable at https://github.com/yascho/probabilistic-unlearning.</description><author>Yan Scholten, Stephan Günnemann, Leo Schwinn</author><pubDate>Wed, 06 Nov 2024 17:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03523v3</guid></item><item><title>Navigating Extremes: Dynamic Sparsity in Large Output Space</title><link>http://arxiv.org/abs/2411.03171v2</link><description>In recent years, Dynamic Sparse Training (DST) has emerged as an alternativeto post-training pruning for generating efficient models. In principle, DSTallows for a more memory efficient training process, as it maintains sparsitythroughout the entire training run. However, current DST implementations failto capitalize on this in practice. Because sparse matrix multiplication is muchless efficient than dense matrix multiplication on GPUs, most implementationssimulate sparsity by masking weights. In this paper, we leverage recentadvances in semi-structured sparse training to apply DST in the domain ofclassification with large output spaces, where memory-efficiency is paramount.With a label space of possibly millions of candidates, the classification layeralone will consume several gigabytes of memory. Switching from a dense to afixed fan-in sparse layer updated with sparse evolutionary training (SET);however, severely hampers training convergence, especially at the largest labelspaces. We find that poor gradient flow from the sparse classifier to the densetext encoder make it difficult to learn good input representations. Byemploying an intermediate layer or adding an auxiliary training objective, werecover most of the generalisation performance of the dense model. Overall, wedemonstrate the applicability and practical benefits of DST in a challengingdomain -- characterized by a highly skewed label distribution that differssubstantially from typical DST benchmark datasets -- which enables end-to-endtraining with millions of labels on commodity hardware.</description><author>Nasib Ullah, Erik Schultheis, Mike Lasby, Yani Ioannou, Rohit Babbar</author><pubDate>Wed, 06 Nov 2024 17:19:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03171v2</guid></item><item><title>Cartan moving frames and the data manifolds</title><link>http://arxiv.org/abs/2409.12057v2</link><description>The purpose of this paper is to employ the language of Cartan moving framesto study the geometry of the data manifolds and its Riemannian structure, viathe data information metric and its curvature at data points. Using thisframework and through experiments, explanations on the response of a neuralnetwork are given by pointing out the output classes that are easily reachablefrom a given input. This emphasizes how the proposed mathematical relationshipbetween the output of the network and the geometry of its inputs can beexploited as an explainable artificial intelligence tool.</description><author>Eliot Tron, Rita Fioresi, Nicolas Couellan, Stéphane Puechmorel</author><pubDate>Wed, 06 Nov 2024 17:13:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12057v2</guid></item><item><title>Pseudo-labeling with Keyword Refining for Few-Supervised Video Captioning</title><link>http://arxiv.org/abs/2411.04059v1</link><description>Video captioning generate a sentence that describes the video content.Existing methods always require a number of captions (\eg, 10 or 20) per videoto train the model, which is quite costly. In this work, we explore thepossibility of using only one or very few ground-truth sentences, and introducea new task named few-supervised video captioning. Specifically, we propose afew-supervised video captioning framework that consists of lexicallyconstrained pseudo-labeling module and keyword-refined captioning module.Unlike the random sampling in natural language processing that may causeinvalid modifications (\ie, edit words), the former module guides the model toedit words using some actions (\eg, copy, replace, insert, and delete) by apretrained token-level classifier, and then fine-tunes candidate sentences by apretrained language model. Meanwhile, the former employs the repetitionpenalized sampling to encourage the model to yield concise pseudo-labeledsentences with less repetition, and selects the most relevant sentences upon apretrained video-text model. Moreover, to keep semantic consistency betweenpseudo-labeled sentences and video content, we develop the transformer-basedkeyword refiner with the video-keyword gated fusion strategy to emphasize moreon relevant words. Extensive experiments on several benchmarks demonstrate theadvantages of the proposed approach in both few-supervised and fully-supervisedscenarios. The code implementation is available athttps://github.com/mlvccn/PKG_VidCap</description><author>Ping Li, Tao Wang, Xinkui Zhao, Xianghua Xu, Mingli Song</author><pubDate>Wed, 06 Nov 2024 17:11:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04059v1</guid></item><item><title>bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction</title><link>http://arxiv.org/abs/2410.23247v2</link><description>Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,producing 1-bit arrays representing photon detection events over exposures asshort as a few nanoseconds. In practice, raw data are post-processed usingheavy spatiotemporal binning to create more useful and interpretable images atthe cost of degrading spatiotemporal resolution. In this work, we proposebit2bit, a new method for reconstructing high-quality image stacks at theoriginal spatiotemporal resolution from sparse binary quanta image data.Inspired by recent work on Poisson denoising, we developed an algorithm thatcreates a dense image sequence from sparse binary photon data by predicting thephoton arrival location probability distribution. However, due to the binarynature of the data, we show that the assumption of a Poisson distribution isinadequate. Instead, we model the process with a Bernoulli lattice process fromthe truncated Poisson. This leads to the proposal of a novel self-supervisedsolution based on a masked loss function. We evaluate our method using bothsimulated and real data. On simulated data from a conventional video, weachieve 34.35 mean PSNR with extremely photon-sparse binary input (&lt;0.06photons per pixel per frame). We also present a novel dataset containing a widerange of real SPAD high-speed videos under various challenging imagingconditions. The scenes cover strong/weak ambient light, strong motion,ultra-fast events, etc., which will be made available to the community, onwhich we demonstrate the promise of our approach. Both reconstruction qualityand throughput substantially surpass the state-of-the-art methods (e.g., QuantaBurst Photography (QBP)). Our approach significantly enhances the visualizationand usability of the data, enabling the application of existing analysistechniques.</description><author>Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W. Jenkins</author><pubDate>Wed, 06 Nov 2024 17:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23247v2</guid></item><item><title>Problem Space Transformations for Generalisation in Behavioural Cloning</title><link>http://arxiv.org/abs/2411.04056v1</link><description>The combination of behavioural cloning and neural networks has drivensignificant progress in robotic manipulation. As these algorithms may require alarge number of demonstrations for each task of interest, they remainfundamentally inefficient in complex scenarios. This issue is aggravated whenthe system is treated as a black-box, ignoring its physical properties. Thiswork characterises widespread properties of robotic manipulation, such as poseequivariance and locality. We empirically demonstrate that transformationsarising from each of these properties allow neural policies trained withbehavioural cloning to better generalise to out-of-distribution probleminstances.</description><author>Kiran Doshi, Marco Bagatella, Stelian Coros</author><pubDate>Wed, 06 Nov 2024 17:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04056v1</guid></item><item><title>Teaching Models to Improve on Tape</title><link>http://arxiv.org/abs/2411.01483v3</link><description>Large Language Models (LLMs) often struggle when prompted to generate contentunder specific constraints. However, in such cases it is often easy to checkwhether these constraints are satisfied or violated. Recent works have shownthat LLMs can benefit from such "corrective feedback". Here we claim that thisskill of LLMs can be significantly enhanced via training. We introduce an RLframework for teaching models to use such rewards, by simulating interactionsessions, and rewarding the model according to its ability to satisfy theconstraints. We refer to our method as CORGI (Controlled Generation with RL forGuided Interaction), and evaluate it on a variety of controlled generationtasks using unlabeled training data. We find that CORGI consistentlyoutperforms the baseline reinforcement learning method that does notincorporate conversational feedback. Furthermore, CORGI's interactive frameworkenables meta-learning, allowing the LLM to generalize better to guidedinteraction in new tasks. Our results clearly show that conversationaloptimization, when combined with reinforcement learning, significantly improvesthe effectiveness of LLMs in controlled generation contexts.</description><author>Liat Bezalel, Eyal Orgad, Amir Globerson</author><pubDate>Wed, 06 Nov 2024 17:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01483v3</guid></item><item><title>Multi-branch Spatio-Temporal Graph Neural Network For Efficient Ice Layer Thickness Prediction</title><link>http://arxiv.org/abs/2411.04055v1</link><description>Understanding spatio-temporal patterns in polar ice layers is essential fortracking changes in ice sheet balance and assessing ice dynamics. Whileconvolutional neural networks are widely used in learning ice layer patternsfrom raw echogram images captured by airborne snow radar sensors, noise in theechogram images prevents researchers from getting high-quality results.Instead, we focus on geometric deep learning using graph neural networks,aiming to build a spatio-temporal graph neural network that learns fromthickness information of the top ice layers and predicts for deeper layers. Inthis paper, we developed a novel multi-branch spatio-temporal graph neuralnetwork that used the GraphSAGE framework for spatio features learning and atemporal convolution operation to capture temporal changes, enabling differentbranches of the network to be more specialized and focusing on a singlelearning task. We found that our proposed multi-branch network can consistentlyoutperform the current fused spatio-temporal graph neural network in bothaccuracy and efficiency.</description><author>Zesheng Liu, Maryam Rahnemoonfar</author><pubDate>Wed, 06 Nov 2024 16:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04055v1</guid></item><item><title>Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits</title><link>http://arxiv.org/abs/2411.04054v1</link><description>Causal knowledge about the relationships among decision variables and areward variable in a bandit setting can accelerate the learning of an optimaldecision. Current works often assume the causal graph is known, which may notalways be available a priori. Motivated by this challenge, we focus on thecausal bandit problem in scenarios where the underlying causal graph is unknownand may include latent confounders. While intervention on the parents of thereward node is optimal in the absence of latent confounders, this is notnecessarily the case in general. Instead, one must consider a set of possiblyoptimal arms/interventions, each being a special subset of the ancestors of thereward node, making causal discovery beyond the parents of the reward nodeessential. For regret minimization, we identify that discovering the fullcausal structure is unnecessary; however, no existing work provides thenecessary and sufficient components of the causal graph. We formallycharacterize the set of necessary and sufficient latent confounders one needsto detect or learn to ensure that all possibly optimal arms are identifiedcorrectly. We also propose a randomized algorithm for learning the causal graphwith a limited number of samples, providing a sample complexity guarantee forany desired confidence level. In the causal bandit setup, we propose atwo-stage approach. In the first stage, we learn the induced subgraph onancestors of the reward, along with a necessary and sufficient subset of latentconfounders, to construct the set of possibly optimal arms. The regret incurredduring this phase scales polynomially with respect to the number of nodes inthe causal graph. The second phase involves the application of a standardbandit algorithm, such as the UCB algorithm. We also establish a regret boundfor our two-phase approach, which is sublinear in the number of rounds.</description><author>Muhammad Qasim Elahi, Mahsa Ghasemi, Murat Kocaoglu</author><pubDate>Wed, 06 Nov 2024 16:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04054v1</guid></item><item><title>LDTrack: Dynamic People Tracking by Service Robots using Diffusion Models</title><link>http://arxiv.org/abs/2402.08774v3</link><description>Tracking of dynamic people in cluttered and crowded human-centeredenvironments is a challenging robotics problem due to the presence ofintraclass variations including occlusions, pose deformations, and lightingvariations. This paper introduces a novel deep learning architecture, usingconditional latent diffusion models, the Latent Diffusion Track (LDTrack), fortracking multiple dynamic people under intraclass variations. By uniquelyutilizing conditional latent diffusion models to capture temporal personembeddings, our architecture can adapt to appearance changes of people overtime. We incorporated a latent feature encoder network which enables thediffusion process to operate within a high-dimensional latent space to allowfor the extraction and spatial-temporal refinement of such rich features asperson appearance, motion, location, identity, and contextual information.Extensive experiments demonstrate the effectiveness of LDTrack over otherstate-of-the-art tracking methods in cluttered and crowded human-centeredenvironments under intraclass variations. Namely, the results show our methodoutperforms existing deep learning robotic people tracking methods in bothtracking accuracy and tracking precision with statistical significance.Additionally, a comprehensive multi-object tracking comparison study wasperformed against the state-of-the-art methods in urban environments,demonstrating the generalizability of LDTrack. An ablation study was performedto validate the design choices of LDTrack.</description><author>Angus Fung, Beno Benhabib, Goldie Nejat</author><pubDate>Wed, 06 Nov 2024 16:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08774v3</guid></item><item><title>A Comparison of Prompt Engineering Techniques for Task Planning and Execution in Service Robotics</title><link>http://arxiv.org/abs/2410.22997v2</link><description>Recent advances in LLM have been instrumental in autonomous robot control andhuman-robot interaction by leveraging their vast general knowledge andcapabilities to understand and reason across a wide range of tasks andscenarios. Previous works have investigated various prompt engineeringtechniques for improving the performance of LLM to accomplish tasks, whileothers have proposed methods that utilize LLMs to plan and execute tasks basedon the available functionalities of a given robot platform. In this work, weconsider both lines of research by comparing prompt engineering techniques andcombinations thereof within the application of high-level task planning andexecution in service robotics. We define a diverse set of tasks and a simpleset of functionalities in simulation, and measure task completion accuracy andexecution time for several state-of-the-art models.</description><author>Jonas Bode, Bastian Pätzold, Raphael Memmesheimer, Sven Behnke</author><pubDate>Wed, 06 Nov 2024 16:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22997v2</guid></item><item><title>DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion Models</title><link>http://arxiv.org/abs/2405.16749v2</link><description>Pretrained diffusion models (DMs) have recently been popularly used insolving inverse problems (IPs). The existing methods mostly interleaveiterative steps in the reverse diffusion process and iterative steps to bringthe iterates closer to satisfying the measurement constraint. However, suchinterleaving methods struggle to produce final results that look like naturalobjects of interest (i.e., manifold feasibility) and fit the measurement (i.e.,measurement feasibility), especially for nonlinear IPs. Moreover, theircapabilities to deal with noisy IPs with unknown types and levels ofmeasurement noise are unknown. In this paper, we advocate viewing the reverseprocess in DMs as a function and propose a novel plug-in method for solving IPsusing pretrained DMs, dubbed DMPlug. DMPlug addresses the issues of manifoldfeasibility and measurement feasibility in a principled manner, and also showsgreat potential for being robust to unknown types and levels of noise. Throughextensive experiments across various IP tasks, including two linear and threenonlinear IPs, we demonstrate that DMPlug consistently outperformsstate-of-the-art methods, often by large margins especially for nonlinear IPs.The code is available at https://github.com/sun-umn/DMPlug.</description><author>Hengkang Wang, Xu Zhang, Taihui Li, Yuxiang Wan, Tiancong Chen, Ju Sun</author><pubDate>Wed, 06 Nov 2024 16:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16749v2</guid></item><item><title>Diverging Preferences: When do Annotators Disagree and do Models Know?</title><link>http://arxiv.org/abs/2410.14632v2</link><description>We examine diverging preferences in human-labeled preference datasets. Wedevelop a taxonomy of disagreement sources spanning 10 categories across fourhigh-level classes -- task underspecification, response style, refusals, andannotation errors. We find that the majority of disagreements are in oppositionwith standard reward modeling approaches, which are designed with theassumption that annotator disagreement is noise. We then explore how thesefindings impact two areas of LLM development: reward modeling and evaluation.In our experiments, we demonstrate how standard reward modeling methods, likethe Bradley-Terry model, fail to differentiate whether a given preferencejudgment is the result of unanimous agreement among annotators or the majorityopinion among diverging user preferences. We also find that these tendenciesare also echoed by popular LLM-as-Judge evaluation methods, which consistentlyidentify a winning response in cases of diverging preferences. These findingshighlight remaining challenges in LLM evaluations, which are greatly influencedby divisive features like response style, and in developing pluralisticallyaligned LLMs. To address these issues, we develop methods for identifyingdiverging preferences to mitigate their influence on evaluation and training.</description><author>Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin</author><pubDate>Wed, 06 Nov 2024 16:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14632v2</guid></item><item><title>Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis</title><link>http://arxiv.org/abs/2411.01929v2</link><description>Artificial Intelligence (AI) research often aims to develop models that cangeneralize reliably across complex datasets, yet this remains challenging infields where data is scarce, intricate, or inaccessible. This paper introducesa novel approach that leverages three generative models of varying complexityto synthesize one of the most demanding structured datasets: Malicious NetworkTraffic. Our approach uniquely transforms numerical data into text, re-framingdata generation as a language modeling task, which not only enhances dataregularization but also significantly improves generalization and the qualityof the synthetic data. Extensive statistical analyses demonstrate that ourmethod surpasses state-of-the-art generative models in producing high-fidelitysynthetic data. Additionally, we conduct a comprehensive study on syntheticdata applications, effectiveness, and evaluation strategies, offering valuableinsights into its role across various domains. Our code and pre-trained modelsare openly accessible at Github, enabling further exploration and applicationof our methodology. Index Terms: Data synthesis, machine learning, trafficgeneration, privacy preserving data, generative models.</description><author>Mohammad Zbeeb, Mohammad Ghorayeb, Mariam Salman</author><pubDate>Wed, 06 Nov 2024 16:50:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01929v2</guid></item><item><title>DexDiffuser: Generating Dexterous Grasps with Diffusion Models</title><link>http://arxiv.org/abs/2402.02989v3</link><description>We introduce DexDiffuser, a novel dexterous grasping method that generates,evaluates, and refines grasps on partial object point clouds. DexDiffuserincludes the conditional diffusion-based grasp sampler DexSampler and thedexterous grasp evaluator DexEvaluator. DexSampler generates high-qualitygrasps conditioned on object point clouds by iterative denoising of randomlysampled grasps. We also introduce two grasp refinement strategies:Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).The experiment results demonstrate that DexDiffuser consistently outperformsthe state-of-the-art multi-finger grasp generation method FFHNet with an, onaverage, 9.12% and 19.44% higher grasp success rate in simulation and realrobot experiments, respectively. Supplementary materials are available athttps://yulihn.github.io/DexDiffuser_page/</description><author>Zehang Weng, Haofei Lu, Danica Kragic, Jens Lundell</author><pubDate>Wed, 06 Nov 2024 16:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02989v3</guid></item><item><title>Stepping Forward on the Last Mile</title><link>http://arxiv.org/abs/2411.04036v1</link><description>Continuously adapting pre-trained models to local data on resourceconstrained edge devices is the $\emph{last mile}$ for model deployment.However, as models increase in size and depth, backpropagation requires a largeamount of memory, which becomes prohibitive for edge devices. In addition, mostexisting low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) aredesigned as fixed-point inference accelerators, without training capabilities.Forward gradients, solely based on directional derivatives computed from twoforward calls, have been recently used for model training, with substantialsavings in computation and memory. However, the performance of quantizedtraining with fixed-point forward gradients remains unclear. In this paper, weinvestigate the feasibility of on-device training using fixed-point forwardgradients, by conducting comprehensive experiments across a variety of deeplearning benchmark tasks in both vision and audio domains. We propose a seriesof algorithm enhancements that further reduce the memory footprint, and theaccuracy gap compared to backpropagation. An empirical study on how trainingwith forward gradients navigates in the loss landscape is further explored. Ourresults demonstrate that on the last mile of model customization on edgedevices, training with fixed-point forward gradients is a feasible andpractical approach.</description><author>Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Andrew Zou Li</author><pubDate>Wed, 06 Nov 2024 16:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04036v1</guid></item><item><title>Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset</title><link>http://arxiv.org/abs/2411.04034v1</link><description>Neural networks are traditionally trained under the assumption that data comefrom a stationary distribution. However, settings which violate this assumptionare becoming more popular; examples include supervised learning underdistributional shifts, reinforcement learning, continual learning andnon-stationary contextual bandits. In this work we introduce a novel learningapproach that automatically models and adapts to non-stationarity, via anOrnstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drifttends to draw the parameters towards the initialisation distribution, so theapproach can be understood as a form of soft parameter reset. We showempirically that our approach performs well in non-stationary supervised andoff-policy reinforcement learning settings.</description><author>Alexandre Galashov, Michalis K. Titsias, András György, Clare Lyle, Razvan Pascanu, Yee Whye Teh, Maneesh Sahani</author><pubDate>Wed, 06 Nov 2024 16:32:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04034v1</guid></item><item><title>Beemo: Benchmark of Expert-edited Machine-generated Outputs</title><link>http://arxiv.org/abs/2411.04032v1</link><description>The rapid proliferation of large language models (LLMs) has increased thevolume of machine-generated texts (MGTs) and blurred text authorship in variousdomains. However, most existing MGT benchmarks include single-author texts(human-written and machine-generated). This conventional design fails tocapture more practical multi-author scenarios, where the user refines the LLMresponse for natural flow, coherence, and factual correctness. Our paperintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),which includes 6.5k texts written by humans, generated by teninstruction-finetuned LLMs, and edited by experts for various use cases,ranging from creative writing to summarization. Beemo additionally comprises13.1k machine-generated and LLM-edited texts, allowing for diverse MGTdetection evaluation across various edit types. We document Beemo's creationprotocol and present the results of benchmarking 33 configurations of MGTdetectors in different experimental setups. We find that expert-based editingevades MGT detection, while LLM-edited texts are unlikely to be recognized ashuman-written. Beemo and all materials are publicly available.</description><author>Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov</author><pubDate>Wed, 06 Nov 2024 16:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04032v1</guid></item><item><title>Deep neural network-based detection of counterfeit products from smartphone images</title><link>http://arxiv.org/abs/2410.05969v2</link><description>Counterfeit products such as drugs and vaccines as well as luxury items suchas high-fashion handbags, watches, jewelry, garments, and cosmetics, representsignificant direct losses of revenue to legitimate manufacturers and vendors,as well as indirect costs to societies at large. We present the world's firstpurely computer-vision-based system to combat such counterfeiting-one that doesnot require special security tags or other alterations to the products ormodifications to supply chain tracking. Our deep neural network system showshigh accuracy on branded garments from our first manufacturer tested (99.71%after 3.06% rejections) using images captured under natural, weakly controlledconditions, such as in retail stores, customs checkpoints, warehouses, andoutdoors. Our system, suitably transfer trained on a small number of fake andgenuine articles, should find application in additional product categories aswell, for example fashion accessories, perfume boxes, medicines, and more.</description><author>Hugo Garcia-Cotte, Dorra Mellouli, Abdul Rehman, Li Wang, David G. Stork</author><pubDate>Wed, 06 Nov 2024 16:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05969v2</guid></item><item><title>Prompt Engineering Using GPT for Word-Level Code-Mixed Language Identification in Low-Resource Dravidian Languages</title><link>http://arxiv.org/abs/2411.04025v1</link><description>Language Identification (LI) is crucial for various natural languageprocessing tasks, serving as a foundational step in applications such assentiment analysis, machine translation, and information retrieval. Inmultilingual societies like India, particularly among the youth engaging onsocial media, text often exhibits code-mixing, blending local languages withEnglish at different linguistic levels. This phenomenon presents formidablechallenges for LI systems, especially when languages intermingle within singlewords. Dravidian languages, prevalent in southern India, possess richmorphological structures yet suffer from under-representation in digitalplatforms, leading to the adoption of Roman or hybrid scripts forcommunication. This paper introduces a prompt based method for a shared taskaimed at addressing word-level LI challenges in Dravidian languages. In thiswork, we leveraged GPT-3.5 Turbo to understand whether the large languagemodels is able to correctly classify words into correct categories. Ourfindings show that the Kannada model consistently outperformed the Tamil modelacross most metrics, indicating a higher accuracy and reliability inidentifying and categorizing Kannada language instances. In contrast, the Tamilmodel showed moderate performance, particularly needing improvement inprecision and recall.</description><author>Aniket Deroy, Subhankar Maity</author><pubDate>Wed, 06 Nov 2024 16:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04025v1</guid></item><item><title>Pretraining and Updates of Domain-Specific LLM: A Case Study in the Japanese Business Domain</title><link>http://arxiv.org/abs/2404.08262v3</link><description>The development of Large Language Models (LLMs) in various languages has beenadvancing, but the combination of non-English languages with domain-specificcontexts remains underexplored. This paper presents our findings from trainingand evaluating a Japanese business domain-specific LLM designed to betterunderstand business-related documents, such as the news on current affairs,technical reports, and patents. Additionally, LLMs in this domain requireregular updates to incorporate the most recent knowledge. Therefore, we alsoreport our findings from the first experiments and evaluations involvingupdates to this LLM using the latest article data, which is an importantproblem setting that has not been addressed in previous research. From ourexperiments on a newly created benchmark dataset for question answering in thetarget domain, we found that (1) our pretrained model improves QA accuracywithout losing general knowledge, and (2) a proper mixture of the latest andolder texts in the training data for the update is necessary. Our pretrainedmodel and business domain benchmark are publicly available to support furtherstudies.</description><author>Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki</author><pubDate>Wed, 06 Nov 2024 16:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08262v3</guid></item><item><title>News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News</title><link>http://arxiv.org/abs/2410.07520v2</link><description>Large Language Models (LLMs) have fast become an essential tools to manyconversational chatbots due to their ability to provide coherent answers forvaried queries. Datasets used to train these LLMs are often a mix of genericand synthetic samples, thus lacking the verification needed to provide correctand verifiable answers for T.V. News. We collect and share a large collection of QA pairs extracted fromtranscripts of news recordings from various news-channels across the UnitedStates. Resultant QA pairs are then used to fine-tune an off-the-shelf LLMmodel. Our model surpasses base models of similar size on several open LLMbenchmarks. We further integrate and propose a RAG method to improvecontextualization of our answers and also point it to a verifiable newsrecording.</description><author>Tarun Jain, Yufei Gao, Sridhar Vanga, Karan Singla</author><pubDate>Wed, 06 Nov 2024 16:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07520v2</guid></item><item><title>Improving Context-Aware Preference Modeling for Language Models</title><link>http://arxiv.org/abs/2407.14916v2</link><description>While finetuning language models from pairwise preferences has provenremarkably effective, the underspecified nature of natural language presentscritical challenges. Direct preference feedback is uninterpretable, difficultto provide where multidimensional criteria may apply, and often inconsistent,either because it is based on incomplete instructions or provided by diverseprincipals. To address these challenges, we consider the two-step preferencemodeling procedure that first resolves the under-specification by selecting acontext, and then evaluates preference with respect to the chosen context. Wedecompose reward modeling error according to these two steps, which suggeststhat supervising context in addition to context-specific preference may be aviable approach to aligning models with diverse human preferences. For this towork, the ability of models to evaluate context-specific preference iscritical. To this end, we contribute context-conditioned preference datasetsand accompanying experiments that investigate the ability of language models toevaluate context-specific preference. We use our datasets to (1) show thatexisting preference models benefit from, but fail to fully consider, addedcontext, (2) finetune a context-aware reward model with context-specificperformance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)investigate the value of context-aware preference modeling.</description><author>Silviu Pitis, Ziang Xiao, Nicolas Le Roux, Alessandro Sordoni</author><pubDate>Wed, 06 Nov 2024 16:11:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14916v2</guid></item><item><title>Skills or Degree? The Rise of Skill-Based Hiring for AI and Green Jobs</title><link>http://arxiv.org/abs/2312.11942v2</link><description>Emerging professions in fields like Artificial Intelligence (AI) andsustainability (green jobs) are experiencing labour shortages as industrydemand outpaces labour supply. In this context, our study aims to understandwhether employers have begun focusing more on individual skills rather thanformal qualifications in their recruitment processes. We analysed a largetime-series dataset of approximately eleven million online job vacancies in theUK from 2018 to mid-2024, drawing on diverse literature on technological changeand labour market signalling. Our findings provide evidence that employers haveinitiated "skill-based hiring" for AI roles, adopting more flexible hiringpractices to expand the available talent pool. From 2018-2023, demand for AIroles grew by 21% as a proportion of all postings (and accelerated into 2024).Simultaneously, mentions of university education requirements for AI rolesdeclined by 15%. Our regression analysis shows that university degrees have asignificantly lower wage premium for both AI and green roles. In contrast, AIskills command a wage premium of 23%, exceeding the value of degrees up untilthe PhD-level (33%). In occupations with high demand for AI skills, the premiumfor skills is high, and the reward for degrees is relatively low. We recommendleveraging alternative skill-building formats such as apprenticeships,on-the-job training, MOOCs, vocational education and training,micro-certificates, and online bootcamps to fully utilise human capital andaddress talent shortages.</description><author>Matthew Bone, Eugenia Ehlinger, Fabian Stephany</author><pubDate>Wed, 06 Nov 2024 16:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11942v2</guid></item><item><title>SHyPar: A Spectral Coarsening Approach to Hypergraph Partitioning</title><link>http://arxiv.org/abs/2410.10875v2</link><description>State-of-the-art hypergraph partitioners utilize a multilevel paradigm toconstruct progressively coarser hypergraphs across multiple layers, guiding cutrefinements at each level of the hierarchy. Traditionally, these partitionersemploy heuristic methods for coarsening and do not consider the structuralfeatures of hypergraphs. In this work, we introduce a multilevel spectralframework, SHyPar, for partitioning large-scale hypergraphs by leveraginghyperedge effective resistances and flow-based community detection techniques.Inspired by the latest theoretical spectral clustering frameworks, such asHyperEF and HyperSF, SHyPar aims to decompose large hypergraphs into multiplesubgraphs with few inter-partition hyperedges (cut size). A key component ofSHyPar is a flow-based local clustering scheme for hypergraph coarsening, whichincorporates a max-flow-based algorithm to produce clusters with substantiallyimproved conductance. Additionally, SHyPar utilizes an effectiveresistance-based rating function for merging nodes that are strongly connected(coupled). Compared with existing state-of-the-art hypergraph partitioningmethods, our extensive experimental results on real-world VLSI designsdemonstrate that SHyPar can more effectively partition hypergraphs, achievingstate-of-the-art solution quality.</description><author>Hamed Sajadinia, Ali Aghdaei, Zhuo Feng</author><pubDate>Wed, 06 Nov 2024 15:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10875v2</guid></item><item><title>Multi-Scale and Multimodal Species Distribution Modeling</title><link>http://arxiv.org/abs/2411.04016v1</link><description>Species distribution models (SDMs) aim to predict the distribution of speciesby relating occurrence data with environmental variables. Recent applicationsof deep learning to SDMs have enabled new avenues, specifically the inclusionof spatial data (environmental rasters, satellite images) as model predictors,allowing the model to consider the spatial context around each species'observations. However, the appropriate spatial extent of the images is notstraightforward to determine and may affect the performance of the model, asscale is recognized as an important factor in SDMs. We develop a modularstructure for SDMs that allows us to test the effect of scale in both single-and multi-scale settings. Furthermore, our model enables different scales to beconsidered for different modalities, using a late fusion approach. Results onthe GeoLifeCLEF 2023 benchmark indicate that considering multimodal data andlearning multi-scale representations leads to more accurate models.</description><author>Nina van Tiel, Robin Zbinden, Emanuele Dalsasso, Benjamin Kellenberger, Loïc Pellissier, Devis Tuia</author><pubDate>Wed, 06 Nov 2024 15:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04016v1</guid></item><item><title>CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</title><link>http://arxiv.org/abs/2405.17537v3</link><description>Measuring biodiversity is crucial for understanding ecosystem health. Whileprior works have developed machine learning models for taxonomic classificationof photographic images and DNA separately, in this work, we introduce amultimodal approach combining both, using CLIP-style contrastive learning toalign images, barcode DNA, and text-based representations of taxonomic labelsin a unified embedding space. This allows for accurate classification of bothknown and unknown insect species without task-specific fine-tuning, leveragingcontrastive learning for the first time to fuse DNA and image data. Our methodsurpasses previous single-modality approaches in accuracy by over 8% onzero-shot learning tasks, showcasing its effectiveness in biodiversity studies.</description><author>ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang</author><pubDate>Wed, 06 Nov 2024 15:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17537v3</guid></item><item><title>$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers</title><link>http://arxiv.org/abs/2411.04013v1</link><description>Despite their power, Transformers face challenges with long sequences due tothe quadratic complexity of self-attention. To address this limitation, methodslike $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy, Saffar,Vaswani, Grangier, 2021] enabling each token to attend to only its $k$ closesttokens. While $k$NN attention has shown empirical success in makingTransformers more efficient, its exact approximation guarantees have not beentheoretically analyzed. In this work, we establish a theoretical framework for$k$NN attention, reformulating self-attention as expectations over softmaxdistributions and leveraging lazy Gumbel sampling [Mussmann, Levy, Ermon, 2017]with $k$NN indices for efficient approximation. Building on this framework, wealso propose novel sub-quadratic algorithms that approximate self-attentiongradients by leveraging efficient sampling techniques, such as MarkovChain-based estimation. Finally, we demonstrate the practical effectiveness ofthese algorithms through empirical experiments, showcasing their benefits inboth training and inference.</description><author>Themistoklis Haris</author><pubDate>Wed, 06 Nov 2024 15:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04013v1</guid></item><item><title>Improving Causal Reasoning in Large Language Models: A Survey</title><link>http://arxiv.org/abs/2410.16676v3</link><description>Causal reasoning (CR) is a crucial aspect of intelligence, essential forproblem-solving, decision-making, and understanding the world. While largelanguage models (LLMs) can generate rationales for their outputs, their abilityto reliably perform causal reasoning remains uncertain, often falling short intasks requiring a deep understanding of causality. In this survey, we provide acomprehensive review of research aimed at enhancing LLMs for causal reasoning.We categorize existing methods based on the role of LLMs: either as reasoningengines or as helpers providing knowledge or data to traditional CR methods,followed by a detailed discussion of the methodologies in each category. Wethen evaluate the performance of LLMs on various causal reasoning tasks,providing key findings and in-depth analysis. Finally, we provide insights fromcurrent studies and highlight promising directions for future research. We aimfor this work to serve as a comprehensive resource, fostering furtheradvancements in causal reasoning with LLMs. Resources are available athttps://github.com/chendl02/Awesome-LLM-causal-reasoning.</description><author>Longxuan Yu, Delin Chen, Siheng Xiong, Qingyang Wu, Qingzhen Liu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan</author><pubDate>Wed, 06 Nov 2024 15:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16676v3</guid></item><item><title>Predicting and Publishing Accurate Imbalance Prices Using Monte Carlo Tree Search</title><link>http://arxiv.org/abs/2411.04011v1</link><description>The growing reliance on renewable energy sources, particularly solar andwind, has introduced challenges due to their uncontrollable production. Thiscomplicates maintaining the electrical grid balance, prompting sometransmission system operators in Western Europe to implement imbalance tariffsthat penalize unsustainable power deviations. These tariffs create an implicitdemand response framework to mitigate grid instability. Yet, several challengeslimit active participation. In Belgium, for example, imbalance prices are onlycalculated at the end of each 15-minute settlement period, creating high riskdue to price uncertainty. This risk is further amplified by the inherentvolatility of imbalance prices, discouraging participation. Althoughtransmission system operators provide minute-based price predictions, thesystem imbalance volatility makes accurate price predictions challenging toobtain and requires sophisticated techniques. Moreover, publishing priceestimates can prompt participants to adjust their schedules, potentiallyaffecting the system balance and the final price, adding further complexity. Toaddress these challenges, we propose a Monte Carlo Tree Search method thatpublishes accurate imbalance prices while accounting for potential responseactions. Our approach models the system dynamics using a neural networkforecaster and a cluster of virtual batteries controlled by reinforcementlearning agents. Compared to Belgium's current publication method, ourtechnique improves price accuracy by 20.4% under ideal conditions and by 12.8%in more realistic scenarios. This research addresses an unexplored, yet crucialproblem, positioning this paper as a pioneering work in analyzing the potentialof more advanced imbalance price publishing techniques.</description><author>Fabio Pavirani, Jonas Van Gompel, Seyed Soroush Karimi Madahi, Bert Claessens, Chris Develder</author><pubDate>Wed, 06 Nov 2024 15:49:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04011v1</guid></item><item><title>Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability</title><link>http://arxiv.org/abs/2411.04008v1</link><description>In mission-critical domains such as law enforcement and medical diagnosis,the ability to explain and interpret the outputs of deep learning models iscrucial for ensuring user trust and supporting informed decision-making.Despite advancements in explainability, existing methods often fall short inproviding explanations that mirror the depth and clarity of those given byhuman experts. Such expert-level explanations are essential for the dependableapplication of deep learning models in law enforcement and medical contexts.Additionally, we recognize that most explanations in real-world scenarios arecommunicated primarily through natural language. Addressing these needs, wepropose a novel approach that utilizes characteristic descriptors to explainmodel decisions by identifying their presence in images, thereby generatingexpert-like explanations. Our method incorporates a concept bottleneck layerwithin the model architecture, which calculates the similarity between imageand descriptor encodings to deliver inherent and faithful explanations. Throughexperiments in face recognition and chest X-ray diagnosis, we demonstrate thatour approach offers a significant contrast over existing techniques, which areoften limited to the use of saliency maps. We believe our approach represents asignificant step toward making deep learning systems more accountable,transparent, and trustworthy in the critical domains of face recognition andmedical diagnosis.</description><author>Bharat Chandra Yalavarthi, Nalini Ratha</author><pubDate>Wed, 06 Nov 2024 15:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04008v1</guid></item><item><title>Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval</title><link>http://arxiv.org/abs/2411.04006v1</link><description>This study explores the potential of off-the-shelf Vision-Language Models(VLMs) for high-level robot planning in the context of autonomous navigation.Indeed, while most of existing learning-based approaches for path planningrequire extensive task-specific training/fine-tuning, we demonstrate how suchtraining can be avoided for most practical cases. To do this, we introduceSelect2Plan (S2P), a novel training-free framework for high-level robotplanning which completely eliminates the need for fine-tuning or specialisedtraining. By leveraging structured Visual Question-Answering (VQA) andIn-Context Learning (ICL), our approach drastically reduces the need for datacollection, requiring a fraction of the task-specific data typically used bytrained models, or even relying only on online data. Our method facilitates theeffective use of a generally trained VLM in a flexible and cost-efficient way,and does not require additional sensing except for a simple monocular camera.We demonstrate its adaptability across various scene types, context sources,and sensing setups. We evaluate our approach in two distinct scenarios:traditional First-Person View (FPV) and infrastructure-driven Third-Person View(TPV) navigation, demonstrating the flexibility and simplicity of our method.Our technique significantly enhances the navigational capabilities of abaseline VLM of approximately 50% in TPV scenario, and is comparable to trainedmodels in the FPV one, with as few as 20 demonstrations.</description><author>Davide Buoso, Luke Robinson, Giuseppe Averta, Philip Torr, Tim Franzmeyer, Daniele De Martini</author><pubDate>Wed, 06 Nov 2024 15:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04006v1</guid></item><item><title>Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for Unsupervised Anomaly Detection in Ultrasound Imaging</title><link>http://arxiv.org/abs/2411.04004v1</link><description>Ultrasound (US) imaging is widely used in routine clinical practice due toits advantages of being radiation-free, cost-effective, and portable. However,the low reproducibility and quality of US images, combined with the scarcity ofexpert-level annotation, make the training of fully supervised segmentationmodels challenging. To address these issues, we propose a novel unsupervisedanomaly detection framework based on a diffusion model that incorporates asynthetic anomaly (Synomaly) noise function and a multi-stage diffusionprocess. Synomaly noise introduces synthetic anomalies into healthy imagesduring training, allowing the model to effectively learn anomaly removal. Themulti-stage diffusion process is introduced to progressively denoise images,preserving fine details while improving the quality of anomaly-freereconstructions. The generated high-fidelity counterfactual healthy images canfurther enhance the interpretability of the segmentation models, as well asprovide a reliable baseline for evaluating the extent of anomalies andsupporting clinical decision-making. Notably, the unsupervised anomalydetection model is trained purely on healthy images, eliminating the need foranomalous training samples and pixel-level annotations. We validate theproposed approach on carotid US, brain MRI, and liver CT datasets. Theexperimental results demonstrate that the proposed framework outperformsexisting state-of-the-art unsupervised anomaly detection methods, achievingperformance comparable to fully supervised segmentation models in the USdataset. Additionally, ablation studies underline the importance ofhyperparameter selection for Synomaly noise and the effectiveness of themulti-stage diffusion process in enhancing model performance.</description><author>Yuan Bi, Lucie Huang, Ricarda Clarenbach, Reza Ghotbi, Angelos Karlas, Nassir Navab, Zhongliang Jiang</author><pubDate>Wed, 06 Nov 2024 15:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04004v1</guid></item><item><title>ParaGAN: A Scalable Distributed Training Framework for Generative Adversarial Networks</title><link>http://arxiv.org/abs/2411.03999v1</link><description>Recent advances in Generative Artificial Intelligence have fueled numerousapplications, particularly those involving Generative Adversarial Networks(GANs), which are essential for synthesizing realistic photos and videos.However, efficiently training GANs remains a critical challenge due to theircomputationally intensive and numerically unstable nature. Existing methodsoften require days or even weeks for training, posing significant resource andtime constraints. In this work, we introduce ParaGAN, a scalable distributed GAN trainingframework that leverages asynchronous training and an asymmetric optimizationpolicy to accelerate GAN training. ParaGAN employs a congestion-aware datapipeline and hardware-aware layout transformation to enhance acceleratorutilization, resulting in over 30% improvements in throughput. With ParaGAN, wereduce the training time of BigGAN from 15 days to 14 hours while achieving 91%scaling efficiency. Additionally, ParaGAN enables unprecedented high-resolutionimage generation using BigGAN.</description><author>Ziji Shi, Jialin Li, Yang You</author><pubDate>Wed, 06 Nov 2024 15:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03999v1</guid></item><item><title>Learning to Compute Gröbner Bases</title><link>http://arxiv.org/abs/2311.12904v3</link><description>Solving a polynomial system, or computing an associated Gr\"obner basis, hasbeen a fundamental task in computational algebra. However, it is also known forits notorious doubly exponential time complexity in the number of variables inthe worst case. This paper is the first to address the learning of Gr\"obnerbasis computation with Transformers. The training requires many pairs of apolynomial system and the associated Gr\"obner basis, raising two novelalgebraic problems: random generation of Gr\"obner bases and transforming theminto non-Gr\"obner ones, termed as backward Gr\"obner problem. We resolve theseproblems with 0-dimensional radical ideals, the ideals appearing in variousapplications. Further, we propose a hybrid input embedding to handlecoefficient tokens with continuity bias and avoid the growth of the vocabularyset. The experiments show that our dataset generation method is a few orders ofmagnitude faster than a naive approach, overcoming a crucial challenge inlearning to compute Gr\"obner bases, and Gr\"obner computation is learnable ina particular class.</description><author>Hiroshi Kera, Yuki Ishihara, Yuta Kambe, Tristan Vaccon, Kazuhiro Yokoyama</author><pubDate>Wed, 06 Nov 2024 15:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12904v3</guid></item><item><title>TableGPT2: A Large Multimodal Model with Tabular Data Integration</title><link>http://arxiv.org/abs/2411.02059v2</link><description>The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AIapplications, presenting vast new opportunities across industries. Yet, theintegration of tabular data remains notably underdeveloped, despite itsfoundational role in numerous real-world domains. This gap is critical for three main reasons. First, database or datawarehouse data integration is essential for advanced applications; second, thevast and largely untapped resource of tabular data offers immense potential foranalysis; and third, the business intelligence domain specifically demandsadaptable, precise solutions that many current LLMs may struggle to provide. In response, we introduce TableGPT2, a model rigorously pre-trained andfine-tuned with over 593.8K tables and 2.36M high-quality query-table-outputtuples, a scale of table-related data unprecedented in prior research. Thisextensive training enables TableGPT2 to excel in table-centric tasks whilemaintaining strong general language and coding abilities. One of TableGPT2's key innovations is its novel table encoder, specificallydesigned to capture schema-level and cell-level information. This encoderstrengthens the model's ability to handle ambiguous queries, missing columnnames, and irregular tables commonly encountered in real-world applications.Similar to visual language models, this pioneering approach integrates with thedecoder to form a robust large multimodal model. We believe the results are compelling: over 23 benchmarking metrics,TableGPT2 achieves an average performance improvement of 35.20% in the 7B modeland 49.32% in the 72B model over prior benchmark-neutral LLMs, with robustgeneral-purpose capabilities intact.</description><author>Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, Liyao Li, Pengzuo Wu, Qi Zhang, Qingyi Huang, Saisai Yang, Tao Zhang, Wentao Ye, Wufang Zhu, Xiaomeng Hu, Xijun Gu, Xinjie Sun, Xiang Li, Yuhang Yang, Zhiqing Xiao</author><pubDate>Wed, 06 Nov 2024 15:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02059v2</guid></item><item><title>Towards Resource-Efficient Federated Learning in Industrial IoT for Multivariate Time Series Analysis</title><link>http://arxiv.org/abs/2411.03996v1</link><description>Anomaly and missing data constitute a thorny problem in industrialapplications. In recent years, deep learning enabled anomaly detection hasemerged as a critical direction, however the improved detection accuracy isachieved with the utilization of large neural networks, increasing theirstorage and computational cost. Moreover, the data collected in edge devicescontain user privacy, introducing challenges that can be successfully addressedby the privacy-preserving distributed paradigm, known as federated learning(FL). This framework allows edge devices to train and exchange modelsincreasing also the communication cost. Thus, to deal with the increasedcommunication, processing and storage challenges of the FL based deep anomalydetection NN pruning is expected to have significant benefits towards reducingthe processing, storage and communication complexity. With this focus, a novelcompression-based optimization problem is proposed at the server-side of a FLparadigm that fusses the received local models broadcast and performs pruninggenerating a more compressed model. Experiments in the context of anomalydetection and missing value imputation demonstrate that the proposed FLscenario along with the proposed compressed-based method are able to achievehigh compression rates (more than $99.7\%$) with negligible performance losses(less than $1.18\%$ ) as compared to the centralized solutions.</description><author>Alexandros Gkillas, Aris Lalos</author><pubDate>Wed, 06 Nov 2024 15:38:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03996v1</guid></item><item><title>Local vs distributed representations: What is the right basis for interpretability?</title><link>http://arxiv.org/abs/2411.03993v1</link><description>Much of the research on the interpretability of deep neural networks hasfocused on studying the visual features that maximally activate individualneurons. However, recent work has cast doubts on the usefulness of such localrepresentations for understanding the behavior of deep neural networks becauseindividual neurons tend to respond to multiple unrelated visual patterns, aphenomenon referred to as "superposition". A promising alternative todisentangle these complex patterns is learning sparsely distributed vectorrepresentations from entire network layers, as the resulting basis vectorsseemingly encode single identifiable visual patterns consistently. Thus, onewould expect the resulting code to align better with human perceivable visualpatterns, but supporting evidence remains, at best, anecdotal. To fill thisgap, we conducted three large-scale psychophysics experiments collected from apool of 560 participants. Our findings provide (i) strong evidence thatfeatures obtained from sparse distributed representations are easier tointerpret by human observers and (ii) that this effect is more pronounced inthe deepest layers of a neural network. Complementary analyses also reveal that(iii) features derived from sparse distributed representations contribute moreto the model's decision. Overall, our results highlight that distributedrepresentations constitute a superior basis for interpretability, underscoringa need for the field to move beyond the interpretation of local neural codes infavor of sparsely distributed ones.</description><author>Julien Colin, Lore Goetschalckx, Thomas Fel, Victor Boutin, Jay Gopal, Thomas Serre, Nuria Oliver</author><pubDate>Wed, 06 Nov 2024 15:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03993v1</guid></item><item><title>ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy</title><link>http://arxiv.org/abs/2411.03990v1</link><description>Imitation learning, e.g., diffusion policy, has been proven effective invarious robotic manipulation tasks. However, extensive demonstrations arerequired for policy robustness and generalization. To reduce the demonstrationreliance, we leverage spatial symmetry and propose ET-SEED, an efficienttrajectory-level SE(3) equivariant diffusion model for generating actionsequences in complex robot manipulation tasks. Further, previous equivariantdiffusion models require the per-step equivariance in the Markov process,making it difficult to learn policy under such strong constraints. Wetheoretically extend equivariant Markov kernels and simplify the condition ofequivariant diffusion process, thereby significantly improving trainingefficiency for trajectory-level SE(3) equivariant diffusion policy in anend-to-end manner. We evaluate ET-SEED on representative robotic manipulationtasks, involving rigid body, articulated and deformable object. Experimentsdemonstrate superior data efficiency and manipulation proficiency of ourproposed method, as well as its ability to generalize to unseen configurationswith only a few demonstrations. Website: https://et-seed.github.io/</description><author>Chenrui Tie, Yue Chen, Ruihai Wu, Boxuan Dong, Zeyi Li, Chongkai Gao, Hao Dong</author><pubDate>Wed, 06 Nov 2024 15:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03990v1</guid></item><item><title>ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models</title><link>http://arxiv.org/abs/2411.03982v1</link><description>Modern Text-to-Image (T2I) Diffusion models have revolutionized image editingby enabling the generation of high-quality photorealistic images. While the defacto method for performing edits with T2I models is through text instructions,this approach non-trivial due to the complex many-to-many mapping betweennatural language and images. In this work, we address exemplar-based imageediting -- the task of transferring an edit from an exemplar pair to a contentimage(s). We propose ReEdit, a modular and efficient end-to-end framework thatcaptures edits in both text and image modalities while ensuring the fidelity ofthe edited image. We validate the effectiveness of ReEdit through extensivecomparisons with state-of-the-art baselines and sensitivity analyses of keydesign choices. Our results demonstrate that ReEdit consistently outperformscontemporary approaches both qualitatively and quantitatively. Additionally,ReEdit boasts high practical applicability, as it does not require anytask-specific optimization and is four times faster than the next bestbaseline.</description><author>Ashutosh Srivastava, Tarun Ram Menta, Abhinav Java, Avadhoot Jadhav, Silky Singh, Surgan Jandial, Balaji Krishnamurthy</author><pubDate>Wed, 06 Nov 2024 15:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03982v1</guid></item><item><title>Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning</title><link>http://arxiv.org/abs/2411.03978v1</link><description>Multiple clustering aims to discover various latent structures of data fromdifferent aspects. Deep multiple clustering methods have achieved remarkableperformance by exploiting complex patterns and relationships in data. However,existing works struggle to flexibly adapt to diverse user-specific needs indata grouping, which may require manual understanding of each clustering. Toaddress these limitations, we introduce Multi-Sub, a novel end-to-end multipleclustering approach that incorporates a multi-modal subspace proxy learningframework in this work. Utilizing the synergistic capabilities of CLIP andGPT-4, Multi-Sub aligns textual prompts expressing user preferences with theircorresponding visual representations. This is achieved by automaticallygenerating proxy words from large language models that act as subspace bases,thus allowing for the customized representation of data in terms specific tothe user's interests. Our method consistently outperforms existing baselinesacross a broad set of datasets in visual multiple clustering tasks. Our code isavailable at https://github.com/Alexander-Yao/Multi-Sub.</description><author>Jiawei Yao, Qi Qian, Juhua Hu</author><pubDate>Wed, 06 Nov 2024 15:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03978v1</guid></item><item><title>HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion Segmentation</title><link>http://arxiv.org/abs/2411.03976v1</link><description>High resolution is crucial for precise segmentation in fundus images, yethandling high-resolution inputs incurs considerable GPU memory costs, withdiminishing performance gains as overhead increases. To address this issuewhile tackling the challenge of segmenting tiny objects, recent studies haveexplored local-global fusion methods. These methods preserve fine details usinglocal regions and capture long-range context information from downscaled globalimages. However, the necessity of multiple forward passes inevitably incurssignificant computational overhead, adversely affecting inference speed. Inthis paper, we propose HRDecoder, a simple High-Resolution Decoder network forfundus lesion segmentation. It integrates a high-resolution representationlearning module to capture fine-grained local features and a high-resolutionfusion module to fuse multi-scale predictions. Our method effectively improvesthe overall segmentation accuracy of fundus lesions while consuming reasonablememory and computational overhead, and maintaining satisfying inference speed.Experimental results on the IDRID and DDR datasets demonstrate theeffectiveness of our method. Code is available athttps://github.com/CVIU-CSU/HRDecoder.</description><author>Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu</author><pubDate>Wed, 06 Nov 2024 15:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03976v1</guid></item><item><title>Topology-guided Hypergraph Transformer Network: Unveiling Structural Insights for Improved Representation</title><link>http://arxiv.org/abs/2310.09657v4</link><description>Hypergraphs, with their capacity to depict high-order relationships, haveemerged as a significant extension of traditional graphs. Although Graph NeuralNetworks (GNNs) have remarkable performance in graph representation learning,their extension to hypergraphs encounters challenges due to their intricatestructures. Furthermore, current hypergraph transformers, a special variant ofGNN, utilize semantic feature-based self-attention, ignoring topologicalattributes of nodes and hyperedges. To address these challenges, we propose aTopology-guided Hypergraph Transformer Network (THTN). In this model, we firstformulate a hypergraph from a graph while retaining its structural essence tolearn higher-order relations within the graph. Then, we design a simple yeteffective structural and spatial encoding module to incorporate the topologicaland spatial information of the nodes into their representation. Further, wepresent a structure-aware self-attention mechanism that discovers the importantnodes and hyperedges from both semantic and structural viewpoints. Byleveraging these two modules, THTN crafts an improved node representation,capturing both local and global topological expressions. Extensive experimentsconducted on node classification tasks demonstrate that the performance of theproposed model consistently exceeds that of the existing approaches.</description><author>Khaled Mohammed Saifuddin, Mehmet Emin Aktas, Esra Akbas</author><pubDate>Wed, 06 Nov 2024 15:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09657v4</guid></item><item><title>WorryWords: Norms of Anxiety Association for over 44k English Words</title><link>http://arxiv.org/abs/2411.03966v1</link><description>Anxiety, the anticipatory unease about a potential negative outcome, is acommon and beneficial human emotion. However, there is still much that is notknown, such as how anxiety relates to our body and how it manifests inlanguage. This is especially pertinent given the increasing impact ofanxiety-related disorders. In this work, we introduce WorryWords, the firstlarge-scale repository of manually derived word--anxiety associations for over44,450 English words. We show that the anxiety associations are highlyreliable. We use WorryWords to study the relationship between anxiety and otheremotion constructs, as well as the rate at which children acquire anxiety wordswith age. Finally, we show that using WorryWords alone, one can accuratelytrack the change of anxiety in streams of text. The lexicon enables a widevariety of anxiety-related research in psychology, NLP, public health, andsocial sciences. WorryWords (and its translations to over 100 languages) isfreely available. http://saifmohammad.com/worrywords.html</description><author>Saif M. Mohammad</author><pubDate>Wed, 06 Nov 2024 15:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03966v1</guid></item><item><title>Lecture Notes on Linear Neural Networks: A Tale of Optimization and Generalization in Deep Learning</title><link>http://arxiv.org/abs/2408.13767v2</link><description>These notes are based on a lecture delivered by NC on March 2021, as part ofan advanced course in Princeton University on the mathematical understanding ofdeep learning. They present a theory (developed by NC, NR and collaborators) oflinear neural networks -- a fundamental model in the study of optimization andgeneralization in deep learning. Practical applications born from the presentedtheory are also discussed. The theory is based on mathematical tools that aredynamical in nature. It showcases the potential of such tools to push theenvelope of our understanding of optimization and generalization in deeplearning. The text assumes familiarity with the basics of statistical learningtheory. Exercises (without solutions) are included.</description><author>Nadav Cohen, Noam Razin</author><pubDate>Wed, 06 Nov 2024 15:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13767v2</guid></item><item><title>Bayesian algorithmic perfumery: A Hierarchical Relevance Vector Machine for the Estimation of Personalized Fragrance Preferences based on Three Sensory Layers and Jungian Personality Archetypes</title><link>http://arxiv.org/abs/2411.03965v1</link><description>This study explores a Bayesian algorithmic approach to personalized fragrancerecommendation by integrating hierarchical Relevance Vector Machines (RVM) andJungian personality archetypes. The paper proposes a structured model thatlinks individual scent preferences for top, middle, and base notes topersonality traits derived from Jungian archetypes, such as the Hero,Caregiver, and Explorer, among others. The algorithm utilizes Bayesian updatingto dynamically refine predictions as users interact with each fragrance note.This iterative process allows for the personalization of fragrance experiencesbased on prior data and personality assessments, leading to adaptive andinterpretable recommendations. By combining psychological theory with Bayesianmachine learning, this approach addresses the complexity of modeling individualpreferences while capturing user-specific and population-level trends. Thestudy highlights the potential of hierarchical Bayesian frameworks in creatingcustomized olfactory experiences, informed by psychological and demographicfactors, contributing to advancements in personalized product design andmachine learning applications in sensory-based industries.</description><author>Rolando Gonzales Martinez</author><pubDate>Wed, 06 Nov 2024 15:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03965v1</guid></item><item><title>Long-term foehn reconstruction combining unsupervised and supervised learning</title><link>http://arxiv.org/abs/2406.01818v2</link><description>Foehn winds, characterized by abrupt temperature increases and wind speedchanges, significantly impact regions on the leeward side of mountain ranges,e.g., by spreading wildfires. Understanding how foehn occurrences change underclimate change is crucial. Unfortunately, foehn cannot be measured directly buthas to be inferred from meteorological measurements employing suitableclassification schemes. Hence, this approach is typically limited to specificperiods for which the necessary data are available. We present a novel approachfor reconstructing historical foehn occurrences using a combination ofunsupervised and supervised probabilistic statistical learning methods. Weutilize in-situ measurements (available for recent decades) to train anunsupervised learner (finite mixture model) for automatic foehn classification.These labeled data are then linked to reanalysis data (covering longer periods)using a supervised learner (lasso or boosting). This allows to reconstruct pastfoehn probabilities based solely on reanalysis data. Applying this method toERA5 reanalysis data for six stations across Switzerland and Austria achievesaccurate hourly reconstructions of north and south foehn occurrence,respectively, dating back to 1940. This paves the way for investigating howseasonal foehn patterns have evolved over the past 83 years, providing valuableinsights into climate change impacts on these critical wind events.</description><author>Reto Stauffer, Achim Zeileis, Georg J. Mayr</author><pubDate>Wed, 06 Nov 2024 14:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01818v2</guid></item><item><title>BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation</title><link>http://arxiv.org/abs/2407.17952v2</link><description>By training over large-scale datasets, zero-shot monocular depth estimation(MDE) methods show robust performance in the wild but often suffer frominsufficient detail. Although recent diffusion-based MDE approaches exhibit asuperior ability to extract details, they struggle in geometrically complexscenes that challenge their geometry prior, trained on less diverse 3D data. Toleverage the complementary merits of both worlds, we propose BetterDepth toachieve geometrically correct affine-invariant MDE while capturing finedetails. Specifically, BetterDepth is a conditional diffusion-based refinerthat takes the prediction from pre-trained MDE models as depth conditioning, inwhich the global depth layout is well-captured, and iteratively refines detailsbased on the input image. For the training of such a refiner, we propose globalpre-alignment and local patch masking methods to ensure BetterDepth remainsfaithful to the depth conditioning while learning to add fine-grained scenedetails. With efficient training on small-scale synthetic datasets, BetterDepthachieves state-of-the-art zero-shot MDE performance on diverse public datasetsand on in-the-wild scenes. Moreover, BetterDepth can improve the performance ofother MDE models in a plug-and-play manner without further re-training.</description><author>Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross, Konrad Schindler, Christopher Schroers</author><pubDate>Wed, 06 Nov 2024 14:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17952v2</guid></item><item><title>What Really is Commonsense Knowledge?</title><link>http://arxiv.org/abs/2411.03964v1</link><description>Commonsense datasets have been well developed in Natural Language Processing,mainly through crowdsource human annotation. However, there are debates on thegenuineness of commonsense reasoning benchmarks. In specific, a significantportion of instances in some commonsense benchmarks do not concern commonsenseknowledge. That problem would undermine the measurement of the true commonsensereasoning ability of evaluated models. It is also suggested that the problemoriginated from a blurry concept of commonsense knowledge, as distinguishedfrom other types of knowledge. To demystify all of the above claims, in thisstudy, we survey existing definitions of commonsense knowledge, ground into thethree frameworks for defining concepts, and consolidate them into amulti-framework unified definition of commonsense knowledge (so-calledconsolidated definition). We then use the consolidated definition forannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasetsto examine the above claims. Our study shows that there exists a large portionof non-commonsense-knowledge instances in the two datasets, and a largeperformance gap on these two subsets where Large Language Models (LLMs) performworse on commonsense-knowledge instances.</description><author>Quyet V. Do, Junze Li, Tung-Duong Vuong, Zhaowei Wang, Yangqiu Song, Xiaojuan Ma</author><pubDate>Wed, 06 Nov 2024 14:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03964v1</guid></item><item><title>Diversity Progress for Goal Selection in Discriminability-Motivated RL</title><link>http://arxiv.org/abs/2411.01521v2</link><description>Non-uniform goal selection has the potential to improve the reinforcementlearning (RL) of skills over uniform-random selection. In this paper, weintroduce a method for learning a goal-selection policy inintrinsically-motivated goal-conditioned RL: "Diversity Progress" (DP). Thelearner forms a curriculum based on observed improvement in discriminabilityover its set of goals. Our proposed method is applicable to the class ofdiscriminability-motivated agents, where the intrinsic reward is computed as afunction of the agent's certainty of following the true goal being pursued.This reward can motivate the agent to learn a set of diverse skills withoutextrinsic rewards. We demonstrate empirically that a DP-motivated agent canlearn a set of distinguishable skills faster than previous approaches, and doso without suffering from a collapse of the goal distribution -- a known issuewith some prior approaches. We end with plans to take this proof-of-conceptforward.</description><author>Erik M. Lintunen, Nadia M. Ady, Christian Guckelsberger</author><pubDate>Wed, 06 Nov 2024 14:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01521v2</guid></item><item><title>How Does A Text Preprocessing Pipeline Affect Ontology Syntactic Matching?</title><link>http://arxiv.org/abs/2411.03962v1</link><description>The generic text preprocessing pipeline, comprising Tokenisation,Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has beenimplemented in many ontology matching (OM) systems. However, the lack ofstandardisation in text preprocessing creates diversity in mapping results. Inthis paper, we investigate the effect of the text preprocessing pipeline on OMtasks at syntactic levels. Our experiments on 8 Ontology Alignment EvaluationInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)Tokenisation and Normalisation are currently more effective than Stop WordsRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation andStemming is task-specific. We recommend standalone Lemmatisation or Stemmingwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmerperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)Tagging does not help Lemmatisation. To repair less effective Stop WordsRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novelcontext-based pipeline repair approach that significantly improves matchingcorrectness and overall matching performance. We also discuss the use of textpreprocessing pipeline in the new era of large language models (LLMs).</description><author>Zhangcheng Qiang, Kerry Taylor, Weiqing Wang</author><pubDate>Wed, 06 Nov 2024 14:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03962v1</guid></item><item><title>BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack</title><link>http://arxiv.org/abs/2406.10149v2</link><description>In recent years, the input context sizes of large language models (LLMs) haveincreased dramatically. However, existing evaluation methods have not keptpace, failing to comprehensively assess the efficiency of models in handlinglong contexts. To bridge this gap, we introduce the BABILong benchmark,designed to test language models' ability to reason across facts distributed inextremely long documents. BABILong includes a diverse set of 20 reasoningtasks, including fact chaining, simple induction, deduction, counting, andhandling lists/sets. These tasks are challenging on their own, and even moredemanding when the required facts are scattered across long natural text. Ourevaluations show that popular LLMs effectively utilize only 10-20\% of thecontext and their performance declines sharply with increased reasoningcomplexity. Among alternatives to in-context reasoning, Retrieval-AugmentedGeneration methods achieve a modest 60\% accuracy on single-fact questionanswering, independent of context length. Among context extension methods, thehighest performance is demonstrated by recurrent memory transformers afterfine-tuning, enabling the processing of lengths up to 50 million tokens. TheBABILong benchmark is extendable to any length to support the evaluation of newupcoming models with increased capabilities, and we provide splits up to 10million token lengths.</description><author>Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev</author><pubDate>Wed, 06 Nov 2024 14:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10149v2</guid></item><item><title>Discrete Aware Matrix Completion via Convexized $\ell_0$-Norm Approximation</title><link>http://arxiv.org/abs/2405.02101v2</link><description>We consider a novel algorithm, for the completion of partially observedlow-rank matrices in a structured setting where each entry can be chosen from afinite discrete alphabet set, such as in common recommender systems. Theproposed low-rank matrix completion (MC) method is an improved variation ofstate-of-the-art (SotA) discrete aware matrix completion method which wepreviously proposed, in which discreteness is enforced by an $\ell_0$-normregularizer, not by replaced with the $\ell_1$-norm, but instead approximatedby a continuous and differentiable function normalized via fractionalprogramming (FP) under a proximal gradient (PG) framework. Simulation resultsdemonstrate the superior performance of the new method compared to the SotAtechniques as well as the earlier $\ell_1$-norm-based discrete-aware matrixcompletion approach.</description><author>Niclas Führling, Kengo Ando, Giuseppe Thadeu Freitas de Abreu, David González G., Osvaldo Gonsa</author><pubDate>Wed, 06 Nov 2024 14:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02101v2</guid></item><item><title>Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology</title><link>http://arxiv.org/abs/2408.00738v3</link><description>Foundation models are rapidly being developed for computational pathologyapplications. However, it remains an open question which factors are mostimportant for downstream performance with data scale and diversity, model size,and training algorithm all playing a role. In this work, we propose algorithmicmodifications, tailored for pathology, and we present the result of scalingboth data and model size, surpassing previous studies in both dimensions. Weintroduce three new models: Virchow2, a 632 million parameter visiontransformer, Virchow2G, a 1.9 billion parameter vision transformer, andVirchow2G Mini, a 22 million parameter distillation of Virchow2G, each trainedwith 3.1 million histopathology whole slide images, with diverse tissues,originating institutions, and stains. We achieve state of the art performanceon 12 tile-level tasks, as compared to the top performing competing models. Ourresults suggest that data diversity and domain-specific methods can outperformmodels that only scale in the number of parameters, but, on average,performance benefits from the combination of domain-specific methods, datascale, and model scale.</description><author>Eric Zimmermann, Eugene Vorontsov, Julian Viret, Adam Casson, Michal Zelechowski, George Shaikovski, Neil Tenenholtz, James Hall, David Klimstra, Razik Yousfi, Thomas Fuchs, Nicolo Fusi, Siqi Liu, Kristen Severson</author><pubDate>Wed, 06 Nov 2024 14:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00738v3</guid></item><item><title>Face Reconstruction from Face Embeddings using Adapter to a Face Foundation Model</title><link>http://arxiv.org/abs/2411.03960v1</link><description>Face recognition systems extract embedding vectors from face images and usethese embeddings to verify or identify individuals. Face reconstruction attack(also known as template inversion) refers to reconstructing face images fromface embeddings and using the reconstructed face image to enter a facerecognition system. In this paper, we propose to use a face foundation model toreconstruct face images from the embeddings of a blackbox face recognitionmodel. The foundation model is trained with 42M images to generate face imagesfrom the facial embeddings of a fixed face recognition model. We propose to usean adapter to translate target embeddings into the embedding space of thefoundation model. The generated images are evaluated on different facerecognition models and different datasets, demonstrating the effectiveness ofour method to translate embeddings of different face recognition models. Wealso evaluate the transferability of reconstructed face images when attackingdifferent face recognition models. Our experimental results show that ourreconstructed face images outperform previous reconstruction attacks againstface recognition models.</description><author>Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel</author><pubDate>Wed, 06 Nov 2024 14:45:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03960v1</guid></item><item><title>Energy Score-based Pseudo-Label Filtering and Adaptive Loss for Imbalanced Semi-supervised SAR target recognition</title><link>http://arxiv.org/abs/2411.03959v1</link><description>Automatic target recognition (ATR) is an important use case for syntheticaperture radar (SAR) image interpretation. Recent years have seen significantadvancements in SAR ATR technology based on semi-supervised learning. However,existing semi-supervised SAR ATR algorithms show low recognition accuracy inthe case of class imbalance. This work offers a non-balanced semi-supervisedSAR target recognition approach using dynamic energy scores and adaptive loss.First, an energy score-based method is developed to dynamically selectunlabeled samples near to the training distribution as pseudo-labels duringtraining, assuring pseudo-label reliability in long-tailed distributioncircumstances. Secondly, loss functions suitable for class imbalances areproposed, including adaptive margin perception loss and adaptive hard tripletloss, the former offsets inter-class confusion of classifiers, alleviating theimbalance issue inherent in pseudo-label generation. The latter effectivelytackles the model's preference for the majority class by focusing on complexdifficult samples during training. Experimental results on extremely imbalancedSAR datasets demonstrate that the proposed method performs well under the dualconstraints of scarce labels and data imbalance, effectively overcoming themodel bias caused by data imbalance and achieving high-precision targetrecognition.</description><author>Xinzheng Zhang, Yuqing Luo, Guopeng Li</author><pubDate>Wed, 06 Nov 2024 14:45:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03959v1</guid></item><item><title>Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2411.03957v1</link><description>Retrieval-Augmented Generation (RAG) has proven to be an effective method formitigating hallucination issues inherent in large language models (LLMs).Previous approaches typically train retrievers based on semantic similarity,lacking optimization for RAG. More recent works have proposed aligningretrievers with the preference signals of LLMs. However, these preferencesignals are often difficult for dense retrievers, which typically have weakerlanguage capabilities, to understand and learn effectively. Drawing inspirationfrom pedagogical theories like Guided Discovery Learning, we propose a novelframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages thelanguage capabilities of LLMs to construct examples from a more granular,information-centric perspective to guide the learning of retrievers.Specifically, our method utilizes LLMs to construct easy-to-understand examplesfrom samples where the retriever performs poorly, focusing on three learningobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,and purity. These examples serve as scaffolding to ultimately align theretriever with the LLM's preferences. Furthermore, we employ a dual curriculumlearning strategy and leverage the reciprocal feedback between LLM andretriever to further enhance the performance of the RAG system. A series ofexperiments demonstrate that our proposed framework enhances the performance ofRAG systems equipped with different retrievers and is applicable to variousLLMs.</description><author>Yuhang Liu, Xueyu Hu, Shengyu Zhang, Jingyuan Chen, Fan Wu, Fei Wu</author><pubDate>Wed, 06 Nov 2024 14:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03957v1</guid></item><item><title>TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models</title><link>http://arxiv.org/abs/2409.16118v3</link><description>Data collection is often difficult in critical fields such as medicine,physics, and chemistry. As a result, classification methods usually performpoorly with these small datasets, leading to weak predictive performance.Increasing the training set with additional synthetic data, similar to dataaugmentation in images, is commonly believed to improve downstreamclassification performance. However, current tabular generative methods thatlearn either the joint distribution $ p(\mathbf{x}, y) $ or theclass-conditional distribution $ p(\mathbf{x} \mid y) $ often overfit on smalldatasets, resulting in poor-quality synthetic data, usually worseningclassification performance compared to using real data alone. To solve thesechallenges, we introduce TabEBM, a novel class-conditional generative methodusing Energy-Based Models (EBMs). Unlike existing methods that use a sharedmodel to approximate all class-conditional densities, our key innovation is tocreate distinct EBM generative models for each class, each modelling itsclass-specific data distribution individually. This approach creates robustenergy landscapes, even in ambiguous class distributions. Our experiments showthat TabEBM generates synthetic data with higher quality and better statisticalfidelity than existing methods. When used for data augmentation, our syntheticdata consistently improves the classification performance across diversedatasets of various sizes, especially small ones. Code is available athttps://github.com/andreimargeloiu/TabEBM.</description><author>Andrei Margeloiu, Xiangjian Jiang, Nikola Simidjievski, Mateja Jamnik</author><pubDate>Wed, 06 Nov 2024 14:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16118v3</guid></item><item><title>Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks</title><link>http://arxiv.org/abs/2411.03948v1</link><description>This paper investigates the capabilities of text-to-audio music generationmodels in producing long-form music with prompts that change over time,focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). Weintroduce Babel Bardo, a system that uses Large Language Models (LLMs) totransform speech transcriptions into music descriptions for controlling atext-to-music model. Four versions of Babel Bardo were compared in two TRPGcampaigns: a baseline using direct speech transcriptions, and three LLM-basedversions with varying approaches to music description generation. Evaluationsconsidered audio quality, story alignment, and transition smoothness. Resultsindicate that detailed music descriptions improve audio quality whilemaintaining consistency across consecutive descriptions enhances storyalignment and transition smoothness.</description><author>Felipe Marra, Lucas N. Ferreira</author><pubDate>Wed, 06 Nov 2024 14:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03948v1</guid></item><item><title>Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models</title><link>http://arxiv.org/abs/2404.07724v2</link><description>Guidance is a crucial technique for extracting the best performance out ofimage-generating diffusion models. Traditionally, a constant guidance weighthas been applied throughout the sampling chain of an image. We show thatguidance is clearly harmful toward the beginning of the chain (high noiselevels), largely unnecessary toward the end (low noise levels), and onlybeneficial in the middle. We thus restrict it to a specific range of noiselevels, improving both the inference speed and result quality. This limitedguidance interval improves the record FID in ImageNet-512 significantly, from1.81 to 1.40. We show that it is quantitatively and qualitatively beneficialacross different sampler parameters, network architectures, and datasets,including the large-scale setting of Stable Diffusion XL. We thus suggestexposing the guidance interval as a hyperparameter in all diffusion models thatuse guidance.</description><author>Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen</author><pubDate>Wed, 06 Nov 2024 14:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07724v2</guid></item><item><title>Policy Mirror Descent with Lookahead</title><link>http://arxiv.org/abs/2403.14156v3</link><description>Policy Mirror Descent (PMD) stands as a versatile algorithmic frameworkencompassing several seminal policy gradient algorithms such as natural policygradient, with connections with state-of-the-art reinforcement learning (RL)algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iterationalgorithm implementing regularized 1-step greedy policy improvement. However,1-step greedy policies might not be the best choice and recent remarkableempirical successes in RL such as AlphaGo and AlphaZero have demonstrated thatgreedy approaches with respect to multiple steps outperform their 1-stepcounterpart. In this work, we propose a new class of PMD algorithms called$h$-PMD which incorporates multi-step greedy policy improvement with lookaheaddepth $h$ to the PMD update rule. To solve discounted infinite horizon MarkovDecision Processes with discount factor $\gamma$, we show that $h$-PMD whichgeneralizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linearconvergence rate, contingent on the computation of multi-step greedy policies.We propose an inexact version of $h$-PMD where lookahead action values areestimated. Under a generative model, we establish a sample complexity for$h$-PMD which improves over prior work. Finally, we extend our result to linearfunction approximation to scale to large state spaces. Under suitableassumptions, our sample complexity only involves dependence on the dimension ofthe feature map space instead of the state space size.</description><author>Kimon Protopapas, Anas Barakat</author><pubDate>Wed, 06 Nov 2024 14:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14156v3</guid></item><item><title>Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks</title><link>http://arxiv.org/abs/2411.03945v1</link><description>In-Context Learning (ICL) is a phenomenon where task learning occurs througha prompt sequence without the necessity of parameter updates. ICL inMulti-Headed Attention (MHA) with absolute positional embedding has been thefocus of more study than other sequence model varieties. We examineimplications of architectural differences between GPT-2 and LLaMa as well asLlaMa and Mamba. We extend work done by Garg et al. (2022) and Park et al.(2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models - examining theinterplay between sequence transformation blocks and regressive performancein-context. We note that certain architectural changes cause degraded trainingefficiency/ICL accuracy by converging to suboptimal predictors or convergingslower. We also find certain hybrids showing optimistic performanceimprovements, informing potential future ICL-focused architecturemodifications. Additionally, we propose the "ICL regression score", a scalarmetric describing a model's whole performance on a specific task. Computelimitations impose restrictions on our architecture-space, training duration,number of training runs, function class complexity, and benchmark complexity.To foster reproducible and extensible research, we provide a typed, modular,and extensible Python package on which we run all experiments.</description><author>Ryan Campbell, Nelson Lojo, Kesava Viswanadha, Christoffer Grondal Tryggestad, Derrick Han Sun, Sriteja Vijapurapu, August Rolfsen, Anant Sahai</author><pubDate>Wed, 06 Nov 2024 14:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03945v1</guid></item><item><title>Continuous Management of Machine Learning-Based Application Behavior</title><link>http://arxiv.org/abs/2311.12686v5</link><description>Modern applications are increasingly driven by Machine Learning (ML) modelswhose non-deterministic behavior is affecting the entire application life cyclefrom design to operation. The pervasive adoption of ML is urgently calling forapproaches that guarantee a stable non-functional behavior of ML-basedapplications over time and across model changes. To this aim, non-functionalproperties of ML models, such as privacy, confidentiality, fairness, andexplainability, must be monitored, verified, and maintained. Existingapproaches mostly focus on i) implementing solutions for classifier selectionaccording to the functional behavior of ML models, ii) finding new algorithmicsolutions, such as continuous re-training. In this paper, we propose amulti-model approach that aims to guarantee a stable non-functional behavior ofML-based applications. An architectural and methodological approach is providedto compare multiple ML models showing similar non-functional properties andselect the model supporting stable non-functional behavior over time accordingto (dynamic and unpredictable) contextual changes. Our approach goes beyond thestate of the art by providing a solution that continuously guarantees a stablenon-functional behavior of ML-based applications, is ML algorithm-agnostic, andis driven by non-functional properties assessed on the ML models themselves. Itconsists of a two-step process working during application operation, wheremodel assessment verifies non-functional properties of ML models trained andselected at development time, and model substitution guarantees continuous andstable support of non-functional properties. We experimentally evaluate oursolution in a real-world scenario focusing on non-functional property fairness.</description><author>Marco Anisetti, Claudio A. Ardagna, Nicola Bena, Ernesto Damiani, Paolo G. Panero</author><pubDate>Wed, 06 Nov 2024 14:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12686v5</guid></item><item><title>Self-supervised 3D Point Cloud Completion via Multi-view Adversarial Learning</title><link>http://arxiv.org/abs/2407.09786v2</link><description>In real-world scenarios, scanned point clouds are often incomplete due toocclusion issues. The task of self-supervised point cloud completion involvesreconstructing missing regions of these incomplete objects without thesupervision of complete ground truth. Current self-supervised methods eitherrely on multiple views of partial observations for supervision or overlook theintrinsic geometric similarity that can be identified and utilized from thegiven partial point clouds. In this paper, we propose MAL-SPC, a framework thateffectively leverages both object-level and category-specific geometricsimilarities to complete missing structures. Our MAL-SPC does not require any3D complete supervision and only necessitates a single partial point cloud foreach object. Specifically, we first introduce a Pattern Retrieval Network toretrieve similar position and curvature patterns between the partial input andthe predicted shape, then leverage these similarities to densify and refine thereconstructed results. Additionally, we render the reconstructed complete shapeinto multi-view depth maps and design an adversarial learning module to learnthe geometry of the target shape from category-specific single-view depthimages. To achieve anisotropic rendering, we design a density-aware radiusestimation algorithm to improve the quality of the rendered images. Our MAL-SPCyields the best results compared to current state-of-the-art methods.We willmake the source code publicly available at \url{https://github.com/ltwu6/malspc</description><author>Lintai Wu, Xianjing Cheng, Yong Xu, Huanqiang Zeng, Junhui Hou</author><pubDate>Wed, 06 Nov 2024 14:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09786v2</guid></item><item><title>Fine-tuning -- a Transfer Learning approach</title><link>http://arxiv.org/abs/2411.03941v1</link><description>Secondary research use of Electronic Health Records (EHRs) is often hamperedby the abundance of missing data in this valuable resource. Missingness in EHRsoccurs naturally as a result of the data recording practices during routineclinical care, but handling it is crucial to the precision of medical analysisand the decision-making that follows. The literature contains a variety ofimputation methodologies based on deep neural networks. Those aim to overcomethe dynamic, heterogeneous and multivariate missingness patterns of EHRs, whichcannot be handled by classical and statistical imputation methods. However, allexisting deep imputation methods rely on end-to-end pipelines that incorporateboth imputation and downstream analyses, e.g. classification. This couplingmakes it difficult to assess the quality of imputation and takes away theflexibility of re-using the imputer for a different task. Furthermore, mostend-to-end deep architectures tend to use complex networks to perform thedownstream task, in addition to the already sophisticated deep imputationnetwork. We, therefore ask if the high performance reported in the literatureis due to the imputer or the classifier and further ask if an optimisedstate-of-the-art imputer is used, a simpler classifier can achieve comparableperformance. This paper explores the development of a modular, deeplearning-based imputation and classification pipeline, specifically built toleverage the capabilities of state-of-the-art imputation models for downstreamclassification tasks. Such a modular approach enables a) objective assessmentof the quality of the imputer and classifier independently, and b) enables theexploration of the performance of simpler classification architectures using anoptimised imputer.</description><author>Joseph Arul Raj, Linglong Qian, Zina Ibrahim</author><pubDate>Wed, 06 Nov 2024 14:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03941v1</guid></item><item><title>Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT</title><link>http://arxiv.org/abs/2411.02964v2</link><description>Speech is the most natural way of expressing ourselves as humans. Identifyingemotion from speech is a nontrivial task due to the ambiguous definition ofemotion itself. Speaker Emotion Recognition (SER) is essential forunderstanding human emotional behavior. The SER task is challenging due to thevariety of speakers, background noise, complexity of emotions, and speakingstyles. It has many applications in education, healthcare, customer service,and Human-Computer Interaction (HCI). Previously, conventional machine learningmethods such as SVM, HMM, and KNN have been used for the SER task. In recentyears, deep learning methods have become popular, with convolutional neuralnetworks and recurrent neural networks being used for SER tasks. The input ofthese methods is mostly spectrograms and hand-crafted features. In this work,we study the use of self-supervised transformer-based models, Wav2Vec2 andHuBERT, to determine the emotion of speakers from their voice. The modelsautomatically extract features from raw audio signals, which are then used forthe classification task. The proposed solution is evaluated on reputabledatasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results showthe effectiveness of the proposed method on different datasets. Moreover, themodel has been used for real-world applications like call center conversations,and the results demonstrate that the model accurately predicts emotions.</description><author>Pourya Jafarzadeh, Amir Mohammad Rostami, Padideh Choobdar</author><pubDate>Wed, 06 Nov 2024 14:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02964v2</guid></item><item><title>Evaluating Morphological Compositional Generalization in Large Language Models</title><link>http://arxiv.org/abs/2410.12656v2</link><description>Large language models (LLMs) have demonstrated significant progress invarious natural language generation and understanding tasks. However, theirlinguistic generalization capabilities remain questionable, raising doubtsabout whether these models learn language similarly to humans. While humansexhibit compositional generalization and linguistic creativity in language use,the extent to which LLMs replicate these abilities, particularly in morphology,is under-explored. In this work, we systematically investigate themorphological generalization abilities of LLMs through the lens ofcompositionality. We define morphemes as compositional primitives and design anovel suite of generative and discriminative tasks to assess morphologicalproductivity and systematicity. Focusing on agglutinative languages such asTurkish and Finnish, we evaluate several state-of-the-art instruction-finetunedmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMsstruggle with morphological compositional generalization particularly whenapplied to novel word roots, with performance declining sharply asmorphological complexity increases. While models can identify individualmorphological combinations better than chance, their performance lackssystematicity, leading to significant accuracy gaps compared to humans.</description><author>Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Lonneke van der Plas, Duygu Ataman</author><pubDate>Wed, 06 Nov 2024 14:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12656v2</guid></item><item><title>GUIDE-VAE: Advancing Data Generation with User Information and Pattern Dictionaries</title><link>http://arxiv.org/abs/2411.03936v1</link><description>Generative modelling of multi-user datasets has become prominent in scienceand engineering. Generating a data point for a given user requires employinguser information, and conventional generative models, including variationalautoencoders (VAEs), often ignore that. This paper introduces GUIDE-VAE, anovel conditional generative model that leverages user embeddings to generateuser-guided data. By allowing the model to benefit from shared patterns acrossusers, GUIDE-VAE enhances performance in multi-user settings, even undersignificant data imbalance. In addition to integrating user information,GUIDE-VAE incorporates a pattern dictionary-based covariance composition (PDCC)to improve the realism of generated samples by capturing complex featuredependencies. While user embeddings drive performance gains, PDCC addressescommon issues such as noise and over-smoothing typically seen in VAEs. The proposed GUIDE-VAE was evaluated on a multi-user smart meter datasetcharacterized by substantial data imbalance across users. Quantitative resultsshow that GUIDE-VAE performs effectively in both synthetic data generation andmissing record imputation tasks, while qualitative evaluations reveal thatGUIDE-VAE produces more plausible and less noisy data. These results establishGUIDE-VAE as a promising tool for controlled, realistic data generation inmulti-user datasets, with potential applications across various domainsrequiring user-informed modelling.</description><author>Kutay Bölat, Simon Tindemans</author><pubDate>Wed, 06 Nov 2024 14:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03936v1</guid></item><item><title>Interactions Across Blocks in Post-Training Quantization of Large Language Models</title><link>http://arxiv.org/abs/2411.03934v1</link><description>Post-training quantization is widely employed to reduce the computationaldemands of neural networks. Typically, individual substructures, such as layersor blocks of layers, are quantized with the objective of minimizingquantization errors in their pre-activations by fine-tuning the correspondingweights. Deriving this local objective from the global objective of minimizingtask loss involves two key simplifications: assuming substructures are mutuallyindependent and ignoring the knowledge of subsequent substructures as well asthe task loss. In this work, we assess the effects of these simplifications onweight-only quantization of large language models. We introduce two multi-blockfine-tuning strategies and compare them against the baseline of fine-tuningsingle transformer blocks. The first captures correlations of weights acrossblocks by jointly optimizing multiple quantized blocks. The second incorporatesknowledge of subsequent blocks by minimizing the error in downstreampre-activations rather than focusing solely on the quantized block. Ourfindings indicate that the effectiveness of these methods depends on thespecific network model, with no impact on some models but demonstratingsignificant benefits for others.</description><author>Khasmamad Shabanovi, Lukas Wiest, Vladimir Golkov, Daniel Cremers, Thomas Pfeil</author><pubDate>Wed, 06 Nov 2024 14:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03934v1</guid></item><item><title>Open Problem: Active Representation Learning</title><link>http://arxiv.org/abs/2406.03845v2</link><description>In this work, we introduce the concept of Active Representation Learning, anovel class of problems that intertwines exploration and representationlearning within partially observable environments. We extend ideas from ActiveSimultaneous Localization and Mapping (active SLAM), and translate them toscientific discovery problems, exemplified by adaptive microscopy. We explorethe need for a framework that derives exploration skills from representationsthat are in some sense actionable, aiming to enhance the efficiency andeffectiveness of data collection and model building in the natural sciences.</description><author>Nikola Milosevic, Gesine Müller, Jan Huisken, Nico Scherf</author><pubDate>Wed, 06 Nov 2024 14:11:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03845v2</guid></item><item><title>Root Cause Analysis of Outliers with Missing Structural Knowledge</title><link>http://arxiv.org/abs/2406.05014v2</link><description>Recent work conceptualized root cause analysis (RCA) of anomalies viaquantitative contribution analysis using causal counterfactuals in structuralcausal models (SCMs).The framework comes with three practical challenges: (1)it requires the causal directed acyclic graph (DAG), together with an SCM, (2)it is statistically ill-posed since it probes regression models in regions oflow probability density, (3) it relies on Shapley values which arecomputationally expensive to find. In this paper, we propose simplified, efficient methods of root causeanalysis when the task is to identify a unique root cause instead ofquantitative contribution analysis. Our proposed methods run in linear order ofSCM nodes and they require only the causal DAG without counterfactuals.Furthermore, for those use cases where the causal DAG is unknown, we justifythe heuristic of identifying root causes as the variables with the highestanomaly score. To this end, we prove that anomalies with small scores areunlikely to cause those with large scores and show upper bounds for thelikelihood of causal pathways with non-monotonic anomaly scores.</description><author>Nastaran Okati, Sergio Hernan Garrido Mejia, William Roy Orchard, Patrick Blöbaum, Dominik Janzing</author><pubDate>Wed, 06 Nov 2024 14:09:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05014v2</guid></item><item><title>Improved Regret of Linear Ensemble Sampling</title><link>http://arxiv.org/abs/2411.03932v1</link><description>In this work, we close the fundamental gap of theory and practice byproviding an improved regret bound for linear ensemble sampling. We prove thatwith an ensemble size logarithmic in $T$, linear ensemble sampling can achievea frequentist regret bound of $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$, matchingstate-of-the-art results for randomized linear bandit algorithms, where $d$ and$T$ are the dimension of the parameter and the time horizon respectively. Ourapproach introduces a general regret analysis framework for linear banditalgorithms. Additionally, we reveal a significant relationship between linearensemble sampling and Linear Perturbed-History Exploration (LinPHE), showingthat LinPHE is a special case of linear ensemble sampling when the ensemblesize equals $T$. This insight allows us to derive a new regret bound of$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ for LinPHE, independent of the number ofarms. Our contributions advance the theoretical foundation of ensemblesampling, bringing its regret bounds in line with the best known bounds forother randomized exploration algorithms.</description><author>Harin Lee, Min-hwan Oh</author><pubDate>Wed, 06 Nov 2024 14:09:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03932v1</guid></item><item><title>Act in Collusion: A Persistent Distributed Multi-Target Backdoor in Federated Learning</title><link>http://arxiv.org/abs/2411.03926v1</link><description>Federated learning, a novel paradigm designed to protect data privacy, isvulnerable to backdoor attacks due to its distributed nature. Current researchoften designs attacks based on a single attacker with a single backdoor,overlooking more realistic and complex threats in federated learning. Wepropose a more practical threat model for federated learning: the distributedmulti-target backdoor. In this model, multiple attackers control differentclients, embedding various triggers and targeting different classes,collaboratively implanting backdoors into the global model via centralaggregation. Empirical validation shows that existing methods struggle tomaintain the effectiveness of multiple backdoors in the global model. Our keyinsight is that similar backdoor triggers cause parameter conflicts andinjecting new backdoors disrupts gradient directions, significantly weakeningsome backdoors performance. To solve this, we propose a DistributedMulti-Target Backdoor Attack (DMBA), ensuring efficiency and persistence ofbackdoors from different malicious clients. To avoid parameter conflicts, wedesign a multi-channel dispersed frequency trigger strategy to maximize triggerdifferences. To mitigate gradient interference, we introduce backdoor replay inlocal training to neutralize conflicting gradients. Extensive validation showsthat 30 rounds after the attack, Attack Success Rates of three differentbackdoors from various clients remain above 93%. The code will be made publiclyavailable after the review period.</description><author>Tao Liu, Wu Yang, Chen Xu, Jiguang Lv, Huanran Wang, Yuhang Zhang, Shuchun Xu, Dapeng Man</author><pubDate>Wed, 06 Nov 2024 13:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03926v1</guid></item><item><title>Quantum Algorithm for Sparse Online Learning with Truncated Gradient Descent</title><link>http://arxiv.org/abs/2411.03925v1</link><description>Logistic regression, the Support Vector Machine (SVM), and least squares arewell-studied methods in the statistical and computer science community, withvarious practical applications. High-dimensional data arriving on a real-timebasis makes the design of online learning algorithms that produce sparsesolutions essential. The seminal work of\hyperlink{cite.langford2009sparse}{Langford, Li, and Zhang (2009)} developed amethod to obtain sparsity via truncated gradient descent, showing anear-optimal online regret bound. Based on this method, we develop a quantumsparse online learning algorithm for logistic regression, the SVM, and leastsquares. Given efficient quantum access to the inputs, we show that a quadraticspeedup in the time complexity with respect to the dimension of the problem isachievable, while maintaining a regret of $O(1/\sqrt{T})$, where $T$ is thenumber of iterations.</description><author>Debbie Lim, Yixian Qiu, Patrick Rebentrost, Qisheng Wang</author><pubDate>Wed, 06 Nov 2024 13:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03925v1</guid></item></channel></rss>