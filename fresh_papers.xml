<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 07 Aug 2023 14:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</title><link>http://arxiv.org/abs/2308.02490v1</link><description>We propose MM-Vet, an evaluation benchmark that examines large multimodalmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown variousintriguing abilities, such as solving math problems written on the blackboard,reasoning about events and celebrities in news images, and explaining visualjokes. Rapid model advancements pose challenges to evaluation benchmarkdevelopment. Problems include: (1) How to systematically structure and evaluatethe complicated multimodal tasks; (2) How to design evaluation metrics thatwork well across question and answer types; and (3) How to give model insightsbeyond a simple performance ranking. To this end, we present MM-Vet, designedbased on the insight that the intriguing ability to solve complicated tasks isoften achieved by a generalist model being able to integrate different corevision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities andexamines the 16 integrations of interest derived from the capabilitycombination. For evaluation metrics, we propose an LLM-based evaluator foropen-ended outputs. The evaluator enables the evaluation across differentquestion types and answer styles, resulting in a unified scoring metric. Weevaluate representative LMMs on MM-Vet, providing insights into thecapabilities of different LMM system paradigms and models. Code and data areavailable at https://github.com/yuweihao/MM-Vet.</description><author>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang</author><pubDate>Fri, 04 Aug 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02490v1</guid></item><item><title>Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP</title><link>http://arxiv.org/abs/2308.02487v1</link><description>Open-vocabulary segmentation is a challenging task requiring segmenting andrecognizing objects from an open set of categories. One way to address thischallenge is to leverage multi-modal models, such as CLIP, to provide image andtext features in a shared embedding space, which bridges the gap betweenclosed-vocabulary and open-vocabulary recognition. Hence, existing methodsoften adopt a two-stage framework to tackle the problem, where the inputs firstgo through a mask generator and then through the CLIP model along with thepredicted masks. This process involves extracting features from images multipletimes, which can be ineffective and inefficient. By contrast, we propose tobuild everything into a single-stage framework using a shared FrozenConvolutional CLIP backbone, which not only significantly simplifies thecurrent two-stage pipeline, but also remarkably yields a better accuracy-costtrade-off. The proposed FC-CLIP, benefits from the following observations: thefrozen CLIP backbone maintains the ability of open-vocabulary classificationand can also serve as a strong mask generator, and the convolutional CLIPgeneralizes well to a larger input resolution than the one used duringcontrastive image-text pretraining. When training on COCO panoptic data onlyand testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2mIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoUon ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,respectively. Additionally, the training and testing time of FC-CLIP is 7.5xand 6.6x significantly faster than the same prior art, while using 5.9x fewerparameters. FC-CLIP also sets a new state-of-the-art performance across variousopen-vocabulary semantic segmentation datasets. Code athttps://github.com/bytedance/fc-clip</description><author>Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, Liang-Chieh Chen</author><pubDate>Fri, 04 Aug 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02487v1</guid></item><item><title>Adapting the NICT-JLE Corpus for Disfluency Detection Models</title><link>http://arxiv.org/abs/2308.02482v1</link><description>The detection of disfluencies such as hesitations, repetitions and falsestarts commonly found in speech is a widely studied area of research. With astandardised process for evaluation using the Switchboard Corpus, modelperformance can be easily compared across approaches. This is not the case fordisfluency detection research on learner speech, however, where such datasetshave restricted access policies, making comparison and subsequent developmentof improved models more challenging. To address this issue, this paperdescribes the adaptation of the NICT-JLE corpus, containing approximately 300hours of English learners' oral proficiency tests, to a format that is suitablefor disfluency detection model training and evaluation. Points of differencebetween the NICT-JLE and Switchboard corpora are explored, followed by adetailed overview of adaptations to the tag set and meta-features of theNICT-JLE corpus. The result of this work provides a standardised train, heldoutand test set for use in future research on disfluency detection for learnerspeech.</description><author>Lucy Skidmore, Roger K. Moore</author><pubDate>Fri, 04 Aug 2023 18:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02482v1</guid></item><item><title>Inference-Based Quantum Sensing</title><link>http://arxiv.org/abs/2206.09919v2</link><description>In a standard Quantum Sensing (QS) task one aims at estimating an unknownparameter $\theta$, encoded into an $n$-qubit probe state, via measurements ofthe system. The success of this task hinges on the ability to correlate changesin the parameter to changes in the system response $\mathcal{R}(\theta)$ (i.e.,changes in the measurement outcomes). For simple cases the form of$\mathcal{R}(\theta)$ is known, but the same cannot be said for realisticscenarios, as no general closed-form expression exists. In this work we presentan inference-based scheme for QS. We show that, for a general class of unitaryfamilies of encoding, $\mathcal{R}(\theta)$ can be fully characterized by onlymeasuring the system response at $2n+1$ parameters. This allows us to infer thevalue of an unknown parameter given the measured response, as well as todetermine the sensitivity of the scheme, which characterizes its overallperformance. We show that inference error is, with high probability, smallerthan $\delta$, if one measures the system response with a number of shots thatscales only as $\Omega(\log^3(n)/\delta^2)$. Furthermore, the frameworkpresented can be broadly applied as it remains valid for arbitrary probe statesand measurement schemes, and, even holds in the presence of quantum noise. Wealso discuss how to extend our results beyond unitary families. Finally, toshowcase our method we implement it for a QS task on real quantum hardware, andin numerical simulations.</description><author>C. Huerta Alderete, Max Hunter Gordon, Frederic Sauvage, Akira Sone, Andrew T. Sornborger, Patrick J. Coles, M. Cerezo</author><pubDate>Fri, 04 Aug 2023 18:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.09919v2</guid></item><item><title>Grounded Image Text Matching with Mismatched Relation Reasoning</title><link>http://arxiv.org/abs/2308.01236v2</link><description>This paper introduces Grounded Image Text Matching with Mismatched Relation(GITM-MR), a novel visual-linguistic joint task that evaluates the relationunderstanding capabilities of transformer-based pre-trained models. GITM-MRrequires a model to first determine if an expression describes an image, thenlocalize referred objects or ground the mismatched parts of the text. Weprovide a benchmark for evaluating pre-trained models on this task, with afocus on the challenging settings of limited data and out-of-distributionsentence lengths. Our evaluation demonstrates that pre-trained models lack dataefficiency and length generalization ability. To address this, we propose theRelation-sensitive Correspondence Reasoning Network (RCRN), which incorporatesrelation-aware reasoning via bi-directional message propagation guided bylanguage structure. RCRN can be interpreted as a modular program and deliversstrong performance in both length generalization and data efficiency.</description><author>Yu Wu, Yana Wei, Haozhe Wang, Yongfei Liu, Sibei Yang, Xuming He</author><pubDate>Fri, 04 Aug 2023 18:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01236v2</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v3</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation. The github repository can be foundhere https://github.com/IntelLabs/baa-ngp.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Fri, 04 Aug 2023 18:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v3</guid></item><item><title>Enhancing Clinical Support for Breast Cancer with Deep Learning Models using Synthetic Correlated Diffusion Imaging</title><link>http://arxiv.org/abs/2211.05308v2</link><description>Breast cancer is the second most common type of cancer in women in Canada andthe United States, representing over 25\% of all new female cancer cases. Assuch, there has been immense research and progress on improving screening andclinical support for breast cancer. In this paper, we investigate enhancingclinical support for breast cancer with deep learning models using a newlyintroduced magnetic resonance imaging (MRI) modality called syntheticcorrelated diffusion imaging (CDI$^s$). More specifically, we leverage avolumetric convolutional neural network to learn volumetric deep radiomicfeatures from a pre-treatment cohort and construct a predictor based on thelearnt features for grade and post-treatment response prediction. As the firststudy to learn CDI$^s$-centric radiomic sequences within a deep learningperspective for clinical decision support, we evaluated the proposed approachusing the ACRIN-6698 study against those learnt using gold-standard imagingmodalities. We find that the proposed approach can achieve better performancefor both grade and post-treatment response prediction and thus may be a usefultool to aid oncologists in improving recommendation of treatment of patients.Subsequently, the approach to leverage volumetric deep radiomic features forbreast cancer can be further extended to other applications of CDI$^s$ in thecancer domain to further improve clinical support.</description><author>Chi-en Amy Tai, Hayden Gunraj, Nedim Hodzic, Nic Flanagan, Ali Sabri, Alexander Wong</author><pubDate>Fri, 04 Aug 2023 18:13:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05308v2</guid></item><item><title>GraphCast: Learning skillful medium-range global weather forecasting</title><link>http://arxiv.org/abs/2212.12794v2</link><description>Global medium-range weather forecasting is critical to decision-making acrossmany social and economic domains. Traditional numerical weather prediction usesincreased compute resources to improve forecast accuracy, but cannot directlyuse historical weather data to improve the underlying model. We introduce amachine learning-based method called "GraphCast", which can be trained directlyfrom reanalysis data. It predicts hundreds of weather variables, over 10 daysat 0.25 degree resolution globally, in under one minute. We show that GraphCastsignificantly outperforms the most accurate operational deterministic systemson 90% of 1380 verification targets, and its forecasts support better severeevent prediction, including tropical cyclones, atmospheric rivers, and extremetemperatures. GraphCast is a key advance in accurate and efficient weatherforecasting, and helps realize the promise of machine learning for modelingcomplex dynamical systems.</description><author>Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, Peter Battaglia</author><pubDate>Fri, 04 Aug 2023 18:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12794v2</guid></item><item><title>BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks</title><link>http://arxiv.org/abs/2308.02465v1</link><description>Federated learning enables collaborative training of machine learning modelsby keeping the raw data of the involved workers private. One of its mainobjectives is to improve the models' privacy, security, and scalability.Vertical Federated Learning (VFL) offers an efficient cross-silo setting wherea few parties collaboratively train a model without sharing the same features.In such a scenario, classification labels are commonly considered sensitiveinformation held exclusively by one (active) party, while other (passive)parties use only their local information. Recent works have uncovered importantflaws of VFL, leading to possible label inference attacks under the assumptionthat the attacker has some, even limited, background knowledge on the relationbetween labels and data. In this work, we are the first (to the best of ourknowledge) to investigate label inference attacks on VFL using azero-background knowledge strategy. To concretely formulate our proposal, wefocus on Graph Neural Networks (GNNs) as a target model for the underlying VFL.In particular, we refer to node classification tasks, which are widely studied,and GNNs have shown promising results. Our proposed attack, BlindSage, providesimpressive results in the experiments, achieving nearly 100% accuracy in mostcases. Even when the attacker has no information about the used architecture orthe number of classes, the accuracy remained above 85% in most instances.Finally, we observe that well-known defenses cannot mitigate our attack withoutaffecting the model's performance on the main classification task.</description><author>Marco Arazzi, Mauro Conti, Stefanos Koffas, Marina Krcek, Antonino Nocera, Stjepan Picek, Jing Xu</author><pubDate>Fri, 04 Aug 2023 18:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02465v1</guid></item><item><title>Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing</title><link>http://arxiv.org/abs/2308.02464v1</link><description>Recurrent neural networks (RNNs) are known to be universal approximators ofdynamic systems under fairly mild and general assumptions, making them goodtools to process temporal information. However, RNNs usually suffer from theissues of vanishing and exploding gradients in the standard RNN training.Reservoir computing (RC), a special RNN where the recurrent weights arerandomized and left untrained, has been introduced to overcome these issues andhas demonstrated superior empirical performance in fields as diverse as naturallanguage processing and wireless communications especially in scenarios wheretraining samples are extremely limited. On the contrary, the theoreticalgrounding to support this observed performance has not been fully developed atthe same pace. In this work, we show that RNNs can provide universalapproximation of linear time-invariant (LTI) systems. Specifically, we showthat RC can universally approximate a general LTI system. We present a clearsignal processing interpretation of RC and utilize this understanding in theproblem of simulating a generic LTI system through RC. Under this setup, weanalytically characterize the optimal probability distribution function forgenerating the recurrent weights of the underlying RNN of the RC. We provideextensive numerical evaluations to validate the optimality of the derivedoptimum distribution of the recurrent weights of the RC for the LTI systemsimulation problem. Our work results in clear signal processing-based modelinterpretability of RC and provides theoretical explanation for the power ofrandomness in setting instead of training RC's recurrent weights. It furtherprovides a complete optimum analytical characterization for the untrainedrecurrent weights, marking an important step towards explainable machinelearning (XML) which is extremely important for applications where trainingsamples are limited.</description><author>Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu</author><pubDate>Fri, 04 Aug 2023 18:04:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02464v1</guid></item><item><title>Towards Generalist Foundation Model for Radiology</title><link>http://arxiv.org/abs/2308.02463v1</link><description>In this study, we aim to initiate the development of Radiology FoundationModel, termed as RadFM.We consider the construction of foundational models fromthe perspectives of data, model design, and evaluation thoroughly. Ourcontribution can be concluded as follows: (i), we construct a large-scaleMedical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.To the best of our knowledge, this is the first multi-modal dataset containing3D medical scans. (ii), We propose an architecture that enables visuallyconditioned generative pre-training, allowing for the integration of text inputinterleaved with 2D or 3D medical scans to generate response for diverseradiologic tasks. The model was initially pre-trained on MedMD and subsequentlydomain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,containing 3M radiologic visual-language pairs. (iii), we propose a newevaluation benchmark that comprises five tasks, aiming to comprehensivelyassess the capability of foundation models in handling practical clinicalproblems. Our experimental results confirm that RadFM significantly outperformsexisting multi-modal foundation models. The codes, data, and model checkpointwill all be made publicly available to promote further research and developmentin the field.</description><author>Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Fri, 04 Aug 2023 18:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02463v1</guid></item><item><title>Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning</title><link>http://arxiv.org/abs/2308.02462v1</link><description>One predominant challenge in additive manufacturing (AM) is to achievespecific material properties by manipulating manufacturing process parametersduring the runtime. Such manipulation tends to increase the computational loadimposed on existing simulation tools employed in AM. The goal of the presentwork is to construct a fast and accurate reduced-order model (ROM) for an AMmodel developed within the Multiphysics Object-Oriented Simulation Environment(MOOSE) framework, ultimately reducing the time/cost of AM control andoptimization processes. Our adoption of the operator learning (OL) approachenabled us to learn a family of differential equations produced by alteringprocess variables in the laser's Gaussian point heat source. More specifically,we used the Fourier neural operator (FNO) and deep operator network (DeepONet)to develop ROMs for time-dependent responses. Furthermore, we benchmarked theperformance of these OL methods against a conventional deep neural network(DNN)-based ROM. Ultimately, we found that OL methods offer comparableperformance and, in terms of accuracy and generalizability, even outperform DNNat predicting scalar model responses. The DNN-based ROM afforded the fastesttraining time. Furthermore, all the ROMs were faster than the original MOOSEmodel yet still provided accurate predictions. FNO had a smaller meanprediction error than DeepONet, with a larger variance for time-dependentresponses. Unlike DNN, both FNO and DeepONet were able to simulate time seriesdata without the need for dimensionality reduction techniques. The present workcan help facilitate the AM optimization process by enabling faster execution ofsimulation tools while still preserving evaluation accuracy.</description><author>Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</author><pubDate>Fri, 04 Aug 2023 18:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02462v1</guid></item><item><title>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration</title><link>http://arxiv.org/abs/2308.02459v1</link><description>Developing robot controllers capable of achieving dexterous nonprehensilemanipulation, such as pushing an object on a table, is challenging. Theunderactuated and hybrid-dynamics nature of the problem, further complicated bythe uncertainty resulting from the frictional interactions, requiressophisticated control behaviors. Reinforcement Learning (RL) is a powerfulframework for developing such robot controllers. However, previous RLliterature addressing the nonprehensile pushing task achieves low accuracy,non-smooth trajectories, and only simple motions, i.e. without rotation of themanipulated object. We conjecture that previously used unimodal explorationstrategies fail to capture the inherent hybrid-dynamics of the task, arisingfrom the different possible contact interaction modes between the robot and theobject, such as sticking, sliding, and separation. In this work, we propose amultimodal exploration approach through categorical distributions, whichenables us to train planar pushing RL policies for arbitrary starting andtarget object poses, i.e. positions and orientations, and with improvedaccuracy. We show that the learned policies are robust to external disturbancesand observation noise, and scale to tasks with multiple pushers. Furthermore,we validate the transferability of the learned policies, trained entirely insimulation, to a physical robot hardware using the KUKA iiwa robot arm. See oursupplemental video: https://youtu.be/vTdva1mgrk4.</description><author>Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar</author><pubDate>Fri, 04 Aug 2023 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02459v1</guid></item><item><title>A Survey on Temporal Knowledge Graph Completion: Taxonomy, Progress, and Prospects</title><link>http://arxiv.org/abs/2308.02457v1</link><description>Temporal characteristics are prominently evident in a substantial volume ofknowledge, which underscores the pivotal role of Temporal Knowledge Graphs(TKGs) in both academia and industry. However, TKGs often suffer fromincompleteness for three main reasons: the continuous emergence of newknowledge, the weakness of the algorithm for extracting structured informationfrom unstructured data, and the lack of information in the source dataset.Thus, the task of Temporal Knowledge Graph Completion (TKGC) has attractedincreasing attention, aiming to predict missing items based on the availableinformation. In this paper, we provide a comprehensive review of TKGC methodsand their details. Specifically, this paper mainly consists of threecomponents, namely, 1)Background, which covers the preliminaries of TKGCmethods, loss functions required for training, as well as the dataset andevaluation protocol; 2)Interpolation, that estimates and predicts the missingelements or set of elements through the relevant available information. Itfurther categorizes related TKGC methods based on how to process temporalinformation; 3)Extrapolation, which typically focuses on continuous TKGs andpredicts future events, and then classifies all extrapolation methods based onthe algorithms they utilize. We further pinpoint the challenges and discussfuture research directions of TKGC.</description><author>Jiapu Wang, Boyue Wang, Meikang Qiu, Shirui Pan, Bo Xiong, Heng Liu, Linhao Luo, Tengfei Liu, Yongli Hu, Baocai Yin, Wen Gao</author><pubDate>Fri, 04 Aug 2023 17:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02457v1</guid></item><item><title>Federated Deep Learning for Intrusion Detection in IoT Networks</title><link>http://arxiv.org/abs/2306.02715v3</link><description>The vast increase of Internet of Things (IoT) technologies and theever-evolving attack vectors have increased cyber-security risks dramatically.A common approach to implementing AI-based Intrusion Detection systems (IDSs)in distributed IoT systems is in a centralised manner. However, this approachmay violate data privacy and prohibit IDS scalability. Therefore, intrusiondetection solutions in IoT ecosystems need to move towards a decentraliseddirection. Federated Learning (FL) has attracted significant interest in recentyears due to its ability to perform collaborative learning while preservingdata confidentiality and locality. Nevertheless, most FL-based IDS for IoTsystems are designed under unrealistic data distribution conditions. To thatend, we design an experiment representative of the real world and evaluate theperformance of an FL-based IDS. For our experiments, we rely on TON-IoT, arealistic IoT network traffic dataset, associating each IP address with asingle FL client. Additionally, we explore pre-training and investigate variousaggregation methods to mitigate the impact of data heterogeneity. Lastly, webenchmark our approach against a centralised solution. The comparison showsthat the heterogeneous nature of the data has a considerable negative impact onthe model's performance when trained in a distributed manner. However, in thecase of a pre-trained initial global FL model, we demonstrate a performanceimprovement of over 20% (F1-score) compared to a randomly initiated globalmodel.</description><author>Othmane Belarbi, Theodoros Spyridopoulos, Eirini Anthi, Ioannis Mavromatis, Pietro Carnelli, Aftab Khan</author><pubDate>Fri, 04 Aug 2023 17:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02715v3</guid></item><item><title>Generative Modelling of Lévy Area for High Order SDE Simulation</title><link>http://arxiv.org/abs/2308.02452v1</link><description>It is well known that, when numerically simulating solutions to SDEs,achieving a strong convergence rate better than O(\sqrt{h}) (where h is thestep size) requires the use of certain iterated integrals of Brownian motion,commonly referred to as its "L\'{e}vy areas". However, these stochasticintegrals are difficult to simulate due to their non-Gaussian nature and for ad-dimensional Brownian motion with d &gt; 2, no fast almost-exact samplingalgorithm is known. In this paper, we propose L\'{e}vyGAN, a deep-learning-based model forgenerating approximate samples of L\'{e}vy area conditional on a Brownianincrement. Due to our "Bridge-flipping" operation, the output samples match alljoint and conditional odd moments exactly. Our generator employs a tailoredGNN-inspired architecture, which enforces the correct dependency structurebetween the output distribution and the conditioning variable. Furthermore, weincorporate a mathematically principled characteristic-function baseddiscriminator. Lastly, we introduce a novel training mechanism termed"Chen-training", which circumvents the need for expensive-to-generate trainingdata-sets. This new training procedure is underpinned by our two maintheoretical results. For 4-dimensional Brownian motion, we show that L\'{e}vyGAN exhibitsstate-of-the-art performance across several metrics which measure both thejoint and marginal distributions. We conclude with a numerical experiment onthe log-Heston model, a popular SDE in mathematical finance, demonstrating thathigh-quality synthetic L\'{e}vy area can lead to high order weak convergenceand variance reduction when using multilevel Monte Carlo (MLMC).</description><author>Andraž Jelinčič, Jiajie Tao, William F. Turner, Thomas Cass, James Foster, Hao Ni</author><pubDate>Fri, 04 Aug 2023 17:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02452v1</guid></item><item><title>Pruning a neural network using Bayesian inference</title><link>http://arxiv.org/abs/2308.02451v1</link><description>Neural network pruning is a highly effective technique aimed at reducing thecomputational and memory demands of large neural networks. In this researchpaper, we present a novel approach to pruning neural networks utilizingBayesian inference, which can seamlessly integrate into the training procedure.Our proposed method leverages the posterior probabilities of the neural networkprior to and following pruning, enabling the calculation of Bayes factors. Thecalculated Bayes factors guide the iterative pruning. Through comprehensiveevaluations conducted on multiple benchmarks, we demonstrate that our methodachieves desired levels of sparsity while maintaining competitive accuracy.</description><author>Sunil Mathew, Daniel B. Rowe</author><pubDate>Fri, 04 Aug 2023 17:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02451v1</guid></item><item><title>DiSProD: Differentiable Symbolic Propagation of Distributions for Planning</title><link>http://arxiv.org/abs/2302.01491v4</link><description>The paper introduces DiSProD, an online planner developed for environmentswith probabilistic transitions in continuous state and action spaces. DiSProDbuilds a symbolic graph that captures the distribution of future trajectories,conditioned on a given policy, using independence assumptions and approximatepropagation of distributions. The symbolic graph provides a differentiablerepresentation of the policy's value, enabling efficient gradient-basedoptimization for long-horizon search. The propagation of approximatedistributions can be seen as an aggregation of many trajectories, making itwell-suited for dealing with sparse rewards and stochastic environments. Anextensive experimental evaluation compares DiSProD to state-of-the-art plannersin discrete-time planning and real-time control of robotic systems. Theproposed method improves over existing planners in handling stochasticenvironments, sensitivity to search depth, sparsity of rewards, and largeaction spaces. Additional real-world experiments demonstrate that DiSProD cancontrol ground vehicles and surface vessels to successfully navigate aroundobstacles.</description><author>Palash Chatterjee, Ashutosh Chapagain, Weizhe Chen, Roni Khardon</author><pubDate>Fri, 04 Aug 2023 17:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01491v4</guid></item><item><title>From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence</title><link>http://arxiv.org/abs/2308.02448v1</link><description>In 2020, the U.S. Department of Defense officially disclosed a set of ethicalprinciples to guide the use of Artificial Intelligence (AI) technologies onfuture battlefields. Despite stark differences, there are core similaritiesbetween the military and medical service. Warriors on battlefields often facelife-altering circumstances that require quick decision-making. Medicalproviders experience similar challenges in a rapidly changing healthcareenvironment, such as in the emergency department or during surgery treating alife-threatening condition. Generative AI, an emerging technology designed toefficiently generate valuable information, holds great promise. As computingpower becomes more accessible and the abundance of health data, such aselectronic health records, electrocardiograms, and medical images, increases,it is inevitable that healthcare will be revolutionized by this technology.Recently, generative AI has captivated the research community, leading todebates about its application in healthcare, mainly due to concerns abouttransparency and related issues. Meanwhile, concerns about the potentialexacerbation of health disparities due to modeling biases have raised notableethical concerns regarding the use of this technology in healthcare. However,the ethical principles for generative AI in healthcare have been understudied,and decision-makers often fail to consider the significance of generative AI.In this paper, we propose GREAT PLEA ethical principles, encompassinggovernance, reliability, equity, accountability, traceability, privacy,lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim toproactively address the ethical dilemmas and challenges posed by theintegration of generative AI in healthcare.</description><author>David Oniani, Jordan Hilsman, Yifan Peng, COL, Ronald K. Poropatich, COL Jeremy C. Pamplin, LTC Gary L. Legault, Yanshan Wang</author><pubDate>Fri, 04 Aug 2023 17:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02448v1</guid></item><item><title>Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation</title><link>http://arxiv.org/abs/2303.17910v2</link><description>Benefiting from the sequence-level knowledge distillation, theNon-Autoregressive Transformer (NAT) achieves great success in neural machinetranslation tasks. However, existing knowledge distillation has side effects,such as propagating errors from the teacher to NAT students, which may limitfurther improvements of NAT models and are rarely discussed in existingresearch. In this paper, we introduce selective knowledge distillation byintroducing an NAT evaluator to select NAT-friendly targets that are of highquality and easy to learn. In addition, we introduce a simple yet effectiveprogressive distillation method to boost NAT performance. Experiment results onmultiple WMT language directions and several representative NAT models showthat our approach can realize a flexible trade-off between the quality andcomplexity of training data for NAT models, achieving strong performances.Further analysis shows that distilling only 5% of the raw translations can helpan NAT outperform its counterpart trained on raw data by about 2.4 BLEU.</description><author>Min Liu, Yu Bao, Chengqi Zhao, Shujian Huang</author><pubDate>Fri, 04 Aug 2023 17:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17910v2</guid></item><item><title>Beating Backdoor Attack at Its Own Game</title><link>http://arxiv.org/abs/2307.15539v3</link><description>Deep neural networks (DNNs) are vulnerable to backdoor attack, which does notaffect the network's performance on clean data but would manipulate the networkbehavior once a trigger pattern is added. Existing defense methods have greatlyreduced attack success rate, but their prediction accuracy on clean data stilllags behind a clean model by a large margin. Inspired by the stealthiness andeffectiveness of backdoor attack, we propose a simple but highly effectivedefense framework which injects non-adversarial backdoors targeting poisonedsamples. Following the general steps in backdoor attack, we detect a small setof suspected samples and then apply a poisoning strategy to them. Thenon-adversarial backdoor, once triggered, suppresses the attacker's backdoor onpoisoned data, but has limited influence on clean data. The defense can becarried out during data preprocessing, without any modification to the standardend-to-end training pipeline. We conduct extensive experiments on multiplebenchmarks with different architectures and representative attacks. Resultsdemonstrate that our method achieves state-of-the-art defense effectivenesswith by far the lowest performance drop on clean data. Considering thesurprising defense ability displayed by our framework, we call for moreattention to utilizing backdoor for backdoor defense. Code is available athttps://github.com/damianliumin/non-adversarial_backdoor.</description><author>Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue</author><pubDate>Fri, 04 Aug 2023 17:16:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15539v3</guid></item><item><title>Adaptive Preferential Attached kNN Graph With Distribution-Awareness</title><link>http://arxiv.org/abs/2308.02442v1</link><description>Graph-based kNN algorithms have garnered widespread popularity for machinelearning tasks, due to their simplicity and effectiveness. However, theconventional kNN graph's reliance on a fixed value of k can hinder itsperformance, especially in scenarios involving complex data distributions.Moreover, like other classification models, the presence of ambiguous samplesalong decision boundaries often presents a challenge, as they are more prone toincorrect classification. To address these issues, we propose the PreferentialAttached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN withdistribution-based graph construction. By incorporating distributioninformation, paNNG can significantly improve performance for ambiguous samplesby "pulling" them towards their original classes and hence enable enhancedoverall accuracy and generalization capability. Through rigorous evaluations ondiverse benchmark datasets, paNNG outperforms state-of-the-art algorithms,showcasing its adaptability and efficacy across various real-world scenarios.</description><author>Shaojie Min, Ji Liu</author><pubDate>Fri, 04 Aug 2023 17:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02442v1</guid></item><item><title>A State-Space Perspective on Modelling and Inference for Online Skill Rating</title><link>http://arxiv.org/abs/2308.02414v1</link><description>This paper offers a comprehensive review of the main methodologies used forskill rating in competitive sports. We advocate for a state-space modelperspective, wherein players' skills are represented as time-varying, and matchresults serve as the sole observed quantities. The state-space modelperspective facilitates the decoupling of modeling and inference, enabling amore focused approach highlighting model assumptions, while also fostering thedevelopment of general-purpose inference tools. We explore the essential stepsinvolved in constructing a state-space model for skill rating before turning toa discussion on the three stages of inference: filtering, smoothing andparameter estimation. Throughout, we examine the computational challenges ofscaling up to high-dimensional scenarios involving numerous players andmatches, highlighting approximations and reductions used to address thesechallenges effectively. We provide concise summaries of popular methodsdocumented in the literature, along with their inferential paradigms andintroduce new approaches to skill rating inference based on sequential MonteCarlo and finite state-spaces. We close with numerical experimentsdemonstrating a practical workflow on real data across different sports.</description><author>Samuel Duffield, Samuel Power, Lorenzo Rimella</author><pubDate>Fri, 04 Aug 2023 17:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02414v1</guid></item><item><title>Mitigating the Bias of Centered Objects in Common Datasets</title><link>http://arxiv.org/abs/2112.09195v3</link><description>Convolutional networks are considered shift invariant, but it wasdemonstrated that their response may vary according to the exact location ofthe objects. In this paper we will demonstrate that most commonly investigateddatasets have a bias, where objects are over-represented at the center of theimage during training. This bias and the boundary condition of these networkscan have a significant effect on the performance of these architectures andtheir accuracy drops significantly as an object approaches the boundary. Wewill also demonstrate how this effect can be mitigated with data augmentationtechniques.</description><author>Gergely Szabo, Andras Horvath</author><pubDate>Fri, 04 Aug 2023 17:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.09195v3</guid></item><item><title>Video Background Music Generation: Dataset, Method and Evaluation</title><link>http://arxiv.org/abs/2211.11248v2</link><description>Music is essential when editing videos, but selecting music manually isdifficult and time-consuming. Thus, we seek to automatically generatebackground music tracks given video input. This is a challenging task since itrequires music-video datasets, efficient architectures for video-to-musicgeneration, and reasonable metrics, none of which currently exist. To closethis gap, we introduce a complete recipe including dataset, benchmark model,and evaluation metric for video background music generation. We present SymMV,a video and symbolic music dataset with various musical annotations. To thebest of our knowledge, it is the first video-music dataset with rich musicalannotations. We also propose a benchmark video background music generationframework named V-MusProd, which utilizes music priors of chords, melody, andaccompaniment along with video-music relations of semantic, color, and motionfeatures. To address the lack of objective metrics for video-musiccorrespondence, we design a retrieval-based metric VMCP built upon a powerfulvideo-music representation learning model. Experiments show that with ourdataset, V-MusProd outperforms the state-of-the-art method in both musicquality and correspondence with videos. We believe our dataset, benchmarkmodel, and evaluation metric will boost the development of video backgroundmusic generation. Our dataset and code are available athttps://github.com/zhuole1025/SymMV.</description><author>Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, Si Liu</author><pubDate>Fri, 04 Aug 2023 16:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11248v2</guid></item><item><title>A Bi-variant Variational Model for Diffeomorphic Image Registration with Relaxed Jacobian Determinant Constraints</title><link>http://arxiv.org/abs/2308.02393v1</link><description>Diffeomorphic registration has become a powerful approach for seeking asmooth and invertible spatial transformation between two coordinate systemswhich have been measured via the template and reference images. While thepointwise volume-preserving constraint is effective for some problems, it istoo stringent for many other problems especially when the local deformationsare relatively large, because it may lead to a poor large-deformation forenforcing local matching.In this paper, we propose a novel bi-variantdiffeomorphic image registration model with the soft constraint of Jacobianequation, which allows local deformations to shrink and grow in a flexiblerange.The Jacobian determinant of the transformation is explicitly controlledby optimizing the relaxation function. To prevent deformation folding andenhance the smoothness of deformation, we not only impose a positivityconstraint in optimizing the relaxation function, but also employ a regularizerto ensure the smoothness of the relaxation function.Furthermore, the positivityconstraint ensures that is as close to one as possible, which helps to obtain avolume-preserving transformation on average.We further analyze the existence ofthe minimizer for the variational model and propose a penalty splitting methodwith a multilevel strategy to solve this model. Numerical experiments show thatthe proposed algorithm is convergent, and the positivity constraint can controlthe range of relative volume and not compromise registration accuracy.Moreover, the proposed model produces diffeomorphic maps for large deformation,and achieves better performance compared to the several existing registrationmodels.</description><author>Yanyan Li, Ke Chen, Chong Chen, Jianping Zhang</author><pubDate>Fri, 04 Aug 2023 16:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02393v1</guid></item><item><title>Mitigating Label Biases for In-context Learning</title><link>http://arxiv.org/abs/2305.19148v3</link><description>Various design settings for in-context learning (ICL), such as the choice andorder of the in-context examples, can bias a model toward a particularprediction without being reflective of an understanding of the task. While manystudies discuss these design choices, there have been few systematicinvestigations into categorizing them and mitigating their impact. In thiswork, we define a typology for three types of label biases in ICL for textclassification: vanilla-label bias, context-label bias, and domain-label bias(which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fallshort of addressing all three types of biases. Specifically, domain-label biasrestricts LLMs to random-level performance on many tasks regardless of thechoice of in-context examples. To mitigate the effect of these biases, wepropose a simple bias calibration method that estimates a language model'slabel bias using random in-domain words from the task corpus. After controllingfor this estimated bias when making predictions, our novel domain-contextcalibration significantly improves the ICL performance of GPT-J and GPT-3 on awide range of tasks. The gain is substantial on tasks with large domain-labelbias (up to 37% in Macro-F1). Furthermore, our results generalize to modelswith different scales, pretraining methods, and manually-designed taskinstructions, showing the prevalence of label biases in ICL.</description><author>Yu Fei, Yifan Hou, Zeming Chen, Antoine Bosselut</author><pubDate>Fri, 04 Aug 2023 16:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19148v3</guid></item><item><title>Learning Optimal Admission Control in Partially Observable Queueing Networks</title><link>http://arxiv.org/abs/2308.02391v1</link><description>We present an efficient reinforcement learning algorithm that learns theoptimal admission control policy in a partially observable queueing network.Specifically, only the arrival and departure times from the network areobservable, and optimality refers to the average holding/rejection cost ininfinite horizon. While reinforcement learning in Partially Observable Markov DecisionProcesses (POMDP) is prohibitively expensive in general, we show that ouralgorithm has a regret that only depends sub-linearly on the maximal number ofjobs in the network, $S$. In particular, in contrast with existing regretanalyses, our regret bound does not depend on the diameter of the underlyingMarkov Decision Process (MDP), which in most queueing systems is at leastexponential in $S$. The novelty of our approach is to leverage Norton's equivalent theorem forclosed product-form queueing networks and an efficient reinforcement learningalgorithm for MDPs with the structure of birth-and-death processes.</description><author>Jonatha Anselmi, Bruno Gaujal, Louis-Sébastien Rebuffi</author><pubDate>Fri, 04 Aug 2023 16:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02391v1</guid></item><item><title>Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics</title><link>http://arxiv.org/abs/2308.02382v1</link><description>Survival analysis is a fundamental tool in medicine, modeling the time untilan event of interest occurs in a population. However, in real-worldapplications, survival data are often incomplete, censored, distributed, andconfidential, especially in healthcare settings where privacy is critical. Thescarcity of data can severely limit the scalability of survival models todistributed applications that rely on large data pools. Federated learning is apromising technique that enables machine learning models to be trained onmultiple datasets without compromising user privacy, making it particularlywell-suited for addressing the challenges of survival data and large-scalesurvival applications. Despite significant developments in federated learningfor classification and regression, many directions remain unexplored in thecontext of survival analysis. In this work, we propose an extension of theFederated Survival Forest algorithm, called FedSurF++. This federated ensemblemethod constructs random survival forests in heterogeneous federations.Specifically, we investigate several new tree sampling methods from clientforests and compare the results with state-of-the-art survival models based onneural networks. The key advantage of FedSurF++ is its ability to achievecomparable performance to existing methods while requiring only a singlecommunication round to complete. The extensive empirical investigation resultsin a significant improvement from the algorithmic and privacy preservationperspectives, making the original FedSurF algorithm more efficient, robust, andprivate. We also present results on two real-world datasets demonstrating thesuccess of FedSurF++ in real-world healthcare studies. Our results underscorethe potential of FedSurF++ to improve the scalability and effectiveness ofsurvival analysis in distributed settings while preserving user privacy.</description><author>Alberto Archetti, Francesca Ieva, Matteo Matteucci</author><pubDate>Fri, 04 Aug 2023 16:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02382v1</guid></item><item><title>Classifying Causal Structures: Ascertaining when Classical Correlations are Constrained by Inequalities</title><link>http://arxiv.org/abs/2308.02380v1</link><description>The classical causal relations between a set of variables, some observed andsome latent, can induce both equality constraints (typically conditionalindependences) as well as inequality constraints (Instrumental and Bellinequalities being prototypical examples) on their compatible distribution overthe observed variables. Enumerating a causal structure's implied inequalityconstraints is generally far more difficult than enumerating its equalities.Furthermore, only inequality constraints ever admit violation by quantumcorrelations. For both those reasons, it is important to classify causalscenarios into those which impose inequality constraints versus those which donot. Here we develop methods for detecting such scenarios by appealing tod-separation, e-separation, and incompatible supports. Many (perhaps all?)scenarios with exclusively equality constraints can be detected via a conditionarticulated by Henson, Lal and Pusey (HLP). Considering all scenarios with upto 4 observed variables, which number in the thousands, we are able to resolveall but three causal scenarios, providing evidence that the HLP condition is,in fact, exhaustive.</description><author>Shashaank Khanna, Marina Maciel Ansanelli, Matthew F. Pusey, Elie Wolfe</author><pubDate>Fri, 04 Aug 2023 16:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02380v1</guid></item><item><title>A closer look at the training dynamics of knowledge distillation</title><link>http://arxiv.org/abs/2303.11098v3</link><description>In this paper we revisit the efficacy of knowledge distillation as a functionmatching and metric learning problem. In doing so we verify three importantdesign decisions, namely the normalisation, soft maximum function, andprojection layers as key ingredients. We theoretically show that the projectorimplicitly encodes information on past examples, enabling relational gradientsfor the student. We then show that the normalisation of representations istightly coupled with the training dynamics of this projector, which can have alarge impact on the students performance. Finally, we show that a simple softmaximum function can be used to address any significant capacity gap problems.Experimental results on various benchmark datasets demonstrate that using theseinsights can lead to superior or comparable performance to state-of-the-artknowledge distillation techniques, despite being much more computationallyefficient. In particular, we obtain these results across image classification(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficultdistillation objectives, such as training data efficient transformers, wherebywe attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.</description><author>Roy Miles, Krystian Mikolajczyk</author><pubDate>Fri, 04 Aug 2023 16:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11098v3</guid></item><item><title>Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training</title><link>http://arxiv.org/abs/2305.05237v2</link><description>New roads are being constructed all the time. However, the capabilities ofprevious deep forecasting models to generalize to new roads not seen in thetraining data (unseen roads) are rarely explored. In this paper, we introduce anovel setup called a spatio-temporal (ST) split to evaluate the models'capabilities to generalize to unseen roads. In this setup, the models aretrained on data from a sample of roads, but tested on roads not seen in thetraining data. Moreover, we also present a novel framework called SpatialContrastive Pre-Training (SCPT) where we introduce a spatial encoder module toextract latent features from unseen roads during inference time. This spatialencoder is pre-trained using contrastive learning. During inference, thespatial encoder only requires two days of traffic data on the new roads anddoes not require any re-training. We also show that the output from the spatialencoder can be used effectively to infer latent node embeddings on unseen roadsduring inference time. The SCPT framework also incorporates a new layer, namedthe spatially gated addition (SGA) layer, to effectively combine the latentfeatures from the output of the spatial encoder to existing backbones.Additionally, since there is limited data on the unseen roads, we argue that itis better to decouple traffic signals to trivial-to-capture periodic signalsand difficult-to-capture Markovian signals, and for the spatial encoder to onlylearn the Markovian signals. Finally, we empirically evaluated SCPT using theST split setup on four real-world datasets. The results showed that adding SCPTto a backbone consistently improves forecasting performance on unseen roads.More importantly, the improvements are greater when forecasting further intothe future. The codes are available on GitHub:\burl{https://github.com/cruiseresearchgroup/forecasting-on-new-roads}.</description><author>Arian Prabowo, Wei Shao, Hao Xue, Piotr Koniusz, Flora D. Salim</author><pubDate>Fri, 04 Aug 2023 16:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05237v2</guid></item><item><title>A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data</title><link>http://arxiv.org/abs/2308.02370v1</link><description>Traffic signals play an important role in transportation by enabling trafficflow management, and ensuring safety at intersections. In addition, knowing thetraffic signal phase and timing data can allow optimal vehicle routing for timeand energy efficiency, eco-driving, and the accurate simulation of signalizedroad networks. In this paper, we present a machine learning (ML) method forestimating traffic signal timing information from vehicle probe data. To theauthors best knowledge, very few works have presented ML techniques fordetermining traffic signal timing parameters from vehicle probe data. In thiswork, we develop an Extreme Gradient Boosting (XGBoost) model to estimatesignal cycle lengths and a neural network model to determine the correspondingred times per phase from probe data. The green times are then be derived fromthe cycle length and red times. Our results show an error of less than 0.56 secfor cycle length, and red times predictions within 7.2 sec error on average.</description><author>Juliette Ugirumurera, Joseph Severino, Erik A. Bensen, Qichao Wang, Jane Macfarlane</author><pubDate>Fri, 04 Aug 2023 16:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02370v1</guid></item><item><title>X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance</title><link>http://arxiv.org/abs/2303.15764v2</link><description>Text-driven 3D stylization is a complex and crucial task in the fields ofcomputer vision (CV) and computer graphics (CG), aimed at transforming a baremesh to fit a target text. Prior methods adopt text-independent multilayerperceptrons (MLPs) to predict the attributes of the target mesh with thesupervision of CLIP loss. However, such text-independent architecture lackstextual guidance during predicting attributes, thus leading to unsatisfactorystylization and slow convergence. To address these limitations, we presentX-Mesh, an innovative text-driven 3D stylization framework that incorporates anovel Text-guided Dynamic Attention Module (TDAM). The TDAM dynamicallyintegrates the guidance of the target text by utilizing text-relevant spatialand channel-wise attentions during vertex feature extraction, resulting in moreaccurate attribute prediction and faster convergence speed. Furthermore,existing works lack standard benchmarks and automated metrics for evaluation,often relying on subjective and non-reproducible user studies to assess thequality of stylized 3D assets. To overcome this limitation, we introduce a newstandard text-mesh benchmark, namely MIT-30, and two automated metrics, whichwill enable future research to achieve fair and objective comparisons. Ourextensive qualitative and quantitative experiments demonstrate that X-Meshoutperforms previous state-of-the-art methods.</description><author>Yiwei Ma, Xiaioqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, Rongrong Ji</author><pubDate>Fri, 04 Aug 2023 16:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15764v2</guid></item><item><title>Universal Defensive Underpainting Patch: Making Your Text Invisible to Optical Character Recognition</title><link>http://arxiv.org/abs/2308.02369v1</link><description>Optical Character Recognition (OCR) enables automatic text extraction fromscanned or digitized text images, but it also makes it easy to pirate valuableor sensitive text from these images. Previous methods to prevent OCR piracy bydistorting characters in text images are impractical in real-world scenarios,as pirates can capture arbitrary portions of the text images, rendering thedefenses ineffective. In this work, we propose a novel and effective defensemechanism termed the Universal Defensive Underpainting Patch (UDUP) thatmodifies the underpainting of text images instead of the characters. UDUP iscreated through an iterative optimization process to craft a small, fixed-sizedefensive patch that can generate non-overlapping underpainting for text imagesof any size. Experimental results show that UDUP effectively defends againstunauthorized OCR under the setting of any screenshot range or complex imagebackground. It is agnostic to the content, size, colors, and languages ofcharacters, and is robust to typical image operations such as scaling andcompressing. In addition, the transferability of UDUP is demonstrated byevading several off-the-shelf OCRs. The code is available athttps://github.com/QRICKDD/UDUP.</description><author>JiaCheng Deng, Li Dong, Jiahao Chen, Diqun Yan, Rangding Wang, Dengpan Ye, Lingchen Zhao, Jinyu Tian</author><pubDate>Fri, 04 Aug 2023 16:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02369v1</guid></item><item><title>Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise</title><link>http://arxiv.org/abs/2305.13498v2</link><description>This article aims to investigate the impact of noise on parameter fitting foran Ornstein-Uhlenbeck process, focusing on the effects of multiplicative andthermal noise on the accuracy of signal separation. To address these issues, wepropose algorithms and methods that can effectively distinguish between thermaland multiplicative noise and improve the precision of parameter estimation foroptimal data analysis. Specifically, we explore the impact of bothmultiplicative and thermal noise on the obfuscation of the actual signal andpropose methods to resolve them. Firstly, we present an algorithm that caneffectively separate thermal noise with comparable performance to HamiltonMonte Carlo (HMC) but with significantly improved speed. Subsequently, weanalyze multiplicative noise and demonstrate that HMC is insufficient forisolating thermal and multiplicative noise. However, we show that, withadditional knowledge of the ratio between thermal and multiplicative noise, wecan accurately distinguish between the two types of noise when provided with asufficiently large sampling rate or an amplitude of multiplicative noisesmaller than thermal noise. This finding results in a situation that initiallyseems counterintuitive. When multiplicative noise dominates the noise spectrum,we can successfully estimate the parameters for such systems after addingadditional white noise to shift the noise balance.</description><author>Simon Carter, Helmut H. Strey</author><pubDate>Fri, 04 Aug 2023 16:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13498v2</guid></item><item><title>Fine-grained Species Recognition with Privileged Pooling: Better Sample Efficiency Through Supervised Attention</title><link>http://arxiv.org/abs/2003.09168v4</link><description>We propose a scheme for supervised image classification that uses privilegedinformation, in the form of keypoint annotations for the training data, tolearn strong models from small and/or biased training sets. Our main motivationis the recognition of animal species for ecological applications such asbiodiversity modelling, which is challenging because of long-tailed speciesdistributions due to rare species, and strong dataset biases such as repetitivescene background in camera traps. To counteract these challenges, we propose avisual attention mechanism that is supervised via keypoint annotations thathighlight important object parts. This privileged information, implemented as anovel privileged pooling operation, is only required during training and helpsthe model to focus on regions that are discriminative. In experiments withthree different animal species datasets, we show that deep networks withprivileged pooling can use small training sets more efficiently and generalizebetter.</description><author>Andres C. Rodriguez, Stefano D'Aronco, Konrad Schindler, Jan Dirk Wegner</author><pubDate>Fri, 04 Aug 2023 15:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.09168v4</guid></item><item><title>Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation</title><link>http://arxiv.org/abs/2308.02363v1</link><description>Deep learning models usually require sufficient training data to achieve highaccuracy, but obtaining labeled data can be time-consuming and labor-intensive.Here we introduce a template-based training method to train a 3D U-Net modelfrom scratch using only one population-averaged brain MRI template and itsassociated segmentation label. The process incorporated visual perceptionaugmentation to enhance the model's robustness in handling diverse image inputsand mitigating overfitting. Leveraging this approach, we trained 3D U-Netmodels for mouse, rat, marmoset, rhesus, and human brain MRI to achievesegmentation tasks such as skull-stripping, brain segmentation, and tissueprobability mapping. This tool effectively addresses the limited availabilityof training data and holds significant potential for expanding deep learningapplications in image analysis, providing researchers with a unified solutionto train deep neural networks with only one image sample.</description><author>Fang-Cheng Yeh</author><pubDate>Fri, 04 Aug 2023 15:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02363v1</guid></item><item><title>Intensity-free Integral-based Learning of Marked Temporal Point Processes</title><link>http://arxiv.org/abs/2308.02360v1</link><description>In the marked temporal point processes (MTPP), a core problem is toparameterize the conditional joint PDF (probability distribution function)$p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history.The majority of existing studies predefine intensity functions. Their utilityis challenged by specifying the intensity function's proper form, which iscritical to balance expressiveness and processing efficiency. Recently, thereare studies moving away from predefining the intensity function -- one models$p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal pointprocesses (TPPs), which do not consider marks. This study aims to develophigh-fidelity $p^*(m,t)$ for discrete events where the event marks are eithercategorical or numeric in a multi-dimensional continuous space. We propose asolution framework IFIB (\underline{I}ntensity-\underline{f}ree\underline{I}ntegral-\underline{b}ased process) that models conditional jointPDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifiesthe process to compel the essential mathematical restrictions. We show thedesired properties of IFIB and the superior experimental results of IFIB onreal-world and synthetic datasets. The code is available at\url{https://github.com/StepinSilence/IFIB}.</description><author>Sishun Liu, Ke Deng, Jenny Zhang, Yongli Ren</author><pubDate>Fri, 04 Aug 2023 15:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02360v1</guid></item><item><title>Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text</title><link>http://arxiv.org/abs/2308.02357v1</link><description>The recent advances in large language models (LLM) and foundation models withemergent capabilities have been shown to improve the performance of many NLPtasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMscan be used for KG construction or completion while existing KGs can be usedfor different tasks such as making LLM outputs explainable or fact-checking inNeuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark toevaluate the capabilities of language models to generate KGs from naturallanguage text guided by an ontology. Given an input ontology and a set ofsentences, the task is to extract facts from the text while complying with thegiven ontology (concepts, relations, domain/range constraints) and beingfaithful to the input sentences. We provide two datasets (i) Wikidata-TekGenwith 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19ontologies and 4,860 sentences. We define seven evaluation metrics to measurefact extraction performance, ontology conformance, and hallucinations by LLMs.Furthermore, we provide results for two baseline models, Vicuna-13B andAlpaca-LoRA-13B using automatic prompt generation from test cases. The baselineresults show that there is room for improvement using both Semantic Web andNatural Language Processing techniques.</description><author>Nandana Mihindukulasooriya, Sanju Tiwari, Carlos F. Enguix, Kusum Lata</author><pubDate>Fri, 04 Aug 2023 15:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02357v1</guid></item><item><title>T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images</title><link>http://arxiv.org/abs/2308.02356v1</link><description>Remote sensing image change detection aims to identify the differencesbetween images acquired at different times in the same area. It is widely usedin land management, environmental monitoring, disaster assessment and otherfields. Currently, most change detection methods are based on Siamese networkstructure or early fusion structure. Siamese structure focuses on extractingobject features at different times but lacks attention to change information,which leads to false alarms and missed detections. Early fusion (EF) structurefocuses on extracting features after the fusion of images of different phasesbut ignores the significance of object features at different times fordetecting change details, making it difficult to accurately discern the edgesof changed objects. To address these issues and obtain more accurate results,we propose a novel network, Triplet UNet(T-UNet), based on a three-branchencoder, which is capable to simultaneously extract the object features and thechange features between the pre- and post-time-phase images through tripletencoder. To effectively interact and fuse the features extracted from the threebranches of triplet encoder, we propose a multi-branch spatial-spectralcross-attention module (MBSSCA). In the decoder stage, we introduce the channelattention mechanism (CAM) and spatial attention mechanism (SAM) to fully mineand integrate detailed textures information at the shallow layer and semanticlocalization information at the deep layer.</description><author>Huan Zhong, Chen Wu</author><pubDate>Fri, 04 Aug 2023 15:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02356v1</guid></item><item><title>Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes</title><link>http://arxiv.org/abs/2308.02353v1</link><description>We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE)methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leveragesinitial knowledge about the data distribution to search for validcounterfactuals while avoiding using information from potentially outdateddecision functions in subsequent time steps. Employing two graph autoencoders(GAEs), DyGRACE learns the representation of each class in a binaryclassification scenario. The GAEs minimise the reconstruction error between theoriginal graph and its learned representation during training. The methodinvolves (i) optimising a parametric density function (implemented as alogistic regression function) to identify counterfactuals by maximising thefactual autoencoder's reconstruction error, (ii) minimising the counterfactualautoencoder's error, and (iii) maximising the similarity between the factualand counterfactual graphs. This semi-supervised approach is independent of anunderlying black-box oracle. A logistic regression model is trained on a set ofgraph pairs to learn weights that aid in finding counterfactuals. At inference,for each unseen graph, the logistic regressor identifies the bestcounterfactual candidate using these learned weights, while the GAEs can beiteratively updated to represent the continual adaptation of the learned graphrepresentation over iterations. DyGRACE is quite effective and can act as adrift detector, identifying distributional drift based on differences inreconstruction errors between iterations. It avoids reliance on the oracle'spredictions in successive iterations, thereby increasing the efficiency ofcounterfactual discovery. DyGRACE, with its capacity for contrastive learningand drift detection, will offer new avenues for semi-supervised learning andexplanation generation.</description><author>Bardh Prenkaj, Mario Villaizan-Vallelado, Tobias Leemann, Gjergji Kasneci</author><pubDate>Fri, 04 Aug 2023 15:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02353v1</guid></item><item><title>A Parameter-efficient Multi-subject Model for Predicting fMRI Activity</title><link>http://arxiv.org/abs/2308.02351v1</link><description>This is the Algonauts 2023 submission report for team "BlobGPT". Our modelconsists of a multi-subject linear encoding head attached to a pretrained trunkmodel. The multi-subject head consists of three components: (1) a sharedmulti-layer feature projection, (2) shared plus subject-specific low-dimensionlinear transformations, and (3) a shared PCA fMRI embedding. In this report, weexplain these components in more detail and present some experimental results.Our code is available at https://github.com/cmi-dair/algonauts23.</description><author>Connor Lane, Gregory Kiar</author><pubDate>Fri, 04 Aug 2023 15:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02351v1</guid></item><item><title>RobustMQ: Benchmarking Robustness of Quantized Models</title><link>http://arxiv.org/abs/2308.02350v1</link><description>Quantization has emerged as an essential technique for deploying deep neuralnetworks (DNNs) on devices with limited resources. However, quantized modelsexhibit vulnerabilities when exposed to various noises in real-worldapplications. Despite the importance of evaluating the impact of quantizationon robustness, existing research on this topic is limited and often disregardsestablished principles of robustness evaluation, resulting in incomplete andinconclusive findings. To address this gap, we thoroughly evaluated therobustness of quantized models against various noises (adversarial attacks,natural corruptions, and systematic noises) on ImageNet. The comprehensiveevaluation results empirically provide valuable insights into the robustness ofquantized models in various scenarios, for example: (1) quantized modelsexhibit higher adversarial robustness than their floating-point counterparts,but are more vulnerable to natural corruptions and systematic noises; (2) ingeneral, increasing the quantization bit-width results in a decrease inadversarial robustness, an increase in natural robustness, and an increase insystematic robustness; (3) among corruption methods, \textit{impulse noise} and\textit{glass blur} are the most harmful to quantized models, while\textit{brightness} has the least impact; (4) among systematic noises, the\textit{nearest neighbor interpolation} has the highest impact, while bilinearinterpolation, cubic interpolation, and area interpolation are the three leastharmful. Our research contributes to advancing the robust quantization ofmodels and their deployment in real-world scenarios.</description><author>Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu</author><pubDate>Fri, 04 Aug 2023 15:37:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02350v1</guid></item><item><title>Data-driven modeling of Landau damping by physics-informed neural networks</title><link>http://arxiv.org/abs/2211.01021v3</link><description>Kinetic approaches are generally accurate in dealing with microscale plasmaphysics problems but are computationally expensive for large-scale ormultiscale systems. One of the long-standing problems in plasma physics is theintegration of kinetic physics into fluid models, which is often achievedthrough sophisticated analytical closure terms. In this paper, we successfullyconstruct a multi-moment fluid model with an implicit fluid closure included inthe neural network using machine learning. The multi-moment fluid model istrained with a small fraction of sparsely sampled data from kinetic simulationsof Landau damping, using the physics-informed neural network (PINN) and thegradient-enhanced physics-informed neural network (gPINN). The multi-momentfluid model constructed using either PINN or gPINN reproduces the timeevolution of the electric field energy, including its damping rate, and theplasma dynamics from the kinetic simulations. In addition, we introduce avariant of the gPINN architecture, namely, gPINN$p$ to capture the Landaudamping process. Instead of including the gradients of all the equationresiduals, gPINN$p$ only adds the gradient of the pressure equation residual asone additional constraint. Among the three approaches, the gPINN$p$-constructedmulti-moment fluid model offers the most accurate results. This work shedslight on the accurate and efficient modeling of large-scale systems, which canbe extended to complex multiscale laboratory, space, and astrophysical plasmaphysics problems.</description><author>Yilan Qin, Jiayu Ma, Mingle Jiang, Chuanfei Dong, Haiyang Fu, Liang Wang, Wenjie Cheng, Yaqiu Jin</author><pubDate>Fri, 04 Aug 2023 15:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01021v3</guid></item><item><title>Exploiting Multiple Abstractions in Episodic RL via Reward Shaping</title><link>http://arxiv.org/abs/2303.00516v2</link><description>One major limitation to the applicability of Reinforcement Learning (RL) tomany practical domains is the large number of samples required to learn anoptimal policy. To address this problem and improve learning efficiency, weconsider a linear hierarchy of abstraction layers of the Markov DecisionProcess (MDP) underlying the target domain. Each layer is an MDP representing acoarser model of the one immediately below in the hierarchy. In this work, wepropose a novel form of Reward Shaping where the solution obtained at theabstract level is used to offer rewards to the more concrete MDP, in such a waythat the abstract solution guides the learning in the more complex domain. Incontrast with other works in Hierarchical RL, our technique has fewrequirements in the design of the abstract models and it is also tolerant tomodeling errors, thus making the proposed approach practical. We formallyanalyze the relationship between the abstract models and the explorationheuristic induced in the lower-level domain. Moreover, we prove that the methodguarantees optimal convergence and we demonstrate its effectivenessexperimentally.</description><author>Roberto Cipollone, Giuseppe De Giacomo, Marco Favorito, Luca Iocchi, Fabio Patrizi</author><pubDate>Fri, 04 Aug 2023 15:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00516v2</guid></item><item><title>Stability and Generalization of Hypergraph Collaborative Networks</title><link>http://arxiv.org/abs/2308.02347v1</link><description>Graph neural networks have been shown to be very effective in utilizingpairwise relationships across samples. Recently, there have been severalsuccessful proposals to generalize graph neural networks to hypergraph neuralnetworks to exploit more complex relationships. In particular, the hypergraphcollaborative networks yield superior results compared to other hypergraphneural networks for various semi-supervised learning tasks. The collaborativenetwork can provide high quality vertex embeddings and hyperedge embeddingstogether by formulating them as a joint optimization problem and by using theirconsistency in reconstructing the given hypergraph. In this paper, we aim toestablish the algorithmic stability of the core layer of the collaborativenetwork and provide generalization guarantees. The analysis sheds light on thedesign of hypergraph filters in collaborative networks, for instance, how thedata and hypergraph filters should be scaled to achieve uniform stability ofthe learning process. Some experimental results on real-world datasets arepresented to illustrate the theory.</description><author>Michael Ng, Hanrui Wu, Andy Yip</author><pubDate>Fri, 04 Aug 2023 15:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02347v1</guid></item><item><title>Class Incremental Learning with Self-Supervised Pre-Training and Prototype Learning</title><link>http://arxiv.org/abs/2308.02346v1</link><description>Deep Neural Network (DNN) has achieved great success on datasets of closedclass set. However, new classes, like new categories of social media topics,are continuously added to the real world, making it necessary to incrementallylearn. This is hard for DNN because it tends to focus on fitting to new classeswhile ignoring old classes, a phenomenon known as catastrophic forgetting.State-of-the-art methods rely on knowledge distillation and data replaytechniques but still have limitations. In this work, we analyze the causes ofcatastrophic forgetting in class incremental learning, which owes to threefactors: representation drift, representation confusion, and classifierdistortion. Based on this view, we propose a two-stage learning framework witha fixed encoder and an incrementally updated prototype classifier. The encoderis trained with self-supervised learning to generate a feature space with highintrinsic dimensionality, thus improving its transferability and generality.The classifier incrementally learns new prototypes while retaining theprototypes of previously learned data, which is crucial in preserving thedecision boundary.Our method does not rely on preserved samples of old classes,is thus a non-exemplar based CIL method. Experiments on public datasets showthat our method can significantly outperform state-of-the-art exemplar-basedmethods when they reserved 5 examplers per class, under the incremental settingof 10 phases, by 18.24% on CIFAR-100 and 9.37% on ImageNet100.</description><author>Wenzhuo Liu, Xinjian Wu, Fei Zhu, Mingming Yu, Chuang Wang, Cheng-Lin Liu</author><pubDate>Fri, 04 Aug 2023 15:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02346v1</guid></item><item><title>Learning Networks from Gaussian Graphical Models and Gaussian Free Fields</title><link>http://arxiv.org/abs/2308.02344v1</link><description>We investigate the problem of estimating the structure of a weighted networkfrom repeated measurements of a Gaussian Graphical Model (GGM) on the network.In this vein, we consider GGMs whose covariance structures align with thegeometry of the weighted network on which they are based. Such GGMs have beenof longstanding interest in statistical physics, and are referred to as theGaussian Free Field (GFF). In recent years, they have attracted considerableinterest in the machine learning and theoretical computer science. In thiswork, we propose a novel estimator for the weighted network (equivalently, itsLaplacian) from repeated measurements of a GFF on the network, based on theFourier analytic properties of the Gaussian distribution. In this pursuit, ourapproach exploits complex-valued statistics constructed from observed data,that are of interest on their own right. We demonstrate the effectiveness ofour estimator with concrete recovery guarantees and bounds on the requiredsample complexity. In particular, we show that the proposed statistic achievesthe parametric rate of estimation for fixed network size. In the setting ofnetworks growing with sample size, our results show that for Erdos-Renyi randomgraphs $G(d,p)$ above the connectivity threshold, we demonstrate that networkrecovery takes place with high probability as soon as the sample size $n$satisfies $n \gg d^4 \log d \cdot p^{-2}$.</description><author>Subhro Ghosh, Soumendu Sundar Mukherjee, Hoang-Son Tran, Ujan Gangopadhyay</author><pubDate>Fri, 04 Aug 2023 15:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02344v1</guid></item><item><title>SVCNet: Scribble-based Video Colorization Network with Temporal Aggregation</title><link>http://arxiv.org/abs/2303.11591v2</link><description>In this paper, we propose a scribble-based video colorization network withtemporal aggregation called SVCNet. It can colorize monochrome videos based ondifferent user-given color scribbles. It addresses three common issues in thescribble-based video colorization area: colorization vividness, temporalconsistency, and color bleeding. To improve the colorization quality andstrengthen the temporal consistency, we adopt two sequential sub-networks inSVCNet for precise colorization and temporal smoothing, respectively. The firststage includes a pyramid feature encoder to incorporate color scribbles with agrayscale frame, and a semantic feature encoder to extract semantics. Thesecond stage finetunes the output from the first stage by aggregating theinformation of neighboring colorized frames (as short-range connections) andthe first colorized frame (as a long-range connection). To alleviate the colorbleeding artifacts, we learn video colorization and segmentationsimultaneously. Furthermore, we set the majority of operations on a fixed smallimage resolution and use a Super-resolution Module at the tail of SVCNet torecover original sizes. It allows the SVCNet to fit different image resolutionsat the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvobenchmarks. The experimental results demonstrate that SVCNet produces bothhigher-quality and more temporally consistent videos than other well-knownvideo colorization approaches. The codes and models can be found athttps://github.com/zhaoyuzhi/SVCNet.</description><author>Yuzhi Zhao, Lai-Man Po, Kangcheng Liu, Xuehui Wang, Wing-Yin Yu, Pengfei Xian, Yujia Zhang, Mengyang Liu</author><pubDate>Fri, 04 Aug 2023 15:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11591v2</guid></item><item><title>Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images</title><link>http://arxiv.org/abs/2308.02340v1</link><description>Purpose: In this work, we present a workflow to construct generic and robustgenerative image priors from magnitude-only images. The priors can then be usedfor regularization in reconstruction to improve image quality. Methods: Theworkflow begins with the preparation of training datasets from magnitude-onlyMR images. This dataset is then augmented with phase information and used totrain generative priors of complex images. Finally, trained priors areevaluated using both linear and nonlinear reconstruction for compressed sensingparallel imaging with various undersampling schemes. Results: The results ofour experiments demonstrate that priors trained on complex images outperformpriors trained only on magnitude images. Additionally, a prior trained on alarger dataset exhibits higher robustness. Finally, we show that the generativepriors are superior to L1 -wavelet regularization for compressed sensingparallel imaging with high undersampling. Conclusion: These findings stress theimportance of incorporating phase information and leveraging large datasets toraise the performance and reliability of the generative priors for MRIreconstruction. Phase augmentation makes it possible to use existing imagedatabases for training.</description><author>Guanxiong Luo, Xiaoqing Wang, Mortiz Blumenthal, Martin Schilling, Erik Hans Ulrich Rauf, Raviteja Kotikalapudi, Niels Focke, Martin Uecker</author><pubDate>Fri, 04 Aug 2023 15:12:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02340v1</guid></item><item><title>Improving Scene Graph Generation with Superpixel-Based Interaction Learning</title><link>http://arxiv.org/abs/2308.02339v1</link><description>Recent advances in Scene Graph Generation (SGG) typically model therelationships among entities utilizing box-level features from pre-defineddetectors. We argue that an overlooked problem in SGG is the coarse-grainedinteractions between boxes, which inadequately capture contextual semantics forrelationship modeling, practically limiting the development of the field. Inthis paper, we take the initiative to explore and propose a generic paradigmtermed Superpixel-based Interaction Learning (SIL) to remedy coarse-grainedinteractions at the box level. It allows us to model fine-grained interactionsat the superpixel level in SGG. Specifically, (i) we treat a scene as a set ofpoints and cluster them into superpixels representing sub-regions of the scene.(ii) We explore intra-entity and cross-entity interactions among thesuperpixels to enrich fine-grained interactions between entities at an earlierstage. Extensive experiments on two challenging benchmarks (Visual Genome andOpen Image V6) prove that our SIL enables fine-grained interaction at thesuperpixel level above previous box-level methods, and significantlyoutperforms previous state-of-the-art methods across all metrics. Moreencouragingly, the proposed method can be applied to boost the performance ofexisting box-level approaches in a plug-and-play fashion. In particular, SILbrings an average improvement of 2.0% mR (even up to 3.4%) of baselines for thePredCls task on Visual Genome, which facilitates its integration into anyexisting box-level method.</description><author>Jingyi Wang, Can Zhang, Jinfa Huang, Botao Ren, Zhidong Deng</author><pubDate>Fri, 04 Aug 2023 15:12:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02339v1</guid></item><item><title>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification</title><link>http://arxiv.org/abs/2308.02335v1</link><description>Graph classification is a crucial task in many real-world multimediaapplications, where graphs can represent various multimedia data types such asimages, videos, and social networks. Previous efforts have applied graph neuralnetworks (GNNs) in balanced situations where the class distribution isbalanced. However, real-world data typically exhibit long-tailed classdistributions, resulting in a bias towards the head classes when using GNNs andlimited generalization ability over the tail classes. Recent approaches mainlyfocus on re-balancing different classes during model training, which fails toexplicitly introduce new knowledge and sacrifices the performance of the headclasses. To address these drawbacks, we propose a novel framework calledRetrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust featureextractor and an unbiased classifier in a decoupled manner. In the featureextractor training stage, we develop a graph retrieval module to search forrelevant graphs that directly enrich the intra-class diversity for the tailclasses. Moreover, we innovatively optimize a category-centered supervisedcontrastive loss to obtain discriminative representations, which is moresuitable for long-tailed scenarios. In the classifier fine-tuning stage, webalance the classifier weights with two weight regularization techniques, i.e.,Max-norm and weight decay. Experiments on various popular benchmarks verify thesuperiority of the proposed method against state-of-the-art approaches.</description><author>Zhengyang Mao, Wei Ju, Yifang Qin, Xiao Luo, Ming Zhang</author><pubDate>Fri, 04 Aug 2023 15:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02335v1</guid></item><item><title>Dataflow Dialogue Generation</title><link>http://arxiv.org/abs/2308.02323v1</link><description>We demonstrate task-oriented dialogue generation within the dataflow dialogueparadigm. We show an example of agenda driven dialogue generation for theMultiWOZ domain, and an example of generation without an agenda for theSMCalFlow domain, where we show an improvement in the accuracy of thetranslation of user requests to dataflow expressions when the generateddialogues are used to augment the translation training dataset.</description><author>Joram Meron, Victor Guimarães</author><pubDate>Fri, 04 Aug 2023 14:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02323v1</guid></item><item><title>Do We Train on Test Data? The Impact of Near-Duplicates on License Plate Recognition</title><link>http://arxiv.org/abs/2304.04653v2</link><description>This work draws attention to the large fraction of near-duplicates in thetraining and test sets of datasets widely adopted in License Plate Recognition(LPR) research. These duplicates refer to images that, although different, showthe same license plate. Our experiments, conducted on the two most populardatasets in the field, show a substantial decrease in recognition rate when sixwell-known models are trained and tested under fair splits, that is, in theabsence of duplicates in the training and test sets. Moreover, in one of thedatasets, the ranking of models changed considerably when they were trained andtested under duplicate-free splits. These findings suggest that such duplicateshave significantly biased the evaluation and development of deep learning-basedmodels for LPR. The list of near-duplicates we have found and proposals forfair splits are publicly available for further research athttps://raysonlaroca.github.io/supp/lpr-train-on-test/</description><author>Rayson Laroca, Valter Estevam, Alceu S. Britto Jr., Rodrigo Minetto, David Menotti</author><pubDate>Fri, 04 Aug 2023 14:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04653v2</guid></item><item><title>A Controllable Co-Creative Agent for Game System Design</title><link>http://arxiv.org/abs/2308.02317v1</link><description>Many advancements have been made in procedural content generation for games,and with mixed-initiative co-creativity, have the potential for great benefitsto human designers. However, co-creative systems for game generation aretypically limited to specific genres, rules, or games, limiting the creativityof the designer. We seek to model games abstractly enough to apply to anygenre, focusing on designing game systems and mechanics, and create acontrollable, co-creative agent that can collaborate on these designs. Wepresent a model of games using state-machine-like components and resourceflows, a set of controllable metrics, a design evaluator simulatingplaythroughs with these metrics, and an evolutionary design balancer andgenerator. We find this system to be both able to express a wide range of gamesand able to be human-controllable for future co-creative applications.</description><author>Rohan Agarwal, Zhiyu Lin, Mark Riedl</author><pubDate>Fri, 04 Aug 2023 14:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02317v1</guid></item><item><title>A simple probabilistic neural networks for machine understanding</title><link>http://arxiv.org/abs/2210.13179v3</link><description>We discuss probabilistic neural networks for unsupervised learning with afixed internal representation as models for machine understanding. Hereunderstanding is intended as mapping data to an already existing representationwhich encodes an {\em a priori} organisation of the feature space. We derivethe internal representation by requiring that it satisfies the principles ofmaximal relevance and of maximal ignorance about how different features arecombined. We show that, when hidden units are binary variables, these twoprinciples identify a unique model -- the Hierarchical Feature Model (HFM) --which is fully solvable and provides a natural interpretation in terms offeatures. We argue that learning machines with this architecture enjoy a numberof interesting properties, like the continuity of the representation withrespect to changes in parameters and data, the possibility to control the levelof compression and the ability to support functions that go beyondgeneralisation. We explore the behaviour of the model with extensive numericalexperiments and argue that models where the internal representation is fixedreproduce a learning modality which is qualitatively different from that ofmore traditional models such as Restricted Boltzmann Machines.</description><author>Rongrong Xie, Matteo Marsili</author><pubDate>Fri, 04 Aug 2023 14:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.13179v3</guid></item><item><title>Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions</title><link>http://arxiv.org/abs/2308.02312v1</link><description>Q&amp;A platforms have been an integral part of the web-help-seeking behavior ofprogrammers over the past decade. However, with the recent introduction ofChatGPT, the paradigm of web-help-seeking behavior is experiencing a shift.Despite the popularity of ChatGPT, no comprehensive study has been conducted toevaluate the characteristics or usability of ChatGPT's answers to softwareengineering questions. To bridge the gap, we conducted the first in-depthanalysis of ChatGPT's answers to 517 Stack Overflow (SO) questions and examinedthe correctness, consistency, comprehensiveness, and conciseness of ChatGPT'sanswers. Furthermore, we conducted a large-scale linguistic analysis, and auser study to understand the characteristics of ChatGPT answers from linguisticand human aspects. Our analysis shows that 52\% of ChatGPT answers areincorrect and 77\% are verbose. Nonetheless, ChatGPT answers are stillpreferred 39.34\% of the time due to their comprehensiveness andwell-articulated language style. Our result implies the necessity of closeexamination and rectification of errors in ChatGPT, at the same time creatingawareness among its users of the risks associated with seemingly correctChatGPT answers.</description><author>Samia Kabir, David N. Udo-Imeh, Bonan Kou, Tianyi Zhang</author><pubDate>Fri, 04 Aug 2023 14:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02312v1</guid></item><item><title>Disentangling Multi-view Representations Beyond Inductive Bias</title><link>http://arxiv.org/abs/2308.01634v2</link><description>Multi-view (or -modality) representation learning aims to understand therelationships between different view representations. Existing methodsdisentangle multi-view representations into consistent and view-specificrepresentations by introducing strong inductive biases, which can limit theirgeneralization ability. In this paper, we propose a novel multi-viewrepresentation disentangling method that aims to go beyond inductive biases,ensuring both interpretability and generalizability of the resultingrepresentations. Our method is based on the observation that discoveringmulti-view consistency in advance can determine the disentangling informationboundary, leading to a decoupled learning objective. We also found that theconsistency can be easily extracted by maximizing the transformation invarianceand clustering consistency between views. These observations drive us topropose a two-stage framework. In the first stage, we obtain multi-viewconsistency by training a consistent encoder to produce semantically-consistentrepresentations across views as well as their corresponding pseudo-labels. Inthe second stage, we disentangle specificity from comprehensive representationsby minimizing the upper bound of mutual information between consistent andcomprehensive representations. Finally, we reconstruct the original data byconcatenating pseudo-labels and view-specific representations. Our experimentson four multi-view datasets demonstrate that our proposed method outperforms 12comparison methods in terms of clustering and classification performance. Thevisualization results also show that the extracted consistency and specificityare compact and interpretable. Our code can be found at\url{https://github.com/Guanzhou-Ke/DMRIB}.</description><author>Guanzhou Ke, Yang Yu, Guoqing Chao, Xiaoli Wang, Chenyang Xu, Shengfeng He</author><pubDate>Fri, 04 Aug 2023 14:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01634v2</guid></item><item><title>A Clustering-guided Contrastive Fusion for Multi-view Representation Learning</title><link>http://arxiv.org/abs/2212.13726v4</link><description>The past two decades have seen increasingly rapid advances in the field ofmulti-view representation learning due to it extracting useful information fromdiverse domains to facilitate the development of multi-view applications.However, the community faces two challenges: i) how to learn robustrepresentations from a large amount of unlabeled data to against noise orincomplete views setting, and ii) how to balance view consistency andcomplementary for various downstream tasks. To this end, we utilize a deepfusion network to fuse view-specific representations into the view-commonrepresentation, extracting high-level semantics for obtaining robustrepresentation. In addition, we employ a clustering task to guide the fusionnetwork to prevent it from leading to trivial solutions. For balancingconsistency and complementary, then, we design an asymmetrical contrastivestrategy that aligns the view-common representation and each view-specificrepresentation. These modules are incorporated into a unified method known asCLustering-guided cOntrastiVE fusioN (CLOVEN). We quantitatively andqualitatively evaluate the proposed method on five datasets, demonstrating thatCLOVEN outperforms 11 competitive multi-view learning methods in clustering andclassification. In the incomplete view scenario, our proposed method resistsnoise interference better than those of our competitors. Furthermore, thevisualization analysis shows that CLOVEN can preserve the intrinsic structureof view-specific representation while also improving the compactness ofview-commom representation. Our source code will be available soon athttps://github.com/guanzhou-ke/cloven.</description><author>Guanzhou Ke, Guoqing Chao, Xiaoli Wang, Chenyang Xu, Yongqi Zhu, Yang Yu</author><pubDate>Fri, 04 Aug 2023 14:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.13726v4</guid></item><item><title>Text-based Person Search without Parallel Image-Text Data</title><link>http://arxiv.org/abs/2305.12964v2</link><description>Text-based person search (TBPS) aims to retrieve the images of the targetperson from a large image gallery based on a given natural languagedescription. Existing methods are dominated by training models with parallelimage-text pairs, which are very costly to collect. In this paper, we make thefirst attempt to explore TBPS without parallel image-text data ($\mu$-TBPS), inwhich only non-parallel images and texts, or even image-only data, can beadopted. Towards this end, we propose a two-stage framework,generation-then-retrieval (GTR), to first generate the corresponding pseudotext for each image and then perform the retrieval in a supervised manner. Inthe generation stage, we propose a fine-grained image captioning strategy toobtain an enriched description of the person image, which firstly utilizes aset of instruction prompts to activate the off-the-shelf pretrainedvision-language model to capture and generate fine-grained person attributes,and then converts the extracted attributes into a textual description via thefinetuned large language model or the hand-crafted template. In the retrievalstage, considering the noise interference of the generated texts for trainingmodel, we develop a confidence score-based training scheme by enabling morereliable texts to contribute more during the training. Experimental results onmultiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show thatthe proposed GTR can achieve a promising performance without relying onparallel image-text data.</description><author>Yang Bai, Jingyao Wang, Min Cao, Chen Chen, Ziqiang Cao, Liqiang Nie, Min Zhang</author><pubDate>Fri, 04 Aug 2023 14:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12964v2</guid></item><item><title>Learning to Select the Relevant History Turns in Conversational Question Answering</title><link>http://arxiv.org/abs/2308.02294v1</link><description>The increasing demand for the web-based digital assistants has given a rapidrise in the interest of the Information Retrieval (IR) community towards thefield of conversational question answering (ConvQA). However, one of thecritical aspects of ConvQA is the effective selection of conversational historyturns to answer the question at hand. The dependency between relevant historyselection and correct answer prediction is an intriguing but under-exploredarea. The selected relevant context can better guide the system so as to whereexactly in the passage to look for an answer. Irrelevant context, on the otherhand, brings noise to the system, thereby resulting in a decline in the model'sperformance. In this paper, we propose a framework, DHS-ConvQA (Dynamic HistorySelection in Conversational Question Answering), that first generates thecontext and question entities for all the history turns, which are then prunedon the basis of similarity they share in common with the question at hand. Wealso propose an attention-based mechanism to re-rank the pruned terms based ontheir calculated weights of how useful they are in answering the question. Inthe end, we further aid the model by highlighting the terms in the re-rankedconversational history using a binary classification task and keeping theuseful terms (predicted as 1) and ignoring the irrelevant terms (predicted as0). We demonstrate the efficacy of our proposed framework with extensiveexperimental results on CANARD and QuAC -- the two popularly utilized datasetsin ConvQA. We demonstrate that selecting relevant turns works better thanrewriting the original question. We also investigate how adding the irrelevanthistory turns negatively impacts the model's performance and discuss theresearch challenges that demand more attention from the IR community.</description><author>Munazza Zaib, Wei Emma Zhang, Quan Z. Sheng, Subhash Sagar, Adnan Mahmood, Yang Zhang</author><pubDate>Fri, 04 Aug 2023 13:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02294v1</guid></item><item><title>A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation</title><link>http://arxiv.org/abs/2308.02293v1</link><description>While highly expressive parametric models including deep neural networks havean advantage to model complicated concepts, training such highly non-linearmodels is known to yield a high risk of notorious overfitting. To address thisissue, this study considers a $k$th order total variation ($k$-TV)regularization, which is defined as the squared integral of the $k$th orderderivative of the parametric models to be trained; penalizing the $k$-TV isexpected to yield a smoother function, which is expected to avoid overfitting.While the $k$-TV terms applied to general parametric models are computationallyintractable due to the integration, this study provides a stochasticoptimization algorithm, that can efficiently train general models with the$k$-TV regularization without conducting explicit numerical integration. Theproposed approach can be applied to the training of even deep neural networkswhose structure is arbitrary, as it can be implemented by only a simplestochastic gradient descent algorithm and automatic differentiation. Ournumerical experiments demonstrate that the neural networks trained with the$K$-TV terms are more ``resilient'' than those with the conventional parameterregularization. The proposed algorithm also can be extended to thephysics-informed training of neural networks (PINNs).</description><author>Akifumi Okuno</author><pubDate>Fri, 04 Aug 2023 13:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02293v1</guid></item><item><title>Mondrian Forest for Data Stream Classification Under Memory Constraints</title><link>http://arxiv.org/abs/2205.07871v3</link><description>Supervised learning algorithms generally assume the availability of enoughmemory to store their data model during the training and test phases. However,in the Internet of Things, this assumption is unrealistic when data comes inthe form of infinite data streams, or when learning algorithms are deployed ondevices with reduced amounts of memory. In this paper, we adapt the onlineMondrian forest classification algorithm to work with memory constraints ondata streams. In particular, we design five out-of-memory strategies to updateMondrian trees with new data points when the memory limit is reached. Moreover,we design trimming mechanisms to make Mondrian trees more robust to conceptdrifts under memory constraints. We evaluate our algorithms on a variety ofreal and simulated datasets, and we conclude with recommendations on their usein different situations: the Extend Node strategy appears as the bestout-of-memory strategy in all configurations, whereas different trimmingmechanisms should be adopted depending on whether a concept drift is expected.All our methods are implemented in the OrpailleCC open-source library and areready to be used on embedded systems and connected objects.</description><author>Martin Khannouz, Tristan Glatard</author><pubDate>Fri, 04 Aug 2023 13:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.07871v3</guid></item><item><title>Initial State Interventions for Deconfounded Imitation Learning</title><link>http://arxiv.org/abs/2307.15980v2</link><description>Imitation learning suffers from causal confusion. This phenomenon occurs whenlearned policies attend to features that do not causally influence the expertactions but are instead spuriously correlated. Causally confused agents producelow open-loop supervised loss but poor closed-loop performance upon deployment.We consider the problem of masking observed confounders in a disentangledrepresentation of the observation space. Our novel masking algorithm leveragesthe usual ability to intervene in the initial system state, avoiding anyrequirement involving expert querying, expert reward functions, or causal graphspecification. Under certain assumptions, we theoretically prove that thisalgorithm is conservative in the sense that it does not incorrectly maskobservations that causally influence the expert; furthermore, intervening onthe initial state serves to strictly reduce excess conservatism. The maskingalgorithm is applied to behavior cloning for two illustrative control systems:CartPole and Reacher.</description><author>Samuel Pfrommer, Yatong Bai, Hyunin Lee, Somayeh Sojoudi</author><pubDate>Fri, 04 Aug 2023 13:46:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15980v2</guid></item><item><title>Frustratingly Easy Model Generalization by Dummy Risk Minimization</title><link>http://arxiv.org/abs/2308.02287v1</link><description>Empirical risk minimization (ERM) is a fundamental machine learning paradigm.However, its generalization ability is limited in various tasks. In this paper,we devise Dummy Risk Minimization (DuRM), a frustratingly easy and generaltechnique to improve the generalization of ERM. DuRM is extremely simple toimplement: just enlarging the dimension of the output logits and thenoptimizing using standard gradient descent. Moreover, we validate the efficacyof DuRM on both theoretical and empirical analysis. Theoretically, we show thatDuRM derives greater variance of the gradient, which facilitates modelgeneralization by observing better flat local minima. Empirically, we conductevaluations of DuRM across different datasets, modalities, and networkarchitectures on diverse tasks, including conventional classification, semanticsegmentation, out-of-distribution generalization, adverserial training, andlong-tailed recognition. Results demonstrate that DuRM could consistentlyimprove the performance under all tasks with an almost free lunch manner.Furthermore, we show that DuRM is compatible with existing generalizationtechniques and we discuss possible limitations. We hope that DuRM could triggernew interest in the fundamental research on risk minimization.</description><author>Juncheng Wang, Jindong Wang, Xixu Hu, Shujun Wang, Xing Xie</author><pubDate>Fri, 04 Aug 2023 13:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02287v1</guid></item><item><title>Learning minimal representations of stochastic processes with variational autoencoders</title><link>http://arxiv.org/abs/2307.11608v2</link><description>Stochastic processes have found numerous applications in science, as they arebroadly used to model a variety of natural phenomena. Due to their intrinsicrandomness and uncertainty, they are however difficult to characterize. Here,we introduce an unsupervised machine learning approach to determine the minimalset of parameters required to effectively describe the dynamics of a stochasticprocess. Our method builds upon an extended $\beta$-variational autoencoderarchitecture. By means of simulated datasets corresponding to paradigmaticdiffusion models, we showcase its effectiveness in extracting the minimalrelevant parameters that accurately describe these dynamics. Furthermore, themethod enables the generation of new trajectories that faithfully replicate theexpected stochastic behavior. Overall, our approach enables for the autonomousdiscovery of unknown parameters describing stochastic processes, henceenhancing our comprehension of complex phenomena across various fields.</description><author>Gabriel Fernández-Fernández, Carlo Manzo, Maciej Lewenstein, Alexandre Dauphin, Gorka Muñoz-Gil</author><pubDate>Fri, 04 Aug 2023 13:40:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11608v2</guid></item><item><title>Diffusion-Augmented Depth Prediction with Sparse Annotations</title><link>http://arxiv.org/abs/2308.02283v1</link><description>Depth estimation aims to predict dense depth maps. In autonomous drivingscenes, sparsity of annotations makes the task challenging. Supervised modelsproduce concave objects due to insufficient structural information. Theyoverfit to valid pixels and fail to restore spatial structures. Self-supervisedmethods are proposed for the problem. Their robustness is limited by poseestimation, leading to erroneous results in natural scenes. In this paper, wepropose a supervised framework termed Diffusion-Augmented Depth Prediction(DADP). We leverage the structural characteristics of diffusion model toenforce depth structures of depth models in a plug-and-play manner. Anobject-guided integrality loss is also proposed to further enhance regionalstructure integrality by fetching objective information. We evaluate DADP onthree driving benchmarks and achieve significant improvements in depthstructures and robustness. Our work provides a new perspective on depthestimation with sparse annotations in autonomous driving scenes.</description><author>Jiaqi Li, Yiran Wang, Zihao Huang, Jinghong Zheng, Ke Xian, Zhiguo Cao, Jianming Zhang</author><pubDate>Fri, 04 Aug 2023 13:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02283v1</guid></item><item><title>DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization</title><link>http://arxiv.org/abs/2308.02282v1</link><description>Time series remains one of the most challenging modalities in machinelearning research. The out-of-distribution (OOD) detection and generalizationon time series tend to suffer due to its non-stationary property, i.e., thedistribution changes over time. The dynamic distributions inside time seriespose great challenges to existing algorithms to identify invariantdistributions since they mainly focus on the scenario where the domaininformation is given as prior knowledge. In this paper, we attempt to exploitsubdomains within a whole dataset to counteract issues induced bynon-stationary for generalized representation learning. We propose DIVERSIFY, ageneral framework, for OOD detection and generalization on dynamicdistributions of time series. DIVERSIFY takes an iterative process: it firstobtains the "worst-case" latent distribution scenario via adversarial training,then reduces the gap between these latent distributions. We implement DIVERSIFYvia combining existing OOD detection methods according to either extractedfeatures or outputs of models for detection while we also directly utilizeoutputs for classification. In addition, theoretical insights illustrate thatDIVERSIFY is theoretically supported. Extensive experiments are conducted onseven datasets with different OOD settings across gesture recognition, speechcommands recognition, wearable stress and affect detection, and sensor-basedhuman activity recognition. Qualitative and quantitative results demonstratethat DIVERSIFY learns more generalized features and significantly outperformsother baselines.</description><author>Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, Xiangyang Ji, Qiang Yang, Xing Xie</author><pubDate>Fri, 04 Aug 2023 13:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02282v1</guid></item><item><title>Eco-evolutionary Dynamics of Non-episodic Neuroevolution in Large Multi-agent Environments</title><link>http://arxiv.org/abs/2302.09334v3</link><description>Neuroevolution (NE) has recently proven a competitive alternative to learningby gradient descent in reinforcement learning tasks. However, the majority ofNE methods and associated simulation environments differ crucially frombiological evolution: the environment is reset to initial conditions at the endof each generation, whereas natural environments are continuously modified bytheir inhabitants; agents reproduce based on their ability to maximize rewardswithin a population, while biological organisms reproduce and die based oninternal physiological variables that depend on their resource consumption;simulation environments are primarily single-agent while the biological worldis inherently multi-agent and evolves alongside the population. In this work wepresent a method for continuously evolving adaptive agents without anyenvironment or population reset. The environment is a large grid world withcomplex spatiotemporal resource generation, containing many agents that areeach controlled by an evolvable recurrent neural network and locally reproducebased on their internal physiology. The entire system is implemented in JAX,allowing very fast simulation on a GPU. We show that NE can operate in anecologically-valid non-episodic multi-agent setting, finding sustainablecollective foraging strategies in the presence of a complex interplay betweenecological and evolutionary dynamics.</description><author>Gautier Hamon, Eleni Nisioti, Clément Moulin-Frier</author><pubDate>Fri, 04 Aug 2023 13:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09334v3</guid></item><item><title>Explainable Contextual Anomaly Detection using Quantile Regression Forests</title><link>http://arxiv.org/abs/2302.11239v3</link><description>Traditional anomaly detection methods aim to identify objects that deviatefrom most other objects by treating all features equally. In contrast,contextual anomaly detection methods aim to detect objects that deviate fromother objects within a context of similar objects by dividing the features intocontextual features and behavioral features. In this paper, we developconnections between dependency-based traditional anomaly detection methods andcontextual anomaly detection methods. Based on resulting insights, we propose anovel approach to inherently interpretable contextual anomaly detection thatuses Quantile Regression Forests to model dependencies between features.Extensive experiments on various synthetic and real-world datasets demonstratethat our method outperforms state-of-the-art anomaly detection methods inidentifying contextual anomalies in terms of accuracy and interpretability.</description><author>Zhong Li, Matthijs van Leeuwen</author><pubDate>Fri, 04 Aug 2023 12:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11239v3</guid></item><item><title>Generative Reasoning Integrated Label Noise Robust Deep Image Representation Learning</title><link>http://arxiv.org/abs/2212.01261v3</link><description>The development of deep learning based image representation learning (IRL)methods has attracted great attention for various image understanding problems.Most of these methods require the availability of a high quantity and qualityof annotated training images, which can be time-consuming and costly to gather.To reduce labeling costs, crowdsourced data, automatic labeling procedures orcitizen science projects can be considered. However, such approaches increasethe risk of including label noise in training data. It may result inoverfitting on noisy labels when discriminative reasoning is employed. Thisleads to sub-optimal learning procedures, and thus inaccurate characterizationof images. To address this, we introduce a generative reasoning integratedlabel noise robust deep representation learning (GRID) approach. Our approachaims to model the complementary characteristics of discriminative andgenerative reasoning for IRL under noisy labels. To this end, we firstintegrate generative reasoning into discriminative reasoning through asupervised variational autoencoder. This allows GRID to automatically detecttraining samples with noisy labels. Then, through our label noise robust hybridrepresentation learning strategy, GRID adjusts the whole learning procedure forIRL of these samples through generative reasoning and that of other samplesthrough discriminative reasoning. Our approach learns discriminative imagerepresentations while preventing interference of noisy labels independentlyfrom the IRL method being selected. Thus, unlike the existing methods, GRIDdoes not depend on the type of annotation, neural network architecture, lossfunction or learning task, and thus can be directly utilized for variousproblems. Experimental results show its effectiveness compared tostate-of-the-art methods. The code of GRID is publicly available athttps://github.com/gencersumbul/GRID.</description><author>Gencer Sumbul, Begüm Demir</author><pubDate>Fri, 04 Aug 2023 12:50:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01261v3</guid></item><item><title>Redundancy Aware Multi-Reference Based Gainwise Evaluation of Extractive Summarization</title><link>http://arxiv.org/abs/2308.02270v1</link><description>While very popular for evaluating extractive summarization task, the ROUGEmetric has long been criticized for its lack of semantic awareness and itsignorance about the ranking quality of the summarizer. Thanks to previousresearch that has addressed these issues by proposing a gain-based automatedmetric called Sem-nCG, which is both rank and semantic aware. However, Sem-nCGdoes not consider the amount of redundancy present in a model-generated summaryand currently does not support evaluation with multiple reference summaries.Unfortunately, addressing both these limitations simultaneously is not trivial.Therefore, in this paper, we propose a redundancy-aware Sem-nCG metric anddemonstrate how this new metric can be used to evaluate model summaries againstmultiple references. We also explore different ways of incorporating redundancyinto the original metric through extensive experiments. Experimental resultsdemonstrate that the new redundancy-aware metric exhibits a higher correlationwith human judgments than the original Sem-nCG metric for both single andmultiple reference scenarios.</description><author>Mousumi Akter, Shubhra Kanti Karmaker Santu</author><pubDate>Fri, 04 Aug 2023 12:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02270v1</guid></item><item><title>SURE-Val: Safe Urban Relevance Extension and Validation</title><link>http://arxiv.org/abs/2308.02266v1</link><description>To evaluate perception components of an automated driving system, it isnecessary to define the relevant objects. While the urban domain is popularamong perception datasets, relevance is insufficiently specified for thisdomain. Therefore, this work adopts an existing method to define relevance inthe highway domain and expands it to the urban domain. While differentconceptualizations and definitions of relevance are present in literature,there is a lack of methods to validate these definitions. Therefore, this workpresents a novel relevance validation method leveraging a motion predictioncomponent. The validation leverages the idea that removing irrelevant objectsshould not influence a prediction component which reflects human drivingbehavior. The influence on the prediction is quantified by considering thestatistical distribution of prediction performance across a large-scaledataset. The validation procedure is verified using criteria specificallydesigned to exclude relevant objects. The validation method is successfullyapplied to the relevance criteria from this work, thus supporting theirvalidity.</description><author>Kai Storms, Ken Mori, Steven Peters</author><pubDate>Fri, 04 Aug 2023 12:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02266v1</guid></item><item><title>Efficient Monaural Speech Enhancement using Spectrum Attention Fusion</title><link>http://arxiv.org/abs/2308.02263v1</link><description>Speech enhancement is a demanding task in automated speech processingpipelines, focusing on separating clean speech from noisy channels. Transformerbased models have recently bested RNN and CNN models in speech enhancement,however at the same time they are much more computationally expensive andrequire much more high quality training data, which is always hard to come by.In this paper, we present an improvement for speech enhancement models thatmaintains the expressiveness of self-attention while significantly reducingmodel complexity, which we have termed Spectrum Attention Fusion. We carefullyconstruct a convolutional module to replace several self-attention layers in aspeech Transformer, allowing the model to more efficiently fuse spectralfeatures. Our proposed model is able to achieve comparable or better resultsagainst SOTA models but with significantly smaller parameters (0.58M) on theVoice Bank + DEMAND dataset.</description><author>Jinyu Long, Jetic Gū, Binhao Bai, Zhibo Yang, Ping Wei, Junli Li</author><pubDate>Fri, 04 Aug 2023 12:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02263v1</guid></item><item><title>Adaptive Proximal Gradient Method for Convex Optimization</title><link>http://arxiv.org/abs/2308.02261v1</link><description>In this paper, we explore two fundamental first-order algorithms in convexoptimization, namely, gradient descent (GD) and proximal gradient method(ProxGD). Our focus is on making these algorithms entirely adaptive byleveraging local curvature information of smooth functions. We propose adaptiveversions of GD and ProxGD that are based on observed gradient differences and,thus, have no added computational costs. Moreover, we prove convergence of ourmethods assuming only local Lipschitzness of the gradient. In addition, theproposed versions allow for even larger stepsizes than those initiallysuggested in [MM20].</description><author>Yura Malitsky, Konstantin Mishchenko</author><pubDate>Fri, 04 Aug 2023 12:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02261v1</guid></item><item><title>Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song</title><link>http://arxiv.org/abs/2308.02249v1</link><description>In this paper, we introduce a computational analysis of the field recordingdataset of approximately 700 hours of Korean folk songs, which were recordedaround 1980-90s. Because most of the songs were sung by non-expert musicianswithout accompaniment, the dataset provides several challenges. To address thischallenge, we utilized self-supervised learning with convolutional neuralnetwork based on pitch contour, then analyzed how the musical concept of tori,a classification system defined by a specific scale, ornamental notes, and anidiomatic melodic contour, is captured by the model. The experimental resultshows that our approach can better capture the characteristics of tori comparedto traditional pitch histograms. Using our approaches, we have examined howmusical discussions proposed in existing academia manifest in the actual fieldrecordings of Korean folk songs.</description><author>Danbinaerin Han, Rafael Caro Repetto, Dasaem Jeong</author><pubDate>Fri, 04 Aug 2023 12:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02249v1</guid></item><item><title>Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings</title><link>http://arxiv.org/abs/2204.03251v2</link><description>Language resources such as wordnets remain indispensable tools for differentnatural language tasks and applications. However, for low-resource languagessuch as Filipino, existing wordnets are old and outdated, and producing newones may be slow and costly in terms of time and resources. In this paper, wepropose an automatic method for constructing a wordnet from scratch using onlyan unlabeled corpus and a sentence embeddings-based language model. Using this,we produce FilWordNet, a new wordnet that supplants and improves the outdatedFilipino WordNet. We evaluate our automatically-induced senses and synsets bymatching them with senses from the Princeton WordNet, as well as comparing thesynsets to the old Filipino WordNet. We empirically show that our method caninduce existing, as well as potentially new, senses and synsets automaticallywithout the need for human supervision.</description><author>Dan John Velasco, Axel Alba, Trisha Gail Pelagio, Bryce Anthony Ramirez, Jan Christian Blaise Cruz, Charibeth Cheng</author><pubDate>Fri, 04 Aug 2023 12:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.03251v2</guid></item><item><title>Turkish Native Language Identification</title><link>http://arxiv.org/abs/2307.14850v3</link><description>In this paper, we present the first application of Native LanguageIdentification (NLI) for the Turkish language. NLI involves predicting thewriter's first language by analysing their writing in different languages.While most NLI research has focused on English, our study extends its scope toTurkish. We used the recently constructed Turkish Learner Corpus and employed acombination of three syntactic features (CFG production rules, part-of-speechn-grams, and function words) with L2 texts to demonstrate their effectivenessin this task.</description><author>Ahmet Yavuz Uluslu, Gerold Schneider</author><pubDate>Fri, 04 Aug 2023 12:11:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14850v3</guid></item><item><title>On the Calibration of Uncertainty Estimation in LiDAR-based Semantic Segmentation</title><link>http://arxiv.org/abs/2308.02248v1</link><description>The confidence calibration of deep learning-based perception models plays acrucial role in their reliability. Especially in the context of autonomousdriving, downstream tasks like prediction and planning depend on accurateconfidence estimates. In point-wise multiclass classification tasks likesematic segmentation the model has to deal with heavy class imbalances. Due totheir underrepresentation, the confidence calibration of classes with smallerinstances is challenging but essential, not only for safety reasons. We proposea metric to measure the confidence calibration quality of a semanticsegmentation model with respect to individual classes. It is calculated bycomputing sparsification curves for each class based on the uncertaintyestimates. We use the classification calibration metric to evaluate uncertaintyestimation methods with respect to their confidence calibration ofunderrepresented classes. We furthermore suggest a double use for the method toautomatically find label problems to improve the quality of hand- orauto-annotated datasets.</description><author>Mariella Dreissig, Florian Piewak, Joschka Boedecker</author><pubDate>Fri, 04 Aug 2023 11:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02248v1</guid></item><item><title>CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patients With Suspected Ischemic Stroke</title><link>http://arxiv.org/abs/2303.08757v3</link><description>Precise and fast prediction methods for ischemic areas comprised of deadtissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)patients are of significant clinical interest. They play an essential role inimproving diagnosis and treatment planning. Computed Tomography (CT) scan isone of the primary modalities for early assessment in patients with suspectedAIS. CT Perfusion (CTP) is often used as a primary assessment to determinestroke location, severity, and volume of ischemic lesions. Current automaticsegmentation methods for CTP mostly use already processed 3D parametric mapsconventionally used for clinical interpretation by radiologists as input.Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+timeinput, where the spatial information over the volume is ignored. In addition,these methods are only interested in segmenting core regions, while predictingpenumbra can be essential for treatment planning. This paper investigatesdifferent methods to utilize the entire 4D CTP as input to fully exploit thespatio-temporal information, leading us to propose a novel 4D convolutionlayer. Our comprehensive experiments on a local dataset of 152 patients dividedinto three groups show that our proposed models generate more precise resultsthan other methods explored. Adopting the proposed 4D mJ-Net, a DiceCoefficient of 0.53 and 0.23 is achieved for segmenting penumbra and coreareas, respectively. The code is available onhttps://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.</description><author>Luca Tomasetti, Kjersti Engan, Liv Jorunn Høllesli, Kathinka Dæhli Kurz, Mahdieh Khanmohammadi</author><pubDate>Fri, 04 Aug 2023 11:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08757v3</guid></item><item><title>DTF-Net: Category-Level Pose Estimation and Shape Reconstruction via Deformable Template Field</title><link>http://arxiv.org/abs/2308.02239v1</link><description>Estimating 6D poses and reconstructing 3D shapes of objects in open-worldscenes from RGB-depth image pairs is challenging. Many existing methods rely onlearning geometric features that correspond to specific templates whiledisregarding shape variations and pose differences among objects in the samecategory. As a result, these methods underperform when handling unseen objectinstances in complex environments. In contrast, other approaches aim to achievecategory-level estimation and reconstruction by leveraging normalized geometricstructure priors, but the static prior-based reconstruction struggles withsubstantial intra-class variations. To solve these problems, we propose theDTF-Net, a novel framework for pose estimation and shape reconstruction basedon implicit neural fields of object categories. In DTF-Net, we design adeformable template field to represent the general category-wise shape latentfeatures and intra-category geometric deformation features. The fieldestablishes continuous shape correspondences, deforming the category templateinto arbitrary observed instances to accomplish shape reconstruction. Weintroduce a pose regression module that shares the deformation features andtemplate codes from the fields to estimate the accurate 6D pose of each objectin the scene. We integrate a multi-modal representation extraction module toextract object features and semantic masks, enabling end-to-end inference.Moreover, during training, we implement a shape-invariant training strategy anda viewpoint sampling method to further enhance the model's capability toextract object pose features. Extensive experiments on the REAL275 and CAMERA25datasets demonstrate the superiority of DTF-Net in both synthetic and realscenes. Furthermore, we show that DTF-Net effectively supports grasping taskswith a real robot arm.</description><author>Haowen Wang, Zhipeng Fan, Zhen Zhao, Zhengping Che, Zhiyuan Xu, Dong Liu, Feifei Feng, Yakun Huang, Xiuquan Qiao, Jian Tang</author><pubDate>Fri, 04 Aug 2023 11:35:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02239v1</guid></item><item><title>MSECNet: Accurate and Robust Normal Estimation for 3D Point Clouds by Multi-Scale Edge Conditioning</title><link>http://arxiv.org/abs/2308.02237v1</link><description>Estimating surface normals from 3D point clouds is critical for variousapplications, including surface reconstruction and rendering. While existingmethods for normal estimation perform well in regions where normals changeslowly, they tend to fail where normals vary rapidly. To address this issue, wepropose a novel approach called MSECNet, which improves estimation in normalvarying regions by treating normal variation modeling as an edge detectionproblem. MSECNet consists of a backbone network and a multi-scale edgeconditioning (MSEC) stream. The MSEC stream achieves robust edge detectionthrough multi-scale feature fusion and adaptive edge detection. The detectededges are then combined with the output of the backbone network using the edgeconditioning module to produce edge-aware representations. Extensiveexperiments show that MSECNet outperforms existing methods on both synthetic(PCPNet) and real-world (SceneNN) datasets while running significantly faster.We also conduct various analyses to investigate the contribution of eachcomponent in the MSEC stream. Finally, we demonstrate the effectiveness of ourapproach in surface reconstruction.</description><author>Haoyi Xiu, Xin Liu, Weimin Wang, Kyoung-Sook Kim, Masashi Matsuoka</author><pubDate>Fri, 04 Aug 2023 11:26:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02237v1</guid></item><item><title>FB-BEV: BEV Representation from Forward-Backward View Transformations</title><link>http://arxiv.org/abs/2308.02236v1</link><description>View Transformation Module (VTM), where transformations happen betweenmulti-view image features and Bird-Eye-View (BEV) representation, is a crucialstep in camera-based BEV perception systems. Currently, the two most prominentVTM paradigms are forward projection and backward projection. Forwardprojection, represented by Lift-Splat-Shoot, leads to sparsely projected BEVfeatures without post-processing. Backward projection, with BEVFormer being anexample, tends to generate false-positive BEV features from incorrectprojections due to the lack of utilization on depth. To address the abovelimitations, we propose a novel forward-backward view transformation module.Our approach compensates for the deficiencies in both existing methods,allowing them to enhance each other to obtain higher quality BEVrepresentations mutually. We instantiate the proposed module with FB-BEV, whichachieves a new state-of-the-art result of 62.4\% NDS on the nuScenes test set.The code will be released at \url{https://github.com/NVlabs/FB-BEV}.</description><author>Zhiqi Li, Zhiding Yu, Wenhai Wang, Anima Anandkumar, Tong Lu, Jose M. Alvarez</author><pubDate>Fri, 04 Aug 2023 11:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02236v1</guid></item><item><title>Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of GPT family LLMs' Question Answering Performance</title><link>http://arxiv.org/abs/2303.07992v2</link><description>ChatGPT is a powerful large language model (LLM) that covers knowledgeresources such as Wikipedia and supports natural language question answeringusing its own knowledge. Therefore, there is growing interest in exploringwhether ChatGPT can replace traditional knowledge-based question answering(KBQA) models. Although there have been some works analyzing the questionanswering performance of ChatGPT, there is still a lack of large-scale,comprehensive testing of various types of complex questions to analyze thelimitations of the model. In this paper, we present a framework that followsthe black-box testing specifications of CheckList proposed by Ribeiro et. al.We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complexquestion answering datasets, which include six English datasets and twomultilingual datasets. The total number of test cases is approximately 190,000.In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5to identify commonalities between the GPT family and other LLMs. The datasetand code are available athttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git</description><author>Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi</author><pubDate>Fri, 04 Aug 2023 11:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07992v2</guid></item><item><title>Sinhala-English Parallel Word Dictionary Dataset</title><link>http://arxiv.org/abs/2308.02234v1</link><description>Parallel datasets are vital for performing and evaluating any kind ofmultilingual task. However, in the cases where one of the considered languagepairs is a low-resource language, the existing top-down parallel data such ascorpora are lacking in both tally and quality due to the dearth of humanannotation. Therefore, for low-resource languages, it is more feasible to movein the bottom-up direction where finer granular pairs such as dictionarydatasets are developed first. They may then be used for mid-level tasks such assupervised multilingual word embedding alignment. These in turn can later guidehigher-level tasks in the order of aligning sentence or paragraph text corporaused for Machine Translation (MT). Even though more approachable thangenerating and aligning a massive corpus for a low-resource language, for thesame reason of apathy from larger research entities, even these finer granulardata sets are lacking for some low-resource languages. We have observed thatthere is no free and open dictionary data set for the low-resource language,Sinhala. Thus, in this work, we introduce three parallel English-Sinhala worddictionaries (En-Si-dict-large, En-Si-dict-filtered, En-Si-dict-FastText) whichhelp in multilingual Natural Language Processing (NLP) tasks related to Englishand Sinhala languages. In this paper, we explain the dataset creation pipelineas well as the experimental results of the tests we have carried out to verifythe quality of the data sets. The data sets and the related scripts areavailable at https://github.com/kasunw22/sinhala-para-dict.</description><author>Kasun Wickramasinghe, Nisansa de Silva</author><pubDate>Fri, 04 Aug 2023 11:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02234v1</guid></item><item><title>Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain</title><link>http://arxiv.org/abs/2308.02233v1</link><description>We present a novel ML framework for modeling the wavelength-dependent gain ofmultiple EDFAs, based on semi-supervised, self-normalizing neural networks,enabling one-shot transfer learning. Our experiments on 22 EDFAs in OpenIreland and COSMOS testbeds show high-accuracy transfer-learning even whenoperated across different amplifier types.</description><author>Agastya Raj, Zehao Wang, Frank Slyne, Tingjun Chen, Dan Kilper, Marco Ruffini</author><pubDate>Fri, 04 Aug 2023 11:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02233v1</guid></item><item><title>Should we trust web-scraped data?</title><link>http://arxiv.org/abs/2308.02231v1</link><description>The increasing adoption of econometric and machine-learning approaches byempirical researchers has led to a widespread use of one data collectionmethod: web scraping. Web scraping refers to the use of automated computerprograms to access websites and download their content. The key argument ofthis paper is that na\"ive web scraping procedures can lead to sampling bias inthe collected data. This article describes three sources of sampling bias inweb-scraped data. More specifically, sampling bias emerges from web contentbeing volatile (i.e., being subject to change), personalized (i.e., presentedin response to request characteristics), and unindexed (i.e., abundance of apopulation register). In a series of examples, I illustrate the prevalence andmagnitude of sampling bias. To support researchers and reviewers, this paperprovides recommendations on anticipating, detecting, and overcoming samplingbias in web-scraped data.</description><author>Jens Foerderer</author><pubDate>Fri, 04 Aug 2023 11:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02231v1</guid></item><item><title>RT-K-Net: Revisiting K-Net for Real-Time Panoptic Segmentation</title><link>http://arxiv.org/abs/2305.01255v2</link><description>Panoptic segmentation is one of the most challenging scene parsing tasks,combining the tasks of semantic segmentation and instance segmentation. Whilemuch progress has been made, few works focus on the real-time application ofpanoptic segmentation methods. In this paper, we revisit the recentlyintroduced K-Net architecture. We propose vital changes to the architecture,training, and inference procedure, which massively decrease latency and improveperformance. Our resulting RT-K-Net sets a new state-of-the-art performance forreal-time panoptic segmentation methods on the Cityscapes dataset and showspromising results on the challenging Mapillary Vistas dataset. On Cityscapes,RT-K-Net reaches 60.2 % PQ with an average inference time of 32 ms for fullresolution 1024x2048 pixel images on a single Titan RTX GPU. On MapillaryVistas, RT-K-Net reaches 33.2 % PQ with an average inference time of 69 ms.Source code is available at https://github.com/markusschoen/RT-K-Net.</description><author>Markus Schön, Michael Buchholz, Klaus Dietmayer</author><pubDate>Fri, 04 Aug 2023 10:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01255v2</guid></item><item><title>Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics</title><link>http://arxiv.org/abs/2212.00679v5</link><description>Automated synthesis of provably correct controllers for cyber-physicalsystems is crucial for deployment in safety-critical scenarios. However, hybridfeatures and stochastic or unknown behaviours make this problem challenging. Wepropose a method for synthesising controllers for Markov jump linear systems(MJLSs), a class of discrete-time models for cyber-physical systems, so thatthey certifiably satisfy probabilistic computation tree logic (PCTL) formulae.An MJLS consists of a finite set of stochastic linear dynamics and discretejumps between these dynamics that are governed by a Markov decision process(MDP). We consider the cases where the transition probabilities of this MDP areeither known up to an interval or completely unknown. Our approach is based ona finite-state abstraction that captures both the discrete (mode-jumping) andcontinuous (stochastic linear) behaviour of the MJLS. We formalise thisabstraction as an interval MDP (iMDP) for which we compute intervals oftransition probabilities using sampling techniques from the so-called 'scenarioapproach', resulting in a probabilistically sound approximation. We apply ourmethod to multiple realistic benchmark problems, in particular, a temperaturecontrol and an aerial vehicle delivery problem.</description><author>Luke Rickard, Thom Badings, Licio Romao, Alessandro Abate</author><pubDate>Fri, 04 Aug 2023 10:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00679v5</guid></item><item><title>Learned harmonic mean estimation of the marginal likelihood with normalizing flows</title><link>http://arxiv.org/abs/2307.00048v2</link><description>Computing the marginal likelihood (also called the Bayesian model evidence)is an important task in Bayesian model selection, providing a principledquantitative way to compare models. The learned harmonic mean estimator solvesthe exploding variance problem of the original harmonic mean estimation of themarginal likelihood. The learned harmonic mean estimator learns an importancesampling target distribution that approximates the optimal distribution. Whilethe approximation need not be highly accurate, it is critical that theprobability mass of the learned distribution is contained within the posteriorin order to avoid the exploding variance problem. In previous work a bespokeoptimization problem is introduced when training models in order to ensure thisproperty is satisfied. In the current article we introduce the use ofnormalizing flows to represent the importance sampling target distribution. Aflow-based model is trained on samples from the posterior by maximum likelihoodestimation. Then, the probability density of the flow is concentrated bylowering the variance of the base distribution, i.e. by lowering its"temperature", ensuring its probability mass is contained within the posterior.This approach avoids the need for a bespoke optimisation problem and carefulfine tuning of parameters, resulting in a more robust method. Moreover, the useof normalizing flows has the potential to scale to high dimensional settings.We present preliminary experiments demonstrating the effectiveness of the useof flows for the learned harmonic mean estimator. The harmonic codeimplementing the learned harmonic mean, which is publicly available, has beenupdated to now support normalizing flows.</description><author>Alicja Polanska, Matthew A. Price, Alessio Spurio Mancini, Jason D. McEwen</author><pubDate>Fri, 04 Aug 2023 10:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00048v2</guid></item><item><title>SSIVD-Net: A Novel Salient Super Image Classification &amp; Detection Technique for Weaponized Violence</title><link>http://arxiv.org/abs/2207.12850v7</link><description>Detection of violence and weaponized violence in closed-circuit television(CCTV) footage requires a comprehensive approach. In this work, we introducethe \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specificallydesigned to facilitate the learning of weapon distribution in surveillancevideos. To tackle the complexities of analyzing 3D surveillance video forviolence recognition tasks, we propose a novel technique called,\emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for\textbf{V}iolence \textbf{D}etection). Our method reduces 3D video datacomplexity, dimensionality, and information loss while improving inference,performance, and explainability through the use of Salient-Super-Imagerepresentations. Considering the scalability and sustainability requirements offuturistic smart cities, the authors introduce the \emph{Salient-Classifier}, anovel architecture combining a kernelized approach with a residual learningstrategy. We evaluate variations of SSIVD-Net and Salient Classifier on ourSCVD dataset and benchmark against state-of-the-art (SOTA) models commonlyemployed in violence detection. Our approach exhibits significant improvementsin detecting both weaponized and non-weaponized violence instances. Byadvancing the SOTA in violence detection, our work offers a practical andscalable solution suitable for real-world applications. The proposedmethodology not only addresses the challenges of violence detection in CCTVfootage but also contributes to the understanding of weapon distribution insmart surveillance. Ultimately, our research findings should enable smarter andmore secure cities, as well as enhance public safety measures.</description><author>Toluwani Aremu, Li Zhiyuan, Reem Alameeri, Mustaqeem Khan, Abdulmotaleb El Saddik</author><pubDate>Fri, 04 Aug 2023 10:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.12850v7</guid></item><item><title>Painterly Image Harmonization using Diffusion Model</title><link>http://arxiv.org/abs/2308.02228v1</link><description>Painterly image harmonization aims to insert photographic objects intopaintings and obtain artistically coherent composite images. Previous methodsfor this task mainly rely on inference optimization or generative adversarialnetwork, but they are either very time-consuming or struggling at fine controlof the foreground objects (e.g., texture and content details). To address theseissues, we propose a novel Painterly Harmonization stable Diffusion model(PHDiffusion), which includes a lightweight adaptive encoder and a Dual EncoderFusion (DEF) module. Specifically, the adaptive encoder and the DEF modulefirst stylize foreground features within each encoder. Then, the stylizedforeground features from both encoders are combined to guide the harmonizationprocess. During training, besides the noise loss in diffusion model, weadditionally employ content loss and two style losses, i.e., AdaIN style lossand contrastive style loss, aiming to balance the trade-off between stylemigration and content preservation. Compared with the state-of-the-art modelsfrom related fields, our PHDiffusion can stylize the foreground moresufficiently and simultaneously retain finer content. Our code and model areavailable at https://github.com/bcmi/PHDiffusion-Painterly-Image-Harmonization.</description><author>Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, Liqing Zhang</author><pubDate>Fri, 04 Aug 2023 10:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02228v1</guid></item><item><title>A multi-functional simulation platform for on-demand ride service operations</title><link>http://arxiv.org/abs/2303.12336v2</link><description>On-demand ride services or ride-sourcing services have been experiencing fastdevelopment in the past decade. Various mathematical models and optimizationalgorithms have been developed to help ride-sourcing platforms designoperational strategies with higher efficiency. However, due to cost andreliability issues (implementing an immature algorithm for real operations mayresult in system turbulence), it is commonly infeasible to validate thesemodels and train/test these optimization algorithms within real-world ridesourcing platforms. Acting as a useful test bed, a simulation platform forride-sourcing systems will be very important to conduct algorithmtraining/testing or model validation through trails and errors. While previousstudies have established a variety of simulators for their own tasks, it lacksa fair and public platform for comparing the models or algorithms proposed bydifferent researchers. In addition, the existing simulators still face manychallenges, ranging from their closeness to real environments of ride-sourcingsystems, to the completeness of different tasks they can implement. To addressthe challenges, we propose a novel multi-functional and open-sourced simulationplatform for ride-sourcing systems, which can simulate the behaviors andmovements of various agents on a real transportation network. It provides a fewaccessible portals for users to train and test various optimization algorithms,especially reinforcement learning algorithms, for a variety of tasks, includingon-demand matching, idle vehicle repositioning, and dynamic pricing. Inaddition, it can be used to test how well the theoretical models approximatethe simulated outcomes. Evaluated on real-world data based experiments, thesimulator is demonstrated to be an efficient and effective test bed for varioustasks related to on-demand ride service operations.</description><author>Siyuan Feng, Taijie Chen, Yuhao Zhang, Jintao Ke, Zhengfei Zheng, Hai Yang</author><pubDate>Fri, 04 Aug 2023 10:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12336v2</guid></item><item><title>Learning to Paraphrase Sentences to Different Complexity Levels</title><link>http://arxiv.org/abs/2308.02226v1</link><description>While sentence simplification is an active research topic in NLP, itsadjacent tasks of sentence complexification and same-level paraphrasing arenot. To train models on all three tasks, we present two new unsuperviseddatasets. We compare these datasets, one labeled by a weak classifier and theother by a rule-based approach, with a single supervised dataset. Using thesethree datasets for training, we perform extensive experiments on bothmultitasking and prompting strategies. Compared to other systems trained onunsupervised parallel data, models trained on our weak classifier labeleddataset achieve state-of-the-art performance on the ASSET simplificationbenchmark. Our models also outperform previous work on sentence leveltargeting. Finally, we establish how a handful of Large Language Models performon these tasks under a zero-shot setting.</description><author>Alison Chi, Li-Kuang Chen, Yi-Chen Chang, Shu-Hui Lee, Jason S. Chang</author><pubDate>Fri, 04 Aug 2023 10:43:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02226v1</guid></item><item><title>Deep Semantic Model Fusion for Ancient Agricultural Terrace Detection</title><link>http://arxiv.org/abs/2308.02225v1</link><description>Discovering ancient agricultural terraces in desert regions is important forthe monitoring of long-term climate changes on the Earth's surface. However,traditional ground surveys are both costly and limited in scale. With theincreasing accessibility of aerial and satellite data, machine learningtechniques bear large potential for the automatic detection and recognition ofarchaeological landscapes. In this paper, we propose a deep semantic modelfusion method for ancient agricultural terrace detection. The input dataincludes aerial images and LiDAR generated terrain features in the Negevdesert. Two deep semantic segmentation models, namely DeepLabv3+ and UNet, withEfficientNet backbone, are trained and fused to provide segmentation maps ofancient terraces and walls. The proposed method won the first prize in theInternational AI Archaeology Challenge. Codes are available athttps://github.com/wangyi111/international-archaeology-ai-challenge.</description><author>Yi Wang, Chenying Liu, Arti Tiwari, Micha Silver, Arnon Karnieli, Xiao Xiang Zhu, Conrad M Albrecht</author><pubDate>Fri, 04 Aug 2023 10:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02225v1</guid></item><item><title>ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation</title><link>http://arxiv.org/abs/2308.02223v1</link><description>Applying Reinforcement Learning (RL) to sequence generation models enablesthe direct optimization of long-term rewards (\textit{e.g.,} BLEU and humanfeedback), but typically requires large-scale sampling over a space of actionsequences. This is a computational challenge as presented by the practice ofsequence generation problems, such as machine translation, where we often dealwith a large action space (\textit{e.g.,} a vocabulary) and a long actionsequence (\textit{e.g.,} a translation). In this work, we introduce two-stagesampling and dynamic sampling approaches to improve the sampling efficiencyduring training sequence generation models via RL. We experiment with ourapproaches on the traditional sequence generation tasks, including machinetranslation and abstractive summarization. Furthermore, we evaluate ourapproaches in RL from human feedback (RLHF) through training a large languagemodel using the reward model. Experimental results show that the efficientsampling-based RL, referred to as ESRL, can outperform all baselines in termsof both training efficiency and memory consumption. Notably, ESRL yieldsconsistent performance gains over the strong REINFORCE, minimum risk training,and proximal policy optimization methods.</description><author>Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao, Jingbo Zhu</author><pubDate>Fri, 04 Aug 2023 10:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02223v1</guid></item><item><title>Likelihood-ratio-based confidence intervals for neural networks</title><link>http://arxiv.org/abs/2308.02221v1</link><description>This paper introduces a first implementation of a novellikelihood-ratio-based approach for constructing confidence intervals forneural networks. Our method, called DeepLR, offers several qualitativeadvantages: most notably, the ability to construct asymmetric intervals thatexpand in regions with a limited amount of data, and the inherent incorporationof factors such as the amount of training time, network architecture, andregularization techniques. While acknowledging that the current implementationof the method is prohibitively expensive for many deep-learning applications,the high cost may already be justified in specific fields like medicalpredictions or astrophysics, where a reliable uncertainty estimate for a singleprediction is essential. This work highlights the significant potential of alikelihood-ratio-based uncertainty estimate and establishes a promising avenuefor future research.</description><author>Laurens Sluijterman, Eric Cator, Tom Heskes</author><pubDate>Fri, 04 Aug 2023 10:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02221v1</guid></item><item><title>Jointly Optimizing Image Compression with Low-light Image Enhancement</title><link>http://arxiv.org/abs/2305.15030v2</link><description>Learning-based image compression methods have made great progress. Most ofthem are designed for generic natural images. In fact, low-light imagesfrequently occur due to unavoidable environmental influences or technicallimitations, such as insufficient lighting or limited exposure time. %Whengeneral-purpose image compression algorithms compress low-light images, usefuldetail information is lost, resulting in a dramatic decrease in imageenhancement. Once low-light images are compressed by existing general imagecompression approaches, useful information(e.g., texture details) would be lostresulting in a dramatic performance decrease in low-light image enhancement. Tosimultaneously achieve a higher compression rate and better enhancementperformance for low-light images, we propose a novel image compressionframework with joint optimization of low-light image enhancement. We design anend-to-end trainable two-branch architecture with lower computational cost,which includes the main enhancement branch and the signal-to-noise ratio~(SNR)aware branch. Experimental results show that our proposed joint optimizationframework achieves a significant improvement over existing ``Compress beforeEnhance" or ``Enhance before Compress" sequential solutions for low-lightimages. Source codes are included in the supplementary material.</description><author>Shilv Cai, Xu Zou, Liqun Chen, Luxin Yan, Sheng Zhong</author><pubDate>Fri, 04 Aug 2023 10:29:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15030v2</guid></item><item><title>Federated Learning: Organizational Opportunities, Challenges, and Adoption Strategies</title><link>http://arxiv.org/abs/2308.02219v1</link><description>Restrictive rules for data sharing in many industries have led to thedevelopment of \ac{FL}. \ac{FL} is a \ac{ML} technique that allows distributedclients to train models collaboratively without the need to share theirrespective training data with others. In this article, we first explore thetechnical basics of FL and its potential applications. Second, we present aconceptual framework for the adoption of \ac{FL}, mapping organizations alongthe lines of their \ac{AI} capabilities and environment. We then discuss whyexemplary organizations in different industries, including industry consortia,established banks, public authorities, and data-intensive SMEs might considerdifferent approaches to \ac{FL}. To conclude, we argue that \ac{FL} presents aninstitutional shift with ample interdisciplinary research opportunities for thebusiness and information systems engineering community.</description><author>Joaquin Delgado Fernandez, Martin Brennecke, Tom Barbereau, Alexander Rieger, Gilbert Fridgen</author><pubDate>Fri, 04 Aug 2023 10:23:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02219v1</guid></item></channel></rss>