<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 26 Oct 2023 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Efficient Estimation of Average-Case Robustness for Multi-Class Classification</title><link>http://arxiv.org/abs/2307.13885v4</link><description>Robustness in machine learning is commonly studied in the adversarialsetting, yet real-world noise (such as measurement noise) is random rather thanadversarial. Model behavior under such noise is captured by average-caserobustness, i.e., the probability of obtaining consistent predictions in alocal region around an input. However, the na\"ive approach to computingaverage-case robustness based on Monte-Carlo sampling is statisticallyinefficient, especially for high-dimensional data, leading to prohibitivecomputational costs for large-scale applications. In this work, we develop thefirst analytical estimators to efficiently compute average-case robustness ofmulti-class discriminative models. These estimators linearize models in thelocal region around an input and analytically compute the robustness of theresulting linear models. We show empirically that these estimators efficientlycompute the robustness of standard deep learning models and demonstrate theseestimators' usefulness for various tasks involving robustness, such asmeasuring robustness bias and identifying dataset samples that are vulnerableto noise perturbation. In doing so, this work not only proposes a new frameworkfor robustness, but also makes its computation practical, enabling the use ofaverage-case robustness in downstream applications.</description><author>Tessa Han, Suraj Srinivas, Himabindu Lakkaraju</author><pubDate>Wed, 25 Oct 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13885v4</guid></item><item><title>Revisiting Deep Learning Models for Tabular Data</title><link>http://arxiv.org/abs/2106.11959v4</link><description>The existing literature on deep learning for tabular data proposes a widerange of novel architectures and reports competitive results on variousdatasets. However, the proposed models are usually not properly compared toeach other and existing works often use different benchmarks and experimentprotocols. As a result, it is unclear for both researchers and practitionerswhat models perform best. Additionally, the field still lacks effectivebaselines, that is, the easy-to-use models that provide competitive performanceacross different problems. In this work, we perform an overview of the main families of DL architecturesfor tabular data and raise the bar of baselines in tabular DL by identifyingtwo simple and powerful deep architectures. The first one is a ResNet-likearchitecture which turns out to be a strong baseline that is often missing inprior works. The second model is our simple adaptation of the Transformerarchitecture for tabular data, which outperforms other solutions on most tasks.Both models are compared to many existing architectures on a diverse set oftasks under the same training and tuning protocols. We also compare the best DLmodels with Gradient Boosted Decision Trees and conclude that there is still nouniversally superior solution.</description><author>Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko</author><pubDate>Wed, 25 Oct 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.11959v4</guid></item><item><title>Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation</title><link>http://arxiv.org/abs/2305.16985v2</link><description>In recent years, domains such as natural language processing and imagerecognition have popularized the paradigm of using large datasets to pretrainrepresentations that can be effectively transferred to downstream tasks. Inthis work we evaluate how such a paradigm should be done in imitation learning,where both pretraining and finetuning data are trajectories collected byexperts interacting with an unknown environment. Namely, we consider a settingwhere the pretraining corpus consists of multitask demonstrations and the taskfor each demonstration is set by an unobserved latent context variable. Thegoal is to use the pretraining corpus to learn a low dimensional representationof the high dimensional (e.g., visual) observation space which can betransferred to a novel context for finetuning on a limited dataset ofdemonstrations. Among a variety of possible pretraining objectives, we arguethat inverse dynamics modeling -- i.e., predicting an action given theobservations appearing before and after it in the demonstration -- iswell-suited to this setting. We provide empirical evidence of this claimthrough evaluations on a variety of simulated visuomotor manipulation problems.While previous work has attempted various theoretical explanations regardingthe benefit of inverse dynamics modeling, we find that these arguments areinsufficient to explain the empirical advantages often observed in oursettings, and so we derive a novel analysis using a simple but generalenvironment model.</description><author>David Brandfonbrener, Ofir Nachum, Joan Bruna</author><pubDate>Wed, 25 Oct 2023 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16985v2</guid></item><item><title>SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation</title><link>http://arxiv.org/abs/2310.16838v1</link><description>Humans excel at transferring manipulation skills across diverse objectshapes, poses, and appearances due to their understanding of semanticcorrespondences between different instances. To endow robots with a similarhigh-level understanding, we develop a Distilled Feature Field (DFF) for 3Dscenes, leveraging large 2D vision models to distill semantic features frommultiview images. While current research demonstrates advanced performance inreconstructing DFFs from dense views, the development of learning a DFF fromsparse views is relatively nascent, despite its prevalence in numerousmanipulation tasks with fixed cameras. In this work, we introduce SparseDFF, anovel method for acquiring view-consistent 3D DFFs from sparse RGBDobservations, enabling one-shot learning of dexterous manipulations that aretransferable to novel scenes. Specifically, we map the image features to the 3Dpoint cloud, allowing for propagation across the 3D space to establish a densefeature field. At the core of SparseDFF is a lightweight feature refinementnetwork, optimized with a contrastive loss between pairwise views afterback-projecting the image features onto the 3D point cloud. Additionally, weimplement a point-pruning mechanism to augment feature continuity within eachlocal neighborhood. By establishing coherent feature fields on both source andtarget scenes, we devise an energy function that facilitates the minimizationof feature discrepancies w.r.t. the end-effector parameters between thedemonstration and the target manipulation. We evaluate our approach using adexterous hand, mastering real-world manipulations on both rigid and deformableobjects, and showcase robust generalization in the face of object andscene-context variations.</description><author>Qianxu Wang, Haotong Zhang, Congyue Deng, Yang You, Hao Dong, Yixin Zhu, Leonidas Guibas</author><pubDate>Wed, 25 Oct 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16838v1</guid></item><item><title>RDBench: ML Benchmark for Relational Databases</title><link>http://arxiv.org/abs/2310.16837v1</link><description>Benefiting from high-quality datasets and standardized evaluation metrics,machine learning (ML) has achieved sustained progress and widespreadapplications. However, while applying machine learning to relational databases(RDBs), the absence of a well-established benchmark remains a significantobstacle to the development of ML. To address this issue, we introduce MLBenchmark For Relational Databases (RDBench), a standardized benchmark thataims to promote reproducible ML research on RDBs that include multiple tables.RDBench offers diverse RDB datasets of varying scales, domains, and relationalstructures, organized into 4 levels. Notably, to simplify the adoption ofRDBench for diverse ML domains, for any given database, RDBench exposes threetypes of interfaces including tabular data, homogeneous graphs, andheterogeneous graphs, sharing the same underlying task definition. For thefirst time, RDBench enables meaningful comparisons between ML methods fromdiverse domains, ranging from XGBoost to Graph Neural Networks, under RDBprediction tasks. We design multiple classification and regression tasks foreach RDB dataset and report averaged results over the same dataset, furtherenhancing the robustness of the experimental findings. RDBench is implementedwith DBGym, a user-friendly platform for ML research and application ondatabases, enabling benchmarking new ML methods with RDBench at ease.</description><author>Zizhao Zhang, Yi Yang, Lutong Zou, He Wen, Tao Feng, Jiaxuan You</author><pubDate>Wed, 25 Oct 2023 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16837v1</guid></item><item><title>LLM-FP4: 4-Bit Floating-Point Quantized Transformers</title><link>http://arxiv.org/abs/2310.16836v1</link><description>We propose LLM-FP4 for quantizing both weights and activations in largelanguage models (LLMs) down to 4-bit floating-point values, in a post-trainingmanner. Existing post-training quantization (PTQ) solutions are primarilyinteger-based and struggle with bit widths below 8 bits. Compared to integerquantization, floating-point (FP) quantization is more flexible and can betterhandle long-tail or bell-shaped distributions, and it has emerged as a defaultchoice in many hardware platforms. One characteristic of FP quantization isthat its performance largely depends on the choice of exponent bits andclipping range. In this regard, we construct a strong FP-PTQ baseline bysearching for the optimal quantization parameters. Furthermore, we observe ahigh inter-channel variance and low intra-channel variance pattern inactivation distributions, which adds activation quantization difficulty. Werecognize this pattern to be consistent across a spectrum of transformer modelsdesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.To tackle this, we propose per-channel activation quantization and show thatthese additional scaling factors can be reparameterized as exponential biasesof weights, incurring a negligible cost. Our method, for the first time, canquantize both weights and activations in the LLaMA-13B to only 4-bit andachieves an average score of 63.1 on the common sense zero-shot reasoningtasks, which is only 5.8 lower than the full-precision model, significantlyoutperforming the previous state-of-the-art by 12.7 points. Code is availableat: https://github.com/nbasyl/LLM-FP4.</description><author>Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, Kwang-Ting Cheng</author><pubDate>Wed, 25 Oct 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16836v1</guid></item><item><title>Proposal-Contrastive Pretraining for Object Detection from Fewer Data</title><link>http://arxiv.org/abs/2310.16835v1</link><description>The use of pretrained deep neural networks represents an attractive way toachieve strong results with few data available. When specialized in denseproblems such as object detection, learning local rather than globalinformation in images has proven to be more efficient. However, forunsupervised pretraining, the popular contrastive learning requires a largebatch size and, therefore, a lot of resources. To address this problem, we areinterested in transformer-based object detectors that have recently gainedtraction in the community with good performance and with the particularity ofgenerating many diverse object proposals. In this work, we present Proposal Selection Contrast (ProSeCo), a novelunsupervised overall pretraining approach that leverages this property. ProSeCouses the large number of object proposals generated by the detector forcontrastive learning, which allows the use of a smaller batch size, combinedwith object-level features to learn local information in the images. To improvethe effectiveness of the contrastive loss, we introduce the object locationinformation in the selection of positive examples to take into account multipleoverlapping object proposals. When reusing pretrained backbone, we advocate forconsistency in learning local information between the backbone and thedetection head. We show that our method outperforms state of the art in unsupervisedpretraining for object detection on standard and novel benchmarks in learningwith fewer data.</description><author>Quentin Bouniot, Romaric Audigier, Angélique Loesch, Amaury Habrard</author><pubDate>Wed, 25 Oct 2023 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16835v1</guid></item><item><title>Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution</title><link>http://arxiv.org/abs/2310.16834v1</link><description>Despite their groundbreaking performance for many generative modeling tasks,diffusion models have fallen short on discrete data domains such as naturallanguage. Crucially, standard diffusion models rely on the well-establishedtheory of score matching, but efforts to generalize this to discrete structureshave not yielded the same empirical gains. In this work, we bridge this gap byproposing score entropy, a novel discrete score matching loss that is morestable than existing methods, forms an ELBO for maximum likelihood training,and can be efficiently optimized with a denoising variant. We scale our ScoreEntropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2,achieving highly competitive likelihoods while also introducing distinctalgorithmic advantages. In particular, when comparing similarly sized SEDD andGPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ ofand sometimes outperforming the baseline). Furthermore, SEDD models learn amore faithful sequence distribution (around $4\times$ better compared to GPT-2models with ancestral sampling as measured by large models), can trade offcompute for generation quality (needing only $16\times$ fewer networkevaluations to match GPT-2), and enables arbitrary infilling beyond thestandard left to right prompting.</description><author>Aaron Lou, Chenlin Meng, Stefano Ermon</author><pubDate>Wed, 25 Oct 2023 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16834v1</guid></item><item><title>LightSpeed: Light and Fast Neural Light Fields on Mobile Devices</title><link>http://arxiv.org/abs/2310.16832v1</link><description>Real-time novel-view image synthesis on mobile devices is prohibitive due tothe limited computational power and storage. Using volumetric renderingmethods, such as NeRF and its derivatives, on mobile devices is not suitabledue to the high computational cost of volumetric rendering. On the other hand,recent advances in neural light field representations have shown promisingreal-time view synthesis results on mobile devices. Neural light field methodslearn a direct mapping from a ray representation to the pixel color. Thecurrent choice of ray representation is either stratified ray sampling orPl\"{u}cker coordinates, overlooking the classic light slab (two-plane)representation, the preferred representation to interpolate between light fieldviews. In this work, we find that using the light slab representation is anefficient representation for learning a neural light field. More importantly,it is a lower-dimensional ray representation enabling us to learn the 4D rayspace using feature grids which are significantly faster to train and render.Although mostly designed for frontal views, we show that the light-slabrepresentation can be further extended to non-frontal scenes using adivide-and-conquer strategy. Our method offers superior rendering qualitycompared to previous light field methods and achieves a significantly improvedtrade-off between rendering quality and speed.</description><author>Aarush Gupta, Junli Cao, Chaoyang Wang, Ju Hu, Sergey Tulyakov, Jian Ren, László A Jeni</author><pubDate>Wed, 25 Oct 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16832v1</guid></item><item><title>PERF: Panoramic Neural Radiance Field from a Single Panorama</title><link>http://arxiv.org/abs/2310.16831v1</link><description>Neural Radiance Field (NeRF) has achieved substantial progress in novel viewsynthesis given multi-view images. Recently, some works have attempted to traina NeRF from a single image with 3D priors. They mainly focus on a limited fieldof view and there are few invisible occlusions, which greatly limits theirscalability to real-world 360-degree panoramic scenarios with large-sizeocclusions. In this paper, we present PERF, a 360-degree novel view synthesisframework that trains a panoramic neural radiance field from a single panorama.Notably, PERF allows 3D roaming in a complex scene without expensive andtedious image collection. To achieve this goal, we propose a novelcollaborative RGBD inpainting method and a progressive inpainting-and-erasingmethod to lift up a 360-degree 2D scene to a 3D scene. Specifically, we firstpredict a panoramic depth map as initialization given a single panorama, andreconstruct visible 3D regions with volume rendering. Then we introduce acollaborative RGBD inpainting approach into a NeRF for completing RGB imagesand depth maps from random views, which is derived from an RGB Stable Diffusionmodel and a monocular depth estimator. Finally, we introduce aninpainting-and-erasing strategy to avoid inconsistent geometry between anewly-sampled view and reference views. The two components are integrated intothe learning of NeRFs in a unified optimization framework and achieve promisingresults. Extensive experiments on Replica and a new dataset PERF-in-the-wilddemonstrate the superiority of our PERF over state-of-the-art methods. Our PERFcan be widely used for real-world applications, such as panorama-to-3D,text-to-3D, and 3D scene stylization applications. Project page and code areavailable at https://perf-project.github.io/.</description><author>Guangcong Wang, Peng Wang, Zhaoxi Chen, Wenping Wang, Chen Change Loy, Ziwei Liu</author><pubDate>Wed, 25 Oct 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16831v1</guid></item><item><title>TD-MPC2: Scalable, Robust World Models for Continuous Control</title><link>http://arxiv.org/abs/2310.16828v1</link><description>TD-MPC is a model-based reinforcement learning (RL) algorithm that performslocal trajectory optimization in the latent space of a learned implicit(decoder-free) world model. In this work, we present TD-MPC2: a series ofimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improvessignificantly over baselines across 104 online RL tasks spanning 4 diverse taskdomains, achieving consistently strong results with a single set ofhyperparameters. We further show that agent capabilities increase with modeland data size, and successfully train a single 317M parameter agent to perform80 tasks across multiple task domains, embodiments, and action spaces. Weconclude with an account of lessons, opportunities, and risks associated withlarge TD-MPC2 agents. Explore videos, models, data, code, and more athttps://nicklashansen.github.io/td-mpc2</description><author>Nicklas Hansen, Hao Su, Xiaolong Wang</author><pubDate>Wed, 25 Oct 2023 18:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16828v1</guid></item><item><title>Deep machine learning for meteor monitoring: advances with transfer learning and gradient-weighted class activation mapping</title><link>http://arxiv.org/abs/2310.16826v1</link><description>In recent decades, the use of optical detection systems for meteor studieshas increased dramatically, resulting in huge amounts of data being analyzed.Automated meteor detection tools are essential for studying the continuousmeteoroid incoming flux, recovering fresh meteorites, and achieving a betterunderstanding of our Solar System. Concerning meteor detection, distinguishingfalse positives between meteor and non-meteor images has traditionally beenperformed by hand, which is significantly time-consuming. To address thisissue, we developed a fully automated pipeline that uses Convolutional NeuralNetworks (CNNs) to classify candidate meteor detections. Our new method is ableto detect meteors even in images that contain static elements such as clouds,the Moon, and buildings. To accurately locate the meteor within each frame, weemploy the Gradient-weighted Class Activation Mapping (Grad-CAM) technique.This method facilitates the identification of the region of interest bymultiplying the activations from the last convolutional layer with the averageof the gradients across the feature map of that layer. By combining thesefindings with the activation map derived from the first convolutional layer, weeffectively pinpoint the most probable pixel location of the meteor. We trainedand evaluated our model on a large dataset collected by the Spanish MeteorNetwork (SPMN) and achieved a precision of 98\%. Our new methodology presentedhere has the potential to reduce the workload of meteor scientists and stationoperators and improve the accuracy of meteor tracking and classification.</description><author>Eloy Peña-Asensio, Josep M. Trigo-Rodríguez, Pau Grèbol-Tomàs, David Regordosa-Avellana, Albert Rimola</author><pubDate>Wed, 25 Oct 2023 18:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16826v1</guid></item><item><title>CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images</title><link>http://arxiv.org/abs/2310.16825v1</link><description>We assemble a dataset of Creative-Commons-licensed (CC) images, which we useto train a set of open diffusion models that are qualitatively competitive withStable Diffusion 2 (SD2). This task presents two challenges: (1)high-resolution CC images lack the captions necessary to train text-to-imagegenerative models; (2) CC images are relatively scarce. In turn, to addressthese challenges, we use an intuitive transfer learning technique to produce aset of high-quality synthetic captions paired with curated CC images. We thendevelop a data- and compute-efficient training recipe that requires as littleas 3% of the LAION-2B data needed to train existing SD2 models, but obtainscomparable quality. These results indicate that we have a sufficient number ofCC images (~70 million) for training high-quality models. Our training recipealso implements a variety of optimizations that achieve ~3X training speed-ups,enabling rapid model iteration. We leverage this recipe to train severalhigh-quality text-to-image models, which we dub the CommonCanvas family. Ourlargest model achieves comparable performance to SD2 on a human evaluation,despite being trained on our CC dataset that is significantly smaller thanLAION and using synthetic captions for training. We release our models, data,and code athttps://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md</description><author>Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, Volodymyr Kuleshov</author><pubDate>Wed, 25 Oct 2023 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16825v1</guid></item><item><title>Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity and Relation Extraction</title><link>http://arxiv.org/abs/2310.16822v1</link><description>How can we better extract entities and relations from text? Using multimodalextraction with images and text obtains more signals for entities andrelations, and aligns them through graphs or hierarchical fusion, aiding inextraction. Despite attempts at various fusions, previous works have overlookedmany unlabeled image-caption pairs, such as NewsCLIPing. This paper proposesinnovative pre-training objectives for entity-object and relation-imagealignment, extracting objects from images and aligning them with entity andrelation prompts for soft pseudo-labels. These labels are used asself-supervised signals for pre-training, enhancing the ability to extractentities and relations. Experiments on three datasets show an average 3.41% F1improvement over prior SOTA. Additionally, our method is orthogonal to previousmultimodal fusions, and using it on prior SOTA fusions further improves 5.47%F1.</description><author>Xuming Hu, Junzhe Chen, Aiwei Liu, Shiao Meng, Lijie Wen, Philip S. Yu</author><pubDate>Wed, 25 Oct 2023 18:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16822v1</guid></item><item><title>CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression</title><link>http://arxiv.org/abs/2310.16819v1</link><description>In causal inference about two treatments, Conditional Average TreatmentEffects (CATEs) play an important role as a quantity representing anindividualized causal effect, defined as a difference between the expectedoutcomes of the two treatments conditioned on covariates. This study assumestwo linear regression models between a potential outcome and covariates of thetwo treatments and defines CATEs as a difference between the linear regressionmodels. Then, we propose a method for consistently estimating CATEs even underhigh-dimensional and non-sparse parameters. In our study, we demonstrate thatdesirable theoretical properties, such as consistency, remain attainable evenwithout assuming sparsity explicitly if we assume a weaker assumption calledimplicit sparsity originating from the definition of CATEs. In this assumption,we suppose that parameters of linear models in potential outcomes can bedivided into treatment-specific and common parameters, where thetreatment-specific parameters take difference values between each linearregression model, while the common parameters remain identical. Thus, in adifference between two linear regression models, the common parametersdisappear, leaving only differences in the treatment-specific parameters.Consequently, the non-zero parameters in CATEs correspond to the differences inthe treatment-specific parameters. Leveraging this assumption, we develop aLasso regression method specialized for CATE estimation and present that theestimator is consistent. Finally, we confirm the soundness of the proposedmethod by simulation studies.</description><author>Masahiro Kato, Masaaki Imaizumi</author><pubDate>Wed, 25 Oct 2023 18:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16819v1</guid></item><item><title>DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior</title><link>http://arxiv.org/abs/2310.16818v1</link><description>We present DreamCraft3D, a hierarchical 3D content generation method thatproduces high-fidelity and coherent 3D objects. We tackle the problem byleveraging a 2D reference image to guide the stages of geometry sculpting andtexture boosting. A central focus of this work is to address the consistencyissue that existing works encounter. To sculpt geometries that rendercoherently, we perform score distillation sampling via a view-dependentdiffusion model. This 3D prior, alongside several training strategies,prioritizes the geometry consistency but compromises the texture fidelity. Wefurther propose Bootstrapped Score Distillation to specifically boost thetexture. We train a personalized diffusion model, Dreambooth, on the augmentedrenderings of the scene, imbuing it with 3D knowledge of the scene beingoptimized. The score distillation from this 3D-aware diffusion prior providesview-consistent guidance for the scene. Notably, through an alternatingoptimization of the diffusion prior and 3D scene representation, we achievemutually reinforcing improvements: the optimized 3D scene aids in training thescene-specific diffusion model, which offers increasingly view-consistentguidance for 3D optimization. The optimization is thus bootstrapped and leadsto substantial texture boosting. With tailored 3D priors throughout thehierarchical generation, DreamCraft3D generates coherent 3D objects withphotorealistic renderings, advancing the state-of-the-art in 3D contentgeneration. Code available at https://github.com/deepseek-ai/DreamCraft3D.</description><author>Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, Yebin Liu</author><pubDate>Wed, 25 Oct 2023 18:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16818v1</guid></item><item><title>Hunayn: Elevating Translation Beyond the Literal</title><link>http://arxiv.org/abs/2310.13613v2</link><description>This project introduces an advanced English-to-Arabic translator surpassingconventional tools. Leveraging the Helsinki transformer (MarianMT), ourapproach involves fine-tuning on a self-scraped, purely literary Arabicdataset. Evaluations against Google Translate show consistent outperformance inqualitative assessments. Notably, it excels in cultural sensitivity and contextaccuracy. This research underscores the Helsinki transformer's superiority forEnglish-to-Arabic translation using a Fusha dataset.</description><author>Nasser Almousa, Nasser Alzamil, Abdullah Alshehri, Ahmad Sait</author><pubDate>Wed, 25 Oct 2023 18:41:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13613v2</guid></item><item><title>Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization</title><link>http://arxiv.org/abs/2310.16810v1</link><description>This study explores the capabilities of prompt-driven Large Language Models(LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialoguesummarization. Experiments employed DialogSum (English social conversations)and DECODA (French call center interactions), testing various prompts:including prompts from existing literature and those from human summarizationguidelines, as well as a two-step prompt approach. Our findings indicate thatGPT models often produce lengthy summaries and deviate from human summarizationguidelines. However, using human guidelines as an intermediate step showspromise, outperforming direct word-length constraint prompts in some cases. Theresults reveal that GPT models exhibit unique stylistic tendencies in theirsummaries. While BERTScores did not dramatically decrease for GPT outputssuggesting semantic similarity to human references and specialised pre-trainedmodels, ROUGE scores reveal grammatical and lexical disparities betweenGPT-generated and human-written summaries. These findings shed light on thecapabilities and limitations of GPT models in following human instructions fordialogue summarization.</description><author>Yongxin Zhou, Fabien Ringeval, François Portet</author><pubDate>Wed, 25 Oct 2023 18:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16810v1</guid></item><item><title>Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation</title><link>http://arxiv.org/abs/2310.16809v1</link><description>This paper presents a comprehensive evaluation of the Optical CharacterRecognition (OCR) capabilities of the recently released GPT-4V(ision), a LargeMultimodal Model (LMM). We assess the model's performance across a range of OCRtasks, including scene text recognition, handwritten text recognition,handwritten mathematical expression recognition, table structure recognition,and information extraction from visually-rich document. The evaluation revealsthat GPT-4V performs well in recognizing and understanding Latin contents, butstruggles with multilingual scenarios and complex tasks. Based on theseobservations, we delve deeper into the necessity of specialized OCR models anddeliberate on the strategies to fully harness the pretrained general LMMs likeGPT-4V for OCR downstream tasks. The study offers a critical reference forfuture research in OCR with LMMs. Evaluation pipeline and results are availableat https://github.com/SCUT-DLVCLab/GPT-4V_OCR.</description><author>Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, Lianwen Jin</author><pubDate>Wed, 25 Oct 2023 18:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16809v1</guid></item><item><title>Fingervein Verification using Convolutional Multi-Head Attention Network</title><link>http://arxiv.org/abs/2310.16808v1</link><description>Biometric verification systems are deployed in various security-basedaccess-control applications that require user-friendly and reliable personverification. Among the different biometric characteristics, fingerveinbiometrics have been extensively studied owing to their reliable verificationperformance. Furthermore, fingervein patterns reside inside the skin and arenot visible outside; therefore, they possess inherent resistance topresentation attacks and degradation due to external factors. In this paper, weintroduce a novel fingervein verification technique using a convolutionalmultihead attention network called VeinAtnNet. The proposed VeinAtnNet isdesigned to achieve light weight with a smaller number of learnable parameterswhile extracting discriminant information from both normal and enhancedfingervein images. The proposed VeinAtnNet was trained on the newly constructedfingervein dataset with 300 unique fingervein patterns that were captured inmultiple sessions to obtain 92 samples per unique fingervein. Extensiveexperiments were performed on the newly collected dataset FV-300 and thepublicly available FV-USM and FV-PolyU fingervein dataset. The performance ofthe proposed method was compared with five state-of-the-art fingerveinverification systems, indicating the efficacy of the proposed VeinAtnNet.</description><author>Raghavendra Ramachandra, Sushma Venkatesh</author><pubDate>Wed, 25 Oct 2023 18:38:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16808v1</guid></item><item><title>Robust Output Analysis with Monte-Carlo Methodology</title><link>http://arxiv.org/abs/2207.13612v3</link><description>In predictive modeling with simulation or machine learning, it is critical toaccurately assess the quality of estimated values through output analysis. Inrecent decades output analysis has become enriched with methods that quantifythe impact of input data uncertainty in the model outputs to increaserobustness. However, most developments are applicable assuming that the inputdata adheres to a parametric family of distributions. We propose a unifiedoutput analysis framework for simulation and machine learning outputs throughthe lens of Monte Carlo sampling. This framework provides nonparametricquantification of the variance and bias induced in the outputs withhigher-order accuracy. Our new bias-corrected estimation from the model outputsleverages the extension of fast iterative bootstrap sampling and higher-orderinfluence functions. For the scalability of the proposed estimation methods, wedevise budget-optimal rules and leverage control variates for variancereduction. Our theoretical and numerical results demonstrate a clear advantagein building more robust confidence intervals from the model outputs with highercoverage probability.</description><author>Kimia Vahdat, Sara Shashaani</author><pubDate>Wed, 25 Oct 2023 18:37:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.13612v3</guid></item><item><title>Learning COVID-19 Regional Transmission Using Universal Differential Equations in a SIR model</title><link>http://arxiv.org/abs/2310.16804v1</link><description>Highly-interconnected societies difficult to model the spread of infectiousdiseases such as COVID-19. Single-region SIR models fail to account forincoming forces of infection and expanding them to a large number ofinteracting regions involves many assumptions that do not hold in the realworld. We propose using Universal Differential Equations (UDEs) to capture theinfluence of neighboring regions and improve the model's predictions in acombined SIR+UDE model. UDEs are differential equations totally or partiallydefined by a deep neural network (DNN). We include an additive term to the SIRequations composed by a DNN that learns the incoming force of infection fromthe other regions. The learning is performed using automatic differentiationand gradient descent to approach the change in the target system caused by thestate of the neighboring regions. We compared the proposed model using asimulated COVID-19 outbreak against a single-region SIR and a fully data-drivenmodel composed only of a DNN. The proposed UDE+SIR model generates predictionsthat capture the outbreak dynamic more accurately, but a decay in performanceis observed at the last stages of the outbreak. The single-area SIR and thefully data-driven approach do not capture the proper dynamics accurately. Oncethe predictions were obtained, we employed the SINDy algorithm to substitutethe DNN with a regression, removing the black box element of the model with noconsiderable increase in the error levels.</description><author>Adrian Rojas-Campos, Lukas Stelz, Pascal Nieters</author><pubDate>Wed, 25 Oct 2023 18:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16804v1</guid></item><item><title>Language Agnostic Code Embeddings</title><link>http://arxiv.org/abs/2310.16803v1</link><description>Recently, code language models have achieved notable advancements inaddressing a diverse array of essential code comprehension and generationtasks. Yet, the field lacks a comprehensive deep dive and understanding of thecode embeddings of multilingual code models. In this paper, we present acomprehensive study on multilingual code embeddings, focusing on thecross-lingual capabilities of these embeddings across different programminglanguages. Through probing experiments, we demonstrate that code embeddingscomprise two distinct components: one deeply tied to the nuances and syntax ofa specific language, and the other remaining agnostic to these details,primarily focusing on semantics. Further, we show that when we isolate andeliminate this language-specific component, we witness significant improvementsin downstream code retrieval tasks, leading to an absolute increase of up to+17 in the Mean Reciprocal Rank (MRR).</description><author>Saiteja Utpala, Alex Gu, Pin Yu Chen</author><pubDate>Wed, 25 Oct 2023 18:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16803v1</guid></item><item><title>From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction</title><link>http://arxiv.org/abs/2310.16802v1</link><description>Foundation models have been transformational in machine learning fields suchas natural language processing and computer vision. Similar success in atomicproperty prediction has been limited due to the challenges of trainingeffective models across multiple chemical domains. To address this, weintroduce Joint Multi-domain Pre-training (JMP), a supervised pre-trainingstrategy that simultaneously trains on multiple datasets from differentchemical domains, treating each dataset as a unique pre-training task within amulti-task framework. Our combined training dataset consists of $\sim$120Msystems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance andgeneralization by fine-tuning over a diverse set of downstream tasks anddatasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMPdemonstrates an average improvement of 59% over training from scratch, andmatches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights thepotential of pre-training strategies that utilize diverse data to advanceproperty prediction across chemical domains, especially for low-data tasks.</description><author>Nima Shoghi, Adeesh Kolluru, John R. Kitchin, Zachary W. Ulissi, C. Lawrence Zitnick, Brandon M. Wood</author><pubDate>Wed, 25 Oct 2023 18:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16802v1</guid></item><item><title>Multispectral Imaging for Differential Face Morphing Attack Detection: A Preliminary Study</title><link>http://arxiv.org/abs/2304.03510v3</link><description>Face morphing attack detection is emerging as an increasingly challengingproblem owing to advancements in high-quality and realistic morphing attackgeneration. Reliable detection of morphing attacks is essential because theseattacks are targeted for border control applications. This paper presents amultispectral framework for differential morphing-attack detection (D-MAD). TheD-MAD methods are based on using two facial images that are captured from theePassport (also called the reference image) and the trusted device (forexample, Automatic Border Control (ABC) gates) to detect whether the face imagepresented in ePassport is morphed. The proposed multispectral D-MAD frameworkintroduce a multispectral image captured as a trusted capture to acquire sevendifferent spectral bands to detect morphing attacks. Extensive experiments wereconducted on the newly created Multispectral Morphed Datasets (MSMD) with 143unique data subjects that were captured using both visible and multispectralcameras in multiple sessions. The results indicate the superior performance ofthe proposed multispectral framework compared to visible images.</description><author>Raghavendra Ramachandra, Sushma Venkatesh, Naser Damer, Narayan Vetrekar, Rajendra Gad</author><pubDate>Wed, 25 Oct 2023 18:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03510v3</guid></item><item><title>QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models</title><link>http://arxiv.org/abs/2310.16795v1</link><description>Mixture-of-Experts (MoE) architectures offer a general solution to the highinference costs of large language models (LLMs) via sparse routing, bringingfaster and more accurate models, at the cost of massive parameter counts. Forexample, the SwitchTransformer-c2048 model has 1.6 trillion parameters,requiring 3.2TB of accelerator memory to run efficiently, which makes practicaldeployment challenging and expensive. In this paper, we present a solution tothis memory problem, in form of a new compression and execution frameworkcalled QMoE. Specifically, QMoE consists of a scalable algorithm whichaccurately compresses trillion-parameter MoEs to less than 1 bit per parameter,in a custom format co-designed with bespoke GPU decoding kernels to facilitateefficient end-to-end compressed inference, with minor runtime overheadsrelative to uncompressed execution. Concretely, QMoE can compress the 1.6trillion parameter SwitchTransformer-c2048 model to less than 160GB (20xcompression, 0.8 bits per parameter) at only minor accuracy loss, in less thana day on a single GPU. This enables, for the first time, the execution of atrillion-parameter model on affordable commodity hardware, like a single serverwith 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overheadrelative to ideal uncompressed inference. The source code and compressed modelsare available at github.com/IST-DASLab/qmoe.</description><author>Elias Frantar, Dan Alistarh</author><pubDate>Wed, 25 Oct 2023 18:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16795v1</guid></item><item><title>ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval</title><link>http://arxiv.org/abs/2302.02285v2</link><description>Diffusion models show promising generation capability for a variety of data.Despite their high generation quality, the inference for diffusion models isstill time-consuming due to the numerous sampling iterations required. Toaccelerate the inference, we propose ReDi, a simple yet learning-freeRetrieval-based Diffusion sampling framework. From a precomputed knowledgebase, ReDi retrieves a trajectory similar to the partially generated trajectoryat an early stage of generation, skips a large portion of intermediate steps,and continues sampling from a later step in the retrieved trajectory. Wetheoretically prove that the generation performance of ReDi is guaranteed. Ourexperiments demonstrate that ReDi improves the model inference efficiency by 2xspeedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domainimage generation such as image stylization.</description><author>Kexun Zhang, Xianjun Yang, William Yang Wang, Lei Li</author><pubDate>Wed, 25 Oct 2023 18:24:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02285v2</guid></item><item><title>Learning Independent Program and Architecture Representations for Generalizable Performance Modeling</title><link>http://arxiv.org/abs/2310.16792v1</link><description>This paper proposes PerfVec, a novel deep learning-based performance modelingframework that learns high-dimensional, independent/orthogonal program andmicroarchitecture representations. Once learned, a program representation canbe used to predict its performance on any microarchitecture, and likewise, amicroarchitecture representation can be applied in the performance predictionof any program. Additionally, PerfVec yields a foundation model that capturesthe performance essence of instructions, which can be directly used bydevelopers in numerous performance modeling related tasks without incurring itstraining cost. The evaluation demonstrates that PerfVec is more general,efficient, and accurate than previous approaches.</description><author>Lingda Li, Thomas Flynn, Adolfy Hoisie</author><pubDate>Wed, 25 Oct 2023 18:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16792v1</guid></item><item><title>Covert Planning against Imperfect Observers</title><link>http://arxiv.org/abs/2310.16791v1</link><description>Covert planning refers to a class of constrained planning problems where anagent aims to accomplish a task with minimal information leaked to a passiveobserver to avoid detection. However, existing methods of covert planning oftenconsider deterministic environments or do not exploit the observer's imperfectinformation. This paper studies how covert planning can leverage the couplingof stochastic dynamics and the observer's imperfect observation to achieveoptimal task performance without being detected. Specifically, we employ aMarkov decision process to model the interaction between the agent and itsstochastic environment, and a partial observation function to capture theleaked information to a passive observer. Assuming the observer employshypothesis testing to detect if the observation deviates from a nominal policy,the covert planning agent aims to maximize the total discounted reward whilekeeping the probability of being detected as an adversary below a giventhreshold. We prove that finite-memory policies are more powerful thanMarkovian policies in covert planning. Then, we develop a primal-dual proximalpolicy gradient method with a two-time-scale update to compute a (locally)optimal covert policy. We demonstrate the effectiveness of our methods using astochastic gridworld example. Our experimental results illustrate that theproposed method computes a policy that maximizes the adversary's expectedreward without violating the detection constraint, and empirically demonstrateshow the environmental noises can influence the performance of the covertpolicies.</description><author>Haoxiang Ma, Chongyang Shi, Shuo Han, Michael R. Dorothy, Jie Fu</author><pubDate>Wed, 25 Oct 2023 18:23:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16791v1</guid></item><item><title>Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances</title><link>http://arxiv.org/abs/2310.16790v1</link><description>To achieve state-of-the-art performance, one still needs to train NER modelson large-scale, high-quality annotated data, an asset that is both costly andtime-intensive to accumulate. In contrast, real-world applications often resortto massive low-quality labeled data through non-expert annotators viacrowdsourcing and external knowledge bases via distant supervision as acost-effective alternative. However, these annotation methods result in noisylabels, which in turn lead to a notable decline in performance. Hence, wepropose to denoise the noisy NER data with guidance from a small set of cleaninstances. Along with the main NER model we train a discriminator model and useits outputs to recalibrate the sample weights. The discriminator is capable ofdetecting both span and category errors with different discriminative prompts.Results on public crowdsourcing and distant supervision datasets show that theproposed method can consistently improve performance with a small guidance set.</description><author>Zhendong Chu, Ruiyi Zhang, Tong Yu, Rajiv Jain, Vlad I Morariu, Jiuxiang Gu, Ani Nenkova</author><pubDate>Wed, 25 Oct 2023 18:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16790v1</guid></item><item><title>Detecting Pretraining Data from Large Language Models</title><link>http://arxiv.org/abs/2310.16789v1</link><description>Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to two real-world scenarios, copyrighted bookdetection, and contaminated downstream example detection, and find it aconsistently effective solution.</description><author>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer</author><pubDate>Wed, 25 Oct 2023 18:21:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16789v1</guid></item><item><title>The GOOSE Dataset for Perception in Unstructured Environments</title><link>http://arxiv.org/abs/2310.16788v1</link><description>The potential for deploying autonomous systems can be significantly increasedby improving the perception and interpretation of the environment. However, thedevelopment of deep learning-based techniques for autonomous systems inunstructured outdoor environments poses challenges due to limited dataavailability for training and testing. To address this gap, we present theGerman Outdoor and Offroad Dataset (GOOSE), a comprehensive datasetspecifically designed for unstructured outdoor environments. The GOOSE datasetincorporates 10 000 labeled pairs of images and point clouds, which areutilized to train a range of state-of-the-art segmentation models on both imageand point cloud data. We open source the dataset, along with an ontology forunstructured terrain, as well as dataset standards and guidelines. Thisinitiative aims to establish a common framework, enabling the seamlessinclusion of existing datasets and a fast way to enhance the perceptioncapabilities of various robots operating in unstructured environments. Thedataset, pre-trained models for offroad perception, and additionaldocumentation can be found at https://goose-dataset.de/.</description><author>Peter Mortimer, Raphael Hagmanns, Miguel Granero, Thorsten Luettel, Janko Petereit, Hans-Joachim Wuensche</author><pubDate>Wed, 25 Oct 2023 18:20:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16788v1</guid></item><item><title>The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing &amp; Attribution in AI</title><link>http://arxiv.org/abs/2310.16787v1</link><description>The race to train language models on vast, diverse, and inconsistentlydocumented datasets has raised pressing concerns about the legal and ethicalrisks for practitioners. To remedy these practices threatening datatransparency and understanding, we convene a multi-disciplinary effort betweenlegal and machine learning experts to systematically audit and trace 1800+ textdatasets. We develop tools and standards to trace the lineage of thesedatasets, from their source, creators, series of license conditions,properties, and subsequent use. Our landscape analysis highlights the sharpdivides in composition and focus of commercially open vs closed datasets, withclosed datasets monopolizing important categories: lower resource languages,more creative tasks, richer topic variety, newer and more synthetic trainingdata. This points to a deepening divide in the types of data that are madeavailable under different license conditions, and heightened implications forjurisdictional legal interpretations of copyright and fair use. We also observefrequent miscategorization of licenses on widely used dataset hosting sites,with license omission of 72%+ and error rates of 50%+. This points to a crisisin misattribution and informed use of the most popular datasets driving manyrecent breakthroughs. As a contribution to ongoing improvements in datasettransparency and responsible use, we release our entire audit, with aninteractive UI, the Data Provenance Explorer, which allows practitioners totrace and filter on data provenance for the most popular open source finetuningdata collections: www.dataprovenance.org.</description><author>Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi, Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, Deb Roy, Sara Hooker</author><pubDate>Wed, 25 Oct 2023 18:20:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16787v1</guid></item><item><title>The Simplest Inflationary Potentials</title><link>http://arxiv.org/abs/2310.16786v1</link><description>Inflation is a highly favoured theory for the early Universe. It iscompatible with current observations of the cosmic microwave background andlarge scale structure and is a driver in the quest to detect primordialgravitational waves. It is also, given the current quality of the data, highlyunder-determined with a large number of candidate implementations. We use a newmethod in symbolic regression to generate all possible simple scalar fieldpotentials for one of two possible basis sets of operators. Treating these assingle-field, slow-roll inflationary models we then score them with aninformation-theoretic metric ("minimum description length") that quantifiestheir efficiency in compressing the information in the Planck data. We exploretwo possible priors on the parameter space of potentials, one related to thefunctions' structural complexity and one that uses a Katz back-off languagemodel to prefer functions that may be theoretically motivated. This enables usto identify the inflaton potentials that optimally balance simplicity withaccuracy at explaining the Planck data, which may subsequently find theoreticalmotivation. Our exploratory study opens the door to extraction of fundamentalphysics directly from data, and may be augmented with more refined theoreticalpriors in the quest for a complete understanding of the early Universe.</description><author>Tomás Sousa, Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira</author><pubDate>Wed, 25 Oct 2023 18:20:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16786v1</guid></item><item><title>S$^3$-TTA: Scale-Style Selection for Test-Time Augmentation in Biomedical Image Segmentation</title><link>http://arxiv.org/abs/2310.16783v1</link><description>Deep-learning models have been successful in biomedical image segmentation.To generalize for real-world deployment, test-time augmentation (TTA) methodsare often used to transform the test image into different versions that arehopefully closer to the training domain. Unfortunately, due to the vastdiversity of instance scale and image styles, many augmented test imagesproduce undesirable results, thus lowering the overall performance. This workproposes a new TTA framework, S$^3$-TTA, which selects the suitable image scaleand style for each test image based on a transformation consistency metric. Inaddition, S$^3$-TTA constructs an end-to-end augmentation-segmentationjoint-training pipeline to ensure a task-oriented augmentation. On publicbenchmarks for cell and lung segmentation, S$^3$-TTA demonstrates improvementsover the prior art by 3.4% and 1.3%, respectively, by simply augmenting theinput data in testing phase.</description><author>Kangxian Xie, Siyu Huang, Sebastian Cajas Ordone, Hanspeter Pfister, Donglai Wei</author><pubDate>Wed, 25 Oct 2023 18:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16783v1</guid></item><item><title>Kiki or Bouba? Sound Symbolism in Vision-and-Language Models</title><link>http://arxiv.org/abs/2310.16781v1</link><description>Although the mapping between sound and meaning in human language is assumedto be largely arbitrary, research in cognitive science has shown that there arenon-trivial correlations between particular sounds and meanings acrosslanguages and demographic groups, a phenomenon known as sound symbolism. Amongthe many dimensions of meaning, sound symbolism is particularly salient andwell-demonstrated with regards to cross-modal associations between language andthe visual domain. In this work, we address the question of whether soundsymbolism is reflected in vision-and-language models such as CLIP and StableDiffusion. Using zero-shot knowledge probing to investigate the inherentknowledge of these models, we find strong evidence that they do show thispattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Ourwork provides a novel method for demonstrating sound symbolism andunderstanding its nature using computational tools. Our code will be madepublicly available.</description><author>Morris Alper, Hadar Averbuch-Elor</author><pubDate>Wed, 25 Oct 2023 18:15:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16781v1</guid></item><item><title>Multi-scale Diffusion Denoised Smoothing</title><link>http://arxiv.org/abs/2310.16779v1</link><description>Along with recent diffusion models, randomized smoothing has become one of afew tangible approaches that offers adversarial robustness to models at scale,e.g., those of large pre-trained models. Specifically, one can performrandomized smoothing on any classifier via a simple "denoise-and-classify"pipeline, so-called denoised smoothing, given that an accurate denoiser isavailable - such as diffusion model. In this paper, we investigate thetrade-off between accuracy and certified robustness of denoised smoothing: forexample, we question on which representation of diffusion model would maximizethe certified robustness of denoised smoothing. We consider a new objectivethat aims collective robustness of smoothed classifiers across multiple noiselevels at a shared diffusion model, which also suggests a new way to compensatethe cost of accuracy in randomized smoothing for its certified robustness. Thisobjective motivates us to fine-tune diffusion model (a) to perform consistentdenoising whenever the original image is recoverable, but (b) to generaterather diverse outputs otherwise. Our experiments show that this fine-tuningscheme of diffusion models combined with the multi-scale smoothing enables astrong certified robustness possible at highest noise level while maintainingthe accuracy closer to non-smoothed classifiers.</description><author>Jongheon Jeong, Jinwoo Shin</author><pubDate>Wed, 25 Oct 2023 18:11:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16779v1</guid></item><item><title>Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model</title><link>http://arxiv.org/abs/2309.09357v3</link><description>Despite the plethora of telehealth applications to assist home-based olderadults and healthcare providers, basic messaging and phone calls are still themost common communication methods, which suffer from limited availability,information loss, and process inefficiencies. One promising solution tofacilitate patient-provider communication is to leverage large language models(LLMs) with their powerful natural conversation and summarization capability.However, there is a limited understanding of LLMs' role during thecommunication. We first conducted two interview studies with both older adults(N=10) and healthcare providers (N=9) to understand their needs andopportunities for LLMs in patient-provider asynchronous communication. Based onthe insights, we built an LLM-powered communication system, Talk2Care, anddesigned interactive components for both groups: (1) For older adults, weleveraged the convenience and accessibility of voice assistants (VAs) and builtan LLM-powered VA interface for effective information collection. (2) Forhealth providers, we built an LLM-based dashboard to summarize and presentimportant health information based on older adults' conversations with the VA.We further conducted two user studies with older adults and providers toevaluate the usability of the system. The results showed that Talk2Care couldfacilitate the communication process, enrich the health information collectedfrom older adults, and considerably save providers' efforts and time. Weenvision our work as an initial exploration of LLMs' capability in theintersection of healthcare and interpersonal communication.</description><author>Ziqi Yang, Xuhai Xu, Bingsheng Yao, Shao Zhang, Ethan Rogers, Stephen Intille, Nawar Shara, Guodong Gordon Gao, Dakuo Wang</author><pubDate>Wed, 25 Oct 2023 18:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09357v3</guid></item><item><title>MixerFlow for Image Modelling</title><link>http://arxiv.org/abs/2310.16777v1</link><description>Normalising flows are statistical models that transform a complex densityinto a simpler density through the use of bijective transformations enablingboth density estimation and data generation from a single model. In the contextof image modelling, the predominant choice has been the Glow-basedarchitecture, whereas alternative architectures remain largely unexplored inthe research community. In this work, we propose a novel architecture calledMixerFlow, based on the MLP-Mixer architecture, further unifying the generativeand discriminative modelling architectures. MixerFlow offers an effectivemechanism for weight sharing for flow-based models. Our results demonstratebetter density estimation on image datasets under a fixed computational budgetand scales well as the image resolution increases, making MixeFlow a powerfulyet simple alternative to the Glow-based architectures. We also show thatMixerFlow provides more informative embeddings than Glow-based architectures.</description><author>Eshant English, Matthias Kirchler, Christoph Lippert</author><pubDate>Wed, 25 Oct 2023 18:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16777v1</guid></item><item><title>DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection</title><link>http://arxiv.org/abs/2310.16776v1</link><description>Recent advances have led to the availability of many pre-trained languagemodels (PLMs); however, a question that remains is how much data is trulyneeded to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,a data-efficient fine-tuning framework that leverages unsupervised core-setselection to minimize the amount of data needed to fine-tune PLMs fordownstream tasks. We demonstrate the efficacy of our DEFT framework in thecontext of text-editing LMs, and compare to the state-of-the art text-editingmodel, CoEDIT (Raheja et al., 2023). Our quantitative and qualitative resultsdemonstrate that DEFT models are just as accurate as CoEDIT while beingfinetuned on ~70% less data.</description><author>Devleena Das, Vivek Khetan</author><pubDate>Wed, 25 Oct 2023 18:06:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16776v1</guid></item><item><title>Estimating Class Separability of Datasets Using Persistent Homology with Application to LLM Fine-Tuning</title><link>http://arxiv.org/abs/2305.15016v3</link><description>This paper proposes a method to estimate the class separability of anunlabeled text dataset by inspecting the topological characteristics ofsentence-transformer embeddings of the text. Experiments conducted involve bothbinary and multi-class cases, with balanced and imbalanced scenarios. Theresults demonstrate a clear correlation and a better consistency between theproposed method and other separability and classification metrics, such asThornton's method and the AUC score of a logistic regression classifier, aswell as unsupervised methods. Finally, we empirically show that the proposedmethod can be part of a stopping criterion for fine-tuning language-modelclassifiers. By monitoring the class separability of the embedding space aftereach training iteration, we can detect when the training process stopsimproving the separability of the embeddings without using additional labels.</description><author>Najah Ghalyan, Kostis Gourgoulias, Yash Satsangi, Sean Moran, Maxime Labonne, Joseph Sabelja</author><pubDate>Wed, 25 Oct 2023 18:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15016v3</guid></item><item><title>AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2310.16772v1</link><description>In urban planning, land use readjustment plays a pivotal role in aligningland use configurations with the current demands for sustainable urbandevelopment. However, present-day urban planning practices face two mainissues. Firstly, land use decisions are predominantly dependent on humanexperts. Besides, while resident engagement in urban planning can promote urbansustainability and livability, it is challenging to reconcile the diverseinterests of stakeholders. To address these challenges, we introduce aConsensus-based Multi-Agent Reinforcement Learning framework for real-worldland use readjustment. This framework serves participatory urban planning,allowing diverse intelligent agents as stakeholder representatives to vote forpreferred land use types. Within this framework, we propose a novel consensusmechanism in reward design to optimize land utilization through collectivedecision making. To abstract the structure of the complex urban system, thegeographic information of cities is transformed into a spatial graph structureand then processed by graph neural networks. Comprehensive experiments on bothtraditional top-down planning and participatory planning methods fromreal-world communities indicate that our computational framework enhancesglobal benefits and accommodates diverse interests, leading to improvedsatisfaction across different demographic groups. By integrating Multi-AgentReinforcement Learning, our framework ensures that participatory urban planningdecisions are more dynamic and adaptive to evolving community needs andprovides a robust platform for automating complex real-world urban planningprocesses.</description><author>Kejiang Qian, Lingjun Mao, Xin Liang, Yimin Ding, Jin Gao, Xinran Wei, Ziyi Guo, Jiajie Li</author><pubDate>Wed, 25 Oct 2023 18:04:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16772v1</guid></item><item><title>ConvNets Match Vision Transformers at Scale</title><link>http://arxiv.org/abs/2310.16764v1</link><description>Many researchers believe that ConvNets perform well on small or moderatelysized datasets, but are not competitive with Vision Transformers when givenaccess to datasets on the web-scale. We challenge this belief by evaluating aperformant ConvNet architecture pre-trained on JFT-4B, a large labelled datasetof images often used for training foundation models. We consider pre-trainingcompute budgets between 0.4k and 110k TPU-v4 core compute hours, and train aseries of networks of increasing depth and width from the NFNet model family.We observe a log-log scaling law between held out loss and compute budget.After fine-tuning on ImageNet, NFNets match the reported performance of VisionTransformers with comparable compute budgets. Our strongest fine-tuned modelachieves a Top-1 accuracy of 90.4%.</description><author>Samuel L. Smith, Andrew Brock, Leonard Berrada, Soham De</author><pubDate>Wed, 25 Oct 2023 17:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16764v1</guid></item><item><title>SuperHF: Supervised Iterative Learning from Human Feedback</title><link>http://arxiv.org/abs/2310.16763v1</link><description>While large language models demonstrate remarkable capabilities, they oftenpresent challenges in terms of safety, alignment with human values, andstability during training. Here, we focus on two prevalent methods used toalign these models, Supervised Fine-Tuning (SFT) and Reinforcement Learningfrom Human Feedback (RLHF). SFT is simple and robust, powering a host ofopen-source models, while RLHF is a more sophisticated method used in top-tiermodels like ChatGPT but also suffers from instability and susceptibility toreward hacking. We propose a novel approach, Supervised Iterative Learning fromHuman Feedback (SuperHF), which seeks to leverage the strengths of bothmethods. Our hypothesis is two-fold: that the reward model used in RLHF iscritical for efficient data use and model generalization and that the use ofProximal Policy Optimization (PPO) in RLHF may not be necessary and couldcontribute to instability issues. SuperHF replaces PPO with a simple supervisedloss and a Kullback-Leibler (KL) divergence prior. It creates its own trainingdata by repeatedly sampling a batch of model outputs and filtering them throughthe reward model in an online learning regime. We then break down the rewardoptimization problem into three components: robustly optimizing the trainingrewards themselves, preventing reward hacking-exploitation of the reward modelthat degrades model performance-as measured by a novel METEOR similaritymetric, and maintaining good performance on downstream evaluations. Ourexperimental results show SuperHF exceeds PPO-based RLHF on the trainingobjective, easily and favorably trades off high reward with low reward hacking,improves downstream calibration, and performs the same on our GPT-4 basedqualitative evaluation scheme all the while being significantly simpler toimplement, highlighting SuperHF's potential as a competitive language modelalignment technique.</description><author>Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim, Gitta Kutyniok, Kush Bhatia, Silas Alberti</author><pubDate>Wed, 25 Oct 2023 17:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16763v1</guid></item><item><title>IntenDD: A Unified Contrastive Learning Approach for Intent Detection and Discovery</title><link>http://arxiv.org/abs/2310.16761v1</link><description>Identifying intents from dialogue utterances forms an integral component oftask-oriented dialogue systems. Intent-related tasks are typically formulatedeither as a classification task, where the utterances are classified intopredefined categories or as a clustering task when new and previously unknownintent categories need to be discovered from these utterances. Further, theintent classification may be modeled in a multiclass (MC) or multilabel (ML)setup. While typically these tasks are modeled as separate tasks, we proposeIntenDD, a unified approach leveraging a shared utterance encoding backbone.IntenDD uses an entirely unsupervised contrastive learning strategy forrepresentation learning, where pseudo-labels for the unlabeled utterances aregenerated based on their lexical features. Additionally, we introduce atwo-step post-processing setup for the classification tasks using modifiedadsorption. Here, first, the residuals in the training data are propagatedfollowed by smoothing the labels both modeled in a transductive setting.Through extensive evaluations on various benchmark datasets, we find that ourapproach consistently outperforms competitive baselines across all three tasks.On average, IntenDD reports percentage improvements of 2.32%, 1.26%, and 1.52%in their respective metrics for few-shot MC, few-shot ML, and the intentdiscovery tasks respectively.</description><author>Bhavuk Singhal, Ashim Gupta, Shivasankaran V P, Amrith Krishna</author><pubDate>Wed, 25 Oct 2023 17:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16761v1</guid></item><item><title>HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2310.16755v1</link><description>Theory of Mind (ToM) is the ability to reason about one's own and others'mental states. ToM plays a critical role in the development of intelligence,language understanding, and cognitive processes. While previous work hasprimarily focused on first and second-order ToM, we explore higher-order ToM,which involves recursive reasoning on others' beliefs. We introduce HI-TOM, aHigher Order Theory of Mind benchmark. Our experimental evaluation usingvarious Large Language Models (LLMs) indicates a decline in performance onhigher-order ToM tasks, demonstrating the limitations of current LLMs. Weconduct a thorough analysis of different failure cases of LLMs, and share ourthoughts on the implications of our findings on the future of NLP.</description><author>Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, Naihao Deng</author><pubDate>Wed, 25 Oct 2023 17:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16755v1</guid></item><item><title>Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark</title><link>http://arxiv.org/abs/2310.16044v2</link><description>We introduce Stanford-ORB, a new real-world 3D Object inverse RenderingBenchmark. Recent advances in inverse rendering have enabled a wide range ofreal-world applications in 3D content generation, moving rapidly from researchand commercial use cases to consumer devices. While the results continue toimprove, there is no real-world benchmark that can quantitatively assess andcompare the performance of various inverse rendering methods. Existingreal-world datasets typically only consist of the shape and multi-view imagesof objects, which are not sufficient for evaluating the quality of materialrecovery and object relighting. Methods capable of recovering material andlighting often resort to synthetic data for quantitative evaluation, which onthe other hand does not guarantee generalization to complex real-worldenvironments. We introduce a new dataset of real-world objects captured under avariety of natural scenes with ground-truth 3D scans, multi-view images, andenvironment lighting. Using this dataset, we establish the first comprehensivereal-world evaluation benchmark for object inverse rendering tasks fromin-the-wild scenes, and compare the performance of various existing methods.</description><author>Zhengfei Kuang, Yunzhi Zhang, Hong-Xing Yu, Samir Agarwala, Shangzhe Wu, Jiajun Wu</author><pubDate>Wed, 25 Oct 2023 17:40:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16044v2</guid></item><item><title>Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction</title><link>http://arxiv.org/abs/2306.01439v2</link><description>The limited priors required by neural networks make them the dominatingchoice to encode and learn policies using reinforcement learning (RL). However,they are also black-boxes, making it hard to understand the agent's behaviour,especially when working on the image level. Therefore, neuro-symbolic RL aimsat creating policies that are interpretable in the first place. Unfortunately,interpretability is not explainability. To achieve both, we introduce NeurallygUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neuralnetwork-based agents to guide the search of candidate-weighted logic rules,then uses differentiable logic to train the logic agents. Our experimentalevaluation demonstrates that NUDGE agents can induce interpretable andexplainable policies while outperforming purely neural ones and showing goodflexibility to environments of different initial states and problem sizes.</description><author>Quentin Delfosse, Hikaru Shindo, Devendra Dhami, Kristian Kersting</author><pubDate>Wed, 25 Oct 2023 17:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01439v2</guid></item><item><title>CAD -- Contextual Multi-modal Alignment for Dynamic AVQA</title><link>http://arxiv.org/abs/2310.16754v1</link><description>In the context of Audio Visual Question Answering (AVQA) tasks, the audiovisual modalities could be learnt on three levels: 1) Spatial, 2) Temporal, and3) Semantic. Existing AVQA methods suffer from two major shortcomings; theaudio-visual (AV) information passing through the network isn't aligned onSpatial and Temporal levels; and, inter-modal (audio and visual) Semanticinformation is often not balanced within a context; this results in poorperformance. In this paper, we propose a novel end-to-end ContextualMulti-modal Alignment (CAD) network that addresses the challenges in AVQAmethods by i) introducing a parameter-free stochastic Contextual block thatensures robust audio and visual alignment on the Spatial level; ii) proposing apre-training technique for dynamic audio and visual alignment on Temporal levelin a self-supervised setting, and iii) introducing a cross-attention mechanismto balance audio and visual information on Semantic level. The proposed novelCAD network improves the overall performance over the state-of-the-art methodson average by 9.4% on the MUSIC-AVQA dataset. We also demonstrate that ourproposed contributions to AVQA can be added to the existing methods to improvetheir performance without additional complexity requirements.</description><author>Asmar Nadeem, Adrian Hilton, Robert Dawes, Graham Thomas, Armin Mustafa</author><pubDate>Wed, 25 Oct 2023 17:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16754v1</guid></item><item><title>PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction</title><link>http://arxiv.org/abs/2310.16753v1</link><description>Email is a widely used tool for business communication, and email marketinghas emerged as a cost-effective strategy for enterprises. While previousstudies have examined factors affecting email marketing performance, limitedresearch has focused on understanding email response behavior by consideringemail content and metadata. This study proposes a Prototype-based Multi-viewNetwork (PROMINET) that incorporates semantic and structural information fromemail data. By utilizing prototype learning, the PROMINET model generateslatent exemplars, enabling interpretable email response prediction. The modelmaps learned semantic and structural exemplars to observed samples in thetraining data at different levels of granularity, such as document, sentence,or phrase. The approach is evaluated on two real-world email datasets: theEnron corpus and an in-house Email Marketing corpus. Experimental resultsdemonstrate that the PROMINET model outperforms baseline models, achieving a~3% improvement in F1 score on both datasets. Additionally, the model providesinterpretability through prototypes at different granularity levels whilemaintaining comparable performance to non-interpretable models. The learnedprototypes also show potential for generating suggestions to enhance email textediting and improve the likelihood of effective email responses. This researchcontributes to enhancing sender-receiver communication and customer engagementin email interactions.</description><author>Yuqing Wang, Prashanth Vijayaraghavan, Ehsan Degan</author><pubDate>Wed, 25 Oct 2023 17:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16753v1</guid></item><item><title>Simple, Scalable and Effective Clustering via One-Dimensional Projections</title><link>http://arxiv.org/abs/2310.16752v1</link><description>Clustering is a fundamental problem in unsupervised machine learning withmany applications in data analysis. Popular clustering algorithms such asLloyd's algorithm and $k$-means++ can take $\Omega(ndk)$ time when clustering$n$ points in a $d$-dimensional space (represented by an $n\times d$ matrix$X$) into $k$ clusters. In applications with moderate to large $k$, themultiplicative $k$ factor can become very expensive. We introduce a simplerandomized clustering algorithm that provably runs in expected time$O(\mathrm{nnz}(X) + n\log n)$ for arbitrary $k$. Here $\mathrm{nnz}(X)$ is thetotal number of non-zero entries in the input dataset $X$, which is upperbounded by $nd$ and can be significantly smaller for sparse datasets. We provethat our algorithm achieves approximation ratio $\smash{\widetilde{O}(k^4)}$ onany input dataset for the $k$-means objective. We also believe that ourtheoretical analysis is of independent interest, as we show that theapproximation ratio of a $k$-means algorithm is approximately preserved under aclass of projections and that $k$-means++ seeding can be implemented inexpected $O(n \log n)$ time in one dimension. Finally, we show experimentallythat our clustering algorithm gives a new tradeoff between running time andcluster quality compared to previous state-of-the-art methods for these tasks.</description><author>Moses Charikar, Monika Henzinger, Lunjia Hu, Maxmilian Vötsch, Erik Waingarten</author><pubDate>Wed, 25 Oct 2023 17:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16752v1</guid></item><item><title>A Vulnerability of Attribution Methods Using Pre-Softmax Scores</title><link>http://arxiv.org/abs/2307.03305v2</link><description>We discuss a vulnerability involving a category of attribution methods usedto provide explanations for the outputs of convolutional neural networksworking as classifiers. It is known that this type of networks are vulnerableto adversarial attacks, in which imperceptible perturbations of the input mayalter the outputs of the model. In contrast, here we focus on effects thatsmall modifications in the model may cause on the attribution method withoutaltering the model outputs.</description><author>Miguel Lerma, Mirtha Lucas</author><pubDate>Wed, 25 Oct 2023 17:35:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03305v2</guid></item><item><title>Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots</title><link>http://arxiv.org/abs/2310.16750v1</link><description>In this work, we address the problem of real-time dense depth estimation frommonocular images for mobile underwater vehicles. We formulate a deep learningmodel that fuses sparse depth measurements from triangulated features toimprove the depth predictions and solve the problem of scale ambiguity. Toallow prior inputs of arbitrary sparsity, we apply a dense parameterizationmethod. Our model extends recent state-of-the-art approaches to monocular imagebased depth estimation, using an efficient encoder-decoder backbone and modernlightweight transformer optimization stage to encode global context. Thenetwork is trained in a supervised fashion on the forward-looking underwaterdataset, FLSea. Evaluation results on this dataset demonstrate significantimprovement in depth prediction accuracy by the fusion of the sparse featurepriors. In addition, without any retraining, our method achieves similar depthprediction accuracy on a downward looking dataset we collected with a diveroperated camera rig, conducting a survey of a coral reef. The method achievesreal-time performance, running at 160 FPS on a laptop GPU and 7 FPS on a singleCPU core and is suitable for direct deployment on embedded systems. Theimplementation of this work is made publicly available athttps://github.com/ebnerluca/uw_depth.</description><author>Luca Ebner, Gideon Billings, Stefan Williams</author><pubDate>Wed, 25 Oct 2023 17:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16750v1</guid></item><item><title>DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages</title><link>http://arxiv.org/abs/2310.16749v1</link><description>Disfluency correction (DC) is the process of removing disfluent elements likefillers, repetitions and corrections from spoken utterances to create readableand interpretable text. DC is a vital post-processing step applied to AutomaticSpeech Recognition (ASR) outputs, before subsequent processing by downstreamlanguage understanding tasks. Existing DC research has primarily focused onEnglish due to the unavailability of large-scale open-source datasets. Towardsthe goal of multilingual disfluency correction, we present a high-qualityhuman-annotated DC corpus covering four important Indo-European languages:English, Hindi, German and French. We provide extensive analysis of results ofstate-of-the-art DC models across all four languages obtaining F1 scores of97.55 (English), 94.29 (Hindi), 95.89 (German) and 92.97 (French). Todemonstrate the benefits of DC on downstream tasks, we show that DC leads to5.65 points increase in BLEU scores on average when used in conjunction with astate-of-the-art Machine Translation (MT) system. We release code to run ourexperiments along with our annotated dataset here.</description><author>Vineet Bhat, Preethi Jyothi, Pushpak Bhattacharyya</author><pubDate>Wed, 25 Oct 2023 17:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16749v1</guid></item><item><title>HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis</title><link>http://arxiv.org/abs/2310.16746v1</link><description>Authorship Analysis, also known as stylometry, has been an essential aspectof Natural Language Processing (NLP) for a long time. Likewise, the recentadvancement of Large Language Models (LLMs) has made authorship analysisincreasingly crucial for distinguishing between human-written and AI-generatedtexts. However, these authorship analysis tasks have primarily been focused onwritten texts, not considering spoken texts. Thus, we introduce the largestbenchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark).HANSEN encompasses meticulous curation of existing speech datasets accompaniedby transcripts, alongside the creation of novel AI-generated spoken textdatasets. Together, it comprises 17 human datasets, and AI-generated spokentexts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. Toevaluate and demonstrate the utility of HANSEN, we perform AuthorshipAttribution (AA) &amp; Author Verification (AV) on human-spoken datasets andconducted Human vs. AI spoken text detection using state-of-the-art (SOTA)models. While SOTA methods, such as, character ngram or Transformer-basedmodel, exhibit similar AA &amp; AV performance in human-spoken datasets compared towritten ones, there is much room for improvement in AI-generated spoken textdetection. The HANSEN benchmark is available at:https://huggingface.co/datasets/HANSEN-REPO/HANSEN.</description><author>Nafis Irtiza Tripto, Adaku Uchendu, Thai Le, Mattia Setzu, Fosca Giannotti, Dongwon Lee</author><pubDate>Wed, 25 Oct 2023 17:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16746v1</guid></item><item><title>Interferometric Neural Networks</title><link>http://arxiv.org/abs/2310.16742v1</link><description>On the one hand, artificial neural networks have many successful applicationsin the field of machine learning and optimization. On the other hand,interferometers are integral parts of any field that deals with waves such asoptics, astronomy, and quantum physics. Here, we introduce neural networkscomposed of interferometers and then build generative adversarial networks fromthem. Our networks do not have any classical layer and can be realized onquantum computers or photonic chips. We demonstrate their applicability forcombinatorial optimization, image classification, and image generation. Forcombinatorial optimization, our network consistently converges to the globaloptimum or remains within a narrow range of it. In multi-class imageclassification tasks, our networks achieve accuracies of 93% and 83%. Lastly,we show their capability to generate images of digits from 0 to 9 as well ashuman faces.</description><author>Arun Sehrawat</author><pubDate>Wed, 25 Oct 2023 17:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16742v1</guid></item><item><title>Stochastic Latent Transformer: Efficient Modelling of Stochastically Forced Zonal Jets</title><link>http://arxiv.org/abs/2310.16741v1</link><description>We introduce the 'Stochastic Latent Transformer', a probabilistic deeplearning approach for efficient reduced-order modelling of stochastic partialdifferential equations (SPDEs). Despite recent advances in deep learning forfluid mechanics, limited research has explored modelling stochastically drivenflows - which play a crucial role in understanding a broad spectrum ofphenomena, from jets on giant planets to ocean circulation and the variabilityof midlatitude weather. The model architecture consists of astochastically-forced transformer, paired with a translation-equivariantautoencoder, that we demonstrate is capable of reproducing system dynamicsacross various integration periods. We demonstrate its effectiveness applied toa well-researched zonal jet system, with the neural network achieving afive-order-of-magnitude speedup compared to numerical integration. Thisfacilitates the cost-effective generation of large ensembles, enabling theexploration of statistical questions concerning probabilities of spontaneoustransition events.</description><author>Ira J. S. Shokar, Rich R. Kerswell, Peter H. Haynes</author><pubDate>Wed, 25 Oct 2023 17:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16741v1</guid></item><item><title>Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation</title><link>http://arxiv.org/abs/2310.16738v1</link><description>Conversational Recommendation System (CRS) is a rapidly growing research areathat has gained significant attention alongside advancements in languagemodelling techniques. However, the current state of conversationalrecommendation faces numerous challenges due to its relative novelty andlimited existing contributions. In this study, we delve into benchmark datasetsfor developing CRS models and address potential biases arising from thefeedback loop inherent in multi-turn interactions, including selection bias andmultiple popularity bias variants. Drawing inspiration from the success ofgenerative data via using language models and data augmentation techniques, wepresent two novel strategies, 'Once-Aug' and 'PopNudge', to enhance modelperformance while mitigating biases. Through extensive experiments on ReDialand TG-ReDial benchmark datasets, we show a consistent improvement of CRStechniques with our data augmentation approaches and offer additional insightson addressing multiple newly formulated biases.</description><author>Xi Wang, Hossein A. Rahmani, Jiqun Liu, Emine Yilmaz</author><pubDate>Wed, 25 Oct 2023 17:11:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16738v1</guid></item><item><title>Translating Universal Scene Descriptions into Knowledge Graphs for Robotic Environment</title><link>http://arxiv.org/abs/2310.16737v1</link><description>Robots performing human-scale manipulation tasks require an extensive amountof knowledge about their surroundings in order to perform their actionscompetently and human-like. In this work, we investigate the use of virtualreality technology as an implementation for robot environment modeling, andpresent a technique for translating scene graphs into knowledge bases. To thisend, we take advantage of the Universal Scene Description (USD) format which isan emerging standard for the authoring, visualization and simulation of complexenvironments. We investigate the conversion of USD-based environment modelsinto Knowledge Graph (KG) representations that facilitate semantic querying andintegration with additional knowledge sources.</description><author>Giang Hoang Nguyen, Daniel Bessler, Simon Stelter, Mihai Pomarlan, Michael Beetz</author><pubDate>Wed, 25 Oct 2023 17:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16737v1</guid></item><item><title>(S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability</title><link>http://arxiv.org/abs/2302.08982v2</link><description>In this paper, we investigate the impact of stochasticity and large stepsizeson the implicit regularisation of gradient descent (GD) and stochastic gradientdescent (SGD) over diagonal linear networks. We prove the convergence of GD andSGD with macroscopic stepsizes in an overparametrised regression setting andcharacterise their solutions through an implicit regularisation problem. Ourcrisp characterisation leads to qualitative insights about the impact ofstochasticity and stepsizes on the recovered solution. Specifically, we showthat large stepsizes consistently benefit SGD for sparse regression problems,while they can hinder the recovery of sparse solutions for GD. These effectsare magnified for stepsizes in a tight window just below the divergencethreshold, in the "edge of stability" regime. Our findings are supported byexperimental results.</description><author>Mathieu Even, Scott Pesme, Suriya Gunasekar, Nicolas Flammarion</author><pubDate>Wed, 25 Oct 2023 17:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08982v2</guid></item><item><title>Scaling Laws for Hyperparameter Optimization</title><link>http://arxiv.org/abs/2302.00441v3</link><description>Hyperparameter optimization is an important subfield of machine learning thatfocuses on tuning the hyperparameters of a chosen algorithm to achieve peakperformance. Recently, there has been a stream of methods that tackle the issueof hyperparameter optimization, however, most of the methods do not exploit thedominant power law nature of learning curves for Bayesian optimization. In thiswork, we propose Deep Power Laws (DPL), an ensemble of neural network modelsconditioned to yield predictions that follow a power-law scaling pattern. Ourmethod dynamically decides which configurations to pause and trainincrementally by making use of gray-box evaluations. We compare our methodagainst 7 state-of-the-art competitors on 3 benchmarks related to tabular,image, and NLP datasets covering 59 diverse tasks. Our method achieves the bestresults across all benchmarks by obtaining the best any-time results comparedto all competitors.</description><author>Arlind Kadra, Maciej Janowski, Martin Wistuba, Josif Grabocka</author><pubDate>Wed, 25 Oct 2023 17:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00441v3</guid></item><item><title>Mapping the Empirical Evidence of the GDPR (In-)Effectiveness: A Systematic Review</title><link>http://arxiv.org/abs/2310.16735v1</link><description>In the realm of data protection, a striking disconnect prevails betweentraditional domains of doctrinal, legal, theoretical, and policy-basedinquiries and a burgeoning body of empirical evidence. Much of the scholarlyand regulatory discourse remains entrenched in abstract legal principles ornormative frameworks, leaving the empirical landscape uncharted or minimallyengaged. Since the birth of EU data protection law, a modest body of empiricalevidence has been generated but remains widely scattered and unexamined. Suchevidence offers vital insights into the perception, impact, clarity, andeffects of data protection measures but languishes on the periphery,inadequately integrated into the broader conversation. To make a meaningfulconnection, we conduct a comprehensive review and synthesis of empiricalresearch spanning nearly three decades (1995- March 2022), advocating for amore robust integration of empirical evidence into the evaluation and review ofthe GDPR, while laying a methodological foundation for future empiricalresearch.</description><author>Wenlong Li, Zihao Li, Wenkai Li, Yueming Zhang, Aolan Li</author><pubDate>Wed, 25 Oct 2023 17:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16735v1</guid></item><item><title>Saddle-to-Saddle Dynamics in Diagonal Linear Networks</title><link>http://arxiv.org/abs/2304.00488v2</link><description>In this paper we fully describe the trajectory of gradient flow over diagonallinear networks in the limit of vanishing initialisation. We show that thelimiting flow successively jumps from a saddle of the training loss to anotheruntil reaching the minimum $\ell_1$-norm solution. This saddle-to-saddledynamics translates to an incremental learning process as each saddlecorresponds to the minimiser of the loss constrained to an active set outsideof which the coordinates must be zero. We explicitly characterise the visitedsaddles as well as the jumping times through a recursive algorithm reminiscentof the LARS algorithm used for computing the Lasso path. Our proof leverages aconvenient arc-length time-reparametrisation which enables to keep track of theheteroclinic transitions between the jumps. Our analysis requires negligibleassumptions on the data, applies to both under and overparametrised settingsand covers complex cases where there is no monotonicity of the number of activecoordinates. We provide numerical experiments to support our findings.</description><author>Scott Pesme, Nicolas Flammarion</author><pubDate>Wed, 25 Oct 2023 17:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00488v2</guid></item><item><title>A No-Reference Quality Assessment Method for Digital Human Head</title><link>http://arxiv.org/abs/2310.16732v1</link><description>In recent years, digital humans have been widely applied in augmented/virtualreality (A/VR), where viewers are allowed to freely observe and interact withthe volumetric content. However, the digital humans may be degraded withvarious distortions during the procedure of generation and transmission.Moreover, little effort has been put into the perceptual quality assessment ofdigital humans. Therefore, it is urgent to carry out objective qualityassessment methods to tackle the challenge of digital human quality assessment(DHQA). In this paper, we develop a novel no-reference (NR) method based onTransformer to deal with DHQA in a multi-task manner. Specifically, the front2D projections of the digital humans are rendered as inputs and the visiontransformer (ViT) is employed for the feature extraction. Then we design amulti-task module to jointly classify the distortion types and predict theperceptual quality levels of digital humans. The experimental results show thatthe proposed method well correlates with the subjective ratings and outperformsthe state-of-the-art quality assessment methods.</description><author>Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiongkuo Min, Xianghe Ma, Guangtao Zhai</author><pubDate>Wed, 25 Oct 2023 17:01:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16732v1</guid></item><item><title>Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning</title><link>http://arxiv.org/abs/2310.16731v1</link><description>Spatial reasoning over text is challenging as the models not only need toextract the direct spatial information from the text but also reason over thoseand infer implicit spatial relations. Recent studies highlight the struggleseven large language models encounter when it comes to performing spatialreasoning over text. In this paper, we explore the potential benefits ofdisentangling the processes of information extraction and reasoning in modelsto address this challenge. To explore this, we design various models thatdisentangle extraction and reasoning(either symbolic or neural) and comparethem with state-of-the-art(SOTA) baselines with no explicit design for theseparts. Our experimental results consistently demonstrate the efficacy ofdisentangling, showcasing its ability to enhance models' generalizabilitywithin realistic data domains.</description><author>Roshanak Mirzaee, Parisa Kordjamshidi</author><pubDate>Wed, 25 Oct 2023 17:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16731v1</guid></item><item><title>Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models</title><link>http://arxiv.org/abs/2306.12747v2</link><description>Recent works have shown that line search methods can speed up StochasticGradient Descent (SGD) and Adam in modern over-parameterized settings. However,existing line searches may take steps that are smaller than necessary sincethey require a monotone decrease of the (mini-)batch objective function. Weexplore nonmonotone line search methods to relax this condition and possiblyaccept larger step sizes. Despite the lack of a monotonic decrease, we provethe same fast rates of convergence as in the monotone case. Our experimentsshow that nonmonotone methods improve the speed of convergence andgeneralization properties of SGD/Adam even beyond the previous monotone linesearches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtainedby combining a nonmonotone line search with a Polyak initial step size.Furthermore, we develop a new resetting technique that in the majority of theiterations reduces the amount of backtracks to zero while still maintaining alarge initial step size. To the best of our knowledge, a first runtimecomparison shows that the epoch-wise advantage of line-search-based methodsgets reflected in the overall computational time.</description><author>Leonardo Galli, Holger Rauhut, Mark Schmidt</author><pubDate>Wed, 25 Oct 2023 16:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12747v2</guid></item><item><title>MultiPrompter: Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2310.16730v1</link><description>Recently, there has been an increasing interest in automated promptoptimization based on reinforcement learning (RL). This approach offersimportant advantages, such as generating interpretable prompts and beingcompatible with black-box foundation models. However, the substantial promptspace size poses challenges for RL-based methods, often leading to suboptimalpolicy convergence. This paper introduces MultiPrompter, a new framework thatviews prompt optimization as a cooperative game between prompters which taketurns composing a prompt together. Our cooperative prompt optimizationeffectively reduces the problem size and helps prompters learn optimal prompts.We test our method on the text-to-image task and show its ability to generatehigher-quality images than baselines.</description><author>Dong-Ki Kim, Sungryull Sohn, Lajanugen Logeswaran, Dongsub Shim, Honglak Lee</author><pubDate>Wed, 25 Oct 2023 16:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16730v1</guid></item><item><title>On Single Index Models beyond Gaussian Data</title><link>http://arxiv.org/abs/2307.15804v2</link><description>Sparse high-dimensional functions have arisen as a rich framework to studythe behavior of gradient-descent methods using shallow neural networks,showcasing their ability to perform feature learning beyond linear models.Amongst those functions, the simplest are single-index models $f(x) = \phi( x\cdot \theta^*)$, where the labels are generated by an arbitrary non-linearscalar link function $\phi$ applied to an unknown one-dimensional projection$\theta^*$ of the input data. By focusing on Gaussian data, several recentworks have built a remarkable picture, where the so-called information exponent(related to the regularity of the link function) controls the required samplecomplexity. In essence, these tools exploit the stability and sphericalsymmetry of Gaussian distributions. In this work, building from the frameworkof \cite{arous2020online}, we explore extensions of this picture beyond theGaussian setting, where both stability or symmetry might be violated. Focusingon the planted setting where $\phi$ is known, our main results establish thatStochastic Gradient Descent can efficiently recover the unknown direction$\theta^*$ in the high-dimensional regime, under assumptions that extendprevious works \cite{yehudai2020learning,wu2022learning}.</description><author>Joan Bruna, Loucas Pillaud-Vivien, Aaron Zweig</author><pubDate>Wed, 25 Oct 2023 16:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15804v2</guid></item><item><title>Implicit Two-Tower Policies</title><link>http://arxiv.org/abs/2208.01191v2</link><description>We present a new class of structured reinforcement learningpolicy-architectures, Implicit Two-Tower (ITT) policies, where the actions arechosen based on the attention scores of their learnable latent representationswith those of the input states. By explicitly disentangling action from stateprocessing in the policy stack, we achieve two main goals: substantialcomputational gains and better performance. Our architectures are compatiblewith both: discrete and continuous action spaces. By conducting tests on 15environments from OpenAI Gym and DeepMind Control Suite, we show thatITT-architectures are particularly suited for blackbox/evolutionaryoptimization and the corresponding policy training algorithms outperform theirvanilla unstructured implicit counterparts as well as commonly used explicitpolicies. We complement our analysis by showing how techniques such as hashingand lazy tower updates, critically relying on the two-tower structure of ITTs,can be applied to obtain additional computational improvements.</description><author>Yunfan Zhao, Qingkai Pan, Krzysztof Choromanski, Deepali Jain, Vikas Sindhwani</author><pubDate>Wed, 25 Oct 2023 16:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.01191v2</guid></item><item><title>AI Hazard Management: A framework for the systematic management of root causes for AI risks</title><link>http://arxiv.org/abs/2310.16727v1</link><description>Recent advancements in the field of Artificial Intelligence (AI) establishthe basis to address challenging tasks. However, with the integration of AI,new risks arise. Therefore, to benefit from its advantages, it is essential toadequately handle the risks associated with AI. Existing risk managementprocesses in related fields, such as software systems, need to sufficientlyconsider the specifics of AI. A key challenge is to systematically andtransparently identify and address AI risks' root causes - also called AIhazards. This paper introduces the AI Hazard Management (AIHM) framework, whichprovides a structured process to systematically identify, assess, and treat AIhazards. The proposed process is conducted in parallel with the development toensure that any AI hazard is captured at the earliest possible stage of the AIsystem's life cycle. In addition, to ensure the AI system's auditability, theproposed framework systematically documents evidence that the potential impactof identified AI hazards could be reduced to a tolerable level. The frameworkbuilds upon an AI hazard list from a comprehensive state-of-the-art analysis.Also, we provide a taxonomy that supports the optimal treatment of theidentified AI hazards. Additionally, we illustrate how the AIHM framework canincrease the overall quality of a power grid AI use case by systematicallyreducing the impact of identified hazards to an acceptable level.</description><author>Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner</author><pubDate>Wed, 25 Oct 2023 16:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16727v1</guid></item><item><title>Are GATs Out of Balance?</title><link>http://arxiv.org/abs/2310.07235v2</link><description>While the expressive power and computational capabilities of graph neuralnetworks (GNNs) have been theoretically studied, their optimization andlearning dynamics, in general, remain largely unexplored. Our study undertakesthe Graph Attention Network (GAT), a popular GNN architecture in which a node'sneighborhood aggregation is weighted by parameterized attention coefficients.We derive a conservation law of GAT gradient flow dynamics, which explains whya high portion of parameters in GATs with standard initialization struggle tochange during training. This effect is amplified in deeper GATs, which performsignificantly worse than their shallow counterparts. To alleviate this problem,we devise an initialization scheme that balances the GAT network. Our approachi) allows more effective propagation of gradients and in turn enablestrainability of deeper networks, and ii) attains a considerable speedup intraining and convergence time in comparison to the standard initialization. Ourmain theorem serves as a stepping stone to studying the learning dynamics ofpositive homogeneous models with attention mechanisms.</description><author>Nimrah Mustafa, Aleksandar Bojchevski, Rebekka Burkholz</author><pubDate>Wed, 25 Oct 2023 16:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07235v2</guid></item><item><title>Rebuild City Buildings from Off-Nadir Aerial Images with Offset-Building Model (OBM)</title><link>http://arxiv.org/abs/2310.16717v1</link><description>Accurate measurement of the offset from roof-to-footprint invery-high-resolution remote sensing imagery is crucial for urban informationextraction tasks. With the help of deep learning, existing methods typicallyrely on two-stage CNN models to extract regions of interest on building featuremaps. At the first stage, a Region Proposal Network (RPN) is applied to extractthousands of ROIs (Region of Interests) which will post-imported into aRegion-based Convolutional Neural Networks (RCNN) to extract wantedinformation. However, because of inflexible RPN, these methods often lackeffective user interaction, encounter difficulties in instance correspondence,and struggle to keep up with the advancements in general artificialintelligence. This paper introduces an interactive Transformer model combinedwith a prompt encoder to precisely extract building segmentation as well as theoffset vectors from roofs to footprints. In our model, a powerful module,namely ROAM, was tailored for common problems in predicting roof-to-footprintoffsets. We tested our model's feasibility on the publicly available BONAIdataset, achieving a significant reduction in Prompt-Instance-Level offseterrors ranging from 14.6% to 16.3%. Additionally, we developed a Distance-NMSalgorithm tailored for large-scale building offsets, significantly enhancingthe accuracy of predicted building offset angles and lengths in astraightforward and efficient manner. To further validate the model'srobustness, we created a new test set using 0.5m remote sensing imagery fromHuizhou, China, for inference testing. Our code, training methods, and theupdated dataset will be accessable at https://github.com/likaiucas.</description><author>Kai Li, Yupeng Deng, Yunlong Kong, Diyou Liu, Jingbo Chen, Yu Meng, Junxian Ma</author><pubDate>Wed, 25 Oct 2023 16:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16717v1</guid></item><item><title>Learning to Receive Help: Intervention-Aware Concept Embedding Models</title><link>http://arxiv.org/abs/2309.16928v2</link><description>Concept Bottleneck Models (CBMs) tackle the opacity of neural architecturesby constructing and explaining their predictions using a set of high-levelconcepts. A special property of these models is that they permit conceptinterventions, wherein users can correct mispredicted concepts and thus improvethe model's performance. Recent work, however, has shown that interventionefficacy can be highly dependent on the order in which concepts are intervenedon and on the model's architecture and training hyperparameters. We argue thatthis is rooted in a CBM's lack of train-time incentives for the model to beappropriately receptive to concept interventions. To address this, we proposeIntervention-aware Concept Embedding models (IntCEMs), a novel CBM-basedarchitecture and training paradigm that improves a model's receptiveness totest-time interventions. Our model learns a concept intervention policy in anend-to-end fashion from where it can sample meaningful interventiontrajectories at train-time. This conditions IntCEMs to effectively select andreceive concept interventions when deployed at test-time. Our experiments showthat IntCEMs significantly outperform state-of-the-art concept-interpretablemodels when provided with test-time concept interventions, demonstrating theeffectiveness of our approach.</description><author>Mateo Espinosa Zarlenga, Katherine M. Collins, Krishnamurthy Dvijotham, Adrian Weller, Zohreh Shams, Mateja Jamnik</author><pubDate>Wed, 25 Oct 2023 16:38:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16928v2</guid></item><item><title>SkyMath: Technical Report</title><link>http://arxiv.org/abs/2310.16713v1</link><description>Large language models (LLMs) have shown great potential to solve varieties ofnatural language processing (NLP) tasks, including mathematical reasoning. Inthis work, we present SkyMath, a large language model for mathematics with 13billion parameters. By applying self-compare fine-tuning, we have enhancedmathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,SkyMath outperforms all known open-source models of similar size and hasestablished a new SOTA performance.</description><author>Liu Yang, Haihua Yang, Wenjun Cheng, Lei Lin, Chenxia Li, Yifu Chen, Lunan Liu, Jianfei Pan, Tianwen Wei, Biye Li, Liang Zhao, Lijie Wang, Bo Zhu, Jujie He, Guoliang Li, Xuejie Wu, Xilin Luo, Rui Hu</author><pubDate>Wed, 25 Oct 2023 16:34:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16713v1</guid></item><item><title>LLM Performance Predictors are good initializers for Architecture Search</title><link>http://arxiv.org/abs/2310.16712v1</link><description>Large language models (LLMs) have become an integral component in solving awide range of NLP tasks. In this work, we explore a novel use case of usingLLMs to build performance predictors (PP): models that, given a specific deepneural network architecture, predict its performance on a downstream task. Wedesign PP prompts for LLMs consisting of: (i) role: description of the roleassigned to the LLM, (ii) instructions: set of instructions to be followed bythe LLM to carry out performance prediction, (iii) hyperparameters: adefinition of each architecture-specific hyperparameter and (iv)demonstrations: sample architectures along with their efficiency metrics and'training from scratch' performance. For machine translation (MT) tasks, wediscover that GPT-4 with our PP prompts (LLM-PP) can predict the performance ofarchitecture with a mean absolute error matching the SOTA and a marginaldegradation in rank correlation coefficient compared to SOTA performancepredictors. Further, we show that the predictions from LLM-PP can be distilledto a small regression model (LLM-Distill-PP). LLM-Distill-PP modelssurprisingly retain the performance of LLM-PP largely and can be acost-effective alternative for heavy use cases of performance estimation.Specifically, for neural architecture search (NAS), we propose a Hybrid-Searchalgorithm for NAS (HS-NAS), which uses LLM-Distill-PP for the initial part ofsearch, resorting to the baseline predictor for rest of the search. We showthat HS-NAS performs very similar to SOTA NAS across benchmarks, reduces searchhours by 50% roughly, and in some cases, improves latency, GFLOPs, and modelsize.</description><author>Ganesh Jawahar, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Dujian Ding</author><pubDate>Wed, 25 Oct 2023 16:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16712v1</guid></item><item><title>Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes</title><link>http://arxiv.org/abs/2305.11772v2</link><description>Humans and animals have a rich and flexible understanding of the physicalworld, which enables them to infer the underlying dynamical trajectories ofobjects and events, plausible future states, and use that to plan andanticipate the consequences of actions. However, the neural mechanismsunderlying these computations are unclear. We combine a goal-driven modelingapproach with dense neurophysiological data and high-throughput humanbehavioral readouts to directly impinge on this question. Specifically, weconstruct and evaluate several classes of sensory-cognitive networks to predictthe future state of rich, ethologically-relevant environments, ranging fromself-supervised end-to-end models with pixel-wise or object-centric objectives,to models that future predict in the latent space of purely static image-basedor dynamic video-based pretrained foundation models. We find strongdifferentiation across these model classes in their ability to predict neuraland behavioral data both within and across diverse environments. In particular,we find that neural responses are currently best predicted by models trained topredict the future state of their environment in the latent space of pretrainedfoundation models optimized for dynamic scenes in a self-supervised manner.Notably, models that future predict in the latent space of video foundationmodels that are optimized to support a diverse range of sensorimotor tasks,reasonably match both human behavioral error patterns and neural dynamicsacross all environmental scenarios that we were able to test. Overall, thesefindings suggest that the neural mechanisms and behaviors of primate mentalsimulation are thus far most consistent with being optimized to future predicton dynamic, reusable visual representations that are useful for Embodied AImore generally.</description><author>Aran Nayebi, Rishi Rajalingham, Mehrdad Jazayeri, Guangyu Robert Yang</author><pubDate>Wed, 25 Oct 2023 16:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11772v2</guid></item><item><title>StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure</title><link>http://arxiv.org/abs/2305.05588v2</link><description>This work presents StrAE: a Structured Autoencoder framework that throughstrict adherence to explicit structure, and use of a novel contrastiveobjective over tree-structured representations, enables effective learning ofmulti-level representations. Through comparison over different forms ofstructure, we verify that our results are directly attributable to theinformativeness of the structure provided as input, and show that this is notthe case for existing tree models. We then further extend StrAE to allow themodel to define its own compositions using a simple localised-merge algorithm.This variant, called Self-StrAE, outperforms baselines that don't involveexplicit hierarchical compositions, and is comparable to models giveninformative structure (e.g. constituency parses). Our experiments are conductedin a data-constrained (circa 10M tokens) setting to help tease apart thecontribution of the inductive bias to effective learning. However, we find thatthis framework can be robust to scale, and when extended to a much largerdataset (circa 100M tokens), our 430 parameter model performs comparably to a6-layer RoBERTa many orders of magnitude larger in size. Our findings supportthe utility of incorporating explicit composition as an inductive bias foreffective representation learning.</description><author>Mattia Opper, Victor Prokhorov, N. Siddharth</author><pubDate>Wed, 25 Oct 2023 16:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05588v2</guid></item><item><title>Nighttime Driver Behavior Prediction Using Taillight Signal Recognition via CNN-SVM Classifier</title><link>http://arxiv.org/abs/2310.16706v1</link><description>This paper aims to enhance the ability to predict nighttime driving behaviorby identifying taillights of both human-driven and autonomous vehicles. Theproposed model incorporates a customized detector designed to accurately detectfront-vehicle taillights on the road. At the beginning of the detector, alearnable pre-processing block is implemented, which extracts deep featuresfrom input images and calculates the data rarity for each feature. In the nextstep, drawing inspiration from soft attention, a weighted binary mask isdesigned that guides the model to focus more on predetermined regions. Thisresearch utilizes Convolutional Neural Networks (CNNs) to extractdistinguishing characteristics from these areas, then reduces dimensions usingPrincipal Component Analysis (PCA). Finally, the Support Vector Machine (SVM)is used to predict the behavior of the vehicles. To train and evaluate themodel, a large-scale dataset is collected from two types of dash-cams andInsta360 cameras from the rear view of Ford Motor Company vehicles. Thisdataset includes over 12k frames captured during both daytime and nighttimehours. To address the limited nighttime data, a unique pixel-wise imageprocessing technique is implemented to convert daytime images into realisticnight images. The findings from the experiments demonstrate that the proposedmethodology can accurately categorize vehicle behavior with 92.14% accuracy,97.38% specificity, 92.09% sensitivity, 92.10% F1-measure, and 0.895 Cohen'sKappa Statistic. Further details are available athttps://github.com/DeepCar/Taillight_Recognition.</description><author>Amir Hossein Barshooi, Elmira Bagheri</author><pubDate>Wed, 25 Oct 2023 16:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16706v1</guid></item><item><title>Is Attention always needed? A Case Study on Language Identification from Speech</title><link>http://arxiv.org/abs/2110.03427v3</link><description>Language Identification (LID) is a crucial preliminary process in the fieldof Automatic Speech Recognition (ASR) that involves the identification of aspoken language from audio samples. Contemporary systems that can processspeech in multiple languages require users to expressly designate one or morelanguages prior to utilization. The LID task assumes a significant role inscenarios where ASR systems are unable to comprehend the spoken language inmultilingual settings, leading to unsuccessful speech recognition outcomes. Thepresent study introduces convolutional recurrent neural network (CRNN) basedLID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC)characteristics of audio samples. Furthermore, we replicate certainstate-of-the-art methodologies, specifically the Convolutional Neural Network(CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN withattention), and conduct a comparative analysis with our CRNN-based approach. Weconducted comprehensive evaluations on thirteen distinct Indian languages andour model resulted in over 98\% classification accuracy. The LID model exhibitshigh-performance levels ranging from 97% to 100% for languages that arelinguistically similar. The proposed LID model exhibits a high degree ofextensibility to additional languages and demonstrates a strong resistance tonoise, achieving 91.2% accuracy in a noisy setting when applied to a EuropeanLanguage (EU) dataset.</description><author>Atanu Mandal, Santanu Pal, Indranil Dutta, Mahidas Bhattacharya, Sudip Kumar Naskar</author><pubDate>Wed, 25 Oct 2023 16:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.03427v3</guid></item><item><title>Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference</title><link>http://arxiv.org/abs/2310.16705v1</link><description>Variational inference (VI) can be cast as an optimization problem in whichthe variational parameters are tuned to closely align a variationaldistribution with the true posterior. The optimization task can be approachedthrough vanilla gradient descent in black-box VI or natural-gradient descent innatural-gradient VI. In this work, we reframe VI as the optimization of anobjective that concerns probability distributions defined over a\textit{variational parameter space}. Subsequently, we propose Wassersteingradient descent for tackling this optimization problem. Notably, theoptimization techniques, namely black-box VI and natural-gradient VI, can bereinterpreted as specific instances of the proposed Wasserstein gradientdescent. To enhance the efficiency of optimization, we develop practicalmethods for numerically solving the discrete gradient flows. We validate theeffectiveness of the proposed methods through empirical experiments on asynthetic dataset, supplemented by theoretical analyses.</description><author>Dai Hai Nguyen, Tetsuya Sakurai, Hiroshi Mamitsuka</author><pubDate>Wed, 25 Oct 2023 16:20:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16705v1</guid></item><item><title>From Tempered to Benign Overfitting in ReLU Neural Networks</title><link>http://arxiv.org/abs/2305.15141v2</link><description>Overparameterized neural networks (NNs) are observed to generalize well evenwhen trained to perfectly fit noisy data. This phenomenon motivated a largebody of work on "benign overfitting", where interpolating predictors achievenear-optimal performance. Recently, it was conjectured and empirically observedthat the behavior of NNs is often better described as "tempered overfitting",where the performance is non-optimal yet also non-trivial, and degrades as afunction of the noise level. However, a theoretical justification of this claimfor non-linear NNs has been lacking so far. In this work, we provide severalresults that aim at bridging these complementing views. We study a simpleclassification setting with 2-layer ReLU NNs, and prove that under variousassumptions, the type of overfitting transitions from tempered in the extremecase of one-dimensional data, to benign in high dimensions. Thus, we show thatthe input dimension has a crucial role on the type of overfitting in thissetting, which we also validate empirically for intermediate dimensions.Overall, our results shed light on the intricate connections between thedimension, sample size, architecture and training algorithm on the one hand,and the type of resulting overfitting on the other hand.</description><author>Guy Kornowski, Gilad Yehudai, Ohad Shamir</author><pubDate>Wed, 25 Oct 2023 16:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15141v2</guid></item><item><title>Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability</title><link>http://arxiv.org/abs/2310.11518v2</link><description>Self-play is a technique for machine learning in multi-agent systems where alearning algorithm learns by interacting with copies of itself. Self-play isuseful for generating large quantities of data for learning, but has thedrawback that the agents the learner will face post-training may havedramatically different behavior than the learner came to expect by interactingwith itself. For the special case of two-player constant-sum games, self-playthat reaches Nash equilibrium is guaranteed to produce strategies that performwell against any post-training opponent; however, no such guarantee exists formultiplayer games. We show that in games that approximately decompose into aset of two-player constant-sum games (called constant-sum polymatrix games)where global $\epsilon$-Nash equilibria are boundedly far from Nash equilibriain each subgame (called subgame stability), any no-external-regret algorithmthat learns by self-play will produce a strategy with bounded vulnerability.For the first time, our results identify a structural property of multiplayergames that enable performance guarantees for the strategies produced by a broadclass of self-play algorithms. We demonstrate our findings through experimentson Leduc poker.</description><author>Revan MacQueen, James R. Wright</author><pubDate>Wed, 25 Oct 2023 16:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11518v2</guid></item><item><title>Human-centred explanation of rule-based decision-making systems in the legal domain</title><link>http://arxiv.org/abs/2310.16704v1</link><description>We propose a human-centred explanation method for rule-based automateddecision-making systems in the legal domain. Firstly, we establish a conceptualframework for developing explanation methods, representing its key internalcomponents (content, communication and adaptation) and external dependencies(decision-making system, human recipient and domain). Secondly, we propose anexplanation method that uses a graph database to enable question-drivenexplanations and multimedia display. This way, we can tailor the explanation tothe user. Finally, we show how our conceptual framework is applicable to areal-world scenario at the Dutch Tax and Customs Administration and implementour explanation method for this scenario.</description><author>Suzan Zuurmond, AnneMarie Borg, Matthijs van Kempen, Remi Wieten</author><pubDate>Wed, 25 Oct 2023 16:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16704v1</guid></item><item><title>PeFLL: Personalized Federated Learning by Learning to Learn</title><link>http://arxiv.org/abs/2306.05515v2</link><description>We present PeFLL, a new personalized federated learning algorithm thatimproves over the state-of-the-art in three aspects: 1) it produces moreaccurate models, especially in the low-data regime, and not only for clientspresent during its training phase, but also for any that may emerge in thefuture; 2) it reduces the amount of on-client computation and client-servercommunication by providing future clients with ready-to-use personalized modelsthat require no additional finetuning or optimization; 3) it comes withtheoretical guarantees that establish generalization from the observed clientsto future ones. At the core of PeFLL lies a learning-to-learn approach thatjointly trains an embedding network and a hypernetwork. The embedding networkis used to represent clients in a latent descriptor space in a way thatreflects their similarity to each other. The hypernetwork takes as input suchdescriptors and outputs the parameters of fully personalized client models. Incombination, both networks constitute a learning algorithm that achievesstate-of-the-art performance in several personalized federated learningbenchmarks.</description><author>Jonathan Scott, Hossein Zakerinia, Christoph H. Lampert</author><pubDate>Wed, 25 Oct 2023 16:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05515v2</guid></item><item><title>Regularized Data Programming with Automated Bayesian Prior Selection</title><link>http://arxiv.org/abs/2210.08677v2</link><description>The cost of manual data labeling can be a significant obstacle in supervisedlearning. Data programming (DP) offers a weakly supervised solution fortraining dataset creation, wherein the outputs of user-defined programmaticlabeling functions (LFs) are reconciled through unsupervised learning. However,DP can fail to outperform an unweighted majority vote in some scenarios,including low-data contexts. This work introduces a Bayesian extension ofclassical DP that mitigates failures of unsupervised learning by augmenting theDP objective with regularization terms. Regularized learning is achievedthrough maximum a posteriori estimation with informative priors. Majority voteis proposed as a proxy signal for automated prior parameter selection. Resultssuggest that regularized DP improves performance relative to maximum likelihoodand majority voting, confers greater interpretability, and bolsters performancein low-data regimes.</description><author>Jacqueline R. M. A. Maasch, Hao Zhang, Qian Yang, Fei Wang, Volodymyr Kuleshov</author><pubDate>Wed, 25 Oct 2023 16:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08677v2</guid></item><item><title>Causal Discovery with Generalized Linear Models through Peeling Algorithms</title><link>http://arxiv.org/abs/2310.16698v1</link><description>This article presents a novel method for causal discovery with generalizedstructural equation models suited for analyzing diverse types of outcomes,including discrete, continuous, and mixed data. Causal discovery often faceschallenges due to unmeasured confounders that hinder the identification ofcausal relationships. The proposed approach addresses this issue by developingtwo peeling algorithms (bottom-up and top-down) to ascertain causalrelationships and valid instruments. This approach first reconstructs asuper-graph to represent ancestral relationships between variables, using apeeling algorithm based on nodewise GLM regressions that exploit relationshipsbetween primary and instrumental variables. Then, it estimates parent-childeffects from the ancestral relationships using another peeling algorithm whiledeconfounding a child's model with information borrowed from its parents'models. The article offers a theoretical analysis of the proposed approach,which establishes conditions for model identifiability and provides statisticalguarantees for accurately discovering parent-child relationships via thepeeling algorithms. Furthermore, the article presents numerical experimentsshowcasing the effectiveness of our approach in comparison to state-of-the-artstructure learning methods without confounders. Lastly, it demonstrates anapplication to Alzheimer's disease (AD), highlighting the utility of the methodin constructing gene-to-gene and gene-to-disease regulatory networks involvingSingle Nucleotide Polymorphisms (SNPs) for healthy and AD subjects.</description><author>Minjie Wang, Xiaotong Shen, Wei Pan</author><pubDate>Wed, 25 Oct 2023 16:12:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16698v1</guid></item><item><title>Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles</title><link>http://arxiv.org/abs/2310.15952v2</link><description>While deep learning models have achieved remarkable success across a range ofmedical image analysis tasks, deployment of these models in real clinicalcontexts requires that they be robust to variability in the acquired images.While many methods apply predefined transformations to augment the trainingdata to enhance test-time robustness, these transformations may not ensure themodel's robustness to the diverse variability seen in patient images. In thispaper, we introduce a novel three-stage approach based on transformers coupledwith conditional diffusion models, with the goal of improving model robustnessto the kinds of imaging variability commonly encountered in practice withoutthe need for pre-determined data augmentation strategies. To this end, multipleimage encoders first learn hierarchical feature representations to builddiscriminative latent spaces. Next, a reverse diffusion process, guided by thelatent code, acts on an informative prior and proposes prediction candidates ina generative manner. Finally, several prediction candidates are aggregated in abi-level aggregation protocol to produce the final output. Through extensiveexperiments on medical imaging benchmark datasets, we show that our methodimproves upon state-of-the-art methods in terms of robustness and confidencecalibration. Additionally, we introduce a strategy to quantify the predictionuncertainty at the instance level, increasing their trustworthiness toclinicians using them in clinical practice.</description><author>Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel</author><pubDate>Wed, 25 Oct 2023 16:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15952v2</guid></item><item><title>Interpretable time series neural representation for classification purposes</title><link>http://arxiv.org/abs/2310.16696v1</link><description>Deep learning has made significant advances in creating efficientrepresentations of time series data by automatically identifying complexpatterns. However, these approaches lack interpretability, as the time seriesis transformed into a latent vector that is not easily interpretable. On theother hand, Symbolic Aggregate approximation (SAX) methods allow the creationof symbolic representations that can be interpreted but do not capture complexpatterns effectively. In this work, we propose a set of requirements for aneural representation of univariate time series to be interpretable. We proposea new unsupervised neural architecture that meets these requirements. Theproposed model produces consistent, discrete, interpretable, and visualizablerepresentations. The model is learned independently of any downstream tasks inan unsupervised setting to ensure robustness. As a demonstration of theeffectiveness of the proposed model, we propose experiments on classificationtasks using UCR archive datasets. The obtained results are extensively comparedto other interpretable models and state-of-the-art neural representationlearning models. The experiments show that the proposed model yields, onaverage better results than other interpretable approaches on multipledatasets. We also present qualitative experiments to asses the interpretabilityof the approach.</description><author>Etienne Le Naour, Ghislain Agoua, Nicolas Baskiotis, Vincent Guigue</author><pubDate>Wed, 25 Oct 2023 16:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16696v1</guid></item><item><title>From Pointwise to Powerhouse: Initialising Neural Networks with Generative Models</title><link>http://arxiv.org/abs/2310.16695v1</link><description>Traditional initialisation methods, e.g. He and Xavier, have been effectivein avoiding the problem of vanishing or exploding gradients in neural networks.However, they only use simple pointwise distributions, which modelone-dimensional variables. Moreover, they ignore most information about thearchitecture and disregard past training experiences. These limitations can beovercome by employing generative models for initialisation. In this paper, weintroduce two groups of new initialisation methods. First, we locallyinitialise weight groups by employing variational autoencoders. Secondly, weglobally initialise full weight sets by employing graph hypernetworks. Wethoroughly evaluate the impact of the employed generative models onstate-of-the-art neural networks in terms of accuracy, convergence speed andensembling. Our results show that global initialisations result in higheraccuracy and faster initial convergence speed. However, the implementationthrough graph hypernetworks leads to diminished ensemble performance on out ofdistribution data. To counteract, we propose a modification called noise graphhypernetwork, which encourages diversity in the produced ensemble members.Furthermore, our approach might be able to transfer learned knowledge todifferent image distributions. Our work provides insights into the potential,the trade-offs and possible modifications of these new initialisation methods.</description><author>Christian Harder, Moritz Fuchs, Yuri Tolkach, Anirban Mukhopadhyay</author><pubDate>Wed, 25 Oct 2023 16:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16695v1</guid></item><item><title>DSAM-GN:Graph Network based on Dynamic Similarity Adjacency Matrices for Vehicle Re-identification</title><link>http://arxiv.org/abs/2310.16694v1</link><description>In recent years, vehicle re-identification (Re-ID) has gained increasingimportance in various applications such as assisted driving systems, trafficflow management, and vehicle tracking, due to the growth of intelligenttransportation systems. However, the presence of extraneous backgroundinformation and occlusions can interfere with the learning of discriminativefeatures, leading to significant variations in the same vehicle image acrossdifferent scenarios. This paper proposes a method, named graph network based ondynamic similarity adjacency matrices (DSAM-GN), which incorporates a novelapproach for constructing adjacency matrices to capture spatial relationshipsof local features and reduce background noise. Specifically, the proposedmethod divides the extracted vehicle features into different patches as nodeswithin the graph network. A spatial attention-based similarity adjacency matrixgeneration (SASAMG) module is employed to compute similarity matrices of nodes,and a dynamic erasure operation is applied to disconnect nodes with lowsimilarity, resulting in similarity adjacency matrices. Finally, the nodes andsimilarity adjacency matrices are fed into graph networks to extract morediscriminative features for vehicle Re-ID. Experimental results on publicdatasets VeRi-776 and VehicleID demonstrate the effectiveness of the proposedmethod compared with recent works.</description><author>Yuejun Jiao, Song Qiu, Mingsong Chen, Dingding Han, Qingli Li, Yue Lu</author><pubDate>Wed, 25 Oct 2023 16:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16694v1</guid></item><item><title>Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration</title><link>http://arxiv.org/abs/2305.18258v2</link><description>In online reinforcement learning (online RL), balancing exploration andexploitation is crucial for finding an optimal policy in a sample-efficientway. To achieve this, existing sample-efficient online RL algorithms typicallyconsist of three components: estimation, planning, and exploration. However, inorder to cope with general function approximators, most of them involveimpractical algorithmic components to incentivize exploration, such asoptimization within data-dependent level-sets or complicated samplingprocedures. To address this challenge, we propose an easy-to-implement RLframework called \textit{Maximize to Explore} (\texttt{MEX}), which only needsto optimize \emph{unconstrainedly} a single objective that integrates theestimation and planning components while balancing exploration and exploitationautomatically. Theoretically, we prove that \texttt{MEX} achieves a sublinearregret with general function approximations for Markov decision processes (MDP)and is further extendable to two-player zero-sum Markov games (MG). Meanwhile,we adapt deep RL baselines to design practical versions of \texttt{MEX}, inboth model-free and model-based manners, which can outperform baselines by astable margin in various MuJoCo environments with sparse rewards. Compared withexisting sample-efficient online RL algorithms with general functionapproximations, \texttt{MEX} achieves similar sample efficiency while enjoyinga lower computational cost and is more compatible with modern deep RL methods.</description><author>Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang</author><pubDate>Wed, 25 Oct 2023 15:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18258v2</guid></item><item><title>Learning-based adaption of robotic friction models</title><link>http://arxiv.org/abs/2310.16688v1</link><description>In the Fourth Industrial Revolution, wherein artificial intelligence and theautomation of machines occupy a central role, the deployment of robots isindispensable. However, the manufacturing process using robots, especially incollaboration with humans, is highly intricate. In particular, modeling thefriction torque in robotic joints is a longstanding problem due to the lack ofa good mathematical description. This motivates the usage of data-drivenmethods in recent works. However, model-based and data-driven models oftenexhibit limitations in their ability to generalize beyond the specific dynamicsthey were trained on, as we demonstrate in this paper. To address thischallenge, we introduce a novel approach based on residual learning, which aimsto adapt an existing friction model to new dynamics using as little data aspossible. We validate our approach by training a base neural network on asymmetric friction data set to learn an accurate relation between the velocityand the friction torque. Subsequently, to adapt to more complex asymmetricsettings, we train a second network on a small dataset, focusing on predictingthe residual of the initial network's output. By combining the output of bothnetworks in a suitable manner, our proposed estimator outperforms theconventional model-based approach and the base neural network significantly.Furthermore, we evaluate our method on trajectories involving external loadsand still observe a substantial improvement, approximately 60-70\%, over theconventional approach. Our method does not rely on data with external loadduring training, eliminating the need for external torque sensors. Thisdemonstrates the generalization capability of our approach, even with a smallamount of data-only 43 seconds of a robot movement-enabling adaptation todiverse scenarios based on prior knowledge about friction in differentsettings.</description><author>Philipp Scholl, Maged Iskandar, Sebastian Wolf, Jinoh Lee, Aras Bacho, Alexander Dietrich, Alin Albu-Schäffer, Gitta Kutyniok</author><pubDate>Wed, 25 Oct 2023 15:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16688v1</guid></item><item><title>Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies</title><link>http://arxiv.org/abs/2310.16686v1</link><description>While reinforcement learning has achieved remarkable successes in severaldomains, its real-world application is limited due to many methods failing togeneralise to unfamiliar conditions. In this work, we consider the problem ofgeneralising to new transition dynamics, corresponding to cases in which theenvironment's response to the agent's actions differs. For example, thegravitational force exerted on a robot depends on its mass and changes therobot's mobility. Consequently, in such cases, it is necessary to condition anagent's actions on extrinsic state information and pertinent contextualinformation reflecting how the environment responds. While the need forcontext-sensitive policies has been established, the manner in which context isincorporated architecturally has received less attention. Thus, in this work,we present an investigation into how context information should be incorporatedinto behaviour learning to improve generalisation. To this end, we introduce aneural network architecture, the Decision Adapter, which generates the weightsof an adapter module and conditions the behaviour of an agent on the contextinformation. We show that the Decision Adapter is a useful generalisation of apreviously proposed architecture and empirically demonstrate that it results insuperior generalisation performance compared to previous approaches in severalenvironments. Beyond this, the Decision Adapter is more robust to irrelevantdistractor variables than several alternative methods.</description><author>Michael Beukman, Devon Jarvis, Richard Klein, Steven James, Benjamin Rosman</author><pubDate>Wed, 25 Oct 2023 15:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16686v1</guid></item><item><title>S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions</title><link>http://arxiv.org/abs/2305.14095v2</link><description>Vision-language models, such as contrastive language-image pre-training(CLIP), have demonstrated impressive results in natural image domains. However,these models often struggle when applied to specialized domains like remotesensing, and adapting to such domains is challenging due to the limited numberof image-text pairs available for training. To address this, we propose S-CLIP,a semi-supervised learning method for training CLIP that utilizes additionalunpaired images. S-CLIP employs two pseudo-labeling strategies specificallydesigned for contrastive learning and the language modality. The caption-levelpseudo-label is given by a combination of captions of paired images, obtainedby solving an optimal transport problem between unpaired and paired images. Thekeyword-level pseudo-label is given by a keyword in the caption of the nearestpaired image, trained through partial label learning that assumes a candidateset of labels for supervision instead of the exact one. By combining theseobjectives, S-CLIP significantly enhances the training of CLIP using only a fewimage-text pairs, as demonstrated in various specialist domains, includingremote sensing, fashion, scientific figures, and comics. For instance, S-CLIPimproves CLIP by 10% for zero-shot classification and 4% for image-textretrieval on the remote sensing benchmark, matching the performance ofsupervised CLIP while using three times fewer image-text pairs.</description><author>Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jinwoo Shin</author><pubDate>Wed, 25 Oct 2023 15:49:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14095v2</guid></item><item><title>Detection of news written by the ChatGPT through authorship attribution performed by a Bidirectional LSTM model</title><link>http://arxiv.org/abs/2310.16685v1</link><description>The large language based-model chatbot ChatGPT gained a lot of popularitysince its launch and has been used in a wide range of situations. This researchcenters around a particular situation, when the ChatGPT is used to produce newsthat will be consumed by the population, causing the facilitation in theproduction of fake news, spread of misinformation and lack of trust in newssources. Aware of these problems, this research aims to build an artificialintelligence model capable of performing authorship attribution on newsarticles, identifying the ones written by the ChatGPT. To achieve this goal, adataset containing equal amounts of human and ChatGPT written news wasassembled and different natural processing language techniques were used toextract features from it that were used to train, validate and test threemodels built with different techniques. The best performance was produced bythe Bidirectional Long Short Term Memory (LSTM) Neural Network model, achiving91.57\% accuracy when tested against the data from the testing set.</description><author>Amanda Ferrari Iaquinta, Gustavo Voltani von Atzingen</author><pubDate>Wed, 25 Oct 2023 15:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16685v1</guid></item><item><title>Local Statistics for Generative Image Detection</title><link>http://arxiv.org/abs/2310.16684v1</link><description>Diffusion models (DMs) are generative models that learn to synthesize imagesfrom Gaussian noise. DMs can be trained to do a variety of tasks such as imagegeneration and image super-resolution. Researchers have made significantimprovement in the capability of synthesizing photorealistic images in the pastfew years. These successes also hasten the need to address the potential misuseof synthesized images. In this paper, we highlight the effectiveness ofcomputing local statistics, as opposed to global statistics, in distinguishingdigital camera images from DM-generated images. We hypothesized that localstatistics should be used to address the spatial non-stationarity problem inimages. We show that our approach produced promising results and it is alsorobust to various perturbations such as image resizing and JPEG compression.</description><author>Yung Jer Wong, Teck Khim Ng</author><pubDate>Wed, 25 Oct 2023 15:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16684v1</guid></item><item><title>BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?</title><link>http://arxiv.org/abs/2310.16681v1</link><description>Language models have seen significant growth in the size of their corpus,leading to notable performance improvements. Yet, there has been limitedprogress in developing models that handle smaller, more human-like datasets. Aspart of the BabyLM shared task, this study explores the impact of reinforcementlearning from human feedback (RLHF) on language models pretrained from scratchwith a limited training corpus. Comparing two GPT-2 variants, the larger modelperforms better in storytelling tasks after RLHF fine-tuning. These findingssuggest that RLHF techniques may be more advantageous for larger models due totheir higher learning and adaptation capacity, though more experiments areneeded to confirm this finding. These insights highlight the potential benefitsof RLHF fine-tuning for language models within limited data, enhancing theirability to maintain narrative focus and coherence while adhering better toinitial instructions in storytelling tasks. The code for this work is publiclyat https://github.com/Zephyr1022/BabyStories-UTSA.</description><author>Xingmeng Zhao, Tongnian Wang, Sheri Osborn, Anthony Rios</author><pubDate>Wed, 25 Oct 2023 15:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16681v1</guid></item><item><title>Robust and Actively Secure Serverless Collaborative Learning</title><link>http://arxiv.org/abs/2310.16678v1</link><description>Collaborative machine learning (ML) is widely used to enable institutions tolearn better models from distributed data. While collaborative approaches tolearning intuitively protect user data, they remain vulnerable to either theserver, the clients, or both, deviating from the protocol. Indeed, because theprotocol is asymmetric, a malicious server can abuse its power to reconstructclient data points. Conversely, malicious clients can corrupt learning withmalicious updates. Thus, both clients and servers require a guarantee when theother cannot be trusted to fully cooperate. In this work, we propose apeer-to-peer (P2P) learning scheme that is secure against malicious servers androbust to malicious clients. Our core contribution is a generic framework thattransforms any (compatible) algorithm for robust aggregation of model updatesto the setting where servers and clients can act maliciously. Finally, wedemonstrate the computational efficiency of our approach even with 1-millionparameter models trained by 100s of peers on standard datasets.</description><author>Olive Franzese, Adam Dziedzic, Christopher A. Choquette-Choo, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang</author><pubDate>Wed, 25 Oct 2023 15:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16678v1</guid></item><item><title>SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations</title><link>http://arxiv.org/abs/2310.16676v1</link><description>Emotion recognition in conversations (ERC) is a rapidly evolving task withinthe natural language processing community, which aims to detect the emotionsexpressed by speakers during a conversation. Recently, a growing number of ERCmethods have focused on leveraging supervised contrastive learning (SCL) toenhance the robustness and generalizability of learned features. However,current SCL-based approaches in ERC are impeded by the constraint of largebatch sizes and the lack of compatibility with most existing ERC models. Toaddress these challenges, we propose an efficient and model-agnostic SCLframework named Supervised Sample-Label Contrastive Learning with Soft-HGRMaximal Correlation (SSLCL), which eliminates the need for a large batch sizeand can be seamlessly integrated with existing ERC models without introducingany model-specific assumptions. Specifically, we introduce a novel perspectiveon utilizing label representations by projecting discrete labels into denseembeddings through a shallow multilayer perceptron, and formulate the trainingobjective to maximize the similarity between sample features and theircorresponding ground-truth label embeddings, while minimizing the similaritybetween sample features and label embeddings of disparate classes. Moreover, weinnovatively adopt the Soft-HGR maximal correlation as a measure of similaritybetween sample features and label embeddings, leading to significantperformance improvements over conventional similarity measures. Additionally,multimodal cues of utterances are effectively leveraged by SSLCL as dataaugmentations to boost model performances. Extensive experiments on two ERCbenchmark datasets, IEMOCAP and MELD, demonstrate the compatibility andsuperiority of our proposed SSLCL framework compared to existingstate-of-the-art SCL methods. Our code is available at\url{https://github.com/TaoShi1998/SSLCL}.</description><author>Tao Shi, Xiao Liang, Yaoyuan Liang, Xinyi Tong, Shao-Lun Huang</author><pubDate>Wed, 25 Oct 2023 15:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16676v1</guid></item><item><title>Agreeing to Stop: Reliable Latency-Adaptive Decision Making via Ensembles of Spiking Neural Networks</title><link>http://arxiv.org/abs/2310.16675v1</link><description>Spiking neural networks (SNNs) are recurrent models that can leveragesparsity in input time series to efficiently carry out tasks such asclassification. Additional efficiency gains can be obtained if decisions aretaken as early as possible as a function of the complexity of the input timeseries. The decision on when to stop inference and produce a decision must relyon an estimate of the current accuracy of the decision. Prior work demonstratedthe use of conformal prediction (CP) as a principled way to quantifyuncertainty and support adaptive-latency decisions in SNNs. In this paper, wepropose to enhance the uncertainty quantification capabilities of SNNs byimplementing ensemble models for the purpose of improving the reliability ofstopping decisions. Intuitively, an ensemble of multiple models can decide whento stop more reliably by selecting times at which most models agree that thecurrent accuracy level is sufficient. The proposed method relies on differentforms of information pooling from ensemble models, and offers theoreticalreliability guarantees. We specifically show that variational inference-basedensembles with p-variable pooling significantly reduce the average latency ofstate-of-the-art methods, while maintaining reliability guarantees.</description><author>Jiechen Chen, Sangwoo Park, Osvaldo Simeone</author><pubDate>Wed, 25 Oct 2023 15:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16675v1</guid></item></channel></rss>