<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 14 Sep 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors</title><link>http://arxiv.org/abs/2409.08278v1</link><description>We present DreamHOI, a novel method for zero-shot synthesis of human-objectinteractions (HOIs), enabling a 3D human model to realistically interact withany given object based on a textual description. This task is complicated bythe varying categories and geometries of real-world objects and the scarcity ofdatasets encompassing diverse HOIs. To circumvent the need for extensive data,we leverage text-to-image diffusion models trained on billions of image-captionpairs. We optimize the articulation of a skinned human mesh using ScoreDistillation Sampling (SDS) gradients obtained from these models, which predictimage-space edits. However, directly backpropagating image-space gradients intocomplex articulation parameters is ineffective due to the local nature of suchgradients. To overcome this, we introduce a dual implicit-explicitrepresentation of a skinned mesh, combining (implicit) neural radiance fields(NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization,we transition between implicit and explicit forms, grounding the NeRFgeneration while refining the mesh articulation. We validate our approachthrough extensive experiments, demonstrating its effectiveness in generatingrealistic HOIs.</description><author>Thomas Hanwen Zhu, Ruining Li, Tomas Jakab</author><pubDate>Thu, 12 Sep 2024 17:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08278v1</guid></item><item><title>Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor</title><link>http://arxiv.org/abs/2409.08277v1</link><description>High frame rate and accurate depth estimation plays an important role inseveral tasks crucial to robotics and automotive perception. To date, this canbe achieved through ToF and LiDAR devices for indoor and outdoor applications,respectively. However, their applicability is limited by low frame rate, energyconsumption, and spatial sparsity. Depth on Demand (DoD) allows for accuratetemporal and spatial depth densification achieved by exploiting a high framerate RGB sensor coupled with a potentially lower frame rate and sparse activedepth sensor. Our proposal jointly enables lower energy consumption and densershape reconstruction, by significantly reducing the streaming requirements onthe depth sensor thanks to its three core stages: i) multi-modal encoding, ii)iterative multi-modal integration, and iii) depth decoding. We present extendedevidence assessing the effectiveness of DoD on indoor and outdoor videodatasets, covering both environment scanning and automotive perception usecases.</description><author>Andrea Conti, Matteo Poggi, Valerio Cambareri, Stefano Mattoccia</author><pubDate>Thu, 12 Sep 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08277v1</guid></item><item><title>AnySkin: Plug-and-play Skin Sensing for Robotic Touch</title><link>http://arxiv.org/abs/2409.08276v1</link><description>While tactile sensing is widely accepted as an important and useful sensingmodality, its use pales in comparison to other sensory modalities like visionand proprioception. AnySkin addresses the critical challenges that impede theuse of tactile sensing -- versatility, replaceability, and data reusability.Building on the simplistic design of ReSkin, and decoupling the sensingelectronics from the sensing interface, AnySkin simplifies integration makingit as straightforward as putting on a phone case and connecting a charger.Furthermore, AnySkin is the first uncalibrated tactile-sensor withcross-instance generalizability of learned manipulation policies. To summarize,this work makes three key contributions: first, we introduce a streamlinedfabrication process and a design tool for creating an adhesive-free, durableand easily replaceable magnetic tactile sensor; second, we characterize slipdetection and policy learning with the AnySkin sensor; and third, wedemonstrate zero-shot generalization of models trained on one instance ofAnySkin to new instances, and compare it with popular existing tactilesolutions like DIGIT and ReSkin.https://any-skin.github.io/</description><author>Raunaq Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto</author><pubDate>Thu, 12 Sep 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08276v1</guid></item><item><title>Hand-Object Interaction Pretraining from Videos</title><link>http://arxiv.org/abs/2409.08273v1</link><description>We present an approach to learn general robot manipulation priors from 3Dhand-object interaction trajectories. We build a framework to use in-the-wildvideos to generate sensorimotor robot trajectories. We do so by lifting boththe human hand and the manipulated object in a shared 3D space and retargetinghuman motions to robot actions. Generative modeling on this data gives us atask-agnostic base policy. This policy captures a general yet flexiblemanipulation prior. We empirically demonstrate that finetuning this policy,with both reinforcement learning (RL) and behavior cloning (BC), enablessample-efficient adaptation to downstream tasks and simultaneously improvesrobustness and generalizability compared to prior approaches. Qualitativeexperiments are available at: \url{https://hgaurav2k.github.io/hop/}.</description><author>Himanshu Gaurav Singh, Antonio Loquercio, Carmelo Sferrazza, Jane Wu, Haozhi Qi, Pieter Abbeel, Jitendra Malik</author><pubDate>Thu, 12 Sep 2024 17:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08273v1</guid></item><item><title>Click2Mask: Local Editing with Dynamic Mask Generation</title><link>http://arxiv.org/abs/2409.08272v1</link><description>Recent advancements in generative models have revolutionized image generationand editing, making these tasks accessible to non-experts. This paper focuseson local image editing, particularly the task of adding new content to aloosely specified area. Existing methods often require a precise mask or adetailed description of the location, which can be cumbersome and prone toerrors. We propose Click2Mask, a novel approach that simplifies the localediting process by requiring only a single point of reference (in addition tothe content description). A mask is dynamically grown around this point duringa Blended Latent Diffusion (BLD) process, guided by a masked CLIP-basedsemantic loss. Click2Mask surpasses the limitations of segmentation-based andfine-tuning dependent methods, offering a more user-friendly and contextuallyaccurate solution. Our experiments demonstrate that Click2Mask not onlyminimizes user effort but also delivers competitive or superior local imagemanipulation results compared to SoTA methods, according to both humanjudgement and automatic metrics. Key contributions include the simplificationof user input, the ability to freely add objects unconstrained by existingsegments, and the integration potential of our dynamic mask approach withinother editing methods.</description><author>Omer Regev, Omri Avrahami, Dani Lischinski</author><pubDate>Thu, 12 Sep 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08272v1</guid></item><item><title>DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer</title><link>http://arxiv.org/abs/2409.08271v1</link><description>We present DreamBeast, a novel method based on score distillation sampling(SDS) for generating fantastical 3D animal assets composed of distinct parts.Existing SDS methods often struggle with this generation task due to a limitedunderstanding of part-level semantics in text-to-image diffusion models. Whilerecent diffusion models, such as Stable Diffusion 3, demonstrate a betterpart-level understanding, they are prohibitively slow and exhibit other commonproblems associated with single-view diffusion models. DreamBeast overcomesthis limitation through a novel part-aware knowledge transfer mechanism. Foreach generated asset, we efficiently extract part-level knowledge from theStable Diffusion 3 model into a 3D Part-Affinity implicit representation. Thisenables us to instantly generate Part-Affinity maps from arbitrary cameraviews, which we then use to modulate the guidance of a multi-view diffusionmodel during SDS to create 3D assets of fantastical animals. DreamBeastsignificantly enhances the quality of generated 3D creatures withuser-specified part compositions while reducing computational overhead, asdemonstrated by extensive quantitative and qualitative evaluations.</description><author>Runjia Li, Junlin Han, Luke Melas-Kyriazi, Chunyi Sun, Zhaochong An, Zhongrui Gui, Shuyang Sun, Philip Torr, Tomas Jakab</author><pubDate>Thu, 12 Sep 2024 17:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08271v1</guid></item><item><title>FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally</title><link>http://arxiv.org/abs/2409.08270v1</link><description>This study addresses the challenge of accurately segmenting 3D GaussianSplatting from 2D masks. Conventional methods often rely on iterative gradientdescent to assign each Gaussian a unique label, leading to lengthy optimizationand sub-optimal solutions. Instead, we propose a straightforward yet globallyoptimal solver for 3D-GS segmentation. The core insight of our method is that,with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentiallya linear function with respect to the labels of each Gaussian. As such, theoptimal label assignment can be solved via linear programming in closed form.This solution capitalizes on the alpha blending characteristic of the splattingprocess for single step optimization. By incorporating the background bias inour objective function, our method shows superior robustness in 3D segmentationagainst noises. Remarkably, our optimization completes within 30 seconds, about50$\times$ faster than the best existing methods. Extensive experimentsdemonstrate the efficiency and robustness of our method in segmenting variousscenes, and its superior performance in downstream tasks such as object removaland inpainting. Demos and code will be available athttps://github.com/florinshen/FlashSplat.</description><author>Qiuhong Shen, Xingyi Yang, Xinchao Wang</author><pubDate>Thu, 12 Sep 2024 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08270v1</guid></item><item><title>Face Reconstruction Transfer Attack as Out-of-Distribution Generalization</title><link>http://arxiv.org/abs/2407.02403v2</link><description>Understanding the vulnerability of face recognition systems to maliciousattacks is of critical importance. Previous works have focused onreconstructing face images that can penetrate a targeted verification system.Even in the white-box scenario, however, naively reconstructed imagesmisrepresent the identity information, hence the attacks are easily neutralizedonce the face system is updated or changed. In this paper, we aim toreconstruct face images which are capable of transferring face attacks onunseen encoders. We term this problem as Face Reconstruction Transfer Attack(FRTA) and show that it can be formulated as an out-of-distribution (OOD)generalization problem. Inspired by its OOD nature, we propose to solve FRTA byAveraged Latent Search and Unsupervised Validation with pseudo target (ALSUV).To strengthen the reconstruction attack on OOD unseen encoders, ALSUVreconstructs the face by searching the latent of amortized generator StyleGAN2through multiple latent optimization, latent optimization trajectory averaging,and unsupervised validation with a pseudo target. We demonstrate the efficacyand generalization of our method on widely used face datasets, accompanying itwith extensive ablation studies and visually, qualitatively, and quantitativelyanalyses. The source code will be released.</description><author>Yoon Gyo Jung, Jaewoo Park, Xingbo Dong, Hojin Park, Andrew Beng Jin Teoh, Octavia Camps</author><pubDate>Thu, 12 Sep 2024 17:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02403v2</guid></item><item><title>Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale</title><link>http://arxiv.org/abs/2409.08264v1</link><description>Large language models (LLMs) show remarkable potential to act as computeragents, enhancing human productivity and software accessibility in multi-modaltasks that require planning and reasoning. However, measuring agent performancein realistic environments remains a challenge since: (i) most benchmarks arelimited to specific modalities or domains (e.g. text-only, web navigation, Q&amp;A,coding) and (ii) full benchmark evaluations are slow (on order of magnitude ofdays) given the multi-step sequential nature of tasks. To address thesechallenges, we introduce the Windows Agent Arena: a reproducible, generalenvironment focusing exclusively on the Windows operating system (OS) whereagents can operate freely within a real Windows OS and use the same wide rangeof applications, tools, and web browsers available to human users when solvingtasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverseWindows tasks across representative domains that require agent abilities inplanning, screen understanding, and tool usage. Our benchmark is scalable andcan be seamlessly parallelized in Azure for a full benchmark evaluation in aslittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, wealso introduce a new multi-modal agent, Navi. Our agent achieves a success rateof 19.5% in the Windows domain, compared to 74.5% performance of an unassistedhuman. Navi also demonstrates strong performance on another popular web-basedbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysisof Navi's performance, and provide insights into the opportunities for futureresearch in agent development and data generation using Windows Agent Arena. Webpage: https://microsoft.github.io/WindowsAgentArena Code: https://github.com/microsoft/WindowsAgentArena</description><author>Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, Zack Hui</author><pubDate>Thu, 12 Sep 2024 17:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08264v1</guid></item><item><title>Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding</title><link>http://arxiv.org/abs/2408.08252v3</link><description>Diffusion models excel at capturing the natural design spaces of images,molecules, DNA, RNA, and protein sequences. However, rather than merelygenerating designs that are natural, we often aim to optimize downstream rewardfunctions while preserving the naturalness of these design spaces. Existingmethods for achieving this goal often require ``differentiable'' proxy models(\textit{e.g.}, classifier guidance or DPS) or involve computationallyexpensive fine-tuning of diffusion models (\textit{e.g.}, classifier-freeguidance, RL-based fine-tuning). In our work, we propose a new method toaddress these challenges. Our algorithm is an iterative sampling method thatintegrates soft value functions, which looks ahead to how intermediate noisystates lead to high rewards in the future, into the standard inferenceprocedure of pre-trained diffusion models. Notably, our approach avoidsfine-tuning generative models and eliminates the need to constructdifferentiable models. This enables us to (1) directly utilizenon-differentiable features/reward feedback, commonly used in many scientificdomains, and (2) apply our method to recent discrete diffusion models in aprincipled way. Finally, we demonstrate the effectiveness of our algorithmacross several domains, including image generation, molecule generation, andDNA/RNA sequence generation. The code is available at\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.</description><author>Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Aviv Regev, Sergey Levine, Masatoshi Uehara</author><pubDate>Thu, 12 Sep 2024 17:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08252v3</guid></item><item><title>Learning incomplete factorization preconditioners for GMRES</title><link>http://arxiv.org/abs/2409.08262v1</link><description>In this paper, we develop a data-driven approach to generate incomplete LUfactorizations of large-scale sparse matrices. The learned approximatefactorization is utilized as a preconditioner for the corresponding linearequation system in the GMRES method. Incomplete factorization methods are oneof the most commonly applied algebraic preconditioners for sparse linearequation systems and are able to speed up the convergence of Krylov subspacemethods. However, they are sensitive to hyper-parameters and might suffer fromnumerical breakdown or lead to slow convergence when not properly applied. Wereplace the typically hand-engineered algorithms with a graph neural networkbased approach that is trained against data to predict an approximatefactorization. This allows us to learn preconditioners tailored for a specificproblem distribution. We analyze and empirically evaluate different lossfunctions to train the learned preconditioners and show their effectiveness todecrease the number of GMRES iterations and improve the spectral properties onour synthetic dataset. The code is available athttps://github.com/paulhausner/neural-incomplete-factorization.</description><author>Paul Häusner, Aleix Nieto Juscafresa, Jens Sjölund</author><pubDate>Thu, 12 Sep 2024 17:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08262v1</guid></item><item><title>Improving Text-guided Object Inpainting with Semantic Pre-inpainting</title><link>http://arxiv.org/abs/2409.08260v1</link><description>Recent years have witnessed the success of large text-to-image diffusionmodels and their remarkable potential to generate high-quality images. Thefurther pursuit of enhancing the editability of images has sparked significantinterest in the downstream task of inpainting a novel object described by atext prompt within a designated region in the image. Nevertheless, the problemis not trivial from two aspects: 1) Solely relying on one single U-Net to aligntext prompt and visual object across all the denoising timesteps isinsufficient to generate desired objects; 2) The controllability of objectgeneration is not guaranteed in the intricate sampling space of diffusionmodel. In this paper, we propose to decompose the typical single-stage objectinpainting into two cascaded processes: 1) semantic pre-inpainting that infersthe semantic features of desired objects in a multi-modal feature space; 2)high-fieldity object generation in diffusion latent space that pivots on suchinpainted semantic features. To achieve this, we cascade a Transformer-basedsemantic inpainter and an object inpainting diffusion model, leading to a novelCAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided objectinpainting. Technically, the semantic inpainter is trained to predict thesemantic features of the target object conditioning on unmasked context andtext prompt. The outputs of the semantic inpainter then act as the informativevisual prompts to guide high-fieldity object generation through a referenceadapter layer, leading to controllable object inpainting. Extensive evaluationson OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion againstthe state-of-the-art methods. Code is available at\url{https://github.com/Nnn-s/CATdiffusion}.</description><author>Yifu Chen, Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Zhineng Chen, Tao Mei</author><pubDate>Thu, 12 Sep 2024 17:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08260v1</guid></item><item><title>Improving Virtual Try-On with Garment-focused Diffusion Models</title><link>http://arxiv.org/abs/2409.08258v1</link><description>Diffusion models have led to the revolutionizing of generative modeling innumerous image synthesis tasks. Nevertheless, it is not trivial to directlyapply diffusion models for synthesizing an image of a target person wearing agiven in-shop garment, i.e., image-based virtual try-on (VTON) task. Thedifficulty originates from the aspect that the diffusion process should notonly produce holistically high-fidelity photorealistic image of the targetperson, but also locally preserve every appearance and texture detail of thegiven garment. To address this, we shape a new Diffusion model, namely GarDiff,which triggers the garment-focused diffusion process with amplified guidance ofboth basic visual appearance and detailed textures (i.e., high-frequencydetails) derived from the given garment. GarDiff first remoulds a pre-trainedlatent diffusion model with additional appearance priors derived from the CLIPand VAE encodings of the reference garment. Meanwhile, a novel garment-focusedadapter is integrated into the UNet of diffusion model, pursuing localfine-grained alignment with the visual appearance of reference garment andhuman pose. We specifically design an appearance loss over the synthesizedgarment to enhance the crucial, high-frequency details. Extensive experimentson VITON-HD and DressCode datasets demonstrate the superiority of our GarDiffwhen compared to state-of-the-art VTON approaches. Code is publicly availableat:\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.</description><author>Siqi Wan, Yehao Li, Jingwen Chen, Yingwei Pan, Ting Yao, Yang Cao, Tao Mei</author><pubDate>Thu, 12 Sep 2024 17:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08258v1</guid></item><item><title>LoRID: Low-Rank Iterative Diffusion for Adversarial Purification</title><link>http://arxiv.org/abs/2409.08255v1</link><description>This work presents an information-theoretic examination of diffusion-basedpurification methods, the state-of-the-art adversarial defenses that utilizediffusion models to remove malicious perturbations in adversarial examples. Bytheoretically characterizing the inherent purification errors associated withthe Markov-based diffusion purifications, we introduce LoRID, a novel Low-RankIterative Diffusion purification method designed to remove adversarialperturbation with low intrinsic purification errors. LoRID centers around amulti-stage purification process that leverages multiple rounds ofdiffusion-denoising loops at the early time-steps of the diffusion models, andthe integration of Tucker decomposition, an extension of matrix factorization,to remove adversarial noise at high-noise regimes. Consequently, LoRIDincreases the effective diffusion time-steps and overcomes strong adversarialattacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ,and ImageNet datasets under both white-box and black-box settings.</description><author>Geigh Zollicoffer, Minh Vu, Ben Nebgen, Juan Castorena, Boian Alexandrov, Manish Bhattarai</author><pubDate>Thu, 12 Sep 2024 17:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08255v1</guid></item><item><title>The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting</title><link>http://arxiv.org/abs/2409.08253v1</link><description>The question of how cyber-physical systems should interact with humanpartners that can take over control or exert oversight is becoming morepressing, as these systems are deployed for an ever larger range of tasks.Drawing on the literatures on handing over control during semi-autonomousdriving and human-robot interaction, we propose a design of a take-over requestthat combines an abstract pre-alert with an informative TOR: Relevant sensorinformation is highlighted on the controller's display, while a spoken messageverbalizes the reason for the TOR. We conduct our study in the context of asemi-autonomous drone control scenario as our testbed. The goal of our onlinestudy is to assess in more detail what form a language-based TOR should take.Specifically, we compare a full sentence condition to shorter fragments, andtest whether the visual highlighting should be done synchronously orasynchronously with the speech. Participants showed a higher accuracy inchoosing the correct solution with our bi-modal TOR and felt that they werebetter able to recognize the critical situation. Using only fragments in thespoken message rather than full sentences did not lead to improved accuracy orfaster reactions. Also, synchronizing the visual highlighting with the spokenmessage did not result in better accuracy and response times were evenincreased in this condition.</description><author>Ashwini Gundappa, Emilia Ellsiepen, Lukas Schmitz, Frederik Wiehr, Vera Demberg</author><pubDate>Thu, 12 Sep 2024 17:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08253v1</guid></item><item><title>A Transfer Attack to Image Watermarks</title><link>http://arxiv.org/abs/2403.15365v3</link><description>Watermark has been widely deployed by industry to detect AI-generated images.The robustness of such watermark-based detector against evasion attacks in thewhite-box and black-box settings is well understood in the literature. However,the robustness in the no-box setting is much less understood. In this work, wepropose a new transfer evasion attack to image watermark in the no-box setting.Our transfer attack adds a perturbation to a watermarked image to evademultiple surrogate watermarking models trained by the attacker itself, and theperturbed watermarked image also evades the target watermarking model. Ourmajor contribution is to show that, both theoretically and empirically,watermark-based AI-generated image detector is not robust to evasion attackseven if the attacker does not have access to the watermarking model nor thedetection API.</description><author>Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong</author><pubDate>Thu, 12 Sep 2024 17:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15365v3</guid></item><item><title>Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding</title><link>http://arxiv.org/abs/2409.08251v1</link><description>Panoptic narrative grounding (PNG), whose core target is fine-grainedimage-text alignment, requires a panoptic segmentation of referred objectsgiven a narrative caption. Previous discriminative methods achieve only weak orcoarse-grained alignment by panoptic segmentation pretraining or CLIP modeladaptation. Given the recent progress of text-to-image Diffusion models,several works have shown their capability to achieve fine-grained image-textalignment through cross-attention maps and improved general segmentationperformance. However, the direct use of phrase features as static prompts toapply frozen Diffusion models to the PNG task still suffers from a large taskgap and insufficient vision-language interaction, yielding inferiorperformance. Therefore, we propose an Extractive-Injective Phrase Adapter(EIPA) bypass within the Diffusion UNet to dynamically update phrase promptswith image features and inject the multimodal cues back, which leverages thefine-grained image-text alignment capability of Diffusion models moresufficiently. In addition, we also design a Multi-Level Mutual Aggregation(MLMA) module to reciprocally fuse multi-level image and phrase features forsegmentation refinement. Extensive experiments on the PNG benchmark show thatour method achieves new state-of-the-art performance.</description><author>Hongyu Li, Tianrui Hui, Zihan Ding, Jing Zhang, Bin Ma, Xiaoming Wei, Jizhong Han, Si Liu</author><pubDate>Thu, 12 Sep 2024 17:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08251v1</guid></item><item><title>OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering</title><link>http://arxiv.org/abs/2409.08250v1</link><description>People often capture memories through photos, screenshots, and videos. Whileexisting AI-based tools enable querying this data using natural language, theymostly only support retrieving individual pieces of information like certainobjects in photos and struggle with answering more complex queries that involveinterpreting interconnected memories like event sequences. We conducted aone-month diary study to collect realistic user queries and generated ataxonomy of necessary contextual information for integrating with capturedmemories. We then introduce OmniQuery, a novel system that is able to answercomplex personal memory-related questions that require extracting and inferringcontextual information. OmniQuery augments single captured memories throughintegrating scattered contextual information from multiple interconnectedmemories, retrieves relevant memories, and uses a large language model (LLM) tocomprehensive answers. In human evaluations, we show the effectiveness ofOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAGsystem, winning or tying in 74.5% of the time.</description><author>Jiahao Nick Li, Zhuohao, Zhang, Jiaju Ma</author><pubDate>Thu, 12 Sep 2024 17:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08250v1</guid></item><item><title>TextBoost: Towards One-Shot Personalization of Text-to-Image Models via Fine-tuning Text Encoder</title><link>http://arxiv.org/abs/2409.08248v1</link><description>Recent breakthroughs in text-to-image models have opened up promisingresearch avenues in personalized image generation, enabling users to creatediverse images of a specific subject using natural language prompts. However,existing methods often suffer from performance degradation when given only asingle reference image. They tend to overfit the input, producing highlysimilar outputs regardless of the text prompt. This paper addresses thechallenge of one-shot personalization by mitigating overfitting, enabling thecreation of controllable images through text prompts. Specifically, we proposea selective fine-tuning strategy that focuses on the text encoder. Furthermore,we introduce three key techniques to enhance personalization performance: (1)augmentation tokens to encourage feature disentanglement and alleviateoverfitting, (2) a knowledge-preservation loss to reduce language drift andpromote generalizability across diverse prompts, and (3) SNR-weighted samplingfor efficient training. Extensive experiments demonstrate that our approachefficiently generates high-quality, diverse images using only a singlereference image while significantly reducing memory and storage requirements.</description><author>NaHyeon Park, Kunhee Kim, Hyunjung Shim</author><pubDate>Thu, 12 Sep 2024 17:47:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08248v1</guid></item><item><title>Style Based Clustering of Visual Artworks</title><link>http://arxiv.org/abs/2409.08245v1</link><description>Clustering artworks based on style has many potential real-world applicationslike art recommendations, style-based search and retrieval, and the study ofartistic style evolution in an artwork corpus. However, clustering artworksbased on style is largely an unaddressed problem. A few present methods forclustering artworks principally rely on generic image feature representationsderived from deep neural networks and do not specifically deal with theartistic style. In this paper, we introduce and deliberate over the notion ofstyle-based clustering of visual artworks. Our main objective is to exploreneural feature representations and architectures that can be used forstyle-based clustering and observe their impact and effectiveness. We developdifferent methods and assess their relative efficacy for style-based clusteringthrough qualitative and quantitative analysis by applying them to four artworkcorpora and four curated synthetically styled datasets. Our analysis providessome key novel insights on architectures, feature representations, andevaluation methods suitable for style-based clustering.</description><author>Abhishek Dangeti, Pavan Gajula, Vivek Srivastava, Vikram Jamwal</author><pubDate>Thu, 12 Sep 2024 17:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08245v1</guid></item><item><title>IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation</title><link>http://arxiv.org/abs/2409.08240v1</link><description>While Text-to-Image (T2I) diffusion models excel at generating visuallyappealing images of individual instances, they struggle to accurately positionand control the features generation of multiple instances. The Layout-to-Image(L2I) task was introduced to address the positioning challenges byincorporating bounding boxes as spatial control signals, but it still fallsshort in generating precise instance features. In response, we propose theInstance Feature Generation (IFG) task, which aims to ensure both positionalaccuracy and feature fidelity in generated instances. To address the IFG task,we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhancesfeature depiction by incorporating additional appearance tokens and utilizingan Instance Semantic Map to align instance-level features with spatiallocations. The IFAdapter guides the diffusion process as a plug-and-playmodule, making it adaptable to various community models. For evaluation, wecontribute an IFG benchmark and develop a verification pipeline to objectivelycompare models' abilities to generate instances with accurate positioning andfeatures. Experimental results demonstrate that IFAdapter outperforms othermodels in both quantitative and qualitative evaluations.</description><author>Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, Xinchao Wang</author><pubDate>Thu, 12 Sep 2024 17:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08240v1</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v1</link><description>Large Language Models still struggle in challenging scenarios that leveragestructured data, complex reasoning, or tool usage. In this paper, we proposeSource2Synth: a new method that can be used for teaching LLMs new skillswithout relying on costly human annotations. Source2Synth takes as input acustom data source and produces synthetic data points with intermediatereasoning steps grounded in real-world sources. Source2Synth improves thedataset quality by discarding low-quality generations based on theiranswerability. We demonstrate the generality of this approach by applying it totwo challenging domains: we test reasoning abilities in multi-hop questionanswering (MHQA), and tool usage in tabular question answering (TQA). Ourmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA onHotPotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Thu, 12 Sep 2024 17:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v1</guid></item><item><title>Adaptive Manipulation using Behavior Trees</title><link>http://arxiv.org/abs/2406.14634v2</link><description>Many manipulation tasks pose a challenge since they depend on non-visualenvironmental information that can only be determined after sustained physicalinteraction has already begun. This is particularly relevant foreffort-sensitive, dynamics-dependent tasks such as tightening a valve. Toperform these tasks safely and reliably, robots must be able to quickly adaptin response to unexpected changes during task execution. Humans can intuitivelyrespond and adapt their manipulation strategy to suit such problems, butrepresenting and implementing such behaviors for robots remains an openquestion. We present the adaptive behavior tree, which enables a robot toquickly adapt to both visual and non-visual observations during task execution,preempting task failure or switching to a different strategy based on data fromprevious attempts. We test our approach on a number of tasks commonly found inindustrial settings. Our results demonstrate safety, robustness (100% successrate for all but one experiment) and efficiency in task completion (eg, anoverall task speedup of 46% on average for valve tightening), and would reducedependency on human supervision and intervention.</description><author>Jacques Cloete, Wolfgang Merkt, Ioannis Havoutis</author><pubDate>Thu, 12 Sep 2024 17:37:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14634v2</guid></item><item><title>Multi-Model based Federated Learning Against Model Poisoning Attack: A Deep Learning Based Model Selection for MEC Systems</title><link>http://arxiv.org/abs/2409.08237v1</link><description>Federated Learning (FL) enables training of a global model from distributeddata, while preserving data privacy. However, the singular-model basedoperation of FL is open with uploading poisoned models compatible with theglobal model structure and can be exploited as a vulnerability to conduct modelpoisoning attacks. This paper proposes a multi-model based FL as a proactivemechanism to enhance the opportunity of model poisoning attack mitigation. Amaster model is trained by a set of slave models. To enhance the opportunity ofattack mitigation, the structure of client models dynamically change withinlearning epochs, and the supporter FL protocol is provided. For a MEC system,the model selection problem is modeled as an optimization to minimize loss andrecognition time, while meeting a robustness confidence. In adaption withdynamic network condition, a deep reinforcement learning based model selectionis proposed. For a DDoS attack detection scenario, results illustrate acompetitive accuracy gain under poisoning attack with the scenario that thesystem is without attack, and also a potential of recognition time improvement.</description><author>Somayeh Kianpisheh, Chafika Benzaid, Tarik Taleb</author><pubDate>Thu, 12 Sep 2024 17:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08237v1</guid></item><item><title>TeXBLEU: Automatic Metric for Evaluate LaTeX Format</title><link>http://arxiv.org/abs/2409.06639v2</link><description>LaTeX is suitable for creating specially formatted documents in science,technology, mathematics, and computer science. Although the use of mathematicalexpressions in LaTeX format along with language models is increasing, there areno proper evaluation matrices to evaluate them. In this study, we proposeTeXBLEU, a metric for evaluating mathematical expressions in the LaTeX formatbuilt on the n-gram-based BLEU metric widely used in translation tasks. Theproposed TeXBLEU consists of a predefined tokenizer trained on the arXiv paperdataset and a fine-tuned embedding model with positional encoding. The TeXBLEUscore was calculated by replacing BLUE's modified precision score with thesimilarity of n-gram-based tokens. TeXBLEU showed improvements of 86\%, 121\%,and 610\% over traditional evaluation metrics, such as BLEU, sacreBLEU, andRouge, respectively, on the MathBridge dataset with 1,000 data points. The codeis available at https://github.com/KyuDan1/TeXBLEU.</description><author>Kyudan Jung, Nam-Joon Kim, Hyongon Ryu, Sieun Hyeon, Seung-jun Lee, Hyeok-jae Lee</author><pubDate>Thu, 12 Sep 2024 17:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06639v2</guid></item><item><title>LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems</title><link>http://arxiv.org/abs/2409.08234v1</link><description>The rapid evolution of cyber threats necessitates innovative solutions fordetecting and analyzing malicious activity. Honeypots, which are decoy systemsdesigned to lure and interact with attackers, have emerged as a criticalcomponent in cybersecurity. In this paper, we present a novel approach tocreating realistic and interactive honeypot systems using Large Language Models(LLMs). By fine-tuning a pre-trained open-source language model on a diversedataset of attacker-generated commands and responses, we developed a honeypotcapable of sophisticated engagement with attackers. Our methodology involvedseveral key steps: data collection and processing, prompt engineering, modelselection, and supervised fine-tuning to optimize the model's performance.Evaluation through similarity metrics and live deployment demonstrated that ourapproach effectively generates accurate and informative responses. The resultshighlight the potential of LLMs to revolutionize honeypot technology, providingcybersecurity professionals with a powerful tool to detect and analyzemalicious activity, thereby enhancing overall security infrastructure.</description><author>Hakan T. Otal, M. Abdullah Canbaz</author><pubDate>Thu, 12 Sep 2024 17:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08234v1</guid></item><item><title>Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging</title><link>http://arxiv.org/abs/2409.08232v1</link><description>Segmenting brain tumors in multi-parametric magnetic resonance imagingenables performing quantitative analysis in support of clinical trials andpersonalized patient care. This analysis provides the potential to impactclinical decision-making processes, including diagnosis and prognosis. In 2023,the well-established Brain Tumor Segmentation (BraTS) challenge presented asubstantial expansion with eight tasks and 4,500 brain tumor cases. In thispaper, we present a deep learning-based ensemble strategy that is evaluated fornewly included tumor cases in three tasks: pediatric brain tumors (PED),intracranial meningioma (MEN), and brain metastases (MET). In particular, weensemble outputs from state-of-the-art nnU-Net and Swin UNETR models on aregion-wise basis. Furthermore, we implemented a targeted post-processingstrategy based on a cross-validated threshold search to improve thesegmentation results for tumor sub-regions. The evaluation of our proposedmethod on unseen test cases for the three tasks resulted in lesion-wise Dicescores for PED: 0.653, 0.809, 0.826; MEN: 0.876, 0.867, 0.849; and MET: 0.555,0.6, 0.58; for the enhancing tumor, tumor core, and whole tumor, respectively.Our method was ranked first for PED, third for MEN, and fourth for MET,respectively.</description><author>Daniel Capellán-Martín, Zhifan Jiang, Abhijeet Parida, Xinyang Liu, Van Lam, Hareem Nisar, Austin Tapp, Sarah Elsharkawi, Maria J. Ledesma-Carbayo, Syed Muhammad Anwar, Marius George Linguraru</author><pubDate>Thu, 12 Sep 2024 17:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08232v1</guid></item><item><title>Design Optimization of Nuclear Fusion Reactor through Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2409.08231v1</link><description>This research explores the application of Deep Reinforcement Learning (DRL)to optimize the design of a nuclear fusion reactor. DRL can efficiently addressthe challenging issues attributed to multiple physics and engineeringconstraints for steady-state operation. The fusion reactor design computationand the optimization code applicable to parallelization with DRL are developed.The proposed framework enables finding the optimal reactor design thatsatisfies the operational requirements while reducing building costs.Multi-objective design optimization for a fusion reactor is now simplified byDRL, indicating the high potential of the proposed framework for advancing theefficient and sustainable design of future reactors.</description><author>Jinsu Kim, Jaemin Seo</author><pubDate>Thu, 12 Sep 2024 17:23:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08231v1</guid></item><item><title>Photonic Quantum Computers</title><link>http://arxiv.org/abs/2409.08229v1</link><description>In the pursuit of scalable and fault-tolerant quantum computingarchitectures, photonic-based quantum computers have emerged as a leadingfrontier. This article provides a comprehensive overview of advancements inphotonic quantum computing, developed by leading industry players, examiningcurrent performance, architectural designs, and strategies for developinglarge-scale, fault-tolerant photonic quantum computers. It also highlightsrecent groundbreaking experiments that leverage the unique advantages ofphotonic technologies, underscoring their transformative potential. This reviewcaptures a pivotal moment of photonic quantum computing in the noisyintermediate-scale quantum (NISQ) era, offering insights into how photonicquantum computers might reshape the future of quantum computing.</description><author>M. AbuGhanem</author><pubDate>Thu, 12 Sep 2024 17:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08229v1</guid></item><item><title>What Makes a Face Look like a Hat: Decoupling Low-level and High-level Visual Properties with Image Triplets</title><link>http://arxiv.org/abs/2409.02241v2</link><description>In visual decision making, high-level features, such as object categories,have a strong influence on choice. However, the impact of low-level features onbehavior is less understood partly due to the high correlation between high-and low-level features in the stimuli presented (e.g., objects of the samecategory are more likely to share low-level features). To disentangle theseeffects, we propose a method that de-correlates low- and high-level visualproperties in a novel set of stimuli. Our method uses two Convolutional NeuralNetworks (CNNs) as candidate models of the ventral visual stream: the CORnet-Sthat has high neural predictivity in high-level, IT-like responses and theVGG-16 that has high neural predictivity in low-level responses. Triplets(root, image1, image2) of stimuli are parametrized by the level of low- andhigh-level similarity of images extracted from the different layers. Thesestimuli are then used in a decision-making task where participants are taskedto choose the most similar-to-the-root image. We found that different networksshow differing abilities to predict the effects of low-versus-high-levelsimilarity: while CORnet-S outperforms VGG-16 in explaining human choices basedon high-level similarity, VGG-16 outperforms CORnet-S in explaining humanchoices based on low-level similarity. Using Brain-Score, we observed that thebehavioral prediction abilities of different layers of these networksqualitatively corresponded to their ability to explain neural activity atdifferent levels of the visual hierarchy. In summary, our algorithm forstimulus set generation enables the study of how different representations inthe visual stream affect high-level cognitive behaviors.</description><author>Maytus Piriyajitakonkij, Sirawaj Itthipuripat, Ian Ballard, Ioannis Pappas</author><pubDate>Thu, 12 Sep 2024 17:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02241v2</guid></item><item><title>Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience</title><link>http://arxiv.org/abs/2404.03084v2</link><description>Teacher-Student Curriculum Learning (TSCL) is a curriculum learning frameworkthat draws inspiration from human cultural transmission and learning. Itinvolves a teacher algorithm shaping the learning process of a learneralgorithm by exposing it to controlled experiences. Despite its success,understanding the conditions under which TSCL is effective remains challenging.In this paper, we propose a data-centric perspective to analyze the underlyingmechanics of the teacher-student interactions in TSCL. We leverage cooperativegame theory to describe how the composition of the set of experiences presentedby the teacher to the learner, as well as their order, influences theperformance of the curriculum that is found by TSCL approaches. To do so, wedemonstrate that for every TSCL problem, an equivalent cooperative game exists,and several key components of the TSCL framework can be reinterpreted usinggame-theoretic principles. Through experiments covering supervised learning,reinforcement learning, and classical games, we estimate the cooperative valuesof experiences and use value-proportional curriculum mechanisms to constructcurricula, even in cases where TSCL struggles. The framework and experimentalsetup we present in this work represents a novel foundation for a deeperexploration of TSCL, shedding light on its underlying mechanisms and providinginsights into its broader applicability in machine learning.</description><author>Manfred Diaz, Liam Paull, Andrea Tacchetti</author><pubDate>Thu, 12 Sep 2024 16:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03084v2</guid></item><item><title>CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs</title><link>http://arxiv.org/abs/2409.08217v1</link><description>Graph neural networks have become the default choice by practitioners forgraph learning tasks such as graph classification and node classification.Nevertheless, popular graph neural network models still struggle to capturehigher-order information, i.e., information that goes \emph{beyond} pairwiseinteractions. Recent work has shown that persistent homology, a tool fromtopological data analysis, can enrich graph neural networks with topologicalinformation that they otherwise could not capture. Calculating such features isefficient for dimension 0 (connected components) and dimension 1 (cycles).However, when it comes to higher-order structures, it does not scale well, witha complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the orderof the structures. In this work, we introduce a novel method that extractsinformation about higher-order structures in the graph while still using theefficient low-dimensional persistent homology algorithm. On standard benchmarkdatasets, we show that our method can lead to up to $31\%$ improvements in testaccuracy.</description><author>Davide Buffelli, Farzin Soleymani, Bastian Rieck</author><pubDate>Thu, 12 Sep 2024 16:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08217v1</guid></item><item><title>LT3SD: Latent Trees for 3D Scene Diffusion</title><link>http://arxiv.org/abs/2409.08215v1</link><description>We present LT3SD, a novel latent diffusion model for large-scale 3D scenegeneration. Recent advances in diffusion models have shown impressive resultsin 3D object generation, but are limited in spatial extent and quality whenextended to 3D scenes. To generate complex and diverse 3D scene structures, weintroduce a latent tree representation to effectively encode bothlower-frequency geometry and higher-frequency detail in a coarse-to-finehierarchy. We can then learn a generative diffusion process in this latent 3Dscene space, modeling the latent components of a scene at each resolutionlevel. To synthesize large-scale scenes with varying sizes, we train ourdiffusion model on scene patches and synthesize arbitrary-sized output 3Dscenes through shared diffusion generation across multiple scene patches.Through extensive experiments, we demonstrate the efficacy and benefits ofLT3SD for large-scale, high-quality unconditional 3D scene generation and forprobabilistic completion for partial scene observations.</description><author>Quan Meng, Lei Li, Matthias Nießner, Angela Dai</author><pubDate>Thu, 12 Sep 2024 16:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08215v1</guid></item><item><title>Adaptive Language-Guided Abstraction from Contrastive Explanations</title><link>http://arxiv.org/abs/2409.08212v1</link><description>Many approaches to robot learning begin by inferring a reward function from aset of human demonstrations. To learn a good reward, it is necessary todetermine which features of the environment are relevant before determining howthese features should be used to compute reward. End-to-end methods for jointfeature and reward learning (e.g., using deep networks or program synthesistechniques) often yield brittle reward functions that are sensitive to spuriousstate features. By contrast, humans can often generalizably learn from a smallnumber of demonstrations by incorporating strong priors about what features ofa demonstration are likely meaningful for a task of interest. How do we buildrobots that leverage this kind of background knowledge when learning from newdemonstrations? This paper describes a method named ALGAE (AdaptiveLanguage-Guided Abstraction from [Contrastive] Explanations) which alternatesbetween using language models to iteratively identify human-meaningful featuresneeded to explain demonstrated behavior, then standard inverse reinforcementlearning techniques to assign weights to these features. Experiments across avariety of both simulated and real-world robot environments show that ALGAElearns generalizable reward functions defined on interpretable features usingonly small numbers of demonstrations. Importantly, ALGAE can recognize whenfeatures are missing, then extract and define those features without any humaninput -- making it possible to quickly and efficiently acquire richrepresentations of user behavior.</description><author>Andi Peng, Belinda Z. Li, Ilia Sucholutsky, Nishanth Kumar, Julie A. Shah, Jacob Andreas, Andreea Bobu</author><pubDate>Thu, 12 Sep 2024 16:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08212v1</guid></item><item><title>Graph Laplacian-based Bayesian Multi-fidelity Modeling</title><link>http://arxiv.org/abs/2409.08211v1</link><description>We present a novel probabilistic approach for generating multi-fidelity datawhile accounting for errors inherent in both low- and high-fidelity data. Inthis approach a graph Laplacian constructed from the low-fidelity data is usedto define a multivariate Gaussian prior density for the coordinates of the truedata points. In addition, few high-fidelity data points are used to construct aconjugate likelihood term. Thereafter, Bayes rule is applied to derive anexplicit expression for the posterior density which is also multivariateGaussian. The maximum \textit{a posteriori} (MAP) estimate of this density isselected to be the optimal multi-fidelity estimate. It is shown that the MAPestimate and the covariance of the posterior density can be determined throughthe solution of linear systems of equations. Thereafter, two methods, one basedon spectral truncation and another based on a low-rank approximation, aredeveloped to solve these equations efficiently. The multi-fidelity approach istested on a variety of problems in solid and fluid mechanics with data thatrepresents vectors of quantities of interest and discretized spatial fields inone and two dimensions. The results demonstrate that by utilizing a smallfraction of high-fidelity data, the multi-fidelity approach can significantlyimprove the accuracy of a large collection of low-fidelity data points.</description><author>Orazio Pinti, Jeremy M. Budd, Franca Hoffmann, Assad A. Oberai</author><pubDate>Thu, 12 Sep 2024 16:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08211v1</guid></item><item><title>Unlocking the Potential of Model Calibration in Federated Learning</title><link>http://arxiv.org/abs/2409.04901v2</link><description>Over the past several years, various federated learning (FL) methodologieshave been developed to improve model accuracy, a primary performance metric inmachine learning. However, to utilize FL in practical decision-makingscenarios, beyond considering accuracy, the trained model must also have areliable confidence in each of its predictions, an aspect that has been largelyoverlooked in existing FL research. Motivated by this gap, we proposeNon-Uniform Calibration for Federated Learning (NUCFL), a generic frameworkthat integrates FL with the concept of model calibration. The inherent dataheterogeneity in FL environments makes model calibration particularlydifficult, as it must ensure reliability across diverse data distributions andclient conditions. Our NUCFL addresses this challenge by dynamically adjustingthe model calibration objectives based on statistical relationships betweeneach client's local model and the global model in FL. In particular, NUCFLassesses the similarity between local and global model relationships, andcontrols the penalty term for the calibration loss during client-side localtraining. By doing so, NUCFL effectively aligns calibration needs for theglobal model in heterogeneous FL settings while not sacrificing accuracy.Extensive experiments show that NUCFL offers flexibility and effectivenessacross various FL algorithms, enhancing accuracy as well as model calibration.</description><author>Yun-Wei Chu, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher Brinton</author><pubDate>Thu, 12 Sep 2024 16:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04901v2</guid></item><item><title>VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis</title><link>http://arxiv.org/abs/2409.08207v1</link><description>Recently, methods like Zero-1-2-3 have focused on single-view based 3Dreconstruction and have achieved remarkable success. However, their predictionsfor unseen areas heavily rely on the inductive bias of large-scale pretraineddiffusion models. Although subsequent work, such as DreamComposer, attempts tomake predictions more controllable by incorporating additional views, theresults remain unrealistic due to feature entanglement in the vanilla latentspace, including factors such as lighting, material, and structure. To addressthese issues, we introduce the Visual Isotropy 3D Reconstruction Model(VI3DRM), a diffusion-based sparse views 3D reconstruction model that operateswithin an ID consistent and perspective-disentangled 3D latent space. Byfacilitating the disentanglement of semantic information, color, materialproperties and lighting, VI3DRM is capable of generating highly realisticimages that are indistinguishable from real photographs. By leveraging bothreal and synthesized images, our approach enables the accurate construction ofpointmaps, ultimately producing finely textured meshes or point clouds. On theNVS task, tested on the GSO dataset, VI3DRM significantly outperformsstate-of-the-art method DreamComposer, achieving a PSNR of 38.61, an SSIM of0.929, and an LPIPS of 0.027. Code will be made available upon publication.</description><author>Hao Chen, Jiafu Wu, Ying Jin, Jinlong Peng, Xiaofeng Mao, Mingmin Chi, Mufeng Yao, Bo Peng, Jian Li, Yun Cao</author><pubDate>Thu, 12 Sep 2024 16:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08207v1</guid></item><item><title>ComAlign: Compositional Alignment in Vision-Language Models</title><link>http://arxiv.org/abs/2409.08206v1</link><description>Vision-language models (VLMs) like CLIP have showcased a remarkable abilityto extract transferable features for downstream tasks. Nonetheless, thetraining process of these models is usually based on a coarse-grainedcontrastive loss between the global embedding of images and texts which maylose the compositional structure of these modalities. Many recent studies haveshown VLMs lack compositional understandings like attribute binding andidentifying object relationships. Although some recent methods have tried toachieve finer-level alignments, they either are not based on extractingmeaningful components of proper granularity or don't properly utilize themodalities' correspondence (especially in image-text pairs with moreingredients). Addressing these limitations, we introduce CompositionalAlignment (ComAlign), a fine-grained approach to discover more exactcorrespondence of text and image components using only the weak supervision inthe form of image-text pairs. Our methodology emphasizes that the compositionalstructure (including entities and relations) extracted from the text modalitymust also be retained in the image modality. To enforce correspondence offine-grained concepts in image and text modalities, we train a lightweightnetwork lying on top of existing visual and language encoders using a smalldataset. The network is trained to align nodes and edges of the structureacross the modalities. Experimental results on various VLMs and datasetsdemonstrate significant improvements in retrieval and compositional benchmarks,affirming the effectiveness of our plugin model.</description><author>Ali Abdollah, Amirmohammad Izadi, Armin Saghafian, Reza Vahidimajd, Mohammad Mozafari, Amirreza Mirzaei, Mohammadmahdi Samiei, Mahdieh Soleymani Baghshah</author><pubDate>Thu, 12 Sep 2024 16:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08206v1</guid></item><item><title>What Makes a Maze Look Like a Maze?</title><link>http://arxiv.org/abs/2409.08202v1</link><description>A unique aspect of human visual understanding is the ability to flexiblyinterpret abstract concepts: acquiring lifted rules explaining what theysymbolize, grounding them across familiar and unfamiliar contexts, and makingpredictions or reasoning about them. While off-the-shelf vision-language modelsexcel at making literal interpretations of images (e.g., recognizing objectcategories such as tree branches), they still struggle to make sense of suchvisual abstractions (e.g., how an arrangement of tree branches may form thewalls of a maze). To address this challenge, we introduce Deep Schema Grounding(DSG), a framework that leverages explicit structured representations of visualabstractions for grounding and reasoning. At the core of DSG areschemas--dependency graph descriptions of abstract concepts that decompose theminto more primitive-level symbols. DSG uses large language models to extractschemas, then hierarchically grounds concrete to abstract components of theschema onto images with vision-language models. The grounded schema is used toaugment visual abstraction understanding. We systematically evaluate DSG anddifferent methods in reasoning on our new Visual Abstractions Dataset, whichconsists of diverse, real-world images of abstract concepts and correspondingquestion-answer pairs labeled by humans. We show that DSG significantlyimproves the abstract visual reasoning performance of vision-language models,and is a step toward human-aligned understanding of visual abstractions.</description><author>Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu</author><pubDate>Thu, 12 Sep 2024 16:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08202v1</guid></item><item><title>Machine Learning for Two-Sample Testing under Right-Censored Data: A Simulation Study</title><link>http://arxiv.org/abs/2409.08201v1</link><description>The focus of this study is to evaluate the effectiveness of Machine Learning(ML) methods for two-sample testing with right-censored observations. Toachieve this, we develop several ML-based methods with varying architecturesand implement them as two-sample tests. Each method is an ensemble (stacking)that combines predictions from classical two-sample tests. This paper presentsthe results of training the proposed ML methods, examines their statisticalpower compared to classical two-sample tests, analyzes the distribution of teststatistics for the proposed methods when the null hypothesis is true, andevaluates the significance of the features incorporated into the proposedmethods. All results from numerical experiments were obtained from a syntheticdataset generated using the Smirnov transform (Inverse Transform Sampling) andreplicated multiple times through Monte Carlo simulation. To test thetwo-sample problem with right-censored observations, one can use the proposedtwo-sample methods. All necessary materials (source code, example scripts,dataset, and samples) are available on GitHub and Hugging Face.</description><author>Petr Philonenko, Sergey Postovalov</author><pubDate>Thu, 12 Sep 2024 16:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08201v1</guid></item><item><title>AudioBERT: Audio Knowledge Augmented Language Model</title><link>http://arxiv.org/abs/2409.08199v1</link><description>Recent studies have identified that language models, pretrained on text-onlydatasets, often lack elementary visual knowledge, \textit{e.g.,} colors ofeveryday objects. Motivated by this observation, we ask whether a similarshortcoming exists in terms of the \textit{auditory} knowledge. To answer thisquestion, we construct a new dataset called AuditoryBench, which consists oftwo novel tasks for evaluating auditory knowledge. Based on our analysis usingthe benchmark, we find that language models also suffer from a severe lack ofauditory knowledge. To address this limitation, we propose AudioBERT, a novelmethod to augment the auditory knowledge of BERT through a retrieval-basedapproach. First, we detect auditory knowledge spans in prompts to query ourretrieval model efficiently. Then, we inject audio knowledge into BERT andswitch on low-rank adaptation for effective adaptation when audio knowledge isrequired. Our experiments demonstrate that AudioBERT is quite effective,achieving superior performance on the AuditoryBench. The dataset and code areavailable at \bulurl{https://github.com/HJ-Ok/AudioBERT}.</description><author>Hyunjong Ok, Suho Yoo, Jaeho Lee</author><pubDate>Thu, 12 Sep 2024 16:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08199v1</guid></item><item><title>Profiling checkpointing schedules in adjoint ST-AD</title><link>http://arxiv.org/abs/2405.15590v2</link><description>Checkpointing is a cornerstone of data-flow reversal in adjoint algorithmicdifferentiation. Checkpointing is a storage/recomputation trade-off that can beapplied at different levels, one of which being the call tree. We are lookingfor good placements of checkpoints onto the call tree of a given application,to reduce run time and memory footprint of its adjoint. There is no knownoptimal solution to this problem other than a combinatorial search on allplacements. We propose a heuristics based on run-time profiling of the adjointcode. We describe implementation of this profiling tool in an existingsource-transformation AD tool. We demonstrate the interest of this approach ontest cases taken from the MITgcm ocean and atmospheric global circulationmodel. We discuss the limitations of our approach and propose directions tolift them.</description><author>Laurent Hascoët, Jean-Luc Bouchot, Shreyas Sunil Gaikwad, Sri Hari Krishna Narayanan, Jan Hückelheim</author><pubDate>Thu, 12 Sep 2024 16:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15590v2</guid></item><item><title>DINOv2 Rocks Geological Image Analysis: Classification, Segmentation, and Interpretability</title><link>http://arxiv.org/abs/2407.18100v3</link><description>Recent advancements in computer vision have significantly improved imageanalysis tasks. Yet, deep learning models often struggle when applied todomains outside their training distribution, such as in geosciences, wheredomain-specific data can be scarce. This study investigates the classification,segmentation, and interpretability of CT-scan images of rock samples, focusingon the application of modern computer vision techniques to geoscientific tasks.We compare a range of segmentation methods to assess their efficacy,efficiency, and adaptability in geological image analysis. The methodsevaluated include Otsu thresholding, clustering techniques (K-means, fuzzyC-means), a supervised machine learning approach (Random Forest), and deeplearning models (UNet, ResNet152, and DINOv2), using ten binary sandstonedatasets and three multi-class calcite datasets. DINOv2 was selected for itspromising results in feature extraction and its potential applicability ingeoscientific tasks, prompting further assessment of its interpretability andeffectiveness in processing CT-scanned rock data. For classification, anon-fine-tuned DINOv2 demonstrates strong performance in classifying rockimages, even when the CT-scans are outside its original training set. Insegmentation tasks, thresholding and clustering techniques, thoughcomputationally efficient, produce subpar results despite preprocessingefforts. In contrast, supervised methods achieve better performance. While deeplearning methods demand greater computational resources, they require minimalintervention and offer superior generalization. A LoRA fine-tuned DINOv2, inparticular, excels in out-of-distribution segmentation and outperforms othermethods in multi-class tasks, even with limited data. Notably, the segmentationmasks generated by DINOv2 often appear more accurate than the original targets,based on visual inspection.</description><author>Florent Brondolo, Samuel Beaussant</author><pubDate>Thu, 12 Sep 2024 16:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18100v3</guid></item><item><title>Gaussian Garments: Reconstructing Simulation-Ready Clothing with Photorealistic Appearance from Multi-View Video</title><link>http://arxiv.org/abs/2409.08189v1</link><description>We introduce Gaussian Garments, a novel approach for reconstructing realisticsimulation-ready garment assets from multi-view videos. Our method representsgarments with a combination of a 3D mesh and a Gaussian texture that encodesboth the color and high-frequency surface details. This representation enablesaccurate registration of garment geometries to multi-view videos and helpsdisentangle albedo textures from lighting effects. Furthermore, we demonstratehow a pre-trained graph neural network (GNN) can be fine-tuned to replicate thereal behavior of each garment. The reconstructed Gaussian Garments can beautomatically combined into multi-garment outfits and animated with thefine-tuned GNN.</description><author>Boxiang Rong, Artur Grigorev, Wenbo Wang, Michael J. Black, Bernhard Thomaszewski, Christina Tsalicoglou, Otmar Hilliges</author><pubDate>Thu, 12 Sep 2024 16:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08189v1</guid></item><item><title>Trading Devil Final: Backdoor attack via Stock market and Bayesian Optimization</title><link>http://arxiv.org/abs/2407.14573v5</link><description>Since the advent of generative artificial intelligence, every company andresearcher has been rushing to develop their own generative models, whethercommercial or not. Given the large number of users of these powerful new tools,there is currently no intrinsically verifiable way to explain from the groundup what happens when LLMs (large language models) learn. For example, thosebased on automatic speech recognition systems, which have to rely on huge andastronomical amounts of data collected from all over the web to produce fastand efficient results, In this article, we develop a backdoor attack calledMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 ismainly based on modern stock market models. In order to show the possiblevulnerabilities of speech-based transformers that may rely on LLMs.</description><author>Orson Mengara</author><pubDate>Thu, 12 Sep 2024 16:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14573v5</guid></item><item><title>Fine-tuning Large Language Models for Entity Matching</title><link>http://arxiv.org/abs/2409.08185v1</link><description>Generative large language models (LLMs) are a promising alternative topre-trained language models for entity matching due to their high zero-shotperformance and their ability to generalize to unseen entities. Existingresearch on using LLMs for entity matching has focused on prompt engineeringand in-context learning. This paper explores the potential of fine-tuning LLMsfor entity matching. We analyze fine-tuning along two dimensions: 1) Therepresentation of training examples, where we experiment with adding differenttypes of LLM-generated explanations to the training set, and 2) the selectionand generation of training examples using LLMs. In addition to the matchingperformance on the source dataset, we investigate how fine-tuning affects themodel's ability to generalize to other in-domain datasets as well as acrosstopical domains. Our experiments show that fine-tuning significantly improvesthe performance of the smaller models while the results for the larger modelsare mixed. Fine-tuning also improves the generalization to in-domain datasetswhile hurting cross-domain transfer. We show that adding structuredexplanations to the training set has a positive impact on the performance ofthree out of four LLMs, while the proposed example selection and generationmethods only improve the performance of Llama 3.1 8B while decreasing theperformance of GPT-4o Mini.</description><author>Aaron Steiner, Ralph Peeters, Christian Bizer</author><pubDate>Thu, 12 Sep 2024 16:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08185v1</guid></item><item><title>Enhancing Canine Musculoskeletal Diagnoses: Leveraging Synthetic Image Data for Pre-Training AI-Models on Visual Documentations</title><link>http://arxiv.org/abs/2409.08181v1</link><description>The examination of the musculoskeletal system in dogs is a challenging taskin veterinary practice. In this work, a novel method has been developed thatenables efficient documentation of a dog's condition through a visualrepresentation. However, since the visual documentation is new, there is noexisting training data. The objective of this work is therefore to mitigate theimpact of data scarcity in order to develop an AI-based diagnostic supportsystem. To this end, the potential of synthetic data that mimics realisticvisual documentations of diseases for pre-training AI models is investigated.We propose a method for generating synthetic image data that mimics realisticvisual documentations. Initially, a basic dataset containing three distinctclasses is generated, followed by the creation of a more sophisticated datasetcontaining 36 different classes. Both datasets are used for the pre-training ofan AI model. Subsequently, an evaluation dataset is created, consisting of 250manually created visual documentations for five different diseases. Thisdataset, along with a subset containing 25 examples. The obtained results onthe evaluation dataset containing 25 examples demonstrate a significantenhancement of approximately 10% in diagnosis accuracy when utilizing generatedsynthetic images that mimic real-world visual documentations. However, theseresults do not hold true for the larger evaluation dataset containing 250examples, indicating that the advantages of using synthetic data forpre-training an AI model emerge primarily when dealing with few examples ofvisual documentations for a given disease. Overall, this work provides valuableinsights into mitigating the limitations imposed by limited training datathrough the strategic use of generated synthetic data, presenting an approachapplicable beyond the canine musculoskeletal assessment domain.</description><author>Martin Thißen, Thi Ngoc Diep Tran, Ben Joel Schönbein, Ute Trapp, Barbara Esteve Ratsch, Beate Egner, Romana Piat, Elke Hergenröther</author><pubDate>Thu, 12 Sep 2024 16:13:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08181v1</guid></item><item><title>GSIFN: A Graph-Structured and Interlaced-Masked Multimodal Transformer-based Fusion Network for Multimodal Sentiment Analysis</title><link>http://arxiv.org/abs/2408.14809v2</link><description>Multimodal Sentiment Analysis (MSA) leverages multiple data modals to analyzehuman sentiment. Existing MSA models generally employ cutting-edge multimodalfusion and representation learning-based methods to promote MSA capability.However, there are two key challenges: (i) in existing multimodal fusionmethods, the decoupling of modal combinations and tremendous parameterredundancy, lead to insufficient fusion performance and efficiency; (ii) achallenging trade-off exists between representation capability andcomputational overhead in unimodal feature extractors and encoders. Ourproposed GSIFN incorporates two main components to solve these problems: (i) agraph-structured and interlaced-masked multimodal Transformer. It adopts theInterlaced Mask mechanism to construct robust multimodal graph embedding,achieve all-modal-in-one Transformer-based fusion, and greatly reduce thecomputational overhead; (ii) a self-supervised learning framework with lowcomputational overhead and high performance, which utilizes a parallelized LSTMwith matrix memory to enhance non-verbal modal features for unimodal labelgeneration. Evaluated on the MSA datasets CMU-MOSI, CMU-MOSEI, and CH-SIMS,GSIFN demonstrates superior performance with significantly lower computationaloverhead compared with previous state-of-the-art models.</description><author>Yijie Jin</author><pubDate>Thu, 12 Sep 2024 16:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14809v2</guid></item><item><title>Identification of head impact locations, speeds, and force based on head kinematics</title><link>http://arxiv.org/abs/2409.08177v1</link><description>Objective: Head impact information including impact directions, speeds andforce are important to study traumatic brain injury, design and evaluateprotective gears. This study presents a deep learning model developed toaccurately predict head impact information, including location, speed,orientation, and force, based on head kinematics during helmeted impacts.Methods: Leveraging a dataset of 16,000 simulated helmeted head impacts usingthe Riddell helmet finite element model, we implemented a Long Short-TermMemory (LSTM) network to process the head kinematics: tri-axial linearaccelerations and angular velocities. Results: The models accurately predictthe impact parameters describing impact location, direction, speed, and theimpact force profile with R2 exceeding 70% for all tasks. Further validationwas conducted using an on-field dataset recorded by instrumented mouthguardsand videos, consisting of 79 head impacts in which the impact location can beclearly identified. The deep learning model significantly outperformed existingmethods, achieving a 79.7% accuracy in identifying impact locations, comparedto lower accuracies with traditional methods (the highest accuracy of existingmethods is 49.4%). Conclusion: The precision underscores the model's potentialin enhancing helmet design and safety in sports by providing more accurateimpact data. Future studies should test the models across various helmets andsports on large in vivo datasets to validate the accuracy of the models,employing techniques like transfer learning to broaden its effectiveness.</description><author>Xianghao Zhan, Yuzhe Liu, Nicholas J. Cecchi, Jessica Towns, Ashlyn A. Callan, Olivier Gevaert, Michael M. Zeineh, David B. Camarillo</author><pubDate>Thu, 12 Sep 2024 16:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08177v1</guid></item><item><title>DrugAgent: Explainable Drug Repurposing Agent with Large Language Model-based Reasoning</title><link>http://arxiv.org/abs/2408.13378v2</link><description>Drug repurposing offers a promising avenue for accelerating drug developmentby identifying new therapeutic potentials of existing drugs. In this paper, wepropose a multi-agent framework to enhance the drug repurposing process usingstate-of-the-art machine learning techniques and knowledge integration. Ourframework comprises several specialized agents: an AI Agent trains robustdrug-target interaction (DTI) models; a Knowledge Graph Agent utilizes thedrug-gene interaction database (DGIdb), DrugBank, Comparative ToxicogenomicsDatabase (CTD), and Search Tool for Interactions of Chemicals (STITCH) tosystematically extract DTIs; and a Search Agent interacts with biomedicalliterature to annotate and verify computational predictions. By integratingoutputs from these agents, our system effectively harnesses diverse datasources, including external databases, to propose viable repurposingcandidates. Preliminary results demonstrate the potential of our approach innot only predicting drug-disease interactions but also in reducing the time andcost associated with traditional drug discovery methods. This paper highlightsthe scalability of multi-agent systems in biomedical research and their role indriving innovation in drug repurposing. Our approach not only outperformsexisting methods in predicting drug repurposing potential but also providesinterpretable results, paving the way for more efficient and cost-effectivedrug discovery processes.</description><author>Yoshitaka Inoue, Tianci Song, Tianfan Fu</author><pubDate>Thu, 12 Sep 2024 16:06:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13378v2</guid></item><item><title>Low-Cost Tree Crown Dieback Estimation Using Deep Learning-Based Segmentation</title><link>http://arxiv.org/abs/2409.08171v1</link><description>The global increase in observed forest dieback, characterised by the death oftree foliage, heralds widespread decline in forest ecosystems. This degradationcauses significant changes to ecosystem services and functions, includinghabitat provision and carbon sequestration, which can be difficult to detectusing traditional monitoring techniques, highlighting the need for large-scaleand high-frequency monitoring. Contemporary developments in the instruments andmethods to gather and process data at large-scales mean this monitoring is nowpossible. In particular, the advancement of low-cost drone technology and deeplearning on consumer-level hardware provide new opportunities. Here, we use anapproach based on deep learning and vegetation indices to assess crown diebackfrom RGB aerial data without the need for expensive instrumentation such asLiDAR. We use an iterative approach to match crown footprints predicted by deeplearning with field-based inventory data from a Mediterranean ecosystemexhibiting drought-induced dieback, and compare expert field-based crowndieback estimation with vegetation index-based estimates. We obtain highoverall segmentation accuracy (mAP: 0.519) without the need for additionaltechnical development of the underlying Mask R-CNN model, underscoring thepotential of these approaches for non-expert use and proving theirapplicability to real-world conservation. We also find colour-coordinate basedestimates of dieback correlate well with expert field-based estimation.Substituting ground truth for Mask R-CNN model predictions showed negligibleimpact on dieback estimates, indicating robustness. Our findings demonstratethe potential of automated data collection and processing, including theapplication of deep learning, to improve the coverage, speed and cost of forestdieback monitoring.</description><author>M. J. Allen, D. Moreno-Fernández, P. Ruiz-Benito, S. W. D. Grieve, E. R. Lines</author><pubDate>Thu, 12 Sep 2024 16:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08171v1</guid></item><item><title>AD-Lite Net: A Lightweight and Concatenated CNN Model for Alzheimer's Detection from MRI Images</title><link>http://arxiv.org/abs/2409.08170v1</link><description>Alzheimer's Disease (AD) is a non-curable progressive neurodegenerativedisorder that affects the human brain, leading to a decline in memory,cognitive abilities, and eventually, the ability to carry out daily tasks.Manual diagnosis of Alzheimer's disease from MRI images is fraught with lesssensitivity and it is a very tedious process for neurologists. Therefore, thereis a need for an automatic Computer Assisted Diagnosis (CAD) system, which candetect AD at early stages with higher accuracy. In this research, we haveproposed a novel AD-Lite Net model (trained from scratch), that could alleviatethe aforementioned problem. The novelties we bring here in this research are,(I) We have proposed a very lightweight CNN model by incorporating Depth WiseSeparable Convolutional (DWSC) layers and Global Average Pooling (GAP) layers.(II) We have leveraged a ``parallel concatenation block'' (pcb), in theproposed AD-Lite Net model. This pcb consists of a Transformation layer(Tx-layer), followed by two convolutional layers, which are therebyconcatenated with the original base model. This Tx-layer converts the featuresinto very distinct kind of features, which are imperative for the Alzheimer'sdisease. As a consequence, the proposed AD-Lite Net model with ``parallelconcatenation'' converges faster and automatically mitigates the classimbalance problem from the MRI datasets in a very generalized way. For thevalidity of our proposed model, we have implemented it on three different MRIdatasets. Furthermore, we have combined the ADNI and AD datasets andsubsequently performed a 10-fold cross-validation experiment to verify themodel's generalization ability. Extensive experimental results showed that ourproposed model has outperformed all the existing CNN models, and one recenttrend Vision Transformer (ViT) model by a significant margin.</description><author>Santanu Roy, Archit Gupta, Shubhi Tiwari, Palak Sahu</author><pubDate>Thu, 12 Sep 2024 16:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08170v1</guid></item><item><title>Learning to Match 2D Keypoints Across Preoperative MR and Intraoperative Ultrasound</title><link>http://arxiv.org/abs/2409.08169v1</link><description>We propose in this paper a texture-invariant 2D keypoints descriptorspecifically designed for matching preoperative Magnetic Resonance (MR) imageswith intraoperative Ultrasound (US) images. We introduce amatching-by-synthesis strategy, where intraoperative US images are synthesizedfrom MR images accounting for multiple MR modalities and intraoperative USvariability. We build our training set by enforcing keypoints localization overall images then train a patient-specific descriptor network that learnstexture-invariant discriminant features in a supervised contrastive manner,leading to robust keypoints descriptors. Our experiments on real cases withground truth show the effectiveness of the proposed approach, outperforming thestate-of-the-art methods and achieving 80.35% matching precision on average.</description><author>Hassan Rasheed, Reuben Dorent, Maximilian Fehrentz, Tina Kapur, William M. Wells III, Alexandra Golby, Sarah Frisken, Julia A. Schnabel, Nazim Haouchine</author><pubDate>Thu, 12 Sep 2024 16:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08169v1</guid></item><item><title>Explicit Mutual Information Maximization for Self-Supervised Learning</title><link>http://arxiv.org/abs/2409.04747v3</link><description>Recently, self-supervised learning (SSL) has been extensively studied.Theoretically, mutual information maximization (MIM) is an optimal criterionfor SSL, with a strong theoretical foundation in information theory. However,it is difficult to directly apply MIM in SSL since the data distribution is notanalytically available in applications. In practice, many existing methods canbe viewed as approximate implementations of the MIM criterion. This work showsthat, based on the invariance property of MI, explicit MI maximization can beapplied to SSL under a generic distribution assumption, i.e., a relaxedcondition of the data distribution. We further illustrate this by analyzing thegeneralized Gaussian distribution. Based on this result, we derive a lossfunction based on the MIM criterion using only second-order statistics. Weimplement the new loss for SSL and demonstrate its effectiveness via extensiveexperiments.</description><author>Lele Chang, Peilin Liu, Qinghai Guo, Fei Wen</author><pubDate>Thu, 12 Sep 2024 16:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04747v3</guid></item><item><title>High-Frequency Anti-DreamBooth: Robust Defense Against Image Synthesis</title><link>http://arxiv.org/abs/2409.08167v1</link><description>Recently, text-to-image generative models have been misused to createunauthorized malicious images of individuals, posing a growing social problem.Previous solutions, such as Anti-DreamBooth, add adversarial noise to images toprotect them from being used as training data for malicious generation.However, we found that the adversarial noise can be removed by adversarialpurification methods such as DiffPure. Therefore, we propose a new adversarialattack method that adds strong perturbation on the high-frequency areas ofimages to make it more robust to adversarial purification. Our experimentshowed that the adversarial images retained noise even after adversarialpurification, hindering malicious image generation.</description><author>Takuto Onikubo, Yusuke Matsui</author><pubDate>Thu, 12 Sep 2024 15:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08167v1</guid></item><item><title>Open Source Infrastructure for Automatic Cell Segmentation</title><link>http://arxiv.org/abs/2409.08163v1</link><description>Automated cell segmentation is crucial for various biological and medicalapplications, facilitating tasks like cell counting, morphology analysis, anddrug discovery. However, manual segmentation is time-consuming and prone tosubjectivity, necessitating robust automated methods. This paper presentsopen-source infrastructure, utilizing the UNet model, a deep-learningarchitecture noted for its effectiveness in image segmentation tasks. Thisimplementation is integrated into the open-source DeepChem package, enhancingaccessibility and usability for researchers and practitioners. The resultingtool offers a convenient and user-friendly interface, reducing the barrier toentry for cell segmentation while maintaining high accuracy. Additionally, webenchmark this model against various datasets, demonstrating its robustness andversatility across different imaging conditions and cell types.</description><author>Aaron Rock Menezes, Bharath Ramsundar</author><pubDate>Thu, 12 Sep 2024 15:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08163v1</guid></item><item><title>Cross-Attention Based Influence Model for Manual and Nonmanual Sign Language Analysis</title><link>http://arxiv.org/abs/2409.08162v1</link><description>Both manual (relating to the use of hands) and non-manual markers (NMM), suchas facial expressions or mouthing cues, are important for providing thecomplete meaning of phrases in American Sign Language (ASL). Efforts have beenmade in advancing sign language to spoken/written language understanding, butmost of these have primarily focused on manual features only. In this work,using advanced neural machine translation methods, we examine and report on theextent to which facial expressions contribute to understanding sign languagephrases. We present a sign language translation architecture consisting oftwo-stream encoders, with one encoder handling the face and the other handlingthe upper body (with hands). We propose a new parallel cross-attention decodingmechanism that is useful for quantifying the influence of each input modalityon the output. The two streams from the encoder are directed simultaneously todifferent attention stacks in the decoder. Examining the properties of theparallel cross-attention weights allows us to analyze the importance of facialmarkers compared to body and hand features during a translating task.</description><author>Lipisha Chaudhary, Fei Xu, Ifeoma Nwogu</author><pubDate>Thu, 12 Sep 2024 15:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08162v1</guid></item><item><title>On the Role of Context in Reading Time Prediction</title><link>http://arxiv.org/abs/2409.08160v1</link><description>We present a new perspective on how readers integrate context duringreal-time language comprehension. Our proposals build on surprisal theory,which posits that the processing effort of a linguistic unit (e.g., a word) isan affine function of its in-context information content. We first observe thatsurprisal is only one out of many potential ways that a contextual predictorcan be derived from a language model. Another one is the pointwise mutualinformation (PMI) between a unit and its context, which turns out to yield thesame predictive power as surprisal when controlling for unigram frequency.Moreover, both PMI and surprisal are correlated with frequency. This means thatneither PMI nor surprisal contains information about context alone. In responseto this, we propose a technique where we project surprisal onto the orthogonalcomplement of frequency, yielding a new contextual predictor that isuncorrelated with frequency. Our experiments show that the proportion ofvariance in reading times explained by context is a lot smaller when context isrepresented by the orthogonalized predictor. From an interpretabilitystandpoint, this indicates that previous studies may have overstated the rolethat context has in predicting reading times.</description><author>Andreas Opedal, Eleanor Chodroff, Ryan Cotterell, Ethan Gotlieb Wilcox</author><pubDate>Thu, 12 Sep 2024 15:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08160v1</guid></item><item><title>SDformer: Efficient End-to-End Transformer for Depth Completion</title><link>http://arxiv.org/abs/2409.08159v1</link><description>Depth completion aims to predict dense depth maps with sparse depthmeasurements from a depth sensor. Currently, Convolutional Neural Network (CNN)based models are the most popular methods applied to depth completion tasks.However, despite the excellent high-end performance, they suffer from a limitedrepresentation area. To overcome the drawbacks of CNNs, a more effective andpowerful method has been presented: the Transformer, which is an adaptiveself-attention setting sequence-to-sequence model. While the standardTransformer quadratically increases the computational cost from the key-querydot-product of input resolution which improperly employs depth completiontasks. In this work, we propose a different window-based Transformerarchitecture for depth completion tasks named Sparse-to-Dense Transformer(SDformer). The network consists of an input module for the depth map and RGBimage features extraction and concatenation, a U-shaped encoder-decoderTransformer for extracting deep features, and a refinement module.Specifically, we first concatenate the depth map features with the RGB imagefeatures through the input model. Then, instead of calculating self-attentionwith the whole feature maps, we apply different window sizes to extract thelong-range depth dependencies. Finally, we refine the predicted features fromthe input module and the U-shaped encoder-decoder Transformer module to get theenriching depth features and employ a convolution layer to obtain the densedepth map. In practice, the SDformer obtains state-of-the-art results againstthe CNN-based depth completion models with lower computing loads and parameterson the NYU Depth V2 and KITTI DC datasets.</description><author>Jian Qian, Miao Sun, Ashley Lee, Jie Li, Shenglong Zhuo, Patrick Yin Chiang</author><pubDate>Thu, 12 Sep 2024 15:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08159v1</guid></item><item><title>MagicStyle: Portrait Stylization Based on Reference Image</title><link>http://arxiv.org/abs/2409.08156v1</link><description>The development of diffusion models has significantly advanced the researchon image stylization, particularly in the area of stylizing a content imagebased on a given style image, which has attracted many scholars. The mainchallenge in this reference image stylization task lies in how to maintain thedetails of the content image while incorporating the color and texture featuresof the style image. This challenge becomes even more pronounced when thecontent image is a portrait which has complex textural details. To address thischallenge, we propose a diffusion model-based reference image stylizationmethod specifically for portraits, called MagicStyle. MagicStyle consists oftwo phases: Content and Style DDIM Inversion (CSDI) and Feature Fusion Forward(FFF). The CSDI phase involves a reverse denoising process, where DDIMInversion is performed separately on the content image and the style image,storing the self-attention query, key and value features of both images duringthe inversion process. The FFF phase executes forward denoising, harmoniouslyintegrating the texture and color information from the pre-stored featurequeries, keys and values into the diffusion generation process based on ourWell-designed Feature Fusion Attention (FFA). We conducted comprehensivecomparative and ablation experiments to validate the effectiveness of ourproposed MagicStyle and FFA.</description><author>Zhaoli Deng, Kaibin Zhou, Fanyi Wang, Zhenpeng Mi</author><pubDate>Thu, 12 Sep 2024 15:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08156v1</guid></item><item><title>The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC 2024</title><link>http://arxiv.org/abs/2408.02369v3</link><description>This paper delineates the visual speech recognition (VSR) system introducedby the NPU-ASLP (Team 237) in the second Chinese Continuous Visual SpeechRecognition Challenge (CNVSRC 2024), engaging in all four tracks, including thefixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. Interms of data processing, we leverage the lip motion extractor from thebaseline1 to produce multiscale video data. Besides, various augmentationtechniques are applied during training, encompassing speed perturbation, randomrotation, horizontal flipping, and color transformation. The VSR model adoptsan end-to-end architecture with joint CTC/attention loss, introducing EnhancedResNet3D visual frontend, E-Branchformer encoder, and Bi-directionalTransformer decoder. Our approach yields a 30.47% CER for the Single-SpeakerTask and 34.30% CER for the Multi-Speaker Task, securing second place in theopen track of the Single-Speaker Task and first place in the other threetracks.</description><author>He Wang, Lei Xie</author><pubDate>Thu, 12 Sep 2024 15:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02369v3</guid></item><item><title>Generating synthetic data for neural operators</title><link>http://arxiv.org/abs/2401.02398v2</link><description>Numerous developments in the recent literature show the promising potentialof deep learning in obtaining numerical solutions to partial differentialequations (PDEs) beyond the reach of current numerical solvers. However,data-driven neural operators all suffer from a similar problem: the data neededto train a network depends on classical numerical solvers such as finitedifference or finite element, among others. In this paper, we propose adifferent approach to generating synthetic functional training data that doesnot require solving a PDE numerically. We draw a large number $N$ ofindependent and identically distributed 'random functions' $u_j$ from theunderlying solution space (e.g., $H_0^1(\Omega)$) in which we know the solutionlies according to classical theory. We then plug each such random candidatesolution into the equation and get a corresponding right-hand side function$f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervisedtraining data for learning the underlying inverse problem $f \rightarrow u$.This `backwards' approach to generating training data only requires derivativecomputations, in contrast to standard `forward' approaches, which require anumerical PDE solver, enabling us to generate many data points quickly andefficiently. While the idea is simple, we hope this method will expand thepotential for developing neural PDE solvers that do not depend on classicalnumerical solvers.</description><author>Erisa Hasani, Rachel A. Ward</author><pubDate>Thu, 12 Sep 2024 15:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02398v2</guid></item><item><title>LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models</title><link>http://arxiv.org/abs/2409.08147v1</link><description>Large language models have demonstrated remarkable capabilities in naturallanguage processing, yet their application to political discourse analysisremains underexplored. This paper introduces a novel approach to evaluatingpresidential debate performances using LLMs, addressing the longstandingchallenge of objectively assessing debate outcomes. We propose a framework thatanalyzes candidates' "Policies, Persona, and Perspective" (3P) and how theyresonate with the "Interests, Ideologies, and Identity" (3I) of four keyaudience groups: voters, businesses, donors, and politicians. Our methodemploys large language models to generate the LLM-POTUS Score, a quantitativemeasure of debate performance based on the alignment between 3P and 3I. Weapply this framework to analyze transcripts from recent U.S. presidentialdebates, demonstrating its ability to provide nuanced, multi-dimensionalassessments of candidate performances. Our results reveal insights into theeffectiveness of different debating strategies and their impact on variousaudience segments. This study not only offers a new tool for political analysisbut also explores the potential and limitations of using LLMs as impartialjudges in complex social contexts. In addition, this framework providesindividual citizens with an independent tool to evaluate presidential debateperformances, which enhances democratic engagement and reduces reliance onpotentially biased media interpretations and institutional influence, therebystrengthening the foundation of informed civic participation.</description><author>Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, Tianming Liu</author><pubDate>Thu, 12 Sep 2024 15:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08147v1</guid></item><item><title>Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of Foundation Models</title><link>http://arxiv.org/abs/2409.04631v2</link><description>We have tested recently published foundation models for histopathology forimage retrieval. We report macro average of F1 score for top-1 retrieval,majority of top-3 retrievals, and majority of top-5 retrievals. We performzero-shot retrievals, i.e., we do not alter embeddings and we do not train anyclassifier. As test data, we used diagnostic slides of TCGA, The Cancer GenomeAtlas, consisting of 23 organs and 117 cancer subtypes. As a search platform weused Yottixel that enabled us to perform WSI search using patches. Achieved F1scores show low performance, e.g., for top-5 retrievals, 27% +/- 13%(Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40%+/-13% (Yottixel-Virchow),41%+/-13% (Yottixel-GigaPath), and 41%+/-14% (GigaPath WSI).</description><author>Saghir Alfasly, Ghazal Alabtah, Sobhan Hemati, Krishna Rani Kalari, H. R. Tizhoosh</author><pubDate>Thu, 12 Sep 2024 15:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04631v2</guid></item><item><title>Robust Robot Walker: Learning Agile Locomotion over Tiny Traps</title><link>http://arxiv.org/abs/2409.07409v2</link><description>Quadruped robots must exhibit robust walking capabilities in practicalapplications. In this work, we propose a novel approach that enables quadrupedrobots to pass various small obstacles, or "tiny traps". Existing methods oftenrely on exteroceptive sensors, which can be unreliable for detecting such tinytraps. To overcome this limitation, our approach focuses solely onproprioceptive inputs. We introduce a two-stage training frameworkincorporating a contact encoder and a classification head to learn implicitrepresentations of different traps. Additionally, we design a set of tailoredreward functions to improve both the stability of training and the ease ofdeployment for goal-tracking tasks. To benefit further research, we design anew benchmark for tiny trap task. Extensive experiments in both simulation andreal-world settings demonstrate the effectiveness and robustness of our method.Project Page: https://robust-robot-walker.github.io/</description><author>Shaoting Zhu, Runhan Huang, Linzhan Mou, Hang Zhao</author><pubDate>Thu, 12 Sep 2024 15:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07409v2</guid></item><item><title>Effective Segmentation of Post-Treatment Gliomas Using Simple Approaches: Artificial Sequence Generation and Ensemble Models</title><link>http://arxiv.org/abs/2409.08143v1</link><description>Segmentation is a crucial task in the medical imaging field and is often animportant primary step or even a prerequisite to the analysis of medicalvolumes. Yet treatments such as surgery complicate the accurate delineation ofregions of interest. The BraTS Post-Treatment 2024 Challenge published thefirst public dataset for post-surgery glioma segmentation and addresses theaforementioned issue by fostering the development of automated segmentationtools for glioma in MRI data. In this effort, we propose two straightforwardapproaches to enhance the segmentation performances of deep learning-basedmethodologies. First, we incorporate an additional input based on a simplelinear combination of the available MRI sequences input, which highlightsenhancing tumors. Second, we employ various ensembling methods to weigh thecontribution of a battery of models. Our results demonstrate that theseapproaches significantly improve segmentation performance compared to baselinemodels, underscoring the effectiveness of these simple approaches in improvingmedical image segmentation tasks.</description><author>Heejong Kim, Leo Milecki, Mina C Moghadam, Fengbei Liu, Minh Nguyen, Eric Qiu, Abhishek Thanki, Mert R Sabuncu</author><pubDate>Thu, 12 Sep 2024 15:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08143v1</guid></item><item><title>DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification</title><link>http://arxiv.org/abs/2409.05587v2</link><description>Driver distraction remains a leading cause of traffic accidents, posing acritical threat to road safety globally. As intelligent transportation systemsevolve, accurate and real-time identification of driver distraction has becomeessential. However, existing methods struggle to capture both global contextualand fine-grained local features while contending with noisy labels in trainingdatasets. To address these challenges, we propose DSDFormer, a novel frameworkthat integrates the strengths of Transformer and Mamba architectures through aDual State Domain Attention (DSDA) mechanism, enabling a balance betweenlong-range dependencies and detailed feature extraction for robust driverbehavior recognition. Additionally, we introduce Temporal Reasoning ConfidentLearning (TRCL), an unsupervised approach that refines noisy labels byleveraging spatiotemporal correlations in video sequences. Our model achievesstate-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets anddemonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orinplatform. Extensive experimental results confirm that DSDFormer and TRCLsignificantly improve both the accuracy and robustness of driver distractiondetection, offering a scalable solution to enhance road safety.</description><author>Junzhou Chen, Zirui Zhang, Jing Yu, Heqiang Huang, Ronghui Zhang, Xuemiao Xu, Bin Sheng, Hong Yan</author><pubDate>Thu, 12 Sep 2024 15:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05587v2</guid></item><item><title>Benchmarking General-Purpose In-Context Learning</title><link>http://arxiv.org/abs/2405.17234v6</link><description>In-context learning (ICL) empowers generative models to address new taskseffectively and efficiently on the fly, without relying on any artificiallycrafted optimization techniques. In this paper, we study extending ICL toaddress a broader range of tasks with an extended learning horizon and higherimprovement potential, namely General Purpose In-Context Learning (GPICL). Tothis end, we introduce two lightweight benchmarks specifically crafted to trainand evaluate GPICL functionalities. Each benchmark encompasses a vast number oftasks characterized by significant task variance. These tasks are also craftedto promote long-horizon in-context learning through continuous generation andinteraction, covering domains such as language modeling, decision-making, andworld modeling. The benchmarks necessitate the models to leverage contexts andhistory interactions to enhance their capabilities, which we believe to be thekey characteristics of GPICL. Our experiments indicate that the diversity oftraining tasks is positively correlated with the ability to generalize withICL, but inversely correlated with zero-shot capabilities. Additionally, ourfindings indicate that the scale of parameters alone may not be crucial for ICLor GPICL, suggesting alternative approaches such as increasing the scale ofcontexts and memory states.</description><author>Fan Wang, Chuan Lin, Yang Cao, Yu Kang</author><pubDate>Thu, 12 Sep 2024 15:22:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17234v6</guid></item><item><title>The JPEG Pleno Learning-based Point Cloud Coding Standard: Serving Man and Machine</title><link>http://arxiv.org/abs/2409.08130v1</link><description>Efficient point cloud coding has become increasingly critical for multipleapplications such as virtual reality, autonomous driving, and digital twinsystems, where rich and interactive 3D data representations may functionallymake the difference. Deep learning has emerged as a powerful tool in thisdomain, offering advanced techniques for compressing point clouds moreefficiently than conventional coding methods while also allowing effectivecomputer vision tasks performed in the compressed domain thus, for the firsttime, making available a common compressed visual representation effective forboth man and machine. Taking advantage of this potential, JPEG has recentlyfinalized the JPEG Pleno Learning-based Point Cloud Coding (PCC) standardoffering efficient lossy coding of static point clouds, targeting both humanvisualization and machine processing by leveraging deep learning models forgeometry and color coding. The geometry is processed directly in its original3D form using sparse convolutional neural networks, while the color data isprojected onto 2D images and encoded using the also learning-based JPEG AIstandard. The goal of this paper is to provide a complete technical descriptionof the JPEG PCC standard, along with a thorough benchmarking of its performanceagainst the state-of-the-art, while highlighting its main strengths andweaknesses. In terms of compression performance, JPEG PCC outperforms theconventional MPEG PCC standards, especially in geometry coding, achievingsignificant rate reductions. Color compression performance is less competitivebut this is overcome by the power of a full learning-based coding framework forboth geometry and color and the associated effective compressed domainprocessing.</description><author>André F. R. Guarda, Nuno M. M. Rodrigues, Fernando Pereira</author><pubDate>Thu, 12 Sep 2024 15:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08130v1</guid></item><item><title>Unified Domain Adaptive Semantic Segmentation</title><link>http://arxiv.org/abs/2311.13254v3</link><description>Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transferthe supervision from a labeled source domain to an unlabeled target domain. Themajority of existing UDA-SS works typically consider images whilst recentattempts have extended further to tackle videos by modeling the temporaldimension. Although the two lines of research share the major challenges --overcoming the underlying domain distribution shift, their studies are largelyindependent, resulting in fragmented insights, a lack of holisticunderstanding, and missed opportunities for cross-pollination of ideas. Thisfragmentation prevents the unification of methods, leading to redundant effortsand suboptimal knowledge transfer across image and video domains. Under thisobservation, we advocate unifying the study of UDA-SS across video and imagescenarios, enabling a more comprehensive understanding, synergisticadvancements, and efficient knowledge sharing. To that end, we explore theunified UDA-SS from a general data augmentation perspective, serving as aunifying conceptual framework, enabling improved generalization, and potentialfor cross-pollination of ideas, ultimately contributing to the overall progressand practical impact of this field of research. Specifically, we propose aQuad-directional Mixup (QuadMix) method, characterized by tackling distinctpoint attributes and feature inconsistencies through four-directional paths forintra- and inter-domain mixing in a feature space. To deal with temporal shiftswith videos, we incorporate optical flow-guided feature aggregation acrossspatial and temporal dimensions for fine-grained domain alignment. Extensiveexperiments show that our method outperforms the state-of-the-art works bylarge margins on four challenging UDA-SS benchmarks. Our source code and modelswill be released at \url{https://github.com/ZHE-SAPI/UDASS}.</description><author>Zhe Zhang, Gaochang Wu, Jing Zhang, Xiatian Zhu, Dacheng Tao, Tianyou Chai</author><pubDate>Thu, 12 Sep 2024 15:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13254v3</guid></item><item><title>Contrastive Learning and the Emergence of Attributes Associations</title><link>http://arxiv.org/abs/2302.10763v4</link><description>In response to an object presentation, supervised learning schemes generallyrespond with a parsimonious label. Upon a similar presentation we humansrespond again with a label, but are flooded, in addition, by a myriad ofassociations. A significant portion of these consist of the presented objectattributes. Contrastive learning is a semi-supervised learning scheme based onthe application of identity preserving transformations on the object inputrepresentations. It is conjectured in this work that these same appliedtransformations preserve, in addition to the identity of the presented object,also the identity of its semantically meaningful attributes. The corollary ofthis is that the output representations of such a contrastive learning schemecontain valuable information not only for the classification of the presentedobject, but also for the presence or absence decision of any attribute ofinterest. Simulation results which demonstrate this idea and the feasibility ofthis conjecture are presented.</description><author>Daniel N. Nissani</author><pubDate>Thu, 12 Sep 2024 15:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10763v4</guid></item><item><title>GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from Avatar Views in VR/MR Devices</title><link>http://arxiv.org/abs/2409.08122v1</link><description>The advent and growing popularity of Virtual Reality (VR) and Mixed Reality(MR) solutions have revolutionized the way we interact with digital platforms.The cutting-edge gaze-controlled typing methods, now prevalent in high-endmodels of these devices, e.g., Apple Vision Pro, have not only improved userexperience but also mitigated traditional keystroke inference attacks thatrelied on hand gestures, head movements and acoustic side-channels. However,this advancement has paradoxically given birth to a new, potentially moreinsidious cyber threat, GAZEploit. In this paper, we unveil GAZEploit, a novel eye-tracking based attackspecifically designed to exploit these eye-tracking information by leveragingthe common use of virtual appearances in VR applications. This widespread usagesignificantly enhances the practicality and feasibility of our attack comparedto existing methods. GAZEploit takes advantage of this vulnerability toremotely extract gaze estimations and steal sensitive keystroke informationacross various typing scenarios-including messages, passwords, URLs, emails,and passcodes. Our research, involving 30 participants, achieved over 80%accuracy in keystroke inference. Alarmingly, our study also identified over 15top-rated apps in the Apple Store as vulnerable to the GAZEploit attack,emphasizing the urgent need for bolstered security measures for thisstate-of-the-art VR/MR text entry method.</description><author>Hanqiu Wang, Zihao Zhan, Haoqi Shan, Siqi Dai, Max Panoff, Shuo Wang</author><pubDate>Thu, 12 Sep 2024 15:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08122v1</guid></item><item><title>Review of Recent Advances in Gaussian Process Regression Methods</title><link>http://arxiv.org/abs/2409.08112v1</link><description>Gaussian process (GP) methods have been widely studied recently, especiallyfor large-scale systems with big data and even more extreme cases when data issparse. Key advantages of these methods consist in: 1) the ability to provideinherent ways to assess the impact of uncertainties (especially in the data,and environment) on the solutions, 2) have efficient factorisation basedimplementations and 3) can be implemented easily in distributed manners andhence provide scalable solutions. This paper reviews the recently developed keyfactorised GP methods such as the hierarchical off-diagonal low-rankapproximation methods and GP with Kronecker structures. An example illustratesthe performance of these methods with respect to accuracy and computationalcomplexity.</description><author>Chenyi Lyu, Xingchi Liu, Lyudmila Mihaylova</author><pubDate>Thu, 12 Sep 2024 15:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08112v1</guid></item><item><title>What is the Role of Small Models in the LLM Era: A Survey</title><link>http://arxiv.org/abs/2409.06857v2</link><description>Large Language Models (LLMs) have made significant progress in advancingartificial general intelligence (AGI), leading to the development ofincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling upmodel sizes results in exponentially higher computational costs and energyconsumption, making these models impractical for academic researchers andbusinesses with limited resources. At the same time, Small Models (SMs) arefrequently used in practical settings, although their significance is currentlyunderestimated. This raises important questions about the role of small modelsin the era of LLMs, a topic that has received limited attention in priorresearch. In this work, we systematically examine the relationship between LLMsand SMs from two key perspectives: Collaboration and Competition. We hope thissurvey provides valuable insights for practitioners, fostering a deeperunderstanding of the contribution of small models and promoting more efficientuse of computational resources. The code is available athttps://github.com/tigerchen52/role_of_small_models</description><author>Lihu Chen, Gaël Varoquaux</author><pubDate>Thu, 12 Sep 2024 15:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06857v2</guid></item><item><title>Towards a graph-based foundation model for network traffic analysis</title><link>http://arxiv.org/abs/2409.08111v1</link><description>Foundation models have shown great promise in various fields of study. Apotential application of such models is in computer network traffic analysis,where these models can grasp the complexities of network traffic dynamics andadapt to any specific task or network environment with minimal fine-tuning.Previous approaches have used tokenized hex-level packet data and the modelarchitecture of large language transformer models. We propose a new, efficientgraph-based alternative at the flow-level. Our approach represents networktraffic as a dynamic spatio-temporal graph, employing a self-supervised linkprediction pretraining task to capture the spatial and temporal dynamics inthis network graph framework. To evaluate the effectiveness of our approach, weconduct a few-shot learning experiment for three distinct downstream networktasks: intrusion detection, traffic classification, and botnet classification.Models finetuned from our pretrained base achieve an average performanceincrease of 6.87\% over training from scratch, demonstrating their ability toeffectively learn general network traffic dynamics during pretraining. Thissuccess suggests the potential for a large-scale version to serve as anoperational foundational model.</description><author>Louis Van Langendonck, Ismael Castell-Uroz, Pere Barlet-Ros</author><pubDate>Thu, 12 Sep 2024 15:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08111v1</guid></item><item><title>WhisperNER: Unified Open Named Entity and Speech Recognition</title><link>http://arxiv.org/abs/2409.08107v1</link><description>Integrating named entity recognition (NER) with automatic speech recognition(ASR) can significantly enhance transcription accuracy and informativeness. Inthis paper, we introduce WhisperNER, a novel model that allows joint speechtranscription and entity recognition. WhisperNER supports open-type NER,enabling recognition of diverse and evolving entities at inference. Building onrecent advancements in open NER research, we augment a large synthetic datasetwith synthetic speech samples. This allows us to train WhisperNER on a largenumber of examples with diverse NER tags. During training, the model isprompted with NER labels and optimized to output the transcribed utterancealong with the corresponding tagged entities. To evaluate WhisperNER, wegenerate synthetic speech for commonly used NER benchmarks and annotateexisting ASR datasets with open NER tags. Our experiments demonstrate thatWhisperNER outperforms natural baselines on both out-of-domain open type NERand supervised finetuning.</description><author>Gil Ayache, Menachem Pirchi, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet</author><pubDate>Thu, 12 Sep 2024 15:00:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08107v1</guid></item><item><title>DEMAU: Decompose, Explore, Model and Analyse Uncertainties</title><link>http://arxiv.org/abs/2409.08105v1</link><description>Recent research in machine learning has given rise to a flourishingliterature on the quantification and decomposition of model uncertainty. Thisinformation can be very useful during interactions with the learner, such as inactive learning or adaptive learning, and especially in uncertainty sampling.To allow a simple representation of these total, epistemic (reducible) andaleatoric (irreducible) uncertainties, we offer DEMAU, an open-sourceeducational, exploratory and analytical tool allowing to visualize and exploreseveral types of uncertainty for classification models in machine learning.</description><author>Arthur Hoarau, Vincent Lemaire</author><pubDate>Thu, 12 Sep 2024 14:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08105v1</guid></item><item><title>Minimum projective linearizations of trees in linear time</title><link>http://arxiv.org/abs/2102.03277v6</link><description>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping$\pi$ from vertices of a graph to distinct integers that minimizes$\sum_{\{u,v\}\in E}|\pi(u) - \pi(v)|$. In that setting, vertices are oftenassumed to lie on a horizontal line and edges are drawn as semicircles abovesaid line. For trees, various algorithms are available to solve the problem inpolynomial time in $n=|V|$. There exist variants of the MLA in which thearrangements are constrained. Iordanskii, and later Hochberg and Stallmann(HS), put forward $O(n)$-time algorithms that solve the problem whenarrangements are constrained to be planar (also known as one-page bookembeddings). We also consider linear arrangements of rooted trees that areconstrained to be projective (planar embeddings where the root is not coveredby any edge). Gildea and Temperley (GT) sketched an algorithm for projectivearrangements which they claimed runs in $O(n)$ but did not provide anyjustification of its cost. In contrast, Park and Levy claimed that GT'salgorithm runs in $O(n \log d_{max})$ where $d_{max}$ is the maximum degree butdid not provide sufficient detail. Here we correct an error in HS's algorithmfor the planar case, show its relationship with the projective case, and derivesimple algorithms for the projective and planar cases that run without a doubtin $O(n)$ time.</description><author>Lluís Alemany-Puig, Juan Luis Esteban, Ramon Ferrer-i-Cancho</author><pubDate>Thu, 12 Sep 2024 14:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.03277v6</guid></item><item><title>Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges</title><link>http://arxiv.org/abs/2409.02387v3</link><description>This comprehensive review explores the intersection of Large Language Models(LLMs) and cognitive science, examining similarities and differences betweenLLMs and human cognitive processes. We analyze methods for evaluating LLMscognitive abilities and discuss their potential as cognitive models. The reviewcovers applications of LLMs in various cognitive fields, highlighting insightsgained for cognitive science research. We assess cognitive biases andlimitations of LLMs, along with proposed methods for improving theirperformance. The integration of LLMs with cognitive architectures is examined,revealing promising avenues for enhancing artificial intelligence (AI)capabilities. Key challenges and future research directions are identified,emphasizing the need for continued refinement of LLMs to better align withhuman cognition. This review provides a balanced perspective on the currentstate and future potential of LLMs in advancing our understanding of bothartificial and human intelligence.</description><author>Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, Ming Li</author><pubDate>Thu, 12 Sep 2024 14:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02387v3</guid></item><item><title>The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language</title><link>http://arxiv.org/abs/2409.08103v1</link><description>We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmarkcorpus designed to push the limits of current approaches to low-resource speechrecognition. Faetar, a Franco-Proven\c{c}al variety spoken primarily in Italy,has no standard orthography, has virtually no existing textual or speechresources other than what is included in the benchmark, and is quite differentfrom other forms of Franco-Proven\c{c}al. The corpus comes from fieldrecordings, most of which are noisy, for which only 5 hrs have matchingtranscriptions, and for which forced alignment is of variable quality. Thecorpus contains an additional 20 hrs of unlabelled speech. We report baselineresults from state-of-the-art multilingual speech foundation models with a bestphone error rate of 30.4%, using a pipeline that continues pre-training on thefoundation model using the unlabelled set.</description><author>Michael Ong, Sean Robertson, Leo Peckham, Alba Jorquera Jimenez de Aberasturi, Paula Arkhangorodsky, Robin Huo, Aman Sakhardande, Mark Hallap, Naomi Nagy, Ewan Dunbar</author><pubDate>Thu, 12 Sep 2024 14:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08103v1</guid></item><item><title>Bayesian Self-Training for Semi-Supervised 3D Segmentation</title><link>http://arxiv.org/abs/2409.08102v1</link><description>3D segmentation is a core problem in computer vision and, similarly to manyother dense prediction tasks, it requires large amounts of annotated data foradequate training. However, densely labeling 3D point clouds to employfully-supervised training remains too labor intensive and expensive.Semi-supervised training provides a more practical alternative, where only asmall set of labeled data is given, accompanied by a larger unlabeled set. Thisarea thus studies the effective use of unlabeled data to reduce the performancegap that arises due to the lack of annotations. In this work, inspired byBayesian deep learning, we first propose a Bayesian self-training framework forsemi-supervised 3D semantic segmentation. Employing stochastic inference, wegenerate an initial set of pseudo-labels and then filter these based onestimated point-wise uncertainty. By constructing a heuristic $n$-partitematching algorithm, we extend the method to semi-supervised 3D instancesegmentation, and finally, with the same building blocks, to dense 3D visualgrounding. We demonstrate state-of-the-art results for our semi-supervisedmethod on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and onScanNet and S3DIS for 3D instance segmentation. We further achieve substantialimprovements in dense 3D visual grounding over supervised-only baselines onScanRefer. Our project page is available at ouenal.github.io/bst/.</description><author>Ozan Unal, Christos Sakaridis, Luc Van Gool</author><pubDate>Thu, 12 Sep 2024 14:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08102v1</guid></item><item><title>The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal</title><link>http://arxiv.org/abs/2409.08098v1</link><description>This paper explores the intersection of technological innovation and accessto justice by developing a benchmark for predicting case outcomes in the UKEmployment Tribunal (UKET). To address the challenge of extensive manualannotation, the study employs a large language model (LLM) for automaticannotation, resulting in the creation of the CLC-UKET dataset. The datasetconsists of approximately 19,000 UKET cases and their metadata. Comprehensivelegal annotations cover facts, claims, precedent references, statutoryreferences, case outcomes, reasons and jurisdiction codes. Facilitated by theCLC-UKET data, we examine a multi-class case outcome prediction task in theUKET. Human predictions are collected to establish a performance reference formodel comparison. Empirical results from baseline models indicate thatfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKETprediction task. The performance of zero-shot LLMs can be enhanced byintegrating task-related information into few-shot examples. We hope that theCLC-UKET dataset, along with human annotations and empirical findings, canserve as a valuable benchmark for employment-related dispute resolution.</description><author>Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford</author><pubDate>Thu, 12 Sep 2024 14:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08098v1</guid></item><item><title>Optimizing Falsification for Learning-Based Control Systems: A Multi-Fidelity Bayesian Approach</title><link>http://arxiv.org/abs/2409.08097v1</link><description>Testing controllers in safety-critical systems is vital for ensuring theirsafety and preventing failures. In this paper, we address the falsificationproblem within learning-based closed-loop control systems through simulation.This problem involves the identification of counterexamples that violate systemsafety requirements and can be formulated as an optimization task based onthese requirements. Using full-fidelity simulator data in this optimizationproblem can be computationally expensive. To improve efficiency, we propose amulti-fidelity Bayesian optimization falsification framework that harnessessimulators with varying levels of accuracy. Our proposed framework cantransition between different simulators and establish meaningful relationshipsbetween them. Through multi-fidelity Bayesian optimization, we determine boththe optimal system input likely to be a counterexample and the appropriatefidelity level for assessment. We evaluated our approach across various Gymenvironments, each featuring different levels of fidelity. Our experimentsdemonstrate that multi-fidelity Bayesian optimization is more computationallyefficient than full-fidelity Bayesian optimization and other baseline methodsin detecting counterexamples. A Python implementation of the algorithm isavailable at https://github.com/SAILRIT/MFBO_Falsification.</description><author>Zahra Shahrooei, Mykel J. Kochenderfer, Ali Baheri</author><pubDate>Thu, 12 Sep 2024 14:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08097v1</guid></item><item><title>EZIGen: Enhancing zero-shot subject-driven image generation with precise subject encoding and decoupled guidance</title><link>http://arxiv.org/abs/2409.08091v1</link><description>Zero-shot subject-driven image generation aims to produce images thatincorporate a subject from a given example image. The challenge lies inpreserving the subject's identity while aligning with the text prompt, whichoften requires modifying certain aspects of the subject's appearance. Despiteadvancements in diffusion model based methods, existing approaches stillstruggle to balance identity preservation with text prompt alignment. In thisstudy, we conducted an in-depth investigation into this issue and uncovered keyinsights for achieving effective identity preservation while maintaining astrong balance. Our key findings include: (1) the design of the subject imageencoder significantly impacts identity preservation quality, and (2) generatingan initial layout is crucial for both text alignment and identity preservation.Building on these insights, we introduce a new approach called EZIGen, whichemploys two main strategies: a carefully crafted subject image Encoder based onthe UNet architecture of the pretrained Stable Diffusion model to ensurehigh-quality identity transfer, following a process that decouples the guidancestages and iteratively refines the initial image layout. Through thesestrategies, EZIGen achieves state-of-the-art results on multiple subject-drivenbenchmarks with a unified model and 100 times less training data.</description><author>Zicheng Duan, Yuxuan Ding, Chenhui Gou, Ziqin Zhou, Ethan Smith, Lingqiao Liu</author><pubDate>Thu, 12 Sep 2024 14:44:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08091v1</guid></item><item><title>Predictability maximization and the origins of word order harmony</title><link>http://arxiv.org/abs/2408.16570v3</link><description>We address the linguistic problem of the sequential arrangement of a head andits dependents from an information theoretic perspective. In particular, weconsider the optimal placement of a head that maximizes the predictability ofthe sequence. We assume that dependents are statistically independent given ahead, in line with the open-choice principle and the core assumptions ofdependency grammar. We demonstrate the optimality of harmonic order, i.e.,placing the head last maximizes the predictability of the head whereas placingthe head first maximizes the predictability of dependents. We also show thatpostponing the head is the optimal strategy to maximize its predictabilitywhile bringing it forward is the optimal strategy to maximize thepredictability of dependents. We unravel the advantages of the strategy ofmaximizing the predictability of the head over maximizing the predictability ofdependents. Our findings shed light on the placements of the head adopted byreal languages or emerging in different kinds of experiments.</description><author>Ramon Ferrer-i-Cancho</author><pubDate>Thu, 12 Sep 2024 14:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16570v3</guid></item><item><title>SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality</title><link>http://arxiv.org/abs/2409.08083v1</link><description>Foundation models like ChatGPT and Sora that are trained on a huge scale ofdata have made a revolutionary social impact. However, it is extremelychallenging for sensors in many different fields to collect similar scales ofnatural images to train strong foundation models. To this end, this workpresents a simple and effective framework SimMAT to study an open problem: thetransferability from vision foundation models trained on natural RGB images toother image modalities of different physical properties (e.g., polarization).SimMAT consists of a modality-agnostic transfer layer (MAT) and a pretrainedfoundation model. We apply SimMAT to a representative vision foundation modelSegment Anything Model (SAM) to support any evaluated new image modality. Giventhe absence of relevant benchmarks, we construct a new benchmark to evaluatethe transfer learning performance. Our experiments confirm the intriguingpotential of transferring vision foundation models in enhancing other sensors'performance. Specifically, SimMAT can improve the segmentation performance(mIoU) from 22.15% to 53.88% on average for evaluated modalities andconsistently outperforms other baselines. We hope that SimMAT can raiseawareness of cross-modal transfer learning and benefit various fields forbetter results with vision foundation models.</description><author>Chenyang Lei, Liyi Chen, Jun Cen, Xiao Chen, Zhen Lei, Felix Heide, Ziwei Liu, Qifeng Chen, Zhaoxiang Zhang</author><pubDate>Thu, 12 Sep 2024 14:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08083v1</guid></item><item><title>Diffusion-Based Image-to-Image Translation by Noise Correction via Prompt Interpolation</title><link>http://arxiv.org/abs/2409.08077v1</link><description>We propose a simple but effective training-free approach tailored todiffusion-based image-to-image translation. Our approach revises the originalnoise prediction network of a pretrained diffusion model by introducing a noisecorrection term. We formulate the noise correction term as the differencebetween two noise predictions; one is computed from the denoising network witha progressive interpolation of the source and target prompt embeddings, whilethe other is the noise prediction with the source prompt embedding. The finalnoise prediction network is given by a linear combination of the standarddenoising term and the noise correction term, where the former is designed toreconstruct must-be-preserved regions while the latter aims to effectively editregions of interest relevant to the target prompt. Our approach can be easilyincorporated into existing image-to-image translation methods based ondiffusion models. Extensive experiments verify that the proposed techniqueachieves outstanding performance with low latency and consistently improvesexisting frameworks when combined with them.</description><author>Junsung Lee, Minsoo Kang, Bohyung Han</author><pubDate>Thu, 12 Sep 2024 14:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08077v1</guid></item><item><title>TravelAgent: An AI Assistant for Personalized Travel Planning</title><link>http://arxiv.org/abs/2409.08069v1</link><description>As global tourism expands and artificial intelligence technology advances,intelligent travel planning services have emerged as a significant researchfocus. Within dynamic real-world travel scenarios with multi-dimensionalconstraints, services that support users in automatically creating practicaland customized travel itineraries must address three key objectives:Rationality, Comprehensiveness, and Personalization. However, existing systemswith rule-based combinations or LLM-based planning methods struggle to fullysatisfy these criteria. To overcome the challenges, we introduce TravelAgent, atravel planning system powered by large language models (LLMs) designed toprovide reasonable, comprehensive, and personalized travel itineraries groundedin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,Recommendation, Planning, and Memory Module. We evaluate TravelAgent'sperformance with human and simulated users, demonstrating its overalleffectiveness in three criteria and confirming the accuracy of personalizedrecommendations.</description><author>Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen</author><pubDate>Thu, 12 Sep 2024 14:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08069v1</guid></item><item><title>AutoPET Challenge: Tumour Synthesis for Data Augmentation</title><link>http://arxiv.org/abs/2409.08068v1</link><description>Accurate lesion segmentation in whole-body PET/CT scans is crucial for cancerdiagnosis and treatment planning, but limited datasets often hinder theperformance of automated segmentation models. In this paper, we explore thepotential of leveraging the deep prior from a generative model to serve as adata augmenter for automated lesion segmentation in PET/CT scans. We adapt theDiffTumor method, originally designed for CT images, to generate syntheticPET-CT images with lesions. Our approach trains the generative model on theAutoPET dataset and uses it to expand the training data. We then compare theperformance of segmentation models trained on the original and augmenteddatasets. Our findings show that the model trained on the augmented datasetachieves a higher Dice score, demonstrating the potential of our dataaugmentation approach. In a nutshell, this work presents a promising directionfor improving lesion segmentation in whole-body PET/CT scans with limiteddatasets, potentially enhancing the accuracy and reliability of cancerdiagnostics.</description><author>Lap Yan Lennon Chan, Chenxin Li, Yixuan Yuan</author><pubDate>Thu, 12 Sep 2024 14:23:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08068v1</guid></item><item><title>KL Convergence Guarantees for Score diffusion models under minimal data assumptions</title><link>http://arxiv.org/abs/2308.12240v2</link><description>Diffusion models are a new class of generative models that revolve around theestimation of the score function associated with a stochastic differentialequation. Subsequent to its acquisition, the approximated score function isthen harnessed to simulate the corresponding time-reversal process, ultimatelyenabling the generation of approximate data samples. Despite their evidentpractical significance these models carry, a notable challenge persists in theform of a lack of comprehensive quantitative results, especially in scenariosinvolving non-regular scores and estimators. In almost all reported bounds inKullback Leibler (KL) divergence, it is assumed that either the score functionor its approximation is Lipschitz uniformly in time. However, this condition isvery restrictive in practice or appears to be difficult to establish. Tocircumvent this issue, previous works mainly focused on establishingconvergence bounds in KL for an early stopped version of the diffusion modeland a smoothed version of the data distribution, or assuming that the datadistribution is supported on a compact manifold. These explorations have led tointeresting bounds in either Wasserstein or Fortet-Mourier metrics. However,the question remains about the relevance of such early-stopping procedure orcompactness conditions. In particular, if there exist a natural and mildcondition ensuring explicit and sharp convergence bounds in KL. In thisarticle, we tackle the aforementioned limitations by focusing on scorediffusion models with fixed step size stemming from the Ornstein-Uhlenbecksemigroup and its kinetic counterpart. Our study provides a rigorous analysis,yielding simple, improved and sharp convergence bounds in KL applicable to anydata distribution with finite Fisher information with respect to the standardGaussian distribution.</description><author>Giovanni Conforti, Alain Durmus, Marta Gentiloni Silveri</author><pubDate>Thu, 12 Sep 2024 14:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12240v2</guid></item><item><title>Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack</title><link>http://arxiv.org/abs/2404.03325v2</link><description>Robotic technologies have been an indispensable part for improving humanproductivity since they have been helping humans in completing diverse,complex, and intensive tasks in a fast yet accurate and efficient way.Therefore, robotic technologies have been deployed in a wide range ofapplications, ranging from personal to industrial use-cases. However, currentrobotic technologies and their computing paradigm still lack embodiedintelligence to efficiently interact with operational environments, respondwith correct/expected actions, and adapt to changes in the environments. Towardthis, recent advances in neuromorphic computing with Spiking Neural Networks(SNN) have demonstrated the potential to enable the embodied intelligence forrobotics through bio-plausible computing paradigm that mimics how thebiological brain works, known as "neuromorphic artificial intelligence (AI)".However, the field of neuromorphic AI-based robotics is still at an earlystage, therefore its development and deployment for solving real-world problemsexpose new challenges in different design aspects, such as accuracy,adaptability, efficiency, reliability, and security. To address thesechallenges, this paper will discuss how we can enable embodied neuromorphic AIfor robotic systems through our perspectives: (P1) Embodied intelligence basedon effective learning rule, training mechanism, and adaptability; (P2)Cross-layer optimizations for energy-efficient neuromorphic computing; (P3)Representative and fair benchmarks; (P4) Low-cost reliability and safetyenhancements; (P5) Security and privacy for neuromorphic computing; and (P6) Asynergistic development for energy-efficient and robust neuromorphic-basedrobotics. Furthermore, this paper identifies research challenges andopportunities, as well as elaborates our vision for future research developmenttoward embodied neuromorphic AI for robotics.</description><author>Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Fakhreddine Zayer, Jorge Dias, Muhammad Shafique</author><pubDate>Thu, 12 Sep 2024 14:18:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03325v2</guid></item><item><title>NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks</title><link>http://arxiv.org/abs/2407.11698v2</link><description>Quantization has become increasingly pivotal in addressing the steadilyincreasing computational and memory requirements of Deep Neural Networks(DNNs). By reducing the number of bits used to represent weights andactivations (typically from 32-bit floating-point to 16-bit or 8-bit integers),quantization reduces the memory footprint, energy consumption, and executiontime of DNN models. However, traditional quantization methods typically focuson the inference of DNNs, while the training process still relies onfloating-point operations. To date, only one work in the literature hasaddressed integer-only training for Multi-Layer Perceptron (MLP) architectures.This work introduces NITRO-D, a new framework for training arbitrarily deepinteger-only Convolutional Neural Networks (CNNs) that operate entirely in theinteger-only domain for both training and inference. NITRO-D is the firstframework in the literature enabling the training of integer-only CNNs withoutthe need to introduce a quantization scheme. Specifically, NITRO-D introduces anovel architecture integrating multiple integer local-loss blocks, whichinclude the proposed NITRO Scaling Layer and the NITRO-ReLU activationfunction. Additionally, it introduces a novel integer-only learning algorithmderived from Local Error Signals (LES), utilizing IntegerSGD, an optimizerspecifically designed to operate in an integer-only context. NITRO-D isimplemented in an open-source Python library. Extensive experimentalevaluations demonstrate its effectiveness across several state-of-the-art imagerecognition datasets. Results show significant performance improvements from2.47% to 5.96% for integer-only MLP architectures over the state-of-the-artsolution, and the capability of training integer-only CNN architectures withminimal accuracy degradation from -0.15% to -4.22% compared to floating-pointLES.</description><author>Alberto Pirillo, Luca Colombo, Manuel Roveri</author><pubDate>Thu, 12 Sep 2024 14:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11698v2</guid></item><item><title>Self-Supervised Learning of Iterative Solvers for Constrained Optimization</title><link>http://arxiv.org/abs/2409.08066v1</link><description>Obtaining the solution of constrained optimization problems as a function ofparameters is very important in a multitude of applications, such as controland planning. Solving such parametric optimization problems in real time canpresent significant challenges, particularly when it is necessary to obtainhighly accurate solutions or batches of solutions. To solve these challenges,we propose a learning-based iterative solver for constrained optimization whichcan obtain very fast and accurate solutions by customizing the solver to aspecific parametric optimization problem. For a given set of parameters of theconstrained optimization problem, we propose a first step with a neural networkpredictor that outputs primal-dual solutions of a reasonable degree ofaccuracy. This primal-dual solution is then improved to a very high degree ofaccuracy in a second step by a learned iterative solver in the form of a neuralnetwork. A novel loss function based on the Karush-Kuhn-Tucker conditions ofoptimality is introduced, enabling fully self-supervised training of bothneural networks without the necessity of prior sampling of optimizer solutions.The evaluation of a variety of quadratic and nonlinear parametric test problemsdemonstrates that the predictor alone is already competitive with recentself-supervised schemes for approximating optimal solutions. The second step ofour proposed learning-based iterative constrained optimizer achieves solutionswith orders of magnitude better accuracy than other learning-based approaches,while being faster to evaluate than state-of-the-art solvers and nativelyallowing for GPU parallelization.</description><author>Lukas Lüken, Sergio Lucia</author><pubDate>Thu, 12 Sep 2024 14:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08066v1</guid></item><item><title>AI-accelerated discovery of high critical temperature superconductors</title><link>http://arxiv.org/abs/2409.08065v1</link><description>The discovery of new superconducting materials, particularly those exhibitinghigh critical temperature ($T_c$), has been a vibrant area of study within thefield of condensed matter physics. Conventional approaches primarily rely onphysical intuition to search for potential superconductors within the existingdatabases. However, the known materials only scratch the surface of theextensive array of possibilities within the realm of materials. Here, wedevelop an AI search engine that integrates deep model pre-training andfine-tuning techniques, diffusion models, and physics-based approaches (e.g.,first-principles electronic structure calculation) for discovery of high-$T_c$superconductors. Utilizing this AI search engine, we have obtained 74dynamically stable materials with critical temperatures predicted by the AImodel to be $T_c \geq$ 15 K based on a very small set of samples. Notably,these materials are not contained in any existing dataset. Furthermore, weanalyze trends in our dataset and individual materials including B$_4$CN$_3$and B$_5$CN$_2$ whose $T_c$s are 24.08 K and 15.93 K, respectively. Wedemonstrate that AI technique can discover a set of new high-$T_c$superconductors, outline its potential for accelerating discovery of thematerials with targeted properties.</description><author>Xiao-Qi Han, Zhenfeng Ouyang, Peng-Jie Guo, Hao Sun, Ze-Feng Gao, Zhong-Yi Lu</author><pubDate>Thu, 12 Sep 2024 14:16:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08065v1</guid></item><item><title>Q-value Regularized Decision ConvFormer for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2409.08062v1</link><description>As a data-driven paradigm, offline reinforcement learning (Offline RL) hasbeen formulated as sequence modeling, where the Decision Transformer (DT) hasdemonstrated exceptional capabilities. Unlike previous reinforcement learningmethods that fit value functions or compute policy gradients, DT adjusts theautoregressive model based on the expected returns, past states, and actions,using a causally masked Transformer to output the optimal action. However, dueto the inconsistency between the sampled returns within a single trajectory andthe optimal returns across multiple trajectories, it is challenging to set anexpected return to output the optimal action and stitch together suboptimaltrajectories. Decision ConvFormer (DC) is easier to understand in the contextof modeling RL trajectories within a Markov Decision Process compared to DT. Wepropose the Q-value Regularized Decision ConvFormer (QDC), which combines theunderstanding of RL trajectories by DC and incorporates a term that maximizesaction values using dynamic programming methods during training. This ensuresthat the expected returns of the sampled actions are consistent with theoptimal returns. QDC achieves excellent performance on the D4RL benchmark,outperforming or approaching the optimal level in all tested environments. Itparticularly demonstrates outstanding competitiveness in trajectory stitchingcapability.</description><author>Teng Yan, Zhendong Ruan, Yaobang Cai, Yu Han, Wenxian Li, Yang Zhang</author><pubDate>Thu, 12 Sep 2024 14:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08062v1</guid></item><item><title>Spatial Adaptation Layer: Interpretable Domain Adaptation For Biosignal Sensor Array Applications</title><link>http://arxiv.org/abs/2409.08058v1</link><description>Biosignal acquisition is key for healthcare applications and wearabledevices, with machine learning offering promising methods for processingsignals like surface electromyography (sEMG) and electroencephalography (EEG).Despite high within-session performance, intersession performance is hinderedby electrode shift, a known issue across modalities. Existing solutions oftenrequire large and expensive datasets and/or lack robustness andinterpretability. Thus, we propose the Spatial Adaptation Layer (SAL), whichcan be prepended to any biosignal array model and learns a parametrized affinetransformation at the input between two recording sessions. We also introducelearnable baseline normalization (LBN) to reduce baseline fluctuations. Testedon two HD-sEMG gesture recognition datasets, SAL and LBN outperform standardfine-tuning on regular arrays, achieving competitive performance even with alogistic regressor, with orders of magnitude less, physically interpretableparameters. Our ablation study shows that forearm circumferential translationsaccount for the majority of performance improvements, in line with sEMGphysiological expectations.</description><author>Joao Pereira, Michael Alummoottil, Dimitrios Halatsis, Dario Farina</author><pubDate>Thu, 12 Sep 2024 14:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08058v1</guid></item><item><title>Expansive Supervision for Neural Radiance Field</title><link>http://arxiv.org/abs/2409.08056v1</link><description>Neural Radiance Fields have achieved success in creating powerful 3D mediarepresentations with their exceptional reconstruction capabilities. However,the computational demands of volume rendering pose significant challengesduring model training. Existing acceleration techniques often involveredesigning the model architecture, leading to limitations in compatibilityacross different frameworks. Furthermore, these methods tend to overlook thesubstantial memory costs incurred. In response to these challenges, weintroduce an expansive supervision mechanism that efficiently balancescomputational load, rendering quality and flexibility for neural radiance fieldtraining. This mechanism operates by selectively rendering a small but crucialsubset of pixels and expanding their values to estimate the error across theentire area for each iteration. Compare to conventional supervision, our methodeffectively bypasses redundant rendering processes, resulting in notablereductions in both time and memory consumption. Experimental resultsdemonstrate that integrating expansive supervision within existingstate-of-the-art acceleration frameworks can achieve 69% memory savings and 42%time savings, with negligible compromise in visual quality.</description><author>Weixiang Zhang, Shuzhao Xie, Shijia Ge, Wei Yao, Chen Tang, Zhi Wang</author><pubDate>Thu, 12 Sep 2024 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08056v1</guid></item><item><title>Predicting and Accelerating Nanomaterials Synthesis Using Machine Learning Featurization</title><link>http://arxiv.org/abs/2409.08054v1</link><description>Solving for the complex conditions of materials synthesis and processingrequires analyzing information gathered from multiple modes ofcharacterization. Currently, quantitative information is extracted seriallywith manual tools and intuition, constraining the feedback cycle for processoptimization. We use machine learning to automate and generalize featureextraction for in-situ reflection high-energy electron diffraction (RHEED) datato establish quantitatively predictive relationships in small sets ($\sim$10)of expert-labeled data, and apply these to save significant time on subsequentepitaxially grown samples. The fidelity of these relationships is tested on arepresentative material system ($W_{1-x}V_xSe2$ growth on c-plane sapphiresubstrate (0001)) at two stages of synthesis with two aims: 1) predicting thegrain alignment of the deposited film from the pre-growth substrate surfacedata, and 2) estimating the vanadium (V) dopant concentration using in-situRHEED as a proxy for ex-situ methods (e.g. x-ray photoelectron spectroscopy).Both tasks are accomplished using the same set of materials agnostic corefeatures, eliminating the need to retrain for specific systems and leading to apotential 80\% time saving over a 100 sample synthesis campaign. Thesepredictions provide guidance for recipe adjustments to avoid doomed trials,reduce follow-on characterization, and improve control resolution for materialssynthesis, ultimately accelerating materials discovery and commercial scale-up.</description><author>Christopher C. Price, Yansong Li, Guanyu Zhou, Rehan Younas, Spencer S. Zeng, Tim H. Scanlon, Jason M. Munro, Christopher L. Hinkle</author><pubDate>Thu, 12 Sep 2024 14:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08054v1</guid></item><item><title>Learning Video Context as Interleaved Multimodal Sequences</title><link>http://arxiv.org/abs/2407.21757v2</link><description>Narrative videos, such as movies, pose significant challenges in videounderstanding due to their rich contexts (characters, dialogues, storylines)and diverse demands (identify who, relationship, and reason). In this paper, weintroduce MovieSeq, a multimodal language model developed to address the widerange of challenges in understanding video contexts. Our core idea is torepresent videos as interleaved multimodal sequences (including images, plots,videos, and subtitles), either by linking external knowledge databases or usingoffline models (such as whisper for subtitles). Through instruction-tuning,this approach empowers the language model to interact with videos usinginterleaved multimodal instructions. For example, instead of solely relying onvideo as input, we jointly provide character photos alongside their names anddialogues, allowing the model to associate these elements and generate morecomprehensive responses. To demonstrate its effectiveness, we validateMovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)across five settings (video classification, audio description, video-textretrieval, video captioning, and video question-answering). The code will bepublic at https://github.com/showlab/MovieSeq.</description><author>Kevin Qinghong Lin, Pengchuan Zhang, Difei Gao, Xide Xia, Joya Chen, Ziteng Gao, Jinheng Xie, Xuhong Xiao, Mike Zheng Shou</author><pubDate>Thu, 12 Sep 2024 14:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21757v2</guid></item><item><title>Noiseless Privacy-Preserving Decentralized Learning</title><link>http://arxiv.org/abs/2404.09536v2</link><description>Decentralized learning (DL) enables collaborative learning without a serverand without training data leaving the users' devices. However, the modelsshared in DL can still be used to infer training data. Conventional defensessuch as differential privacy and secure aggregation fall short in effectivelysafeguarding user privacy in DL, either sacrificing model utility orefficiency. We introduce Shatter, a novel DL approach in which nodes createvirtual nodes (VNs) to disseminate chunks of their full model on their behalf.This enhances privacy by (i) preventing attackers from collecting full modelsfrom other nodes, and (ii) hiding the identity of the original node thatproduced a given model chunk. We theoretically prove the convergence of Shatterand provide a formal analysis demonstrating how Shatter reduces the efficacy ofattacks compared to when exchanging full models between nodes. We evaluate theconvergence and attack resilience of Shatter with existing DL algorithms, withheterogeneous datasets, and against three standard privacy attacks. Ourevaluation shows that Shatter not only renders these privacy attacks infeasiblewhen each node operates 16 VNs but also exhibits a positive impact on modelutility compared to standard DL. In summary, Shatter enhances the privacy of DLwhile maintaining the utility and efficiency of the model.</description><author>Sayan Biswas, Mathieu Even, Anne-Marie Kermarrec, Laurent Massoulie, Rafael Pires, Rishi Sharma, Martijn de Vos</author><pubDate>Thu, 12 Sep 2024 13:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09536v2</guid></item></channel></rss>