<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 06 Jul 2023 06:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LongNet: Scaling Transformers to 1,000,000,000 Tokens</title><link>http://arxiv.org/abs/2307.02486v1</link><description>Scaling sequence length has become a critical demand in the era of largelanguage models. However, existing methods struggle with either computationalcomplexity or model expressivity, rendering the maximum sequence lengthrestricted. In this work, we introduce LongNet, a Transformer variant that canscale sequence length to more than 1 billion tokens, without sacrificing theperformance on shorter sequences. Specifically, we propose dilated attention,which expands the attentive field exponentially as the distance grows. LongNethas significant advantages: 1) it has a linear computation complexity and alogarithm dependency between tokens; 2) it can be served as a distributedtrainer for extremely long sequences; 3) its dilated attention is a drop-inreplacement for standard attention, which can be seamlessly integrated with theexisting Transformer-based optimization. Experiments results demonstrate thatLongNet yields strong performance on both long-sequence modeling and generallanguage tasks. Our work opens up new possibilities for modeling very longsequences, e.g., treating a whole corpus or even the entire Internet as asequence.</description><author>Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Furu Wei</author><pubDate>Wed, 05 Jul 2023 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02486v1</guid></item><item><title>Compositionality as Lexical Symmetry</title><link>http://arxiv.org/abs/2201.12926v2</link><description>In tasks like semantic parsing, instruction following, and questionanswering, standard deep networks fail to generalize compositionally from smalldatasets. Many existing approaches overcome this limitation with modelarchitectures that enforce a compositional process of sentence interpretation.In this paper, we present a domain-general and model-agnostic formulation ofcompositionality as a constraint on symmetries of data distributions ratherthan models. Informally, we prove that whenever a task can be solved by acompositional model, there is a corresponding data augmentation scheme -- aprocedure for transforming examples into other well formed examples -- thatimparts compositional inductive bias on any model trained to solve the sametask. We describe a procedure called LEXSYM that discovers thesetransformations automatically, then applies them to training data for ordinaryneural sequence models. Unlike existing compositional data augmentationprocedures, LEXSYM can be deployed agnostically across text, structured data,and even images. It matches or surpasses state-of-the-art, task-specific modelson COGS semantic parsing, SCAN and ALCHEMY instruction following, andCLEVR-COGENT visual question answering datasets.</description><author>Ekin Aky√ºrek, Jacob Andreas</author><pubDate>Wed, 05 Jul 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.12926v2</guid></item><item><title>Building Cooperative Embodied Agents Modularly with Large Language Models</title><link>http://arxiv.org/abs/2307.02485v1</link><description>Large Language Models (LLMs) have demonstrated impressive planning abilitiesin single-agent embodied tasks across various domains. However, their capacityfor planning and communication in multi-agent cooperation remains unclear, eventhough these are crucial skills for intelligent embodied agents. In this paper,we present a novel framework that utilizes LLMs for multi-agent cooperation andtests it in various embodied environments. Our framework enables embodiedagents to plan, communicate, and cooperate with other embodied agents or humansto accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs,such as GPT-4, can surpass strong planning-based methods and exhibit emergenteffective communication using our framework without requiring fine-tuning orfew-shot prompting. We also discover that LLM-based agents that communicate innatural language can earn more trust and cooperate more effectively withhumans. Our research underscores the potential of LLMs for embodied AI and laysthe foundation for future research in multi-agent cooperation. Videos can befound on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.</description><author>Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan</author><pubDate>Wed, 05 Jul 2023 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02485v1</guid></item><item><title>Elastic Decision Transformer</title><link>http://arxiv.org/abs/2307.02484v1</link><description>This paper introduces Elastic Decision Transformer (EDT), a significantadvancement over the existing Decision Transformer (DT) and its variants.Although DT purports to generate an optimal trajectory, empirical evidencesuggests it struggles with trajectory stitching, a process involving thegeneration of an optimal or near-optimal trajectory from the best parts of aset of sub-optimal trajectories. The proposed EDT differentiates itself byfacilitating trajectory stitching during action inference at test time,achieved by adjusting the history length maintained in DT. Further, the EDToptimizes the trajectory by retaining a longer history when the previoustrajectory is optimal and a shorter one when it is sub-optimal, enabling it to"stitch" with a more optimal trajectory. Extensive experimentation demonstratesEDT's ability to bridge the performance gap between DT-based and QLearning-based approaches. In particular, the EDT outperforms Q Learning-basedmethods in a multi-task regime on the D4RL locomotion benchmark and Atarigames. Videos are available at: https://kristery.github.io/edt/</description><author>Yueh-Hua Wu, Xiaolong Wang, Masashi Hamaya</author><pubDate>Wed, 05 Jul 2023 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02484v1</guid></item><item><title>Jailbroken: How Does LLM Safety Training Fail?</title><link>http://arxiv.org/abs/2307.02483v1</link><description>Large language models trained for safety and harmlessness remain susceptibleto adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks onearly releases of ChatGPT that elicit undesired behavior. Going beyondrecognition of the issue, we investigate why such attacks succeed and how theycan be created. We hypothesize two failure modes of safety training: competingobjectives and mismatched generalization. Competing objectives arise when amodel's capabilities and safety goals conflict, while mismatched generalizationoccurs when safety training fails to generalize to a domain for whichcapabilities exist. We use these failure modes to guide jailbreak design andthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic'sClaude v1.3, against both existing and newly designed attacks. We find thatvulnerabilities persist despite the extensive red-teaming and safety-trainingefforts behind these models. Notably, new attacks utilizing our failure modessucceed on every prompt in a collection of unsafe requests from the models'red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Ouranalysis emphasizes the need for safety-capability parity -- that safetymechanisms should be as sophisticated as the underlying model -- and arguesagainst the idea that scaling alone can resolve these safety failure modes.</description><author>Alexander Wei, Nika Haghtalab, Jacob Steinhardt</author><pubDate>Wed, 05 Jul 2023 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02483v1</guid></item><item><title>A Dataset of Inertial Measurement Units for Handwritten English Alphabets</title><link>http://arxiv.org/abs/2307.02480v1</link><description>This paper presents an end-to-end methodology for collecting datasets torecognize handwritten English alphabets by utilizing Inertial Measurement Units(IMUs) and leveraging the diversity present in the Indian writing style. TheIMUs are utilized to capture the dynamic movement patterns associated withhandwriting, enabling more accurate recognition of alphabets. The Indiancontext introduces various challenges due to the heterogeneity in writingstyles across different regions and languages. By leveraging this diversity,the collected dataset and the collection system aim to achieve higherrecognition accuracy. Some preliminary experimental results demonstrate theeffectiveness of the dataset in accurately recognizing handwritten Englishalphabet in the Indian context. This research can be extended and contributesto the field of pattern recognition and offers valuable insights for developingimproved systems for handwriting recognition, particularly in diverselinguistic and cultural contexts.</description><author>Hari Prabhat Gupta, Rahul Mishra</author><pubDate>Wed, 05 Jul 2023 18:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02480v1</guid></item><item><title>STEdge: Self-training Edge Detection with Multi-layer Teaching and Regularization</title><link>http://arxiv.org/abs/2201.05121v2</link><description>Learning-based edge detection has hereunto been strongly supervised withpixel-wise annotations which are tedious to obtain manually. We study theproblem of self-training edge detection, leveraging the untapped wealth oflarge-scale unlabeled image datasets. We design a self-supervised frameworkwith multi-layer regularization and self-teaching. In particular, we impose aconsistency regularization which enforces the outputs from each of the multiplelayers to be consistent for the input image and its perturbed counterpart. Weadopt L0-smoothing as the 'perturbation' to encourage edge prediction lying onsalient boundaries following the cluster assumption in self-supervisedlearning. Meanwhile, the network is trained with multi-layer supervision bypseudo labels which are initialized with Canny edges and then iterativelyrefined by the network as the training proceeds. The regularization andself-teaching together attain a good balance of precision and recall, leadingto a significant performance boost over supervised methods, with lightweightrefinement on the target dataset. Furthermore, our method demonstrates strongcross-dataset generality. For example, it attains 4.8% improvement for ODS and5.8% for OIS when tested on the unseen BIPED dataset, compared to thestate-of-the-art methods.</description><author>Yunfan Ye, Renjiao Yi, Zhiping Cai, Kai Xu</author><pubDate>Wed, 05 Jul 2023 18:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.05121v2</guid></item><item><title>Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions</title><link>http://arxiv.org/abs/2307.02478v1</link><description>In this paper, we study linear regression applied to data structured on amanifold. We assume that the data manifold is smooth and is embedded in aEuclidean space, and our objective is to reveal the impact of the datamanifold's extrinsic geometry on the regression. Specifically, we analyze theimpact of the manifold's curvatures (or higher order nonlinearity in theparameterization when the curvatures are locally zero) on the uniqueness of theregression solution. Our findings suggest that the corresponding linearregression does not have a unique solution when the embedded submanifold isflat in some dimensions. Otherwise, the manifold's curvature (or higher ordernonlinearity in the embedding) may contribute significantly, particularly inthe solution associated with the normal directions of the manifold. Ourfindings thus reveal the role of data manifold geometry in ensuring thestability of regression models for out-of-distribution inferences.</description><author>Liangchen Liu, Juncai He, Richard Tsai</author><pubDate>Wed, 05 Jul 2023 18:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02478v1</guid></item><item><title>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</title><link>http://arxiv.org/abs/2307.02477v1</link><description>The impressive performance of recent language models across a wide range oftasks suggests that they possess a degree of abstract reasoning skills. Arethese skills general and transferable, or specialized to specific tasks seenduring pretraining? To disentangle these effects, we propose an evaluationframework based on "counterfactual" task variants that deviate from the defaultassumptions underlying standard tasks. Across a suite of 11 tasks, we observenontrivial performance on the counterfactual variants, but nevertheless findthat performance substantially and consistently degrades compared to thedefault conditions. This suggests that while current LMs may possess abstracttask-solving skills to a degree, they often also rely on narrow,non-transferable procedures for task-solving. These results motivate a morecareful interpretation of language model performance that teases apart theseaspects of behavior.</description><author>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky√ºrek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim</author><pubDate>Wed, 05 Jul 2023 18:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02477v1</guid></item><item><title>Natural Language Deduction with Incomplete Information</title><link>http://arxiv.org/abs/2307.02472v1</link><description>A growing body of work studies how to answer a question or verify a claim bygenerating a natural language "proof": a chain of deductive inferences yieldingthe answer based on a set of premises. However, these methods can only makesound deductions when they follow from evidence that is given. We propose a newsystem that can handle the underspecified setting where not all premises arestated at the outset; that is, additional assumptions need to be materializedto prove a claim. By using a natural language generation model to abductivelyinfer a premise given another premise and a conclusion, we can impute missingpieces of evidence needed for the conclusion to be true. Our system searchesover two fringes in a bidirectional fashion, interleaving deductive(forward-chaining) and abductive (backward-chaining) generation steps. Wesample multiple possible outputs for each step to achieve coverage of thesearch space, at the same time ensuring correctness by filtering low-qualitygenerations with a round-trip validation procedure. Results on a modifiedversion of the EntailmentBank dataset and a new dataset called Everyday Norms:Why Not? show that abductive generation with validation can recover premisesacross in- and out-of-domain settings</description><author>Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett</author><pubDate>Wed, 05 Jul 2023 18:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02472v1</guid></item><item><title>What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</title><link>http://arxiv.org/abs/2307.02469v1</link><description>Recent advancements in Large Language Models (LLMs) such as GPT4 havedisplayed exceptional multi-modal capabilities in following open-endedinstructions given images. However, the performance of these models heavilyrelies on design choices such as network structures, training data, andtraining strategies, and these choices have not been extensively discussed inthe literature, making it difficult to quantify progress in this field. Toaddress this issue, this paper presents a systematic and comprehensive study,quantitatively and qualitatively, on training such models. We implement over 20variants with controlled settings. Concretely, for network structures, wecompare different LLM backbones and model designs. For training data, weinvestigate the impact of data and sampling strategies. For instructions, weexplore the influence of diversified prompts on the instruction-followingability of the trained models. For benchmarks, we contribute the first, to ourbest knowledge, comprehensive evaluation set including both image and videotasks through crowd-sourcing. Based on our findings, we present Lynx, whichperforms the most accurate multi-modal understanding while keeping the bestmulti-modal generation ability compared to existing open-sourced GPT4-stylemodels.</description><author>Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong</author><pubDate>Wed, 05 Jul 2023 18:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02469v1</guid></item><item><title>Large-scale Detection of Marine Debris in Coastal Areas with Sentinel-2</title><link>http://arxiv.org/abs/2307.02465v1</link><description>Detecting and quantifying marine pollution and macro-plastics is anincreasingly pressing ecological issue that directly impacts ecology and humanhealth. Efforts to quantify marine pollution are often conducted with sparseand expensive beach surveys, which are difficult to conduct on a large scale.Here, remote sensing can provide reliable estimates of plastic pollution byregularly monitoring and detecting marine debris in coastal areas.Medium-resolution satellite data of coastal areas is readily available and canbe leveraged to detect aggregations of marine debris containing plastic litter.In this work, we present a detector for marine debris built on a deepsegmentation model that outputs a probability for marine debris at the pixellevel. We train this detector with a combination of annotated datasets ofmarine debris and evaluate it on specifically selected test sites where it ishighly probable that plastic pollution is present in the detected marinedebris. We demonstrate quantitatively and qualitatively that a deep learningmodel trained on this dataset issued from multiple sources outperforms existingdetection models trained on previous datasets by a large margin. Ourexperiments show, consistent with the principles of data-centric AI, that thisperformance is due to our particular dataset design with extensive sampling ofnegative examples and label refinements rather than depending on the particulardeep learning model. We hope to accelerate advances in the large-scaleautomated detection of marine debris, which is a step towards quantifying andmonitoring marine litter with remote sensing at global scales, and release themodel weights and training source code underhttps://github.com/marccoru/marinedebrisdetector</description><author>Marc Ru√üwurm, Sushen Jilla Venkatesa, Devis Tuia</author><pubDate>Wed, 05 Jul 2023 18:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02465v1</guid></item><item><title>AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus Callosum cross section from EM Images</title><link>http://arxiv.org/abs/2307.02464v1</link><description>The electron microscope (EM) remains the predominant technique forelucidating intricate details of the animal nervous system at the nanometerscale. However, accurately reconstructing the complex morphology of axons andmyelin sheaths poses a significant challenge. Furthermore, the absence ofpublicly available, large-scale EM datasets encompassing complete crosssections of the corpus callosum, with dense ground truth segmentation for axonsand myelin sheaths, hinders the advancement and evaluation of holistic corpuscallosum reconstructions. To surmount these obstacles, we introduce theAxonCallosumEM dataset, comprising a 1.83 times 5.76mm EM image captured fromthe corpus callosum of the Rett Syndrome (RTT) mouse model, which entailextensive axon bundles. We meticulously proofread over 600,000 patches at aresolution of 1024 times 1024, thus providing a comprehensive ground truth formyelinated axons and myelin sheaths. Additionally, we extensively annotatedthree distinct regions within the dataset for the purposes of training,testing, and validation. Utilizing this dataset, we develop a fine-tuningmethodology that adapts Segment Anything Model (SAM) to EM images segmentationtasks, called EM-SAM, enabling outperforms other state-of-the-art methods.Furthermore, we present the evaluation results of EM-SAM as a baseline.</description><author>Ao Cheng, Guoqiang Zhao, Lirong Wang, Ruobing Zhang</author><pubDate>Wed, 05 Jul 2023 18:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02464v1</guid></item><item><title>Expert-Agnostic Ultrasound Image Quality Assessment using Deep Variational Clustering</title><link>http://arxiv.org/abs/2307.02462v1</link><description>Ultrasound imaging is a commonly used modality for several diagnostic andtherapeutic procedures. However, the diagnosis by ultrasound relies heavily onthe quality of images assessed manually by sonographers, which diminishes theobjectivity of the diagnosis and makes it operator-dependent. The supervisedlearning-based methods for automated quality assessment require manuallyannotated datasets, which are highly labour-intensive to acquire. Theseultrasound images are low in quality and suffer from noisy annotations causedby inter-observer perceptual variations, which hampers learning efficiency. Wepropose an UnSupervised UltraSound image Quality assessment Network, US2QNet,that eliminates the burden and uncertainty of manual annotations. US2QNet usesthe variational autoencoder embedded with the three modules, pre-processing,clustering and post-processing, to jointly enhance, extract, cluster andvisualize the quality feature representation of ultrasound images. Thepre-processing module uses filtering of images to point the network's attentiontowards salient quality features, rather than getting distracted by noise.Post-processing is proposed for visualizing the clusters of featurerepresentations in 2D space. We validated the proposed framework for qualityassessment of the urinary bladder ultrasound images. The proposed frameworkachieved 78% accuracy and superior performance to state-of-the-art clusteringmethods.</description><author>Deepak Raina, Dimitrios Ntentia, SH Chandrashekhara, Richard Voyles, Subir Kumar Saha</author><pubDate>Wed, 05 Jul 2023 18:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02462v1</guid></item><item><title>Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources</title><link>http://arxiv.org/abs/2307.02460v1</link><description>Traditionally, data selection has been studied in settings where all samplesfrom prospective sources are fully revealed to a machine learning developer.However, in practical data exchange scenarios, data providers often reveal onlya limited subset of samples before an acquisition decision is made. Recently,there have been efforts to fit scaling laws that predict model performance atany size and data source composition using the limited available samples.However, these scaling functions are black-box, computationally expensive tofit, highly susceptible to overfitting, or/and difficult to optimize for dataselection. This paper proposes a framework called &lt;projektor&gt;, which predictsmodel performance and supports data selection decisions based on partialsamples of prospective data sources. Our approach distinguishes itself fromexisting work by introducing a novel *two-stage* performance inference process.In the first stage, we leverage the Optimal Transport distance to predict themodel's performance for any data mixture ratio within the range of discloseddata sizes. In the second stage, we extrapolate the performance to largerundisclosed data sizes based on a novel parameter-free mapping techniqueinspired by neural scaling laws. We further derive an efficient gradient-basedmethod to select data sources based on the projected model performance.Evaluation over a diverse range of applications demonstrates that &lt;projektor&gt;significantly improves existing performance scaling approaches in terms of boththe accuracy of performance inference and the computation costs associated withconstructing the performance predictor. Also, &lt;projektor&gt; outperforms by a widemargin in data selection effectiveness compared to a range of otheroff-the-shelf solutions.</description><author>Feiyang Kang, Hoang Anh Just, Anit Kumar Sahu, Ruoxi Jia</author><pubDate>Wed, 05 Jul 2023 18:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02460v1</guid></item><item><title>Gaussian Database Alignment and Gaussian Planted Matching</title><link>http://arxiv.org/abs/2307.02459v1</link><description>Database alignment is a variant of the graph alignment problem: Given a pairof anonymized databases containing separate yet correlated features for a setof users, the problem is to identify the correspondence between the featuresand align the anonymized user sets based on correlation alone. This closelyrelates to planted matching, where given a bigraph with random weights, thegoal is to identify the underlying matching that generated the given weights.We study an instance of the database alignment problem with multivariateGaussian features and derive results that apply both for database alignment andfor planted matching, demonstrating the connection between them. Theperformance thresholds for database alignment converge to that for plantedmatching when the dimensionality of the database features is \(\omega(\logn)\), where \(n\) is the size of the alignment, and no individual feature istoo strong. The maximum likelihood algorithms for both planted matching anddatabase alignment take the form of a linear program and we study relaxationsto better understand the significance of various constraints under variousconditions and present achievability and converse bounds. Our results show thatthe almost-exact alignment threshold for the relaxed algorithms coincide withthat of maximum likelihood, while there is a gap between the exact alignmentthresholds. Our analysis and results extend to the unbalanced case where oneuser set is not fully covered by the alignment.</description><author>Osman Emre Dai, Daniel Cullina, Negar Kiyavash</author><pubDate>Wed, 05 Jul 2023 18:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02459v1</guid></item><item><title>DeSRA: Detect and Delete the Artifacts of GAN-based Real-World Super-Resolution Models</title><link>http://arxiv.org/abs/2307.02457v1</link><description>Image super-resolution (SR) with generative adversarial networks (GAN) hasachieved great success in restoring realistic details. However, it is notoriousthat GAN-based SR models will inevitably produce unpleasant and undesirableartifacts, especially in practical scenarios. Previous works typically suppressartifacts with an extra loss penalty in the training phase. They only work forin-distribution artifact types generated during training. When applied inreal-world scenarios, we observe that those improved methods still generateobviously annoying artifacts during inference. In this paper, we analyze thecause and characteristics of the GAN artifacts produced in unseen test datawithout ground-truths. We then develop a novel method, namely, DeSRA, to Detectand then Delete those SR Artifacts in practice. Specifically, we propose tomeasure a relative local variance distance from MSE-SR results and GAN-SRresults, and locate the problematic areas based on the above distance andsemantic-aware thresholds. After detecting the artifact regions, we develop afinetune procedure to improve GAN-based SR models with a few samples, so thatthey can deal with similar types of artifacts in more unseen real data.Equipped with our DeSRA, we can successfully eliminate artifacts from inferenceand improve the ability of SR models to be applied in real-world scenarios. Thecode will be available at https://github.com/TencentARC/DeSRA.</description><author>Liangbin Xie, Xintao Wang, Xiangyu Chen, Gen Li, Ying Shan, Jiantao Zhou, Chao Dong</author><pubDate>Wed, 05 Jul 2023 18:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02457v1</guid></item><item><title>Transgressing the boundaries: towards a rigorous understanding of deep learning and its (non-)robustness</title><link>http://arxiv.org/abs/2307.02454v1</link><description>The recent advances in machine learning in various fields of applications canbe largely attributed to the rise of deep learning (DL) methods andarchitectures. Despite being a key technology behind autonomous cars, imageprocessing, speech recognition, etc., a notorious problem remains the lack oftheoretical understanding of DL and related interpretability and (adversarial)robustness issues. Understanding the specifics of DL, as compared to, say,other forms of nonlinear regression methods or statistical learning, isinteresting from a mathematical perspective, but at the same time it is ofcrucial importance in practice: treating neural networks as mere black boxesmight be sufficient in certain cases, but many applications require waterproofperformance guarantees and a deeper understanding of what could go wrong andwhy it could go wrong. It is probably fair to say that, despite beingmathematically well founded as a method to approximate complicated functions,DL is mostly still more like modern alchemy that is firmly in the hands ofengineers and computer scientists. Nevertheless, it is evident that certainspecifics of DL that could explain its success in applications demandssystematic mathematical approaches. In this work, we review robustness issuesof DL and particularly bridge concerns and attempts from approximation theoryto statistical learning theory. Further, we review Bayesian Deep Learning as ameans for uncertainty quantification and rigorous explainability.</description><author>Carsten Hartmann, Lorenz Richter</author><pubDate>Wed, 05 Jul 2023 18:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02454v1</guid></item><item><title>Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models</title><link>http://arxiv.org/abs/2305.14705v2</link><description>Sparse Mixture-of-Experts (MoE) is a neural architecture design that can beutilized to add learnable parameters to Large Language Models (LLMs) withoutincreasing inference cost. Instruction tuning is a technique for training LLMsto follow instructions. We advocate combining these two approaches, as we findthat MoE models benefit more from instruction tuning than dense models. Inparticular, we conduct empirical studies across three experimental setups: (i)Direct finetuning on individual downstream tasks devoid of instruction tuning;(ii) Instructiontuning followed by in-context few-shot or zero-shotgeneralization on downstream tasks; and (iii) Instruction tuning supplementedby further finetuning on individual downstream tasks. In the first scenario,MoE models overall underperform dense models of identical computationalcapacity. This narrative, however, dramatically changes with the introductionof instruction tuning (second and third scenario), used independently or inconjunction with task-specific finetuning. Our most powerful model,FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmarktasks, while using only a third of the FLOPs. The advancements embodiedbyFLAN-MOE inspire a reevaluation of the design principles of large-scale,high-performance language models in the framework of task-agnostic learning.</description><author>Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, Denny Zhou</author><pubDate>Wed, 05 Jul 2023 18:24:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14705v2</guid></item><item><title>LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion</title><link>http://arxiv.org/abs/2307.02452v1</link><description>Wireless capsule endoscopy (WCE) is a painless and non-invasive diagnostictool for gastrointestinal (GI) diseases. However, due to GI anatomicalconstraints and hardware manufacturing limitations, WCE vision signals maysuffer from insufficient illumination, leading to a complicated screening andexamination procedure. Deep learning-based low-light image enhancement (LLIE)in the medical field gradually attracts researchers. Given the exuberantdevelopment of the denoising diffusion probabilistic model (DDPM) in computervision, we introduce a WCE LLIE framework based on the multi-scaleconvolutional neural network (CNN) and reverse diffusion process. Themulti-scale design allows models to preserve high-resolution representation andcontext information from low-resolution, while the curved wavelet attention(CWA) block is proposed for high-frequency and local feature learning.Furthermore, we combine the reverse diffusion procedure to further optimize theshallow output and generate the most realistic image. The proposed method iscompared with ten state-of-the-art (SOTA) LLIE methods and significantlyoutperforms quantitatively and qualitatively. The superior performance on GIdisease segmentation further demonstrates the clinical potential of ourproposed model. Our code is publicly accessible.</description><author>Long Bai, Tong Chen, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren</author><pubDate>Wed, 05 Jul 2023 18:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02452v1</guid></item><item><title>An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code</title><link>http://arxiv.org/abs/2307.02443v1</link><description>Large language models trained on source code can support a variety ofsoftware development tasks, such as code recommendation and program repair.Large amounts of data for training such models benefit the models' performance.However, the size of the data and models results in long training times andhigh energy consumption. While publishing source code allows for replicability,users need to repeat the expensive training process if models are not shared.The main goal of the study is to investigate if publications that trainedlanguage models for software engineering (SE) tasks share source code andtrained artifacts. The second goal is to analyze the transparency on trainingenergy usage. We perform a snowballing-based literature search to findpublications on language models for source code, and analyze their reusabilityfrom a sustainability standpoint. From 494 unique publications, we identified 293 relevant publications thatuse language models to address code-related tasks. Among them, 27% (79 out of293) make artifacts available for reuse. This can be in the form of tools orIDE plugins designed for specific tasks or task-agnostic models that can befine-tuned for a variety of downstream tasks. Moreover, we collect insights onthe hardware used for model training, as well as training time, which togetherdetermine the energy consumption of the development process. We find that thereare deficiencies in the sharing of information and artifacts for currentstudies on source code models for software engineering tasks, with 40% of thesurveyed papers not sharing source code or trained artifacts. We recommend thesharing of source code as well as trained artifacts, to enable sustainablereproducibility. Moreover, comprehensive information on training times andhardware configurations should be shared for transparency on a model's carbonfootprint.</description><author>Max Hort, Anastasiia Grishina, Leon Moonen</author><pubDate>Wed, 05 Jul 2023 18:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02443v1</guid></item><item><title>Phase Unwrapping of Color Doppler Echocardiography using Deep Learning</title><link>http://arxiv.org/abs/2306.13695v2</link><description>Color Doppler echocardiography is a widely used non-invasive imaging modalitythat provides real-time information about the intracardiac blood flow. In anapical long-axis view of the left ventricle, color Doppler is subject to phasewrapping, or aliasing, especially during cardiac filling and ejection. Whensetting up quantitative methods based on color Doppler, it is necessary tocorrect this wrapping artifact. We developed an unfolded primal-dual network tounwrap (dealias) color Doppler echocardiographic images and compared itseffectiveness against two state-of-the-art segmentation approaches based onnnU-Net and transformer models. We trained and evaluated the performance ofeach method on an in-house dataset and found that the nnU-Net-based methodprovided the best dealiased results, followed by the primal-dual approach andthe transformer-based technique. Noteworthy, the primal-dual network, which hadsignificantly fewer trainable parameters, performed competitively with respectto the other two methods, demonstrating the high potential of deep unfoldingmethods. Our results suggest that deep learning-based methods can effectivelyremove aliasing artifacts in color Doppler echocardiographic images,outperforming DeAN, a state-of-the-art semi-automatic technique. Overall, ourresults show that deep learning-based methods have the potential to effectivelypreprocess color Doppler images for downstream quantitative analysis.</description><author>Hang Jung Ling, Olivier Bernard, Nicolas Ducros, Damien Garcia</author><pubDate>Wed, 05 Jul 2023 18:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13695v2</guid></item><item><title>Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</title><link>http://arxiv.org/abs/2305.02301v2</link><description>Deploying large language models (LLMs) is challenging because they are memoryinefficient and compute-intensive for practical applications. In reaction,researchers train smaller task-specific models by either finetuning with humanlabels or distilling using LLM-generated labels. However, finetuning anddistillation require large amounts of training data to achieve comparableperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that(a) trains smaller models that outperform LLMs, and (b) achieves so byleveraging less training data needed by finetuning or distillation. Our methodextracts LLM rationales as additional supervision for training small modelswithin a multi-task framework. We present three findings across 4 NLPbenchmarks: First, compared to both finetuning and distillation, our mechanismachieves better performance with much fewer labeled/unlabeled trainingexamples. Second, compared to few-shot prompted LLMs, we achieve betterperformance using substantially smaller model sizes. Third, we reduce both themodel size and the amount of data required to outperform LLMs; our finetuned770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%of available data on a benchmark, whereas standard finetuning the same T5 modelstruggles to match even by using 100% of the dataset. We release the code at:https://github.com/google-research/distilling-step-by-step .</description><author>Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister</author><pubDate>Wed, 05 Jul 2023 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02301v2</guid></item><item><title>Exploring Continual Learning for Code Generation Models</title><link>http://arxiv.org/abs/2307.02435v1</link><description>Large-scale code generation models such as Codex and CodeT5 have achievedimpressive performance. However, libraries are upgraded or deprecated veryfrequently and re-training large-scale language models is computationallyexpensive. Therefore, Continual Learning (CL) is an important aspect thatremains underexplored in the code domain. In this paper, we introduce abenchmark called CodeTask-CL that covers a wide range of tasks, including codegeneration, translation, summarization, and refinement, with different inputand output programming languages. Next, on our CodeTask-CL benchmark, wecompare popular CL techniques from NLP and Vision domains. We find thateffective methods like Prompt Pooling (PP) suffer from catastrophic forgettingdue to the unstable training of the prompt selection mechanism caused by starkdistribution shifts in coding tasks. We address this issue with our proposedmethod, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes trainingby enforcing constraints on the prompt selection mechanism and leads to a21.54% improvement over Prompt Pooling. Along with the benchmark, we establisha training pipeline that can be used for CL on code models, which we believecan motivate further development of CL methods for code models. Our code isavailable at https://github.com/amazon-science/codetaskcl-pptf</description><author>Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, Bing Xiang</author><pubDate>Wed, 05 Jul 2023 17:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02435v1</guid></item><item><title>Exclusive Supermask Subnetwork Training for Continual Learning</title><link>http://arxiv.org/abs/2210.10209v2</link><description>Continual Learning (CL) methods focus on accumulating knowledge over timewhile avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)proposed a CL method, SupSup, which uses a randomly initialized, fixed basenetwork (model) and finds a supermask for each new task that selectively keepsor removes each weight to produce a subnetwork. They prevent forgetting as thenetwork weights are not being updated. Although there is no forgetting, theperformance of SupSup is sub-optimal because fixed weights restrict itsrepresentational power. Furthermore, there is no accumulation or transfer ofknowledge inside the model when new tasks are learned. Hence, we proposeExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive andnon-overlapping subnetwork weight training. This avoids conflicting updates tothe shared weights by subsequent tasks to improve performance while stillpreventing forgetting. Furthermore, we propose a novel KNN-based KnowledgeTransfer (KKT) module that utilizes previously acquired knowledge to learn newtasks better and faster. We demonstrate that ExSSNeT outperforms strongprevious methods on both NLP and Vision domains while preventing forgetting.Moreover, ExSSNeT is particularly advantageous for sparse masks that activate2-10% of the model parameters, resulting in an average improvement of 8.3% overSupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our codeis available at https://github.com/prateeky2806/exessnet.</description><author>Prateek Yadav, Mohit Bansal</author><pubDate>Wed, 05 Jul 2023 17:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10209v2</guid></item><item><title>A probabilistic, data-driven closure model for RANS simulations with aleatoric, model uncertainty</title><link>http://arxiv.org/abs/2307.02432v1</link><description>We propose a data-driven, closure model for Reynolds-averaged Navier-Stokes(RANS) simulations that incorporates aleatoric, model uncertainty. The proposedclosure consists of two parts. A parametric one, which utilizes previouslyproposed, neural-network-based tensor basis functions dependent on the rate ofstrain and rotation tensor invariants. This is complemented by latent, randomvariables which account for aleatoric model errors. A fully Bayesianformulation is proposed, combined with a sparsity-inducing prior in order toidentify regions in the problem domain where the parametric closure isinsufficient and where stochastic corrections to the Reynolds stress tensor areneeded. Training is performed using sparse, indirect data, such as meanvelocities and pressures, in contrast to the majority of alternatives thatrequire direct Reynolds stress data. For inference and learning, a StochasticVariational Inference scheme is employed, which is based on Monte Carloestimates of the pertinent objective in conjunction with the reparametrizationtrick. This necessitates derivatives of the output of the RANS solver, forwhich we developed an adjoint-based formulation. In this manner, the parametricsensitivities from the differentiable solver can be combined with the built-in,automatic differentiation capability of the neural network library in order toenable an end-to-end differentiable framework. We demonstrate the capability ofthe proposed model to produce accurate, probabilistic, predictive estimates forall flow quantities, even in regions where model errors are present, on aseparated flow in the backward-facing step benchmark problem.</description><author>Atul Agrawal, Phaedon-Stelios Koutsourelakis</author><pubDate>Wed, 05 Jul 2023 17:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02432v1</guid></item><item><title>Base Layer Efficiency in Scalable Human-Machine Coding</title><link>http://arxiv.org/abs/2307.02430v1</link><description>A basic premise in scalable human-machine coding is that the base layer isintended for automated machine analysis and is therefore more compressible thanthe same content would be for human viewing. Use cases for such coding includevideo surveillance and traffic monitoring, where the majority of the contentwill never be seen by humans. Therefore, base layer efficiency is of paramountimportance because the system would most frequently operate at the base-layerrate. In this paper, we analyze the coding efficiency of the base layer in astate-of-the-art scalable human-machine image codec, and show that it can beimproved. In particular, we demonstrate that gains of 20-40% in BD-Ratecompared to the currently best results on object detection and instancesegmentation are possible.</description><author>Yalda Foroutan, Alon Harell, Anderson de Andrade, Ivan V. Bajiƒá</author><pubDate>Wed, 05 Jul 2023 17:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02430v1</guid></item><item><title>FOCUS: Object-Centric World Models for Robotics Manipulation</title><link>http://arxiv.org/abs/2307.02427v1</link><description>Understanding the world in terms of objects and the possible interplays withthem is an important cognition ability, especially in robotics manipulation,where many tasks require robot-object interactions. However, learning such astructured world model, which specifically captures entities and relationships,remains a challenging and underexplored problem. To address this, we proposeFOCUS, a model-based agent that learns an object-centric world model. Thanks toa novel exploration bonus that stems from the object-centric representation,FOCUS can be deployed on robotics manipulation tasks to explore objectinteractions more easily. Evaluating our approach on manipulation tasks acrossdifferent settings, we show that object-centric world models allow the agent tosolve tasks more efficiently and enable consistent exploration of robot-objectinteractions. Using a Franka Emika robot arm, we also showcase how FOCUS couldbe adopted in real-world settings.</description><author>Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt</author><pubDate>Wed, 05 Jul 2023 17:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02427v1</guid></item><item><title>DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models</title><link>http://arxiv.org/abs/2307.02421v1</link><description>Despite the ability of existing large-scale text-to-image (T2I) models togenerate high-quality images from detailed textual descriptions, they oftenlack the ability to precisely edit the generated or real images. In this paper,we propose a novel image editing method, DragonDiffusion, enabling Drag-stylemanipulation on Diffusion models. Specifically, we construct classifierguidance based on the strong correspondence of intermediate features in thediffusion model. It can transform the editing signals into gradients viafeature correspondence loss to modify the intermediate representation of thediffusion model. Based on this guidance strategy, we also build a multi-scaleguidance to consider both semantic and geometric alignment. Moreover, across-branch self-attention is added to maintain the consistency between theoriginal image and the editing result. Our method, through an efficient design,achieves various editing modes for the generated or real images, such as objectmoving, object resizing, object appearance replacement, and content dragging.It is worth noting that all editing and content preservation signals come fromthe image itself, and the model does not require fine-tuning or additionalmodules. Our source code will be available athttps://github.com/MC-E/DragonDiffusion.</description><author>Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang</author><pubDate>Wed, 05 Jul 2023 17:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02421v1</guid></item><item><title>Deep Learning Hydrodynamic Forecasting for Flooded Region Assessment in Near-Real-Time (DL Hydro-FRAN)</title><link>http://arxiv.org/abs/2305.12052v2</link><description>Hydrodynamic flood modeling improves hydrologic and hydraulic prediction ofstorm events. However, the computationally intensive numerical solutionsrequired for high-resolution hydrodynamics have historically prevented theirimplementation in near-real-time flood forecasting. This study examines whetherseveral Deep Neural Network (DNN) architectures are suitable for optimizinghydrodynamic flood models. Several pluvial flooding events were simulated in alow-relief high-resolution urban environment using a 2D HEC-RAS hydrodynamicmodel. These simulations were assembled into a training set for the DNNs, whichwere then used to forecast flooding depths and velocities. The DNNs' forecastswere compared to the hydrodynamic flood models, and showed good agreement, witha median RMSE of around 2 mm for cell flooding depths in the study area. TheDNNs also improved forecast computation time significantly, with the DNNsproviding forecasts between 34.2 and 72.4 times faster than conventionalhydrodynamic models. The study area showed little change between HEC-RAS' FullMomentum Equations and Diffusion Equations, however, important numericalstability considerations were discovered that impact equation selection and DNNarchitecture configuration. Overall, the results from this study show that DNNscan greatly optimize hydrodynamic flood modeling, and enable near-real-timehydrodynamic flood forecasting.</description><author>Francisco Haces-Garcia, Natalya Maslennikova, Craig L Glennie, Hanadi S Rifai, Vedhus Hoskere, Nima Ekhtari</author><pubDate>Wed, 05 Jul 2023 17:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12052v2</guid></item><item><title>In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick</title><link>http://arxiv.org/abs/2307.02419v1</link><description>Large language models (LLMs) have brought significant and transformativechanges in human society. These models have demonstrated remarkablecapabilities in natural language understanding and generation, leading tovarious advancements and impacts across several domains. We consider the in-context learning under two formulation for attentionrelated regression in this work. Given matrices $A_1 \in \mathbb{R}^{n \timesd}$, and $A_2 \in \mathbb{R}^{n \times d}$ and $B \in \mathbb{R}^{n \times n}$,the purpose is to solve some certain optimization problems: Normalized version$\min_{X} \| D(X)^{-1} \exp(A_1 X A_2^\top) - B \|_F^2$ and Rescaled version$\| \exp(A_1 X A_2^\top) - D(X) \cdot B \|_F^2$. Here $D(X) := \mathrm{diag}(\exp(A_1 X A_2^\top) {\bf 1}_n )$. Our regression problem shares similarities with previous studies onsoftmax-related regression. Prior research has extensively investigatedregression techniques related to softmax regression: Normalized version $\|\langle \exp(Ax) , {\bf 1}_n \rangle^{-1} \exp(Ax) - b \|_2^2$ and Resscaledversion $\| \exp(Ax) - \langle \exp(Ax), {\bf 1}_n \rangle b \|_2^2 $ In contrast to previous approaches, we adopt a vectorization technique toaddress the regression problem in matrix formulation. This approach expands thedimension from $d$ to $d^2$, resembling the formulation of the regressionproblem mentioned earlier. Upon completing the lipschitz analysis of our regression function, we havederived our main result concerning in-context learning.</description><author>Yeqi Gao, Zhao Song, Shenghao Xie</author><pubDate>Wed, 05 Jul 2023 17:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02419v1</guid></item><item><title>$ŒΩ^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows</title><link>http://arxiv.org/abs/2307.02405v1</link><description>In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flowsmethod to final states containing multiple neutrinos. The architecture cannatively scale for all combinations of object types and multiplicities in thefinal state for any desired neutrino multiplicities. In $t\bar{t}$ dileptonevents, the momenta of both neutrinos and correlations between them arereconstructed more accurately than when using the most popular standardanalytical techniques, and solutions are found for all events. Inference timeis significantly faster than competing methods, and can be reduced further byevaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to$t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfoldeddistributions is much closer to the limit of performance set by perfectneutrino reconstruction than standard techniques. For the chosen doubledifferential observables $\nu^2$-Flows results in improved statisticalprecision for each bin by a factor of 1.5 to 2 in comparison to the NeutrinoWeighting method and up to a factor of four in comparison to the Ellipseapproach.</description><author>John Andrew Raine, Matthew Leigh, Knut Zoch, Tobias Golling</author><pubDate>Wed, 05 Jul 2023 17:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02405v1</guid></item><item><title>Cross-Shape Attention for Part Segmentation of 3D Point Clouds</title><link>http://arxiv.org/abs/2003.09053v6</link><description>We present a deep learning method that propagates point-wise featurerepresentations across shapes within a collection for the purpose of 3D shapesegmentation. We propose a cross-shape attention mechanism to enableinteractions between a shape's point-wise features and those of other shapes.The mechanism assesses both the degree of interaction between points and alsomediates feature propagation across shapes, improving the accuracy andconsistency of the resulting point-wise feature representations for shapesegmentation. Our method also proposes a shape retrieval measure to selectsuitable shapes for cross-shape attention operations for each test shape. Ourexperiments demonstrate that our approach yields state-of-the-art results inthe popular PartNet dataset.</description><author>Marios Loizou, Siddhant Garg, Dmitry Petrov, Melinos Averkiou, Evangelos Kalogerakis</author><pubDate>Wed, 05 Jul 2023 17:26:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.09053v6</guid></item><item><title>Unbalanced Optimal Transport: A Unified Framework for Object Detection</title><link>http://arxiv.org/abs/2307.02402v1</link><description>During training, supervised object detection tries to correctly match thepredicted bounding boxes and associated classification scores to the groundtruth. This is essential to determine which predictions are to be pushedtowards which solutions, or to be discarded. Popular matching strategiesinclude matching to the closest ground truth box (mostly used in combinationwith anchors), or matching via the Hungarian algorithm (mostly used inanchor-free methods). Each of these strategies comes with its own properties,underlying losses, and heuristics. We show how Unbalanced Optimal Transportunifies these different approaches and opens a whole continuum of methods inbetween. This allows for a finer selection of the desired properties.Experimentally, we show that training an object detection model with UnbalancedOptimal Transport is able to reach the state-of-the-art both in terms ofAverage Precision and Average Recall as well as to provide a faster initialconvergence. The approach is well suited for GPU implementation, which provesto be an advantage for large-scale models.</description><author>Henri De Plaen, Pierre-Fran√ßois De Plaen, Johan A. K. Suykens, Marc Proesmans, Tinne Tuytelaars, Luc Van Gool</author><pubDate>Wed, 05 Jul 2023 17:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02402v1</guid></item><item><title>Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection</title><link>http://arxiv.org/abs/2306.17431v2</link><description>Detecting the salient objects in a remote sensing image has wide applicationsfor the interdisciplinary research. Many existing deep learning methods havebeen proposed for Salient Object Detection (SOD) in remote sensing images andget remarkable results. However, the recent adversarial attack examples,generated by changing a few pixel values on the original remote sensing image,could result in a collapse for the well-trained deep learning based SOD model.Different with existing methods adding perturbation to original images, wepropose to jointly tune adversarial exposure and additive perturbation forattack and constrain image close to cloudy image as Adversarial Cloud. Cloud isnatural and common in remote sensing images, however, camouflaging cloud basedadversarial attack and defense for remote sensing images are not well studiedbefore. Furthermore, we design DefenseNet as a learn-able pre-processing to theadversarial cloudy images so as to preserve the performance of the deeplearning based remote sensing SOD model, without tuning the already deployeddeep SOD model. By considering both regular and generalized adversarialexamples, the proposed DefenseNet can defend the proposed Adversarial Cloud inwhite-box setting and other attack methods in black-box setting. Experimentalresults on a synthesized benchmark from the public remote sensing SOD dataset(EORSSD) show the promising defense against adversarial cloud attacks.</description><author>Huiming Sun, Lan Fu, Jinlong Li, Qing Guo, Zibo Meng, Tianyun Zhang, Yuewei Lin, Hongkai Yu</author><pubDate>Wed, 05 Jul 2023 17:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17431v2</guid></item><item><title>A Versatile Hub Model For Efficient Information Propagation And Feature Selection</title><link>http://arxiv.org/abs/2307.02398v1</link><description>Hub structure, characterized by a few highly interconnected nodes surroundedby a larger number of nodes with fewer connections, is a prominent topologicalfeature of biological brains, contributing to efficient information transferand cognitive processing across various species. In this paper, a mathematicalmodel of hub structure is presented. The proposed method is versatile and canbe broadly applied to both computational neuroscience and Recurrent NeuralNetworks (RNNs) research. We employ the Echo State Network (ESN) as a means toinvestigate the mechanistic underpinnings of hub structures. Our findingsdemonstrate a substantial enhancement in performance upon incorporating the hubstructure. Through comprehensive mechanistic analyses, we show that the hubstructure improves model performance by facilitating efficient informationprocessing and better feature extractions.</description><author>Zhaoze Wang, Junsong Wang</author><pubDate>Wed, 05 Jul 2023 17:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02398v1</guid></item><item><title>Lightweight Vision Transformer with Cross Feature Attention</title><link>http://arxiv.org/abs/2207.07268v2</link><description>Recent advances in vision transformers (ViTs) have achieved great performancein visual recognition tasks. Convolutional neural networks (CNNs) exploitspatial inductive bias to learn visual representations, but these networks arespatially local. ViTs can learn global representations with theirself-attention mechanism, but they are usually heavy-weight and unsuitable formobile devices. In this paper, we propose cross feature attention (XFA) tobring down computation cost for transformers, and combine efficient mobile CNNsto form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which canserve as a general-purpose backbone to learn both global and localrepresentation. Experimental results show that XFormer outperforms numerous CNNand ViT-based models across different tasks and datasets. On ImageNet1Kdataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters,which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT(ViT-based) for similar number of parameters. Our model also performs well whentransferring to object detection and semantic segmentation tasks. On MS COCOdataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 -&gt; 33.2 AP) in YOLOv3framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, withonly a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3,surpassing state-of-the-art lightweight segmentation networks.</description><author>Youpeng Zhao, Huadong Tang, Yingying Jiang, Yong A, Qiang Wu</author><pubDate>Wed, 05 Jul 2023 17:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07268v2</guid></item><item><title>Won't Get Fooled Again: Answering Questions with False Premises</title><link>http://arxiv.org/abs/2307.02394v1</link><description>Pre-trained language models (PLMs) have shown unprecedented potential invarious fields, especially as the backbones for question-answering (QA)systems. However, they tend to be easily deceived by tricky questions such as"How many eyes does the sun have?". Such frailties of PLMs often allude to thelack of knowledge within them. In this paper, we find that the PLMs alreadypossess the knowledge required to rebut such questions, and the key is how toactivate the knowledge. To systematize this observation, we investigate thePLMs' responses to one kind of tricky questions, i.e., the false premisesquestions (FPQs). We annotate a FalseQA dataset containing 2365 human-writtenFPQs, with the corresponding explanations for the false premises and therevised true premise questions. Using FalseQA, we discover that PLMs arecapable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256)of examples. PLMs also generate reasonable explanations for the false premise,which serve as rebuttals. Further replaying a few general questions duringtraining allows PLMs to excel on FPQs and general questions simultaneously. Ourwork suggests that once the rebuttal ability is stimulated, knowledge insidethe PLMs can be effectively utilized to handle FPQs, which incentivizes theresearch on PLM-based QA systems.</description><author>Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, Maosong Sun</author><pubDate>Wed, 05 Jul 2023 17:09:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02394v1</guid></item><item><title>Learning Models of Adversarial Agent Behavior under Partial Observability</title><link>http://arxiv.org/abs/2306.11168v2</link><description>The need for opponent modeling and tracking arises in several real-worldscenarios, such as professional sports, video game design, and drug-traffickinginterdiction. In this work, we present Graph based Adversarial Modeling withMutal Information (GrAMMI) for modeling the behavior of an adversarial opponentagent. GrAMMI is a novel graph neural network (GNN) based approach that usesmutual information maximization as an auxiliary objective to predict thecurrent and future states of an adversarial opponent with partialobservability. To evaluate GrAMMI, we design two large-scale, pursuit-evasiondomains inspired by real-world scenarios, where a team of heterogeneous agentsis tasked with tracking and interdicting a single adversarial agent, and theadversarial agent must evade detection while achieving its own objectives. Withthe mutual information formulation, GrAMMI outperforms all baselines in bothdomains and achieves 31.68% higher log-likelihood on average for futureadversarial state predictions across both domains.</description><author>Sean Ye, Manisha Natarajan, Zixuan Wu, Rohan Paleja, Letian Chen, Matthew C. Gombolay</author><pubDate>Wed, 05 Jul 2023 17:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11168v2</guid></item><item><title>RADiff: Controllable Diffusion Models for Radio Astronomical Maps Generation</title><link>http://arxiv.org/abs/2307.02392v1</link><description>Along with the nearing completion of the Square Kilometre Array (SKA), comesan increasing demand for accurate and reliable automated solutions to extractvaluable information from the vast amount of data it will allow acquiring.Automated source finding is a particularly important task in this context, asit enables the detection and classification of astronomical objects.Deep-learning-based object detection and semantic segmentation models haveproven to be suitable for this purpose. However, training such deep networksrequires a high volume of labeled data, which is not trivial to obtain in thecontext of radio astronomy. Since data needs to be manually labeled by experts,this process is not scalable to large dataset sizes, limiting the possibilitiesof leveraging deep networks to address several tasks. In this work, we proposeRADiff, a generative approach based on conditional diffusion models trainedover an annotated radio dataset to generate synthetic images, containing radiosources of different morphologies, to augment existing datasets and reduce theproblems caused by class imbalances. We also show that it is possible togenerate fully-synthetic image-annotation pairs to automatically augment anyannotated dataset. We evaluate the effectiveness of this approach by training asemantic segmentation model on a real dataset augmented in two ways: 1) usingsynthetic images obtained from real masks, and 2) generating images fromsynthetic semantic masks. We show an improvement in performance when applyingaugmentation, gaining up to 18% in performance when using real masks and 4%when augmenting with synthetic masks. Finally, we employ this model to generatelarge-scale radio maps with the objective of simulating Data Challenges.</description><author>Renato Sortino, Thomas Cecconello, Andrea DeMarco, Giuseppe Fiameni, Andrea Pilzer, Andrew M. Hopkins, Daniel Magro, Simone Riggi, Eva Sciacca, Adriano Ingallinera, Cristobal Bordiu, Filomena Bufano, Concetto Spampinato</author><pubDate>Wed, 05 Jul 2023 17:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02392v1</guid></item><item><title>Causal Discovery with Language Models as Imperfect Experts</title><link>http://arxiv.org/abs/2307.02390v1</link><description>Understanding the causal relationships that underlie a system is afundamental prerequisite to accurate decision-making. In this work, we explorehow expert knowledge can be used to improve the data-driven identification ofcausal graphs, beyond Markov equivalence classes. In doing so, we consider asetting where we can query an expert about the orientation of causalrelationships between variables, but where the expert may provide erroneousinformation. We propose strategies for amending such expert knowledge based onconsistency properties, e.g., acyclicity and conditional independencies in theequivalence class. We then report a case study, on real data, where a largelanguage model is used as an imperfect expert.</description><author>Stephanie Long, Alexandre Pich√©, Valentina Zantedeschi, Tibor Schuster, Alexandre Drouin</author><pubDate>Wed, 05 Jul 2023 17:01:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02390v1</guid></item><item><title>Synthetic Data for Model Selection</title><link>http://arxiv.org/abs/2105.00717v2</link><description>Recent breakthroughs in synthetic data generation approaches made it possibleto produce highly photorealistic images which are hardly distinguishable fromreal ones. Furthermore, synthetic generation pipelines have the potential togenerate an unlimited number of images. The combination of high photorealismand scale turn synthetic data into a promising candidate for improving variousmachine learning (ML) pipelines. Thus far, a large body of research in thisfield has focused on using synthetic images for training, by augmenting andenlarging training data. In contrast to using synthetic data for training, inthis work we explore whether synthetic data can be beneficial for modelselection. Considering the task of image classification, we demonstrate thatwhen data is scarce, synthetic data can be used to replace the held outvalidation set, thus allowing to train on a larger dataset. We also introduce anovel method to calibrate the synthetic error estimation to fit that of thereal domain. We show that such calibration significantly improves theusefulness of synthetic data for model selection.</description><author>Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Matan Fintz, Gerard Medioni</author><pubDate>Wed, 05 Jul 2023 16:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.00717v2</guid></item><item><title>Multi-Task Learning with Summary Statistics</title><link>http://arxiv.org/abs/2307.02388v1</link><description>Multi-task learning has emerged as a powerful machine learning paradigm forintegrating data from multiple sources, leveraging similarities between tasksto improve overall model performance. However, the application of multi-tasklearning to real-world settings is hindered by data-sharing constraints,especially in healthcare settings. To address this challenge, we propose aflexible multi-task learning framework utilizing summary statistics fromvarious sources. Additionally, we present an adaptive parameter selectionapproach based on a variant of Lepski's method, allowing for data-driven tuningparameter selection when only summary statistics are available. Our systematicnon-asymptotic analysis characterizes the performance of the proposed methodsunder various regimes of the sample complexity and overlap. We demonstrate ourtheoretical findings and the performance of the method through extensivesimulations. This work offers a more flexible tool for training related modelsacross various domains, with practical implications in genetic risk predictionand many other fields.</description><author>Parker Knight, Rui Duan</author><pubDate>Wed, 05 Jul 2023 16:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02388v1</guid></item><item><title>Exploring Local Norms in Exp-concave Statistical Learning</title><link>http://arxiv.org/abs/2302.10726v2</link><description>We consider the problem of stochastic convex optimization with exp-concavelosses using Empirical Risk Minimization in a convex class. Answering aquestion raised in several prior works, we provide a $O( d / n + \log( 1 /\delta) / n )$ excess risk bound valid for a wide class of bounded exp-concavelosses, where $d$ is the dimension of the convex reference set, $n$ is thesample size, and $\delta$ is the confidence level. Our result is based on aunified geometric assumption on the gradient of losses and the notion of localnorms.</description><author>Nikita Puchkin, Nikita Zhivotovskiy</author><pubDate>Wed, 05 Jul 2023 16:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10726v2</guid></item><item><title>Machine learning at the mesoscale: a computation-dissipation bottleneck</title><link>http://arxiv.org/abs/2307.02379v1</link><description>The cost of information processing in physical systems calls for a trade-offbetween performance and energetic expenditure. Here we formulate and study acomputation-dissipation bottleneck in mesoscopic systems used as input-outputdevices. Using both real datasets and synthetic tasks, we show hownon-equilibrium leads to enhanced performance. Our framework sheds light on acrucial compromise between information compression, input-output computationand dynamic irreversibility induced by non-reciprocal interactions.</description><author>Alessandro Ingrosso, Emanuele Panizon</author><pubDate>Wed, 05 Jul 2023 16:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02379v1</guid></item><item><title>Continuum Limits of Ollivier's Ricci Curvature on data clouds: pointwise consistency and global lower bounds</title><link>http://arxiv.org/abs/2307.02378v1</link><description>Let $\mathcal{M} \subseteq \mathbb{R}^d$ denote a low-dimensional manifoldand let $\mathcal{X}= \{ x_1, \dots, x_n \}$ be a collection of pointsuniformly sampled from $\mathcal{M}$. We study the relationship between thecurvature of a random geometric graph built from $\mathcal{X}$ and thecurvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier'sdiscrete Ricci curvature. We prove pointwise, non-asymptotic consistencyresults and also show that if $\mathcal{M}$ has Ricci curvature bounded frombelow by a positive constant, then the random geometric graph will inherit thisglobal structural property with high probability. We discuss applications ofthe global discrete curvature bounds to contraction properties of heat kernelson graphs, as well as implications for manifold learning from data clouds. Inparticular, we show that the consistency results allow for characterizing theintrinsic curvature of a manifold from extrinsic curvature.</description><author>Nicolas Garcia Trillos, Melanie Weber</author><pubDate>Wed, 05 Jul 2023 16:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02378v1</guid></item><item><title>Causal Dependence Plots</title><link>http://arxiv.org/abs/2303.04209v2</link><description>Explaining artificial intelligence or machine learning models is increasinglyimportant. To use such data-driven systems wisely we must understand how theyinteract with the world, including how they depend causally on data inputs. Inthis work we develop Causal Dependence Plots (CDPs) to visualize how onevariable--an outcome--depends on changes in another variable--apredictor--$\textit{along with any consequent causal changes in other predictorvariables}$. Crucially, CDPs differ from standard methods based on holdingother predictors constant or assuming they are independent. CDPs make use of anauxiliary causal model because causal conclusions require causal assumptions.With simulations and real data experiments, we show CDPs can be combined in amodular way with methods for causal learning or sensitivity analysis. Sincepeople often think causally about input-output dependence, CDPs can be powerfultools in the xAI or interpretable machine learning toolkit and contribute toapplications like scientific machine learning and algorithmic fairness.</description><author>Joshua R. Loftus, Lucius E. J. Bynum, Sakina Hansen</author><pubDate>Wed, 05 Jul 2023 16:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04209v2</guid></item><item><title>Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance Predictions</title><link>http://arxiv.org/abs/2307.02367v1</link><description>Providing accurate uncertainty estimations is essential for producingreliable machine learning models, especially in safety-critical applicationssuch as accelerator systems. Gaussian process models are generally regarded asthe gold standard method for this task, but they can struggle with large,high-dimensional datasets. Combining deep neural networks with Gaussian processapproximation techniques have shown promising results, but dimensionalityreduction through standard deep neural network layers is not guaranteed tomaintain the distance information necessary for Gaussian process models. Webuild on previous work by comparing the use of the singular value decompositionagainst a spectral-normalized dense layer as a feature extractor for a deepneural Gaussian process approximation model and apply it to a capacitanceprediction problem for the High Voltage Converter Modulators in the Oak RidgeSpallation Neutron Source. Our model shows improved distance preservation andpredicts in-distribution capacitance values with less than 1% error.</description><author>Steven Goldenberg, Malachi Schram, Kishansingh Rajput, Thomas Britton, Chris Pappas, Dan Lu, Jared Walden, Majdi I. Radaideh, Sarah Cousineau, Sudarshan Harave</author><pubDate>Wed, 05 Jul 2023 16:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02367v1</guid></item><item><title>TransRUPNet for Improved Out-of-Distribution Generalization in Polyp Segmentation</title><link>http://arxiv.org/abs/2306.02176v2</link><description>Out-of-distribution (OOD) generalization is a critical challenge in deeplearning. It is specifically important when the test samples are drawn from adifferent distribution than the training data. We develop a novel real-timedeep learning based architecture, TransRUPNet that is based on a Transformerand residual upsampling network for colorectal polyp segmentation to improveOOD generalization. The proposed architecture, TransRUPNet, is anencoder-decoder network that consists of three encoder blocks, three decoderblocks, and some additional upsampling blocks at the end of the network. Withthe image size of $256\times256$, the proposed method achieves an excellentreal-time operation speed of \textbf{47.07} frames per second with an averagemean dice coefficient score of 0.7786 and mean Intersection over Union of0.7210 on the out-of-distribution polyp datasets. The results on the publiclyavailable PolypGen dataset (OOD dataset in our case) suggest that TransRUPNetcan give real-time feedback while retaining high accuracy for in-distributiondataset. Furthermore, we demonstrate the generalizability of the proposedmethod by showing that it significantly improves performance on OOD datasetscompared to the existing methods.</description><author>Debesh Jha, Nikhil Kumar Tomar, Debayan Bhattacharya, Ulas Bagci</author><pubDate>Wed, 05 Jul 2023 16:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02176v2</guid></item><item><title>Broadcasted Residual Learning for Efficient Keyword Spotting</title><link>http://arxiv.org/abs/2106.04140v4</link><description>Keyword spotting is an important research field because it plays a key rolein device wake-up and user interaction on smart devices. However, it ischallenging to minimize errors while operating efficiently in devices withlimited resources such as mobile phones. We present a broadcasted residuallearning method to achieve high accuracy with small model size andcomputational load. Our method configures most of the residual functions as 1Dtemporal convolution while still allows 2D convolution together using abroadcasted-residual connection that expands temporal output tofrequency-temporal dimension. This residual mapping enables the network toeffectively represent useful audio features with much less computation thanconventional convolutional neural networks. We also propose a novel networkarchitecture, Broadcasting-residual network (BC-ResNet), based on broadcastedresidual learning and describe how to scale up the model according to thetarget device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%top-1 accuracy on Google speech command datasets v1 and v2, respectively, andconsistently outperform previous approaches, using fewer computations andparameters. Code is available athttps://github.com/Qualcomm-AI-research/bcresnet.</description><author>Byeonggeun Kim, Simyung Chang, Jinkyu Lee, Dooyong Sung</author><pubDate>Wed, 05 Jul 2023 16:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.04140v4</guid></item><item><title>Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning</title><link>http://arxiv.org/abs/2307.01200v1</link><description>Learning-based approaches to monocular motion capture have recently shownpromising results by learning to regress in a data-driven manner. However, dueto the challenges in data collection and network designs, it remainschallenging for existing solutions to achieve real-time full-body capture whilebeing accurate in world space. In this work, we contribute a sequentialproxy-to-motion learning scheme together with a proxy dataset of 2D skeletonsequences and 3D rotational motions in world space. Such proxy data enables usto build a learning-based network with accurate full-body supervision whilealso mitigating the generalization issues. For more accurate and physicallyplausible predictions, a contact-aware neural motion descent module is proposedin our network so that it can be aware of foot-ground contact and motionmisalignment with the proxy observations. Additionally, we share the body-handcontext information in our network for more compatible wrist poses recoverywith the full-body model. With the proposed learning-based solution, wedemonstrate the first real-time monocular full-body capture system withplausible foot-ground contact in world space. More video results can be foundat our project page: https://liuyebin.com/proxycap.</description><author>Yuxiang Zhang, Hongwen Zhang, Liangxiao Hu, Hongwei Yi, Shengping Zhang, Yebin Liu</author><pubDate>Mon, 03 Jul 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01200v1</guid></item><item><title>NeuBTF: Neural fields for BTF encoding and transfer</title><link>http://arxiv.org/abs/2307.01199v1</link><description>Neural material representations are becoming a popular way to representmaterials for rendering. They are more expressive than analytic models andoccupy less memory than tabulated BTFs. However, existing neural materials areimmutable, meaning that their output for a certain query of UVs, camera, andlight vector is fixed once they are trained. While this is practical when thereis no need to edit the material, it can become very limiting when the fragmentof the material used for training is too small or not tileable, whichfrequently happens when the material has been captured with agonioreflectometer. In this paper, we propose a novel neural materialrepresentation which jointly tackles the problems of BTF compression, tiling,and extrapolation. At test time, our method uses a guidance image as input tocondition the neural BTF to the structural features of this input image. Then,the neural BTF can be queried as a regular BTF using UVs, camera, and lightvectors. Every component in our framework is purposefully designed to maximizeBTF encoding quality at minimal parameter count and computational complexity,achieving competitive compression rates compared with previous work. Wedemonstrate the results of our method on a variety of synthetic and capturedmaterials, showing its generality and capacity to learn to represent manyoptical properties.</description><author>Carlos Rodriguez-Pardo, Konstantinos Kazatzis, Jorge Lopez-Moreno, Elena Garces</author><pubDate>Mon, 03 Jul 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01199v1</guid></item><item><title>Improved sampling via learned diffusions</title><link>http://arxiv.org/abs/2307.01198v1</link><description>Recently, a series of papers proposed deep learning-based approaches tosample from unnormalized target densities using controlled diffusion processes.In this work, we identify these approaches as special cases of theSchr\"odinger bridge problem, seeking the most likely stochastic evolutionbetween a given prior distribution and the specified target. We furthergeneralize this framework by introducing a variational formulation based ondivergences between path space measures of time-reversed diffusion processes.This abstract perspective leads to practical losses that can be optimized bygradient-based algorithms and includes previous objectives as special cases. Atthe same time, it allows us to consider divergences other than the reverseKullback-Leibler divergence that is known to suffer from mode collapse. Inparticular, we propose the so-called log-variance loss, which exhibitsfavorable numerical properties and leads to significantly improved performanceacross all considered approaches.</description><author>Lorenz Richter, Julius Berner, Guan-Horng Liu</author><pubDate>Mon, 03 Jul 2023 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01198v1</guid></item><item><title>Segment Anything Meets Point Tracking</title><link>http://arxiv.org/abs/2307.01197v1</link><description>The Segment Anything Model (SAM) has established itself as a powerfulzero-shot image segmentation model, employing interactive prompts such aspoints to generate masks. This paper presents SAM-PT, a method extending SAM'scapability to tracking and segmenting anything in dynamic videos. SAM-PTleverages robust and sparse point selection and propagation techniques for maskgeneration, demonstrating that a SAM-based segmentation tracker can yieldstrong zero-shot performance across popular video object segmentationbenchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditionalobject-centric mask propagation strategies, we uniquely use point propagationto exploit local structure information that is agnostic to object semantics. Wehighlight the merits of point-based tracking through direct evaluation on thezero-shot open-world Unidentified Video Objects (UVO) benchmark. To furtherenhance our approach, we utilize K-Medoids clustering for point initializationand track both positive and negative points to clearly distinguish the targetobject. We also employ multiple mask decoding passes for mask refinement anddevise a point re-initialization strategy to improve tracking accuracy. Ourcode integrates different point trackers and video segmentation benchmarks andwill be released at https://github.com/SysCV/sam-pt.</description><author>Frano Rajiƒç, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu</author><pubDate>Mon, 03 Jul 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01197v1</guid></item><item><title>Online Heavy-tailed Change-point detection</title><link>http://arxiv.org/abs/2306.09548v2</link><description>We study algorithms for online change-point detection (OCPD), where samplesthat are potentially heavy-tailed, are presented one at a time and a change inthe underlying mean must be detected as early as possible. We present analgorithm based on clipped Stochastic Gradient Descent (SGD), that works evenif we only assume that the second moment of the data generating process isbounded. We derive guarantees on worst-case, finite-sample false-positive rate(FPR) over the family of all distributions with bounded second moment. Thus,our method is the first OCPD algorithm that guarantees finite-sample FPR, evenif the data is high dimensional and the underlying distributions areheavy-tailed. The technical contribution of our paper is to show thatclipped-SGD can estimate the mean of a random vector and simultaneously provideconfidence bounds at all confidence values. We combine this robust estimatewith a union bound argument and construct a sequential change-point algorithmwith finite-sample FPR guarantees. We show empirically that our algorithm workswell in a variety of situations, whether the underlying data are heavy-tailed,light-tailed, high dimensional or discrete. No other algorithm achieves boundedFPR theoretically or empirically, over all settings we study simultaneously.</description><author>Abishek Sankararaman, Balakrishnan, Narayanaswamy</author><pubDate>Mon, 03 Jul 2023 18:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09548v2</guid></item><item><title>Squeezing Large-Scale Diffusion Models for Mobile</title><link>http://arxiv.org/abs/2307.01193v1</link><description>The emergence of diffusion models has greatly broadened the scope ofhigh-fidelity image synthesis, resulting in notable advancements in bothpractical implementation and academic research. With the active adoption of themodel in various real-world applications, the need for on-device deployment hasgrown considerably. However, deploying large diffusion models such as StableDiffusion with more than one billion parameters to mobile devices posesdistinctive challenges due to the limited computational and memory resources,which may vary according to the device. In this paper, we present thechallenges and solutions for deploying Stable Diffusion on mobile devices withTensorFlow Lite framework, which supports both iOS and Android devices. Theresulting Mobile Stable Diffusion achieves the inference latency of smallerthan 7 seconds for a 512x512 image generation on Android devices with mobileGPUs.</description><author>Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, Hyungjun Kim</author><pubDate>Mon, 03 Jul 2023 18:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01193v1</guid></item><item><title>Trainable Transformer in Transformer</title><link>http://arxiv.org/abs/2307.01189v1</link><description>Recent works attribute the capability of in-context learning (ICL) in largepre-trained language models to implicitly simulating and fine-tuning aninternal model (e.g., linear or 2-layer MLP) during inference. However, suchconstructions require large memory overhead, which makes simulation of moresophisticated internal models intractable. In this work, we propose anefficient construction, Transformer in Transformer (in short, TinT), thatallows a transformer to simulate and fine-tune complex models internally duringinference (e.g., pre-trained language models). In particular, we introduceinnovative approximation techniques that allow a TinT model with less than 2billion parameters to simulate and fine-tune a 125 million parametertransformer model within a single forward pass. TinT accommodates many commontransformer variants and its design ideas also improve the efficiency of pastinstantiations of simple models inside transformers. We conduct end-to-endexperiments to validate the internal fine-tuning procedure of TinT on variouslanguage modeling and downstream tasks. For example, even with a limitedone-step budget, we observe TinT for a OPT-125M model improves performance by4-16% absolute on average compared to OPT-125M. These findings suggest thatlarge pre-trained language models are capable of performing intricatesubroutines. To facilitate further work, a modular and extensible codebase forTinT is included.</description><author>Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora</author><pubDate>Mon, 03 Jul 2023 18:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01189v1</guid></item><item><title>SAMAug: Point Prompt Augmentation for Segment Anything Model</title><link>http://arxiv.org/abs/2307.01187v1</link><description>This paper introduces SAMAug, a novel visual point augmentation method forthe Segment Anything Model (SAM) that enhances interactive image segmentationperformance. SAMAug generates augmented point prompts to provide moreinformation to SAM. From the initial point prompt, SAM produces the initialmask, which is then fed into our proposed SAMAug to generate augmented pointprompts. By incorporating these extra points, SAM can generate augmentedsegmentation masks based on the augmented point prompts and the initial prompt,resulting in improved segmentation performance. We evaluate four pointaugmentation techniques: random selection, maximum difference entropy, maximumdistance, and a saliency model. Experiments on the COCO, Fundus, and ChestX-ray datasets demonstrate that SAMAug can boost SAM's segmentation results,especially using the maximum distance and saliency model methods. SAMAugunderscores the potential of visual prompt engineering to advance interactivecomputer vision models.</description><author>Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei Liu, Quanzheng Li, Tianming Liu, Xiang Li</author><pubDate>Mon, 03 Jul 2023 18:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01187v1</guid></item><item><title>Fitting an ellipsoid to a quadratic number of random points</title><link>http://arxiv.org/abs/2307.01181v1</link><description>We consider the problem $(\mathrm{P})$ of fitting $n$ standard Gaussianrandom vectors in $\mathbb{R}^d$ to the boundary of a centered ellipsoid, as$n, d \to \infty$. This problem is conjectured to have a sharp feasibilitytransition: for any $\varepsilon &gt; 0$, if $n \leq (1 - \varepsilon) d^2 / 4$then $(\mathrm{P})$ has a solution with high probability, while $(\mathrm{P})$has no solutions with high probability if $n \geq (1 + \varepsilon) d^2 /4$. Sofar, only a trivial bound $n \geq d^2 / 2$ is known on the negative side, whilethe best results on the positive side assume $n \leq d^2 /\mathrm{polylog}(d)$. In this work, we improve over previous approaches using akey result of Bartl &amp; Mendelson on the concentration of Gram matrices of randomvectors under mild assumptions on their tail behavior. This allows us to give asimple proof that $(\mathrm{P})$ is feasible with high probability when $n \leqd^2 / C$, for a (possibly large) constant $C &gt; 0$.</description><author>Afonso S. Bandeira, Antoine Maillard, Shahar Mendelson, Elliot Paquette</author><pubDate>Mon, 03 Jul 2023 18:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01181v1</guid></item><item><title>PlanE: Representation Learning over Planar Graphs</title><link>http://arxiv.org/abs/2307.01180v1</link><description>Graph neural networks are prominent models for representation learning overgraphs, where the idea is to iteratively compute representations of nodes of aninput graph through a series of transformations in such a way that the learnedgraph function is isomorphism invariant on graphs, which makes the learnedrepresentations graph invariants. On the other hand, it is well-known thatgraph invariants learned by these class of models are incomplete: there arepairs of non-isomorphic graphs which cannot be distinguished by standard graphneural networks. This is unsurprising given the computational difficulty ofgraph isomorphism testing on general graphs, but the situation begs to differfor special graph classes, for which efficient graph isomorphism testingalgorithms are known, such as planar graphs. The goal of this work is to designarchitectures for efficiently learning complete invariants of planar graphs.Inspired by the classical planar graph isomorphism algorithm of Hopcroft andTarjan, we propose PlanE as a framework for planar representation learning.PlanE includes architectures which can learn complete invariants over planargraphs while remaining practically scalable. We empirically validate the strongperformance of the resulting model architectures on well-known planar graphbenchmarks, achieving multiple state-of-the-art results.</description><author>Radoslav Dimitrov, Zeyang Zhao, Ralph Abboud, ƒ∞smail ƒ∞lkan Ceylan</author><pubDate>Mon, 03 Jul 2023 18:45:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01180v1</guid></item><item><title>Learning Mixtures of Gaussians Using the DDPM Objective</title><link>http://arxiv.org/abs/2307.01178v1</link><description>Recent works have shown that diffusion models can learn essentially anydistribution provided one can perform score estimation. Yet it remains poorlyunderstood under what settings score estimation is possible, let alone whenpractical gradient-based algorithms for this task can provably succeed. In this work, we give the first provably efficient results along these linesfor one of the most fundamental distribution families, Gaussian mixture models.We prove that gradient descent on the denoising diffusion probabilistic model(DDPM) objective can efficiently recover the ground truth parameters of themixture model in the following two settings: 1) We show gradient descent withrandom initialization learns mixtures of two spherical Gaussians in $d$dimensions with $1/\text{poly}(d)$-separated centers. 2) We show gradientdescent with a warm start learns mixtures of $K$ spherical Gaussians with$\Omega(\sqrt{\log(\min(K,d))})$-separated centers. A key ingredient in ourproofs is a new connection between score-based methods and two other approachesto distribution learning, the EM algorithm and spectral methods.</description><author>Kulin Shah, Sitan Chen, Adam Klivans</author><pubDate>Mon, 03 Jul 2023 18:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01178v1</guid></item><item><title>ELQA: A Corpus of Metalinguistic Questions and Answers about English</title><link>http://arxiv.org/abs/2205.00395v2</link><description>We present ELQA, a corpus of questions and answers in and about the Englishlanguage. Collected from two online forums, the &gt;70k questions (from Englishlearners and others) cover wide-ranging topics including grammar, meaning,fluency, and etymology. The answers include descriptions of general propertiesof English vocabulary and grammar as well as explanations about specific(correct and incorrect) usage examples. Unlike most NLP datasets, this corpusis metalinguistic -- it consists of language about language. As such, it canfacilitate investigations of the metalinguistic capabilities of NLU models, aswell as educational applications in the language learning domain. To studythis, we define a free-form question answering task on our dataset and conductevaluations on multiple LLMs (Large Language Models) to analyze their capacityto generate metalinguistic answers.</description><author>Shabnam Behzad, Keisuke Sakaguchi, Nathan Schneider, Amir Zeldes</author><pubDate>Mon, 03 Jul 2023 18:42:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.00395v2</guid></item><item><title>Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space</title><link>http://arxiv.org/abs/2307.01177v1</link><description>The characterization of the functions spaces explored by neural networks(NNs) is an important aspect of deep learning theory. In this work, we view amulti-layer NN with arbitrary width as defining a particular hierarchy ofreproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL).This allows us to define a function space and a complexity measure thatgeneralize prior results for shallow NNs, and we then examine their theoreticalproperties and implications in several aspects. First, we prove acorrespondence between functions expressed by L-layer NNs and those belongingto L-level NHLs. Second, we prove generalization guarantees for learning an NHLwith the complexity measure controlled. Third, corresponding to the training ofmulti-layer NNs in the infinite-width mean-field limit, we derive an evolutionof the NHL characterized as the dynamics of multiple random fields. Fourth, weshow examples of depth separation in NHLs under ReLU and quadratic activationfunctions. Finally, we complement the theory with numerical results toillustrate the learning of RKHS in NN training.</description><author>Zhengdao Chen</author><pubDate>Mon, 03 Jul 2023 18:40:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01177v1</guid></item><item><title>Quantum Neural Estimation of Entropies</title><link>http://arxiv.org/abs/2307.01171v1</link><description>Entropy measures quantify the amount of information and correlations presentin a quantum system. In practice, when the quantum state is unknown and onlycopies thereof are available, one must resort to the estimation of such entropymeasures. Here we propose a variational quantum algorithm for estimating thevon Neumann and R\'enyi entropies, as well as the measured relative entropy andmeasured R\'enyi relative entropy. Our approach first parameterizes avariational formula for the measure of interest by a quantum circuit and aclassical neural network, and then optimizes the resulting objective overparameter space. Numerical simulations of our quantum algorithm are provided,using a noiseless quantum simulator. The algorithm provides accurate estimatesof the various entropy measures for the examples tested, which renders it as apromising approach for usage in downstream tasks.</description><author>Ziv Goldfeld, Dhrumil Patel, Sreejith Sreekumar, Mark M. Wilde</author><pubDate>Mon, 03 Jul 2023 18:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01171v1</guid></item><item><title>Online nearest neighbor classification</title><link>http://arxiv.org/abs/2307.01170v1</link><description>We study an instance of online non-parametric classification in therealizable setting. In particular, we consider the classical 1-nearest neighboralgorithm, and show that it achieves sublinear regret - that is, a vanishingmistake rate - against dominated or smoothed adversaries in the realizablesetting.</description><author>Sanjoy Dasgupta, Geelon So</author><pubDate>Mon, 03 Jul 2023 18:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01170v1</guid></item><item><title>Analyzing and Improving Greedy 2-Coordinate Updates for Equality-Constrained Optimization via Steepest Descent in the 1-Norm</title><link>http://arxiv.org/abs/2307.01169v1</link><description>We consider minimizing a smooth function subject to a summation constraintover its variables. By exploiting a connection between the greedy 2-coordinateupdate for this problem and equality-constrained steepest descent in the1-norm, we give a convergence rate for greedy selection under a proximalPolyak-Lojasiewicz assumption that is faster than random selection andindependent of the problem dimension $n$. We then consider minimizing with botha summation constraint and bound constraints, as arises in the support vectormachine dual problem. Existing greedy rules for this setting either guaranteetrivial progress only or require $O(n^2)$ time to compute. We show that bound-and summation-constrained steepest descent in the L1-norm guarantees moreprogress per iteration than previous rules and can be computed in only $O(n\log n)$ time.</description><author>Amrutha Varshini Ramesh, Aaron Mishkin, Mark Schmidt, Yihan Zhou, Jonathan Wilder Lavington, Jennifer She</author><pubDate>Mon, 03 Jul 2023 18:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01169v1</guid></item><item><title>Don't freeze: Finetune encoders for better Self-Supervised HAR</title><link>http://arxiv.org/abs/2307.01168v1</link><description>Recently self-supervised learning has been proposed in the field of humanactivity recognition as a solution to the labelled data availability problem.The idea being that by using pretext tasks such as reconstruction orcontrastive predictive coding, useful representations can be learned that thencan be used for classification. Those approaches follow the pretrain, freezeand fine-tune procedure. In this paper we will show how a simple change - notfreezing the representation - leads to substantial performance gains acrosspretext tasks. The improvement was found in all four investigated datasets andacross all four pretext tasks and is inversely proportional to amount oflabelled data. Moreover the effect is present whether the pretext task iscarried on the Capture24 dataset or directly in unlabelled data of the targetdataset.</description><author>Vitor Fortes Rey, Dominique Nshimyimana, Paul Lukowicz</author><pubDate>Mon, 03 Jul 2023 18:23:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01168v1</guid></item><item><title>Coupled Gradient Flows for Strategic Non-Local Distribution Shift</title><link>http://arxiv.org/abs/2307.01166v1</link><description>We propose a novel framework for analyzing the dynamics of distribution shiftin real-world systems that captures the feedback loop between learningalgorithms and the distributions on which they are deployed. Prior work largelymodels feedback-induced distribution shift as adversarial or via an overlysimplistic distribution-shift structure. In contrast, we propose a coupledpartial differential equation model that captures fine-grained changes in thedistribution over time by accounting for complex dynamics that arise due tostrategic responses to algorithmic decision-making, non-local endogenouspopulation interactions, and other exogenous sources of distribution shift. Weconsider two common settings in machine learning: cooperative settings withinformation asymmetries, and competitive settings where a learner facesstrategic users. For both of these settings, when the algorithm retrains viagradient descent, we prove asymptotic convergence of the retraining procedureto a steady-state, both in finite and in infinite dimensions, obtainingexplicit rates in terms of the model parameters. To do so we derive new resultson the convergence of coupled PDEs that extends what is known on multi-speciessystems. Empirically, we show that our approach captures well-documented formsof distribution shifts like polarization and disparate impacts that simplermodels cannot capture.</description><author>Lauren Conger, Franca Hoffmann, Eric Mazumdar, Lillian Ratliff</author><pubDate>Mon, 03 Jul 2023 18:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01166v1</guid></item><item><title>Improving Language Plasticity via Pretraining with Active Forgetting</title><link>http://arxiv.org/abs/2307.01163v1</link><description>Pretrained language models (PLMs) are today the primary model for naturallanguage processing. Despite their impressive downstream performance, it can bedifficult to apply PLMs to new languages, a barrier to making theircapabilities universally accessible. While prior work has shown it possible toaddress this issue by learning a new embedding layer for the new language,doing so is both data and compute inefficient. We propose to use an activeforgetting mechanism during pretraining, as a simple way of creating PLMs thatcan quickly adapt to new languages. Concretely, by resetting the embeddinglayer every K updates during pretraining, we encourage the PLM to improve itsability of learning new embeddings within a limited number of updates, similarto a meta-learning effect. Experiments with RoBERTa show that models pretrainedwith our forgetting mechanism not only demonstrate faster convergence duringlanguage adaptation but also outperform standard ones in a low-data regime,particularly for languages that are distant from English.</description><author>Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetor, Sebastian Riedel, Mikel Artetx</author><pubDate>Mon, 03 Jul 2023 18:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01163v1</guid></item><item><title>Soft Gripping: Specifying for Trustworthiness</title><link>http://arxiv.org/abs/2307.01159v1</link><description>Soft robotics is an emerging technology in which engineers create flexibledevices for use in a variety of applications. In order to advance the wideadoption of soft robots, ensuring their trustworthiness is essential; if softrobots are not trusted, they will not be used to their full potential. In orderto demonstrate trustworthiness, a specification needs to be formulated todefine what is trustworthy. However, even for soft robotic grippers, which isone of the most mature areas in soft robotics, the soft robotics community hasso far given very little attention to formulating specifications. In this work,we discuss the importance of developing specifications during development ofsoft robotic systems, and present an extensive example specification for a softgripper for pick-and-place tasks for grocery items. The proposed specificationcovers both functional and non-functional requirements, such as reliability,safety, adaptability, predictability, ethics, and regulations. We alsohighlight the need to promote verifiability as a first-class objective in thedesign of a soft gripper.</description><author>Dhaminda B. Abeywickrama, Nguyen Hao Le, Greg Chance, Peter D. Winter, Arianna Manzini, Alix J. Partridge, Jonathan Ives, John Downer, Graham Deacon, Jonathan Rossiter, Kerstin Eder, Shane Windsor</author><pubDate>Mon, 03 Jul 2023 18:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01159v1</guid></item><item><title>Neural Algorithmic Reasoning with Causal Regularisation</title><link>http://arxiv.org/abs/2302.10258v2</link><description>Recent work on neural algorithmic reasoning has investigated the reasoningcapabilities of neural networks, effectively demonstrating they can learn toexecute classical algorithms on unseen data coming from the train distribution.However, the performance of existing neural reasoners significantly degrades onout-of-distribution (OOD) test data, where inputs have larger sizes. In thiswork, we make an important observation: there are many different inputs forwhich an algorithm will perform certain intermediate computations identically.This insight allows us to develop data augmentation procedures that, given analgorithm's intermediate trajectory, produce inputs for which the targetalgorithm would have exactly the same next trajectory step. We ensureinvariance in the next-step prediction across such inputs, by employing aself-supervised objective derived by our observation, formalised in a causalgraph. We prove that the resulting method, which we call Hint-ReLIC, improvesthe OOD generalisation capabilities of the reasoner. We evaluate our method onthe CLRS algorithmic reasoning benchmark, where we show up to 3$\times$improvements on the OOD test data.</description><author>Beatrice Bevilacqua, Kyriacos Nikiforou, Borja Ibarz, Ioana Bica, Michela Paganini, Charles Blundell, Jovana Mitrovic, Petar Veliƒçkoviƒá</author><pubDate>Mon, 03 Jul 2023 18:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10258v2</guid></item><item><title>Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2307.01158v1</link><description>The ability to model the mental states of others is crucial to human socialintelligence, and can offer similar benefits to artificial agents with respectto the social dynamics induced in multi-agent settings. We present a method ofgrounding semantically meaningful, human-interpretable beliefs within policiesmodeled by deep networks. We then consider the task of 2nd-order beliefprediction. We propose that ability of each agent to predict the beliefs of theother agents can be used as an intrinsic reward signal for multi-agentreinforcement learning. Finally, we present preliminary empirical results in amixed cooperative-competitive environment.</description><author>Ini Oguntola, Joseph Campbell, Simon Stepputtis, Katia Sycara</author><pubDate>Mon, 03 Jul 2023 18:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01158v1</guid></item><item><title>A novel approach for predicting epidemiological forecasting parameters based on real-time signals and Data Assimilation</title><link>http://arxiv.org/abs/2307.01157v1</link><description>This paper proposes a novel approach to predict epidemiological parameters byintegrating new real-time signals from various sources of information, such asnovel social media-based population density maps and Air Quality data. Weimplement an ensemble of Convolutional Neural Networks (CNN) models usingvarious data sources and fusion methodology to build robust predictions andsimulate several dynamic parameters that could improve the decision-makingprocess for policymakers. Additionally, we used data assimilation to estimatethe state of our system from fused CNN predictions. The combination ofmeteorological signals and social media-based population density maps improvedthe performance and flexibility of our prediction of the COVID-19 outbreak inLondon. While the proposed approach outperforms standard models, such ascompartmental models traditionally used in disease forecasting (SEIR),generating robust and consistent predictions allows us to increase thestability of our model while increasing its accuracy.</description><author>Romain Molinas, C√©sar Quilodr√°n Casas, Rossella Arcucci, Ovidiu ≈ûerban</author><pubDate>Mon, 03 Jul 2023 18:05:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01157v1</guid></item><item><title>Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis</title><link>http://arxiv.org/abs/2307.01148v1</link><description>Generative latent diffusion models have been established as state-of-the-artin data generation. One promising application is generation of realisticsynthetic medical imaging data for open data sharing without compromisingpatient privacy. Despite the promise, the capacity of such models to memorizesensitive patient training data and synthesize samples showing high resemblanceto training data samples is relatively unexplored. Here, we assess thememorization capacity of 3D latent diffusion models on photon-counting coronarycomputed tomography angiography and knee magnetic resonance imaging datasets.To detect potential memorization of training samples, we utilizeself-supervised models based on contrastive learning. Our results suggest thatsuch latent diffusion models indeed memorize training data, and there is a direneed for devising strategies to mitigate memorization.</description><author>Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliou, Stefan O. Schoenberg, Sandy Engelhardt</author><pubDate>Mon, 03 Jul 2023 17:39:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01148v1</guid></item><item><title>AVSegFormer: Audio-Visual Segmentation with Transformer</title><link>http://arxiv.org/abs/2307.01146v1</link><description>The combination of audio and vision has long been a topic of interest in themulti-modal community. Recently, a new audio-visual segmentation (AVS) task hasbeen introduced, aiming to locate and segment the sounding objects in a givenvideo. This task demands audio-driven pixel-level scene understanding for thefirst time, posing significant challenges. In this paper, we proposeAVSegFormer, a novel framework for AVS tasks that leverages the transformerarchitecture. Specifically, we introduce audio queries and learnable queriesinto the transformer decoder, enabling the network to selectively attend tointerested visual features. Besides, we present an audio-visual mixer, whichcan dynamically adjust visual features by amplifying relevant and suppressingirrelevant spatial channels. Additionally, we devise an intermediate mask lossto enhance the supervision of the decoder, encouraging the network to producemore accurate intermediate predictions. Extensive experiments demonstrate thatAVSegFormer achieves state-of-the-art results on the AVS benchmark. The code isavailable at https://github.com/vvvb-github/AVSegFormer.</description><author>Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu</author><pubDate>Mon, 03 Jul 2023 17:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01146v1</guid></item><item><title>Lower Complexity Adaptation for Empirical Entropic Optimal Transport</title><link>http://arxiv.org/abs/2306.13580v2</link><description>Entropic optimal transport (EOT) presents an effective and computationallyviable alternative to unregularized optimal transport (OT), offering diverseapplications for large-scale data analysis. In this work, we derive novelstatistical bounds for empirical plug-in estimators of the EOT cost and showthat their statistical performance in the entropy regularization parameter$\epsilon$ and the sample size $n$ only depends on the simpler of the twoprobability measures. For instance, under sufficiently smooth costs this yieldsthe parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is theminimum dimension of the two population measures. This confirms that empiricalEOT also adheres to the lower complexity adaptation principle, a hallmarkfeature only recently identified for unregularized OT. As a consequence of ourtheory, we show that the empirical entropic Gromov-Wasserstein distance and itsunregularized version for measures on Euclidean spaces also obey thisprinciple. Additionally, we comment on computational aspects and complement ourfindings with Monte Carlo simulations. Our techniques employ empirical processtheory and rely on a dual formulation of EOT over a single function class.Crucial to our analysis is the observation that the entropiccost-transformation of a function class does not increase its uniform metricentropy by much.</description><author>Michel Groppe, Shayan Hundrieser</author><pubDate>Mon, 03 Jul 2023 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13580v2</guid></item><item><title>Statler: State-Maintaining Language Models for Embodied Reasoning</title><link>http://arxiv.org/abs/2306.17840v2</link><description>Large language models (LLMs) provide a promising tool that enable robots toperform complex robot reasoning tasks. However, the limited context window ofcontemporary LLMs makes reasoning over long time horizons difficult. Embodiedtasks such as those that one might expect a household robot to performtypically require that the planner consider information acquired a long timeago (e.g., properties of the many objects that the robot previously encounteredin the environment). Attempts to capture the world state using an LLM'simplicit internal representation is complicated by the paucity of task- andenvironment-relevant information available in a robot's action history, whilemethods that rely on the ability to convey information via the prompt to theLLM are subject to its limited context window. In this paper, we proposeStatler, a framework that endows LLMs with an explicit representation of theworld state as a form of ``memory'' that is maintained over time. Integral toStatler is its use of two instances of general LLMs -- a world-model reader anda world-model writer -- that interface with and maintain the world state. Byproviding access to this world state ``memory'', Statler improves the abilityof existing LLMs to reason over longer time horizons without the constraint ofcontext length. We evaluate the effectiveness of our approach on threesimulated table-top manipulation domains and a real robot domain, and show thatit improves the state-of-the-art in LLM-based robot reasoning. Project website:https://statler-lm.github.io/</description><author>Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, Matthew R. Walter</author><pubDate>Mon, 03 Jul 2023 17:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17840v2</guid></item><item><title>SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions</title><link>http://arxiv.org/abs/2307.01139v1</link><description>Instruction finetuning is a popular paradigm to align large language models(LLM) with human intent. Despite its popularity, this idea is less explored inimproving the LLMs to align existing foundation models with scientificdisciplines, concepts and goals. In this work, we present SciTune as a tuningframework to improve the ability of LLMs to follow scientific multimodalinstructions. To test our methodology, we use a human-generated scientificinstruction tuning dataset and train a large multimodal model LLaMA-SciTunethat connects a vision encoder and LLM for science-focused visual and languageunderstanding. In comparison to the models that are finetuned with machinegenerated data only, LLaMA-SciTune surpasses human performance on average andin many sub-categories on the ScienceQA benchmark.</description><author>Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge</author><pubDate>Mon, 03 Jul 2023 17:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01139v1</guid></item><item><title>SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design</title><link>http://arxiv.org/abs/2306.15656v2</link><description>This paper introduces SparseOptimizer, a novel deep learning optimizer thatexploits Moreau-Yosida regularization to naturally induce sparsity in largelanguage models such as BERT, ALBERT and GPT. Key to the design ofSparseOptimizer is an embedded shrinkage operator, which imparts sparsitydirectly within the optimization process. This operator, backed by a soundtheoretical framework, includes an analytical solution, thereby reinforcing theoptimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-playfunctionality eradicates the need for code modifications, making it auniversally adaptable tool for a wide array of large language models. Empiricalevaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2confirm that SparseBERT and SparseALBERT, when sparsified usingSparseOptimizer, achieve performance comparable to their dense counterparts,BERT and ALBERT, while significantly reducing their parameter count. Further,this work proposes an innovative optimizer-compiler co-design strategy,demonstrating the potential of inference acceleration (\textbf{3.37x},\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, andLLVM generic compile, respectively) in SparseBERT when paired with anappropriately designed compiler. This study represents a significant stepforward in the evolution of efficient, scalable, and high-performing largelanguage models, setting a precedent for future exploration and optimization inthis domain. The SparseOptimizer code and SparseALBERT model will be publiclyavailable upon paper acceptance.</description><author>Fu-Ming Guo</author><pubDate>Mon, 03 Jul 2023 17:25:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15656v2</guid></item><item><title>Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking</title><link>http://arxiv.org/abs/2307.01137v1</link><description>The biomedical field relies heavily on concept linking in various areas suchas literature mining, graph alignment, information retrieval,question-answering, data, and knowledge integration. Although large languagemodels (LLMs) have made significant strides in many natural language processingtasks, their effectiveness in biomedical concept mapping is yet to be fullyexplored. This research investigates a method that exploits the in-contextlearning (ICL) capabilities of large models for biomedical concept linking. Theproposed approach adopts a two-stage retrieve-and-rank framework. Initially,biomedical concepts are embedded using language models, and then embeddingsimilarity is utilized to retrieve the top candidates. These candidates'contextual information is subsequently incorporated into the prompt andprocessed by a large language model to re-rank the concepts. This approachachieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%in chemical entity normalization, exhibiting a competitive performance relativeto supervised learning methods. Further, it showed a significant improvement,with an over 20-point absolute increase in F1 score on an oncology matchingdataset. Extensive qualitative assessments were conducted, and the benefits andpotential shortcomings of using large language models within the biomedicaldomain were discussed. were discussed.</description><author>Qinyong Wang, Zhenxiang Gao, Rong Xu</author><pubDate>Mon, 03 Jul 2023 17:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01137v1</guid></item><item><title>Networked Time Series Prediction with Incomplete Data</title><link>http://arxiv.org/abs/2110.02271v2</link><description>A networked time series (NETS) is a family of time series on a given graph,one for each node. It has found a wide range of applications from intelligenttransportation, environment monitoring to mobile network management. Animportant task in such applications is to predict the future values of a NETSbased on its historical values and the underlying graph. Most existing methodsrequire complete data for training. However, in real-world scenarios, it is notuncommon to have missing data due to sensor malfunction, incomplete sensingcoverage, etc. In this paper, we study the problem of NETS prediction withincomplete data. We propose NETS-ImpGAN, a novel deep learning framework thatcan be trained on incomplete data with missing values in both history andfuture. Furthermore, we propose novel Graph Temporal Attention Networks byincorporating the attention mechanism to capture both inter-time seriescorrelations and temporal correlations. We conduct extensive experiments onthree real-world datasets under different missing patterns and missing rates.The experimental results show that NETS-ImpGAN outperforms existing methodsexcept when data exhibit very low variance, in which case NETS-ImpGAN stillachieves competitive performance.</description><author>Yichen Zhu, Mengtian Zhang, Bo Jiang, Haiming Jin, Jianqiang Huang, Xinbing Wang</author><pubDate>Mon, 03 Jul 2023 17:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.02271v2</guid></item><item><title>ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience</title><link>http://arxiv.org/abs/2307.01135v1</link><description>The advent of ChatGPT, a large language model-powered chatbot, has promptedquestions about its potential implications for traditional search engines. Inthis study, we investigate the differences in user behavior when employingsearch engines and chatbot tools for information-seeking tasks. We carry out arandomized online experiment, dividing participants into two groups: one usinga ChatGPT-like tool and the other using a Google Search-like tool. Our findingsreveal that the ChatGPT group consistently spends less time on all tasks, withno significant difference in overall task performance between the groups.Notably, ChatGPT levels user search performance across different educationlevels and excels in answering straightforward questions and providing generalsolutions but falls short in fact-checking tasks. Users perceive ChatGPT'sresponses as having higher information quality compared to Google Search,despite displaying a similar level of trust in both tools. Furthermore,participants using ChatGPT report significantly better user experiences interms of usefulness, enjoyment, and satisfaction, while perceived ease of useremains comparable between the two tools. However, ChatGPT may also lead tooverreliance and generate or replicate misinformation, yielding inconsistentresults. Our study offers valuable insights for search engine management andhighlights opportunities for integrating chatbot technologies into searchengine designs.</description><author>Ruiyun Xu, Yue Feng, Hailiang Chen</author><pubDate>Mon, 03 Jul 2023 17:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01135v1</guid></item><item><title>Increasing Fairness via Combination with Learning Guarantees</title><link>http://arxiv.org/abs/2301.10813v2</link><description>The concern about underlying discrimination hidden in ML models isincreasing, as ML systems have been widely applied in more and more real-worldscenarios and any discrimination hidden in them will directly affect humanlife. Many techniques have been developed to enhance fairness includingcommonly-used group fairness measures and several fairness-aware methodscombining ensemble learning. However, existing fairness measures can only focuson one aspect -- either group or individual fairness, and the hardcompatibility among them indicates a possibility of remaining biases even ifone of them is satisfied. Moreover, existing mechanisms to boost fairnessusually present empirical results to show validity, yet few of them discusswhether fairness can be boosted with certain theoretical guarantees. To addressthese issues, we propose a fairness quality measure named discriminative riskin this paper to reflect both individual and group fairness aspects.Furthermore, we investigate the properties of the proposed measure and proposefirst- and second-order oracle bounds to show that fairness can be boosted viaensemble combination with theoretical learning guarantees. Note that theanalysis is suitable for both binary and multi-class classification. A pruningmethod is also proposed to utilise our proposed measure and comprehensiveexperiments are conducted to evaluate the effectiveness of the proposed methodsin this paper.</description><author>Yijun Bian, Kun Zhang, Anqi Qiu</author><pubDate>Mon, 03 Jul 2023 17:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10813v2</guid></item><item><title>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</title><link>http://arxiv.org/abs/2306.15195v2</link><description>In human conversations, individuals can indicate relevant regions within ascene while addressing others. In turn, the other person can then respond byreferring to specific regions if necessary. This natural referential ability indialogue remains absent in current Multimodal Large Language Models (MLLMs). Tofill this gap, this paper proposes an MLLM called Shikra, which can handlespatial coordinate inputs and outputs in natural language. Its architectureconsists of a vision encoder, an alignment layer, and a LLM. It is designed tobe straightforward and simple, without the need for extra vocabularies,position encoder, pre-/post-detection modules, or external plug-in models. Allinputs and outputs are in natural language form. Referential dialogue is asuperset of various vision-language (VL) tasks. Shikra can naturally handlelocation-related tasks like REC and PointQA, as well as conventional VL taskssuch as Image Captioning and VQA. Experimental results showcase Shikra'spromising performance. Furthermore, it enables numerous exciting applications,like providing mentioned objects' coordinates in chains of thoughts andcomparing user-pointed regions similarities. Our code, model and dataset areaccessed at https://github.com/shikras/shikra.</description><author>Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao</author><pubDate>Mon, 03 Jul 2023 17:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15195v2</guid></item><item><title>Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction</title><link>http://arxiv.org/abs/2307.01128v1</link><description>In the current digitalization era, capturing and effectively representingknowledge is crucial in most real-world scenarios. In this context, knowledgegraphs represent a potent tool for retrieving and organizing a vast amount ofinformation in a properly interconnected and interpretable structure. However,their generation is still challenging and often requires considerable humaneffort and domain expertise, hampering the scalability and flexibility acrossdifferent application fields. This paper proposes an innovative knowledge graphgeneration approach that leverages the potential of the latest generative largelanguage models, such as GPT-3.5, that can address all the main critical issuesin knowledge graph building. The approach is conveyed in a pipeline thatcomprises novel iterative zero-shot and external knowledge-agnostic strategiesin the main stages of the generation process. Our unique manifold approach mayencompass significant benefits to the scientific community. In particular, themain contribution can be summarized by: (i) an innovative strategy foriteratively prompting large language models to extract relevant components ofthe final graph; (ii) a zero-shot strategy for each prompt, meaning that thereis no need for providing examples for "guiding" the prompt result; (iii) ascalable solution, as the adoption of LLMs avoids the need for any externalresources or human expertise. To assess the effectiveness of our proposedmodel, we performed experiments on a dataset that covered a specific domain. Weclaim that our proposal is a suitable solution for scalable and versatileknowledge graph construction and may be applied to different and novelcontexts.</description><author>Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, Sandro Gabriele Tiddia</author><pubDate>Mon, 03 Jul 2023 17:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01128v1</guid></item><item><title>Cross-modality Attention Adapter: A Glioma Segmentation Fine-tuning Method for SAM Using Multimodal Brain MR Images</title><link>http://arxiv.org/abs/2307.01124v1</link><description>According to the 2021 World Health Organization (WHO) Classification schemefor gliomas, glioma segmentation is a very important basis for diagnosis andgenotype prediction. In general, 3D multimodal brain MRI is an effectivediagnostic tool. In the past decade, there has been an increase in the use ofmachine learning, particularly deep learning, for medical images processing.Thanks to the development of foundation models, models pre-trained withlarge-scale datasets have achieved better results on a variety of tasks.However, for medical images with small dataset sizes, deep learning methodsstruggle to achieve better results on real-world image datasets. In this paper,we propose a cross-modality attention adapter based on multimodal fusion tofine-tune the foundation model to accomplish the task of glioma segmentation inmultimodal MRI brain images with better results. The effectiveness of theproposed method is validated via our private glioma data set from the FirstAffiliated Hospital of Zhengzhou University (FHZU) in Zhengzhou, China. Ourproposed method is superior to current state-of-the-art methods with a Dice of88.38% and Hausdorff distance of 10.64, thereby exhibiting a 4% increase inDice to segment the glioma region for glioma treatment.</description><author>Xiaoyu Shi, Shurong Chai, Yinhao Li, Jingliang Cheng, Jie Bai, Guohua Zhao, Yen-Wei Chen</author><pubDate>Mon, 03 Jul 2023 16:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01124v1</guid></item><item><title>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</title><link>http://arxiv.org/abs/2307.01121v1</link><description>Geometric navigation is nowadays a well-established field of robotics and theresearch focus is shifting towards higher-level scene understanding, such asSemantic Mapping. When a robot needs to interact with its environment, it mustbe able to comprehend the contextual information of its surroundings. This workfocuses on classifying and localising objects within a map, which is underconstruction (SLAM) or already built. To further explore this direction, wepropose a framework that can autonomously detect and localize predefinedobjects in a known environment using a multi-modal sensor fusion approach(combining RGB and depth data from an RGB-D camera and a lidar). The frameworkconsists of three key elements: understanding the environment through RGB data,estimating depth through multi-modal sensor fusion, and managing artifacts(i.e., filtering and stabilizing measurements). The experiments show that theproposed framework can accurately detect 98% of the objects in the real sampleenvironment, without post-processing, while 85% and 80% of the objects weremapped using the single RGBD camera or RGB + lidar setup respectively. Thecomparison with single-sensor (camera or lidar) experiments is performed toshow that sensor fusion allows the robot to accurately detect near and farobstacles, which would have been noisy or imprecise in a purely visual orlaser-based approach.</description><author>Federico Rollo, Gennaro Raiola, Andrea Zunino, Nikolaos Tsagarakis, Arash Ajoudani</author><pubDate>Mon, 03 Jul 2023 16:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01121v1</guid></item><item><title>Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge</title><link>http://arxiv.org/abs/2210.05723v2</link><description>Various neural network architectures rely on pooling operators to aggregateinformation coming from different sources. It is often implicitly assumed insuch contexts that vectors encode epistemic states, i.e. that vectors capturethe evidence that has been obtained about some properties of interest, and thatpooling these vectors yields a vector that combines this evidence. We study,for a number of standard pooling operators, under what conditions they arecompatible with this idea, which we call the epistemic pooling principle. Whilewe find that all the considered pooling operators can satisfy the epistemicpooling principle, this only holds when embeddings are sufficientlyhigh-dimensional and, for most pooling operators, when the embeddings satisfyparticular constraints (e.g. having non-negative coordinates). We furthermoreshow that these constraints have important implications on how the embeddingscan be used in practice. In particular, we find that when the epistemic poolingprinciple is satisfied, in most cases it is impossible to verify thesatisfaction of propositional formulas using linear scoring functions, with twoexceptions: (i) max-pooling with embeddings that are upper-bounded and (ii)Hadamard pooling with non-negative embeddings. This finding helps to clarify,among others, why Graph Neural Networks sometimes under-perform in reasoningtasks. Finally, we also study an extension of the epistemic pooling principleto weighted epistemic states, which are important in the context ofnon-monotonic reasoning, where max-pooling emerges as the most suitableoperator.</description><author>Steven Schockaert</author><pubDate>Mon, 03 Jul 2023 16:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05723v2</guid></item><item><title>MeT: A Graph Transformer for Semantic Segmentation of 3D Meshes</title><link>http://arxiv.org/abs/2307.01115v1</link><description>Polygonal meshes have become the standard for discretely approximating 3Dshapes, thanks to their efficiency and high flexibility in capturingnon-uniform shapes. This non-uniformity, however, leads to irregularity in themesh structure, making tasks like segmentation of 3D meshes particularlychallenging. Semantic segmentation of 3D mesh has been typically addressedthrough CNN-based approaches, leading to good accuracy. Recently, transformershave gained enough momentum both in NLP and computer vision fields, achievingperformance at least on par with CNN models, supporting the long-soughtarchitecture universalism. Following this trend, we propose a transformer-basedmethod for semantic segmentation of 3D mesh motivated by a better modeling ofthe graph structure of meshes, by means of global attention mechanisms. Inorder to address the limitations of standard transformer architectures inmodeling relative positions of non-sequential data, as in the case of 3Dmeshes, as well as in capturing the local context, we perform positionalencoding by means the Laplacian eigenvectors of the adjacency matrix, replacingthe traditional sinusoidal positional encodings, and by introducingclustering-based features into the self-attention and cross-attentionoperators. Experimental results, carried out on three sets of the Shape COSEGDataset, on the human segmentation dataset proposed in Maron et al., 2017 andon the ShapeNet benchmark, show how the proposed approach yieldsstate-of-the-art performance on semantic segmentation of 3D meshes.</description><author>Giuseppe Vecchio, Luca Prezzavento, Carmelo Pino, Francesco Rundo, Simone Palazzo, Concetto Spampinato</author><pubDate>Mon, 03 Jul 2023 16:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01115v1</guid></item><item><title>EVD Surgical Guidance with Retro-Reflective Tool Tracking and Spatial Reconstruction using Head-Mounted Augmented Reality Device</title><link>http://arxiv.org/abs/2306.15490v2</link><description>Augmented Reality (AR) has been used to facilitate surgical guidance duringExternal Ventricular Drain (EVD) surgery, reducing the risks of misplacement inmanual operations. During this procedure, the key challenge is accuratelyestimating the spatial relationship between pre-operative images and actualpatient anatomy in AR environment. This research proposes a novel frameworkutilizing Time of Flight (ToF) depth sensors integrated in commerciallyavailable AR Head Mounted Devices (HMD) for precise EVD surgical guidance. Asprevious studies have proven depth errors for ToF sensors, we first assessedtheir properties on AR-HMDs. Subsequently, a depth error model andpatient-specific parameter identification method are introduced for accuratesurface information. A tracking pipeline combining retro-reflective markers andpoint clouds is then proposed for accurate head tracking. The head surface isreconstructed using depth data for spatial registration, avoiding fixingtracking targets rigidly on the patient's skull. Firstly, $7.580\pm 1.488 mm$depth value error was revealed on human skin, indicating the significance ofdepth correction. Our results showed that the error was reduced by over $85\%$using proposed depth correction method on head phantoms in different materials.Meanwhile, the head surface reconstructed with corrected depth data achievedsub-millimetre accuracy. An experiment on sheep head revealed $0.79 mm$reconstruction error. Furthermore, a user study was conducted for theperformance in simulated EVD surgery, where five surgeons performed nine k-wireinjections on a head phantom with virtual guidance. Results of this studyrevealed $2.09 \pm 0.16 mm$ translational accuracy and $2.97\pm 0.91$ degreeorientational accuracy.</description><author>Haowei Li, Wenqing Yan, Du Liu, Long Qian, Yuxing Yang, Yihao Liu, Zhe Zhao, Hui Ding, Guangzhi Wang</author><pubDate>Mon, 03 Jul 2023 16:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15490v2</guid></item><item><title>A Survey on Generative Diffusion Model</title><link>http://arxiv.org/abs/2209.02646v9</link><description>Deep generative models are a prominent approach for data generation, and havebeen used to produce high quality samples in various domains. Diffusion models,an emerging class of deep generative models, have attracted considerableattention owing to their exceptional generative quality. Despite this, theyhave certain limitations, including a time-consuming iterative generationprocess and confinement to high-dimensional Euclidean space. This surveypresents a plethora of advanced techniques aimed at enhancing diffusion models,including sampling acceleration and the design of new diffusion processes. Inaddition, we delve into strategies for implementing diffusion models inmanifold and discrete spaces, maximum likelihood training for diffusion models,and methods for creating bridges between two arbitrary distributions. Theinnovations we discuss represent the efforts for improving the functionalityand efficiency of diffusion models in recent years. To examine the efficacy ofexisting models, a benchmark of FID score, IS, and NLL is presented in aspecific NFE. Furthermore, diffusion models are found to be useful in variousdomains such as computer vision, audio, sequence modeling, and AI for science.The paper concludes with a summary of this field, along with existinglimitations and future directions. Summation of existing well-classifiedmethods is in our Github:https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model</description><author>Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li</author><pubDate>Mon, 03 Jul 2023 16:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.02646v9</guid></item><item><title>Sampling the lattice Nambu-Goto string using Continuous Normalizing Flows</title><link>http://arxiv.org/abs/2307.01107v1</link><description>Effective String Theory (EST) represents a powerful non-perturbative approachto describe confinement in Yang-Mills theory that models the confining fluxtube as a thin vibrating string. EST calculations are usually performed usingthe zeta-function regularization: however there are situations (for instancethe study of the shape of the flux tube or of the higher order correctionsbeyond the Nambu-Goto EST) which involve observables that are too complex to beaddressed in this way. In this paper we propose a numerical approach based onrecent advances in machine learning methods to circumvent this problem. Usingas a laboratory the Nambu-Goto string, we show that by using a new class ofdeep generative models called Continuous Normalizing Flows it is possible toobtain reliable numerical estimates of EST predictions.</description><author>Michele Caselle, Elia Cellini, Alessandro Nada</author><pubDate>Mon, 03 Jul 2023 16:34:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01107v1</guid></item><item><title>Spatio-Angular Convolutions for Super-resolution in Diffusion MRI</title><link>http://arxiv.org/abs/2306.00854v2</link><description>Diffusion MRI (dMRI) is a widely used imaging modality, but requires longscanning times to acquire high resolution datasets. By leveraging the uniquegeometry present within this domain, we present a novel approach to dMRIangular super-resolution that extends upon the parametric continuousconvolution (PCConv) framework. We introduce several additions to the operationincluding a Fourier feature mapping, global coordinates, and domain specificcontext. Using this framework, we build a fully parametric continuousconvolution network (PCCNN) and compare against existing models. We demonstratethe PCCNN performs competitively while using significantly less parameters.Moreover, we show that this formulation generalises well to clinically relevantdownstream analyses such as fixel-based analysis, and neurite orientationdispersion and density imaging.</description><author>Matthew Lyon, Paul Armitage, Mauricio A √Ålvarez</author><pubDate>Mon, 03 Jul 2023 16:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00854v2</guid></item><item><title>ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis</title><link>http://arxiv.org/abs/2306.04527v2</link><description>Domain generalization is critical for real-world applications of machinelearning models to microscopy images, including histopathology and fluorescenceimaging. Artifacts in histopathology arise through a complex combination offactors relating to tissue collection and laboratory processing, as well asfactors intrinsic to patient samples. In fluorescence imaging, these artifactsstem from variations across experimental batches. The complexity and subtletyof these artifacts make the enumeration of data domains intractable. Therefore,augmentation-based methods of domain generalization that require domainidentifiers and manual fine-tuning are inadequate in this setting. To overcomethis challenge, we introduce ContriMix, a domain generalization technique thatlearns to generate synthetic images by disentangling and permuting thebiological content ("content") and technical variations ("attributes") inmicroscopy images. ContriMix does not rely on domain identifiers or handcraftedaugmentations and makes no assumptions about the input characteristics ofimages. We assess the performance of ContriMix on two pathology datasets(Camelyon17-WILDS and a prostate cell classification dataset) and onefluorescence microscopy dataset (RxRx1-WILDS). ContriMix outperforms currentstate-of-the-art methods in all datasets, motivating its usage for microscopyimage analysis in real-world settings where domain information is hard to comeby.</description><author>Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah, Chintan Shah, Sai Chowdary Gullapally, Michael Griffin, Anand Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner</author><pubDate>Mon, 03 Jul 2023 16:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04527v2</guid></item><item><title>Generalized iterated-sums signatures</title><link>http://arxiv.org/abs/2012.04597v3</link><description>We explore the algebraic properties of a generalized version of theiterated-sums signature, inspired by previous work of F.~Kir\'aly andH.~Oberhauser. In particular, we show how to recover the character property ofthe associated linear map over the tensor algebra by considering a deformedquasi-shuffle product of words on the latter. We introduce three non-lineartransformations on iterated-sums signatures, close in spirit to MachineLearning applications, and show some of their properties.</description><author>Joscha Diehl, Kurusch Ebrahimi-Fard, Nikolas Tapia</author><pubDate>Mon, 03 Jul 2023 16:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.04597v3</guid></item><item><title>Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation</title><link>http://arxiv.org/abs/2305.07609v2</link><description>The remarkable achievements of Large Language Models (LLMs) have led to theemergence of a novel recommendation paradigm -- Recommendation via LLM(RecLLM). Nevertheless, it is important to note that LLMs may contain socialprejudices, and therefore, the fairness of recommendations made by RecLLMrequires further investigation. To avoid the potential risks of RecLLM, it isimperative to evaluate the fairness of RecLLM with respect to various sensitiveattributes on the user side. Due to the differences between the RecLLM paradigmand the traditional recommendation paradigm, it is problematic to directly usethe fairness benchmark of traditional recommendation. To address the dilemma,we propose a novel benchmark called Fairness of Recommendation via LLM(FaiRLLM). This benchmark comprises carefully crafted metrics and a datasetthat accounts for eight sensitive attributes1 in two recommendation scenarios:music and movies. By utilizing our FaiRLLM benchmark, we conducted anevaluation of ChatGPT and discovered that it still exhibits unfairness to somesensitive attributes when generating recommendations. Our code and dataset canbe found at https://github.com/jizhi-zhang/FaiRLLM.</description><author>Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He</author><pubDate>Mon, 03 Jul 2023 16:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07609v2</guid></item><item><title>Variations of Squeeze and Excitation networks</title><link>http://arxiv.org/abs/2304.06502v2</link><description>Convolutional neural networks learns spatial features and are heavilyinterlinked within kernels. The SE module have broken the traditional route ofneural networks passing the entire result to next layer. Instead SE only passesimportant features to be learned with its squeeze and excitation (SE) module.We propose variations of the SE module which improvises the process of squeezeand excitation and enhances the performance. The proposed squeezing or excitingthe layer makes it possible for having a smooth transition of layer weights.These proposed variations also retain the characteristics of SE module. Theexperimented results are carried out on residual networks and the results aretabulated.</description><author>Mahendran NV</author><pubDate>Mon, 03 Jul 2023 16:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06502v2</guid></item><item><title>MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion</title><link>http://arxiv.org/abs/2307.01097v1</link><description>This paper introduces MVDiffusion, a simple yet effective multi-view imagegeneration method for scenarios where pixel-to-pixel correspondences areavailable, such as perspective crops from panorama or multi-view images givengeometry (depth maps and poses). Unlike prior models that rely on iterativeimage warping and inpainting, MVDiffusion concurrently generates all imageswith a global awareness, encompassing high resolution and rich content,effectively addressing the error accumulation prevalent in preceding models.MVDiffusion specifically incorporates a correspondence-aware attentionmechanism, enabling effective cross-view interaction. This mechanism underpinsthree pivotal modules: 1) a generation module that produces low-resolutionimages while maintaining global correspondence, 2) an interpolation module thatdensifies spatial coverage between images, and 3) a super-resolution modulethat upscales into high-resolution outputs. In terms of panoramic imagery,MVDiffusion can generate high-resolution photorealistic images up to1024$\times$1024 pixels. For geometry-conditioned multi-view image generation,MVDiffusion demonstrates the first method capable of generating a textured mapof a scene mesh. The project page is at https://mvdiffusion.github.io.</description><author>Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, Yasutaka Furukawa</author><pubDate>Mon, 03 Jul 2023 16:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01097v1</guid></item><item><title>Att-KGCN: Tourist Attractions Recommendation System by using Attention mechanism and Knowledge Graph Convolution Network</title><link>http://arxiv.org/abs/2306.10946v4</link><description>The recommendation algorithm based on knowledge graphs is at a relativelymature stage. However, there are still some problems in the recommendation ofspecific areas. For example, in the tourism field, selecting suitable touristattraction attributes process is complicated as the recommendation basis fortourist attractions. In this paper, we propose the improved Attention KnowledgeGraph Convolution Network model, named ($Att-KGCN$), which automaticallydiscovers the neighboring entities of the target scenic spot semantically. Theattention layer aggregates relatively similar locations and represents themwith an adjacent vector. Then, according to the tourist's preferred choices,the model predicts the probability of similar spots as a recommendation system.A knowledge graph dataset of tourist attractions used based on tourism data onSocotra Island-Yemen. Through experiments, it is verified that the AttentionKnowledge Graph Convolution Network has a good effect on the recommendation oftourist attractions and can make more recommendations for tourists' choices.</description><author>Ahmad A. Mubarak, JingJing Li, Han Cao</author><pubDate>Mon, 03 Jul 2023 16:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10946v4</guid></item><item><title>UW-ProCCaps: UnderWater Progressive Colourisation with Capsules</title><link>http://arxiv.org/abs/2307.01091v1</link><description>Underwater images are fundamental for studying and understanding the statusof marine life. We focus on reducing the memory space required for imagestorage while the memory space consumption in the collecting phase limits thetime lasting of this phase leading to the need for more image collectioncampaigns. We present a novel machine-learning model that reconstructs thecolours of underwater images from their luminescence channel, thus saving 2/3of the available storage space. Our model specialises in underwater colourreconstruction and consists of an encoder-decoder architecture. The encoder iscomposed of a convolutional encoder and a parallel specialised classifiertrained with webly-supervised data. The encoder and the decoder use layers ofcapsules to capture the features of the entities in the image. The colourreconstruction process recalls the progressive and the generative adversarialtraining procedures. The progressive training gives the ground for a generativeadversarial routine focused on the refining of colours giving the image brightand saturated colours which bring the image back to life. We validate the modelboth qualitatively and quantitatively on four benchmark datasets. This is thefirst attempt at colour reconstruction in greyscale underwater images.Extensive results on four benchmark datasets demonstrate that our solutionoutperforms state-of-the-art (SOTA) solutions. We also demonstrate that thegenerated colourisation enhances the quality of images compared to enhancementmodels at the SOTA.</description><author>Rita Pucci, Niki Martine</author><pubDate>Mon, 03 Jul 2023 16:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01091v1</guid></item></channel></rss>