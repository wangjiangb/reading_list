<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 29 Jan 2024 06:00:27 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ECG-Image-Kit: A Synthetic Image Generation Toolbox to Facilitate Deep Learning-Based Electrocardiogram Digitization</title><link>http://arxiv.org/abs/2307.01946v3</link><description>We introduce ECG-Image-Kit, an open-source toolbox for generating syntheticECG images with realistic artifacts from time-series data, and showcase itsapplication in developing algorithms for data augmentation and ECG imagedigitization. Synthetic data is generated by producing distortionless ECGimages on a standard ECG paper background. Subsequently, various distortions,including handwritten text artifacts, wrinkles, creases, and perspectivetransformations, are applied to these ECG images. The artifacts and text aresynthetically generated, excluding personally identifiable information. Thetoolbox is used for data augmentation in the 2024 PhysioNet Challenge onDigitization and Classification of ECG Images. As a case study, we employed ECG-Image-Kit to create an ECG image dataset of21,801 records from the PhysioNet QT database. A denoising convolutional neuralnetwork (DnCNN)-based model was developed and trained on this synthetic datasetand used to convert the synthetically generated images back into time-seriesdata for evaluation. SNR was calculated to assess the quality of imagedigitization compared to the ground truth ECG time-series. The results show anaverage signal recovery SNR of 11.17 +/- 9.19 dB, indicating the synthetic ECGimage dataset's significance for training deep learning models. For clinicalevaluation, we measured the error between the estimated and ground-truthtime-series data's RR and QT-intervals. The accuracy of the estimated RR andQT-intervals also suggests that the respective clinical parameters aremaintained. These results demonstrate the effectiveness of a deeplearning-based pipeline in accurately digitizing paper ECGs and highlight agenerative approach to digitization.</description><author>Kshama Kodthalu Shivashankara, Deepanshi, Afagh Mehri Shervedani, Gari D. Clifford, Matthew A. Reyna, Reza Sameni</author><pubDate>Fri, 26 Jan 2024 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01946v3</guid></item><item><title>EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</title><link>http://arxiv.org/abs/2401.15077v1</link><description>Auto-regressive decoding makes the inference of Large Language Models (LLMs)time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithmfor Greater Language-model Efficiency), for lossless acceleration. Unliketraditional speculative sampling methods, EAGLE operates the drafting processauto-regressively at the more regular (second-top-layer) feature level andaddresses the sampling uncertainty issues in the next-feature predictionproblems by integrating tokens from one time step ahead. The accelerationprovided by EAGLE is lossless: it involves no fine-tuning of the target LLM,and the generated text maintains the same distribution as that of vanillaauto-regressive decoding. As of the submission of this paper, EAGLE is thefastest known framework within the speculative sampling family. On MT-bench,EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6xfaster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s withLLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s ofHuggingface's implementations.</description><author>Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang</author><pubDate>Fri, 26 Jan 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15077v1</guid></item><item><title>Annotated Hands for Generative Models</title><link>http://arxiv.org/abs/2401.15075v1</link><description>Generative models such as GANs and diffusion models have demonstratedimpressive image generation capabilities. Despite these successes, thesesystems are surprisingly poor at creating images with hands. We propose a noveltraining framework for generative models that substantially improves theability of such systems to create hand images. Our approach is to augment thetraining images with three additional channels that provide annotations tohands in the image. These annotations provide additional structure that coaxthe generative model to produce higher quality hand images. We demonstrate thisapproach on two different generative models: a generative adversarial networkand a diffusion model. We demonstrate our method both on a new syntheticdataset of hand images and also on real photographs that contain hands. Wemeasure the improved quality of the generated hands through higher confidencein finger joint identification using an off-the-shelf hand detector.</description><author>Yue Yang, Atith N Gandhi, Greg Turk</author><pubDate>Fri, 26 Jan 2024 18:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15075v1</guid></item><item><title>ViR: Towards Efficient Vision Retention Backbones</title><link>http://arxiv.org/abs/2310.19731v2</link><description>Vision Transformers (ViTs) have attracted a lot of popularity in recentyears, due to their exceptional capabilities in modeling long-range spatialdependencies and scalability for large scale training. Although the trainingparallelism of self-attention mechanism plays an important role in retaininggreat performance, its quadratic complexity baffles the application of ViTs inmany scenarios which demand fast inference. This effect is even more pronouncedin applications in which autoregressive modeling of input features is required.In Natural Language Processing (NLP), a new stream of efforts has proposedparallelizable models with recurrent formulation that allows for efficientinference in generative applications. Inspired by this trend, we propose a newclass of computer vision models, dubbed Vision Retention Networks (ViR), withdual parallel and recurrent formulations, which strike an optimal balancebetween fast inference and parallel training with competitive performance. Inparticular, ViR scales favorably for image throughput and memory consumption intasks that require higher-resolution images due to its flexible formulation inprocessing large sequence lengths. The ViR is the first attempt to realize dualparallel and recurrent equivalency in a general vision backbone for recognitiontasks. We have validated the effectiveness of ViR through extensive experimentswith different dataset sizes and various image resolutions and achievedcompetitive performance. Code: https://github.com/NVlabs/ViR</description><author>Ali Hatamizadeh, Michael Ranzinger, Shiyi Lan, Jose M. Alvarez, Sanja Fidler, Jan Kautz</author><pubDate>Fri, 26 Jan 2024 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19731v2</guid></item><item><title>From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities</title><link>http://arxiv.org/abs/2401.15071v1</link><description>Multi-modal Large Language Models (MLLMs) have shown impressive abilities ingenerating reasonable responses with respect to multi-modal contents. However,there is still a wide gap between the performance of recent MLLM-basedapplications and the expectation of the broad public, even though the mostpowerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paperstrives to enhance understanding of the gap through the lens of a qualitativestudy on the generalizability, trustworthiness, and causal reasoningcapabilities of recent proprietary and open-source MLLMs across fourmodalities: ie, text, code, image, and video, ultimately aiming to improve thetransparency of MLLMs. We believe these properties are several representativefactors that define the reliability of MLLMs, in supporting various downstreamapplications. To be specific, we evaluate the closed-source GPT-4 and Geminiand 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designedcases, where the qualitative results are then summarized into 12 scores (ie, 4modalities times 3 properties). In total, we uncover 14 empirical findings thatare useful to understand the capabilities and limitations of both proprietaryand open-source MLLMs, towards more reliable downstream multi-modalapplications.</description><author>Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang, Kunchang Li, Lijun Li, Limin Wang, Lu Sheng, Meiqi Chen, Ming Zhang, Qibing Ren, Sirui Chen, Tao Gui, Wanli Ouyang, Yali Wang, Yan Teng, Yaru Wang, Yi Wang, Yinan He, Yingchun Wang, Yixu Wang, Yongting Zhang, Yu Qiao, Yujiong Shen, Yurong Mou, Yuxi Chen, Zaibin Zhang, Zhelun Shi, Zhenfei Yin, Zhipin Wang</author><pubDate>Fri, 26 Jan 2024 18:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15071v1</guid></item><item><title>Pairing Orthographically Variant Literary Words to Standard Equivalents Using Neural Edit Distance Models</title><link>http://arxiv.org/abs/2401.15068v1</link><description>We present a novel corpus consisting of orthographically variant words foundin works of 19th century U.S. literature annotated with their corresponding"standard" word pair. We train a set of neural edit distance models to pairthese variants with their standard forms, and compare the performance of thesemodels to the performance of a set of neural edit distance models trained on acorpus of orthographic errors made by L2 English learners. Finally, we analyzethe relative performance of these models in the light of different negativetraining sample generation strategies, and offer concluding remarks on theunique challenge literary orthographic variation poses to string pairingmethodologies.</description><author>Craig Messner, Tom Lippincott</author><pubDate>Fri, 26 Jan 2024 18:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15068v1</guid></item><item><title>ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models</title><link>http://arxiv.org/abs/2310.02998v2</link><description>Large Vision-Language Models (LVLMs) can understand the world comprehensivelyby integrating rich information from different modalities, achieving remarkableadvancements on various multimodal downstream tasks. However, deploying LVLMsis often problematic due to their massive computational/energy costs and carbonconsumption. Such issues make it infeasible to adopt conventional iterativeglobal pruning, which is costly due to computing the Hessian matrix of theentire large model for sparsification. Alternatively, several studies haverecently proposed layer-wise pruning approaches to avoid the expensivecomputation of global pruning and efficiently compress model weights accordingto their importance within a layer. However, they often suffer from suboptimalmodel compression due to their lack of a global perspective. To address thislimitation in recent efficient pruning methods for large models, we proposeEfficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stagecoarse-to-fine weight pruning approach for LVLMs. We first determine thesparsity ratios of different layers or blocks by leveraging the globalimportance score, which is efficiently computed based on the zeroth-orderapproximation of the global model gradients. Then, the model performs locallayer-wise unstructured weight pruning based on globally-informed sparsityratios. We validate our proposed method across various multimodal and unimodalmodels and datasets, demonstrating significant performance improvements overprevalent pruning techniques in the high-sparsity regime.</description><author>Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</author><pubDate>Fri, 26 Jan 2024 18:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02998v2</guid></item><item><title>Expert with Clustering: Hierarchical Online Preference Learning Framework</title><link>http://arxiv.org/abs/2401.15062v1</link><description>Emerging mobility systems are increasingly capable of recommending options tomobility users, to guide them towards personalized yet sustainable systemoutcomes. Even more so than the typical recommendation system, it is crucial tominimize regret, because 1) the mobility options directly affect the lives ofthe users, and 2) the system sustainability relies on sufficient userparticipation. In this study, we consider accelerating user preference learningby exploiting a low-dimensional latent space that captures the mobilitypreferences of users. We introduce a hierarchical contextual bandit frameworknamed Expert with Clustering (EWC), which integrates clustering techniques andprediction with expert advice. EWC efficiently utilizes hierarchical userinformation and incorporates a novel Loss-guided Distance metric. This metricis instrumental in generating more representative cluster centroids. In arecommendation scenario with $N$ users, $T$ rounds per user, and $K$ options,our algorithm achieves a regret bound of $O(N\sqrt{T\log K} + NT)$. This boundconsists of two parts: the first term is the regret from the Hedge algorithm,and the second term depends on the average loss from clustering. The algorithmperforms with low regret, especially when a latent hierarchical structureexists among users. This regret bound underscores the theoretical andexperimental efficacy of EWC, particularly in scenarios that demand rapidlearning and adaptation. Experimental results highlight that EWC cansubstantially reduce regret by 27.57% compared to the LinUCB baseline. Our workoffers a data-efficient approach to capturing both individual and collectivebehaviors, making it highly applicable to contexts with hierarchicalstructures. We expect the algorithm to be applicable to other settings withlayered nuances of user preferences and information.</description><author>Tianyue Zhou, Jung-Hoon Cho, Babak Rahimi Ardabili, Hamed Tabkhi, Cathy Wu</author><pubDate>Fri, 26 Jan 2024 18:44:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15062v1</guid></item><item><title>Representation Disentaglement via Regularization by Causal Identification</title><link>http://arxiv.org/abs/2303.00128v3</link><description>In this work, we propose the use of a causal collider structured model todescribe the underlying data generative process assumptions in disentangledrepresentation learning. This extends the conventional i.i.d. factorizationassumption model $p(\mathbf{y}) = \prod_{i} p(\mathbf{y}_i )$, inadequate tohandle learning from biased datasets (e.g., with sampling selection bias). Thecollider structure, explains that conditional dependencies between theunderlying generating variables may be exist, even when these are in realityunrelated, complicating disentanglement. Under the rubric of causal inference,we show this issue can be reconciled under the condition of causalidentification; attainable from data and a combination of constraints, aimed atcontrolling the dependencies characteristic of the \textit{collider} model. Forthis, we propose regularization by identification (ReI), a modularregularization engine designed to align the behavior of large scale generativemodels with the disentanglement constraints imposed by causal identification.Empirical evidence on standard benchmarks demonstrates the superiority of ReIin learning disentangled representations in a variational framework. In areal-world dataset we additionally show that our framework, results ininterpretable representations robust to out-of-distribution examples and thatalign with the true expected effect from domain knowledge.</description><author>Juan Castorena</author><pubDate>Fri, 26 Jan 2024 18:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00128v3</guid></item><item><title>Digital-analog hybrid matrix multiplication processor for optical neural networks</title><link>http://arxiv.org/abs/2401.15061v1</link><description>The computational demands of modern AI have spurred interest in opticalneural networks (ONNs) which offer the potential benefits of increased speedand lower power consumption. However, current ONNs face various challenges,mostsignificantly a limited calculation precision (typically around 4 bits) and therequirement for high-resolution signal format converters (digital-to-analogueconversions (DACs) and analogue-to-digital conversions (ADCs)). Thesechallenges are inherent to their analog computing nature and pose significantobstacles in practical implementation. Here, we propose a digital-analog hybridoptical computing architecture for ONNs, which utilizes digital optical inputsin the form of binary words. By introducing the logic levels and decisionsbased on thresholding, the calculation precision can be significantly enhanced.The DACs for input data can be removed and the resolution of the ADCs can begreatly reduced. This can increase the operating speed at a high calculationprecision and facilitate the compatibility with microelectronics. To validateour approach, we have fabricated a proof-of-concept photonic chip and built upa hybrid optical processor (HOP) system for neural network applications. Wehave demonstrated an unprecedented 16-bit calculation precision forhigh-definition image processing, with a pixel error rate (PER) as low as$1.8\times10^{-3}$ at an signal-to-noise ratio (SNR) of 18.2 dB. We have alsoimplemented a convolutional neural network for handwritten digit recognitionthat shows the same accuracy as the one achieved by a desktop computer. Theconcept of the digital-analog hybrid optical computing architecture offers amethodology that could potentially be applied to various ONN implementationsand may intrigue new research into efficient and accurate domain-specificoptical computing architectures for neural networks.</description><author>Xiansong Meng, Deming Kong, Kwangwoong Kim, Qiuchi Li, Po Dong, Ingemar J. Cox, Christina Lioma, Hao Hu</author><pubDate>Fri, 26 Jan 2024 18:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15061v1</guid></item><item><title>Fully Independent Communication in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2401.15059v1</link><description>Multi-Agent Reinforcement Learning (MARL) comprises a broad area of researchwithin the field of multi-agent systems. Several recent works have focusedspecifically on the study of communication approaches in MARL. While multiplecommunication methods have been proposed, these might still be too complex andnot easily transferable to more practical contexts. One of the reasons for thatis due to the use of the famous parameter sharing trick. In this paper, weinvestigate how independent learners in MARL that do not share parameters cancommunicate. We demonstrate that this setting might incur into some problems,to which we propose a new learning scheme as a solution. Our results show that,despite the challenges, independent agents can still learn communicationstrategies following our method. Additionally, we use this method toinvestigate how communication in MARL is affected by different networkcapacities, both for sharing and not sharing parameters. We observe thatcommunication may not always be needed and that the chosen agent network sizesneed to be considered when used together with communication in order to achieveefficient learning.</description><author>Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu</author><pubDate>Fri, 26 Jan 2024 18:42:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15059v1</guid></item><item><title>Deep learning-based approach for tomato classification in complex scenes</title><link>http://arxiv.org/abs/2401.15055v1</link><description>Tracking ripening tomatoes is time consuming and labor intensive. Artificialintelligence technologies combined with those of computer vision can help usersoptimize the process of monitoring the ripening status of plants. To this end,we have proposed a tomato ripening monitoring approach based on deep learningin complex scenes. The objective is to detect mature tomatoes and harvest themin a timely manner. The proposed approach is declined in two parts. Firstly,the images of the scene are transmitted to the pre-processing layer. Thisprocess allows the detection of areas of interest (area of the image containingtomatoes). Then, these images are used as input to the maturity detectionlayer. This layer, based on a deep neural network learning algorithm,classifies the tomato thumbnails provided to it in one of the following fivecategories: green, brittle, pink, pale red, mature red. The experiments arebased on images collected from the internet gathered through searches usingtomato state across diverse languages including English, German, French, andSpanish. The experimental results of the maturity detection layer on a datasetcomposed of images of tomatoes taken under the extreme conditions, gave a goodclassification rate.</description><author>Mikael A. Mousse, Bethel C. A. R. K. Atohoun, Cina Motamed</author><pubDate>Fri, 26 Jan 2024 18:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15055v1</guid></item><item><title>LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents</title><link>http://arxiv.org/abs/2401.15050v1</link><description>Document AI is a growing research field that focuses on the comprehension andextraction of information from scanned and digital documents to make everydaybusiness operations more efficient. Numerous downstream tasks and datasets havebeen introduced to facilitate the training of AI models capable of parsing andextracting information from various document types such as receipts and scannedforms. Despite these advancements, both existing datasets and models fail toaddress critical challenges that arise in industrial contexts. Existingdatasets primarily comprise short documents consisting of a single page, whileexisting models are constrained by a limited maximum length, often set at 512tokens. Consequently, the practical application of these methods in financialservices, where documents can span multiple pages, is severely impeded. Toovercome these challenges, we introduce LongFin, a multimodal document AI modelcapable of encoding up to 4K tokens. We also propose the LongForms dataset, acomprehensive financial dataset that encapsulates several industrial challengesin financial documents. Through an extensive evaluation, we demonstrate theeffectiveness of the LongFin model on the LongForms dataset, surpassing theperformance of existing public models while maintaining comparable results onexisting single-page benchmarks.</description><author>Ahmed Masry, Amir Hajian</author><pubDate>Fri, 26 Jan 2024 18:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15050v1</guid></item><item><title>Unrecognizable Yet Identifiable: Image Distortion with Preserved Embeddings</title><link>http://arxiv.org/abs/2401.15048v1</link><description>In the realm of security applications, biometric authentication systems playa crucial role, yet one often encounters challenges concerning privacy andsecurity while developing one. One of the most fundamental challenges lies inavoiding storing biometrics directly in the storage but still achievingdecently high accuracy. Addressing this issue, we contribute to both artificialintelligence and engineering fields. We introduce an innovative imagedistortion technique that effectively renders facial images unrecognizable tothe eye while maintaining their identifiability by neural network models. Fromthe theoretical perspective, we explore how reliable state-of-the-artbiometrics recognition neural networks are by checking the maximal degree ofimage distortion, which leaves the predicted identity unchanged. On the otherhand, applying this technique demonstrates a practical solution to theengineering challenge of balancing security, precision, and performance inbiometric authentication systems. Through experimenting on the widely useddatasets, we assess the effectiveness of our method in preserving AI featurerepresentation and distorting relative to conventional metrics. We also compareour method with previously used approaches.</description><author>Dmytro Zakharov, Oleksandr Kuznetsov, Emanuele Frontoni</author><pubDate>Fri, 26 Jan 2024 18:20:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15048v1</guid></item><item><title>Emulating Complex Synapses Using Interlinked Proton Conductors</title><link>http://arxiv.org/abs/2401.15045v1</link><description>In terms of energy efficiency and computational speed, neuromorphicelectronics based on non-volatile memory devices is expected to be one of mostpromising hardware candidates for future artificial intelligence (AI). However,catastrophic forgetting, networks rapidly overwriting previously learnedweights when learning new tasks, remains as a pivotal hurdle in either digitalor analog AI chips for unleashing the true power of brain-like computing. Toaddress catastrophic forgetting in the context of online memory storage, acomplex synapse model (the Benna-Fusi model) has been proposed recently[1],whose synaptic weight and internal variables evolve following a diffusiondynamics. In this work, by designing a proton transistor with a series ofcharge-diffusion-controlled storage components, we have experimentally realizedthe Benna-Fusi artificial complex synapse. The memory consolidation fromcoupled storage components is revealed by both numerical simulations andexperimental observations. Different memory timescales for the complex synapseare engineered by the diffusion length of charge carriers, the capacity andnumber of coupled storage components. The advantage of the demonstrated complexsynapse in both memory capacity and memory consolidation is revealed by neuralnetwork simulations of face familiarity detection. Our experimental realizationof the complex synapse suggests a promising approach to enhance memory capacityand to enable continual learning.</description><author>Lifu Zhang, Ji-An Li, Yang Hu, Jie Jiang, Rongjie Lai, Marcus K. Benna, Jian Shi</author><pubDate>Fri, 26 Jan 2024 18:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15045v1</guid></item><item><title>Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning</title><link>http://arxiv.org/abs/2401.15043v1</link><description>Objective: The reading level of health educational materials significantlyinfluences information understandability and accessibility, particularly forminoritized populations. Many patient educational resources surpass the readinglevel and complexity of widely accepted standards. There is a critical need forhigh-performing text simplification models in health information to enhancedissemination and literacy. This need is particularly acute in cancereducation, where effective prevention and screening education can substantiallyreduce morbidity and mortality. Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallelcorpus of cancer education materials tailored for health text simplificationresearch. Utilizing SimpleDC alongside the existing Med-EASi corpus, we exploreLarge Language Model (LLM)-based simplification methods, including fine-tuning,reinforcement learning (RL), reinforcement learning with human feedback (RLHF),domain adaptation, and prompt-based approaches. Our experimentation encompassesLlama 2 and GPT-4. A novel RLHF reward function is introduced, featuring alightweight model adept at distinguishing between original and simplifiedtexts, thereby enhancing the model's effectiveness with unlabeled data. Results: Fine-tuned Llama 2 models demonstrated high performance acrossvarious metrics. Our innovative RLHF reward function surpassed existing RL textsimplification reward functions in effectiveness. The results underscore thatRL/RLHF can augment fine-tuning, facilitating model training on unlabeled textand improving performance. Additionally, these methods effectively adaptout-of-domain text simplification models to targeted domains.</description><author>Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S. Williams, Marcos Zampieri, Kevin Lybarger</author><pubDate>Fri, 26 Jan 2024 18:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15043v1</guid></item><item><title>PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models</title><link>http://arxiv.org/abs/2401.15042v1</link><description>Large Language Models (LLMs) have exhibited remarkable success in long-formcontext comprehension tasks. However, their capacity to generate long contents,such as reports and articles, remains insufficiently explored. Currentbenchmarks do not adequately assess LLMs' ability to produce informative andcomprehensive content, necessitating a more rigorous evaluation approach. Inthis study, we introduce \textsc{ProxyQA}, a framework for evaluating long-formtext generation, comprising in-depth human-curated \textit{meta-questions}spanning various domains. Each meta-question contains corresponding\textit{proxy-questions} with annotated answers. LLMs are prompted to generateextensive content in response to these meta-questions. Utilizing an evaluatorand incorporating generated content as background context, \textsc{ProxyQA}evaluates the quality of generated content based on the evaluator's performancein answering the \textit{proxy-questions}. We examine multiple LLMs,emphasizing \textsc{ProxyQA}'s demanding nature as a high-quality assessmenttool. Human evaluation demonstrates that evaluating through\textit{proxy-questions} is a highly self-consistent andhuman-criteria-correlated validation method. The dataset and leaderboard willbe available at \url{https://github.com/Namco0816/ProxyQA}.</description><author>Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song</author><pubDate>Fri, 26 Jan 2024 18:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15042v1</guid></item><item><title>ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications</title><link>http://arxiv.org/abs/2312.01339v4</link><description>This paper presents the first Arabic crossword puzzle generator driven byadvanced AI technology. Leveraging cutting-edge large language models includingGPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the systemgenerates distinctive and challenging clues. Based on a dataset comprising over50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shotlearning strategies, and rigorous quality-checking protocols to enforce thegeneration of high-quality clue-answer pairs. Importantly, educationalcrosswords contribute to enhancing memory, expanding vocabulary, and promotingproblem-solving skills, thereby augmenting the learning experience through afun and engaging approach, reshaping the landscape of traditional learningmethods. The overall system can be exploited as a powerful educational toolthat amalgamates AI and innovative learning techniques, heralding atransformative era for Arabic crossword puzzles and the intersection oftechnology and education.</description><author>Kamyar Zeinalipour, Mohamed Zaky Saad, Marco Maggini, Marco Gori</author><pubDate>Fri, 26 Jan 2024 18:11:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01339v4</guid></item><item><title>A Robot Web for Distributed Many-Device Localisation</title><link>http://arxiv.org/abs/2202.03314v2</link><description>We show that a distributed network of robots or other devices which makemeasurements of each other can collaborate to globally localise via efficientad-hoc peer to peer communication. Our Robot Web solution is based on GaussianBelief Propagation on the fundamental non-linear factor graph describing theprobabilistic structure of all of the observations robots make internally or ofeach other, and is flexible for any type of robot, motion or sensor. We definea simple and efficient communication protocol which can be implemented by thepublishing and reading of web pages or other asynchronous communicationtechnologies. We show in simulations with up to 1000 robots interacting inarbitrary patterns that our solution convergently achieves global accuracy asaccurate as a centralised non-linear factor graph solver while operating withhigh distributed efficiency of computation and communication. Via the use ofrobust factors in GBP, our method is tolerant to a high percentage of faults insensor measurements or dropped communication packets.</description><author>Riku Murai, Joseph Ortiz, Sajad Saeedi, Paul H. J. Kelly, Andrew J. Davison</author><pubDate>Fri, 26 Jan 2024 18:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.03314v2</guid></item><item><title>On the generalization capacity of neural networks during generic multimodal reasoning</title><link>http://arxiv.org/abs/2401.15030v1</link><description>The advent of the Transformer has led to the development of large languagemodels (LLM), which appear to demonstrate human-like capabilities. To assessthe generality of this class of models and a variety of other base neuralnetwork architectures to multimodal domains, we evaluated and compared theircapacity for multimodal generalization. We introduce a multimodalquestion-answer benchmark to evaluate three specific types ofout-of-distribution (OOD) generalization performance: distractor generalization(generalization in the presence of distractors), systematic compositionalgeneralization (generalization to new task permutations), and productivecompositional generalization (generalization to more complex tasks structures).We found that across model architectures (e.g., RNNs, Transformers, Perceivers,etc.), models with multiple attention layers, or models that leveragedcross-attention mechanisms between input domains, fared better. Our positiveresults demonstrate that for multimodal distractor and systematicgeneralization, either cross-modal attention or models with deeper attentionlayers are key architectural features required to integrate multimodal inputs.On the other hand, neither of these architectural features led to productivegeneralization, suggesting fundamental limitations of existing architecturesfor specific types of multimodal generalization. These results demonstrate thestrengths and limitations of specific architectural components underlyingmodern neural models for multimodal reasoning. Finally, we provide Generic COG(gCOG), a configurable benchmark with several multimodal generalization splits,for future studies to explore.</description><author>Takuya Ito, Soham Dan, Mattia Rigotti, James Kozloski, Murray Campbell</author><pubDate>Fri, 26 Jan 2024 17:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15030v1</guid></item><item><title>Learning Neural Radiance Fields of Forest Structure for Scalable and Fine Monitoring</title><link>http://arxiv.org/abs/2401.15029v1</link><description>This work leverages neural radiance fields and remote sensing for forestryapplications. Here, we show neural radiance fields offer a wide range ofpossibilities to improve upon existing remote sensing methods in forestmonitoring. We present experiments that demonstrate their potential to: (1)express fine features of forest 3D structure, (2) fuse available remote sensingmodalities and (3), improve upon 3D structure derived forest metrics.Altogether, these properties make neural fields an attractive computationaltool with great potential to further advance the scalability and accuracy offorest monitoring programs.</description><author>Juan Castorena</author><pubDate>Fri, 26 Jan 2024 17:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15029v1</guid></item><item><title>SliceGPT: Compress Large Language Models by Deleting Rows and Columns</title><link>http://arxiv.org/abs/2401.15024v1</link><description>Large language models have become the cornerstone of natural languageprocessing, but their use comes with substantial costs in terms of compute andmemory resources. Sparsification provides a solution to alleviate theseresource constraints, and recent works have shown that trained models can besparsified post-hoc. Existing sparsification techniques face challenges as theyneed additional data structures and offer constrained speedup with currenthardware. In this paper we present SliceGPT, a new post-training sparsificationscheme which replaces each weight matrix with a smaller (dense) matrix,reducing the embedding dimension of the network. Through extensiveexperimentation, we show that SliceGPT can remove up to 25% of the modelparameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 modelswhile maintaining 99%, 99% and 90% zero-shot task performance of the densemodel respectively. Our sliced models run on fewer GPUs and run faster withoutany additional code optimization: on 24GB consumer GPUs we reduce the totalcompute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GBA100 GPUs we reduce it to 66%. We offer a new insight, computational invariancein transformer networks, which enables SliceGPT and we hope it will inspire andenable future avenues to reduce memory and computation demands for pre-trainedmodels. Code is available at:https://github.com/microsoft/TransformerCompression</description><author>Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James Hensman</author><pubDate>Fri, 26 Jan 2024 17:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15024v1</guid></item><item><title>Optimal Low-Rank Matrix Completion: Semidefinite Relaxations and Eigenvector Disjunctions</title><link>http://arxiv.org/abs/2305.12292v2</link><description>Low-rank matrix completion consists of computing a matrix of minimalcomplexity that recovers a given set of observations as accurately as possible.Unfortunately, existing methods for matrix completion are heuristics that,while highly scalable and often identifying high-quality solutions, do notpossess any optimality guarantees. We reexamine matrix completion with anoptimality-oriented eye. We reformulate these low-rank problems as convexproblems over the non-convex set of projection matrices and implement adisjunctive branch-and-bound scheme that solves them to certifiable optimality.Further, we derive a novel and often tight class of convex relaxations bydecomposing a low-rank matrix as a sum of rank-one matrices and incentivizingthat two-by-two minors in each rank-one matrix have determinant zero. Innumerical experiments, our new convex relaxations decrease the optimality gapby two orders of magnitude compared to existing attempts, and our disjunctivebranch-and-bound scheme solves nxn rank-r matrix completion problems tocertifiable optimality in hours for n&lt;=150 and r&lt;=5.</description><author>Dimitris Bertsimas, Ryan Cory-Wright, Sean Lo, Jean Pauphilet</author><pubDate>Fri, 26 Jan 2024 17:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12292v2</guid></item><item><title>Machine learning-based analysis of glioma tissue sections: a review</title><link>http://arxiv.org/abs/2401.15022v1</link><description>In recent years, the diagnosis of gliomas has become increasingly complex.Histological assessment of glioma tissue using modern machine learningtechniques offers new opportunities to support diagnosis and outcomeprediction. To give an overview of the current state of research, this reviewexamines 70 publicly available research studies on machine learning-basedanalysis of stained human glioma tissue sections, covering the diagnostic tasksof subtyping (16/70), grading (23/70), molecular marker prediction (13/70), andsurvival prediction (27/70). All studies were reviewed with regard tomethodological aspects as well as clinical applicability. It was found that thefocus of current research is the assessment of hematoxylin and eosin-stainedtissue sections of adult-type diffuse gliomas. The majority of studies (49/70)are based on the publicly available glioblastoma and low-grade glioma datasetsfrom The Cancer Genome Atlas (TCGA) and only a few studies employed otherdatasets in isolation (10/70) or in addition to the TCGA datasets (11/70).Current approaches mostly rely on convolutional neural networks (53/70) foranalyzing tissue at 20x magnification (30/70). A new field of research is theintegration of clinical data, omics data, or magnetic resonance imaging(27/70). So far, machine learning-based methods have achieved promisingresults, but are not yet used in real clinical settings. Future work shouldfocus on the independent validation of methods on larger, multi-site datasetswith high-quality and up-to-date clinical and molecular pathology annotationsto demonstrate routine applicability.</description><author>Jan-Philipp Redlich, Friedrich Feuerhake, Joachim Weis, Nadine S. Schaadt, Sarah Teuber-Hanselmann, Christoph Buck, Sabine Luttmann, Andrea Eberle, Stefan Nikolin, Arno Appenzeller, Andreas Portmann, André Homeyer</author><pubDate>Fri, 26 Jan 2024 17:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15022v1</guid></item><item><title>Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers</title><link>http://arxiv.org/abs/2401.15018v1</link><description>Speaker Verification (SV) systems involve mainly two individual stages:feature extraction and classification. In this paper, we explore these twomodules with the aim of improving the performance of a speaker verificationsystem under noisy conditions. On the one hand, the choice of the mostappropriate acoustic features is a crucial factor for performing robust speakerverification. The acoustic parameters used in the proposed system are: MelFrequency Cepstral Coefficients (MFCC), their first and second derivatives(Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC),Perceptual Linear Predictive (PLP), and Relative Spectral Transform -Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparisonof different combinations of the previous features is discussed. On the otherhand, the major weakness of a conventional Support Vector Machine (SVM)classifier is the use of generic traditional kernel functions to compute thedistances among data points. However, the kernel function of an SVM has greatinfluence on its performance. In this work, we propose the combination of twoSVM-based classifiers with different kernel functions: Linear kernel andGaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR)classifier. The combination is carried out by means of a parallel structureapproach, in which different voting rules to take the final decision areconsidered. Results show that significant improvement in the performance of theSV system is achieved by using the combined features with the combinedclassifiers either with clean speech or in the presence of noise. Finally, toenhance the system more in noisy environments, the inclusion of the multibandnoise removal technique as a preprocessing stage is proposed.</description><author>Kerlos Atia Abdalmalak, Ascensión Gallardo-Antol'in</author><pubDate>Fri, 26 Jan 2024 17:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15018v1</guid></item><item><title>Convergence Error Analysis of Reflected Gradient Langevin Dynamics for Globally Optimizing Non-Convex Constrained Problems</title><link>http://arxiv.org/abs/2203.10215v2</link><description>Gradient Langevin dynamics and a variety of its variants have attractedincreasing attention owing to their convergence towards the global optimalsolution, initially in the unconstrained convex framework while recently evenin convex constrained non-convex problems. In the present work, we extend thoseframeworks to non-convex problems on a non-convex feasible region with a globaloptimization algorithm built upon reflected gradient Langevin dynamics andderive its convergence rates. By effectively making use of its reflection atthe boundary in combination with the probabilistic representation for thePoisson equation with the Neumann boundary condition, we present promisingconvergence rates, particularly faster than the existing one for convexconstrained non-convex problems.</description><author>Kanji Sato, Akiko Takeda, Reiichiro Kawai, Taiji Suzuki</author><pubDate>Fri, 26 Jan 2024 17:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.10215v2</guid></item><item><title>Airavata: Introducing Hindi Instruction-tuned LLM</title><link>http://arxiv.org/abs/2401.15006v1</link><description>We announce the initial release of "Airavata," an instruction-tuned LLM forHindi. Airavata was created by fine-tuning OpenHathi with diverse,instruction-tuning Hindi datasets to make it better suited for assistive tasks.Along with the model, we also share the IndicInstruct dataset, which is acollection of diverse instruction-tuning datasets to enable further researchfor Indic LLMs. Additionally, we present evaluation benchmarks and a frameworkfor assessing LLM performance across tasks in Hindi. Currently, Airavatasupports Hindi, but we plan to expand this to all 22 scheduled Indic languages.You can access all artifacts at https://ai4bharat.github.io/airavata.</description><author>Jay Gala, Thanmay Jayakumar, Jaavid Aktar Husain, Aswanth Kumar M, Mohammed Safi Ur Rahman Khan, Diptesh Kanojia, Ratish Puduppully, Mitesh M. Khapra, Raj Dabre, Rudra Murthy, Anoop Kunchukuttan</author><pubDate>Fri, 26 Jan 2024 17:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15006v1</guid></item><item><title>BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning</title><link>http://arxiv.org/abs/2401.15002v1</link><description>As an emerging and vital topic for studying deep neural networks'vulnerability (DNNs), backdoor learning has attracted increasing interest inrecent years, and many seminal backdoor attack and defense algorithms are beingdeveloped successively or concurrently, in the status of a rapid arms race.However, mainly due to the diverse settings, and the difficulties ofimplementation and reproducibility of existing works, there is a lack of aunified and standardized benchmark of backdoor learning, causing unfaircomparisons, and unreliable conclusions (e.g., misleading, biased or even falseconclusions). Consequently, it is difficult to evaluate the current progressand design the future development roadmap of this literature. To alleviate thisdilemma, we build a comprehensive benchmark of backdoor learning calledBackdoorBench. Our benchmark makes three valuable contributions to the researchcommunity. 1) We provide an integrated implementation of state-of-the-art(SOTA) backdoor learning algorithms (currently including 16 attack and 27defense algorithms), based on an extensible modular-based codebase. 2) Weconduct comprehensive evaluations of 12 attacks against 16 defenses, with 5poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs ofevaluations in total. 3) Based on above evaluations, we present abundantanalysis from 8 perspectives via 18 useful analysis tools, and provide severalinspiring insights about backdoor learning. We hope that our efforts couldbuild a solid foundation of backdoor learning to facilitate researchers toinvestigate existing algorithms, develop more innovative algorithms, andexplore the intrinsic mechanism of backdoor learning. Finally, we have createda user-friendly website at http://backdoorbench.com, which collects allimportant information of BackdoorBench, including codebase, docs, leaderboard,and model Zoo.</description><author>Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen</author><pubDate>Fri, 26 Jan 2024 17:03:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15002v1</guid></item><item><title>Dual RL: Unification and New Methods for Reinforcement and Imitation Learning</title><link>http://arxiv.org/abs/2302.08560v3</link><description>The goal of reinforcement learning (RL) is to find a policy that maximizesthe expected cumulative return. It has been shown that this objective can berepresented as an optimization problem of state-action visitation distributionunder linear constraints. The dual problem of this formulation, which we referto as dual RL, is unconstrained and easier to optimize. In this work, we firstcast several state-of-the-art offline RL and offline imitation learning (IL)algorithms as instances of dual RL approaches with shared structures. Suchunification allows us to identify the root cause of the shortcomings of priormethods. For offline IL, our analysis shows that prior methods are based on arestrictive coverage assumption that greatly limits their performance inpractice. To fix this limitation, we propose a new discriminator-free methodReCOIL that learns to imitate from arbitrary off-policy data to obtainnear-expert performance. For offline RL, our analysis frames a recent offlineRL method XQL in the dual framework, and we further propose a new method f-DVLthat provides alternative choices to the Gumbel regression loss that fixes theknown training instability issue of XQL. The performance improvements by bothof our proposed methods, ReCOIL and f-DVL, in IL and RL are validated on anextensive suite of simulated robot locomotion and manipulation tasks. Projectcode and details can be found at this https://hari-sikchi.github.io/dual-rl.</description><author>Harshit Sikchi, Qinqing Zheng, Amy Zhang, Scott Niekum</author><pubDate>Fri, 26 Jan 2024 16:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08560v3</guid></item><item><title>On minimizers and convolutional filters: theoretical connections and applications to genome analysis</title><link>http://arxiv.org/abs/2111.08452v6</link><description>Minimizers and convolutional neural networks (CNNs) are two quite distinctpopular techniques that have both been employed to analyze categoricalbiological sequences. At face value, the methods seem entirely dissimilar.Minimizers use min-wise hashing on a rolling window to extract a singleimportant k-mer feature per window. CNNs start with a wide array of randomlyinitialized convolutional filters, paired with a pooling operation, and thenmultiple additional neural layers to learn both the filters themselves and howthey can be used to classify the sequence. Here, our main result is a careful mathematical analysis of hash functionproperties showing that for sequences over a categorical alphabet, randomGaussian initialization of convolutional filters with max-pooling is equivalentto choosing a minimizer ordering such that selected k-mers are (in Hammingdistance) far from the k-mers within the sequence but close to otherminimizers. In empirical experiments, we find that this property manifests asdecreased density in repetitive regions, both in simulation and on real humantelomeres. We additionally train from scratch a CNN embedding of syntheticshort-reads from the SARS-CoV-2 genome into 3D Euclidean space that locallyrecapitulates the linear sequence distance of the read origins, a modest steptowards building a deep learning assembler, though it is at present too slow tobe practical. In total, this manuscript provides a partial explanation for theeffectiveness of CNNs in categorical sequence analysis.</description><author>Yun William Yu</author><pubDate>Fri, 26 Jan 2024 16:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.08452v6</guid></item><item><title>Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2302.11464v4</link><description>There is a growing consensus in the research community that the optimizationof low-light image enhancement approaches should be guided by the visualquality perceived by end users. Despite the substantial efforts invested in thedesign of low-light enhancement algorithms, there has been comparativelylimited focus on assessing subjective and objective quality systematically. Tomitigate this gap and provide a clear path towards optimizing low-light imageenhancement for better visual quality, we propose a gap-closing framework. Inparticular, our gap-closing framework starts with the creation of a large-scaledataset for Subjective QUality Assessment of REconstructed LOw-Light Images(SQUARE-LOL). This database serves as the foundation for studying the qualityof enhanced images and conducting a comprehensive subjective user study.Subsequently, we propose an objective quality assessment measure that plays acritical role in bridging the gap between visual quality and enhancement.Finally, we demonstrate that our proposed objective quality measure can beincorporated into the process of optimizing the learning of the enhancementmodel toward perceptual optimality. We validate the effectiveness of ourproposed framework through both the accuracy of quality prediction and theperceptual quality of image enhancement. Our database and code will be madepublicly available to facilitate further research in this area.</description><author>Baoliang Chen, Lingyu Zhu, Hanwei Zhu, Wenhan Yang, Linqi Song, Shiqi Wang</author><pubDate>Fri, 26 Jan 2024 16:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11464v4</guid></item><item><title>OYXOY: A Modern NLP Test Suite for Modern Greek</title><link>http://arxiv.org/abs/2309.07009v2</link><description>This paper serves as a foundational step towards the development of alinguistically motivated and technically relevant evaluation suite for GreekNLP. We initiate this endeavor by introducing four expert-verified evaluationtasks, specifically targeted at natural language inference, word sensedisambiguation (through example comparison or sense selection) and metaphordetection. More than language-adapted replicas of existing tasks, we contributetwo innovations which will resonate with the broader resource and evaluationcommunity. Firstly, our inference dataset is the first of its kind, marking notjust \textit{one}, but rather \textit{all} possible inference labels,accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, wedemonstrate a cost-efficient method to obtain datasets for under-resourcedlanguages. Using ChatGPT as a language-neutral parser, we transform theDictionary of Standard Modern Greek into a structured format, from which wederive the other three tasks through simple projections. Alongside each task,we conduct experiments using currently available state of the art machinery.Our experimental baselines affirm the challenging nature of our tasks andhighlight the need for expedited progress in order for the Greek NLP ecosystemto keep pace with contemporary mainstream research.</description><author>Konstantinos Kogkalidis, Stergios Chatzikyriakidis, Eirini Chrysovalantou Giannikouri, Vassiliki Katsouli, Christina Klironomou, Christina Koula, Dimitris Papadakis, Thelka Pasparaki, Erofili Psaltaki, Efthymia Sakellariou, Hara Soupiona</author><pubDate>Fri, 26 Jan 2024 16:45:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07009v2</guid></item><item><title>Graph-based Active Learning for Entity Cluster Repair</title><link>http://arxiv.org/abs/2401.14992v1</link><description>Cluster repair methods aim to determine errors in clusters and modify them sothat each cluster consists of records representing the same entity. Currentcluster repair methodologies primarily assume duplicate-free data sources,where each record from one source corresponds to a unique record from another.However, real-world data often deviates from this assumption due to qualityissues. Recent approaches apply clustering methods in combination with linkcategorization methods so they can be applied to data sources with duplicates.Nevertheless, the results do not show a clear picture since the quality highlyvaries depending on the configuration and dataset. In this study, we introducea novel approach for cluster repair that utilizes graph metrics derived fromthe underlying similarity graphs. These metrics are pivotal in constructing aclassification model to distinguish between correct and incorrect edges. Toaddress the challenge of limited training data, we integrate an active learningmechanism tailored to cluster-specific attributes. The evaluation shows thatthe method outperforms existing cluster repair methods without distinguishingbetween duplicate-free or dirty data sources. Notably, our modified activelearning strategy exhibits enhanced performance when dealing with datasetscontaining duplicates, showcasing its effectiveness in such scenarios.</description><author>Victor Christen, Daniel Obraczka, Marvin Hofer, Martin Franke, Erhard Rahm</author><pubDate>Fri, 26 Jan 2024 16:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14992v1</guid></item><item><title>Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies</title><link>http://arxiv.org/abs/2401.12888v2</link><description>The aspiration of the next generation's autonomous driving (AD) technologyrelies on the dedicated integration and interaction among intelligentperception, prediction, planning, and low-level control. There has been a hugebottleneck regarding the upper bound of autonomous driving algorithmperformance, a consensus from academia and industry believes that the key tosurmount the bottleneck lies in data-centric autonomous driving technology.Recent advancement in AD simulation, closed-loop model training, and AD bigdata engine have gained some valuable experience. However, there is a lack ofsystematic knowledge and deep understanding regarding how to build efficientdata-centric AD technology for AD algorithm self-evolution and better AD bigdata accumulation. To fill in the identified research gaps, this article willclosely focus on reviewing the state-of-the-art data-driven autonomous drivingtechnologies, with an emphasis on the comprehensive taxonomy of autonomousdriving datasets characterized by milestone generations, key features, dataacquisition settings, etc. Furthermore, we provide a systematic review of theexisting benchmark closed-loop AD big data pipelines from the industrialfrontier, including the procedure of closed-loop frameworks, key technologies,and empirical studies. Finally, the future directions, potential applications,limitations and concerns are discussed to arouse efforts from both academia andindustry for promoting the further development of autonomous driving. Theproject repository is available at:https://github.com/LincanLi98/Awesome-Data-Centric-Autonomous-Driving.</description><author>Lincan Li, Wei Shao, Wei Dong, Yijun Tian, Qiming Zhang, Kaixiang Yang, Wenjie Zhang</author><pubDate>Fri, 26 Jan 2024 16:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12888v2</guid></item><item><title>Mapping-to-Parameter Nonlinear Functional Regression with Novel B-spline Free Knot Placement Algorithm</title><link>http://arxiv.org/abs/2401.14989v1</link><description>We propose a novel approach to nonlinear functional regression, called theMapping-to-Parameter function model, which addresses complex and nonlinearfunctional regression problems in parameter space by employing any supervisedlearning technique. Central to this model is the mapping of function data froman infinite-dimensional function space to a finite-dimensional parameter space.This is accomplished by concurrently approximating multiple functions with acommon set of B-spline basis functions by any chosen order, with their knotdistribution determined by the Iterative Local Placement Algorithm, a newlyproposed free knot placement algorithm. In contrast to the conventionalequidistant knot placement strategy that uniformly distributes knot locationsbased on a predefined number of knots, our proposed algorithms determine knotlocation according to the local complexity of the input or output functions.The performance of our knot placement algorithms is shown to be robust in bothsingle-function approximation and multiple-function approximation contexts.Furthermore, the effectiveness and advantage of the proposed prediction modelin handling both function-on-scalar regression and function-on-functionregression problems are demonstrated through several real data applications, incomparison with four groups of state-of-the-art methods.</description><author>Chengdong Shi, Ching-Hsun Tseng, Wei Zhao, Xiao-Jun Zeng</author><pubDate>Fri, 26 Jan 2024 16:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14989v1</guid></item><item><title>Signature Methods in Machine Learning</title><link>http://arxiv.org/abs/2206.14674v5</link><description>Signature-based techniques give mathematical insight into the interactionsbetween complex streams of evolving data. These insights can be quite naturallytranslated into numerical approaches to understanding streamed data, andperhaps because of their mathematical precision, have proved useful inanalysing streamed data in situations where the data is irregular, and notstationary, and the dimension of the data and the sample sizes are bothmoderate. Understanding streamed multi-modal data is exponential: a word in $n$letters from an alphabet of size $d$ can be any one of $d^n$ messages.Signatures remove the exponential amount of noise that arises from samplingirregularity, but an exponential amount of information still remain. Thissurvey aims to stay in the domain where that exponential scaling can be manageddirectly. Scalability issues are an important challenge in many problems butwould require another survey article and further ideas. This survey describes arange of contexts where the data sets are small enough to remove thepossibility of massive machine learning, and the existence of small sets ofcontext free and principled features can be used effectively. The mathematicalnature of the tools can make their use intimidating to non-mathematicians. Theexamples presented in this article are intended to bridge this communicationgap and provide tractable working examples drawn from the machine learningcontext. Notebooks are available online for several of these examples. Thissurvey builds on the earlier paper of Ilya Chevryev and Andrey Kormilitzinwhich had broadly similar aims at an earlier point in the development of thismachinery. This article illustrates how the theoretical insights offered bysignatures are simply realised in the analysis of application data in a waythat is largely agnostic to the data type.</description><author>Terry Lyons, Andrew D. McLeod</author><pubDate>Fri, 26 Jan 2024 16:33:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.14674v5</guid></item><item><title>Prompt-based Distribution Alignment for Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2312.09553v2</link><description>Recently, despite the unprecedented success of large pre-trainedvisual-language models (VLMs) on a wide range of downstream tasks, thereal-world unsupervised domain adaptation (UDA) problem is still not wellexplored. Therefore, in this paper, we first experimentally demonstrate thatthe unsupervised-trained VLMs can significantly reduce the distributiondiscrepancy between source and target domains, thereby improving theperformance of UDA. However, a major challenge for directly deploying suchmodels on downstream UDA tasks is prompt engineering, which requires aligningthe domain knowledge of source and target domains, since the performance of UDAis severely influenced by a good domain-invariant representation. We furtherpropose a Prompt-based Distribution Alignment (PDA) method to incorporate thedomain knowledge into prompt learning. Specifically, PDA employs a two-branchprompt-tuning paradigm, namely base branch and alignment branch. The basebranch focuses on integrating class-related representation into prompts,ensuring discrimination among different classes. To further minimize domaindiscrepancy, for the alignment branch, we construct feature banks for both thesource and target domains and propose image-guided feature tuning (IFT) to makethe input attend to feature banks, which effectively integrates self-enhancedand cross-domain features into the model. In this way, these two branches canbe mutually promoted to enhance the adaptation of VLMs for UDA. We conductextensive experiments on three benchmarks to demonstrate that our proposed PDAachieves state-of-the-art performance. The code is available athttps://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.</description><author>Shuanghao Bai, Min Zhang, Wanqi Zhou, Siteng Huang, Zhirong Luan, Donglin Wang, Badong Chen</author><pubDate>Fri, 26 Jan 2024 16:31:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09553v2</guid></item><item><title>Incorporating Crowdsourced Annotator Distributions into Ensemble Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri</title><link>http://arxiv.org/abs/2210.16380v4</link><description>Performing classification on noisy, crowdsourced image datasets can provechallenging even for the best neural networks. Two issues which complicate theproblem on such datasets are class imbalance and ground-truth uncertainty inlabeling. The AL-ALL and AL-PUB datasets - consisting of tightly cropped,individual characters from images of ancient Greek papyri - are stronglyaffected by both issues. The application of ensemble modeling to such datasetscan help identify images where the ground-truth is questionable and quantifythe trustworthiness of those samples. As such, we apply stacked generalizationconsisting of nearly identical ResNets with different loss functions: oneutilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence(KLD). Both networks use labels drawn from a crowd-sourced consensus. Thisconsensus is derived from a Normalized Distribution of Annotations (NDA) basedon all annotations for a given character in the dataset. For the secondnetwork, the KLD is calculated with respect to the NDA. For our ensemble model,we apply a k-nearest neighbors model to the outputs of the CXE and KLDnetworks. Individually, the ResNet models have approximately 93% accuracy,while the ensemble model achieves an accuracy of &gt; 95%, increasing theclassification trustworthiness. We also perform an analysis of the Shannonentropy of the various models' output distributions to measure classificationuncertainty. Our results suggest that entropy is useful for predicting modelmisclassifications.</description><author>Graham West, Matthew I. Swindall, Ben Keener, Timothy Player, Alex C. Williams, James H. Brusuelas, John F. Wallin</author><pubDate>Fri, 26 Jan 2024 16:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16380v4</guid></item><item><title>Adjustable Visual Appearance for Generalizable Novel View Synthesis</title><link>http://arxiv.org/abs/2306.01344v3</link><description>We present a generalizable novel view synthesis method which enablesmodifying the visual appearance of an observed scene so rendered views match atarget weather or lighting condition without any scene specific training oraccess to reference views at the target condition. Our method is based on apretrained generalizable transformer architecture and is fine-tuned onsynthetically generated scenes under different appearance conditions. Thisallows for rendering novel views in a consistent manner for 3D scenes that werenot included in the training set, along with the ability to (i) modify theirappearance to match the target condition and (ii) smoothly interpolate betweendifferent conditions. Experiments on real and synthetic scenes show that ourmethod is able to generate 3D consistent renderings while making realisticappearance changes, including qualitative and quantitative comparisons. Pleaserefer to our project page for video results: https://ava-nvs.github.io/</description><author>Josef Bengtson, David Nilsson, Che-Tsung Lin, Marcel Büsching, Fredrik Kahl</author><pubDate>Fri, 26 Jan 2024 16:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01344v3</guid></item><item><title>Embedding-based search in JetBrains IDEs</title><link>http://arxiv.org/abs/2401.14975v1</link><description>Most modern Integrated Development Environments (IDEs) and code editors havea feature to search across available functionality and items in an openproject. In JetBrains IDEs, this feature is called Search Everywhere: it allowsusers to search for files, actions, classes, symbols, settings, and anythingfrom VCS history from a single entry point. However, it works with thecandidates obtained by algorithms that don't account for semantics, e.g.,synonyms, complex word permutations, part of the speech modifications, andtypos. In this work, we describe the machine learning approach we implementedto improve the discoverability of search items. We also share the obstaclesencountered during this process and how we overcame them.</description><author>Evgeny Abramov, Nikolai Palchikov</author><pubDate>Fri, 26 Jan 2024 16:07:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14975v1</guid></item><item><title>Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models</title><link>http://arxiv.org/abs/2401.14973v1</link><description>We seek to model a collection of time series arising from multiple entitiesinteracting over the same time period. Recent work focused on modelingindividual time series is inadequate for our intended applications, wherecollective system-level behavior influences the trajectories of individualentities. To address such problems, we present a new hierarchicalswitching-state model that can be trained in an unsupervised fashion tosimultaneously explain both system-level and individual-level dynamics. Weemploy a latent system-level discrete state Markov chain that drives latententity-level chains which in turn govern the dynamics of each observed timeseries. Feedback from the observations to the chains at both the entity andsystem levels improves flexibility via context-dependent state transitions. Ourhierarchical switching recurrent dynamical models can be learned viaclosed-form variational coordinate ascent updates to all latent chains thatscale linearly in the number of individual time series. This is asymptoticallyno more costly than fitting separate models for each entity. Experiments onsynthetic and real datasets show that our model can produce better forecasts offuture entity behavior than existing methods. Moreover, the availability oflatent state chains at both the entity and system level enables interpretationof group dynamics.</description><author>Michael Wojnowicz, Preetish Rath, Eric Miller, Jeffrey Miller, Clifford Hancock, Meghan O'Donovan, Seth Elkin-Frankston, Thaddeus Brunye, Michael C. Hughes</author><pubDate>Fri, 26 Jan 2024 16:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14973v1</guid></item><item><title>Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs</title><link>http://arxiv.org/abs/2211.16468v4</link><description>Causal effect estimation from observational data is a fundamental task inempirical sciences. It becomes particularly challenging when unobservedconfounders are involved in a system. This paper focuses on front-dooradjustment -- a classic technique which, using observed mediators allows toidentify causal effects even in the presence of unobserved confounding. Whilethe statistical properties of the front-door estimation are quite wellunderstood, its algorithmic aspects remained unexplored for a long time. In2022, Jeong, Tian, and Bareinboim presented the first polynomial-time algorithmfor finding sets satisfying the front-door criterion in a given directedacyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes thenumber of variables and $m$ the number of edges of the causal graph. In ourwork, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task,which thus reaches the asymptotically optimal time complexity. This resultimplies an $O(n(n+m))$ delay enumeration algorithm of all front-door adjustmentsets, again improving previous work by a factor of $n^3$. Moreover, we providethe first linear-time algorithm for finding a minimal front-door adjustmentset. We offer implementations of our algorithms in multiple programminglanguages to facilitate practical usage and empirically validate theirfeasibility, even for large graphs.</description><author>Marcel Wienöbst, Benito van der Zander, Maciej Liśkiewicz</author><pubDate>Fri, 26 Jan 2024 16:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16468v4</guid></item><item><title>Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation</title><link>http://arxiv.org/abs/2302.14220v3</link><description>Pretrained character-level and byte-level language models have been shown tobe competitive with popular subword models across a range of Natural LanguageProcessing (NLP) tasks. However, there has been little research on theireffectiveness for neural machine translation (NMT), particularly within thepopular pretrain-then-finetune paradigm. This work performs an extensivecomparison across multiple languages and experimental conditions of character-and subword-level pretrained models (ByT5 and mT5, respectively) on NMT. Weshow the effectiveness of character-level modeling in translation, particularlyin cases where fine-tuning data is limited. In our analysis, we show howcharacter models' gains in translation quality are reflected in bettertranslations of orthographically similar words and rare words. While evaluatingthe importance of source texts in driving model predictions, we highlightword-level patterns within ByT5, suggesting an ability to modulate word-leveland character-level information during generation. We conclude by assessing theefficiency tradeoff of byte models, suggesting their usage in non-time-criticalscenarios to boost translation quality.</description><author>Lukas Edman, Gabriele Sarti, Antonio Toral, Gertjan van Noord, Arianna Bisazza</author><pubDate>Fri, 26 Jan 2024 16:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14220v3</guid></item><item><title>ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events</title><link>http://arxiv.org/abs/2309.12244v3</link><description>Children typically learn to identify and express emotions through sharingtheir stories and feelings with others, particularly their family. However, itis challenging for parents or siblings to have emotional communication withchildren since children are still developing their communication skills. Wepresent ChaCha, a chatbot that encourages and guides children to share personalevents and associated emotions. ChaCha combines a state machine and largelanguage models (LLMs) to keep the dialogue on track while carrying onfree-form conversations. Through an exploratory study with 20 children (aged8-12), we examine how ChaCha prompts children to share personal events andguides them to describe associated emotions. Participants perceived ChaCha as aclose friend and shared their stories on various topics, such as family tripsand personal achievements. Based on the findings, we discuss opportunities forleveraging LLMs to design child-friendly chatbots to support children insharing emotions.</description><author>Woosuk Seo, Chanmo Yang, Young-Ho Kim</author><pubDate>Fri, 26 Jan 2024 16:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12244v3</guid></item><item><title>Atmosphere: Context and situational-aware collaborative IoT architecture for edge-fog-cloud computing</title><link>http://arxiv.org/abs/2401.14968v1</link><description>The Internet of Things (IoT) has grown significantly in popularity,accompanied by increased capacity and lower cost of communications, andoverwhelming development of technologies. At the same time, big data andreal-time data analysis have taken on great importance and have beenaccompanied by unprecedented interest in sharing data among citizens, publicadministrations and other organisms, giving rise to what is known as theCollaborative Internet of Things. This growth in data and infrastructure mustbe accompanied by a software architecture that allows its exploitation.Although there are various proposals focused on the exploitation of the IoT atedge, fog and/or cloud levels, it is not easy to find a software solution thatexploits the three tiers together, taking maximum advantage not only of theanalysis of contextual and situational data at each tier, but also of two-waycommunications between adjacent ones. In this paper, we propose an architecturethat solves these deficiencies by proposing novel technologies which areappropriate for managing the resources of each tier: edge, fog and cloud. Inaddition, the fact that two-way communications along the three tiers of thearchitecture is allowed considerably enriches the contextual and situationalinformation in each layer, and substantially assists decision making in realtime. The paper illustrates the proposed software architecture through a casestudy of respiratory disease surveillance in hospitals. As a result, theproposed architecture permits efficient communications between the differenttiers responding to the needs of these types of IoT scenarios.</description><author>Guadalupe Ortiz, Meftah Zouai, Okba Kazar, Alfonso Garcia-de-Prado, Juan Boubeta-Puig</author><pubDate>Fri, 26 Jan 2024 16:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14968v1</guid></item><item><title>Masked Pre-trained Model Enables Universal Zero-shot Denoiser</title><link>http://arxiv.org/abs/2401.14966v1</link><description>In this work, we observe that the model, which is trained on vast generalimages using masking strategy, has been naturally embedded with thedistribution knowledge regarding natural images, and thus spontaneously attainsthe underlying potential for strong image denoising. Based on this observation,we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train thenIterative fill (MPI). MPI pre-trains a model with masking and fine-tunes it fordenoising of a single image with unseen noise degradation. Concretely, theproposed MPI comprises two key procedures: 1) Masked Pre-training involvestraining a model on multiple natural images with random masks to gathergeneralizable representations, allowing for practical applications in varyingnoise degradation and even in distinct image types. 2) Iterative filling isdevised to efficiently fuse pre-trained knowledge for denoising. Similar to butdistinct from pre-training, random masking is retained to bridge the gap, butonly the predicted parts covered by masks are assembled for efficiency, whichenables high-quality denoising within a limited number of iterations.Comprehensive experiments across various noisy scenarios underscore the notableadvances of proposed MPI over previous approaches with a marked reduction ininference time. Code is available at https://github.com/krennic999/MPI.git.</description><author>Xiaoxiao Ma, Zhixiang Wei, Yi Jin, Pengyang Ling, Tianle Liu, Ben Wang, Junkang Dai, Huaian Chen, Enhong Chen</author><pubDate>Fri, 26 Jan 2024 15:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14966v1</guid></item><item><title>HiNeRV: Video Compression with Hierarchical Encoding-based Neural Representation</title><link>http://arxiv.org/abs/2306.09818v3</link><description>Learning-based video compression is currently a popular research topic,offering the potential to compete with conventional standard video codecs. Inthis context, Implicit Neural Representations (INRs) have previously been usedto represent and compress image and video content, demonstrating relativelyhigh decoding speed compared to other methods. However, existing INR-basedmethods have failed to deliver rate quality performance comparable with thestate of the art in video compression. This is mainly due to the simplicity ofthe employed network architectures, which limit their representationcapability. In this paper, we propose HiNeRV, an INR that combines light weightlayers with novel hierarchical positional encodings. We employs depth-wiseconvolutional, MLP and interpolation layers to build the deep and wide networkarchitecture with high capacity. HiNeRV is also a unified representationencoding videos in both frames and patches at the same time, which offershigher performance and flexibility than existing methods. We further build avideo codec based on HiNeRV and a refined pipeline for training, pruning andquantization that can better preserve HiNeRV's performance during lossy modelcompression. The proposed method has been evaluated on both UVG and MCL-JCVdatasets for video compression, demonstrating significant improvement over allexisting INRs baselines and competitive performance when compared tolearning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% overDCVC on the UVG dataset, measured in PSNR).</description><author>Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David Bull</author><pubDate>Fri, 26 Jan 2024 15:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09818v3</guid></item><item><title>End-To-End Set-Based Training for Neural Network Verification</title><link>http://arxiv.org/abs/2401.14961v1</link><description>Neural networks are vulnerable to adversarial attacks, i.e., small inputperturbations can result in substantially different outputs of a neuralnetwork. Safety-critical environments require neural networks that are robustagainst input perturbations. However, training and formally verifying robustneural networks is challenging. We address this challenge by employing, for thefirst time, a end-to-end set-based training procedure that trains robust neuralnetworks for formal verification. Our training procedure drastically simplifiesthe subsequent formal robustness verification of the trained neural network.While previous research has predominantly focused on augmenting neural networktraining with adversarial attacks, our approach leverages set-based computingto train neural networks with entire sets of perturbed inputs. Moreover, wedemonstrate that our set-based training procedure effectively trains robustneural networks, which are easier to verify. In many cases, set-based trainedneural networks outperform neural networks trained with state-of-the-artadversarial attacks.</description><author>Lukas Koller, Tobias Ladner, Matthias Althoff</author><pubDate>Fri, 26 Jan 2024 15:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14961v1</guid></item><item><title>Has the Virtualization of the Face Changed Facial Perception? A Study of the Impact of Augmented Reality on Facial Perception</title><link>http://arxiv.org/abs/2303.00612v2</link><description>Augmented reality and other photo editing filters are popular methods used tomodify faces online. Considering the important role of facial perception incommunication, how do we perceive this increasing number of modified faces? Inthis paper we present the results of six surveys that measure familiarity withdifferent styles of facial filters, perceived strangeness of faces edited withdifferent filters, and ability to discern whether images are filtered. Ourresults demonstrate that faces modified with more traditional face filters areperceived similarly to unmodified faces, and faces filtered with augmentedreality filters are perceived differently from unmodified faces. We discusspossible explanations for these results, including a societal adjustment totraditional photo editing techniques or the inherent differences in thedifferent types of filters. We conclude with a discussion of how to buildonline spaces more responsibly based on our results.</description><author>Louisa Conwill, Samuel Anthony, Walter Scheirer</author><pubDate>Fri, 26 Jan 2024 15:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00612v2</guid></item><item><title>Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset</title><link>http://arxiv.org/abs/2307.00818v2</link><description>In this paper, we present Motion-X, a large-scale 3D expressive whole-bodymotion dataset. Existing motion datasets predominantly contain body-only poses,lacking facial expressions, hand gestures, and fine-grained pose descriptions.Moreover, they are primarily collected from limited laboratory scenes withtextual descriptions manually labeled, which greatly limits their scalability.To overcome these limitations, we develop a whole-body motion and textannotation pipeline, which can automatically annotate motion from eithersingle- or multi-view videos and provide comprehensive semantic labels for eachvideo and fine-grained whole-body pose descriptions for each frame. Thispipeline is of high precision, cost-effective, and scalable for furtherresearch. Based on it, we construct Motion-X, which comprises 15.6M precise 3Dwhole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences frommassive scenes. Besides, Motion-X provides 15.6M frame-level whole-body posedescriptions and 81.1K sequence-level semantic labels. Comprehensiveexperiments demonstrate the accuracy of the annotation pipeline and thesignificant benefit of Motion-X in enhancing expressive, diverse, and naturalmotion generation, as well as 3D whole-body human mesh recovery.</description><author>Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, Lei Zhang</author><pubDate>Fri, 26 Jan 2024 15:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00818v2</guid></item><item><title>Learning Universal Predictors</title><link>http://arxiv.org/abs/2401.14953v1</link><description>Meta-learning has emerged as a powerful approach to train neural networks tolearn new tasks quickly from limited data. Broad exposure to different tasksleads to versatile representations enabling general problem solving. But, whatare the limits of meta-learning? In this work, we explore the potential ofamortizing the most powerful universal predictor, namely Solomonoff Induction(SI), into neural networks via leveraging meta-learning to its limits. We useUniversal Turing Machines (UTMs) to generate training data used to exposenetworks to a broad range of patterns. We provide theoretical analysis of theUTM data generation processes and meta-training protocols. We conductcomprehensive experiments with neural architectures (e.g. LSTMs, Transformers)and algorithmic data generators of varying complexity and universality. Ourresults suggest that UTM data is a valuable resource for meta-learning, andthat it can be used to train neural networks capable of learning universalprediction strategies.</description><author>Jordi Grau-Moya, Tim Genewein, Marcus Hutter, Laurent Orseau, Grégoire Delétang, Elliot Catt, Anian Ruoss, Li Kevin Wenliang, Christopher Mattern, Matthew Aitchison, Joel Veness</author><pubDate>Fri, 26 Jan 2024 15:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14953v1</guid></item><item><title>Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training</title><link>http://arxiv.org/abs/2401.14948v1</link><description>Adversarial training improves the robustness of neural networks againstadversarial attacks, albeit at the expense of the trade-off between standardand robust generalization. To unveil the underlying factors driving thisphenomenon, we examine the layer-wise learning capabilities of neural networksduring the transition from a standard to an adversarial setting. Our empiricalfindings demonstrate that selectively updating specific layers while preservingothers can substantially enhance the network's learning capacity. We thereforepropose CURE, a novel training framework that leverages a gradient prominencecriterion to perform selective conservation, updating, and revision of weights.Importantly, CURE is designed to be dataset- and architecture-agnostic,ensuring its applicability across various scenarios. It effectively tacklesboth memorization and overfitting issues, thus enhancing the trade-off betweenrobustness and generalization and additionally, this training approach alsoaids in mitigating "robust overfitting". Furthermore, our study providesvaluable insights into the mechanisms of selective adversarial training andoffers a promising avenue for future research.</description><author>Shruthi Gowda, Bahram Zonooz, Elahe Arani</author><pubDate>Fri, 26 Jan 2024 15:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14948v1</guid></item><item><title>Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock Recommendation via Split Variational Adversarial Training</title><link>http://arxiv.org/abs/2304.11043v2</link><description>In the stock market, a successful investment requires a good balance betweenprofits and risks. Based on the learning to rank paradigm, stock recommendationhas been widely studied in quantitative finance to recommend stocks with higherreturn ratios for investors. Despite the efforts to make profits, many existingrecommendation approaches still have some limitations in risk control, whichmay lead to intolerable paper losses in practical stock investing. Toeffectively reduce risks, we draw inspiration from adversarial learning andpropose a novel Split Variational Adversarial Training (SVAT) method forrisk-aware stock recommendation. Essentially, SVAT encourages the stock modelto be sensitive to adversarial perturbations of risky stock examples andenhances the model's risk awareness by learning from perturbations. To generaterepresentative adversarial examples as risk indicators, we devise a variationalperturbation generator to model diverse risk factors. Particularly, thevariational architecture enables our method to provide a rough riskquantification for investors, showing an additional advantage ofinterpretability. Experiments on several real-world stock market datasetsdemonstrate the superiority of our SVAT method. By lowering the volatility ofthe stock recommendation model, SVAT effectively reduces investment risks andoutperforms state-of-the-art baselines by more than 30% in terms ofrisk-adjusted profits. All the experimental data and source code are availableathttps://drive.google.com/drive/folders/14AdM7WENEvIp5x5bV3zV_i4Aev21C9g6?usp=sharing.</description><author>Jiezhu Cheng, Kaizhu Huang, Zibin Zheng</author><pubDate>Fri, 26 Jan 2024 15:32:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11043v2</guid></item><item><title>DAM: Diffusion Activation Maximization for 3D Global Explanations</title><link>http://arxiv.org/abs/2401.14938v1</link><description>In recent years, the performance of point cloud models has been rapidlyimproved. However, due to the limited amount of relevant explainabilitystudies, the unreliability and opacity of these black-box models may lead topotential risks in applications where human lives are at stake, e.g. autonomousdriving or healthcare. This work proposes a DDPM-based point cloud globalexplainability method (DAM) that leverages Point Diffusion Transformer (PDT), anovel point-wise symmetric model, with dual-classifier guidance to generatehigh-quality global explanations. In addition, an adapted path gradientintegration method for DAM is proposed, which not only provides a globaloverview of the saliency maps for point cloud categories, but also sheds lighton how the attributions of the explanations vary during the generation process.Extensive experiments indicate that our method outperforms existing ones interms of perceptibility, representativeness, and diversity, with a significantreduction in generation time. Our code is available at:https://github.com/Explain3D/DAM</description><author>Hanxiao Tan</author><pubDate>Fri, 26 Jan 2024 15:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14938v1</guid></item><item><title>Modification-Fair Cluster Editing</title><link>http://arxiv.org/abs/2112.03183v2</link><description>The classic Cluster Editing problem (also known as Correlation Clustering)asks to transform a given graph into a disjoint union of cliques (clusters) bya small number of edge modifications. When applied to vertex-colored graphs(the colors representing subgroups), standard algorithms for the NP-hardCluster Editing problem may yield solutions that are biased towards subgroupsof data (e.g., demographic groups), measured in the number of modificationsincident to the members of the subgroups. We propose a modification fairnessconstraint which ensures that the number of edits incident to each subgroup isproportional to its size. To start with, we study Modification-Fair ClusterEditing for graphs with two vertex colors. We show that the problem is NP-hardeven if one may only insert edges within a subgroup; note that in the classic"non-fair" setting, this case is trivially polynomial-time solvable. However,in the more general editing form, the modification-fair variant remainsfixed-parameter tractable with respect to the number of edge edits. Wecomplement these and further theoretical results with an empirical analysis ofour model on real-world social networks where we find that the price ofmodification-fairness is surprisingly low, that is, the cost of optimalmodification-fair solutions differs from the cost of optimal "non-fair"solutions only by a small percentage.</description><author>Vincent Froese, Leon Kellerhals, Rolf Niedermeier</author><pubDate>Fri, 26 Jan 2024 15:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.03183v2</guid></item><item><title>SSDOnt: an Ontology for representing Single-Subject Design Studies</title><link>http://arxiv.org/abs/2401.14933v1</link><description>Background: Single-Subject Design is used in several areas such as educationand biomedicine. However, no suited formal vocabulary exists for annotating thedetailed configuration and the results of this type of research studies withthe appropriate granularity for looking for information about them. Therefore,the search for those study designs relies heavily on a syntactical search onthe abstract, keywords or full text of the publications about the study, whichentails some limitations. Objective: To present SSDOnt, a specific purposeontology for describing and annotating single-subject design studies, so thatcomplex questions can be asked about them afterwards. Methods: The ontology wasdeveloped following the NeOn methodology. Once the requirements of the ontologywere defined, a formal model was described in a Description Logic and laterimplemented in the ontology language OWL 2 DL. Results: We show how theontology provides a reference model with a suitable terminology for theannotation and searching of single-subject design studies and their maincomponents, such as the phases, the intervention types, the outcomes and theresults. Some mappings with terms of related ontologies have been established.We show as proof-of-concept that classes in the ontology can be easily extendedto annotate more precise information about specific interventions and outcomessuch as those related to autism. Moreover, we provide examples of some types ofqueries that can be posed to the ontology. Conclusions: SSDOnt has achieved thepurpose of covering the descriptions of the domain of single-subject researchstudies.</description><author>Idoia Berges, Jesús Bermúdez, Arantza Illarramendi</author><pubDate>Fri, 26 Jan 2024 15:11:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14933v1</guid></item><item><title>Do LLMs Dream of Ontologies?</title><link>http://arxiv.org/abs/2401.14931v1</link><description>Large language models (LLMs) have recently revolutionized automated textunderstanding and generation. The performance of these models relies on thehigh number of parameters of the underlying neural architectures, which allowsLLMs to memorize part of the vast quantity of data seen during the training.This paper investigates whether and to what extent general-purpose pre-trainedLLMs have memorized information from known ontologies. Our results show thatLLMs partially know ontologies: they can, and do indeed, memorize concepts fromontologies mentioned in the text, but the level of memorization of theirconcepts seems to vary proportionally to their popularity on the Web, theprimary source of their training material. We additionally propose new metricsto estimate the degree of memorization of ontological information in LLMs bymeasuring the consistency of the output produced across different promptrepetitions, query languages, and degrees of determinism.</description><author>Marco Bombieri, Paolo Fiorini, Simone Paolo Ponzetto, Marco Rospocher</author><pubDate>Fri, 26 Jan 2024 15:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14931v1</guid></item><item><title>Decoupled Prioritized Resampling for Offline RL</title><link>http://arxiv.org/abs/2306.05412v3</link><description>Offline reinforcement learning (RL) is challenged by the distributional shiftproblem. To address this problem, existing works mainly focus on designingsophisticated policy constraints between the learned policy and the behaviorpolicy. However, these constraints are applied equally to well-performing andinferior actions through uniform sampling, which might negatively affect thelearned policy. To alleviate this issue, we propose Offline PrioritizedExperience Replay (OPER), featuring a class of priority functions designed toprioritize highly-rewarding transitions, making them more frequently visitedduring training. Through theoretical analysis, we show that this class ofpriority functions induce an improved behavior policy, and when constrained tothis improved policy, a policy-constrained offline RL algorithm is likely toyield a better solution. We develop two practical strategies to obtain priorityweights by estimating advantages based on a fitted value network (OPER-A) orutilizing trajectory returns (OPER-R) for quick computation. OPER is aplug-and-play component for offline RL algorithms. As case studies, we evaluateOPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, andIQL. Extensive experiments demonstrate that both OPER-A and OPER-Rsignificantly improve the performance for all baseline methods. Codes andpriority weights are availiable at https://github.com/sail-sg/OPER.</description><author>Yang Yue, Bingyi Kang, Xiao Ma, Qisen Yang, Gao Huang, Shiji Song, Shuicheng Yan</author><pubDate>Fri, 26 Jan 2024 15:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05412v3</guid></item><item><title>Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks</title><link>http://arxiv.org/abs/2401.14923v1</link><description>Many important behavior changes are frictionful; they require individuals toexpend effort over a long period with little immediate gratification. Here, anartificial intelligence (AI) agent can provide personalized interventions tohelp individuals stick to their goals. In these settings, the AI agent mustpersonalize rapidly (before the individual disengages) and interpretably, tohelp us understand the behavioral interventions. In this paper, we introduceBehavior Model Reinforcement Learning (BMRL), a framework in which an AI agentintervenes on the parameters of a Markov Decision Process (MDP) belonging to aboundedly rational human agent. Our formulation of the human decision-maker asa planning agent allows us to attribute undesirable human policies (ones thatdo not lead to the goal) to their maladapted MDP parameters, such as anextremely low discount factor. Furthermore, we propose a class of tractablehuman models that captures fundamental behaviors in frictionful tasks.Introducing a notion of MDP equivalence specific to BMRL, we theoretically andempirically show that AI planning with our human models can lead to helpfulpolicies on a wide range of more complex, ground-truth humans.</description><author>Eura Nofshin, Siddharth Swaroop, Weiwei Pan, Susan Murphy, Finale Doshi-Velez</author><pubDate>Fri, 26 Jan 2024 14:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14923v1</guid></item><item><title>Function Space and Critical Points of Linear Convolutional Networks</title><link>http://arxiv.org/abs/2304.05752v2</link><description>We study the geometry of linear networks with one-dimensional convolutionallayers. The function spaces of these networks can be identified withsemi-algebraic families of polynomials admitting sparse factorizations. Weanalyze the impact of the network's architecture on the function space'sdimension, boundary, and singular points. We also describe the critical pointsof the network's parameterization map. Furthermore, we study the optimizationproblem of training a network with the squared error loss. We prove that forarchitectures where all strides are larger than one and generic data, thenon-zero critical points of that optimization problem are smooth interiorpoints of the function space. This property is known to be false for denselinear networks and linear convolutional networks with stride one.</description><author>Kathlén Kohn, Guido Montúfar, Vahid Shahverdi, Matthew Trager</author><pubDate>Fri, 26 Jan 2024 14:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05752v2</guid></item><item><title>PARSAC: Accelerating Robust Multi-Model Fitting with Parallel Sample Consensus</title><link>http://arxiv.org/abs/2401.14919v1</link><description>We present a real-time method for robust estimation of multiple instances ofgeometric models from noisy data. Geometric models such as vanishing points,planar homographies or fundamental matrices are essential for 3D sceneanalysis. Previous approaches discover distinct model instances in an iterativemanner, thus limiting their potential for speedup via parallel computation. Incontrast, our method detects all model instances independently and in parallel.A neural network segments the input data into clusters representing potentialmodel instances by predicting multiple sets of sample and inlier weights. Usingthe predicted weights, we determine the model parameters for each potentialinstance separately in a RANSAC-like fashion. We train the neural network viatask-specific loss functions, i.e. we do not require a ground-truthsegmentation of the input data. As suitable training data for homography andfundamental matrix fitting is scarce, we additionally present two new syntheticdatasets. We demonstrate state-of-the-art performance on these as well asmultiple established datasets, with inference times as small as fivemilliseconds per image.</description><author>Florian Kluger, Bodo Rosenhahn</author><pubDate>Fri, 26 Jan 2024 14:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14919v1</guid></item><item><title>Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students</title><link>http://arxiv.org/abs/2401.14915v1</link><description>The increasing use of Artificial Intelligence (AI) by students in learningpresents new challenges for assessing their learning outcomes in project-basedlearning (PBL). This paper introduces a co-design study to explore thepotential of students' AI usage data as a novel material for PBL assessment. Weconducted workshops with 18 college students, encouraging them to speculate analternative world where they could freely employ AI in PBL while needing toreport this process to assess their skills and contributions. Our workshopsyielded various scenarios of students' use of AI in PBL and ways of analyzingthese uses grounded by students' vision of education goal transformation. Wealso found students with different attitudes toward AI exhibited distinctpreferences in how to analyze and understand the use of AI. Based on thesefindings, we discuss future research opportunities on student-AI interactionsand understanding AI-enhanced learning.</description><author>Chengbo Zheng, Kangyu Yuan, Bingcan Guo, Reza Hadi Mogavi, Zhenhui Peng, Shuai Ma, Xiaojuan Ma</author><pubDate>Fri, 26 Jan 2024 14:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14915v1</guid></item><item><title>DiConStruct: Causal Concept-based Explanations through Black-Box Distillation</title><link>http://arxiv.org/abs/2401.08534v4</link><description>Model interpretability plays a central role in human-AI decision-makingsystems. Ideally, explanations should be expressed using human-interpretablesemantic concepts. Moreover, the causal relations between these concepts shouldbe captured by the explainer to allow for reasoning about the explanations.Lastly, explanation methods should be efficient and not compromise theperformance of the predictive task. Despite the rapid advances in AIexplainability in recent years, as far as we know to date, no method fulfillsthese three properties. Indeed, mainstream methods for local conceptexplainability do not produce causal explanations and incur a trade-off betweenexplainability and prediction performance. We present DiConStruct, anexplanation method that is both concept-based and causal, with the goal ofcreating more interpretable local explanations in the form of structural causalmodels and concept attributions. Our explainer works as a distillation model toany black-box machine learning model by approximating its predictions whileproducing the respective explanations. Because of this, DiConStruct generatesexplanations efficiently while not impacting the black-box prediction task. Wevalidate our method on an image dataset and a tabular dataset, showing thatDiConStruct approximates the black-box models with higher fidelity than otherconcept explainability baselines, while providing explanations that include thecausal relations between the concepts.</description><author>Ricardo Moreira, Jacopo Bono, Mário Cardoso, Pedro Saleiro, Mário A. T. Figueiredo, Pedro Bizarro</author><pubDate>Fri, 26 Jan 2024 14:48:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08534v4</guid></item><item><title>MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</title><link>http://arxiv.org/abs/2305.17191v2</link><description>Contrastive self-supervised learning has gained attention for its ability tocreate high-quality representations from large unlabelled data sets. A keyreason that these powerful features enable data-efficient learning ofdownstream tasks is that they provide augmentation invariance, which is often auseful inductive bias. However, the amount and type of invariances preferred isnot known apriori, and varies across different downstream tasks. We thereforepropose a multi-task self-supervised framework (MT-SLVR) that learns bothvariant and invariant features in a parameter-efficient manner. Our multi-taskrepresentation provides a strong and flexible feature that benefits diversedownstream tasks. We evaluate our approach on few-shot classification tasksdrawn from a variety of audio domains and demonstrate improved classificationperformance on all of them</description><author>Calum Heggan, Tim Hospedales, Sam Budgett, Mehrdad Yaghoobi</author><pubDate>Fri, 26 Jan 2024 14:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17191v2</guid></item><item><title>Learning Local Control Barrier Functions for Safety Control of Hybrid Systems</title><link>http://arxiv.org/abs/2401.14907v1</link><description>Hybrid dynamical systems are ubiquitous as practical robotic applicationsoften involve both continuous states and discrete switchings. Safety is aprimary concern for hybrid robotic systems. Existing safety-critical controlapproaches for hybrid systems are either computationally inefficient,detrimental to system performance, or limited to small-scale systems. To amendthese drawbacks, in this paper, we propose a learningenabled approach toconstruct local Control Barrier Functions (CBFs) to guarantee the safety of awide class of nonlinear hybrid dynamical systems. The end result is a safeneural CBFbased switching controller. Our approach is computationallyefficient, minimally invasive to any reference controller, and applicable tolarge-scale systems. We empirically evaluate our framework and demonstrate itsefficacy and flexibility through two robotic examples including ahigh-dimensional autonomous racing case, against other CBF-based approaches andmodel predictive control.</description><author>Shuo Yang, Yu Chen, Xiang Yin, Rahul Mangharam</author><pubDate>Fri, 26 Jan 2024 14:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14907v1</guid></item><item><title>Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning</title><link>http://arxiv.org/abs/2305.15260v3</link><description>Training offline reinforcement learning (RL) models using visual inputs posestwo significant challenges, i.e., the overfitting problem in representationlearning and the overestimation bias for expected future rewards. Recent workhas attempted to alleviate the overestimation bias by encouraging conservativebehaviors. This paper, in contrast, tries to build more flexible constraintsfor value estimation without impeding the exploration of potential advantages.The key idea is to leverage off-the-shelf RL simulators, which can be easilyinteracted with in an online manner, as the "test bed" for offline policies. Toenable effective online-to-offline knowledge transfer, we introduce CoWorld, amodel-based RL approach that mitigates cross-domain discrepancies in state andreward spaces. Experimental results demonstrate the effectiveness of CoWorld,outperforming existing RL approaches by large margins.</description><author>Qi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, Xiaokang Yang</author><pubDate>Fri, 26 Jan 2024 14:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15260v3</guid></item><item><title>WeatherBench 2: A benchmark for the next generation of data-driven global weather models</title><link>http://arxiv.org/abs/2308.15560v2</link><description>WeatherBench 2 is an update to the global, medium-range (1-14 day) weatherforecasting benchmark proposed by Rasp et al. (2020), designed with the aim toaccelerate progress in data-driven weather modeling. WeatherBench 2 consists ofan open-source evaluation framework, publicly available training, ground truthand baseline data as well as a continuously updated website with the latestmetrics and state-of-the-art models:https://sites.research.google/weatherbench. This paper describes the designprinciples of the evaluation framework and presents results for currentstate-of-the-art physical and data-driven weather models. The metrics are basedon established practices for evaluating weather forecasts at leadingoperational weather centers. We define a set of headline scores to provide anoverview of model performance. In addition, we also discuss caveats in thecurrent evaluation setup and challenges for the future of data-driven weatherforecasting.</description><author>Stephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter Battaglia, Tyler Russel, Alvaro Sanchez-Gonzalez, Vivian Yang, Rob Carver, Shreya Agrawal, Matthew Chantry, Zied Ben Bouallegue, Peter Dueben, Carla Bromberg, Jared Sisk, Luke Barrington, Aaron Bell, Fei Sha</author><pubDate>Fri, 26 Jan 2024 14:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15560v2</guid></item><item><title>MPTQ-ViT:Mixed-PrecisionPost-TrainingQuantizationforVisionTransformer</title><link>http://arxiv.org/abs/2401.14895v1</link><description>While vision transformers (ViTs) have shown great potential in computervision tasks, their intense computation and memory requirements pose challengesfor practical applications. Existing post-training quantization methodsleverage value redistribution or specialized quantizers to address thenon-normal distribution in ViTs. However, without considering the asymmetry inactivations and relying on hand-crafted settings, these methods often struggleto maintain performance under low-bit quantization. To overcome thesechallenges, we introduce SmoothQuant with bias term (SQ-b) to alleviate theasymmetry issue and reduce the clamping loss. We also introduce optimal scalingfactor ratio search (OPT-m) to determine quantization parameters by adata-dependent mechanism automatically. To further enhance the compressibility,we incorporate the above-mentioned techniques and propose a mixed-precisionpost-training quantization framework for vision transformers (MPTQ-ViT). Wedevelop greedy mixed-precision quantization (Greedy MP) to allocate layer-wisebit-width considering both model performance and compressibility. Ourexperiments on ViT, DeiT, and Swin demonstrate significant accuracyimprovements compared with SOTA on the ImageNet dataset. Specifically, ourproposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fullyquantized ViTs with mixed-precision.</description><author>Yu-Shan Tai, An-Yeu, Wu</author><pubDate>Fri, 26 Jan 2024 14:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14895v1</guid></item><item><title>Networked Communication for Decentralised Agents in Mean-Field Games</title><link>http://arxiv.org/abs/2306.02766v2</link><description>We introduce networked communication to the mean-field game framework, inparticular to oracle-free settings where $N$ decentralised agents learn along asingle, non-episodic evolution path of the empirical system. We prove that ourarchitecture, with only a few reasonable assumptions about network structure,has sample guarantees bounded between those of the centralised- andindependent-learning cases. We discuss how the sample guarantees of the threetheoretical algorithms do not actually result in practical convergence.Accordingly, we show that in practical settings where the theoreticalparameters are not observed (leading to poor estimation of the Q-function), ourcommunication scheme significantly accelerates convergence over the independentcase, without relying on the undesirable assumption of a centralisedcontroller. We contribute several further practical enhancements to all threetheoretical algorithms, allowing us to showcase their first empiricaldemonstrations. Our experiments confirm that we can remove several of the keytheoretical assumptions of the algorithms, and display the empiricalconvergence benefits brought by our new networked communication. Weadditionally show that the networked approach has significant advantages, overboth the centralised and independent alternatives, in terms of robustness tounexpected learning failures and to changes in population size.</description><author>Patrick Benjamin, Alessandro Abate</author><pubDate>Fri, 26 Jan 2024 14:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02766v2</guid></item><item><title>A structured regression approach for evaluating model performance across intersectional subgroups</title><link>http://arxiv.org/abs/2401.14893v1</link><description>Disaggregated evaluation is a central task in AI fairness assessment, withthe goal to measure an AI system's performance across different subgroupsdefined by combinations of demographic or other sensitive attributes. Thestandard approach is to stratify the evaluation data across subgroups andcompute performance metrics separately for each group. However, even formoderately-sized evaluation datasets, sample sizes quickly get small onceconsidering intersectional subgroups, which greatly limits the extent to whichintersectional groups are considered in many disaggregated evaluations. In thiswork, we introduce a structured regression approach to disaggregated evaluationthat we demonstrate can yield reliable system performance estimates even forvery small subgroups. We also provide corresponding inference strategies forconstructing confidence intervals and explore how goodness-of-fit testing canyield insight into the structure of fairness-related harms experienced byintersectional groups. We evaluate our approach on two publicly availabledatasets, and several variants of semi-synthetic data. The results show thatour method is considerably more accurate than the standard approach, especiallyfor small subgroups, and goodness-of-fit testing helps identify the key factorsthat drive differences in performance.</description><author>Christine Herlihy, Kimberly Truong, Alexandra Chouldechova, Miroslav Dudik</author><pubDate>Fri, 26 Jan 2024 14:21:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14893v1</guid></item><item><title>Sparse random hypergraphs: Non-backtracking spectra and community detection</title><link>http://arxiv.org/abs/2203.07346v4</link><description>We consider the community detection problem in a sparse $q$-uniformhypergraph $G$, assuming that $G$ is generated according to the HypergraphStochastic Block Model (HSBM). We prove that a spectral method based on thenon-backtracking operator for hypergraphs works with high probability down tothe generalized Kesten-Stigum detection threshold conjectured by Angelini etal. (2015). We characterize the spectrum of the non-backtracking operator forthe sparse HSBM and provide an efficient dimension reduction procedure usingthe Ihara-Bass formula for hypergraphs. As a result, community detection forthe sparse HSBM on $n$ vertices can be reduced to an eigenvector problem of a$2n\times 2n$ non-normal matrix constructed from the adjacency matrix and thedegree matrix of the hypergraph. To the best of our knowledge, this is thefirst provable and efficient spectral algorithm that achieves the conjecturedthreshold for HSBMs with $r$ blocks generated according to a general symmetricprobability tensor.</description><author>Ludovic Stephan, Yizhe Zhu</author><pubDate>Fri, 26 Jan 2024 14:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07346v4</guid></item><item><title>Comparison of parameters of vowel sounds of russian and english languages</title><link>http://arxiv.org/abs/2401.14890v1</link><description>In multilingual speech recognition systems, a situation can often arise whenthe language is not known in advance, but the signal has already been receivedand is being processed. For such cases, some generalized model is needed thatwill be able to respond to phonetic differences and, depending on them,correctly recog-nize speech in the desired language. To build such a model, itis necessary to set the values of phonetic parameters, and then compare similarsounds, establishing significant differences.</description><author>V. I. Fedoseev, A. A. Konev, A. Yu. Yakimuk</author><pubDate>Fri, 26 Jan 2024 14:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14890v1</guid></item><item><title>The Power of Noise: Redefining Retrieval for RAG Systems</title><link>http://arxiv.org/abs/2401.14887v1</link><description>Retrieval-Augmented Generation (RAG) systems represent a significantadvancement over traditional Large Language Models (LLMs). RAG systems enhancetheir generation ability by incorporating external data retrieved through anInformation Retrieval (IR) phase, overcoming the limitations of standard LLMs,which are restricted to their pre-trained knowledge and limited context window.Most research in this area has predominantly concentrated on the generativeaspect of LLMs within RAG systems. Our study fills this gap by thoroughly andcritically analyzing the influence of IR components on RAG systems. This paperanalyzes which characteristics a retriever should possess for an effectiveRAG's prompt formulation, focusing on the type of documents that should beretrieved. We evaluate various elements, such as the relevance of the documentsto the prompt, their position, and the number included in the context. Ourfindings reveal, among other insights, that including irrelevant documents canunexpectedly enhance performance by more than 30% in accuracy, contradictingour initial assumption of diminished quality. These findings call fordeveloping specialized approaches tailored to the specific demands ofintegrating retrieval with language generation models and pave the way forfuture research. These results underscore the need for developing specializedstrategies to integrate retrieval with language generation models, therebylaying the groundwork for future research in this field.</description><author>Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio Silvestri</author><pubDate>Fri, 26 Jan 2024 14:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14887v1</guid></item><item><title>Neuromorphic quadratic programming for efficient and scalable model predictive control</title><link>http://arxiv.org/abs/2401.14885v1</link><description>Applications in robotics or other size-, weight- and power-constrainedautonomous systems at the edge often require real-time and low-energy solutionsto large optimization problems. Event-based and memory-integrated neuromorphicarchitectures promise to solve such optimization problems with superior energyefficiency and performance compared to conventional von Neumann architectures.Here, we present a method to solve convex continuous optimization problems withquadratic cost functions and linear constraints on Intel's scalableneuromorphic research chip Loihi 2. When applied to model predictive control(MPC) problems for the quadruped robotic platform ANYmal, this method achievesover two orders of magnitude reduction in combined energy-delay productcompared to the state-of-the-art solver, OSQP, on (edge) CPUs and GPUs withsolution times under ten milliseconds for various problem sizes. These resultsdemonstrate the benefit of non-von-Neumann architectures for robotic controlapplications.</description><author>Ashish Rao Mangalore, Gabriel Andreas Fonseca Guerra, Sumedh R. Risbud, Philipp Stratmann, Andreas Wild</author><pubDate>Fri, 26 Jan 2024 14:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14885v1</guid></item><item><title>P3LS: Partial Least Squares under Privacy Preservation</title><link>http://arxiv.org/abs/2401.14884v1</link><description>Modern manufacturing value chains require intelligent orchestration ofprocesses across company borders in order to maximize profits while fosteringsocial and environmental sustainability. However, the implementation ofintegrated, systems-level approaches for data-informed decision-making alongvalue chains is currently hampered by privacy concerns associated withcross-organizational data exchange and integration. We here proposePrivacy-Preserving Partial Least Squares (P3LS) regression, a novel federatedlearning technique that enables cross-organizational data integration andprocess modeling with privacy guarantees. P3LS involves a singular valuedecomposition (SVD) based PLS algorithm and employs removable, random masksgenerated by a trusted authority in order to protect the privacy of the datacontributed by each data holder. We demonstrate the capability of P3LS tovertically integrate process data along a hypothetical value chain consistingof three parties and to improve the prediction performance on severalprocess-related key performance indicators. Furthermore, we show the numericalequivalence of P3LS and PLS model components on simulated data and provide athorough privacy analysis of the former. Moreover, we propose a mechanism fordetermining the relevance of the contributed data to the problem beingaddressed, thus creating a basis for quantifying the contribution ofparticipants.</description><author>Du Nguyen Duy, Ramin Nikzad-Langerodi</author><pubDate>Fri, 26 Jan 2024 14:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14884v1</guid></item><item><title>Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem</title><link>http://arxiv.org/abs/2401.14876v1</link><description>The vanilla Graph Convolutional Network (GCN) uses a low-pass filter toextract low-frequency signals from graph topology, which may lead to theover-smoothing problem when GCN goes deep. To this end, various methods havebeen proposed to create an adaptive filter by incorporating an extra filter(e.g., a high-pass filter) extracted from the graph topology. However, thesemethods heavily rely on topological information and ignore the node attributespace, which severely sacrifices the expressive power of the deep GCNs,especially when dealing with disassortative graphs. In this paper, we propose across-space adaptive filter, called CSF, to produce the adaptive-frequencyinformation extracted from both the topology and attribute spaces.Specifically, we first derive a tailored attribute-based high-pass filter thatcan be interpreted theoretically as a minimizer for semi-supervised kernelridge regression. Then, we cast the topology-based low-pass filter as aMercer's kernel within the context of GCNs. This serves as a foundation forcombining it with the attribute-based filter to capture the adaptive-frequencyinformation. Finally, we derive the cross-space filter via an effectivemultiple-kernel learning strategy, which unifies the attribute-based high-passfilter and the topology-based low-pass filter. This helps to address theover-smoothing problem while maintaining effectiveness. Extensive experimentsdemonstrate that CSF not only successfully alleviates the over-smoothingproblem but also promotes the effectiveness of the node classification task.</description><author>Chen Huang, Haoyang Li, Yifan Zhang, Wenqiang Lei, Jiancheng Lv</author><pubDate>Fri, 26 Jan 2024 14:02:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14876v1</guid></item><item><title>MGTUNet: An new UNet for colon nuclei instance segmentation and quantification</title><link>http://arxiv.org/abs/2210.10981v2</link><description>Colorectal cancer (CRC) is among the top three malignant tumor types in termsof morbidity and mortality. Histopathological images are the gold standard fordiagnosing colon cancer. Cellular nuclei instance segmentation andclassification, and nuclear component regression tasks can aid in the analysisof the tumor microenvironment in colon tissue. Traditional methods are stillunable to handle both types of tasks end-to-end at the same time, and have poorprediction accuracy and high application costs. This paper proposes a new UNetmodel for handling nuclei based on the UNet framework, called MGTUNet, whichuses Mish, Group normalization and transposed convolution layer to improve thesegmentation model, and a ranger optimizer to adjust the SmoothL1Loss values.Secondly, it uses different channels to segment and classify different types ofnucleus, ultimately completing the nuclei instance segmentation andclassification task, and the nuclei component regression task simultaneously.Finally, we did extensive comparison experiments using eight segmentationmodels. By comparing the three evaluation metrics and the parameter sizes ofthe models, MGTUNet obtained 0.6254 on PQ, 0.6359 on mPQ, and 0.8695 on R2.Thus, the experiments demonstrated that MGTUNet is now a state-of-the-artmethod for quantifying histopathological images of colon cancer.</description><author>Liangrui Pan, Lian Wang, Zhichao Feng, Zhujun Xu, Liwen Xu, Shaoliang Peng</author><pubDate>Fri, 26 Jan 2024 13:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10981v2</guid></item><item><title>F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods</title><link>http://arxiv.org/abs/2401.14869v1</link><description>Large language models (LLMs) garner significant attention for theirunprecedented performance, leading to an increasing number of researchesevaluating LLMs. However, these evaluation benchmarks are limited to assessingthe instruction-following capabilities, overlooking the fundamental abilitiesthat emerge during the pre-training stage. Previous subjective evaluationmethods mainly reply on scoring by API models. However, in the absence ofreferences, large models have shown limited ability to discern subtledifferences. To bridge the gap, we propose F-Eval, a bilingual evaluationbenchmark to evaluate the fundamental abilities, including expression,commonsense and logic. The tasks in F-Eval include multi-choice objectivetasks, open-ended objective tasks, reference-based subjective tasks andreference-free subjective tasks. For reference-free subjective tasks, we devisenew evaluation methods, serving as alternatives to scoring by API models. Weconduct evaluations on 13 advanced LLMs. Results show that our evaluationmethods show higher correlation coefficients and larger distinction than otherevaluators. Additionally, we discuss the influence of different model sizes,dimensions, and normalization methods. We anticipate that F-Eval willfacilitate the study of LLMs' fundamental abilities.</description><author>Yu Sun, Keyu Chen, Shujie Wang, Qipeng Guo, Hang Yan, Xipeng Qiu, Xuanjing Huang, Dahua Lin</author><pubDate>Fri, 26 Jan 2024 13:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14869v1</guid></item><item><title>Particle-MALA and Particle-mGRAD: Gradient-based MCMC methods for high-dimensional state-space models</title><link>http://arxiv.org/abs/2401.14868v1</link><description>State-of-the-art methods for Bayesian inference in state-space models are (a)conditional sequential Monte Carlo (CSMC) algorithms; (b) sophisticated'classical' MCMC algorithms like MALA, or mGRAD from Titsias andPapaspiliopoulos (2018, arXiv:1610.09641v3 [stat.ML]). The former propose $N$particles at each time step to exploit the model's 'decorrelation-over-time'property and thus scale favourably with the time horizon, $T$ , but break downif the dimension of the latent states, $D$, is large. The latter leveragegradient-/prior-informed local proposals to scale favourably with $D$ butexhibit sub-optimal scalability with $T$ due to a lack of model-structureexploitation. We introduce methods which combine the strengths of bothapproaches. The first, Particle-MALA, spreads $N$ particles locally around thecurrent state using gradient information, thus extending MALA to $T &gt; 1$ timesteps and $N &gt; 1$ proposals. The second, Particle-mGRAD, additionallyincorporates (conditionally) Gaussian prior dynamics into the proposal, thusextending the mGRAD algorithm to $T &gt; 1$ time steps and $N &gt; 1$ proposals. Weprove that Particle-mGRAD interpolates between CSMC and Particle-MALA,resolving the 'tuning problem' of choosing between CSMC (superior for highlyinformative prior dynamics) and Particle-MALA (superior for weakly informativeprior dynamics). We similarly extend other 'classical' MCMC approaches likeauxiliary MALA, aGRAD, and preconditioned Crank-Nicolson-Langevin (PCNL) to $T&gt; 1$ time steps and $N &gt; 1$ proposals. In experiments, for both highly andweakly informative prior dynamics, our methods substantially improve upon bothCSMC and sophisticated 'classical' MCMC approaches.</description><author>Adrien Corenflos, Axel Finke</author><pubDate>Fri, 26 Jan 2024 13:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14868v1</guid></item><item><title>Implicit Neural Representation for Physics-driven Actuated Soft Bodies</title><link>http://arxiv.org/abs/2401.14861v1</link><description>Active soft bodies can affect their shape through an internal actuationmechanism that induces a deformation. Similar to recent work, this paperutilizes a differentiable, quasi-static, and physics-based simulation layer tooptimize for actuation signals parameterized by neural networks. Our keycontribution is a general and implicit formulation to control active softbodies by defining a function that enables a continuous mapping from a spatialpoint in the material space to the actuation value. This property allows us tocapture the signal's dominant frequencies, making the method discretizationagnostic and widely applicable. We extend our implicit model to mandiblekinematics for the particular case of facial animation and show that we canreliably reproduce facial expressions captured with high-quality capturesystems. We apply the method to volumetric soft bodies, human poses, and facialexpressions, demonstrating artist-friendly properties, such as simple controlover the latent space and resolution invariance at test time.</description><author>Lingchen Yang, Byungsoo Kim, Gaspard Zoss, Baran Gözcü, Markus Gross, Barbara Solenthaler</author><pubDate>Fri, 26 Jan 2024 13:42:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14861v1</guid></item><item><title>Progressive Fourier Neural Representation for Sequential Video Compilation</title><link>http://arxiv.org/abs/2306.11305v2</link><description>Neural Implicit Representation (NIR) has recently gained significantattention due to its remarkable ability to encode complex and high-dimensionaldata into representation space and easily reconstruct it through a trainablemapping function. However, NIR methods assume a one-to-one mapping between thetarget data and representation models regardless of data relevancy orsimilarity. This results in poor generalization over multiple complex data andlimits their efficiency and scalability. Motivated by continual learning, thiswork investigates how to accumulate and transfer neural implicitrepresentations for multiple complex video data over sequential encodingsessions. To overcome the limitation of NIR, we propose a novel method,Progressive Fourier Neural Representation (PFNR), that aims to find an adaptiveand compact sub-module in Fourier space to encode videos in each trainingsession. This sparsified neural encoding allows the neural network to hold freeweights, enabling an improved adaptation for future videos. In addition, whenlearning a representation for a new video, PFNR transfers the representation ofprevious videos with frozen weights. This design allows the model tocontinuously accumulate high-quality neural representations for multiple videoswhile ensuring lossless decoding that perfectly preserves the learnedrepresentations for previous videos. We validate our PFNR method on the UVG8/17and DAVIS50 video sequence benchmarks and achieve impressive performance gainsover strong continual learning baselines. The PFNR code is available athttps://github.com/ihaeyong/PFNR.git.</description><author>Haeyong Kang, Jaehong Yoon, DaHyun Kim, Sung Ju Hwang, Chang D Yoo</author><pubDate>Fri, 26 Jan 2024 13:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11305v2</guid></item><item><title>Memory-Inspired Temporal Prompt Interaction for Text-Image Classification</title><link>http://arxiv.org/abs/2401.14856v1</link><description>In recent years, large-scale pre-trained multimodal models (LMM) generallyemerge to integrate the vision and language modalities, achieving considerablesuccess in various natural language processing and computer vision tasks. Thegrowing size of LMMs, however, results in a significant computational cost forfine-tuning these models for downstream tasks. Hence, prompt-based interactionstrategy is studied to align modalities more efficiently. In this contex, wepropose a novel prompt-based multimodal interaction strategy inspired by humanmemory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Ourproposed method involves in two stages as in human memory strategy: theacquiring stage, and the consolidation and activation stage. We utilizetemporal prompts on intermediate layers to imitate the acquiring stage,leverage similarity-based prompt interaction to imitate memory consolidation,and employ prompt generation strategy to imitate memory activation. The mainstrength of our paper is that we interact the prompt vectors on intermediatelayers to leverage sufficient information exchange between modalities, withcompressed trainable parameters and memory usage. We achieve competitiveresults on several datasets with relatively small memory usage and 2.0M oftrainable parameters (about 1% of the pre-trained foundation model).</description><author>Xinyao Yu, Hao Sun, Ziwei Niu, Rui Qin, Zhenjia Bai, Yen-Wei Chen, Lanfen Lin</author><pubDate>Fri, 26 Jan 2024 13:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14856v1</guid></item><item><title>Extracting Process-Aware Decision Models from Object-Centric Process Data</title><link>http://arxiv.org/abs/2401.14847v1</link><description>Organizations execute decisions within business processes on a daily basiswhilst having to take into account multiple stakeholders who might requiremultiple point of views of the same process. Moreover, the complexity of theinformation systems running these business processes is generally high as theyare linked to databases storing all the relevant data and aspects of theprocesses. Given the presence of multiple objects within an information systemwhich support the processes in their enactment, decisions are naturallyinfluenced by both these perspectives, logged in object-centric process logs.However, the discovery of such decisions from object-centric process logs isnot straightforward as it requires to correctly link the involved objectswhilst considering the sequential constraints that business processes impose aswell as correctly discovering what a decision actually does. This paperproposes the first object-centric decision-mining algorithm called IntegratedObject-centric Decision Discovery Algorithm (IODDA). IODDA is able to discoverhow a decision is structured as well as how a decision is made. Moreover, IODDAis able to discover which activities and object types are involved in thedecision-making process. Next, IODDA is demonstrated with the first artificialknowledge-intensive process logs whose log generators are provided to theresearch community.</description><author>Alexandre Goossens, Johannes De Smedt, Jan Vanthienen</author><pubDate>Fri, 26 Jan 2024 13:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14847v1</guid></item><item><title>Understanding Domain Generalization: A Noise Robustness Perspective</title><link>http://arxiv.org/abs/2401.14846v1</link><description>Despite the rapid development of machine learning algorithms for domaingeneralization (DG), there is no clear empirical evidence that the existing DGalgorithms outperform the classic empirical risk minimization (ERM) acrossstandard benchmarks. To better understand this phenomenon, we investigatewhether there are benefits of DG algorithms over ERM through the lens of labelnoise. Specifically, our finite-sample analysis reveals that label noiseexacerbates the effect of spurious correlations for ERM, undermininggeneralization. Conversely, we illustrate that DG algorithms exhibit implicitlabel-noise robustness during finite-sample training even when spuriouscorrelation is present. Such desirable property helps mitigate spuriouscorrelations and improve generalization in synthetic experiments. However,additional comprehensive experiments on real-world benchmark datasets indicatethat label-noise robustness does not necessarily translate to betterperformance compared to ERM. We conjecture that the failure mode of ERM arisingfrom spurious correlations may be less pronounced in practice.</description><author>Rui Qiao, Bryan Kian Hsiang Low</author><pubDate>Fri, 26 Jan 2024 13:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14846v1</guid></item><item><title>Adaptive Point Transformer</title><link>http://arxiv.org/abs/2401.14845v1</link><description>The recent surge in 3D data acquisition has spurred the development ofgeometric deep learning models for point cloud processing, boosted by theremarkable success of transformers in natural language processing. While pointcloud transformers (PTs) have achieved impressive results recently, theirquadratic scaling with respect to the point cloud size poses a significantscalability challenge for real-world applications. To address this issue, wepropose the Adaptive Point Cloud Transformer (AdaPT), a standard PT modelaugmented by an adaptive token selection mechanism. AdaPT dynamically reducesthe number of tokens during inference, enabling efficient processing of largepoint clouds. Furthermore, we introduce a budget mechanism to flexibly adjustthe computational cost of the model at inference time without the need forretraining or fine-tuning separate models. Our extensive experimentalevaluation on point cloud classification tasks demonstrates that AdaPTsignificantly reduces computational complexity while maintaining competitiveaccuracy compared to standard PTs. The code for AdaPT is made publiclyavailable.</description><author>Alessandro Baiocchi, Indro Spinelli, Alessandro Nicolosi, Simone Scardapane</author><pubDate>Fri, 26 Jan 2024 13:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14845v1</guid></item><item><title>Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups</title><link>http://arxiv.org/abs/2309.13736v2</link><description>The set of functions parameterized by a linear fully-connected neural networkis a determinantal variety. We investigate the subvariety of functions that areequivariant or invariant under the action of a permutation group. Examples ofsuch group actions are translations or $90^\circ$ rotations on images. Wedescribe such equivariant or invariant subvarieties as direct products ofdeterminantal varieties, from which we deduce their dimension, degree,Euclidean distance degree, and their singularities. We fully characterizeinvariance for arbitrary permutation groups, and equivariance for cyclicgroups. We draw conclusions for the parameterization and the design ofequivariant and invariant linear networks in terms of sparsity andweight-sharing properties. We prove that all invariant linear functions can beparameterized by a single linear autoencoder with a weight-sharing propertyimposed by the cycle decomposition of the considered permutation. The space ofrank-bounded equivariant functions has several irreducible components, so itcan {\em not} be parameterized by a single network -- but each irreduciblecomponent can. Finally, we show that minimizing the squared-error loss on ourinvariant or equivariant networks reduces to minimizing the Euclidean distancefrom determinantal varieties via the Eckart--Young theorem.</description><author>Kathlén Kohn, Anna-Laura Sattelberger, Vahid Shahverdi</author><pubDate>Fri, 26 Jan 2024 13:13:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13736v2</guid></item><item><title>GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption</title><link>http://arxiv.org/abs/2401.14840v1</link><description>Machine Learning (ML) has emerged as one of data science's mosttransformative and influential domains. However, the widespread adoption of MLintroduces privacy-related concerns owing to the increasing number of maliciousattacks targeting ML models. To address these concerns, Privacy-PreservingMachine Learning (PPML) methods have been introduced to safeguard the privacyand security of ML models. One such approach is the use of HomomorphicEncryption (HE). However, the significant drawbacks and inefficiencies oftraditional HE render it impractical for highly scalable scenarios.Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption(HHE), has recently emerged, combining the strengths of symmetric cryptographyand HE to surmount these challenges. Our work seeks to introduce HHE to ML bydesigning a PPML scheme tailored for end devices. We leverage HHE as thefundamental building block to enable secure learning of classification outcomesover encrypted data, all while preserving the privacy of the input data and MLmodel. We demonstrate the real-world applicability of our construction bydeveloping and evaluating an HHE-based PPML application for classifying heartdisease based on sensitive ECG data. Notably, our evaluations revealed a slightreduction in accuracy compared to inference on plaintext data. Additionally,both the analyst and end devices experience minimal communication andcomputation costs, underscoring the practical viability of our approach. Thesuccessful integration of HHE into PPML provides a glimpse into a more secureand privacy-conscious future for machine learning on relatively constrained enddevices.</description><author>Eugene Frimpong, Khoa Nguyen, Mindaugas Budzys, Tanveer Khan, Antonis Michalas</author><pubDate>Fri, 26 Jan 2024 13:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14840v1</guid></item><item><title>Better Explain Transformers by Illuminating Important Information</title><link>http://arxiv.org/abs/2401.09972v3</link><description>Transformer-based models excel in various natural language processing (NLP)tasks, attracting countless efforts to explain their inner workings. Priormethods explain Transformers by focusing on the raw gradient and attention astoken attribution scores, where non-relevant information is often consideredduring explanation computation, resulting in confusing results. In this work,we propose highlighting the important information and eliminating irrelevantinformation by a refined information flow on top of the layer-wise relevancepropagation (LRP) method. Specifically, we consider identifying syntactic andpositional heads as important attention heads and focus on the relevanceobtained from these important heads. Experimental results demonstrate thatirrelevant information does distort output attribution scores and then shouldbe masked during explanation computation. Compared to eight baselines on bothclassification and question-answering datasets, our method consistentlyoutperforms with over 3\% to 33\% improvement on explanation metrics, providingsuperior explanation performance. Our anonymous code repository is availableat: https://github.com/LinxinS97/Mask-LRP</description><author>Linxin Song, Yan Cui, Ao Luo, Freddy Lecue, Irene Li</author><pubDate>Fri, 26 Jan 2024 13:12:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09972v3</guid></item><item><title>Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring</title><link>http://arxiv.org/abs/2401.14838v1</link><description>Driver Action Recognition (DAR) is crucial in vehicle cabin monitoringsystems. In real-world applications, it is common for vehicle cabins to beequipped with cameras featuring different modalities. However, multi-modalityfusion strategies for the DAR task within car cabins have rarely been studied.In this paper, we propose a novel yet efficient multi-modality driver actionrecognition method based on dual feature shift, named DFS. DFS first integratescomplementary features across modalities by performing modality featureinteraction. Meanwhile, DFS achieves the neighbour feature propagation withinsingle modalities, by feature shifting among temporal frames. To learn commonpatterns and improve model efficiency, DFS shares feature extracting stagesamong multiple modalities. Extensive experiments have been carried out toverify the effectiveness of the proposed DFS model on the Drive\&amp;Act dataset.The results demonstrate that DFS achieves good performance and improves theefficiency of multi-modality driver action recognition.</description><author>Dan Lin, Philip Hann Yung Lee, Yiming Li, Ruoyu Wang, Kim-Hui Yap, Bingbing Li, You Shing Ngim</author><pubDate>Fri, 26 Jan 2024 13:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14838v1</guid></item><item><title>Efficiently Quantifying Individual Agent Importance in Cooperative MARL</title><link>http://arxiv.org/abs/2312.08466v2</link><description>Measuring the contribution of individual agents is challenging in cooperativemulti-agent reinforcement learning (MARL). In cooperative MARL, teamperformance is typically inferred from a single shared global reward. Arguably,among the best current approaches to effectively measure individual agentcontributions is to use Shapley values. However, calculating these values isexpensive as the computational complexity grows exponentially with respect tothe number of agents. In this paper, we adapt difference rewards into anefficient method for quantifying the contribution of individual agents,referred to as Agent Importance, offering a linear computational complexityrelative to the number of agents. We show empirically that the computed valuesare strongly correlated with the true Shapley values, as well as the trueunderlying individual agent rewards, used as the ground truth in environmentswhere these are available. We demonstrate how Agent Importance can be used tohelp study MARL systems by diagnosing algorithmic failures discovered in priorMARL benchmarking work. Our analysis illustrates Agent Importance as a valuableexplainability component for future MARL benchmarks.</description><author>Omayma Mahjoub, Ruan de Kock, Siddarth Singh, Wiem Khlifi, Abidine Vall, Kale-ab Tessera, Arnu Pretorius</author><pubDate>Fri, 26 Jan 2024 13:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08466v2</guid></item><item><title>Text Image Inpainting via Global Structure-Guided Diffusion Models</title><link>http://arxiv.org/abs/2401.14832v1</link><description>Real-world text can be damaged by corrosion issues caused by environmental orhuman factors, which hinder the preservation of the complete styles of texts,e.g., texture and structure. These corrosion issues, such as graffiti signs andincomplete signatures, bring difficulties in understanding the texts, therebyposing significant challenges to downstream applications, e.g., scene textrecognition and signature identification. Notably, current inpaintingtechniques often fail to adequately address this problem and have difficultiesrestoring accurate text images along with reasonable and consistent styles.Formulating this as an open problem of text image inpainting, this paper aimsto build a benchmark to facilitate its study. In doing so, we establish twospecific text inpainting datasets which contain scene text images andhandwritten text images, respectively. Each of them includes images revamped byreal-life and synthetic datasets, featuring pairs of original images, corruptedimages, and other assistant information. On top of the datasets, we furtherdevelop a novel neural framework, Global Structure-guided Diffusion Model(GSDM), as a potential solution. Leveraging the global structure of the text asa prior, the proposed GSDM develops an efficient diffusion model to recoverclean texts. The efficacy of our approach is demonstrated by thorough empiricalstudy, including a substantial boost in both recognition accuracy and imagequality. These findings not only highlight the effectiveness of our method butalso underscore its potential to enhance the broader field of text imageunderstanding and processing. Code and datasets are available at:https://github.com/blackprotoss/GSDM.</description><author>Shipeng Zhu, Pengfei Fang, Chenjie Zhu, Zuoyan Zhao, Qiang Xu, Hui Xue</author><pubDate>Fri, 26 Jan 2024 13:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14832v1</guid></item><item><title>The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances</title><link>http://arxiv.org/abs/2401.14831v1</link><description>Are we heading for an iceberg with the current testing of machine vision?This work delves into the landscape of Machine Vision (MV) testing, which isheavily required in Highly Automated Driving (HAD) systems. Utilizing themetaphorical notion of navigating towards an iceberg, we discuss the potentialshortcomings concealed within current testing strategies. We emphasize theurgent need for a deeper understanding of how to deal with the opaque functionsof MV in development processes. As overlooked considerations can cost lives.Our main contribution is the hierarchical level model, which we callGranularity Grades. The model encourages a refined exploration of themulti-scaled depths of understanding about the circumstances of environments inwhich MV is intended to operate. This model aims to provide a holistic overviewof all entities that may impact MV functions, ranging from relations ofindividual entities like object attributes to entire environmental scenes. Theapplication of our model delivers a structured exploration of entities in aspecific domain, their relationships and assigning results of a MV-under-testto construct an entity-relationship graph. Through clustering patterns ofrelations in the graph general MV deficits are arguable. In Summary, our workcontributes to a more nuanced and systematized identification of deficits of aMV test object in correlation to holistic circumstances in HAD operatingdomains.</description><author>Hubert Padusinski, Thilo Braun, Christian Steinhauser, Lennart Ries, Eric Sax</author><pubDate>Fri, 26 Jan 2024 12:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14831v1</guid></item><item><title>TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts</title><link>http://arxiv.org/abs/2401.14828v1</link><description>Text-driven 3D scene editing has gained significant attention owing to itsconvenience and user-friendliness. However, existing methods still lackaccurate control of the specified appearance and location of the editing resultdue to the inherent limitations of the text description. To this end, wepropose a 3D scene editing framework, TIPEditor, that accepts both text andimage prompts and a 3D bounding box to specify the editing region. With theimage prompt, users can conveniently specify the detailed appearance/style ofthe target content in complement to the text description, enabling accuratecontrol of the appearance. Specifically, TIP-Editor employs a stepwise 2Dpersonalization strategy to better learn the representation of the existingscene and the reference image, in which a localization loss is proposed toencourage correct object placement as specified by the bounding box.Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting asthe 3D representation to facilitate local editing while keeping the backgroundunchanged. Extensive experiments have demonstrated that TIP-Editor conductsaccurate editing following the text and image prompts in the specified boundingbox region, consistently outperforming the baselines in editing quality, andthe alignment to the prompts, qualitatively and quantitatively.</description><author>Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan</author><pubDate>Fri, 26 Jan 2024 12:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14828v1</guid></item><item><title>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering</title><link>http://arxiv.org/abs/2308.12060v3</link><description>Knowledge base question answering (KBQA) is a critical yet challenging taskdue to the vast number of entities within knowledge bases and the diversity ofnatural language questions posed by users. Unfortunately, the performance ofmost KBQA models tends to decline significantly in real-world scenarios wherehigh-quality annotated data is insufficient. To mitigate the burden associatedwith manual annotation, we introduce FlexKBQA by utilizing Large LanguageModels (LLMs) as program translators for addressing the challenges inherent inthe few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithmsto sample diverse programs, such as SPARQL queries, from the knowledge base,which are subsequently converted into natural language questions via LLMs. Thissynthetic dataset facilitates training a specialized lightweight model for theKB. Additionally, to reduce the barriers of distribution shift betweensynthetic data and real user questions, FlexKBQA introduces an executionguidedself-training method to iterative leverage unlabeled user questions.Furthermore, we explore harnessing the inherent reasoning capability of LLMs toenhance the entire framework. Consequently, FlexKBQA delivers substantialflexibility, encompassing data annotation, deployment, and being domainagnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, weobserve that under the few-shot even the more challenging zero-shot scenarios,FlexKBQA achieves impressive results with a few annotations, surpassing allprevious baselines and even approaching the performance of supervised models,achieving a remarkable 93% performance relative to the fully-supervised models.We posit that FlexKBQA represents a significant advancement towards exploringbetter integration of large and lightweight models. The code is open-sourced.</description><author>Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang</author><pubDate>Fri, 26 Jan 2024 12:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12060v3</guid></item><item><title>Endowing Protein Language Models with Structural Knowledge</title><link>http://arxiv.org/abs/2401.14819v1</link><description>Understanding the relationships between protein sequence, structure andfunction is a long-standing biological challenge with manifold implicationsfrom drug design to our understanding of evolution. Recently, protein languagemodels have emerged as the preferred method for this challenge, thanks to theirability to harness large sequence databases. Yet, their reliance on expansivesequence data and parameter sets limits their flexibility and practicality inreal-world scenarios. Concurrently, the recent surge in computationallypredicted protein structures unlocks new opportunities in proteinrepresentation learning. While promising, the computational burden carried bysuch complex data still hinders widely-adopted practical applications. Toaddress these limitations, we introduce a novel framework that enhances proteinlanguage models by integrating protein structural data. Drawing from recentadvances in graph transformers, our approach refines the self-attentionmechanisms of pretrained language transformers by integrating structuralinformation with structure extractor modules. This refined model, termedProtein Structure Transformer (PST), is further pretrained on a small proteinstructure database, using the same masked language modeling objective astraditional protein language models. Empirical evaluations of PST demonstrateits superior parameter efficiency relative to protein language models, despitebeing pretrained on a dataset comprising only 542K structures. Notably, PSTconsistently outperforms the state-of-the-art foundation model for proteinsequences, ESM-2, setting a new benchmark in protein function prediction. Ourfindings underscore the potential of integrating structural information intoprotein language models, paving the way for more effective and efficientprotein modeling Code and pretrained models are available athttps://github.com/BorgwardtLab/PST.</description><author>Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten Borgwardt</author><pubDate>Fri, 26 Jan 2024 12:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14819v1</guid></item><item><title>How much can change in a year? Revisiting Evaluation in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2312.08463v2</link><description>Establishing sound experimental standards and rigour is important in anygrowing field of research. Deep Multi-Agent Reinforcement Learning (MARL) isone such nascent field. Although exciting progress has been made, MARL hasrecently come under scrutiny for replicability issues and a lack ofstandardised evaluation methodology, specifically in the cooperative setting.Although protocols have been proposed to help alleviate the issue, it remainsimportant to actively monitor the health of the field. In this work, we extendthe database of evaluation methodology previously published by containingmeta-data on MARL publications from top-rated conferences and compare thefindings extracted from this updated database to the trends identified in theirwork. Our analysis shows that many of the worrying trends in performancereporting remain. This includes the omission of uncertainty quantification, notreporting all relevant evaluation details and a narrowing of algorithmicdevelopment classes. Promisingly, we do observe a trend towards more difficultscenarios in SMAC-v1, which if continued into SMAC-v2 will encourage novelalgorithmic development. Our data indicate that replicability needs to beapproached more proactively by the MARL community to ensure trust in the fieldas we move towards exciting new frontiers.</description><author>Siddarth Singh, Omayma Mahjoub, Ruan de Kock, Wiem Khlifi, Abidine Vall, Kale-ab Tessera, Arnu Pretorius</author><pubDate>Fri, 26 Jan 2024 12:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08463v2</guid></item><item><title>ChemDFM: Dialogue Foundation Model for Chemistry</title><link>http://arxiv.org/abs/2401.14818v1</link><description>Large language models (LLMs) have established great success in the generaldomain of natural language processing. Their emerging task generalization andfree-form dialogue capabilities can greatly help to design Chemical GeneralIntelligence (CGI) to assist real-world research in chemistry. However, theexistence of specialized language and knowledge in the field of chemistry, suchas the highly informative SMILES notation, hinders the performance ofgeneral-domain LLMs in chemistry. To this end, we develop ChemDFM, the firstLLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature,textbooks, and instructions as well as various data from the general domain.Therefore, it can store, understand, and reason over chemical knowledge andlanguages while still possessing advanced free-form language comprehensioncapabilities. Extensive quantitative evaluation shows that ChemDFM cansignificantly outperform the representative open-sourced LLMs. Moreover,ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despitethe significant size difference. Further qualitative evaluations demonstratethe efficiency and effectiveness of ChemDFM in real-world research scenarios.We will open-source the ChemDFM model soon.</description><author>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Xin Chen, Kai Yu</author><pubDate>Fri, 26 Jan 2024 12:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14818v1</guid></item><item><title>Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck</title><link>http://arxiv.org/abs/2309.07200v2</link><description>Markov processes are widely used mathematical models for describing dynamicsystems in various fields. However, accurately simulating large-scale systemsat long time scales is computationally expensive due to the short time stepsrequired for accurate integration. In this paper, we introduce an inferenceprocess that maps complex systems into a simplified representational space andmodels large jumps in time. To achieve this, we propose Time-lagged InformationBottleneck (T-IB), a principled objective rooted in information theory, whichaims to capture relevant temporal features while discarding high-frequencyinformation to simplify the simulation task and minimize the inference error.Our experiments demonstrate that T-IB learns information-optimalrepresentations for accurately modeling the statistical properties and dynamicsof the original process at a selected time lag, outperforming existingtime-lagged dimensionality reduction methods.</description><author>Marco Federici, Patrick Forré, Ryota Tomioka, Bastiaan S. Veeling</author><pubDate>Fri, 26 Jan 2024 12:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07200v2</guid></item><item><title>Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness</title><link>http://arxiv.org/abs/2311.11211v2</link><description>Evidence-based medicine promises to improve the quality of healthcare byempowering medical decisions and practices with the best available evidence.The rapid growth of medical evidence, which can be obtained from varioussources, poses a challenge in collecting, appraising, and synthesizing theevidential information. Recent advancements in generative AI, exemplified bylarge language models, hold promise in facilitating the arduous task. However,developing accountable, fair, and inclusive models remains a complicatedundertaking. In this perspective, we discuss the trustworthiness of generativeAI in the context of automated summarization of medical evidence.</description><author>Gongbo Zhang, Qiao Jin, Denis Jered McInerney, Yong Chen, Fei Wang, Curtis L. Cole, Qian Yang, Yanshan Wang, Bradley A. Malin, Mor Peleg, Byron C. Wallace, Zhiyong Lu, Chunhua Weng, Yifan Peng</author><pubDate>Fri, 26 Jan 2024 12:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11211v2</guid></item><item><title>On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks</title><link>http://arxiv.org/abs/2401.14811v1</link><description>In this paper, we study the expressivity of scalar, Markovian rewardfunctions in Reinforcement Learning (RL), and identify several limitations towhat they can express. Specifically, we look at three classes of RL tasks;multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derivenecessary and sufficient conditions that describe when a problem in this classcan be expressed using a scalar, Markovian reward. Moreover, we find thatscalar, Markovian rewards are unable to express most of the instances in eachof these three classes. We thereby contribute to a more complete understandingof what standard reward functions can and cannot express. In addition to this,we also call attention to modal problems as a new class of problems, since theyhave so far not been given any systematic treatment in the RL literature. Wealso briefly outline some approaches for solving some of the problems wediscuss, by means of bespoke RL algorithms.</description><author>Joar Skalse, Alessandro Abate</author><pubDate>Fri, 26 Jan 2024 12:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14811v1</guid></item></channel></rss>