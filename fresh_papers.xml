<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 06 Oct 2023 14:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Improved Baselines with Visual Instruction Tuning</title><link>http://arxiv.org/abs/2310.03744v1</link><description>Large multimodal models (LMM) have recently shown encouraging progress withvisual instruction tuning. In this note, we show that the fully-connectedvision-language cross-modal connector in LLaVA is surprisingly powerful anddata-efficient. With simple modifications to LLaVA, namely, usingCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQAdata with simple response formatting prompts, we establish stronger baselinesthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpointuses merely 1.2M publicly available data, and finishes full training in ~1 dayon a single 8-A100 node. We hope this can make state-of-the-art LMM researchmore accessible. Code and model will be publicly available.</description><author>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee</author><pubDate>Thu, 05 Oct 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03744v1</guid></item><item><title>The Un-Kidnappable Robot: Acoustic Localization of Sneaking People</title><link>http://arxiv.org/abs/2310.03743v1</link><description>How easy is it to sneak up on a robot? We examine whether we can detectpeople using only the incidental sounds they produce as they move, even whenthey try to be quiet. We collect a robotic dataset of high-quality 4-channelaudio paired with 360 degree RGB data of people moving in different indoorsettings. We train models that predict if there is a moving person nearby andtheir location using only audio. We implement our method on a robot, allowingit to track a single person moving quietly with only passive audio sensing. Fordemonstration videos, see our project page:https://sites.google.com/view/unkidnappable-robot</description><author>Mengyu Yang, Patrick Grady, Samarth Brahmbhatt, Arun Balajee Vasudevan, Charles C. Kemp, James Hays</author><pubDate>Thu, 05 Oct 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03743v1</guid></item><item><title>ContactGen: Generative Contact Modeling for Grasp Generation</title><link>http://arxiv.org/abs/2310.03740v1</link><description>This paper presents a novel object-centric contact representation ContactGenfor hand-object interaction. The ContactGen comprises three components: acontact map indicates the contact location, a part map represents the contacthand part, and a direction map tells the contact direction within each part.Given an input object, we propose a conditional generative model to predictContactGen and adopt model-based optimization to predict diverse andgeometrically feasible grasps. Experimental results demonstrate our method cangenerate high-fidelity and diverse human grasps for various objects. Projectpage: https://stevenlsw.github.io/contactgen/</description><author>Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, Shenlong Wang</author><pubDate>Thu, 05 Oct 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03740v1</guid></item><item><title>Aligning Text-to-Image Diffusion Models with Reward Backpropagation</title><link>http://arxiv.org/abs/2310.03739v1</link><description>Text-to-image diffusion models have recently emerged at the forefront ofimage generation, powered by very large-scale unsupervised or weakly supervisedtext-to-image training datasets. Due to their unsupervised training,controlling their behavior in downstream tasks, such as maximizinghuman-perceived image quality, image-text alignment, or ethical imagegeneration, is difficult. Recent works finetune diffusion models to downstreamreward functions using vanilla reinforcement learning, notorious for the highvariance of the gradient estimators. In this paper, we propose AlignProp, amethod that aligns diffusion models to downstream reward functions usingend-to-end backpropagation of the reward gradient through the denoisingprocess. While naive implementation of such backpropagation would requireprohibitive memory resources for storing the partial derivatives of moderntext-to-image models, AlignProp finetunes low-rank adapter weight modules anduses gradient checkpointing, to render its memory usage viable. We testAlignProp in finetuning diffusion models to various objectives, such asimage-text semantic alignment, aesthetics, compressibility and controllabilityof the number of objects present, as well as their combinations. We showAlignProp achieves higher rewards in fewer training steps than alternatives,while being conceptually simpler, making it a straightforward choice foroptimizing diffusion models for differentiable reward functions of interest.Code and Visualization results are available at https://align-prop.github.io/.</description><author>Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki</author><pubDate>Thu, 05 Oct 2023 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03739v1</guid></item><item><title>Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases</title><link>http://arxiv.org/abs/2212.02648v2</link><description>We present a simple but effective method to measure and mitigate model biasescaused by reliance on spurious cues. Instead of requiring costly changes toone's data or model training, our method better utilizes the data one alreadyhas by sorting them. Specifically, we rank images within their classes based onspuriosity (the degree to which common spurious cues are present), proxied viadeep neural features of an interpretable network. With spuriosity rankings, itis easy to identify minority subpopulations (i.e. low spuriosity images) andassess model bias as the gap in accuracy between high and low spuriosityimages. One can even efficiently remove a model's bias at little cost toaccuracy by finetuning its classification head on low spuriosity images,resulting in fairer treatment of samples regardless of spuriosity. Wedemonstrate our method on ImageNet, annotating $5000$ class-featuredependencies ($630$ of which we find to be spurious) and generating a datasetof $325k$ soft segmentations for these features along the way. Having computedspuriosity rankings via the identified spurious neural features, we assessbiases for $89$ diverse models and find that class-wise biases are highlycorrelated across models. Our results suggest that model bias due to spuriousfeature reliance is influenced far more by what the model is trained on thanhow it is trained.</description><author>Mazda Moayeri, Wenxiao Wang, Sahil Singla, Soheil Feizi</author><pubDate>Thu, 05 Oct 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02648v2</guid></item><item><title>IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning</title><link>http://arxiv.org/abs/2310.02995v2</link><description>Like generic multi-task learning, continual learning has the nature ofmulti-objective optimization, and therefore faces a trade-off between theperformance of different tasks. That is, to optimize for the current taskdistribution, it may need to compromise performance on some previous tasks.This means that there exist multiple models that are Pareto-optimal atdifferent times, each addressing a distinct task performance trade-off.Researchers have discussed how to train particular models to address specifictrade-off preferences. However, existing algorithms require training overheadsproportional to the number of preferences -- a large burden when there aremultiple, possibly infinitely many, preferences. As a response, we proposeImprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updatesa knowledge base in the form of a convex hull of model parameter distributionsand (2) obtains particular models to address task trade-off preferences withzero-shot. That is, IBCL does not require any additional training overhead togenerate preference-addressing models from its knowledge base. We show thatmodels obtained by IBCL have guarantees in identifying the Pareto optimalparameters. Moreover, experiments on standard image classification and NLPtasks support this guarantee. Statistically, IBCL improves average per-taskaccuracy by at most 23\% and peak per-task accuracy by at most 15\% withrespect to the baseline methods, with steadily near-zero or positive backwardtransfer. Most importantly, IBCL significantly reduces the training overheadfrom training 1 model per preference to at most 3 models for all preferences.</description><author>Pengyuan Lu, Michele Caprio, Eric Eaton, Insup Lee</author><pubDate>Thu, 05 Oct 2023 18:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02995v2</guid></item><item><title>Stylist: Style-Driven Feature Ranking for Robust Novelty Detection</title><link>http://arxiv.org/abs/2310.03738v1</link><description>Novelty detection aims at finding samples that differ in some form from thedistribution of seen samples. But not all changes are created equal. Data cansuffer a multitude of distribution shifts, and we might want to detect onlysome types of relevant changes. Similar to works in out-of-distributiongeneralization, we propose to use the formalization of separating into semanticor content changes, that are relevant to our task, and style changes, that areirrelevant. Within this formalization, we define the robust novelty detectionas the task of finding semantic changes while being robust to styledistributional shifts. Leveraging pretrained, large-scale modelrepresentations, we introduce Stylist, a novel method that focuses on droppingenvironment-biased features. First, we compute a per-feature score based on thefeature distribution distances between environments. Next, we show that ourselection manages to remove features responsible for spurious correlations andimprove novelty detection performance. For evaluation, we adapt domaingeneralization datasets to our task and analyze the methods behaviors. Weadditionally built a large synthetic dataset where we have control over thespurious correlations degree. We prove that our selection mechanism improvesnovelty detection algorithms across multiple datasets, containing bothstylistic and content shifts.</description><author>Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu</author><pubDate>Thu, 05 Oct 2023 18:58:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03738v1</guid></item><item><title>Pattern recognition in the nucleation kinetics of non-equilibrium self-assembly</title><link>http://arxiv.org/abs/2207.06399v3</link><description>Inspired by biology's most sophisticated computer, the brain, neural networksconstitute a profound reformulation of computational principles. Remarkably,analogous high-dimensional, highly-interconnected computational architecturesalso arise within information-processing molecular systems inside living cells,such as signal transduction cascades and genetic regulatory networks. Mightneuromorphic collective modes be found more broadly in other physical andchemical processes, even those that ostensibly play non-information-processingroles such as protein synthesis, metabolism, or structural self-assembly? Herewe examine nucleation during self-assembly of multicomponent structures,showing that high-dimensional patterns of concentrations can be discriminatedand classified in a manner similar to neural network computation. Specifically,we design a set of 917 DNA tiles that can self-assemble in three alternativeways such that competitive nucleation depends sensitively on the extent ofco-localization of high-concentration tiles within the three structures. Thesystem was trained in-silico to classify a set of 18 grayscale 30 x 30 pixelimages into three categories. Experimentally, fluorescence and atomic forcemicroscopy monitoring during and after a 150-hour anneal established that alltrained images were correctly classified, while a test set of image variationsprobed the robustness of the results. While slow compared to prior biochemicalneural networks, our approach is surprisingly compact, robust, and scalable.This success suggests that ubiquitous physical phenomena, such as nucleation,may hold powerful information processing capabilities when scaled up ashigh-dimensional multicomponent systems.</description><author>Constantine Glen Evans, Jackson O'Brien, Erik Winfree, Arvind Murugan</author><pubDate>Thu, 05 Oct 2023 18:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.06399v3</guid></item><item><title>Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency</title><link>http://arxiv.org/abs/2310.03734v1</link><description>Current vision-language generative models rely on expansive corpora of pairedimage-text data to attain optimal performance and generalization capabilities.However, automatically collecting such data (e.g. via large-scale web scraping)leads to low quality and poor image-text correlation, while human annotation ismore accurate but requires significant manual effort and expense. We introduce$\textbf{ITIT}$ ($\textbf{I}$n$\textbf{T}$egrating $\textbf{I}$mage$\textbf{T}$ext): an innovative training paradigm grounded in the concept ofcycle consistency which allows vision-language training on unpaired image andtext data. ITIT is comprised of a joint image-text encoder with disjoint imageand text decoders that enable bidirectional image-to-text and text-to-imagegeneration in a single framework. During training, ITIT leverages a small setof paired image-text data to ensure its output matches the input reasonablywell in both directions. Simultaneously, the model is also trained on muchlarger datasets containing only images or texts. This is achieved by enforcingcycle consistency between the original unpaired samples and the cycle-generatedcounterparts. For instance, it generates a caption for a given input image andthen uses the caption to create an output image, and enforces similaritybetween the input and output images. Our experiments show that ITIT withunpaired datasets exhibits similar scaling behavior as using high-qualitypaired data. We demonstrate image generation and captioning performance on parwith state-of-the-art text-to-image and image-to-text models with orders ofmagnitude fewer (only 3M) paired image-text data.</description><author>Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred Barber, Dina Katabi, Guillaume Lajoie, Huiwen Chang, Dilip Krishnan</author><pubDate>Thu, 05 Oct 2023 18:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03734v1</guid></item><item><title>MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning</title><link>http://arxiv.org/abs/2310.03731v1</link><description>The recently released GPT-4 Code Interpreter has demonstrated remarkableproficiency in solving challenging math problems, primarily attributed to itsability to seamlessly reason with natural language, generate code, executecode, and continue reasoning based on the execution output. In this paper, wepresent a method to fine-tune open-source language models, enabling them to usecode for modeling and deriving math equations and, consequently, enhancingtheir mathematical reasoning abilities. We propose a method of generating noveland high-quality datasets with math problems and their code-based solutions,referred to as MathCodeInstruct. Each solution interleaves natural language,code, and execution results. We also introduce a customized supervisedfine-tuning and inference approach. This approach yields the MathCoder models,a family of models capable of generating code-based solutions for solvingchallenging math problems. Impressively, the MathCoder models achievestate-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K(83.9%) datasets, substantially outperforming other open-source alternatives.Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8Kand MATH but also outperforms GPT-4 on the competition-level MATH dataset. Thedataset and models will be released at https://github.com/mathllm/MathCoder.</description><author>Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li</author><pubDate>Thu, 05 Oct 2023 18:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03731v1</guid></item><item><title>Stochastic interpolants with data-dependent couplings</title><link>http://arxiv.org/abs/2310.03725v1</link><description>Generative models inspired by dynamical transport of measure -- such as flowsand diffusions -- construct a continuous-time map between two probabilitydensities. Conventionally, one of these is the target density, only accessiblethrough samples, while the other is taken as a simple base density that isdata-agnostic. In this work, using the framework of stochastic interpolants, weformalize how to \textit{couple} the base and the target densities. Thisenables us to incorporate information about class labels or continuousembeddings to construct dynamical transport maps that serve as conditionalgenerative models. We show that these transport maps can be learned by solvinga simple square loss regression problem analogous to the standard independentsetting. We demonstrate the usefulness of constructing dependent couplings inpractice through experiments in super-resolution and in-painting.</description><author>Michael S. Albergo, Mark Goldstein, Nicholas M. Boffi, Rajesh Ranganath, Eric Vanden-Eijnden</author><pubDate>Thu, 05 Oct 2023 18:46:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03725v1</guid></item><item><title>Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment</title><link>http://arxiv.org/abs/2308.12960v2</link><description>Large-scale pre-trained Vision Language Models (VLMs) have proven effectivefor zero-shot classification. Despite the success, most traditional VLMs-basedmethods are restricted by the assumption of partial source supervision or idealvocabularies, which rarely satisfy the open-world scenario. In this paper, weaim at a more challenging setting, Realistic Zero-Shot Classification, whichassumes no annotation but instead a broad vocabulary. To address thischallenge, we propose the Self Structural Semantic Alignment (S^3A) framework,which extracts the structural semantic information from unlabeled data whilesimultaneously self-learning. Our S^3A framework adopts a uniqueCluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groupsunlabeled data to derive structural semantics for pseudo-supervision. Our CVPRprocess includes iterative clustering on images, voting within each cluster toidentify initial class candidates from the vocabulary, generatingdiscriminative prompts with large language models to discern confusingcandidates, and realigning images and the vocabulary as structural semanticalignment. Finally, we propose to self-learn the CLIP image encoder with bothindividual and structural semantic alignment through a teacher-student learningstrategy. Our comprehensive experiments across various generic and fine-grainedbenchmarks demonstrate that the S^3A method offers substantial improvementsover existing VLMs-based approaches, achieving a more than 15% accuracyimprovement over CLIP on average. Our codes, models, and prompts are publiclyreleased at https://github.com/sheng-eatamath/S3A.</description><author>Sheng Zhang, Muzammal Naseer, Guangyi Chen, Zhiqiang Shen, Salman Khan, Kun Zhang, Fahad Khan</author><pubDate>Thu, 05 Oct 2023 18:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12960v2</guid></item><item><title>Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer</title><link>http://arxiv.org/abs/2310.03724v1</link><description>Recent research has shown that independently trained encoders and decoders,combined through a shared fixed-size representation, can achieve competitiveperformance in speech-to-text translation. In this work, we show that this typeof approach can be further improved with multilingual training. We observesignificant improvements in zero-shot cross-modal speech translation, evenoutperforming a supervised approach based on XLSR for several languages.</description><author>Paul-Ambroise Duquenne, Holger Schwenk, Benoît Sagot</author><pubDate>Thu, 05 Oct 2023 18:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03724v1</guid></item><item><title>Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance</title><link>http://arxiv.org/abs/2310.03722v1</link><description>In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$of a Gaussian distribution with unknown variance $\sigma$. Curiously, heemployed both an improper (right Haar) mixture over $\sigma$ and an improper(flat) mixture over $\mu$. Here, we elaborate carefully on the details of hisconstruction, which use generalized nonintegrable martingales and an extendedVille's inequality. While this does yield a sequential t-test, it does notyield an ``e-process'' (due to the nonintegrability of his martingale). In thispaper, we develop two new e-processes and confidence sequences for the samesetting: one is a test martingale in a reduced filtration, while the other isan e-process in the canonical data filtration. These are respectively obtainedby swapping Lai's flat mixture for a Gaussian mixture, and swapping the rightHaar mixture over $\sigma$ with the maximum likelihood estimate under the null,as done in universal inference. We also analyze the width of resultingconfidence sequences, which have a curious dependence on the error probability$\alpha$. Numerical experiments are provided along the way to compare andcontrast the various approaches.</description><author>Hongjian Wang, Aaditya Ramdas</author><pubDate>Thu, 05 Oct 2023 18:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03722v1</guid></item><item><title>Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance</title><link>http://arxiv.org/abs/2305.20057v3</link><description>Multi-objective learning (MOL) problems often arise in emerging machinelearning problems when there are multiple learning criteria, data modalities,or learning tasks. Different from single-objective learning, one of thecritical challenges in MOL is the potential conflict among different objectivesduring the iterative optimization process. Recent works have developed variousdynamic weighting algorithms for MOL such as MGDA and its variants, where thecentral idea is to find an update direction that avoids conflicts amongobjectives. Albeit its appealing intuition, empirical studies show that dynamicweighting methods may not always outperform static ones. To understand thistheory-practical gap, we focus on a new stochastic variant of MGDA - theMulti-objective gradient with Double sampling (MoDo) algorithm, and study thegeneralization performance of the dynamic weighting-based MoDo and itsinterplay with optimization through the lens of algorithm stability. Perhapssurprisingly, we find that the key rationale behind MGDA -- updating alongconflict-avoidant direction - may hinder dynamic weighting algorithms fromachieving the optimal ${\cal O}(1/\sqrt{n})$ population risk, where $n$ is thenumber of training samples. We further demonstrate the impact of thevariability of dynamic weights on the three-way trade-off among optimization,generalization, and conflict avoidance that is unique in MOL. We showcase thegenerality of our theoretical framework by analyzing other existing stochasticMOL algorithms under the framework. Experiments on various multi-task learningbenchmarks are performed to demonstrate the practical applicability. Code isavailable at https://github.com/heshandevaka/Trade-Off-MOL.</description><author>Lisha Chen, Heshan Fernando, Yiming Ying, Tianyi Chen</author><pubDate>Thu, 05 Oct 2023 18:41:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20057v3</guid></item><item><title>HeaP: Hierarchical Policies for Web Actions using LLMs</title><link>http://arxiv.org/abs/2310.03720v1</link><description>Large language models (LLMs) have demonstrated remarkable capabilities inperforming a range of instruction following tasks in few and zero-shotsettings. However, teaching LLMs to perform tasks on the web presentsfundamental challenges -- combinatorially large open-world tasks and variationsacross web interfaces. We tackle these challenges by leveraging LLMs todecompose web tasks into a collection of sub-tasks, each of which can be solvedby a low-level, closed-loop policy. These policies constitute a shared grammaracross tasks, i.e., new web tasks can be expressed as a composition of thesepolicies. We propose a novel framework, Hierarchical Policies for Web Actionsusing LLMs (HeaP), that learns a set of hierarchical LLM prompts fromdemonstrations for planning high-level tasks and executing them via a sequenceof low-level policies. We evaluate HeaP against a range of baselines on a suiteof web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well aslive website interactions, and show that it is able to outperform prior worksusing orders of magnitude less data.</description><author>Paloma Sodhi, S. R. K. Branavan, Ryan McDonald</author><pubDate>Thu, 05 Oct 2023 18:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03720v1</guid></item><item><title>Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning</title><link>http://arxiv.org/abs/2310.03718v1</link><description>Safe reinforcement learning (RL) focuses on training reward-maximizing agentssubject to pre-defined safety constraints. Yet, learning versatile safepolicies that can adapt to varying safety constraint requirements duringdeployment without retraining remains a largely unexplored and challengingarea. In this work, we formulate the versatile safe RL problem and consider twoprimary requirements: training efficiency and zero-shot adaptation capability.To address them, we introduce the Conditioned Constrained Policy Optimization(CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation(VVE) for approximating value functions under unseen threshold conditions, and(2) Conditioned Variational Inference (CVI) for encoding arbitrary constraintthresholds during policy optimization. Our extensive experiments demonstratethat CCPO outperforms the baselines in terms of safety and task performancewhile preserving zero-shot adaptation capabilities to different constraintthresholds data-efficiently. This makes our approach suitable for real-worlddynamic applications.</description><author>Yihang Yao, Zuxin Liu, Zhepeng Cen, Jiacheng Zhu, Wenhao Yu, Tingnan Zhang, Ding Zhao</author><pubDate>Thu, 05 Oct 2023 18:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03718v1</guid></item><item><title>A Long Way to Go: Investigating Length Correlations in RLHF</title><link>http://arxiv.org/abs/2310.03716v1</link><description>Great successes have been reported using Reinforcement Learning from HumanFeedback (RLHF) to align large language models. Open-source preference datasetsand reward models have enabled wider experimentation beyond generic chatsettings, particularly to make systems more "helpful" for tasks like webquestion answering, summarization, and multi-turn dialogue. When optimizing forhelpfulness, RLHF has been consistently observed to drive models to producelonger outputs. This paper demonstrates that optimizing for response length isa significant factor behind RLHF's reported improvements in these settings.First, we study the relationship between reward and length for reward modelstrained on three open-source preference datasets for helpfulness. Here, lengthcorrelates strongly with reward, and improvements in reward score are driven inlarge part by shifting the distribution over output lengths. We then exploreinterventions during both RL and reward model learning to see if we can achievethe same downstream improvements as RLHF without increasing length. While ourinterventions mitigate length increases, they aren't uniformly effective acrosssettings. Furthermore, we find that even running RLHF with a reward basedsolely on length can reproduce most of the downstream improvements over theinitial policy model, showing that reward models in these settings have a longway to go.</description><author>Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett</author><pubDate>Thu, 05 Oct 2023 18:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03716v1</guid></item><item><title>Time-Varying Propensity Score to Bridge the Gap between the Past and Present</title><link>http://arxiv.org/abs/2210.01422v4</link><description>Real-world deployment of machine learning models is challenging because dataevolves over time. While no model can work when data evolves in an arbitraryfashion, if there is some pattern to these changes, we might be able to designmethods to address it. This paper addresses situations when data evolvesgradually. We introduce a time-varying propensity score that can detect gradualshifts in the distribution of data which allows us to selectively sample pastdata to update the model -- not just similar data from the past like that of astandard propensity score but also data that evolved in a similar fashion inthe past. The time-varying propensity score is quite general: we demonstratedifferent ways of implementing it and evaluate it on a variety of problemsranging from supervised learning (e.g., image classification problems) wheredata undergoes a sequence of gradual shifts, to reinforcement learning tasks(e.g., robotic manipulation and continuous control) where data shifts as thepolicy or the task changes.</description><author>Rasool Fakoor, Jonas Mueller, Zachary C. Lipton, Pratik Chaudhari, Alexander J. Smola</author><pubDate>Thu, 05 Oct 2023 18:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01422v4</guid></item><item><title>Artificial Intelligence Index Report 2023</title><link>http://arxiv.org/abs/2310.03715v1</link><description>Welcome to the sixth edition of the AI Index Report. This year, the reportintroduces more original data than any previous edition, including a newchapter on AI public opinion, a more thorough technical performance chapter,original analysis about large language and multimodal models, detailed trendsin global AI legislation records, a study of the environmental impact of AIsystems, and more. The AI Index Report tracks, collates, distills, andvisualizes data related to artificial intelligence. Our mission is to provideunbiased, rigorously vetted, broadly sourced data in order for policymakers,researchers, executives, journalists, and the general public to develop a morethorough and nuanced understanding of the complex field of AI. The report aimsto be the world's most credible and authoritative source for data and insightsabout AI.</description><author>Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, Raymond Perrault</author><pubDate>Thu, 05 Oct 2023 18:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03715v1</guid></item><item><title>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</title><link>http://arxiv.org/abs/2310.03714v1</link><description>The ML community is rapidly exploring techniques for prompting languagemodels (LMs) and for stacking them into pipelines that solve complex tasks.Unfortunately, existing LM pipelines are typically implemented using hard-coded"prompt templates", i.e. lengthy strings discovered via trial and error. Towarda more systematic approach for developing and optimizing LM pipelines, weintroduce DSPy, a programming model that abstracts LM pipelines as texttransformation graphs, i.e. imperative computational graphs where LMs areinvoked through declarative modules. DSPy modules are parameterized, meaningthey can learn (by creating and collecting demonstrations) how to applycompositions of prompting, finetuning, augmentation, and reasoning techniques.We design a compiler that will optimize any DSPy pipeline to maximize a givenmetric. We conduct two case studies, showing that succinct DSPy programs canexpress and optimize sophisticated LM pipelines that reason about math wordproblems, tackle multi-hop retrieval, answer complex questions, and controlagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 andllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shotprompting (generally by over 25% and 65%, respectively) and pipelines withexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On topof that, DSPy programs compiled to open and relatively small LMs like770M-parameter T5 and llama2-13b-chat are competitive with approaches that relyon expert-written prompt chains for proprietary GPT-3.5. DSPy is available athttps://github.com/stanfordnlp/dspy</description><author>Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts</author><pubDate>Thu, 05 Oct 2023 18:37:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03714v1</guid></item><item><title>Agent Instructs Large Language Models to be General Zero-Shot Reasoners</title><link>http://arxiv.org/abs/2310.03710v1</link><description>We introduce a method to improve the zero-shot reasoning abilities of largelanguage models on general language understanding tasks. Specifically, we buildan autonomous agent to instruct the reasoning process of large language models.We show this approach further unleashes the zero-shot reasoning abilities oflarge language models to more tasks. We study the performance of our method ona wide set of datasets spanning generation, classification, and reasoning. Weshow that our method generalizes to most tasks and obtains state-of-the-artzero-shot performance on 20 of the 29 datasets that we evaluate. For instance,our method boosts the performance of state-of-the-art large language models bya large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), andGPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvementin reasoning is striking, with an average increase of 10.5%. With our method,Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.</description><author>Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, Chenguang Wang</author><pubDate>Thu, 05 Oct 2023 18:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03710v1</guid></item><item><title>Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization</title><link>http://arxiv.org/abs/2310.03708v1</link><description>Language models (LMs), despite aligning well with an average labeler throughreinforcement learning from human feedback (RLHF), may not universally suitdiverse human preferences. Recent approaches therefore opt for customization bycollecting multi-dimensional feedback and creating distinct rewards for eachdimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailoredto different preferences using multi-objective RL (MORL) with different rewardweightings. Yet, RL fine-tuning is unstable and resource-heavy, especially forMORLHF with diverse and usually conflicting objectives. In this paper, wepresent Multi-Objective Direct Preference Optimization (MODPO), an RL-freealgorithm that extends Direct Preference Optimization (DPO) for multiplealignment objectives. Essentially, MODPO trains different LMs to representdifferent collective reward models that combine all objectives with specificweightings. With a simple cross-entropy loss, the LMs optimized against theMODPO objective are analytically the exact solutions of the original MORLHFobjective. Empirical results in safety alignment and long-form questionanswering confirm that MODPO matches or outperforms existing methods,efficiently producing a Pareto-optimal set of LMs that cater to diversepreferences with 3 times less computational resources compared with MORLHF.</description><author>Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao</author><pubDate>Thu, 05 Oct 2023 18:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03708v1</guid></item><item><title>OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks</title><link>http://arxiv.org/abs/2310.03707v1</link><description>Evasion Attacks (EA) are used to test the robustness of trained neuralnetworks by distorting input data to misguide the model into incorrectclassifications. Creating these attacks is a challenging task, especially withthe ever-increasing complexity of models and datasets. In this work, weintroduce a self-supervised, computationally economical method for generatingadversarial examples, designed for the unseen black-box setting. Adaptingtechniques from representation learning, our method generates on-manifold EAsthat are encouraged to resemble the data distribution. These attacks arecomparable in effectiveness compared to the state-of-the-art when attacking themodel trained on, but are significantly more effective when attacking unseenmodels, as the attacks are more related to the data rather than the modelitself. Our experiments consistently demonstrate the method is effective acrossvarious models, unseen data categories, and even defended models, suggesting asignificant role for on-manifold EAs when targeting unseen models.</description><author>Ofir Bar Tal, Adi Haviv, Amit H. Bermano</author><pubDate>Thu, 05 Oct 2023 18:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03707v1</guid></item><item><title>Towards Understanding the Effect of Pretraining Label Granularity</title><link>http://arxiv.org/abs/2303.16887v2</link><description>In this paper, we study how the granularity of pretraining labels affects thegeneralization of deep neural networks in image classification tasks. We focuson the "fine-to-coarse" transfer learning setting, where the pretraining labelspace is more fine-grained than that of the target problem. Empirically, weshow that pretraining on the leaf labels of ImageNet21k produces bettertransfer results on ImageNet1k than pretraining on other coarser granularitylevels, which supports the common practice used in the community.Theoretically, we explain the benefit of fine-grained pretraining by provingthat, for a data distribution satisfying certain hierarchy conditions, 1)coarse-grained pretraining only allows a neural network to learn the "common"or "easy-to-learn" features well, while 2) fine-grained pretraining helps thenetwork learn the "rarer" or "fine-grained" features in addition to the commonones, thus improving its accuracy on hard downstream test samples in whichcommon features are missing or weak in strength. Furthermore, we performcomprehensive experiments using the label hierarchies of iNaturalist 2021 andobserve that the following conditions, in addition to proper choice of labelgranularity, enable the transfer to work well in practice: 1) the pretrainingdataset needs to have a meaningful label hierarchy, and 2) the pretraining andtarget label functions need to align well.</description><author>Guan Zhe Hong, Yin Cui, Ariel Fuxman, Stanley H. Chan, Enming Luo</author><pubDate>Thu, 05 Oct 2023 18:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16887v2</guid></item><item><title>Conditional Generative Models for Simulation of EMG During Naturalistic Movements</title><link>http://arxiv.org/abs/2211.01856v4</link><description>Numerical models of electromyographic (EMG) signals have provided a hugecontribution to our fundamental understanding of human neurophysiology andremain a central pillar of motor neuroscience and the development ofhuman-machine interfaces. However, whilst modern biophysical simulations basedon finite element methods are highly accurate, they are extremelycomputationally expensive and thus are generally limited to modelling staticsystems such as isometrically contracting limbs. As a solution to this problem,we propose a transfer learning approach, in which a conditional generativemodel is trained to mimic the output of an advanced numerical model. To thisend, we present BioMime, a conditional generative neural network trainedadversarially to generate motor unit activation potential waveforms under awide variety of volume conductor parameters. We demonstrate the ability of sucha model to predictively interpolate between a much smaller number of numericalmodel's outputs with a high accuracy. Consequently, the computational load isdramatically reduced, which allows the rapid simulation of EMG signals duringtruly dynamic and naturalistic movements.</description><author>Shihan Ma, Alexander Kenneth Clarke, Kostiantyn Maksymenko, Samuel Deslauriers-Gauthier, Xinjun Sheng, Xiangyang Zhu, Dario Farina</author><pubDate>Thu, 05 Oct 2023 18:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01856v4</guid></item><item><title>Drag View: Generalizable Novel View Synthesis with Unposed Imagery</title><link>http://arxiv.org/abs/2310.03704v1</link><description>We introduce DragView, a novel and interactive framework for generating novelviews of unseen scenes. DragView initializes the new view from a single sourceimage, and the rendering is supported by a sparse set of unposed multi-viewimages, all seamlessly executed within a single feed-forward pass. Our approachbegins with users dragging a source view through a local relative coordinatesystem. Pixel-aligned features are obtained by projecting the sampled 3D pointsalong the target ray onto the source view. We then incorporate a view-dependentmodulation layer to effectively handle occlusion during the projection.Additionally, we broaden the epipolar attention mechanism to encompass allsource pixels, facilitating the aggregation of initialized coordinate-alignedpoint features from other unposed views. Finally, we employ another transformerto decode ray features into final pixel intensities. Crucially, our frameworkdoes not rely on either 2D prior models or the explicit estimation of cameraposes. During testing, DragView showcases the capability to generalize to newscenes unseen during training, also utilizing only unposed support images,enabling the generation of photo-realistic new views characterized by flexiblecamera trajectories. In our experiments, we conduct a comprehensive comparisonof the performance of DragView with recent scene representation networksoperating under pose-free conditions, as well as with generalizable NeRFssubject to noisy test camera poses. DragView consistently demonstrates itssuperior performance in view synthesis quality, while also being moreuser-friendly. Project page: https://zhiwenfan.github.io/DragView/.</description><author>Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Hanwen Jiang, Dejia Xu, Zehao Zhu, Dilin Wang, Zhangyang Wang</author><pubDate>Thu, 05 Oct 2023 18:24:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03704v1</guid></item><item><title>Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities</title><link>http://arxiv.org/abs/2310.03696v1</link><description>We investigate the variational optimality (specifically, the Banach spaceoptimality) of a large class of neural architectures with multivariatenonlinearities/activation functions. To that end, we construct a new family ofBanach spaces defined via a regularization operator and the $k$-planetransform. We prove a representer theorem that states that the solution sets tolearning problems posed over these Banach spaces are completely characterizedby neural architectures with multivariate nonlinearities. These optimalarchitectures have skip connections and are tightly connected to orthogonalweight normalization and multi-index models, both of which have receivedconsiderable interest in the neural network community. Our framework iscompatible with a number of classical nonlinearities including the rectifiedlinear unit (ReLU) activation function, the norm activation function, and theradial basis functions found in the theory of thin-plate/polyharmonic splines.We also show that the underlying spaces are special instances of reproducingkernel Banach spaces and variation spaces. Our results shed light on theregularity of functions learned by neural networks trained on data,particularly with multivariate nonlinearities, and provide new theoreticalmotivation for several architectural choices found in practice.</description><author>Rahul Parhi, Michael Unser</author><pubDate>Thu, 05 Oct 2023 18:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03696v1</guid></item><item><title>Multimarginal generative modeling with stochastic interpolants</title><link>http://arxiv.org/abs/2310.03695v1</link><description>Given a set of $K$ probability densities, we consider the multimarginalgenerative modeling problem of learning a joint distribution that recoversthese densities as marginals. The structure of this joint distribution shouldidentify multi-way correspondences among the prescribed marginals. We formalizean approach to this task within a generalization of the stochastic interpolantframework, leading to efficient learning algorithms built upon dynamicaltransport of measure. Our generative models are defined by velocity and scorefields that can be characterized as the minimizers of simple quadraticobjectives, and they are defined on a simplex that generalizes the timevariable in the usual dynamical transport framework. The resulting transport onthe simplex is influenced by all marginals, and we show that multi-waycorrespondences can be extracted. The identification of such correspondenceshas applications to style transfer, algorithmic fairness, and datadecorruption. In addition, the multimarginal perspective enables an efficientalgorithm for reducing the dynamical transport cost in the ordinarytwo-marginal setting. We demonstrate these capacities with several numericalexamples.</description><author>Michael S. Albergo, Nicholas M. Boffi, Michael Lindsey, Eric Vanden-Eijnden</author><pubDate>Thu, 05 Oct 2023 18:12:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03695v1</guid></item><item><title>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</title><link>http://arxiv.org/abs/2310.03693v1</link><description>Optimizing large language models (LLMs) for downstream use cases ofteninvolves the customization of pre-trained LLMs through further fine-tuning.Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5Turbo on custom datasets also encourage this practice. But, what are the safetycosts associated with such custom fine-tuning? We note that while existingsafety alignment infrastructures can restrict harmful behaviors of LLMs atinference time, they do not cover safety risks when fine-tuning privileges areextended to end-users. Our red teaming studies find that the safety alignmentof LLMs can be compromised by fine-tuning with only a few adversariallydesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safetyguardrails by fine-tuning it on only 10 such examples at a cost of less than$0.20 via OpenAI's APIs, making the model responsive to nearly any harmfulinstructions. Disconcertingly, our research also reveals that, even withoutmalicious intent, simply fine-tuning with benign and commonly used datasets canalso inadvertently degrade the safety alignment of LLMs, though to a lesserextent. These findings suggest that fine-tuning aligned LLMs introduces newsafety risks that current safety infrastructures fall short of addressing --even if a model's initial safety alignment is impeccable, it is not necessarilyto be maintained after custom fine-tuning. We outline and critically analyzepotential mitigations and advocate for further research efforts towardreinforcing safety protocols for the custom fine-tuning of aligned LLMs.</description><author>Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson</author><pubDate>Thu, 05 Oct 2023 18:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03693v1</guid></item><item><title>Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries</title><link>http://arxiv.org/abs/2310.03687v1</link><description>Due to limited resources and fast economic growth, designing optimaltransportation road networks with traffic simulation and validation in acost-effective manner is vital for developing countries, where extensive manualtesting is expensive and often infeasible. Current rule-based road designgenerators lack diversity, a key feature for design robustness. Generative FlowNetworks (GFlowNets) learn stochastic policies to sample from an unnormalizedreward distribution, thus generating high-quality solutions while preservingtheir diversity. In this work, we formulate the problem of linking incidentroads to the circular junction of a roundabout by a Markov decision process,and we leverage GFlowNets as the Junction-Art road generator. We compare ourmethod with related methods and our empirical results show that our methodachieves better diversity while preserving a high validity score.</description><author>Zarif Ikram, Ling Pan, Dianbo Liu</author><pubDate>Thu, 05 Oct 2023 18:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03687v1</guid></item><item><title>DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers</title><link>http://arxiv.org/abs/2310.03686v1</link><description>In recent years, many interpretability methods have been proposed to helpinterpret the internal states of Transformer-models, at different levels ofprecision and complexity. Here, to analyze encoder-decoder Transformers, wepropose a simple, new method: DecoderLens. Inspired by the LogitLens (fordecoder-only Transformers), this method involves allowing the decoder tocross-attend representations of intermediate encoder layers instead of usingthe final encoder output, as is normally done in encoder-decoder models. Themethod thus maps previously uninterpretable vector representations tohuman-interpretable sequences of words or symbols. We report results from theDecoderLens applied to models trained on question answering, logical reasoning,speech recognition and machine translation. The DecoderLens reveals severalspecific subtasks that are solved at low or intermediate layers, shedding newlight on the information flow inside the encoder component of this importantclass of models.</description><author>Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, Jaap Jumelet</author><pubDate>Thu, 05 Oct 2023 18:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03686v1</guid></item><item><title>SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks</title><link>http://arxiv.org/abs/2310.03684v1</link><description>Despite efforts to align large language models (LLMs) with human values,widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible tojailbreaking attacks, wherein an adversary fools a targeted LLM into generatingobjectionable content. To address this vulnerability, we propose SmoothLLM, thefirst algorithm designed to mitigate jailbreaking attacks on LLMs. Based on ourfinding that adversarially-generated prompts are brittle to character-levelchanges, our defense first randomly perturbs multiple copies of a given inputprompt, and then aggregates the corresponding predictions to detect adversarialinputs. SmoothLLM reduces the attack success rate on numerous popular LLMs tobelow one percentage point, avoids unnecessary conservatism, and admitsprovable guarantees on attack mitigation. Moreover, our defense usesexponentially fewer queries than existing attacks and is compatible with anyLLM.</description><author>Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas</author><pubDate>Thu, 05 Oct 2023 18:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03684v1</guid></item><item><title>ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal</title><link>http://arxiv.org/abs/2310.00818v2</link><description>Electrocardiogram (ECG) is an essential signal in monitoring human heartactivities. Researchers have achieved promising results in leveraging ECGs inclinical applications with deep learning models. However, the mainstream deeplearning approaches usually neglect the periodic and formative attribute of theECG heartbeat waveform. In this work, we propose a novel ECG-Segment basedLearning (ECG-SL) framework to explicitly model the periodic nature of ECGsignals. More specifically, ECG signals are first split into heartbeatsegments, and then structural features are extracted from each of the segments.Based on the structural features, a temporal model is designed to learn thetemporal information for various clinical tasks. Further, due to the fact thatmassive ECG signals are available but the labeled data are very limited, wealso explore self-supervised learning strategy to pre-train the models,resulting significant improvement for downstream tasks. The proposed methodoutperforms the baseline model and shows competitive performances compared withtask-specific methods in three clinical applications: cardiac conditiondiagnosis, sleep apnea detection, and arrhythmia classification. Further, wefind that the ECG-SL tends to focus more on each heartbeat's peak and ST rangethan ResNet by visualizing the saliency maps.</description><author>Han Yu, Huiyuan Yang, Akane Sano</author><pubDate>Thu, 05 Oct 2023 18:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00818v2</guid></item><item><title>Hadamard Domain Training with Integers for Class Incremental Quantized Learning</title><link>http://arxiv.org/abs/2310.03675v1</link><description>Continual learning is a desirable feature in many modern machine learningapplications, which allows in-field adaptation and updating, ranging fromaccommodating distribution shift, to fine-tuning, and to learning new tasks.For applications with privacy and low latency requirements, the compute andmemory demands imposed by continual learning can be cost-prohibitive forresource-constraint edge platforms. Reducing computational precision throughfully quantized training (FQT) simultaneously reduces memory footprint andincreases compute efficiency for both training and inference. However,aggressive quantization especially integer FQT typically degrades modelaccuracy to unacceptable levels. In this paper, we propose a technique thatleverages inexpensive Hadamard transforms to enable low-precision training withonly integer matrix multiplications. We further determine which tensors needstochastic rounding and propose tiled matrix multiplication to enable low-bitwidth accumulators. We demonstrate the effectiveness of our technique onseveral human activity recognition datasets and CIFAR100 in a class incrementallearning setting. We achieve less than 0.5% and 3% accuracy degradation whilewe quantize all matrix multiplications inputs down to 4-bits with 8-bitaccumulators.</description><author>Martin Schiemer, Clemens JS Schaefer, Jayden Parker Vap, Mark James Horeni, Yu Emma Wang, Juan Ye, Siddharth Joshi</author><pubDate>Thu, 05 Oct 2023 17:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03675v1</guid></item><item><title>Non-Asymptotic Analysis of Ensemble Kalman Updates: Effective Dimension and Localization</title><link>http://arxiv.org/abs/2208.03246v3</link><description>Many modern algorithms for inverse problems and data assimilation rely onensemble Kalman updates to blend prior predictions with observed data. EnsembleKalman methods often perform well with a small ensemble size, which isessential in applications where generating each particle is costly. This paperdevelops a non-asymptotic analysis of ensemble Kalman updates that rigorouslyexplains why a small ensemble size suffices if the prior covariance hasmoderate effective dimension due to fast spectrum decay or approximatesparsity. We present our theory in a unified framework, comparing severalimplementations of ensemble Kalman updates that use perturbed observations,square root filtering, and localization. As part of our analysis, we developnew dimension-free covariance estimation bounds for approximately sparsematrices that may be of independent interest.</description><author>Omar Al Ghattas, Daniel Sanz-Alonso</author><pubDate>Thu, 05 Oct 2023 17:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.03246v3</guid></item><item><title>Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</title><link>http://arxiv.org/abs/2310.02244v3</link><description>By classifying infinite-width neural networks and identifying the *optimal*limit, Tensor Programs IV and V demonstrated a universal way, called $\mu$P,for *widthwise hyperparameter transfer*, i.e., predicting optimalhyperparameters of wide neural networks from narrow ones. Here we investigatethe analogous classification for *depthwise parametrizations* of deep residualnetworks (resnets). We classify depthwise parametrizations of block multiplierand learning rate by their infinite-width-then-depth limits. In resnets whereeach block has only one layer, we identify a unique optimal parametrization,called Depth-$\mu$P that extends $\mu$P and show empirically it admitsdepthwise hyperparameter transfer. We identify *feature diversity* as a crucialfactor in deep networks, and Depth-$\mu$P can be characterized as maximizingboth feature learning and feature diversity. Exploiting this, we find thatabsolute value, among all homogeneous nonlinearities, maximizes featurediversity and indeed empirically leads to significantly better performance.However, if each block is deeper (such as modern transformers), then we findfundamental limitations in all possible infinite-depth limits of suchparametrizations, which we illustrate both theoretically and empirically onsimple networks as well as Megatron transformer trained on Common Crawl.</description><author>Greg Yang, Dingli Yu, Chen Zhu, Soufiane Hayou</author><pubDate>Thu, 05 Oct 2023 17:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02244v3</guid></item><item><title>LumiNet: The Bright Side of Perceptual Knowledge Distillation</title><link>http://arxiv.org/abs/2310.03669v1</link><description>In knowledge distillation research, feature-based methods have dominated dueto their ability to effectively tap into extensive teacher models. In contrast,logit-based approaches are considered to be less adept at extracting hidden'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novelknowledge-transfer algorithm designed to enhance logit-based distillation. Weintroduce a perception matrix that aims to recalibrate logits throughadjustments based on the model's representation capability. By meticulouslyanalyzing intra-class dynamics, LumiNet reconstructs more granular inter-classrelationships, enabling the student model to learn a richer breadth ofknowledge. Both teacher and student models are mapped onto this refined matrix,with the student's goal being to minimize representational discrepancies.Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)attests to LumiNet's efficacy, revealing its competitive edge over leadingfeature-based methods. Moreover, in exploring the realm of transfer learning,we assess how effectively the student model, trained using our method, adaptsto downstream tasks. Notably, when applied to Tiny ImageNet, the transferredfeatures exhibit remarkable performance, further underscoring LumiNet'sversatility and robustness in diverse settings. With LumiNet, we hope to steerthe research discourse towards a renewed interest in the latent capabilities oflogit-based knowledge distillation.</description><author>Md. Ismail Hossain, M M Lutfe Elahi, Sameera Ramasinghe, Ali Cheraghian, Fuad Rahman, Nabeel Mohammed, Shafin Rahman</author><pubDate>Thu, 05 Oct 2023 17:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03669v1</guid></item><item><title>GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction</title><link>http://arxiv.org/abs/2310.03668v1</link><description>Large Language Models (LLMs) combined with instruction tuning have madesignificant progress when generalizing to unseen tasks. However, they have beenless successful in Information Extraction (IE), lagging behind task-specificmodels. Typically, IE tasks are characterized by complex annotation guidelineswhich describe the task and give examples to humans. Previous attempts toleverage such information have failed, even with the largest models, as theyare not able to follow the guidelines out-of-the-box. In this paper we proposeGoLLIE (Guideline-following Large Language Model for IE), a model able toimprove zero-shot results on unseen IE tasks by virtue of being fine-tuned tocomply with annotation guidelines. Comprehensive evaluation empiricallydemonstrates that GoLLIE is able to generalize to and follow unseen guidelines,outperforming previous attempts at zero-shot information extraction. Theablation study shows that detailed guidelines is key for good results.</description><author>Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, Eneko Agirre</author><pubDate>Thu, 05 Oct 2023 17:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03668v1</guid></item><item><title>MapperGPT: Large Language Models for Linking and Mapping Entities</title><link>http://arxiv.org/abs/2310.03666v1</link><description>Aligning terminological resources, including ontologies, controlledvocabularies, taxonomies, and value sets is a critical part of data integrationin many domains such as healthcare, chemistry, and biomedical research. Entitymapping is the process of determining correspondences between entities acrossthese resources, such as gene identifiers, disease concepts, or chemical entityidentifiers. Many tools have been developed to compute such mappings based oncommon structural features and lexical information such as labels and synonyms.Lexical approaches in particular often provide very high recall, but lowprecision, due to lexical ambiguity. As a consequence of this, mapping effortsoften resort to a labor intensive manual mapping refinement through a humancurator. Large Language Models (LLMs), such as the ones employed by ChatGPT, havegeneralizable abilities to perform a wide range of tasks, includingquestion-answering and information extraction. Here we present MapperGPT, anapproach that uses LLMs to review and refine mapping relationships as apost-processing step, in concert with existing high-recall methods that arebased on lexical and structural heuristics. We evaluated MapperGPT on a series of alignment tasks from different domains,including anatomy, developmental biology, and renal diseases. We devised acollection of tasks that are designed to be particularly challenging forlexical methods. We show that when used in combination with high-recallmethods, MapperGPT can provide a substantial improvement in accuracy, beatingstate-of-the-art (SOTA) methods such as LogMap.</description><author>Nicolas Matentzoglu, J. Harry Caufield, Harshad B. Hegde, Justin T. Reese, Sierra Moxon, Hyeongsik Kim, Nomi L. Harris, Melissa A Haendel, Christopher J. Mungall</author><pubDate>Thu, 05 Oct 2023 17:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03666v1</guid></item><item><title>Certification of Deep Learning Models for Medical Image Segmentation</title><link>http://arxiv.org/abs/2310.03664v1</link><description>In medical imaging, segmentation models have known a significant improvementin the past decade and are now used daily in clinical practice. However,similar to classification models, segmentation models are affected byadversarial attacks. In a safety-critical field like healthcare, certifyingmodel predictions is of the utmost importance. Randomized smoothing has beenintroduced lately and provides a framework to certify models and obtaintheoretical guarantees. In this paper, we present for the first time acertified segmentation baseline for medical imaging based on randomizedsmoothing and diffusion models. Our results show that leveraging the power ofdenoising diffusion probabilistic models helps us overcome the limits ofrandomized smoothing. We conduct extensive experiments on five public datasetsof chest X-rays, skin lesions, and colonoscopies, and empirically show that weare able to maintain high certified Dice scores even for highly perturbedimages. Our work represents the first attempt to certify medical imagesegmentation models, and we aspire for it to set a foundation for futurebenchmarks in this crucial and largely uncharted area.</description><author>Othmane Laousy, Alexandre Araujo, Guillaume Chassagnon, Nikos Paragios, Marie-Pierre Revel, Maria Vakalopoulou</author><pubDate>Thu, 05 Oct 2023 17:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03664v1</guid></item><item><title>Chatting Makes Perfect: Chat-based Image Retrieval</title><link>http://arxiv.org/abs/2305.20062v2</link><description>Chats emerge as an effective user-friendly approach for informationretrieval, and are successfully employed in many domains, such as customerservice, healthcare, and finance. However, existing image retrieval approachestypically address the case of a single query-to-image round, and the use ofchats for image retrieval has been mostly overlooked. In this work, weintroduce ChatIR: a chat-based image retrieval system that engages in aconversation with the user to elicit information, in addition to an initialquery, in order to clarify the user's search intent. Motivated by thecapabilities of today's foundation models, we leverage Large Language Models togenerate follow-up questions to an initial image description. These questionsform a dialog with the user in order to retrieve the desired image from a largecorpus. In this study, we explore the capabilities of such a system tested on alarge dataset and reveal that engaging in a dialog yields significant gains inimage retrieval. We start by building an evaluation pipeline from an existingmanually generated dataset and explore different modules and trainingstrategies for ChatIR. Our comparison includes strong baselines derived fromrelated applications trained with Reinforcement Learning. Our system is capableof retrieving the target image from a pool of 50K images with over 78% successrate after 5 dialogue rounds, compared to 75% when questions are asked byhumans, and 64% for a single shot text-to-image retrieval. Extensiveevaluations reveal the strong capabilities and examine the limitations ofCharIR under different settings. Project repository is available athttps://github.com/levymsn/ChatIR.</description><author>Matan Levy, Rami Ben-Ari, Nir Darshan, Dani Lischinski</author><pubDate>Thu, 05 Oct 2023 17:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20062v2</guid></item><item><title>Robustness-Guided Image Synthesis for Data-Free Quantization</title><link>http://arxiv.org/abs/2310.03661v1</link><description>Quantization has emerged as a promising direction for model compression.Recently, data-free quantization has been widely studied as a promising methodto avoid privacy concerns, which synthesizes images as an alternative to realtraining data. Existing methods use classification loss to ensure thereliability of the synthesized images. Unfortunately, even if these images arewell-classified by the pre-trained model, they still suffer from low semanticsand homogenization issues. Intuitively, these low-semantic images are sensitiveto perturbations, and the pre-trained model tends to have inconsistent outputwhen the generator synthesizes an image with poor semantics. To this end, wepropose Robustness-Guided Image Synthesis (RIS), a simple but effective methodto enrich the semantics of synthetic images and improve image diversity,further boosting the performance of downstream data-free compression tasks.Concretely, we first introduce perturbations on input and model weight, thendefine the inconsistency metrics at feature and prediction levels before andafter perturbations. On the basis of inconsistency on two levels, we design arobustness optimization objective to enhance the semantics of synthetic images.Moreover, we also make our approach diversity-aware by forcing the generator tosynthesize images with small correlations in the label space. With RIS, weachieve state-of-the-art performance for various settings on data-freequantization and can be extended to other data-free compression tasks.</description><author>Jianhong Bai, Yuchen Yang, Huanpeng Chu, Hualiang Wang, Zuozhu Liu, Ruizhe Chen, Xiaoxuan He, Lianrui Mu, Chengfei Cai, Haoji Hu</author><pubDate>Thu, 05 Oct 2023 17:39:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03661v1</guid></item><item><title>Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures</title><link>http://arxiv.org/abs/2310.03659v1</link><description>Large language models (LLMs) have revolutionized the field of artificialintelligence, endowing it with sophisticated language understanding andgeneration capabilities. However, when faced with more complex andinterconnected tasks that demand a profound and iterative thought process, LLMsreveal their inherent limitations. Autonomous LLM-powered multi-agent systemsrepresent a strategic response to these challenges. Such systems strive forautonomously tackling user-prompted goals by decomposing them into manageabletasks and orchestrating their execution and result synthesis through acollective of specialized intelligent agents. Equipped with LLM-poweredreasoning capabilities, these agents harness the cognitive synergy ofcollaborating with their peers, enhanced by leveraging contextual resourcessuch as tools and datasets. While these architectures hold promising potentialin amplifying AI capabilities, striking the right balance between differentlevels of autonomy and alignment remains the crucial challenge for theireffective operation. This paper proposes a comprehensive multi-dimensionaltaxonomy, engineered to analyze how autonomous LLM-powered multi-agent systemsbalance the dynamic interplay between autonomy and alignment across variousaspects inherent to architectural viewpoints such as goal-driven taskmanagement, agent composition, multi-agent collaboration, and contextinteraction. It also includes a domain-ontology model specifying fundamentalarchitectural concepts. Our taxonomy aims to empower researchers, engineers,and AI practitioners to systematically analyze the architectural dynamics andbalancing strategies employed by these increasingly prevalent AI systems. Theexploratory taxonomic classification of selected representative LLM-poweredmulti-agent systems illustrates its practical utility and reveals potential forfuture research and development.</description><author>Thorsten Händler</author><pubDate>Thu, 05 Oct 2023 17:37:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03659v1</guid></item><item><title>Visual inspection for illicit items in X-ray images using Deep Learning</title><link>http://arxiv.org/abs/2310.03658v1</link><description>Automated detection of contraband items in X-ray images can significantlyincrease public safety, by enhancing the productivity and alleviating themental load of security officers in airports, subways, customs/post offices,etc. The large volume and high throughput of passengers, mailed parcels, etc.,during rush hours practically make it a Big Data problem. Modern computervision algorithms relying on Deep Neural Networks (DNNs) have proven capable ofundertaking this task even under resource-constrained and embedded executionscenarios, e.g., as is the case with fast, single-stage object detectors.However, no comparative experimental assessment of the various relevant DNNcomponents/methods has been performed under a common evaluation protocol, whichmeans that reliable cross-method comparisons are missing. This paper presentsexactly such a comparative assessment, utilizing a public relevant dataset anda well-defined methodology for selecting the specific DNN components/modulesthat are being evaluated. The results indicate the superiority of Transformerdetectors, the obsolete nature of auxiliary neural modules that have beendeveloped in the past few years for security applications and the efficiency ofthe CSP-DarkNet backbone CNN.</description><author>Ioannis Mademlis, Georgios Batsis, Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos</author><pubDate>Thu, 05 Oct 2023 17:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03658v1</guid></item><item><title>Strategic Evaluation: Subjects, Evaluators, and Society</title><link>http://arxiv.org/abs/2310.03655v1</link><description>A broad current application of algorithms is in formal and quantitativemeasures of murky concepts -- like merit -- to make decisions. When peoplestrategically respond to these sorts of evaluations in order to gain favorabledecision outcomes, their behavior can be subjected to moral judgments. They maybe described as 'gaming the system' or 'cheating,' or (in other cases)investing 'honest effort' or 'improving.' Machine learning literature onstrategic behavior has tried to describe these dynamics by emphasizing theefforts expended by decision subjects hoping to obtain a more favorableassessment -- some works offer ways to preempt or prevent such manipulations,some differentiate 'gaming' from 'improvement' behavior, while others aim tomeasure the effort burden or disparate effects of classification systems. Webegin from a different starting point: that the design of an evaluation itselfcan be understood as furthering goals held by the evaluator which may bemisaligned with broader societal goals. To develop the idea that evaluationrepresents a strategic interaction in which both the evaluator and the subjectof their evaluation are operating out of self-interest, we put forward a modelthat represents the process of evaluation using three interacting agents: adecision subject, an evaluator, and society, representing a bundle of valuesand oversight mechanisms. We highlight our model's applicability to a number ofsocial systems where one or two players strategically undermine the others'interests to advance their own. Treating evaluators as themselves strategicallows us to re-cast the scrutiny directed at decision subjects, towards theincentives that underpin institutional designs of evaluations. The moralstanding of strategic behaviors often depend on the moral standing of theevaluations and incentives that provoke such behaviors.</description><author>Benjamin Laufer, Jon Kleinberg, Karen Levy, Helen Nissenbaum</author><pubDate>Thu, 05 Oct 2023 17:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03655v1</guid></item><item><title>PMSSC: Parallelizable multi-subset based self-expressive model for subspace clustering</title><link>http://arxiv.org/abs/2111.12232v2</link><description>Subspace clustering methods which embrace a self-expressive model thatrepresents each data point as a linear combination of other data points in thedataset provide powerful unsupervised learning techniques. However, whendealing with large datasets, representation of each data point by referring toall data points via a dictionary suffers from high computational complexity. Toalleviate this issue, we introduce a parallelizable multi-subset basedself-expressive model (PMS) which represents each data point by combiningmultiple subsets, with each consisting of only a small proportion of thesamples. The adoption of PMS in subspace clustering (PMSSC) leads tocomputational advantages because the optimization problems decomposed over eachsubset are small, and can be solved efficiently in parallel. Furthermore, PMSSCis able to combine multiple self-expressive coefficient vectors obtained fromsubsets, which contributes to an improvement in self-expressiveness. Extensiveexperiments on synthetic and real-world datasets show the efficiency andeffectiveness of our approach in comparison to other methods.</description><author>Katsuya Hotta, Takuya Akashi, Shogo Tokai, Chao Zhang</author><pubDate>Thu, 05 Oct 2023 17:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.12232v2</guid></item><item><title>Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics</title><link>http://arxiv.org/abs/2310.03652v1</link><description>Data-driven constitutive modeling with neural networks has received increasedinterest in recent years due to its ability to easily incorporate physical andmechanistic constraints and to overcome the challenging and time-consuming taskof formulating phenomenological constitutive laws that can accurately capturethe observed material response. However, even though neural network-basedconstitutive laws have been shown to generalize proficiently, the generatedrepresentations are not easily interpretable due to their high number oftrainable parameters. Sparse regression approaches exist that allow toobtaining interpretable expressions, but the user is tasked with creating alibrary of model forms which by construction limits their expressiveness to thefunctional forms provided in the libraries. In this work, we propose to trainregularized physics-augmented neural network-based constitutive modelsutilizing a smoothed version of $L^{0}$-regularization. This aims to maintainthe trustworthiness inherited by the physical constraints, but also enablesinterpretability which has not been possible thus far on any type of machinelearning-based constitutive model where model forms were not assumed a-priorybut were actually discovered. During the training process, the networksimultaneously fits the training data and penalizes the number of activeparameters, while also ensuring constitutive constraints such as thermodynamicconsistency. We show that the method can reliably obtain interpretable andtrustworthy constitutive models for compressible and incompressiblehyperelasticity, yield functions, and hardening models for elastoplasticity,for synthetic and experimental data.</description><author>Jan N. Fuhg, Reese E. Jones, Nikolaos Bouklas</author><pubDate>Thu, 05 Oct 2023 17:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03652v1</guid></item><item><title>Boost Video Frame Interpolation via Motion Adaptation</title><link>http://arxiv.org/abs/2306.13933v3</link><description>Video frame interpolation (VFI) is a challenging task that aims to generateintermediate frames between two consecutive frames in a video. Existinglearning-based VFI methods have achieved great success, but they still sufferfrom limited generalization ability due to the limited motion distribution oftraining datasets. In this paper, we propose a novel optimization-based VFImethod that can adapt to unseen motions at test time. Our method is based on acycle-consistency adaptation strategy that leverages the motion characteristicsamong video frames. We also introduce a lightweight adapter that can beinserted into the motion estimation module of existing pre-trained VFI modelsto improve the efficiency of adaptation. Extensive experiments on variousbenchmarks demonstrate that our method can boost the performance of two-frameVFI models, outperforming the existing state-of-the-art methods, even thosethat use extra input.</description><author>Haoning Wu, Xiaoyun Zhang, Weidi Xie, Ya Zhang, Yanfeng Wang</author><pubDate>Thu, 05 Oct 2023 17:25:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13933v3</guid></item><item><title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</title><link>http://arxiv.org/abs/2310.02954v2</link><description>Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5\% to 94.2\%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundaries ofin-context learning and opens up new avenues for addressing complex reasoningchallenges. We will release the code soon.</description><author>Jiong Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</author><pubDate>Thu, 05 Oct 2023 17:24:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02954v2</guid></item><item><title>Rethinking Fairness for Human-AI Collaboration</title><link>http://arxiv.org/abs/2310.03647v1</link><description>Existing approaches to algorithmic fairness aim to ensure equitable outcomesif human decision-makers comply perfectly with algorithmic decisions. However,perfect compliance with the algorithm is rarely a reality or even a desirableoutcome in human-AI collaboration. Yet, recent studies have shown thatselective compliance with fair algorithms can amplify discrimination relativeto the prior human policy. As a consequence, ensuring equitable outcomesrequires fundamentally different algorithmic design principles that ensurerobustness to the decision-maker's (a priori unknown) compliance pattern. Wedefine the notion of compliance-robustly fair algorithmic recommendations thatare guaranteed to (weakly) improve fairness in decisions, regardless of thehuman's compliance pattern. We propose a simple optimization strategy toidentify the best performance-improving compliance-robustly fair policy.However, we show that it may be infeasible to design algorithmicrecommendations that are simultaneously fair in isolation, compliance-robustlyfair, and more accurate than the human policy; thus, if our goal is to improvethe equity and accuracy of human-AI collaboration, it may not be desirable toenforce traditional fairness constraints.</description><author>Haosen Ge, Hamsa Bastani, Osbert Bastani</author><pubDate>Thu, 05 Oct 2023 17:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03647v1</guid></item><item><title>TRAM: Bridging Trust Regions and Sharpness Aware Minimization</title><link>http://arxiv.org/abs/2310.03646v1</link><description>By reducing the curvature of the loss surface in the parameter space,Sharpness-aware minimization (SAM) yields widespread robustness improvementunder domain transfer. Instead of focusing on parameters, however, this workconsiders the transferability of representations as the optimization target forout-of-domain generalization in a fine-tuning setup. To encourage the retentionof transferable representations, we consider trust region-based fine-tuningmethods, which exploit task-specific skills without forgetting task-agnosticrepresentations from pre-training. We unify parameter- and representation-spacesmoothing approaches by using trust region bounds to inform SAM-styleregularizers on both of these optimization surfaces. We propose Trust RegionAware Minimization (TRAM), a fine-tuning algorithm that optimizes for flatminima and smooth, informative representations without forgetting pre-trainedstructure. We find that TRAM outperforms both sharpness-aware and trustregion-based optimization methods on cross-domain language modeling andcross-lingual transfer, where robustness to domain transfer and representationgenerality are critical for success. TRAM establishes a new standard intraining generalizable models with minimal additional computation.</description><author>Tom Sherborne, Naomi Saphra, Pradeep Dasigi, Hao Peng</author><pubDate>Thu, 05 Oct 2023 17:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03646v1</guid></item><item><title>Distributional PAC-Learning from Nisan's Natural Proofs</title><link>http://arxiv.org/abs/2310.03641v1</link><description>(Abridged) Carmosino et al. (2016) demonstrated that natural proofs ofcircuit lower bounds for \Lambda imply efficient algorithms for learning\Lambda-circuits, but only over the uniform distribution, with membershipqueries, and provided \AC^0[p] \subseteq \Lambda. We consider whether thisimplication can be generalized to \Lambda \not\supseteq \AC^0[p], and tolearning algorithms in Valiant's PAC model, which use only random examples andlearn over arbitrary example distributions. We give results of both positiveand negative flavor. On the negative side, we observe that if, for every circuit class \Lambda,the implication from natural proofs for \Lambda to learning \Lambda-circuits inValiant's PAC model holds, then there is a polynomial time solution toO(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantumsolutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP(Shortest Independent Vector Problem). This indicates that whether naturalproofs for \Lambda imply efficient learning algorithms for \Lambda in Valiant'sPAC model may depend on \Lambda. On the positive side, our main result is that specific natural proofs arisingfrom a type of communication complexity argument (e.g., Nisan (1993), fordepth-2 majority circuits) imply PAC-learning algorithms in a newdistributional variant of Valiant's model. Our distributional PAC model isstronger than the average-case prediction model of Blum et al (1993) and theheuristic PAC model of Nanashima (2021), and has several important propertieswhich make it of independent interest, such as being boosting-friendly. Themain applications of our result are new distributional PAC-learning algorithmsfor depth-2 majority circuits, polytopes and DNFs over natural targetdistributions, as well as the nonexistence of encoded-input weak PRFs that canbe evaluated by depth-2 majority circuits.</description><author>Ari Karchmer</author><pubDate>Thu, 05 Oct 2023 17:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03641v1</guid></item><item><title>RedMotion: Motion Prediction via Redundancy Reduction</title><link>http://arxiv.org/abs/2306.10840v2</link><description>Predicting the future motion of traffic agents is vital for self-drivingvehicles to ensure their safe operation. We introduce RedMotion, a transformermodel for motion prediction that incorporates two types of redundancyreduction. The first type of redundancy reduction is induced by an internaltransformer decoder and reduces a variable-sized set of road environmenttokens, such as road graphs with agent data, to a fixed-sized embedding. Thesecond type of redundancy reduction is a self-supervised learning objective andapplies the redundancy reduction principle to embeddings generated fromaugmented views of road environments. Our experiments reveal that ourrepresentation learning approach can outperform PreTraM, Traj-MAE, andGraphDINO in a semi-supervised setting. Our RedMotion model achieves resultsthat are competitive with those of Scene Transformer or MTR++. We provide anopen source implementation that is accessible via GitHub(https://github.com/kit-mrt/red-motion) and Colab(https://colab.research.google.com/drive/1Q-Z9VdiqvfPfctNG8oqzPcgm0lP3y1il).</description><author>Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez Lopez</author><pubDate>Thu, 05 Oct 2023 17:13:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10840v2</guid></item><item><title>Evaluating Self-Supervised Speech Representations for Indigenous American Languages</title><link>http://arxiv.org/abs/2310.03639v1</link><description>The application of self-supervision to speech representation learning hasgarnered significant interest in recent years, due to its scalability to largeamounts of unlabeled data. However, much progress, both in terms ofpre-training and downstream evaluation, has remained concentrated inmonolingual models that only consider English. Few models consider otherlanguages, and even fewer consider indigenous ones. In our submission to theNew Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASRcorpus for Quechua, an indigenous South American Language. We benchmark theefficacy of large SSL models on Quechua, along with 6 other indigenouslanguages such as Guarani and Bribri, on low-resource ASR. Our results showsurprisingly strong performance by state-of-the-art SSL models, showing thepotential generalizability of large-scale models to real-world data.</description><author>Chih-Chen Chen, William Chen, Rodolfo Zevallos, John Ortega</author><pubDate>Thu, 05 Oct 2023 17:11:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03639v1</guid></item><item><title>CLEVRER-Humans: Describing Physical and Causal Events the Human Way</title><link>http://arxiv.org/abs/2310.03635v1</link><description>Building machines that can reason about physical events and their causalrelationships is crucial for flexible interaction with the physical world.However, most existing physical and causal reasoning benchmarks are exclusivelybased on synthetically generated events and synthetic natural languagedescriptions of causal relationships. This design brings up two issues. First,there is a lack of diversity in both event types and natural languagedescriptions; second, causal relationships based on manually-defined heuristicsare different from human judgments. To address both shortcomings, we presentthe CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment ofphysical events with human labels. We employ two techniques to improve datacollection efficiency: first, a novel iterative event cloze task to elicit anew representation of events in videos, which we term Causal Event Graphs(CEGs); second, a data augmentation technique based on neural languagegenerative models. We convert the collected CEGs into questions and answers tobe consistent with prior work. Finally, we study a collection of baselineapproaches for CLEVRER-Humans question-answering, highlighting the greatchallenges set forth by our benchmark.</description><author>Jiayuan Mao, Xuelin Yang, Xikun Zhang, Noah D. Goodman, Jiajun Wu</author><pubDate>Thu, 05 Oct 2023 17:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03635v1</guid></item><item><title>Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF</title><link>http://arxiv.org/abs/2309.08621v2</link><description>Fairness problems in recommender systems often have a complexity in practicethat is not adequately captured in simplified research formulations. A socialchoice formulation of the fairness problem, operating within a multi-agentarchitecture of fairness concerns, offers a flexible and multi-aspectalternative to fairness-aware recommendation approaches. Leveraging socialchoice allows for increased generality and the possibility of tapping intowell-studied social choice algorithms for resolving the tension betweenmultiple, competing fairness concerns. This paper explores a range of optionsfor choice mechanisms in multi-aspect fairness applications using both real andsynthetic data and shows that different classes of choice and allocationmechanisms yield different but consistent fairness / accuracy tradeoffs. Wealso show that a multi-agent formulation offers flexibility in adapting to userpopulation dynamics.</description><author>Amanda Aird, Cassidy All, Paresha Farastu, Elena Stefancova, Joshua Sun, Nicholas Mattei, Robin Burke</author><pubDate>Thu, 05 Oct 2023 17:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08621v2</guid></item><item><title>Explaining Emergent In-Context Learning as Kernel Regression</title><link>http://arxiv.org/abs/2305.12766v2</link><description>Large language models (LLMs) have initiated a paradigm shift in transferlearning. In contrast to the classic pretraining-then-finetuning procedure, inorder to use LLMs for downstream prediction tasks, one only needs to provide afew demonstrations, known as in-context examples, without adding more orupdating existing model parameters. This in-context learning (ICL) capabilityof LLMs is intriguing, and it is not yet fully understood how pretrained LLMsacquire such capabilities. In this paper, we investigate the reason why atransformer-based language model can accomplish in-context learning afterpre-training on a general language corpus by proposing one hypothesis that LLMscan simulate kernel regression with internal representations when faced within-context examples. More concretely, we first prove that Bayesian inference onin-context prompts can be asymptotically understood as kernel regression $\haty = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-contextdemonstrations grows. Then, we empirically investigate the in-context behaviorsof language models. We find that during ICL, the attention and hidden featuresin LLMs match the behaviors of a kernel regression. Finally, our theoryprovides insights into multiple phenomena observed in the ICL field: whyretrieving demonstrative samples similar to test samples can help, why ICLperformance is sensitive to the output formats, and why ICL accuracy benefitsfrom selecting in-distribution and representative samples.</description><author>Chi Han, Ziqi Wang, Han Zhao, Heng Ji</author><pubDate>Thu, 05 Oct 2023 17:04:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12766v2</guid></item><item><title>Deep Momentum Multi-Marginal Schrödinger Bridge</title><link>http://arxiv.org/abs/2303.01751v3</link><description>It is a crucial challenge to reconstruct population dynamics using unlabeledsamples from distributions at coarse time intervals. Recent approaches such asflow-based models or Schr\"odinger Bridge (SB) models have demonstratedappealing performance, yet the inferred sample trajectories either fail toaccount for the underlying stochasticity or are $\underline{D}$eep$\underline{M}$omentum Multi-Marginal $\underline{S}$chr\"odinger$\underline{B}$ridge(DMSB), a novel computational framework that learns thesmooth measure-valued spline for stochastic systems that satisfy positionmarginal constraints across time. By tailoring the celebrated Bregman Iterationand extending the Iteration Proportional Fitting to phase space, we manage tohandle high-dimensional multi-marginal trajectory inference tasks efficiently.Our algorithm outperforms baselines significantly, as evidenced by experimentsfor synthetic datasets and a real-world single-cell RNA sequence dataset.Additionally, the proposed approach can reasonably reconstruct the evolution ofvelocity distribution, from position snapshots only, when there is a groundtruth velocity that is nevertheless inaccessible.</description><author>Tianrong Chen, Guan-Horng Liu, Molei Tao, Evangelos A. Theodorou</author><pubDate>Thu, 05 Oct 2023 17:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01751v3</guid></item><item><title>Wasserstein Distortion: Unifying Fidelity and Realism</title><link>http://arxiv.org/abs/2310.03629v1</link><description>We introduce a distortion measure for images, Wasserstein distortion, thatsimultaneously generalizes pixel-level fidelity on the one hand and realism onthe other. We show how Wasserstein distortion reduces mathematically to a purefidelity constraint or a pure realism constraint under different parameterchoices. Pairs of images that are close under Wasserstein distortion illustrateits utility. In particular, we generate random textures that have high fidelityto a reference texture in one location of the image and smoothly transition toan independent realization of the texture as one moves away from this point.Connections between Wasserstein distortion and models of the human visualsystem are noted.</description><author>Yang Qiu, Aaron B. Wagner, Johannes Ballé, Lucas Theis</author><pubDate>Thu, 05 Oct 2023 17:03:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03629v1</guid></item><item><title>High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning</title><link>http://arxiv.org/abs/2310.03624v1</link><description>A robot self-model is a task-agnostic representation of the robot's physicalmorphology that can be used for motion planning tasks in absence of classicalgeometric kinematic models. In particular, when the latter are hard to engineeror the robot's kinematics change unexpectedly, human-free self-modeling is anecessary feature of truly autonomous agents. In this work, we leverage neuralfields to allow a robot to self-model its kinematics as a neural-implicit querymodel learned only from 2D images annotated with camera poses andconfigurations. This enables significantly greater applicability than existingapproaches which have been dependent on depth images or geometry knowledge. Tothis end, alongside a curricular data sampling strategy, we propose a newencoder-based neural density field architecture for dynamic object-centricscenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOFrobot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%of the robot's workspace dimension. We demonstrate the capabilities of thismodel on a motion planning task as an exemplary downstream application.</description><author>Lennart Schulze, Hod Lipson</author><pubDate>Thu, 05 Oct 2023 17:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03624v1</guid></item><item><title>Sharpness-Aware Minimization and the Edge of Stability</title><link>http://arxiv.org/abs/2309.12488v3</link><description>Recent experiments have shown that, often, when training a neural networkwith gradient descent (GD) with a step size $\eta$, the operator norm of theHessian of the loss grows until it approximately reaches $2/\eta$, after whichit fluctuates around this value. The quantity $2/\eta$ has been called the"edge of stability" based on consideration of a local quadratic approximationof the loss. We perform a similar calculation to arrive at an "edge ofstability" for Sharpness-Aware Minimization (SAM), a variant of GD which hasbeen shown to improve its generalization. Unlike the case for GD, the resultingSAM-edge depends on the norm of the gradient. Using three deep learningtraining tasks, we see empirically that SAM operates on the edge of stabilityidentified by this analysis.</description><author>Philip M. Long, Peter L. Bartlett</author><pubDate>Thu, 05 Oct 2023 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12488v3</guid></item><item><title>PeaTMOSS: Mining Pre-Trained Models in Open-Source Software</title><link>http://arxiv.org/abs/2310.03620v1</link><description>Developing and training deep learning models is expensive, so softwareengineers have begun to reuse pre-trained deep learning models (PTMs) andfine-tune them for downstream tasks. Despite the wide-spread use of PTMs, weknow little about the corresponding software engineering behaviors andchallenges. To enable the study of software engineering with PTMs, we present thePeaTMOSS dataset: Pre-Trained Models in Open-Source Software. PeaTMOSS hasthree parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source softwarerepositories that use PTMs, and (3) a mapping between PTMs and the projectsthat use them. We challenge PeaTMOSS miners to discover software engineeringpractices around PTMs. A demo and link to the full dataset are available at:https://github.com/PurdueDualityLab/PeaTMOSS-Demos.</description><author>Wenxin Jiang, Jason Jones, Jerin Yasmin, Nicholas Synovic, Rajeev Sashti, Sophie Chen, George K. Thiruvathukal, Yuan Tian, James C. Davis</author><pubDate>Thu, 05 Oct 2023 16:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03620v1</guid></item><item><title>Modeling Inverse Demand Function with Explainable Dual Neural Networks</title><link>http://arxiv.org/abs/2307.14322v2</link><description>Financial contagion has been widely recognized as a fundamental risk to thefinancial system. Particularly potent is price-mediated contagion, whereinforced liquidations by firms depress asset prices and propagate financialstress, enabling crises to proliferate across a broad spectrum of seeminglyunrelated entities. Price impacts are currently modeled via exogenous inversedemand functions. However, in real-world scenarios, only the initial shocks andthe final equilibrium asset prices are typically observable, leaving actualasset liquidations largely obscured. This missing data presents significantlimitations to calibrating the existing models. To address these challenges, weintroduce a novel dual neural network structure that operates in two sequentialstages: the first neural network maps initial shocks to predicted assetliquidations, and the second network utilizes these liquidations to deriveresultant equilibrium prices. This data-driven approach can capture both linearand non-linear forms without pre-specifying an analytical structure;furthermore, it functions effectively even in the absence of observableliquidation data. Experiments with simulated datasets demonstrate that ourmodel can accurately predict equilibrium asset prices based solely on initialshocks, while revealing a strong alignment between predicted and trueliquidations. Our explainable framework contributes to the understanding andmodeling of price-mediated contagion and provides valuable insights forfinancial authorities to construct effective stress tests and regulatorypolicies.</description><author>Zhiyu Cao, Zihan Chen, Prerna Mishra, Hamed Amini, Zachary Feinstein</author><pubDate>Thu, 05 Oct 2023 16:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14322v2</guid></item><item><title>Decoding speech perception from non-invasive brain recordings</title><link>http://arxiv.org/abs/2208.12266v2</link><description>Decoding speech from brain activity is a long-awaited goal in both healthcareand neuroscience. Invasive devices have recently led to major milestones inthat regard: deep learning algorithms trained on intracranial recordings nowstart to decode elementary linguistic features (e.g. letters, words,spectrograms). However, extending this approach to natural speech andnon-invasive brain recordings remains a major challenge. Here, we introduce amodel trained with contrastive-learning to decode self-supervisedrepresentations of perceived speech from the non-invasive recordings of a largecohort of healthy individuals. To evaluate this approach, we curate andintegrate four public datasets, encompassing 175 volunteers recorded withmagneto- or electro-encephalography (M/EEG), while they listened to shortstories and isolated sentences. The results show that our model can identify,from 3 seconds of MEG signals, the corresponding speech segment with up to 41%accuracy out of more than 1,000 distinct possibilities on average acrossparticipants, and more than 80% in the very best participants - a performancethat allows the decoding of words and phrases absent from the training set. Thecomparison of our model to a variety of baselines highlights the importance of(i) a contrastive objective, (ii) pretrained representations of speech and(iii) a common convolutional architecture simultaneously trained acrossmultiple participants. Finally, the analysis of the decoder's predictionssuggests that they primarily depend on lexical and contextual semanticrepresentations. Overall, this effective decoding of perceived speech fromnon-invasive recordings delineates a promising path to decode language frombrain activity, without putting patients at risk for brain surgery.</description><author>Alexandre Défossez, Charlotte Caucheteux, Jérémy Rapin, Ori Kabeli, Jean-Rémi King</author><pubDate>Thu, 05 Oct 2023 16:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12266v2</guid></item><item><title>CLASSify: A Web-Based Tool for Machine Learning</title><link>http://arxiv.org/abs/2310.03618v1</link><description>Machine learning classification problems are widespread in bioinformatics,but the technical knowledge required to perform model training, optimization,and inference can prevent researchers from utilizing this technology. Thisarticle presents an automated tool for machine learning classification problemsto simplify the process of training models and producing results whileproviding informative visualizations and insights into the data. This toolsupports both binary and multiclass classification problems, and it providesaccess to a variety of models and methods. Synthetic data can be generatedwithin the interface to fill missing values, balance class labels, or generateentirely new datasets. It also provides support for feature evaluation andgenerates explainability scores to indicate which features influence the outputthe most. We present CLASSify, an open-source tool for simplifying the userexperience of solving classification problems without the need for knowledge ofmachine learning.</description><author>Aaron D. Mullen, Samuel E. Armstrong, Jeff Talbert, V. K. Cody Bumgardner</author><pubDate>Thu, 05 Oct 2023 16:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03618v1</guid></item><item><title>Optimal 1-Wasserstein Distance for WGANs</title><link>http://arxiv.org/abs/2201.02824v2</link><description>The mathematical forces at work behind Generative Adversarial Networks raisechallenging theoretical issues. Motivated by the important question ofcharacterizing the geometrical properties of the generated distributions, weprovide a thorough analysis of Wasserstein GANs (WGANs) in both the finitesample and asymptotic regimes. We study the specific case where the latentspace is univariate and derive results valid regardless of the dimension of theoutput space. We show in particular that for a fixed sample size, the optimalWGANs are closely linked with connected paths minimizing the sum of the squaredEuclidean distances between the sample points. We also highlight the fact thatWGANs are able to approach (for the 1-Wasserstein distance) the targetdistribution as the sample size tends to infinity, at a given convergence rateand provided the family of generative Lipschitz functions grows appropriately.We derive in passing new results on optimal transport theory in thesemi-discrete setting.</description><author>Arthur Stéphanovitch, Ugo Tanielian, Benoît Cadre, Nicolas Klutchnikoff, Gérard Biau</author><pubDate>Thu, 05 Oct 2023 16:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.02824v2</guid></item><item><title>Animatable Virtual Humans: Learning pose-dependent human representations in UV space for interactive performance synthesis</title><link>http://arxiv.org/abs/2310.03615v1</link><description>We propose a novel representation of virtual humans for highly realisticreal-time animation and rendering in 3D applications. We learn pose dependentappearance and geometry from highly accurate dynamic mesh sequences obtainedfrom state-of-the-art multiview-video reconstruction. Learning pose-dependentappearance and geometry from mesh sequences poses significant challenges, as itrequires the network to learn the intricate shape and articulated motion of ahuman body. However, statistical body models like SMPL provide valuablea-priori knowledge which we leverage in order to constrain the dimension of thesearch space enabling more efficient and targeted learning and definepose-dependency. Instead of directly learning absolute pose-dependent geometry,we learn the difference between the observed geometry and the fitted SMPLmodel. This allows us to encode both pose-dependent appearance and geometry inthe consistent UV space of the SMPL model. This approach not only ensures ahigh level of realism but also facilitates streamlined processing and renderingof virtual humans in real-time scenarios.</description><author>Wieland Morgenstern, Milena T. Bagdasarian, Anna Hilsmann, Peter Eisert</author><pubDate>Thu, 05 Oct 2023 16:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03615v1</guid></item><item><title>Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally</title><link>http://arxiv.org/abs/2310.03614v1</link><description>Deep Neural Networks (DNNs) have been the driving force behind many of therecent advances in machine learning. However, research has shown that DNNs arevulnerable to adversarial examples -- input samples that have been perturbed toforce DNN-based models to make errors. As a result, Adversarial MachineLearning (AdvML) has gained a lot of attention, and researchers haveinvestigated these vulnerabilities in various settings and modalities. Inaddition, DNNs have also been found to incorporate embedded bias and oftenproduce unexplainable predictions, which can result in anti-social AIapplications. The emergence of new AI technologies that leverage Large LanguageModels (LLMs), such as ChatGPT and GPT-4, increases the risk of producinganti-social applications at scale. AdvML for Social Good (AdvML4G) is anemerging field that repurposes the AdvML bug to invent pro-social applications.Regulators, practitioners, and researchers should collaborate to encourage thedevelopment of pro-social applications and hinder the development ofanti-social ones. In this work, we provide the first comprehensive review ofthe emerging field of AdvML4G. This paper encompasses a taxonomy thathighlights the emergence of AdvML4G, a discussion of the differences andsimilarities between AdvML4G and AdvML, a taxonomy covering social good-relatedconcepts and aspects, an exploration of the motivations behind the emergence ofAdvML4G at the intersection of ML4G and AdvML, and an extensive summary of theworks that utilize AdvML4G as an auxiliary tool for innovating pro-socialapplications. Finally, we elaborate upon various challenges and open researchissues that require significant attention from the research community.</description><author>Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha</author><pubDate>Thu, 05 Oct 2023 16:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03614v1</guid></item><item><title>Solving a Class of Non-Convex Minimax Optimization in Federated Learning</title><link>http://arxiv.org/abs/2310.03613v1</link><description>The minimax problems arise throughout machine learning applications, rangingfrom adversarial training and policy evaluation in reinforcement learning toAUROC maximization. To address the large-scale data challenges across multipleclients with communication-efficient distributed training, federated learning(FL) is gaining popularity. Many optimization algorithms for minimax problemshave been developed in the centralized setting (\emph{i.e.} single-machine).Nonetheless, the algorithm for minimax problems under FL is stillunderexplored. In this paper, we study a class of federated nonconvex minimaxoptimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) andreduce existing complexity results for the most common minimax problems. Fornonconvex-concave problems, we propose FedSGDA+ and reduce the communicationcomplexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave andnonconvex-PL minimax settings, we prove that FedSGDA-M has the best-knownsample complexity of $O(\kappa^{3} N^{-1}\varepsilon^{-3})$ and the best-knowncommunication complexity of $O(\kappa^{2}\varepsilon^{-2})$. FedSGDA-M is thefirst algorithm to match the best sample complexity $O(\varepsilon^{-3})$achieved by the single-machine method under the nonconvex-strongly-concavesetting. Extensive experimental results on fair classification and AUROCmaximization show the efficiency of our algorithms.</description><author>Xidong Wu, Jianhui Sun, Zhengmian Hu, Aidong Zhang, Heng Huang</author><pubDate>Thu, 05 Oct 2023 16:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03613v1</guid></item><item><title>GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data</title><link>http://arxiv.org/abs/2310.03611v1</link><description>Detecting and discovering new gene interactions based on known geneexpressions and gene interaction data presents a significant challenge. Variousstatistical and deep learning methods have attempted to tackle this challengeby leveraging the topological structure of gene interactions and geneexpression patterns to predict novel gene interactions. In contrast, someapproaches have focused exclusively on utilizing gene expression profiles. Inthis context, we introduce GENER, a parallel-layer deep learning networkdesigned exclusively for the identification of gene-gene relationships usinggene expression data. We conducted two training experiments and compared theperformance of our network with that of existing statistical and deep learningapproaches. Notably, our model achieved an average AUROC score of 0.834 on thecombined BioGRID&amp;DREAM5 dataset, outperforming competing methods in predictinggene-gene interactions.</description><author>Ahmed Fakhry Elnaggar, Raneem Ali Khafagy, Adriaan-Alexander Ludl</author><pubDate>Thu, 05 Oct 2023 16:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03611v1</guid></item><item><title>Quantitative CLTs in Deep Neural Networks</title><link>http://arxiv.org/abs/2307.06092v4</link><description>We study the distribution of a fully connected neural network with randomGaussian weights and biases in which the hidden layer widths are proportionalto a large constant $n$. Under mild assumptions on the non-linearity, we obtainquantitative bounds on normal approximations valid at large but finite $n$ andany fixed network depth. Our theorems show both for the finite-dimensionaldistributions and the entire process, that the distance between a random fullyconnected network (and its derivatives) to the corresponding infinite widthGaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0$, with the exponentdepending on the metric used to measure discrepancy. Our bounds are strictlystronger in terms of their dependence on network width than any previouslyavailable in the literature; in the one-dimensional case, we also prove thatthey are optimal, i.e., we establish matching lower bounds.</description><author>Stefano Favaro, Boris Hanin, Domenico Marinucci, Ivan Nourdin, Giovanni Peccati</author><pubDate>Thu, 05 Oct 2023 16:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06092v4</guid></item><item><title>How Good Are Synthetic Medical Images? An Empirical Study with Lung Ultrasound</title><link>http://arxiv.org/abs/2310.03608v1</link><description>Acquiring large quantities of data and annotations is known to be effectivefor developing high-performing deep learning models, but is difficult andexpensive to do in the healthcare context. Adding synthetic training data usinggenerative models offers a low-cost method to deal effectively with the datascarcity challenge, and can also address data imbalance and patient privacyissues. In this study, we propose a comprehensive framework that fitsseamlessly into model development workflows for medical image analysis. Wedemonstrate, with datasets of varying size, (i) the benefits of generativemodels as a data augmentation method; (ii) how adversarial methods can protectpatient privacy via data substitution; (iii) novel performance metrics forthese use cases by testing models on real holdout data. We show that trainingwith both synthetic and real data outperforms training with real data alone,and that models trained solely with synthetic data approach their real-onlycounterparts. Code is available athttps://github.com/Global-Health-Labs/US-DCGAN.</description><author>Menghan Yu, Sourabh Kulhare, Courosh Mehanian, Charles B Delahunt, Daniel E Shea, Zohreh Laverriere, Ishan Shah, Matthew P Horning</author><pubDate>Thu, 05 Oct 2023 16:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03608v1</guid></item><item><title>Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review</title><link>http://arxiv.org/abs/2310.03606v1</link><description>This literature review aimed to compare various time-series analysisapproaches utilized in forecasting COVID-19 cases in Africa. The study involveda methodical search for English-language research papers published betweenJanuary 2020 and July 2023, focusing specifically on papers that utilizedtime-series analysis approaches on COVID-19 datasets in Africa. A variety ofdatabases including PubMed, Google Scholar, Scopus, and Web of Science wereutilized for this process. The research papers underwent an evaluation processto extract relevant information regarding the implementation and performance ofthe time-series analysis models. The study highlighted the differentmethodologies employed, evaluating their effectiveness and limitations inforecasting the spread of the virus. The result of this review could contributedeeper insights into the field, and future research should consider theseinsights to improve time series analysis models and explore the integration ofdifferent approaches for enhanced public health decision-making.</description><author>Ali Ebadi, Ebrahim Sahafizadeh</author><pubDate>Thu, 05 Oct 2023 16:36:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03606v1</guid></item><item><title>FASER: Binary Code Similarity Search through the use of Intermediate Representations</title><link>http://arxiv.org/abs/2310.03605v1</link><description>Being able to identify functions of interest in cross-architecture softwareis useful whether you are analysing for malware, securing the software supplychain or conducting vulnerability research. Cross-Architecture Binary CodeSimilarity Search has been explored in numerous studies and has used a widerange of different data sources to achieve its goals. The data sourcestypically used draw on common structures derived from binaries such as functioncontrol flow graphs or binary level call graphs, the output of the disassemblyprocess or the outputs of a dynamic analysis approach. One data source whichhas received less attention is binary intermediate representations. BinaryIntermediate representations possess two interesting properties: they are crossarchitecture by their very nature and encode the semantics of a functionexplicitly to support downstream usage. Within this paper we propose Functionas a String Encoded Representation (FASER) which combines long documenttransformers with the use of intermediate representations to create a modelcapable of cross architecture function search without the need for manualfeature engineering, pre-training or a dynamic analysis step. We compare ourapproach against a series of baseline approaches for two tasks; A generalfunction search task and a targeted vulnerability search task. Our approachdemonstrates strong performance across both tasks, performing better than allbaseline approaches.</description><author>Josh Collyer, Tim Watson, Iain Phillips</author><pubDate>Thu, 05 Oct 2023 16:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03605v1</guid></item><item><title>Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints</title><link>http://arxiv.org/abs/2310.03602v1</link><description>Text-driven 3D indoor scene generation could be useful for gaming, filmindustry, and AR/VR applications. However, existing methods cannot faithfullycapture the room layout, nor do they allow flexible editing of individualobjects in the room. To address these problems, we present Ctrl-Room, which isable to generate convincing 3D rooms with designer-style layouts andhigh-fidelity textures from just a text prompt. Moreover, Ctrl-Room enablesversatile interactive editing operations such as resizing or moving individualfurniture items. Our key insight is to separate the modeling of layouts andappearance. %how to model the room that takes into account both scene textureand geometry at the same time. To this end, Our proposed method consists of twostages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The`Layout Generation Stage' trains a text-conditional diffusion model to learnthe layout distribution with our holistic scene code parameterization. Next,the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce avivid panoramic image of the room guided by the 3D scene layout and textprompt. In this way, we achieve a high-quality 3D room with convincing layoutsand lively textures. Benefiting from the scene code parameterization, we caneasily edit the generated room model through our mask-guided editing module,without expensive editing-specific training. Extensive experiments on theStructured3D dataset demonstrate that our method outperforms existing methodsin producing more reasonable, view-consistent, and editable 3D rooms fromnatural language prompts.</description><author>Chuan Fang, Xiaotao Hu, Kunming Luo, Ping Tan</author><pubDate>Thu, 05 Oct 2023 16:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03602v1</guid></item><item><title>Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance</title><link>http://arxiv.org/abs/2304.06715v3</link><description>Interpretability methods are valuable only if their explanations faithfullydescribe the explained model. In this work, we consider neural networks whosepredictions are invariant under a specific symmetry group. This includespopular architectures, ranging from convolutional to graph neural networks. Anyexplanation that faithfully explains this type of model needs to be inagreement with this invariance property. We formalize this intuition throughthe notion of explanation invariance and equivariance by leveraging theformalism from geometric deep learning. Through this rigorous formalism, wederive (1) two metrics to measure the robustness of any interpretability methodwith respect to the model symmetry group; (2) theoretical robustness guaranteesfor some popular interpretability methods and (3) a systematic approach toincrease the invariance of any interpretability method with respect to asymmetry group. By empirically measuring our metrics for explanations of modelsassociated with various modalities and symmetry groups, we derive a set of 5guidelines to allow users and developers of interpretability methods to producerobust explanations.</description><author>Jonathan Crabbé, Mihaela van der Schaar</author><pubDate>Thu, 05 Oct 2023 16:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06715v3</guid></item><item><title>Sampling via Gradient Flows in the Space of Probability Measures</title><link>http://arxiv.org/abs/2310.03597v1</link><description>Sampling a target probability distribution with an unknown normalizationconstant is a fundamental challenge in computational science and engineering.Recent work shows that algorithms derived by considering gradient flows in thespace of probability measures open up new avenues for algorithm development.This paper makes three contributions to this sampling approach by scrutinizingthe design components of such gradient flows. Any instantiation of a gradientflow for sampling needs an energy functional and a metric to determine theflow, as well as numerical approximations of the flow to derive algorithms. Ourfirst contribution is to show that the Kullback-Leibler divergence, as anenergy functional, has the unique property (among all f-divergences) thatgradient flows resulting from it do not depend on the normalization constant ofthe target distribution. Our second contribution is to study the choice ofmetric from the perspective of invariance. The Fisher-Rao metric is known asthe unique choice (up to scaling) that is diffeomorphism invariant. As acomputationally tractable alternative, we introduce a relaxed, affineinvariance property for the metrics and gradient flows. In particular, weconstruct various affine invariant Wasserstein and Stein gradient flows. Affineinvariant gradient flows are shown to behave more favorably than theirnon-affine-invariant counterparts when sampling highly anisotropicdistributions, in theory and by using particle methods. Our third contributionis to study, and develop efficient algorithms based on Gaussian approximationsof the gradient flows; this leads to an alternative to particle methods. Weestablish connections between various Gaussian approximate gradient flows,discuss their relation to gradient methods arising from parametric variationalinference, and study their convergence properties both theoretically andnumerically.</description><author>Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, Andrew M Stuart</author><pubDate>Thu, 05 Oct 2023 16:20:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03597v1</guid></item><item><title>OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</title><link>http://arxiv.org/abs/2309.00616v3</link><description>Current 3D open-vocabulary scene understanding methods mostly utilizewell-aligned 2D images as the bridge to learn 3D features with language.However, applying these approaches becomes challenging in scenarios where 2Dimages are absent. In this work, we introduce a new pipeline, namely,OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary sceneunderstanding at the instance level. The OpenIns3D framework employs a"Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic maskproposals in 3D point clouds. The "Snap" module generates synthetic scene-levelimages at multiple scales and leverages 2D vision language models to extractinteresting objects. The "Lookup" module searches through the outcomes of"Snap" with the help of Mask2Pixel maps, which contain the precisecorrespondence between 3D masks and synthetic images, to assign category namesto the proposed masks. This 2D input-free and flexible approach achievesstate-of-the-art results on a wide range of indoor and outdoor datasets by alarge margin. Moreover, OpenIns3D allows for effortless switching of 2Ddetectors without re-training. When integrated with powerful 2D open-worldmodels such as ODISE and GroundingDINO, excellent results were observed onopen-vocabulary instance segmentation. When integrated with LLM-powered 2Dmodels like LISA, it demonstrates a remarkable capacity to process highlycomplex text queries which require intricate reasoning and world knowledge.Project page: https://zheninghuang.github.io/OpenIns3D/</description><author>Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby</author><pubDate>Thu, 05 Oct 2023 16:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00616v3</guid></item><item><title>TimeGPT-1</title><link>http://arxiv.org/abs/2310.03589v1</link><description>In this paper, we introduce TimeGPT, the first foundation model for timeseries, capable of generating accurate predictions for diverse datasets notseen during training. We evaluate our pre-trained model against establishedstatistical, machine learning, and deep learning methods, demonstrating thatTimeGPT zero-shot inference excels in performance, efficiency, and simplicity.Our study provides compelling evidence that insights from other domains ofartificial intelligence can be effectively applied to time series analysis. Weconclude that large-scale time series models offer an exciting opportunity todemocratize access to precise predictions and reduce uncertainty by leveragingthe capabilities of contemporary advancements in deep learning.</description><author>Azul Garza, Max Mergenthaler-Canseco</author><pubDate>Thu, 05 Oct 2023 16:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03589v1</guid></item><item><title>On Convergence of Federated Averaging Langevin Dynamics</title><link>http://arxiv.org/abs/2112.05120v4</link><description>We propose a federated averaging Langevin algorithm (FA-LD) for uncertaintyquantification and mean predictions with distributed clients. In particular, wegeneralize beyond normal posterior distributions and consider a general classof models. We develop theoretical guarantees for FA-LD for strongly log-concavedistributions with non-i.i.d data and study how the injected noise and thestochastic-gradient noise, the heterogeneity of data, and the varying learningrates affect the convergence. Such an analysis sheds light on the optimalchoice of local updates to minimize communication costs. Important to ourapproach is that the communication efficiency does not deteriorate with theinjected noise in the Langevin algorithms. In addition, we examine in our FA-LDalgorithm both independent and correlated noise used over different clients. Weobserve there is a trade-off between the pairs among communication, accuracy,and data privacy. As local devices may become inactive in federated networks,we also show convergence results based on different averaging schemes whereonly partial device updates are available. In such a case, we discover anadditional bias that does not decay to zero.</description><author>Wei Deng, Qian Zhang, Yi-An Ma, Zhao Song, Guang Lin</author><pubDate>Thu, 05 Oct 2023 16:11:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.05120v4</guid></item><item><title>Algebraic and Geometric Models for Space Networking</title><link>http://arxiv.org/abs/2304.01150v2</link><description>In this paper we introduce some new algebraic and geometric perspectives onnetworked space communications. Our main contribution is a novel definition ofa time-varying graph (TVG), defined in terms of a matrix with values in subsetsof the real line P(R). We leverage semi-ring properties of P(R) to modelmulti-hop communication in a TVG using matrix multiplication and a truncatedKleene star. This leads to novel statistics on the communication capacity ofTVGs called lifetime curves, which we generate for large samples of randomlychosen STARLINK satellites, whose connectivity is modeled over day-longsimulations. Determining when a large subsample of STARLINK is temporallystrongly connected is further analyzed using novel metrics introduced here thatare inspired by topological data analysis (TDA). To better model networkingscenarios between the Earth and Mars, we introduce various semi-rings capableof modeling propagation delay as well as protocols common to Delay TolerantNetworking (DTN), such as store-and-forward. Finally, we illustrate theapplicability of zigzag persistence for featurizing different space networksand demonstrate the efficacy of K-Nearest Neighbors (KNN) classification fordistinguishing Earth-Mars and Earth-Moon satellite systems using time-varyingtopology alone.</description><author>William Bernardoni, Robert Cardona, Jacob Cleveland, Justin Curry, Robert Green, Brian Heller, Alan Hylton, Tung Lam, Robert Kassouf-Short</author><pubDate>Thu, 05 Oct 2023 16:09:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01150v2</guid></item><item><title>Smoothing Methods for Automatic Differentiation Across Conditional Branches</title><link>http://arxiv.org/abs/2310.03585v1</link><description>Programs involving discontinuities introduced by control flow constructs suchas conditional branches pose challenges to mathematical optimization methodsthat assume a degree of smoothness in the objective function's responsesurface. Smooth interpretation (SI) is a form of abstract interpretation thatapproximates the convolution of a program's output with a Gaussian kernel, thussmoothing its output in a principled manner. Here, we combine SI with automaticdifferentiation (AD) to efficiently compute gradients of smoothed programs. Incontrast to AD across a regular program execution, these gradients also capturethe effects of alternative control flow paths. The combination of SI with ADenables the direct gradient-based parameter synthesis for branching programs,allowing for instance the calibration of simulation models or their combinationwith neural network models in machine learning pipelines. We detail the effectsof the approximations made for tractability in SI and propose a novel MonteCarlo estimator that avoids the underlying assumptions by estimating thesmoothed programs' gradients through a combination of AD and sampling. UsingDiscoGrad, our tool for automatically translating simple C++ programs to asmooth differentiable form, we perform an extensive evaluation. We compare thecombination of SI with AD and our Monte Carlo estimator to existinggradient-free and stochastic methods on four non-trivial and originallydiscontinuous problems ranging from classical simulation-based optimization toneural network-driven control. While the optimization progress with theSI-based estimator depends on the complexity of the programs' control flow, ourMonte Carlo estimator is competitive in all problems, exhibiting the fastestconvergence by a substantial margin in our highest-dimensional problem.</description><author>Justin N. Kreikemeyer, Philipp Andelfinger</author><pubDate>Thu, 05 Oct 2023 16:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03585v1</guid></item><item><title>Demystifying Oversmoothing in Attention-Based Graph Neural Networks</title><link>http://arxiv.org/abs/2305.16102v2</link><description>Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon whereincreasing network depth leads to homogeneous node representations. Whileprevious work has established that Graph Convolutional Networks (GCNs)exponentially lose expressive power, it remains controversial whether the graphattention mechanism can mitigate oversmoothing. In this work, we provide adefinitive answer to this question through a rigorous mathematical analysis, byviewing attention-based GNNs as nonlinear time-varying dynamical systems andincorporating tools and techniques from the theory of products of inhomogeneousmatrices and the joint spectral radius. We establish that, contrary to popularbelief, the graph attention mechanism cannot prevent oversmoothing and losesexpressive power exponentially. The proposed framework extends the existingresults on oversmoothing for symmetric GCNs to a significantly broader class ofGNN models, including random walk GCNs, Graph Attention Networks (GATs) and(graph) transformers. In particular, our analysis accounts for asymmetric,state-dependent and time-varying aggregation operators and a wide range ofcommon nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.</description><author>Xinyi Wu, Amir Ajorlou, Zihui Wu, Ali Jadbabaie</author><pubDate>Thu, 05 Oct 2023 16:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16102v2</guid></item><item><title>Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End</title><link>http://arxiv.org/abs/2310.03581v1</link><description>Autonomous robots must navigate reliably in unknown environments even undercompromised exteroceptive perception, or perception failures. Such failuresoften occur when harsh environments lead to degraded sensing, or when theperception algorithm misinterprets the scene due to limited generalization. Inthis paper, we model perception failures as invisible obstacles and pits, andtrain a reinforcement learning (RL) based local navigation policy to guide ourlegged robot. Unlike previous works relying on heuristics and anomaly detectionto update navigational information, we train our navigation policy toreconstruct the environment information in the latent space from corruptedperception and react to perception failures end-to-end. To this end, weincorporate both proprioception and exteroception into our policy inputs,thereby enabling the policy to sense collisions on different body parts andpits, prompting corresponding reactions. We validate our approach in simulationand on the real quadruped robot ANYmal running in real-time (&lt;10 ms CPUinference). In a quantitative comparison with existing heuristic-based locallyreactive planners, our policy increases the success rate over 30% when facingperception failures. Project Page: https://bit.ly/45NBTuh.</description><author>Jin Jin, Chong Zhang, Jonas Frey, Nikita Rudin, Matias Mattamala, Cesar Cadena, Marco Hutter</author><pubDate>Thu, 05 Oct 2023 16:01:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03581v1</guid></item><item><title>Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems</title><link>http://arxiv.org/abs/2310.03579v1</link><description>Understanding causal relationships within Gene Regulatory Networks (GRNs) isessential for unraveling the gene interactions in cellular processes. However,causal discovery in GRNs is a challenging problem for multiple reasonsincluding the existence of cyclic feedback loops and uncertainty that yieldsdiverse possible causal structures. Previous works in this area either ignorecyclic dynamics (assume acyclic structure) or struggle with scalability. Weintroduce Swift-DynGFN as a novel framework that enhances causal structurelearning in GRNs while addressing scalability concerns. Specifically,Swift-DynGFN exploits gene-wise independence to boost parallelization and tolower computational cost. Experiments on real single-cell RNA velocity andsynthetic GRN datasets showcase the advancement in learning causal structure inGRNs and scalability in larger systems.</description><author>Trang Nguyen, Alexander Tong, Kanika Madan, Yoshua Bengio, Dianbo Liu</author><pubDate>Thu, 05 Oct 2023 15:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03579v1</guid></item><item><title>Targeted Adversarial Attacks on Generalizable Neural Radiance Fields</title><link>http://arxiv.org/abs/2310.03578v1</link><description>Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for3D scene representation and rendering. These data-driven models can learn tosynthesize high-quality images from sparse 2D observations, enabling realisticand interactive scene reconstructions. However, the growing usage of NeRFs incritical applications such as augmented reality, robotics, and virtualenvironments could be threatened by adversarial attacks. In this paper we present how generalizable NeRFs can be attacked by bothlow-intensity adversarial attacks and adversarial patches, where the latercould be robust enough to be used in real world applications. We alsodemonstrate targeted attacks, where a specific, predefined output scene isgenerated by these attack with success.</description><author>Andras Horvath, Csaba M. Jozsa</author><pubDate>Thu, 05 Oct 2023 15:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03578v1</guid></item><item><title>Module-wise Training of Neural Networks via the Minimizing Movement Scheme</title><link>http://arxiv.org/abs/2309.17357v3</link><description>Greedy layer-wise or module-wise training of neural networks is compelling inconstrained and on-device settings where memory is limited, as it circumvents anumber of problems of end-to-end back-propagation. However, it suffers from astagnation problem, whereby early layers overfit and deeper layers stopincreasing the test accuracy after a certain depth. We propose to solve thisissue by introducing a module-wise regularization inspired by the minimizingmovement scheme for gradient flows in distribution space. We call the methodTRGL for Transport Regularized Greedy Learning and study it theoretically,proving that it leads to greedy modules that are regular and that progressivelysolve the task. Experimentally, we show improved accuracy of module-wisetraining of various architectures such as ResNets, Transformers and VGG, whenour regularization is added, superior to that of other module-wise trainingmethods and often to end-to-end training, with as much as 60% less memoryusage.</description><author>Skander Karkar, Ibrahim Ayed, Emmanuel de Bézenac, Patrick Gallinari</author><pubDate>Thu, 05 Oct 2023 15:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17357v3</guid></item><item><title>Analysis of learning a flow-based generative model from limited sample complexity</title><link>http://arxiv.org/abs/2310.03575v1</link><description>We study the problem of training a flow-based generative model, parametrizedby a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture.We provide a sharp end-to-end analysis of the problem. First, we provide atight closed-form characterization of the learnt velocity field, whenparametrized by a shallow denoising auto-encoder trained on a finite number $n$of samples from the target distribution. Building on this analysis, we providea sharp description of the corresponding generative flow, which pushes the baseGaussian density forward to an approximation of the target density. Inparticular, we provide closed-form formulae for the distance between the meanof the generated mixture and the mean of the target mixture, which we showdecays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in factBayes-optimal.</description><author>Hugo Cui, Florent Krzakala, Eric Vanden-Eijnden, Lenka Zdeborová</author><pubDate>Thu, 05 Oct 2023 15:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03575v1</guid></item><item><title>Residual Multi-Fidelity Neural Network Computing</title><link>http://arxiv.org/abs/2310.03572v1</link><description>In this work, we consider the general problem of constructing a neuralnetwork surrogate model using multi-fidelity information. Given an inexpensivelow-fidelity and an expensive high-fidelity computational model, we present aresidual multi-fidelity computational framework that formulates the correlationbetween models as a residual function, a possibly non-linear mapping between 1)the shared input space of the models together with the low-fidelity modeloutput and 2) the discrepancy between the two model outputs. To accomplishthis, we train two neural networks to work in concert. The first network learnsthe residual function on a small set of high-fidelity and low-fidelity data.Once trained, this network is used to generate additional synthetichigh-fidelity data, which is used in the training of a second network. Thissecond network, once trained, acts as our surrogate for the high-fidelityquantity of interest. We present three numerical examples to demonstrate thepower of the proposed framework. In particular, we show that dramatic savingsin computational cost may be achieved when the output predictions are desiredto be accurate within small tolerances.</description><author>Owen Davis, Mohammad Motamed, Raul Tempone</author><pubDate>Thu, 05 Oct 2023 15:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03572v1</guid></item><item><title>One-Versus-Others Attention: Scalable Multimodal Integration</title><link>http://arxiv.org/abs/2307.05435v2</link><description>Multimodal learning models have become increasingly important as they surpasssingle-modality approaches on diverse tasks ranging from question-answering toautonomous driving. Despite the importance of multimodal learning, existingefforts focus on NLP applications, where the number of modalities is typicallyless than four (audio, video, text, images). However, data inputs in otherdomains, such as the medical field, may include X-rays, PET scans, MRIs,genetic screening, clinical notes, and more, creating a need for both efficientand accurate information fusion. Many state-of-the-art models rely on pairwisecross-modal attention, which does not scale well for applications with morethan three modalities. For $n$ modalities, computing attention will result in$n \choose 2$ operations, potentially requiring considerable amounts ofcomputational resources. To address this, we propose a new domain-neutralattention mechanism, One-Versus-Others (OvO) attention, that scales linearlywith the number of modalities and requires only $n$ attention operations, thusoffering a significant reduction in computational complexity compared toexisting cross-modal attention algorithms. Using three diverse real-worlddatasets as well as an additional simulation experiment, we show that ourmethod improves performance compared to popular fusion techniques whiledecreasing computation costs.</description><author>Michal Golovanevsky, Eva Schiller, Akira Nair, Ritambhara Singh, Carsten Eickhoff</author><pubDate>Thu, 05 Oct 2023 15:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05435v2</guid></item><item><title>Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion</title><link>http://arxiv.org/abs/2302.02394v3</link><description>The recent success of text-to-image generation diffusion models has alsorevolutionized semantic image editing, enabling the manipulation of imagesbased on query/target texts. Despite these advancements, a significantchallenge lies in the potential introduction of contextual prior bias inpre-trained models during image editing, e.g., making unexpected modificationsto inappropriate regions. To address this issue, we present a novel approachcalled Dual-Cycle Diffusion, which generates an unbiased mask to guide imageediting. The proposed model incorporates a Bias Elimination Cycle that consistsof both a forward path and an inverted path, each featuring a StructuralConsistency Cycle to ensure the preservation of image content during theediting process. The forward path utilizes the pre-trained model to produce theedited image, while the inverted path converts the result back to the sourceimage. The unbiased mask is generated by comparing differences between theprocessed source image and the edited image to ensure that both conform to thesame distribution. Our experiments demonstrate the effectiveness of theproposed method, as it significantly improves the D-CLIP score from 0.272 to0.283. The code will be available athttps://github.com/JohnDreamer/DualCycleDiffsion.</description><author>Zuopeng Yang, Tianshu Chu, Xin Lin, Erdun Gao, Daqing Liu, Jie Yang, Chaoyue Wang</author><pubDate>Thu, 05 Oct 2023 15:35:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02394v3</guid></item><item><title>Spatial-temporal associations representation and application for process monitoring using graph convolution neural network</title><link>http://arxiv.org/abs/2205.05250v2</link><description>Thank you very much for the attention and concern of colleagues and scholarsin this work. With the comments and guidance of experts, editors, andreviewers, this work has been accepted for publishing in the journal "ProcessSafety and Environmental Protection". The theme of this paper relies on theSpatial-temporal associations of numerous variables in the same industrialprocesses, which refers to numerous variables obtained in dynamic industrialprocesses with Spatial-temporal correlation characteristics, i.e., thesevariables are not only highly correlated in time but also interrelated inspace. To handle this problem, three key issues need to be well addressed:variable characteristics modeling and representation, graph networkconstruction (temporal information), and graph characteristics perception. Thefirst issue is implemented by assuming the data follows one improved Gaussiandistribution, while the graph network can be defined by the monitoringvariables and their edges which are calculated by their characteristics intime. Finally, these networks corresponding to process states at differenttimes are fed into a graph convolutional neural network to implement graphclassification to achieve process monitoring. A benchmark experiment (TennesseeEastman chemical process) and one application study (cobalt purification fromzinc solution) are employed to demonstrate the feasibility and applicability ofthis paper.</description><author>Hao Ren, Xiaojun Liang, Chunhua Yang, Zhiwen Chen, Weihua Gui</author><pubDate>Thu, 05 Oct 2023 15:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.05250v2</guid></item><item><title>LC-Score: Reference-less estimation of Text Comprehension Difficulty</title><link>http://arxiv.org/abs/2310.02754v2</link><description>Being able to read and understand written text is critical in a digital era.However, studies shows that a large fraction of the population experiencescomprehension issues. In this context, further initiatives in accessibility arerequired to improve the audience text comprehension. However, writers arehardly assisted nor encouraged to produce easy-to-understand content. Moreover,Automatic Text Simplification (ATS) model development suffers from the lack ofmetric to accurately estimate comprehension difficulty We present\textsc{LC-Score}, a simple approach for training text comprehension metric forany French text without reference \ie predicting how easy to understand a giventext is on a $[0, 100]$ scale. Our objective with this scale is toquantitatively capture the extend to which a text suits to the \textit{LangageClair} (LC, \textit{Clear Language}) guidelines, a French initiative closelyrelated to English Plain Language. We explore two approaches: (i) usinglinguistically motivated indicators used to train statistical models, and (ii)neural learning directly from text leveraging pre-trained language models. Weintroduce a simple proxy task for comprehension difficulty training as aclassification task. To evaluate our models, we run two distinct humanannotation experiments, and find that both approaches (indicator based andneural) outperforms commonly used readability and comprehension metrics such asFKGL.</description><author>Paul Tardy, Charlotte Roze, Paul Poupet</author><pubDate>Thu, 05 Oct 2023 15:28:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02754v2</guid></item><item><title>BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields</title><link>http://arxiv.org/abs/2310.03563v1</link><description>We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm whichdefines the image pose estimation problem as a NeRF based iterative linearoptimization. NeRFs are novel neural space representation models that cansynthesize photorealistic novel views of real-world scenes or objects. Ourcontributions are as follows: we extend the localization optimization objectivewith a depth-based loss function, we introduce a multi-image based lossfunction where a sequence of images with known relative poses are used withoutincreasing the computational complexity, we omit hierarchical sampling duringvolumetric rendering, meaning only the coarse model is used for poseestimation, and we how that by extending the sampling interval convergence canbe achieved even or higher initial pose estimate errors. With the proposedmodifications the convergence speed is significantly improved, and the basin ofconvergence is substantially extended.</description><author>Ágoston István Csehi, Csaba Máté Józsa</author><pubDate>Thu, 05 Oct 2023 15:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03563v1</guid></item><item><title>Towards Inferential Reproducibility of Machine Learning Research</title><link>http://arxiv.org/abs/2302.04054v6</link><description>Reliability of machine learning evaluation -- the consistency of observedevaluation scores across replicated model training runs -- is affected byseveral sources of nondeterminism which can be regarded as measurement noise.Current tendencies to remove noise in order to enforce reproducibility ofresearch results neglect inherent nondeterminism at the implementation leveland disregard crucial interaction effects between algorithmic noise factors anddata properties. This limits the scope of conclusions that can be drawn fromsuch experiments. Instead of removing noise, we propose to incorporate severalsources of variance, including their interaction with data properties, into ananalysis of significance and reliability of machine learning evaluation, withthe aim to draw inferences beyond particular instances of trained models. Weshow how to use linear mixed effects models (LMEMs) to analyze performanceevaluation scores, and to conduct statistical inference with a generalizedlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sourcesof noise like meta-parameter variations into statistical significance testing,and to assess performance differences conditional on data properties.Furthermore, a variance component analysis (VCA) enables the analysis of thecontribution of noise sources to overall variance and the computation of areliability coefficient by the ratio of substantial to total variance.</description><author>Michael Hagmann, Philipp Meier, Stefan Riezler</author><pubDate>Thu, 05 Oct 2023 15:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04054v6</guid></item><item><title>Redefining Digital Health Interfaces with Large Language Models</title><link>http://arxiv.org/abs/2310.03560v1</link><description>Digital health tools have the potential to significantly improve the deliveryof healthcare services. However, their use remains comparatively limited due,in part, to challenges surrounding usability and trust. Recently, LargeLanguage Models (LLMs) have emerged as general-purpose models with the abilityto process complex information and produce human-quality text, presenting awealth of potential applications in healthcare. Directly applying LLMs inclinical settings is not straightforward, with LLMs susceptible to providinginconsistent or nonsensical answers. We demonstrate how LLMs can utilizeexternal tools to provide a novel interface between clinicians and digitaltechnologies. This enhances the utility and practical impact of digitalhealthcare tools and AI models while addressing current issues with using LLMin clinical settings such as hallucinations. We illustrate our approach withexamples from cardiovascular disease and diabetes risk prediction, highlightingthe benefit compared to traditional interfaces for digital tools.</description><author>Fergus Imrie, Paulius Rauba, Mihaela van der Schaar</author><pubDate>Thu, 05 Oct 2023 15:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03560v1</guid></item><item><title>Large-scale investigation of weakly-supervised deep learning for the fine-grained semantic indexing of biomedical literature</title><link>http://arxiv.org/abs/2301.09350v2</link><description>Objective: Semantic indexing of biomedical literature is usually done at thelevel of MeSH descriptors with several related but distinct biomedical conceptsoften grouped together and treated as a single topic. This study proposes a newmethod for the automated refinement of subject annotations at the level of MeSHconcepts. Methods: Lacking labelled data, we rely on weak supervision based onconcept occurrence in the abstract of an article, which is also enhanced bydictionary-based heuristics. In addition, we investigate deep learningapproaches, making design choices to tackle the particular challenges of thistask. The new method is evaluated on a large-scale retrospective scenario,based on concepts that have been promoted to descriptors. Results: In ourexperiments concept occurrence was the strongest heuristic achieving a macro-F1score of about 0.63 across several labels. The proposed method improved itfurther by more than 4pp. Conclusion: The results suggest that conceptoccurrence is a strong heuristic for refining the coarse-grained labels at thelevel of MeSH concepts and the proposed method improves it further.</description><author>Anastasios Nentidis, Thomas Chatzopoulos, Anastasia Krithara, Grigorios Tsoumakas, Georgios Paliouras</author><pubDate>Thu, 05 Oct 2023 15:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09350v2</guid></item><item><title>MedSynV1: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images</title><link>http://arxiv.org/abs/2310.03559v1</link><description>This paper introduces an innovative methodology for producing high-quality 3Dlung CT images guided by textual information. While diffusion-based generativemodels are increasingly used in medical imaging, current state-of-the-artapproaches are limited to low-resolution outputs and underutilize radiologyreports' abundant information. The radiology reports can enhance the generationprocess by providing additional guidance and offering fine-grained control overthe synthesis of images. Nevertheless, expanding text-guided generation tohigh-resolution 3D images poses significant memory and anatomicaldetail-preserving challenges. Addressing the memory issue, we introduce ahierarchical scheme that uses a modified UNet architecture. We start bysynthesizing low-resolution images conditioned on the text, serving as afoundation for subsequent generators for complete volumetric data. To ensurethe anatomical plausibility of the generated samples, we provide furtherguidance by generating vascular, airway, and lobular segmentation masks inconjunction with the CT images. The model demonstrates the capability to usetextual input and segmentation tasks to generate synthesized images. Theresults of comparative assessments indicate that our approach exhibits superiorperformance compared to the most advanced models based on GAN and diffusiontechniques, especially in accurately retaining crucial anatomical features suchas fissure lines, airways, and vascular structures. This innovation introducesnovel possibilities. This study focuses on two main objectives: (1) thedevelopment of a method for creating images based on textual prompts andanatomical components, and (2) the capability to generate new imagesconditioning on anatomical elements. The advancements in image generation canbe applied to enhance numerous downstream tasks.</description><author>Yanwu Xu, Li Sun, Wei Peng, Shyam Visweswaran, Kayhan Batmanghelich</author><pubDate>Thu, 05 Oct 2023 15:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03559v1</guid></item><item><title>Improving Facade Parsing with Vision Transformers and Line Integration</title><link>http://arxiv.org/abs/2309.15523v4</link><description>Facade parsing stands as a pivotal computer vision task with far-reachingapplications in areas like architecture, urban planning, and energy efficiency.Despite the recent success of deep learning-based methods in yieldingimpressive results on certain open-source datasets, their viability forreal-world applications remains uncertain. Real-world scenarios areconsiderably more intricate, demanding greater computational efficiency.Existing datasets often fall short in representing these settings, and previousmethods frequently rely on extra models to enhance accuracy, which requiresmuch computation cost. In this paper, we introduce Comprehensive Facade Parsing(CFP), a dataset meticulously designed to encompass the intricacies ofreal-world facade parsing tasks. Comprising a total of 602 high-resolutionstreet-view images, this dataset captures a diverse array of challengingscenarios, including sloping angles and densely clustered buildings, withpainstakingly curated annotations for each image. We introduce a new pipelineknown as Revision-based Transformer Facade Parsing (RTFP). This marks thepioneering utilization of Vision Transformers (ViT) in facade parsing, and ourexperimental results definitively substantiate its merit. We also design LineAcquisition, Filtering, and Revision (LAFR), an efficient yet accurate revisionalgorithm that can improve the segment result solely from simple line detectionusing prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP,we evaluate the superiority of our method. The dataset and code are availableat https://github.com/wbw520/RTFP.</description><author>Bowen Wang, Jiaxing Zhang, Ran Zhang, Yunqin Li, Liangzhi Li, Yuta Nakashima</author><pubDate>Thu, 05 Oct 2023 15:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15523v4</guid></item></channel></rss>