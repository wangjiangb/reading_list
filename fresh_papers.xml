<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Nov 2024 13:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</title><link>http://arxiv.org/abs/2411.14432v1</link><description>Large Language Models (LLMs) demonstrate enhanced capabilities andreliability by reasoning more, evolving from Chain-of-Thought prompting toproduct-level solutions like OpenAI o1. Despite various efforts to improve LLMreasoning, high-quality long-chain reasoning data and optimized trainingpipelines still remain inadequately explored in vision-language tasks. In thispaper, we present Insight-V, an early effort to 1) scalably produce long androbust reasoning data for complex multi-modal tasks, and 2) an effectivetraining pipeline to enhance the reasoning capabilities of multi-modal largelanguage models (MLLMs). Specifically, to create long and structured reasoningdata without human labor, we design a two-step pipeline with a progressivestrategy to generate sufficiently long and diverse reasoning paths and amulti-granularity assessment method to ensure data quality. We observe thatdirectly supervising MLLMs with such long and complex reasoning data will notyield ideal reasoning ability. To tackle this problem, we design a multi-agentsystem consisting of a reasoning agent dedicated to performing long-chainreasoning and a summary agent trained to judge and summarize reasoning results.We further incorporate an iterative DPO algorithm to enhance the reasoningagent's generation stability and quality. Based on the popular LLaVA-NeXT modeland our stronger base MLLM, we demonstrate significant performance gains acrosschallenging multi-modal benchmarks requiring visual reasoning. Benefiting fromour multi-agent system, Insight-V can also easily maintain or improveperformance on perception-focused multi-modal tasks.</description><author>Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu</author><pubDate>Thu, 21 Nov 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14432v1</guid></item><item><title>Stable Flow: Vital Layers for Training-Free Image Editing</title><link>http://arxiv.org/abs/2411.14430v1</link><description>Diffusion models have revolutionized the field of content synthesis andediting. Recent models have replaced the traditional UNet architecture with theDiffusion Transformer (DiT), and employed flow-matching for improved trainingand sampling. However, they exhibit limited generation diversity. In this work,we leverage this limitation to perform consistent image edits via selectiveinjection of attention features. The main challenge is that, unlike theUNet-based models, DiT lacks a coarse-to-fine synthesis structure, making itunclear in which layers to perform the injection. Therefore, we propose anautomatic method to identify "vital layers" within DiT, crucial for imageformation, and demonstrate how these layers facilitate a range of controlledstable edits, from non-rigid modifications to object addition, using the samemechanism. Next, to enable real-image editing, we introduce an improved imageinversion method for flow models. Finally, we evaluate our approach throughqualitative and quantitative comparisons, along with a user study, anddemonstrate its effectiveness across multiple applications. The project page isavailable at https://omriavrahami.com/stable-flow</description><author>Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, Daniel Cohen-Or</author><pubDate>Thu, 21 Nov 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14430v1</guid></item><item><title>Revisiting the Integration of Convolution and Attention for Vision Backbone</title><link>http://arxiv.org/abs/2411.14429v1</link><description>Convolutions (Convs) and multi-head self-attentions (MHSAs) are typicallyconsidered alternatives to each other for building vision backbones. Althoughsome works try to integrate both, they apply the two operators simultaneouslyat the finest pixel granularity. With Convs responsible for per-pixel featureextraction already, the question is whether we still need to include the heavyMHSAs at such a fine-grained level. In fact, this is the root cause of thescalability issue w.r.t. the input resolution for vision transformers. Toaddress this important problem, we propose in this work to use MSHAs and Convsin parallel \textbf{at different granularity levels} instead. Specifically, ineach layer, we use two different ways to represent an image: a fine-grainedregular grid and a coarse-grained set of semantic slots. We apply differentoperations to these two representations: Convs to the grid for local features,and MHSAs to the slots for global features. A pair of fully differentiable softclustering and dispatching modules is introduced to bridge the grid and setrepresentations, thus enabling local-global fusion. Through extensiveexperiments on various vision tasks, we empirically verify the potential of theproposed integration scheme, named \textit{GLMix}: by offloading the burden offine-grained features to light-weight Convs, it is sufficient to use MHSAs in afew (e.g., 64) semantic slots to match the performance of recentstate-of-the-art backbones, while being more efficient. Our visualizationresults also demonstrate that the soft clustering module produces a meaningfulsemantic grouping effect with only IN1k classification supervision, which mayinduce better interpretability and inspire new weakly-supervised semanticsegmentation approaches. Code will be available at\url{https://github.com/rayleizhu/GLMix}.</description><author>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau</author><pubDate>Thu, 21 Nov 2024 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14429v1</guid></item><item><title>Whack-a-Chip: The Futility of Hardware-Centric Export Controls</title><link>http://arxiv.org/abs/2411.14425v1</link><description>U.S. export controls on semiconductors are widely known to be permeable, withthe People's Republic of China (PRC) steadily creating state-of-the-artartificial intelligence (AI) models with exfiltrated chips. This paper presentsthe first concrete, public evidence of how leading PRC AI labs evade andcircumvent U.S. export controls. We examine how Chinese companies, notablyTencent, are not only using chips that are restricted under U.S. exportcontrols but are also finding ways to circumvent these regulations by usingsoftware and modeling techniques that maximize less capable hardware.Specifically, we argue that Tencent's ability to power its Hunyuan-Large modelwith non-export controlled NVIDIA H20s exemplifies broader gains in efficiencyin machine learning that have eroded the moat that the United States initiallybuilt via its existing export controls. Finally, we examine the implications ofthis finding for the future of the United States' export control strategy.</description><author>Ritwik Gupta, Leah Walker, Andrew W. Reddie</author><pubDate>Thu, 21 Nov 2024 18:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14425v1</guid></item><item><title>Learning Fair Robustness via Domain Mixup</title><link>http://arxiv.org/abs/2411.14424v1</link><description>Adversarial training is one of the predominant techniques for trainingclassifiers that are robust to adversarial attacks. Recent work, however hasfound that adversarial training, which makes the overall classifier robust, itdoes not necessarily provide equal amount of robustness for all classes. Inthis paper, we propose the use of mixup for the problem of learning fair robustclassifiers, which can provide similar robustness across all classes.Specifically, the idea is to mix inputs from the same classes and performadversarial training on mixed up inputs. We present a theoretical analysis ofthis idea for the case of linear classifiers and show that mixup combined withadversarial training can provably reduce the class-wise robustness disparity.This method not only contributes to reducing the disparity in class-wiseadversarial risk, but also the class-wise natural risk. Complementing ourtheoretical analysis, we also provide experimental results on both syntheticdata and the real world dataset (CIFAR-10), which shows improvement in classwise disparities for both natural and adversarial risks.</description><author>Meiyu Zhong, Ravi Tandon</author><pubDate>Thu, 21 Nov 2024 18:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14424v1</guid></item><item><title>Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation</title><link>http://arxiv.org/abs/2411.14423v1</link><description>Realistic simulation of dynamic scenes requires accurately capturing diversematerial properties and modeling complex object interactions grounded inphysical principles. However, existing methods are constrained to basicmaterial types with limited predictable parameters, making them insufficient torepresent the complexity of real-world materials. We introduce a novel approachthat leverages multi-modal foundation models and video diffusion to achieveenhanced 4D dynamic scene simulation. Our method utilizes multi-modal models toidentify material types and initialize material parameters through imagequeries, while simultaneously inferring 3D Gaussian splats for detailed scenerepresentation. We further refine these material parameters using videodiffusion with a differentiable Material Point Method (MPM) and optical flowguidance rather than render loss or Score Distillation Sampling (SDS) loss.This integrated framework enables accurate prediction and realistic simulationof dynamic interactions in real-world scenarios, advancing both accuracy andflexibility in physics-based simulations.</description><author>Zhuoman Liu, Weicai Ye, Yan Luximon, Pengfei Wan, Di Zhang</author><pubDate>Thu, 21 Nov 2024 18:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14423v1</guid></item><item><title>From RNNs to Foundation Models: An Empirical Study on Commercial Building Energy Consumption</title><link>http://arxiv.org/abs/2411.14421v1</link><description>Accurate short-term energy consumption forecasting for commercial buildingsis crucial for smart grid operations. While smart meters and deep learningmodels enable forecasting using past data from multiple buildings, dataheterogeneity from diverse buildings can reduce model performance. The impactof increasing dataset heterogeneity in time series forecasting, while keepingsize and model constant, is understudied. We tackle this issue using theComStock dataset, which provides synthetic energy consumption data for U.S.commercial buildings. Two curated subsets, identical in size and region butdiffering in building type diversity, are used to assess the performance ofvarious time series forecasting models, including fine-tuned open-sourcefoundation models (FMs). The results show that dataset heterogeneity and modelarchitecture have a greater impact on post-training forecasting performancethan the parameter count. Moreover, despite the higher computational cost,fine-tuned FMs demonstrate competitive performance compared to base modelstrained from scratch.</description><author>Shourya Bose, Yijiang Li, Amy Van Sant, Yu Zhang, Kibaek Kim</author><pubDate>Thu, 21 Nov 2024 18:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14421v1</guid></item><item><title>Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Model</title><link>http://arxiv.org/abs/2408.00754v2</link><description>Multimodal language models (MLLMs) are increasingly being applied inreal-world environments, necessitating their ability to interpret 3D spaces andcomprehend temporal dynamics. Current methods often rely on specializedarchitectural designs or task-specific fine-tuning to achieve this. Weintroduce Coarse Correspondences, a simple lightweight method that enhancesMLLMs' spatial-temporal reasoning with 2D images as input, without modifyingthe architecture or requiring task-specific fine-tuning. Our method uses alightweight tracking model to identify primary object correspondences betweenframes in a video or across different image viewpoints, and then conveys thisinformation to MLLMs through visual prompting. We demonstrate that this simpletraining-free approach brings substantial gains to GPT4-V/O consistently onfour benchmarks that require spatial-temporal reasoning, including +20.5\%improvement on ScanQA, +9.7\% on OpenEQA's episodic memory subset, +6.0\% onthe long-form video benchmark EgoSchema, and +11\% on the R2R navigationbenchmark. Additionally, we show that Coarse Correspondences can also enhanceopen-source MLLMs' spatial reasoning (by +6.9\% on ScanQA) when applied in bothtraining and inference and that the improvement can generalize to unseendatasets such as SQA3D (+3.1\%). Taken together, we show that CoarseCorrespondences effectively and efficiently boosts models' performance ondownstream tasks requiring spatial-temporal reasoning.</description><author>Benlin Liu, Yuhao Dong, Yiqin Wang, Zixian Ma, Yansong Tang, Luming Tang, Yongming Rao, Wei-Chiu Ma, Ranjay Krishna</author><pubDate>Thu, 21 Nov 2024 18:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00754v2</guid></item><item><title>Multimodal 3D Brain Tumor Segmentation with Adversarial Training and Conditional Random Field</title><link>http://arxiv.org/abs/2411.14418v1</link><description>Accurate brain tumor segmentation remains a challenging task due tostructural complexity and great individual differences of gliomas. Leveragingthe pre-eminent detail resilience of CRF and spatial feature extractioncapacity of V-net, we propose a multimodal 3D Volume Generative AdversarialNetwork (3D-vGAN) for precise segmentation. The model utilizes Pseudo-3D forV-net improvement, adds conditional random field after generator and useoriginal image as supplemental guidance. Results, using the BraTS-2018 dataset,show that 3D-vGAN outperforms classical segmentation models, including U-net,Gan, FCN and 3D V-net, reaching specificity over 99.8%.</description><author>Lan Jiang, Yuchao Zheng, Miao Yu, Haiqing Zhang, Fatemah Aladwani, Alessandro Perelli</author><pubDate>Thu, 21 Nov 2024 18:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14418v1</guid></item><item><title>Adversarial Poisoning Attack on Quantum Machine Learning Models</title><link>http://arxiv.org/abs/2411.14412v1</link><description>With the growing interest in Quantum Machine Learning (QML) and theincreasing availability of quantum computers through cloud providers,addressing the potential security risks associated with QML has become anurgent priority. One key concern in the QML domain is the threat of datapoisoning attacks in the current quantum cloud setting. Adversarial access totraining data could severely compromise the integrity and availability of QMLmodels. Classical data poisoning techniques require significant knowledge andtraining to generate poisoned data, and lack noise resilience, making themineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era.In this work, we first propose a simple yet effective technique to measureintra-class encoder state similarity (ESS) by analyzing the outputs of encodingcircuits. Leveraging this approach, we introduce a quantum indiscriminate datapoisoning attack, QUID. Through extensive experiments conducted in bothnoiseless and noisy environments (e.g., IBM\_Brisbane's noise), across variousarchitectures and datasets, QUID achieves up to $92\%$ accuracy degradation inmodel performance compared to baseline models and up to $75\%$ accuracydegradation compared to random label-flipping. We also tested QUID againststate-of-the-art classical defenses, with accuracy degradation still exceeding$50\%$, demonstrating its effectiveness. This work represents the first attemptto reevaluate data poisoning attacks in the context of QML.</description><author>Satwik Kundu, Swaroop Ghosh</author><pubDate>Thu, 21 Nov 2024 18:46:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14412v1</guid></item><item><title>Multi-Agent Environments for Vehicle Routing Problems</title><link>http://arxiv.org/abs/2411.14411v1</link><description>Research on Reinforcement Learning (RL) approaches for discrete optimizationproblems has increased considerably, extending RL to an area classicallydominated by Operations Research (OR). Vehicle routing problems are a goodexample of discrete optimization problems with high practical relevance whereRL techniques have had considerable success. Despite these advances,open-source development frameworks remain scarce, hampering both the testing ofalgorithms and the ability to objectively compare results. This ultimatelyslows down progress in the field and limits the exchange of ideas between theRL and OR communities. Here we propose a library composed of multi-agent environments that simulatesclassic vehicle routing problems. The library, built on PyTorch, provides aflexible modular architecture design that allows easy customization andincorporation of new routing problems. It follows the Agent Environment Cycle("AEC") games model and has an intuitive API, enabling rapid adoption and easyintegration into existing reinforcement learning frameworks. The library allows for a straightforward use of classical OR benchmarkinstances in order to narrow the gap between the test beds for algorithmbenchmarking used by the RL and OR communities. Additionally, we providebenchmark instance sets for each environment, as well as baseline RL models andtraining code.</description><author>Ricardo Gama, Daniel Fuertes, Carlos R. del-Blanco, Hugo L. Fernandes</author><pubDate>Thu, 21 Nov 2024 18:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14411v1</guid></item><item><title>Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</title><link>http://arxiv.org/abs/2411.14405v1</link><description>Currently OpenAI o1 has sparked a surge of interest in the study of largereasoning models (LRM). Building on this momentum, Marco-o1 not only focuses ondisciplines with standard answers, such as mathematics, physics, and coding --which are well-suited for reinforcement learning (RL) -- but also placesgreater emphasis on open-ended resolutions. We aim to address the question:"Can the o1 model effectively generalize to broader domains where clearstandards are absent and rewards are challenging to quantify?" Marco-o1 ispowered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS),reflection mechanisms, and innovative reasoning strategies -- optimized forcomplex real-world problem-solving tasks.</description><author>Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang</author><pubDate>Thu, 21 Nov 2024 18:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14405v1</guid></item><item><title>Resolving Multiple-Dynamic Model Uncertainty in Hypothesis-Driven Belief-MDPs</title><link>http://arxiv.org/abs/2411.14404v1</link><description>When human operators of cyber-physical systems encounter surprising behavior,they often consider multiple hypotheses that might explain it. In some cases,taking information-gathering actions such as additional measurements or controlinputs given to the system can help resolve uncertainty and determine the mostaccurate hypothesis. The task of optimizing these actions can be formulated asa belief-space Markov decision process that we call a hypothesis-driven beliefMDP. Unfortunately, this problem suffers from the curse of history similar to apartially observable Markov decision process (POMDP). To plan in continuousdomains, an agent needs to reason over countlessly many possibleaction-observation histories, each resulting in a different belief over theunknown state. The problem is exacerbated in the hypothesis-driven contextbecause each action-observation pair spawns a different belief for eachhypothesis, leading to additional branching. This paper considers the case inwhich each hypothesis corresponds to a different dynamic model in an underlyingPOMDP. We present a new belief MDP formulation that: (i) enables reasoning overmultiple hypotheses, (ii) balances the goals of determining the (most likely)correct hypothesis and performing well in the underlying POMDP, and (iii) canbe solved with sparse tree search.</description><author>Ofer Dagan, Tyler Becker, Zachary N. Sunberg</author><pubDate>Thu, 21 Nov 2024 18:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14404v1</guid></item><item><title>Pushing the Limits of Sparsity: A Bag of Tricks for Extreme Pruning</title><link>http://arxiv.org/abs/2411.13545v2</link><description>Pruning of deep neural networks has been an effective technique for reducingmodel size while preserving most of the performance of dense networks, crucialfor deploying models on memory and power-constrained devices. While recentsparse learning methods have shown promising performance up to moderatesparsity levels such as 95% and 98%, accuracy quickly deteriorates when pushingsparsities to extreme levels. Obtaining sparse networks at such extremesparsity levels presents unique challenges, such as fragile gradient flow andheightened risk of layer collapse. In this work, we explore network performancebeyond the commonly studied sparsities, and propose a collection of techniquesthat enable the continuous learning of networks without accuracy collapse evenat extreme sparsities, including 99.90%, 99.95% and 99.99% on ResNetarchitectures. Our approach combines 1) Dynamic ReLU phasing, where DyReLUinitially allows for richer parameter exploration before being graduallyreplaced by standard ReLU, 2) weight sharing which reuses parameters within aresidual layer while maintaining the same number of learnable parameters, and3) cyclic sparsity, where both sparsity levels and sparsity patterns evolvedynamically throughout training to better encourage parameter exploration. Weevaluate our method, which we term Extreme Adaptive Sparse Training (EAST) atextreme sparsities using ResNet-34 and ResNet-50 on CIFAR-10, CIFAR-100, andImageNet, achieving significant performance improvements over state-of-the-artmethods we compared with.</description><author>Andy Li, Aiden Durrant, Milan Markovic, Lu Yin, Georgios Leontidis</author><pubDate>Thu, 21 Nov 2024 18:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13545v2</guid></item><item><title>Landing Trajectory Prediction for UAS Based on Generative Adversarial Network</title><link>http://arxiv.org/abs/2411.14403v1</link><description>Models for trajectory prediction are an essential component of many advancedair mobility studies. These models help aircraft detect conflict and planavoidance maneuvers, which is especially important in Unmanned Aircraft systems(UAS) landing management due to the congested airspace near vertiports. In thispaper, we propose a landing trajectory prediction model for UAS based onGenerative Adversarial Network (GAN). The GAN is a prestigious neural networkthat has been developed for many years. In previous research, GAN has achievedmany state-of-the-art results in many generation tasks. The GAN consists of oneneural network generator and a neural network discriminator. Because of thelearning capacity of the neural networks, the generator is capable tounderstand the features of the sample trajectory. The generator takes theprevious trajectory as input and outputs some random status of a flight.According to the results of the experiences, the proposed model can output moreaccurate predictions than the baseline method(GMR) in various datasets. Toevaluate the proposed model, we also create a real UAV landing dataset thatincludes more than 2600 trajectories of drone control manually by real pilots.</description><author>Jun Xiang, Drake Essick, Luiz Gonzalez Bautista, Junfei Xie, Jun Chen</author><pubDate>Thu, 21 Nov 2024 18:34:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14403v1</guid></item><item><title>Multimodal Autoregressive Pre-training of Large Vision Encoders</title><link>http://arxiv.org/abs/2411.14402v1</link><description>We introduce a novel method for pre-training of large-scale vision encoders.Building on recent advancements in autoregressive pre-training of visionmodels, we extend this framework to a multimodal setting, i.e., images andtext. In this paper, we present AIMV2, a family of generalist vision encoderscharacterized by a straightforward pre-training process, scalability, andremarkable performance across a range of downstream tasks. This is achieved bypairing the vision encoder with a multimodal decoder that autoregressivelygenerates raw image patches and text tokens. Our encoders excel not only inmultimodal evaluations but also in vision benchmarks such as localization,grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5%accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistentlyoutperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) inmultimodal image understanding across diverse settings.</description><author>Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis Béthune, Zhe Gan, Alexander T Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, Alaaeldin El-Nouby</author><pubDate>Thu, 21 Nov 2024 18:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14402v1</guid></item><item><title>Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding</title><link>http://arxiv.org/abs/2411.14401v1</link><description>Recent advancements in multimodal large language models (MLLMs) have openednew avenues for video understanding. However, achieving high fidelity inzero-shot video tasks remains challenging. Traditional video processing methodsrely heavily on fine-tuning to capture nuanced spatial-temporal details, whichincurs significant data and computation costs. In contrast, training-freeapproaches, though efficient, often lack robustness in preserving context-richfeatures across complex video content. To this end, we propose DYTO, a noveldynamic token merging framework for zero-shot video understanding thatadaptively optimizes token efficiency while preserving crucial scene details.DYTO integrates a hierarchical frame selection and a bipartite token mergingstrategy to dynamically cluster key frames and selectively compress tokensequences, striking a balance between computational efficiency with semanticrichness. Extensive experiments across multiple benchmarks demonstrate theeffectiveness of DYTO, achieving superior performance compared to bothfine-tuned and training-free methods and setting a new state-of-the-art forzero-shot video understanding.</description><author>Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zenghui Ding, Xianjun Yang, Yining Sun</author><pubDate>Thu, 21 Nov 2024 18:30:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14401v1</guid></item><item><title>Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings</title><link>http://arxiv.org/abs/2411.14398v1</link><description>With the recent proliferation of large language models (LLMs), enterpriseshave been able to rapidly develop proof-of-concepts and prototypes. As aresult, there is a growing need to implement robust guardrails that monitor,quantize and control an LLM's behavior, ensuring that the use is reliable,safe, accurate and also aligned with the users' expectations. Previousapproaches for filtering out inappropriate user prompts or system outputs, suchas LlamaGuard and OpenAI's MOD API, have achieved significant success byfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrailsintroduces increased latency and higher maintenance costs, which may not bepractical or scalable for cost-efficient deployments. We take a differentapproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.This method reduces the model size from LlamaGuard's 7 billion parameters toapproximately 67 million, while maintaining comparable performance on the AEGISsafety benchmark.</description><author>Aaron Zheng, Mansi Rana, Andreas Stolcke</author><pubDate>Thu, 21 Nov 2024 18:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14398v1</guid></item><item><title>POS-tagging to highlight the skeletal structure of sentences</title><link>http://arxiv.org/abs/2411.14393v1</link><description>This study presents the development of a part-of-speech (POS) tagging modelto extract the skeletal structure of sentences using transfer learning with theBERT architecture for token classification. The model, fine-tuned on Russiantext, demonstrating its effectiveness. The approach offers potentialapplications in enhancing natural language processing tasks, such as improvingmachine translation. Keywords: part of speech tagging, morphological analysis, natural languageprocessing, BERT.</description><author>Grigorii Churakov</author><pubDate>Thu, 21 Nov 2024 18:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14393v1</guid></item><item><title>Persistent Homology for Structural Characterization in Disordered Systems</title><link>http://arxiv.org/abs/2411.14390v1</link><description>We propose a unified framework based on persistent homology (PH) tocharacterize both local and global structures in disordered systems. It cansimultaneously generate local and global descriptors using the same algorithmand data structure, and has shown to be highly effective and interpretable inpredicting particle rearrangements and classifying global phases. Based on thisframework, we define a non-parametric metric, the Separation Index (SI), whichnot only outperforms traditional bond-orientational order parameters in phaseclassification tasks but also establishes a connection between particleenvironments and the global phase structure. Our methods provide an effectiveframework for understanding and analyzing the properties of disorderedmaterials, with broad potential applications in materials science and evenwider studies of complex systems.</description><author>An Wang, Li Zou</author><pubDate>Thu, 21 Nov 2024 18:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14390v1</guid></item><item><title>Enhancing Diagnostic Precision in Gastric Bleeding through Automated Lesion Segmentation: A Deep DuS-KFCM Approach</title><link>http://arxiv.org/abs/2411.14385v1</link><description>Timely and precise classification and segmentation of gastric bleeding inendoscopic imagery are pivotal for the rapid diagnosis and intervention ofgastric complications, which is critical in life-saving medical procedures.Traditional methods grapple with the challenge posed by the indistinguishableintensity values of bleeding tissues adjacent to other gastric structures. Ourstudy seeks to revolutionize this domain by introducing a novel deep learningmodel, the Dual Spatial Kernelized Constrained Fuzzy C-Means (Deep DuS-KFCM)clustering algorithm. This Hybrid Neuro-Fuzzy system synergizes Neural Networkswith Fuzzy Logic to offer a highly precise and efficient identification ofbleeding regions. Implementing a two-fold coarse-to-fine strategy forsegmentation, this model initially employs the Spatial Kernelized Fuzzy C-Means(SKFCM) algorithm enhanced with spatial intensity profiles and subsequentlyharnesses the state-of-the-art DeepLabv3+ with ResNet50 architecture to refinethe segmentation output. Through extensive experiments across mainstreamgastric bleeding and red spots datasets, our Deep DuS-KFCM model demonstratedunprecedented accuracy rates of 87.95%, coupled with a specificity of 96.33%,outperforming contemporary segmentation methods. The findings underscore themodel's robustness against noise and its outstanding segmentation capabilities,particularly for identifying subtle bleeding symptoms, thereby presenting asignificant leap forward in medical image processing.</description><author>Xian-Xian Liu, Mingkun Xu, Yuanyuan Wei, Huafeng Qin, Qun Song, Simon Fong, Feng Tien, Wei Luo, Juntao Gao, Zhihua Zhang, Shirley Siu</author><pubDate>Thu, 21 Nov 2024 18:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14385v1</guid></item><item><title>Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation</title><link>http://arxiv.org/abs/2411.14384v1</link><description>Existing feed-forward image-to-3D methods mainly rely on 2D multi-viewdiffusion models that cannot guarantee 3D consistency. These methods easilycollapse when changing the prompt view direction and mainly handleobject-centric prompt images. In this paper, we propose a novel single-stage 3Ddiffusion model, DiffusionGS, for object and scene generation from a singleview. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep toenforce view consistency and allow the model to generate robustly given promptviews of any directions, beyond object-centric inputs. Plus, to improve thecapability and generalization ability of DiffusionGS, we scale up 3D trainingdata by developing a scene-object mixed training strategy. Experiments showthat our method enjoys better generation quality (2.20 dB higher in PSNR and23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTAmethods. The user study and text-to-3D applications also reveals the practicalvalues of our method. Our Project page athttps://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video andinteractive generation results.</description><author>Yuanhao Cai, He Zhang, Kai Zhang, Yixun Liang, Mengwei Ren, Fujun Luan, Qing Liu, Soo Ye Kim, Jianming Zhang, Zhifei Zhang, Yuqian Zhou, Zhe Lin, Alan Yuille</author><pubDate>Thu, 21 Nov 2024 18:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14384v1</guid></item><item><title>A Sociotechnical Lens for Evaluating Computer Vision Models: A Case Study on Detecting and Reasoning about Gender and Emotion</title><link>http://arxiv.org/abs/2406.08222v2</link><description>In the evolving landscape of computer vision (CV) technologies, the automaticdetection and interpretation of gender and emotion in images is a critical areaof study. This paper investigates social biases in CV models, emphasizing thelimitations of traditional evaluation metrics such as precision, recall, andaccuracy. These metrics often fall short in capturing the complexities ofgender and emotion, which are fluid and culturally nuanced constructs. Ourstudy proposes a sociotechnical framework for evaluating CV models,incorporating both technical performance measures and considerations of socialfairness. Using a dataset of 5,570 images related to vaccination and climatechange, we empirically compared the performance of various CV models, includingtraditional models like DeepFace and FER, and generative models like GPT-4Vision. Our analysis involved manually validating the gender and emotionalexpressions in a subset of images to serve as benchmarks. Our findings revealthat while GPT-4 Vision outperforms other models in technical accuracy forgender classification, it exhibits discriminatory biases, particularly inresponse to transgender and non-binary personas. Furthermore, the model'semotion detection skew heavily towards positive emotions, with a notable biastowards associating female images with happiness, especially when prompted bymale personas. These findings underscore the necessity of developing morecomprehensive evaluation criteria that address both validity and discriminatorybiases in CV models. Our proposed framework provides guidelines for researchersto critically assess CV tools, ensuring their application in communicationresearch is both ethical and effective. The significant contribution of thisstudy lies in its emphasis on a sociotechnical approach, advocating for CVtechnologies that support social good and mitigate biases rather thanperpetuate them.</description><author>Sha Luo, Sang Jung Kim, Zening Duan, Kaiping Chen</author><pubDate>Thu, 21 Nov 2024 18:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08222v2</guid></item><item><title>CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent Diffusion Models with Neural Fields</title><link>http://arxiv.org/abs/2411.14378v1</link><description>Eddy-resolving turbulence simulations require stochastic inflow conditionsthat accurately replicate the complex, multi-scale structures of turbulence.Traditional recycling-based methods rely on computationally expensive precursorsimulations, while existing synthetic inflow generators often fail to reproducerealistic coherent structures of turbulence. Recent advances in deep learning(DL) have opened new possibilities for inflow turbulence generation, yet manyDL-based methods rely on deterministic, autoregressive frameworks prone toerror accumulation, resulting in poor robustness for long-term predictions. Inthis work, we present CoNFiLD-inlet, a novel DL-based inflow turbulencegenerator that integrates diffusion models with a conditional neural field(CNF)-encoded latent space to produce realistic, stochastic inflow turbulence.By parameterizing inflow conditions using Reynolds numbers, CoNFiLD-inletgeneralizes effectively across a wide range of Reynolds numbers ($Re_\tau$between $10^3$ and $10^4$) without requiring retraining or parameter tuning.Comprehensive validation through a priori and a posteriori tests in DirectNumerical Simulation (DNS) and Wall-Modeled Large Eddy Simulation (WMLES)demonstrates its high fidelity, robustness, and scalability, positioning it asan efficient and versatile solution for inflow turbulence synthesis.</description><author>Xin-Yang Liu, Meet Hemant Parikh, Xiantao Fan, Pan Du, Qing Wang, Yi-Fan Chen, Jian-Xun Wang</author><pubDate>Thu, 21 Nov 2024 18:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14378v1</guid></item><item><title>Quantum Policy Gradient in Reproducing Kernel Hilbert Space</title><link>http://arxiv.org/abs/2411.06650v2</link><description>Parametrised quantum circuits offer expressive and data-efficientrepresentations for machine learning. Due to quantum states residing in ahigh-dimensional Hilbert space, parametrised quantum circuits have a naturalinterpretation in terms of kernel methods. The representation of quantumcircuits in terms of quantum kernels has been studied widely in quantumsupervised learning, but has been overlooked in the context of quantumreinforcement learning. This paper proposes parametric and non-parametricpolicy gradient and actor-critic algorithms with quantum kernel policies inquantum environments. This approach, implemented with both numerical andanalytical quantum policy gradient techniques, allows exploiting the manyadvantages of kernel methods, including available analytic forms for thegradient of the policy and tunable expressiveness. The proposed approach issuitable for vector-valued action spaces and each of the formulationsdemonstrates a quadratic reduction in query complexity compared to theirclassical counterparts. Two actor-critic algorithms, one based on stochasticpolicy gradient and one based on deterministic policy gradient (comparable tothe popular DDPG algorithm), demonstrate additional query complexity reductionscompared to quantum policy gradient algorithms under favourable conditions.</description><author>David M. Bossens, Kishor Bharti, Jayne Thompson</author><pubDate>Thu, 21 Nov 2024 18:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06650v2</guid></item><item><title>Model Checking for Reinforcement Learning in Autonomous Driving: One Can Do More Than You Think!</title><link>http://arxiv.org/abs/2411.14375v1</link><description>Most reinforcement learning (RL) platforms use high-level programminglanguages, such as OpenAI Gymnasium using Python. These frameworks providevarious API and benchmarks for testing RL algorithms in different domains, suchas autonomous driving (AD) and robotics. These platforms often emphasise thedesign of RL algorithms and the training performance but neglect thecorrectness of models and reward functions, which can be crucial for thesuccessful application of RL. This paper proposes using formal methods to modelAD systems and demonstrates how model checking (MC) can be used in RL for AD.Most studies combining MC and RL focus on safety, such as safety shields.However, this paper shows different facets where MC can strengthen RL. First,an MC-based model pre-analysis can reveal bugs with respect to sensor accuracyand learning step size. This step serves as a preparation of RL, which savestime if bugs exist and deepens users' understanding of the target system.Second, reward automata can benefit the design of reward functions and greatlyimprove learning performance especially when the learning objectives aremultiple. All these findings are supported by experiments.</description><author>Rong Gu</author><pubDate>Thu, 21 Nov 2024 18:09:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14375v1</guid></item><item><title>Using Formal Models, Safety Shields and Certified Control to Validate AI-Based Train Systems</title><link>http://arxiv.org/abs/2411.14374v1</link><description>The certification of autonomous systems is an important concern in scienceand industry. The KI-LOK project explores new methods for certifying and safelyintegrating AI components into autonomous trains. We pursued a two-layeredapproach: (1) ensuring the safety of the steering system by formal analysisusing the B method, and (2) improving the reliability of the perception systemwith a runtime certificate checker. This work links both strategies within ademonstrator that runs simulations on the formal model, controlled by the realAI output and the real certificate checker. The demonstrator is integrated intothe validation tool ProB. This enables runtime monitoring, runtimeverification, and statistical validation of formal safety properties using aformal B model. Consequently, one can detect and analyse potentialvulnerabilities and weaknesses of the AI and the certificate checker. We applythese techniques to a signal detection case study and present our findings.</description><author>Jan Gruteser, Jan Roßbach, Fabian Vu, Michael Leuschel</author><pubDate>Thu, 21 Nov 2024 18:09:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14374v1</guid></item><item><title>Integrating Physics of the Problem into Data-Driven Methods to Enhance Elastic Full-Waveform Inversion with Uncertainty Quantification</title><link>http://arxiv.org/abs/2406.05153v2</link><description>Full-Waveform Inversion (FWI) is a nonlinear iterative seismic imagingtechnique that, by reducing the misfit between recorded and predicted seismicwaveforms, can produce detailed estimates of subsurface geophysical properties.Nevertheless, the strong nonlinearity of FWI can trap the optimization in localminima. This issue arises due to factors such as improper initial values, theabsence of low frequencies in the measurements, noise, and other relatedconsiderations. To address this challenge and with the advent of advancedmachine-learning techniques, data-driven methods, such as deep learning, haveattracted significantly increasing attention in the geophysical community.Furthermore, the elastic wave equation should be included in FWI to representelastic effects accurately. The intersection of data-driven techniques andelastic scattering theories presents opportunities and challenges. In thispaper, by using the knowledge of elastic scattering (physics of the problem)and integrating it with machine learning techniques, we propose methods for thesolution of time-harmonic FWI to enhance accuracy compared to pure data-drivenand physics-based approaches. Moreover, to address uncertainty quantification,by modifying the structure of the Variational Autoencoder, we introduce aprobabilistic deep learning method based on the physics of the problem thatenables us to explore the uncertainties of the solution. According to thelimited availability of datasets in this field and to assess the performanceand accuracy of the proposed methods, we create a comprehensive dataset closeto reality and conduct a comparative analysis of the presented approaches toit.</description><author>Vahid Negahdari, Seyed Reza Moghadasi, Mohammad Reza Razvan</author><pubDate>Thu, 21 Nov 2024 18:08:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05153v2</guid></item><item><title>Synthesising Robust Controllers for Robot Collectives with Recurrent Tasks: A Case Study</title><link>http://arxiv.org/abs/2411.14371v1</link><description>When designing correct-by-construction controllers for autonomouscollectives, three key challenges are the task specification, the modelling,and its use at practical scale. In this paper, we focus on a simple yet usefulabstraction for high-level controller synthesis for robot collectives withoptimisation goals (e.g., maximum cleanliness, minimum energy consumption) andrecurrence (e.g., re-establish contamination and charge thresholds) and safety(e.g., avoid full discharge, mutually exclusive room occupation) constraints.Due to technical limitations (related to scalability and using constraints inthe synthesis), we simplify our graph-based setting from a stochastictwo-player game into a single-player game on a partially observable Markovdecision process (POMDP). Robustness against environmental uncertainty isencoded via partial observability. Linear-time correctness properties areverified separately after synthesising the POMDP strategy. We contributeat-scale guidance on POMDP modelling and controller synthesis for tasked robotcollectives exemplified by the scenario of battery-driven robots responsiblefor cleaning public buildings with utilisation constraints.</description><author>Till Schnittka, Mario Gleirscher</author><pubDate>Thu, 21 Nov 2024 18:08:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14371v1</guid></item><item><title>RV4Chatbot: Are Chatbots Allowed to Dream of Electric Sheep?</title><link>http://arxiv.org/abs/2411.14368v1</link><description>Chatbots have become integral to various application domains, including thosewith safety-critical considerations. As a result, there is a pressing need formethods that ensure chatbots consistently adhere to expected, safe behaviours.In this paper, we introduce RV4Chatbot, a Runtime Verification frameworkdesigned to monitor deviations in chatbot behaviour. We formalise expectedbehaviours as interaction protocols between the user and the chatbot. Wepresent the RV4Chatbot design and describe two implementations that instantiateit: RV4Rasa, for monitoring chatbots created with the Rasa framework, andRV4Dialogflow, for monitoring Dialogflow chatbots. Additionally, we detailexperiments conducted in a factory automation scenario using both RV4Rasa andRV4Dialogflow.</description><author>Andrea Gatti, Viviana Mascardi, Angelo Ferrando</author><pubDate>Thu, 21 Nov 2024 18:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14368v1</guid></item><item><title>ROSMonitoring 2.0: Extending ROS Runtime Verification to Services and Ordered Topics</title><link>http://arxiv.org/abs/2411.14367v1</link><description>Formal verification of robotic applications presents challenges due to theirhybrid nature and distributed architecture. This paper introduces ROSMonitoring2.0, an extension of ROSMonitoring designed to facilitate the monitoring ofboth topics and services while considering the order in which messages arepublished and received. The framework has been enhanced to support these novelfeatures for ROS1 -- and partially ROS2 environments -- offering improvedreal-time support, security, scalability, and interoperability. We discuss themodifications made to accommodate these advancements and present resultsobtained from a case study involving the runtime monitoring of specificcomponents of a fire-fighting Uncrewed Aerial Vehicle (UAV).</description><author>Maryam Ghaffari Saadat, Angelo Ferrando, Louise A. Dennis, Michael Fisher</author><pubDate>Thu, 21 Nov 2024 18:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14367v1</guid></item><item><title>Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning</title><link>http://arxiv.org/abs/2410.16162v2</link><description>Vision language models (VLMs) have demonstrated impressive performance acrossa wide range of downstream tasks. However, their proficiency in spatialreasoning remains limited, despite its crucial role in tasks involvingnavigation and interaction with physical environments. Specifically, most ofthese tasks rely on the core spatial reasoning capabilities in two-dimensional(2D) environments, and our evaluation reveals that state-of-the-art VLMsfrequently generate implausible and incorrect responses to composite spatialreasoning problems, including simple pathfinding tasks that humans can solveeffortlessly at a glance. To address this, we explore an effective approach toenhance 2D spatial reasoning within VLMs by training the model solely on basicspatial capabilities. We begin by disentangling the key components of 2Dspatial reasoning: direction comprehension, distance estimation, andlocalization. Our central hypothesis is that mastering these basic spatialcapabilities can significantly enhance a model's performance on compositespatial tasks requiring advanced spatial understanding and combinatorialproblem-solving, with generalized improvements in visual-spatial tasks. Toinvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunesVLMs on these three basic spatial capabilities by synthetic data generation andtargeted supervision to form an instruction dataset for each capability. Ourexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significantperformance gains, not only in the basic tasks themselves but also ingeneralizing to composite and out-of-distribution spatial reasoning tasks.These findings underscore the effectiveness of mastering basic spatialcapabilities in enhancing composite spatial problem-solving, offering insightsinto systematic strategies for improving VLMs' spatial reasoning capabilities.</description><author>Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao</author><pubDate>Thu, 21 Nov 2024 18:05:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16162v2</guid></item><item><title>Differentiable Weightless Neural Networks</title><link>http://arxiv.org/abs/2410.11112v2</link><description>We introduce the Differentiable Weightless Neural Network (DWN), a modelbased on interconnected lookup tables. Training of DWNs is enabled by a novelExtended Finite Difference technique for approximate differentiation of binaryvalues. We propose Learnable Mapping, Learnable Reduction, and SpectralRegularization to further improve the accuracy and efficiency of these models.We evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardwareaccelerator, where they demonstrate superior latency, throughput, energyefficiency, and model area compared to state-of-the-art solutions, (2) alow-power microcontroller, where they achieve preferable accuracy to XGBoostwhile subject to stringent memory constraints, and (3) ultra-low-cost chips,where they consistently outperform small models in both accuracy and projectedhardware area. DWNs also compare favorably against leading approaches fortabular datasets, with higher average rank. Overall, our work positions DWNs asa pioneering solution for edge-compatible high-throughput neural networks.</description><author>Alan T. L. Bacellar, Zachary Susskind, Mauricio Breternitz Jr., Eugene John, Lizy K. John, Priscila M. V. Lima, Felipe M. G. França</author><pubDate>Thu, 21 Nov 2024 18:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11112v2</guid></item><item><title>Localizing Events in Videos with Multimodal Queries</title><link>http://arxiv.org/abs/2406.10079v3</link><description>Localizing events in videos based on semantic queries is a pivotal task invideo understanding, with the growing significance of user-orientedapplications like video search. Yet, current research predominantly relies onnatural language queries (NLQs), overlooking the potential of using multimodalqueries (MQs) that integrate images to more flexibly represent semantic queries-- especially when it is difficult to express non-verbal or unfamiliar conceptsin words. To bridge this gap, we introduce ICQ, a new benchmark designed forlocalizing events in videos with MQs, alongside an evaluation datasetICQ-Highlight. To accommodate and evaluate existing video localization modelsfor this new task, we propose 3 Multimodal Query Adaptation methods and a novelSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12state-of-the-art backbone models, spanning from specialized video localizationmodels to Video LLMs, across diverse application domains. Our experimentshighlight the high potential of MQs in real-world applications. We believe thisbenchmark is a first step toward advancing MQs in video event localization.</description><author>Gengyuan Zhang, Mang Ling Ada Fok, Jialu Ma, Yan Xia, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu</author><pubDate>Thu, 21 Nov 2024 17:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10079v3</guid></item><item><title>InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating SLAM in Indoor Pedestrian-Rich Spaces for Human Navigation</title><link>http://arxiv.org/abs/2411.14358v1</link><description>Simultaneous localization and mapping (SLAM) techniques can be used tonavigate the visually impaired, but the development of robust SLAM solutionsfor crowded spaces is limited by the lack of realistic datasets. To addressthis, we introduce InCrowd-VI, a novel visual-inertial dataset specificallydesigned for human navigation in indoor pedestrian-rich environments. Recordedusing Meta Aria Project glasses, it captures realistic scenarios withoutenvironmental control. InCrowd-VI features 58 sequences totaling a 5 kmtrajectory length and 1.5 hours of recording time, including RGB, stereoimages, and IMU measurements. The dataset captures important challenges such aspedestrian occlusions, varying crowd densities, complex layouts, and lightingchanges. Ground-truth trajectories, accurate to approximately 2 cm, areprovided in the dataset, originating from the Meta Aria project machineperception SLAM service. In addition, a semi-dense 3D point cloud of scenes isprovided for each sequence. The evaluation of state-of-the-art visual odometry(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitationsin these realistic scenarios, demonstrating the need and value of the newdataset to advance SLAM research for visually impaired navigation in complexindoor environments.</description><author>Marziyeh Bamdad, Hans-Peter Hutter, Alireza Darvishy</author><pubDate>Thu, 21 Nov 2024 17:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14358v1</guid></item><item><title>Contrasting local and global modeling with machine learning and satellite data: A case study estimating tree canopy height in African savannas</title><link>http://arxiv.org/abs/2411.14354v1</link><description>While advances in machine learning with satellite imagery (SatML) arefacilitating environmental monitoring at a global scale, developing SatMLmodels that are accurate and useful for local regions remains critical tounderstanding and acting on an ever-changing planet. As increasing attentionand resources are being devoted to training SatML models with global data, itis important to understand when improvements in global models will make iteasier to train or fine-tune models that are accurate in specific regions. Toexplore this question, we contrast local and global training paradigms forSatML through a case study of tree canopy height (TCH) mapping in the KaringaniGame Reserve, Mozambique. We find that recent advances in global TCH mapping donot necessarily translate to better local modeling abilities in our studyregion. Specifically, small models trained only with locally-collected dataoutperform published global TCH maps, and even outperform globally pretrainedmodels that we fine-tune using local data. Analyzing these results further, weidentify specific points of conflict and synergy between local and globalmodeling paradigms that can inform future research toward aligning local andglobal performance objectives in geospatial machine learning.</description><author>Esther Rolf, Lucia Gordon, Milind Tambe, Andrew Davies</author><pubDate>Thu, 21 Nov 2024 17:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14354v1</guid></item><item><title>Enhancing Medical Image Segmentation with Deep Learning and Diffusion Models</title><link>http://arxiv.org/abs/2411.14353v1</link><description>Medical image segmentation is crucial for accurate clinical diagnoses, yet itfaces challenges such as low contrast between lesions and normal tissues,unclear boundaries, and high variability across patients. Deep learning hasimproved segmentation accuracy and efficiency, but it still relies heavily onexpert annotations and struggles with the complexities of medical images. Thesmall size of medical image datasets and the high cost of data acquisitionfurther limit the performance of segmentation networks. Diffusion models, withtheir iterative denoising process, offer a promising alternative for betterdetail capture in segmentation. However, they face difficulties in accuratelysegmenting small targets and maintaining the precision of boundary details.This article discusses the importance of medical image segmentation, thelimitations of current deep learning approaches, and the potential of diffusionmodels to address these challenges.</description><author>Houze Liu, Tong Zhou, Yanlin Xiang, Aoran Shen, Jiacheng Hu, Junliang Du</author><pubDate>Thu, 21 Nov 2024 17:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14353v1</guid></item><item><title>LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings</title><link>http://arxiv.org/abs/2408.14512v2</link><description>Zero-shot graph machine learning, especially with graph neural networks(GNNs), has garnered significant interest due to the challenge of scarcelabeled data. While methods like self-supervised learning and graph promptlearning have been extensively explored, they often rely on fine-tuning withtask-specific labels, limiting their effectiveness in zero-shot scenarios.Inspired by the zero-shot capabilities of instruction-fine-tuned large languagemodels (LLMs), we introduce a novel framework named Token Embedding-AlignedGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset andcross-task zero-shot learners for graph machine learning. Concretely, wepretrain a GNN, aligning its representations with token embeddings of an LLM.We then train a linear projector that transforms the GNN's representations intoa fixed number of graph token embeddings without tuning the LLM. A unifiedinstruction is designed for various graph tasks at different levels, such asnode classification (node-level) and link prediction (edge-level). These designchoices collectively enhance our method's effectiveness in zero-shot learning,setting it apart from existing methods. Experiments show that our graph tokenembeddings help the LLM predictor achieve state-of-the-art performance onunseen datasets and tasks compared to other methods using LLMs as predictors.</description><author>Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu</author><pubDate>Thu, 21 Nov 2024 17:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14512v2</guid></item><item><title>Indiscriminate Disruption of Conditional Inference on Multivariate Gaussians</title><link>http://arxiv.org/abs/2411.14351v1</link><description>The multivariate Gaussian distribution underpins myriad operations-research,decision-analytic, and machine-learning models (e.g., Bayesian optimization,Gaussian influence diagrams, and variational autoencoders). However, despiterecent advances in adversarial machine learning (AML), inference for Gaussianmodels in the presence of an adversary is notably understudied. Therefore, weconsider a self-interested attacker who wishes to disrupt a decisionmaker'sconditional inference and subsequent actions by corrupting a set of evidentiaryvariables. To avoid detection, the attacker also desires the attack to appearplausible wherein plausibility is determined by the density of the corruptedevidence. We consider white- and grey-box settings such that the attacker hascomplete and incomplete knowledge about the decisionmaker's underlyingmultivariate Gaussian distribution, respectively. Select instances are shown toreduce to quadratic and stochastic quadratic programs, and structuralproperties are derived to inform solution methods. We assess the impact andefficacy of these attacks in three examples, including, real estate evaluation,interest rate estimation and signals processing. Each example leverages analternative underlying model, thereby highlighting the attacks' broadapplicability. Through these applications, we also juxtapose the behavior ofthe white- and grey-box attacks to understand how uncertainty and structureaffect attacker behavior.</description><author>William N. Caballero, Matthew LaRosa, Alexander Fisher, Vahid Tarokh</author><pubDate>Thu, 21 Nov 2024 17:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14351v1</guid></item><item><title>Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals</title><link>http://arxiv.org/abs/2411.14349v1</link><description>We consider the problem of learning an arbitrarily-biased ReLU activation (orneuron) over Gaussian marginals with the squared loss objective. Despite theReLU neuron being the basic building block of modern neural networks, we stilldo not understand the basic algorithmic question of whether one arbitrary ReLUneuron is learnable in the non-realizable setting. In particular, all existingpolynomial time algorithms only provide approximation guarantees for thebetter-behaved unbiased setting or restricted bias setting. Our main result is a polynomial time statistical query (SQ) algorithm thatgives the first constant factor approximation for arbitrary bias. It outputs aReLU activation that achieves a loss of $O(\mathrm{OPT}) + \varepsilon$ in time$\mathrm{poly}(d,1/\varepsilon)$, where $\mathrm{OPT}$ is the loss obtained bythe optimal ReLU activation. Our algorithm presents an interesting departurefrom existing algorithms, which are all based on gradient descent and thus fallwithin the class of correlational statistical query (CSQ) algorithms. Wecomplement our algorithmic result by showing that no polynomial time CSQalgorithm can achieve a constant factor approximation. Together, these resultsshed light on the intrinsic limitation of gradient descent, while identifyingarguably the simplest setting (a single neuron) where there is a separationbetween SQ and CSQ algorithms.</description><author>Anxin Guo, Aravindan Vijayaraghavan</author><pubDate>Thu, 21 Nov 2024 17:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14349v1</guid></item><item><title>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</title><link>http://arxiv.org/abs/2411.14347v1</link><description>In this paper, we introduce DINO-X, which is a unified object-centric visionmodel developed by IDEA Research with the best open-world object detectionperformance to date. DINO-X employs the same Transformer-based encoder-decoderarchitecture as Grounding DINO 1.5 to pursue an object-level representation foropen-world object understanding. To make long-tailed object detection easy,DINO-X extends its input options to support text prompt, visual prompt, andcustomized prompt. With such flexible prompt options, we develop a universalobject prompt to support prompt-free open-world detection, making it possibleto detect anything in an image without requiring users to provide any prompt.To enhance the model's core grounding capability, we have constructed alarge-scale dataset with over 100 million high-quality grounding samples,referred to as Grounding-100M, for advancing the model's open-vocabularydetection performance. Pre-training on such a large-scale grounding datasetleads to a foundational object-level representation, which enables DINO-X tointegrate multiple perception heads to simultaneously support multiple objectperception and understanding tasks, including detection, segmentation, poseestimation, object captioning, object-based QA, etc. Experimental resultsdemonstrate the superior performance of DINO-X. Specifically, the DINO-X Promodel achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, andLVIS-val zero-shot object detection benchmarks, respectively. Notably, itscores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-valbenchmarks, both improving the previous SOTA performance by 5.8 AP. Such aresult underscores its significantly improved capacity for recognizinglong-tailed objects.</description><author>Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, Xingyu Chen, Zhuheng Song, Yuhong Zhang, Hongjie Huang, Han Gao, Shilong Liu, Hao Zhang, Feng Li, Kent Yu, Lei Zhang</author><pubDate>Thu, 21 Nov 2024 17:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14347v1</guid></item><item><title>Layer Pruning with Consensus: A Triple-Win Solution</title><link>http://arxiv.org/abs/2411.14345v1</link><description>Layer pruning offers a promising alternative to standard structured pruning,effectively reducing computational costs, latency, and memory footprint. Whilenotable layer-pruning approaches aim to detect unimportant layers for removal,they often rely on single criteria that may not fully capture the complex,underlying properties of layers. We propose a novel approach that combinesmultiple similarity metrics into a single expressive measure of low-importancelayers, called the Consensus criterion. Our technique delivers a triple-winsolution: low accuracy drop, high-performance improvement, and increasedrobustness to adversarial attacks. With up to 78.80% FLOPs reduction andperformance on par with state-of-the-art methods across different benchmarks,our approach reduces energy consumption and carbon emissions by up to 66.99%and 68.75%, respectively. Additionally, it avoids shortcut learning andimproves robustness by up to 4 percentage points under various adversarialattacks. Overall, the Consensus criterion demonstrates its effectiveness increating robust, efficient, and environmentally friendly pruned models.</description><author>Leandro Giusti Mugnaini, Carolina Tavares Duarte, Anna H. Reali Costa, Artur Jordao</author><pubDate>Thu, 21 Nov 2024 17:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14345v1</guid></item><item><title>Overcomplete Tensor Decomposition via Koszul-Young Flattenings</title><link>http://arxiv.org/abs/2411.14344v1</link><description>Motivated by connections between algebraic complexity lower bounds and tensordecompositions, we investigate Koszul-Young flattenings, which are the mainingredient in recent lower bounds for matrix multiplication. Based on this toolwe give a new algorithm for decomposing an $n_1 \times n_2 \times n_3$ tensoras the sum of a minimal number of rank-1 terms, and certifying uniqueness ofthis decomposition. For $n_1 \le n_2 \le n_3$ with $n_1 \to \infty$ and$n_3/n_2 = O(1)$, our algorithm is guaranteed to succeed when the tensor rankis bounded by $r \le (1-\epsilon)(n_2 + n_3)$ for an arbitrary $\epsilon &gt; 0$,provided the tensor components are generically chosen. For any fixed$\epsilon$, the runtime is polynomial in $n_3$. When $n_2 = n_3 = n$, ourcondition on the rank gives a factor-of-2 improvement over the classicalsimultaneous diagonalization algorithm, which requires $r \le n$, and alsoimproves on the recent algorithm of Koiran (2024) which requires $r \le 4n/3$.It also improves on the PhD thesis of Persu (2018) which solves rank detectionfor $r \leq 3n/2$. We complement our upper bounds by showing limitations, in particular that noflattening of the style we consider can surpass rank $n_2 + n_3$. Furthermore,for $n \times n \times n$ tensors, we show that an even more general class ofdegree-$d$ polynomial flattenings cannot surpass rank $Cn$ for a constant $C =C(d)$. This suggests that for tensor decompositions, the case of genericcomponents may be fundamentally harder than that of random components, whereefficient decomposition is possible even in highly overcomplete settings.</description><author>Pravesh K. Kothari, Ankur Moitra, Alexander S. Wein</author><pubDate>Thu, 21 Nov 2024 17:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14344v1</guid></item><item><title>UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages</title><link>http://arxiv.org/abs/2411.14343v1</link><description>Large language models (LLMs) under-perform on low-resource languages due tolimited training data. We present a method to efficiently collect text data forlow-resource languages from the entire Common Crawl corpus. Our approach,UnifiedCrawl, filters and extracts common crawl using minimal computeresources, yielding mono-lingual datasets much larger than previously availablesources. We demonstrate that leveraging this data to fine-tuning multilingualLLMs via efficient adapter methods (QLoRA) significantly boosts performance onthe low-resource language, while minimizing VRAM usage. Our experiments showlarge improvements in language modeling perplexity and an increase in few-shotprompting scores. Our work and released source code provide an affordableapproach to improve LLMs for low-resource languages using consumer hardware.Our source code is available here athttps://github.com/bethelmelesse/unifiedcrawl.</description><author>Bethel Melesse Tessema, Akhil Kedia, Tae-Sun Chung</author><pubDate>Thu, 21 Nov 2024 17:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14343v1</guid></item><item><title>Logarithmic Neyman Regret for Adaptive Estimation of the Average Treatment Effect</title><link>http://arxiv.org/abs/2411.14341v1</link><description>Estimation of the Average Treatment Effect (ATE) is a core problem in causalinference with strong connections to Off-Policy Evaluation in ReinforcementLearning. This paper considers the problem of adaptively selecting thetreatment allocation probability in order to improve estimation of the ATE. Themajority of prior work on adaptive ATE estimation focus on asymptoticguarantees, and in turn overlooks important practical considerations such asthe difficulty of learning the optimal treatment allocation as well ashyper-parameter selection. Existing non-asymptotic methods are limited by poorempirical performance and exponential scaling of the Neyman regret with respectto problem parameters. In order to address these gaps, we propose and analyzethe Clipped Second Moment Tracking (ClipSMT) algorithm, a variant of anexisting algorithm with strong asymptotic optimality guarantees, and providefinite sample bounds on its Neyman regret. Our analysis shows that ClipSMTachieves exponential improvements in Neyman regret on two fronts: improving thedependence on $T$ from $O(\sqrt{T})$ to $O(\log T)$, as well as reducing theexponential dependence on problem parameters to a polynomial dependence.Finally, we conclude with simulations which show the marked improvement ofClipSMT over existing approaches.</description><author>Ojash Neopane, Aaditya Ramdas, Aarti Singh</author><pubDate>Thu, 21 Nov 2024 17:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14341v1</guid></item><item><title>Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models</title><link>http://arxiv.org/abs/2410.24079v2</link><description>Bayesian reasoning in linear mixed-effects models (LMMs) is challenging andoften requires advanced sampling techniques like Markov chain Monte Carlo(MCMC). A common approach is to write the model in a probabilistic programminglanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there aremany ways a user can transform a model that make inference more or lessefficient. In particular, marginalizing some variables can greatly improveinference but is difficult for users to do manually. We develop an algorithm toeasily marginalize random effects in LMMs. A naive approach introduces cubictime operations within an inference algorithm like HMC, but we reduce therunning time to linear using fast linear algebra techniques. We show thatmarginalization is always beneficial when applicable and highlight improvementsin various models, especially ones from cognitive sciences.</description><author>Jinlin Lai, Justin Domke, Daniel Sheldon</author><pubDate>Thu, 21 Nov 2024 17:33:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24079v2</guid></item><item><title>Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN</title><link>http://arxiv.org/abs/2407.10689v5</link><description>This paper presents a fast and cost-effective method for diagnosing cardiacabnormalities with high accuracy and reliability using low-cost systems inclinics. The primary limitation of automatic diagnosing of cardiac diseases isthe rarity of correct and acceptable labeled samples, which can be expensive toprepare. To address this issue, two methods are proposed in this work. Thefirst method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)architecture inspired by human auditory processing, specifically designed tooptimize feature extraction by employing various sizes of convolutional filtersand audio signal power spectrum as input. In the second method, called as Longshort-term memory-Convolutional Neural (LSCN) model, Additionally, the networkarchitecture includes Long Short-Term Memory (LSTM) network blocks to improvefeature extraction in the time domain. The innovative approach of combiningmultiple parallel branches consisting of the one-dimensional convolutionallayers along with LSTM blocks helps in achieving superior results in audiosignal processing tasks. The experimental results demonstrate superiority ofthe proposed methods over the state-of-the-art techniques. The overallclassification accuracy of heart sounds with the LSCN network is more than 96%.The efficiency of this network is significant compared to common featureextraction methods such as Mel Frequency Cepstral Coefficients (MFCC) andwavelet transform. Therefore, the proposed method shows promising results inthe automatic analysis of heart sounds and has potential applications in thediagnosis and early detection of cardiovascular diseases.</description><author>Seyed Amir Latifi, Hassan Ghassemian, Maryam Imani</author><pubDate>Thu, 21 Nov 2024 17:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10689v5</guid></item><item><title>SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting and Dense Feature Matching</title><link>http://arxiv.org/abs/2411.14322v1</link><description>Experience Goal Visual Rearrangement task stands as a foundational challengewithin Embodied AI, requiring an agent to construct a robust world model thataccurately captures the goal state. The agent uses this world model to restorea shuffled scene to its original configuration, making an accuraterepresentation of the world essential for successfully completing the task. Inthis work, we present a novel framework that leverages on 3D Gaussian Splattingas a 3D scene representation for experience goal visual rearrangement task.Recent advances in volumetric scene representation like 3D Gaussian Splatting,offer fast rendering of high quality and photo-realistic novel views. Ourapproach enables the agent to have consistent views of the current and the goalsetting of the rearrangement task, which enables the agent to directly comparethe goal state and the shuffled state of the world in image space. To comparethese views, we propose to use a dense feature matching method with visualfeatures extracted from a foundation model, leveraging its advantages of a moreuniversal feature representation, which facilitates robustness, andgeneralization. We validate our approach on the AI2-THOR rearrangementchallenge benchmark and demonstrate improvements over the current state of theart methods</description><author>Arjun P S, Andrew Melnik, Gora Chand Nandi</author><pubDate>Thu, 21 Nov 2024 17:12:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14322v1</guid></item><item><title>Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training</title><link>http://arxiv.org/abs/2411.14318v1</link><description>It is well-known that a diverse corpus is critical for training largelanguage models, which are typically constructed from a mixture of variousdomains. In general, previous efforts resort to sampling training data fromdifferent domains with static proportions, as well as adjusting dataproportions during training. However, few methods have addressed thecomplexities of domain-adaptive continual pre-training. To fill this gap, wepropose Velocitune, a novel framework dynamically assesses learning velocityand adjusts data proportions accordingly, favoring slower-learning domainswhile shunning faster-learning ones, which is guided by a scaling law toindicate the desired learning goal for each domain with less associated cost.To evaluate the effectiveness of Velocitune, we conduct experiments in areasoning-focused dataset with CodeLlama, as well as in a corpus specialisedfor system command generation with Llama3 and Mistral. Velocitune achievesperformance gains in both math and code reasoning tasks and command-linegeneration benchmarks. Further analysis reveals that key factors drivingVelocitune's effectiveness include target loss prediction and data ordering.</description><author>Zheheng Luo, Xin Zhang, Xiao Liu, Haoling Li, Yeyun Gong, Chen Qi, Peng Cheng</author><pubDate>Thu, 21 Nov 2024 17:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14318v1</guid></item><item><title>Model-free learning of probability flows: Elucidating the nonequilibrium dynamics of flocking</title><link>http://arxiv.org/abs/2411.14317v1</link><description>Active systems comprise a class of nonequilibrium dynamics in whichindividual components autonomously dissipate energy. Efforts towardsunderstanding the role played by activity have centered on computation of theentropy production rate (EPR), which quantifies the breakdown of time reversalsymmetry. A fundamental difficulty in this program is that high dimensionalityof the phase space renders traditional computational techniques infeasible forestimating the EPR. Here, we overcome this challenge with a novel deep learningapproach that estimates probability currents directly from stochastic systemtrajectories. We derive a new physical connection between the probabilitycurrent and two local definitions of the EPR for inertial systems, which weapply to characterize the departure from equilibrium in a canonical model offlocking. Our results highlight that entropy is produced and consumed on thespatial interface of a flock as the interplay between alignment and fluctuationdynamically creates and annihilates order. By enabling the direct visualizationof when and where a given system is out of equilibrium, we anticipate that ourmethodology will advance the understanding of a broad class of complexnonequilibrium dynamics.</description><author>Nicholas M. Boffi, Eric Vanden-Eijnden</author><pubDate>Thu, 21 Nov 2024 17:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14317v1</guid></item><item><title>Outlier-robust Mean Estimation near the Breakdown Point via Sum-of-Squares</title><link>http://arxiv.org/abs/2411.14305v1</link><description>We revisit the problem of estimating the mean of a high-dimensionaldistribution in the presence of an $\varepsilon$-fraction of adversarialoutliers. When $\varepsilon$ is at most some sufficiently small constant, previousworks can achieve optimal error rate efficiently\cite{diakonikolas2018robustly, kothari2018robust}. As $\varepsilon$ approachesthe breakdown point $\frac{1}{2}$, all previous algorithms incur eithersub-optimal error rates or exponential running time. In this paper we give a new analysis of the canonical sum-of-squares programintroduced in \cite{kothari2018robust} and show that this program efficientlyachieves optimal error rate for all $\varepsilon \in[0,\frac{1}{2})$. The keyingredient for our results is a new identifiability proof for robust meanestimation that focuses on the overlap between the distributions instead oftheir statistical distance as in previous works. We capture this proof withinthe sum-of-squares proof system, thus obtaining efficient algorithms using thesum-of-squares proofs to algorithms paradigm \cite{raghavendra2018high}.</description><author>Hongjie Chen, Deepak Narayanan Sridharan, David Steurer</author><pubDate>Thu, 21 Nov 2024 16:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14305v1</guid></item><item><title>Automated Generation of Code Debugging Exercises</title><link>http://arxiv.org/abs/2411.14303v1</link><description>Debugging is an essential skill when learning to program, yet its instructionand emphasis often vary widely across introductory courses. In the era ofcode-generating large language models (LLMs), the ability for students toreason about code and identify errors is increasingly important. However,students frequently resort to trial-and-error methods to resolve bugs withoutfully understanding the underlying issues. Developing the ability to identifyand hypothesize the cause of bugs is crucial but can be time-consuming to teacheffectively through traditional means. This paper introduces BugSpotter, aninnovative tool that leverages an LLM to generate buggy code from a problemdescription and verify the synthesized bugs via a test suite. Students interactwith BugSpotter by designing failing test cases, where the buggy code's outputdiffers from the expected result as defined by the problem specification. Thisnot only provides opportunities for students to enhance their debugging skills,but also to practice reading and understanding problem specifications. Wedeployed BugSpotter in a large classroom setting and compared the debuggingexercises it generated to exercises hand-crafted by an instructor for the sameproblems. We found that the LLM-generated exercises produced by BugSpottervaried in difficulty and were well-matched to the problem specifications.Importantly, the LLM-generated exercises were comparable to those manuallycreated by instructors with respect to student performance, suggesting thatBugSpotter could be an effective and efficient aid for learning debugging.</description><author>Victor-Alexandru Pădurean, Paul Denny, Adish Singla</author><pubDate>Thu, 21 Nov 2024 16:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14303v1</guid></item><item><title>A TVD neural network closure and application to turbulent combustion</title><link>http://arxiv.org/abs/2408.03413v2</link><description>Trained neural networks (NN) have attractive features for closing governingequations. There are many methods that are showing promise, but all can fail incases when small errors consequentially violate physical reality, such as asolution boundedness condition. A NN formulation is introduced to precludespurious oscillations that violate solution boundedness or positivity. It isembedded in the discretized equations as a machine learning closure andstrictly constrained, inspired by total variation diminishing (TVD) methods forhyperbolic conservation laws. The constraint is exactly enforced duringgradient-descent training by rescaling the NN parameters, which maps them ontoan explicit feasible set. Demonstrations show that the constrained NN closuremodel usefully recovers linear and nonlinear hyperbolic phenomena andanti-diffusion while enforcing the non-oscillatory property. Finally, the modelis applied to subgrid-scale (SGS) modeling of a turbulent reacting flow, forwhich it suppresses spurious oscillations in scalar fields that otherwiseviolate the solution boundedness. It outperforms a simple penalization ofoscillations in the loss function.</description><author>Seung Won Suh, Jonathan F MacArt, Luke N Olson, Jonathan B Freund</author><pubDate>Thu, 21 Nov 2024 16:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03413v2</guid></item><item><title>Variational Nearest Neighbor Gaussian Process</title><link>http://arxiv.org/abs/2202.01694v4</link><description>Variational approximations to Gaussian processes (GPs) typically use a smallset of inducing points to form a low-rank approximation to the covariancematrix. In this work, we instead exploit a sparse approximation of theprecision matrix. We propose variational nearest neighbor Gaussian process(VNNGP), which introduces a prior that only retains correlations within $K$nearest-neighboring observations, thereby inducing sparse precision structure.Using the variational framework, VNNGP's objective can be factorized over bothobservations and inducing points, enabling stochastic optimization with a timecomplexity of $O(K^3)$. Hence, we can arbitrarily scale the inducing pointsize, even to the point of putting inducing points at every observed location.We compare VNNGP to other scalable GPs through various experiments, anddemonstrate that VNNGP (1) can dramatically outperform low-rank methods, and(2) is less prone to overfitting than other nearest neighbor methods.</description><author>Luhuan Wu, Geoff Pleiss, John Cunningham</author><pubDate>Thu, 21 Nov 2024 16:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.01694v4</guid></item><item><title>LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts</title><link>http://arxiv.org/abs/2411.13009v2</link><description>As large language models (LLMs) show impressive performance on complex tasks,they still struggle with longer contextual understanding and high computationalcosts. To balance efficiency and quality, we introduce LLMSteer, afine-tuning-free framework that enhances LLMs through query-independentattention steering. Tested on popular LLMs and datasets, LLMSteer narrows theperformance gap with baselines by 65.9% and reduces the runtime delay by up to4.8x compared to recent attention steering methods.</description><author>Zhuohan Gu, Jiayi Yao, Kuntai Du, Junchen Jiang</author><pubDate>Thu, 21 Nov 2024 16:49:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13009v2</guid></item><item><title>Pairwise Judgment Formulation for Semantic Embedding Model in Web Search</title><link>http://arxiv.org/abs/2408.04197v2</link><description>Semantic Embedding Model (SEM), a neural network-based Siamese architecture,is gaining momentum in information retrieval and natural language processing.In order to train SEM in a supervised fashion for Web search, the search enginequery log is typically utilized to automatically formulate pairwise judgmentsas training data. Despite the growing application of semantic embeddings in thesearch engine industry, little work has been done on formulating effectivepairwise judgments for training SEM. In this paper, we make the first in-depthinvestigation of a wide range of strategies for generating pairwise judgmentsfor SEM. An interesting (perhaps surprising) discovery reveals that theconventional pairwise judgment formulation strategy wildly used in the field ofpairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM.Through a large-scale empirical study based on query logs and click-throughactivities from a major commercial search engine, we demonstrate the effectivestrategies for SEM and highlight the advantages of a hybrid heuristic (i.e.,Clicked &gt; Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked &gt;Skipped) in LTR. We conclude with best practices for training SEM and offerpromising insights for future research.</description><author>Mengze Hong, Wailing Ng, Zichang Guo, Chen Jason Zhang</author><pubDate>Thu, 21 Nov 2024 16:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04197v2</guid></item><item><title>AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context</title><link>http://arxiv.org/abs/2410.16520v2</link><description>As our understanding of autism and ableism continues to increase, so does ourunderstanding of ableist language towards autistic people. Such language posesa significant challenge in NLP research due to its subtle and context-dependentnature. Yet, detecting anti-autistic ableist language remains underexplored,with existing NLP tools often failing to capture its nuanced expressions. Wepresent AUTALIC, the first benchmark dataset dedicated to the detection ofanti-autistic ableist language in context, addressing a significant gap in thefield. The dataset comprises 2,400 autism-related sentences collected fromReddit, accompanied by surrounding context, and is annotated by trained expertswith backgrounds in neurodiversity. Our comprehensive evaluation reveals thatcurrent language models, including state-of-the-art LLMs, struggle to reliablyidentify anti-autistic ableism and align with human judgments, underscoringtheir limitations in this domain. We publicly release AUTALIC along with theindividual annotations which serve as a valuable resource to researchersworking on ableism, neurodiversity, and also studying disagreements inannotation tasks. This dataset serves as a crucial step towards developing moreinclusive and context-aware NLP systems that better reflect diverseperspectives.</description><author>Naba Rizvi, Harper Strickland, Daniel Gitelman, Tristan Cooper, Alexis Morales-Flores, Michael Golden, Aekta Kallepalli, Akshat Alurkar, Haaset Owens, Saleha Ahmedi, Isha Khirwadkar, Imani Munyaka, Nedjma Ousidhoum</author><pubDate>Thu, 21 Nov 2024 16:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16520v2</guid></item><item><title>Improving Routability Prediction via NAS Using a Smooth One-shot Augmented Predictor</title><link>http://arxiv.org/abs/2411.14296v1</link><description>Routability optimization in modern EDA tools has benefited greatly from usingmachine learning (ML) models. Constructing and optimizing the performance of MLmodels continues to be a challenge. Neural Architecture Search (NAS) serves asa tool to aid in the construction and improvement of these models. TraditionalNAS techniques struggle to perform well on routability prediction as a resultof two primary factors. First, the separation between the training objectiveand the search objective adds noise to the NAS process. Secondly, the increasedvariance of the search objective further complicates performing NAS. We craft anovel NAS technique, coined SOAP-NAS, to address these challenges through noveldata augmentation techniques and a novel combination of one-shot andpredictor-based NAS. Results show that our technique outperforms existingsolutions by 40% closer to the ideal performance measured by ROC-AUC (areaunder the receiver operating characteristic curve) in DRC hotspot detection.SOAPNet is able to achieve an ROC-AUC of 0.9802 and a query time of only 0.461ms.</description><author>Arjun Sridhar, Chen-Chia Chang, Junyao Zhang, Yiran Chen</author><pubDate>Thu, 21 Nov 2024 16:42:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14296v1</guid></item><item><title>StereoCrafter-Zero: Zero-Shot Stereo Video Generation with Noisy Restart</title><link>http://arxiv.org/abs/2411.14295v1</link><description>Generating high-quality stereo videos that mimic human binocular visionrequires maintaining consistent depth perception and temporal coherence acrossframes. While diffusion models have advanced image and video synthesis,generating high-quality stereo videos remains challenging due to the difficultyof maintaining consistent temporal and spatial coherence between left and rightviews. We introduce \textit{StereoCrafter-Zero}, a novel framework forzero-shot stereo video generation that leverages video diffusion priors withoutthe need for paired training data. Key innovations include a noisy restartstrategy to initialize stereo-aware latents and an iterative refinement processthat progressively harmonizes the latent space, addressing issues like temporalflickering and view inconsistencies. Comprehensive evaluations, includingquantitative metrics and user studies, demonstrate that\textit{StereoCrafter-Zero} produces high-quality stereo videos with improveddepth consistency and temporal smoothness, even when depth estimations areimperfect. Our framework is robust and adaptable across various diffusionmodels, setting a new benchmark for zero-shot stereo video generation andenabling more immersive visual experiences. Our code can be foundin~\url{https://github.com/shijianjian/StereoCrafter-Zero}.</description><author>Jian Shi, Qian Wang, Zhenyu Li, Peter Wonka</author><pubDate>Thu, 21 Nov 2024 16:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14295v1</guid></item><item><title>Model-Based Transfer Learning for Contextual Reinforcement Learning</title><link>http://arxiv.org/abs/2408.04498v2</link><description>Deep reinforcement learning (RL) is a powerful approach to complex decisionmaking. However, one issue that limits its practical application is itsbrittleness, sometimes failing to train in the presence of small changes in theenvironment. Motivated by the success of zero-shot transfer-where pre-trainedmodels perform well on related tasks-we consider the problem of selecting agood set of training tasks to maximize generalization performance across arange of tasks. Given the high cost of training, it is critical to selecttraining tasks strategically, but not well understood how to do so. We henceintroduce Model-Based Transfer Learning (MBTL), which layers on top of existingRL methods to effectively solve contextual RL problems. MBTL models thegeneralization performance in two parts: 1) the performance set point, modeledusing Gaussian processes, and 2) performance loss (generalization gap), modeledas a linear function of contextual similarity. MBTL combines these two piecesof information within a Bayesian optimization (BO) framework to strategicallyselect training tasks. We show theoretically that the method exhibits sublinearregret in the number of training tasks and discuss conditions to furthertighten regret bounds. We experimentally validate our methods using urbantraffic and standard continuous control benchmarks. The experimental resultssuggest that MBTL can achieve up to 50x improved sample efficiency comparedwith canonical independent training and multi-task training. Furtherexperiments demonstrate the efficacy of BO and the insensitivity to theunderlying RL algorithm and hyperparameters. This work lays the foundations forinvestigating explicit modeling of generalization, thereby enabling principledyet effective methods for contextual RL.</description><author>Jung-Hoon Cho, Vindula Jayawardana, Sirui Li, Cathy Wu</author><pubDate>Thu, 21 Nov 2024 16:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04498v2</guid></item><item><title>ViSTa Dataset: Do vision-language models understand sequential tasks?</title><link>http://arxiv.org/abs/2411.13211v2</link><description>Using vision-language models (VLMs) as reward models in reinforcementlearning holds promise for reducing costs and improving safety. So far, VLMreward models have only been used for goal-oriented tasks, where the agent mustreach a particular final outcome. We explore VLMs' potential to supervise tasksthat cannot be scored by the final state alone. To this end, we introduceViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.ViSTa comprises over 4,000 videos with step-by-step descriptions in virtualhome, Minecraft, and real-world environments. Its novel hierarchical structure-- basic single-step tasks composed into more and more complex sequential tasks-- allows a fine-grained understanding of how well VLMs can judge tasks withvarying complexity. To illustrate this, we use ViSTa to evaluatestate-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, whilethey are all good at object recognition, they fail to understand sequentialtasks, with only GPT-4o achieving non-trivial performance.</description><author>Evžen Wybitul, Evan Ryan Gunter, Mikhail Seleznyov, David Lindner</author><pubDate>Thu, 21 Nov 2024 16:37:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13211v2</guid></item><item><title>On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality and Weight Sharing</title><link>http://arxiv.org/abs/2411.14288v1</link><description>Weight sharing, equivariance, and local filters, as in convolutional neuralnetworks, are believed to contribute to the sample efficiency of neuralnetworks. However, it is not clear how each one of these design choicescontribute to the generalization error. Through the lens of statisticallearning theory, we aim to provide an insight into this question bycharacterizing the relative impact of each choice on the sample complexity. Weobtain lower and upper sample complexity bounds for a class of single hiddenlayer networks. It is shown that the gain of equivariance is directlymanifested in the bound, while getting a similar increase for weight sharingdepends on the sharing mechanism. Among our results, we obtain a completelydimension-free bound for equivariant networks for a class of poolingoperations. We show that the bound depends merely on the norm of filters, whichis tighter than using the spectral norm of the respective matrix. We alsocharacterize the trade-off in sample complexity between the parametrization offilters in spatial and frequency domains, particularly when spatial filters arelocalized as in vanilla convolutional neural networks.</description><author>Arash Behboodi, Gabriele Cesa</author><pubDate>Thu, 21 Nov 2024 16:36:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14288v1</guid></item><item><title>EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild</title><link>http://arxiv.org/abs/2411.14280v1</link><description>Our work aims to reconstruct hand-object interactions from a single-viewimage, which is a fundamental but ill-posed task. Unlike methods thatreconstruct from videos, multi-view images, or predefined 3D templates,single-view reconstruction faces significant challenges due to inherentambiguities and occlusions. These challenges are further amplified by thediverse nature of hand poses and the vast variety of object shapes and sizes.Our key insight is that current foundational models for segmentation,inpainting, and 3D reconstruction robustly generalize to in-the-wild images,which could provide strong visual and geometric priors for reconstructinghand-object interactions. Specifically, given a single image, we first design anovel pipeline to estimate the underlying hand pose and object shape usingoff-the-shelf large models. Furthermore, with the initial reconstruction, weemploy a prior-guided optimization scheme, which optimizes hand pose to complywith 3D physical constraints and the 2D input image content. We performexperiments across several datasets and show that our method consistentlyoutperforms baselines and faithfully reconstructs a diverse set of hand-objectinteractions. Here is the link of our project page:https://lym29.github.io/EasyHOI-page/</description><author>Yumeng Liu, Xiaoxiao Long, Zemin Yang, Yuan Liu, Marc Habermann, Christian Theobalt, Yuexin Ma, Wenping Wang</author><pubDate>Thu, 21 Nov 2024 16:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14280v1</guid></item><item><title>Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance</title><link>http://arxiv.org/abs/2411.14279v1</link><description>Large vision-language models (LVLMs) have achieved impressive results invarious vision-language tasks. However, despite showing promising performance,LVLMs suffer from hallucinations caused by language bias, leading to diminishedfocus on images and ineffective visual comprehension. We identify two primaryreasons for this bias: 1. Different scales of training data between thepretraining stage of LLM and multimodal alignment stage. 2. The learnedinference bias due to short-term dependency of text data. Therefore, we proposeLACING, a systemic framework designed to address the language bias of LVLMswith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).Specifically, MDA introduces a parallel dual-attention mechanism that enhancesthe integration of visual inputs across the model. IFG introduces a learnablesoft visual prompt during training and inference to replace visual inputs,designed to compel LVLMs to prioritize text inputs. Then, IFG further proposesa novel decoding strategy using the soft visual prompt to mitigate the model'sover-reliance on adjacent text inputs. Comprehensive experiments demonstratethat our method effectively debiases LVLMs from their language bias, enhancingvisual comprehension and reducing hallucinations without requiring additionaltraining resources or data. The code and model are available at[lacing-lvlm.github.io](https://lacing-lvlm.github.io).</description><author>Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang</author><pubDate>Thu, 21 Nov 2024 16:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14279v1</guid></item><item><title>Neuro-Symbolic Query Optimization in Knowledge Graphs</title><link>http://arxiv.org/abs/2411.14277v1</link><description>This chapter delves into the emerging field of neuro-symbolic queryoptimization for knowledge graphs (KGs), presenting a comprehensive explorationof how neural and symbolic techniques can be integrated to enhance queryprocessing. Traditional query optimizers in knowledge graphs rely heavily onsymbolic methods, utilizing dataset summaries, statistics, and cost models toselect efficient execution plans. However, these approaches often suffer frommisestimations and inaccuracies, particularly when dealing with complex queriesor large-scale datasets. Recent advancements have introduced neural models,which capture non-linear aspects of query optimization, offering promisingalternatives to purely symbolic methods. In this chapter, we introduceneuro-symbolic query optimizers, a novel approach that combines the strengthsof symbolic reasoning with the adaptability of neural computation. We discussthe architecture of these hybrid systems, highlighting the interplay betweenneural and symbolic components to improve the optimizer's ability to navigatethe search space and produce efficient execution plans. Additionally, thechapter reviews existing neural components tailored for optimizing queries overknowledge graphs and examines the limitations and challenges in deployingneuro-symbolic query optimizers in real-world environments.</description><author>Maribel Acosta, Chang Qin, Tim Schwabe</author><pubDate>Thu, 21 Nov 2024 16:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14277v1</guid></item><item><title>Dual Attention Model with Reinforcement Learning for Classification of Histology Whole-Slide Images</title><link>http://arxiv.org/abs/2302.09682v2</link><description>Digital whole slide images (WSIs) are generally captured at microscopicresolution and encompass extensive spatial data. Directly feeding these imagesto deep learning models is computationally intractable due to memoryconstraints, while downsampling the WSIs risks incurring information loss.Alternatively, splitting the WSIs into smaller patches may result in a loss ofimportant contextual information. In this paper, we propose a novel dualattention approach, consisting of two main components, both inspired by thevisual examination process of a pathologist: The first soft attention modelprocesses a low magnification view of the WSI to identify relevant regions ofinterest, followed by a custom sampling method to extract diverse and spatiallydistinct image tiles from the selected ROIs. The second component, the hardattention classification model further extracts a sequence of multi-resolutionglimpses from each tile for classification. Since hard attention isnon-differentiable, we train this component using reinforcement learning topredict the location of the glimpses. This approach allows the model to focuson essential regions instead of processing the entire tile, thereby aligningwith a pathologist's way of diagnosis. The two components are trained in anend-to-end fashion using a joint loss function to demonstrate the efficacy ofthe model. The proposed model was evaluated on two WSI-level classificationproblems: Human epidermal growth factor receptor 2 scoring on breast cancerhistology images and prediction of Intact/Loss status of two Mismatch Repairbiomarkers from colorectal cancer histology images. We show that the proposedmodel achieves performance better than or comparable to the state-of-the-artmethods while processing less than 10% of the WSI at the highest magnificationand reducing the time required to infer the WSI-level label by more than 75%.</description><author>Manahil Raza, Ruqayya Awan, Raja Muhammad Saad Bashir, Talha Qaiser, Nasir M. Rajpoot</author><pubDate>Thu, 21 Nov 2024 16:29:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09682v2</guid></item><item><title>Efficient Aspect-Based Summarization of Climate Change Reports with Small Language Models</title><link>http://arxiv.org/abs/2411.14272v1</link><description>The use of Natural Language Processing (NLP) for helping decision-makers withClimate Change action has recently been highlighted as a use case aligning witha broader drive towards NLP technologies for social good. In this context,Aspect-Based Summarization (ABS) systems that extract and summarize relevantinformation are particularly useful as they provide stakeholders with aconvenient way of finding relevant information in expert-curated reports. Inthis work, we release a new dataset for ABS of Climate Change reports and weemploy different Large Language Models (LLMs) and so-called Small LanguageModels (SLMs) to tackle this problem in an unsupervised way. Considering theproblem at hand, we also show how SLMs are not significantly worse for theproblem while leading to reduced carbon footprint; we do so by applying for thefirst time an existing framework considering both energy efficiency and taskperformance to the evaluation of zero-shot generative models for ABS. Overall,our results show that modern language models, both big and small, caneffectively tackle ABS for Climate Change reports but more research is neededwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem andour work and dataset will help foster efforts in this direction.</description><author>Iacopo Ghinassi, Leonardo Catalano, Tommaso Colella</author><pubDate>Thu, 21 Nov 2024 16:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14272v1</guid></item><item><title>Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines</title><link>http://arxiv.org/abs/2403.11585v3</link><description>In the ever-evolving landscape of machine learning, seamless translation ofnatural language descriptions into executable code remains a formidablechallenge. This paper introduces Linguacodus, an innovative framework designedto tackle this challenge by deploying a dynamic pipeline that iterativelytransforms natural language task descriptions into code through high-leveldata-shaping instructions. The core of Linguacodus is a fine-tuned largelanguage model (LLM), empowered to evaluate diverse solutions for variousproblems and select the most fitting one for a given task. This paper detailsthe fine-tuning process, and sheds light on how natural language descriptionscan be translated into functional code. Linguacodus represents a substantialleap towards automated code generation, effectively bridging the gap betweentask descriptions and executable code. It holds great promise for advancingmachine learning applications across diverse domains. Additionally, we proposean algorithm capable of transforming a natural description of an ML task intocode with minimal human interaction. In extensive experiments on a vast machinelearning code dataset originating from Kaggle, we showcase the effectiveness ofLinguacodus. The investigations highlight its potential applications acrossdiverse domains, emphasizing its impact on applied machine learning in variousscientific fields.</description><author>Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin</author><pubDate>Thu, 21 Nov 2024 16:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11585v3</guid></item><item><title>Guided MRI Reconstruction via Schrödinger Bridge</title><link>http://arxiv.org/abs/2411.14269v1</link><description>Magnetic Resonance Imaging (MRI) is a multi-contrast imaging technique inwhich different contrast images share similar structural information. However,conventional diffusion models struggle to effectively leverage this structuralsimilarity. Recently, the Schr\"odinger Bridge (SB), a nonlinear extension ofthe diffusion model, has been proposed to establish diffusion paths between anydistributions, allowing the incorporation of guided priors. This study proposesan SB-based, multi-contrast image-guided reconstruction framework thatestablishes a diffusion bridge between the guiding and target imagedistributions. By using the guiding image along with data consistency duringsampling, the target image is reconstructed more accurately. To better addressstructural differences between images, we introduce an inversion strategy fromthe field of image editing, termed $\mathbf{I}^2$SB-inversion. Experiments on aparied T1 and T2-FLAIR datasets demonstrate that $\mathbf{I}^2$SB-inversionachieve a high acceleration up to 14.4 and outperforms existing methods interms of both reconstruction accuracy and stability.</description><author>Yue Wang, Tian Zhou, Zhuo-xu Cui, Bingsheng Huang, Hairong Zheng, Dong Liang, Yanjie Zhu</author><pubDate>Thu, 21 Nov 2024 16:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14269v1</guid></item><item><title>Embedding-based Multimodal Learning on Pan-Squamous Cell Carcinomas for Improved Survival Outcomes</title><link>http://arxiv.org/abs/2406.08521v2</link><description>Cancer clinics capture disease data at various scales, from genetic to organlevel. Current bioinformatic methods struggle to handle the heterogeneousnature of this data, especially with missing modalities. We propose PARADIGM, aGraph Neural Network (GNN) framework that learns from multimodal, heterogeneousdatasets to improve clinical outcome prediction. PARADIGM generates embeddingsfrom multi-resolution data using foundation models, aggregates them intopatient-level representations, fuses them into a unified graph, and enhancesperformance for tasks like survival analysis. We train GNNs on pan-SquamousCell Carcinomas and validate our approach on Moffitt Cancer Center lung SCCdata. Multimodal GNN outperforms other models in patient survival prediction.Converging individual data modalities across varying scales provides a moreinsightful disease view. Our solution aims to understand the patient'scircumstances comprehensively, offering insights on heterogeneous dataintegration and the benefits of converging maximum data views.</description><author>Asim Waqas, Aakash Tripathi, Paul Stewart, Mia Naeini, Matthew B. Schabath, Ghulam Rasool</author><pubDate>Thu, 21 Nov 2024 16:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08521v2</guid></item><item><title>VG-SSL: Benchmarking Self-supervised Representation Learning Approaches for Visual Geo-localization</title><link>http://arxiv.org/abs/2308.00090v3</link><description>Visual Geo-localization (VG) is a critical research area for identifyinggeo-locations from visual inputs, particularly in autonomous navigation forrobotics and vehicles. Current VG methods often learn feature extractors fromgeo-labeled images to create dense, geographically relevant representations.Recent advances in Self-Supervised Learning (SSL) have demonstrated itscapability to achieve performance on par with supervised techniques withunlabeled images. This study presents a novel VG-SSL framework, designed forversatile integration and benchmarking of diverse SSL methods forrepresentation learning in VG, featuring a unique geo-related pair strategy,GeoPair. Through extensive performance analysis, we adapt SSL techniques toimprove VG on datasets from hand-held and car-mounted cameras used in roboticsand autonomous vehicles. Our results show that contrastive learning andinformation maximization methods yield superior geo-specific representationquality, matching or surpassing the performance of state-of-the-art VGtechniques. To our knowledge, This is the first benchmarking study of SSL inVG, highlighting its potential in enhancing geo-specific visual representationsfor robotics and autonomous vehicles. The code is publicly available athttps://github.com/arplaboratory/VG-SSL.</description><author>Jiuhong Xiao, Gao Zhu, Giuseppe Loianno</author><pubDate>Thu, 21 Nov 2024 16:21:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00090v3</guid></item><item><title>Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction</title><link>http://arxiv.org/abs/2402.15368v4</link><description>This paper addresses task planning problems for language-instructed robotteams. Tasks are expressed in natural language (NL), requiring the robots toapply their capabilities at various locations and semantic objects. Severalrecent works have addressed similar planning problems by leveraging pre-trainedLarge Language Models (LLMs) to design effective multi-robot plans. However,these approaches lack performance guarantees. To address this challenge, weintroduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnningfor Teams of Language-instructed AgentS, that is capable of achievinguser-defined mission success rates. This is accomplished by leveragingconformal prediction (CP), a distribution-free uncertainty quantification toolin black-box models. CP allows the proposed multi-robot planner to reason aboutits inherent uncertainty in a distributed fashion, enabling robots to makeindividual decisions when they are sufficiently certain and seek helpotherwise. We show, both theoretically and empirically, that the proposedplanner can achieve user-specified task success rates, assuming successful planexecution, while minimizing the overall number of help requests. We providecomparative experiments against related works showing that our method issignificantly more computational efficient and achieves lower help rates. Theadvantage of our algorithm over baselines becomes more pronounced withincreasing robot team size.</description><author>Jun Wang, Guocheng He, Yiannis Kantaros</author><pubDate>Thu, 21 Nov 2024 16:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15368v4</guid></item><item><title>Generating Realistic Adversarial Examples for Business Processes using Variational Autoencoders</title><link>http://arxiv.org/abs/2411.14263v1</link><description>In predictive process monitoring, predictive models are vulnerable toadversarial attacks, where input perturbations can lead to incorrectpredictions. Unlike in computer vision, where these perturbations are designedto be imperceptible to the human eye, the generation of adversarial examples inpredictive process monitoring poses unique challenges. Minor changes to theactivity sequences can create improbable or even impossible scenarios to occurdue to underlying constraints such as regulatory rules or process constraints.To address this, we focus on generating realistic adversarial examples tailoredto the business process context, in contrast to the imperceptible, pixel-levelchanges commonly seen in computer vision adversarial attacks. This paperintroduces two novel latent space attacks, which generate adversaries by addingnoise to the latent space representation of the input data, rather thandirectly modifying the input attributes. These latent space methods aredomain-agnostic and do not rely on process-specific knowledge, as we restrictthe generation of adversarial examples to the learned class-specific datadistributions by directly perturbing the latent space representation of thebusiness process executions. We evaluate these two latent space methods withsix other adversarial attacking methods on eleven real-life event logs and fourpredictive models. The first three attacking methods directly permute theactivities of the historically observed business process executions. The fourthmethod constrains the adversarial examples to lie within the same datadistribution as the original instances, by projecting the adversarial examplesto the original data distribution.</description><author>Alexander Stevens, Jari Peeperkorn, Johannes De Smedt, Jochen De Weerdt</author><pubDate>Thu, 21 Nov 2024 16:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14263v1</guid></item><item><title>High-performance real-world optical computing trained by in situ gradient-based model-free optimization</title><link>http://arxiv.org/abs/2307.11957v6</link><description>Optical computing systems provide high-speed and low-energy data processingbut face deficiencies in computationally demanding training andsimulation-to-reality gaps. We propose a gradient-based model-free optimization(G-MFO) method based on a Monte Carlo gradient estimation algorithm forcomputationally efficient in situ training of optical computing systems. Thisapproach treats an optical computing system as a black box and back-propagatesthe loss directly to the optical computing weights' probability distributions,circumventing the need for a computationally heavy and biased systemsimulation. Our experiments on diffractive optical computing systems show thatG-MFO outperforms hybrid training on the MNIST and FMNIST datasets.Furthermore, we demonstrate image-free and high-speed classification of cellsfrom their marker-free phase maps. Our method's model-free and high-performancenature, combined with its low demand for computational resources, paves the wayfor accelerating the transition of optical computing from laboratorydemonstrations to practical, real-world applications.</description><author>Guangyuan Zhao, Xin Shu, Renjie Zhou</author><pubDate>Thu, 21 Nov 2024 16:13:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11957v6</guid></item><item><title>HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology Datasets with Foundational Embedding Models</title><link>http://arxiv.org/abs/2405.07460v4</link><description>Developing accurate machine learning models for oncology requireslarge-scale, high-quality multimodal datasets. However, creating such datasetsremains challenging due to the complexity and heterogeneity of medical data. Toaddress this challenge, we introduce HoneyBee, a scalable modular framework forbuilding multimodal oncology datasets that leverages foundation models togenerate representative embeddings. HoneyBee integrates various datamodalities, including clinical diagnostic and pathology imaging data, medicalnotes, reports, records, and molecular data. It employs data preprocessingtechniques and foundation models to generate embeddings that capture theessential features and relationships within the raw medical data. The generatedembeddings are stored in a structured format using Hugging Face datasets andPyTorch dataloaders for accessibility. Vector databases enable efficientquerying and retrieval for machine learning applications. We demonstrate theeffectiveness of HoneyBee through experiments assessing the quality andrepresentativeness of these embeddings. The framework is designed to beextensible to other medical domains and aims to accelerate oncology research byproviding high-quality, machine learning-ready datasets. HoneyBee is an ongoingopen-source effort, and the code, datasets, and models are available at theproject repository.</description><author>Aakash Tripathi, Asim Waqas, Matthew B. Schabath, Yasin Yilmaz, Ghulam Rasool</author><pubDate>Thu, 21 Nov 2024 16:12:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07460v4</guid></item><item><title>EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation</title><link>http://arxiv.org/abs/2410.21271v2</link><description>In this work, we re-formulate the model compression problem into thecustomized compensation problem: Given a compressed model, we aim to introduceresidual low-rank paths to compensate for compression errors under customizedrequirements from users (e.g., tasks, compression ratios), resulting in greaterflexibility in adjusting overall capacity without being constrained by specificcompression formats. However, naively applying SVD to derive residual pathscauses suboptimal utilization of the low-rank representation capacity. Instead,we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a methodthat directly minimizes compression-induced errors without requiringgradient-based training, achieving fast optimization in minutes using a smallamount of calibration data. EoRA projects compression errors into theeigenspace of input activations, leveraging eigenvalues to effectivelyprioritize the reconstruction of high-importance error components. Moreover,EoRA can be seamlessly integrated with fine-tuning and quantization to furtherimprove effectiveness and efficiency. EoRA consistently outperforms previousmethods in compensating errors for compressed LLaMA2/3 models on various tasks,such as language generation, commonsense reasoning, and math reasoning tasks(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge andMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4sparsity). EoRA offers a scalable, training-free solution to compensate forcompression errors, making it a powerful tool to deploy LLMs in variouscapacity and efficiency requirements.</description><author>Shih-Yang Liu, Huck Yang, Chien-Yi Wang, Nai Chit Fung, Hongxu Yin, Charbel Sakr, Saurav Muralidharan, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen</author><pubDate>Thu, 21 Nov 2024 16:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21271v2</guid></item><item><title>Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective</title><link>http://arxiv.org/abs/2411.14258v1</link><description>Large Language Models (LLMs) have revolutionized Natural Language Processing(NLP) based applications including automated text generation, questionanswering, chatbots, and others. However, they face a significant challenge:hallucinations, where models produce plausible-sounding but factually incorrectresponses. This undermines trust and limits the applicability of LLMs indifferent domains. Knowledge Graphs (KGs), on the other hand, provide astructured collection of interconnected facts represented as entities (nodes)and their relationships (edges). In recent research, KGs have been leveraged toprovide context that can fill gaps in an LLM understanding of certain topicsoffering a promising approach to mitigate hallucinations in LLMs, enhancingtheir reliability and accuracy while benefiting from their wide applicability.Nonetheless, it is still a very active area of research with various unresolvedopen problems. In this paper, we discuss these open challenges coveringstate-of-the-art datasets and benchmarks as well as methods for knowledgeintegration and evaluating hallucinations. In our discussion, we consider thecurrent use of KGs in LLM systems and identify future directions within each ofthese challenges.</description><author>Ernests Lavrinovics, Russa Biswas, Johannes Bjerva, Katja Hose</author><pubDate>Thu, 21 Nov 2024 16:09:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14258v1</guid></item><item><title>BERTrend: Neural Topic Modeling for Emerging Trends Detection</title><link>http://arxiv.org/abs/2411.05930v2</link><description>Detecting and tracking emerging trends and weak signals in large, evolvingtext corpora is vital for applications such as monitoring scientificliterature, managing brand reputation, surveilling critical infrastructure andmore generally to any kind of text-based event detection. Existing solutionsoften fail to capture the nuanced context or dynamically track evolvingpatterns over time. BERTrend, a novel method, addresses these limitations usingneural topic modeling in an online setting. It introduces a new metric toquantify topic popularity over time by considering both the number of documentsand update frequency. This metric classifies topics as noise, weak, or strongsignals, flagging emerging, rapidly growing topics for further investigation.Experimentation on two large real-world datasets demonstrates BERTrend'sability to accurately detect and track meaningful weak signals while filteringout noise, offering a comprehensive solution for monitoring emerging trends inlarge-scale, evolving text corpora. The method can also be used forretrospective analysis of past events. In addition, the use of Large LanguageModels together with BERTrend offers efficient means for the interpretabilityof trends of events.</description><author>Allaa Boutaleb, Jerome Picault, Guillaume Grosjean</author><pubDate>Thu, 21 Nov 2024 16:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05930v2</guid></item><item><title>Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</title><link>http://arxiv.org/abs/2411.14257v1</link><description>Hallucinations in large language models are a widespread problem, yet themechanisms behind whether models will hallucinate are poorly understood,limiting our ability to solve this problem. Using sparse autoencoders as aninterpretability tool, we discover that a key part of these mechanisms isentity recognition, where the model detects if an entity is one it can recallfacts about. Sparse autoencoders uncover meaningful directions in therepresentation space, these detect whether the model recognizes an entity, e.g.detecting it doesn't know about an athlete or a movie. This suggests thatmodels can have self-knowledge: internal representations about their owncapabilities. These directions are causally relevant: capable of steering themodel to refuse to answer questions about known entities, or to hallucinateattributes of unknown entities when it would otherwise refuse. We demonstratethat despite the sparse autoencoders being trained on the base model, thesedirections have a causal effect on the chat model's refusal behavior,suggesting that chat finetuning has repurposed this existing mechanism.Furthermore, we provide an initial exploration into the mechanistic role ofthese directions in the model, finding that they disrupt the attention ofdownstream heads that typically move entity attributes to the final token.</description><author>Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan, Neel Nanda</author><pubDate>Thu, 21 Nov 2024 16:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14257v1</guid></item><item><title>BERT-Based Approach for Automating Course Articulation Matrix Construction with Explainable AI</title><link>http://arxiv.org/abs/2411.14254v1</link><description>Course Outcome (CO) and Program Outcome (PO)/Program-Specific Outcome (PSO)alignment is a crucial task for ensuring curriculum coherence and assessingeducational effectiveness. The construction of a Course Articulation Matrix(CAM), which quantifies the relationship between COs and POs/PSOs, typicallyinvolves assigning numerical values (0, 1, 2, 3) to represent the degree ofalignment. In this study, We experiment with four models from the BERT family:BERT Base, DistilBERT, ALBERT, and RoBERTa, and use multiclass classificationto assess the alignment between CO and PO/PSO pairs. We first evaluatetraditional machine learning classifiers, such as Decision Tree, Random Forest,and XGBoost, and then apply transfer learning to evaluate the performance ofthe pretrained BERT models. To enhance model interpretability, we applyExplainable AI technique, specifically Local Interpretable Model-agnosticExplanations (LIME), to provide transparency into the decision-making process.Our system achieves accuracy, precision, recall, and F1-score values of 98.66%,98.67%, 98.66%, and 98.66%, respectively. This work demonstrates the potentialof utilizing transfer learning with BERT-based models for the automatedgeneration of CAMs, offering high performance and interpretability ineducational outcome assessment.</description><author>Natenaile Asmamaw Shiferaw, Simpenzwe Honore Leandre, Aman Sinha, Dillip Rout</author><pubDate>Thu, 21 Nov 2024 16:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14254v1</guid></item><item><title>Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for Multi-Turn Intent Classification</title><link>http://arxiv.org/abs/2411.14252v1</link><description>Generating large-scale, domain-specific, multilingual multi-turn dialoguedatasets remains a significant hurdle for training effective Multi-Turn IntentClassification models in chatbot systems. In this paper, we introduceChain-of-Intent, a novel mechanism that combines Hidden Markov Models withLarge Language Models (LLMs) to generate contextually aware, intent-drivenconversations through self-play. By extracting domain-specific knowledge frome-commerce chat logs, we estimate conversation turns and intent transitions,which guide the generation of coherent dialogues. Leveraging LLMs to enhanceemission probabilities, our approach produces natural and contextuallyconsistent questions and answers. We also propose MINT-CL, a framework formulti-turn intent classification using multi-task contrastive learning,improving classification accuracy without the need for extensive annotateddata. Evaluations show that our methods outperform baselines in dialoguequality and intent classification accuracy, especially in multilingualsettings, while significantly reducing data generation efforts. Furthermore, werelease MINT-E, a multilingual, intent-aware multi-turn e-commerce dialoguecorpus to support future research in this area.</description><author>Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim</author><pubDate>Thu, 21 Nov 2024 15:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14252v1</guid></item><item><title>Natural Language Reinforcement Learning</title><link>http://arxiv.org/abs/2411.14251v1</link><description>Reinforcement Learning (RL) mathematically formulates decision-making withMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkablebreakthroughs across various domains, including games, robotics, and languagemodels. This paper seeks a new possibility, Natural Language ReinforcementLearning (NLRL), by extending traditional MDP to natural language-basedrepresentation space. Specifically, NLRL innovatively redefines RL principles,including task objectives, policy, value function, Bellman equation, and policyiteration, into their language counterparts. With recent advancements in largelanguage models (LLMs), NLRL can be practically implemented to achieve RL-likepolicy and value improvement by either pure prompting or gradient-basedtraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe gamesdemonstrate the effectiveness, efficiency, and interpretability of the NLRLframework among diverse use cases. Our code will be released athttps://github.com/waterhorse1/Natural-language-RL.</description><author>Xidong Feng, Ziyu Wan, Haotian Fu, Bo Liu, Mengyue Yang, Girish A. Koushik, Zhiyuan Hu, Ying Wen, Jun Wang</author><pubDate>Thu, 21 Nov 2024 15:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14251v1</guid></item><item><title>VeriGraph: Scene Graphs for Execution Verifiable Robot Planning</title><link>http://arxiv.org/abs/2411.10446v2</link><description>Recent advancements in vision-language models (VLMs) offer potential forrobot task planning, but challenges remain due to VLMs' tendency to generateincorrect action sequences. To address these limitations, we propose VeriGraph,a novel framework that integrates VLMs for robotic planning while verifyingaction feasibility. VeriGraph employs scene graphs as an intermediaterepresentation, capturing key objects and spatial relationships to improve planverification and refinement. The system generates a scene graph from inputimages and uses it to iteratively check and correct action sequences generatedby an LLM-based task planner, ensuring constraints are respected and actionsare executable. Our approach significantly enhances task completion ratesacross diverse manipulation scenarios, outperforming baseline methods by 58%for language-based tasks and 30% for image-based tasks.</description><author>Daniel Ekpo, Mara Levy, Saksham Suri, Chuong Huynh, Abhinav Shrivastava</author><pubDate>Thu, 21 Nov 2024 15:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10446v2</guid></item><item><title>CP-UNet: Contour-based Probabilistic Model for Medical Ultrasound Images Segmentation</title><link>http://arxiv.org/abs/2411.14250v1</link><description>Deep learning-based segmentation methods are widely utilized for detectinglesions in ultrasound images. Throughout the imaging procedure, the attenuationand scattering of ultrasound waves cause contour blurring and the formation ofartifacts, limiting the clarity of the acquired ultrasound images. To overcomethis challenge, we propose a contour-based probabilistic segmentation modelCP-UNet, which guides the segmentation network to enhance its focus on contourduring decoding. We design a novel down-sampling module to enable the contourprobability distribution modeling and encoding stages to acquire global-localfeatures. Furthermore, the Gaussian Mixture Model utilizes optimized featuresto model the contour distribution, capturing the uncertainty of lesionboundaries. Extensive experiments with several state-of-the-art deep learningsegmentation methods on three ultrasound image datasets show that our methodperforms better on breast and thyroid lesions segmentation.</description><author>Ruiguo Yu, Yiyang Zhang, Yuan Tian, Zhiqiang Liu, Xuewei Li, Jie Gao</author><pubDate>Thu, 21 Nov 2024 15:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14250v1</guid></item><item><title>Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation</title><link>http://arxiv.org/abs/2407.19523v3</link><description>Meta-learning is a practical learning paradigm to transfer skills acrosstasks from a few examples. Nevertheless, the existence of task distributionshifts tends to weaken meta-learners' generalization capability, particularlywhen the training task distribution is naively hand-crafted or based on simplepriors that fail to cover critical scenarios sufficiently. Here, we considerexplicitly generative modeling task distributions placed over task identifiersand propose robustifying fast adaptation from adversarial training. Ourapproach, which can be interpreted as a model of a Stackelberg game, not onlyuncovers the task structure during problem-solving from an explicit generativemodel but also theoretically increases the adaptation robustness in worstcases. This work has practical implications, particularly in dealing with taskdistribution shifts in meta-learning, and contributes to theoretical insightsin the field. Our method demonstrates its robustness in the presence of tasksubpopulation shifts and improved performance over SOTA baselines in extensiveexperiments. The code will be available at the project sitehttps://sites.google.com/view/ar-metalearn.</description><author>Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, Xiangyang Ji</author><pubDate>Thu, 21 Nov 2024 15:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19523v3</guid></item><item><title>Simulation-Aided Policy Tuning for Black-Box Robot Learning</title><link>http://arxiv.org/abs/2411.14246v1</link><description>How can robots learn and adapt to new tasks and situations with little data?Systematic exploration and simulation are crucial tools for efficient robotlearning. We present a novel black-box policy search algorithm focused ondata-efficient policy improvements. The algorithm learns directly on the robotand treats simulation as an additional information source to speed up thelearning process. At the core of the algorithm, a probabilistic model learnsthe dependence of the policy parameters and the robot learning objective notonly by performing experiments on the robot, but also by leveraging data from asimulator. This substantially reduces interaction time with the robot. Usingthis model, we can guarantee improvements with high probability for each policyupdate, thereby facilitating fast, goal-oriented learning. We evaluate ouralgorithm on simulated fine-tuning tasks and demonstrate the data-efficiency ofthe proposed dual-information source optimization algorithm. In a real robotlearning experiment, we show fast and successful task learning on a robotmanipulator with the aid of an imperfect simulator.</description><author>Shiming He, Alexander von Rohr, Dominik Baumann, Ji Xiang, Sebastian Trimpe</author><pubDate>Thu, 21 Nov 2024 15:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14246v1</guid></item><item><title>AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection</title><link>http://arxiv.org/abs/2411.14243v1</link><description>As object detection becomes integral to many safety-critical applications,understanding its vulnerabilities is essential. Backdoor attacks, inparticular, pose a significant threat by implanting hidden backdoor in a victimmodel, which adversaries can later exploit to trigger malicious behaviorsduring inference. However, current backdoor techniques are limited to staticscenarios where attackers must define a malicious objective before training,locking the attack into a predetermined action without inference-timeadaptability. Given the expressive output space in object detection, includingobject existence detection, bounding box estimation, and object classification,the feasibility of implanting a backdoor that provides inference-time controlwith a high degree of freedom remains unexplored. This paper introducesAnywhereDoor, a flexible backdoor attack tailored for object detection. Onceimplanted, AnywhereDoor enables adversaries to specify different attack types(object vanishing, fabrication, or misclassification) and configurations(untargeted or targeted with specific classes) to dynamically control detectionbehavior. This flexibility is achieved through three key innovations: (i)objective disentanglement to support a broader range of attack combinationswell beyond what existing methods allow; (ii) trigger mosaicking to ensurebackdoor activations are robust, even against those object detectors thatextract localized regions from the input image for recognition; and (iii)strategic batching to address object-level data imbalances that otherwisehinders a balanced manipulation. Extensive experiments demonstrate thatAnywhereDoor provides attackers with a high degree of control, achieving anattack success rate improvement of nearly 80% compared to adaptations ofexisting methods for such flexible control.</description><author>Jialin Lu, Junjie Shan, Ziqi Zhao, Ka-Ho Chow</author><pubDate>Thu, 21 Nov 2024 15:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14243v1</guid></item><item><title>Data-centric Graph Learning: A Survey</title><link>http://arxiv.org/abs/2310.04987v3</link><description>The history of artificial intelligence (AI) has witnessed the significantimpact of high-quality data on various deep learning models, such as ImageNetfor AlexNet and ResNet. Recently, instead of designing more complex neuralarchitectures as model-centric approaches, the attention of AI community hasshifted to data-centric ones, which focuses on better processing data tostrengthen the ability of neural models. Graph learning, which operates onubiquitous topological data, also plays an important role in the era of deeplearning. In this survey, we comprehensively review graph learning approachesfrom the data-centric perspective, and aim to answer three crucial questions:(1) when to modify graph data, (2) what part of the graph data needsmodification to unlock the potential of various graph models, and (3) how tosafeguard graph models from problematic data influence. Accordingly, we proposea novel taxonomy based on the stages in the graph learning pipeline, andhighlight the processing methods for different data structures in the graphdata, i.e., topology, feature and label. Furthermore, we analyze some potentialproblems embedded in graph data and discuss how to solve them in a data-centricmanner. Finally, we provide some promising future directions for data-centricgraph learning.</description><author>Yuxin Guo, Deyu Bo, Cheng Yang, Zhiyuan Lu, Zhongjian Zhang, Jixi Liu, Yufei Peng, Chuan Shi</author><pubDate>Thu, 21 Nov 2024 15:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04987v3</guid></item><item><title>FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression</title><link>http://arxiv.org/abs/2411.14228v1</link><description>Recent advances on Multi-modal Large Language Models have demonstrated thathigh-resolution image input is crucial for model capabilities, especially forfine-grained tasks. However, high-resolution images lead to a quadraticincrease in the number of visual tokens input into LLMs, resulting insignificant computational costs. Current work develop visual token compressionmethods to achieve efficiency improvements, often at the expense ofperformance. We argue that removing visual redundancy can simultaneouslyimprove both efficiency and performance. We build a coarse-to-fine visual tokencompression method, with a vision-guided sampler for compressing redundantregions with low information density, and a text-guided sampler for selectingvisual tokens that are strongly correlated with the user instructions.Withthese two modules, the proposed FocusLLaVA achieves improvements in bothefficiency and performance. We validate the effectiveness of our approach on awide range of evaluation datasets.</description><author>Yuke Zhu, Chi Xie, Shuang Liang, Bo Zheng, Sheng Guo</author><pubDate>Thu, 21 Nov 2024 15:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14228v1</guid></item><item><title>Graph Neural Networks and Arithmetic Circuits</title><link>http://arxiv.org/abs/2402.17805v2</link><description>We characterize the computational power of neural networks that follow thegraph neural network (GNN) architecture, not restricted to aggregate-combineGNNs or other particular types. We establish an exact correspondence betweenthe expressivity of GNNs using diverse activation functions and arithmeticcircuits over real numbers. In our results the activation function of thenetwork becomes a gate type in the circuit. Our result holds for families ofconstant depth circuits and networks, both uniformly and non-uniformly, for allcommon activation functions.</description><author>Timon Barlag, Vivian Holzapfel, Laura Strieker, Jonni Virtema, Heribert Vollmer</author><pubDate>Thu, 21 Nov 2024 15:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17805v2</guid></item><item><title>VerA: Versatile Anonymization Applicable to Clinical Facial Photographs</title><link>http://arxiv.org/abs/2312.02124v2</link><description>The demand for privacy in facial image dissemination is gaining groundinternationally, echoed by the proliferation of regulations such as GDPR,DPDPA, CCPA, PIPL, and APPI. While recent advances in anonymization surpasspixelation or blur methods, additional constraints to the task pose challenges.Largely unaddressed by current anonymization methods are clinical images andpairs of before-and-after clinical images illustrating facial medicalinterventions, e.g., facial surgeries or dental procedures. We present VerA,the first Versatile Anonymization framework that solves two challenges inclinical applications: A) it preserves selected semantic areas (e.g., mouthregion) to show medical intervention results, that is, anonymization is onlyapplied to the areas outside the preserved area; and B) it produces anonymizedimages with consistent personal identity across multiple photographs, which iscrucial for anonymizing photographs of the same person taken before and after aclinical intervention. We validate our results on both single and pairedanonymization of clinical images through extensive quantitative and qualitativeevaluation. We also demonstrate that VerA reaches the state of the art onestablished anonymization tasks, in terms of photorealism andde-identification.</description><author>Majed El Helou, Doruk Cetin, Petar Stamenkovic, Niko Benjamin Huber, Fabio Zünd</author><pubDate>Thu, 21 Nov 2024 15:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02124v2</guid></item><item><title>Towards Context-Rich Automated Biodiversity Assessments: Deriving AI-Powered Insights from Camera Trap Data</title><link>http://arxiv.org/abs/2411.14219v1</link><description>Camera traps offer enormous new opportunities in ecological studies, butcurrent automated image analysis methods often lack the contextual richnessneeded to support impactful conservation outcomes. Here we present anintegrated approach that combines deep learning-based vision and languagemodels to improve ecological reporting using data from camera traps. Weintroduce a two-stage system: YOLOv10-X to localise and classify species(mammals and birds) within images, and a Phi-3.5-vision-instruct model to readYOLOv10-X binding box labels to identify species, overcoming its limitationwith hard to classify objects in images. Additionally, Phi-3.5 detects broadervariables, such as vegetation type, and time of day, providing rich ecologicaland environmental context to YOLO's species detection output. When combined,this output is processed by the model's natural language system to answercomplex queries, and retrieval-augmented generation (RAG) is employed to enrichresponses with external information, like species weight and IUCN status(information that cannot be obtained through direct visual analysis). Thisinformation is used to automatically generate structured reports, providingbiodiversity stakeholders with deeper insights into, for example, speciesabundance, distribution, animal behaviour, and habitat selection. Our approachdelivers contextually rich narratives that aid in wildlife managementdecisions. By providing contextually rich insights, our approach not onlyreduces manual effort but also supports timely decision-making in conservation,potentially shifting efforts from reactive to proactive management.</description><author>Paul Fergus, Carl Chalmers, Naomi Matthews, Stuart Nixon, Andre Burger, Oliver Hartley, Chris Sutherland, Xavier Lambin, Steven Longmore, Serge Wich</author><pubDate>Thu, 21 Nov 2024 15:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14219v1</guid></item><item><title>Learning multivariate Gaussians with imperfect advice</title><link>http://arxiv.org/abs/2411.12700v2</link><description>We revisit the problem of distribution learning within the framework oflearning-augmented algorithms. In this setting, we explore the scenario where aprobability distribution is provided as potentially inaccurate advice on thetrue, unknown distribution. Our objective is to develop learning algorithmswhose sample complexity decreases as the quality of the advice improves,thereby surpassing standard learning lower bounds when the advice issufficiently accurate. Specifically, we demonstrate that this outcome is achievable for the problemof learning a multivariate Gaussian distribution $N(\boldsymbol{\mu},\boldsymbol{\Sigma})$ in the PAC learning setting. Classically, in theadvice-free setting, $\tilde{\Theta}(d^2/\varepsilon^2)$ samples are sufficientand worst case necessary to learn $d$-dimensional Gaussians up to TV distance$\varepsilon$ with constant probability. When we are additionally given aparameter $\tilde{\boldsymbol{\Sigma}}$ as advice, we show that$\tilde{O}(d^{2-\beta}/\varepsilon^2)$ samples suffices whenever $\|\tilde{\boldsymbol{\Sigma}}^{-1/2} \boldsymbol{\Sigma}\tilde{\boldsymbol{\Sigma}}^{-1/2} - \boldsymbol{I_d} \|_1 \leq \varepsilond^{1-\beta}$ (where $\|\cdot\|_1$ denotes the entrywise $\ell_1$ norm) for any$\beta &gt; 0$, yielding a polynomial improvement over the advice-free setting.</description><author>Arnab Bhattacharyya, Davin Choo, Philips George John, Themis Gouleakis</author><pubDate>Thu, 21 Nov 2024 15:27:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12700v2</guid></item><item><title>Evaluating the Robustness of Analogical Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2411.14215v1</link><description>LLMs have performed well on several reasoning benchmarks, including ones thattest analogical reasoning abilities. However, there is debate on the extent towhich they are performing general abstract reasoning versus employingnon-robust processes, e.g., that overly rely on similarity to pre-trainingdata. Here we investigate the robustness of analogy-making abilities previouslyclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu(2023): letter-string analogies, digit matrices, and story analogies. For eachdomain we test humans and GPT models on robustness to variants of the originalanalogy problems that test the same abstract reasoning abilities but are likelydissimilar from tasks in the pre-training data. The performance of a systemthat uses robust abstract reasoning should not decline substantially on thesevariants. On simple letter-string analogies, we find that while the performance ofhumans remains high for two types of variants we tested, the GPT models'performance declines sharply. This pattern is less pronounced as the complexityof these problems is increased, as both humans and GPT models perform poorly onboth the original and variant problems requiring more complex analogies. Ondigit-matrix problems, we find a similar pattern but only on one out of the twotypes of variants we tested. On story-based analogy problems, we find that,unlike humans, the performance of GPT models are susceptible to answer-ordereffects, and that GPT models also may be more sensitive than humans toparaphrasing. This work provides evidence that LLMs often lack the robustness of zero-shothuman analogy-making, exhibiting brittleness on most of the variations wetested. More generally, this work points to the importance of carefullyevaluating AI systems not only for accuracy but also robustness when testingtheir cognitive capabilities.</description><author>Martha Lewis, Melanie Mitchell</author><pubDate>Thu, 21 Nov 2024 15:25:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14215v1</guid></item><item><title>Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems</title><link>http://arxiv.org/abs/2411.14214v1</link><description>LLM-based autonomous agents have demonstrated outstanding performance insolving complex industrial tasks. However, in the pursuit of carbon neutralityand high-performance renewable energy systems, existing AI-assisted designautomation faces significant limitations in explainability, scalability, andusability. To address these challenges, we propose LP-COMDA, an LLM-based,physics-informed autonomous agent that automates the modulation design of powerconverters in Power Electronics Systems with minimal human supervision. Unliketraditional AI-assisted approaches, LP-COMDA contains an LLM-based planner thatgathers and validates design specifications through a user-friendly chatinterface. The planner then coordinates with physics-informed design andoptimization tools to iteratively generate and refine modulation designsautonomously. Through the chat interface, LP-COMDA provides an explainabledesign process, presenting explanations and charts. Experiments show thatLP-COMDA outperforms all baseline methods, achieving a 63.2% reduction in errorcompared to the second-best benchmark method in terms of standard mean absoluteerror. Furthermore, empirical studies with 20 experts conclude that design timewith LP-COMDA is over 33 times faster than conventional methods, showing itssignificant improvement on design efficiency over the current processes.</description><author>Junhua Liu, Fanfan Lin, Xinze Li, Kwan Hui Lim, Shuai Zhao</author><pubDate>Thu, 21 Nov 2024 15:24:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14214v1</guid></item><item><title>Generative Outpainting To Enhance the Memorability of Short-Form Videos</title><link>http://arxiv.org/abs/2411.14213v1</link><description>With the expanding use of the short-form video format in advertising, socialmedia, entertainment, education and more, there is a need for such media toboth captivate and be remembered. Video memorability indicates to us how likelya video is to be remembered by a viewer who has no emotional or personalconnection with its content. This paper presents the results of usinggenerative outpainting to expand the screen size of a short-form video with aview to improving its memorability. Advances in machine learning and deeplearning are compared and leveraged to understand how extending the borders ofvideo screensizes can affect their memorability to viewers. Using quantitativeevaluation we determine the best-performing model for outpainting and theimpact of outpainting based on image saliency on video memorability scores</description><author>Alan Byju, Aman Sudhindra Ladwa, Lorin Sweeney, Alan F. Smeaton</author><pubDate>Thu, 21 Nov 2024 15:24:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14213v1</guid></item><item><title>A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction</title><link>http://arxiv.org/abs/2308.08812v2</link><description>Single-image 3D reconstruction is a research challenge focused on predicting3D object shapes from single-view images. This task requires significant dataacquisition to predict both visible and occluded portions of the shape.Furthermore, learning-based methods face the difficulty of creating acomprehensive training dataset for all possible classes. To this end, wepropose a continual learning-based 3D reconstruction method where our goal isto design a model using Variational Priors that can still reconstruct thepreviously seen classes reasonably even after training on new classes.Variational Priors represent abstract shapes and combat forgetting, whereassaliency maps preserve object attributes with less memory usage. This is vitaldue to resource constraints in storing extensive training data. Additionally,we introduce saliency map-based experience replay to capture global anddistinct object features. Thorough experiments show competitive resultscompared to established methods, both quantitatively and qualitatively.</description><author>Sanchar Palit, Sandika Biswas</author><pubDate>Thu, 21 Nov 2024 15:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08812v2</guid></item><item><title>HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset</title><link>http://arxiv.org/abs/2411.14207v1</link><description>This contribution introduces a dataset of 7th-order Ambisonic Room ImpulseResponses (HOA-RIRs), created using the Image Source Method. By employinghigher-order Ambisonics, our dataset enables precise spatial audioreproduction, a critical requirement for realistic immersive audioapplications. Leveraging the virtual simulation, we present a unique microphoneconfiguration, based on the superposition principle, designed to optimize soundfield coverage while addressing the limitations of traditional microphonearrays. The presented 64-microphone configuration allows us to capture RIRsdirectly in the Spherical Harmonics domain. The dataset features a wide rangeof room configurations, encompassing variations in room geometry, acousticabsorption materials, and source-receiver distances. A detailed description ofthe simulation setup is provided alongside for an accurate reproduction. Thedataset serves as a vital resource for researchers working on spatial audio,particularly in applications involving machine learning to improve roomacoustics modeling and sound field synthesis. It further provides a very highlevel of spatial resolution and realism crucial for tasks such as sourcelocalization, reverberation prediction, and immersive sound reproduction.</description><author>Shivam Saini, Jürgen Peissig</author><pubDate>Thu, 21 Nov 2024 15:16:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14207v1</guid></item><item><title>Novel View Extrapolation with Video Diffusion Priors</title><link>http://arxiv.org/abs/2411.14208v1</link><description>The field of novel view synthesis has made significant strides thanks to thedevelopment of radiance field methods. However, most radiance field techniquesare far better at novel view interpolation than novel view extrapolation wherethe synthesis novel views are far beyond the observed training views. We designViewExtrapolator, a novel view synthesis approach that leverages the generativepriors of Stable Video Diffusion (SVD) for realistic novel view extrapolation.By redesigning the SVD denoising process, ViewExtrapolator refines theartifact-prone views rendered by radiance fields, greatly enhancing the clarityand realism of the synthesized novel views. ViewExtrapolator is a generic novelview extrapolator that can work with different types of 3D rendering such asviews rendered from point clouds when only a single view or monocular video isavailable. Additionally, ViewExtrapolator requires no fine-tuning of SVD,making it both data-efficient and computation-efficient. Extensive experimentsdemonstrate the superiority of ViewExtrapolator in novel view extrapolation.Project page: \url{https://kunhao-liu.github.io/ViewExtrapolator/}.</description><author>Kunhao Liu, Ling Shao, Shijian Lu</author><pubDate>Thu, 21 Nov 2024 15:16:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14208v1</guid></item><item><title>Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression</title><link>http://arxiv.org/abs/2401.13721v3</link><description>Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt modelsfrom a labeled source domain to an unlabeled target domain for regressiontasks. Traditional feature alignment methods, successful in classification,often prove ineffective for regression due to the correlated nature ofregression features. To address this challenge, we propose Uncertainty-GuidedAlignment (UGA), a novel method that integrates predictive uncertainty into thefeature alignment process. UGA employs Evidential Deep Learning to predict bothtarget values and their associated uncertainties. This uncertainty informationguides the alignment process and fuses information within the embedding space,effectively mitigating issues such as feature collapse in out-of-distributionscenarios. We evaluate UGA on two computer vision benchmarks and a real-worldbattery state-of-charge prediction across different manufacturers and operatingtemperatures. Across 52 transfer tasks, UGA on average outperforms existingstate-of-the-art methods. Our approach not only improves adaptation performancebut also provides well-calibrated uncertainty estimates.</description><author>Ismail Nejjar, Gaetan Frusque, Florent Forest, Olga Fink</author><pubDate>Thu, 21 Nov 2024 15:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13721v3</guid></item></channel></rss>