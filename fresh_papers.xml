<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Dec 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera</title><link>http://arxiv.org/abs/2312.14157v1</link><description>3D hand tracking from a monocular video is a very challenging problem due tohand interactions, occlusions, left-right hand ambiguity, and fast motion. Mostexisting methods rely on RGB inputs, which have severe limitations underlow-light conditions and suffer from motion blur. In contrast, event camerascapture local brightness changes instead of full image frames and do not sufferfrom the described effects. Unfortunately, existing image-based techniquescannot be directly applied to events due to significant differences in the datamodalities. In response to these challenges, this paper introduces the firstframework for 3D tracking of two fast-moving and interacting hands from asingle monocular event camera. Our approach tackles the left-right handambiguity with a novel semi-supervised feature-wise attention mechanism andintegrates an intersection loss to fix hand collisions. To facilitate advancesin this research domain, we release a new synthetic large-scale dataset of twointeracting hands, Ev2Hands-S, and a new real benchmark with real event streamsand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existingmethods in terms of the 3D reconstruction accuracy and generalises to real dataunder severe light conditions.</description><author>Christen Millerdurai, Diogo Luvizon, Viktor Rudnev, André Jonas, Jiayi Wang, Christian Theobalt, Vladislav Golyanik</author><pubDate>Thu, 21 Dec 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14157v1</guid></item><item><title>Virtual Pets: Animatable Animal Generation in 3D Scenes</title><link>http://arxiv.org/abs/2312.14154v1</link><description>Toward unlocking the potential of generative models in immersive 4Dexperiences, we introduce Virtual Pet, a novel pipeline to model realistic anddiverse motions for target animal species within a 3D environment. Tocircumvent the limited availability of 3D motion data aligned withenvironmental geometry, we leverage monocular internet videos and extractdeformable NeRF representations for the foreground and static NeRFrepresentations for the background. For this, we develop a reconstructionstrategy, encompassing species-level shared template learning and per-videofine-tuning. Utilizing the reconstructed data, we then train a conditional 3Dmotion model to learn the trajectory and articulation of foreground animals inthe context of 3D backgrounds. We showcase the efficacy of our pipeline withcomprehensive qualitative and quantitative evaluations using cat videos. Wealso demonstrate versatility across unseen cats and indoor environments,producing temporally coherent 4D outputs for enriched virtual experiences.</description><author>Yen-Chi Cheng, Chieh Hubert Lin, Chaoyang Wang, Yash Kant, Sergey Tulyakov, Alexander Schwing, Liangyan Gui, Hsin-Ying Lee</author><pubDate>Thu, 21 Dec 2023 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14154v1</guid></item><item><title>DriveLM: Driving with Graph Visual Question Answering</title><link>http://arxiv.org/abs/2312.14150v1</link><description>We study how vision-language models (VLMs) trained on web-scale data can beintegrated into end-to-end driving systems to boost generalization and enableinteractivity with human users. While recent approaches adapt VLMs to drivingvia single-round visual question answering (VQA), human drivers reason aboutdecisions in multiple steps. Starting from the localization of key objects,humans estimate object interactions before taking actions. The key insight isthat with our proposed task, Graph VQA, where we model graph-structuredreasoning through perception, prediction and planning question-answer pairs, weobtain a suitable proxy task to mimic the human reasoning process. Weinstantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and proposea VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQAand end-to-end driving. The experiments demonstrate that Graph VQA provides asimple, principled framework for reasoning about a driving scene, andDriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agentbaseline performs end-to-end autonomous driving competitively in comparison tostate-of-the-art driving-specific architectures. Notably, its benefits arepronounced when it is evaluated zero-shot on unseen objects or sensorconfigurations. We hope this work can be the starting point to shed new lighton how to apply VLMs for autonomous driving. To facilitate future research, allcode, data, and models are available to the public.</description><author>Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, Hongyang Li</author><pubDate>Thu, 21 Dec 2023 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14150v1</guid></item><item><title>TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification</title><link>http://arxiv.org/abs/2312.14149v1</link><description>The crux of learning vision-language models is to extract semanticallyaligned information from visual and linguistic data. Existing attempts usuallyface the problem of coarse alignment, \textit{e.g.}, the vision encoderstruggles in localizing an attribute-specified object. In this work, we proposean embarrassingly simple approach to better align image and text features withno need of additional data formats other than image-text pairs. Concretely,given an image and its paired text, we manage to parse objects (\textit{e.g.},cat) and attributes (\textit{e.g.}, black) from the description, which arehighly likely to exist in the image. It is noteworthy that the parsing pipelineis fully automatic and thus enjoys good scalability. With these parsedsemantics as supervision signals, we can complement the commonly usedimage-text contrastive loss with the multi-tag classification loss. Extensiveexperimental results on a broad suite of semantic segmentation datasetssubstantiate the average 3.65\% improvement of our framework over existingalternatives. Furthermore, the visualization results indicate that attributesupervision makes vision-language models accurately localizeattribute-specified objects. Project page can be found athttps://qinying-liu.github.io/Tag-Align/</description><author>Qinying Liu, Kecheng Zheng, Wu Wei, Zhan Tong, Yu Liu, Wei Chen, Zilei Wang, Yujun Shen</author><pubDate>Thu, 21 Dec 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14149v1</guid></item><item><title>Quantum Algorithms for the Pathwise Lasso</title><link>http://arxiv.org/abs/2312.14141v1</link><description>We present a novel quantum high-dimensional linear regression algorithm withan $\ell_1$-penalty based on the classical LARS (Least Angle Regression)pathwise algorithm. Similarly to available classical numerical algorithms forLasso, our quantum algorithm provides the full regularisation path as thepenalty term varies, but quadratically faster per iteration under specificconditions. A quadratic speedup on the number of features/predictors $d$ ispossible by using the simple quantum minimum-finding subroutine from D\"urr andHoyer (arXiv'96) in order to obtain the joining time at each iteration. We thenimprove upon this simple quantum algorithm and obtain a quadratic speedup bothin the number of features $d$ and the number of observations $n$ by using therecent approximate quantum minimum-finding subroutine from Chen and de Wolf(ICALP'23). As one of our main contributions, we construct a quantum unitarybased on quantum amplitude estimation to approximately compute the joiningtimes to be searched over by the approximate quantum minimum finding. Since thejoining times are no longer exactly computed, it is no longer clear that theresulting approximate quantum algorithm obtains a good solution. As our secondmain contribution, we prove, via an approximate version of the KKT conditionsand a duality gap, that the LARS algorithm (and therefore our quantumalgorithm) is robust to errors. This means that it still outputs a path thatminimises the Lasso cost function up to a small error if the joining times areonly approximately computed. Finally, in the model where the observations aregenerated by an underlying linear model with an unknown coefficient vector, weprove bounds on the difference between the unknown coefficient vector and theapproximate Lasso solution, which generalises known results about convergencerates in classical statistical learning theory analysis.</description><author>João F. Doriguello, Debbie Lim, Chi Seng Pun, Patrick Rebentrost, Tushar Vaidya</author><pubDate>Thu, 21 Dec 2023 18:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14141v1</guid></item><item><title>HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs</title><link>http://arxiv.org/abs/2312.14140v1</link><description>Current advances in human head modeling allow to generate plausible-looking3D head models via neural representations. Nevertheless, constructing completehigh-fidelity head models with explicitly controlled animation remains anissue. Furthermore, completing the head geometry based on a partialobservation, e.g. coming from a depth sensor, while preserving details is oftenproblematic for the existing methods. We introduce a generative model fordetailed 3D head meshes on top of an articulated 3DMM which allows explicitanimation and high-detail preservation at the same time. Our method is trainedin two stages. First, we register a parametric head model with vertexdisplacements to each mesh of the recently introduced NPHM dataset of accurate3D head scans. The estimated displacements are baked into a hand-crafted UVlayout. Second, we train a StyleGAN model in order to generalize over the UVmaps of displacements. The decomposition of the parametric model andhigh-quality vertex displacements allows us to animate the model and modify itsemantically. We demonstrate the results of unconditional generation andfitting to the full or partial observation. The project page is available athttps://seva100.github.io/headcraft.</description><author>Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner</author><pubDate>Thu, 21 Dec 2023 18:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14140v1</guid></item><item><title>Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach</title><link>http://arxiv.org/abs/2312.14138v1</link><description>Weakly-supervised temporal action localization aims to localize actioninstances in videos with only video-level action labels. Existing methodsmainly embrace a localization-by-classification pipeline that optimizes thesnippet-level prediction with a video classification loss. However, thisformulation suffers from the discrepancy between classification and detection,resulting in inaccurate separation of foreground and background (F\&amp;B)snippets. To alleviate this problem, we propose to explore the underlyingstructure among the snippets by resorting to unsupervised snippet clustering,rather than heavily relying on the video classification loss. Specifically, wepropose a novel clustering-based F\&amp;B separation algorithm. It comprises twocore components: a snippet clustering component that groups the snippets intomultiple latent clusters and a cluster classification component that furtherclassifies the cluster as foreground or background. As there are noground-truth labels to train these two components, we introduce a unifiedself-labeling mechanism based on optimal transport to produce high-qualitypseudo-labels that match several plausible prior distributions. This ensuresthat the cluster assignments of the snippets can be accurately associated withtheir F\&amp;B labels, thereby boosting the F\&amp;B separation. We evaluate our methodon three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achievespromising performance on all three benchmarks while being significantly morelightweight than previous methods. Code is available athttps://github.com/Qinying-Liu/CASE</description><author>Qinying Liu, Zilei Wang, Shenghai Rong, Junjie Li, Yixin Zhang</author><pubDate>Thu, 21 Dec 2023 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14138v1</guid></item><item><title>Fast kernel half-space depth for data with non-convex supports</title><link>http://arxiv.org/abs/2312.14136v1</link><description>Data depth is a statistical function that generalizes order and quantiles tothe multivariate setting and beyond, with applications spanning overdescriptive and visual statistics, anomaly detection, testing, etc. Thecelebrated halfspace depth exploits data geometry via an optimization programto deliver properties of invariances, robustness, and non-parametricity.Nevertheless, it implicitly assumes convex data supports and requiresexponential computational cost. To tackle distribution's multimodality, weextend the halfspace depth in a Reproducing Kernel Hilbert Space (RKHS). Weshow that the obtained depth is intuitive and establish its consistency withprovable concentration bounds that allow for homogeneity testing. The proposeddepth can be computed using manifold gradient making faster than halfspacedepth by several orders of magnitude. The performance of our depth isdemonstrated through numerical simulations as well as applications such asanomaly detection on real data and homogeneity testing.</description><author>Arturo Castellanos, Pavlo Mozharovskyi, Florence d'Alché-Buc, Hicham Janati</author><pubDate>Thu, 21 Dec 2023 18:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14136v1</guid></item><item><title>$\textit{V}^*$: Guided Visual Search as a Core Mechanism in Multimodal LLMs</title><link>http://arxiv.org/abs/2312.14135v1</link><description>When we look around and perform complex tasks, how we see and selectivelyprocess what we see is crucial. However, the lack of this visual searchmechanism in current multimodal LLMs (MLLMs) hinders their ability to focus onimportant visual details, especially when handling high-resolution and visuallycrowded images. To address this, we introduce $\textit{V}^*$, an LLM-guidedvisual search mechanism that employs the world knowledge in LLMs for efficientvisual querying. When combined with an MLLM, this mechanism enhancescollaborative reasoning, contextual understanding, and precise targeting ofspecific visual elements. This integration results in a new MLLMmeta-architecture, named $\textbf{S}$how, s$\textbf{EA}$rch, andTel$\textbf{L}$ (SEAL). We further create $\textit{V}^*$Bench, a benchmarkspecifically designed to evaluate MLLMs in their ability to processhigh-resolution images and focus on visual details. Our study highlights thenecessity of incorporating visual search capabilities into multimodal systems.The code is available https://github.com/penghao-wu/vstar.</description><author>Penghao Wu, Saining Xie</author><pubDate>Thu, 21 Dec 2023 18:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14135v1</guid></item><item><title>Diffusion Reward: Learning Rewards via Conditional Video Diffusion</title><link>http://arxiv.org/abs/2312.14134v1</link><description>Learning rewards from expert videos offers an affordable and effectivesolution to specify the intended behaviors for reinforcement learning tasks. Inthis work, we propose Diffusion Reward, a novel framework that learns rewardsfrom expert videos via conditional video diffusion models for solving complexvisual RL problems. Our key insight is that lower generative diversity isobserved when conditioned on expert trajectories. Diffusion Reward isaccordingly formalized by the negative of conditional entropy that encouragesproductive exploration of expert-like behaviors. We show the efficacy of ourmethod over 10 robotic manipulation tasks from MetaWorld and Adroit with visualinput and sparse reward. Moreover, Diffusion Reward could even solve unseentasks successfully and effectively, largely surpassing baseline methods.Project page and code: https://diffusion-reward.github.io/.</description><author>Tao Huang, Guangqi Jiang, Yanjie Ze, Huazhe Xu</author><pubDate>Thu, 21 Dec 2023 18:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14134v1</guid></item><item><title>DUSt3R: Geometric 3D Vision Made Easy</title><link>http://arxiv.org/abs/2312.14132v1</link><description>Multi-view stereo reconstruction (MVS) in the wild requires to first estimatethe camera parameters e.g. intrinsic and extrinsic parameters. These areusually tedious and cumbersome to obtain, yet they are mandatory to triangulatecorresponding pixels in 3D space, which is the core of all best performing MVSalgorithms. In this work, we take an opposite stance and introduce DUSt3R, aradically novel paradigm for Dense and Unconstrained Stereo 3D Reconstructionof arbitrary image collections, i.e. operating without prior information aboutcamera calibration nor viewpoint poses. We cast the pairwise reconstructionproblem as a regression of pointmaps, relaxing the hard constraints of usualprojective camera models. We show that this formulation smoothly unifies themonocular and binocular reconstruction cases. In the case where more than twoimages are provided, we further propose a simple yet effective global alignmentstrategy that expresses all pairwise pointmaps in a common reference frame. Webase our network architecture on standard Transformer encoders and decoders,allowing us to leverage powerful pretrained models. Our formulation directlyprovides a 3D model of the scene as well as depth information, butinterestingly, we can seamlessly recover from it, pixel matches, relative andabsolute camera. Exhaustive experiments on all these tasks showcase that theproposed DUSt3R can unify various 3D vision tasks and set new SoTAs onmonocular/multi-view depth estimation as well as relative pose estimation. Insummary, DUSt3R makes many geometric 3D vision tasks easy.</description><author>Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud</author><pubDate>Thu, 21 Dec 2023 18:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14132v1</guid></item><item><title>Convex Clustering through MM: An Efficient Algorithm to Perform Hierarchical Clustering</title><link>http://arxiv.org/abs/2211.01877v2</link><description>Convex clustering is a modern method with both hierarchical and $k$-meansclustering characteristics. Although convex clustering can capture complexclustering structures hidden in data, the existing convex clustering algorithmsare not scalable to large data sets with sample sizes greater than severalthousands. Moreover, it is known that convex clustering sometimes fails toproduce a complete hierarchical clustering structure. This issue arises ifclusters split up or the minimum number of possible clusters is larger than thedesired number of clusters. In this paper, we propose convex clustering throughmajorization-minimization (CCMM) -- an iterative algorithm that uses clusterfusions and a highly efficient updating scheme derived using diagonalmajorization. Additionally, we explore different strategies to ensure that thehierarchical clustering structure terminates in a single cluster. With acurrent desktop computer, CCMM efficiently solves convex clustering problemsfeaturing over one million objects in seven-dimensional space, achieving asolution time of 51 seconds on average.</description><author>Daniel J. W. Touw, Patrick J. F. Groenen, Yoshikazu Terada</author><pubDate>Thu, 21 Dec 2023 18:51:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01877v2</guid></item><item><title>WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data</title><link>http://arxiv.org/abs/2312.14129v1</link><description>In the rapidly evolving healthcare industry, platforms now have access to notonly traditional medical records, but also diverse data sets encompassingvarious patient interactions, such as those from healthcare web portals. Toaddress this rich diversity of data, we introduce WellFactor: a method thatderives patient profiles by integrating information from these sources. Centralto our approach is the utilization of constrained low-rank approximation.WellFactor is optimized to handle the sparsity that is often inherent inhealthcare data. Moreover, by incorporating task-specific label information,our method refines the embedding results, offering a more informed perspectiveon patients. One important feature of WellFactor is its ability to computeembeddings for new, previously unobserved patient data instantaneously,eliminating the need to revisit the entire data set or recomputing theembedding. Comprehensive evaluations on real-world healthcare data demonstrateWellFactor's effectiveness. It produces better results compared to otherexisting methods in classification performance, yields meaningful clustering ofpatients, and delivers consistent results in patient similarity searches andpredictions.</description><author>Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</author><pubDate>Thu, 21 Dec 2023 18:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14129v1</guid></item><item><title>Entropic Open-set Active Learning</title><link>http://arxiv.org/abs/2312.14126v1</link><description>Active Learning (AL) aims to enhance the performance of deep models byselecting the most informative samples for annotation from a pool of unlabeleddata. Despite impressive performance in closed-set settings, most AL methodsfail in real-world scenarios where the unlabeled data contains unknowncategories. Recently, a few studies have attempted to tackle the AL problem forthe open-set setting. However, these methods focus more on selecting knownsamples and do not efficiently utilize unknown samples obtained during ALrounds. In this work, we propose an Entropic Open-set AL (EOAL) framework whichleverages both known and unknown distributions effectively to selectinformative samples during AL rounds. Specifically, our approach employs twodifferent entropy scores. One measures the uncertainty of a sample with respectto the known-class distributions. The other measures the uncertainty of thesample with respect to the unknown-class distributions. By utilizing these twoentropy scores we effectively separate the known and unknown samples from theunlabeled data resulting in better sampling. Through extensive experiments, weshow that the proposed method outperforms existing state-of-the-art methods onCIFAR-10, CIFAR-100, and TinyImageNet datasets. Code is available at\url{https://github.com/bardisafa/EOAL}.</description><author>Bardia Safaei, Vibashan VS, Celso M. de Melo, Vishal M. Patel</author><pubDate>Thu, 21 Dec 2023 18:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14126v1</guid></item><item><title>Cascade Speculative Drafting for Even Faster LLM Inference</title><link>http://arxiv.org/abs/2312.11462v2</link><description>Speculative decoding enhances the efficiency of large language models (LLMs)by leveraging a draft model to draft for a larger target model to review.However, drafting in speculative decoding involves slow autoregressivegeneration and generating tokens of different importance with the same timeallocation. These two inefficiencies lead to its suboptimal performance. Toaddress this issue, we introduce Cascade Speculative Drafting (CS. Drafting), anovel approach that employs two types of cascades. The Vertical Cascadeeliminates autoregressive generation from neural models. The Horizontal Cascadeconstitutes efficient time allocation in drafting with its optimality supportedby our theoretical analysis. Combining both cascades, our CS. Draftingalgorithm has achieved up to 72 percent additional speedup over speculativedecoding in our experiments while keeping the same output distribution.</description><author>Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, Kevin Chen-Chuan Chang</author><pubDate>Thu, 21 Dec 2023 18:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11462v2</guid></item><item><title>VideoPoet: A Large Language Model for Zero-Shot Video Generation</title><link>http://arxiv.org/abs/2312.14125v1</link><description>We present VideoPoet, a language model capable of synthesizing high-qualityvideo, with matching audio, from a large variety of conditioning signals.VideoPoet employs a decoder-only transformer architecture that processesmultimodal inputs -- including images, videos, text, and audio. The trainingprotocol follows that of Large Language Models (LLMs), consisting of twostages: pretraining and task-specific adaptation. During pretraining, VideoPoetincorporates a mixture of multimodal generative objectives within anautoregressive Transformer framework. The pretrained LLM serves as a foundationthat can be adapted for a range of video generation tasks. We present empiricalresults demonstrating the model's state-of-the-art capabilities in zero-shotvideo generation, specifically highlighting VideoPoet's ability to generatehigh-fidelity motions. Project page: http://sites.research.google/videopoet/</description><author>Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, Lu Jiang</author><pubDate>Thu, 21 Dec 2023 18:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14125v1</guid></item><item><title>Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation</title><link>http://arxiv.org/abs/2312.14124v1</link><description>Controllable generation of 3D assets is important for many practicalapplications like content creation in movies, games and engineering, as well asin AR/VR. Recently, diffusion models have shown remarkable results ingeneration quality of 3D objects. However, none of the existing models enabledisentangled generation to control the shape and appearance separately. For thefirst time, we present a suitable representation for 3D diffusion models toenable such disentanglement by introducing a hybrid point cloud and neuralradiance field approach. We model a diffusion process over point positionsjointly with a high-dimensional feature space for a local density and radiancedecoder. While the point positions represent the coarse shape of the object,the point features allow modeling the geometry and appearance details. Thisdisentanglement enables us to sample both independently and therefore tocontrol both separately. Our approach sets a new state of the art in generationcompared to previous disentanglement-capable methods by reduced FID scores of30-90% and is on-par with other non disentanglement-capable state-of-the artmethods.</description><author>Philipp Schröppel, Christopher Wewer, Jan Eric Lenssen, Eddy Ilg, Thomas Brox</author><pubDate>Thu, 21 Dec 2023 18:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14124v1</guid></item><item><title>Fast and Knowledge-Free Deep Learning for General Game Playing (Student Abstract)</title><link>http://arxiv.org/abs/2312.14121v1</link><description>We develop a method of adapting the AlphaZero model to General Game Playing(GGP) that focuses on faster model generation and requires less knowledge to beextracted from the game rules. The dataset generation uses MCTS playing insteadof self-play; only the value network is used, and attention layers replace theconvolutional ones. This allows us to abandon any assumptions about the actionspace and board topology. We implement the method within the Regular BoardgamesGGP system and show that we can build models outperforming the UCT baseline formost games efficiently.</description><author>Michał Maras, Michał Kępa, Jakub Kowalski, Marek Szykuła</author><pubDate>Thu, 21 Dec 2023 18:44:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14121v1</guid></item><item><title>Are Graph Neural Networks Optimal Approximation Algorithms?</title><link>http://arxiv.org/abs/2310.00526v4</link><description>In this work we design graph neural network architectures that can be used toobtain optimal approximation algorithms for a large class of combinatorialoptimization problems using powerful algorithmic tools from semidefiniteprogramming (SDP). Concretely, we prove that polynomial-sized message passingalgorithms can represent the most powerful polynomial time algorithms for MaxConstraint Satisfaction Problems assuming the Unique Games Conjecture. Weleverage this result to construct efficient graph neural network architectures,OptGNN, that obtain high-quality approximate solutions on landmarkcombinatorial optimization problems such as Max Cut and maximum independentset. Our approach achieves strong empirical results across a wide range ofreal-world and synthetic datasets against both neural baselines and classicalalgorithms. Finally, we take advantage of OptGNN's ability to capture convexrelaxations to design an algorithm for producing dual certificates ofoptimality (bounds on the optimal solution) from the learned embeddings ofOptGNN.</description><author>Morris Yau, Eric Lu, Nikolaos Karalias, Jessica Xu, Stefanie Jegelka</author><pubDate>Thu, 21 Dec 2023 18:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00526v4</guid></item><item><title>LingoQA: Video Question Answering for Autonomous Driving</title><link>http://arxiv.org/abs/2312.14115v1</link><description>Autonomous driving has long faced a challenge with public acceptance due tothe lack of explainability in the decision-making process. Videoquestion-answering (QA) in natural language provides the opportunity forbridging this gap. Nonetheless, evaluating the performance of Video QA modelshas proved particularly tough due to the absence of comprehensive benchmarks.To fill this gap, we introduce LingoQA, a benchmark specifically for autonomousdriving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearmancorrelation coefficient with human evaluations. We introduce a Video QA datasetof central London consisting of 419k samples that we release with the paper. Weestablish a baseline vision-language model and run extensive ablation studiesto understand its performance.</description><author>Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Oleg Sinavski</author><pubDate>Thu, 21 Dec 2023 18:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14115v1</guid></item><item><title>Learning Human-like Representations to Enable Learning Human Values</title><link>http://arxiv.org/abs/2312.14106v1</link><description>How can we build AI systems that are aligned with human values and objectivesin order to avoid causing harm or violating societal standards for acceptablebehavior? Making AI systems learn human-like representations of the world hasmany known benefits, including improving generalization, robustness to domainshifts, and few-shot learning performance, among others. We propose that thiskind of representational alignment between machine learning (ML) models andhumans is also a necessary condition for value alignment, where ML systemsconform to human values and societal norms. We focus on ethics as one aspect ofvalue alignment and train multiple ML agents (support vector regression andkernel regression) in a multi-armed bandit setting, where rewards are sampledfrom a distribution that reflects the morality of the chosen action. We thenstudy the relationship between each agent's degree of representationalalignment with humans and their performance when learning to take the mostethical actions.</description><author>Andrea Wynn, Ilia Sucholutsky, Thomas L. Griffiths</author><pubDate>Thu, 21 Dec 2023 18:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14106v1</guid></item><item><title>Proof Number Based Monte-Carlo Tree Search</title><link>http://arxiv.org/abs/2303.09449v2</link><description>This paper proposes a new game-search algorithm, PN-MCTS, which combinesMonte-Carlo Tree Search (MCTS) and Proof-Number Search (PNS). These twoalgorithms have been successfully applied for decision making in a range ofdomains. We define three areas where the additional knowledge provided by theproof and disproof numbers gathered in MCTS trees might be used: final moveselection, solving subtrees, and the UCB1 selection mechanism. We test allpossible combinations on different time settings, playing against vanilla UCTon several games: Lines of Action ($7$$\times$$7$ and $8$$\times$$8$ boardsizes), MiniShogi, Knightthrough, and Awari. Furthermore, we extend this newalgorithm to properly address games with draws, like Awari, by adding anadditional layer of PNS on top of the MCTS tree. The experiments show thatPN-MCTS confidently outperforms MCTS in all tested game domains, achieving winrates up to 96.2\% for Lines of Action.</description><author>Jakub Kowalski, Elliot Doe, Mark H. M. Winands, Daniel Górski, Dennis J. N. J. Soemers</author><pubDate>Thu, 21 Dec 2023 18:30:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09449v2</guid></item><item><title>Hierarchical Open-vocabulary Universal Image Segmentation</title><link>http://arxiv.org/abs/2307.00764v2</link><description>Open-vocabulary image segmentation aims to partition an image into semanticregions according to arbitrary text descriptions. However, complex visualscenes can be naturally decomposed into simpler parts and abstracted atmultiple levels of granularity, introducing inherent segmentation ambiguity.Unlike existing methods that typically sidestep this ambiguity and treat it asan external factor, our approach actively incorporates a hierarchicalrepresentation encompassing different semantic-levels into the learningprocess. We propose a decoupled text-image fusion mechanism and representationlearning modules for both "things" and "stuff". Additionally, we systematicallyexamine the differences that exist in the textual and visual features betweenthese types of categories. Our resulting model, named HIPIE, tacklesHIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within aunified framework. Benchmarked on over 40 datasets, e.g., ADE20K, COCO,Pascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves thestate-of-the-art results at various levels of image comprehension, includingsemantic-level (e.g., semantic segmentation), instance-level (e.g.,panoptic/referring segmentation and object detection), as well as part-level(e.g., part/subpart segmentation) tasks. Our code is released athttps://github.com/berkeley-hipie/HIPIE.</description><author>Xudong Wang, Shufan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, Trevor Darrell</author><pubDate>Thu, 21 Dec 2023 18:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00764v2</guid></item><item><title>Optimistic Policy Gradient in Multi-Player Markov Games with a Single Controller: Convergence Beyond the Minty Property</title><link>http://arxiv.org/abs/2312.12067v2</link><description>Policy gradient methods enjoy strong practical performance in numerous tasksin reinforcement learning. Their theoretical understanding in multiagentsettings, however, remains limited, especially beyond two-player competitiveand potential Markov games. In this paper, we develop a new framework tocharacterize optimistic policy gradient methods in multi-player Markov gameswith a single controller. Specifically, under the further assumption that thegame exhibits an equilibrium collapse, in that the marginals of coarsecorrelated equilibria (CCE) induce Nash equilibria (NE), we show convergence tostationary $\epsilon$-NE in $O(1/\epsilon^2)$ iterations, where $O(\cdot)$suppresses polynomial factors in the natural parameters of the game. Such anequilibrium collapse is well-known to manifest itself in two-player zero-sumMarkov games, but also occurs even in a class of multi-player Markov games withseparable interactions, as established by recent work. As a result, we bypassknown complexity barriers for computing stationary NE when either of ourassumptions fails. Our approach relies on a natural generalization of theclassical Minty property that we introduce, which we anticipate to have furtherapplications beyond Markov games.</description><author>Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, Tuomas Sandholm</author><pubDate>Thu, 21 Dec 2023 18:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12067v2</guid></item><item><title>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</title><link>http://arxiv.org/abs/2312.13016v2</link><description>We present DiffPortrait3D, a conditional diffusion model that is capable ofsynthesizing 3D-consistent photo-realistic novel views from as few as a singlein-the-wild portrait. Specifically, given a single RGB input, we aim tosynthesize plausible but consistent facial details rendered from novel cameraviews with retained both identity and facial expression. In lieu oftime-consuming optimization and fine-tuning, our zero-shot method generalizeswell to arbitrary face portraits with unposed camera views, extreme facialexpressions, and diverse artistic depictions. At its core, we leverage thegenerative prior of 2D diffusion models pre-trained on large-scale imagedatasets as our rendering backbone, while the denoising is guided withdisentangled attentive control of appearance and camera pose. To achieve this,we first inject the appearance context from the reference image into theself-attention layers of the frozen UNets. The rendering view is thenmanipulated with a novel conditional control module that interprets the camerapose by watching a condition image of a crossed subject from the same view.Furthermore, we insert a trainable cross-view attention module to enhance viewconsistency, which is further strengthened with a novel 3D-aware noisegeneration process during inference. We demonstrate state-of-the-art resultsboth qualitatively and quantitatively on our challenging in-the-wild andmulti-view benchmarks.</description><author>Yuming Gu, Hongyi Xu, You Xie, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, Linjie Luo</author><pubDate>Thu, 21 Dec 2023 18:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13016v2</guid></item><item><title>Image Captioners Are Scalable Vision Learners Too</title><link>http://arxiv.org/abs/2306.07915v5</link><description>Contrastive pretraining on image-text pairs from the web is one of the mostpopular large-scale pretraining strategies for vision backbones, especially inthe context of large multimodal models. At the same time, image captioning onthis type of data is commonly considered an inferior pretraining strategy. Inthis paper, we perform a fair comparison of these two pretraining strategies,carefully matching training data, compute, and model capacity. Using a standardencoder-decoder transformer, we find that captioning alone is surprisinglyeffective: on classification tasks, captioning produces vision encoderscompetitive with contrastively pretrained encoders, while surpassing them onvision &amp; language tasks. We further analyze the effect of the modelarchitecture and scale, as well as the pretraining data on the representationquality, and find that captioning exhibits the same or better scaling behavioralong these axes. Overall our results show that plain image captioning is amore powerful pretraining strategy than was previously believed.</description><author>Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, Lucas Beyer</author><pubDate>Thu, 21 Dec 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07915v5</guid></item><item><title>One-Line-of-Code Data Mollification Improves Optimization of Likelihood-based Generative Models</title><link>http://arxiv.org/abs/2305.18900v2</link><description>Generative Models (GMs) have attracted considerable attention due to theirtremendous success in various domains, such as computer vision where they arecapable to generate impressive realistic-looking images. Likelihood-based GMsare attractive due to the possibility to generate new data by a single modelevaluation. However, they typically achieve lower sample quality compared tostate-of-the-art score-based diffusion models (DMs). This paper provides asignificant step in the direction of addressing this limitation. The idea is toborrow one of the strengths of score-based DMs, which is the ability to performaccurate density estimation in low-density regions and to address manifoldoverfitting by means of data mollification. We connect data mollificationthrough the addition of Gaussian noise to Gaussian homotopy, which is awell-known technique to improve optimization. Data mollification can beimplemented by adding one line of code in the optimization loop, and wedemonstrate that this provides a boost in generation quality oflikelihood-based GMs, without computational overheads. We report results onimage data sets with popular likelihood-based GMs, including variants ofvariational autoencoders and normalizing flows, showing large improvements inFID score.</description><author>Ba-Hien Tran, Giulio Franzese, Pietro Michiardi, Maurizio Filippone</author><pubDate>Thu, 21 Dec 2023 18:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18900v2</guid></item><item><title>3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction</title><link>http://arxiv.org/abs/2310.14859v3</link><description>Predicting turn-taking in multiparty conversations has many practicalapplications in human-computer/robot interaction. However, the complexity ofhuman communication makes it a challenging task. Recent advances have shownthat synchronous multi-perspective egocentric data can significantly improveturn-taking prediction compared to asynchronous, single-perspectivetranscriptions. Building on this research, we propose a new multimodaltransformer-based architecture for predicting turn-taking in embodied,synchronized multi-perspective data. Our experimental results on the recentlyintroduced EgoCom dataset show a substantial performance improvement of up to14.01% on average compared to existing baselines and alternativetransformer-based approaches. The source code, and the pre-trained models ofour 3M-Transformer will be available upon acceptance.</description><author>Mehdi Fatan, Emanuele Mincato, Dimitra Pintzou, Mariella Dimiccoli</author><pubDate>Thu, 21 Dec 2023 18:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14859v3</guid></item><item><title>RetailSynth: Synthetic Data Generation for Retail AI Systems Evaluation</title><link>http://arxiv.org/abs/2312.14095v1</link><description>Significant research effort has been devoted in recent years to developingpersonalized pricing, promotions, and product recommendation algorithms thatcan leverage rich customer data to learn and earn. Systematic benchmarking andevaluation of these causal learning systems remains a critical challenge, dueto the lack of suitable datasets and simulation environments. In this work, wepropose a multi-stage model for simulating customer shopping behavior thatcaptures important sources of heterogeneity, including price sensitivity andpast experiences. We embedded this model into a working simulation environment-- RetailSynth. RetailSynth was carefully calibrated on publicly availablegrocery data to create realistic synthetic shopping transactions. Multiplepricing policies were implemented within the simulator and analyzed for impacton revenue, category penetration, and customer retention. Applied researcherscan use RetailSynth to validate causal demand models for multi-category retailand to incorporate realistic price sensitivity into emerging benchmarkingsuites for personalized pricing, promotions, and product recommendations.</description><author>Yu Xia, Ali Arian, Sriram Narayanamoorthy, Joshua Mabry</author><pubDate>Thu, 21 Dec 2023 18:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14095v1</guid></item><item><title>Unifying GANs and Score-Based Diffusion as Generative Particle Models</title><link>http://arxiv.org/abs/2305.16150v3</link><description>Particle-based deep generative models, such as gradient flows and score-baseddiffusion models, have recently gained traction thanks to their strikingperformance. Their principle of displacing particle distributions usingdifferential equations is conventionally seen as opposed to the previouslywidespread generative adversarial networks (GANs), which involve training apushforward generator network. In this paper we challenge this interpretation,and propose a novel framework that unifies particle and adversarial generativemodels by framing generator training as a generalization of particle models.This suggests that a generator is an optional addition to any such generativemodel. Consequently, integrating a generator into a score-based diffusion modeland training a GAN without a generator naturally emerge from our framework. Weempirically test the viability of these original models as proofs of conceptsof potential applications of our framework.</description><author>Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy</author><pubDate>Thu, 21 Dec 2023 18:16:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16150v3</guid></item><item><title>Physics-Informed Neural Network Lyapunov Functions: PDE Characterization, Learning, and Verification</title><link>http://arxiv.org/abs/2312.09131v3</link><description>We provide a systematic investigation of using physics-informed neuralnetworks to compute Lyapunov functions. We encode Lyapunov conditions as apartial differential equation (PDE) and use this for training neural networkLyapunov functions. We analyze the analytical properties of the solutions tothe Lyapunov and Zubov PDEs. In particular, we show that employing the Zubovequation in training neural Lyapunov functions can lead to approximate regionsof attraction close to the true domain of attraction. We also examineapproximation errors and the convergence of neural approximations to the uniquesolution of Zubov's equation. We then provide sufficient conditions for thelearned neural Lyapunov functions that can be readily verified bysatisfiability modulo theories (SMT) solvers, enabling formal verification ofboth local stability analysis and region-of-attraction estimates in the large.Through a number of nonlinear examples, ranging from low to high dimensions, wedemonstrate that the proposed framework can outperform traditionalsums-of-squares (SOS) Lyapunov functions obtained using semidefiniteprogramming (SDP).</description><author>Jun Liu, Yiming Meng, Maxwell Fitzsimmons, Ruikun Zhou</author><pubDate>Thu, 21 Dec 2023 18:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09131v3</guid></item><item><title>HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models</title><link>http://arxiv.org/abs/2312.14091v1</link><description>Recent progress in text-guided image inpainting, based on the unprecedentedsuccess of text-to-image diffusion models, has led to exceptionally realisticand visually plausible results. However, there is still significant potentialfor improvement in current text-to-image inpainting models, particularly inbetter aligning the inpainted area with user prompts and performinghigh-resolution inpainting. Therefore, in this paper we introduce HD-Painter, acompletely training-free approach that accurately follows to prompts andcoherently scales to high-resolution image inpainting. To this end, we designthe Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attentionscores by prompt information and resulting in better text alignmentgenerations. To further improve the prompt coherence we introduce theReweighting Attention Score Guidance (RASG) mechanism seamlessly integrating apost-hoc sampling strategy into general form of DDIM to preventout-of-distribution latent shifts. Moreover, HD-Painter allows extension tolarger scales by introducing a specialized super-resolution techniquecustomized for inpainting, enabling the completion of missing regions in imagesof up to 2K resolution. Our experiments demonstrate that HD-Painter surpassesexisting state-of-the-art approaches qualitatively and quantitatively,achieving an impressive generation accuracy improvement of 61.4% vs 51.9%. Wewill make the codes publicly available at:https://github.com/Picsart-AI-Research/HD-Painter</description><author>Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi</author><pubDate>Thu, 21 Dec 2023 18:09:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14091v1</guid></item><item><title>Learned reconstruction methods for inverse problems: sample error estimates</title><link>http://arxiv.org/abs/2312.14078v1</link><description>Learning-based and data-driven techniques have recently become a subject ofprimary interest in the field of reconstruction and regularization of inverseproblems. Besides the development of novel methods, yielding excellent resultsin several applications, their theoretical investigation has attracted growinginterest, e.g., on the topics of reliability, stability, and interpretability.In this work, a general framework is described, allowing us to interpret manyof these techniques in the context of statistical learning. This is notintended to provide a complete survey of existing methods, but rather to putthem in a working perspective, which naturally allows their theoreticaltreatment. The main goal of this dissertation is thereby to address thegeneralization properties of learned reconstruction methods, and specificallyto perform their sample error analysis. This task, well-developed instatistical learning, consists in estimating the dependence of the learnedoperators with respect to the data employed for their training. A rathergeneral strategy is proposed, whose assumptions are met for a large class ofinverse problems and learned methods, as depicted via a selection of examples.</description><author>Luca Ratti</author><pubDate>Thu, 21 Dec 2023 17:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14078v1</guid></item><item><title>LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding</title><link>http://arxiv.org/abs/2312.14074v1</link><description>Recently, Large Language Models (LLMs) and Multimodal Large Language Models(MLLMs) have shown promise in instruction following and 2D image understanding.While these models are powerful, they have not yet been developed to comprehendthe more challenging 3D physical scenes, especially when it comes to the sparseoutdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes rawLiDAR data as input and harnesses the remarkable reasoning capabilities of LLMsto gain a comprehensive understanding of outdoor 3D scenes. The central insightof our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as alanguage modeling problem, encompassing tasks such as 3D captioning, 3Dgrounding, 3D question answering, etc. Specifically, due to the scarcity of 3DLiDAR-text pairing data, we introduce a three-stage training strategy andgenerate relevant datasets, progressively aligning the 3D modality with thelanguage embedding space of LLM. Furthermore, we design a View-AwareTransformer (VAT) to connect the 3D encoder with the LLM, which effectivelybridges the modality gap and enhances the LLM's spatial orientationcomprehension of visual features. Our experiments show that LiDAR-LLM possessesfavorable capabilities to comprehend various instructions regarding 3D scenesand engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the3D captioning task and achieves a 63.1\% classification accuracy and a 14.3\%BEV mIoU on the 3D grounding task. Web page:https://sites.google.com/view/lidar-llm</description><author>Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, Shanghang Zhang</author><pubDate>Thu, 21 Dec 2023 17:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14074v1</guid></item><item><title>EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models</title><link>http://arxiv.org/abs/2312.14069v1</link><description>We introduce EmphAssess, a prosodic benchmark designed to evaluate thecapability of speech-to-speech models to encode and reproduce prosodicemphasis. We apply this to two tasks: speech resynthesis and speech-to-speechtranslation. In both cases, the benchmark evaluates the ability of the model toencode emphasis in the speech input and accurately reproduce it in the output,potentially across a change of speaker and language. As part of the evaluationpipeline, we introduce EmphaClass, a new model that classifies emphasis at theframe or word level.</description><author>Maureen de Seyssel, Antony D'Avirro, Adina Williams, Emmanuel Dupoux</author><pubDate>Thu, 21 Dec 2023 17:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14069v1</guid></item><item><title>Upper Bounding Barlow Twins: A Novel Filter for Multi-Relational Clustering</title><link>http://arxiv.org/abs/2312.14066v1</link><description>Multi-relational clustering is a challenging task due to the fact thatdiverse semantic information conveyed in multi-layer graphs is difficult toextract and fuse. Recent methods integrate topology structure and nodeattribute information through graph filtering. However, they often use alow-pass filter without fully considering the correlation among multiplegraphs. To overcome this drawback, we propose to learn a graph filter motivatedby the theoretical analysis of Barlow Twins. We find that input with a negativesemi-definite inner product provides a lower bound for Barlow Twins loss, whichprevents it from reaching a better solution. We thus learn a filter that yieldsan upper bound for Barlow Twins. Afterward, we design a simple clusteringarchitecture and demonstrate its state-of-the-art performance on four benchmarkdatasets.</description><author>Xiaowei Qian, Bingheng Li, Zhao Kang</author><pubDate>Thu, 21 Dec 2023 17:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14066v1</guid></item><item><title>ThoraX-PriorNet: A Novel Attention-Based Architecture Using Anatomical Prior Probability Maps for Thoracic Disease Classification</title><link>http://arxiv.org/abs/2210.02998v3</link><description>Objective: Computer-aided disease diagnosis and prognosis based on medicalimages is a rapidly emerging field. Many Convolutional Neural Network (CNN)architectures have been developed by researchers for disease classification andlocalization from chest X-ray images. It is known that different thoracicdisease lesions are more likely to occur in specific anatomical regionscompared to others. This article aims to incorporate this disease andregion-dependent prior probability distribution within a deep learningframework. Methods: We present the ThoraX-PriorNet, a novel attention-based CNNmodel for thoracic disease classification. We first estimate adisease-dependent spatial probability, i.e., an anatomical prior, thatindicates the probability of occurrence of a disease in a specific region in achest X-ray image. Next, we develop a novel attention-based classificationmodel that combines information from the estimated anatomical prior andautomatically extracted chest region of interest (ROI) masks to provideattention to the feature maps generated from a deep convolution network. Unlikeprevious works that utilize various self-attention mechanisms, the proposedmethod leverages the extracted chest ROI masks along with the probabilisticanatomical prior information, which selects the region of interest fordifferent diseases to provide attention. Results: The proposed method showssuperior performance in disease classification on the NIH ChestX-ray14 datasetcompared to existing state-of-the-art methods while reaching an area under theROC curve (%AUC) of 84.67. Regarding disease localization, the anatomy priorattention method shows competitive performance compared to state-of-the-artmethods, achieving an accuracy of 0.80, 0.63, 0.49, 0.33, 0.28, 0.21, and 0.04with an Intersection over Union (IoU) threshold of 0.1, 0.2, 0.3, 0.4, 0.5,0.6, and 0.7, respectively.</description><author>Md. Iqbal Hossain, Mohammad Zunaed, Md. Kawsar Ahmed, S. M. Jawwad Hossain, Anwarul Hasan, Taufiq Hasan</author><pubDate>Thu, 21 Dec 2023 17:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.02998v3</guid></item><item><title>Weighted least-squares approximation with determinantal point processes and generalized volume sampling</title><link>http://arxiv.org/abs/2312.14057v1</link><description>We consider the problem of approximating a function from $L^2$ by an elementof a given $m$-dimensional space $V_m$, associated with some feature map$\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$.After recalling some results on optimal weighted least-squares usingindependent and identically distributed points, we consider weightedleast-squares using projection determinantal point processes (DPP) or volumesampling. These distributions introduce dependence between the points thatpromotes diversity in the selected features $\varphi(x_i)$. We first provide ageneralized version of volume-rescaled sampling yielding quasi-optimalityresults in expectation with a number of samples $n = O(m\log(m))$, that meansthat the expected $L^2$ error is bounded by a constant times the bestapproximation error in $L^2$. Also, further assuming that the function is insome normed vector space $H$ continuously embedded in $L^2$, we further provethat the approximation is almost surely bounded by the best approximation errormeasured in the $H$-norm. This includes the cases of functions from $L^\infty$or reproducing kernel Hilbert spaces. Finally, we present an alternativestrategy consisting in using independent repetitions of projection DPP (orvolume sampling), yielding similar error bounds as with i.i.d. or volumesampling, but in practice with a much lower number of samples. Numericalexperiments illustrate the performance of the different strategies.</description><author>Anthony Nouy, Bertrand Michel</author><pubDate>Thu, 21 Dec 2023 17:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14057v1</guid></item><item><title>A Strong Baseline for Temporal Video-Text Alignment</title><link>http://arxiv.org/abs/2312.14055v1</link><description>In this paper, we consider the problem of temporally aligning the video andtexts from instructional videos, specifically, given a long-term video, andassociated text sentences, our goal is to determine their correspondingtimestamps in the video. To this end, we establish a simple, yet strong modelthat adopts a Transformer-based architecture with all texts as queries,iteratively attending to the visual features, to infer the optimal timestamp.We conduct thorough experiments to investigate: (i) the effect of upgrading ASRsystems to reduce errors from speech recognition, (ii) the effect of variousvisual-textual backbones, ranging from CLIP to S3D, to the more recentInternVideo, (iii) the effect of transforming noisy ASR transcripts intodescriptive steps by prompting a large language model (LLM), to summarize thecore activities within the ASR transcript as a new training dataset. As aresult, our proposed simple model demonstrates superior performance on bothnarration alignment and procedural step grounding tasks, surpassing existingstate-of-the-art methods by a significant margin on three public benchmarks,namely, 9.3% on HT-Step, 3.4% on HTM-Align and 4.7% on CrossTask. We believethe proposed model and dataset with descriptive steps can be treated as astrong baseline for future research in temporal video-text alignment. Allcodes, models, and the resulting dataset will be publicly released to theresearch community.</description><author>Zeqian Li, Qirui Chen, Tengda Han, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Thu, 21 Dec 2023 17:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14055v1</guid></item><item><title>Dual Attention U-Net with Feature Infusion: Pushing the Boundaries of Multiclass Defect Segmentation</title><link>http://arxiv.org/abs/2312.14053v1</link><description>The proposed architecture, Dual Attentive U-Net with Feature Infusion (DAU-FINet), addresses challenges in semantic segmentation, particularly on multiclassimbalanced datasets with limited samples. DAU-FI Net integrates multiscalespatial-channel attention mechanisms and feature injection to enhance precisionin object localization. The core employs a multiscale depth-separableconvolution block, capturing localized patterns across scales. This block iscomplemented by a spatial-channel squeeze and excitation (scSE) attention unit,modeling inter-dependencies between channels and spatial regions in featuremaps. Additionally, additive attention gates refine segmentation by connectingencoder-decoder pathways. To augment the model, engineered features using Gabor filters for texturalanalysis, Sobel and Canny filters for edge detection are injected guided bysemantic masks to expand the feature space strategically. Comprehensiveexperiments on a challenging sewer pipe and culvert defect dataset and abenchmark dataset validate DAU-FI Net's capabilities. Ablation studieshighlight incremental benefits from attention blocks and feature injection.DAU-FI Net achieves state-of-the-art mean Intersection over Union (IoU) of95.6% and 98.8% on the defect test set and benchmark respectively, surpassingprior methods by 8.9% and 12.6%, respectively. Ablation studies highlightincremental benefits from attention blocks and feature injection. The proposedarchitecture provides a robust solution, advancing semantic segmentation formulticlass problems with limited training data. Our sewer-culvert defectsdataset, featuring pixel-level annotations, opens avenues for further researchin this crucial domain. Overall, this work delivers key innovations inarchitecture, attention, and feature engineering to elevate semanticsegmentation efficacy.</description><author>Rasha Alshawi, Md Tamjidul Hoque, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Kendall Niles, Ken Prathak, Joe Tom, Jordan Klein, Murtada Mousa, Johny Javier Lopez</author><pubDate>Thu, 21 Dec 2023 17:23:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14053v1</guid></item><item><title>Machine learning and domain decomposition methods -- a survey</title><link>http://arxiv.org/abs/2312.14050v1</link><description>Hybrid algorithms, which combine black-box machine learning methods withexperience from traditional numerical methods and domain expertise from diverseapplication areas, are progressively gaining importance in scientific machinelearning and various industrial domains, especially in computational scienceand engineering. In the present survey, several promising avenues of researchwill be examined which focus on the combination of machine learning (ML) anddomain decomposition methods (DDMs). The aim of this survey is to provide anoverview of existing work within this field and to structure it into domaindecomposition for machine learning and machine learning-enhanced domaindecomposition, including: domain decomposition for classical machine learning,domain decomposition to accelerate the training of physics-aware neuralnetworks, machine learning to enhance the convergence properties orcomputational efficiency of DDMs, and machine learning as a discretizationmethod in a DDM for the solution of PDEs. In each of these fields, we summarizeexisting work and key advances within a common framework and, finally, disussongoing challenges and opportunities for future research.</description><author>Axel Klawonn, Martin Lanser, Janine Weber</author><pubDate>Thu, 21 Dec 2023 17:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14050v1</guid></item><item><title>Restricted Bernoulli Matrix Factorization: Balancing the trade-off between prediction accuracy and coverage in classification based collaborative filtering</title><link>http://arxiv.org/abs/2210.10619v2</link><description>Reliability measures associated with the prediction of the machine learningmodels are critical to strengthening user confidence in artificialintelligence. Therefore, those models that are able to provide not onlypredictions, but also reliability, enjoy greater popularity. In the field ofrecommender systems, reliability is crucial, since users tend to prefer thoserecommendations that are sure to interest them, that is, high predictions withhigh reliabilities. In this paper, we propose Restricted Bernoulli MatrixFactorization (ResBeMF), a new algorithm aimed at enhancing the performance ofclassification-based collaborative filtering. The proposed model has beencompared to other existing solutions in the literature in terms of predictionquality (Mean Absolute Error and accuracy scores), prediction quantity(coverage score) and recommendation quality (Mean Average Precision score). Theexperimental results demonstrate that the proposed model provides a goodbalance in terms of the quality measures used compared to other recommendationmodels.</description><author>Ángel González-Prieto, Abraham Gutiérrez, Fernando Ortega, Raúl Lara-Cabrera</author><pubDate>Thu, 21 Dec 2023 17:18:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10619v2</guid></item><item><title>Estimating Generic 3D Room Structures from 2D Annotations</title><link>http://arxiv.org/abs/2306.09077v2</link><description>Indoor rooms are among the most common use cases in 3D scene understanding.Current state-of-the-art methods for this task are driven by large annotateddatasets. Room layouts are especially important, consisting of structuralelements in 3D, such as wall, floor, and ceiling. However, they are difficultto annotate, especially on pure RGB video. We propose a novel method to producegeneric 3D room layouts just from 2D segmentation masks, which are easy toannotate for humans. Based on these 2D annotations, we automaticallyreconstruct 3D plane equations for the structural elements and their spatialextent in the scene, and connect adjacent elements at the appropriate contactedges. We annotate and publicly release 2246 3D room layouts on theRealEstate10k dataset, containing YouTube videos. We demonstrate the highquality of these 3D layouts annotations with extensive experiments.</description><author>Denys Rozumnyi, Stefan Popov, Kevis-Kokitsi Maninis, Matthias Nießner, Vittorio Ferrari</author><pubDate>Thu, 21 Dec 2023 17:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09077v2</guid></item><item><title>Neural Contextual Bandits for Personalized Recommendation</title><link>http://arxiv.org/abs/2312.14037v1</link><description>In the dynamic landscape of online businesses, recommender systems arepivotal in enhancing user experiences. While traditional approaches have reliedon static supervised learning, the quest for adaptive, user-centricrecommendations has led to the emergence of the formulation of contextualbandits. This tutorial investigates the contextual bandits as a powerfulframework for personalized recommendations. We delve into the challenges,advanced algorithms and theories, collaborative strategies, and open challengesand future prospects within this field. Different from existing relatedtutorials, (1) we focus on the exploration perspective of contextual bandits toalleviate the ``Matthew Effect'' in the recommender systems, i.e., the rich getricher and the poor get poorer, concerning the popularity of items; (2) inaddition to the conventional linear contextual bandits, we will also dedicatedto neural contextual bandits which have emerged as an important branch inrecent years, to investigate how neural networks benefit contextual bandits forpersonalized recommendation both empirically and theoretically; (3) we willcover the latest topic, collaborative neural contextual bandits, to incorporateboth user heterogeneity and user correlations customized for recommendersystem; (4) we will provide and discuss the new emerging challenges and openquestions for neural contextual bandits with applications in the personalizedrecommendation, especially for large neural models.</description><author>Yikun Ban, Yunzhe Qi, Jingrui He</author><pubDate>Thu, 21 Dec 2023 17:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14037v1</guid></item><item><title>T-Eval: Evaluating the Tool Utilization Capability Step by Step</title><link>http://arxiv.org/abs/2312.14033v1</link><description>Large language models (LLM) have achieved remarkable performance on variousNLP tasks and are augmented by tools for broader applications. Yet, how toevaluate and analyze the tool-utilization capability of LLMs is stillunder-explored. In contrast to previous works that evaluate modelsholistically, we comprehensively decompose the tool utilization into multiplesub-processes, including instruction following, planning, reasoning, retrieval,understanding, and review. Based on that, we further introduce \shortname~toevaluate the tool utilization capability step by step. \shortname~disentanglesthe tool utilization evaluation into several sub-domains along modelcapabilities, facilitating the inner understanding of both holistic andisolated competency of LLMs. We conduct extensive experiments on \shortname~andin-depth analysis of various LLMs. \shortname~ not only exhibits consistencywith the outcome-oriented evaluation but also provides a more fine-grainedanalysis of the capabilities of LLMs, providing a new perspective in LLMevaluation on tool-utilization ability. The benchmark will be available at\href{https://github.com/open-compass/T-Eval}{https://github.com/open-compass/T-Eval}.</description><author>Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, Feng Zhao</author><pubDate>Thu, 21 Dec 2023 17:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14033v1</guid></item><item><title>ChessGPT: Bridging Policy Learning and Language Modeling</title><link>http://arxiv.org/abs/2306.09200v2</link><description>When solving decision-making tasks, humans typically depend on informationfrom two key sources: (1) Historical policy data, which provides interactionreplay from the environment, and (2) Analytical insights in natural languageform, exposing the invaluable thought process or strategic considerations.Despite this, the majority of preceding research focuses on only one source:they either use historical replay exclusively to directly learn policy or valuefunctions, or engaged in language model training utilizing mere languagecorpus. In this paper, we argue that a powerful autonomous agent should coverboth sources. Thus, we propose ChessGPT, a GPT model bridging policy learningand language modeling by integrating data from these two sources in Chessgames. Specifically, we build a large-scale game and language dataset relatedto chess. Leveraging the dataset, we showcase two model examples ChessCLIP andChessGPT, integrating policy learning and language modeling. Finally, wepropose a full evaluation framework for evaluating language model's chessability. Experimental results validate our model and dataset's effectiveness.We open source our code, model, and dataset athttps://github.com/waterhorse1/ChessGPT.</description><author>Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang</author><pubDate>Thu, 21 Dec 2023 16:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09200v2</guid></item><item><title>AdamMCMC: Combining Metropolis Adjusted Langevin with Momentum-based Optimization</title><link>http://arxiv.org/abs/2312.14027v1</link><description>Uncertainty estimation is a key issue when considering the application ofdeep neural network methods in science and engineering. In this work, weintroduce a novel algorithm that quantifies epistemic uncertainty via MonteCarlo sampling from a tempered posterior distribution. It combines the wellestablished Metropolis Adjusted Langevin Algorithm (MALA) with momentum-basedoptimization using Adam and leverages a prolate proposal distribution, toefficiently draw from the posterior. We prove that the constructed chain admitsthe Gibbs posterior as an invariant distribution and converges to this Gibbsposterior in total variation distance. Numerical evaluations are postponed to afirst revision.</description><author>Sebastian Bieringer, Gregor Kasieczka, Maximilian F. Steffen, Mathias Trabs</author><pubDate>Thu, 21 Dec 2023 16:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14027v1</guid></item><item><title>Geometric Awareness in Neural Fields for 3D Human Registration</title><link>http://arxiv.org/abs/2312.14024v1</link><description>Aligning a template to 3D human point clouds is a long-standing problemcrucial for tasks like animation, reconstruction, and enabling supervisedlearning pipelines. Recent data-driven methods leverage predicted surfacecorrespondences; however, they are not robust to varied poses or distributions.In contrast, industrial solutions often rely on expensive manual annotations ormulti-view capturing systems. Recently, neural fields have shown promisingresults, but their purely data-driven nature lacks geometric awareness, oftenresulting in a trivial misalignment of the template registration. In this work,we propose two solutions: LoVD, a novel neural field model that predicts thedirection towards the localized SMPL vertices on the target surface; and INT,the first self-supervised task dedicated to neural fields that, at test time,refines the backbone, exploiting the target geometry. We combine them intoINLoVD, a robust 3D Human body registration pipeline trained on a large MoCapdataset. INLoVD is efficient (takes less than a minute), solidly achieves thestate of the art over public benchmarks, and provides unprecedentedgeneralization on out-of-distribution data. We will release code andcheckpoints in \url{url}.</description><author>Riccardo Marin, Enric Corona, Gerard Pons-Moll</author><pubDate>Thu, 21 Dec 2023 16:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14024v1</guid></item><item><title>Leveraging Visual Supervision for Array-based Active Speaker Detection and Localization</title><link>http://arxiv.org/abs/2312.14021v1</link><description>Conventional audio-visual approaches for active speaker detection (ASD)typically rely on visually pre-extracted face tracks and the correspondingsingle-channel audio to find the speaker in a video. Therefore, they tend tofail every time the face of the speaker is not visible. We demonstrate that asimple audio convolutional recurrent neural network (CRNN) trained with spatialinput features extracted from multichannel audio can perform simultaneoushorizontal active speaker detection and localization (ASDL), independently ofthe visual modality. To address the time and cost of generating ground truthlabels to train such a system, we propose a new self-supervised trainingpipeline that embraces a ``student-teacher'' learning approach. A conventionalpre-trained active speaker detector is adopted as a ``teacher'' network toprovide the position of the speakers as pseudo-labels. The multichannel audio``student'' network is trained to generate the same results. At inference, thestudent network can generalize and locate also the occluded speakers that theteacher network is not able to detect visually, yielding considerableimprovements in recall rate. Experiments on the TragicTalkers dataset show thatan audio network trained with the proposed self-supervised learning approachcan exceed the performance of the typical audio-visual methods and produceresults competitive with the costly conventional supervised training. Wedemonstrate that improvements can be achieved when minimal manual supervisionis introduced in the learning pipeline. Further gains may be sought with largertraining sets and integrating vision with the multichannel audio system.</description><author>Davide Berghi, Philip J. B. Jackson</author><pubDate>Thu, 21 Dec 2023 16:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14021v1</guid></item><item><title>BANSpEmo: A Bangla Emotional Speech Recognition Dataset</title><link>http://arxiv.org/abs/2312.14020v1</link><description>In the field of audio and speech analysis, the ability to identify emotionsfrom acoustic signals is essential. Human-computer interaction (HCI) andbehavioural analysis are only a few of the many areas where the capacity todistinguish emotions from speech signals has an extensive range ofapplications. Here, we are introducing BanSpEmo, a corpus of emotional speechthat only consists of audio recordings and has been created specifically forthe Bangla language. This corpus contains 792 audio recordings over a durationof more than 1 hour and 23 minutes. 22 native speakers took part in therecording of two sets of sentences that represent the six desired emotions. Thedata set consists of 12 Bangla sentences which are uttered in 6 emotions asDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also genderbalanced. Ten individuals who either have experience in related field or haveacting experience took part in the assessment of this corpus. It has a balancednumber of audio recordings in each emotion class. BanSpEmo can be considered asa useful resource to promote emotion and speech recognition research andrelated applications in the Bangla language. The dataset can be found here:https://data.mendeley.com/datasets/rdwn4bs5ky and might be employed foracademic research.</description><author>Md Gulzar Hussain, Mahmuda Rahman, Babe Sultana, Ye Shiren</author><pubDate>Thu, 21 Dec 2023 16:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14020v1</guid></item><item><title>Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers</title><link>http://arxiv.org/abs/2307.14367v2</link><description>The complex nature of big biological systems pushed some scientists toclassify its understanding under the inconceivable missions. Different leveledchallenges complicated this task, one of is the prediction of a protein'sfunction. In recent years, significant progress has been made in this fieldthrough the development of various machine learning approaches. However, mostexisting methods formulate the task as a multi-classification problem, i.eassigning predefined labels to proteins. In this work, we propose a novelapproach, \textbf{Prot2Text}, which predicts a protein function's in a freetext style, moving beyond the conventional binary or categoricalclassifications. By combining Graph Neural Networks(GNNs) and Large LanguageModels(LLMs), in an encoder-decoder framework, our model effectively integratesdiverse data types including proteins' sequences, structures, and textualannotations. This multimodal approach allows for a holistic representation ofproteins' functions, enabling the generation of detailed and accuratedescriptions. To evaluate our model, we extracted a multimodal protein datasetfrom SwissProt, and demonstrate empirically the effectiveness of Prot2Text.These results highlight the transformative impact of multimodal models,specifically the fusion of GNNs and LLMs, empowering researchers with powerfultools for more accurate prediction of proteins' functions. The code, the modelsand a demo will be publicly released.</description><author>Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, Michalis Vazirgiannis</author><pubDate>Thu, 21 Dec 2023 16:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14367v2</guid></item><item><title>Multi-task Planar Reconstruction with Feature Warping Guidance</title><link>http://arxiv.org/abs/2311.14981v2</link><description>Piece-wise planar 3D reconstruction simultaneously segments plane instancesand recovers their 3D plane parameters from an image, which is particularlyuseful for indoor or man-made environments. Efficient reconstruction of 3Dplanes coupled with semantic predictions offers advantages for a wide range ofapplications requiring scene understanding and concurrent spatial mapping.However, most existing planar reconstruction models either neglect semanticpredictions or do not run efficiently enough for real-time applications. Weintroduce SOLOPlanes, a real-time planar reconstruction model based on amodified instance segmentation architecture which simultaneously predictssemantics for each plane instance, along with plane parameters and piece-wiseplane instance masks. We achieve an improvement in instance mask segmentationby including multi-view guidance for plane predictions in the training process.This cross-task improvement, training for plane prediction but improving themask segmentation, is due to the nature of feature sharing in multi-tasklearning. Our model simultaneously predicts semantics using single images atinference time, while achieving real-time predictions at 43 FPS.</description><author>Luan Wei, Anna Hilsmann, Peter Eisert</author><pubDate>Thu, 21 Dec 2023 16:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14981v2</guid></item><item><title>On the choice of the optimal temporal support for audio classification with Pre-trained embeddings</title><link>http://arxiv.org/abs/2312.14005v1</link><description>Current state-of-the-art audio analysis systems rely on pre-trained embeddingmodels, often used off-the-shelf as (frozen) feature extractors. Choosing thebest one for a set of tasks is the subject of many recent publications.However, one aspect often overlooked in these works is the influence of theduration of audio input considered to extract an embedding, which we refer toas Temporal Support (TS). In this work, we study the influence of the TS forwell-established or emerging pre-trained embeddings, chosen to representdifferent types of architectures and learning paradigms. We conduct thisevaluation using both musical instrument and environmental sound datasets,namely OpenMIC, TAU Urban Acoustic Scenes 2020 Mobile, and ESC-50. Weespecially highlight that Audio Spectrogram Transformer-based systems (PaSSTand BEATs) remain effective with smaller TS, which therefore allows for adrastic reduction in memory and computational cost. Moreover, we show that bychoosing the optimal TS we reach competitive results across all tasks. Inparticular, we improve the state-of-the-art results on OpenMIC, using BEATs andPaSST without any fine-tuning.</description><author>Aurian Quelennec, Michel Olvera, Geoffroy Peeters, Slim Essid</author><pubDate>Thu, 21 Dec 2023 16:36:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14005v1</guid></item><item><title>Deep Learning Based Face Recognition Method using Siamese Network</title><link>http://arxiv.org/abs/2312.14001v1</link><description>Achieving state-of-the-art results in face verification systems typicallyhinges on the availability of labeled face training data, a resource that oftenproves challenging to acquire in substantial quantities. In this researchendeavor, we proposed employing Siamese networks for face recognition,eliminating the need for labeled face images. We achieve this by strategicallyleveraging negative samples alongside nearest neighbor counterparts, therebyestablishing positive and negative pairs through an unsupervised methodology.The architectural framework adopts a VGG encoder, trained as a double branchsiamese network. Our primary aim is to circumvent the necessity for labeledface image data, thus proposing the generation of training pairs in an entirelyunsupervised manner. Positive training data are selected within a dataset basedon their highest cosine similarity scores with a designated anchor, whilenegative training data are culled in a parallel fashion, though drawn from analternate dataset. During training, the proposed siamese network conductsbinary classification via cross-entropy loss. Subsequently, during the testingphase, we directly extract face verification scores from the network's outputlayer. Experimental results reveal that the proposed unsupervised systemdelivers a performance on par with a similar but fully supervised baseline.</description><author>Enoch Solomon, Abraham Woubie, Eyael Solomon Emiru</author><pubDate>Thu, 21 Dec 2023 16:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14001v1</guid></item><item><title>Risk-Sensitive Stochastic Optimal Control as Rao-Blackwellized Markovian Score Climbing</title><link>http://arxiv.org/abs/2312.14000v1</link><description>Stochastic optimal control of dynamical systems is a crucial challenge insequential decision-making. Recently, control-as-inference approaches have hadconsiderable success, providing a viable risk-sensitive framework to addressthe exploration-exploitation dilemma. Nonetheless, a majority of thesetechniques only invoke the inference-control duality to derive a modified riskobjective that is then addressed within a reinforcement learning framework.This paper introduces a novel perspective by framing risk-sensitive stochasticcontrol as Markovian score climbing under samples drawn from a conditionalparticle filter. Our approach, while purely inference-centric, providesasymptotically unbiased estimates for gradient-based policy optimization withoptimal importance weighting and no explicit value function learning. Tovalidate our methodology, we apply it to the task of learning neuralnon-Gaussian feedback policies, showcasing its efficacy on numerical benchmarksof stochastic dynamical systems.</description><author>Hany Abdulsamad, Sahel Iqbal, Adrien Corenflos, Simo Särkkä</author><pubDate>Thu, 21 Dec 2023 16:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14000v1</guid></item><item><title>Open-Set: ID Card Presentation Attack Detection using Neural Transfer Style</title><link>http://arxiv.org/abs/2312.13993v1</link><description>The accurate detection of ID card Presentation Attacks (PA) is becomingincreasingly important due to the rising number of online/remote services thatrequire the presentation of digital photographs of ID cards for digitalonboarding or authentication. Furthermore, cybercriminals are continuouslysearching for innovative ways to fool authentication systems to gainunauthorized access to these services. Although advances in neural networkdesign and training have pushed image classification to the state of the art,one of the main challenges faced by the development of fraud detection systemsis the curation of representative datasets for training and evaluation. Thehandcrafted creation of representative presentation attack samples oftenrequires expertise and is very time-consuming, thus an automatic process ofobtaining high-quality data is highly desirable. This work explores ID cardPresentation Attack Instruments (PAI) in order to improve the generation ofsamples with four Generative Adversarial Networks (GANs) based imagetranslation models and analyses the effectiveness of the generated data fortraining fraud detection systems. Using open-source data, we show thatsynthetic attack presentations are an adequate complement for additional realattack presentations, where we obtain an EER performance increase of 0.63%points for print attacks and a loss of 0.29% for screen capture attacks.</description><author>Reuben Markham, Juan M. Espin, Mario Nieto-Hidalgo, Juan E. Tapia</author><pubDate>Thu, 21 Dec 2023 16:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13993v1</guid></item><item><title>Invariant Learning via Probability of Sufficient and Necessary Causes</title><link>http://arxiv.org/abs/2309.12559v4</link><description>Out-of-distribution (OOD) generalization is indispensable for learning modelsin the wild, where testing distribution typically unknown and different fromthe training. Recent methods derived from causality have shown great potentialin achieving OOD generalization. However, existing methods mainly focus on theinvariance property of causes, while largely overlooking the property of\textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary butinsufficient cause (feature) is invariant to distribution shift, yet it may nothave required accuracy. By contrast, a sufficient yet unnecessary cause(feature) tends to fit specific data well but may have a risk of adapting to anew domain. To capture the information of sufficient and necessary causes, weemploy a classical concept, the probability of sufficiency and necessary causes(PNS), which indicates the probability of whether one is the necessary andsufficient cause. To associate PNS with OOD generalization, we propose PNS riskand formulate an algorithm to learn representation with a high PNS value. Wetheoretically analyze and prove the generalizability of the PNS risk.Experiments on both synthetic and real-world benchmarks demonstrate theeffectiveness of the proposed method. The details of the implementation can befound at the GitHub repository: https://github.com/ymy4323460/CaSN.</description><author>Mengyue Yang, Zhen Fang, Yonggang Zhang, Yali Du, Furui Liu, Jean-Francois Ton, Jianhong Wang, Jun Wang</author><pubDate>Thu, 21 Dec 2023 16:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12559v4</guid></item><item><title>Fair GANs through model rebalancing for extremely imbalanced class distributions</title><link>http://arxiv.org/abs/2308.08638v2</link><description>Deep generative models require large amounts of training data. This oftenposes a problem as the collection of datasets can be expensive and difficult,in particular datasets that are representative of the appropriate underlyingdistribution (e.g. demographic). This introduces biases in datasets which arefurther propagated in the models. We present an approach to construct anunbiased generative adversarial network (GAN) from an existing biased GAN byrebalancing the model distribution. We do so by generating balanced data froman existing imbalanced deep generative model using an evolutionary algorithmand then using this data to train a balanced generative model. Additionally, wepropose a bias mitigation loss function that minimizes the deviation of thelearned class distribution from being equiprobable. We show results for theStyleGAN2 models while training on the Flickr Faces High Quality (FFHQ) datasetfor racial fairness and see that the proposed approach improves on the fairnessmetric by almost 5 times, whilst maintaining image quality. We further validateour approach by applying it to an imbalanced CIFAR10 dataset where we show thatwe can obtain comparable fairness and image quality as when training on abalanced CIFAR10 dataset which is also twice as large. Lastly, we argue thatthe traditionally used image quality metrics such as Frechet inception distance(FID) are unsuitable for scenarios where the class distributions are imbalancedand a balanced reference set is not available.</description><author>Anubhav Jain, Nasir Memon, Julian Togelius</author><pubDate>Thu, 21 Dec 2023 16:22:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08638v2</guid></item><item><title>Modular Neural Network Policies for Learning In-Flight Object Catching with a Robot Hand-Arm System</title><link>http://arxiv.org/abs/2312.13987v1</link><description>We present a modular framework designed to enable a robot hand-arm system tolearn how to catch flying objects, a task that requires fast, reactive, andaccurately-timed robot motions. Our framework consists of five core modules:(i) an object state estimator that learns object trajectory prediction, (ii) acatching pose quality network that learns to score and rank object poses forcatching, (iii) a reaching control policy trained to move the robot hand topre-catch poses, (iv) a grasping control policy trained to perform softcatching motions for safe and robust grasping, and (v) a gating network trainedto synthesize the actions given by the reaching and grasping policy. The formertwo modules are trained via supervised learning and the latter three use deepreinforcement learning in a simulated environment. We conduct extensiveevaluations of our framework in simulation for each module and the integratedsystem, to demonstrate high success rates of in-flight catching and robustnessto perturbations and sensory noise. Whilst only simple cylindrical andspherical objects are used for training, the integrated system shows successfulgeneralization to a variety of household objects that are not used in training.</description><author>Wenbin Hu, Fernando Acero, Eleftherios Triantafyllidis, Zhaocheng Liu, Zhibin Li</author><pubDate>Thu, 21 Dec 2023 16:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13987v1</guid></item><item><title>Rényi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration</title><link>http://arxiv.org/abs/2312.13985v1</link><description>Pufferfish privacy is a flexible generalization of differential privacy thatallows to model arbitrary secrets and adversary's prior knowledge about thedata. Unfortunately, designing general and tractable Pufferfish mechanisms thatdo not compromise utility is challenging. Furthermore, this framework does notprovide the composition guarantees needed for a direct use in iterative machinelearning algorithms. To mitigate these issues, we introduce a R\'enyidivergence-based variant of Pufferfish and show that it allows us to extend theapplicability of the Pufferfish framework. We first generalize the Wassersteinmechanism to cover a wide range of noise distributions and introduce severalways to improve its utility. We also derive stronger guarantees againstout-of-distribution adversaries. Finally, as an alternative to composition, weprove privacy amplification results for contractive noisy iterations andshowcase the first use of Pufferfish in private convex optimization. A commoningredient underlying our results is the use and extension of shift reductionlemmas.</description><author>Clément Pierquin, Aurélien Bellet, Marc Tommasi, Matthieu Boussard</author><pubDate>Thu, 21 Dec 2023 16:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13985v1</guid></item><item><title>DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4</title><link>http://arxiv.org/abs/2303.11032v2</link><description>The digitization of healthcare has facilitated the sharing and re-using ofmedical data but has also raised concerns about confidentiality and privacy.HIPAA (Health Insurance Portability and Accountability Act) mandates removingre-identifying information before the dissemination of medical records. Thus,effective and efficient solutions for de-identifying medical data, especiallythose in free-text forms, are highly needed. While various computer-assistedde-identification methods, including both rule-based and learning-based, havebeen developed and used in prior practice, such solutions still lackgeneralizability or need to be fine-tuned according to different scenarios,significantly imposing restrictions in wider use. The advancement of largelanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential inprocessing text data in the medical domain with zero-shot in-context learning,especially in the task of privacy protection, as these models can identifyconfidential information by their powerful named entity recognition (NER)capability. In this work, we developed a novel GPT4-enabled de-identificationframework (``DeID-GPT") to automatically identify and remove the identifyinginformation. Compared to existing commonly used medical text datade-identification methods, our developed DeID-GPT showed the highest accuracyand remarkable reliability in masking private information from the unstructuredmedical text while preserving the original structure and meaning of the text.This study is one of the earliest to utilize ChatGPT and GPT-4 for medical textdata processing and de-identification, which provides insights for furtherresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 inhealthcare. Codes and benchmarking data information are available athttps://github.com/yhydhx/ChatGPT-API.</description><author>Zhengliang Liu, Yue Huang, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Yiwei Li, Peng Shu, Fang Zeng, Lichao Sun, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu, Xiang Li</author><pubDate>Thu, 21 Dec 2023 16:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11032v2</guid></item><item><title>Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning</title><link>http://arxiv.org/abs/2312.13980v1</link><description>Recent advancements in the text-to-3D task leverage finetuned text-to-imagediffusion models to generate multi-view images, followed by NeRFreconstruction. Yet, existing supervised finetuned (SFT) diffusion models stillsuffer from multi-view inconsistency and the resulting NeRF artifacts. Althoughtraining longer with SFT improves consistency, it also causes distributionshift, which reduces diversity and realistic details. We argue that the SFT ofmulti-view diffusion models resembles the instruction finetuning stage of theLLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.Essentially, RLFT methods optimize models beyond their SFT data distribution byusing their own outputs, effectively mitigating distribution shift. To thisend, we introduce Carve3D, a RLFT method coupled with the Multi-viewReconstruction Consistency (MRC) metric, to improve the consistency ofmulti-view diffusion models. To compute MRC on a set of multi-view images, wecompare them with their corresponding renderings of the reconstructed NeRF atthe same viewpoints. We validate the robustness of MRC with extensiveexperiments conducted under controlled inconsistency levels. We enhance thebase RLFT algorithm to stabilize the training process, reduce distributionshift, and identify scaling laws. Through qualitative and quantitativeexperiments, along with a user study, we demonstrate Carve3D's improvedmulti-view consistency, the resulting superior NeRF reconstruction quality, andminimal distribution shift compared to longer SFT. Project webpage:https://desaixie.github.io/carve-3d.</description><author>Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Sören Pirk, Arie E. Kaufman</author><pubDate>Thu, 21 Dec 2023 16:10:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13980v1</guid></item><item><title>Metalearning with Very Few Samples Per Task</title><link>http://arxiv.org/abs/2312.13978v1</link><description>Metalearning and multitask learning are two frameworks for solving a group ofrelated learning tasks more efficiently than we could hope to solve each of theindividual tasks on their own. In multitask learning, we are given a fixed setof related learning tasks and need to output one accurate model per task,whereas in metalearning we are given tasks that are drawn i.i.d. from ametadistribution and need to output some common information that can be easilyspecialized to new, previously unseen tasks from the metadistribution. In this work, we consider a binary classification setting where tasks arerelated by a shared representation, that is, every task $P$ of interest can besolved by a classifier of the form $f_{P} \circ h$ where $h \in H$ is a mapfrom features to some representation space that is shared across tasks, and$f_{P} \in F$ is a task-specific classifier from the representation space tolabels. The main question we ask in this work is how much data do we need tometalearn a good representation? Here, the amount of data is measured in termsof both the number of tasks $t$ that we need to see and the number of samples$n$ per task. We focus on the regime where the number of samples per task isextremely small. Our main result shows that, in a distribution-free settingwhere the feature vectors are in $\mathbb{R}^d$, the representation is a linearmap from $\mathbb{R}^d \to \mathbb{R}^k$, and the task-specific classifiers arehalfspaces in $\mathbb{R}^k$, we can metalearn a representation with error$\varepsilon$ using just $n = k+2$ samples per task, and $d \cdot(1/\varepsilon)^{O(k)}$ tasks. Learning with so few samples per task isremarkable because metalearning would be impossible with $k+1$ samples pertask, and because we cannot even hope to learn an accurate task-specificclassifier with just $k+2$ samples per task.</description><author>Maryam Aliakbarpour, Konstantina Bairaktari, Gavin Brown, Adam Smith, Jonathan Ullman</author><pubDate>Thu, 21 Dec 2023 16:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13978v1</guid></item><item><title>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</title><link>http://arxiv.org/abs/2312.13977v1</link><description>Recently, neural implicit functions have demonstrated remarkable results inthe field of multi-view reconstruction. However, most existing methods aretailored for dense views and exhibit unsatisfactory performance when dealingwith sparse views. Several latest methods have been proposed for generalizingimplicit reconstruction to address the sparse view reconstruction task, butthey still suffer from high training costs and are merely valid under carefullyselected perspectives. In this paper, we propose a novel sparse viewreconstruction framework that leverages on-surface priors to achieve highlyfaithful surface reconstruction. Specifically, we design several constraints onglobal geometry alignment and local geometry refinement for jointly optimizingcoarse shapes and fine details. To achieve this, we train a neural network tolearn a global implicit field from the on-surface points obtained from SfM andthen leverage it as a coarse geometric constraint. To exploit local geometricconsistency, we project on-surface points onto seen and unseen views, treatingthe consistent loss of projected features as a fine geometric constraint. Theexperimental results with DTU and BlendedMVS datasets in two prevalent sparsesettings demonstrate significant improvements over the state-of-the-artmethods.</description><author>Han Huang, Yulun Wu, Junsheng Zhou, Ge Gao, Ming Gu, Yushen Liu</author><pubDate>Thu, 21 Dec 2023 16:04:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13977v1</guid></item><item><title>Anatomical basis of sex differences in human post-myocardial infarction ECG phenotypes identified by novel automated torso-cardiac 3D reconstruction</title><link>http://arxiv.org/abs/2312.13976v1</link><description>The electrocardiogram (ECG) is routinely used in cardiology, though itsinterpretation is confounded by anatomical variability. A novel, automatedcomputational pipeline enables quantification of torso-ventricular anatomymetrics from magnetic resonance imaging, and comparison to ECG characteristics.Sex and myocardial infarction differences are investigated based on 1051healthy and 425 post-MI subjects from UK Biobank. Smaller ventricles in femalesexplain ~50% of shorter QRS durations than in males, and contribute to lowerSTJ amplitudes in females (also due to more superior and posterior position).In females, torso-ventricular anatomy, particularly from larger BMI, is astronger modulator of T wave amplitude reductions and left-deviated R axisangles in post-MI than in males. Thus, female MI phenotype is less reflectiveof pathology, and baseline STJ amplitudes and QRS durations are further fromclinical thresholds. Therefore, quantification of anatomical sex-differencesand impact on ECG in health and disease is critical to avoid clinical sex-bias.</description><author>Hannah J. Smith, Blanca Rodriguez, Yuling Sang, Marcel Beetz, Robin Choudhury, Vicente Grau, Abhirup Banerjee</author><pubDate>Thu, 21 Dec 2023 16:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13976v1</guid></item><item><title>On Partial Optimal Transport: Revising the Infeasibility of Sinkhorn and Efficient Gradient Methods</title><link>http://arxiv.org/abs/2312.13970v1</link><description>This paper studies the Partial Optimal Transport (POT) problem between twounbalanced measures with at most $n$ supports and its applications in variousAI tasks such as color transfer or domain adaptation. There is hence the needfor fast approximations of POT with increasingly large problem sizes in arisingapplications. We first theoretically and experimentally investigate theinfeasibility of the state-of-the-art Sinkhorn algorithm for POT due to itsincompatible rounding procedure, which consequently degrades its qualitativeperformance in real world applications like point-cloud registration. To thisend, we propose a novel rounding algorithm for POT, and then provide a feasibleSinkhorn procedure with a revised computation complexity of$\mathcal{\widetilde O}(n^2/\varepsilon^4)$. Our rounding algorithm alsopermits the development of two first-order methods to approximate the POTproblem. The first algorithm, Adaptive Primal-Dual Accelerated Gradient Descent(APDAGD), finds an $\varepsilon$-approximate solution to the POT problem in$\mathcal{\widetilde O}(n^{2.5}/\varepsilon)$, which is better in $\varepsilon$than revised Sinkhorn. The second method, Dual Extrapolation, achieves thecomputation complexity of $\mathcal{\widetilde O}(n^2/\varepsilon)$, therebybeing the best in the literature. We further demonstrate the flexibility of POTcompared to standard OT as well as the practicality of our algorithms on realapplications where two marginal distributions are unbalanced.</description><author>Anh Duc Nguyen, Tuan Dung Nguyen, Quang Minh Nguyen, Hoang H. Nguyen, Kim-Chuan Toh</author><pubDate>Thu, 21 Dec 2023 15:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13970v1</guid></item><item><title>PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models</title><link>http://arxiv.org/abs/2312.13964v1</link><description>Recent advancements in personalized text-to-image (T2I) models haverevolutionized content creation, empowering non-experts to generate stunningimages with unique styles. While promising, adding realistic motions into thesepersonalized images by text poses significant challenges in preserving distinctstyles, high-fidelity details, and achieving motion controllability by text. Inthis paper, we present PIA, a Personalized Image Animator that excels inaligning with condition images, achieving motion controllability by text, andthe compatibility with various personalized T2I models without specific tuning.To achieve these goals, PIA builds upon a base T2I model with well-trainedtemporal alignment layers, allowing for the seamless transformation of anypersonalized T2I model into an image animation model. A key component of PIA isthe introduction of the condition module, which utilizes the condition frameand inter-frame affinity as input to transfer appearance information guided bythe affinity hint for individual frame synthesis in the latent space. Thisdesign mitigates the challenges of appearance-related image alignment withinand allows for a stronger focus on aligning with motion-related guidance.</description><author>Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen</author><pubDate>Thu, 21 Dec 2023 15:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13964v1</guid></item><item><title>ChatGPT as a commenter to the news: can LLMs generate human-like opinions?</title><link>http://arxiv.org/abs/2312.13961v1</link><description>ChatGPT, GPT-3.5, and other large language models (LLMs) have drawnsignificant attention since their release, and the abilities of these modelshave been investigated for a wide variety of tasks. In this research weinvestigate to what extent GPT-3.5 can generate human-like comments on Dutchnews articles. We define human likeness as `not distinguishable from humancomments', approximated by the difficulty of automatic classification betweenhuman and GPT comments. We analyze human likeness across multiple promptingtechniques. In particular, we utilize zero-shot, few-shot and context prompts,for two generated personas. We found that our fine-tuned BERT models can easilydistinguish human-written comments from GPT-3.5 generated comments, with noneof the used prompting methods performing noticeably better. We further analyzedthat human comments consistently showed higher lexical diversity thanGPT-generated comments. This indicates that although generative LLMs cangenerate fluent text, their capability to create human-like opinionatedcomments is still limited.</description><author>Rayden Tseng, Suzan Verberne, Peter van der Putten</author><pubDate>Thu, 21 Dec 2023 15:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13961v1</guid></item><item><title>Typhoon: Thai Large Language Models</title><link>http://arxiv.org/abs/2312.13951v1</link><description>Typhoon is a series of Thai large language models (LLMs) developedspecifically for the Thai language. This technical report presents challengesand insights in developing Thai LLMs, including data preparation, pretraining,instruction-tuning, and evaluation. As one of the challenges of low-resourcelanguages is the amount of pretraining data, we apply continual training totransfer existing world knowledge from a strong LLM. To evaluate the Thaiknowledge encapsulated in each model from the pretraining stage, we developThaiExam, a benchmark based on examinations for high-school students andinvestment professionals in Thailand. In addition, we fine-tune Typhoon tofollow Thai instructions, and we evaluate instruction-tuned models on Thaiinstruction datasets as well as translation, summarization, andquestion-answering tasks. Experimental results on a suite of Thai benchmarksshow that Typhoon outperforms all open-source Thai language models, and itsperformance is on par with GPT-3.5 in Thai while having only 7 billionparameters and being 2.62 times more efficient in tokenizing Thai text.</description><author>Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarnmongkol, Ruangsak Patomwong, Pathomporn Chokchainant, Kasima Tharnpipitchai</author><pubDate>Thu, 21 Dec 2023 15:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13951v1</guid></item><item><title>PhysRFANet: Physics-Guided Neural Network for Real-Time Prediction of Thermal Effect During Radiofrequency Ablation Treatment</title><link>http://arxiv.org/abs/2312.13947v1</link><description>Radiofrequency ablation (RFA) is a widely used minimally invasive techniquefor ablating solid tumors. Achieving precise personalized treatmentnecessitates feedback information on in situ thermal effects induced by the RFAprocedure. While computer simulation facilitates the prediction of electricaland thermal phenomena associated with RFA, its practical implementation inclinical settings is hindered by high computational demands. In this paper, wepropose a physics-guided neural network model, named PhysRFANet, to enablereal-time prediction of thermal effect during RFA treatment. The networks,designed for predicting temperature distribution and the corresponding ablationlesion, were trained using biophysical computational models that integratedelectrostatics, bio-heat transfer, and cell necrosis, alongside magneticresonance (MR) images of breast cancer patients. Validation of thecomputational model was performed through experiments on ex vivo bovine livertissue. Our model demonstrated a 96% Dice score in predicting the lesion volumeand an RMSE of 0.4854 for temperature distribution when tested with foreseentumor images. Notably, even with unforeseen images, it achieved a 93% Dicescore for the ablation lesion and an RMSE of 0.6783 for temperaturedistribution. All networks were capable of inferring results within 10 ms. Thepresented technique, applied to optimize the placement of the electrode for aspecific target region, holds significant promise in enhancing the safety andefficacy of RFA treatments.</description><author>Minwoo Shin, Minjee Seo, Seonaeng Cho, Juil Park, Joon Ho Kwon, Deukhee Lee, Kyungho Yoon</author><pubDate>Thu, 21 Dec 2023 15:36:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13947v1</guid></item><item><title>Controllable 3D Face Generation with Conditional Style Code Diffusion</title><link>http://arxiv.org/abs/2312.13941v1</link><description>Generating photorealistic 3D faces from given conditions is a challengingtask. Existing methods often rely on time-consuming one-by-one optimizationapproaches, which are not efficient for modeling the same distribution content,e.g., faces. Additionally, an ideal controllable 3D face generation modelshould consider both facial attributes and expressions. Thus we propose a novelapproach called TEx-Face(TExt &amp; Expression-to-Face) that addresses thesechallenges by dividing the task into three components, i.e., 3D GAN Inversion,Conditional Style Code Diffusion, and 3D Face Decoding. For 3D GAN inversion,we introduce two methods which aim to enhance the representation of style codesand alleviate 3D inconsistencies. Furthermore, we design a style code denoiserto incorporate multiple conditions into the style code and propose a dataaugmentation strategy to address the issue of insufficient pairedvisual-language data. Extensive experiments conducted on FFHQ, CelebA-HQ, andCelebA-Dialog demonstrate the promising performance of our TEx-Face inachieving the efficient and controllable generation of photorealistic 3D faces.The code will be available at https://github.com/sxl142/TEx-Face.</description><author>Xiaolong Shen, Jianxin Ma, Chang Zhou, Zongxin Yang</author><pubDate>Thu, 21 Dec 2023 15:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13941v1</guid></item><item><title>Structured Probabilistic Coding</title><link>http://arxiv.org/abs/2312.13933v1</link><description>This paper presents a new supervised representation learning framework,namely Structured Probabilistic Coding (SPC), to learn compact and informativerepresentations from input related to the target task. SPC is an encoder-onlyprobabilistic coding technology with a structured regularization from thetarget label space. By extracting compact and informative representations frominput related to the target task, SPC can enhance the generalization ability ofpre-trained language models for better language understanding. Specifically,the hidden representation is encoded into a Gaussian distribution space, whilemaximizing the prior entropy of latent representations concerning label space.This technique can simultaneously perform information encoding and taskprediction in one module to more fully utilize the effective information frominput data, and use variational inference in the output space to reducerandomness and uncertainty. To better control the probability distribution inthe latent space, a structured regularization is proposed to promoteclass-level uniformity in the latent space. With the regularization term, SPCcan preserve the Gaussian distribution structure of latent code as well asbetter cover the hidden space with class uniformly. We conduct evaluations on12 natural language understanding tasks. The results show that our SPC caneffectively improve the performance of pre-trained language models for variousclassification and regression tasks. Experiments demonstrate that SPC canenhance the generalization capability, robustness to label noise, andclustering quality of output representations.</description><author>Dou Hu, Lingwei Wei, Yaxin Liu, Wei Zhou, Songlin Hu</author><pubDate>Thu, 21 Dec 2023 15:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13933v1</guid></item><item><title>Joint Sensing and Task-Oriented Communications with Image and Wireless Data Modalities for Dynamic Spectrum Access</title><link>http://arxiv.org/abs/2312.13931v1</link><description>This paper introduces a deep learning approach to dynamic spectrum access,leveraging the synergy of multi-modal image and spectrum data for theidentification of potential transmitters. We consider an edge device equippedwith a camera that is taking images of potential objects such as vehicles thatmay harbor transmitters. Recognizing the computational constraints and trustissues associated with on-device computation, we propose a collaborative systemwherein the edge device communicates selectively processed information to atrusted receiver acting as a fusion center, where a decision is made toidentify whether a potential transmitter is present, or not. To achieve this,we employ task-oriented communications, utilizing an encoder at the transmitterfor joint source coding, channel coding, and modulation. This architectureefficiently transmits essential information of reduced dimension for objectclassification. Simultaneously, the transmitted signals may reflect off objectsand return to the transmitter, allowing for the collection of target sensingdata. Then the collected sensing data undergoes a second round of encoding atthe transmitter, with the reduced-dimensional information communicated back tothe fusion center through task-oriented communications. On the receiver side, adecoder performs the task of identifying a transmitter by fusing data receivedthrough joint sensing and task-oriented communications. The two encoders at thetransmitter and the decoder at the receiver are jointly trained, enabling aseamless integration of image classification and wireless signal detection.Using AWGN and Rayleigh channel models, we demonstrate the effectiveness of theproposed approach, showcasing high accuracy in transmitter identificationacross diverse channel conditions while sustaining low latency in decisionmaking.</description><author>Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</author><pubDate>Thu, 21 Dec 2023 15:26:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13931v1</guid></item><item><title>Limitations of Face Image Generation</title><link>http://arxiv.org/abs/2309.07277v2</link><description>Text-to-image diffusion models have achieved widespread popularity due totheir unprecedented image generation capability. In particular, their abilityto synthesize and modify human faces has spurred research into using generatedface images in both training data augmentation and model performanceassessments. In this paper, we study the efficacy and shortcomings ofgenerative models in the context of face generation. Utilizing a combination ofqualitative and quantitative measures, including embedding-based metrics anduser studies, we present a framework to audit the characteristics of generatedfaces conditioned on a set of social attributes. We applied our framework onfaces generated through state-of-the-art text-to-image diffusion models. Weidentify several limitations of face image generation that include faithfulnessto the text prompt, demographic disparities, and distributional shifts.Furthermore, we present an analytical model that provides insights into howtraining data selection contributes to the performance of generative models.</description><author>Harrison Rosenberg, Shimaa Ahmed, Guruprasad V Ramesh, Ramya Korlakai Vinayak, Kassem Fawaz</author><pubDate>Thu, 21 Dec 2023 15:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07277v2</guid></item><item><title>On the convergence of loss and uncertainty-based active learning algorithms</title><link>http://arxiv.org/abs/2312.13927v1</link><description>We study convergence rates of loss and uncertainty-based active learningalgorithms under various assumptions. First, we provide a set of conditionsunder which a convergence rate guarantee holds, and use this for linearclassifiers and linearly separable datasets to show convergence rate guaranteesfor loss-based sampling and different loss functions. Second, we provide aframework that allows us to derive convergence rate bounds for loss-basedsampling by deploying known convergence rate bounds for stochastic gradientdescent algorithms. Third, and last, we propose an active learning algorithmthat combines sampling of points and stochastic Polyak's step size. We show acondition on the sampling that ensures a convergence rate guarantee for thisalgorithm for smooth convex loss functions. Our numerical results demonstrateefficiency of our proposed algorithm.</description><author>Daniel Haimovich, Dima Karamshuk, Fridolin Linder, Niek Tax, Milan Vojnovic</author><pubDate>Thu, 21 Dec 2023 15:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13927v1</guid></item><item><title>Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models</title><link>http://arxiv.org/abs/2311.03830v2</link><description>Denoising Diffusion models have exhibited remarkable capabilities in imagegeneration. However, generating high-quality samples requires a large number ofiterations. Knowledge distillation for diffusion models is an effective methodto address this limitation with a shortened sampling process but causesdegraded generative quality. Based on our analysis with bias-variancedecomposition and experimental observations, we attribute the degradation tothe spatial fitting error occurring in the training of both the teacher andstudent model. Accordingly, we propose $\textbf{S}$patial$\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction$\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attentionguidance from the teacher model and a designed semantic gradient predictor toreduce the student's fitting error. Empirically, our proposed model facilitateshigh-quality sample generation in a few function evaluations. We achieve an FIDof 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step,outperforming existing diffusion methods. Our study provides a new perspectiveon diffusion distillation by highlighting the intrinsic denoising ability ofmodels. Project link: \url{https://github.com/Sainzerjj/SFERD}.</description><author>Shengzhe Zhou, Zejian Lee, Shengyuan Zhang, Lefan Hou, Changyuan Yang, Guang Yang, Zhiyuan Yang, Lingyun Sun</author><pubDate>Thu, 21 Dec 2023 15:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03830v2</guid></item><item><title>Strategyproof Decision-Making in Panel Data Settings and Beyond</title><link>http://arxiv.org/abs/2211.14236v4</link><description>We consider the problem of decision-making using panel data, in which adecision-maker gets noisy, repeated measurements of multiple units (or agents).We consider a setup where there is a pre-intervention period, when theprincipal observes the outcomes of each unit, after which the principal usesthese observations to assign a treatment to each unit. Unlike this classicalsetting, we permit the units generating the panel data to be strategic, i.e.units may modify their pre-intervention outcomes in order to receive a moredesirable intervention. The principal's goal is to design a strategyproofintervention policy, i.e. a policy that assigns units to theirutility-maximizing interventions despite their potential strategizing. We firstidentify a necessary and sufficient condition under which a strategyproofintervention policy exists, and provide a strategyproof mechanism with a simpleclosed form when one does exist. Along the way, we prove impossibility resultsfor strategic multiclass classification, which may be of independent interest.When there are two interventions, we establish that there always exists astrategyproof mechanism, and provide an algorithm for learning such amechanism. For three or more interventions, we provide an algorithm forlearning a strategyproof mechanism if there exists a sufficiently large gap inthe principal's rewards between different interventions. Finally, weempirically evaluate our model using real-world panel data collected fromproduct sales over 18 months. We find that our methods compare favorably tobaselines which do not take strategic interactions into consideration, even inthe presence of model misspecification.</description><author>Keegan Harris, Anish Agarwal, Chara Podimata, Zhiwei Steven Wu</author><pubDate>Thu, 21 Dec 2023 15:17:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14236v4</guid></item><item><title>Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization</title><link>http://arxiv.org/abs/2303.06088v5</link><description>In Self-Supervised Learning (SSL), models are typically pretrained,fine-tuned, and evaluated on the same domains. However, they tend to performpoorly when evaluated on unseen domains, a challenge that Unsupervised DomainGeneralization (UDG) seeks to address. Current UDG methods rely on domainlabels, which are often challenging to collect, and domain-specificarchitectures that lack scalability when confronted with numerous domains,making the current methodology impractical and rigid. Inspired bycontrastive-based UDG methods that mitigate spurious correlations byrestricting comparisons to examples from the same domain, we hypothesize thateliminating style variability within a batch could provide a more convenientand flexible way to reduce spurious correlations without requiring domainlabels. To verify this hypothesis, we introduce Batch Styles Standardization(BSS), a relatively simple yet powerful Fourier-based method to standardize thestyle of images in a batch specifically designed for integration with SSLmethods to tackle UDG. Combining BSS with existing SSL methods offers seriousadvantages over prior UDG methods: (1) It eliminates the need for domain labelsor domain-specific network components to enhance domain-invariance in SSLrepresentations, and (2) offers flexibility as BSS can be seamlessly integratedwith diverse contrastive-based but also non-contrastive-based SSL methods.Experiments on several UDG datasets demonstrate that it significantly improvesdownstream task performances on unseen domains, often outperforming or rivalingwith UDG methods. Finally, this work clarifies the underlying mechanismscontributing to BSS's effectiveness in improving domain-invariance in SSLrepresentations and performances on unseen domain.</description><author>Marin Scalbert, Maria Vakalopoulou, Florent Couzinié-Devy</author><pubDate>Thu, 21 Dec 2023 15:16:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06088v5</guid></item><item><title>Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis</title><link>http://arxiv.org/abs/2308.12466v2</link><description>Since the introduction of ChatGPT and GPT-4, these models have been testedacross a large number of tasks. Their adeptness across domains is evident, buttheir aptitude in playing games, and specifically their aptitude in the realmof poker has remained unexplored. Poker is a game that requires decision makingunder uncertainty and incomplete information. In this paper, we put ChatGPT andGPT-4 through the poker test and evaluate their poker skills. Our findingsreveal that while both models display an advanced understanding of poker,encompassing concepts like the valuation of starting hands, playing positionsand other intricacies of game theory optimal (GTO) poker, both ChatGPT andGPT-4 are NOT game theory optimal poker players. Profitable strategies in poker are evaluated in expectations over largesamples. Through a series of experiments, we first discover the characteristicsof optimal prompts and model parameters for playing poker with these models.Our observations then unveil the distinct playing personas of the two models.We first conclude that GPT-4 is a more advanced poker player than ChatGPT. Thisexploration then sheds light on the divergent poker tactics of the two models:ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In pokervernacular, when tasked to play GTO poker, ChatGPT plays like a nit, whichmeans that it has a propensity to only engage with premium hands and folds amajority of hands. When subjected to the same directive, GPT-4 plays like amaniac, showcasing a loose and aggressive style of play. Both strategies,although relatively advanced, are not game theory optimal.</description><author>Akshat Gupta</author><pubDate>Thu, 21 Dec 2023 15:14:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12466v2</guid></item><item><title>GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo</title><link>http://arxiv.org/abs/2310.19583v3</link><description>Traditional multi-view stereo (MVS) methods rely heavily on photometric andgeometric consistency constraints, but newer machine learning-based MVS methodscheck geometric consistency across multiple source views only as apost-processing step. In this paper, we present a novel approach thatexplicitly encourages geometric consistency of reference view depth maps acrossmultiple source views at different scales during learning (see Fig. 1). We findthat adding this geometric consistency loss significantly accelerates learningby explicitly penalizing geometrically inconsistent pixels, reducing thetraining iteration requirements to nearly half that of other MVS methods. Ourextensive experiments show that our approach achieves a new state-of-the-art onthe DTU and BlendedMVS datasets, and competitive results on the Tanks andTemples benchmark. To the best of our knowledge, GC-MVSNet is the first attemptto enforce multi-view, multi-scale geometric consistency during learning.</description><author>Vibhas K. Vats, Sripad Joshi, David J. Crandall, Md. Alimoor Reza, Soon-heung Jung</author><pubDate>Thu, 21 Dec 2023 15:14:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19583v3</guid></item><item><title>AsyncMLD: Asynchronous Multi-LLM Framework for Dialogue Recommendation System</title><link>http://arxiv.org/abs/2312.13925v1</link><description>We have reached a practical and realistic phase in human-support dialogueagents by developing a large language model (LLM). However, when requiringexpert knowledge or anticipating the utterance content using the massive sizeof the dialogue database, we still need help with the utterance content'seffectiveness and the efficiency of its output speed, even if using LLM.Therefore, we propose a framework that uses LLM asynchronously in the part ofthe system that returns an appropriate response and in the part thatunderstands the user's intention and searches the database. In particular,noting that it takes time for the robot to speak, threading related to databasesearches is performed while the robot is speaking.</description><author>Naoki Yoshimaru, Motoharu Okuma, Takamasa Iio, Kenji Hatano</author><pubDate>Thu, 21 Dec 2023 15:12:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13925v1</guid></item><item><title>Fed-CO$_{2}$: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning</title><link>http://arxiv.org/abs/2312.13923v1</link><description>Federated Learning (FL) has emerged as a promising distributed learningparadigm that enables multiple clients to learn a global model collaborativelywithout sharing their private data. However, the effectiveness of FL is highlydependent on the quality of the data that is being used for training. Inparticular, data heterogeneity issues, such as label distribution skew andfeature skew, can significantly impact the performance of FL. Previous studiesin FL have primarily focused on addressing label distribution skew dataheterogeneity, while only a few recent works have made initial progress intackling feature skew issues. Notably, these two forms of data heterogeneityhave been studied separately and have not been well explored within a unifiedFL framework. To address this gap, we propose Fed-CO$_{2}$, a universal FLframework that handles both label distribution skew and feature skew within a\textbf{C}ooperation mechanism between the \textbf{O}nline and \textbf{O}fflinemodels. Specifically, the online model learns general knowledge that is sharedamong all clients, while the offline model is trained locally to learn thespecialized knowledge of each individual client. To further enhance modelcooperation in the presence of feature shifts, we design an intra-clientknowledge transfer mechanism that reinforces mutual learning between the onlineand offline models, and an inter-client knowledge transfer mechanism toincrease the models' domain generalization ability. Extensive experiments showthat our Fed-CO$_{2}$ outperforms a wide range of existing personalizedfederated learning algorithms in terms of handling label distribution skew andfeature skew, both individually and collectively. The empirical results aresupported by our convergence analyses in a simplified setting.</description><author>Zhongyi Cai, Ye Shi, Wei Huang, Jingya Wang</author><pubDate>Thu, 21 Dec 2023 15:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13923v1</guid></item><item><title>Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models</title><link>http://arxiv.org/abs/2312.13913v1</link><description>This paper presents Paint3D, a novel coarse-to-fine generative framework thatis capable of producing high-resolution, lighting-less, and diverse 2K UVtexture maps for untextured 3D meshes conditioned on text or image inputs. Thekey challenge addressed is generating high-quality textures without embeddedillumination information, which allows the textures to be re-lighted orre-edited within modern graphics pipelines. To achieve this, our method firstleverages a pre-trained depth-aware 2D diffusion model to generateview-conditional images and perform multi-view texture fusion, producing aninitial coarse texture map. However, as 2D models cannot fully represent 3Dshapes and disable lighting effects, the coarse texture map exhibits incompleteareas and illumination artifacts. To resolve this, we train separate UVInpainting and UVHD diffusion models specialized for the shape-aware refinementof incomplete areas and the removal of illumination artifacts. Through thiscoarse-to-fine process, Paint3D can produce high-quality 2K UV textures thatmaintain semantic consistency while being lighting-less, significantlyadvancing the state-of-the-art in texturing 3D objects.</description><author>Xianfang Zeng</author><pubDate>Thu, 21 Dec 2023 15:01:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13913v1</guid></item><item><title>Solving Long-run Average Reward Robust MDPs via Stochastic Games</title><link>http://arxiv.org/abs/2312.13912v1</link><description>Markov decision processes (MDPs) provide a standard framework for sequentialdecision making under uncertainty. However, transition probabilities in MDPsare often estimated from data and MDPs do not take data uncertainty intoaccount. Robust Markov decision processes (RMDPs) address this shortcoming ofMDPs by assigning to each transition an uncertainty set rather than a singleprobability value. The goal of solving RMDPs is then to find a policy whichmaximizes the worst-case performance over the uncertainty sets. In this work,we consider polytopic RMDPs in which all uncertainty sets are polytopes andstudy the problem of solving long-run average reward polytopic RMDPs. Our focusis on computational complexity aspects and efficient algorithms. We present anovel perspective on this problem and show that it can be reduced to solvinglong-run average reward turn-based stochastic games with finite state andaction spaces. This reduction allows us to derive several importantconsequences that were hitherto not known to hold for polytopic RMDPs. First,we derive new computational complexity bounds for solving long-run averagereward polytopic RMDPs, showing for the first time that the threshold decisionproblem for them is in NP coNP and that they admit a randomized algorithm withsub-exponential expected runtime. Second, we present Robust Polytopic PolicyIteration (RPPI), a novel policy iteration algorithm for solving long-runaverage reward polytopic RMDPs. Our experimental evaluation shows that RPPI ismuch more efficient in solving long-run average reward polytopic RMDPs comparedto state-of-the-art methods based on value iteration.</description><author>Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Mehrdad Karrabi, Petr Novotný, Đorđe Žikelić</author><pubDate>Thu, 21 Dec 2023 15:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13912v1</guid></item><item><title>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</title><link>http://arxiv.org/abs/2310.16898v3</link><description>Due to the high price and heavy energy consumption of GPUs, deploying deepmodels on IoT devices such as microcontrollers makes significant contributionsfor ecological AI. Conventional methods successfully enable convolutionalneural network inference of high resolution images on microcontrollers, whilethe framework for vision transformers that achieve the state-of-the-artperformance in many vision applications still remains unexplored. In thispaper, we propose a hardware-algorithm co-optimizations method called MCUFormerto deploy vision transformers on microcontrollers with extremely limitedmemory, where we jointly design transformer architecture and construct theinference operator library to fit the memory resource constraint. Morespecifically, we generalize the one-shot network architecture search (NAS) todiscover the optimal architecture with highest task performance given thememory budget from the microcontrollers, where we enlarge the existing searchspace of vision transformers by considering the low-rank decompositiondimensions and patch resolution for memory reduction. For the construction ofthe inference operator library of vision transformers, we schedule the memorybuffer during inference through operator integration, patch embeddingdecomposition, and token overwriting, allowing the memory buffer to be fullyutilized to adapt to the forward pass of the vision transformer. Experimentalresults demonstrate that our MCUFormer achieves 73.62\% top-1 accuracy onImageNet for image classification with 320KB memory on STM32F746microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.</description><author>Yinan Liang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 21 Dec 2023 14:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16898v3</guid></item><item><title>Multi-Agent Probabilistic Ensembles with Trajectory Sampling for Connected Autonomous Vehicles</title><link>http://arxiv.org/abs/2312.13910v1</link><description>Autonomous Vehicles (AVs) have attracted significant attention in recentyears and Reinforcement Learning (RL) has shown remarkable performance inimproving the autonomy of vehicles. In that regard, the widely adoptedModel-Free RL (MFRL) promises to solve decision-making tasks in connected AVs(CAVs), contingent on the readiness of a significant amount of data samples fortraining. Nevertheless, it might be infeasible in practice and possibly lead tolearning instability. In contrast, Model-Based RL (MBRL) manifests itself insample-efficient learning, but the asymptotic performance of MBRL might lagbehind the state-of-the-art MFRL algorithms. Furthermore, most studies for CAVsare limited to the decision-making of a single AV only, thus underscoring theperformance due to the absence of communications. In this study, we try toaddress the decision-making problem of multiple CAVs with limitedcommunications and propose a decentralized Multi-Agent Probabilistic Ensembleswith Trajectory Sampling algorithm MA-PETS. In particular, in order to bettercapture the uncertainty of the unknown environment, MA-PETS leveragesProbabilistic Ensemble (PE) neural networks to learn from communicated samplesamong neighboring CAVs. Afterwards, MA-PETS capably develops TrajectorySampling (TS)-based model-predictive control for decision-making. On thisbasis, we derive the multi-agent group regret bound affected by the number ofagents within the communication range and mathematically validate thatincorporating effective information exchange among agents into the multi-agentlearning scheme contributes to reducing the group regret bound in the worstcase. Finally, we empirically demonstrate the superiority of MA-PETS in termsof the sample efficiency comparable to MFBL.</description><author>Ruoqi Wen, Jiahao Huang, Rongpeng Li, Guoru Ding, Zhifeng Zhao</author><pubDate>Thu, 21 Dec 2023 14:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13910v1</guid></item><item><title>EfficientPPS: Part-aware Panoptic Segmentation of Transparent Objects for Robotic Manipulation</title><link>http://arxiv.org/abs/2312.13906v1</link><description>The use of autonomous robots for assistance tasks in hospitals has thepotential to free up qualified staff and im-prove patient care. However, theubiquity of deformable and transparent objects in hospital settings posessignif-icant challenges to vision-based perception systems. We presentEfficientPPS, a neural architecture for part-aware panoptic segmentation thatprovides robots with semantically rich visual information for grasping andma-nipulation tasks. We also present an unsupervised data collection andlabelling method to reduce the need for human involvement in the trainingprocess. EfficientPPS is evaluated on a dataset containing real-world hospitalobjects and demonstrated to be robust and efficient in grasping transparenttransfusion bags with a collaborative robot arm.</description><author>Benjamin Alt, Minh Dang Nguyen, Andreas Hermann, Darko Katic, Rainer Jäkel, Rüdiger Dillmann, Eric Sax</author><pubDate>Thu, 21 Dec 2023 14:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13906v1</guid></item><item><title>Domain-Specific Fine-Tuning of Large Language Models for Interactive Robot Programming</title><link>http://arxiv.org/abs/2312.13905v1</link><description>Industrial robots are applied in a widening range of industries, but robotprogramming mostly remains a task limited to programming experts. We propose anatural language-based assistant for programming of advanced, industrialrobotic applications and investigate strategies for domain-specific fine-tuningof foundation models with limited data and compute.</description><author>Benjamin Alt, Urs Keßner, Aleksandar Taranovic, Darko Katic, Andreas Hermann, Rainer Jäkel, Gerhard Neumann</author><pubDate>Thu, 21 Dec 2023 14:51:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13905v1</guid></item><item><title>Comparative Evaluation of Anomaly Detection Methods for Fraud Detection in Online Credit Card Payments</title><link>http://arxiv.org/abs/2312.13896v1</link><description>This study explores the application of anomaly detection (AD) methods inimbalanced learning tasks, focusing on fraud detection using real online creditcard payment data. We assess the performance of several recent AD methods andcompare their effectiveness against standard supervised learning methods.Offering evidence of distribution shift within our dataset, we analyze itsimpact on the tested models' performances. Our findings reveal that LightGBMexhibits significantly superior performance across all evaluated metrics butsuffers more from distribution shifts than AD methods. Furthermore, ourinvestigation reveals that LightGBM also captures the majority of fraudsdetected by AD methods. This observation challenges the potential benefits ofensemble methods to combine supervised, and AD approaches to enhanceperformance. In summary, this research provides practical insights into theutility of these techniques in real-world scenarios, showing LightGBM'ssuperiority in fraud detection while highlighting challenges related todistribution shifts.</description><author>Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan, Fabrice Daniel</author><pubDate>Thu, 21 Dec 2023 14:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13896v1</guid></item><item><title>Reduced Policy Optimization for Continuous Control with Hard Constraints</title><link>http://arxiv.org/abs/2310.09574v2</link><description>Recent advances in constrained reinforcement learning (RL) have endowedreinforcement learning with certain safety guarantees. However, deployingexisting constrained RL algorithms in continuous control tasks with generalhard constraints remains challenging, particularly in those situations withnon-convex hard constraints. Inspired by the generalized reduced gradient (GRG)algorithm, a classical constrained optimization technique, we propose a reducedpolicy optimization (RPO) algorithm that combines RL with GRG to addressgeneral hard constraints. RPO partitions actions into basic actions andnonbasic actions following the GRG method and outputs the basic actions via apolicy network. Subsequently, RPO calculates the nonbasic actions by solvingequations based on equality constraints using the obtained basic actions. Thepolicy network is then updated by implicitly differentiating nonbasic actionswith respect to basic actions. Additionally, we introduce an action projectionprocedure based on the reduced gradient and apply a modified Lagrangianrelaxation technique to ensure inequality constraints are satisfied. To thebest of our knowledge, RPO is the first attempt that introduces GRG to RL as away of efficiently handling both equality and inequality hard constraints. Itis worth noting that there is currently a lack of RL environments with complexhard constraints, which motivates us to develop three new benchmarks: tworobotics manipulation tasks and a smart grid operation control task. With thesebenchmarks, RPO achieves better performance than previous constrained RLalgorithms in terms of both cumulative reward and constraint violation. Webelieve RPO, along with the new benchmarks, will open up new opportunities forapplying RL to real-world problems with complex constraints.</description><author>Shutong Ding, Jingya Wang, Yali Du, Ye Shi</author><pubDate>Thu, 21 Dec 2023 14:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09574v2</guid></item><item><title>Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural ODEs via Homotopy Continuation</title><link>http://arxiv.org/abs/2310.09583v2</link><description>Deep Equilibrium Models (DEQs) and Neural Ordinary Differential Equations(Neural ODEs) are two branches of implicit models that have achieved remarkablesuccess owing to their superior performance and low memory consumption. Whileboth are implicit models, DEQs and Neural ODEs are derived from differentmathematical formulations. Inspired by homotopy continuation, we establish aconnection between these two models and illustrate that they are actually twosides of the same coin. Homotopy continuation is a classical method of solvingnonlinear equations based on a corresponding ODE. Given this connection, weproposed a new implicit model called HomoODE that inherits the property of highaccuracy from DEQs and the property of stability from Neural ODEs. Unlike DEQs,which explicitly solve an equilibrium-point-finding problem via Newton'smethods in the forward pass, HomoODE solves the equilibrium-point-findingproblem implicitly using a modified Neural ODE via homotopy continuation.Further, we developed an acceleration method for HomoODE with a sharedlearnable initial point. It is worth noting that our model also provides abetter understanding of why Augmented Neural ODEs work as long as the augmentedpart is regarded as the equilibrium point to find. Comprehensive experimentswith several image classification tasks demonstrate that HomoODE surpassesexisting implicit models in terms of both accuracy and memory consumption.</description><author>Shutong Ding, Tianyu Cui, Jingya Wang, Ye Shi</author><pubDate>Thu, 21 Dec 2023 14:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09583v2</guid></item><item><title>Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs</title><link>http://arxiv.org/abs/2312.13881v1</link><description>Recent advances in natural language processing (NLP) owe their success topre-training language models on large amounts of unstructured data. Still,there is an increasing effort to combine the unstructured nature of LMs withstructured knowledge and reasoning. Particularly in the rapidly evolving fieldof biomedical NLP, knowledge-enhanced language models (KELMs) have emerged aspromising tools to bridge the gap between large language models anddomain-specific knowledge, considering the available biomedical knowledgegraphs (KGs) curated by experts over the decades. In this paper, we develop anapproach that uses lightweight adapter modules to inject structured biomedicalknowledge into pre-trained language models (PLMs). We use two large KGs, thebiomedical knowledge system UMLS and the novel biochemical ontology OntoChem,with two prominent biomedical PLMs, PubMedBERT and BioLinkBERT. The approachincludes partitioning knowledge graphs into smaller subgraphs, fine-tuningadapter modules for each subgraph, and combining the knowledge in a fusionlayer. We test the performance on three downstream tasks: documentclassification,question answering, and natural language inference. We show thatour methodology leads to performance improvements in several instances whilekeeping requirements in computing power low. Finally, we provide a detailedinterpretation of the results and report valuable insights for future work.</description><author>Juraj Vladika, Alexander Fichtl, Florian Matthes</author><pubDate>Thu, 21 Dec 2023 14:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13881v1</guid></item><item><title>Short Boolean Formulas as Explanations in Practice</title><link>http://arxiv.org/abs/2307.06971v2</link><description>We investigate explainability via short Boolean formulas in the data modelbased on unary relations. As an explanation of length k, we take a Booleanformula of length k that minimizes the error with respect to the targetattribute to be explained. We first provide novel quantitative bounds for theexpected error in this scenario. We then also demonstrate how the setting worksin practice by studying three concrete data sets. In each case, we calculateexplanation formulas of different lengths using an encoding in Answer SetProgramming. The most accurate formulas we obtain achieve errors similar toother methods on the same data sets. However, due to overfitting, theseformulas are not necessarily ideal explanations, so we use cross validation toidentify a suitable length for explanations. By limiting to shorter formulas,we obtain explanations that avoid overfitting but are still reasonably accurateand also, importantly, human interpretable.</description><author>Reijo Jaakkola, Tomi Janhunen, Antti Kuusisto, Masood Feyzbakhsh Rankooh, Miikka Vilander</author><pubDate>Thu, 21 Dec 2023 14:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06971v2</guid></item><item><title>Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting</title><link>http://arxiv.org/abs/2312.13271v2</link><description>Recent one image to 3D generation methods commonly adopt Score DistillationSampling (SDS). Despite the impressive results, there are multiple deficienciesincluding multi-view inconsistency, over-saturated and over-smoothed textures,as well as the slow generation speed. To address these deficiencies, we presentRepaint123 to alleviate multi-view bias as well as texture degradation andspeed up the generation process. The core idea is to combine the powerful imagegeneration capability of the 2D diffusion model and the texture alignmentability of the repainting strategy for generating high-quality multi-viewimages with consistency. We further propose visibility-aware adaptiverepainting strength for overlap regions to enhance the generated image qualityin the repainting process. The generated high-quality and multi-view consistentimages enable the use of simple Mean Square Error (MSE) loss for fast 3Dcontent generation. We conduct extensive experiments and show that our methodhas a superior ability to generate high-quality 3D content with multi-viewconsistency and fine textures in 2 minutes from scratch. Our webpage isavailable at https://junwuzhang19.github.io/repaint123/.</description><author>Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Munan Ning, Li Yuan</author><pubDate>Thu, 21 Dec 2023 14:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13271v2</guid></item><item><title>Capture the Flag: Uncovering Data Insights with Large Language Models</title><link>http://arxiv.org/abs/2312.13876v1</link><description>The extraction of a small number of relevant insights from vast amounts ofdata is a crucial component of data-driven decision-making. However,accomplishing this task requires considerable technical skills, domainexpertise, and human labor. This study explores the potential of using LargeLanguage Models (LLMs) to automate the discovery of insights in data,leveraging recent advances in reasoning and code generation techniques. Wepropose a new evaluation methodology based on a "capture the flag" principle,measuring the ability of such models to recognize meaningful and pertinentinformation (flags) in a dataset. We further propose two proof-of-conceptagents, with different inner workings, and compare their ability to capturesuch flags in a real-world sales dataset. While the work reported here ispreliminary, our results are sufficiently interesting to mandate futureexploration by the community.</description><author>Issam Laradji, Perouz Taslakian, Sai Rajeswar, Valentina Zantedeschi, Alexandre Lacoste, Nicolas Chapados, David Vazquez, Christopher Pal, Alexandre Drouin</author><pubDate>Thu, 21 Dec 2023 14:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13876v1</guid></item><item><title>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</title><link>http://arxiv.org/abs/2308.06668v3</link><description>The past decade has witnessed the rapid development of ML and DLmethodologies in agricultural systems, showcased by great successes in varietyof agricultural applications. However, these conventional ML/DL models havecertain limitations: They heavily rely on large, costly-to-acquire labeleddatasets for training, require specialized expertise for development andmaintenance, and are mostly tailored for specific tasks, thus lackinggeneralizability. Recently, foundation models have demonstrated remarkablesuccesses in language and vision tasks across various domains. These models aretrained on a vast amount of data from multiple domains and modalities. Oncetrained, they can accomplish versatile tasks with just minor fine-tuning andminimal task-specific labeled data. Despite their proven effectiveness and hugepotential, there has been little exploration of applying FMs to agriculturefields. Therefore, this study aims to explore the potential of FMs in the fieldof smart agriculture. In particular, we present conceptual tools and technicalbackground to facilitate the understanding of the problem space and uncover newresearch directions in this field. To this end, we first review recent FMs inthe general computer science domain and categorize them into four categories:language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs.Subsequently, we outline the process of developing agriculture FMs and discusstheir potential applications in smart agriculture. We also discuss the uniquechallenges associated with developing AFMs, including model training,validation, and deployment. Through this study, we contribute to theadvancement of AI in agriculture by introducing AFMs as a promising paradigmthat can significantly mitigate the reliance on extensive labeled datasets andenhance the efficiency, effectiveness, and generalization of agricultural AIsystems.</description><author>Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</author><pubDate>Thu, 21 Dec 2023 14:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06668v3</guid></item><item><title>Best Arm Identification in Batched Multi-armed Bandit Problems</title><link>http://arxiv.org/abs/2312.13875v1</link><description>Recently multi-armed bandit problem arises in many real-life scenarios wherearms must be sampled in batches, due to limited time the agent can wait for thefeedback. Such applications include biological experimentation and onlinemarketing. The problem is further complicated when the number of arms is largeand the number of batches is small. We consider pure exploration in a batchedmulti-armed bandit problem. We introduce a general linear programming frameworkthat can incorporate objectives of different theoretical settings in best armidentification. The linear program leads to a two-stage algorithm that canachieve good theoretical properties. We demonstrate by numerical studies thatthe algorithm also has good performance compared to certain UCB-type orThompson sampling methods.</description><author>Shengyu Cao, Simai He, Ruoqing Jiang, Jin Xu, Hongsong Yuan</author><pubDate>Thu, 21 Dec 2023 14:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13875v1</guid></item><item><title>Evaluating Task-oriented Dialogue Systems: A Systematic Review of Measures, Constructs and their Operationalisations</title><link>http://arxiv.org/abs/2312.13871v1</link><description>This review gives an extensive overview of evaluation methods fortask-oriented dialogue systems, paying special attention to practicalapplications of dialogue systems, for example for customer service. The review(1) provides an overview of the used constructs and metrics in previous work,(2) discusses challenges in the context of dialogue system evaluation and (3)develops a research agenda for the future of dialogue system evaluation. Weconducted a systematic review of four databases (ACL, ACM, IEEE and Web ofScience), which after screening resulted in 122 studies. Those studies werecarefully analysed for the constructs and methods they proposed for evaluation.We found a wide variety in both constructs and methods. Especially theoperationalisation is not always clearly reported. We hope that future workwill take a more critical approach to the operationalisation and specificationof the used constructs. To work towards this aim, this review ends withrecommendations for evaluation and suggestions for outstanding questions.</description><author>Anouck Braggaar, Christine Liebrecht, Emiel van Miltenburg, Emiel Krahmer</author><pubDate>Thu, 21 Dec 2023 14:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13871v1</guid></item><item><title>A General Recipe for the Analysis of Randomized Multi-Armed Bandit Algorithms</title><link>http://arxiv.org/abs/2303.06058v2</link><description>In this paper we propose a general methodology to derive regret bounds forrandomized multi-armed bandit algorithms. It consists in checking a set ofsufficient conditions on the sampling probability of each arm and on the familyof distributions to prove a logarithmic regret. As a direct application werevisit two famous bandit algorithms, Minimum Empirical Divergence (MED) andThompson Sampling (TS), under various models for the distributions includingsingle parameter exponential families, Gaussian distributions, boundeddistributions, or distributions satisfying some conditions on their moments. Inparticular, we prove that MED is asymptotically optimal for all these models,but also provide a simple regret analysis of some TS algorithms for which theoptimality is already known. We then further illustrate the interest of ourapproach, by analyzing a new Non-Parametric TS algorithm (h-NPTS), adapted tosome families of unbounded reward distributions with a bounded h-moment. Thismodel can for instance capture some non-parametric families of distributionswhose variance is upper bounded by a known constant.</description><author>Dorian Baudry, Kazuya Suzuki, Junya Honda</author><pubDate>Thu, 21 Dec 2023 14:11:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06058v2</guid></item><item><title>Data-driven path collective variables</title><link>http://arxiv.org/abs/2312.13868v1</link><description>Identifying optimal collective variables to model transformations, usingatomic-scale simulations, is a long-standing challenge. We propose a new methodfor the generation, optimization, and comparison of collective variables, whichcan be thought of as a data-driven generalization of the path collectivevariable concept. It consists in a kernel ridge regression of the committorprobability, which encodes a transformation's progress. The resultingcollective variable is one-dimensional, interpretable, and differentiable,making it appropriate for enhanced sampling simulations requiring biasing. Wedemonstrate the validity of the method on two different applications: aprecipitation model, and the association of Li$^+$ and F$^-$ in water. For theformer, we show that global descriptors such as the permutation invariantvector allow to reach an accuracy far from the one achieved \textit{via}simpler, more intuitive variables. For the latter, we show that informationcorrelated with the transformation mechanism is contained in the firstsolvation shell only, and that inertial effects prevent the derivation ofoptimal collective variables from the atomic positions only.</description><author>Arthur France-Lanord, Hadrien Vroylandt, Mathieu Salanne, Benjamin Rotenberg, A. Marco Saitta, Fabio Pietrucci</author><pubDate>Thu, 21 Dec 2023 14:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13868v1</guid></item></channel></rss>