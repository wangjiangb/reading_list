<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 29 Nov 2023 06:00:24 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Mission-driven Exploration for Accelerated Deep Reinforcement Learning with Temporal Logic Task Specifications</title><link>http://arxiv.org/abs/2311.17059v1</link><description>This paper addresses the problem of designing optimal control policies formobile robots with mission and safety requirements specified using LinearTemporal Logic (LTL). We consider robots with unknown stochastic dynamicsoperating in environments with unknown geometric structure. The robots areequipped with sensors allowing them to detect obstacles. Our goal is tosynthesize a control policy that maximizes the probability of satisfying anLTL-encoded task in the presence of motion and environmental uncertainty.Several deep reinforcement learning (DRL) algorithms have been proposedrecently to address similar problems. A common limitation in related works isthat of slow learning performance. In order to address this issue, we propose anovel DRL algorithm, which has the capability to learn control policies at anotably faster rate compared to similar methods. Its sample efficiency is dueto a mission-driven exploration strategy that prioritizes exploration towardsdirections that may contribute to mission accomplishment. Identifying thesedirections relies on an automaton representation of the LTL task as well as alearned neural network that (partially) models the unknown system dynamics. Weprovide comparative experiments demonstrating the efficiency of our algorithmon robot navigation tasks in unknown environments.</description><author>Jun Wang, Hosein Hasanbeig, Kaiyuan Tan, Zihe Sun, Yiannis Kantaros</author><pubDate>Tue, 28 Nov 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17059v1</guid></item><item><title>Material Palette: Extraction of Materials from a Single Image</title><link>http://arxiv.org/abs/2311.17060v1</link><description>In this paper, we propose a method to extract physically-based rendering(PBR) materials from a single real-world image. We do so in two steps: first,we map regions of the image to material concepts using a diffusion model, whichallows the sampling of texture images resembling each material in the scene.Second, we benefit from a separate network to decompose the generated texturesinto Spatially Varying BRDFs (SVBRDFs), providing us with materials ready to beused in rendering applications. Our approach builds on existing syntheticmaterial libraries with SVBRDF ground truth, but also exploits adiffusion-generated RGB texture dataset to allow generalization to new samplesusing unsupervised domain adaptation (UDA). Our contributions are thoroughlyevaluated on synthetic and real-world datasets. We further demonstrate theapplicability of our method for editing 3D scenes with materials estimated fromreal photographs. The code and models will be made open-source. Project page:https://astra-vision.github.io/MaterialPalette/</description><author>Ivan Lopes, Fabio Pizzati, Raoul de Charette</author><pubDate>Tue, 28 Nov 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17060v1</guid></item><item><title>HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting</title><link>http://arxiv.org/abs/2311.17061v1</link><description>Realistic 3D human generation from text prompts is a desirable yetchallenging task. Existing methods optimize 3D representations like mesh orneural fields via score distillation sampling (SDS), which suffers frominadequate fine details or excessive training time. In this paper, we proposean efficient yet effective framework, HumanGaussian, that generateshigh-quality 3D humans with fine-grained geometry and realistic appearance. Ourkey insight is that 3D Gaussian Splatting is an efficient renderer withperiodic Gaussian shrinkage or growing, where such adaptive density control canbe naturally guided by intrinsic human structures. Specifically, 1) we firstpropose a Structure-Aware SDS that simultaneously optimizes human appearanceand geometry. The multi-modal score function from both RGB and depth space isleveraged to distill the Gaussian densification and pruning process. 2)Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDSinto a noisier generative score and a cleaner classifier score, which welladdresses the over-saturation issue. The floating artifacts are furthereliminated based on Gaussian size in a prune-only phase to enhance generationsmoothness. Extensive experiments demonstrate the superior efficiency andcompetitive quality of our framework, rendering vivid 3D humans under diversescenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian</description><author>Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu</author><pubDate>Tue, 28 Nov 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17061v1</guid></item><item><title>Panoptic Video Scene Graph Generation</title><link>http://arxiv.org/abs/2311.17058v1</link><description>Towards building comprehensive real-world visual perception systems, wepropose and study a new problem called panoptic scene graph generation (PVSG).PVSG relates to the existing video scene graph generation (VidSGG) problem,which focuses on temporal interactions between humans and objects grounded withbounding boxes in videos. However, the limitation of bounding boxes indetecting non-rigid objects and backgrounds often causes VidSGG to miss keydetails crucial for comprehensive video understanding. In contrast, PVSGrequires nodes in scene graphs to be grounded by more precise, pixel-levelsegmentation masks, which facilitate holistic scene understanding. To advanceresearch in this new area, we contribute the PVSG dataset, which consists of400 videos (289 third-person + 111 egocentric videos) with a total of 150Kframes labeled with panoptic segmentation masks as well as fine, temporal scenegraphs. We also provide a variety of baseline methods and share useful designpractices for future work.</description><author>Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, Ziwei Liu</author><pubDate>Tue, 28 Nov 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17058v1</guid></item><item><title>ReMoS: Reactive 3D Motion Synthesis for Two-Person Interactions</title><link>http://arxiv.org/abs/2311.17057v1</link><description>Current approaches for 3D human motion synthesis can generate high-quality 3Danimations of digital humans performing a wide variety of actions and gestures.However, there is still a notable technological gap in addressing the complexdynamics of multi-human interactions within this paradigm. In this work, weintroduce ReMoS, a denoising diffusion-based probabilistic model for reactivemotion synthesis that explores two-person interactions. Given the motion of oneperson, we synthesize the reactive motion of the second person to complete theinteractions between the two. In addition to synthesizing the full-bodymotions, we also synthesize plausible hand interactions. We show theperformance of ReMoS under a wide range of challenging two-person scenariosincluding pair-dancing, Ninjutsu, kickboxing, and acrobatics, where oneperson's movements have complex and diverse influences on the motions of theother. We further propose the ReMoCap dataset for two-person interactionsconsisting of full-body and hand motions. We evaluate our approach throughmultiple quantitative metrics, qualitative visualizations, and a user study.Our results are usable in interactive applications while also providing anadequate amount of control for animators.</description><author>Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek</author><pubDate>Tue, 28 Nov 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17057v1</guid></item><item><title>Self-Supervised Motion Magnification by Backpropagating Through Optical Flow</title><link>http://arxiv.org/abs/2311.17056v1</link><description>This paper presents a simple, self-supervised method for magnifying subtlemotions in video: given an input video and a magnification factor, wemanipulate the video such that its new optical flow is scaled by the desiredamount. To train our model, we propose a loss function that estimates theoptical flow of the generated video and penalizes how far if deviates from thegiven magnification factor. Thus, training involves differentiating through apretrained optical flow network. Since our model is self-supervised, we canfurther improve its performance through test-time adaptation, by finetuning iton the input video. It can also be easily extended to magnify the motions ofonly user-selected objects. Our approach avoids the need for syntheticmagnification datasets that have been used to train prior learning-basedapproaches. Instead, it leverages the existing capabilities of off-the-shelfmotion estimators. We demonstrate the effectiveness of our method throughevaluations of both visual quality and quantitative metrics on a range ofreal-world and synthetic videos, and we show our method works for bothsupervised and unsupervised optical flow methods.</description><author>Zhaoying Pan, Daniel Geng, Andrew Owens</author><pubDate>Tue, 28 Nov 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17056v1</guid></item><item><title>Rethinking Directional Integration in Neural Radiance Fields</title><link>http://arxiv.org/abs/2311.16504v1</link><description>Recent works use the Neural radiance field (NeRF) to perform multi-view 3Dreconstruction, providing a significant leap in rendering photorealisticscenes. However, despite its efficacy, NeRF exhibits limited capability oflearning view-dependent effects compared to light field rendering orimage-based view synthesis. To that end, we introduce a modification to theNeRF rendering equation which is as simple as a few lines of code change forany NeRF variations, while greatly improving the rendering quality ofview-dependent effects. By swapping the integration operator and the directiondecoder network, we only integrate the positional features along the ray andmove the directional terms out of the integration, resulting in adisentanglement of the view-dependent and independent components. The modifiedequation is equivalent to the classical volumetric rendering in ideal cases onobject surfaces with Dirac densities. Furthermore, we prove that with theerrors caused by network approximation and numerical integration, our renderingequation exhibits better convergence properties with lower error accumulationscompared to the classical NeRF. We also show that the modified equation can beinterpreted as light field rendering with learned ray embeddings. Experimentson different NeRF variations show consistent improvements in the quality ofview-dependent effects with our simple modification.</description><author>Congyue Deng, Jiawei Yang, Leonidas Guibas, Yue Wang</author><pubDate>Tue, 28 Nov 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16504v1</guid></item><item><title>No Representation Rules Them All in Category Discovery</title><link>http://arxiv.org/abs/2311.17055v1</link><description>In this paper we tackle the problem of Generalized Category Discovery (GCD).Specifically, given a dataset with labelled and unlabelled images, the task isto cluster all images in the unlabelled subset, whether or not they belong tothe labelled categories. Our first contribution is to recognize that mostexisting GCD benchmarks only contain labels for a single clustering of thedata, making it difficult to ascertain whether models are using the availablelabels to solve the GCD task, or simply solving an unsupervised clusteringproblem. As such, we present a synthetic dataset, named 'Clevr-4', for categorydiscovery. Clevr-4 contains four equally valid partitions of the data, i.ebased on object shape, texture, color or count. To solve the task, models arerequired to extrapolate the taxonomy specified by the labelled set, rather thansimply latching onto a single natural grouping of the data. We use this datasetto demonstrate the limitations of unsupervised clustering in the GCD setting,showing that even very strong unsupervised models fail on Clevr-4. We furtheruse Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose anew method which addresses these shortcomings, leveraging consistent findingsfrom the representation learning literature to do so. Our simple solution,which is based on 'mean teachers' and termed $\mu$GCD, substantiallyoutperforms implemented baselines on Clevr-4. Finally, when we transfer thesefindings to real data on the challenging Semantic Shift Benchmark (SSB), wefind that $\mu$GCD outperforms all prior work, setting a new state-of-the-art.For the project webpage, see https://www.robots.ox.ac.uk/~vgg/data/clevr4/</description><author>Sagar Vaze, Andrea Vedaldi, Andrew Zisserman</author><pubDate>Tue, 28 Nov 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17055v1</guid></item><item><title>Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models</title><link>http://arxiv.org/abs/2306.03089v2</link><description>A long standing goal in neuroscience has been to elucidate the functionalorganization of the brain. Within higher visual cortex, functional accountshave remained relatively coarse, focusing on regions of interest (ROIs) andtaking the form of selectivity for broad categories such as faces, places,bodies, food, or words. Because the identification of such ROIs has typicallyrelied on manually assembled stimulus sets consisting of isolated objects innon-ecological contexts, exploring functional organization without robust apriori hypotheses has been challenging. To overcome these limitations, weintroduce a data-driven approach in which we synthesize images predicted toactivate a given brain region using paired natural images and fMRI recordings,bypassing the need for category-specific stimuli. Our approach -- BrainDiffusion for Visual Exploration ("BrainDiVE") -- builds on recent generativemethods by combining large-scale diffusion models with brain-guided imagesynthesis. Validating our method, we demonstrate the ability to synthesizepreferred images with appropriate semantic specificity for well-characterizedcategory-selective ROIs. We then show that BrainDiVE can characterizedifferences between ROIs selective for the same high-level category. Finally weidentify novel functional subdivisions within these ROIs, validated withbehavioral data. These results advance our understanding of the fine-grainedfunctional organization of human visual cortex, and provide well-specifiedconstraints for further examination of cortical organization usinghypothesis-driven methods.</description><author>Andrew F. Luo, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr</author><pubDate>Tue, 28 Nov 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03089v2</guid></item><item><title>Generative Social Choice</title><link>http://arxiv.org/abs/2309.01291v2</link><description>Traditionally, social choice theory has only been applicable to choices amonga few predetermined alternatives but not to more complex decisions such ascollectively selecting a textual statement. We introduce generative socialchoice, a framework that combines the mathematical rigor of social choicetheory with the capability of large language models to generate text andextrapolate preferences. This framework divides the design of AI-augmenteddemocratic processes into two components: first, proving that the processsatisfies rigorous representation guarantees when given access to oraclequeries; second, empirically validating that these queries can be approximatelyimplemented using a large language model. We apply this framework to theproblem of generating a slate of statements that is representative of opinionsexpressed as free-form text; specifically, we develop a democratic process withrepresentation guarantees and use this process to represent the opinions ofparticipants in a survey about chatbot personalization. We find that 93 out of100 participants feel "mostly" or "perfectly" represented by the slate of fivestatements we extracted.</description><author>Sara Fish, Paul Gölz, David C. Parkes, Ariel D. Procaccia, Gili Rusak, Itai Shapira, Manuel Wüthrich</author><pubDate>Tue, 28 Nov 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01291v2</guid></item><item><title>DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models</title><link>http://arxiv.org/abs/2311.17053v1</link><description>Nature evolves creatures with a high complexity of morphological andbehavioral intelligence, meanwhile computational methods lag in approachingthat diversity and efficacy. Co-optimization of artificial creatures'morphology and control in silico shows promise for applications in physicalsoft robotics and virtual character creation; such approaches, however, requiredeveloping new learning algorithms that can reason about function atop purestructure. In this paper, we present DiffuseBot, a physics-augmented diffusionmodel that generates soft robot morphologies capable of excelling in a widespectrum of tasks. DiffuseBot bridges the gap between virtually generatedcontent and physical utility by (i) augmenting the diffusion process with aphysical dynamical simulation which provides a certificate of performance, and(ii) introducing a co-design procedure that jointly optimizes physical designand control by leveraging information about physical sensitivities fromdifferentiable simulation. We showcase a range of simulated and fabricatedrobots along with their capabilities. Check our website athttps://diffusebot.github.io/</description><author>Tsun-Hsuan Wang, Juntian Zheng, Pingchuan Ma, Yilun Du, Byungchul Kim, Andrew Spielberg, Joshua Tenenbaum, Chuang Gan, Daniela Rus</author><pubDate>Tue, 28 Nov 2023 18:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17053v1</guid></item><item><title>Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models</title><link>http://arxiv.org/abs/2311.17050v1</link><description>In this paper, we present Surf-D, a novel method for generating high-quality3D shapes as Surfaces with arbitrary topologies using Diffusion models.Specifically, we adopt Unsigned Distance Field (UDF) as the surfacerepresentation, as it excels in handling arbitrary topologies, enabling thegeneration of complex shapes. While the prior methods explored shape generationwith different representations, they suffer from limited topologies andgeometry details. Moreover, it's non-trivial to directly extend prior diffusionmodels to UDF because they lack spatial continuity due to the discrete volumestructure. However, UDF requires accurate gradients for mesh extraction andlearning. To tackle the issues, we first leverage a point-based auto-encoder tolearn a compact latent space, which supports gradient querying for any inputpoint through differentiation to effectively capture intricate geometry at ahigh resolution. Since the learning difficulty for various shapes can differ, acurriculum learning strategy is employed to efficiently embed various surfaces,enhancing the whole embedding process. With pretrained shape latent space, weemploy a latent diffusion model to acquire the distribution of various shapes.Our approach demonstrates superior performance in shape generation acrossmultiple modalities and conducts extensive experiments in unconditionalgeneration, category conditional generation, 3D reconstruction from images, andtext-to-shape tasks.</description><author>Zhengming Yu, Zhiyang Dou, Xiaoxiao Long, Cheng Lin, Zekun Li, Yuan Liu, Norman Müller, Taku Komura, Marc Habermann, Christian Theobalt, Xin Li, Wenping Wang</author><pubDate>Tue, 28 Nov 2023 18:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17050v1</guid></item><item><title>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</title><link>http://arxiv.org/abs/2311.17049v1</link><description>Contrastive pretraining of image-text foundation models, such as CLIP,demonstrated excellent zero-shot performance and improved robustness on a widerange of downstream tasks. However, these models utilize largetransformer-based encoders with significant memory and latency overhead whichpose challenges for deployment on mobile devices. In this work, we introduceMobileCLIP -- a new family of efficient image-text models optimized for runtimeperformance along with a novel and efficient training approach, namelymulti-modal reinforced training. The proposed training approach leveragesknowledge transfer from an image captioning model and an ensemble of strongCLIP encoders to improve the accuracy of efficient models. Our approach avoidstrain-time compute overhead by storing the additional knowledge in a reinforceddataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff forzero-shot classification and retrieval tasks on several datasets. OurMobileCLIP-S2 variant is 2.3$\times$ faster while more accurate compared toprevious best CLIP model based on ViT-B/16. We further demonstrate theeffectiveness of our multi-modal reinforced training by training a CLIP modelbased on ViT-B/16 image backbone and achieving +2.9% average performanceimprovement on 38 evaluation benchmarks compared to the previous best.Moreover, we show that the proposed approach achieves 10$\times$-1000$\times$improved learning efficiency when compared with non-reinforced CLIP training.</description><author>Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel</author><pubDate>Tue, 28 Nov 2023 18:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17049v1</guid></item><item><title>Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions</title><link>http://arxiv.org/abs/2311.17048v1</link><description>Zero-shot referring expression comprehension aims at localizing boundingboxes in an image corresponding to the provided textual prompts, whichrequires: (i) a fine-grained disentanglement of complex visual scene andtextual context, and (ii) a capacity to understand relationships amongdisentangled entities. Unfortunately, existing large vision-language alignment(VLA) models, e.g., CLIP, struggle with both aspects so cannot be directly usedfor this task. To mitigate this gap, we leverage large foundation models todisentangle both images and texts into triplets in the format of (subject,predicate, object). After that, grounding is accomplished by calculating thestructural similarity matrix between visual and textual triplets with a VLAmodel, and subsequently propagate it to an instance-level similarity matrix.Furthermore, to equip VLA models with the ability of relationshipunderstanding, we design a triplet-matching objective to fine-tune the VLAmodels on a collection of curated dataset containing abundant entityrelationships. Experiments demonstrate that our visual grounding performanceincrease of up to 19.5% over the SOTA zero-shot model on RefCOCO/+/g. On themore challenging Who's Waldo dataset, our zero-shot approach achievescomparable accuracy to the fully supervised model.</description><author>Zeyu Han, Fangrui Zhu, Qianru Lao, Huaizu Jiang</author><pubDate>Tue, 28 Nov 2023 18:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17048v1</guid></item><item><title>LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</title><link>http://arxiv.org/abs/2311.17043v1</link><description>In this work, we present a novel method to tackle the token generationchallenge in Vision Language Models (VLMs) for video and image understanding,called LLaMA-VID. Current VLMs, while proficient in tasks like image captioningand visual question answering, face computational burdens when processing longvideos due to the excessive visual tokens. LLaMA-VID addresses this issue byrepresenting each frame with two distinct tokens, namely context token andcontent token. The context token encodes the overall image context based onuser input, whereas the content token encapsulates visual cues in each frame.This dual-token strategy significantly reduces the overload of long videoswhile preserving critical information. Generally, LLaMA-VID empowers existingframeworks to support hour-long videos and pushes their upper limit with anextra context token. It is proved to surpass previous methods on most of video-or image-based benchmarks. Code is availablehttps://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID</description><author>Yanwei Li, Chengyao Wang, Jiaya Jia</author><pubDate>Tue, 28 Nov 2023 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17043v1</guid></item><item><title>Adversarial Diffusion Distillation</title><link>http://arxiv.org/abs/2311.17042v1</link><description>We introduce Adversarial Diffusion Distillation (ADD), a novel trainingapproach that efficiently samples large-scale foundational image diffusionmodels in just 1-4 steps while maintaining high image quality. We use scoredistillation to leverage large-scale off-the-shelf image diffusion models as ateacher signal in combination with an adversarial loss to ensure high imagefidelity even in the low-step regime of one or two sampling steps. Our analysesshow that our model clearly outperforms existing few-step methods (GANs, LatentConsistency Models) in a single step and reaches the performance ofstate-of-the-art diffusion models (SDXL) in only four steps. ADD is the firstmethod to unlock single-step, real-time image synthesis with foundation models.Code and weights available underhttps://github.com/Stability-AI/generative-models andhttps://huggingface.co/stabilityai/ .</description><author>Axel Sauer, Dominik Lorenz, Andreas Blattmann, Robin Rombach</author><pubDate>Tue, 28 Nov 2023 18:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17042v1</guid></item><item><title>Efficient In-Context Learning in Vision-Language Models for Egocentric Videos</title><link>http://arxiv.org/abs/2311.17041v1</link><description>Recent advancements in text-only large language models (LLMs) havehighlighted the benefit of in-context learning for adapting to new tasks with afew demonstrations. However, extending in-context learning to largevision-language models (VLMs) using a huge amount of naturalisticvision-language data has shown limited success, particularly for egocentricvideos, due to high data collection costs. We propose a novel training method$\mathbb{E}$fficient $\mathbb{I}$n-context $\mathbb{L}$earning on$\mathbb{E}$gocentric $\mathbb{V}$ideos ($\mathbb{EILEV}$), which elicitsin-context learning in VLMs for egocentric videos without requiring massive,naturalistic egocentric video datasets. $\mathbb{EILEV}$ involves architecturaland training data adaptations to allow the model to process contextsinterleaved with video clips and narrations, sampling of in-context exampleswith clusters of similar verbs and nouns, use of data with skewed marginaldistributions with a long tail of infrequent verbs and nouns, as well ashomonyms and synonyms. Our evaluations show that $\mathbb{EILEV}$-trainedmodels outperform larger VLMs trained on a huge amount of naturalistic data inin-context learning. Furthermore, they can generalize to not onlyout-of-distribution, but also novel, rare egocentric videos and texts viain-context learning, demonstrating potential for applications requiringcost-effective training, and rapid post-deployment adaptability. Our code anddemo are available at \url{https://github.com/yukw777/EILEV}.</description><author>Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Joyce Chai</author><pubDate>Tue, 28 Nov 2023 18:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17041v1</guid></item><item><title>Scalable Extraction of Training Data from (Production) Language Models</title><link>http://arxiv.org/abs/2311.17035v1</link><description>This paper studies extractable memorization: training data that an adversarycan efficiently extract by querying a machine learning model without priorknowledge of the training dataset. We show an adversary can extract gigabytesof training data from open-source language models like Pythia or GPT-Neo,semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existingtechniques from the literature suffice to attack unaligned models; in order toattack the aligned ChatGPT, we develop a new divergence attack that causes themodel to diverge from its chatbot-style generations and emit training data at arate 150x higher than when behaving properly. Our methods show practicalattacks can recover far more data than previously thought, and reveal thatcurrent alignment techniques do not eliminate memorization.</description><author>Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee</author><pubDate>Tue, 28 Nov 2023 18:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17035v1</guid></item><item><title>Telling Left from Right: Identifying Geometry-Aware Semantic Correspondence</title><link>http://arxiv.org/abs/2311.17034v1</link><description>While pre-trained large-scale vision models have shown significant promisefor semantic correspondence, their features often struggle to grasp thegeometry and orientation of instances. This paper identifies the importance ofbeing geometry-aware for semantic correspondence and reveals a limitation ofthe features of current foundation models under simple post-processing. We showthat incorporating this information can markedly enhance semanticcorrespondence performance with simple but effective solutions in bothzero-shot and supervised settings. We also construct a new challengingbenchmark for semantic correspondence built from an existing animal poseestimation dataset, for both pre-training validating models. Our methodachieves a PCK@0.10 score of 64.2 (zero-shot) and 85.6 (supervised) on thechallenging SPair-71k dataset, outperforming the state-of-the-art by 4.3p and11.0p absolute gains, respectively. Our code and datasets will be publiclyavailable.</description><author>Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen, Varun Jampani, Deqing Sun, Ming-Hsuan Yang</author><pubDate>Tue, 28 Nov 2023 18:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17034v1</guid></item><item><title>Forward Gradients for Data-Driven CFD Wall Modeling</title><link>http://arxiv.org/abs/2311.11876v2</link><description>Computational Fluid Dynamics (CFD) is used in the design and optimization ofgas turbines and many other industrial/ scientific applications. However, thepractical use is often limited by the high computational cost, and the accurateresolution of near-wall flow is a significant contributor to this cost. Machinelearning (ML) and other data-driven methods can complement existing wallmodels. Nevertheless, training these models is bottlenecked by the largecomputational effort and memory footprint demanded by back-propagation. Recentwork has presented alternatives for computing gradients of neural networkswhere a separate forward and backward sweep is not needed and storage ofintermediate results between sweeps is not required because an unbiasedestimator for the gradient is computed in a single forward sweep. In thispaper, we discuss the application of this approach for training a subgrid wallmodel that could potentially be used as a surrogate in wall-bounded flow CFDsimulations to reduce the computational overhead while preserving predictiveaccuracy.</description><author>Jan Hückelheim, Tadbhagya Kumar, Krishnan Raghavan, Pinaki Pal</author><pubDate>Tue, 28 Nov 2023 18:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11876v2</guid></item><item><title>Edge Directionality Improves Learning on Heterophilic Graphs</title><link>http://arxiv.org/abs/2305.10498v3</link><description>Graph Neural Networks (GNNs) have become the de-facto standard tool formodeling relational data. However, while many real-world graphs are directed,the majority of today's GNN models discard this information altogether bysimply making the graph undirected. The reasons for this are historical: 1)many early variants of spectral GNNs explicitly required undirected graphs, and2) the first benchmarks on homophilic graphs did not find significant gain fromusing direction. In this paper, we show that in heterophilic settings, treatingthe graph as directed increases the effective homophily of the graph,suggesting a potential gain from the correct use of directionality information.To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novelgeneral framework for deep learning on directed graphs. Dir-GNN can be used toextend any Message Passing Neural Network (MPNN) to account for edgedirectionality information by performing separate aggregations of the incomingand outgoing edges. We prove that Dir-GNN matches the expressivity of theDirected Weisfeiler-Lehman test, exceeding that of conventional MPNNs. Inextensive experiments, we validate that while our framework leaves performanceunchanged on homophilic datasets, it leads to large gains over base models suchas GCN, GAT and GraphSage on heterophilic benchmarks, outperforming much morecomplex methods and achieving new state-of-the-art results.</description><author>Emanuele Rossi, Bertrand Charpentier, Francesco Di Giovanni, Fabrizio Frasca, Stephan Günnemann, Michael Bronstein</author><pubDate>Tue, 28 Nov 2023 18:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10498v3</guid></item><item><title>Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching</title><link>http://arxiv.org/abs/2311.17030v1</link><description>Mechanistic interpretability aims to understand model behaviors in terms ofspecific, interpretable features, often hypothesized to manifest aslow-dimensional subspaces of activations. Specifically, recent studies haveexplored subspace interventions (such as activation patching) as a way tosimultaneously manipulate model behavior and attribute the features behind itto given subspaces. In this work, we demonstrate that these two aims diverge, potentially leadingto an illusory sense of interpretability. Counterintuitively, even if asubspace intervention makes the model's output behave as if the value of afeature was changed, this effect may be achieved by activating a dormantparallel pathway leveraging another subspace that is causally disconnected frommodel outputs. We demonstrate this phenomenon in a distilled mathematicalexample, in two real-world domains (the indirect object identification task andfactual recall), and present evidence for its prevalence in practice. In thecontext of factual recall, we further show a link to rank-1 fact editing,providing a mechanistic explanation for previous work observing aninconsistency between fact editing performance and fact localization. However, this does not imply that activation patching of subspaces isintrinsically unfit for interpretability. To contextualize our findings, wealso show what a success case looks like in a task (indirect objectidentification) where prior manual circuit analysis informs an understanding ofthe location of a feature. We explore the additional evidence needed to arguethat a patched subspace is faithful.</description><author>Aleksandar Makelov, Georg Lange, Neel Nanda</author><pubDate>Tue, 28 Nov 2023 18:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17030v1</guid></item><item><title>H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing</title><link>http://arxiv.org/abs/2311.09312v2</link><description>Accurately modeling protein 3D structure is essential for the design offunctional proteins. An important sub-task of structure modeling is proteinside-chain packing: predicting the conformation of side-chains (rotamers) giventhe protein's backbone structure and amino-acid sequence. Conventionalapproaches for this task rely on expensive sampling procedures overhand-crafted energy functions and rotamer libraries. Recently, several deeplearning methods have been developed to tackle the problem in a data-drivenway, albeit with vastly different formulations (from image-to-image translationto directly predicting atomic coordinates). Here, we frame the problem as ajoint regression over the side-chains' true degrees of freedom: the dihedral$\chi$ angles. We carefully study possible objective functions for this task,while accounting for the underlying symmetries of the task. We proposeHolographic Packer (H-Packer), a novel two-stage algorithm for side-chainpacking built on top of two light-weight rotationally equivariant neuralnetworks. We evaluate our method on CASP13 and CASP14 targets. H-Packer iscomputationally efficient and shows favorable performance against conventionalphysics-based algorithms and is competitive against alternative deep learningsolutions.</description><author>Gian Marco Visani, William Galvin, Michael Neal Pun, Armita Nourmohammad</author><pubDate>Tue, 28 Nov 2023 18:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09312v2</guid></item><item><title>Adaptive Bayesian Learning with Action and State-Dependent Signal Variance</title><link>http://arxiv.org/abs/2311.12878v2</link><description>This manuscript presents an advanced framework for Bayesian learning byincorporating action and state-dependent signal variances into decision-makingmodels. This framework is pivotal in understanding complex data-feedback loopsand decision-making processes in various economic systems. Through a series ofexamples, we demonstrate the versatility of this approach in differentcontexts, ranging from simple Bayesian updating in stable environments tocomplex models involving social learning and state-dependent uncertainties. Thepaper uniquely contributes to the understanding of the nuanced interplaybetween data, actions, outcomes, and the inherent uncertainty in economicmodels.</description><author>Kaiwen Hou</author><pubDate>Tue, 28 Nov 2023 18:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12878v2</guid></item><item><title>When the Few Outweigh the Many: Illicit Content Recognition with Few-Shot Learning</title><link>http://arxiv.org/abs/2311.17026v1</link><description>The anonymity and untraceability benefits of the Dark web account for theexponentially-increased potential of its popularity while creating a suitablewomb for many illicit activities, to date. Hence, in collaboration withcybersecurity and law enforcement agencies, research has provided approachesfor recognizing and classifying illicit activities with most exploiting textualdark web markets' content recognition; few such approaches use images thatoriginated from dark web content. This paper investigates this alternativetechnique for recognizing illegal activities from images. In particular, weinvestigate label-agnostic learning techniques like One-Shot and Few-Shotlearning featuring the use Siamese neural networks, a state-of-the-art approachin the field. Our solution manages to handle small-scale datasets withpromising accuracy. In particular, Siamese neural networks reach 90.9% on20-Shot experiments over a 10-class dataset; this leads us to conclude thatsuch models are a promising and cheaper alternative to the definition ofautomated law-enforcing machinery over the dark web.</description><author>G. Cascavilla, G. Catolino, M. Conti, D. Mellios, D. A. Tamburri</author><pubDate>Tue, 28 Nov 2023 18:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17026v1</guid></item><item><title>A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)</title><link>http://arxiv.org/abs/2310.04438v2</link><description>This paper presents a comprehensive exploration of the evolution of promptengineering and generation in the field of natural language processing (NLP).Starting from the early language models and information retrieval systems, wetrace the key developments that have shaped prompt engineering over the years.The introduction of attention mechanisms in 2015 revolutionized languageunderstanding, leading to advancements in controllability andcontext-awareness. Subsequent breakthroughs in reinforcement learningtechniques further enhanced prompt engineering, addressing issues like exposurebias and biases in generated text. We examine the significant contributions in2018 and 2019, focusing on fine-tuning strategies, control codes, andtemplate-based generation. The paper also discusses the growing importance offairness, human-AI collaboration, and low-resource adaptation. In 2020 and2021, contextual prompting and transfer learning gained prominence, while 2022and 2023 witnessed the emergence of advanced techniques like unsupervisedpre-training and novel reward shaping. Throughout the paper, we referencespecific research studies that exemplify the impact of various developments onprompt engineering. The journey of prompt engineering continues, with ethicalconsiderations being paramount for the responsible and inclusive future of AIsystems.</description><author>Golam Md Muktadir</author><pubDate>Tue, 28 Nov 2023 18:27:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04438v2</guid></item><item><title>Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features</title><link>http://arxiv.org/abs/2311.17024v1</link><description>We present Diff3F as a simple, robust, and class-agnostic feature descriptorthat can be computed for untextured input shapes (meshes or point clouds). Ourmethod distills diffusion features from image foundational models onto inputshapes. Specifically, we use the input shapes to produce depth and normal mapsas guidance for conditional image synthesis, and in the process produce(diffusion) features in 2D that we subsequently lift and aggregate on theoriginal surface. Our key observation is that even if the conditional imagegenerations obtained from multi-view rendering of the input shapes areinconsistent, the associated image features are robust and can be directlyaggregated across views. This produces semantic features on the input shapes,without requiring additional data or training. We perform extensive experimentson multiple benchmarks (SHREC'19, SHREC'20, and TOSCA) and demonstrate that ourfeatures, being semantic instead of geometric, produce reliable correspondenceacross both isometeric and non-isometrically related shape families.</description><author>Niladri Shekhar Dutt, Sanjeev Muralikrishnan, Niloy J. Mitra</author><pubDate>Tue, 28 Nov 2023 18:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17024v1</guid></item><item><title>People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection</title><link>http://arxiv.org/abs/2311.01270v2</link><description>NLP models are used in a variety of critical social computing tasks, such asdetecting sexist, racist, or otherwise hateful content. Therefore, it isimperative that these models are robust to spurious features. Past work hasattempted to tackle such spurious features using training data augmentation,including Counterfactually Augmented Data (CADs). CADs introduce minimalchanges to existing training data points and flip their labels; training onthem may reduce model dependency on spurious features. However, manuallygenerating CADs can be time-consuming and expensive. Hence in this work, weassess if this task can be automated using generative NLP models. Weautomatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluatetheir usefulness in improving model robustness compared to manually-generatedCADs. By testing both model performance on multiple out-of-domain test sets andindividual data point efficacy, our results show that while manual CADs arestill the most effective, CADs generated by ChatGPT come a close second. Onekey reason for the lower performance of automated methods is that the changesthey introduce are often insufficient to flip the original label.</description><author>Indira Sen, Dennis Assenmacher, Mattia Samory, Isabelle Augenstein, Wil van der Aalst, Claudia Wagner</author><pubDate>Tue, 28 Nov 2023 18:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01270v2</guid></item><item><title>Towards Responsible Governance of Biological Design Tools</title><link>http://arxiv.org/abs/2311.15936v2</link><description>Recent advancements in generative machine learning have enabled rapidprogress in biological design tools (BDTs) such as protein structure andsequence prediction models. The unprecedented predictive accuracy and noveldesign capabilities of BDTs present new and significant dual-use risks. Forexample, their predictive accuracy allows biological agents, whether vaccinesor pathogens, to be developed more quickly, while the design capabilities couldbe used to discover drugs or evade DNA screening techniques. Similar to otherdual-use AI systems, BDTs present a wicked problem: how can regulators upholdpublic safety without stifling innovation? We highlight how current regulatoryproposals that are primarily tailored toward large language models may be lesseffective for BDTs, which require fewer computational resources to train andare often developed in an open-source manner. We propose a range of measures tomitigate the risk that BDTs are misused, across the areas of responsibledevelopment, risk assessment, transparency, access management, cybersecurity,and investing in resilience. Implementing such measures will require closecoordination between developers and governments.</description><author>Richard Moulange, Max Langenkamp, Tessa Alexanian, Samuel Curtis, Morgan Livingston</author><pubDate>Tue, 28 Nov 2023 18:22:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15936v2</guid></item><item><title>Optimal Categorical Instrumental Variables</title><link>http://arxiv.org/abs/2311.17021v1</link><description>This paper discusses estimation with a categorical instrumental variable insettings with potentially few observations per category. The proposedcategorical instrumental variable estimator (CIV) leverages a regularizationassumption that implies existence of a latent categorical variable with fixedfinite support achieving the same first stage fit as the observed instrument.In asymptotic regimes that allow the number of observations per category togrow at arbitrary small polynomial rate with the sample size, I show that whenthe cardinality of the support of the optimal instrument is known, CIV isroot-n asymptotically normal, achieves the same asymptotic variance as theoracle IV estimator that presumes knowledge of the optimal instrument, and issemiparametrically efficient under homoskedasticity. Under-specifying thenumber of support points reduces efficiency but maintains asymptotic normality.</description><author>Thomas Wiemann</author><pubDate>Tue, 28 Nov 2023 18:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17021v1</guid></item><item><title>Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models</title><link>http://arxiv.org/abs/2311.16103v2</link><description>Video-based large language models (Video-LLMs) have been recently introduced,targeting both fundamental improvements in perception and comprehension, and adiverse range of user inquiries. In pursuit of the ultimate goal of achievingartificial general intelligence, a truly intelligent Video-LLM model should notonly see and understand the surroundings, but also possess human-levelcommonsense, and make well-informed decisions for the users. To guide thedevelopment of such a model, the establishment of a robust and comprehensiveevaluation system becomes crucial. To this end, this paper proposes\textit{Video-Bench}, a new comprehensive benchmark along with a toolkitspecifically designed for evaluating Video-LLMs. The benchmark comprises 10meticulously crafted tasks, evaluating the capabilities of Video-LLMs acrossthree distinct levels: Video-exclusive Understanding, Prior Knowledge-basedQuestion-Answering, and Comprehension and Decision-making. In addition, weintroduce an automatic toolkit tailored to process model outputs for varioustasks, facilitating the calculation of metrics and generating convenient finalscores. We evaluate 8 representative Video-LLMs using \textit{Video-Bench}. Thefindings reveal that current Video-LLMs still fall considerably short ofachieving human-like comprehension and analysis of real-world videos, offeringvaluable insights for future research directions. The benchmark and toolkit areavailable at: \url{https://github.com/PKU-YuanGroup/Video-Bench}.</description><author>Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, Li Yuan</author><pubDate>Tue, 28 Nov 2023 18:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16103v2</guid></item><item><title>Foundational Moral Values for AI Alignment</title><link>http://arxiv.org/abs/2311.17017v1</link><description>Solving the AI alignment problem requires having clear, defensible valuestowards which AI systems can align. Currently, targets for alignment remainunderspecified and do not seem to be built from a philosophically robuststructure. We begin the discussion of this problem by presenting five core,foundational values, drawn from moral philosophy and built on the requisitesfor human existence: survival, sustainable intergenerational existence,society, education, and truth. We show that these values not only provide aclearer direction for technical alignment work, but also serve as a frameworkto highlight threats and opportunities from AI systems to both obtain andsustain these values.</description><author>Betty Li Hou, Brian Patrick Green</author><pubDate>Tue, 28 Nov 2023 18:11:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17017v1</guid></item><item><title>Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer</title><link>http://arxiv.org/abs/2311.17009v1</link><description>We present a new method for text-driven motion transfer - synthesizing avideo that complies with an input text prompt describing the target objects andscene while maintaining an input video's motion and scene layout. Prior methodsare confined to transferring motion across two subjects within the same orclosely related object categories and are applicable for limited domains (e.g.,humans). In this work, we consider a significantly more challenging setting inwhich the target and source objects differ drastically in shape andfine-grained motion characteristics (e.g., translating a jumping dog into adolphin). To this end, we leverage a pre-trained and fixed text-to-videodiffusion model, which provides us with generative and motion priors. Thepillar of our method is a new space-time feature loss derived directly from themodel. This loss guides the generation process to preserve the overall motionof the input video while complying with the target object in terms of shape andfine-grained motion traits.</description><author>Danah Yatim, Rafail Fridman, Omer Bar Tal, Yoni Kasten, Tali Dekel</author><pubDate>Tue, 28 Nov 2023 18:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17009v1</guid></item><item><title>An Investigation of Time Reversal Symmetry in Reinforcement Learning</title><link>http://arxiv.org/abs/2311.17008v1</link><description>One of the fundamental challenges associated with reinforcement learning (RL)is that collecting sufficient data can be both time-consuming and expensive. Inthis paper, we formalize a concept of time reversal symmetry in a Markovdecision process (MDP), which builds upon the established structure ofdynamically reversible Markov chains (DRMCs) and time-reversibility inclassical physics. Specifically, we investigate the utility of this concept inreducing the sample complexity of reinforcement learning. We observe thatutilizing the structure of time reversal in an MDP allows every environmenttransition experienced by an agent to be transformed into a feasiblereverse-time transition, effectively doubling the number of experiences in theenvironment. To test the usefulness of this newly synthesized data, we developa novel approach called time symmetric data augmentation (TSDA) and investigateits application in both proprioceptive and pixel-based state within the realmof off-policy, model-free RL. Empirical evaluations showcase how thesesynthetic transitions can enhance the sample efficiency of RL agents in timereversible scenarios without friction or contact. We also test this method inmore realistic environments where these assumptions are not globally satisfied.We find that TSDA can significantly degrade sample efficiency and policyperformance, but can also improve sample efficiency under the right conditions.Ultimately we conclude that time symmetry shows promise in enhancing the sampleefficiency of reinforcement learning and provide guidance when the environmentand reward structures are of an appropriate form for TSDA to be employedeffectively.</description><author>Brett Barkley, Amy Zhang, David Fridovich-Keil</author><pubDate>Tue, 28 Nov 2023 18:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17008v1</guid></item><item><title>Computational Hypergraph Discovery, a Gaussian Process framework for connecting the dots</title><link>http://arxiv.org/abs/2311.17007v1</link><description>Most scientific challenges can be framed into one of the following threelevels of complexity of function approximation. Type 1: Approximate an unknownfunction given input/output data. Type 2: Consider a collection of variablesand functions, some of which are unknown, indexed by the nodes and hyperedgesof a hypergraph (a generalized graph where edges can connect more than twovertices). Given partial observations of the variables of the hypergraph(satisfying the functional dependencies imposed by its structure), approximateall the unobserved variables and unknown functions. Type 3: Expanding on Type2, if the hypergraph structure itself is unknown, use partial observations ofthe variables of the hypergraph to discover its structure and approximate itsunknown functions. While most Computational Science and Engineering andScientific Machine Learning challenges can be framed as Type 1 and Type 2problems, many scientific problems can only be categorized as Type 3. Despitetheir prevalence, these Type 3 challenges have been largely overlooked due totheir inherent complexity. Although Gaussian Process (GP) methods are sometimesperceived as well-founded but old technology limited to Type 1 curve fitting,their scope has recently been expanded to Type 2 problems. In this paper, weintroduce an interpretable GP framework for Type 3 problems, targeting thedata-driven discovery and completion of computational hypergraphs. Our approachis based on a kernel generalization of Row Echelon Form reduction from linearsystems to nonlinear ones and variance-based analysis. Here, variables arelinked via GPs and those contributing to the highest data variance unveil thehypergraph's structure. We illustrate the scope and efficiency of the proposedapproach with applications to (algebraic) equation discovery, network discovery(gene pathways, chemical, and mechanical) and raw data analysis.</description><author>Théo Bourdais, Pau Batlle, Xianjin Yang, Ricardo Baptista, Nicolas Rouquette, Houman Owhadi</author><pubDate>Tue, 28 Nov 2023 18:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17007v1</guid></item><item><title>On the Impact of Sampling on Deep Sequential State Estimation</title><link>http://arxiv.org/abs/2311.17006v1</link><description>State inference and parameter learning in sequential models can besuccessfully performed with approximation techniques that maximize the evidencelower bound to the marginal log-likelihood of the data distribution. Thesemethods may be referred to as Dynamical Variational Autoencoders, and ourspecific focus lies on the deep Kalman filter. It has been shown that the ELBOobjective can oversimplify data representations, potentially compromisingestimation quality. Tighter Monte Carlo objectives have been proposed in theliterature to enhance generative modeling performance. For instance, the IWAEobjective uses importance weights to reduce the variance of marginallog-likelihood estimates. In this paper, importance sampling is applied to theDKF framework for learning deep Markov models, resulting in the IW-DKF, whichshows an improvement in terms of log-likelihood estimates and KL divergencebetween the variational distribution and the transition model. The frameworkusing the sampled DKF update rule is also accommodated to address sequentialstate and parameter estimation when working with highly non-linearphysics-based models. An experiment with the 3-space Lorenz attractor shows anenhanced generative modeling performance and also a decrease in RMSE whenestimating the model parameters and latent states, indicating that tighter MCOslead to improved state inference performance.</description><author>Helena Calatrava, Ricardo Augusto Borsoi, Tales Imbiriba, Pau Closas</author><pubDate>Tue, 28 Nov 2023 17:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17006v1</guid></item><item><title>MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</title><link>http://arxiv.org/abs/2311.17005v1</link><description>With the rapid development of Multi-modal Large Language Models (MLLMs), anumber of diagnostic benchmarks have recently emerged to evaluate thecomprehension capabilities of these models. However, most benchmarkspredominantly assess spatial understanding in the static image tasks, whileoverlooking temporal understanding in the dynamic video tasks. To alleviatethis issue, we introduce a comprehensive Multi-modal Video understandingBenchmark, namely MVBench, which covers 20 challenging video tasks that cannotbe effectively solved with a single frame. Specifically, we first introduce anovel static-to-dynamic method to define these temporal-related tasks. Bytransforming various static tasks into dynamic ones, we enable the systematicgeneration of video tasks that require a broad spectrum of temporal skills,ranging from perception to cognition. Then, guided by the task definition, weautomatically convert public video annotations into multiple-choice QA toevaluate each task. On one hand, such a distinct paradigm allows us to buildMVBench efficiently, without much manual intervention. On the other hand, itguarantees evaluation fairness with ground-truth video annotations, avoidingthe biased scoring of LLMs. Moreover, we further develop a robust video MLLMbaseline, i.e., VideoChat2, by progressive multi-modal training with diverseinstruction-tuning data. The extensive results on our MVBench reveal that, theexisting MLLMs are far from satisfactory in temporal understanding, while ourVideoChat2 largely surpasses these leading models by over 15% on MVBench. Allmodels and data are available at https://github.com/OpenGVLab/Ask-Anything.</description><author>Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao</author><pubDate>Tue, 28 Nov 2023 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17005v1</guid></item><item><title>Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following</title><link>http://arxiv.org/abs/2311.17002v1</link><description>Existing text-to-image (T2I) diffusion models usually struggle ininterpreting complex prompts, especially those with quantity, object-attributebinding, and multi-subject descriptions. In this work, we introduce a semanticpanel as the middleware in decoding texts to images, supporting the generatorto better follow instructions. The panel is obtained through arranging thevisual concepts parsed from the input text by the aid of large language models,and then injected into the denoising network as a detailed control signal tocomplement the text condition. To facilitate text-to-panel learning, we come upwith a carefully designed semantic formatting protocol, accompanied by afully-automatic data preparation pipeline. Thanks to such a design, ourapproach, which we call Ranni, manages to enhance a pre-trained T2I generatorregarding its textual controllability. More importantly, the introduction ofthe generative middleware brings a more convenient form of interaction (i.e.,directly adjusting the elements in the panel or using language instructions)and further allows users to finely customize their generation, based on whichwe develop a practical system and showcase its potential in continuousgeneration and chatting-based editing.</description><author>Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, Jingren Zhou</author><pubDate>Tue, 28 Nov 2023 17:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17002v1</guid></item><item><title>Goal-conditioned Offline Planning from Curious Exploration</title><link>http://arxiv.org/abs/2311.16996v1</link><description>Curiosity has established itself as a powerful exploration strategy in deepreinforcement learning. Notably, leveraging expected future novelty asintrinsic motivation has been shown to efficiently generate exploratorytrajectories, as well as a robust dynamics model. We consider the challenge ofextracting goal-conditioned behavior from the products of such unsupervisedexploration techniques, without any additional environment interaction. We findthat conventional goal-conditioned reinforcement learning approaches forextracting a value function and policy fall short in this difficult offlinesetting. By analyzing the geometry of optimal goal-conditioned value functions,we relate this issue to a specific class of estimation artifacts in learnedvalues. In order to mitigate their occurrence, we propose to combinemodel-based planning over learned value landscapes with a graph-based valueaggregation scheme. We show how this combination can correct both local andglobal artifacts, obtaining significant improvements in zero-shot goal-reachingperformance across diverse simulated environments.</description><author>Marco Bagatella, Georg Martius</author><pubDate>Tue, 28 Nov 2023 17:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16996v1</guid></item><item><title>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</title><link>http://arxiv.org/abs/2305.15347v2</link><description>Text-to-image diffusion models have made significant advances in generatingand editing high-quality images. As a result, numerous approaches have exploredthe ability of diffusion model features to understand and process single imagesfor downstream tasks, e.g., classification, semantic segmentation, andstylization. However, significantly less is known about what these featuresreveal across multiple, different images and objects. In this work, we exploitStable Diffusion (SD) features for semantic and dense correspondence anddiscover that with simple post-processing, SD features can performquantitatively similar to SOTA representations. Interestingly, the qualitativeanalysis reveals that SD features have very different properties compared toexisting representation learning features, such as the recently releasedDINOv2: while DINOv2 provides sparse but accurate matches, SD features providehigh-quality spatial information but sometimes inaccurate semantic matches. Wedemonstrate that a simple fusion of these two features works surprisingly well,and a zero-shot evaluation using nearest neighbors on these fused featuresprovides a significant performance gain over state-of-the-art methods onbenchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show thatthese correspondences can enable interesting applications such as instanceswapping in two images.</description><author>Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, Ming-Hsuan Yang</author><pubDate>Tue, 28 Nov 2023 17:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15347v2</guid></item><item><title>ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?</title><link>http://arxiv.org/abs/2311.16989v1</link><description>Upon its release in late 2022, ChatGPT has brought a seismic shift in theentire landscape of AI, both in research and commerce. Throughinstruction-tuning a large language model (LLM) with supervised fine-tuning andreinforcement learning from human feedback, it showed that a model could answerhuman questions and follow instructions on a broad panel of tasks. Followingthis success, interests in LLMs have intensified, with new LLMs flourishing atfrequent interval across academia and industry, including many start-upsfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic'sClaude) generally outperform their open-source counterparts, the progress onthe latter has been rapid with claims of achieving parity or even better oncertain tasks. This has crucial implications not only on research but also onbusiness. In this work, on the first anniversary of ChatGPT, we provide anexhaustive overview of this success, surveying all tasks where an open-sourceLLM has claimed to be on par or better than ChatGPT.</description><author>Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty</author><pubDate>Tue, 28 Nov 2023 17:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16989v1</guid></item><item><title>FedECA: A Federated External Control Arm Method for Causal Inference with Time-To-Event Data in Distributed Settings</title><link>http://arxiv.org/abs/2311.16984v1</link><description>External control arms (ECA) can inform the early clinical development ofexperimental drugs and provide efficacy evidence for regulatory approval innon-randomized settings. However, the main challenge of implementing ECA liesin accessing real-world data or historical clinical trials. Indeed, datasharing is often not feasible due to privacy considerations related to dataleaving the original collection centers, along with pharmaceutical companies'competitive motives. In this paper, we leverage a privacy-enhancing technologycalled federated learning (FL) to remove some of the barriers to data sharing.We introduce a federated learning inverse probability of treatment weighted(IPTW) method for time-to-event outcomes called FedECA which eases theimplementation of ECA by limiting patients' data exposure. We show withextensive experiments that FedECA outperforms its closest competitor,matching-adjusted indirect comparison (MAIC), in terms of statistical power andability to balance the treatment and control groups. To encourage the use ofsuch methods, we publicly release our code which relies on Substra, anopen-source FL software with proven experience in privacy-sensitive contexts.</description><author>Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Félix Balazard, Mathieu Andreux</author><pubDate>Tue, 28 Nov 2023 17:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16984v1</guid></item><item><title>AGI: Artificial General Intelligence for Education</title><link>http://arxiv.org/abs/2304.12479v4</link><description>Artificial general intelligence (AGI) has gained global recognition as afuture technology due to the emergence of breakthrough large language modelsand chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventionalAI models, typically designed for a limited range of tasks, demand significantamounts of domain-specific data for training and may not always considerintricate interpersonal dynamics in education. AGI, driven by the recent largepre-trained models, represents a significant leap in the capability of machinesto perform tasks that require human-level intelligence, such as reasoning,problem-solving, decision-making, and even understanding human emotions andsocial interactions. This position paper reviews AGI's key concepts,capabilities, scope, and potential within future education, including achievingfuture educational goals, designing pedagogy and curriculum, and performingassessments. It highlights that AGI can significantly improve intelligenttutoring systems, educational assessment, and evaluation procedures. AGIsystems can adapt to individual student needs, offering tailored learningexperiences. They can also provide comprehensive feedback on studentperformance and dynamically adjust teaching methods based on student progress.The paper emphasizes that AGI's capabilities extend to understanding humanemotions and social interactions, which are critical in educational settings.The paper discusses that ethical issues in education with AGI include databias, fairness, and privacy and emphasizes the need for codes of conduct toensure responsible AGI use in academic settings like homework, teaching, andrecruitment. We also conclude that the development of AGI necessitatesinterdisciplinary collaborations between educators and AI engineers to advanceresearch and application efforts.</description><author>Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, Xiaoming Zhai</author><pubDate>Tue, 28 Nov 2023 17:26:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12479v4</guid></item><item><title>Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models</title><link>http://arxiv.org/abs/2311.16978v1</link><description>Subject-verb agreement in the presence of an attractor noun located betweenthe main noun and the verb elicits complex behavior: judgments ofgrammaticality are modulated by the grammatical features of the attractor. Forexample, in the sentence "The girl near the boys likes climbing", the attractor(boys) disagrees in grammatical number with the verb (likes), creating alocally implausible transition probability. Here, we parametrically modulatethe distance between the attractor and the verb while keeping the length of thesentence equal. We evaluate the performance of both humans and two artificialneural network models: both make more mistakes when the attractor is closer tothe verb, but neural networks get close to the chance level while humans aremostly able to overcome the attractor interference. Additionally, we report alinear effect of attractor distance on reaction times. We hypothesize that apossible reason for the proximity effect is the calculation of transitionprobabilities between adjacent words. Nevertheless, classical models ofattraction such as the cue-based model might suffice to explain thisphenomenon, thus paving the way for new research. Data and analyses availableat https://osf.io/d4g6k</description><author>Christos-Nikolaos Zacharopoulos, Théo Desbordes, Mathias Sablé-Meyer</author><pubDate>Tue, 28 Nov 2023 17:25:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16978v1</guid></item><item><title>Bidirectional Reactive Programming for Machine Learning</title><link>http://arxiv.org/abs/2311.16977v1</link><description>Reactive languages are dedicated to the programming of systems which interactcontinuously and concurrently with their environment. Values take the form ofunbounded streams modeling the (discrete) passing of time or the sequence ofconcurrent interactions. While conventional reactivity models recurrencesforward in time, we introduce a symmetric reactive construct enabling backwardrecurrences. Constraints on the latter allow to make the implementationpractical. Machine Learning (ML) systems provide numerous motivations for allof this: we demonstrate that reverse-mode automatic differentiation,backpropagation, batch normalization, bidirectional recurrent neural networks,training and reinforcement learning algorithms, are all naturally captured asbidirectional reactive programs.</description><author>Dumitru Potop Butucaru, Albert Cohen, Gordon Plotkin, Hugo Pompougnac</author><pubDate>Tue, 28 Nov 2023 17:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16977v1</guid></item><item><title>Diffusion Models for Interferometric Satellite Aperture Radar</title><link>http://arxiv.org/abs/2308.16847v2</link><description>Probabilistic Diffusion Models (PDMs) have recently emerged as a verypromising class of generative models, achieving high performance in naturalimage generation. However, their performance relative to non-natural images,like radar-based satellite data, remains largely unknown. Generating largeamounts of synthetic (and especially labelled) satellite data is crucial toimplement deep-learning approaches for the processing and analysis of(interferometric) satellite aperture radar data. Here, we leverage PDMs togenerate several radar-based satellite image datasets. We show that PDMssucceed in generating images with complex and realistic structures, but thatsampling time remains an issue. Indeed, accelerated sampling strategies, whichwork well on simple image datasets like MNIST, fail on our radar datasets. Weprovide a simple and versatile open-sourcehttps://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample andevaluate PDMs using any dataset on a single GPU.</description><author>Alexandre Tuel, Thomas Kerdreux, Claudia Hulbert, Bertrand Rouet-Leduc</author><pubDate>Tue, 28 Nov 2023 17:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16847v2</guid></item><item><title>COLE: A Hierarchical Generation Framework for Graphic Design</title><link>http://arxiv.org/abs/2311.16974v1</link><description>Graphic design, which has been evolving since the 15th century, plays acrucial role in advertising. The creation of high-quality designs demandscreativity, innovation, and lateral thinking. This intricate task involvesunderstanding the objective, crafting visual elements such as the background,decoration, font, color, and shape, formulating diverse professional layouts,and adhering to fundamental visual design principles. In this paper, weintroduce COLE, a hierarchical generation framework designed to comprehensivelyaddress these challenges. This COLE system can transform a straightforwardintention prompt into a high-quality graphic design, while also supportingflexible editing based on user input. Examples of such input might includedirectives like ``design a poster for Hisaishi's concert.'' The key insight isto dissect the complex task of text-to-design generation into a hierarchy ofsimpler sub-tasks, each addressed by specialized models workingcollaboratively. The results from these models are then consolidated to producea cohesive final output. Our hierarchical task decomposition can streamline thecomplex process and significantly enhance generation reliability. Our COLEsystem consists of multiple fine-tuned Large Language Models (LLMs), LargeMultimodal Models (LMMs), and Diffusion Models (DMs), each specificallytailored for a design-aware text or image generation task. Furthermore, weconstruct the DESIGNERINTENTION benchmark to highlight the superiority of ourCOLE over existing methods in generating high-quality graphic designs from userintent. We perceive our COLE as an important step towards addressing morecomplex visual design generation tasks in the future.</description><author>Peidong Jia, Chenxuan Li, Zeyu Liu, Yichao Shen, Xingru Chen, Yuhui Yuan, Yinglin Zheng, Dong Chen, Ji Li, Xiaodong Xie, Shanghang Zhang, Baining Guo</author><pubDate>Tue, 28 Nov 2023 17:22:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16974v1</guid></item><item><title>Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results</title><link>http://arxiv.org/abs/2309.10399v2</link><description>We present a novel technique to discover and exploit weak causal signalsdirectly from images via neural networks for classification purposes. This way,we model how the presence of a feature in one part of the image affects theappearance of another feature in a different part of the image. Our methodconsists of a convolutional neural network backbone and a causality-factorsextractor module, which computes weights to enhance each feature map accordingto its causal influence in the scene. We developed different architecturevariants and empirically evaluated all of our models on two public datasets ofprostate MRI images and breast histopathology slides for cancer diagnosis. Toconfirm our quantitative results, we conduct ablation studies and investigatethe explainability of our models via class activation maps. Our findings showthat our lightweight block extracts meaningful information and improves theoverall classification, together with producing more robust predictions thatfocus on relevant parts of the image. That is crucial in medical imaging, whereaccurate and reliable classifications are essential for effective diagnosis andtreatment planning.</description><author>Gianluca Carloni, Sara Colantonio</author><pubDate>Tue, 28 Nov 2023 17:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10399v2</guid></item><item><title>Defining the boundaries: challenges and advances in identifying cells in microscopy images</title><link>http://arxiv.org/abs/2311.08269v2</link><description>Segmentation, or the outlining of objects within images, is a critical stepin the measurement and analysis of cells within microscopy images. Whileimprovements continue to be made in tools that rely on classical methods forsegmentation, deep learning-based tools increasingly dominate advances in thetechnology. Specialist models such as Cellpose continue to improve in accuracyand user-friendliness, and segmentation challenges such as the Multi-ModalityCell Segmentation Challenge continue to push innovation in accuracy acrosswidely-varying test data as well as efficiency and usability. Increasedattention on documentation, sharing, and evaluation standards are leading toincreased user-friendliness and acceleration towards the goal of a trulyuniversal method.</description><author>Nodar Gogoberidze, Beth A. Cimini</author><pubDate>Tue, 28 Nov 2023 17:18:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08269v2</guid></item><item><title>Natural Language Processing Through Transfer Learning: A Case Study on Sentiment Analysis</title><link>http://arxiv.org/abs/2311.16965v1</link><description>Artificial intelligence and machine learning have significantly bolstered thetechnological world. This paper explores the potential of transfer learning innatural language processing focusing mainly on sentiment analysis. The modelstrained on the big data can also be used where data are scarce. The claim isthat, compared to training models from scratch, transfer learning, usingpre-trained BERT models, can increase sentiment classification accuracy. Thestudy adopts a sophisticated experimental design that uses the IMDb dataset ofsentimentally labelled movie reviews. Pre-processing includes tokenization andencoding of text data, making it suitable for NLP models. The dataset is usedon a BERT based model, measuring its performance using accuracy. The resultcomes out to be 100 per cent accurate. Although the complete accuracy couldappear impressive, it might be the result of overfitting or a lack ofgeneralization. Further analysis is required to ensure the model's ability tohandle diverse and unseen data. The findings underscore the effectiveness oftransfer learning in NLP, showcasing its potential to excel in sentimentanalysis tasks. However, the research calls for a cautious interpretation ofperfect accuracy and emphasizes the need for additional measures to validatethe model's generalization.</description><author>Aman Yadav, Abhishek Vichare</author><pubDate>Tue, 28 Nov 2023 17:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16965v1</guid></item><item><title>Machine learning force-field models for metallic spin glass</title><link>http://arxiv.org/abs/2311.16964v1</link><description>Metallic spin glass systems, such as dilute magnetic alloys, arecharacterized by randomly distributed local moments coupled to each otherthrough a long-range electron-mediated effective interaction. We present ascalable machine learning (ML) framework for dynamical simulations of metallicspin glasses. A Behler-Parrinello type neural-network model, based on theprinciple of locality, is developed to accurately and efficiently predictelectron-induced local magnetic fields that drive the spin dynamics. A crucialcomponent of the ML model is a proper symmetry-invariant representation oflocal magnetic environment which is direct input to the neural net. We developsuch a magnetic descriptor by incorporating the spin degrees of freedom intothe atom-centered symmetry function methods which are widely used in MLforce-field models for quantum molecular dynamics. We apply our approach tostudy the relaxation dynamics of an amorphous generalization of the s-d model.Our work highlights the promising potential of ML models for large-scaledynamical modeling of itinerant magnets with quenched disorder.</description><author>Menglin Shi, Sheng Zhang, Gia-Wei Chern</author><pubDate>Tue, 28 Nov 2023 17:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16964v1</guid></item><item><title>HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion</title><link>http://arxiv.org/abs/2311.16961v1</link><description>Generating a 3D human model from a single reference image is challengingbecause it requires inferring textures and geometries in invisible views whilemaintaining consistency with the reference image. Previous methods utilizing 3Dgenerative models are limited by the availability of 3D training data.Optimization-based methods that lift text-to-image diffusion models to 3Dgeneration often fail to preserve the texture details of the reference image,resulting in inconsistent appearances in different views. In this paper, wepropose HumanRef, a 3D human generation framework from a single-view input. Toensure the generated 3D model is photorealistic and consistent with the inputimage, HumanRef introduces a novel method called reference-guided scoredistillation sampling (Ref-SDS), which effectively incorporates image guidanceinto the generation process. Furthermore, we introduce region-aware attentionto Ref-SDS, ensuring accurate correspondence between different body regions.Experimental results demonstrate that HumanRef outperforms state-of-the-artmethods in generating 3D clothed humans with fine geometry, photorealistictextures, and view-consistent appearances.</description><author>Jingbo Zhang, Xiaoyu Li, Qi Zhang, Yanpei Cao, Ying Shan, Jing Liao</author><pubDate>Tue, 28 Nov 2023 17:06:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16961v1</guid></item><item><title>From Simulations to Reality: Enhancing Multi-Robot Exploration for Urban Search and Rescue</title><link>http://arxiv.org/abs/2311.16958v1</link><description>In this study, we present a novel hybrid algorithm, combining Levy Flight(LF) and Particle Swarm Optimization (PSO) (LF-PSO), tailored for efficientmulti-robot exploration in unknown environments with limited communication andno global positioning information. The research addresses the growing interestin employing multiple autonomous robots for exploration tasks, particularly inscenarios such as Urban Search and Rescue (USAR) operations. Multiple robotsoffer advantages like increased task coverage, robustness, flexibility, andscalability. However, existing approaches often make assumptions such as searcharea, robot positioning, communication restrictions, and target informationthat may not hold in real-world situations. The hybrid algorithm leverages LF,known for its effectiveness in large space exploration with sparse targets, andincorporates inter-robot repulsion as a social component through PSO. Thiscombination enhances area exploration efficiency. We redefine the local bestand global best positions to suit scenarios without continuous targetinformation. Experimental simulations in a controlled environment demonstratethe algorithm's effectiveness, showcasing improved area coverage compared totraditional methods. In the process of refining our approach and testing it incomplex, obstacle-rich environments, the presented work holds promise forenhancing multi-robot exploration in scenarios with limited information andcommunication capabilities.</description><author>Gautam Siddharth Kashyap, Deepkashi Mahajan, Orchid Chetia Phukan, Ankit Kumar, Alexander E. I. Brownlee, Jiechao Gao</author><pubDate>Tue, 28 Nov 2023 17:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16958v1</guid></item><item><title>Adaptive Step Sizes for Preconditioned Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2311.16956v1</link><description>This paper proposes a novel approach to adaptive step sizes in stochasticgradient descent (SGD) by utilizing quantities that we have identified asnumerically traceable -- the Lipschitz constant for gradients and a concept ofthe local variance in search directions. Our findings yield a nearlyhyperparameter-free algorithm for stochastic optimization, which has provableconvergence properties when applied to quadratic problems and exhibits trulyproblem adaptive behavior on classical image classification tasks. Ourframework enables the potential inclusion of a preconditioner, thereby enablingthe implementation of adaptive step sizes for stochastic second-orderoptimization methods.</description><author>Frederik Köhne, Leonie Kreis, Anton Schiela, Roland Herzog</author><pubDate>Tue, 28 Nov 2023 17:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16956v1</guid></item><item><title>T-Rep: Representation Learning for Time Series using Time-Embeddings</title><link>http://arxiv.org/abs/2310.04486v2</link><description>Multivariate time series present challenges to standard machine learningtechniques, as they are often unlabeled, high dimensional, noisy, and containmissing data. To address this, we propose T-Rep, a self-supervised method tolearn time series representations at a timestep granularity. T-Rep learnsvector embeddings of time alongside its feature extractor, to extract temporalfeatures such as trend, periodicity, or distribution shifts from the signal.These time-embeddings are leveraged in pretext tasks, to incorporate smooth andfine-grained temporal dependencies in the representations, as well as reinforcerobustness to missing data. We evaluate T-Rep on downstream classification,forecasting, and anomaly detection tasks. It is compared to existingself-supervised algorithms for time series, which it outperforms in all threetasks. We test T-Rep in missing data regimes, where it proves more resilientthan its counterparts. Finally, we provide latent space visualisationexperiments, highlighting the interpretability of the learned representations.</description><author>Archibald Fraikin, Adrien Bennetot, Stéphanie Allassonnière</author><pubDate>Tue, 28 Nov 2023 17:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04486v2</guid></item><item><title>Antenna Response Consistency Driven Self-supervised Learning for WIFI-based Human Activity Recognition</title><link>http://arxiv.org/abs/2310.06328v3</link><description>Self-supervised learning (SSL) for WiFi-based human activity recognition(HAR) holds great promise due to its ability to address the challenge ofinsufficient labeled data. However, directly transplanting SSL algorithms,especially contrastive learning, originally designed for other domains to CSIdata, often fails to achieve the expected performance. We attribute this issueto the inappropriate alignment criteria, which disrupt the semantic distanceconsistency between the feature space and the input space. To address thischallenge, we introduce \textbf{A}ntenna \textbf{R}esponse \textbf{C}onsistency(ARC) as a solution to define proper alignment criteria. ARC is designed toretain semantic information from the input space while introducing robustnessto real-world noise. Moreover, we substantiate the effectiveness of ARC througha comprehensive set of experiments, demonstrating its capability to enhance theperformance of self-supervised learning for WiFi-based HAR by achieving anincrease of over 5\% in accuracy in most cases and achieving a best accuracy of94.97\%.</description><author>Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</author><pubDate>Tue, 28 Nov 2023 16:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06328v3</guid></item><item><title>Exploring Semantic Attributes from A Foundation Model for Federated Learning of Disjoint Label Spaces</title><link>http://arxiv.org/abs/2208.13465v2</link><description>Conventional centralised deep learning paradigms are not feasible when datafrom different sources cannot be shared due to data privacy or transmissionlimitation. To resolve this problem, federated learning has been introduced totransfer knowledge across multiple sources (clients) with non-shared data whileoptimising a globally generalised central model (server). Existing federatedlearning paradigms mostly focus on transferring holistic high-level knowledge(such as class) across models, which are closely related to specific objects ofinterest so may suffer from inverse attack. In contrast, in this work, weconsider transferring mid-level semantic knowledge (such as attribute) which isnot sensitive to specific objects of interest and therefore is moreprivacy-preserving and scalable. To this end, we formulate a new FederatedZero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge atmultiple local clients with non-shared local data and cumulatively aggregate aglobally generalised central model for deployment. To improve modeldiscriminative ability, we propose to explore semantic knowledge augmentationfrom external knowledge for enriching the mid-level semantic space in FZSL.Extensive experiments on five zeroshot learning benchmark datasets validate theeffectiveness of our approach for optimising a generalisable federated learningmodel with mid-level semantic knowledge transfer.</description><author>Shitong Sun, Chenyang Si, Guile Wu, Shaogang Gong</author><pubDate>Tue, 28 Nov 2023 16:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.13465v2</guid></item><item><title>UC-NeRF: Neural Radiance Field for Under-Calibrated multi-view cameras in autonomous driving</title><link>http://arxiv.org/abs/2311.16945v1</link><description>Multi-camera setups find widespread use across various applications, such asautonomous driving, as they greatly expand sensing capabilities. Despite thefast development of Neural radiance field (NeRF) techniques and their wideapplications in both indoor and outdoor scenes, applying NeRF to multi-camerasystems remains very challenging. This is primarily due to the inherentunder-calibration issues in multi-camera setup, including inconsistent imagingeffects stemming from separately calibrated image signal processing units indiverse cameras, and system errors arising from mechanical vibrations duringdriving that affect relative camera poses. In this paper, we present UC-NeRF, anovel method tailored for novel view synthesis in under-calibrated multi-viewcamera systems. Firstly, we propose a layer-based color correction to rectifythe color inconsistency in different image regions. Second, we propose virtualwarping to generate more viewpoint-diverse but color-consistent virtual viewsfor color correction and 3D recovery. Finally, a spatiotemporally constrainedpose refinement is designed for more robust and accurate pose calibration inmulti-camera systems. Our method not only achieves state-of-the-art performanceof novel view synthesis in multi-camera setups, but also effectivelyfacilitates depth estimation in large-scale outdoor scenes with the synthesizednovel views.</description><author>Kai Cheng, Xiaoxiao Long, Wei Yin, Jin Wang, Zhiqiang Wu, Yuexin Ma, Kaixuan Wang, Xiaozhi Chen, Xuejin Chen</author><pubDate>Tue, 28 Nov 2023 16:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16945v1</guid></item><item><title>Fantastic Generalization Measures are Nowhere to be Found</title><link>http://arxiv.org/abs/2309.13658v3</link><description>We study the notion of a generalization bound being uniformly tight, meaningthat the difference between the bound and the population loss is small for alllearning algorithms and all population distributions. Numerous generalizationbounds have been proposed in the literature as potential explanations for theability of neural networks to generalize in the overparameterized setting.However, in their paper ``Fantastic Generalization Measures and Where to FindThem,'' Jiang et al. (2020) examine more than a dozen generalization bounds,and show empirically that none of them are uniformly tight. This raises thequestion of whether uniformly-tight generalization bounds are at all possiblein the overparameterized setting. We consider two types of generalizationbounds: (1) bounds that may depend on the training set and the learnedhypothesis (e.g., margin bounds). We prove mathematically that no such boundcan be uniformly tight in the overparameterized setting; (2) bounds that may inaddition also depend on the learning algorithm (e.g., stability bounds). Forthese bounds, we show a trade-off between the algorithm's performance and thebound's tightness. Namely, if the algorithm achieves good accuracy on certaindistributions, then no generalization bound can be uniformly tight for it inthe overparameterized setting. We explain how these formal results can, in ourview, inform research on generalization bounds for neural networks, whilestressing that other interpretations of these results are also possible.</description><author>Michael Gastpar, Ido Nachum, Jonathan Shafer, Thomas Weinberger</author><pubDate>Tue, 28 Nov 2023 16:47:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13658v3</guid></item><item><title>Understanding Practices around Computational News Discovery Tools in the Domain of Science Journalism</title><link>http://arxiv.org/abs/2311.06864v2</link><description>Science and technology journalists today face challenges in findingnewsworthy leads due to increased workloads, reduced resources, and expandingscientific publishing ecosystems. Given this context, we explore computationalmethods to aid these journalists' news discovery in terms of time-efficiencyand agency. In particular, we prototyped three computational informationsubsidies into an interactive tool that we used as a probe to better understandhow such a tool may offer utility or more broadly shape the practices ofprofessional science journalists. Our findings highlight central considerationsaround science journalists' agency, context, and responsibilities that suchtools can influence and could account for in design. Based on this, we suggestdesign opportunities for greater and longer-term user agency; incorporatingcontextual, personal and collaborative notions of newsworthiness; andleveraging flexible interfaces and generative models. Overall, our findingscontribute a richer view of the sociotechnical system around computational newsdiscovery tools, and suggest ways to improve such tools to better support thepractices of science journalists.</description><author>Sachita Nishal, Jasmine Sinchai, Nicholas Diakopoulos</author><pubDate>Tue, 28 Nov 2023 16:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06864v2</guid></item><item><title>Image segmentation with traveling waves in an exactly solvable recurrent neural network</title><link>http://arxiv.org/abs/2311.16943v1</link><description>We study image segmentation using spatiotemporal dynamics in a recurrentneural network where the state of each unit is given by a complex number. Weshow that this network generates sophisticated spatiotemporal dynamics that caneffectively divide an image into groups according to a scene's structuralcharacteristics. Using an exact solution of the recurrent network's dynamics,we present a precise description of the mechanism underlying objectsegmentation in this network, providing a clear mathematical interpretation ofhow the network performs this task. We then demonstrate a simple algorithm forobject segmentation that generalizes across inputs ranging from simplegeometric objects in grayscale images to natural images. Object segmentationacross all images is accomplished with one recurrent neural network that has asingle, fixed set of weights. This demonstrates the expressive potential ofrecurrent neural networks when constructed using a mathematical approach thatbrings together their structure, dynamics, and computation.</description><author>Luisa H. B. Liboni, Roberto C. Budzinski, Alexandra N. Busch, Sindy Löwe, Thomas A. Keller, Max Welling, Lyle E. Muller</author><pubDate>Tue, 28 Nov 2023 16:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16943v1</guid></item><item><title>Debiasing Multimodal Models via Causal Information Minimization</title><link>http://arxiv.org/abs/2311.16941v1</link><description>Most existing debiasing methods for multimodal models, including causalintervention and inference methods, utilize approximate heuristics to representthe biases, such as shallow features from early stages of training or unimodalfeatures for multimodal tasks like VQA, etc., which may not be accurate. Inthis paper, we study bias arising from confounders in a causal graph formultimodal data and examine a novel approach that leverages causally-motivatedinformation minimization to learn the confounder representations. Robustpredictive features contain diverse information that helps a model generalizeto out-of-distribution data. Hence, minimizing the information content offeatures obtained from a pretrained biased model helps learn the simplestpredictive features that capture the underlying data distribution. We treatthese features as confounder representations and use them via methods motivatedby causal theory to remove bias from models. We find that the learnedconfounder representations indeed capture dataset biases, and the proposeddebiasing methods improve out-of-distribution (OOD) performance on multiplemultimodal datasets without sacrificing in-distribution performance.Additionally, we introduce a novel metric to quantify the sufficiency ofspurious features in models' predictions that further demonstrates theeffectiveness of our proposed methods. Our code is available at:https://github.com/Vaidehi99/CausalInfoMin</description><author>Vaidehi Patil, Adyasha Maharana, Mohit Bansal</author><pubDate>Tue, 28 Nov 2023 16:46:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16941v1</guid></item><item><title>360Roam: Real-Time Indoor Roaming Using Geometry-Aware 360$^\circ$ Radiance Fields</title><link>http://arxiv.org/abs/2208.02705v2</link><description>Virtual tour among sparse 360$^\circ$ images is widely used while hinderingsmooth and immersive roaming experiences. The emergence of Neural RadianceField (NeRF) has showcased significant progress in synthesizing novel views,unlocking the potential for immersive scene exploration. Nevertheless, previousNeRF works primarily focused on object-centric scenarios, resulting innoticeable performance degradation when applied to outward-facing andlarge-scale scenes due to limitations in scene parameterization. To achieveseamless and real-time indoor roaming, we propose a novel approach usinggeometry-aware radiance fields with adaptively assigned local radiance fields.Initially, we employ multiple 360$^\circ$ images of an indoor scene toprogressively reconstruct explicit geometry in the form of a probabilisticoccupancy map, derived from a global omnidirectional radiance field.Subsequently, we assign local radiance fields through an adaptivedivide-and-conquer strategy based on the recovered geometry. By incorporatinggeometry-aware sampling and decomposition of the global radiance field, oursystem effectively utilizes positional encoding and compact neural networks toenhance rendering quality and speed. Additionally, the extracted floorplan ofthe scene aids in providing visual guidance, contributing to a realisticroaming experience. To demonstrate the effectiveness of our system, we curateda diverse dataset of 360$^\circ$ images encompassing various real-life scenes,on which we conducted extensive experiments. Quantitative and qualitativecomparisons against baseline approaches illustrated the superior performance ofour system in large-scale indoor scene roaming.</description><author>Huajian Huang, Yingshu Chen, Tianjia Zhang, Sai-Kit Yeung</author><pubDate>Tue, 28 Nov 2023 16:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.02705v2</guid></item><item><title>Towards Attributions of Input Variables in a Coalition</title><link>http://arxiv.org/abs/2309.13411v2</link><description>This paper aims to develop a new attribution method to explain the conflictbetween individual variables' attributions and their coalition's attributionfrom a fully new perspective. First, we find that the Shapley value can bereformulated as the allocation of Harsanyi interactions encoded by the AImodel. Second, based the re-alloction of interactions, we extend the Shapleyvalue to the attribution of coalitions. Third we ective. We derive thefundamental mechanism behind the conflict. This conflict come from theinteraction containing partial variables in their coalition.</description><author>Xinhao Zheng, Huiqi Deng, Bo Fan, Quanshi Zhang</author><pubDate>Tue, 28 Nov 2023 16:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13411v2</guid></item><item><title>The Sky's the Limit: Re-lightable Outdoor Scenes via a Sky-pixel Constrained Illumination Prior and Outside-In Visibility</title><link>http://arxiv.org/abs/2311.16937v1</link><description>Inverse rendering of outdoor scenes from unconstrained image collections is achallenging task, particularly illumination/albedo ambiguities and occlusion ofthe illumination environment (shadowing) caused by geometry. However, there aremany cues in an image that can aid in the disentanglement of geometry, albedoand shadows. We exploit the fact that any sky pixel provides a directmeasurement of distant lighting in the corresponding direction and, via aneural illumination prior, a statistical cue as to the remaining illuminationenvironment. We also introduce a novel `outside-in' method for computingdifferentiable sky visibility based on a neural directional distance function.This is efficient and can be trained in parallel with the neural scenerepresentation, allowing gradients from appearance loss to flow from shadows toinfluence estimation of illumination and geometry. Our method estimateshigh-quality albedo, geometry, illumination and sky visibility, achievingstate-of-the-art results on the NeRF-OSR relighting benchmark. Our code andmodels can be found https://github.com/JADGardner/neusky</description><author>James A. D. Gardner, Evgenii Kashin, Bernhard Egger, William A. P. Smith</author><pubDate>Tue, 28 Nov 2023 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16937v1</guid></item><item><title>SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models</title><link>http://arxiv.org/abs/2311.16933v1</link><description>The development of text-to-video (T2V), i.e., generating videos with a giventext prompt, has been significantly advanced in recent years. However, relyingsolely on text prompts often results in ambiguous frame composition due tospatial uncertainty. The research community thus leverages the dense structuresignals, e.g., per-frame depth/edge sequences, to enhance controllability,whose collection accordingly increases the burden of inference. In this work,we present SparseCtrl to enable flexible structure control with temporallysparse signals, requiring only one or a few inputs, as shown in Figure 1. Itincorporates an additional condition encoder to process these sparse signalswhile leaving the pre-trained T2V model untouched. The proposed approach iscompatible with various modalities, including sketches, depth maps, and RGBimages, providing more practical control for video generation and promotingapplications such as storyboarding, depth rendering, keyframe animation, andinterpolation. Extensive experiments demonstrate the generalization ofSparseCtrl on both original and personalized T2V generators. Codes and modelswill be publicly available at https://guoyww.github.io/projects/SparseCtrl .</description><author>Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, Bo Dai</author><pubDate>Tue, 28 Nov 2023 16:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16933v1</guid></item><item><title>Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models</title><link>http://arxiv.org/abs/2310.03059v3</link><description>The popularity of pre-trained large models has revolutionized downstreamtasks across diverse fields, such as language, vision, and multi-modality. Tominimize the adaption cost for downstream tasks, many Parameter-EfficientFine-Tuning (PEFT) techniques are proposed for language and 2D imagepre-trained models. However, the specialized PEFT method for 3D pre-trainedmodels is still under-explored. To this end, we introduce Point-PEFT, a novelframework for adapting point cloud pre-trained models with minimal learnableparameters. Specifically, for a pre-trained 3D model, we freeze most of itsparameters, and only tune the newly added PEFT modules on downstream tasks,which consist of a Point-prior Prompt and a Geometry-aware Adapter. ThePoint-prior Prompt adopts a set of learnable prompt tokens, for which wepropose to construct a memory bank with domain-specific knowledge, and utilizea parameter-free attention to enhance the prompt tokens. The Geometry-awareAdapter aims to aggregate point cloud features within spatial neighborhoods tocapture fine-grained geometric information through local interactions.Extensive experiments indicate that our Point-PEFT can achieve betterperformance than the full fine-tuning on various downstream tasks, while usingonly 5% of the trainable parameters, demonstrating the efficiency andeffectiveness of our approach. Code will be released athttps://github.com/Even-JK/PEFT-3D.</description><author>Ivan Tang, Ray Zhang, Zoey Guo, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li</author><pubDate>Tue, 28 Nov 2023 16:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03059v3</guid></item><item><title>LLaFS: When Large-Language Models Meet Few-Shot Segmentation</title><link>http://arxiv.org/abs/2311.16926v1</link><description>This paper proposes LLaFS, the first attempt to leverage large languagemodels (LLMs) in few-shot segmentation. In contrast to the conventionalfew-shot segmentation methods that only rely on the limited and biasedinformation from the annotated support images, LLaFS leverages the vast priorknowledge gained by LLM as an effective supplement and directly uses the LLM tosegment images in a few-shot manner. To enable the text-based LLM to handleimage-related tasks, we carefully design an input instruction that allows theLLM to produce segmentation results represented as polygons, and propose aregion-attribute table to simulate the human visual mechanism and providemulti-modal guidance. We also synthesize pseudo samples and use curriculumlearning for pretraining to augment data and achieve better optimization. LLaFSachieves state-of-the-art results on multiple datasets, showing the potentialof using LLMs for few-shot computer vision tasks. Code will be available athttps://github.com/lanyunzhu99/LLaFS.</description><author>Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, Jun Liu</author><pubDate>Tue, 28 Nov 2023 16:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16926v1</guid></item><item><title>Super-Resolution through StyleGAN Regularized Latent Search: A Realism-Fidelity Trade-off</title><link>http://arxiv.org/abs/2311.16923v1</link><description>This paper addresses the problem of super-resolution: constructing a highlyresolved (HR) image from a low resolved (LR) one. Recent unsupervisedapproaches search the latent space of a StyleGAN pre-trained on HR images, forthe image that best downscales to the input LR image. However, they tend toproduce out-of-domain images and fail to accurately reconstruct HR images thatare far from the original domain. Our contribution is twofold. Firstly, weintroduce a new regularizer to constrain the search in the latent space,ensuring that the inverted code lies in the original image manifold. Secondly,we further enhanced the reconstruction through expanding the image prior aroundthe optimal latent code. Our results show that the proposed approach recoversrealistic high-quality images for large magnification factors. Furthermore, forlow magnification factors, it can still reconstruct details that the generatorcould not have produced otherwise. Altogether, our approach achieves a goodtrade-off between fidelity and realism for the super-resolution task.</description><author>Marzieh Gheisari, Auguste Genovesio</author><pubDate>Tue, 28 Nov 2023 16:27:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16923v1</guid></item><item><title>Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding</title><link>http://arxiv.org/abs/2311.16922v1</link><description>Large Vision-Language Models (LVLMs) have advanced considerably, intertwiningvisual recognition and language understanding to generate content that is notonly coherent but also contextually attuned. Despite their success, LVLMs stillsuffer from the issue of object hallucinations, where models generate plausibleyet incorrect outputs that include objects that do not exist in the images. Tomitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simpleand training-free method that contrasts output distributions derived fromoriginal and distorted visual inputs. The proposed VCD effectively reduces theover-reliance on statistical bias and unimodal priors, two essential causes ofobject hallucinations. This adjustment ensures the generated content is closelygrounded to visual inputs, resulting in contextually accurate outputs. Ourexperiments show that VCD, without either additional training or the usage ofexternal tools, significantly mitigates the object hallucination issue acrossdifferent LVLM families. Beyond mitigating object hallucinations, VCD alsoexcels in general LVLM benchmarks, highlighting its wide-ranging applicability.</description><author>Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing</author><pubDate>Tue, 28 Nov 2023 16:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16922v1</guid></item><item><title>Policy Learning with Asymmetric Counterfactual Utilities</title><link>http://arxiv.org/abs/2206.10479v3</link><description>Data-driven decision making plays an important role even in high stakessettings like medicine and public policy. Learning optimal policies fromobserved data requires a careful formulation of the utility function whoseexpected value is maximized across a population. Although researchers typicallyuse utilities that depend on observed outcomes alone, in many settings thedecision maker's utility function is more properly characterized by the jointset of potential outcomes under all actions. For example, the Hippocraticprinciple to "do no harm" implies that the cost of causing death to a patientwho would otherwise survive without treatment is greater than the cost offorgoing life-saving treatment. We consider optimal policy learning withasymmetric counterfactual utility functions of this form that consider thejoint set of potential outcomes. We show that asymmetric counterfactualutilities lead to an unidentifiable expected utility function, and so we firstpartially identify it. Drawing on statistical decision theory, we then deriveminimax decision rules by minimizing the maximum expected utility loss relativeto different alternative policies. We show that one can learn minimax lossdecision rules from observed data by solving intermediate classificationproblems, and establish that the finite sample excess expected utility loss ofthis procedure is bounded by the regret of these intermediate classifiers. Weapply this conceptual framework and methodology to the decision about whetheror not to use right heart catheterization for patients with possible pulmonaryhypertension.</description><author>Eli Ben-Michael, Kosuke Imai, Zhichao Jiang</author><pubDate>Tue, 28 Nov 2023 16:23:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10479v3</guid></item><item><title>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D</title><link>http://arxiv.org/abs/2311.16918v1</link><description>Lifting 2D diffusion for 3D generation is a challenging problem due to thelack of geometric prior and the complex entanglement of materials and lightingin natural images. Existing methods have shown promise by first creating thegeometry through score-distillation sampling (SDS) applied to rendered surfacenormals, followed by appearance modeling. However, relying on a 2D RGBdiffusion model to optimize surface normals is suboptimal due to thedistribution discrepancy between natural images and normals maps, leading toinstability in optimization. In this paper, recognizing that the normal anddepth information effectively describe scene geometry and be automaticallyestimated from images, we propose to learn a generalizable Normal-Depthdiffusion model for 3D generation. We achieve this by training on thelarge-scale LAION dataset together with the generalizable image-to-depth andnormal prior models. In an attempt to alleviate the mixed illumination effectsin the generated materials, we introduce an albedo diffusion model to imposedata-driven constraints on the albedo component. Our experiments show that whenintegrated into existing text-to-3D pipelines, our models significantly enhancethe detail richness, achieving state-of-the-art results. Our project page ishttps://lingtengqiu.github.io/RichDreamer/.</description><author>Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, Xiaoguang Han</author><pubDate>Tue, 28 Nov 2023 16:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16918v1</guid></item><item><title>Comparative Analysis of Shear Strength Prediction Models for Reinforced Concrete Slab-Column Connections</title><link>http://arxiv.org/abs/2311.12824v2</link><description>This research aims at comparative analysis of shear strength prediction atslab-column connection, unifying machine learning, design codes and FiniteElement Analysis. Current design codes (CDCs) of ACI 318-19 (ACI), Eurocode 2(EC2), Compressive Force Path (CFP) method, Feed Forward Neural Network (FNN)based Artificial Neural Network (ANN), PSO-based FNN (PSOFNN), and BATalgorithm-based BATFNN are used. The study is complemented with FEA of slab forvalidating the experimental results and machine learning predictions.In thecase of hybrid models of PSOFNN and BATFNN, mean square error is used as anobjective function to obtain the optimized values of the weights, that are usedby Feed Forward Neural Network to perform predictions on the slab data. Sevendifferent models of PSOFNN, BATFNN, and FNN are trained on this data and theresults exhibited that PSOFNN is the best model overall. PSOFNN has the bestresults for SCS=1 with highest value of R as 99.37% and lowest of MSE, and MAEvalues of 0.0275%, and 1.214% respectively which are better than the best FNNmodel for SCS=4 having the values of R, MSE, and MAE as 97.464%, 0.0492%, and1.43%, respectively.</description><author>Sarmed Wahab, Nasim Shakouri Mahmoudabadi, Sarmad Waqas, Nouman Herl, Muhammad Iqbal, Khurshid Alam, Afaq Ahmad</author><pubDate>Tue, 28 Nov 2023 16:22:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12824v2</guid></item><item><title>UGG: Unified Generative Grasping</title><link>http://arxiv.org/abs/2311.16917v1</link><description>Dexterous grasping aims to produce diverse grasping postures with a highgrasping success rate. Regression-based methods that directly predict graspingparameters given the object may achieve a high success rate but often lackdiversity. Generation-based methods that generate grasping postures conditionedon the object can often produce diverse grasping, but they are insufficient forhigh grasping success due to lack of discriminative information. To mitigate,we introduce a unified diffusion-based dexterous grasp generation model, dubbedthe name UGG, which operates within the object point cloud and hand parameterspaces. Our all-transformer architecture unifies the information from theobject, the hand, and the contacts, introducing a novel representation ofcontact points for improved contact modeling. The flexibility and quality ofour model enable the integration of a lightweight discriminator, benefitingfrom simulated discriminative data, which pushes for a high success rate whilepreserving high diversity. Beyond grasp generation, our model can also generateobjects based on hand information, offering valuable insights into objectdesign and studying how the generative model perceives objects. Our modelachieves state-of-the-art dexterous grasping on the large-scale DexGraspNetdataset while facilitating human-centric object design, marking a significantadvancement in dexterous grasping research. Our project page ishttps://jiaxin-lu.github.io/ugg/ .</description><author>Jiaxin Lu, Hao Kang, Haoxiang Li, Bo Liu, Yiding Yang, Qixing Huang, Gang Hua</author><pubDate>Tue, 28 Nov 2023 16:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16917v1</guid></item><item><title>Brain-ID: Learning Robust Feature Representations for Brain Imaging</title><link>http://arxiv.org/abs/2311.16914v1</link><description>Recent learning-based approaches have made astonishing advances in calibratedmedical imaging like computerized tomography, yet they struggle to generalizein uncalibrated modalities -- notoriously magnetic resonance imaging (MRI),where performance is highly sensitive to the differences in MR contrast,resolution, and orientation between the training and testing data. Thisprevents broad applicability to the diverse clinical acquisition protocols inthe real world. We introduce Brain-ID, a robust feature representation learningstrategy for brain imaging, which is contrast-agnostic, and robust to the brainanatomy of each subject regardless of the appearance of acquired images (i.e.,deformation, contrast, resolution, orientation, artifacts, etc). Brain-ID istrained entirely on synthetic data, and easily adapts to downstream tasks withour proposed simple one-layer solution. We validate the robustness of Brain-IDfeatures, and evaluate their performance in a variety of downstreamapplications, including both contrast-independent (anatomyreconstruction/contrast synthesis, brain segmentation), and contrast-dependent(super-resolution, bias field estimation) tasks. Extensive experiments on 6public datasets demonstrate that Brain-ID achieves state-of-the-art performancein all tasks, and more importantly, preserves its performance when only limitedtraining data is available.</description><author>Peirong Liu, Oula Puonti, Xiaoling Hu, Daniel C. Alexander, Juan Eugenio Iglesias</author><pubDate>Tue, 28 Nov 2023 16:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16914v1</guid></item><item><title>Multinomial belief networks</title><link>http://arxiv.org/abs/2311.16909v1</link><description>A Bayesian approach to machine learning is attractive when we need toquantify uncertainty, deal with missing observations, when samples are scarce,or when the data is sparse. All of these commonly apply when analysinghealthcare data. To address these analytical requirements, we propose a deepgenerative model for multinomial count data where both the weights and hiddenunits of the network are Dirichlet distributed. A Gibbs sampling procedure isformulated that takes advantage of a series of augmentation relations,analogous to the Zhou-Cong-Chen model. We apply the model on small handwrittendigits, and a large experimental dataset of DNA mutations in cancer, and weshow how the model is able to extract biologically meaningful meta-signaturesin a fully data-driven way.</description><author>H. C. Donker, D. Neijzen, G. A. Lunter</author><pubDate>Tue, 28 Nov 2023 16:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16909v1</guid></item><item><title>LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models</title><link>http://arxiv.org/abs/2310.08659v4</link><description>Quantization is an indispensable technique for serving Large Language Models(LLMs) and has recently found its way into LoRA fine-tuning. In this work wefocus on the scenario where quantization and LoRA fine-tuning are appliedtogether on a pre-trained model. In such cases it is common to observe aconsistent gap in the performance on downstream tasks between full fine-tuningand quantization plus LoRA fine-tuning approach. In response, we propose LoftQ(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework thatsimultaneously quantizes an LLM and finds a proper low-rank initialization forLoRA fine-tuning. Such an initialization alleviates the discrepancy between thequantized and full-precision model and significantly improves generalization indownstream tasks. We evaluate our method on natural language understanding,question answering, summarization, and natural language generation tasks.Experiments show that our method is highly effective and outperforms existingquantization methods, especially in the challenging 2-bit and 2/4-bit mixedprecision regimes. The code is available on https://github.com/yxli2123/LoftQ.</description><author>Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo Zhao</author><pubDate>Tue, 28 Nov 2023 16:06:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08659v4</guid></item><item><title>Strategyproof and Proportionally Fair Facility Location</title><link>http://arxiv.org/abs/2111.01566v3</link><description>We focus on a simple, one-dimensional collective decision problem (oftenreferred to as the facility location problem) and explore issues ofstrategyproofness and proportionality-based fairness. We introduce and analyzea hierarchy of proportionality-based fairness axioms of varying strength:Individual Fair Share (IFS), Unanimous Fair Share (UFS), Proportionality (as inFreeman et al, 2021), and Proportional Fairness (PF). For each axiom, wecharacterize the family of mechanisms that satisfy the axiom andstrategyproofness. We show that imposing strategyproofness renders many of theaxioms to be equivalent: the family of mechanisms that satisfy proportionality,unanimity, and strategyproofness is equivalent to the family of mechanisms thatsatisfy UFS and strategyproofness, which, in turn, is equivalent to the familyof mechanisms that satisfy PF and strategyproofness. Furthermore, there is aunique such mechanism: the Uniform Phantom mechanism, which is studied inFreeman et al. (2021). We also characterize the outcomes of the Uniform Phantommechanism as the unique (pure) equilibrium outcome for any mechanism thatsatisfies continuity, strict monotonicity, and UFS. Finally, we analyze theapproximation guarantees, in terms of optimal social welfare and minimum totalcost, obtained by mechanisms that are strategyproof and satisfy eachproportionality-based fairness axiom. We show that the Uniform Phantommechanism provides the best approximation of the optimal social welfare (andalso minimum total cost) among all mechanisms that satisfy UFS.</description><author>Haris Aziz, Alexander Lam, Barton E. Lee, Toby Walsh</author><pubDate>Tue, 28 Nov 2023 16:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.01566v3</guid></item><item><title>Lane-Keeping Control of Autonomous Vehicles Through a Soft-Constrained Iterative LQR</title><link>http://arxiv.org/abs/2311.16900v1</link><description>The accurate prediction of smooth steering inputs is crucial for autonomousvehicle applications because control actions with jitter might cause thevehicle system to become unstable. To address this problem in automobilelane-keeping control without the use of additional smoothing algorithms, wedeveloped a soft-constrained iterative linear-quadratic regulator (soft-CILQR)algorithm by integrating CILQR algorithm and a model predictive control (MPC)constraint relaxation method. We incorporated slack variables into the stateand control barrier functions of the soft-CILQR solver to soften theconstraints in the optimization process so that stabilizing control inputs canbe calculated in a relatively simple manner. Two types of automotivelane-keeping experiments were conducted with a linear system dynamics model totest the performance of the proposed soft-CILQR algorithm and to compare itsperformance with that of the CILQR algorithm: numerical simulations andexperiments involving challenging vision-based maneuvers. In the numericalsimulations, the soft-CILQR and CILQR solvers managed to drive the systemtoward the reference state asymptotically; however, the soft-CILQR solverobtained smooth steering input trajectories more easily than did the CILQRsolver under conditions involving additive disturbances. In the experimentswith visual inputs, the soft-CILQR controller outperformed the CILQR controllerin terms of tracking accuracy and steering smoothness during the driving of anego vehicle on TORCS.</description><author>Der-Hau Lee</author><pubDate>Tue, 28 Nov 2023 15:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16900v1</guid></item><item><title>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</title><link>http://arxiv.org/abs/2310.06627v3</link><description>Counterfactual reasoning, a fundamental aspect of human cognition, involvescontemplating alternatives to established facts or past events, significantlyenhancing our abilities in planning and decision-making. In light of theadvancements in current multi-modal large language models, we explore theireffectiveness in counterfactual reasoning. To facilitate this investigation, weintroduce a novel dataset, C-VQA, specifically designed to test thecounterfactual reasoning capabilities of modern multi-modal large languagemodels. This dataset is constructed by infusing original questions withcounterfactual presuppositions, spanning various types such as numerical andboolean queries. It encompasses a mix of real and synthetic data, representinga wide range of difficulty levels. Our thorough evaluations of contemporaryvision-language models using this dataset have revealed substantial performancedrops, with some models showing up to a 40% decrease, highlighting asignificant gap between current models and human-like vision reasoningcapabilities. We hope our dataset will serve as a vital benchmark forevaluating the counterfactual reasoning capabilities of models. Code anddataset are publicly available at https://bzhao.me/C-VQA/.</description><author>Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao</author><pubDate>Tue, 28 Nov 2023 15:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06627v3</guid></item><item><title>Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting</title><link>http://arxiv.org/abs/2311.09790v3</link><description>Balancing the trade-off between accuracy and robustness is a long-standingchallenge in time series forecasting. While most of existing robust algorithmshave achieved certain suboptimal performance on clean data, sustaining the sameperformance level in the presence of data perturbations remains extremely hard.In this paper, we study a wide array of perturbation scenarios and proposenovel defense mechanisms against adversarial attacks using real-world telecomdata. We compare our strategy against two existing adversarial trainingalgorithms under a range of maximal allowed perturbations, defined using$\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybridstrategy, which is composed of a classifier to detect adversarial examples, adenoiser to eliminate noise from the perturbed data samples, and a standardforecaster, achieves the best performance on both clean and perturbed data. Ouroptimal model can retain up to $92.02\%$ the performance of the originalforecasting model in terms of Mean Squared Error (MSE) on clean data, whilebeing more robust than the standard adversarially trained models on perturbeddata. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparingmethods on normal and perturbed data, respectively. In addition, the componentsof our models can be trained in parallel, resulting in better computationalefficiency. Our results indicate that we can optimally balance the trade-offbetween the performance and robustness of forecasting models by improving theclassifier and denoiser, even in the presence of sophisticated and destructivepoisoning attacks.</description><author>Romain Ilbert, Thai V. Hoang, Zonghua Zhang, Themis Palpanas</author><pubDate>Tue, 28 Nov 2023 15:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09790v3</guid></item><item><title>Optimization Theory Based Deep Reinforcement Learning for Resource Allocation in Ultra-Reliable Wireless Networked Control Systems</title><link>http://arxiv.org/abs/2311.16895v1</link><description>The design of Wireless Networked Control System (WNCS) requires addressingcritical interactions between control and communication systems with minimalcomplexity and communication overhead while providing ultra-high reliability.This paper introduces a novel optimization theory based deep reinforcementlearning (DRL) framework for the joint design of controller and communicationsystems. The objective of minimum power consumption is targeted whilesatisfying the schedulability and rate constraints of the communication systemin the finite blocklength regime and stability constraint of the controlsystem. Decision variables include the sampling period in the control system,and blocklength and packet error probability in the communication system. Theproposed framework contains two stages: optimization theory and DRL. In theoptimization theory stage, following the formulation of the joint optimizationproblem, optimality conditions are derived to find the mathematical relationsbetween the optimal values of the decision variables. These relations allow thedecomposition of the problem into multiple building blocks. In the DRL stage,the blocks that are simplified but not tractable are replaced by DRL. Viaextensive simulations, the proposed optimization theory based DRL approach isdemonstrated to outperform the optimization theory and pure DRL basedapproaches, with close to optimal performance and much lower complexity.</description><author>Hamida Qumber Ali, Amirhassan Babazadeh Darabi, Sinem Coleri</author><pubDate>Tue, 28 Nov 2023 15:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16895v1</guid></item><item><title>Dendrogram distance: an evaluation metric for generative networks using hierarchical clustering</title><link>http://arxiv.org/abs/2311.16894v1</link><description>We present a novel metric for generative modeling evaluation, focusingprimarily on generative networks. The method uses dendrograms to represent realand fake data, allowing for the divergence between training and generatedsamples to be computed. This metric focus on mode collapse, targetinggenerators that are not able to capture all modes in the training set. Toevaluate the proposed method it is introduced a validation scheme based onsampling from real datasets, therefore the metric is evaluated in a controlledenvironment and proves to be competitive with other state-of-the-artapproaches.</description><author>Gustavo Sutter Carvalho, Moacir Antonelli Ponti</author><pubDate>Tue, 28 Nov 2023 15:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16894v1</guid></item><item><title>FeTrIL: Feature Translation for Exemplar-Free Class-Incremental Learning</title><link>http://arxiv.org/abs/2211.13131v2</link><description>Exemplar-free class-incremental learning is very challenging due to thenegative effect of catastrophic forgetting. A balance between stability andplasticity of the incremental process is needed in order to obtain goodaccuracy for past as well as new classes. Existing exemplar-freeclass-incremental methods focus either on successive fine tuning of the model,thus favoring plasticity, or on using a feature extractor fixed after theinitial incremental state, thus favoring stability. We introduce a method whichcombines a fixed feature extractor and a pseudo-features generator to improvethe stability-plasticity balance. The generator uses a simple yet effectivegeometric translation of new class features to create representations of pastclasses, made of pseudo-features. The translation of features only requires thestorage of the centroid representations of past classes to produce theirpseudo-features. Actual features of new classes and pseudo-features of pastclasses are fed into a linear classifier which is trained incrementally todiscriminate between all classes. The incremental process is much faster withthe proposed method compared to mainstream ones which update the entire deepmodel. Experiments are performed with three challenging datasets, and differentincremental settings. A comparison with ten existing methods shows that ourmethod outperforms the others in most cases.</description><author>Grégoire Petit, Adrian Popescu, Hugo Schindler, David Picard, Bertrand Delezoide</author><pubDate>Tue, 28 Nov 2023 15:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13131v2</guid></item><item><title>Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing</title><link>http://arxiv.org/abs/2305.08415v3</link><description>Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT)System-on-a-Chip (SoC) for augmented reality, personalized healthcare, andnano-robotics need to run many diverse tasks within a power envelope of a fewtens of mW over a wide range of operating conditions: compute-intensive butstrongly quantized Deep Neural Network (DNN) inference, as well as signalprocessing and control requiring high-precision floating-point. We presentMarsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated inGlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16RISC-V Digital Signal Processing (DSP) cores attuned for the execution of adiverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions(XpulpNN), combined with fused MAC&amp;LOAD operations and floating-point support;2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1(pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocksconnected to an Adaptive Body Biasing (ABB) generator and a hardware controlloop, enabling on-the-fly adaptation of transistor threshold voltages.Marsellus achieves up to 180 Gop/s or 3.32 Top/s/W on 2-bit precisionarithmetic in software, and up to 637 Gop/s or 12.4 Top/s/W onhardware-accelerated DNN layers.</description><author>Francesco Conti, Gianna Paulin, Angelo Garofalo, Davide Rossi, Alfio Di Mauro, Georg Rutishauser, Gianmarco Ottavi, Manuel Eggimann, Hayate Okuhara, Luca Benini</author><pubDate>Tue, 28 Nov 2023 15:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08415v3</guid></item><item><title>BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis</title><link>http://arxiv.org/abs/2311.05521v2</link><description>Synthesizing photorealistic 4D human head avatars from videos is essentialfor VR/AR, telepresence, and video game applications. Although existing NeuralRadiance Fields (NeRF)-based methods achieve high-fidelity results, thecomputational expense limits their use in real-time applications. To overcomethis limitation, we introduce BakedAvatar, a novel representation for real-timeneural head avatar synthesis, deployable in a standard polygon rasterizationpipeline. Our approach extracts deformable multi-layer meshes from learnedisosurfaces of the head and computes expression-, pose-, and view-dependentappearances that can be baked into static textures for efficient rasterization.We thus propose a three-stage pipeline for neural head avatar synthesis, whichincludes learning continuous deformation, manifold, and radiance fields,extracting layered meshes and textures, and fine-tuning texture details withdifferential rasterization. Experimental results demonstrate that ourrepresentation generates synthesis results of comparable quality to otherstate-of-the-art methods while significantly reducing the inference timerequired. We further showcase various head avatar synthesis results frommonocular videos, including view synthesis, face reenactment, expressionediting, and pose editing, all at interactive frame rates.</description><author>Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, Yan-Pei Cao</author><pubDate>Tue, 28 Nov 2023 15:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05521v2</guid></item><item><title>Compressing the Backward Pass of Large-Scale Neural Architectures by Structured Activation Pruning</title><link>http://arxiv.org/abs/2311.16883v1</link><description>The rise of Deep Neural Networks (DNNs) has led to an increase in model sizeand complexity, straining the memory capacity of GPUs. Sparsity in DNNs,characterized as structural or ephemeral, has gained attention as a solution.This work focuses on ephemeral sparsity, aiming to reduce memory consumptionduring training. It emphasizes the significance of activations, an oftenoverlooked component, and their role in memory usage. This work employsstructured pruning in Block Sparse Compressed Row (BSR) format in combinationwith a magnitude-based criterion to efficiently prune activations. Wefurthermore introduce efficient block-sparse operators for GPUs and showcasetheir effectiveness, as well as the superior compression offered by blocksparsity. We report the effectiveness of activation pruning by evaluatingtraining speed, accuracy, and memory usage of large-scale neural architectureson the example of ResMLP on image classification tasks. As a result, we observea memory reduction of up to 32\% while maintaining accuracy. Ultimately, ourapproach aims to democratize large-scale model training, reduce GPUrequirements, and address ecological concerns.</description><author>Daniel Barley, Holger Fröning</author><pubDate>Tue, 28 Nov 2023 15:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16883v1</guid></item><item><title>Optimisation-Based Multi-Modal Semantic Image Editing</title><link>http://arxiv.org/abs/2311.16882v1</link><description>Image editing affords increased control over the aesthetics and content ofgenerated images. Pre-existing works focus predominantly on text-basedinstructions to achieve desired image modifications, which limit edit precisionand accuracy. In this work, we propose an inference-time editing optimisation,designed to extend beyond textual edits to accommodate multiple editinginstruction types (e.g. spatial layout-based; pose, scribbles, edge maps). Wepropose to disentangle the editing task into two competing subtasks: successfullocal image modifications and global content consistency preservation, wheresubtasks are guided through two dedicated loss functions. By allowing to adjustthe influence of each loss function, we build a flexible editing solution thatcan be adjusted to user preferences. We evaluate our method using text, poseand scribble edit conditions, and highlight our ability to achieve complexedits, through both qualitative and quantitative experiments.</description><author>Bowen Li, Yongxin Yang, Steven McDonagh, Shifeng Zhang, Petru-Daniel Tudosiu, Sarah Parisot</author><pubDate>Tue, 28 Nov 2023 15:31:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16882v1</guid></item><item><title>Continuously Controllable Facial Expression Editing in Talking Face Videos</title><link>http://arxiv.org/abs/2209.08289v2</link><description>Recently audio-driven talking face video generation has attractedconsiderable attention. However, very few researches address the issue ofemotional editing of these talking face videos with continuously controllableexpressions, which is a strong demand in the industry. The challenge is thatspeech-related expressions and emotion-related expressions are often highlycoupled. Meanwhile, traditional image-to-image translation methods cannot workwell in our application due to the coupling of expressions with otherattributes such as poses, i.e., translating the expression of the character ineach frame may simultaneously change the head pose due to the bias of thetraining data distribution. In this paper, we propose a high-quality facialexpression editing method for talking face videos, allowing the user to controlthe target emotion in the edited video continuously. We present a newperspective for this task as a special case of motion information editing,where we use a 3DMM to capture major facial movements and an associated texturemap modeled by a StyleGAN to capture appearance details. Both representations(3DMM and texture map) contain emotional information and can be continuouslymodified by neural networks and easily smoothed by averaging incoefficient/latent spaces, making our method simple yet effective. We alsointroduce a mouth shape preservation loss to control the trade-off between lipsynchronization and the degree of exaggeration of the edited expression.Extensive experiments and a user study show that our method achievesstate-of-the-art performance across various evaluation criteria.</description><author>Zhiyao Sun, Yu-Hui Wen, Tian Lv, Yanan Sun, Ziyang Zhang, Yaoyuan Wang, Yong-Jin Liu</author><pubDate>Tue, 28 Nov 2023 15:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08289v2</guid></item><item><title>Proximal Algorithms for Accelerated Langevin Dynamics</title><link>http://arxiv.org/abs/2311.14829v2</link><description>We develop a novel class of MCMC algorithms based on a stochastized Nesterovscheme. With an appropriate addition of noise, the result is atime-inhomogeneous underdamped Langevin equation, which we prove emits aspecified target distribution as its invariant measure. Convergence rates tostationarity under Wasserstein-2 distance are established as well.Metropolis-adjusted and stochastic gradient versions of the proposed Langevindynamics are also provided. Experimental illustrations show superiorperformance of the proposed method over typical Langevin samplers for differentmodels in statistics and image processing including better mixing of theresulting Markov chains.</description><author>Duy H. Thai, Alexander L. Young, David B. Dunson</author><pubDate>Tue, 28 Nov 2023 15:27:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14829v2</guid></item><item><title>Imputation using training labels and classification via label imputation</title><link>http://arxiv.org/abs/2311.16877v1</link><description>Missing data is a common problem in practical settings. Various imputationmethods have been developed to deal with missing data. However, even though thelabel is usually available in the training data, the common practice ofimputation usually only relies on the input and ignores the label. In thiswork, we illustrate how stacking the label into the input can significantlyimprove the imputation of the input. In addition, we propose a classificationstrategy that initializes the predicted test label with missing values andstacks the label with the input for imputation. This allows imputing the labeland the input at the same time. Also, the technique is capable of handling datatraining with missing labels without any prior imputation and is applicable tocontinuous, categorical, or mixed-type data. Experiments show promising resultsin terms of accuracy.</description><author>Thu Nguyen, Pål Halvorsen, Michael A. Riegler</author><pubDate>Tue, 28 Nov 2023 15:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16877v1</guid></item><item><title>Digital Twin-Enhanced Deep Reinforcement Learning for Resource Management in Networks Slicing</title><link>http://arxiv.org/abs/2311.16876v1</link><description>Network slicing-based communication systems can dynamically and efficientlyallocate resources for diversified services. However, due to the limitation ofthe network interface on channel access and the complexity of the resourceallocation, it is challenging to achieve an acceptable solution in thepractical system without precise prior knowledge of the dynamics probabilitymodel of the service requests. Existing work attempts to solve this problemusing deep reinforcement learning (DRL), however, such methods usually requirea lot of interaction with the real environment in order to achieve goodresults. In this paper, a framework consisting of a digital twin andreinforcement learning agents is present to handle the issue. Specifically, wepropose to use the historical data and the neural networks to build a digitaltwin model to simulate the state variation law of the real environment. Then,we use the data generated by the network slicing environment to calibrate thedigital twin so that it is in sync with the real environment. Finally, DRL forslice optimization optimizes its own performance in this virtualpre-verification environment. We conducted an exhaustive verification of theproposed digital twin framework to confirm its scalability. Specifically, wepropose to use loss landscapes to visualize the generalization of DRLsolutions. We explore a distillation-based optimization scheme for lightweightslicing strategies. In addition, we also extend the framework to offlinereinforcement learning, where solutions can be used to obtain intelligentdecisions based solely on historical data. Numerical simulation experimentsshow that the proposed digital twin can significantly improve the performanceof the slice optimization strategy.</description><author>Zhengming Zhang, Yongming Huang, Cheng Zhang, Qingbi Zheng, Luxi Yang, Xiaohu You</author><pubDate>Tue, 28 Nov 2023 15:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16876v1</guid></item><item><title>A unified weighting framework for evaluating nearest neighbour classification</title><link>http://arxiv.org/abs/2311.16872v1</link><description>We present the first comprehensive and large-scale evaluation of classical(NN), fuzzy (FNN) and fuzzy rough (FRNN) nearest neighbour classification. Weshow that existing proposals for nearest neighbour weighting can bestandardised in the form of kernel functions, applied to the distance valuesand/or ranks of the nearest neighbours of a test instance. Furthermore, weidentify three commonly used distance functions and four scaling measures. Wesystematically evaluate these choices on a collection of 85 real-lifeclassification datasets. We find that NN, FNN and FRNN all perform best withBoscovich distance. NN and FRNN perform best with a combination of Samworthrank- and distance weights and scaling by the mean absolute deviation aroundthe median ($r_1$), the standard deviaton ($r_2$) or the interquartile range($r_{\infty}^*$), while FNN performs best with only Samworth distance-weightsand $r_1$- or $r_2$-scaling. We also introduce a new kernel based on fuzzyYager negation, and show that NN achieves comparable performance with Yagerdistance-weights, which are simpler to implement than a combination of Samworthdistance- and rank-weights. Finally, we demonstrate that FRNN generallyoutperforms NN, which in turns performs systematically better than FNN.</description><author>Oliver Urs Lenz, Henri Bollaert, Chris Cornelis</author><pubDate>Tue, 28 Nov 2023 15:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16872v1</guid></item><item><title>Replay across Experiments: A Natural Extension of Off-Policy RL</title><link>http://arxiv.org/abs/2311.15951v2</link><description>Replaying data is a principal mechanism underlying the stability and dataefficiency of off-policy reinforcement learning (RL). We present an effectiveyet simple framework to extend the use of replays across multiple experiments,minimally adapting the RL workflow for sizeable improvements in controllerperformance and research iteration times. At its core, Replay AcrossExperiments (RaE) involves reusing experience from previous experiments toimprove exploration and bootstrap learning while reducing required changes to aminimum in comparison to prior work. We empirically show benefits across anumber of RL algorithms and challenging control domains spanning bothlocomotion and manipulation, including hard exploration tasks from egocentricvision. Through comprehensive ablations, we demonstrate robustness to thequality and amount of data available and various hyperparameter choices.Finally, we discuss how our approach can be applied more broadly acrossresearch life cycles and can increase resilience by reloading data acrossrandom seeds or hyperparameter variations.</description><author>Dhruva Tirumala, Thomas Lampe, Jose Enrique Chen, Tuomas Haarnoja, Sandy Huang, Guy Lever, Ben Moran, Tim Hertweck, Leonard Hasenclever, Martin Riedmiller, Nicolas Heess, Markus Wulfmeier</author><pubDate>Tue, 28 Nov 2023 15:18:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15951v2</guid></item><item><title>The Falcon Series of Open Language Models</title><link>http://arxiv.org/abs/2311.16867v1</link><description>We introduce the Falcon series: 7B, 40B, and 180B parameters causaldecoder-only models trained on a diverse high-quality corpora predominantlyassembled from web data. The largest model, Falcon-180B, has been trained onover 3.5 trillion tokens of text--the largest openly documented pretrainingrun. Falcon-180B significantly outperforms models such as PaLM or Chinchilla,and improves upon concurrently developed models such as LLaMA 2 orInflection-1. It nears the performance of PaLM-2-Large at a reduced pretrainingand inference cost, making it, to our knowledge, one of the three best languagemodels in the world along with GPT-4 and PaLM-2-Large. We report detailedevaluations, as well as a deep dive into the methods and custom toolingemployed to pretrain Falcon. Notably, we report on our custom distributedtraining codebase, allowing us to efficiently pretrain these models on up to4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a600B tokens extract of our web dataset, as well as the Falcon-7/40/180B modelsunder a permissive license to foster open-science and accelerate thedevelopment of an open ecosystem of large language models.</description><author>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo</author><pubDate>Tue, 28 Nov 2023 15:12:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16867v1</guid></item><item><title>A Benchmark for Evaluating Machine Translation Metrics on Dialects Without Standard Orthography</title><link>http://arxiv.org/abs/2311.16865v1</link><description>For sensible progress in natural language processing, it is important that weare aware of the limitations of the evaluation metrics we use. In this work, weevaluate how robust metrics are to non-standardized dialects, i.e. spellingdifferences in language varieties that do not have a standard orthography. Toinvestigate this, we collect a dataset of human translations and humanjudgments for automatic machine translations from English to two Swiss Germandialects. We further create a challenge set for dialect variation and benchmarkexisting metrics' performances. Our results show that existing metrics cannotreliably evaluate Swiss German text generation outputs, especially on segmentlevel. We propose initial design adaptations that increase robustness in theface of non-standardized dialects, although there remains much room for furtherimprovement. The dataset, code, and models are available here:https://github.com/textshuttle/dialect_eval</description><author>Noëmi Aepli, Chantal Amrhein, Florian Schottmann, Rico Sennrich</author><pubDate>Tue, 28 Nov 2023 15:12:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16865v1</guid></item><item><title>Power Hungry Processing: Watts Driving the Cost of AI Deployment?</title><link>http://arxiv.org/abs/2311.16863v1</link><description>Recent years have seen a surge in the popularity of commercial AI productsbased on generative, multi-purpose AI systems promising a unified approach tobuilding machine learning (ML) models into technology. However, this ambitionof "generality" comes at a steep cost to the environment, given the amount ofenergy these systems require and the amount of carbon that they emit. In thiswork, we propose the first systematic comparison of the ongoing inference costof various categories of ML systems, covering both task-specific (i.e.finetuned models that carry out a single task) and `general-purpose' models,(i.e. those trained for multiple tasks). We measure deployment cost as theamount of energy and carbon required to perform 1,000 inferences onrepresentative benchmark dataset using these models. We find thatmulti-purpose, generative architectures are orders of magnitude more expensivethan task-specific systems for a variety of tasks, even when controlling forthe number of model parameters. We conclude with a discussion around thecurrent trend of deploying multi-purpose generative ML systems, and cautionthat their utility should be more intentionally weighed against increased costsin terms of energy and emissions. All the data from our study can be accessedvia an interactive demo to carry out further exploration and analysis.</description><author>Alexandra Sasha Luccioni, Yacine Jernite, Emma Strubell</author><pubDate>Tue, 28 Nov 2023 15:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16863v1</guid></item><item><title>Data-efficient operator learning for solving high Mach number fluid flow problems</title><link>http://arxiv.org/abs/2311.16860v1</link><description>We consider the problem of using SciML to predict solutions of high Machfluid flows over irregular geometries. In this setting, data is limited, and soit is desirable for models to perform well in the low-data setting. We showthat Neural Basis Functions (NBF), which learns a basis of behavior modes fromthe data and then uses this basis to make predictions, is more effective than abasis-unaware baseline model. In addition, we identify continuing challenges inthe space of predicting solutions for this type of problem.</description><author>Noah Ford, Victor J. Leon, Honest Merman, Jeffrey Gilbert, Alexander New</author><pubDate>Tue, 28 Nov 2023 15:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16860v1</guid></item><item><title>Attentional Graph Neural Networks for Robust Massive Network Localization</title><link>http://arxiv.org/abs/2311.16856v1</link><description>Graph neural networks (GNNs) have gained significant popularity forclassification tasks in machine learning, yet their applications to regressionproblems remain limited. Concurrently, attention mechanisms have emerged aspowerful tools in sequential learning tasks. In this paper, we employ GNNs andattention mechanisms to address a classical but challenging nonlinearregression problem: network localization. We propose a novel GNN-based networklocalization method that achieves exceptional stability and accuracy in thepresence of severe non-line-of-sight (NLOS) propagations, while eliminating theneed for laborious offline calibration or NLOS identification. Extensiveexperimental results validate the effectiveness and high accuracy of ourGNN-based localization model, particularly in challenging NLOS scenarios.However, the proposed GNN-based model exhibits limited flexibility, and itsaccuracy is highly sensitive to a specific hyperparameter that determines thegraph structure. To address the limitations and extend the applicability of theGNN-based model to real scenarios, we introduce two attentional graph neuralnetworks (AGNNs) that offer enhanced flexibility and the ability toautomatically learn the optimal hyperparameter for each node. Experimentalresults confirm that the AGNN models are able to enhance localization accuracy,providing a promising solution for real-world applications. We also providesome analyses of the improved performance achieved by the AGNN models from theperspectives of dynamic attention and signal denoising characteristics.</description><author>Wenzhong Yan, Juntao Wang, Feng Yin, Abdelhak M. Zoubir</author><pubDate>Tue, 28 Nov 2023 15:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16856v1</guid></item><item><title>A Unified Approach for Text- and Image-guided 4D Scene Generation</title><link>http://arxiv.org/abs/2311.16854v1</link><description>Large-scale diffusion generative models are greatly simplifying image, videoand 3D asset creation from user-provided text prompts and images. However, thechallenging problem of text-to-4D dynamic 3D scene generation with diffusionguidance remains largely unexplored. We propose Dream-in-4D, which features anovel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2Ddiffusion guidance to effectively learn a high-quality static 3D asset in thefirst stage; (2) a deformable neural radiance field that explicitlydisentangles the learned static asset from its deformation, preserving qualityduring motion learning; and (3) a multi-resolution feature grid for thedeformation field with a displacement total variation loss to effectively learnmotion with video diffusion guidance in the second stage. Through a userpreference study, we demonstrate that our approach significantly advances imageand motion quality, 3D consistency and text fidelity for text-to-4D generationcompared to baseline approaches. Thanks to its motion-disentangledrepresentation, Dream-in-4D can also be easily adapted for controllablegeneration where appearance is defined by one or multiple images, without theneed to modify the motion learning stage. Thus, our method offers, for thefirst time, a unified approach for text-to-4D, image-to-4D and personalized 4Dgeneration tasks.</description><author>Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, Shalini De Mello</author><pubDate>Tue, 28 Nov 2023 15:03:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16854v1</guid></item></channel></rss>