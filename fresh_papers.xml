<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 12 Sep 2024 01:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis</title><link>http://arxiv.org/abs/2409.06644v2</link><description>Early detection of eye diseases like glaucoma, macular degeneration, anddiabetic retinopathy is crucial for preventing vision loss. While artificialintelligence (AI) foundation models hold significant promise for addressingthese challenges, existing ophthalmic foundation models primarily focus on asingle modality, whereas diagnosing eye diseases requires multiple modalities.A critical yet often overlooked aspect is harnessing the multi-view informationacross various modalities for the same patient. Additionally, due to thelong-tail nature of ophthalmic diseases, standard fully supervised orunsupervised learning approaches often struggle. Therefore, it is essential tointegrate clinical text to capture a broader spectrum of diseases. We proposeEyeCLIP, a visual-language foundation model developed using over 2.77 millionmulti-modal ophthalmology images with partial text data. To fully leverage thelarge multi-modal unlabeled and labeled data, we introduced a pretrainingstrategy that combines self-supervised reconstructions, multi-modal imagecontrastive learning, and image-text contrastive learning to learn a sharedrepresentation of multiple modalities. Through evaluation using 14 benchmarkdatasets, EyeCLIP can be transferred to a wide range of downstream tasksinvolving ocular and systemic diseases, achieving state-of-the-art performancein disease classification, visual question answering, and cross-modalretrieval. EyeCLIP represents a significant advancement over previous methods,especially showcasing few-shot, even zero-shot capabilities in real-worldlong-tail scenarios.</description><author>Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He</author><pubDate>Wed, 11 Sep 2024 17:00:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06644v2</guid></item><item><title>Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences</title><link>http://arxiv.org/abs/2409.06683v2</link><description>Object pose distribution estimation is crucial in robotics for better pathplanning and handling of symmetric objects. Recent distribution estimationapproaches employ contrastive learning-based approaches by maximizing thelikelihood of a single pose estimate in the absence of a CAD model. We proposea pose distribution estimation method leveraging symmetry respectingcorrespondence distributions and shape information obtained using a CAD model.Contrastive learning-based approaches require an exhaustive amount of trainingimages from different viewpoints to learn the distribution properly, which isnot possible in realistic scenarios. Instead, we propose a pipeline that canleverage correspondence distributions and shape information from the CAD model,which are later used to learn pose distributions. Besides, having access topose distribution based on correspondences before learning pose distributionsconditioned on images, can help formulate the loss between distributions. Theprior knowledge of distribution also helps the network to focus on gettingsharper modes instead. With the CAD prior, our approach converges much fasterand learns distribution better by focusing on learning sharper distributionnear all the valid modes, unlike contrastive approaches, which focus on asingle mode at a time. We achieve benchmark results on SYMSOL-I and T-Lessdatasets.</description><author>Shishir Reddy Vutukur, Rasmus Laurvig Haugaard, Junwen Huang, Benjamin Busam, Tolga Birdal</author><pubDate>Wed, 11 Sep 2024 16:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06683v2</guid></item><item><title>GeoCalib: Learning Single-image Calibration with Geometric Optimization</title><link>http://arxiv.org/abs/2409.06704v1</link><description>From a single image, visual cues can help deduce intrinsic and extrinsiccamera parameters like the focal length and the gravity direction. Thissingle-image calibration can benefit various downstream applications like imageediting and 3D mapping. Current approaches to this problem are based on eitherclassical geometry with lines and vanishing points or on deep neural networkstrained end-to-end. The learned approaches are more robust but struggle togeneralize to new environments and are less accurate than their classicalcounterparts. We hypothesize that they lack the constraints that 3D geometryprovides. In this work, we introduce GeoCalib, a deep neural network thatleverages universal rules of 3D geometry through an optimization process.GeoCalib is trained end-to-end to estimate camera parameters and learns to finduseful visual cues from the data. Experiments on various benchmarks show thatGeoCalib is more robust and more accurate than existing classical and learnedapproaches. Its internal optimization estimates uncertainties, which help flagfailure cases and benefit downstream applications like visual localization. Thecode and trained models are publicly available athttps://github.com/cvg/GeoCalib.</description><author>Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, Marc Pollefeys</author><pubDate>Tue, 10 Sep 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06704v1</guid></item><item><title>LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation</title><link>http://arxiv.org/abs/2409.06703v1</link><description>Neural Radiance Fields (NeRFs) have revolutionized the reconstruction ofstatic scenes and objects in 3D, offering unprecedented quality. However,extending NeRFs to model dynamic objects or object articulations remains achallenging problem. Previous works have tackled this issue by focusing onpart-level reconstruction and motion estimation for objects, but they oftenrely on heuristics regarding the number of moving parts or object categories,which can limit their practical use. In this work, we introduce LEIA, a novelapproach for representing dynamic 3D objects. Our method involves observing theobject at distinct time steps or "states" and conditioning a hypernetwork onthe current state, using this to parameterize our NeRF. This approach allows usto learn a view-invariant latent representation for each state. We furtherdemonstrate that by interpolating between these states, we can generate novelarticulation configurations in 3D space that were previously unseen. Ourexperimental results highlight the effectiveness of our method in articulatingobjects in a manner that is independent of the viewing angle and jointconfiguration. Notably, our approach outperforms previous methods that rely onmotion information for articulation registration.</description><author>Archana Swaminathan, Anubhav Gupta, Kamal Gupta, Shishira R. Maiya, Vatsal Agarwal, Abhinav Shrivastava</author><pubDate>Tue, 10 Sep 2024 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06703v1</guid></item><item><title>Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2409.06702v1</link><description>End-to-end architectures in autonomous driving (AD) face a significantchallenge in interpretability, impeding human-AI trust. Human-friendly naturallanguage has been explored for tasks such as driving explanation and 3Dcaptioning. However, previous works primarily focused on the paradigm ofdeclarative interpretability, where the natural language interpretations arenot grounded in the intermediate outputs of AD systems, making theinterpretations only declarative. In contrast, aligned interpretabilityestablishes a connection between language and the intermediate outputs of ADsystems. Here we introduce Hint-AD, an integrated AD-language system thatgenerates language aligned with the holistic perception-prediction-planningoutputs of the AD model. By incorporating the intermediate outputs and aholistic token mixer sub-network for effective feature adaptation, Hint-ADachieves desirable accuracy, achieving state-of-the-art results in drivinglanguage tasks including driving explanation, 3D dense captioning, and commandprediction. To facilitate further study on driving explanation task onnuScenes, we also introduce a human-labeled dataset, Nu-X. Codes, dataset, andmodels will be publicly available.</description><author>Kairui Ding, Boyuan Chen, Yuchen Su, Huan-ang Gao, Bu Jin, Chonghao Sima, Wuqiang Zhang, Xiaohui Li, Paul Barsch, Hongyang Li, Hao Zhao</author><pubDate>Tue, 10 Sep 2024 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06702v1</guid></item><item><title>A study on Deep Convolutional Neural Networks, Transfer Learning and Ensemble Model for Breast Cancer Detection</title><link>http://arxiv.org/abs/2409.06699v1</link><description>In deep learning, transfer learning and ensemble models have shown promise inimproving computer-aided disease diagnosis. However, applying the transferlearning and ensemble model is still relatively limited. Moreover, the ensemblemodel's development is ad-hoc, overlooks redundant layers, and suffers fromimbalanced datasets and inadequate augmentation. Lastly, significant DeepConvolutional Neural Networks (D-CNNs) have been introduced to detect andclassify breast cancer. Still, very few comparative studies were conducted toinvestigate the accuracy and efficiency of existing CNN architectures.Realising the gaps, this study compares the performance of D-CNN, whichincludes the original CNN, transfer learning, and an ensemble model, indetecting breast cancer. The comparison study of this paper consists ofcomparison using six CNN-based deep learning architectures (SE-ResNet152,MobileNetV2, VGG19, ResNet18, InceptionV3, and DenseNet-121), a transferlearning, and an ensemble model on breast cancer detection. Among thecomparison of these models, the ensemble model provides the highest detectionand classification accuracy of 99.94% for breast cancer detection andclassification. However, this study also provides a negative result in the caseof transfer learning, as the transfer learning did not increase the accuracy ofthe original SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, andDenseNet-121 model. The high accuracy in detecting and categorising breastcancer detection using CNN suggests that the CNN model is promising in breastcancer disease detection. This research is significant in biomedicalengineering, computer-aided disease diagnosis, and ML-based disease detection.</description><author>Md Taimur Ahad, Sumaya Mustofa, Faruk Ahmed, Yousuf Rayhan Emon, Aunirudra Dey Anu</author><pubDate>Tue, 10 Sep 2024 17:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06699v1</guid></item><item><title>QueryBuilder: Human-in-the-Loop Query Development for Information Retrieval</title><link>http://arxiv.org/abs/2409.04667v2</link><description>Frequently, users of an Information Retrieval (IR) system start with anoverarching information need (a.k.a., an analytic task) and proceed to definefiner-grained queries covering various important aspects (i.e., sub-topics) ofthat analytic task. We present a novel, interactive system called$\textit{QueryBuilder}$, which allows a novice, English-speaking user to createqueries with a small amount of effort, through efficient exploration of anEnglish development corpus in order to rapidly develop cross-lingualinformation retrieval queries corresponding to the user's information needs.QueryBuilder performs near real-time retrieval of documents based onuser-entered search terms; the user looks through the retrieved documents andmarks sentences as relevant to the information needed. The marked sentences areused by the system as additional information in query formation and refinement:query terms (and, optionally, event features, which capture event $'triggers'$(indicator terms) and agent/patient roles) are appropriately weighted, and aneural-based system, which better captures textual meaning, retrieves otherrelevant content. The process of retrieval and marking is repeated as manytimes as desired, giving rise to increasingly refined queries in eachiteration. The final product is a fine-grained query used in Cross-LingualInformation Retrieval (CLIR). Our experiments using analytic tasks and requestsfrom the IARPA BETTER IR datasets show that with a small amount of effort (atmost 10 minutes per sub-topic), novice users can form $\textit{useful}$fine-grained queries including in languages they don't understand. QueryBuilderalso provides beneficial capabilities to the traditional corpus exploration andquery formation process. A demonstration video is released athttps://vimeo.com/734795835</description><author>Hemanth Kandula, Damianos Karakos, Haoling Qiu, Benjamin Rozonoyer, Ian Soboroff, Lee Tarlin, Bonan Min</author><pubDate>Tue, 10 Sep 2024 17:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04667v2</guid></item><item><title>DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images</title><link>http://arxiv.org/abs/2409.06694v1</link><description>Cancer is a complex disease characterized by uncontrolled cell growth. T cellreceptors (TCRs), crucial proteins in the immune system, play a key role inrecognizing antigens, including those associated with cancer. Recentadvancements in sequencing technologies have facilitated comprehensiveprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activityand enabling TCR-based immunotherapies. However, analyzing these intricatebiomolecules necessitates efficient representations that capture theirstructural and functional information. T-cell protein sequences pose uniquechallenges due to their relatively smaller lengths compared to otherbiomolecules. An image-based representation approach becomes a preferred choicefor efficient embeddings, allowing for the preservation of essential detailsand enabling comprehensive analysis of T-cell protein sequences. In this paper,we propose to generate images from the protein sequences using the idea ofChaos Game Representation (CGR) using the Kaleidoscopic images approach. ThisDeep Learning Assisted Analysis of Protein Sequences Using Chaos EnhancedKaleidoscopic Images (called DANCE) provides a unique way to visualize proteinsequences by recursively applying chaos game rules around a central seed point.we perform the classification of the T cell receptors (TCRs) protein sequencesin terms of their respective target cancer cells, as TCRs are known for theirimmune response against cancer disease. The TCR sequences are converted intoimages using the DANCE method. We employ deep-learning vision models to performthe classification to obtain insights into the relationship between the visualpatterns observed in the generated kaleidoscopic images and the underlyingprotein properties. By combining CGR-based image generation with deep learningclassification, this study opens novel possibilities in the protein analysisdomain.</description><author>Taslim Murad, Prakash Chourasia, Sarwan Ali, Murray Patterson</author><pubDate>Tue, 10 Sep 2024 17:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06694v1</guid></item><item><title>HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs</title><link>http://arxiv.org/abs/2409.06692v1</link><description>We consider fact-checking approaches that aim to predict the veracity ofassertions in knowledge graphs. Five main categories of fact-checkingapproaches for knowledge graphs have been proposed in the recent literature, ofwhich each is subject to partially overlapping limitations. In particular,current text-based approaches are limited by manual feature engineering.Path-based and rule-based approaches are limited by their exclusive use ofknowledge graphs as background knowledge, and embedding-based approaches sufferfrom low accuracy scores on current fact-checking tasks. We propose a hybridapproach -- dubbed HybridFC -- that exploits the diversity of existingcategories of fact-checking approaches within an ensemble learning setting toachieve a significantly better prediction performance. In particular, ourapproach outperforms the state of the art by 0.14 to 0.27 in terms of AreaUnder the Receiver Operating Characteristic curve on the FactBench dataset. Ourcode is open-source and can be found at https://github.com/dice-group/HybridFC.</description><author>Umair Qudus, Michael Roeder, Muhammad Saleem, Axel-Cyrille Ngonga Ngomo</author><pubDate>Tue, 10 Sep 2024 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06692v1</guid></item><item><title>WonderWorld: Interactive 3D Scene Generation from a Single Image</title><link>http://arxiv.org/abs/2406.09394v3</link><description>We present WonderWorld, a novel framework for interactive 3D scene generationthat enables users to interactively specify scene contents and layout and seethe created scenes in low latency. The major challenge lies in achieving fastgeneration of 3D scenes. Existing scene generation approaches fall short ofspeed as they often require (1) progressively generating many views and depthmaps, and (2) time-consuming optimization of the scene geometryrepresentations. We introduce the Fast Layered Gaussian Surfels (FLAGS) as ourscene representation and an algorithm to generate it from a single view. Ourapproach does not need multiple views, and it leverages a geometry-basedinitialization that significantly reduces optimization time. Another challengeis generating coherent geometry that allows all scenes to be connected. Weintroduce the guided depth diffusion that allows partial conditioning of depthestimation. WonderWorld generates connected and diverse 3D scenes in less than10 seconds on a single A6000 GPU, enabling real-time user interaction andexploration. We demonstrate the potential of WonderWorld for user-drivencontent creation and exploration in virtual environments. We will release fullcode and software for reproducibility. Project website:https://kovenyu.com/WonderWorld/.</description><author>Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, Jiajun Wu</author><pubDate>Tue, 10 Sep 2024 17:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09394v3</guid></item><item><title>Geometric-Averaged Preference Optimization for Soft Preference Labels</title><link>http://arxiv.org/abs/2409.06691v1</link><description>Many algorithms for aligning LLMs with human preferences assume that humanpreferences are binary and deterministic. However, it is reasonable to thinkthat they can vary with different individuals, and thus should bedistributional to reflect the fine-grained relationship between the responses.In this work, we introduce the distributional soft preference labels andimprove Direct Preference Optimization (DPO) with a weighted geometric averageof the LLM output likelihood in the loss function. In doing so, the scale oflearning loss is adjusted based on the soft labels, and the loss with equallypreferred responses would be close to zero. This simple modification can beeasily applied to any DPO family and helps the models escape from theover-optimization and objective mismatch prior works suffer from. In ourexperiments, we simulate the soft preference labels with AI feedback from LLMsand demonstrate that geometric averaging consistently improves performance onstandard benchmarks for alignment research. In particular, we observe morepreferable responses than binary labels and significant improvements with datawhere modestly-confident labels are in the majority.</description><author>Hiroki Furuta, Kuang-Huei Lee, Shixiang Shane Gu, Yutaka Matsuo, Aleksandra Faust, Heiga Zen, Izzeddin Gur</author><pubDate>Tue, 10 Sep 2024 17:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06691v1</guid></item><item><title>Benchmarking Sub-Genre Classification For Mainstage Dance Music</title><link>http://arxiv.org/abs/2409.06690v1</link><description>Music classification, with a wide range of applications, is one of the mostprominent tasks in music information retrieval. To address the absence ofcomprehensive datasets and high-performing methods in the classification ofmainstage dance music, this work introduces a novel benchmark comprising a newdataset and a baseline. Our dataset extends the number of sub-genres to covermost recent mainstage live sets by top DJs worldwide in music festivals. Acontinuous soft labeling approach is employed to account for tracks that spanmultiple sub-genres, preserving the inherent sophistication. For the baseline,we developed deep learning models that outperform current state-of-the-artmultimodel language models, which struggle to identify house music sub-genres,emphasizing the need for specialized models trained on fine-grained datasets.Our benchmark is applicable to serve for application scenarios such as musicrecommendation, DJ set curation, and interactive multimedia, where we alsoprovide video demos. Our code is on\url{https://anonymous.4open.science/r/Mainstage-EDM-Benchmark/}.</description><author>Hongzhi Shu, Xinglin Li, Hongyu Jiang, Minghao Fu, Xinyu Li</author><pubDate>Tue, 10 Sep 2024 17:54:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06690v1</guid></item><item><title>A comprehensive study on Blood Cancer detection and classification using Convolutional Neural Network</title><link>http://arxiv.org/abs/2409.06689v1</link><description>Over the years in object detection several efficient Convolutional NeuralNetworks (CNN) networks, such as DenseNet201, InceptionV3, ResNet152v2,SEresNet152, VGG19, Xception gained significant attention due to theirperformance. Moreover, CNN paradigms have expanded to transfer learning andensemble models from original CNN architectures. Research studies suggest thattransfer learning and ensemble models are capable of increasing the accuracy ofdeep learning (DL) models. However, very few studies have conductedcomprehensive experiments utilizing these techniques in detecting andlocalizing blood malignancies. Realizing the gap, this study conducted threeexperiments; in the first experiment -- six original CNNs were used, in thesecond experiment -- transfer learning and, in the third experiment a novelensemble model DIX (DenseNet201, InceptionV3, and Xception) was developed todetect and classify blood cancer. The statistical result suggests that DIXoutperformed the original and transfer learning performance, providing anaccuracy of 99.12%. However, this study also provides a negative result in thecase of transfer learning, as the transfer learning did not increase theaccuracy of the original CNNs. Like many other cancers, blood cancer diseasesrequire timely identification for effective treatment plans and increasedsurvival possibilities. The high accuracy in detecting and categorization bloodcancer detection using CNN suggests that the CNN model is promising in bloodcancer disease detection. This research is significant in the fields ofbiomedical engineering, computer-aided disease diagnosis, and ML-based diseasedetection.</description><author>Md Taimur Ahad, Sajib Bin Mamun, Sumaya Mustofa, Bo Song, Yan Li</author><pubDate>Tue, 10 Sep 2024 17:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06689v1</guid></item><item><title>A study on deep feature extraction to detect and classify Acute Lymphoblastic Leukemia (ALL)</title><link>http://arxiv.org/abs/2409.06687v1</link><description>Acute lymphoblastic leukaemia (ALL) is a blood malignancy that mainly affectsadults and children. This study looks into the use of deep learning,specifically Convolutional Neural Networks (CNNs), for the detection andclassification of ALL. Conventional techniques for ALL diagnosis, such bonemarrow biopsy, are costly and prone to mistakes made by hand. By utilisingautomated technologies, the research seeks to improve diagnostic accuracy. Theresearch uses a variety of pre-trained CNN models, such as InceptionV3,ResNet101, VGG19, DenseNet121, MobileNetV2, and DenseNet121, to extractcharacteristics from pictures of blood smears. ANOVA, Recursive FeatureElimination (RFE), Random Forest, Lasso, and Principal Component Analysis (PCA)are a few of the selection approaches used to find the most relevant featuresafter feature extraction. Following that, machine learning methods like Na\"iveBayes, Random Forest, Support Vector Machine (SVM), and K-Nearest Neighbours(KNN) are used to classify these features. With an 87% accuracy rate, theResNet101 model produced the best results, closely followed by DenseNet121 andVGG19. According to the study, CNN-based models have the potential to decreasethe need for medical specialists by increasing the speed and accuracy of ALLdiagnosis. To improve model performance, the study also recommends expandingand diversifying datasets and investigating more sophisticated designs such astransformers. This study highlights how well automated deep learning systems domedical diagnosis.</description><author>Sabit Ahamed Preanto, Md. Taimur Ahad, Yousuf Rayhan Emon, Sumaya Mustofa, Md Alamin</author><pubDate>Tue, 10 Sep 2024 17:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06687v1</guid></item><item><title>GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction</title><link>http://arxiv.org/abs/2409.06685v1</link><description>3D Gaussian Splatting (3DGS) has shown promising performance in novel viewsynthesis. Previous methods adapt it to obtaining surfaces of either individual3D objects or within limited scenes. In this paper, we make the first attemptto tackle the challenging task of large-scale scene surface reconstruction.This task is particularly difficult due to the high GPU memory consumption,different levels of details for geometric representation, and noticeableinconsistencies in appearance. To this end, we propose GigaGS, the first workfor high-quality surface reconstruction for large-scale scenes using 3DGS.GigaGS first applies a partitioning strategy based on the mutual visibility ofspatial regions, which effectively grouping cameras for parallel processing. Toenhance the quality of the surface, we also propose novel multi-viewphotometric and geometric consistency constraints based on Level-of-Detailrepresentation. In doing so, our method can reconstruct detailed surfacestructures. Comprehensive experiments are conducted on various datasets. Theconsistent improvement demonstrates the superiority of GigaGS.</description><author>Junyi Chen, Weicai Ye, Yifan Wang, Danpeng Chen, Di Huang, Wanli Ouyang, Guofeng Zhang, Yu Qiao, Tong He</author><pubDate>Tue, 10 Sep 2024 17:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06685v1</guid></item><item><title>Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences</title><link>http://arxiv.org/abs/2409.06683v1</link><description>Object pose distribution estimation is crucial in robotics for better pathplanning and handling of symmetric objects. Recent distribution estimationapproaches employ contrastive learning-based approaches by maximizing thelikelihood of a single pose estimate in the absence of a CAD model. We proposea pose distribution estimation method leveraging symmetry respectingcorrespondence distributions and shape information obtained using a CAD model.Contrastive learning-based approaches require an exhaustive amount of trainingimages from different viewpoints to learn the distribution properly, which isnot possible in realistic scenarios. Instead, we propose a pipeline that canleverage correspondence distributions and shape information from the CAD model,which are later used to learn pose distributions. Besides, having access topose distribution based on correspondences before learning pose distributionsconditioned on images, can help formulate the loss between distributions. Theprior knowledge of distribution also helps the network to focus on gettingsharper modes instead. With the CAD prior, our approach converges much fasterand learns distribution better by focusing on learning sharper distributionnear all the valid modes, unlike contrastive approaches, which focus on asingle mode at a time. We achieve benchmark results on SYMSOL-I and T-Lessdatasets.</description><author>Shishir Reddy Vutukur, Rasmus Laurvig Haugaard, Junwen Huang, Benjamin Busam, Tolga Birdal</author><pubDate>Tue, 10 Sep 2024 17:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06683v1</guid></item><item><title>E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning</title><link>http://arxiv.org/abs/2409.06679v1</link><description>In the realm of Large Language Models (LLMs), the ability to process longcontexts is increasingly crucial for tasks such as multi-round dialogues, codegeneration, and document summarization. This paper addresses the challenges ofenhancing the long-context performance, reducing computational complexity, andleveraging pretrained models collectively termed the "impossible triangle." Weintroduce E2LLM (Encoder Elongated Large Language Models), a novel approachthat effectively navigates this paradox. The method involves splitting longcontexts into chunks, compressing each into embedding vectors via a pretrainedtext encoder, and utilizing an adapter to align these representations with adecoder-only LLM. Two training objectives, focusing on reconstruction of theencoder output and long-context instruction fine-tuning, are employed tofacilitate the understanding of soft prompts by the LLM. Experimental resultsdemonstrate that E2LLM achieves superior performance in long-context scenarioswhile balancing efficiency, performance, and compatibility with pretrainedmodels. Our framework thus represents a significant advancement in the field,contributing to effective long-text modeling.</description><author>Zihan Liao, Jun Wang, Hang Yu, Lingxiao Wei, Jianguo Li, Jun Wang, Wei Zhang</author><pubDate>Tue, 10 Sep 2024 17:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06679v1</guid></item><item><title>Constructing an Interpretable Deep Denoiser by Unrolling Graph Laplacian Regularizer</title><link>http://arxiv.org/abs/2409.06676v1</link><description>An image denoiser can be used for a wide range of restoration problems viathe Plug-and-Play (PnP) architecture. In this paper, we propose a generalframework to build an interpretable graph-based deep denoiser (GDD) byunrolling a solution to a maximum a posteriori (MAP) problem equipped with agraph Laplacian regularizer (GLR) as signal prior. Leveraging a recent theoremshowing that any (pseudo-)linear denoiser $\boldsymbol \Psi$, under mildconditions, can be mapped to a solution of a MAP denoising problem regularizedusing GLR, we first initialize a graph Laplacian matrix $\mathbf L$ viatruncated Taylor Series Expansion (TSE) of $\boldsymbol \Psi^{-1}$. Then, wecompute the MAP linear system solution by unrolling iterations of the conjugategradient (CG) algorithm into a sequence of neural layers as a feed-forwardnetwork -- one that is amenable to parameter tuning. The resulting GDD networkis "graph-interpretable", low in parameter count, and easy to initialize thanksto $\mathbf L$ derived from a known well-performing denoiser $\boldsymbol\Psi$. Experimental results show that GDD achieves competitive image denoisingperformance compared to competitors, but employing far fewer parameters, and ismore robust to covariate shift.</description><author>Seyed Alireza Hosseini, Tam Thuc Do, Gene Cheung, Yuichi Tanaka</author><pubDate>Tue, 10 Sep 2024 17:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06676v1</guid></item><item><title>Liability and Insurance for Catastrophic Losses: the Nuclear Power Precedent and Lessons for AI</title><link>http://arxiv.org/abs/2409.06673v1</link><description>As AI systems become more autonomous and capable, experts warn of thempotentially causing catastrophic losses. Drawing on the successful precedentset by the nuclear power industry, this paper argues that developers offrontier AI models should be assigned limited, strict, and exclusive thirdparty liability for harms resulting from Critical AI Occurrences (CAIOs) -events that cause or easily could have caused catastrophic losses. Mandatoryinsurance for CAIO liability is recommended to overcome developers'judgment-proofness, mitigate winner's curse dynamics, and leverage insurers'quasi-regulatory abilities. Based on theoretical arguments and observationsfrom the analogous nuclear power context, insurers are expected to engage in amix of causal risk-modeling, monitoring, lobbying for stricter regulation, andproviding loss prevention guidance in the context of insuring againstheavy-tail risks from AI. While not a substitute for regulation, clearliability assignment and mandatory insurance can help efficiently allocateresources to risk-modeling and safe design, facilitating future regulatoryefforts.</description><author>Cristian Trout</author><pubDate>Tue, 10 Sep 2024 17:41:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06673v1</guid></item><item><title>Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort</title><link>http://arxiv.org/abs/2409.06672v1</link><description>Many experts believe that AI systems will sooner or later pose uninsurablerisks, including existential risks. This creates an extreme judgment-proofproblem: few if any parties can be held accountable ex post in the event ofsuch a catastrophe. This paper proposes a novel solution: agovernment-provided, mandatory indemnification program for AI developers. Theprogram uses risk-priced indemnity fees to induce socially optimal levels ofcare. Risk-estimates are determined by surveying experts, including indemnifieddevelopers. The Bayesian Truth Serum mechanism is employed to incent honest andeffortful responses. Compared to alternatives, this approach arguably betterleverages all private information, and provides a clearer signal to indemnifieddevelopers regarding what risks they must mitigate to lower their fees. It'srecommended that collected fees be used to help fund the safety researchdevelopers need, employing a fund matching mechanism (Quadratic Financing) toinduce an optimal supply of this public good. Under Quadratic Financing, safetyresearch projects would compete for private contributions from developers,signaling how much each is to be supplemented with public funds.</description><author>Cristian Trout</author><pubDate>Tue, 10 Sep 2024 17:41:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06672v1</guid></item><item><title>A Semantic Segmentation Approach on Sweet Orange Leaf Diseases Detection Utilizing YOLO</title><link>http://arxiv.org/abs/2409.06671v1</link><description>This research introduces an advanced method for diagnosing diseases in sweetorange leaves by utilising advanced artificial intelligence models like YOLOv8. Due to their significance as a vital agricultural product, sweet orangesencounter significant threats from a variety of diseases that harmfully affectboth their yield and quality. Conventional methods for disease detectionprimarily depend on manual inspection which is ineffective and frequently leadsto errors, resulting in delayed treatment and increased financial losses. Inresponse to this challenge, the research utilized YOLOv8 , harnessing theirproficiencies in detecting objects and analyzing images. YOLOv8 is recognizedfor its rapid and precise performance, while VIT is acknowledged for itsdetailed feature extraction abilities. Impressively, during both the trainingand validation stages, YOLOv8 exhibited a perfect accuracy of 80.4%, while VITachieved an accuracy of 99.12%, showcasing their potential to transform diseasedetection in agriculture. The study comprehensively examined the practicalchallenges related to the implementation of AI technologies in agriculture,encompassing the computational demands and user accessibility, and offeringviable solutions for broader usage. Moreover, it underscores the environmentalconsiderations, particularly the potential for reduced pesticide usage, therebypromoting sustainable farming and environmental conservation. These findingsprovide encouraging insights into the application of AI in agriculture,suggesting a transition towards more effective, sustainable, andtechnologically advanced farming methods. This research not only highlights theefficacy of YOLOv8 within a specific agricultural domain but also lays thefoundation for further studies that encompass a broader application in cropmanagement and sustainable agricultural practices.</description><author>Sabit Ahamed Preanto, Md. Taimur Ahad, Yousuf Rayhan Emon, Sumaya Mustofa, Md Alamin</author><pubDate>Tue, 10 Sep 2024 17:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06671v1</guid></item><item><title>Mitigating the Influence of Distractor Tasks in LMs with Prior-Aware Decoding</title><link>http://arxiv.org/abs/2401.17692v2</link><description>The broad capabilities of Language Models (LMs) can be limited by theirsensitivity to distractor tasks: LMs can infer secondary tasks from the promptin addition to the intended one, leading to unwanted outputs. For example,prompt injection attacks can cause models to deviate from explicit directives.In some 'inverse scaling' cases, this unwanted behaviour actually worsens asmodels scale up to at least 540B parameters. We present a theoretical frameworkthat interprets LMs as a product of experts that combine multiple datageneration processes. Based on this framework, we demonstrate prior-awaredecoding (PAD) - a simple contrastive inference method to reduce the influenceof distractor tasks. We apply PAD to eleven models, across four datasets, andfind improvements in 41 out of 44 task-model combinations, with a medianincrease in task completion proportion of 40%. The results suggest a promisingdirection for further development towards more reliable language models.</description><author>Raymond Douglas, Andis Draguns, Tomáš Gavenčiak</author><pubDate>Tue, 10 Sep 2024 17:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17692v2</guid></item><item><title>DA-MoE: Towards Dynamic Expert Allocation for Mixture-of-Experts Models</title><link>http://arxiv.org/abs/2409.06669v1</link><description>Transformer-based Mixture-of-Experts (MoE) models have been driving severalrecent technological advancements in Natural Language Processing (NLP). TheseMoE models adopt a router mechanism to determine which experts to activate forrouting input tokens. However, existing router mechanisms allocate a fixednumber of experts to each token, which neglects the varying importance ofdifferent input tokens. In this study, we propose a novel dynamic routermechanism that Dynamically Allocates a variable number of experts forMixture-of-Experts (DA-MoE) models based on an effective token importancemeasure. First, we show that the Transformer attention mechanism provides anatural and effective way of calculating token importance. Second, we propose adynamic router mechanism that effectively decides the optimal number of experts(K) and allocates the top-K experts for each input token. Third, comprehensiveexperiments on several benchmark datasets demonstrate that our DA-MoE approachconsistently outperforms the state-of-the-art Transformer based MoE model onthe popular GLUE benchmark.</description><author>Maryam Akhavan Aghdam, Hongpeng Jin, Yanzhao Wu</author><pubDate>Tue, 10 Sep 2024 17:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06669v1</guid></item><item><title>LLaMA-Omni: Seamless Speech Interaction with Large Language Models</title><link>http://arxiv.org/abs/2409.06666v1</link><description>Models like GPT-4o enable real-time interaction with large language models(LLMs) through speech, significantly enhancing user experience compared totraditional text-based interaction. However, there is still a lack ofexploration on how to build speech interaction models based on open-sourceLLMs. To address this, we propose LLaMA-Omni, a novel model architecturedesigned for low-latency and high-quality speech interaction with LLMs.LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,and a streaming speech decoder. It eliminates the need for speechtranscription, and can simultaneously generate text and speech responsesdirectly from speech instructions with extremely low latency. We build ourmodel based on the latest Llama-3.1-8B-Instruct model. To align the model withspeech interaction scenarios, we construct a dataset named InstructS2S-200K,which includes 200K speech instructions and corresponding speech responses.Experimental results show that compared to previous speech-language models,LLaMA-Omni provides better responses in both content and style, with a responselatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3days on just 4 GPUs, paving the way for the efficient development ofspeech-language models in the future.</description><author>Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng</author><pubDate>Tue, 10 Sep 2024 17:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06666v1</guid></item><item><title>Data Collection-free Masked Video Modeling</title><link>http://arxiv.org/abs/2409.06665v1</link><description>Pre-training video transformers generally requires a large amount of data,presenting significant challenges in terms of data collection costs andconcerns related to privacy, licensing, and inherent biases. Synthesizing datais one of the promising ways to solve these issues, yet pre-training solely onsynthetic data has its own challenges. In this paper, we introduce an effectiveself-supervised learning framework for videos that leverages readily availableand less costly static images. Specifically, we define the Pseudo MotionGenerator (PMG) module that recursively applies image transformations togenerate pseudo-motion videos from images. These pseudo-motion videos are thenleveraged in masked video modeling. Our approach is applicable to syntheticimages as well, thus entirely freeing video pre-training from data collectioncosts and other concerns in real data. Through experiments in actionrecognition tasks, we demonstrate that this framework allows effective learningof spatio-temporal features through pseudo-motion videos, significantlyimproving over existing methods which also use static images and partiallyoutperforming those using both real and synthetic videos. These results uncoverfragments of what video transformers learn through masked video modeling.</description><author>Yuchi Ishikawa, Masayoshi Kondo, Yoshimitsu Aoki</author><pubDate>Tue, 10 Sep 2024 17:34:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06665v1</guid></item><item><title>SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation of Large Language Models</title><link>http://arxiv.org/abs/2409.00055v2</link><description>The rapid advancement in large language models (LLMs) comes with asignificant increase in their parameter size, presenting challenges foradaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods arewidely used to adapt LLMs for downstream tasks efficiently. In this paper, wepropose Singular Values and Orthonormal Regularized Singular VectorsAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze thevariation of the parameters by performing singular value decomposition (SVD)and discuss and analyze SORSA's superiority in minimizing the alteration in theSVD aspect. Each SORSA adapter consists of two main parts: trainable principalsingular weights $W_p = U_p \Sigma_p V^\top_p$, and frozen residual weights$W_r = U_r \Sigma_r V^\top_r$. These parts are initialized by performing SVD onpre-trained weights. Moreover, we implement and analyze an orthonormalregularizer, which could effectively transfer the scaling information into$\Sigma_p$ and ultimately allows the training process to be more efficient.SORSA adapters could be merged during inference, thus eliminating any inferencelatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in ourexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassingLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSAoffers a new perspective on parameter-efficient fine-tuning, demonstratingremarkable performance. The code is available athttps://github.com/Gunale0926/SORSA.</description><author>Yang Cao</author><pubDate>Tue, 10 Sep 2024 17:26:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00055v2</guid></item><item><title>World-Grounded Human Motion Recovery via Gravity-View Coordinates</title><link>http://arxiv.org/abs/2409.06662v1</link><description>We present a novel method for recovering world-grounded human motion frommonocular video. The main challenge lies in the ambiguity of defining the worldcoordinate system, which varies between sequences. Previous approaches attemptto alleviate this issue by predicting relative motion in an autoregressivemanner, but are prone to accumulating errors. Instead, we propose estimatinghuman poses in a novel Gravity-View (GV) coordinate system, which is defined bythe world gravity and the camera view direction. The proposed GV system isnaturally gravity-aligned and uniquely defined for each video frame, largelyreducing the ambiguity of learning image-pose mapping. The estimated poses canbe transformed back to the world coordinate system using camera rotations,forming a global motion sequence. Additionally, the per-frame estimation avoidserror accumulation in the autoregressive methods. Experiments on in-the-wildbenchmarks demonstrate that our method recovers more realistic motion in boththe camera space and world-grounded settings, outperforming state-of-the-artmethods in both accuracy and speed. The code is available athttps://zju3dv.github.io/gvhmr/.</description><author>Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, Xiaowei Zhou</author><pubDate>Tue, 10 Sep 2024 17:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06662v1</guid></item><item><title>Sortformer: Seamless Integration of Speaker Diarization and ASR by Bridging Timestamps and Tokens</title><link>http://arxiv.org/abs/2409.06656v1</link><description>We propose Sortformer, a novel neural model for speaker diarization, trainedwith unconventional objectives compared to existing end-to-end diarizationmodels. The permutation problem in speaker diarization has long been regardedas a critical challenge. Most prior end-to-end diarization systems employpermutation invariant loss (PIL), which optimizes for the permutation thatyields the lowest error. In contrast, we introduce Sort Loss, which enables adiarization model to autonomously resolve permutation, with or without PIL. Wedemonstrate that combining Sort Loss and PIL achieves performance competitivewith state-of-the-art end-to-end diarization models trained exclusively withPIL. Crucially, we present a streamlined multispeaker ASR architecture thatleverages Sortformer as a speaker supervision model, embedding speaker labelestimation within the ASR encoder state using a sinusoidal kernel function.This approach resolves the speaker permutation problem through sortedobjectives, effectively bridging speaker-label timestamps and speaker tokens.In our experiments, we show that the proposed multispeaker ASR architecture,enhanced with speaker supervision, improves performance via adapter techniques.Code and trained models will be made publicly available via the NVIDIA NeMoframework</description><author>Taejin Park, Ivan Medennikov, Kunal Dhawan, Weiqing Wang, He Huang, Nithin Rao Koluguri, Krishna C. Puvvada, Jagadeesh Balam, Boris Ginsburg</author><pubDate>Tue, 10 Sep 2024 17:20:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06656v1</guid></item><item><title>Estimation and Inference for Causal Functions with Multiway Clustered Data</title><link>http://arxiv.org/abs/2409.06654v1</link><description>This paper proposes methods of estimation and uniform inference for a generalclass of causal functions, such as the conditional average treatment effectsand the continuous treatment effects, under multiway clustering. The causalfunction is identified as a conditional expectation of an adjusted(Neyman-orthogonal) signal that depends on high-dimensional nuisanceparameters. We propose a two-step procedure where the first step uses machinelearning to estimate the high-dimensional nuisance parameters. The second stepprojects the estimated Neyman-orthogonal signal onto a dictionary of basisfunctions whose dimension grows with the sample size. For this two-stepprocedure, we propose both the full-sample and the multiway cross-fittingestimation approaches. A functional limit theory is derived for theseestimators. To construct the uniform confidence bands, we develop a novelresampling procedure, called the multiway cluster-robust sieve score bootstrap,that extends the sieve score bootstrap (Chen and Christensen, 2018) to thenovel setting with multiway clustering. Extensive numerical simulationsshowcase that our methods achieve desirable finite-sample behaviors. We applythe proposed methods to analyze the causal relationship between mistrust levelsin Africa and the historical slave trade. Our analysis rejects the nullhypothesis of uniformly zero effects and reveals heterogeneous treatmenteffects, with significant impacts at higher levels of trade volumes.</description><author>Nan Liu, Yanbo Liu, Yuya Sasaki</author><pubDate>Tue, 10 Sep 2024 17:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06654v1</guid></item><item><title>KANtrol: A Physics-Informed Kolmogorov-Arnold Network Framework for Solving Multi-Dimensional and Fractional Optimal Control Problems</title><link>http://arxiv.org/abs/2409.06649v1</link><description>In this paper, we introduce the KANtrol framework, which utilizesKolmogorov-Arnold Networks (KANs) to solve optimal control problems involvingcontinuous time variables. We explain how Gaussian quadrature can be employedto approximate the integral parts within the problem, particularly forintegro-differential state equations. We also demonstrate how automaticdifferentiation is utilized to compute exact derivatives for integer-orderdynamics, while for fractional derivatives of non-integer order, we employmatrix-vector product discretization within the KAN framework. We tacklemulti-dimensional problems, including the optimal control of a 2D heat partialdifferential equation. The results of our simulations, which cover both forwardand parameter identification problems, show that the KANtrol frameworkoutperforms classical MLPs in terms of accuracy and efficiency.</description><author>Alireza Afzal Aghaei</author><pubDate>Tue, 10 Sep 2024 17:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06649v1</guid></item><item><title>CAMS: Convolution and Attention-Free Mamba-based Cardiac Image Segmentation</title><link>http://arxiv.org/abs/2406.05786v2</link><description>Convolutional Neural Networks (CNNs) and Transformer-based self-attentionmodels have become the standard for medical image segmentation. This paperdemonstrates that convolution and self-attention, while widely used, are notthe only effective methods for segmentation. Breaking with convention, wepresent a Convolution and self-Attention-free Mamba-based semantic SegmentationNetwork named CAMS-Net. Specifically, we design Mamba-based Channel Aggregatorand Spatial Aggregator, which are applied independently in each encoder-decoderstage. The Channel Aggregator extracts information across different channels,and the Spatial Aggregator learns features across different spatial locations.We also propose a Linearly Interconnected Factorized Mamba (LIFM) block toreduce the computational complexity of a Mamba block and to enhance itsdecision function by introducing a non-linearity between two factorized Mambablocks. Our model outperforms the existing state-of-the-art CNN,self-attention, and Mamba-based methods on CMR and M&amp;Ms-2 Cardiac segmentationdatasets, showing how this innovative, convolution, and self-attention-freemethod can inspire further research beyond CNN and Transformer paradigms,achieving linear complexity and reducing the number of parameters. Source codeand pre-trained models will be publicly available upon acceptance.</description><author>Abbas Khan, Muhammad Asad, Martin Benning, Caroline Roney, Gregory Slabaugh</author><pubDate>Tue, 10 Sep 2024 17:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05786v2</guid></item><item><title>Image Vectorization with Depth: convexified shape layers with depth ordering</title><link>http://arxiv.org/abs/2409.06648v1</link><description>Image vectorization is a process to convert a raster image into a scalablevector graphic format. Objective is to effectively remove the pixelizationeffect while representing boundaries of image by scaleable parameterizedcurves. We propose new image vectorization with depth which considers depthordering among shapes and use curvature-based inpainting for convexifyingshapes in vectorization process.From a given color quantized raster image, wefirst define each connected component of the same color as a shape layer, andconstruct depth ordering among them using a newly proposed depth orderingenergy. Global depth ordering among all shapes is described by a directedgraph, and we propose an energy to remove cycle within the graph. Afterconstructing depth ordering of shapes, we convexify occluded regions by Euler'selastica curvature-based variational inpainting, and leverage on the stabilityof Modica-Mortola double-well potential energy to inpaint large regions. Thisis following human vision perception that boundaries of shapes extend smoothly,and we assume shapes are likely to be convex. Finally, we fit B\'{e}zier curvesto the boundaries and save vectorization as a SVG file which allowssuperposition of curvature-based inpainted shapes following the depth ordering.This is a new way to vectorize images, by decomposing an image into scalableshape layers with computed depth ordering. This approach makes editing shapesand images more natural and intuitive. We also consider grouping shape layersfor semantic vectorization. We present various numerical results andcomparisons against recent layer-based vectorization methods to validate theproposed model.</description><author>Ho Law, Sung Ha Kang</author><pubDate>Tue, 10 Sep 2024 17:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06648v1</guid></item><item><title>Deep Visual Odometry with Events and Frames</title><link>http://arxiv.org/abs/2309.09947v3</link><description>Visual Odometry (VO) is crucial for autonomous robotic navigation, especiallyin GPS-denied environments like planetary terrains. To improve robustness,recent model-based VO systems have begun combining standard and event-basedcameras. While event cameras excel in low-light and high-speed motion, standardcameras provide dense and easier-to-track features. However, the field ofimage- and event-based VO still predominantly relies on model-based methods andis yet to fully integrate recent image-only advancements leveraging end-to-endlearning-based architectures. Seamlessly integrating the two modalities remainschallenging due to their different nature, one asynchronous, the other not,limiting the potential for a more effective image- and event-based VO. Weintroduce RAMP-VO, the first end-to-end learned image- and event-based VOsystem. It leverages novel Recurrent, Asynchronous, and Massively Parallel(RAMP) encoders capable of fusing asynchronous events with image data,providing 8x faster inference and 33% more accurate predictions than existingsolutions. Despite being trained only in simulation, RAMP-VO outperformsprevious methods on the newly introduced Apollo and Malapert datasets, and onexisting benchmarks, where it improves image- and event-based methods by 58.8%and 30.6%, paving the way for robust and asynchronous VO in space.</description><author>Roberto Pellerito, Marco Cannici, Daniel Gehrig, Joris Belhadj, Olivier Dubois-Matra, Massimo Casasco, Davide Scaramuzza</author><pubDate>Tue, 10 Sep 2024 17:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09947v3</guid></item><item><title>EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis</title><link>http://arxiv.org/abs/2409.06644v1</link><description>Early detection of eye diseases like glaucoma, macular degeneration, anddiabetic retinopathy is crucial for preventing vision loss. While artificialintelligence (AI) foundation models hold significant promise for addressingthese challenges, existing ophthalmic foundation models primarily focus on asingle modality, whereas diagnosing eye diseases requires multiple modalities.A critical yet often overlooked aspect is harnessing the multi-view informationacross various modalities for the same patient. Additionally, due to thelong-tail nature of ophthalmic diseases, standard fully supervised orunsupervised learning approaches often struggle. Therefore, it is essential tointegrate clinical text to capture a broader spectrum of diseases. We proposeEyeCLIP, a visual-language foundation model developed using over 2.77 millionmulti-modal ophthalmology images with partial text data. To fully leverage thelarge multi-modal unlabeled and labeled data, we introduced a pretrainingstrategy that combines self-supervised reconstructions, multi-modal imagecontrastive learning, and image-text contrastive learning to learn a sharedrepresentation of multiple modalities. Through evaluation using 14 benchmarkdatasets, EyeCLIP can be transferred to a wide range of downstream tasksinvolving ocular and systemic diseases, achieving state-of-the-art performancein disease classification, visual question answering, and cross-modalretrieval. EyeCLIP represents a significant advancement over previous methods,especially showcasing few-shot, even zero-shot capabilities in real-worldlong-tail scenarios.</description><author>Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He</author><pubDate>Tue, 10 Sep 2024 17:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06644v1</guid></item><item><title>Robust Single Rotation Averaging Revisited</title><link>http://arxiv.org/abs/2309.05388v5</link><description>In this work, we propose a novel method for robust single rotation averagingthat can efficiently handle an extremely large fraction of outliers. Ourapproach is to minimize the total truncated least unsquared deviations (TLUD)cost of geodesic distances. The proposed algorithm consists of three steps:First, we consider each input rotation as a potential initial solution andchoose the one that yields the least sum of truncated chordal deviations. Next,we obtain the inlier set using the initial solution and compute its chordal$L_2$-mean. Finally, starting from this estimate, we iteratively compute thegeodesic $L_1$-mean of the inliers using the Weiszfeld algorithm on $SO(3)$. Anextensive evaluation shows that our method is robust against up to 99% outliersgiven a sufficient number of accurate inliers, outperforming the current stateof the art.</description><author>Seong Hun Lee, Javier Civera</author><pubDate>Tue, 10 Sep 2024 16:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05388v5</guid></item><item><title>TeXBLEU: Automatic Metric for Evaluate LaTeX Format</title><link>http://arxiv.org/abs/2409.06639v1</link><description>LaTeX is highly suited to creating documents with special formatting,particularly in the fields of science, technology, mathematics, and computerscience. Despite the increasing use of mathematical expressions in LaTeX formatwith language models, there are no evaluation metrics for evaluating them. Inthis study, we propose TeXBLEU, an evaluation metric tailored for mathematicalexpressions in LaTeX format, based on the n-gram-based BLEU metric that iswidely used for translation tasks. The proposed TeXBLEU includes a predefinedtokenizer trained on the arXiv paper dataset and a finetuned embedding model.It also considers the positional embedding of tokens. Simultaneously, TeXBLEUcompares tokens based on n-grams and computes the score using exponentiation ofa logarithmic sum, similar to the original BLEU. Experimental results show thatTeXBLEU outperformed traditional evaluation metrics such as BLEU, Rouge, CER,and WER when compared to human evaluation data on the test dataset of theMathBridge dataset, which contains 1,000 data points. The average correlationcoefficient with human evaluation was 0.71, which is an improvement of 87%compared with BLEU, which had the highest correlation with human evaluationdata among the existing metrics. The code is available athttps://github.com/KyuDan1/TeXBLEU.</description><author>Kyudan Jung, Nam-Joon Kim, Hyongon Ryu, Sieun Hyeon, Seung-jun Lee, Hyeok-jae Lee</author><pubDate>Tue, 10 Sep 2024 16:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06639v1</guid></item><item><title>MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders</title><link>http://arxiv.org/abs/2409.06635v1</link><description>The rapid advancements in large language models (LLMs) have significantlyenhanced natural language processing capabilities, facilitating the developmentof AudioLLMs that process and understand speech and audio inputs alongsidetext. Existing AudioLLMs typically combine a pre-trained audio encoder with apre-trained LLM, which are subsequently finetuned on specific audio tasks.However, the pre-trained audio encoder has constrained capacity to capturefeatures for new tasks and datasets. To address this, we propose to incorporatemixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWEsupplements a base encoder with a pool of relatively light weight encoders,selectively activated based on the audio input to enhance feature extractionwithout significantly increasing model size. Our empirical results demonstratethat MoWE effectively improves multi-task performance, broadening theapplicability of AudioLLMs to more diverse audio tasks.</description><author>Wenyu Zhang, Shuo Sun, Bin Wang, Xunlong Zou, Zhuohan Liu, Yingxu He, Geyu Lin, Nancy F. Chen, Ai Ti Aw</author><pubDate>Tue, 10 Sep 2024 16:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06635v1</guid></item><item><title>SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation</title><link>http://arxiv.org/abs/2409.06633v1</link><description>In recent years, the development of diffusion models has led to significantprogress in image and video generation tasks, with pre-trained models like theStable Diffusion series playing a crucial role. Inspired by model pruning whichlightens large pre-trained models by removing unimportant parameters, wepropose a novel model fine-tuning method to make full use of these ineffectiveparameters and enable the pre-trained model with new task-specifiedcapabilities. In this work, we first investigate the importance of parametersin pre-trained diffusion models, and discover that the smallest 10% to 20% ofparameters by absolute values do not contribute to the generation process.Based on this observation, we propose a method termed SaRA that re-utilizesthese temporarily ineffective parameters, equating to optimizing a sparseweight matrix to learn the task-specific knowledge. To mitigate overfitting, wepropose a nuclear-norm-based low-rank sparse training scheme for efficientfine-tuning. Furthermore, we design a new progressive parameter adjustmentstrategy to make full use of the re-trained/finetuned parameters. Finally, wepropose a novel unstructural backpropagation strategy, which significantlyreduces memory costs during fine-tuning. Our method enhances the generativecapabilities of pre-trained models in downstream applications and outperformstraditional fine-tuning methods like LoRA in maintaining model's generalizationability. We validate our approach through fine-tuning experiments on SD models,demonstrating significant improvements. SaRA also offers a practical advantagethat requires only a single line of code modification for efficientimplementation and is seamlessly compatible with existing methods.</description><author>Teng Hu, Jiangning Zhang, Ran Yi, Hongrui Huang, Yabiao Wang, Lizhuang Ma</author><pubDate>Tue, 10 Sep 2024 16:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06633v1</guid></item><item><title>System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning</title><link>http://arxiv.org/abs/2305.02128v2</link><description>Evolutionary science provides evidence that diversity confers resilience innatural systems. Yet, traditional multi-agent reinforcement learning techniquescommonly enforce homogeneity to increase training sample efficiency. When asystem of learning agents is not constrained to homogeneous policies,individuals may develop diverse behaviors, resulting in emergentcomplementarity that benefits the system. Despite this, there is a surprisinglack of tools that quantify behavioral diversity. Such techniques would pavethe way towards understanding the impact of diversity in collective artificialintelligence and enabling its control. In this paper, we introduce SystemNeural Diversity (SND): a measure of behavioral heterogeneity in multi-agentsystems. We discuss and prove its theoretical properties, and compare it withalternate, state-of-the-art behavioral diversity metrics used in the roboticsdomain. Through simulations of a variety of cooperative multi-robot tasks, weshow how our metric constitutes an important tool that enables measurement andcontrol of behavioral heterogeneity. In dynamic tasks, where the problem isaffected by repeated disturbances during training, we show that SND allows usto measure latent resilience skills acquired by the agents, while otherproxies, such as task performance (reward), fail to. Finally, we show how themetric can be employed to control diversity, allowing us to enforce a desiredheterogeneity set-point or range. We demonstrate how this paradigm can be usedto bootstrap the exploration phase, finding optimal policies faster, thusenabling novel and more efficient MARL paradigms.</description><author>Matteo Bettini, Ajay Shankar, Amanda Prorok</author><pubDate>Tue, 10 Sep 2024 16:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02128v2</guid></item><item><title>Caterpillar: A Pure-MLP Architecture with Shifted-Pillars-Concatenation</title><link>http://arxiv.org/abs/2305.17644v3</link><description>Modeling in Computer Vision has evolved to MLPs. Vision MLPs naturally lacklocal modeling capability, to which the simplest treatment is combined withconvolutional layers. Convolution, famous for its sliding window scheme, alsosuffers from this scheme of redundancy and lower parallel computation. In thispaper, we seek to dispense with the windowing scheme and introduce a moreelaborate and parallelizable method to exploit locality. To this end, wepropose a new MLP module, namely Shifted-Pillars-Concatenation (SPC), thatconsists of two steps of processes: (1) Pillars-Shift, which generates fourneighboring maps by shifting the input image along four directions, and (2)Pillars-Concatenation, which applies linear transformations and concatenationon the maps to aggregate local features. SPC module offers superior localmodeling power and performance gains, making it a promising alternative to theconvolutional layer. Then, we build a pure-MLP architecture called Caterpillarby replacing the convolutional layer with the SPC module in a hybrid model ofsMLPNet. Extensive experiments show Caterpillar's excellent performance on bothsmall-scale and ImageNet-1k classification benchmarks, with remarkablescalability and transfer capability possessed as well. The code is available athttps://github.com/sunjin19126/Caterpillar.</description><author>Jin Sun, Xiaoshuang Shi, Zhiyuan Wang, Kaidi Xu, Heng Tao Shen, Xiaofeng Zhu</author><pubDate>Tue, 10 Sep 2024 16:42:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17644v3</guid></item><item><title>Optimal Regularization for a Data Source</title><link>http://arxiv.org/abs/2212.13597v4</link><description>In optimization-based approaches to inverse problems and to statisticalestimation, it is common to augment criteria that enforce data fidelity with aregularizer that promotes desired structural properties in the solution. Thechoice of a suitable regularizer is typically driven by a combination of priordomain information and computational considerations. Convex regularizers areattractive computationally but they are limited in the types of structure theycan promote. On the other hand, nonconvex regularizers are more flexible in theforms of structure they can promote and they have showcased strong empiricalperformance in some applications, but they come with the computationalchallenge of solving the associated optimization problems. In this paper, weseek a systematic understanding of the power and the limitations of convexregularization by investigating the following questions: Given a distribution,what is the optimal regularizer for data drawn from the distribution? Whatproperties of a data source govern whether the optimal regularizer is convex?We address these questions for the class of regularizers specified byfunctionals that are continuous, positively homogeneous, and positive away fromthe origin. We say that a regularizer is optimal for a data distribution if theGibbs density with energy given by the regularizer maximizes the populationlikelihood (or equivalently, minimizes cross-entropy loss) over allregularizer-induced Gibbs densities. As the regularizers we consider are inone-to-one correspondence with star bodies, we leverage dual Brunn-Minkowskitheory to show that a radial function derived from a data distribution is akinto a ``computational sufficient statistic'' as it is the key quantity foridentifying optimal regularizers and for assessing the amenability of a datasource to convex regularization.</description><author>Oscar Leong, Eliza O'Reilly, Yong Sheng Soh, Venkat Chandrasekaran</author><pubDate>Tue, 10 Sep 2024 16:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.13597v4</guid></item><item><title>Generalization of Graph Neural Networks is Robust to Model Mismatch</title><link>http://arxiv.org/abs/2408.13878v2</link><description>Graph neural networks (GNNs) have demonstrated their effectiveness in varioustasks supported by their generalization capabilities. However, the currentanalysis of GNN generalization relies on the assumption that training andtesting data are independent and identically distributed (i.i.d). This imposeslimitations on the cases where a model mismatch exists when generating testingdata. In this paper, we examine GNNs that operate on geometric graphs generatedfrom manifold models, explicitly focusing on scenarios where there is amismatch between manifold models generating training and testing data. Ouranalysis reveals the robustness of the GNN generalization in the presence ofsuch model mismatch. This indicates that GNNs trained on graphs generated froma manifold can still generalize well to unseen nodes and graphs generated froma mismatched manifold. We attribute this mismatch to both node featureperturbations and edge perturbations within the generated graph. Our findingsindicate that the generalization gap decreases as the number of nodes grows inthe training graph while increasing with larger manifold dimension as well aslarger mismatch. Importantly, we observe a trade-off between the generalizationof GNNs and the capability to discriminate high-frequency components whenfacing a model mismatch. The most important practical consequence of thisanalysis is to shed light on the filter design of generalizable GNNs robust tomodel mismatch. We verify our theoretical findings with experiments on multiplereal-world datasets.</description><author>Zhiyang Wang, Juan Cervino, Alejandro Ribeiro</author><pubDate>Tue, 10 Sep 2024 16:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13878v2</guid></item><item><title>Towards Localizing Structural Elements: Merging Geometrical Detection with Semantic Verification in RGB-D Data</title><link>http://arxiv.org/abs/2409.06625v1</link><description>RGB-D cameras supply rich and dense visual and spatial information forvarious robotics tasks such as scene understanding, map reconstruction, andlocalization. Integrating depth and visual information can aid robots inlocalization and element mapping, advancing applications like 3D scene graphgeneration and Visual Simultaneous Localization and Mapping (VSLAM). Whilepoint cloud data containing such information is primarily used for enhancedscene understanding, exploiting their potential to capture and represent richsemantic information has yet to be adequately targeted. This paper presents areal-time pipeline for localizing building components, including wall andground surfaces, by integrating geometric calculations for pure 3D planedetection followed by validating their semantic category using point cloud datafrom RGB-D cameras. It has a parallel multi-thread architecture to preciselyestimate poses and equations of all the planes detected in the environment,filters the ones forming the map structure using a panoptic segmentationvalidation, and keeps only the validated building components. Incorporating theproposed method into a VSLAM framework confirmed that constraining the map withthe detected environment-driven semantic elements can improve sceneunderstanding and map reconstruction accuracy. It can also ensure(re-)association of these detected components into a unified 3D scene graph,bridging the gap between geometric accuracy and semantic understanding.Additionally, the pipeline allows for the detection of potential higher-levelstructural entities, such as rooms, by identifying the relationships betweenbuilding components based on their layout.</description><author>Ali Tourani, Saad Ejaz, Hriday Bavle, Jose Luis Sanchez-Lopez, Holger Voos</author><pubDate>Tue, 10 Sep 2024 16:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06625v1</guid></item><item><title>A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio</title><link>http://arxiv.org/abs/2409.06624v1</link><description>Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) toobtain the unfamiliar language skill or adapt into new domains. The hugetraining cost of CPT often asks for cautious choice of key hyper-parameterssuch as the mixture ratio of extra language or domain corpus. However, there isno systematic study which bridge the gap between the optimal mixture ratio andthe actual model performance, and the gap between experimental scaling law andthe actual deployment in the full model size. In this paper, we perform CPT onLlama-3 8B and 70B to enhance its Chinese ability. We study the optimalcorrelation between the Additional Language Mixture Ratio (ALMR) and theLearning Rate (LR) on the 8B size which directly indicate the optimalexperimental set up. By thorough choice of hyper-parameter, and subsequentfine-tuning, the model capability is improved not only on the Chinese-relatedbenchmark, but also some specific domains including math, coding and emotionalintelligence. We deploy the final 70B version of LLM on an real-life chatsystem which obtain satisfying performance.</description><author>Ningyuan Xi, Yetao Wu, Kun Fan, Teng Chen, Qingqing Gu, Peng Yu, Jinxian Qu, Chenxi Liu, Zhonglin Jiang, Yong Chen, Luo Ji</author><pubDate>Tue, 10 Sep 2024 16:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06624v1</guid></item><item><title>What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence</title><link>http://arxiv.org/abs/2409.05731v2</link><description>Explanations for autonomous vehicle (AV) decisions may build trust, however,explanations can contain errors. In a simulated driving study (n = 232), wetested how AV explanation errors, driving context characteristics (perceivedharm and driving difficulty), and personal traits (prior trust and expertise)affected a passenger's comfort in relying on an AV, preference for control,confidence in the AV's ability, and explanation satisfaction. Errors negativelyaffected all outcomes. Surprisingly, despite identical driving, explanationerrors reduced ratings of the AV's driving ability. Severity and potential harmamplified the negative impact of errors. Contextual harm and driving difficultydirectly impacted outcome ratings and influenced the relationship betweenerrors and outcomes. Prior trust and expertise were positively associated withoutcome ratings. Results emphasize the need for accurate, contextuallyadaptive, and personalized AV explanations to foster trust, reliance,satisfaction, and confidence. We conclude with design, research, and deploymentrecommendations for trustworthy AV explanation systems.</description><author>Robert Kaufman, Aaron Broukhim, David Kirsh, Nadir Weibel</author><pubDate>Tue, 10 Sep 2024 16:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05731v2</guid></item><item><title>Exploring Italian sentence embeddings properties through multi-tasking</title><link>http://arxiv.org/abs/2409.06622v1</link><description>We investigate to what degree existing LLMs encode abstract linguisticinformation in Italian in a multi-task setting. We exploit curated syntheticdata on a large scale -- several Blackbird Language Matrices (BLMs) problems inItalian -- and use them to study how sentence representations built usingpre-trained language models encode specific syntactic and semantic information.We use a two-level architecture to model separately a compression of thesentence embeddings into a representation that contains relevant informationfor a task, and a BLM task. We then investigate whether we can obtaincompressed sentence representations that encode syntactic and semanticinformation relevant to several BLM tasks. While we expected that the sentencestructure -- in terms of sequence of phrases/chunks -- and chunk propertiescould be shared across tasks, performance and error analysis show that theclues for the different tasks are encoded in different manners in the sentenceembeddings, suggesting that abstract linguistic notions such as constituents orthematic roles does not seem to be present in the pretrained sentenceembeddings.</description><author>Vivi Nastase, Giuseppe Samo, Chunyang Jiang, Paola Merlo</author><pubDate>Tue, 10 Sep 2024 16:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06622v1</guid></item><item><title>MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View Guidance and Surface Densification</title><link>http://arxiv.org/abs/2409.06620v1</link><description>The field of text-to-3D content generation has made significant progress ingenerating realistic 3D objects, with existing methodologies like ScoreDistillation Sampling (SDS) offering promising guidance. However, these methodsoften encounter the "Janus" problem-multi-face ambiguities due to impreciseguidance. Additionally, while recent advancements in 3D gaussian splitting haveshown its efficacy in representing 3D volumes, optimization of thisrepresentation remains largely unexplored. This paper introduces a unifiedframework for text-to-3D content generation that addresses these critical gaps.Our approach utilizes multi-view guidance to iteratively form the structure ofthe 3D model, progressively enhancing detail and accuracy. We also introduce anovel densification algorithm that aligns gaussians close to the surface,optimizing the structural integrity and fidelity of the generated models.Extensive experiments validate our approach, demonstrating that it produceshigh-quality visual outputs with minimal time cost. Notably, our methodachieves high-quality results within half an hour of training, offering asubstantial efficiency gain over most existing methods, which require hours oftraining time to achieve comparable results.</description><author>Phu Pham, Aradhya N. Mathur, Ojaswa Sharma, Aniket Bera</author><pubDate>Tue, 10 Sep 2024 16:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06620v1</guid></item><item><title>A Manifold Perspective on the Statistical Generalization of Graph Neural Networks</title><link>http://arxiv.org/abs/2406.05225v3</link><description>Convolutional neural networks have been successfully extended to operate ongraphs, giving rise to Graph Neural Networks (GNNs). GNNs combine informationfrom adjacent nodes by successive applications of graph convolutions. GNNs havebeen implemented successfully in various learning tasks while the theoreticalunderstanding of their generalization capability is still in progress. In thispaper, we leverage manifold theory to analyze the statistical generalizationgap of GNNs operating on graphs constructed on sampled points from manifolds.We study the generalization gaps of GNNs on both node-level and graph-leveltasks. We show that the generalization gaps decrease with the number of nodesin the training graphs, which guarantees the generalization of GNNs to unseenpoints over manifolds. We validate our theoretical results in multiplereal-world datasets.</description><author>Zhiyang Wang, Juan Cervino, Alejandro Ribeiro</author><pubDate>Tue, 10 Sep 2024 16:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05225v3</guid></item><item><title>Hierarchical Multi-Label Classification with Missing Information for Benthic Habitat Imagery</title><link>http://arxiv.org/abs/2409.06618v1</link><description>In this work, we apply state-of-the-art self-supervised learning techniqueson a large dataset of seafloor imagery, \textit{BenthicNet}, and study theirperformance for a complex hierarchical multi-label (HML) classificationdownstream task. In particular, we demonstrate the capacity to conduct HMLtraining in scenarios where there exist multiple levels of missing annotationinformation, an important scenario for handling heterogeneous real-world datacollected by multiple research groups with differing data collection protocols.We find that, when using smaller one-hot image label datasets typical of localor regional scale benthic science projects, models pre-trained withself-supervision on a larger collection of in-domain benthic data outperformmodels pre-trained on ImageNet. In the HML setting, we find the model canattain a deeper and more precise classification if it is pre-trained withself-supervision on in-domain data. We hope this work can establish a benchmarkfor future models in the field of automated underwater image annotation tasksand can guide work in other domains with hierarchical annotations of mixedresolution.</description><author>Isaac Xu, Benjamin Misiuk, Scott C. Lowe, Martin Gillis, Craig J. Brown, Thomas Trappenberg</author><pubDate>Tue, 10 Sep 2024 16:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06618v1</guid></item><item><title>When to Extract ReID Features: A Selective Approach for Improved Multiple Object Tracking</title><link>http://arxiv.org/abs/2409.06617v1</link><description>Extracting and matching Re-Identification (ReID) features is used by manystate-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularlyeffective against frequent and long-term occlusions. While end-to-end objectdetection and tracking have been the main focus of recent research, they haveyet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus,from an application standpoint, methods with separate detection and embeddingremain the best option for accuracy, modularity, and ease of implementation,though they are impractical for edge devices due to the overhead involved. Inthis paper, we investigate a selective approach to minimize the overhead offeature extraction while preserving accuracy, modularity, and ease ofimplementation. This approach can be integrated into various SOTA methods. Wedemonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT.Experiments on MOT17, MOT20, and DanceTrack datasets show that our mechanismretains the advantages of feature extraction during occlusions whilesignificantly reducing runtime. Additionally, it improves accuracy bypreventing confusion in the feature-matching stage, particularly in cases ofdeformation and appearance similarity, which are common in DanceTrack.https://github.com/emirhanbayar/Fast-StrongSORT,https://github.com/emirhanbayar/Fast-Deep-OC-SORT</description><author>Emirhan Bayar, Cemal Aker</author><pubDate>Tue, 10 Sep 2024 16:14:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06617v1</guid></item><item><title>One-Shot Imitation under Mismatched Execution</title><link>http://arxiv.org/abs/2409.06615v1</link><description>Human demonstrations as prompts are a powerful way to program robots to dolong-horizon manipulation tasks. However, directly translating suchdemonstrations into robot-executable actions poses significant challenges dueto execution mismatches, such as different movement styles and physicalcapabilities. Existing methods either rely on robot-demonstrator paired data,which is infeasible to scale, or overly rely on frame-level visualsimilarities, which fail to hold. To address these challenges, we proposeRHyME, a novel framework that automatically establishes task executioncorrespondences between the robot and the demonstrator by using optimaltransport costs. Given long-horizon robot demonstrations, RHyME synthesizessemantically equivalent human demonstrations by retrieving and composingsimilar short-horizon human clips, facilitating effective policy trainingwithout the need for paired data. We show that RHyME outperforms a range ofbaselines across various cross-embodiment datasets on all degrees ofmismatches. Through detailed analysis, we uncover insights for learning andleveraging cross-embodiment visual representations.</description><author>Kushal Kedia, Prithwish Dan, Sanjiban Choudhury</author><pubDate>Tue, 10 Sep 2024 16:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06615v1</guid></item><item><title>DemoStart: Demonstration-led auto-curriculum applied to sim-to-real with multi-fingered robots</title><link>http://arxiv.org/abs/2409.06613v1</link><description>We present DemoStart, a novel auto-curriculum reinforcement learning methodcapable of learning complex manipulation behaviors on an arm equipped with athree-fingered robotic hand, from only a sparse reward and a handful ofdemonstrations in simulation. Learning from simulation drastically reduces thedevelopment cycle of behavior generation, and domain randomization techniquesare leveraged to achieve successful zero-shot sim-to-real transfer. Transferredpolicies are learned directly from raw pixels from multiple cameras and robotproprioception. Our approach outperforms policies learned from demonstrationson the real robot and requires 100 times fewer demonstrations, collected insimulation. More details and videos in https://sites.google.com/view/demostart.</description><author>Maria Bauza, Jose Enrique Chen, Valentin Dalibard, Nimrod Gileadi, Roland Hafner, Murilo F. Martins, Joss Moore, Rugile Pevceviciute, Antoine Laurens, Dushyant Rao, Martina Zambelli, Martin Riedmiller, Jon Scholz, Konstantinos Bousmalis, Francesco Nori, Nicolas Heess</author><pubDate>Tue, 10 Sep 2024 16:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06613v1</guid></item><item><title>Unlocking the Use of Raw Multispectral Earth Observation Imagery for Onboard Artificial Intelligence</title><link>http://arxiv.org/abs/2305.11891v2</link><description>Nowadays, there is growing interest in applying Artificial Intelligence (AI)on board Earth Observation (EO) satellites for time-critical applications, suchas natural disaster response. However, the unavailability of raw satellite datacurrently hinders research on lightweight pre-processing techniques and limitsthe exploration of end-to-end pipelines, which could offer more efficient andaccurate extraction of insights directly from the source data. To fill thisgap, this work presents a novel methodology to automate the creation ofdatasets for the detection of target events (e.g., warm thermal hotspots) orobjects (e.g., vessels) from Sentinel-2 raw data and other multispectral EOpushbroom raw imagery. The presented approach first processes the raw data byapplying a pipeline consisting of spatial band registration and georeferencingof the raw data pixels. Then, it detects the target events by leveragingevent-specific state-of-the-art algorithms on the Level-1C products, which aremosaicked and cropped on the georeferenced correspondent raw granule area. Thedetected events are finally re-projected back onto the corresponding rawimages. We apply the proposed methodology to realize THRawS (Thermal Hotspotsin Raw Sentinel-2 data), the first dataset of Sentinel-2 raw data containingwarm thermal hotspots. THRawS includes 1090 samples containing wildfires,volcanic eruptions, and 33,335 event-free acquisitions to enable thermalhotspot detection and general classification applications. This dataset andassociated toolkits provide the community with both an immediately usefulresource as well as a framework and methodology acting as a template for futureadditions. With this work, we hope to pave the way for research onenergy-efficient pre-processing algorithms and AI-based end-to-end processingsystems on board EO satellites.</description><author>Gabriele Meoni, Roberto Del Prete, Federico Serva, Alix De Beussche, Olivier Colin, Nicolas Longépé</author><pubDate>Tue, 10 Sep 2024 16:04:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11891v2</guid></item><item><title>Label-free Monitoring of Self-Supervised Learning Progress</title><link>http://arxiv.org/abs/2409.06612v1</link><description>Self-supervised learning (SSL) is an effective method for exploitingunlabelled data to learn a high-level embedding space that can be used forvarious downstream tasks. However, existing methods to monitor the quality ofthe encoder -- either during training for one model or to compare severaltrained models -- still rely on access to annotated data. When SSLmethodologies are applied to new data domains, a sufficiently large labelleddataset may not always be available. In this study, we propose severalevaluation metrics which can be applied on the embeddings of unlabelled dataand investigate their viability by comparing them to linear probe accuracy (acommon metric which utilizes an annotated dataset). In particular, we apply$k$-means clustering and measure the clustering quality with the silhouettescore and clustering agreement. We also measure the entropy of the embeddingdistribution. We find that while the clusters did correspond better to theground truth annotations as training of the network progressed, label-freeclustering metrics correlated with the linear probe accuracy only when trainingwith SSL methods SimCLR and MoCo-v2, but not with SimSiam. Additionally,although entropy did not always have strong correlations with LP accuracy, thisappears to be due to instability arising from early training, with the metricstabilizing and becoming more reliable at later stages of learning.Furthermore, while entropy generally decreases as learning progresses, thistrend reverses for SimSiam. More research is required to establish the causefor this unexpected behaviour. Lastly, we find that while clustering basedapproaches are likely only viable for same-architecture comparisons, entropymay be architecture-independent.</description><author>Isaac Xu, Scott Lowe, Thomas Trappenberg</author><pubDate>Tue, 10 Sep 2024 16:04:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06612v1</guid></item><item><title>NeFL: Nested Model Scaling for Federated Learning with System Heterogeneous Clients</title><link>http://arxiv.org/abs/2308.07761v3</link><description>Federated learning (FL) enables distributed training while preserving dataprivacy, but stragglers-slow or incapable clients-can significantly slow downthe total training time and degrade performance. To mitigate the impact ofstragglers, system heterogeneity, including heterogeneous computing and networkbandwidth, has been addressed. While previous studies have addressed systemheterogeneity by splitting models into submodels, they offer limitedflexibility in model architecture design, without considering potentialinconsistencies arising from training multiple submodel architectures. Wepropose nested federated learning (NeFL), a generalized framework thatefficiently divides deep neural networks into submodels using both depthwiseand widthwise scaling. To address the inconsistency arising from trainingmultiple submodel architectures, NeFL decouples a subset of parameters fromthose being trained for each submodel. An averaging method is proposed tohandle these decoupled parameters during aggregation. NeFL enablesresource-constrained devices to effectively participate in the FL pipeline,facilitating larger datasets for model training. Experiments demonstrate thatNeFL achieves performance gain, especially for the worst-case submodel comparedto baseline approaches (7.63% improvement on CIFAR-100). Furthermore, NeFLaligns with recent advances in FL, such as leveraging pre-trained models andaccounting for statistical heterogeneity. Our code is available online.</description><author>Honggu Kang, Seohyeon Cha, Jinwoo Shin, Jongmyeong Lee, Joonhyuk Kang</author><pubDate>Tue, 10 Sep 2024 16:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07761v3</guid></item><item><title>DNN-Defender: A Victim-Focused In-DRAM Defense Mechanism for Taming Adversarial Weight Attack on DNNs</title><link>http://arxiv.org/abs/2305.08034v2</link><description>With deep learning deployed in many security-sensitive areas, machinelearning security is becoming progressively important. Recent studiesdemonstrate attackers can exploit system-level techniques exploiting theRowHammer vulnerability of DRAM to deterministically and precisely flip bits inDeep Neural Networks (DNN) model weights to affect inference accuracy. Theexisting defense mechanisms are software-based, such as weight reconstructionrequiring expensive training overhead or performance degradation. On the otherhand, generic hardware-based victim-/aggressor-focused mechanisms imposeexpensive hardware overheads and preserve the spatial connection between victimand aggressor rows. In this paper, we present the first DRAM-basedvictim-focused defense mechanism tailored for quantized DNNs, namedDNN-Defender that leverages the potential of in-DRAM swapping to withstand thetargeted bit-flip attacks with a priority protection mechanism. Our resultsindicate that DNN-Defender can deliver a high level of protection downgradingthe performance of targeted RowHammer attacks to a random attack level. Inaddition, the proposed defense has no accuracy drop on CIFAR-10 and ImageNetdatasets without requiring any software training or incurring hardwareoverhead.</description><author>Ranyang Zhou, Sabbir Ahmed, Adnan Siraj Rakin, Shaahin Angizi</author><pubDate>Tue, 10 Sep 2024 16:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08034v2</guid></item><item><title>Improving the Precision of CNNs for Magnetic Resonance Spectral Modeling</title><link>http://arxiv.org/abs/2409.06609v1</link><description>Magnetic resonance spectroscopic imaging is a widely available imagingmodality that can non-invasively provide a metabolic profile of the tissue ofinterest, yet is challenging to integrate clinically. One major reason is theexpensive, expert data processing and analysis that is required. Using machinelearning to predict MRS-related quantities offers avenues around this problem,but deep learning models bring their own challenges, especially model trust.Current research trends focus primarily on mean error metrics, butcomprehensive precision metrics are also needed, e.g. standard deviations,confidence intervals, etc.. This work highlights why more comprehensive errorcharacterization is important and how to improve the precision of CNNs forspectral modeling, a quantitative task. The results highlight advantages andtrade-offs of these techniques that should be considered when addressing suchregression tasks with CNNs. Detailed insights into the underlying mechanisms ofeach technique, and how they interact with other techniques, are discussed indepth.</description><author>John LaMaster, Dhritiman Das, Florian Kofler, Jason Crane, Yan Li, Tobias Lasser, Bjoern H Menze</author><pubDate>Tue, 10 Sep 2024 16:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06609v1</guid></item><item><title>Simulation-based Scenario Generation for Robust Hybrid AI for Autonomy</title><link>http://arxiv.org/abs/2409.06608v1</link><description>Application of Unmanned Aerial Vehicles (UAVs) in search and rescue,emergency management, and law enforcement has gained traction with the adventof low-cost platforms and sensor payloads. The emergence of hybrid neural andsymbolic AI approaches for complex reasoning is expected to further push theboundaries of these applications with decreasing levels of human intervention.However, current UAV simulation environments lack semantic context suited tothis hybrid approach. To address this gap, HAMERITT (Hybrid Ai MissionEnvironment for RapId Training and Testing) provides a simulation-basedautonomy software framework that supports the training, testing and assuranceof neuro-symbolic algorithms for autonomous maneuver and perception reasoning.HAMERITT includes scenario generation capabilities that offer mission-relevantcontextual symbolic information in addition to raw sensor data. Scenariosinclude symbolic descriptions for entities of interest and their relations toscene elements, as well as spatial-temporal constraints in the form oftime-bounded areas of interest with prior probabilities and restricted zoneswithin those areas. HAMERITT also features support for training distinctalgorithm threads for maneuver vs. perception within an end-to-end mission run.Future work includes improving scenario realism and scaling symbolic contextgeneration through automated workflow.</description><author>Hambisa Keno, Nicholas J. Pioch, Christopher Guagliano, Timothy H. Chung</author><pubDate>Tue, 10 Sep 2024 16:00:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06608v1</guid></item><item><title>An Ontology-based Approach Towards Traceable Behavior Specifications in Automated Driving</title><link>http://arxiv.org/abs/2409.06607v1</link><description>Vehicles in public traffic that are equipped with Automated Driving Systemsare subject to a number of expectations: Among other aspects, their behaviorshould be safe, conforming to the rules of the road and provide mobility totheir users. This poses challenges for the developers of such systems:Developers are responsible for specifying this behavior, for example, in termsof requirements at system design time. As we will discuss in the article, thisspecification always involves the need for assumptions and trade-offs. As aresult, insufficiencies in such a behavior specification can occur that canpotentially lead to unsafe system behavior. In order to support theidentification of specification insufficiencies, requirements and respectiveassumptions need to be made explicit. In this article, we propose the SemanticNorm Behavior Analysis as an ontology-based approach to specify the behaviorfor an Automated Driving System equipped vehicle. We use ontologies to formallyrepresent specified behavior for a targeted operational environment, and toestablish traceability between specified behavior and the addressed stakeholderneeds. Furthermore, we illustrate the application of the Semantic Norm BehaviorAnalysis in two example scenarios and evaluate our results.</description><author>Nayel Fabian Salem, Marcus Nolte, Veronica Haber, Till Menzel, Hans Steege, Robert Graubohm, Markus Maurer</author><pubDate>Tue, 10 Sep 2024 16:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06607v1</guid></item><item><title>Interactive 3D Segmentation for Primary Gross Tumor Volume in Oropharyngeal Cancer</title><link>http://arxiv.org/abs/2409.06605v1</link><description>The main treatment modality for oropharyngeal cancer (OPC) is radiotherapy,where accurate segmentation of the primary gross tumor volume (GTVp) isessential. However, accurate GTVp segmentation is challenging due tosignificant interobserver variability and the time-consuming nature of manualannotation, while fully automated methods can occasionally fail. An interactivedeep learning (DL) model offers the advantage of automatic high-performancesegmentation with the flexibility for user correction when necessary. In thisstudy, we examine interactive DL for GTVp segmentation in OPC. We implementstate-of-the-art algorithms and propose a novel two-stage Interactive ClickRefinement (2S-ICR) framework. Using the 2021 HEad and neCK TumOR (HECKTOR)dataset for development and an external dataset from The University of Texas MDAnderson Cancer Center for evaluation, the 2S-ICR framework achieves a Dicesimilarity coefficient of 0.713 $\pm$ 0.152 without user interaction and 0.824$\pm$ 0.099 after five interactions, outperforming existing methods in bothcases.</description><author>Mikko Saukkoriipi, Jaakko Sahlsten, Joel Jaskari, Lotta Orasmaa, Jari Kangas, Nastaran Rasouli, Roope Raisamo, Jussi Hirvonen, Helena Mehtonen, Jorma Järnstedt, Antti Mäkitie, Mohamed Naser, Clifton Fuller, Benjamin Kann, Kimmo Kaski</author><pubDate>Tue, 10 Sep 2024 15:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06605v1</guid></item><item><title>Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases</title><link>http://arxiv.org/abs/2306.09138v3</link><description>The necessity to manage inconsistency in Description Logics Knowledge Bases(KBs) has come to the fore with the increasing importance gained by theSemantic Web, where information comes from different sources that constantlychange their content and may contain contradictory descriptions when consideredeither alone or together. Classical reasoning algorithms do not handleinconsistent KBs, forcing the debugging of the KB in order to remove theinconsistency. In this paper, we exploit an existing probabilistic semanticscalled DISPONTE to overcome this problem and allow queries also in case ofinconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLEand empirically tested the validity of our proposal. Moreover, we formallycompare the presented approach to that of the repair semantics, one of the mostestablished semantics when considering DL reasoning tasks.</description><author>Riccardo Zese, Evelina Lamma, Fabrizio Riguzzi</author><pubDate>Tue, 10 Sep 2024 15:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09138v3</guid></item><item><title>A Practical Gated Recurrent Transformer Network Incorporating Multiple Fusions for Video Denoising</title><link>http://arxiv.org/abs/2409.06603v1</link><description>State-of-the-art (SOTA) video denoising methods employ multi-framesimultaneous denoising mechanisms, resulting in significant delays (e.g., 16frames), making them impractical for real-time cameras. To overcome thislimitation, we propose a multi-fusion gated recurrent Transformer network(GRTN) that achieves SOTA denoising performance with only a single-frame delay.Specifically, the spatial denoising module extracts features from the currentframe, while the reset gate selects relevant information from the previousframe and fuses it with current frame features via the temporal denoisingmodule. The update gate then further blends this result with the previous framefeatures, and the reconstruction module integrates it with the current frame.To robustly compute attention for noisy features, we propose a residualsimplified Swin Transformer with Euclidean distance (RSSTE) in the spatial andtemporal denoising modules. Comparative objective and subjective results showthat our GRTN achieves denoising performance comparable to SOTA multi-framedelay networks, with only a single-frame delay.</description><author>Kai Guo, Seungwon Choi, Jongseong Choi, Lae-Hoon Kim</author><pubDate>Tue, 10 Sep 2024 15:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06603v1</guid></item><item><title>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks</title><link>http://arxiv.org/abs/2307.08131v4</link><description>Leveraging network information for predictive modeling has become widespreadin many domains. Within the realm of referral and targeted marketing,influencer detection stands out as an area that could greatly benefit from theincorporation of dynamic network representation due to the continuous evolutionof customer-brand relationships. In this paper, we present INFLECT-DGNN, a newmethod for profit-driven INFLuencer prEdiCTion with Dynamic Graph NeuralNetworks that innovatively combines Graph Neural Networks (GNNs) and RecurrentNeural Networks (RNNs) with weighted loss functions, synthetic minorityoversampling adapted to graph data, and a carefully crafted rolling-windowstrategy. We introduce a novel profit-driven framework that supportsdecision-making based on model predictions. To test the framework, we use aunique corporate dataset with diverse networks, capturing the customerinteractions across three cities with different socioeconomic and demographiccharacteristics. Our results show how using RNNs to encode temporal attributesalongside GNNs significantly improves predictive performance, while theprofit-driven framework determines the optimal classification threshold forprofit maximization. We compare the results of different models to demonstratethe importance of capturing network representation, temporal dependencies, andusing a profit-driven evaluation. Our research has significant implications forthe fields of referral and targeted marketing, expanding the technical use ofdeep graph learning within corporate environments.</description><author>Elena Tiukhova, Emiliano Penaloza, María Óskarsdóttir, Bart Baesens, Monique Snoeck, Cristián Bravo</author><pubDate>Tue, 10 Sep 2024 15:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08131v4</guid></item><item><title>Alleviating Hallucinations in Large Language Models with Scepticism Modeling</title><link>http://arxiv.org/abs/2409.06601v1</link><description>Hallucinations is a major challenge for large language models (LLMs),prevents adoption in diverse fields. Uncertainty estimation could be used foralleviating the damages of hallucinations. The skeptical emotion of human couldbe useful for enhancing the ability of self estimation. Inspirited by thisobservation, we proposed a new approach called Skepticism Modeling (SM). Thisapproach is formalized by combining the information of token and logits forself estimation. We construct the doubt emotion aware data, perform continualpre-training, and then fine-tune the LLMs, improve their ability of selfestimation. Experimental results demonstrate this new approach effectivelyenhances a model's ability to estimate their uncertainty, and validate itsgeneralization ability of other tasks by out-of-domain experiments.</description><author>Yetao Wu, Yihong Wang, Teng Chen, Chenxi Liu, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Zhonglin Jiang, Yong Chen, Luo Ji</author><pubDate>Tue, 10 Sep 2024 15:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06601v1</guid></item><item><title>GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering</title><link>http://arxiv.org/abs/2409.06595v1</link><description>Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to useLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.In this work, we address the challenges of using LLM-as-a-Judge when evaluatinggrounded answers generated by RAG systems. To assess the calibration anddiscrimination capabilities of judge models, we identify 7 generator failuremodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), ameta-evaluation benchmark of 144 unit tests. This benchmark reveals thatexisting automated RAG evaluation frameworks often overlook important failuremodes, even when using GPT-4 as a judge. To improve on the current design of automated RAG evaluation frameworks, wepropose a novel pipeline and find that while closed models perform well onGroUSE, state-of-the-art open-source judges do not generalize to our proposedcriteria, despite strong correlation with GPT-4's judgement. Our findingssuggest that correlation with GPT-4 is an incomplete proxy for the practicalperformance of judge models and should be supplemented with evaluations on unittests for precise failure mode detection. We further show that finetuning Llama-3 on GPT-4's reasoning tracessignificantly boosts its evaluation capabilities, improving upon bothcorrelation with GPT-4's evaluations and calibration on reference situations.</description><author>Sacha Muller, António Loison, Bilel Omrani, Gautier Viaud</author><pubDate>Tue, 10 Sep 2024 15:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06595v1</guid></item><item><title>Graph Retrieval-Augmented Generation: A Survey</title><link>http://arxiv.org/abs/2408.08921v2</link><description>Recently, Retrieval-Augmented Generation (RAG) has achieved remarkablesuccess in addressing the challenges of Large Language Models (LLMs) withoutnecessitating retraining. By referencing an external knowledge base, RAGrefines LLM outputs, effectively mitigating issues such as ``hallucination'',lack of domain-specific knowledge, and outdated information. However, thecomplex structure of relationships among different entities in databasespresents challenges for RAG systems. In response, GraphRAG leverages structuralinformation across entities to enable more precise and comprehensive retrieval,capturing relational knowledge and facilitating more accurate, context-awareresponses. Given the novelty and potential of GraphRAG, a systematic review ofcurrent technologies is imperative. This paper provides the first comprehensiveoverview of GraphRAG methodologies. We formalize the GraphRAG workflow,encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-EnhancedGeneration. We then outline the core technologies and training methods at eachstage. Additionally, we examine downstream tasks, application domains,evaluation methodologies, and industrial use cases of GraphRAG. Finally, weexplore future research directions to inspire further inquiries and advanceprogress in the field. In order to track recent progress in this field, we setup a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.</description><author>Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang</author><pubDate>Tue, 10 Sep 2024 15:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08921v2</guid></item><item><title>Advancing Causal Inference: A Nonparametric Approach to ATE and CATE Estimation with Continuous Treatments</title><link>http://arxiv.org/abs/2409.06593v1</link><description>This paper introduces a generalized ps-BART model for the estimation ofAverage Treatment Effect (ATE) and Conditional Average Treatment Effect (CATE)in continuous treatments, addressing limitations of the Bayesian Causal Forest(BCF) model. The ps-BART model's nonparametric nature allows for flexibility incapturing nonlinear relationships between treatment and outcome variables.Across three distinct sets of Data Generating Processes (DGPs), the ps-BARTmodel consistently outperforms the BCF model, particularly in highly nonlinearsettings. The ps-BART model's robustness in uncertainty estimation and accuracyin both point-wise and probabilistic estimation demonstrate its utility forreal-world applications. This research fills a crucial gap in causal inferenceliterature, providing a tool better suited for nonlinear treatment-outcomerelationships and opening avenues for further exploration in the domain ofcontinuous treatment effect estimation.</description><author>Hugo Gobato Souto, Francisco Louzada Neto</author><pubDate>Tue, 10 Sep 2024 15:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06593v1</guid></item><item><title>Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management</title><link>http://arxiv.org/abs/2402.00515v4</link><description>Deep or reinforcement learning (RL) approaches have been adapted as reactiveagents to quickly learn and respond with new investment strategies forportfolio management under the highly turbulent financial market environmentsin recent years. In many cases, due to the very complex correlations amongvarious financial sectors, and the fluctuating trends in different financialmarkets, a deep or reinforcement learning based agent can be biased inmaximising the total returns of the newly formulated investment portfolio whileneglecting its potential risks under the turmoil of various market conditionsin the global or regional sectors. Accordingly, a multi-agent and self-adaptiveframework namely the MASA is proposed in which a sophisticated multi-agentreinforcement learning (RL) approach is adopted through two cooperating andreactive agents to carefully and dynamically balance the trade-off between theoverall portfolio returns and their potential risks. Besides, a very flexibleand proactive agent as the market observer is integrated into the MASAframework to provide some additional information on the estimated market trendsas valuable feedbacks for multi-agent RL approach to quickly adapt to theever-changing market conditions. The obtained empirical results clearly revealthe potential strengths of our proposed MASA framework based on the multi-agentRL approach against many well-known RL-based approaches on the challenging datasets of the CSI 300, Dow Jones Industrial Average and S&amp;P 500 indexes over thepast 10 years. More importantly, our proposed MASA framework shed lights onmany possible directions for future investigation.</description><author>Zhenglong Li, Vincent Tam, Kwan L. Yeung</author><pubDate>Tue, 10 Sep 2024 15:34:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00515v4</guid></item><item><title>Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer</title><link>http://arxiv.org/abs/2409.06590v1</link><description>The single image super-resolution(SISR) algorithms under deep learningcurrently have two main models, one based on convolutional neural networks andthe other based on Transformer. The former uses the stacking of convolutionallayers with different convolutional kernel sizes to design the model, whichenables the model to better extract the local features of the image; the latteruses the self-attention mechanism to design the model, which allows the modelto establish long-distance dependencies between image pixel points through theself-attention mechanism and then better extract the global features of theimage. However, both of the above methods face their problems. Based on this,this paper proposes a new lightweight multi-scale feature fusion network modelbased on two-way complementary convolutional and Transformer, which integratesthe respective features of Transformer and convolutional neural networksthrough a two-branch network architecture, to realize the mutual fusion ofglobal and local information. Meanwhile, considering the partial loss ofinformation caused by the low-pixel images trained by the deep neural network,this paper designs a modular connection method of multi-stage featuresupplementation to fuse the feature maps extracted from the shallow stage ofthe model with those extracted from the deep stage of the model, to minimizethe loss of the information in the feature images that is beneficial to theimage restoration as much as possible, to facilitate the obtaining of ahigher-quality restored image. The practical results finally show that themodel proposed in this paper is optimal in image recovery performance whencompared with other lightweight models with the same amount of parameters.</description><author>Li Ke, Liu Yukai</author><pubDate>Tue, 10 Sep 2024 15:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06590v1</guid></item><item><title>Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with Hyperbolic Graph Neural Networks</title><link>http://arxiv.org/abs/2409.06589v1</link><description>Image analysis in the euclidean space through linear hyperspaces is wellstudied. However, in the quest for more effective image representations, weturn to hyperbolic manifolds. They provide a compelling alternative to capturecomplex hierarchical relationships in images with remarkably smalldimensionality. To demonstrate hyperbolic embeddings' competence, we introducea light-weight hyperbolic graph neural network for image segmentation,encompassing patch-level features in a very small embedding size. Our solution,Seg-HGNN, surpasses the current best unsupervised method by 2.5\%, 4\% onVOC-07, VOC-12 for localization, and by 0.8\%, 1.3\% on CUB-200, ECSSD forsegmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNNdelivers effective and fast ($\approx 2$ images/second) results on verystandard GPUs like the GTX1650. This empirical evaluation presents compellingevidence of the efficacy and potential of hyperbolic representations for visiontasks.</description><author>Debjyoti Mondal, Rahul Mishra, Chandan Pandey</author><pubDate>Tue, 10 Sep 2024 15:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06589v1</guid></item><item><title>Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records</title><link>http://arxiv.org/abs/2409.06585v1</link><description>Background: Hip replacement procedures improve patient lives by relievingpain and restoring mobility. Predicting hip replacement in advance could reducepain by enabling timely interventions, prioritising individuals for surgery orrehabilitation, and utilising physiotherapy to potentially delay the need forjoint replacement. This study predicts hip replacement a year in advance toenhance quality of life and health service efficiency. Methods: Adaptingprevious work using Temporal Graph Convolutional Neural Network (TG-CNN)models, we construct temporal graphs from primary care medical event codes,sourced from ResearchOne EHRs of 40-75-year-old patients, to predict hipreplacement risk. We match hip replacement cases to controls by age, sex, andIndex of Multiple Deprivation. The model, trained on 9,187 cases and 9,187controls, predicts hip replacement one year in advance. We validate the modelon two unseen datasets, recalibrating for class imbalance. Additionally, weconduct an ablation study and compare against four baseline models. Results:Our best model predicts hip replacement risk one year in advance with an AUROCof 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),achieving a calibration slope of 1.107 (95% CI: 1.074-1.139) afterrecalibration. Conclusions: The TG-CNN model effectively predicts hipreplacement risk by identifying patterns in patient trajectories, potentiallyimproving understanding and management of hip-related conditions.</description><author>Zoe Hancox, Sarah R. Kingsbury, Andrew Clegg, Philip G. Conaghan, Samuel D. Relton</author><pubDate>Tue, 10 Sep 2024 15:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06585v1</guid></item><item><title>Transtreaming: Adaptive Delay-aware Transformer for Real-time Streaming Perception</title><link>http://arxiv.org/abs/2409.06584v1</link><description>Real-time object detection is critical for the decision-making process formany real-world applications, such as collision avoidance and path planning inautonomous driving. This work presents an innovative real-time streamingperception method, Transtreaming, which addresses the challenge of real-timeobject detection with dynamic computational delay. The core innovation ofTranstreaming lies in its adaptive delay-aware transformer, which canconcurrently predict multiple future frames and select the output that bestmatches the real-world present time, compensating for any system-inducedcomputation delays. The proposed model outperforms the existingstate-of-the-art methods, even in single-frame detection scenarios, byleveraging a transformer-based methodology. It demonstrates robust performanceacross a range of devices, from powerful V100 to modest 2080Ti, achieving thehighest level of perceptual accuracy on all platforms. Unlike moststate-of-the-art methods that struggle to complete computation within a singleframe on less powerful devices, Transtreaming meets the stringent real-timeprocessing requirements on all kinds of devices. The experimental resultsemphasize the system's adaptability and its potential to significantly improvethe safety and reliability for many real-world systems, such as autonomousdriving.</description><author>Xiang Zhang, Yufei Cui, Chenchen Fu, Weiwei Wu, Zihao Wang, Yuyang Sun, Xue Liu</author><pubDate>Tue, 10 Sep 2024 15:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06584v1</guid></item><item><title>Semi-Supervised 3D Object Detection with Chanel Augmentation using Transformation Equivariance</title><link>http://arxiv.org/abs/2409.06583v1</link><description>Accurate 3D object detection is crucial for autonomous vehicles and robots tonavigate and interact with the environment safely and effectively. Meanwhile,the performance of 3D detector relies on the data size and annotation which isexpensive. Consequently, the demand of training with limited labeled data isgrowing. We explore a novel teacher-student framework employing channelaugmentation for 3D semi-supervised object detection. The teacher-student SSLtypically adopts a weak augmentation and strong augmentation to teacher andstudent, respectively. In this work, we apply multiple channel augmentations toboth networks using the transformation equivariance detector (TED). The TEDallows us to explore different combinations of augmentation on point clouds andefficiently aggregates multi-channel transformation equivariance features. Inprinciple, by adopting fixed channel augmentations for the teacher network, thestudent can train stably on reliable pseudo-labels. Adopting strong channelaugmentations can enrich the diversity of data, fostering robustness totransformations and enhancing generalization performance of the studentnetwork. We use SOTA hierarchical supervision as a baseline and adapt itsdual-threshold to TED, which is called channel IoU consistency. We evaluate ourmethod with KITTI dataset, and achieved a significant performance leap,surpassing SOTA 3D semi-supervised object detection models.</description><author>Minju Kang, Taehun Kong, Tae-Kyun Kim</author><pubDate>Tue, 10 Sep 2024 15:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06583v1</guid></item><item><title>State-of-the-art review and synthesis: A requirement-based roadmap for standardized predictive maintenance automation using digital twin technologies</title><link>http://arxiv.org/abs/2311.06993v2</link><description>Recent digital advances have popularized predictive maintenance (PMx),offering enhanced efficiency, automation, accuracy, cost savings, andindependence in maintenance processes. Yet, PMx continues to face numerouslimitations such as poor explainability, sample inefficiency of data-drivenmethods, complexity of physics-based methods, and limited generalizability andscalability of knowledge-based methods. This paper proposes leveraging DigitalTwins (DTs) to address these challenges and enable automated PMx adoption on alarger scale. While DTs have the potential to be transformative, they have notyet reached the maturity needed to bridge these gaps in a standardized manner.Without a standard definition guiding this evolution, the transformation lacksa solid foundation for development. This paper provides a requirement-basedroadmap to support standardized PMx automation using DT technologies. Oursystematic approach comprises two primary stages. First, we methodicallyidentify the Informational Requirements (IRs) and Functional Requirements (FRs)for PMx, which serve as a foundation from which any unified framework mustemerge. Our approach to defining and using IRs and FRs as the backbone of anyPMx DT is supported by the proven success of these requirements as blueprintsin other areas, such as product development in the software industry. Second,we conduct a thorough literature review across various fields to assess howthese IRs and FRs are currently being applied within DTs, enabling us toidentify specific areas where further research is needed to support theprogress and maturation of requirement-based PMx DTs.</description><author>Sizhe Ma, Katherine A. Flanigan, Mario Bergés</author><pubDate>Tue, 10 Sep 2024 15:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06993v2</guid></item><item><title>Exploring Differences between Human Perception and Model Inference in Audio Event Recognition</title><link>http://arxiv.org/abs/2409.06580v1</link><description>Audio Event Recognition (AER) traditionally focuses on detecting andidentifying audio events. Most existing AER models tend to detect all potentialevents without considering their varying significance across differentcontexts. This makes the AER results detected by existing models often have alarge discrepancy with human auditory perception. Although this is a criticaland significant issue, it has not been extensively studied by the Detection andClassification of Sound Scenes and Events (DCASE) community because solving itis time-consuming and labour-intensive. To address this issue, this paperintroduces the concept of semantic importance in AER, focusing on exploring thedifferences between human perception and model inference. This paper constructsa Multi-Annotated Foreground Audio Event Recognition (MAFAR) dataset, whichcomprises audio recordings labelled by 10 professional annotators. Throughlabelling frequency and variance, the MAFAR dataset facilitates thequantification of semantic importance and analysis of human perception. Bycomparing human annotations with the predictions of ensemble pre-trainedmodels, this paper uncovers a significant gap between human perception andmodel inference in both semantic identification and existence detection ofaudio events. Experimental results reveal that human perception tends to ignoresubtle or trivial events in the event semantic identification, while modelinference is easily affected by events with noises. Meanwhile, in eventexistence detection, models are usually more sensitive than humans.</description><author>Yizhou Tan, Yanru Wu, Yuanbo Hou, Xin Xu, Hui Bu, Shengchen Li, Dick Botteldooren, Mark D. Plumbley</author><pubDate>Tue, 10 Sep 2024 15:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06580v1</guid></item><item><title>Quantifying and Enabling the Interpretability of CLIP-like Models</title><link>http://arxiv.org/abs/2409.06579v1</link><description>CLIP is one of the most popular foundational models and is heavily used formany vision-language tasks. However, little is known about the inner workingsof CLIP. To bridge this gap we propose a study to quantify the interpretabilityin CLIP like models. We conduct this study on six different CLIP models fromOpenAI and OpenCLIP which vary by size, type of pre-training data and patchsize. Our approach begins with using the TEXTSPAN algorithm and in-contextlearning to break down individual attention heads into specific properties. Wethen evaluate how easily these heads can be interpreted using new metrics whichmeasure property consistency within heads and property disentanglement acrossheads. Our findings reveal that larger CLIP models are generally moreinterpretable than their smaller counterparts. To further assist users inunderstanding the inner workings of CLIP models, we introduce CLIP-InterpreT, atool designed for interpretability analysis. CLIP-InterpreT offers five typesof analyses: property-based nearest neighbor search, per-head topicsegmentation, contrastive segmentation, per-head nearest neighbors of an image,and per-head nearest neighbors of text.</description><author>Avinash Madasu, Yossi Gandelsman, Vasudev Lal, Phillip Howard</author><pubDate>Tue, 10 Sep 2024 15:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06579v1</guid></item><item><title>Deeper-PINNs: Element-wise Multiplication Based Physics-informed Neural Networks</title><link>http://arxiv.org/abs/2406.04170v3</link><description>As a promising framework for resolving partial differential equations (PDEs),physics-informed neural networks (PINNs) have received widespread attentionfrom industrial and scientific fields. However, lack of expressive ability andinitialization pathology issues are found to prevent the application of PINNsin complex PDEs. In this work, we propose Deeper Physics-Informed NeuralNetwork (Deeper-PINN) to resolve these issues. The element-wise multiplicationoperation is adopted to transform features into high-dimensional, non-linearspaces. Benefiting from element-wise multiplication operation, Deeper-PINNs canalleviate the initialization pathologies of PINNs and enhance the expressivecapability of PINNs. The proposed structure is verified on various benchmarks.The results show that Deeper-PINNs can effectively resolve the initializationpathology and exhibit strong expressive ability.</description><author>Feilong Jiang, Xiaonan Hou, Min Xia</author><pubDate>Tue, 10 Sep 2024 15:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04170v3</guid></item><item><title>Analysis of Unstructured High-Density Crowded Scenes for Crowd Monitoring</title><link>http://arxiv.org/abs/2408.11836v4</link><description>We are interested in developing an automated system for detection oforganized movements in human crowds. Computer vision algorithms can extractinformation from videos of crowded scenes and automatically detect and trackgroups of individuals undergoing organized motion that represents an anomalousbehavior in the context of conflict aversion. Our system can detect organizedcohorts against the background of randomly moving objects and we can estimatethe number of participants in an organized cohort, the speed and direction ofmotion in real time, within three to four video frames, which is less than onesecond from the onset of motion captured on a CCTV. We have performedpreliminary analysis in this context in biological cell data containing up tofour thousand objects per frame and will extend this numerically to ahundred-fold for public safety applications. We envisage using the existing infrastructure of video cameras for acquiringimage datasets on-the-fly and deploying an easy-to-use data-driven softwaresystem for parsing of significant events by analyzing image sequences takeninside and outside of sports stadiums or other public venues. Other prospectiveusers are organizers of political rallies, civic and wildlife organizations,security firms, and the military. We will optimize the performance of thesoftware by implementing a classification method able to distinguish betweenactivities posing a threat and those not posing a threat.</description><author>Alexandre Matov</author><pubDate>Tue, 10 Sep 2024 15:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11836v4</guid></item><item><title>Exploring syntactic information in sentence embeddings through multilingual subject-verb agreement</title><link>http://arxiv.org/abs/2409.06567v1</link><description>In this paper, our goal is to investigate to what degree multilingualpretrained language models capture cross-linguistically valid abstractlinguistic representations. We take the approach of developing curatedsynthetic data on a large scale, with specific properties, and using them tostudy sentence representations built using pretrained language models. We use anew multiple-choice task and datasets, Blackbird Language Matrices (BLMs), tofocus on a specific grammatical structural phenomenon -- subject-verb agreementacross a variety of sentence structures -- in several languages. Finding asolution to this task requires a system detecting complex linguistic patternsand paradigms in text representations. Using a two-level architecture thatsolves the problem in two steps -- detect syntactic objects and theirproperties in individual sentences, and find patterns across an input sequenceof sentences -- we show that despite having been trained on multilingual textsin a consistent manner, multilingual pretrained language models havelanguage-specific differences, and syntactic structure is not shared, evenacross closely related languages.</description><author>Vivi Nastase, Chunyang Jiang, Giuseppe Samo, Paola Merlo</author><pubDate>Tue, 10 Sep 2024 14:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06567v1</guid></item><item><title>Indirect Dynamic Negotiation in the Nash Demand Game</title><link>http://arxiv.org/abs/2409.06566v1</link><description>The paper addresses a problem of sequential bilateral bargaining withincomplete information. We proposed a decision model that helps agents tosuccessfully bargain by performing indirect negotiation and learning theopponent's model. Methodologically the paper casts heuristically-motivatedbargaining of a self-interested independent player into a framework of Bayesianlearning and Markov decision processes. The special form of the rewardimplicitly motivates the players to negotiate indirectly, via closed-loopinteraction. We illustrate the approach by applying our model to the Nashdemand game, which is an abstract model of bargaining. The results indicatethat the established negotiation: i) leads to coordinating players' actions;ii) results in maximising success rate of the game and iii) brings moreindividual profit to the players.</description><author>Tatiana V. Guy, Jitka Homolová, Aleksej Gaj</author><pubDate>Tue, 10 Sep 2024 14:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06566v1</guid></item><item><title>Implicit Filtering for Learning Neural Signed Distance Functions from 3D Point Clouds</title><link>http://arxiv.org/abs/2407.13342v2</link><description>Neural signed distance functions (SDFs) have shown powerful ability infitting the shape geometry. However, inferring continuous signed distancefields from discrete unoriented point clouds still remains a challenge. Theneural network typically fits the shape with a rough surface and omitsfine-grained geometric details such as shape edges and corners. In this paper,we propose a novel non-linear implicit filter to smooth the implicit fieldwhile preserving high-frequency geometry details. Our novelty lies in that wecan filter the surface (zero level set) by the neighbor input points withgradients of the signed distance field. By moving the input raw point cloudsalong the gradient, our proposed implicit filtering can be extended to non-zerolevel sets to keep the promise consistency between different level sets, whichconsequently results in a better regularization of the zero level set. Weconduct comprehensive experiments in surface reconstruction from objects andcomplex scene point clouds, the numerical and visual comparisons demonstrateour improvements over the state-of-the-art methods under the widely usedbenchmarks.</description><author>Shengtao Li, Ge Gao, Yudong Liu, Ming Gu, Yu-Shen Liu</author><pubDate>Tue, 10 Sep 2024 14:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13342v2</guid></item><item><title>ChatGPT's Potential in Cryptography Misuse Detection: A Comparative Analysis with Static Analysis Tools</title><link>http://arxiv.org/abs/2409.06561v1</link><description>The correct adoption of cryptography APIs is challenging for mainstreamdevelopers, often resulting in widespread API misuse. Meanwhile, cryptographymisuse detectors have demonstrated inconsistent performance and remain largelyinaccessible to most developers. We investigated the extent to which ChatGPTcan detect cryptography misuses and compared its performance with that of thestate-of-the-art static analysis tools. Our investigation, mainly based on theCryptoAPI-Bench benchmark, demonstrated that ChatGPT is effective inidentifying cryptography API misuses, and with the use of prompt engineering,it can even outperform leading static cryptography misuse detectors.</description><author>Ehsan Firouzi, Mohammad Ghafari, Mike Ebrahimi</author><pubDate>Tue, 10 Sep 2024 14:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06561v1</guid></item><item><title>SUMix: Mixup with Semantic and Uncertain Information</title><link>http://arxiv.org/abs/2407.07805v4</link><description>Mixup data augmentation approaches have been applied for various tasks ofdeep learning to improve the generalization ability of deep neural networks.Some existing approaches CutMix, SaliencyMix, etc. randomly replace a patch inone image with patches from another to generate the mixed image. Similarly, thecorresponding labels are linearly combined by a fixed ratio $\lambda$ by l. Theobjects in two images may be overlapped during the mixing process, so somesemantic information is corrupted in the mixed samples. In this case, the mixedimage does not match the mixed label information. Besides, such a label maymislead the deep learning model training, which results in poor performance. Tosolve this problem, we proposed a novel approach named SUMix to learn themixing ratio as well as the uncertainty for the mixed samples during thetraining process. First, we design a learnable similarity function to computean accurate mix ratio. Second, an approach is investigated as a regularizedterm to model the uncertainty of the mixed samples. We conduct experiments onfive image benchmarks, and extensive experimental results imply that our methodis capable of improving the performance of classifiers with differentcutting-based mixup approaches. The source code is available athttps://github.com/JinXins/SUMix.</description><author>Huafeng Qin, Xin Jin, Hongyu Zhu, Hongchao Liao, Mounîm A. El-Yacoubi, Xinbo Gao</author><pubDate>Tue, 10 Sep 2024 14:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07805v4</guid></item><item><title>LAST: Language Model Aware Speech Tokenization</title><link>http://arxiv.org/abs/2409.03701v2</link><description>Speech tokenization serves as the foundation of speech language model (LM),enabling them to perform various tasks such as spoken language modeling,text-to-speech, speech-to-text, etc. Most speech tokenizers are trainedindependently of the LM training process, relying on separate acoustic modelsand quantization methods. Following such an approach may create a mismatchbetween the tokenization process and its usage afterward. In this study, wepropose a novel approach to training a speech tokenizer by leveragingobjectives from pre-trained textual LMs. We advocate for the integration ofthis objective into the process of learning discrete speech representations.Our aim is to transform features from a pre-trained speech model into a newfeature space that enables better clustering for speech LMs. We empiricallyinvestigate the impact of various model design choices, including speechvocabulary size and text LM size. Our results demonstrate the proposedtokenization method outperforms the evaluated baselines considering both spokenlanguage modeling and speech-to-text. More importantly, unlike prior work, theproposed method allows the utilization of a single pre-trained LM forprocessing both speech and text inputs, setting it apart from conventionaltokenization approaches.</description><author>Arnon Turetzky, Yossi Adi</author><pubDate>Tue, 10 Sep 2024 14:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03701v2</guid></item><item><title>A Primer on Variational Inference for Physics-Informed Deep Generative Modelling</title><link>http://arxiv.org/abs/2409.06560v1</link><description>Variational inference (VI) is a computationally efficient and scalablemethodology for approximate Bayesian inference. It strikes a balance betweenaccuracy of uncertainty quantification and practical tractability. It excels atgenerative modelling and inversion tasks due to its built-in Bayesianregularisation and flexibility, essential qualities for physics relatedproblems. Deriving the central learning objective for VI must often be tailoredto new learning tasks where the nature of the problems dictates the conditionaldependence between variables of interest, such as arising in physics problems.In this paper, we provide an accessible and thorough technical introduction toVI for forward and inverse problems, guiding the reader through standardderivations of the VI framework and how it can best be realized through deeplearning. We then review and unify recent literature exemplifying the creativeflexibility allowed by VI. This paper is designed for a general scientificaudience looking to solve physics-based problems with an emphasis onuncertainty quantification.</description><author>Alex Glyn-Davies, Arnaud Vadeboncoeur, O. Deniz Akyildiz, Ieva Kazlauskaite, Mark Girolami</author><pubDate>Tue, 10 Sep 2024 14:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06560v1</guid></item><item><title>Learn2Aggregate: Supervised Generation of Chvátal-Gomory Cuts Using Graph Neural Networks</title><link>http://arxiv.org/abs/2409.06559v1</link><description>We present $\textit{Learn2Aggregate}$, a machine learning (ML) framework foroptimizing the generation of Chv\'atal-Gomory (CG) cuts in mixed integer linearprogramming (MILP). The framework trains a graph neural network to classifyuseful constraints for aggregation in CG cut generation. The ML-driven CGseparator selectively focuses on a small set of impactful constraints,improving runtimes without compromising the strength of the generated cuts. Keyto our approach is the formulation of a constraint classification task whichfavours sparse aggregation of constraints, consistent with empirical findings.This, in conjunction with a careful constraint labeling scheme and a hybrid ofdeep learning and feature engineering, results in enhanced CG cut generationacross five diverse MILP benchmarks. On the largest test sets, our methodcloses roughly $\textit{twice}$ as much of the integrality gap as the standardCG method while running 40$% faster. This performance improvement is due to ourmethod eliminating 75% of the constraints prior to aggregation.</description><author>Arnaud Deza, Elias B. Khalil, Zhenan Fan, Zirui Zhou, Yong Zhang</author><pubDate>Tue, 10 Sep 2024 14:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06559v1</guid></item><item><title>DispaRisk: Auditing Fairness Through Usable Information</title><link>http://arxiv.org/abs/2405.12372v2</link><description>Machine Learning algorithms (ML) impact virtually every aspect of human livesand have found use across diverse sectors including healthcare, finance, andeducation. Often, ML algorithms have been found to exacerbate societal biasespresent in datasets leading to adversarial impacts on subsets/groups ofindividuals and in many cases on minority groups. To effectively mitigate theseuntoward effects, it is crucial that disparities/biases are identified early ina ML pipeline. This proactive approach facilitates timely interventions toprevent bias amplification and reduce complexity at later stages of modeldevelopment. In this paper, we leverage recent advancements in usableinformation theory to introduce DispaRisk, a novel framework designed toproactively assess the potential risks of disparities in datasets during theinitial stages of the ML pipeline. We evaluate DispaRisk's effectiveness bybenchmarking it against commonly used datasets in fairness research. Ourfindings demonstrate DispaRisk's capabilities to identify datasets with a highrisk of discrimination, detect model families prone to biases within an MLpipeline, and enhance the explainability of these bias risks. This workcontributes to the development of fairer ML systems by providing a robust toolfor early bias detection and mitigation. The code for our experiments isavailable in the following repository: https://github.com/jovasque156/disparisk</description><author>Jonathan Vasquez, Carlotta Domeniconi, Huzefa Rangwala</author><pubDate>Tue, 10 Sep 2024 14:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12372v2</guid></item><item><title>Multi-Margin Cosine Loss: Proposal and Application in Recommender Systems</title><link>http://arxiv.org/abs/2405.04614v3</link><description>Recommender systems guide users through vast amounts of information bysuggesting items based on their predicted preferences. Collaborativefiltering-based deep learning techniques have regained popularity due to theirstraightforward nature, relying only on user-item interactions. Typically,these systems consist of three main components: an interaction module, a lossfunction, and a negative sampling strategy. Initially, researchers focused onenhancing performance by developing complex interaction modules. However, therehas been a recent shift toward refining loss functions and negative samplingstrategies. This shift has led to an increased interest in contrastivelearning, which pulls similar pairs closer while pushing dissimilar ones apart.Contrastive learning may bring challenges like high memory demands andunder-utilization of some negative samples. The proposed Multi-Margin CosineLoss (MMCL) addresses these challenges by introducing multiple margins andvarying weights for negative samples. It efficiently utilizes not only thehardest negatives but also other non-trivial negatives, offers a simpler yeteffective loss function that outperforms more complex methods, especially whenresources are limited. Experiments on two well-known datasets demonstrated thatMMCL achieved up to a 20\% performance improvement compared to a baseline lossfunction when fewer number of negative samples are used.</description><author>Makbule Gulcin Ozsoy</author><pubDate>Tue, 10 Sep 2024 14:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04614v3</guid></item><item><title>Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity</title><link>http://arxiv.org/abs/2407.09733v2</link><description>In this paper, we introduce Textured-GS, an innovative method for renderingGaussian splatting that incorporates spatially defined color and opacityvariations using Spherical Harmonics (SH). This approach enables each Gaussianto exhibit a richer representation by accommodating varying colors andopacities across its surface, significantly enhancing rendering qualitycompared to traditional methods. To demonstrate the merits of our approach, wehave adapted the Mini-Splatting architecture to integrate textured Gaussianswithout increasing the number of Gaussians. Our experiments across multiplereal-world datasets show that Textured-GS consistently outperforms both thebaseline Mini-Splatting and standard 3DGS in terms of visual fidelity. Theresults highlight the potential of Textured-GS to advance Gaussian-basedrendering technologies, promising more efficient and high-quality scenereconstructions.</description><author>Zhentao Huang, Minglun Gong</author><pubDate>Tue, 10 Sep 2024 14:34:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09733v2</guid></item><item><title>Some Results on Neural Network Stability, Consistency, and Convergence: Insights into Non-IID Data, High-Dimensional Settings, and Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2409.05030v2</link><description>This paper addresses critical challenges in machine learning, particularlythe stability, consistency, and convergence of neural networks under non-IIDdata, distribution shifts, and high-dimensional settings. We provide newtheoretical results on uniform stability for neural networks with dynamiclearning rates in non-convex settings. Further, we establish consistency boundsfor federated learning models in non-Euclidean spaces, accounting fordistribution shifts and curvature effects. For Physics-Informed Neural Networks(PINNs), we derive stability, consistency, and convergence guarantees forsolving Partial Differential Equations (PDEs) in noisy environments. Theseresults fill significant gaps in understanding model behavior in complex,non-ideal conditions, paving the way for more robust and reliable machinelearning applications.</description><author>Ronald Katende, Henry Kasumba, Godwin Kakuba, John M. Mango</author><pubDate>Tue, 10 Sep 2024 14:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05030v2</guid></item><item><title>Deep Neural Networks: Multi-Classification and Universal Approximation</title><link>http://arxiv.org/abs/2409.06555v1</link><description>We demonstrate that a ReLU deep neural network with a width of $2$ and adepth of $2N+4M-1$ layers can achieve finite sample memorization for anydataset comprising $N$ elements in $\mathbb{R}^d$, where $d\ge1,$ and $M$classes, thereby ensuring accurate classification. By modeling the neural network as a time-discrete nonlinear dynamical system,we interpret the memorization property as a problem of simultaneous or ensemblecontrollability. This problem is addressed by constructing the networkparameters inductively and explicitly, bypassing the need for training orsolving any optimization problem. Additionally, we establish that such a network can achieve universalapproximation in $L^p(\Omega;\mathbb{R}_+)$, where $\Omega$ is a bounded subsetof $\mathbb{R}^d$ and $p\in[1,\infty)$, using a ReLU deep neural network with awidth of $d+1$. We also provide depth estimates for approximating $W^{1,p}$functions and width estimates for approximating $L^p(\Omega;\mathbb{R}^m)$ for$m\geq1$. Our proofs are constructive, offering explicit values for the biasesand weights involved.</description><author>Martín Hernández, Enrique Zuazua</author><pubDate>Tue, 10 Sep 2024 14:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06555v1</guid></item><item><title>Modelling Global Trade with Optimal Transport</title><link>http://arxiv.org/abs/2409.06554v1</link><description>Global trade is shaped by a complex mix of factors beyond supply and demand,including tangible variables like transport costs and tariffs, as well as lessquantifiable influences such as political and economic relations.Traditionally, economists model trade using gravity models, which rely onexplicit covariates but often struggle to capture these subtler drivers oftrade. In this work, we employ optimal transport and a deep neural network tolearn a time-dependent cost function from data, without imposing a specificfunctional form. This approach consistently outperforms traditional gravitymodels in accuracy while providing natural uncertainty quantification. Applyingour framework to global food and agricultural trade, we show that the globalSouth suffered disproportionately from the war in Ukraine's impact on wheatmarkets. We also analyze the effects of free-trade agreements and tradedisputes with China, as well as Brexit's impact on British trade with Europe,uncovering hidden patterns that trade volumes alone cannot reveal.</description><author>Thomas Gaskin, Marie-Therese Wolfram, Andrew Duncan, Guven Demirel</author><pubDate>Tue, 10 Sep 2024 14:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06554v1</guid></item><item><title>From LIMA to DeepLIMA: following a new path of interoperability</title><link>http://arxiv.org/abs/2409.06550v1</link><description>In this article, we describe the architecture of the LIMA (Libre MultilingualAnalyzer) framework and its recent evolution with the addition of new textanalysis modules based on deep neural networks. We extended the functionalityof LIMA in terms of the number of supported languages while preserving existingconfigurable architecture and the availability of previously developedrule-based and statistical analysis components. Models were trained for morethan 60 languages on the Universal Dependencies 2.5 corpora, WikiNer corpora,and CoNLL-03 dataset. Universal Dependencies allowed us to increase the numberof supported languages and to generate models that could be integrated intoother platforms. This integration of ubiquitous Deep Learning Natural LanguageProcessing models and the use of standard annotated collections using UniversalDependencies can be viewed as a new path of interoperability, through thenormalization of models and data, that are complementary to a more standardtechnical interoperability, implemented in LIMA through services available inDocker containers on Docker Hub.</description><author>Victor Bocharov, Romaric Besançon, Gaël de Chalendar, Olivier Ferret, Nasredine Semmar</author><pubDate>Tue, 10 Sep 2024 14:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06550v1</guid></item><item><title>Inverse Particle Filter</title><link>http://arxiv.org/abs/2407.16623v2</link><description>In cognitive systems, recent emphasis has been placed on studying thecognitive processes of the subject whose behavior was the primary focus of thesystem's cognitive response. This approach, known as inverse cognition, arisesin counter-adversarial applications and has motivated the development ofinverse Bayesian filters. In this context, a cognitive adversary, such as aradar, uses a forward Bayesian filter to track its target of interest. Aninverse filter is then employed to infer the adversary's estimate of thetarget's or defender's state. Previous studies have addressed this inversefiltering problem by introducing methods like the inverse Kalman filter (I-KF),inverse extended KF (I-EKF), and inverse unscented KF (I-UKF). However, thesefilters typically assume additive Gaussian noise models and/or rely on localapproximations of non-linear dynamics at the state estimates, limiting theirpractical application. In contrast, this paper adopts a global filteringapproach and presents the development of an inverse particle filter (I-PF). Theparticle filter framework employs Monte Carlo (MC) methods to approximatearbitrary posterior distributions. Moreover, under mild system-levelconditions, the proposed I-PF demonstrates convergence to the optimal inversefilter. Additionally, we propose the differentiable I-PF to address scenarioswhere system information is unknown to the defender. Using the recursiveCramer-Rao lower bound and non-credibility index (NCI), our numericalexperiments for different systems demonstrate the estimation performance andtime complexity of the proposed filter.</description><author>Himali Singh, Arpan Chattopadhyay, Kumar Vijay Mishra</author><pubDate>Tue, 10 Sep 2024 14:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16623v2</guid></item><item><title>CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers</title><link>http://arxiv.org/abs/2403.14465v2</link><description>In minimally invasive endovascular procedures, contrast-enhanced angiographyremains the most robust imaging technique. However, it is at the expense of thepatient and clinician's health due to prolonged radiation exposure. As analternative, interventional ultrasound has notable benefits such as beingradiation-free, fast to deploy, and having a small footprint in the operatingroom. Yet, ultrasound is hard to interpret, and highly prone to artifacts andnoise. Additionally, interventional radiologists must undergo extensivetraining before they become qualified to diagnose and treat patientseffectively, leading to a shortage of staff, and a lack of open-sourcedatasets. In this work, we seek to address both problems by introducing aself-supervised deep learning architecture to segment catheters in longitudinalultrasound images, without demanding any labeled data. The network architecturebuilds upon AiAReSeg, a segmentation transformer built with the Attention inAttention mechanism, and is capable of learning feature changes across time andspace. To facilitate training, we used synthetic ultrasound data based onphysics-driven catheter insertion simulations, and translated the data into aunique CT-Ultrasound common domain, CACTUSS, to improve the segmentationperformance. We generated ground truth segmentation masks by computing theoptical flow between adjacent frames using FlowNet2, and performed thresholdingto obtain a binary map estimate. Finally, we validated our model on a testdataset, consisting of unseen synthetic data and images collected from siliconaorta phantoms, thus demonstrating its potential for applications to clinicaldata in the future.</description><author>Alex Ranne, Liming Kuang, Yordanka Velikova, Nassir Navab, Ferdinando Rodriguez y Baena</author><pubDate>Tue, 10 Sep 2024 14:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14465v2</guid></item><item><title>Dynamic Decoupling of Placid Terminal Attractor-based Gradient Descent Algorithm</title><link>http://arxiv.org/abs/2409.06542v1</link><description>Gradient descent (GD) and stochastic gradient descent (SGD) have been widelyused in a large number of application domains. Therefore, understanding thedynamics of GD and improving its convergence speed is still of greatimportance. This paper carefully analyzes the dynamics of GD based on theterminal attractor at different stages of its gradient flow. On the basis ofthe terminal sliding mode theory and the terminal attractor theory, fouradaptive learning rates are designed. Their performances are investigated inlight of a detailed theoretical investigation, and the running times of thelearning procedures are evaluated and compared. The total times of theirlearning processes are also studied in detail. To evaluate their effectiveness,various simulation results are investigated on a function approximation problemand an image classification problem.</description><author>Jinwei Zhao, Marco Gori, Alessandro Betti, Stefano Melacci, Hongtao Zhang, Jiedong Liu, Xinhong Hei</author><pubDate>Tue, 10 Sep 2024 14:15:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06542v1</guid></item><item><title>Mapping News Narratives Using LLMs and Narrative-Structured Text Embeddings</title><link>http://arxiv.org/abs/2409.06540v1</link><description>Given the profound impact of narratives across various societal levels, frompersonal identities to international politics, it is crucial to understandtheir distribution and development over time. This is particularly important inonline spaces. On the Web, narratives can spread rapidly and intensify societaldivides and conflicts. While many qualitative approaches exist, quantifyingnarratives remains a significant challenge. Computational narrative analysislacks frameworks that are both comprehensive and generalizable. To address thisgap, we introduce a numerical narrative representation grounded instructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents anarrative through a constellation of six functional character roles. Theseso-called actants are genre-agnostic, making the model highly generalizable. Weextract the actants using an open-source LLM and integrate them into aNarrative-Structured Text Embedding that captures both the semantics andnarrative structure of a text. We demonstrate the analytical insights of themethod on the example of 5000 full-text news articles from Al Jazeera and TheWashington Post on the Israel-Palestine conflict. Our method successfullydistinguishes articles that cover the same topics but differ in narrativestructure.</description><author>Jan Elfes</author><pubDate>Tue, 10 Sep 2024 14:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06540v1</guid></item><item><title>PoseEmbroider: Towards a 3D, Visual, Semantic-aware Human Pose Representation</title><link>http://arxiv.org/abs/2409.06535v1</link><description>Aligning multiple modalities in a latent space, such as images and texts, hasshown to produce powerful semantic visual representations, fueling tasks likeimage captioning, text-to-image generation, or image grounding. In the contextof human-centric vision, albeit CLIP-like representations encode most standardhuman poses relatively well (such as standing or sitting), they lack sufficientacuteness to discern detailed or uncommon ones. Actually, while 3D human poseshave been often associated with images (e.g. to perform pose estimation orpose-conditioned image generation), or more recently with text (e.g. fortext-to-pose generation), they have seldom been paired with both. In this work,we combine 3D poses, person's pictures and textual pose descriptions to producean enhanced 3D-, visual- and semantic-aware human pose representation. Weintroduce a new transformer-based model, trained in a retrieval fashion, whichcan take as input any combination of the aforementioned modalities. Whencomposing modalities, it outperforms a standard multi-modal alignment retrievalmodel, making it possible to sort out partial information (e.g. image with thelower body occluded). We showcase the potential of such an embroidered poserepresentation for (1) SMPL regression from image with optional text cue; and(2) on the task of fine-grained instruction generation, which consists ingenerating a text that describes how to move from one 3D pose to another (as afitness coach). Unlike prior works, our model can take any kind of input (imageand/or pose) without retraining.</description><author>Ginger Delmas, Philippe Weinzaepfel, Francesc Moreno-Noguer, Grégory Rogez</author><pubDate>Tue, 10 Sep 2024 14:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06535v1</guid></item><item><title>Check-Eval: A Checklist-based Approach for Evaluating Text Quality</title><link>http://arxiv.org/abs/2407.14467v2</link><description>Evaluating the quality of text generated by large language models (LLMs)remains a significant challenge. Traditional metrics often fail to align wellwith human judgments, particularly in tasks requiring creativity and nuance. Inthis paper, we propose \textsc{Check-Eval}, a novel evaluation frameworkleveraging LLMs to assess the quality of generated text through achecklist-based approach. \textsc{Check-Eval} can be employed as both areference-free and reference-dependent evaluation method, providing astructured and interpretable assessment of text quality. The framework consistsof two main stages: checklist generation and checklist evaluation. We validate\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal SemanticTextual Similarity and \textsc{SummEval}. Our results demonstrate that\textsc{Check-Eval} achieves higher correlations with human judgments comparedto existing metrics, such as \textsc{G-Eval} and \textsc{GPTScore},underscoring its potential as a more reliable and effective evaluationframework for natural language generation tasks. The code for our experimentsis available at \url{https://anonymous.4open.science/r/check-eval-0DB4}</description><author>Jayr Pereira, Andre Assumpcao, Roberto Lotufo</author><pubDate>Tue, 10 Sep 2024 14:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14467v2</guid></item><item><title>Functionally Constrained Algorithm Solves Convex Simple Bilevel Problems</title><link>http://arxiv.org/abs/2409.06530v1</link><description>This paper studies simple bilevel problems, where a convex upper-levelfunction is minimized over the optimal solutions of a convex lower-levelproblem. We first show the fundamental difficulty of simple bilevel problems,that the approximate optimal value of such problems is not obtainable byfirst-order zero-respecting algorithms. Then we follow recent works to pursuethe weak approximate solutions. For this goal, we propose novel near-optimalmethods for smooth and nonsmooth problems by reformulating them intofunctionally constrained problems.</description><author>Huaqing Zhang, Lesi Chen, Jing Xu, Jingzhao Zhang</author><pubDate>Tue, 10 Sep 2024 14:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06530v1</guid></item></channel></rss>