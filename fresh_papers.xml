<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 12 Dec 2023 06:01:33 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CAD: Photorealistic 3D Generation via Adversarial Distillation</title><link>http://arxiv.org/abs/2312.06663v1</link><description>The increased demand for 3D data in AR/VR, robotics and gaming applications,gave rise to powerful generative pipelines capable of synthesizing high-quality3D objects. Most of these models rely on the Score Distillation Sampling (SDS)algorithm to optimize a 3D representation such that the rendered imagemaintains a high likelihood as evaluated by a pre-trained diffusion model.However, finding a correct mode in the high-dimensional distribution producedby the diffusion model is challenging and often leads to issues such asover-saturation, over-smoothing, and Janus-like artifacts. In this paper, wepropose a novel learning paradigm for 3D synthesis that utilizes pre-traineddiffusion models. Instead of focusing on mode-seeking, our method directlymodels the distribution discrepancy between multi-view renderings and diffusionpriors in an adversarial manner, which unlocks the generation of high-fidelityand photorealistic 3D content, conditioned on a single image and prompt.Moreover, by harnessing the latent space of GANs and expressive diffusion modelpriors, our method facilitates a wide variety of 3D applications includingsingle-view reconstruction, high diversity generation and continuous 3Dinterpolation in the open domain. The experiments demonstrate the superiorityof our pipeline compared to previous works in terms of generation quality anddiversity.</description><author>Ziyu Wan, Despoina Paschalidou, Ian Huang, Hongyu Liu, Bokui Shen, Xiaoyu Xiang, Jing Liao, Leonidas Guibas</author><pubDate>Mon, 11 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06663v1</guid></item><item><title>Photorealistic Video Generation with Diffusion Models</title><link>http://arxiv.org/abs/2312.06662v1</link><description>We present W.A.L.T, a transformer-based approach for photorealistic videogeneration via diffusion modeling. Our approach has two key design decisions.First, we use a causal encoder to jointly compress images and videos within aunified latent space, enabling training and generation across modalities.Second, for memory and training efficiency, we use a window attentionarchitecture tailored for joint spatial and spatiotemporal generative modeling.Taken together these design decisions enable us to achieve state-of-the-artperformance on established video (UCF-101 and Kinetics-600) and image(ImageNet) generation benchmarks without using classifier free guidance.Finally, we also train a cascade of three models for the task of text-to-videogeneration consisting of a base latent video diffusion model, and two videosuper-resolution diffusion models to generate videos of $512 \times 896$resolution at $8$ frames per second.</description><author>Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, Jos√© Lezama</author><pubDate>Mon, 11 Dec 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06662v1</guid></item><item><title>UpFusion: Novel View Diffusion from Unposed Sparse View Observations</title><link>http://arxiv.org/abs/2312.06661v1</link><description>We propose UpFusion, a system that can perform novel view synthesis and infer3D representations for an object given a sparse set of reference images withoutcorresponding pose information. Current sparse-view 3D inference methodstypically rely on camera poses to geometrically aggregate information frominput views, but are not robust in-the-wild when such information isunavailable/inaccurate. In contrast, UpFusion sidesteps this requirement bylearning to implicitly leverage the available images as context in aconditional generative model for synthesizing novel views. We incorporate twocomplementary forms of conditioning into diffusion models for leveraging theinput views: a) via inferring query-view aligned features using a scene-leveltransformer, b) via intermediate attentional layers that can directly observethe input image tokens. We show that this mechanism allows generatinghigh-fidelity novel views while improving the synthesis quality givenadditional (unposed) images. We evaluate our approach on the Co3Dv2 and GoogleScanned Objects datasets and demonstrate the benefits of our method overpose-reliant sparse-view methods as well as single-view methods that cannotleverage additional views. Finally, we also show that our learned model cangeneralize beyond the training categories and even allow reconstruction fromself-captured images of generic objects in-the-wild.</description><author>Bharath Raj Nagoor Kani, Hsin-Ying Lee, Sergey Tulyakov, Shubham Tulsiani</author><pubDate>Mon, 11 Dec 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06661v1</guid></item><item><title>EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM</title><link>http://arxiv.org/abs/2312.06660v1</link><description>This paper presents EdgeSAM, an accelerated variant of the Segment AnythingModel (SAM), optimized for efficient execution on edge devices with minimalcompromise in performance. Our approach involves distilling the originalViT-based SAM image encoder into a purely CNN-based architecture, better suitedfor edge devices. We carefully benchmark various distillation strategies anddemonstrate that task-agnostic encoder distillation fails to capture the fullknowledge embodied in SAM. To overcome this bottleneck, we include both theprompt encoder and mask decoder in the distillation process, with box and pointprompts in the loop, so that the distilled model can accurately capture theintricate dynamics between user input and mask generation. To mitigate datasetbias issues stemming from point prompt distillation, we incorporate alightweight module within the encoder. EdgeSAM achieves a 40-fold speedincrease compared to the original SAM, and it also outperforms MobileSAM, being14 times as fast when deployed on edge devices while enhancing the mIoUs onCOCO and LVIS by 2.3 and 3.2 respectively. It is also the first SAM variantthat can run at over 30 FPS on an iPhone 14. Code and models are available athttps://github.com/chongzhou96/EdgeSAM.</description><author>Chong Zhou, Xiangtai Li, Chen Change Loy, Bo Dai</author><pubDate>Mon, 11 Dec 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06660v1</guid></item><item><title>Mean estimation in the add-remove model of differential privacy</title><link>http://arxiv.org/abs/2312.06658v1</link><description>Differential privacy is often studied under two different models ofneighboring datasets: the add-remove model and the swap model. While the swapmodel is used extensively in the academic literature, many practical librariesuse the more conservative add-remove model. However, analysis under theadd-remove model can be cumbersome, and obtaining results with tight constantsrequires some additional work. Here, we study the problem of one-dimensionalmean estimation under the add-remove model of differential privacy. We proposea new algorithm and show that it is min-max optimal, that it has the correctconstant in the leading term of the mean squared error, and that this constantis the same as the optimal algorithm in the swap model. Our results show that,for mean estimation, the add-remove and swap model give nearly identical erroreven though the add-remove model cannot treat the size of the dataset as publicinformation. In addition, we demonstrate empirically that our proposedalgorithm yields a factor of two improvement in mean squared error overalgorithms often used in practice.</description><author>Alex Kulesza, Ananda Theertha Suresh, Yuyan Wang</author><pubDate>Mon, 11 Dec 2023 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06658v1</guid></item><item><title>Learning Naturally Aggregated Appearance for Efficient 3D Editing</title><link>http://arxiv.org/abs/2312.06657v1</link><description>Neural radiance fields, which represent a 3D scene as a color field and adensity field, have demonstrated great progress in novel view synthesis yet areunfavorable for editing due to the implicitness. In view of such a deficiency,we propose to replace the color field with an explicit 2D appearanceaggregation, also called canonical image, with which users can easily customizetheir 3D editing via 2D image processing. To avoid the distortion effect andfacilitate convenient editing, we complement the canonical image with aprojection field that maps 3D points onto 2D pixels for texture lookup. Thisfield is carefully initialized with a pseudo canonical camera model andoptimized with offset regularity to ensure naturalness of the aggregatedappearance. Extensive experimental results on three datasets suggest that ourrepresentation, dubbed AGAP, well supports various ways of 3D editing (e.g.,stylization, interactive drawing, and content extraction) with no need ofre-optimization for each case, demonstrating its generalizability andefficiency. Project page is available at https://felixcheng97.github.io/AGAP/.</description><author>Ka Leong Cheng, Qiuyu Wang, Zifan Shi, Kecheng Zheng, Yinghao Xu, Hao Ouyang, Qifeng Chen, Yujun Shen</author><pubDate>Mon, 11 Dec 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06657v1</guid></item><item><title>Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</title><link>http://arxiv.org/abs/2312.06655v1</link><description>Recently, 3D content creation from text prompts has demonstrated remarkableprogress by utilizing 2D and 3D diffusion models. While 3D diffusion modelsensure great multi-view consistency, their ability to generate high-quality anddiverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusionmodels find a distillation approach that achieves excellent generalization andrich details without any 3D data. However, 2D lifting methods suffer frominherent view-agnostic ambiguity thereby leading to serious multi-face Janusissues, where text prompts fail to provide sufficient guidance to learncoherent 3D results. Instead of retraining a costly viewpoint-aware model, westudy how to fully exploit easily accessible coarse 3D knowledge to enhance theprompts and guide 2D lifting optimization for refinement. In this paper, wepropose Sherpa3D, a new text-to-3D framework that achieves high-fidelity,generalizability, and geometric consistency simultaneously. Specifically, wedesign a pair of guiding strategies derived from the coarse 3D prior generatedby the 3D diffusion model: a structural guidance for geometric fidelity and asemantic guidance for 3D coherence. Employing the two types of guidance, the 2Ddiffusion model enriches the 3D content with diversified and high-qualityresults. Extensive experiments show the superiority of our Sherpa3D over thestate-of-the-art text-to-3D methods in terms of quality and 3D consistency.</description><author>Fangfu Liu, Diankun Wu, Yi Wei, Yongming Rao, Yueqi Duan</author><pubDate>Mon, 11 Dec 2023 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06655v1</guid></item><item><title>LightSim: Neural Lighting Simulation for Urban Scenes</title><link>http://arxiv.org/abs/2312.06654v1</link><description>Different outdoor illumination conditions drastically alter the appearance ofurban scenes, and they can harm the performance of image-based robot perceptionsystems if not seen during training. Camera simulation provides acost-effective solution to create a large dataset of images captured underdifferent lighting conditions. Towards this goal, we propose LightSim, a neurallighting camera simulation system that enables diverse, realistic, andcontrollable data generation. LightSim automatically builds lighting-awaredigital twins at scale from collected raw sensor data and decomposes the sceneinto dynamic actors and static background with accurate geometry, appearance,and estimated scene lighting. These digital twins enable actor insertion,modification, removal, and rendering from a new viewpoint, all in alighting-aware manner. LightSim then combines physically-based and learnabledeferred rendering to perform realistic relighting of modified scenes, such asaltering the sun location and modifying the shadows or changing the sunbrightness, producing spatially- and temporally-consistent camera videos. Ourexperiments show that LightSim generates more realistic relighting results thanprior work. Importantly, training perception models on data generated byLightSim can significantly improve their performance.</description><author>Ava Pun, Gary Sun, Jingkang Wang, Yun Chen, Ze Yang, Sivabalan Manivasagam, Wei-Chiu Ma, Raquel Urtasun</author><pubDate>Mon, 11 Dec 2023 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06654v1</guid></item><item><title>Adaptive Human Trajectory Prediction via Latent Corridors</title><link>http://arxiv.org/abs/2312.06653v1</link><description>Human trajectory prediction is typically posed as a zero-shot generalizationproblem: a predictor is learnt on a dataset of human motion in training scenes,and then deployed on unseen test scenes. While this paradigm has yieldedtremendous progress, it fundamentally assumes that trends in human behaviorwithin the deployment scene are constant over time. As such, current predictionmodels are unable to adapt to scene-specific transient human behaviors, such ascrowds temporarily gathering to see buskers, pedestrians hurrying through therain and avoiding puddles, or a protest breaking out. We formalize the problemof scene-specific adaptive trajectory prediction and propose a new adaptationapproach inspired by prompt tuning called latent corridors. By augmenting theinput of any pre-trained human trajectory predictor with learnable imageprompts, the predictor can improve in the deployment scene by inferring trendsfrom extremely small amounts of new data (e.g., 2 humans observed for 30seconds). With less than 0.1% additional model parameters, we see up to 23.9%ADE improvement in MOTSynth simulated data and 16.4% ADE in MOT and Wildtrackreal pedestrian data. Qualitatively, we observe that latent corridors imbuepredictors with an awareness of scene geometry and scene-specific humanbehaviors that non-adaptive predictors struggle to capture. The project websitecan be found at https://neerja.me/atp_latent_corridors/.</description><author>Neerja Thakkar, Karttikeya Mangalam, Andrea Bajcsy, Jitendra Malik</author><pubDate>Mon, 11 Dec 2023 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06653v1</guid></item><item><title>Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?</title><link>http://arxiv.org/abs/2312.06652v1</link><description>Large Language Models (LLMs) have demonstrated remarkable performance acrossnumerous natural language understanding use cases. However, this impressiveperformance comes with inherent limitations, such as the tendency to perpetuatestereotypical biases or fabricate non-existent facts. In the context of Islamand its representation, accurate and factual representation of its beliefs andteachings rooted in the Quran and Sunnah is key. This work focuses on thechallenge of building domain-specific LLMs faithful to the Islamic worldviewand proposes ways to build and evaluate such systems. Firstly, we define thisopen-ended goal as a technical problem and propose various solutions.Subsequently, we critically examine known challenges inherent to each approachand highlight evaluation methodologies that can be used to assess such systems.This work highlights the need for high-quality datasets, evaluations, andinterdisciplinary work blending machine learning with Islamic scholarship.</description><author>Shabaz Patel, Hassan Kane, Rayhan Patel</author><pubDate>Mon, 11 Dec 2023 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06652v1</guid></item><item><title>Nuvo: Neural UV Mapping for Unruly 3D Representations</title><link>http://arxiv.org/abs/2312.05283v1</link><description>Existing UV mapping algorithms are designed to operate on well-behavedmeshes, instead of the geometry representations produced by state-of-the-art 3Dreconstruction and generation techniques. As such, applying these methods tothe volume densities recovered by neural radiance fields and related techniques(or meshes triangulated from such fields) results in texture atlases that aretoo fragmented to be useful for tasks such as view synthesis or appearanceediting. We present a UV mapping method designed to operate on geometryproduced by 3D reconstruction and generation techniques. Instead of computing amapping defined on a mesh's vertices, our method Nuvo uses a neural field torepresent a continuous UV mapping, and optimizes it to be a valid andwell-behaved mapping for just the set of visible points, i.e. only points thataffect the scene's appearance. We show that our model is robust to thechallenges posed by ill-behaved geometry, and that it produces editable UVmappings that can represent detailed appearance.</description><author>Pratul P. Srinivasan, Stephan J. Garbin, Dor Verbin, Jonathan T. Barron, Ben Mildenhall</author><pubDate>Mon, 11 Dec 2023 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05283v1</guid></item><item><title>The Waymo Open Sim Agents Challenge</title><link>http://arxiv.org/abs/2305.12032v4</link><description>Simulation with realistic, interactive agents represents a key task forautonomous vehicle software development. In this work, we introduce the WaymoOpen Sim Agents Challenge (WOSAC). WOSAC is the first public challenge totackle this task and propose corresponding metrics. The goal of the challengeis to stimulate the design of realistic simulators that can be used to evaluateand train a behavior model for autonomous driving. We outline our evaluationmethodology, present results for a number of different baseline simulationagent methods, and analyze several submissions to the 2023 competition whichran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remainsopen for submissions and we discuss open problems for the task.</description><author>Nico Montali, John Lambert, Paul Mougin, Alex Kuefler, Nick Rhinehart, Michelle Li, Cole Gulino, Tristan Emrich, Zoey Yang, Shimon Whiteson, Brandyn White, Dragomir Anguelov</author><pubDate>Mon, 11 Dec 2023 18:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12032v4</guid></item><item><title>4M: Massively Multimodal Masked Modeling</title><link>http://arxiv.org/abs/2312.06647v1</link><description>Current machine learning models for vision are often highly specialized andlimited to a single modality and task. In contrast, recent large languagemodels exhibit a wide range of capabilities, hinting at a possibility forsimilarly versatile models in computer vision. In this paper, we take a step inthis direction and propose a multimodal training scheme called 4M. It consistsof training a single unified Transformer encoder-decoder using a maskedmodeling objective across a wide range of input/output modalities - includingtext, images, geometric, and semantic modalities, as well as neural networkfeature maps. 4M achieves scalability by unifying the representation space ofall modalities through mapping them into discrete tokens and performingmultimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they canperform a diverse set of vision tasks out of the box, (2) they excel whenfine-tuned for unseen downstream tasks or new input modalities, and (3) theycan function as a generative model that can be conditioned on arbitrarymodalities, enabling a wide variety of expressive multimodal editingcapabilities with remarkable flexibility. Through experimental analyses, we demonstrate the potential of 4M fortraining versatile and scalable foundation models for vision tasks, setting thestage for further exploration in multimodal learning for vision and otherdomains.</description><author>David Mizrahi, Roman Bachmann, Oƒüuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir</author><pubDate>Mon, 11 Dec 2023 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06647v1</guid></item><item><title>Dense X Retrieval: What Retrieval Granularity Should We Use?</title><link>http://arxiv.org/abs/2312.06648v1</link><description>Dense retrieval has become a prominent method to obtain relevant context orworld knowledge in open-domain NLP tasks. When we use a learned dense retrieveron a retrieval corpus at inference time, an often-overlooked design choice isthe retrieval unit in which the corpus is indexed, e.g. document, passage, orsentence. We discover that the retrieval unit choice significantly impacts theperformance of both retrieval and downstream tasks. Distinct from the typicalapproach of using passages or sentences, we introduce a novel retrieval unit,proposition, for dense retrieval. Propositions are defined as atomicexpressions within text, each encapsulating a distinct factoid and presented ina concise, self-contained natural language format. We conduct an empiricalcomparison of different retrieval granularity. Our results reveal thatproposition-based retrieval significantly outperforms traditional passage orsentence-based methods in dense retrieval. Moreover, retrieval by propositionalso enhances the performance of downstream QA tasks, since the retrieved textsare more condensed with question-relevant information, reducing the need forlengthy input tokens and minimizing the inclusion of extraneous, irrelevantinformation.</description><author>Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Dong Yu, Hongming Zhang</author><pubDate>Mon, 11 Dec 2023 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06648v1</guid></item><item><title>Computational Copyright: Towards A Royalty Model for AI Music Generation Platforms</title><link>http://arxiv.org/abs/2312.06646v1</link><description>The advancement of generative AI has given rise to pressing copyrightchallenges, particularly in music industry. This paper focuses on the economicaspects of these challenges, emphasizing that the economic impact constitutes acentral issue in the copyright arena. The complexity of the black-boxgenerative AI technologies not only suggests but necessitates algorithmicsolutions. However, such solutions have been largely missing, leading toregulatory challenges in this landscape. We aim to bridge the gap in currentapproaches by proposing potential royalty models for revenue sharing on AImusic generation platforms. Our methodology involves a detailed analysis ofexisting royalty models in platforms like Spotify and YouTube, and adaptingthese to the unique context of AI-generated music. A significant challenge weaddress is the attribution of AI-generated music to influential copyrightedcontent in the training data. To this end, we present algorithmic solutionsemploying data attribution techniques. Our experimental results verify theeffectiveness of these solutions. This research represents a pioneering effortin integrating technical advancements with economic and legal considerations inthe field of generative AI, offering a computational copyright solution for thechallenges posed by the opaque nature of AI technologies.</description><author>Junwei Deng, Jiaqi Ma</author><pubDate>Mon, 11 Dec 2023 18:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06646v1</guid></item><item><title>Beyond Classification: Definition and Density-based Estimation of Calibration in Object Detection</title><link>http://arxiv.org/abs/2312.06645v1</link><description>Despite their impressive predictive performance in various computer visiontasks, deep neural networks (DNNs) tend to make overly confident predictions,which hinders their widespread use in safety-critical applications. While therehave been recent attempts to calibrate DNNs, most of these efforts haveprimarily been focused on classification tasks, thus neglecting DNN-basedobject detectors. Although several recent works addressed calibration forobject detection and proposed differentiable penalties, none of them areconsistent estimators of established concepts in calibration. In this work, wetackle the challenge of defining and estimating calibration error specificallyfor this task. In particular, we adapt the definition of classificationcalibration error to handle the nuances associated with object detection, andpredictions in structured output spaces more generally. Furthermore, we proposea consistent and differentiable estimator of the detection calibration error,utilizing kernel density estimation. Our experiments demonstrate theeffectiveness of our estimator against competing train-time and post-hoccalibration methods, while maintaining similar detection performance.</description><author>Teodora Popordanoska, Aleksei Tiulpin, Matthew B. Blaschko</author><pubDate>Mon, 11 Dec 2023 18:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06645v1</guid></item><item><title>AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes</title><link>http://arxiv.org/abs/2312.06644v1</link><description>We introduce AnyHome, a framework that translates open-vocabularydescriptions, ranging from simple labels to elaborate paragraphs, intowell-structured and textured 3D indoor scenes at a house-scale. Inspired bycognition theories, AnyHome employs an amodal structured representation tocapture 3D spatial cues from textual narratives and then uses egocentricinpainting to enrich these scenes. To this end, we begin by using speciallydesigned template prompts for Large Language Models (LLMs), which enableprecise control over the textual input. We then utilize intermediaterepresentations to maintain the spatial structure's consistency, ensuring thatthe 3D scenes align closely with the textual description. Then, we apply aScore Distillation Sampling process to refine the placement of objects. Lastly,an egocentric inpainting process is incorporated to enhance the realism andappearance of the scenes. AnyHome stands out due to its hierarchical structuredrepresentation combined with the versatility of open-vocabulary textinterpretation. This allows for extensive customization of indoor scenes atvarious levels of granularity. We demonstrate that AnyHome can reliablygenerate a range of diverse indoor scenes, characterized by their detailedspatial structures and textures, all corresponding to the free-form textualinputs.</description><author>Zehao Wen, Zichen Liu, Srinath Sridhar, Rao Fu</author><pubDate>Mon, 11 Dec 2023 18:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06644v1</guid></item><item><title>Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration</title><link>http://arxiv.org/abs/2312.06643v1</link><description>Collaborative robots (cobots) are widely used in industrial applications, yetextensive research is still needed to enhance human-robot collaborations andoperator experience. A potential approach to improve the collaborationexperience involves adapting cobot behavior based on natural cues from theoperator. Inspired by the literature on human-human interactions, we conducteda wizard-of-oz study to examine whether a gaze towards the cobot can serve as atrigger for initiating joint activities in collaborative sessions. In thisstudy, 37 participants engaged in an assembly task while their gaze behaviorwas analyzed. We employ a gaze-based attention recognition model to identifywhen the participants look at the cobot. Our results indicate that in mostcases (84.88\%), the joint activity is preceded by a gaze towards the cobot.Furthermore, during the entire assembly cycle, the participants tend to look atthe cobot around the time of the joint activity. To the best of our knowledge,this is the first study to analyze the natural gaze behavior of participantsworking on a joint activity with a robot during a collaborative assembly task.</description><author>Pooja Prajod, Matteo Lavit Nicora, Marta Mondellini, Giovanni Tauro, Rocco Vertechy, Matteo Malosio, Elisabeth Andr√©</author><pubDate>Mon, 11 Dec 2023 18:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06643v1</guid></item><item><title>CorresNeRF: Image Correspondence Priors for Neural Radiance Fields</title><link>http://arxiv.org/abs/2312.06642v1</link><description>Neural Radiance Fields (NeRFs) have achieved impressive results in novel viewsynthesis and surface reconstruction tasks. However, their performance suffersunder challenging scenarios with sparse input views. We present CorresNeRF, anovel method that leverages image correspondence priors computed byoff-the-shelf methods to supervise NeRF training. We design adaptive processesfor augmentation and filtering to generate dense and high-qualitycorrespondences. The correspondences are then used to regularize NeRF trainingvia the correspondence pixel reprojection and depth loss terms. We evaluate ourmethods on novel view synthesis and surface reconstruction tasks withdensity-based and SDF-based NeRF models on different datasets. Our methodoutperforms previous methods in both photometric and geometric metrics. We showthat this simple yet effective technique of using correspondence priors can beapplied as a plug-and-play module across different NeRF variants. The projectpage is at https://yxlao.github.io/corres-nerf.</description><author>Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao</author><pubDate>Mon, 11 Dec 2023 18:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06642v1</guid></item><item><title>Online Decision Making with History-Average Dependent Costs (Extended)</title><link>http://arxiv.org/abs/2312.06641v1</link><description>In many online sequential decision-making scenarios, a learner's choicesaffect not just their current costs but also the future ones. In this work, welook at one particular case of such a situation where the costs depend on thetime average of past decisions over a history horizon. We first recast thisproblem with history dependent costs as a problem of decision making understage-wise constraints. To tackle this, we then propose the novelFollow-The-Adaptively-Regularized-Leader (FTARL) algorithm. Our innovativealgorithm incorporates adaptive regularizers that depend explicitly on pastdecisions, allowing us to enforce stage-wise constraints while simultaneouslyenabling us to establish tight regret bounds. We also discuss the implicationsof the length of history horizon on design of no-regret algorithms for ourproblem and present impossibility results when it is the full learning horizon.</description><author>Vijeth Hebbar, Cedric Langbort</author><pubDate>Mon, 11 Dec 2023 18:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06641v1</guid></item><item><title>Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution</title><link>http://arxiv.org/abs/2312.06640v1</link><description>Text-based diffusion models have exhibited remarkable success in generationand editing, showing great promise for enhancing visual content with theirgenerative prior. However, applying these models to video super-resolutionremains challenging due to the high demands for output fidelity and temporalconsistency, which is complicated by the inherent randomness in diffusionmodels. Our study introduces Upscale-A-Video, a text-guided latent diffusionframework for video upscaling. This framework ensures temporal coherencethrough two key mechanisms: locally, it integrates temporal layers into U-Netand VAE-Decoder, maintaining consistency within short sequences; globally,without training, a flow-guided recurrent latent propagation module isintroduced to enhance overall video stability by propagating and fusing latentacross the entire sequences. Thanks to the diffusion paradigm, our model alsooffers greater flexibility by allowing text prompts to guide texture creationand adjustable noise levels to balance restoration and generation, enabling atrade-off between fidelity and quality. Extensive experiments show thatUpscale-A-Video surpasses existing methods in both synthetic and real-worldbenchmarks, as well as in AI-generated videos, showcasing impressive visualrealism and temporal consistency.</description><author>Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, Chen Change Loy</author><pubDate>Mon, 11 Dec 2023 18:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06640v1</guid></item><item><title>Harmonic Mobile Manipulation</title><link>http://arxiv.org/abs/2312.06639v1</link><description>Recent advancements in robotics have enabled robots to navigate complexscenes or manipulate diverse objects independently. However, robots are stillimpotent in many household tasks requiring coordinated behaviors such asopening doors. The factorization of navigation and manipulation, whileeffective for some tasks, fails in scenarios requiring coordinated actions. Toaddress this challenge, we introduce, HarmonicMM, an end-to-end learning methodthat optimizes both navigation and manipulation, showing notable improvementover existing techniques in everyday tasks. This approach is validated insimulated and real-world environments and adapts to novel unseen settingswithout additional tuning. Our contributions include a new benchmark for mobilemanipulation and the successful deployment in a real unseen apartment,demonstrating the potential for practical indoor robot deployment in dailylife. More results are on our project site:https://rchalyang.github.io/HarmonicMM/</description><author>Ruihan Yang, Yejin Kim, Aniruddha Kembhavi, Xiaolong Wang, Kiana Ehsani</author><pubDate>Mon, 11 Dec 2023 18:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06639v1</guid></item><item><title>SurvBeNIM: The Beran-Based Neural Importance Model for Explaining the Survival Models</title><link>http://arxiv.org/abs/2312.06638v1</link><description>A new method called the Survival Beran-based Neural Importance Model(SurvBeNIM) is proposed. It aims to explain predictions of machine learningsurvival models, which are in the form of survival or cumulative hazardfunctions. The main idea behind SurvBeNIM is to extend the Beran estimator byincorporating the importance functions into its kernels and by implementingthese importance functions as a set of neural networks which are jointlytrained in an end-to-end manner. Two strategies of using and training the wholeneural network implementing SurvBeNIM are proposed. The first one explains asingle instance, and the neural network is trained for each explained instance.According to the second strategy, the neural network only learns once on allinstances from the dataset and on all generated instances. Then the neuralnetwork is used to explain any instance in a dataset domain. Various numericalexperiments compare the method with different existing explanation methods. Acode implementing the proposed method is publicly available.</description><author>Lev V. Utkin, Danila Y. Eremenko, Andrei V. Konstantinov</author><pubDate>Mon, 11 Dec 2023 18:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06638v1</guid></item><item><title>Multi-Tier Hierarchical Federated Learning-assisted NTN for Intelligent IoT Services</title><link>http://arxiv.org/abs/2305.05463v2</link><description>In the ever-expanding landscape of the IoT, managing the intricate network ofinterconnected devices presents a fundamental challenge. This leads us to ask:"What if we invite the IoT devices to collaboratively participate in real-timenetwork management and IoT data-handling decisions?" This inquiry forms thefoundation of our innovative approach, addressing the burgeoning complexitiesin IoT through the integration of NTN architecture, in particular, VHetNet, andan MT-HFL framework. VHetNets transcend traditional network paradigms byharmonizing terrestrial and non-terrestrial elements, thus ensuring expansiveconnectivity and resilience, especially crucial in areas with limitedterrestrial infrastructure. The incorporation of MT-HFL further revolutionizesthis architecture, distributing intelligent data processing across amulti-tiered network spectrum, from edge devices on the ground to aerialplatforms and satellites above. This study explores MT-HFL's role in fosteringa decentralized, collaborative learning environment, enabling IoT devices tonot only contribute but also make informed decisions in network management.This methodology adeptly handles the challenges posed by the non-IID nature ofIoT data and efficiently curtails communication overheads prevalent inextensive IoT networks. Significantly, MT-HFL enhances data privacy, aparamount aspect in IoT ecosystems, by facilitating local data processing andlimiting the sharing of model updates instead of raw data. By evaluating acase-study, our findings demonstrate that the synergistic integration of MT-HFLwithin VHetNets creates an intelligent network architecture that is robust,scalable, and dynamically adaptive to the ever-changing demands of IoTenvironments. This setup ensures efficient data handling, advanced privacy andsecurity measures, and responsive adaptability to fluctuating networkconditions.</description><author>Amin Farajzadeh, Animesh Yadav, Halim Yanikomeroglu</author><pubDate>Mon, 11 Dec 2023 18:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05463v2</guid></item><item><title>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception</title><link>http://arxiv.org/abs/2305.06324v2</link><description>We present Integrated Multimodal Perception (IMP), a simple and scalablemultimodal multi-task training and modeling approach. IMP integrates multimodalinputs including image, video, text, and audio into a single Transformerencoder with minimal modality-specific components. IMP makes use of a noveldesign that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts(MoE) for efficient model and task scaling. We conduct extensive empiricalstudies and reveal the following key insights: 1) Performing gradient descentupdates by alternating on diverse modalities, loss functions, and tasks, withvarying input resolutions, efficiently improves the model. 2) Sparsificationwith MoE on a single modality-agnostic encoder substantially improves theperformance, outperforming dense models that use modality-specific encoders oradditional fusion layers and greatly mitigates the conflicts betweenmodalities. IMP achieves competitive performance on a wide range of downstreamtasks including video classification, image classification, image-text, andvideo-text retrieval. Most notably, we train a sparse IMP-MoE-L variantfocusing on video tasks that achieves new state-of-the-art in zero-shot videoclassification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% onKinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%,respectively, while using only 15% of their total training computational cost.</description><author>Hassan Akbari, Dan Kondratyuk, Yin Cui, Rachel Hornung, Huisheng Wang, Hartwig Adam</author><pubDate>Mon, 11 Dec 2023 18:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06324v2</guid></item><item><title>Gated Linear Attention Transformers with Hardware-Efficient Training</title><link>http://arxiv.org/abs/2312.06635v1</link><description>Transformers with linear attention allow for efficient parallel training butcan simultaneously be formulated as an RNN with 2D (matrix-valued) hiddenstates, thus enjoying linear (with respect to output length) inferencecomplexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM(Qin et al., 2023a) observe that adding a global decay term to the additive RNNupdate rule greatly improves performance, sometimes outperforming standardTransformers with softmax attention when trained at scale. In this work we showthat adding a data-dependent gating mechanism further improves performance. Wederive a parallel form of this gated linear attention layer that enablesefficient training. However, a straightforward, numerically stableimplementation of this parallel form requires generalized matrixmultiplications in log-space for numerical stability, and thus cannot takeadvantage of tensor cores on modern GPUs which are optimized for standardmatrix multiplications. We develop a hardware-efficient version of the parallelform that can still make use of tensor cores through block-parallelcomputations over sequence chunks. Experiments on moderate-scale languagemodeling (340M-parameter models trained on 15B tokens, 1.3B-parameter modelstrained on 100B tokens) show that gated linear attention (GLA) Transformersperform competitively against a strong LLaMA-architecture Transformer baseline(Touvron et al., 2023) as well as Mamba (Gu &amp; Dao, 2023), a recently introducedstate-space model with a data-dependent state transition mechanism. Fortraining speed, our Triton-based implementation performs comparably toCUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 traininglength setting, while outperforming FlashAttention-2 when training on longersequences beyond 4096.</description><author>Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim</author><pubDate>Mon, 11 Dec 2023 18:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06635v1</guid></item><item><title>Examining the Effect of Implementation Factors on Deep Learning Reproducibility</title><link>http://arxiv.org/abs/2312.06633v1</link><description>Reproducing published deep learning papers to validate their conclusions canbe difficult due to sources of irreproducibility. We investigate the impactthat implementation factors have on the results and how they affectreproducibility of deep learning studies. Three deep learning experiments wereran five times each on 13 different hardware environments and four differentsoftware environments. The analysis of the 780 combined results showed thatthere was a greater than 6% accuracy range on the same deterministic examplesintroduced from hardware or software environment variations alone. To accountfor these implementation factors, researchers should run their experimentsmultiple times in different hardware and software environments to verify theirconclusions are not affected.</description><author>Kevin Coakley, Christine R. Kirkpatrick, Odd Erik Gundersen</author><pubDate>Mon, 11 Dec 2023 18:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06633v1</guid></item><item><title>Control Risk for Potential Misuse of Artificial Intelligence in Science</title><link>http://arxiv.org/abs/2312.06632v1</link><description>The expanding application of Artificial Intelligence (AI) in scientificfields presents unprecedented opportunities for discovery and innovation.However, this growth is not without risks. AI models in science, if misused,can amplify risks like creation of harmful substances, or circumvention ofestablished regulations. In this study, we aim to raise awareness of thedangers of AI misuse in science, and call for responsible AI development anduse in this domain. We first itemize the risks posed by AI in scientificcontexts, then demonstrate the risks by highlighting real-world examples ofmisuse in chemical science. These instances underscore the need for effectiverisk management strategies. In response, we propose a system called SciGuard tocontrol misuse risks for AI models in science. We also propose a red-teamingbenchmark SciMT-Safety to assess the safety of different systems. Our proposedSciGuard shows the least harmful impact in the assessment without compromisingperformance in benign tests. Finally, we highlight the need for amultidisciplinary and collaborative effort to ensure the safe and ethical useof AI models in science. We hope that our study can spark productivediscussions on using AI ethically in science among researchers, practitioners,policymakers, and the public, to maximize benefits and minimize the risks ofmisuse.</description><author>Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo Zhou, Xing Xie, Weiming Zhang, Nenghai Yu, Shuxin Zheng</author><pubDate>Mon, 11 Dec 2023 18:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06632v1</guid></item><item><title>TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation</title><link>http://arxiv.org/abs/2312.06630v1</link><description>Training on large-scale datasets can boost the performance of video instancesegmentation while the annotated datasets for VIS are hard to scale up due tothe high labor cost. What we possess are numerous isolated filed-specificdatasets, thus, it is appealing to jointly train models across the aggregationof datasets to enhance data volume and diversity. However, due to theheterogeneity in category space, as mask precision increases with the datavolume, simply utilizing multiple datasets will dilute the attention of modelson different taxonomies. Thus, increasing the data scale and enriching taxonomyspace while improving classification precision is important. In this work, weanalyze that providing extra taxonomy information can help models concentrateon specific taxonomy, and propose our model named Taxonomy-aware Multi-datasetJoint Training for Video Instance Segmentation (TMT-VIS) to address this vitalchallenge. Specifically, we design a two-stage taxonomy aggregation module thatfirst compiles taxonomy information from input videos and then aggregates thesetaxonomy priors into instance queries before the transformer decoder. Weconduct extensive experimental evaluations on four popular and challengingbenchmarks, including YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and UVO. Ourmodel shows significant improvement over the baseline solutions, and sets newstate-of-the-art records on all benchmarks. These appealing and encouragingresults demonstrate the effectiveness and generality of our approach. The codeis available athttps://github.com/rkzheng99/TMT-VIS(https://github.com/rkzheng99/TMT-VIS)</description><author>Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, Hengshuang Zhao</author><pubDate>Mon, 11 Dec 2023 18:50:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06630v1</guid></item><item><title>AttenScribble: Attentive Similarity Learning for Scribble-Supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2312.06614v1</link><description>The success of deep networks in medical image segmentation relies heavily onmassive labeled training data. However, acquiring dense annotations is atime-consuming process. Weakly-supervised methods normally employ lessexpensive forms of supervision, among which scribbles started to gainpopularity lately thanks to its flexibility. However, due to lack of shape andboundary information, it is extremely challenging to train a deep network onscribbles that generalizes on unlabeled pixels. In this paper, we present astraightforward yet effective scribble supervised learning framework. Inspiredby recent advances of transformer based segmentation, we create a pluggablespatial self-attention module which could be attached on top of any internalfeature layers of arbitrary fully convolutional network (FCN) backbone. Themodule infuses global interaction while keeping the efficiency of convolutions.Descended from this module, we construct a similarity metric based onnormalized and symmetrized attention. This attentive similarity leads to anovel regularization loss that imposes consistency between segmentationprediction and visual affinity. This attentive similarity loss optimizes thealignment of FCN encoders, attention mapping and model prediction. Ultimately,the proposed FCN+Attention architecture can be trained end-to-end guided by acombination of three learning objectives: partial segmentation loss, acustomized masked conditional random fields and the proposed attentivesimilarity loss. Extensive experiments on public datasets (ACDC and CHAOS)showed that our framework not just out-performs existing state-of-the-art, butalso delivers close performance to fully-supervised benchmark. Code will beavailable upon publication.</description><author>Mu Tian, Qinzhu Yang, Yi Gao</author><pubDate>Mon, 11 Dec 2023 18:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06614v1</guid></item><item><title>Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism</title><link>http://arxiv.org/abs/2312.06613v1</link><description>Recent advances in deep learning for sequential data have given rise to fastand powerful models that produce realistic videos of talking humans. The stateof the art in talking face generation focuses mainly on lip-syncing, beingconditioned on audio clips. However, having the ability to synthesize talkinghumans from text transcriptions rather than audio is particularly beneficialfor many applications and is expected to receive more and more attention,following the recent breakthroughs in large language models. For that, mostmethods implement a cascaded 2-stage architecture of a text-to-speech modulefollowed by an audio-driven talking face generator, but this ignores the highlycomplex interplay between audio and visual streams that occurs during speaking.In this paper, we propose the first, to the best of our knowledge, text-drivenaudiovisual speech synthesizer that uses Transformers and does not follow acascaded approach. Our method, which we call NEUral Text to ARticulate Talk(NEUTART), is a talking face generator that uses a joint audiovisual featurespace, as well as speech-informed 3D facial reconstructions and a lip-readingloss for visual supervision. The proposed model produces photorealistic talkingface videos with human-like articulation and well-synced audiovisual streams.Our experiments on audiovisual datasets as well as in-the-wild videos revealstate-of-the-art generation quality both in terms of objective metrics andhuman evaluation.</description><author>Georgios Milis, Panagiotis P. Filntisis, Anastasios Roussos, Petros Maragos</author><pubDate>Mon, 11 Dec 2023 18:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06613v1</guid></item><item><title>Information theory for model reduction in stochastic dynamical systems</title><link>http://arxiv.org/abs/2312.06608v1</link><description>Model reduction is the construction of simple yet predictive descriptions ofthe dynamics of many-body systems in terms of a few relevant variables. Aprerequisite to model reduction is the identification of these relevantvariables, a task for which no general method exists. Here, we develop asystematic approach based on the information bottleneck to identify therelevant variables, defined as those most predictive of the future. Weelucidate analytically the relation between these relevant variables and theeigenfunctions of the transfer operator describing the dynamics. Further, weshow that in the limit of high compression, the relevant variables are directlydetermined by the slowest-decaying eigenfunctions. Our information-basedapproach indicates when to optimally stop increasing the complexity of thereduced model. Further, it provides a firm foundation to constructinterpretable deep learning tools that perform model reduction. We illustratehow these tools work on benchmark dynamical systems and deploy them onuncurated datasets, such as satellite movies of atmospheric flows downloadeddirectly from YouTube.</description><author>Matthew S. Schmitt, Maciej Koch-Janusz, Michel Fruchart, Daniel S. Seara, Vincenzo Vitelli</author><pubDate>Mon, 11 Dec 2023 18:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06608v1</guid></item><item><title>DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection</title><link>http://arxiv.org/abs/2312.06607v1</link><description>Reconstruction-based approaches have achieved remarkable outcomes in anomalydetection. The exceptional image reconstruction capabilities of recentlypopular diffusion models have sparked research efforts to utilize them forenhanced reconstruction of anomalous images. Nonetheless, these methods mightface challenges related to the preservation of image categories and pixel-wisestructural integrity in the more practical multi-class setting. To solve theabove problems, we propose a Difusion-based Anomaly Detection (DiAD) frameworkfor multi-class anomaly detection, which consists of a pixel-space autoencoder,a latent-space Semantic-Guided (SG) network with a connection to the stablediffusion's denoising network, and a feature-space pre-trained featureextractor. Firstly, The SG network is proposed for reconstructing anomalousregions while preserving the original image's semantic information. Secondly,we introduce Spatial-aware Feature Fusion (SFF) block to maximizereconstruction accuracy when dealing with extensively reconstructed areas.Thirdly, the input and reconstructed images are processed by a pre-trainedfeature extractor to generate anomaly maps based on features extracted atdifferent scales. Experiments on MVTec-AD and VisA datasets demonstrate theeffectiveness of our approach which surpasses the state-of-the-art methods,e.g., achieving 96.8/52.6 and 97.2/99.0 (AUROC/AP) for localization anddetection respectively on multi-class MVTec-AD dataset. Code will be availableat https://lewandofskee.github.io/projects/diad.</description><author>Haoyang He, Jiangning Zhang, Hongxu Chen, Xuhai Chen, Zhishan Li, Xu Chen, Yabiao Wang, Chengjie Wang, Lei Xie</author><pubDate>Mon, 11 Dec 2023 18:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06607v1</guid></item><item><title>EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models</title><link>http://arxiv.org/abs/2307.02028v3</link><description>While the general machine learning (ML) community has benefited from publicdatasets, tasks, and models, the progress of ML in healthcare has been hamperedby a lack of such shared assets. The success of foundation models creates newchallenges for healthcare ML by requiring access to shared pretrained models tovalidate performance benefits. We help address these challenges through threecontributions. First, we publish a new dataset, EHRSHOT, which containsdeidentified structured data from the electronic health records (EHRs) of 6,739patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHRdatasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.Second, we publish the weights of CLMBR-T-base, a 141M parameter clinicalfoundation model pretrained on the structured EHR data of 2.57M patients. Weare one of the first to fully release such a model for coded EHR data; incontrast, most prior models released for clinical data (e.g. GatorTron,ClinicalBERT) only work with unstructured text and cannot process the rich,structured data within an EHR. We provide an end-to-end pipeline for thecommunity to validate and build upon its performance. Third, we define 15few-shot clinical prediction tasks, enabling evaluation of foundation models onbenefits such as sample efficiency and task adaptation. Our model and datasetare available via a research data use agreement from our website:https://ehrshot.stanford.edu. Code to reproduce our results are available atour Github repo: https://github.com/som-shahlab/ehrshot-benchmark</description><author>Michael Wornow, Rahul Thapa, Ethan Steinberg, Jason A. Fries, Nigam H. Shah</author><pubDate>Mon, 11 Dec 2023 18:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02028v3</guid></item><item><title>Fast Policy Learning for Linear Quadratic Control with Entropy Regularization</title><link>http://arxiv.org/abs/2311.14168v3</link><description>This paper proposes and analyzes two new policy learning methods: regularizedpolicy gradient (RPG) and iterative policy optimization (IPO), for a class ofdiscounted linear-quadratic control (LQC) problems over an infinite timehorizon with entropy regularization. Assuming access to the exact policyevaluation, both proposed approaches are proven to converge linearly in findingoptimal policies of the regularized LQC. Moreover, the IPO method can achieve asuper-linear convergence rate once it enters a local region around the optimalpolicy. Finally, when the optimal policy for an RL problem with a knownenvironment is appropriately transferred as the initial policy to an RL problemwith an unknown environment, the IPO method is shown to enable a super-linearconvergence rate if the two environments are sufficiently close. Performancesof these proposed algorithms are supported by numerical examples.</description><author>Xin Guo, Xinyu Li, Renyuan Xu</author><pubDate>Mon, 11 Dec 2023 18:32:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14168v3</guid></item><item><title>Early Action Recognition with Action Prototypes</title><link>http://arxiv.org/abs/2312.06598v1</link><description>Early action recognition is an important and challenging problem that enablesthe recognition of an action from a partially observed video stream where theactivity is potentially unfinished or even not started. In this work, wepropose a novel model that learns a prototypical representation of the fullaction for each class and uses it to regularize the architecture and the visualrepresentations of the partial observations. Our model is very simple in designand also efficient. We decompose the video into short clips, where a visualencoder extracts features from each clip independently. Later, a decoderaggregates together in an online fashion features from all the clips for thefinal class prediction. During training, for each partial observation, themodel is jointly trained to both predict the label as well as the actionprototypical representation which acts as a regularizer. We evaluate our methodon multiple challenging real-world datasets and outperform the currentstate-of-the-art by a significant margin. For example, on early recognitionobserving only the first 10% of each video, our method improves the SOTA by+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 onSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used eithermulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we alsopresent exhaustive ablation studies to motivate the design choices we made, aswell as gather insights regarding what our model is learning semantically.</description><author>Guglielmo Camporese, Alessandro Bergamo, Xunyu Lin, Joseph Tighe, Davide Modolo</author><pubDate>Mon, 11 Dec 2023 18:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06598v1</guid></item><item><title>Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops</title><link>http://arxiv.org/abs/2312.06594v1</link><description>Objects undergo varying amounts of perspective distortion as they move acrossa camera's field of view. Models for predicting 3D from a single image oftenwork with crops around the object of interest and ignore the location of theobject in the camera's field of view. We note that ignoring this locationinformation further exaggerates the inherent ambiguity in making 3D inferencesfrom 2D images and can prevent models from even fitting to the training data.To mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding(KPE), which incorporates information about the location of crops in the imageand camera intrinsics. Experiments on three popular 3D-from-a-single-imagebenchmarks: depth prediction on NYU, 3D object detection on KITTI &amp; nuScenes,and predicting 3D shapes of articulated objects on ARCTIC, show the benefits ofKPE.</description><author>Aditya Prakash, Arjun Gupta, Saurabh Gupta</author><pubDate>Mon, 11 Dec 2023 18:28:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06594v1</guid></item><item><title>Flexible visual prompts for in-context learning in computer vision</title><link>http://arxiv.org/abs/2312.06592v1</link><description>In this work, we address in-context learning (ICL) for the task of imagesegmentation, introducing a novel approach that adapts a modern Video ObjectSegmentation (VOS) technique for visual in-context learning. This adaptation isinspired by the VOS method's ability to efficiently and flexibly learn objectsfrom a few examples. Through evaluations across a range of support set sizesand on diverse segmentation datasets, our method consistently surpassesexisting techniques. Notably, it excels with data containing classes notencountered during training. Additionally, we propose a technique for supportset selection, which involves choosing the most relevant images to include inthis set. By employing support set selection, the performance increases for alltested methods without the need for additional training or prompt tuning. Thecode can be found at https://github.com/v7labs/XMem_ICL/.</description><author>Thomas Foster, Ioana Croitoru, Robert Dorfman, Christoffer Edlund, Thomas Varsavsky, Jon Almaz√°n</author><pubDate>Mon, 11 Dec 2023 18:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06592v1</guid></item><item><title>Concurrent Density Estimation with Wasserstein Autoencoders: Some Statistical Insights</title><link>http://arxiv.org/abs/2312.06591v1</link><description>Variational Autoencoders (VAEs) have been a pioneering force in the realm ofdeep generative models. Amongst its legions of progenies, WassersteinAutoencoders (WAEs) stand out in particular due to the dual offering ofheightened generative quality and a strong theoretical backbone. WAEs consistof an encoding and a decoding network forming a bottleneck with the primeobjective of generating new samples resembling the ones it was catered to. Inthe process, they aim to achieve a target latent representation of the encodeddata. Our work is an attempt to offer a theoretical understanding of themachinery behind WAEs. From a statistical viewpoint, we pose the problem asconcurrent density estimation tasks based on neural network-inducedtransformations. This allows us to establish deterministic upper bounds on therealized errors WAEs commit. We also analyze the propagation of thesestochastic errors in the presence of adversaries. As a result, both the largesample properties of the reconstructed distribution and the resilience of WAEmodels are explored.</description><author>Anish Chakrabarty, Arkaprabha Basu, Swagatam Das</author><pubDate>Mon, 11 Dec 2023 18:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06591v1</guid></item><item><title>Unsupervised Change Detection for Space Habitats Using 3D Point Clouds</title><link>http://arxiv.org/abs/2312.02396v2</link><description>This work presents an algorithm for scene change detection from point cloudsto enable autonomous robotic caretaking in future space habitats. Autonomousrobotic systems will help maintain future deep-space habitats, such as theGateway space station, which will be uncrewed for extended periods. Existingscene analysis software used on the International Space Station (ISS) relies onmanually-labeled images for detecting changes. In contrast, the algorithmpresented in this work uses raw, unlabeled point clouds as inputs. Thealgorithm first applies modified Expectation-Maximization Gaussian MixtureModel (GMM) clustering to two input point clouds. It then performs changedetection by comparing the GMMs using the Earth Mover's Distance. The algorithmis validated quantitatively and qualitatively using a test dataset collected byan Astrobee robot in the NASA Ames Granite Lab comprising single frame depthimages taken directly by Astrobee and full-scene reconstructed maps built withRGB-D and pose data from Astrobee. The runtimes of the approach are alsoanalyzed in depth. The source code is publicly released to promote furtherdevelopment.</description><author>Jamie Santos, Holly Dinkel, Julia Di, Paulo V. K. Borges, Marina Moreira, Oleg Alexandrov, Brian Coltin, Trey Smith</author><pubDate>Mon, 11 Dec 2023 18:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02396v2</guid></item><item><title>QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</title><link>http://arxiv.org/abs/2312.06587v1</link><description>Quick and automated earthquake-damaged building detection from post-eventsatellite imagery is crucial, yet it is challenging due to the scarcity oftraining data required to develop robust algorithms. This letter presents thefirst dataset dedicated to detecting earthquake-damaged buildings frompost-event very high resolution (VHR) Synthetic Aperture Radar (SAR) andoptical imagery. Utilizing open satellite imagery and annotations acquiredafter the 2023 Turkey-Syria earthquakes, we deliver a dataset of coregisteredbuilding footprints and satellite image patches of both SAR and optical data,encompassing more than four thousand buildings. The task of damaged buildingdetection is formulated as a binary image classification problem, that can alsobe treated as an anomaly detection problem due to extreme class imbalance. Weprovide baseline methods and results to serve as references for comparison.Researchers can utilize this dataset to expedite algorithm development,facilitating the rapid detection of damaged buildings in response to futureevents. The dataset and codes together with detailed explanations are madepublicly available at\url{https://github.com/ya0-sun/PostEQ-SARopt-BuildingDamage}.</description><author>Yao Sun, Yi Wang, Michael Eineder</author><pubDate>Mon, 11 Dec 2023 18:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06587v1</guid></item><item><title>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</title><link>http://arxiv.org/abs/2312.06585v1</link><description>Fine-tuning language models~(LMs) on human-generated data remains a prevalentpractice. However, the performance of such models is often limited by thequantity and diversity of high-quality human data. In this paper, we explorewhether we can go beyond human data on tasks where we have access to scalarfeedback, for example, on math problems where one can verify correctness. To doso, we investigate a simple self-training method based onexpectation-maximization, which we call ReST$^{EM}$, where we (1) generatesamples from the model and filter them using binary feedback, (2) fine-tune themodel on these samples, and (3) repeat this process a few times. Testing onadvanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we findthat ReST$^{EM}$ scales favorably with model size and significantly surpassesfine-tuning only on human data. Overall, our findings suggest self-trainingwith feedback can substantially reduce dependence on human-generated data.</description><author>Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, Noah Fiedel</author><pubDate>Mon, 11 Dec 2023 18:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06585v1</guid></item><item><title>3D Hand Pose Estimation in Egocentric Images in the Wild</title><link>http://arxiv.org/abs/2312.06583v1</link><description>We present WildHands, a method for 3D hand pose estimation in egocentricimages in the wild. This is challenging due to (a) lack of 3D hand poseannotations for images in the wild, and (b) a form of perspectivedistortion-induced shape ambiguity that arises in the analysis of crops aroundhands. For the former, we use auxiliary supervision on in-the-wild data in theform of segmentation masks &amp; grasp labels in addition to 3D supervisionavailable in lab datasets. For the latter, we provide spatial cues about thelocation of the hand crop in the camera's field of view. Our approach achievesthe best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, apopular and robust approach for estimating hand pose in the wild, by 45.3% whenevaluated on 2D hand pose on our EPIC-HandKps dataset.</description><author>Aditya Prakash, Ruisen Tu, Matthew Chang, Saurabh Gupta</author><pubDate>Mon, 11 Dec 2023 18:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06583v1</guid></item><item><title>Grokking Group Multiplication with Cosets</title><link>http://arxiv.org/abs/2312.06581v1</link><description>We use the group Fourier transform over the symmetric group $S_n$ to reverseengineer a 1-layer feedforward network that has "grokked" the multiplication of$S_5$ and $S_6$. Each model discovers the true subgroup structure of the fullgroup and converges on circuits that decompose the group multiplication intothe multiplication of the group's conjugate subgroups. We demonstrate the valueof using the symmetries of the data and models to understand their mechanismsand hold up the ``coset circuit'' that the model uses as a fascinating exampleof the way neural networks implement computations. We also draw attention tocurrent challenges in conducting mechanistic interpretability research bycomparing our work to Chughtai et al. [6] which alleges to find a differentalgorithm for this same problem.</description><author>Dashiell Stander, Qinan Yu, Honglu Fan, Stella Biderman</author><pubDate>Mon, 11 Dec 2023 18:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06581v1</guid></item><item><title>A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges</title><link>http://arxiv.org/abs/2311.05112v2</link><description>Large language models (LLMs), such as ChatGPT, have received substantialattention due to their impressive human language understanding and generationcapabilities. Therefore, the application of LLMs in medicine to assistphysicians and patient care emerges as a promising research direction in bothartificial intelligence and clinical medicine. To reflect this trend, thissurvey provides a comprehensive overview of the principles, applications, andchallenges faced by LLMs in medicine. Specifically, we aim to address thefollowing questions: 1) How can medical LLMs be built? 2) What are thedownstream performances of medical LLMs? 3) How can medical LLMs be utilized inreal-world clinical practice? 4) What challenges arise from the use of medicalLLMs? and 5) How can we better construct and utilize medical LLMs? As a result,this survey aims to provide insights into the opportunities and challenges ofLLMs in medicine and serve as a valuable resource for constructing practicaland effective medical LLMs. A regularly updated list of practical guides onmedical LLMs can be found athttps://github.com/AI-in-Health/MedLLMsPracticalGuide.</description><author>Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, David A. Clifton</author><pubDate>Mon, 11 Dec 2023 18:10:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05112v2</guid></item><item><title>Amazon Locker Capacity Management</title><link>http://arxiv.org/abs/2312.06579v1</link><description>Amazon Locker is a self-service delivery or pickup location where customerscan pick up packages and drop off returns. A basic first-come-first-servedpolicy for accepting package delivery requests to lockers results in lockersbecoming full with standard shipping speed (3-5 day shipping) packages, andleaving no space left for expedited packages which are mostly Next-Day orTwo-Day shipping. This paper proposes a solution to the problem of determininghow much locker capacity to reserve for different ship-option packages. Yieldmanagement is a much researched field with popular applications in the airline,car rental, and hotel industries. However, Amazon Locker poses a uniquechallenge in this field since the number of days a package will wait in alocker (package dwell time) is, in general, unknown. The proposed solutioncombines machine learning techniques to predict locker demand and package dwelltime, and linear programming to maximize throughput in lockers. The decisionvariables from this optimization provide optimal capacity reservation valuesfor different ship options. This resulted in a year-over-year increase of 9% inLocker throughput worldwide during holiday season of 2018, impacting millionsof customers.</description><author>Samyukta Sethuraman, Ankur Bansal, Setareh Mardan, Mauricio G. C. Resende, Timothy L. Jacobs</author><pubDate>Mon, 11 Dec 2023 18:10:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06579v1</guid></item><item><title>Multi-class Support Vector Machine with Maximizing Minimum Margin</title><link>http://arxiv.org/abs/2312.06578v1</link><description>Support Vector Machine (SVM) stands out as a prominent machine learningtechnique widely applied in practical pattern recognition tasks. It achievesbinary classification by maximizing the "margin", which represents the minimumdistance between instances and the decision boundary. Although many effortshave been dedicated to expanding SVM for multi-class case through strategiessuch as one versus one and one versus the rest, satisfactory solutions remainto be developed. In this paper, we propose a novel method for multi-class SVMthat incorporates pairwise class loss considerations and maximizes the minimummargin. Adhering to this concept, we embrace a new formulation that impartsheightened flexibility to multi-class SVM. Furthermore, the correlationsbetween the proposed method and multiple forms of multi-class SVM are analyzed.The proposed regularizer, akin to the concept of "margin", can serve as aseamless enhancement over the softmax in deep learning, providing guidance fornetwork parameter learning. Empirical evaluations demonstrate the effectivenessand superiority of our proposed method over existing multi-classificationmethods.Code is available at https://github.com/zz-haooo/M3SVM.</description><author>Feiping Nie, Zhezheng Hao, Rong Wang</author><pubDate>Mon, 11 Dec 2023 18:09:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06578v1</guid></item><item><title>Cheap Talking Algorithms</title><link>http://arxiv.org/abs/2310.07867v3</link><description>We simulate behaviour of independent reinforcement learning algorithmsplaying the Crawford and Sobel (1982) game of strategic informationtransmission. We show that a sender and a receiver training together convergeto strategies approximating the ex-ante optimal equilibrium of the game.Communication occurs to the largest extent predicted by Nash equilibrium. Theconclusion is robust to alternative specifications of the learninghyperparameters and of the game. We discuss implications for theories ofequilibrium selection in information transmission games, for work on emergingcommunication among algorithms in computer science, and for the economics ofcollusions in markets populated by artificially intelligent agents.</description><author>Daniele Condorelli, Massimiliano Furlan</author><pubDate>Mon, 11 Dec 2023 18:08:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07867v3</guid></item><item><title>Individual Fairness under Uncertainty</title><link>http://arxiv.org/abs/2302.08015v2</link><description>Algorithmic fairness, the research field of making machine learning (ML)algorithms fair, is an established area in ML. As ML technologies expand theirapplication domains, including ones with high societal impact, it becomesessential to take fairness into consideration during the building of MLsystems. Yet, despite its wide range of socially sensitive applications, mostwork treats the issue of algorithmic bias as an intrinsic property ofsupervised learning, i.e., the class label is given as a precondition. Unlikeprior studies in fairness, we propose an individual fairness measure and acorresponding algorithm that deal with the challenges of uncertainty arisingfrom censorship in class labels, while enforcing similar individuals to betreated similarly from a ranking perspective, free of the Lipschitz conditionin the conventional individual fairness definition. We argue that thisperspective represents a more realistic model of fairness research forreal-world application deployment and show how learning with such a relaxedprecondition draws new insights that better explains algorithmic fairness. Weconducted experiments on four real-world datasets to evaluate our proposedmethod compared to other fairness models, demonstrating its superiority inminimizing discrimination while maintaining predictive performance withuncertainty present.</description><author>Wenbin Zhang, Zichong Wang, Juyong Kim, Cheng Cheng, Thomas Oommen, Pradeep Ravikumar, Jeremy Weiss</author><pubDate>Mon, 11 Dec 2023 18:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08015v2</guid></item><item><title>BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials</title><link>http://arxiv.org/abs/2309.08788v2</link><description>The study of biological materials and bio-inspired materials science is wellestablished; however, surprisingly little knowledge has been systematicallytranslated to engineering solutions. To accelerate discovery and guideinsights, an open-source autoregressive transformer large language model (LLM),BioinspiredLLM, is reported. The model was finetuned with a corpus of over athousand peer-reviewed articles in the field of structural biological andbio-inspired materials and can be prompted to recall information, assist withresearch tasks, and function as an engine for creativity. The model has proventhat it is able to accurately recall information about biological materials andis further enhanced with enhanced reasoning ability, as well as withretrieval-augmented generation to incorporate new data during generation thatcan also help to traceback sources, update the knowledge base, and connectknowledge domains. BioinspiredLLM also has been shown to develop soundhypotheses regarding biological materials design and remarkably so formaterials that have never been explicitly studied before. Lastly, the modelshowed impressive promise in collaborating with other generative artificialintelligence models in a workflow that can reshape the traditional materialsdesign process. This collaborative generative artificial intelligence methodcan stimulate and enhance bio-inspired materials design workflows. Biologicalmaterials are at a critical intersection of multiple scientific fields andmodels like BioinspiredLLM help to connect knowledge domains.</description><author>Rachel K. Luu, Markus J. Buehler</author><pubDate>Mon, 11 Dec 2023 18:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08788v2</guid></item><item><title>Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity</title><link>http://arxiv.org/abs/2310.19614v2</link><description>How neuronal circuits achieve credit assignment remains a central unsolvedquestion in systems neuroscience. Various studies have suggested plausiblesolutions for back-propagating error signals through multi-layer networks.These purely functionally motivated models assume distinct neuronalcompartments to represent local error signals that determine the sign ofsynaptic plasticity. However, this explicit error modulation is inconsistentwith phenomenological plasticity models in which the sign depends primarily onpostsynaptic activity. Here we show how a plausible microcircuit model andHebbian learning rule derived within an adaptive control theory framework canresolve this discrepancy. Assuming errors are encoded in top-downdis-inhibitory synaptic afferents, we show that error-modulated learningemerges naturally at the circuit level when recurrent inhibition explicitlyinfluences Hebbian plasticity. The same learning rule accounts forexperimentally observed plasticity in the absence of inhibition and performscomparably to back-propagation of error (BP) on several non-linearly separablebenchmarks. Our findings bridge the gap between functional and experimentallyobserved plasticity rules and make concrete predictions on inhibitorymodulation of excitatory plasticity.</description><author>Julian Rossbroich, Friedemann Zenke</author><pubDate>Mon, 11 Dec 2023 18:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19614v2</guid></item><item><title>Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization</title><link>http://arxiv.org/abs/2306.09803v3</link><description>This paper introduces a modular framework for Mixed-variable andCombinatorial Bayesian Optimization (MCBO) to address the lack of systematicbenchmarking and standardized evaluation in the field. Current MCBO papersoften introduce non-diverse or non-standard benchmarks to evaluate theirmethods, impeding the proper assessment of different MCBO primitives and theircombinations. Additionally, papers introducing a solution for a single MCBOprimitive often omit benchmarking against baselines that utilize the samemethods for the remaining primitives. This omission is primarily due to thesignificant implementation overhead involved, resulting in a lack of controlledassessments and an inability to showcase the merits of a contributioneffectively. To overcome these challenges, our proposed framework enables aneffortless combination of Bayesian Optimization components, and provides adiverse set of synthetic and real-world benchmarking tasks. Leveraging thisflexibility, we implement 47 novel MCBO algorithms and benchmark them againstseven existing MCBO solvers and five standard black-box optimization algorithmson ten tasks, conducting over 4000 experiments. Our findings reveal a superiorcombination of MCBO primitives outperforming existing approaches and illustratethe significance of model fit and the use of a trust region. We make our MCBOlibrary available under the MIT license at\url{https://github.com/huawei-noah/HEBO/tree/master/MCBO}.</description><author>Kamil Dreczkowski, Antoine Grosnit, Haitham Bou Ammar</author><pubDate>Mon, 11 Dec 2023 18:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09803v3</guid></item><item><title>HyPE-GT: where Graph Transformers meet Hyperbolic Positional Encodings</title><link>http://arxiv.org/abs/2312.06576v1</link><description>Graph Transformers (GTs) facilitate the comprehension of graph-structureddata by calculating the self-attention of node pairs without considering nodeposition information. To address this limitation, we introduce an innovativeand efficient framework that introduces Positional Encodings (PEs) into theTransformer, generating a set of learnable positional encodings in thehyperbolic space, a non-Euclidean domain. This approach empowers us to explorediverse options for optimal selection of PEs for specific downstream tasks,leveraging hyperbolic neural networks or hyperbolic graph convolutionalnetworks. Additionally, we repurpose these positional encodings to mitigate theimpact of over-smoothing in deep Graph Neural Networks (GNNs). Comprehensiveexperiments on molecular benchmark datasets, co-author, and co-purchasenetworks substantiate the effectiveness of hyperbolic positional encodings inenhancing the performance of deep GNNs.</description><author>Kushal Bose, Swagatam Das</author><pubDate>Mon, 11 Dec 2023 18:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06576v1</guid></item><item><title>EasyVolcap: Accelerating Neural Volumetric Video Research</title><link>http://arxiv.org/abs/2312.06575v1</link><description>Volumetric video is a technology that digitally records dynamic events suchas artistic performances, sporting events, and remote conversations. Whenacquired, such volumography can be viewed from any viewpoint and timestamp onflat screens, 3D displays, or VR headsets, enabling immersive viewingexperiences and more flexible content creation in a variety of applicationssuch as sports broadcasting, video conferencing, gaming, and movie productions.With the recent advances and fast-growing interest in neural scenerepresentations for volumetric video, there is an urgent need for a unifiedopen-source library to streamline the process of volumetric video capturing,reconstruction, and rendering for both researchers and non-professional usersto develop various algorithms and applications of this emerging technology. Inthis paper, we present EasyVolcap, a Python &amp; Pytorch library for acceleratingneural volumetric video research with the goal of unifying the process ofmulti-view data processing, 4D scene reconstruction, and efficient dynamicvolumetric video rendering. Our source code is available athttps://github.com/zju3dv/EasyVolcap.</description><author>Zhen Xu, Tao Xie, Sida Peng, Haotong Lin, Qing Shuai, Zhiyuan Yu, Guangzhao He, Jiaming Sun, Hujun Bao, Xiaowei Zhou</author><pubDate>Mon, 11 Dec 2023 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06575v1</guid></item><item><title>A sampling criterion for constrained Bayesian optimization with uncertainties</title><link>http://arxiv.org/abs/2103.05706v4</link><description>We consider the problem of chance constrained optimization where it is soughtto optimize a function and satisfy constraints, both of which are affected byuncertainties. The real world declinations of this problem are particularlychallenging because of their inherent computational cost. To tackle such problems, we propose a new Bayesian optimization method. Itapplies to the situation where the uncertainty comes from some of the inputs,so that it becomes possible to define an acquisition criterion in the jointcontrolled-uncontrolled input space. The main contribution of this work is anacquisition criterion that accounts for both the average improvement inobjective function and the constraint reliability. The criterion is derivedfollowing the Stepwise Uncertainty Reduction logic and its maximizationprovides both optimal controlled and uncontrolled parameters. Analyticalexpressions are given to efficiently calculate the criterion. Numerical studieson test functions are presented. It is found through experimental comparisonswith alternative sampling criteria that the adequation between the samplingcriterion and the problem contributes to the efficiency of the overalloptimization. As a side result, an expression for the variance of theimprovement is given.</description><author>Reda El Amri, Rodolphe Le Riche, C√©line Helbert, Christophette Blanchet-Scalliet, S√©bastien Da Veiga</author><pubDate>Mon, 11 Dec 2023 17:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.05706v4</guid></item><item><title>ControlNet-XS: Designing an Efficient and Effective Architecture for Controlling Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2312.06573v1</link><description>The field of image synthesis has made tremendous strides forward in the lastyears. Besides defining the desired output image with text-prompts, anintuitive approach is to additionally use spatial guidance in form of an image,such as a depth map. For this, a recent and highly popular approach is to use acontrolling network, such as ControlNet, in combination with a pre-trainedimage generation model, such as Stable Diffusion. When evaluating the design ofexisting controlling networks, we observe that they all suffer from the sameproblem of a delay in information flowing between the generation andcontrolling process. This, in turn, means that the controlling network musthave generative capabilities. In this work we propose a new controllingarchitecture, called ControlNet-XS, which does not suffer from this problem,and hence can focus on the given task of learning to control. In contrast toControlNet, our model needs only a fraction of parameters, and hence is abouttwice as fast during inference and training time. Furthermore, the generatedimages are of higher quality and the control is of higher fidelity. All codeand pre-trained models will be made publicly available.</description><author>Denis Zavadski, Johann-Friedrich Feiden, Carsten Rother</author><pubDate>Mon, 11 Dec 2023 17:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06573v1</guid></item><item><title>Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets</title><link>http://arxiv.org/abs/2312.06568v1</link><description>Graph Lottery Tickets (GLTs), comprising a sparse adjacency matrix and asparse graph neural network (GNN), can significantly reduce the inferencelatency and compute footprint compared to their dense counterparts. Despitethese benefits, their performance against adversarial structure perturbationsremains to be fully explored. In this work, we first investigate the resilienceof GLTs against different structure perturbation attacks and observe that theyare highly vulnerable and show a large drop in classification accuracy. Basedon this observation, we then present an adversarially robust graphsparsification (ARGS) framework that prunes the adjacency matrix and the GNNweights by optimizing a novel loss function capturing the graph homophilyproperty and information associated with both the true labels of the trainnodes and the pseudo labels of the test nodes. By iteratively applying ARGS toprune both the perturbed graph adjacency matrix and the GNN model weights, wecan find adversarially robust graph lottery tickets that are highly sparse yetachieve competitive performance under different untargeted training-timestructure attacks. Evaluations conducted on various benchmarks, consideringdifferent poisoning structure attacks, namely, PGD, MetaAttack, Meta-PGD, andPR-BCD demonstrate that the GLTs generated by ARGS can significantly improvethe robustness, even when subjected to high levels of sparsity.</description><author>Subhajit Dutta Chowdhury, Zhiyu Ni, Qingyuan Peng, Souvik Kundu, Pierluigi Nuzzo</author><pubDate>Mon, 11 Dec 2023 17:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06568v1</guid></item><item><title>Supply-Side Equilibria in Recommender Systems</title><link>http://arxiv.org/abs/2206.13489v3</link><description>Algorithmic recommender systems such as Spotify and Netflix affect not onlyconsumer behavior but also producer incentives. Producers seek to createcontent that will be shown by the recommendation algorithm, which can impactboth the diversity and quality of their content. In this work, we investigatethe resulting supply-side equilibria in personalized content recommendersystems. We model users and content as $D$-dimensional vectors, therecommendation algorithm as showing each user the content with highest dotproduct, and producers as maximizing the number of users who are recommendedtheir content minus the cost of production. Two key features of our model arethat the producer decision space is multi-dimensional and the user base isheterogeneous, which contrasts with classical low-dimensional models. Multi-dimensionality and heterogeneity create the potential forspecialization, where different producers create different types of content atequilibrium. Using a duality argument, we derive necessary and sufficientconditions for whether specialization occurs: these conditions depend on theextent to which users are heterogeneous and to which producers can perform wellon all dimensions at once without incurring a high cost. Then, we characterizethe distribution of content at equilibrium in concrete settings with twopopulations of users. Lastly, we show that specialization can enable producersto achieve positive profit at equilibrium, which means that specialization canreduce the competitiveness of the marketplace. At a conceptual level, ouranalysis of supply-side competition takes a step towards elucidating howpersonalized recommendations shape the marketplace of digital goods, andtowards understanding what new phenomena arise in multi-dimensional competitivesettings.</description><author>Meena Jagadeesan, Nikhil Garg, Jacob Steinhardt</author><pubDate>Mon, 11 Dec 2023 17:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.13489v3</guid></item><item><title>Promoting Counterfactual Robustness through Diversity</title><link>http://arxiv.org/abs/2312.06564v1</link><description>Counterfactual explanations shed light on the decisions of black-box modelsby explaining how an input can be altered to obtain a favourable decision fromthe model (e.g., when a loan application has been rejected). However, as notedrecently, counterfactual explainers may lack robustness in the sense that aminor change in the input can cause a major change in the explanation. This cancause confusion on the user side and open the door for adversarial attacks. Inthis paper, we study some sources of non-robustness. While there arefundamental reasons for why an explainer that returns a single counterfactualcannot be robust in all instances, we show that some interesting robustnessguarantees can be given by reporting multiple rather than a singlecounterfactual. Unfortunately, the number of counterfactuals that need to bereported for the theoretical guarantees to hold can be prohibitively large. Wetherefore propose an approximation algorithm that uses a diversity criterion toselect a feasible number of most relevant explanations and study its robustnessempirically. Our experiments indicate that our method improves thestate-of-the-art in generating robust explanations, while maintaining otherdesirable properties and providing competitive computational performance.</description><author>Francesco Leofante, Nico Potyka</author><pubDate>Mon, 11 Dec 2023 17:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06564v1</guid></item><item><title>Improving Computational Efficiency for Powered Descent Guidance via Transformer-based Tight Constraint Prediction</title><link>http://arxiv.org/abs/2311.05135v2</link><description>In this work, we present Transformer-based Powered Descent Guidance (T-PDG),a scalable algorithm for reducing the computational complexity of the directoptimization formulation of the spacecraft powered descent guidance problem.T-PDG uses data from prior runs of trajectory optimization algorithms to traina transformer neural network, which accurately predicts the relationshipbetween problem parameters and the globally optimal solution for the powereddescent guidance problem. The solution is encoded as the set of tightconstraints corresponding to the constrained minimum-cost trajectory and theoptimal final time of landing. By leveraging the attention mechanism oftransformer neural networks, large sequences of time series data can beaccurately predicted when given only the spacecraft state and landing siteparameters. When applied to the real problem of Mars powered descent guidance,T-PDG reduces the time for computing the 3 degree of freedom fuel-optimaltrajectory, when compared to lossless convexification, from an order of 1-8seconds to less than 500 milliseconds. A safe and optimal solution isguaranteed by including a feasibility check in T-PDG before returning the finaltrajectory.</description><author>Julia Briden, Trey Gurga, Breanna Johnson, Abhishek Cauligi, Richard Linares</author><pubDate>Mon, 11 Dec 2023 17:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05135v2</guid></item><item><title>On Meta-Prompting</title><link>http://arxiv.org/abs/2312.06562v1</link><description>Certain statistical models are capable of interpreting input strings asinstructions, or prompts, and carry out tasks based on them. Many approaches toprompting and pre-training these models involve the automated generation ofthese prompts. We call these approaches meta-prompting, or prompting to obtainprompts. We propose a theoretical framework based on category theory togeneralize and describe them. This framework is flexible enough to account forLLM stochasticity; and allows us to obtain formal results around taskagnosticity and equivalence of various meta-prompting approaches. We experimentwith meta-prompting in two active areas of model research: creativity andideation. We find that user preference favors (p &lt; 0.01) the prompts generatedunder meta-prompting, as well as their corresponding outputs, over a series ofhardcoded baseline prompts that include the original task prompt. Using ourframework, we argue that meta-prompting is more effective than basic promptingat generating desirable outputs.</description><author>Adrian de Wynter, Xun Wang, Qilong Gu, Si-Qing Chen</author><pubDate>Mon, 11 Dec 2023 17:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06562v1</guid></item><item><title>Inferring Hybrid Neural Fluid Fields from Videos</title><link>http://arxiv.org/abs/2312.06561v1</link><description>We study recovering fluid density and velocity from sparse multiview videos.Existing neural dynamic reconstruction methods predominantly rely on opticalflows; therefore, they cannot accurately estimate the density and uncover theunderlying velocity due to the inherent visual ambiguities of fluid velocity,as fluids are often shapeless and lack stable visual features. The challenge isfurther pronounced by the turbulent nature of fluid flows, which calls forproperly designed fluid velocity representations. To address these challenges,we propose hybrid neural fluid fields (HyFluid), a neural approach to jointlyinfer fluid density and velocity fields. Specifically, to deal with visualambiguities of fluid velocity, we introduce a set of physics-based losses thatenforce inferring a physically plausible velocity field, which isdivergence-free and drives the transport of density. To deal with the turbulentnature of fluid velocity, we design a hybrid neural velocity representationthat includes a base neural velocity field that captures most irrotationalenergy and a vortex particle-based velocity that models residual turbulentvelocity. We show that our method enables recovering vortical flow details. Ourapproach opens up possibilities for various learning and reconstructionapplications centered around 3D incompressible flow, including fluidre-simulation and editing, future prediction, and neural dynamic scenecomposition. Project website: https://kovenyu.com/HyFluid/</description><author>Hong-Xing Yu, Yang Zheng, Yuan Gao, Yitong Deng, Bo Zhu, Jiajun Wu</author><pubDate>Mon, 11 Dec 2023 17:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06561v1</guid></item><item><title>Automatic Regularization for Linear MMSE Filters</title><link>http://arxiv.org/abs/2312.06560v1</link><description>In this work, we consider the problem of regularization in minimummean-squared error (MMSE) linear filters. Exploiting the relationship withstatistical machine learning methods, the regularization parameter is foundfrom the observed signals in a simple and automatic manner. The proposedapproach is illustrated through system identification examples, where theautomatic regularization yields near-optimal results.</description><author>Daniel Gomes de Pinho Zanco, Leszek Szczecinski, Jacob Benesty</author><pubDate>Mon, 11 Dec 2023 17:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06560v1</guid></item><item><title>Deep Photonic Reservoir Computer for Speech Recognition</title><link>http://arxiv.org/abs/2312.06558v1</link><description>Speech recognition is a critical task in the field of artificial intelligenceand has witnessed remarkable advancements thanks to large and complex neuralnetworks, whose training process typically requires massive amounts of labeleddata and computationally intensive operations. An alternative paradigm,reservoir computing, is energy efficient and is well adapted to implementationin physical substrates, but exhibits limitations in performance when comparedto more resource-intensive machine learning algorithms. In this work we addressthis challenge by investigating different architectures of interconnectedreservoirs, all falling under the umbrella of deep reservoir computing. Wepropose a photonic-based deep reservoir computer and evaluate its effectivenesson different speech recognition tasks. We show specific design choices that aimto simplify the practical implementation of a reservoir computer whilesimultaneously achieving high-speed processing of high-dimensional audiosignals. Overall, with the present work we hope to help the advancement oflow-power and high-performance neuromorphic hardware.</description><author>Enrico Picco, Alessandro Lupo, Serge Massar</author><pubDate>Mon, 11 Dec 2023 17:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06558v1</guid></item><item><title>Robust Graph Neural Network based on Graph Denoising</title><link>http://arxiv.org/abs/2312.06557v1</link><description>Graph Neural Networks (GNNs) have emerged as a notorious alternative toaddress learning problems dealing with non-Euclidean datasets. However,although most works assume that the graph is perfectly known, the observedtopology is prone to errors stemming from observational noise, graph-learninglimitations, or adversarial attacks. If ignored, these perturbations maydrastically hinder the performance of GNNs. To address this limitation, thiswork proposes a robust implementation of GNNs that explicitly accounts for thepresence of perturbations in the observed topology. For any task involvingGNNs, our core idea is to i) solve an optimization problem not only over thelearnable parameters of the GNN but also over the true graph, and ii) augmentthe fitting cost with a term accounting for discrepancies on the graph.Specifically, we consider a convolutional GNN based on graph filters and followan alternating optimization approach to handle the (non-differentiable andconstrained) optimization problem by combining gradient descent and projectedproximal updates. The resulting algorithm is not limited to a particular typeof graph and is amenable to incorporating prior information about theperturbations. Finally, we assess the performance of the proposed methodthrough several numerical experiments.</description><author>Victor M. Tenorio, Samuel Rey, Antonio G. Marques</author><pubDate>Mon, 11 Dec 2023 17:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06557v1</guid></item><item><title>HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models</title><link>http://arxiv.org/abs/2312.06553v1</link><description>We address the problem of generating realistic 3D human-object interactions(HOIs) driven by textual prompts. Instead of a single model, our key insight isto take a modular design and decompose the complex task into simpler sub-tasks.We first develop a dual-branch diffusion model (HOI-DM) to generate both humanand object motions conditioning on the input text, and encourage coherentmotions by a cross-attention communication module between the human and objectmotion generation branches. We also develop an affordance prediction diffusionmodel (APDM) to predict the contacting area between the human and object duringthe interactions driven by the textual prompt. The APDM is independent of theresults by the HOI-DM and thus can correct potential errors by the latter.Moreover, it stochastically generates the contacting points to diversify thegenerated motions. Finally, we incorporate the estimated contacting points intothe classifier-guidance to achieve accurate and close contact between humansand objects. To train and evaluate our approach, we annotate BEHAVE datasetwith text descriptions. Experimental results demonstrate that our approach isable to produce realistic HOIs with various interactions and different types ofobjects.</description><author>Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, Huaizu Jiang</author><pubDate>Mon, 11 Dec 2023 17:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06553v1</guid></item><item><title>LLM360: Towards Fully Transparent Open-Source LLMs</title><link>http://arxiv.org/abs/2312.06550v1</link><description>The recent surge in open-source Large Language Models (LLMs), such as LLaMA,Falcon, and Mistral, provides diverse options for AI practitioners andresearchers. However, most LLMs have only released partial artifacts, such asthe final model weights or inference code, and technical reports increasinglylimit their scope to high-level design choices and surface statistics. Thesechoices hinder progress in the field by degrading transparency into thetraining of LLMs and forcing teams to rediscover many details in the trainingprocess. We present LLM360, an initiative to fully open-source LLMs, whichadvocates for all training code and data, model checkpoints, and intermediateresults to be made available to the community. The goal of LLM360 is to supportopen and collaborative AI research by making the end-to-end LLM trainingprocess transparent and reproducible by everyone. As a first step of LLM360, werelease two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder,including their training code, data, intermediate checkpoints, and analyses (athttps://www.llm360.ai). We are committed to continually pushing the boundariesof LLMs through this open-source effort. More large-scale and stronger modelsare underway and will be released in the future.</description><author>Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P. Xing</author><pubDate>Mon, 11 Dec 2023 17:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06550v1</guid></item><item><title>Temporal Dynamic Quantization for Diffusion Models</title><link>http://arxiv.org/abs/2306.02316v2</link><description>The diffusion model has gained popularity in vision applications due to itsremarkable generative performance and versatility. However, high storage andcomputation demands, resulting from the model size and iterative generation,hinder its use on mobile devices. Existing quantization techniques struggle tomaintain performance even in 8-bit precision due to the diffusion model'sunique property of temporal variation in activation. We introduce a novelquantization method that dynamically adjusts the quantization interval based ontime step information, significantly improving output quality. Unlikeconventional dynamic quantization techniques, our approach has no computationaloverhead during inference and is compatible with both post-trainingquantization (PTQ) and quantization-aware training (QAT). Our extensiveexperiments demonstrate substantial improvements in output quality with thequantized diffusion model across various datasets.</description><author>Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, Eunhyeok Park</author><pubDate>Mon, 11 Dec 2023 17:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02316v2</guid></item><item><title>Grad DFT: a software library for machine learning enhanced density functional theory</title><link>http://arxiv.org/abs/2309.15127v2</link><description>Density functional theory (DFT) stands as a cornerstone method incomputational quantum chemistry and materials science due to its remarkableversatility and scalability. Yet, it suffers from limitations in accuracy,particularly when dealing with strongly correlated systems. To address theseshortcomings, recent work has begun to explore how machine learning can expandthe capabilities of DFT; an endeavor with many open questions and technicalchallenges. In this work, we present Grad DFT: a fully differentiable JAX-basedDFT library, enabling quick prototyping and experimentation with machinelearning-enhanced exchange-correlation energy functionals. Grad DFT employs apioneering parametrization of exchange-correlation functionals constructedusing a weighted sum of energy densities, where the weights are determinedusing neural networks. Moreover, Grad DFT encompasses a comprehensive suite ofauxiliary functions, notably featuring a just-in-time compilable and fullydifferentiable self-consistent iterative procedure. To support training andbenchmarking efforts, we additionally compile a curated dataset of experimentaldissociation energies of dimers, half of which contain transition metal atomscharacterized by strong electronic correlations. The software library is testedagainst experimental results to study the generalization capabilities of aneural functional across potential energy surfaces and atomic species, as wellas the effect of training data noise on the resulting model accuracy.</description><author>Pablo A. M. Casares, Jack S. Baker, Matija Medvidovic, Roberto dos Reis, Juan Miguel Arrazola</author><pubDate>Mon, 11 Dec 2023 17:38:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15127v2</guid></item><item><title>KF-PLS: Optimizing Kernel Partial Least-Squares (K-PLS) with Kernel Flows</title><link>http://arxiv.org/abs/2312.06547v1</link><description>Partial Least-Squares (PLS) Regression is a widely used tool in chemometricsfor performing multivariate regression. PLS is a bi-linear method that has alimited capacity of modelling non-linear relations between the predictorvariables and the response. Kernel PLS (K-PLS) has been introduced formodelling non-linear predictor-response relations. In K-PLS, the input data ismapped via a kernel function to a Reproducing Kernel Hilbert space (RKH), wherethe dependencies between the response and the input matrix are assumed to belinear. K-PLS is performed in the RKH space between the kernel matrix and thedependent variable. Most available studies use fixed kernel parameters. Only afew studies have been conducted on optimizing the kernel parameters for K-PLS.In this article, we propose a methodology for the kernel function optimizationbased on Kernel Flows (KF), a technique developed for Gaussian processregression (GPR). The results are illustrated with four case studies. The casestudies represent both numerical examples and real data used in classificationand regression tasks. K-PLS optimized with KF, called KF-PLS in this study, isshown to yield good results in all illustrated scenarios. The paper presentscross-validation studies and hyperparameter analysis of the KF methodology whenapplied to K-PLS.</description><author>Zina-Sabrina Duma, Jouni Susiluoto, Otto Lamminp√§√§, Tuomas Sihvonen, Satu-Pia Reinikainen, Heikki Haario</author><pubDate>Mon, 11 Dec 2023 17:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06547v1</guid></item><item><title>Unsupervised KPIs-Based Clustering of Jobs in HPC Data Centers</title><link>http://arxiv.org/abs/2312.06546v1</link><description>Performance analysis is an essential task in High-Performance Computing (HPC)systems and it is applied for different purposes such as anomaly detection,optimal resource allocation, and budget planning. HPC monitoring tasks generatea huge number of Key Performance Indicators (KPIs) to supervise the status ofthe jobs running in these systems. KPIs give data about CPU usage, memoryusage, network (interface) traffic, or other sensors that monitor the hardware.Analyzing this data, it is possible to obtain insightful information aboutrunning jobs, such as their characteristics, performance, and failures. Themain contribution in this paper is to identify which metric/s (KPIs) is/are themost appropriate to identify/classify different types of jobs according totheir behavior in the HPC system. With this aim, we have applied differentclustering techniques (partition and hierarchical clustering algorithms) usinga real dataset from the Galician Computation Center (CESGA). We have concludedthat (i) those metrics (KPIs) related to the Network (interface) trafficmonitoring provide the best cohesion and separation to cluster HPC jobs, and(ii) hierarchical clustering algorithms are the most suitable for this task.Our approach was validated using a different real dataset from the same HPCcenter.</description><author>Mohamed S. Halawa, Rebeca P. D√≠az-Redondo, Ana Fern√°ndez-Vilas</author><pubDate>Mon, 11 Dec 2023 17:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06546v1</guid></item><item><title>KPIs-Based Clustering and Visualization of HPC jobs: a Feature Reduction Approach</title><link>http://arxiv.org/abs/2312.06534v1</link><description>High-Performance Computing (HPC) systems need to be constantly monitored toensure their stability. The monitoring systems collect a tremendous amount ofdata about different parameters or Key Performance Indicators (KPIs), such asresource usage, IO waiting time, etc. A proper analysis of this data, usuallystored as time series, can provide insight in choosing the right managementstrategies as well as the early detection of issues. In this paper, weintroduce a methodology to cluster HPC jobs according to their KPI indicators.Our approach reduces the inherent high dimensionality of the collected data byapplying two techniques to the time series: literature-based and variance-basedfeature extraction. We also define a procedure to visualize the obtainedclusters by combining the two previous approaches and the Principal ComponentAnalysis (PCA). Finally, we have validated our contributions on a real data setto conclude that those KPIs related to CPU usage provide the best cohesion andseparation for clustering analysis and the good results of our visualizationmethodology.</description><author>Mohamed Soliman Halawa, Rebeca P. D√≠az-Redondo, Ana Fern√°ndez-Vilas</author><pubDate>Mon, 11 Dec 2023 17:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06534v1</guid></item><item><title>"You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation</title><link>http://arxiv.org/abs/2310.17793v2</link><description>Large language models (LLMs) show amazing proficiency and fluency in the useof language. Does this mean that they have also acquired insightful linguisticknowledge about the language, to an extent that they can serve as an "expertlinguistic annotator"? In this paper, we examine the successes and limitationsof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaningstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu etal. 2013) parsing formalism, which provides rich graphical representations ofsentence meaning structure while abstracting away from surface forms. Wecompare models' analysis of this semantic structure across two settings: 1)direct production of AMR parses based on zero- and few-shot prompts, and 2)indirect partial reconstruction of AMR via metalinguistic natural languagequeries (e.g., "Identify the primary event of this sentence, and the predicatecorresponding to that event."). Across these settings, we find that models canreliably reproduce the basic format of AMR, and can often capture core event,argument, and modifier structure -- however, model outputs are prone tofrequent and major errors, and holistic analysis of parse acceptability showsthat even with few-shot demonstrations, models have virtually 0% success inproducing fully accurate parses. Eliciting natural language responses producessimilar patterns of errors. Overall, our findings indicate that these modelsout-of-the-box can capture aspects of semantic structure, but there remain keylimitations in their ability to support fully accurate semantic analyses orparses.</description><author>Allyson Ettinger, Jena D. Hwang, Valentina Pyatkin, Chandra Bhagavatula, Yejin Choi</author><pubDate>Mon, 11 Dec 2023 17:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17793v2</guid></item><item><title>Uncertainty quantification in automated valuation models with locally weighted conformal prediction</title><link>http://arxiv.org/abs/2312.06531v1</link><description>Non-parametric machine learning models, such as random forests and gradientboosted trees, are frequently used to estimate house prices due to theirpredictive accuracy, but such methods are often limited in their ability toquantify prediction uncertainty. Conformal Prediction (CP) is a model-agnosticframework for constructing confidence sets around machine learning predictionmodels with minimal assumptions. However, due to the spatial dependenciesobserved in house prices, direct application of CP leads to confidence setsthat are not calibrated everywhere, i.e., too large of confidence sets incertain geographical regions and too small in others. We survey variousapproaches to adjust the CP confidence set to account for this and demonstratetheir performance on a data set from the housing market in Oslo, Norway. Ourfindings indicate that calibrating the confidence sets on a \textit{locallyweighted} version of the non-conformity scores makes the coverage moreconsistently calibrated in different geographical regions. We also perform asimulation study on synthetically generated sale prices to empirically explorethe performance of CP on housing market data under idealized conditions withknown data-generating mechanisms.</description><author>Anders Hjort, Gudmund Horn Hermansen, Johan Pensar, Jonathan P. Williams</author><pubDate>Mon, 11 Dec 2023 17:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06531v1</guid></item><item><title>Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context</title><link>http://arxiv.org/abs/2312.06528v1</link><description>Many neural network architectures have been shown to be Turing Complete, andcan thus implement arbitrary algorithms. However, Transformers are unique inthat they can implement gradient-based learning algorithms \emph{under simpleparameter configurations}. A line of recent work shows that linear Transformersnaturally learn to implement gradient descent (GD) when trained on a linearregression in-context learning task. But the linearity assumption (either inthe Transformer architecture or in the learning task) is far from realisticsettings where non-linear activations crucially enable Transformers to learncomplicated non-linear functions. In this paper, we provide theoretical andempirical evidence that non-linear Transformers can, and \emph{in fact do},learn to implement learning algorithms to learn non-linear functions incontext. Our results apply to a broad class of combinations of non-lineararchitectures, and non-linear in-context learning tasks. Interestingly, we showthat the optimal choice of non-linear activation depends in a natural way onthe non-linearity of the learning task.</description><author>Xiang Cheng, Yuxin Chen, Suvrit Sra</author><pubDate>Mon, 11 Dec 2023 17:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06528v1</guid></item><item><title>Can Reinforcement Learning support policy makers? A preliminary study with Integrated Assessment Models</title><link>http://arxiv.org/abs/2312.06527v1</link><description>Governments around the world aspire to ground decision-making on evidence.Many of the foundations of policy making - e.g. sensing patterns that relate tosocietal needs, developing evidence-based programs, forecasting potentialoutcomes of policy changes, and monitoring effectiveness of policy programs -have the potential to benefit from the use of large-scale datasets orsimulations together with intelligent algorithms. These could, if designed anddeployed in a way that is well grounded on scientific evidence, enable a morecomprehensive, faster, and rigorous approach to policy making. IntegratedAssessment Models (IAM) is a broad umbrella covering scientific models thatattempt to link main features of society and economy with the biosphere intoone modelling framework. At present, these systems are probed by policy makersand advisory groups in a hypothesis-driven manner. In this paper, weempirically demonstrate that modern Reinforcement Learning can be used to probeIAMs and explore the space of solutions in a more principled manner. While theimplication of our results are modest since the environment is simplistic, webelieve that this is a stepping stone towards more ambitious use cases, whichcould allow for effective exploration of policies and understanding of theirconsequences and limitations.</description><author>Theodore Wolf, Nantas Nardelli, John Shawe-Taylor, Maria Perez-Ortiz</author><pubDate>Mon, 11 Dec 2023 17:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06527v1</guid></item><item><title>MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video</title><link>http://arxiv.org/abs/2303.10275v2</link><description>We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRFavatars are rendered in real-time on mobile devices, learned from monocularvideos, and have high realism. We use SMPL-X as a proxy geometry and render itwith DNR (neural texture and image-2-image network). We improve on prior work,by overfitting per-frame warping fields in the neural texture space, allowingto better align the training signal between different frames. We also refineSMPL-X mesh fitting procedure to improve the overall avatar quality. In thecomparisons to other monocular video-based avatar systems, MoRF avatars achievehigher image sharpness and temporal consistency. Participants of our user studyalso preferred avatars generated by MoRF.</description><author>Renat Bashirov, Alexey Larionov, Evgeniya Ustinova, Mikhail Sidorenko, David Svitov, Ilya Zakharkin, Victor Lempitsky</author><pubDate>Mon, 11 Dec 2023 17:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10275v2</guid></item><item><title>Label Smoothing for Enhanced Text Sentiment Classification</title><link>http://arxiv.org/abs/2312.06522v1</link><description>Label smoothing is a widely used technique in various domains, such as imageclassification and speech recognition, known for effectively combating modeloverfitting. However, there is few research on its application to textsentiment classification. To fill in the gap, this study investigates theimplementation of label smoothing for sentiment classification by utilizingdifferent levels of smoothing. The primary objective is to enhance sentimentclassification accuracy by transforming discrete labels into smoothed labeldistributions. Through extensive experiments, we demonstrate the superiorperformance of label smoothing in text sentiment classification tasks acrosseight diverse datasets and deep learning architectures: TextCNN, BERT, andRoBERTa, under two learning schemes: training from scratch and fine-tuning.</description><author>Yijie Gao, Shijing Si</author><pubDate>Mon, 11 Dec 2023 17:00:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06522v1</guid></item><item><title>Meta-Value Learning: a General Framework for Learning with Learning Awareness</title><link>http://arxiv.org/abs/2307.08863v3</link><description>Gradient-based learning in multi-agent systems is difficult because thegradient derives from a first-order model which does not account for theinteraction between agents' learning processes. LOLA (arXiv:1709.04326)accounts for this by differentiating through one step of optimization. Wepropose to judge joint policies by their long-term prospects as measured by themeta-value, a discounted sum over the returns of future optimization iterates.We apply a form of Q-learning to the meta-game of optimization, in a way thatavoids the need to explicitly represent the continuous action space of policyupdates. The resulting method, MeVa, is consistent and far-sighted, and doesnot require REINFORCE estimators. We analyze the behavior of our method on atoy game and compare to prior work on repeated matrix games.</description><author>Tim Cooijmans, Milad Aghajohari, Aaron Courville</author><pubDate>Mon, 11 Dec 2023 16:52:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08863v3</guid></item><item><title>A GAN Approach for Node Embedding in Heterogeneous Graphs Using Subgraph Sampling</title><link>http://arxiv.org/abs/2312.06519v1</link><description>Our research addresses class imbalance issues in heterogeneous graphs usinggraph neural networks (GNNs). We propose a novel method combining the strengthsof Generative Adversarial Networks (GANs) with GNNs, creating synthetic nodesand edges that effectively balance the dataset. This approach directly targetsand rectifies imbalances at the data level. The proposed framework resolvesissues such as neglecting graph structures during data generation and creatingsynthetic structures usable with GNN-based classifiers in downstream tasks. Itprocesses node and edge information concurrently, improving edge balancethrough node augmentation and subgraph sampling. Additionally, our frameworkintegrates a threshold strategy, aiding in determining optimal edge thresholdsduring training without time-consuming parameter adjustments. Experiments onthe Amazon and Yelp Review datasets highlight the effectiveness of theframework we proposed, especially in minority node identification, where itconsistently outperforms baseline models across key performance metrics,demonstrating its potential in the field.</description><author>Hung Chun Hsu, Bo-Jun Wu, Ming-Yi Hong, Che Lin, Chih-Yu Wang</author><pubDate>Mon, 11 Dec 2023 16:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06519v1</guid></item><item><title>Decoupling Meta-Reinforcement Learning with Gaussian Task Contexts and Skills</title><link>http://arxiv.org/abs/2312.06518v1</link><description>Offline meta-reinforcement learning (meta-RL) methods, which adapt to unseentarget tasks with prior experience, are essential in robot control tasks.Current methods typically utilize task contexts and skills as prior experience,where task contexts are related to the information within each task and skillsrepresent a set of temporally extended actions for solving subtasks. However,these methods still suffer from limited performance when adapting to unseentarget tasks, mainly because the learned prior experience lacks generalization,i.e., they are unable to extract effective prior experience from meta-trainingtasks by exploration and learning of continuous latent spaces. We propose aframework called decoupled meta-reinforcement learning (DCMRL), which (1)contrastively restricts the learning of task contexts through pulling insimilar task contexts within the same task and pushing away different taskcontexts of different tasks, and (2) utilizes a Gaussian quantizationvariational autoencoder (GQ-VAE) for clustering the Gaussian distributions ofthe task contexts and skills respectively, and decoupling the exploration andlearning processes of their spaces. These cluster centers which serve asrepresentative and discrete distributions of task context and skill are storedin task context codebook and skill codebook, respectively. DCMRL can acquiregeneralizable prior experience and achieve effective adaptation to unseentarget tasks during the meta-testing phase. Experiments in the navigation androbot manipulation continuous control tasks show that DCMRL is more effectivethan previous meta-RL methods with more generalizable prior experience.</description><author>Hongcai He, Anjie Zhu, Shuang Liang, Feiyu Chen, Jie Shao</author><pubDate>Mon, 11 Dec 2023 16:50:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06518v1</guid></item><item><title>Where exactly does contextualization in a PLM happen?</title><link>http://arxiv.org/abs/2312.06514v1</link><description>Pre-trained Language Models (PLMs) have shown to be consistently successfulin a plethora of NLP tasks due to their ability to learn contextualizedrepresentations of words (Ethayarajh, 2019). BERT (Devlin et al., 2018), ELMo(Peters et al., 2018) and other PLMs encode word meaning via textual context,as opposed to static word embeddings, which encode all meanings of a word in asingle vector representation. In this work, we present a study that aims tolocalize where exactly in a PLM word contextualization happens. In order tofind the location of this word meaning transformation, we investigaterepresentations of polysemous words in the basic BERT uncased 12 layerarchitecture (Devlin et al., 2018), a masked language model trained on anadditional sentence adjacency objective, using qualitative and quantitativemeasures.</description><author>Soniya Vijayakumar, Tanja B√§umel, Simon Ostermann, Josef van Genabith</author><pubDate>Mon, 11 Dec 2023 16:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06514v1</guid></item><item><title>Asynchronous Distributed Optimization with Delay-free Parameters</title><link>http://arxiv.org/abs/2312.06508v1</link><description>Existing asynchronous distributed optimization algorithms often usediminishing step-sizes that cause slow practical convergence, or use fixedstep-sizes that depend on and decrease with an upper bound of the delays. Notonly are such delay bounds hard to obtain in advance, but they also tend to belarge and rarely attained, resulting in unnecessarily slow convergence. Thispaper develops asynchronous versions of two distributed algorithms, Prox-DGDand DGD-ATC, for solving consensus optimization problems over undirectednetworks. In contrast to alternatives, our algorithms can converge to the fixedpoint set of their synchronous counterparts using step-sizes that areindependent of the delays. We establish convergence guarantees for strongly andweakly convex problems under both partial and total asynchrony. We also showthat the convergence speed of the two asynchronous methods adapts to the actuallevel of asynchrony rather than being constrained by the worst-case. Numericalexperiments demonstrate a strong practical performance of our asynchronousalgorithms.</description><author>Xuyang Wu, Changxin Liu, Sindri Magnusson, Mikael Johansson</author><pubDate>Mon, 11 Dec 2023 16:33:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06508v1</guid></item><item><title>Allocating Indivisible Goods to Strategic Agents: Pure Nash Equilibria and Fairness</title><link>http://arxiv.org/abs/2109.08644v2</link><description>We consider the problem of fairly allocating a set of indivisible goods to aset of strategic agents with additive valuation functions. We assume nomonetary transfers and, therefore, a mechanism in our setting is an algorithmthat takes as input the reported -- rather than the true -- values of theagents. Our main goal is to explore whether there exist mechanisms that havepure Nash equilibria for every instance and, at the same time, provide fairnessguarantees for the allocations that correspond to these equilibria. We focus ontwo relaxations of envy-freeness, namely envy-freeness up to one good (EF1),and envy-freeness up to any good (EFX), and we positively answer the abovequestion. In particular, we study two algorithms that are known to produce suchallocations in the non-strategic setting: Round-Robin (EF1 allocations for anynumber of agents) and a cut-and-choose algorithm of Plaut and Roughgarden [SIAMJournal of Discrete Mathematics, 2020] (EFX allocations for two agents). ForRound-Robin we show that all of its pure Nash equilibria induce allocationsthat are EF1 with respect to the underlying true values, while for thealgorithm of Plaut and Roughgarden we show that the corresponding allocationsnot only are EFX but also satisfy maximin share fairness, something that is nottrue for this algorithm in the non-strategic setting! Further, we show that aweaker version of the latter result holds for any mechanism for two agents thatalways has pure Nash equilibria which all induce EFX allocations.</description><author>Georgios Amanatidis, Georgios Birmpas, Federico Fusco, Philip Lazos, Stefano Leonardi, Rebecca Reiffenh√§user</author><pubDate>Mon, 11 Dec 2023 16:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.08644v2</guid></item><item><title>ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications</title><link>http://arxiv.org/abs/2312.01339v2</link><description>This paper presents the first Arabic crossword puzzle generator driven byadvanced AI technology. Leveraging cutting-edge large language models includingGPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the systemgenerates distinctive and challenging clues. Based on a dataset comprising over50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shotlearning strategies, and rigorous quality-checking protocols to enforce thegeneration of high-quality clue-answer pairs. Importantly, educationalcrosswords contribute to enhancing memory, expanding vocabulary, and promotingproblem-solving skills, thereby augmenting the learning experience through afun and engaging approach, reshaping the landscape of traditional learningmethods. The overall system can be exploited as a powerful educational toolthat amalgamates AI and innovative learning techniques, heralding atransformative era for Arabic crossword puzzles and the intersection oftechnology and education.</description><author>Kamyar Zeinalipour, Mohamed Zaky Saad, Marco Maggini, Marco Gori</author><pubDate>Mon, 11 Dec 2023 16:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01339v2</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v1</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. We plan to publicly release the codes,model, and constructed datasets for future research.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Mon, 11 Dec 2023 16:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v1</guid></item><item><title>TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability</title><link>http://arxiv.org/abs/2312.06499v1</link><description>The fairness of Natural Language Processing (NLP) models has emerged as acrucial concern. Information theory indicates that to achieve fairness, a modelshould not be able to predict sensitive variables, such as gender, ethnicity,and age. However, information related to these variables often appearsimplicitly in language, posing a challenge in identifying and mitigating biaseseffectively. To tackle this issue, we present a novel approach that operates atthe embedding level of an NLP model, independent of the specific architecture.Our method leverages insights from recent advances in XAI techniques andemploys an embedding transformation to eliminate implicit information from aselected variable. By directly manipulating the embeddings in the final layer,our approach enables a seamless integration into existing models withoutrequiring significant modifications or retraining. In evaluation, we show thatthe proposed post-hoc approach significantly reduces gender-relatedassociations in NLP models while preserving the overall performance andfunctionality of the models. An implementation of our method is available:https://github.com/fanny-jourdan/TaCo</description><author>Fanny Jourdan, Louis B√©thune, Agustin Picard, Laurent Risser, Nicholas Asher</author><pubDate>Mon, 11 Dec 2023 16:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06499v1</guid></item><item><title>Detecting Events in Crowds Through Changes in Geometrical Dimensions of Pedestrians</title><link>http://arxiv.org/abs/2312.06495v1</link><description>Security is an important topic in our contemporary world, and the ability toautomate the detection of any events of interest that can take place in a crowdis of great interest to a population. We hypothesize that the detection ofevents in videos is correlated with significant changes in pedestrianbehaviors. In this paper, we examine three different scenarios of crowdbehavior, containing both the cases where an event triggers a change in thebehavior of the crowd and two video sequences where the crowd and its motionremain mostly unchanged. With both the videos and the tracking of theindividual pedestrians (performed in a pre-processed phase), we use Geomind, asoftware we developed to extract significant data about the scene, inparticular, the geometrical features, personalities, and emotions of eachperson. We then examine the output, seeking a significant change in the wayeach person acts as a function of the time, that could be used as a basis toidentify events or to model realistic crowd actions. When applied to the gamesarea, our method can use the detected events to find some sort of pattern to bethen used in agent simulation. Results indicate that our hypothesis seems validin the sense that the visually observed events could be automatically detectedusing GeoMind.</description><author>Matheus Schreiner Homrich da Silva, Paulo Brossard de Souza Pinto Neto, Rodolfo Migon Favaretto, Soraia Raupp Musse</author><pubDate>Mon, 11 Dec 2023 16:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06495v1</guid></item><item><title>Automated Planning Techniques for Elementary Proofs in Abstract Algebra</title><link>http://arxiv.org/abs/2312.06490v1</link><description>This paper explores the application of automated planning to automatedtheorem proving, which is a branch of automated reasoning concerned with thedevelopment of algorithms and computer programs to construct mathematicalproofs. In particular, we investigate the use of planning to constructelementary proofs in abstract algebra, which provides a rigorous and axiomaticframework for studying algebraic structures such as groups, rings, fields, andmodules. We implement basic implications, equalities, and rules in bothdeterministic and non-deterministic domains to model commutative rings anddeduce elementary results about them. The success of this initialimplementation suggests that the well-established techniques seen in automatedplanning are applicable to the relatively newer field of automated theoremproving. Likewise, automated theorem proving provides a new, challenging domainfor automated planning.</description><author>Alice Petrov, Christian Muise</author><pubDate>Mon, 11 Dec 2023 16:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06490v1</guid></item><item><title>Diffusion Models for Reinforcement Learning: A Survey</title><link>http://arxiv.org/abs/2311.01223v2</link><description>Diffusion models have emerged as a prominent class of generative models,surpassing previous methods regarding sample quality and training stability.Recent works have shown the advantages of diffusion models in improvingreinforcement learning (RL) solutions, including as trajectory planners,expressive policy classes, data synthesizers, etc. This survey aims to providean overview of the advancements in this emerging field and hopes to inspire newavenues of research. First, we examine several challenges encountered bycurrent RL algorithms. Then, we present a taxonomy of existing methods based onthe roles played by diffusion models in RL and explore how the existingchallenges are addressed. We further outline successful applications ofdiffusion models in various RL-related tasks while discussing the limitationsof current approaches. Finally, we conclude the survey and offer insights intofuture research directions, focusing on enhancing model performance andapplying diffusion models to broader tasks. We are actively maintaining aGitHub repository for papers and other related resources in applying diffusionmodels in RL: https://github.com/apexrl/Diff4RLSurvey</description><author>Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, Weinan Zhang</author><pubDate>Mon, 11 Dec 2023 16:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01223v2</guid></item><item><title>STDiff: Spatio-temporal Diffusion for Continuous Stochastic Video Prediction</title><link>http://arxiv.org/abs/2312.06486v1</link><description>Predicting future frames of a video is challenging because it is difficult tolearn the uncertainty of the underlying factors influencing their contents. Inthis paper, we propose a novel video prediction model, which hasinfinite-dimensional latent variables over the spatio-temporal domain.Specifically, we first decompose the video motion and content information, thentake a neural stochastic differential equation to predict the temporal motioninformation, and finally, an image diffusion model autoregressively generatesthe video frame by conditioning on the predicted motion feature and theprevious frame. The better expressiveness and stronger stochasticity learningcapability of our model lead to state-of-the-art video prediction performances.As well, our model is able to achieve temporal continuous prediction, i.e.,predicting in an unsupervised way the future video frames with an arbitrarilyhigh frame rate. Our code is available at\url{https://github.com/XiYe20/STDiffProject}.</description><author>Xi Ye, Guillaume-Alexandre Bilodeau</author><pubDate>Mon, 11 Dec 2023 16:12:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06486v1</guid></item><item><title>Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation</title><link>http://arxiv.org/abs/2312.06474v1</link><description>For few-shot semantic segmentation, the primary task is to extractclass-specific intrinsic information from limited labeled data. However, thesemantic ambiguity and inter-class similarity of previous methods limit theaccuracy of pixel-level foreground-background classification. To alleviatethese issues, we propose the Relevant Intrinsic Feature Enhancement Network(RiFeNet). To improve the semantic consistency of foreground instances, wepropose an unlabeled branch as an efficient data utilization method, whichteaches the model how to extract intrinsic features robust to intra-classdifferences. Notably, during testing, the proposed unlabeled branch is excludedwithout extra unlabeled data and computation. Furthermore, we extend theinter-class variability between foreground and background by proposing a novelmulti-level prototype generation and interaction module. The different-grainedcomplementarity between global and local prototypes allows for betterdistinction between similar categories. The qualitative and quantitativeperformance of RiFeNet surpasses the state-of-the-art methods on PASCAL-5i andCOCO benchmarks.</description><author>Xiaoyi Bao, Jie Qin, Siyang Sun, Yun Zheng, Xingang Wang</author><pubDate>Mon, 11 Dec 2023 16:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06474v1</guid></item><item><title>NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields</title><link>http://arxiv.org/abs/2309.14293v3</link><description>Neural radiance fields (NeRFs) enable high-quality novel view synthesis, buttheir high computational complexity limits deployability. While existingneural-based solutions strive for efficiency, they use one-size-fits-allarchitectures regardless of scene complexity. The same architecture may beunnecessarily large for simple scenes but insufficient for complex ones. Thus,there is a need to dynamically optimize the neural network component of NeRFsto achieve a balance between computational complexity and specific targets forsynthesis quality. We introduce NAS-NeRF, a generative neural architecturesearch strategy that generates compact, scene-specialized NeRF architectures bybalancing architecture complexity and target synthesis quality metrics. Ourmethod incorporates constraints on target metrics and budgets to guide thesearch towards architectures tailored for each scene. Experiments on theBlender synthetic dataset show the proposed NAS-NeRF can generate architecturesup to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster thanbaseline NeRFs with only a 5.3% average SSIM drop. Our source code is also madepublicly available at https://saeejithnair.github.io/NAS-NeRF.</description><author>Saeejith Nair, Yuhao Chen, Mohammad Javad Shafiee, Alexander Wong</author><pubDate>Mon, 11 Dec 2023 16:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14293v3</guid></item><item><title>Aligning brain functions boosts the decoding of visual semantics in novel subjects</title><link>http://arxiv.org/abs/2312.06467v1</link><description>Deep learning is leading to major advances in the realm of brain decodingfrom functional Magnetic Resonance Imaging (fMRI). However, the largeinter-subject variability in brain characteristics has limited most studies totrain models on one subject at a time. Consequently, this approach hampers thetraining of deep learning models, which typically requires very large datasets.Here, we propose to boost brain decoding by aligning brain responses to videosand static images across subjects. Compared to the anatomically-alignedbaseline, our method improves out-of-subject decoding performance by up to 75%.Moreover, it also outperforms classical single-subject approaches when fewerthan 100 minutes of data is available for the tested subject. Furthermore, wepropose a new multi-subject alignment method, which obtains comparable resultsto that of classical single-subject approaches while improving out-of-subjectgeneralization. Finally, we show that this method aligns neural representationsin accordance with brain anatomy. Overall, this study lays the foundations forleveraging extensive neuroimaging datasets and enhancing the decoding ofindividuals with a limited amount of brain recordings.</description><author>Alexis Thual, Yohann Benchetrit, Felix Geilert, J√©r√©my Rapin, Iurii Makarov, Hubert Banville, Jean-R√©mi King</author><pubDate>Mon, 11 Dec 2023 15:55:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06467v1</guid></item><item><title>Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation</title><link>http://arxiv.org/abs/2312.06462v1</link><description>Recently, an audio-visual segmentation (AVS) task has been introduced, aimingto group pixels with sounding objects within a given video. This tasknecessitates a first-ever audio-driven pixel-level understanding of the scene,posing significant challenges. In this paper, we propose an innovativeaudio-visual transformer framework, termed COMBO, an acronym for COoperation ofMulti-order Bilateral relatiOns. For the first time, our framework exploresthree types of bilateral entanglements within AVS: pixel entanglement, modalityentanglement, and temporal entanglement. Regarding pixel entanglement, weemploy a Siam-Encoder Module (SEM) that leverages prior knowledge to generatemore precise visual features from the foundational model. For modalityentanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO toalign corresponding visual and auditory signals bi-directionally. As fortemporal entanglement, we introduce an innovative adaptive inter-frameconsistency loss according to the inherent rules of temporal. Comprehensiveexperiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIouon MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate thatCOMBO surpasses previous state-of-the-art methods. Code and more results willbe publicly available at https://combo-avs.github.io/.</description><author>Qi Yang, Xing Nie, Tong Li, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, Shiming Xiang</author><pubDate>Mon, 11 Dec 2023 15:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06462v1</guid></item><item><title>Variational Auto-Encoder Based Deep Learning Technique For Filling Gaps in Reacting PIV Data</title><link>http://arxiv.org/abs/2312.06461v1</link><description>In this study, a deep learning based conditional density estimation techniqueknown as conditional variational auto-encoder (CVAE) is used to fill gapstypically observed in particle image velocimetry (PIV) measurements incombustion systems. The proposed CVAE technique is trained using time resolvedgappy PIV fields, typically observed in industrially relevant combustors.Stereo-PIV (SPIV) data from a swirl combustor with very a high vector yield isused to showcase the accuracy of the proposed CVAE technique. Various errormetrics evaluated on the reconstructed velocity field in the gaps are presentedfrom data sets corresponding to three sets of combustor operating conditions.In addition to accurate data reproduction, the proposed CVAE technique offersdata compression by reducing the latent space dimension, enabling the efficientprocessing of large-scale PIV data.</description><author>Shashank Yellapantula</author><pubDate>Mon, 11 Dec 2023 15:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06461v1</guid></item><item><title>ASF-YOLO: A Novel YOLO Model with Attentional Scale Sequence Fusion for Cell Instance Segmentation</title><link>http://arxiv.org/abs/2312.06458v1</link><description>We propose a novel Attentional Scale Sequence Fusion based You Only Look Once(YOLO) framework (ASF-YOLO) which combines spatial and scale features foraccurate and fast cell instance segmentation. Built on the YOLO segmentationframework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhancethe multi-scale information extraction capability of the network, and theTriple Feature Encoder (TPE) module to fuse feature maps of different scales toincrease detailed information. We further introduce a Channel and PositionAttention Mechanism (CPAM) to integrate both the SSFF and TPE modules, whichfocus on informative channels and spatial position-related small objects forimproved detection and segmentation performance. Experimental validations ontwo cell datasets show remarkable segmentation accuracy and speed of theproposed ASF-YOLO model. It achieves a box mAP of 0.91, mask mAP of 0.887, andan inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset,outperforming the state-of-the-art methods. The source code is available athttps://github.com/mkang315/ASF-YOLO.</description><author>Ming Kang, Chee-Ming Ting, Fung Fung Ting, Rapha√´l C. -W. Phan</author><pubDate>Mon, 11 Dec 2023 15:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06458v1</guid></item><item><title>TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting</title><link>http://arxiv.org/abs/2306.09364v4</link><description>Transformers have gained popularity in time series forecasting for theirability to capture long-sequence interactions. However, their high memory andcomputing requirements pose a critical bottleneck for long-term forecasting. Toaddress this, we propose TSMixer, a lightweight neural architecture exclusivelycomposed of multi-layer perceptron (MLP) modules for multivariate forecastingand representation learning on patched time series. Inspired by MLP-Mixer'ssuccess in computer vision, we adapt it for time series, addressing challengesand introducing validated components for enhanced accuracy. This includes anovel design paradigm of attaching online reconciliation heads to the MLP-Mixerbackbone, for explicitly modeling the time-series properties such as hierarchyand channel-correlations. We also propose a novel Hybrid channel modeling andinfusion of a simple gating approach to effectively handle noisy channelinteractions and generalization across diverse datasets. By incorporating theselightweight components, we significantly enhance the learning capability ofsimple MLP structures, outperforming complex Transformer models with minimalcomputing usage. Moreover, TSMixer's modular design enables compatibility withboth supervised and masked self-supervised learning methods, making it apromising building block for time-series Foundation Models. TSMixer outperformsstate-of-the-art MLP and Transformer models in forecasting by a considerablemargin of 8-60%. It also outperforms the latest strong benchmarks ofPatch-Transformer models (by 1-2%) with a significant reduction in memory andruntime (2-3X). The source code of our model is officially released asPatchTSMixer in the HuggingFace. Model:https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixerExamples: https://github.com/ibm/tsfm/#notebooks-links</description><author>Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam</author><pubDate>Mon, 11 Dec 2023 15:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09364v4</guid></item><item><title>Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping</title><link>http://arxiv.org/abs/2312.06457v1</link><description>Identifying disease phenotypes from electronic health records (EHRs) iscritical for numerous secondary uses. Manually encoding physician knowledgeinto rules is particularly challenging for rare diseases due to inadequate EHRcoding, necessitating review of clinical notes. Large language models (LLMs)offer promise in text understanding but may not efficiently handle real-worldclinical documentation. We propose a zero-shot LLM-based method enriched byretrieval-augmented generation and MapReduce, which pre-identifiesdisease-related text snippets to be used in parallel as queries for the LLM toestablish diagnosis. We show that this method as applied to pulmonaryhypertension (PH), a rare disease characterized by elevated arterial pressuresin the lungs, significantly outperforms physician logic rules ($F_1$ score of0.62 vs. 0.75). This method has the potential to enhance rare disease cohortidentification, expanding the scope of robust clinical research and care gapidentification.</description><author>Will E. Thompson, David M. Vidmar, Jessica K. De Freitas, John M. Pfeifer, Brandon K. Fornwalt, Ruijun Chen, Gabriel Altay, Kabir Manghnani, Andrew C. Nelsen, Kellie Morland, Martin C. Stumpe, Riccardo Miotto</author><pubDate>Mon, 11 Dec 2023 15:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06457v1</guid></item><item><title>Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images</title><link>http://arxiv.org/abs/2312.06454v1</link><description>Directly predicting human epidermal growth factor receptor 2 (HER2) statusfrom widely available hematoxylin and eosin (HE)-stained whole slide images(WSIs) can reduce technical costs and expedite treatment selection. Accuratelypredicting HER2 requires large collections of multi-site WSIs. Federatedlearning enables collaborative training of these WSIs without gigabyte-sizeWSIs transportation and data privacy concerns. However, federated learningencounters challenges in addressing label imbalance in multi-site WSIs from thereal world. Moreover, existing WSI classification methods cannot simultaneouslyexploit local context information and long-range dependencies in the site-endfeature representation of federated learning. To address these issues, wepresent a point transformer with federated learning for multi-site HER2 statusprediction from HE-stained WSIs. Our approach incorporates two novel designs.We propose a dynamic label distribution strategy and an auxiliary classifier,which helps to establish a well-initialized model and mitigate labeldistribution variations across sites. Additionally, we propose a farthestcosine sampling based on cosine distance. It can sample the most distinctivefeatures and capture the long-range dependencies. Extensive experiments andanalysis show that our method achieves state-of-the-art performance at foursites with a total of 2687 WSIs. Furthermore, we demonstrate that our model cangeneralize to two unseen sites with 229 WSIs.</description><author>Bao Li, Zhenyu Liu, Lizhi Shao, Bensheng Qiu, Hong Bu, Jie Tian</author><pubDate>Mon, 11 Dec 2023 15:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06454v1</guid></item></channel></rss>