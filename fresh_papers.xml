<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 01 May 2024 06:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Lightplane: Highly-Scalable Components for Neural 3D Fields</title><link>http://arxiv.org/abs/2404.19760v1</link><description>Contemporary 3D research, particularly in reconstruction and generation,heavily relies on 2D images for inputs or supervision. However, current designsfor these 2D-3D mapping are memory-intensive, posing a significant bottleneckfor existing methods and hindering new applications. In response, we propose apair of highly scalable components for 3D neural fields: Lightplane Render andSplatter, which significantly reduce memory usage in 2D-3D mapping. Theseinnovations enable the processing of vastly more and higher resolution imageswith small memory and computational costs. We demonstrate their utility invarious applications, from benefiting single-scene optimization withimage-level losses to realizing a versatile pipeline for dramatically scaling3D reconstruction and generation. Code:\url{https://github.com/facebookresearch/lightplane}.</description><author>Ang Cao, Justin Johnson, Andrea Vedaldi, David Novotny</author><pubDate>Tue, 30 Apr 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19760v1</guid></item><item><title>MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model</title><link>http://arxiv.org/abs/2404.19759v1</link><description>This work introduces MotionLCM, extending controllable motion generation to areal-time level. Existing methods for spatial control in text-conditionedmotion generation suffer from significant runtime inefficiency. To address thisissue, we first propose the motion latent consistency model (MotionLCM) formotion generation, building upon the latent diffusion model (MLD). By employingone-step (or few-step) inference, we further improve the runtime efficiency ofthe motion latent diffusion model for motion generation. To ensure effectivecontrollability, we incorporate a motion ControlNet within the latent space ofMotionLCM and enable explicit control signals (e.g., pelvis trajectory) in thevanilla motion space to control the generation process directly, similar tocontrolling other latent-free diffusion models for motion generation. Byemploying these techniques, our approach can generate human motions with textand control signals in real-time. Experimental results demonstrate theremarkable generation and controlling capabilities of MotionLCM whilemaintaining real-time runtime efficiency.</description><author>Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, Yansong Tang</author><pubDate>Tue, 30 Apr 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19759v1</guid></item><item><title>Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting</title><link>http://arxiv.org/abs/2404.19758v1</link><description>3D scene generation has quickly become a challenging new research direction,fueled by consistent improvements of 2D generative diffusion models. Most priorwork in this area generates scenes by iteratively stitching newly generatedframes with existing geometry. These works often depend on pre-trainedmonocular depth estimators to lift the generated images into 3D, fusing themwith the existing scene representation. These approaches are then oftenevaluated via a text metric, measuring the similarity between the generatedimages and a given text prompt. In this work, we make two fundamentalcontributions to the field of 3D scene generation. First, we note that liftingimages to 3D with a monocular depth estimation model is suboptimal as itignores the geometry of the existing scene. We thus introduce a novel depthcompletion model, trained via teacher distillation and self-training to learnthe 3D fusion process, resulting in improved geometric coherence of the scene.Second, we introduce a new benchmarking scheme for scene generation methodsthat is based on ground truth geometry, and thus measures the quality of thestructure of the scene.</description><author>Paul Engstler, Andrea Vedaldi, Iro Laina, Christian Rupprecht</author><pubDate>Tue, 30 Apr 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19758v1</guid></item><item><title>A Survey of Reinforcement Learning from Human Feedback</title><link>http://arxiv.org/abs/2312.14925v2</link><description>Reinforcement learning from human feedback (RLHF) is a variant ofreinforcement learning (RL) that learns from human feedback instead of relyingon an engineered reward function. Building on prior work on the related settingof preference-based reinforcement learning (PbRL), it stands at theintersection of artificial intelligence and human-computer interaction. Thispositioning offers a promising avenue to enhance the performance andadaptability of intelligent systems while also improving the alignment of theirobjectives with human values. The training of large language models (LLMs) hasimpressively demonstrated this potential in recent years, where RLHF played adecisive role in directing the model's capabilities toward human objectives.This article provides a comprehensive overview of the fundamentals of RLHF,exploring the intricate dynamics between RL agents and human input. Whilerecent focus has been on RLHF for LLMs, our survey adopts a broaderperspective, examining the diverse applications and wide-ranging impact of thetechnique. We delve into the core principles that underpin RLHF, shedding lighton the symbiotic relationship between algorithms and human feedback, anddiscuss the main research trends in the field. By synthesizing the currentlandscape of RLHF research, this article aims to provide researchers as well aspractitioners with a comprehensive understanding of this rapidly growing fieldof research.</description><author>Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke Hüllermeier</author><pubDate>Tue, 30 Apr 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14925v2</guid></item><item><title>KAN: Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2404.19756v1</link><description>Inspired by the Kolmogorov-Arnold representation theorem, we proposeKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-LayerPerceptrons (MLPs). While MLPs have fixed activation functions on nodes("neurons"), KANs have learnable activation functions on edges ("weights").KANs have no linear weights at all -- every weight parameter is replaced by aunivariate function parametrized as a spline. We show that this seeminglysimple change makes KANs outperform MLPs in terms of accuracy andinterpretability. For accuracy, much smaller KANs can achieve comparable orbetter accuracy than much larger MLPs in data fitting and PDE solving.Theoretically and empirically, KANs possess faster neural scaling laws thanMLPs. For interpretability, KANs can be intuitively visualized and can easilyinteract with human users. Through two examples in mathematics and physics,KANs are shown to be useful collaborators helping scientists (re)discovermathematical and physical laws. In summary, KANs are promising alternatives forMLPs, opening opportunities for further improving today's deep learning modelswhich rely heavily on MLPs.</description><author>Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljačić, Thomas Y. Hou, Max Tegmark</author><pubDate>Tue, 30 Apr 2024 18:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19756v1</guid></item><item><title>Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines</title><link>http://arxiv.org/abs/2403.11585v2</link><description>In the ever-evolving landscape of machine learning, seamless translation ofnatural language descriptions into executable code remains a formidablechallenge. This paper introduces Linguacodus, an innovative framework designedto tackle this challenge by deploying a dynamic pipeline that iterativelytransforms natural language task descriptions into code through high-leveldata-shaping instructions. The core of Linguacodus is a fine-tuned largelanguage model (LLM), empowered to evaluate diverse solutions for variousproblems and select the most fitting one for a given task. This paper detailsthe fine-tuning process, and sheds light on how natural language descriptionscan be translated into functional code. Linguacodus represents a substantialleap towards automated code generation, effectively bridging the gap betweentask descriptions and executable code. It holds great promise for advancingmachine learning applications across diverse domains. Additionally, we proposean algorithm capable of transforming a natural description of an ML task intocode with minimal human interaction. In extensive experiments on a vast machinelearning code dataset originating from Kaggle, we showcase the effectiveness ofLinguacodus. The investigations highlight its potential applications acrossdiverse domains, emphasizing its impact on applied machine learning in variousscientific fields.</description><author>Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin</author><pubDate>Tue, 30 Apr 2024 18:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11585v2</guid></item><item><title>DOCCI: Descriptions of Connected and Contrasting Images</title><link>http://arxiv.org/abs/2404.19753v1</link><description>Vision-language datasets are vital for both text-to-image (T2I) andimage-to-text (I2T) research. However, current datasets lack descriptions withfine-grained detail that would allow for richer associations to be learned bymodels. To fill the gap, we introduce Descriptions of Connected and ContrastingImages (DOCCI), a dataset with long, human-annotated English descriptions for15k images that were taken, curated and donated by a single researcher intenton capturing key challenges such as spatial relations, counting, textrendering, world knowledge, and more. We instruct human annotators to createcomprehensive descriptions for each image; these average 136 words in lengthand are crafted to clearly distinguish each image from those that are relatedor similar. Each description is highly compositional and typically encompassesmultiple challenges. Through both quantitative and qualitative analyses, wedemonstrate that DOCCI serves as an effective training resource forimage-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal orsuperior results compared to highly-performant larger models like LLaVA-1.5 7Band InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed fortext-to-image generation, highlighting the limitations of current text-to-imagemodels in capturing long descriptions and fine details.</description><author>Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, Jason Baldridge</author><pubDate>Tue, 30 Apr 2024 18:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19753v1</guid></item><item><title>Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation</title><link>http://arxiv.org/abs/2404.19752v1</link><description>Existing automatic captioning methods for visual content face challenges suchas lack of detail, content hallucination, and poor instruction following. Inthis work, we propose VisualFactChecker (VFC), a flexible training-freepipeline that generates high-fidelity and detailed captions for both 2D imagesand 3D objects. VFC consists of three steps: 1) proposal, where image-to-textcaptioning models propose multiple initial captions; 2) verification, where alarge language model (LLM) utilizes tools such as object detection and VQAmodels to fact-check proposed captions; 3) captioning, where an LLM generatesthe final caption by summarizing caption proposals and the fact checkverification results. In this step, VFC can flexibly generate captions invarious styles following complex instructions. We conduct comprehensivecaptioning evaluations using four metrics: 1) CLIP-Score for image-textsimilarity; 2) CLIP-Image-Score for measuring the image-image similaritybetween the original and the reconstructed image generated by a text-to-imagemodel using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4Vfor fine-grained evaluation. Evaluation results show that VFC outperformsstate-of-the-art open-sourced captioning methods for 2D images on the COCOdataset and 3D assets on the Objaverse dataset. Our study demonstrates that bycombining open-source models into a pipeline, we can attain captioningcapability comparable to proprietary models such as GPT-4V, despite being over10x smaller in model size.</description><author>Yunhao Ge, Xiaohui Zeng, Jacob Samuel Huffman, Tsung-Yi Lin, Ming-Yu Liu, Yin Cui</author><pubDate>Tue, 30 Apr 2024 18:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19752v1</guid></item><item><title>Kernel Density Matrices for Probabilistic Deep Learning</title><link>http://arxiv.org/abs/2305.18204v3</link><description>This paper introduces a novel approach to probabilistic deep learning, kerneldensity matrices, which provide a simpler yet effective mechanism forrepresenting joint probability distributions of both continuous and discreterandom variables. In quantum mechanics, a density matrix is the most generalway to describe the state of a quantum system. This work extends the concept ofdensity matrices by allowing them to be defined in a reproducing kernel Hilbertspace. This abstraction allows the construction of differentiable models fordensity estimation, inference, and sampling, and enables their integration intoend-to-end deep neural models. In doing so, we provide a versatilerepresentation of marginal and joint probability distributions that allows usto develop a differentiable, compositional, and reversible inference procedurethat covers a wide range of machine learning tasks, including densityestimation, discriminative learning, and generative modeling. The broadapplicability of the framework is illustrated by two examples: an imageclassification model that can be naturally transformed into a conditionalgenerative model, and a model for learning with label proportions thatdemonstrates the framework's ability to deal with uncertainty in the trainingsamples. The framework is implemented as a library and is available at:https://github.com/fagonzalezo/kdm.</description><author>Fabio A. González, Raúl Ramos-Pollán, Joseph A. Gallego-Mejia</author><pubDate>Tue, 30 Apr 2024 18:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18204v3</guid></item><item><title>Scale-Robust Timely Asynchronous Decentralized Learning</title><link>http://arxiv.org/abs/2404.19749v1</link><description>We consider an asynchronous decentralized learning system, which consists ofa network of connected devices trying to learn a machine learning model withoutany centralized parameter server. The users in the network have their own localtraining data, which is used for learning across all the nodes in the network.The learning method consists of two processes, evolving simultaneously withoutany necessary synchronization. The first process is the model update, where theusers update their local model via a fixed number of stochastic gradientdescent steps. The second process is model mixing, where the users communicatewith each other via randomized gossiping to exchange their models and averagethem to reach consensus. In this work, we investigate the staleness criteriafor such a system, which is a sufficient condition for convergence ofindividual user models. We show that for network scaling, i.e., when the numberof user devices $n$ is very large, if the gossip capacity of individual usersscales as $\Omega(\log n)$, we can guarantee the convergence of user models infinite time. Furthermore, we show that the bounded staleness can only beguaranteed by any distributed opportunistic scheme by $\Omega(n)$ scaling.</description><author>Purbesh Mitra, Sennur Ulukus</author><pubDate>Tue, 30 Apr 2024 18:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19749v1</guid></item><item><title>Improving Dictionary Learning with Gated Sparse Autoencoders</title><link>http://arxiv.org/abs/2404.16014v2</link><description>Recent work has found that sparse autoencoders (SAEs) are an effectivetechnique for unsupervised discovery of interpretable features in languagemodels' (LMs) activations, by finding sparse, linear reconstructions of LMactivations. We introduce the Gated Sparse Autoencoder (Gated SAE), whichachieves a Pareto improvement over training with prevailing methods. In SAEs,the L1 penalty used to encourage sparsity introduces many undesirable biases,such as shrinkage -- systematic underestimation of feature activations. The keyinsight of Gated SAEs is to separate the functionality of (a) determining whichdirections to use and (b) estimating the magnitudes of those directions: thisenables us to apply the L1 penalty only to the former, limiting the scope ofundesirable side effects. Through training SAEs on LMs of up to 7B parameterswe find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage,are similarly interpretable, and require half as many firing features toachieve comparable reconstruction fidelity.</description><author>Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, János Kramár, Rohin Shah, Neel Nanda</author><pubDate>Tue, 30 Apr 2024 18:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16014v2</guid></item><item><title>Quantifying Nematodes through Images: Datasets, Models, and Baselines of Deep Learning</title><link>http://arxiv.org/abs/2404.19748v1</link><description>Every year, plant parasitic nematodes, one of the major groups of plantpathogens, cause a significant loss of crops worldwide. To mitigate crop yieldlosses caused by nematodes, an efficient nematode monitoring method isessential for plant and crop disease management. In other respects, efficientnematode detection contributes to medical research and drug discovery, asnematodes are model organisms. With the rapid development of computertechnology, computer vision techniques provide a feasible solution forquantifying nematodes or nematode infections. In this paper, we survey andcategorise the studies and available datasets on nematode detection throughdeep-learning models. To stimulate progress in related research, this surveypresents the potential state-of-the-art object detection models, trainingtechniques, optimisation techniques, and evaluation metrics for deep learningbeginners. Moreover, seven state-of-the-art object detection models arevalidated on three public datasets and the AgriNema dataset for plant parasiticnematodes to construct a baseline for nematode detection.</description><author>Zhipeng Yuan, Nasamu Musa, Katarzyna Dybal, Matthew Back, Daniel Leybourne, Po Yang</author><pubDate>Tue, 30 Apr 2024 18:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19748v1</guid></item><item><title>PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification</title><link>http://arxiv.org/abs/2404.19744v1</link><description>Data protection and privacy is becoming increasingly crucial in the digitalera. Numerous companies depend on third-party vendors and service providers tocarry out critical functions within their operations, encompassing tasks suchas data handling and storage. However, this reliance introduces potentialvulnerabilities, as these vendors' security measures and practices may notalways align with the standards expected by regulatory bodies. Businesses arerequired, often under the penalty of law, to ensure compliance with theevolving regulatory rules. Interpreting and implementing these regulations posechallenges due to their complexity. Regulatory documents are extensive,demanding significant effort for interpretation, while vendor-drafted privacypolicies often lack the detail required for full legal compliance, leading toambiguity. To ensure a concise interpretation of the regulatory requirementsand compliance of organizational privacy policy with said regulations, wepropose a Large Language Model (LLM) and Semantic Web based approach forprivacy compliance. In this paper, we develop the novel Privacy PolicyCompliance Verification Knowledge Graph, PrivComp-KG. It is designed toefficiently store and retrieve comprehensive information concerning privacypolicies, regulatory frameworks, and domain-specific knowledge pertaining tothe legal landscape of privacy. Using Retrieval Augmented Generation, weidentify the relevant sections in a privacy policy with correspondingregulatory rules. This information about individual privacy policies ispopulated into the PrivComp-KG. Combining this with the domain context andrules, the PrivComp-KG can be queried to check for compliance with privacypolicies by each vendor against relevant policy regulations. We demonstrate therelevance of the PrivComp-KG, by verifying compliance of privacy policydocuments for various organizations.</description><author>Leon Garza, Lavanya Elluri, Anantaa Kotal, Aritran Piplai, Deepti Gupta, Anupam Joshi</author><pubDate>Tue, 30 Apr 2024 18:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19744v1</guid></item><item><title>People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior</title><link>http://arxiv.org/abs/2403.08828v2</link><description>Cognitive science can help us understand which explanations people mightexpect, and in which format they frame these explanations, whether causal,counterfactual, or teleological (i.e., purpose-oriented). Understanding therelevance of these concepts is crucial for building good explainable AI (XAI)which offers recourse and actionability. Focusing on autonomous driving, acomplex decision-making domain, we report empirical data from two surveys on(i) how people explain the behavior of autonomous vehicles in 14 uniquescenarios (N1=54), and (ii) how they perceive these explanations in terms ofcomplexity, quality, and trustworthiness (N2=356). Participants deemedteleological explanations significantly better quality than counterfactualones, with perceived teleology being the best predictor of perceived qualityand trustworthiness. Neither the perceived teleology nor the quality wereaffected by whether the car was an autonomous vehicle or driven by a person.This indicates that people use teleology to evaluate information about not justother people but also autonomous vehicles. Taken together, our findingshighlight the importance of explanations that are framed in terms of purposerather than just, as is standard in XAI, the causal mechanisms involved. Werelease the 14 scenarios and more than 1,300 elicited explanations publicly asthe Human Explanations for Autonomous Driving Decisions (HEADD) dataset.</description><author>Balint Gyevnar, Stephanie Droop, Tadeg Quillien, Shay B. Cohen, Neil R. Bramley, Christopher G. Lucas, Stefano V. Albrecht</author><pubDate>Tue, 30 Apr 2024 18:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08828v2</guid></item><item><title>Mixed Continuous and Categorical Flow Matching for 3D De Novo Molecule Generation</title><link>http://arxiv.org/abs/2404.19739v1</link><description>Deep generative models that produce novel molecular structures have thepotential to facilitate chemical discovery. Diffusion models currently achievestate of the art performance for 3D molecule generation. In this work, weexplore the use of flow matching, a recently proposed generative modelingframework that generalizes diffusion models, for the task of de novo moleculegeneration. Flow matching provides flexibility in model design; however, theframework is predicated on the assumption of continuously-valued data. 3D denovo molecule generation requires jointly sampling continuous and categoricalvariables such as atom position and atom type. We extend the flow matchingframework to categorical data by constructing flows that are constrained toexist on a continuous representation of categorical data known as theprobability simplex. We call this extension SimplexFlow. We explore the use ofSimplexFlow for de novo molecule generation. However, we find that, inpractice, a simpler approach that makes no accommodations for the categoricalnature of the data yields equivalent or superior performance. As a result ofthese experiments, we present FlowMol, a flow matching model for 3D de novogenerative model that achieves improved performance over prior flow matchingmethods, and we raise important questions about the design of priordistributions for achieving strong performance in flow matching models. Codeand trained models for reproducing this work are available athttps://github.com/dunni3/FlowMol</description><author>Ian Dunn, David Ryan Koes</author><pubDate>Tue, 30 Apr 2024 18:37:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19739v1</guid></item><item><title>Learning with Density Matrices and Random Features</title><link>http://arxiv.org/abs/2102.04394v5</link><description>A density matrix describes the statistical state of a quantum system. It is apowerful formalism to represent both the quantum and classical uncertainty ofquantum systems and to express different statistical operations such asmeasurement, system combination and expectations as linear algebra operations.This paper explores how density matrices can be used as a building block formachine learning models exploiting their ability to straightforwardly combinelinear algebra and probability. One of the main results of the paper is to showthat density matrices coupled with random Fourier features could approximatearbitrary probability distributions over $\mathbb{R}^n$. Based on this findingthe paper builds different models for density estimation, classification andregression. These models are differentiable, so it is possible to integratethem with other differentiable components, such as deep learning architecturesand to learn their parameters using gradient-based optimization. In addition,the paper presents optimization-less training strategies based on estimationand model averaging. The models are evaluated in benchmark tasks and theresults are reported and discussed.</description><author>Fabio A. González, Alejandro Gallego, Santiago Toledo-Cortés, Vladimir Vargas-Calderón</author><pubDate>Tue, 30 Apr 2024 18:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.04394v5</guid></item><item><title>Better &amp; Faster Large Language Models via Multi-token Prediction</title><link>http://arxiv.org/abs/2404.19737v1</link><description>Large language models such as GPT and Llama are trained with a next-tokenprediction loss. In this work, we suggest that training language models topredict multiple future tokens at once results in higher sample efficiency.More specifically, at each position in the training corpus, we ask the model topredict the following n tokens using n independent output heads, operating ontop of a shared model trunk. Considering multi-token prediction as an auxiliarytraining task, we measure improved downstream capabilities with no overhead intraining time for both code and natural language models. The method isincreasingly useful for larger model sizes, and keeps its appeal when trainingfor multiple epochs. Gains are especially pronounced on generative benchmarkslike coding, where our models consistently outperform strong baselines byseveral percentage points. Our 13B parameter models solves 12 % more problemson HumanEval and 17 % more on MBPP than comparable next-token models.Experiments on small algorithmic tasks demonstrate that multi-token predictionis favorable for the development of induction heads and algorithmic reasoningcapabilities. As an additional benefit, models trained with 4-token predictionare up to 3 times faster at inference, even with large batch sizes.</description><author>Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, Gabriel Synnaeve</author><pubDate>Tue, 30 Apr 2024 18:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19737v1</guid></item><item><title>Can a Machine be Conscious? Towards Universal Criteria for Machine Consciousness</title><link>http://arxiv.org/abs/2404.15369v2</link><description>As artificially intelligent systems become more anthropomorphic andpervasive, and their potential impact on humanity more urgent, discussionsabout the possibility of machine consciousness have significantly intensified,and it is sometimes seen as 'the holy grail'. Many concerns have been voicedabout the ramifications of creating an artificial conscious entity. This iscompounded by a marked lack of consensus around what constitutes consciousnessand by an absence of a universal set of criteria for determining consciousness.By going into depth on the foundations and characteristics of consciousness, wepropose five criteria for determining whether a machine is conscious, which canalso be applied more generally to any entity. This paper aims to serve as aprimer and stepping stone for researchers of consciousness, be they inphilosophy, computer science, medicine, or any other field, to further pursuethis holy grail of philosophy, neuroscience and artificial intelligence.</description><author>Nur Aizaan Anwar, Cosmin Badea</author><pubDate>Tue, 30 Apr 2024 18:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15369v2</guid></item><item><title>Iterative Reasoning Preference Optimization</title><link>http://arxiv.org/abs/2404.19733v1</link><description>Iterative preference optimization methods have recently been shown to performwell for general instruction tuning tasks, but typically make littleimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In thiswork we develop an iterative approach that optimizes the preference betweencompeting generated Chain-of-Thought (CoT) candidates by optimizing for winningvs. losing reasoning steps that lead to the correct answer. We train using amodified DPO loss (Rafailov et al., 2023) with an additional negativelog-likelihood term, which we find to be crucial. We show reasoning improvesacross repeated iterations of this scheme. While only relying on examples inthe training set, our approach results in increasing accuracy forLlama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority votingout of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% onARC-Challenge, which outperforms other Llama-2-based models not relying onadditionally sourced datasets.</description><author>Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston</author><pubDate>Tue, 30 Apr 2024 18:28:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19733v1</guid></item><item><title>A Framework for Leveraging Human Computation Gaming to Enhance Knowledge Graphs for Accuracy Critical Generative AI Applications</title><link>http://arxiv.org/abs/2404.19729v1</link><description>External knowledge graphs (KGs) can be used to augment large language models(LLMs), while simultaneously providing an explainable knowledge base of factsthat can be inspected by a human. This approach may be particularly valuable indomains where explainability is critical, like human trafficking data analysis.However, creating KGs can pose challenges. KGs parsed from documents maycomprise explicit connections (those directly stated by a document) but missimplicit connections (those obvious to a human although not directly stated).To address these challenges, this preliminary research introduces the GAME-KGframework, standing for "Gaming for Augmenting Metadata and Enhancing KnowledgeGraphs." GAME-KG is a federated approach to modifying explicit as well asimplicit connections in KGs by using crowdsourced feedback collected throughvideo games. GAME-KG is shown through two demonstrations: a Unity test scenariofrom Dark Shadows, a video game that collects feedback on KGs parsed from USDepartment of Justice (DOJ) Press Releases on human trafficking, and afollowing experiment where OpenAI's GPT-4 is prompted to answer questions basedon a modified and unmodified KG. Initial results suggest that GAME-KG can be aneffective framework for enhancing KGs, while simultaneously providing anexplainable set of structured facts verified by humans.</description><author>Steph Buongiorno, Corey Clark</author><pubDate>Tue, 30 Apr 2024 18:24:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19729v1</guid></item><item><title>UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes</title><link>http://arxiv.org/abs/2303.14541v2</link><description>3D instance segmentation is fundamental to geometric understanding of theworld around us. Existing methods for instance segmentation of 3D scenes relyon supervision from expensive, manual 3D annotations. We propose UnScene3D, thefirst fully unsupervised 3D learning approach for class-agnostic 3D instancesegmentation of indoor scans. UnScene3D first generates pseudo masks byleveraging self-supervised color and geometry features to find potential objectregions. We operate on a basis of geometric oversegmentation, enablingefficient representation and learning on high-resolution 3D data. The coarseproposals are then refined through self-training our model on its predictions.Our approach improves over state-of-the-art unsupervised 3D instancesegmentation methods by more than 300% Average Precision score, demonstratingeffective instance segmentation even in challenging, cluttered 3D scenes.</description><author>David Rozenberszki, Or Litany, Angela Dai</author><pubDate>Tue, 30 Apr 2024 18:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14541v2</guid></item><item><title>Fairness Without Demographics in Human-Centered Federated Learning</title><link>http://arxiv.org/abs/2404.19725v1</link><description>Federated learning (FL) enables collaborative model training while preservingdata privacy, making it suitable for decentralized human-centered AIapplications. However, a significant research gap remains in ensuring fairnessin these systems. Current fairness strategies in FL require knowledge ofbias-creating/sensitive attributes, clashing with FL's privacy principles.Moreover, in human-centered datasets, sensitive attributes may remain latent.To tackle these challenges, we present a novel bias mitigation approachinspired by "Fairness without Demographics" in machine learning. The presentedapproach achieves fairness without needing knowledge of sensitive attributes byminimizing the top eigenvalue of the Hessian matrix during training, ensuringequitable loss landscapes across FL participants. Notably, we introduce a novelFL aggregation scheme that promotes participating models based on error ratesand loss landscape curvature attributes, fostering fairness across the FLsystem. This work represents the first approach to attaining "Fairness withoutDemographics" in human-centered FL. Through comprehensive evaluation, ourapproach demonstrates effectiveness in balancing fairness and efficacy acrossvarious real-world applications, FL setups, and scenarios involving single andmultiple bias-inducing factors, representing a significant advancement inhuman-centered FL.</description><author>Roy Shaily, Sharma Harshit, Salekin Asif</author><pubDate>Tue, 30 Apr 2024 18:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19725v1</guid></item><item><title>PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</title><link>http://arxiv.org/abs/2404.19722v1</link><description>We address the challenge of content diversity and controllability inpedestrian simulation for driving scenarios. Recent pedestrian animationframeworks have a significant limitation wherein they primarily focus on eitherfollowing trajectory [46] or the content of the reference video [57],consequently overlooking the potential diversity of human motion within suchscenarios. This limitation restricts the ability to generate pedestrianbehaviors that exhibit a wider range of variations and realistic motions andtherefore restricts its usage to provide rich motion content for othercomponents in the driving simulation system, e.g., suddenly changed motion towhich the autonomous vehicle should respond. In our approach, we strive tosurpass the limitation by showcasing diverse human motions obtained fromvarious sources, such as generated human motions, in addition to following thegiven trajectory. The fundamental contribution of our framework lies incombining the motion tracking task with trajectory following, which enables thetracking of specific motion parts (e.g., upper body) while simultaneouslyfollowing the given trajectory by a single policy. This way, we significantlyenhance both the diversity of simulated human motion within the given scenarioand the controllability of the content, including language-based control. Ourframework facilitates the generation of a wide range of human motions,contributing to greater realism and adaptability in pedestrian simulations fordriving scenarios. More information is on our project pagehttps://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html .</description><author>Jingbo Wang, Zhengyi Luo, Ye Yuan, Yixuan Li, Bo Dai</author><pubDate>Tue, 30 Apr 2024 18:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19722v1</guid></item><item><title>Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators</title><link>http://arxiv.org/abs/2402.12365v2</link><description>Neural operators, serving as physics surrogate models, have recently gainedincreased interest. With ever increasing problem complexity, the naturalquestion arises: what is an efficient way to scale neural operators to largerand more complex simulations - most importantly by taking into accountdifferent types of simulation datasets. This is of special interest since, akinto their numerical counterparts, different techniques are used acrossapplications, even if the underlying dynamics of the systems are similar.Whereas the flexibility of transformers has enabled unified architecturesacross domains, neural operators mostly follow a problem specific design, whereGNNs are commonly used for Lagrangian simulations and grid-based modelspredominate Eulerian simulations. We introduce Universal Physics Transformers(UPTs), an efficient and unified learning paradigm for a wide range ofspatio-temporal problems. UPTs operate without grid- or particle-based latentstructures, enabling flexibility and scalability across meshes and particles.UPTs efficiently propagate dynamics in the latent space, emphasized by inverseencoding and decoding techniques. Finally, UPTs allow for queries of the latentspace representation at any point in space-time. We demonstrate diverseapplicability and efficacy of UPTs in mesh-based fluid simulations, andsteady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-baseddynamics.</description><author>Benedikt Alkin, Andreas Fürst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter</author><pubDate>Tue, 30 Apr 2024 18:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12365v2</guid></item><item><title>Causal Discovery from Time Series with Hybrids of Constraint-Based and Noise-Based Algorithms</title><link>http://arxiv.org/abs/2306.08765v2</link><description>Constraint-based methods and noise-based methods are two distinct families ofmethods proposed for uncovering causal graphs from observational data. However,both operate under strong assumptions that may be challenging to validate orcould be violated in real-world scenarios. In response to these challenges,there is a growing interest in hybrid methods that amalgamate principles fromboth methods, showing robustness to assumption violations. This paperintroduces a novel comprehensive framework for hybridizing constraint-based andnoise-based methods designed to uncover causal graphs from observational timeseries. The framework is structured into two classes. The first class employs anoise-based strategy to identify a super graph, containing the true graph,followed by a constraint-based strategy to eliminate unnecessary edges. In thesecond class, a constraint-based strategy is applied to identify a skeleton,which is then oriented using a noise-based strategy. The paper providestheoretical guarantees for each class under the condition that all assumptionsare satisfied, and it outlines some properties when assumptions are violated.To validate the efficacy of the framework, two algorithms from each class areexperimentally tested on simulated data, realistic ecological data, and realdatasets sourced from diverse applications. Notably, two novel datasets relatedto Information Technology monitoring are introduced within the set ofconsidered real datasets. The experimental results underscore the robustnessand effectiveness of the hybrid approaches across a broad spectrum of datasets.</description><author>Daria Bystrova, Charles K. Assaad, Julyan Arbel, Emilie Devijver, Eric Gaussier, Wilfried Thuiller</author><pubDate>Tue, 30 Apr 2024 18:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08765v2</guid></item><item><title>PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games</title><link>http://arxiv.org/abs/2404.19721v1</link><description>This research introduces Procedural Artificial Narrative using Generative AI(PANGeA), a structured approach for leveraging large language models (LLMs),guided by a game designer's high-level criteria, to generate narrative contentfor turn-based role-playing video games (RPGs). Distinct from priorapplications of LLMs used for video game design, PANGeA innovates by not onlygenerating game level data (which includes, but is not limited to, setting, keyitems, and non-playable characters (NPCs)), but by also fostering dynamic,free-form interactions between the player and the environment that align withthe procedural game narrative. The NPCs generated by PANGeA arepersonality-biased and express traits from the Big 5 Personality Model in theirgenerated responses. PANGeA addresses challenges behind ingesting free-formtext input, which can prompt LLM responses beyond the scope of the gamenarrative. A novel validation system that uses the LLM's intelligence evaluatestext input and aligns generated responses with the unfolding narrative. Makingthese interactions possible, PANGeA is supported by a server that hosts acustom memory system that supplies context for augmenting generated responsesthus aligning them with the procedural narrative. For its broad application,the server has a REST interface enabling any game engine to integrate directlywith PANGeA, as well as an LLM interface adaptable with local or private LLMs.PANGeA's ability to foster dynamic narrative generation by aligning responseswith the procedural narrative is demonstrated through an empirical study andablation test of two versions of a demo game. These are, a custom,browser-based GPT and a Unity demo. As the results show, PANGeA holds potentialto assist game designers in using LLMs to generate narrative-consistent contenteven when provided varied and unpredictable, free-form text input.</description><author>Steph Buongiorno, Lawrence Jake Klinkert, Tanishq Chawla, Zixin Zhuang, Corey Clark</author><pubDate>Tue, 30 Apr 2024 18:11:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19721v1</guid></item><item><title>The lazy (NTK) and rich ($μ$P) regimes: a gentle tutorial</title><link>http://arxiv.org/abs/2404.19719v1</link><description>A central theme of the modern machine learning paradigm is that larger neuralnetworks achieve better performance on a variety of metrics. Theoreticalanalyses of these overparameterized models have recently centered aroundstudying very wide neural networks. In this tutorial, we provide a nonrigorousbut illustrative derivation of the following fact: in order to train widenetworks effectively, there is only one degree of freedom in choosinghyperparameters such as the learning rate and the size of the initial weights.This degree of freedom controls the richness of training behavior: at minimum,the wide network trains lazily like a kernel machine, and at maximum, itexhibits feature learning in the so-called $\mu$P regime. In this paper, weexplain this richness scale, synthesize recent research results into a coherentwhole, offer new perspectives and intuitions, and provide empirical evidencesupporting our claims. In doing so, we hope to encourage further study of therichness scale, as it may be key to developing a scientific theory of featurelearning in practical deep neural networks.</description><author>Dhruva Karkada</author><pubDate>Tue, 30 Apr 2024 18:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19719v1</guid></item><item><title>ThangDLU at #SMM4H 2024: Encoder-decoder models for classifying text data on social disorders in children and adolescents</title><link>http://arxiv.org/abs/2404.19714v1</link><description>This paper describes our participation in Task 3 and Task 5 of the #SMM4H(Social Media Mining for Health) 2024 Workshop, explicitly targeting theclassification challenges within tweet data. Task 3 is a multi-classclassification task centered on tweets discussing the impact of outdoorenvironments on symptoms of social anxiety. Task 5 involves a binaryclassification task focusing on tweets reporting medical disorders in children.We applied transfer learning from pre-trained encoder-decoder models such asBART-base and T5-small to identify the labels of a set of given tweets. We alsopresented some data augmentation methods to see their impact on the modelperformance. Finally, the systems obtained the best F1 score of 0.627 in Task 3and the best F1 score of 0.841 in Task 5.</description><author>Hoang-Thang Ta, Abu Bakar Siddiqur Rahman, Lotfollah Najjar, Alexander Gelbukh</author><pubDate>Tue, 30 Apr 2024 18:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19714v1</guid></item><item><title>Automated Generation of High-Quality Medical Simulation Scenarios Through Integration of Semi-Structured Data and Large Language Models</title><link>http://arxiv.org/abs/2404.19713v1</link><description>This study introduces a transformative framework for medical education byintegrating semi-structured data with Large Language Models (LLMs), primarilyOpenAIs ChatGPT3.5, to automate the creation of medical simulation scenarios.Traditionally, developing these scenarios was a time-intensive process withlimited flexibility to meet diverse educational needs. The proposed approachutilizes AI to efficiently generate detailed, clinically relevant scenariosthat are tailored to specific educational objectives. This innovation hassignificantly reduced the time and resources required for scenario development,allowing for a broader variety of simulations. Preliminary feedback fromeducators and learners has shown enhanced engagement and improved knowledgeacquisition, confirming the effectiveness of this AI-enhanced methodology insimulation-based learning. The integration of structured data with LLMs notonly streamlines the creation process but also offers a scalable, dynamicsolution that could revolutionize medical training, highlighting the criticalrole of AI in advancing educational outcomes and patient care standards.</description><author>Scott Sumpter</author><pubDate>Tue, 30 Apr 2024 18:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19713v1</guid></item><item><title>Solving Long-run Average Reward Robust MDPs via Stochastic Games</title><link>http://arxiv.org/abs/2312.13912v2</link><description>Markov decision processes (MDPs) provide a standard framework for sequentialdecision making under uncertainty. However, MDPs do not take uncertainty intransition probabilities into account. Robust Markov decision processes (RMDPs)address this shortcoming of MDPs by assigning to each transition an uncertaintyset rather than a single probability value. In this work, we consider polytopicRMDPs in which all uncertainty sets are polytopes and study the problem ofsolving long-run average reward polytopic RMDPs. We present a novel perspectiveon this problem and show that it can be reduced to solving long-run averagereward turn-based stochastic games with finite state and action spaces. Thisreduction allows us to derive several important consequences that were hithertonot known to hold for polytopic RMDPs. First, we derive new computationalcomplexity bounds for solving long-run average reward polytopic RMDPs, showingfor the first time that the threshold decision problem for them is in $NP \capcoNP$ and that they admit a randomized algorithm with sub-exponential expectedruntime. Second, we present Robust Polytopic Policy Iteration (RPPI), a novelpolicy iteration algorithm for solving long-run average reward polytopic RMDPs.Our experimental evaluation shows that RPPI is much more efficient in solvinglong-run average reward polytopic RMDPs compared to state-of-the-art methodsbased on value iteration.</description><author>Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Mehrdad Karrabi, Petr Novotný, Đorđe Žikelić</author><pubDate>Tue, 30 Apr 2024 18:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13912v2</guid></item><item><title>A rank decomposition for the topological classification of neural representations</title><link>http://arxiv.org/abs/2404.19710v1</link><description>Neural networks can be thought of as applying a transformation to an inputdataset. The way in which they change the topology of such a dataset oftenholds practical significance for many tasks, particularly those demandingnon-homeomorphic mappings for optimal solutions, such as classificationproblems. In this work, we leverage the fact that neural networks areequivalent to continuous piecewise-affine maps, whose rank can be used topinpoint regions in the input space that undergo non-homeomorphictransformations, leading to alterations in the topological structure of theinput dataset. Our approach enables us to make use of the relative homologysequence, with which one can study the homology groups of the quotient of amanifold $\mathcal{M}$ and a subset $A$, assuming some minimal properties onthese spaces. As a proof of principle, we empirically investigate the presence of low-rank(topology-changing) affine maps as a function of network width and mean weight.We show that in randomly initialized narrow networks, there will be regions inwhich the (co)homology groups of a data manifold can change. As the widthincreases, the homology groups of the input manifold become more likely to bepreserved. We end this part of our work by constructing highly non-random widenetworks that do not have this property and relating this non-random regime toDale's principle, which is a defining characteristic of biological neuralnetworks. Finally, we study simple feedforward networks trained on MNIST, as well as ontoy classification and regression tasks, and show that networks manipulate thetopology of data differently depending on the continuity of the task they aretrained on.</description><author>Kosio Beshkov, Gaute T. Einevoll</author><pubDate>Tue, 30 Apr 2024 18:01:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19710v1</guid></item><item><title>Harmonic LLMs are Trustworthy</title><link>http://arxiv.org/abs/2404.19708v1</link><description>We introduce an intuitive method to test the robustness (stability andexplainability) of any black-box LLM in real-time, based upon the localdeviation from harmoniticity, denoted as $\gamma$. To the best of our knowledgethis is the first completely model-agnostic and unsupervised method ofmeasuring the robustness of any given response from an LLM, based upon themodel itself conforming to a purely mathematical standard. We conduct humanannotation experiments to show the positive correlation of $\gamma$ with falseor misleading answers, and demonstrate that following the gradient of $\gamma$in stochastic gradient ascent efficiently exposes adversarial prompts.Measuring $\gamma$ across thousands of queries in popular LLMs (GPT-4, ChatGPT,Claude-2.1, Mixtral-8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us toestimate the liklihood of wrong or hallucinatory answers automatically andquantitatively rank the reliability of these models in various objectivedomains (Web QA, TruthfulQA, and Programming QA). Across all models and domainstested, human ratings confirm that $\gamma \to 0$ indicates trustworthiness,and the low-$\gamma$ leaders among these models are GPT-4, ChatGPT, andSmaug-72B.</description><author>Nicholas S. Kersting, Mohammad Rahman, Suchismitha Vedala, Yang Wang</author><pubDate>Tue, 30 Apr 2024 18:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19708v1</guid></item><item><title>RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting</title><link>http://arxiv.org/abs/2404.19706v1</link><description>We propose RTG-SLAM, a real-time 3D reconstruction system with an RGBD camerafor large-scale environments using Gaussian splatting. RTG-SLAM features acompact Gaussian representation and a highly efficient on-the-fly Gaussianoptimization scheme. We force each Gaussian to be either opaque or nearlytransparent, with the opaque ones fitting the surface and dominant colors, andtransparent ones fitting residual colors. By rendering depth in a different wayfrom color rendering, we let a single opaque Gaussian well fit a local surfaceregion without the need of multiple overlapping Gaussians, hence largelyreducing the memory and computation cost. For on-the-fly Gaussian optimization,we explicitly add Gaussians for three types of pixels per frame: newlyobserved, with large color errors and with large depth errors. We alsocategorize all Gaussians into stable and unstable ones, where the stableGaussians are expected to well fit previously observed RGBD images andotherwise unstable. We only optimize the unstable Gaussians and only render thepixels occupied by unstable Gaussians. In this way, both the number ofGaussians to be optimized and pixels to be rendered are largely reduced, andthe optimization can be done in real time. We show real-time reconstructions ofa variety of real large scenes. Compared with the state-of-the-art NeRF-basedRGBD SLAM, our system achieves comparable high-quality reconstruction but witharound twice the speed and half the memory cost, and shows superior performancein the realism of novel view synthesis and camera tracking accuracy.</description><author>Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</author><pubDate>Tue, 30 Apr 2024 17:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19706v1</guid></item><item><title>When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</title><link>http://arxiv.org/abs/2404.19705v1</link><description>In this paper, we demonstrate how Large Language Models (LLMs) caneffectively learn to use an off-the-shelf information retrieval (IR) systemspecifically when additional context is required to answer a given question.Given the performance of IR systems, the optimal strategy for questionanswering does not always entail external information retrieval; rather, itoften involves leveraging the parametric memory of the LLM itself. Priorresearch has identified this phenomenon in the PopQA dataset, wherein the mostpopular questions are effectively addressed using the LLM's parametric memory,while less popular ones require IR system usage. Following this, we propose atailored training approach for LLMs, leveraging existing open-domain questionanswering datasets. Here, LLMs are trained to generate a special token, &lt;RET&gt;,when they do not know the answer to a question. Our evaluation of the AdaptiveRetrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over thesame LLM under three configurations: (i) retrieving information for all thequestions, (ii) using always the parametric memory of the LLM, and (iii) usinga popularity threshold to decide when to use a retriever. Through our analysis,we demonstrate that Adapt-LLM is able to generate the &lt;RET&gt; token when itdetermines that it does not know how to answer a question, indicating the needfor IR, while it achieves notably high accuracy levels when it chooses to relyonly on its parametric memory.</description><author>Tiziano Labruna, Jon Ander Campos, Gorka Azkune</author><pubDate>Tue, 30 Apr 2024 17:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19705v1</guid></item><item><title>Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification</title><link>http://arxiv.org/abs/2402.13523v2</link><description>The prospect of future treatment warrants the development of cost-effectivescreening for Alzheimer's disease (AD). A promising candidate in this regard iselectroencephalography (EEG), as it is one of the most economic imagingmodalities. Recent efforts in EEG analysis have shifted towards leveragingspatial information, employing novel frameworks such as graph signal processingor graph neural networks. Here, we investigate the importance of spatialinformation relative to spectral or temporal information by varying theproportion of each dimension for AD classification. To do so, we systematicallytest various dimension resolution configurations on two routine EEG datasets.Our findings show that spatial information is more important than temporalinformation and equally valuable as spectral information. On the larger seconddataset, substituting spectral with spatial information even led to an increaseof 1.1% in accuracy, which emphasises the importance of spatial information forEEG-based AD classification. We argue that our resolution-based featureextraction has the potential to improve AD classification specifically, andmultivariate signal classification generally.</description><author>Stephan Goerttler, Fei He, Min Wu</author><pubDate>Tue, 30 Apr 2024 17:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13523v2</guid></item><item><title>GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2404.19702v1</link><description>We propose GS-LRM, a scalable large reconstruction model that can predicthigh-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23seconds on single A100 GPU. Our model features a very simple transformer-basedarchitecture; we patchify input posed images, pass the concatenated multi-viewimage tokens through a sequence of transformer blocks, and decode finalper-pixel Gaussian parameters directly from these tokens for differentiablerendering. In contrast to previous LRMs that can only reconstruct objects, bypredicting per-pixel Gaussians, GS-LRM naturally handles scenes with largevariations in scale and complexity. We show that our model can work on bothobject and scene captures by training it on Objaverse and RealEstate10Krespectively. In both scenarios, the models outperform state-of-the-artbaselines by a wide margin. We also demonstrate applications of our model indownstream 3D generation tasks. Our project webpage is available at:https://sai-bi.github.io/project/gs-lrm/ .</description><author>Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, Zexiang Xu</author><pubDate>Tue, 30 Apr 2024 17:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19702v1</guid></item><item><title>Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners</title><link>http://arxiv.org/abs/2404.19696v1</link><description>3D visual grounding is a challenging task that often requires direct anddense supervision, notably the semantic label for each object in the scene. Inthis paper, we instead study the naturally supervised setting that learns fromonly 3D scene and QA pairs, where prior works underperform. We propose theLanguage-Regularized Concept Learner (LARC), which uses constraints fromlanguage as regularization to significantly improve the accuracy ofneuro-symbolic concept learners in the naturally supervised setting. Ourapproach is based on two core insights: the first is that language constraints(e.g., a word's relation to another) can serve as effective regularization forstructured representations in neuro-symbolic models; the second is that we canquery large language models to distill such constraints from languageproperties. We show that LARC improves performance of prior works in naturallysupervised 3D visual grounding, and demonstrates a wide range of 3D visualreasoning capabilities-from zero-shot composition, to data efficiency andtransferability. Our method represents a promising step towards regularizingstructured visual reasoning frameworks with language-based priors, for learningin settings without dense supervision.</description><author>Chun Feng, Joy Hsu, Weiyu Liu, Jiajun Wu</author><pubDate>Tue, 30 Apr 2024 17:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19696v1</guid></item><item><title>Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL</title><link>http://arxiv.org/abs/2404.02113v2</link><description>In continual or lifelong reinforcement learning access to the environmentshould be limited. If we aspire to design algorithms that can run forlong-periods of time, continually adapting to new, unexpected situations thenwe must be willing to deploy our agents without tuning their hyperparametersover the agent's entire lifetime. The standard practice in deep RL -- and evencontinual RL -- is to assume unfettered access to deployment environment forthe full lifetime of the agent. This paper explores the notion that progress inlifelong RL research has been held back by inappropriate empiricalmethodologies. In this paper we propose a new approach for tuning andevaluating lifelong RL agents where only one percent of the experiment data canbe used for hyperparameter tuning. We then conduct an empirical study of DQNand Soft Actor Critic across a variety of continuing and non-stationarydomains. We find both methods generally perform poorly when restricted toone-percent tuning, whereas several algorithmic mitigations designed tomaintain network plasticity perform surprising well. In addition, we find thatproperties designed to measure the network's ability to learn continuallyindeed correlate with performance under one-percent tuning.</description><author>Golnaz Mesbahi, Olya Mastikhina, Parham Mohammad Panahi, Martha White, Adam White</author><pubDate>Tue, 30 Apr 2024 17:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02113v2</guid></item><item><title>SwipeGANSpace: Swipe-to-Compare Image Generation via Efficient Latent Space Exploration</title><link>http://arxiv.org/abs/2404.19693v1</link><description>Generating preferred images using generative adversarial networks (GANs) ischallenging owing to the high-dimensional nature of latent space. In thisstudy, we propose a novel approach that uses simple user-swipe interactions togenerate preferred images for users. To effectively explore the latent spacewith only swipe interactions, we apply principal component analysis to thelatent space of the StyleGAN, creating meaningful subspaces. We use amulti-armed bandit algorithm to decide the dimensions to explore, focusing onthe preferences of the user. Experiments show that our method is more efficientin generating preferred images than the baseline methods. Furthermore, changesin preferred images during image generation or the display of entirelydifferent image styles were observed to provide new inspirations, subsequentlyaltering user preferences. This highlights the dynamic nature of userpreferences, which our proposed approach recognizes and enhances.</description><author>Yuto Nakashima, Mingzhe Yang, Yukino Baba</author><pubDate>Tue, 30 Apr 2024 17:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19693v1</guid></item><item><title>Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis</title><link>http://arxiv.org/abs/2401.17159v2</link><description>Modern SMT solvers, such as Z3, offer user-controllable strategies, enablingusers to tailor solving strategies for their unique set of instances, thusdramatically enhancing solver performance for their use case. However, thisapproach of strategy customization presents a significant challenge:handcrafting an optimized strategy for a class of SMT instances remains acomplex and demanding task for both solver developers and users alike. In this paper, we address this problem of automatic SMT strategy synthesisvia a novel Monte Carlo Tree Search (MCTS) based method. Our method treatsstrategy synthesis as a sequential decision-making process, whose search treecorresponds to the strategy space, and employs MCTS to navigate this vastsearch space. The key innovations that enable our method to identify effectivestrategies, while keeping costs low, are the ideas of layered and staged MCTSsearch. These novel heuristics allow for a deeper and more efficientexploration of the strategy space, enabling us to synthesize more effectivestrategies than the default ones in state-of-the-art (SOTA) SMT solvers. Weimplement our method, dubbed Z3alpha, as part of the Z3 SMT solver. Throughextensive evaluations across six important SMT logics, Z3alpha demonstratessuperior performance compared to the SOTA synthesis tool FastSMT, the defaultZ3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challengingQF_BV benchmark set, Z3alpha solves 42.7% more instances than the defaultstrategy in the Z3 SMT solver.</description><author>Zhengyang Lu, Stefan Siemer, Piyush Jha, Joel Day, Florin Manea, Vijay Ganesh</author><pubDate>Tue, 30 Apr 2024 17:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17159v2</guid></item><item><title>Continuum limit of $p$-biharmonic equations on graphs</title><link>http://arxiv.org/abs/2404.19689v1</link><description>This paper studies the $p$-biharmonic equation on graphs, which arises inpoint cloud processing and can be interpreted as a natural extension of thegraph $p$-Laplacian from the perspective of hypergraph. The asymptotic behaviorof the solution is investigated when the random geometric graph is consideredand the number of data points goes to infinity. We show that the continuumlimit is an appropriately weighted $p$-biharmonic equation with homogeneousNeumann boundary conditions. The result relies on the uniform $L^p$ estimatesfor solutions and gradients of nonlocal and graph Poisson equations. The$L^\infty$ estimates of solutions are also obtained as a byproduct.</description><author>Kehan Shi, Martin Burger</author><pubDate>Tue, 30 Apr 2024 17:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19689v1</guid></item><item><title>Neural Controlled Differential Equations with Quantum Hidden Evolutions</title><link>http://arxiv.org/abs/2404.19673v1</link><description>We introduce a class of neural controlled differential equation inspired byquantum mechanics. Neural quantum controlled differential equations (NQDEs)model the dynamics by analogue of the Schr\"{o}dinger equation. Specifically,the hidden state represents the wave function, and its collapse leads to aninterpretation of the classification probability. We implement and compare theresults of four variants of NQDEs on a toy spiral classification problem.</description><author>Lingyi Yang, Zhen Shao</author><pubDate>Tue, 30 Apr 2024 17:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19673v1</guid></item><item><title>How good are Large Language Models on African Languages?</title><link>http://arxiv.org/abs/2311.07978v2</link><description>Recent advancements in natural language processing have led to theproliferation of large language models (LLMs). These models have been shown toyield good performance, using in-context learning, even on tasks and languagesthey are not trained on. However, their performance on African languages islargely understudied relative to high-resource languages. We present ananalysis of four popular large language models (mT0, Aya, LLaMa 2, and GPT-4)on six tasks (topic classification, sentiment classification, machinetranslation, summarization, question answering, and named entity recognition)across 60 African languages, spanning different language families andgeographical regions. Our results suggest that all LLMs produce lowerperformance for African languages, and there is a large gap in performancecompared to high-resource languages (such as English) for most tasks. We findthat GPT-4 has an average to good performance on classification tasks, yet itsperformance on generative tasks such as machine translation and summarizationis significantly lacking. Surprisingly, we find that mT0 had the best overallperformance for cross-lingual QA, better than the state-of-the-art supervisedmodel (i.e. fine-tuned mT5) and GPT-4 on African languages. Similarly, we findthe recent Aya model to have comparable result to mT0 in almost all tasksexcept for topic classification where it outperform mT0. Overall, LLaMa 2showed the worst performance, which we believe is due to its English andcode-centric~(around 98%) pre-training corpus. Our findings confirms thatperformance on African languages continues to remain a hurdle for the currentLLMs, underscoring the need for additional efforts to close this gap.</description><author>Jessica Ojo, Kelechi Ogueji, Pontus Stenetorp, David Ifeoluwa Adelani</author><pubDate>Tue, 30 Apr 2024 17:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07978v2</guid></item><item><title>Beyond MOS: Subjective Image Quality Score Preprocessing Method Based on Perceptual Similarity</title><link>http://arxiv.org/abs/2404.19666v1</link><description>Image quality assessment often relies on raw opinion scores provided bysubjects in subjective experiments, which can be noisy and unreliable. Toaddress this issue, postprocessing procedures such as ITU-R BT.500, ITU-TP.910, and ITU-T P.913 have been standardized to clean up the original opinionscores. These methods use annotator-based statistical priors, but they do nottake into account extensive information about the image itself, which limitstheir performance in less annotated scenarios. Generally speaking, imagequality datasets usually contain similar scenes or distortions, and it isinevitable for subjects to compare images to score a reasonable score whenscoring. Therefore, In this paper, we proposed Subjective Image Quality ScorePreprocessing Method perceptual similarity Subjective Preprocessing (PSP),which exploit the perceptual similarity between images to alleviate subjectivebias in less annotated scenarios. Specifically, we model subjective scoring asa conditional probability model based on perceptual similarity with previouslyscored images, called subconscious reference scoring. The reference images arestored by a neighbor dictionary, which is obtained by a normalized vectordot-product based nearest neighbor search of the images' perceptual depthfeatures. Then the preprocessed score is updated by the exponential movingaverage (EMA) of the subconscious reference scoring, called similarityregularized EMA. Our experiments on multiple datasets (LIVE, TID2013, CID2013)show that this method can effectively remove the bias of the subjective scores.Additionally, Experiments prove that the Preprocesed dataset can improve theperformance of downstream IQA tasks very well.</description><author>Lei Wang, Desen Yuan</author><pubDate>Tue, 30 Apr 2024 17:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19666v1</guid></item><item><title>ATOMMIC: An Advanced Toolbox for Multitask Medical Imaging Consistency to facilitate Artificial Intelligence applications from acquisition to analysis in Magnetic Resonance Imaging</title><link>http://arxiv.org/abs/2404.19665v1</link><description>AI is revolutionizing MRI along the acquisition and processing chain.Advanced AI frameworks have been developed to apply AI in various successivetasks, such as image reconstruction, quantitative parameter map estimation, andimage segmentation. Existing frameworks are often designed to perform tasksindependently or are focused on specific models or datasets, limitinggeneralization. We introduce ATOMMIC, an open-source toolbox that streamlinesAI applications for accelerated MRI reconstruction and analysis. ATOMMICimplements several tasks using DL networks and enables MultiTask Learning (MTL)to perform related tasks integrated, targeting generalization in the MRIdomain. We first review the current state of AI frameworks for MRI through acomprehensive literature search and by parsing 12,479 GitHub repositories. Webenchmark 25 DL models on eight publicly available datasets to present distinctapplications of ATOMMIC on accelerated MRI reconstruction, image segmentation,quantitative parameter map estimation, and joint accelerated MRI reconstructionand image segmentation utilizing MTL. Our findings demonstrate that ATOMMIC isthe only MTL framework with harmonized complex-valued and real-valued datasupport. Evaluations on single tasks show that physics-based models, whichenforce data consistency by leveraging the physical properties of MRI,outperform other models in reconstructing highly accelerated acquisitions.Physics-based models that produce high reconstruction quality can accuratelyestimate quantitative parameter maps. When high-performing reconstructionmodels are combined with robust segmentation networks utilizing MTL,performance is improved in both tasks. ATOMMIC facilitates MRI reconstructionand analysis by standardizing workflows, enhancing data interoperability,integrating unique features like MTL, and effectively benchmarking DL models.</description><author>Dimitrios Karkalousos, Ivana Išgum, Henk A. Marquering, Matthan W. A. Caan</author><pubDate>Tue, 30 Apr 2024 17:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19665v1</guid></item><item><title>Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation</title><link>http://arxiv.org/abs/2404.06362v2</link><description>The Segment Anything Model (SAM) and CLIP are remarkable vision foundationmodels (VFMs). SAM, a prompt driven segmentation model, excels in segmentationtasks across diverse domains, while CLIP is renowned for its zero shotrecognition capabilities. However, their unified potential has not yet beenexplored in medical image segmentation. To adapt SAM to medical imaging,existing methods primarily rely on tuning strategies that require extensivedata or prior prompts tailored to the specific task, making it particularlychallenging when only a limited number of data samples are available. This workpresents an in depth exploration of integrating SAM and CLIP into a unifiedframework for medical image segmentation. Specifically, we propose a simpleunified framework, SaLIP, for organ segmentation. Initially, SAM is used forpart based segmentation within the image, followed by CLIP to retrieve the maskcorresponding to the region of interest (ROI) from the pool of SAM generatedmasks. Finally, SAM is prompted by the retrieved ROI to segment a specificorgan. Thus, SaLIP is training and fine tuning free and does not rely on domainexpertise or labeled data for prompt engineering. Our method shows substantialenhancements in zero shot segmentation, showcasing notable improvements in DICEscores across diverse segmentation tasks like brain (63.46%), lung (50.11%),and fetal head (30.82%), when compared to un prompted SAM. Code and textprompts are available at: https://github.com/aleemsidra/SaLIP.</description><author>Sidra Aleem, Fangyijie Wang, Mayug Maniparambil, Eric Arazo, Julia Dietlmeier, Guenole Silvestre, Kathleen Curran, Noel E. O'Connor, Suzanne Little</author><pubDate>Tue, 30 Apr 2024 16:58:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06362v2</guid></item><item><title>Towards Generalist Robot Learning from Internet Video: A Survey</title><link>http://arxiv.org/abs/2404.19664v1</link><description>This survey presents an overview of methods for learning from video (LfV) inthe context of reinforcement learning (RL) and robotics. We focus on methodscapable of scaling to large internet video datasets and, in the process,extracting foundational knowledge about the world's dynamics and physical humanbehaviour. Such methods hold great promise for developing general-purposerobots. We open with an overview of fundamental concepts relevant to theLfV-for-robotics setting. This includes a discussion of the exciting benefitsLfV methods can offer (e.g., improved generalization beyond the available robotdata) and commentary on key LfV challenges (e.g., challenges related to missinginformation in video and LfV distribution shifts). Our literature review beginswith an analysis of video foundation model techniques that can extractknowledge from large, heterogeneous video datasets. Next, we review methodsthat specifically leverage video data for robot learning. Here, we categorisework according to which RL knowledge modality benefits from the use of videodata. We additionally highlight techniques for mitigating LfV challenges,including reviewing action representations that address the issue of missingaction labels in video. Finally, we examine LfV datasets and benchmarks, before concluding the surveyby discussing challenges and opportunities in LfV. Here, we advocate forscalable approaches that can leverage the full range of available data and thattarget the key benefits of LfV. Overall, we hope this survey will serve as acomprehensive reference for the emerging field of LfV, catalysing furtherresearch in the area, and ultimately facilitating progress towards obtaininggeneral-purpose robots.</description><author>Robert McCarthy, Daniel C. H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, Zhibin Li</author><pubDate>Tue, 30 Apr 2024 16:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19664v1</guid></item><item><title>PCA for Point Processes</title><link>http://arxiv.org/abs/2404.19661v1</link><description>We introduce a novel statistical framework for the analysis of replicatedpoint processes that allows for the study of point pattern variability at apopulation level. By treating point process realizations as random measures, weadopt a functional analysis perspective and propose a form of functionalPrincipal Component Analysis (fPCA) for point processes. The originality of ourmethod is to base our analysis on the cumulative mass functions of the randommeasures which gives us a direct and interpretable analysis. Key theoreticalcontributions include establishing a Karhunen-Lo\`{e}ve expansion for therandom measures and a Mercer Theorem for covariance measures. We establishconvergence in a strong sense, and introduce the concept of principal measures,which can be seen as latent processes governing the dynamics of the observedpoint patterns. We propose an easy-to-implement estimation strategy ofeigenelements for which parametric rates are achieved. We fully characterizethe solutions of our approach to Poisson and Hawkes processes and validate ourmethodology via simulations and diverse applications in seismology, single-cellbiology and neurosiences, demonstrating its versatility and effectiveness. Ourmethod is implemented in the pppca R-package.</description><author>Franck Picard, Vincent Rivoirard, Angelina Roche, Victor Panaretos</author><pubDate>Tue, 30 Apr 2024 16:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19661v1</guid></item><item><title>Towards Scenario- and Capability-Driven Dataset Development and Evaluation: An Approach in the Context of Mapless Automated Driving</title><link>http://arxiv.org/abs/2404.19656v1</link><description>The foundational role of datasets in defining the capabilities of deeplearning models has led to their rapid proliferation. At the same time,published research focusing on the process of dataset development forenvironment perception in automated driving has been scarce, thereby reducingthe applicability of openly available datasets and impeding the development ofeffective environment perception systems. Sensor-based, mapless automateddriving is one of the contexts where this limitation is evident. Whileleveraging real-time sensor data, instead of pre-defined HD maps promisesenhanced adaptability and safety by effectively navigating unexpectedenvironmental changes, it also increases the demands on the scope andcomplexity of the information provided by the perception system. To address these challenges, we propose a scenario- and capability-basedapproach to dataset development. Grounded in the principles of ISO 21448(safety of the intended functionality, SOTIF), extended by ISO/TR 4804, ourapproach facilitates the structured derivation of dataset requirements. Thisnot only aids in the development of meaningful new datasets but also enablesthe effective comparison of existing ones. Applying this methodology to a broadrange of existing lane detection datasets, we identify significant limitationsin current datasets, particularly in terms of real-world applicability, a lackof labeling of critical features, and an absence of comprehensive informationfor complex driving maneuvers.</description><author>Felix Grün, Marcus Nolte, Markus Maurer</author><pubDate>Tue, 30 Apr 2024 16:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19656v1</guid></item><item><title>Can Query Expansion Improve Generalization of Strong Cross-Encoder Rankers?</title><link>http://arxiv.org/abs/2311.09175v2</link><description>Query expansion has been widely used to improve the search results offirst-stage retrievers, yet its influence on second-stage, cross-encoderrankers remains under-explored. A recent work of Weller et al. [44] shows thatcurrent expansion techniques benefit weaker models such as DPR and BM25 butharm stronger rankers such as MonoT5. In this paper, we re-examine thisconclusion and raise the following question: Can query expansion improvegeneralization of strong cross-encoder rankers? To answer this question, wefirst apply popular query expansion methods to state-of-the-art cross-encoderrankers and verify the deteriorated zero-shot performance. We identify twovital steps for cross-encoders in the experiment: high-quality keywordgeneration and minimal-disruptive query modification. We show that it ispossible to improve the generalization of a strong neural ranker, by promptengineering and aggregating the ranking results of each expanded query viafusion. Specifically, we first call an instruction-following language model togenerate keywords through a reasoning chain. Leveraging self-consistency andreciprocal rank weighting, we further combine the ranking results of eachexpanded query dynamically. Experiments on BEIR and TREC Deep Learning2019/2020 show that the nDCG@10 scores of both MonoT5 and RankT5 followingthese steps are improved, which points out a direction for applying queryexpansion to strong cross-encoder rankers.</description><author>Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky</author><pubDate>Tue, 30 Apr 2024 16:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09175v2</guid></item><item><title>Sparse Interaction Neighborhood Selection for Markov Random Fields via Reversible Jump and Pseudoposteriors</title><link>http://arxiv.org/abs/2204.05933v4</link><description>We consider the problem of estimating the interacting neighborhood of aMarkov Random Field model with finite support and homogeneous pairwiseinteractions based on relative positions of a two-dimensional lattice. Using aBayesian framework, we propose a Reversible Jump Monte Carlo Markov Chainalgorithm that jumps across subsets of a maximal range neighborhood, allowingus to perform model selection based on a marginal pseudoposterior distributionof models. To show the strength of our proposed methodology we perform asimulation study and apply it to a real dataset from a discrete texture imageanalysis.</description><author>Victor Freguglia, Nancy Lopes Garcia</author><pubDate>Tue, 30 Apr 2024 16:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05933v4</guid></item><item><title>Enhancing Lip Reading with Multi-Scale Video and Multi-Encoder</title><link>http://arxiv.org/abs/2404.05466v2</link><description>Automatic lip-reading (ALR) aims to automatically transcribe spoken contentfrom a speaker's silent lip motion captured in video. Current mainstreamlip-reading approaches only use a single visual encoder to model input videosof a single scale. In this paper, we propose to enhance lip-reading byincorporating multi-scale video data and multi-encoder. Specifically, we firstpropose a novel multi-scale lip motion extraction algorithm based on the sizeof the speaker's face and an Enhanced ResNet3D visual front-end (VFE) toextract lip features at different scales. For the multi-encoder, in addition tothe mainstream Transformer and Conformer, we also incorporate the recentlyproposed Branchformer and E-Branchformer as visual encoders. In theexperiments, we explore the influence of different video data scales andencoders on ALR system performance and fuse the texts transcribed by all ALRsystems using recognizer output voting error reduction (ROVER). Finally, ourproposed approach placed second in the ICME 2024 ChatCLR Challenge Task 2, witha 21.52% reduction in character error rate (CER) compared to the officialbaseline on the evaluation set.</description><author>He Wang, Pengcheng Guo, Xucheng Wan, Huan Zhou, Lei Xie</author><pubDate>Tue, 30 Apr 2024 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05466v2</guid></item><item><title>Masked Multi-Query Slot Attention for Unsupervised Object Discovery</title><link>http://arxiv.org/abs/2404.19654v1</link><description>Unsupervised object discovery is becoming an essential line of research fortackling recognition problems that require decomposing an image into entities,such as semantic segmentation and object detection. Recently, object-centricmethods that leverage self-supervision have gained popularity, due to theirsimplicity and adaptability to different settings and conditions. However,those methods do not exploit effective techniques already employed in modernself-supervised approaches. In this work, we consider an object-centricapproach in which DINO ViT features are reconstructed via a set of queriedrepresentations called slots. Based on that, we propose a masking scheme oninput features that selectively disregards the background regions, inducing ourmodel to focus more on salient objects during the reconstruction phase.Moreover, we extend the slot attention to a multi-query approach, allowing themodel to learn multiple sets of slots, producing more stable masks. Duringtraining, these multiple sets of slots are learned independently while, at testtime, these sets are merged through Hungarian matching to obtain the finalslots. Our experimental results and ablations on the PASCAL-VOC 2012 datasetshow the importance of each component and highlight how their combinationconsistently improves object localization. Our source code is available at:https://github.com/rishavpramanik/maskedmultiqueryslot</description><author>Rishav Pramanik, José-Fabian Villa-Vásquez, Marco Pedersoli</author><pubDate>Tue, 30 Apr 2024 16:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19654v1</guid></item><item><title>VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization</title><link>http://arxiv.org/abs/2404.19652v1</link><description>Text spotting, a task involving the extraction of textual information fromimage or video sequences, faces challenges in cross-domain adaption, such asimage-to-image and image-to-video generalization. In this paper, we introduce anew method, termed VimTS, which enhances the generalization ability of themodel by achieving better synergy among different tasks. Typically, we proposea Prompt Queries Generation Module and a Tasks-aware Adapter to effectivelyconvert the original single-task model into a multi-task model suitable forboth image and video scenarios with minimal additional parameters. The PromptQueries Generation Module facilitates explicit interaction between differenttasks, while the Tasks-aware Adapter helps the model dynamically learn suitablefeatures for each task. Additionally, to further enable the model to learntemporal information at a lower cost, we propose a synthetic video text dataset(VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm.Notably, our method outperforms the state-of-the-art method by an average of2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, andTT-to-CTW1500. For video-level cross-domain adaption, our method even surpassesthe previous end-to-end video spotting method in ICDAR2015 video and DSText v2by an average of 5.5% on the MOTA metric, using only image-level data. Wefurther demonstrate that existing Large Multimodal Models exhibit limitationsin generating cross-domain scene text spotting, in contrast to our VimTS modelwhich requires significantly fewer parameters and data. The code and datasetswill be made available at the https://VimTextSpotter.github.io.</description><author>Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai</author><pubDate>Tue, 30 Apr 2024 16:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19652v1</guid></item><item><title>Provably Robust Conformal Prediction with Improved Efficiency</title><link>http://arxiv.org/abs/2404.19651v1</link><description>Conformal prediction is a powerful tool to generate uncertainty sets withguaranteed coverage using any predictive model, under the assumption that thetraining and test data are i.i.d.. Recently, it has been shown that adversarialexamples are able to manipulate conformal methods to construct prediction setswith invalid coverage rates, as the i.i.d. assumption is violated. To addressthis issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), wasfirst proposed to certify the robustness of conformal prediction methods toadversarial noise. However, RSCP has two major limitations: (i) its robustnessguarantee is flawed when used in practice and (ii) it tends to produce largeuncertainty sets. To address these limitations, we first propose a novelframework called RSCP+ to provide provable robustness guarantee in evaluation,which fixes the issues in the original RSCP method. Next, we propose two novelmethods, Post-Training Transformation (PTT) and Robust Conformal Training(RCT), to effectively reduce prediction set size with little computationoverhead. Experimental results in CIFAR10, CIFAR100, and ImageNet suggest thebaseline method only yields trivial predictions including full label set, whileour methods could boost the efficiency by up to $4.36\times$, $5.46\times$, and$16.9\times$ respectively and provide practical robustness guarantee. Our codesare available athttps://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.</description><author>Ge Yan, Yaniv Romano, Tsui-Wei Weng</author><pubDate>Tue, 30 Apr 2024 16:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19651v1</guid></item><item><title>MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation</title><link>http://arxiv.org/abs/2404.19644v1</link><description>Out-of-distribution (OOD) problems in few-shot classification (FSC) occurwhen novel classes sampled from testing distributions differ from base classesdrawn from training distributions, which considerably degrades the performanceof deep learning models deployed in real-world applications. Recent studiessuggest that the OOD problems in FSC mainly including: (a) cross-domainfew-shot classification (CD-FSC) and (b) spurious-correlation few-shotclassification (SC-FSC). Specifically, CD-FSC occurs when a classifier learnstransferring knowledge from base classes drawn from seen training distributionsbut recognizes novel classes sampled from unseen testing distributions. Incontrast, SC-FSC arises when a classifier relies on non-causal features (orcontexts) that happen to be correlated with the labels (or concepts) in baseclasses but such relationships no longer hold during the model deployment.Despite CD-FSC has been extensively studied, SC-FSC remains understudied due tolack of the corresponding evaluation benchmarks. To this end, we present MetaConcept Context (MetaCoCo), a benchmark with spurious-correlation shiftscollected from real-world scenarios. Moreover, to quantify the extent ofspurious-correlation shifts of the presented MetaCoCo, we further propose ametric by using CLIP as a pre-trained vision-language model. Extensiveexperiments on the proposed benchmark are performed to evaluate thestate-of-the-art methods in FSC, cross-domain shifts, and self-supervisedlearning. The experimental results show that the performance of the existingmethods degrades significantly in the presence of spurious-correlation shifts.We open-source all codes of our benchmark and hope that the proposed MetaCoCocan facilitate future research on spurious-correlation shifts problems in FSC.The code is available at: https://github.com/remiMZ/MetaCoCo-ICLR24.</description><author>Min Zhang, Haoxuan Li, Fei Wu, Kun Kuang</author><pubDate>Tue, 30 Apr 2024 16:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19644v1</guid></item><item><title>ESP-Zero: Unsupervised enhancement of zero-shot classification for Extremely Sparse Point cloud</title><link>http://arxiv.org/abs/2404.19639v1</link><description>In recent years, zero-shot learning has attracted the focus of manyresearchers, due to its flexibility and generality. Many approaches have beenproposed to achieve the zero-shot classification of the point clouds for 3Dobject understanding, following the schema of CLIP. However, in the real world,the point clouds could be extremely sparse, dramatically limiting theeffectiveness of the 3D point cloud encoders, and resulting in the misalignmentof point cloud features and text embeddings. To the point cloud encoders to fitthe extremely sparse point clouds without re-running the pre-training procedurewhich could be time-consuming and expensive, in this work, we propose anunsupervised model adaptation approach to enhance the point cloud encoder forthe extremely sparse point clouds. We propose a novel fused-cross attentionlayer that expands the pre-trained self-attention layer with additionallearnable tokens and attention blocks, which effectively modifies the pointcloud features while maintaining the alignment between point cloud features andtext embeddings. We also propose a complementary learning-basedself-distillation schema that encourages the modified features to be pulledapart from the irrelevant text embeddings without overfitting the feature spaceto the observed text embeddings. Extensive experiments demonstrate that theproposed approach effectively increases the zero-shot capability on extremelysparse point clouds, and overwhelms other state-of-the-art model adaptationapproaches.</description><author>Jiayi Han, Zidi Cao, Weibo Zheng, Xiangguo Zhou, Xiangjian He, Yuanfang Zhang, Daisen Wei</author><pubDate>Tue, 30 Apr 2024 16:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19639v1</guid></item><item><title>ShadowMaskFormer: Mask Augmented Patch Embeddings for Shadow Removal</title><link>http://arxiv.org/abs/2404.18433v2</link><description>Transformer recently emerged as the de facto model for computer vision tasksand has also been successfully applied to shadow removal. However, theseexisting methods heavily rely on intricate modifications to the attentionmechanisms within the transformer blocks while using a generic patch embedding.As a result, it often leads to complex architectural designs requiringadditional computation resources. In this work, we aim to explore the efficacyof incorporating shadow information within the early processing stage.Accordingly, we propose a transformer-based framework with a novel patchembedding that is tailored for shadow removal, dubbed ShadowMaskFormer.Specifically, we present a simple and effective mask-augmented patch embeddingto integrate shadow information and promote the model's emphasis on acquiringknowledge for shadow regions. Extensive experiments conducted on the ISTD,ISTD+, and SRD benchmark datasets demonstrate the efficacy of our methodagainst state-of-the-art approaches while using fewer model parameters.</description><author>Zhuohao Li, Guoyang Xie, Guannan Jiang, Zhichao Lu</author><pubDate>Tue, 30 Apr 2024 16:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18433v2</guid></item><item><title>Risk-aware Meta-level Decision Making for Exploration Under Uncertainty</title><link>http://arxiv.org/abs/2209.05580v3</link><description>Robotic exploration of unknown environments is fundamentally a problem ofdecision making under uncertainty where the robot must account for uncertaintyin sensor measurements, localization, action execution, as well as many otherfactors. For large-scale exploration applications, autonomous systems mustovercome the challenges of sequentially deciding which areas of the environmentare valuable to explore while safely evaluating the risks associated withobstacles and hazardous terrain. In this work, we propose a risk-awaremeta-level decision making framework to balance the tradeoffs associated withlocal and global exploration. Meta-level decision making builds upon classicalhierarchical coverage planners by switching between local and global policieswith the overall objective of selecting the policy that is most likely tomaximize reward in a stochastic environment. We use information about theenvironment history, traversability risk, and kinodynamic constraints to reasonabout the probability of successful policy execution to switch between localand global policies. We have validated our solution in both simulation and on avariety of large-scale real world hardware tests. Our results show that bybalancing local and global exploration we are able to significantly explorelarge-scale environments more efficiently.</description><author>Joshua Ott, Sung-Kyun Kim, Amanda Bouman, Oriana Peltzer, Mamoru Sobue, Harrison Delecki, Mykel J. Kochenderfer, Joel Burdick, Ali-akbar Agha-mohammadi</author><pubDate>Tue, 30 Apr 2024 16:38:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05580v3</guid></item><item><title>On Training a Neural Network to Explain Binaries</title><link>http://arxiv.org/abs/2404.19631v1</link><description>In this work, we begin to investigate the possibility of training a deepneural network on the task of binary code understanding. Specifically, thenetwork would take, as input, features derived directly from binaries andoutput English descriptions of functionality to aid a reverse engineer ininvestigating the capabilities of a piece of closed-source software, be itmalicious or benign. Given recent success in applying large language models(generative AI) to the task of source code summarization, this seems apromising direction. However, in our initial survey of the available datasets,we found nothing of sufficiently high quality and volume to train these complexmodels. Instead, we build our own dataset derived from a capture of StackOverflow containing 1.1M entries. A major result of our work is a novel datasetevaluation method using the correlation between two distances on sample pairs:one distance in the embedding space of inputs and the other in the embeddingspace of outputs. Intuitively, if two samples have inputs close in the inputembedding space, their outputs should also be close in the output embeddingspace. We found this Embedding Distance Correlation (EDC) test to be highlydiagnostic, indicating that our collected dataset and several existingopen-source datasets are of low quality as the distances are not wellcorrelated. We proceed to explore the general applicability of EDC, applying itto a number of qualitatively known good datasets and a number of syntheticallyknown bad ones and found it to be a reliable indicator of dataset value.</description><author>Alexander Interrante-Grant, Andy Davis, Heather Preslier, Tim Leek</author><pubDate>Tue, 30 Apr 2024 16:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19631v1</guid></item><item><title>Analyzing and Exploring Training Recipes for Large-Scale Transformer-Based Weather Prediction</title><link>http://arxiv.org/abs/2404.19630v1</link><description>The rapid rise of deep learning (DL) in numerical weather prediction (NWP)has led to a proliferation of models which forecast atmospheric variables withcomparable or superior skill than traditional physics-based NWP. However, amongthese leading DL models, there is a wide variance in both the training settingsand architecture used. Further, the lack of thorough ablation studies makes ithard to discern which components are most critical to success. In this work, weshow that it is possible to attain high forecast skill even with relativelyoff-the-shelf architectures, simple training procedures, and moderate computebudgets. Specifically, we train a minimally modified SwinV2 transformer on ERA5data, and find that it attains superior forecast skill when compared againstIFS. We present some ablations on key aspects of the training pipeline,exploring different loss functions, model sizes and depths, and multi-stepfine-tuning to investigate their effect. We also examine the model performancewith metrics beyond the typical ACC and RMSE, and investigate how theperformance scales with model size.</description><author>Jared D. Willard, Peter Harrington, Shashank Subramanian, Ankur Mahesh, Travis A. O'Brien, William D. Collins</author><pubDate>Tue, 30 Apr 2024 16:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19630v1</guid></item><item><title>Conditional validity of heteroskedastic conformal regression</title><link>http://arxiv.org/abs/2309.08313v2</link><description>Conformal prediction, and split conformal prediction as a specificimplementation, offer a distribution-free approach to estimating predictionintervals with statistical guarantees. Recent work has shown that splitconformal prediction can produce state-of-the-art prediction intervals whenfocusing on marginal coverage, i.e. on a calibration dataset the methodproduces on average prediction intervals that contain the ground truth with apredefined coverage level. However, such intervals are often not adaptive,which can be problematic for regression problems with heteroskedastic noise.This paper tries to shed new light on how prediction intervals can beconstructed, using methods such as normalized and Mondrian conformalprediction, in such a way that they adapt to the heteroskedasticity of theunderlying process. Theoretical and experimental results are presented in whichthese methods are compared in a systematic way. In particular, it is shown howthe conditional validity of a chosen conformal predictor can be related to(implicit) assumptions about the data-generating distribution.</description><author>Nicolas Dewolf, Bernard De Baets, Willem Waegeman</author><pubDate>Tue, 30 Apr 2024 16:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08313v2</guid></item><item><title>Open Information Extraction from 2007 to 2022 -- A Survey</title><link>http://arxiv.org/abs/2208.08690v5</link><description>Open information extraction is an important NLP task that targets extractingstructured information from unstructured text without limitations on therelation type or the domain of the text. This survey paper covers openinformation extraction technologies from 2007 to 2022 with a focus on newmodels not covered by previous surveys. We propose a new categorization methodfrom the source of information perspective to accommodate the development ofrecent OIE technologies. In addition, we summarize three major approaches basedon task settings as well as current popular datasets and model evaluationmetrics. Given the comprehensive review, several future directions are shownfrom datasets, source of information, output form, method, and evaluationmetric aspects.</description><author>Pai Liu, Wenyang Gao, Wenjie Dong, Songfang Huang, Yue Zhang</author><pubDate>Tue, 30 Apr 2024 16:27:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08690v5</guid></item><item><title>auto-sktime: Automated Time Series Forecasting</title><link>http://arxiv.org/abs/2312.08528v3</link><description>In today's data-driven landscape, time series forecasting is pivotal indecision-making across various sectors. Yet, the proliferation of more diversetime series data, coupled with the expanding landscape of available forecastingmethods, poses significant challenges for forecasters. To meet the growingdemand for efficient forecasting, we introduce auto-sktime, a novel frameworkfor automated time series forecasting. The proposed framework uses the power ofautomated machine learning (AutoML) techniques to automate the creation of theentire forecasting pipeline. The framework employs Bayesian optimization, toautomatically construct pipelines from statistical, machine learning (ML) anddeep neural network (DNN) models. Furthermore, we propose three essentialimprovements to adapt AutoML to time series data. First, pipeline templates toaccount for the different supported forecasting models. Second, a novelwarm-starting technique to start the optimization from prior optimization runs.Third, we adapt multi-fidelity optimizations to make them applicable to asearch space containing statistical, ML and DNN models. Experimental results on64 diverse real-world time series datasets demonstrate the effectiveness andefficiency of the framework, outperforming traditional methods while requiringminimal human involvement.</description><author>Marc-André Zöller, Marius Lindauer, Marco F. Huber</author><pubDate>Tue, 30 Apr 2024 16:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08528v3</guid></item><item><title>Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis</title><link>http://arxiv.org/abs/2404.19622v1</link><description>Although humans engaged in face-to-face conversation simultaneouslycommunicate both verbally and non-verbally, methods for joint and unifiedsynthesis of speech audio and co-speech 3D gesture motion from text are a newand emerging field. These technologies hold great promise for more human-like,efficient, expressive, and robust synthetic communication, but are currentlyheld back by the lack of suitably large datasets, as existing methods aretrained on parallel data from all constituent modalities. Inspired bystudent-teacher methods, we propose a straightforward solution to the datashortage, by simply synthesising additional training material. Specifically, weuse unimodal synthesis models trained on large datasets to create multimodal(but synthetic) parallel training data, and then pre-train a joint synthesismodel on that material. In addition, we propose a new synthesis architecturethat adds better and more controllable prosody modelling to thestate-of-the-art method in the field. Our results confirm that pre-training onlarge amounts of synthetic data improves the quality of both the speech and themotion synthesised by the multimodal model, with the proposed architectureyielding further benefits when pre-trained on the synthetic data. Seehttps://shivammehta25.github.io/MAGI/ for example output.</description><author>Shivam Mehta, Anna Deichler, Jim O'Regan, Birger Moëll, Jonas Beskow, Gustav Eje Henter, Simon Alexanderson</author><pubDate>Tue, 30 Apr 2024 16:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19622v1</guid></item><item><title>Just Say the Name: Online Continual Learning with Category Names Only via Data Generation</title><link>http://arxiv.org/abs/2403.10853v2</link><description>In real-world scenarios, extensive manual annotation for continual learningis impractical due to prohibitive costs. Although prior arts, influenced bylarge-scale webly supervised training, suggest leveraging web-scraped data incontinual learning, this poses challenges such as data imbalance, usagerestrictions, and privacy concerns. Addressing the risks of continual weblysupervised training, we present an online continual learning framework -Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses aset of generators G along with the learner. When encountering new concepts(i.e., classes), G-NoCL employs the novel sample complexity-guided dataensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) tooptimally sample training data from generated data. Through extensiveexperimentation, we demonstrate superior performance of DISCOBER in G-NoCLonline CL benchmarks, covering both In-Distribution (ID) andOut-of-Distribution (OOD) generalization evaluations, compared to naivegenerator-ensembling, web-supervised, and manually annotated data.</description><author>Minhyuk Seo, Diganta Misra, Seongwon Cho, Minjae Lee, Jonghyun Choi</author><pubDate>Tue, 30 Apr 2024 16:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10853v2</guid></item><item><title>Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference</title><link>http://arxiv.org/abs/2404.19620v1</link><description>Selection bias in recommender system arises from the recommendation processof system filtering and the interactive process of user selection. Manyprevious studies have focused on addressing selection bias to achieve unbiasedlearning of the prediction model, but ignore the fact that potential outcomesfor a given user-item pair may vary with the treatments assigned to otheruser-item pairs, named neighborhood effect. To fill the gap, this paperformally formulates the neighborhood effect as an interference problem from theperspective of causal inference and introduces a treatment representation tocapture the neighborhood effect. On this basis, we propose a novel ideal lossthat can be used to deal with selection bias in the presence of neighborhoodeffect. We further develop two new estimators for estimating the proposed idealloss. We theoretically establish the connection between the proposed andprevious debiasing methods ignoring the neighborhood effect, showing that theproposed methods can achieve unbiased learning when both selection bias andneighborhood effect are present, while the existing methods are biased.Extensive semi-synthetic and real-world experiments are conducted todemonstrate the effectiveness of the proposed methods.</description><author>Haoxuan Li, Chunyuan Zheng, Sihao Ding, Peng Wu, Zhi Geng, Fuli Feng, Xiangnan He</author><pubDate>Tue, 30 Apr 2024 16:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19620v1</guid></item><item><title>ProgDTD: Progressive Learned Image Compression with Double-Tail-Drop Training</title><link>http://arxiv.org/abs/2305.02145v2</link><description>Progressive compression allows images to start loading as low-resolutionversions, becoming clearer as more data is received. This increases userexperience when, for example, network connections are slow. Today, mostapproaches for image compression, both classical and learned ones, are designedto be non-progressive. This paper introduces ProgDTD, a training method thattransforms learned, non-progressive image compression approaches intoprogressive ones. The design of ProgDTD is based on the observation that theinformation stored within the bottleneck of a compression model commonly variesin importance. To create a progressive compression model, ProgDTD modifies thetraining steps to enforce the model to store the data in the bottleneck sortedby priority. We achieve progressive compression by transmitting the data inorder of its sorted index. ProgDTD is designed for CNN-based learned imagecompression models, does not need additional parameters, and has a customizablerange of progressiveness. For evaluation, we apply ProgDTDto the hyperpriormodel, one of the most common structures in learned image compression. Ourexperimental results show that ProgDTD performs comparably to itsnon-progressive counterparts and other state-of-the-art progressive models interms of MS-SSIM and accuracy.</description><author>Ali Hojjat, Janek Haberer, Olaf Landsiedel</author><pubDate>Tue, 30 Apr 2024 16:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02145v2</guid></item><item><title>SemiPL: A Semi-supervised Method for Event Sound Source Localization</title><link>http://arxiv.org/abs/2404.19615v1</link><description>In recent years, Event Sound Source Localization has been widely applied invarious fields. Recent works typically relying on the contrastive learningframework show impressive performance. However, all work is based on largerelatively simple datasets. It's also crucial to understand and analyze humanbehaviors (actions and interactions of people), voices, and sounds in chaoticevents in many applications, e.g., crowd management, and emergency responseservices. In this paper, we apply the existing model to a more complex dataset,explore the influence of parameters on the model, and propose a semi-supervisedimprovement method SemiPL. With the increase in data quantity and the influenceof label quality, self-supervised learning will be an unstoppable trend. Theexperiment shows that the parameter adjustment will positively affect theexisting model. In particular, SSPL achieved an improvement of 12.2% cIoU and0.56% AUC in Chaotic World compared to the results provided. The code isavailable at: https://github.com/ly245422/SSPL</description><author>Yue Li, Baiqiao Yin, Jinfu Liu, Jiajun Wen, Jiaying Lin, Mengyuan Liu</author><pubDate>Tue, 30 Apr 2024 16:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19615v1</guid></item><item><title>PANDAS: Prototype-based Novel Class Discovery and Detection</title><link>http://arxiv.org/abs/2402.17420v2</link><description>Object detectors are typically trained once and for all on a fixed set ofclasses. However, this closed-world assumption is unrealistic in practice, asnew classes will inevitably emerge after the detector is deployed in the wild.In this work, we look at ways to extend a detector trained for a set of baseclasses so it can i) spot the presence of novel classes, and ii) automaticallyenrich its repertoire to be able to detect those newly discovered classestogether with the base ones. We propose PANDAS, a method for novel classdiscovery and detection. It discovers clusters representing novel classes fromunlabeled data, and represents old and new classes with prototypes. Duringinference, a distance-based classifier uses these prototypes to assign a labelto each detected object instance. The simplicity of our method makes it widelyapplicable. We experimentally demonstrate the effectiveness of PANDAS on theVOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the stateof the art for this task while being computationally more affordable.</description><author>Tyler L. Hayes, César R. de Souza, Namil Kim, Jiwon Kim, Riccardo Volpi, Diane Larlus</author><pubDate>Tue, 30 Apr 2024 16:05:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17420v2</guid></item><item><title>Seeing Through the Clouds: Cloud Gap Imputation with Prithvi Foundation Model</title><link>http://arxiv.org/abs/2404.19609v1</link><description>Filling cloudy pixels in multispectral satellite imagery is essential foraccurate data analysis and downstream applications, especially for tasks whichrequire time series data. To address this issue, we compare the performance ofa foundational Vision Transformer (ViT) model with a baseline ConditionalGenerative Adversarial Network (CGAN) model for missing value imputation intime series of multispectral satellite imagery. We randomly mask time series ofsatellite images using real-world cloud masks and train each model toreconstruct the missing pixels. The ViT model is fine-tuned from a pretrainedmodel, while the CGAN is trained from scratch. Using quantitative evaluationmetrics such as structural similarity index and mean absolute error as well asqualitative visual analysis, we assess imputation accuracy and contextualpreservation.</description><author>Denys Godwin, Hanxi Li, Michael Cecil, Hamed Alemohammad</author><pubDate>Tue, 30 Apr 2024 16:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19609v1</guid></item><item><title>SimAC: A Simple Anti-Customization Method for Protecting Face Privacy against Text-to-Image Synthesis of Diffusion Models</title><link>http://arxiv.org/abs/2312.07865v2</link><description>Despite the success of diffusion-based customization methods on visualcontent creation, increasing concerns have been raised about such techniquesfrom both privacy and political perspectives. To tackle this issue, severalanti-customization methods have been proposed in very recent months,predominantly grounded in adversarial attacks. Unfortunately, most of thesemethods adopt straightforward designs, such as end-to-end optimization with afocus on adversarially maximizing the original training loss, therebyneglecting nuanced internal properties intrinsic to the diffusion model, andeven leading to ineffective optimization in some diffusion time steps.In thispaper, we strive to bridge this gap by undertaking a comprehensive explorationof these inherent properties, to boost the performance of currentanti-customization approaches. Two aspects of properties are investigated: 1)We examine the relationship between time step selection and the model'sperception in the frequency domain of images and find that lower time steps cangive much more contributions to adversarial noises. This inspires us to proposean adaptive greedy search for optimal time steps that seamlessly integrateswith existing anti-customization methods. 2) We scrutinize the roles offeatures at different layers during denoising and devise a sophisticatedfeature-based optimization framework for anti-customization.Experiments onfacial benchmarks demonstrate that our approach significantly increasesidentity disruption, thereby protecting user privacy and copyright. Our code isavailable at: https://github.com/somuchtome/SimAC.</description><author>Feifei Wang, Zhentao Tan, Tianyi Wei, Yue Wu, Qidong Huang</author><pubDate>Tue, 30 Apr 2024 16:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07865v2</guid></item><item><title>Data-Driven Invertible Neural Surrogates of Atmospheric Transmission</title><link>http://arxiv.org/abs/2404.19605v1</link><description>We present a framework for inferring an atmospheric transmission profile froma spectral scene. This framework leverages a lightweight, physics-basedsimulator that is automatically tuned - by virtue of autodifferentiation anddifferentiable programming - to construct a surrogate atmospheric profile tomodel the observed data. We demonstrate utility of the methodology by (i)performing atmospheric correction, (ii) recasting spectral data between variousmodalities (e.g. radiance and reflectance at the surface and at the sensor),and (iii) inferring atmospheric transmission profiles, such as absorbing bandsand their relative magnitudes.</description><author>James Koch, Brenda Forland, Bruce Bernacki, Timothy Doster, Tegan Emerson</author><pubDate>Tue, 30 Apr 2024 15:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19605v1</guid></item><item><title>X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models</title><link>http://arxiv.org/abs/2404.19604v1</link><description>In this work, we present X-Diffusion, a cross-sectional diffusion modeltailored for Magnetic Resonance Imaging (MRI) data. X-Diffusion is capable ofgenerating the entire MRI volume from just a single MRI slice or optionallyfrom few multiple slices, setting new benchmarks in the precision ofsynthesized MRIs from extremely sparse observations. The uniqueness lies in thenovel view-conditional training and inference of X-Diffusion on MRI volumes,allowing for generalized MRI learning. Our evaluations span both brain tumourMRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset.Utilizing the paired pre-registered Dual-energy X-ray Absorptiometry (DXA) andMRI modalities in the UK Biobank dataset, X-Diffusion is able to generatedetailed 3D MRI volume from a single full-body DXA. Remarkably, the resultantMRIs not only stand out in precision on unseen examples (surpassingstate-of-the-art results by large margins) but also flawlessly retain essentialfeatures of the original MRI, including tumour profiles, spine curvature, brainvolume, and beyond. Furthermore, the trained X-Diffusion model on the MRIdatasets attains a generalization capacity out-of-domain (e.g. generating kneeMRIs even though it is trained on brains). The code is available on the projectwebsite https://emmanuelleb985.github.io/XDiffusion/ .</description><author>Emmanuelle Bourigault, Abdullah Hamdi, Amir Jamaludin</author><pubDate>Tue, 30 Apr 2024 15:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19604v1</guid></item><item><title>SeaTurtleID2022: A long-span dataset for reliable sea turtle re-identification</title><link>http://arxiv.org/abs/2311.05524v2</link><description>This paper introduces the first public large-scale, long-span dataset withsea turtle photographs captured in the wild -- SeaTurtleID2022(https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022). The datasetcontains 8729 photographs of 438 unique individuals collected within 13 years,making it the longest-spanned dataset for animal re-identification. Allphotographs include various annotations, e.g., identity, encounter timestamp,and body parts segmentation masks. Instead of standard "random" splits, thedataset allows for two realistic and ecologically motivated splits: (i) atime-aware closed-set with training, validation, and test data from differentdays/years, and (ii) a time-aware open-set with new unknown individuals in testand validation sets. We show that time-aware splits are essential forbenchmarking re-identification methods, as random splits lead to performanceoverestimation. Furthermore, a baseline instance segmentation andre-identification performance over various body parts is provided. Finally, anend-to-end system for sea turtle re-identification is proposed and evaluated.The proposed system based on Hybrid Task Cascade for head instance segmentationand ArcFace-trained feature-extractor achieved an accuracy of 86.8%.</description><author>Lukáš Adam, Vojtěch Čermák, Kostas Papafitsoros, Lukáš Picek</author><pubDate>Tue, 30 Apr 2024 15:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05524v2</guid></item><item><title>Artificial Intelligence in Bone Metastasis Analysis: Current Advancements, Opportunities and Challenges</title><link>http://arxiv.org/abs/2404.19598v1</link><description>In recent years, Artificial Intelligence (AI) has been widely used inmedicine, particularly in the analysis of medical imaging, which has beendriven by advances in computer vision and deep learning methods. This isparticularly important in overcoming the challenges posed by diseases such asBone Metastases (BM), a common and complex malignancy of the bones. Indeed,there have been an increasing interest in developing Machine Learning (ML)techniques into oncologic imaging for BM analysis. In order to provide acomprehensive overview of the current state-of-the-art and advancements for BManalysis using artificial intelligence, this review is conducted with theaccordance with PRISMA guidelines. Firstly, this review highlights the clinicaland oncologic perspectives of BM and the used medical imaging modalities, withdiscussing their advantages and limitations. Then the review focuses on modernapproaches with considering the main BM analysis tasks, which includes:classification, detection and segmentation. The results analysis show that MLtechnologies can achieve promising performance for BM analysis and havesignificant potential to improve clinician efficiency and cope with time andcost limitations. Furthermore, there are requirements for further research tovalidate the clinical performance of ML tools and facilitate their integrationinto routine clinical practice.</description><author>Marwa Afnouch, Fares Bougourzi, Olfa Gaddour, Fadi Dornaika, Abdelmalik Taleb-Ahmed</author><pubDate>Tue, 30 Apr 2024 15:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19598v1</guid></item><item><title>FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition</title><link>http://arxiv.org/abs/2404.18848v2</link><description>Pre-trained Language Models (PLMs) have shown excellent performance onvarious downstream tasks after fine-tuning. Nevertheless, the escalatingconcerns surrounding user privacy have posed significant challenges tocentralized training reliant on extensive data collection. Federated learning,which only requires training on the clients and aggregates weights on theserver without sharing data, has emerged as a solution. However, thesubstantial parameter size of PLMs places a significant burden on thecomputational resources of client devices, while also leading to costlycommunication expenses. Introducing Parameter-Efficient Fine-Tuning(PEFT) intofederated learning can effectively address this problem. However, we observethat the non-IID data in federated learning leads to a gap in performancebetween the PEFT method and full parameter fine-tuning(FFT). To overcome this,we propose FeDeRA, an improvement over the Low-Rank Adaption(LoRA) method infederated learning. FeDeRA uses the same adapter module as LoRA. However, thedifference lies in FeDeRA's initialization of the adapter module by performingSingular Value Decomposition (SVD) on the pre-trained matrix and selecting itsprincipal components. We conducted extensive experiments, using RoBERTa andDeBERTaV3, on six datasets, comparing the methods including FFT and the otherthree different PEFT methods. FeDeRA outperforms all other PEFT methods and iscomparable to or even surpasses the performance of FFT method. We also deployedfederated learning on Jetson AGX Orin and compared the time required bydifferent methods to achieve the target accuracy on specific tasks. Compared toFFT, FeDeRA reduces the training time by 95.9\%, 97.9\%, 96.9\% and 97.3\%,96.5\%, 96.5\% respectively on three tasks using RoBERTa and DeBERTaV3. Theoverall experiments indicate that FeDeRA achieves good performance while alsomaintaining efficiency.</description><author>Yuxuan Yan, Shunpu Tang, Zhiguo Shi, Qianqian Yang</author><pubDate>Tue, 30 Apr 2024 15:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18848v2</guid></item><item><title>Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning</title><link>http://arxiv.org/abs/2404.19597v1</link><description>The implications of backdoor attacks on English-centric large language models(LLMs) have been widely examined - such attacks can be achieved by embeddingmalicious behaviors during training and activated under specific conditionsthat trigger malicious outputs. However, the impact of backdoor attacks onmultilingual models remains under-explored. Our research focuses oncross-lingual backdoor attacks against multilingual LLMs, particularlyinvestigating how poisoning the instruction-tuning data in one or two languagescan affect the outputs in languages whose instruction-tuning data was notpoisoned. Despite its simplicity, our empirical analysis reveals that ourmethod exhibits remarkable efficacy in models like mT5, BLOOM, andGPT-3.5-turbo, with high attack success rates, surpassing 95% in severallanguages across various scenarios. Alarmingly, our findings also indicate thatlarger models show increased susceptibility to transferable cross-lingualbackdoor attacks, which also applies to LLMs predominantly pre-trained onEnglish data, such as Llama2, Llama3, and Gemma. Moreover, our experiments showthat triggers can still work even after paraphrasing, and the backdoormechanism proves highly effective in cross-lingual response settings across 25languages, achieving an average attack success rate of 50%. Our study aims tohighlight the vulnerabilities and significant security risks present in currentmultilingual LLMs, underscoring the emergent need for targeted securitymeasures.</description><author>Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn</author><pubDate>Tue, 30 Apr 2024 15:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19597v1</guid></item><item><title>Debiased Collaborative Filtering with Kernel-Based Causal Balancing</title><link>http://arxiv.org/abs/2404.19596v1</link><description>Debiased collaborative filtering aims to learn an unbiased prediction modelby removing different biases in observational datasets. To solve this problem,one of the simple and effective methods is based on the propensity score, whichadjusts the observational sample distribution to the target one by reweightingobserved instances. Ideally, propensity scores should be learned with causalbalancing constraints. However, existing methods usually ignore suchconstraints or implement them with unreasonable approximations, which mayaffect the accuracy of the learned propensity scores. To bridge this gap, inthis paper, we first analyze the gaps between the causal balancing requirementsand existing methods such as learning the propensity with cross-entropy loss ormanually selecting functions to balance. Inspired by these gaps, we propose toapproximate the balancing functions in reproducing kernel Hilbert space anddemonstrate that, based on the universal property and representer theorem ofkernel functions, the causal balancing constraints can be better satisfied.Meanwhile, we propose an algorithm that adaptively balances the kernel functionand theoretically analyze the generalization error bound of our methods. Weconduct extensive experiments to demonstrate the effectiveness of our methods,and to promote this research direction, we have released our project athttps://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.</description><author>Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Peng Wu, Zhi Geng, Xu Chen, Peng Cui</author><pubDate>Tue, 30 Apr 2024 15:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19596v1</guid></item><item><title>Object Detection for Automated Coronary Artery Using Deep Learning</title><link>http://arxiv.org/abs/2312.12135v2</link><description>In the era of digital medicine, medical imaging serves as a widespreadtechnique for early disease detection, with a substantial volume of imagesbeing generated and stored daily in electronic patient records. X-rayangiography imaging is a standard and one of the most common methods forrapidly diagnosing coronary artery diseases. The notable achievements of recentdeep learning algorithms align with the increased use of electronic healthrecords and diagnostic imaging. Deep neural networks, leveraging abundant data,advanced algorithms, and powerful computational capabilities, prove highlyeffective in the analysis and interpretation of images. In this context, Objectdetection methods have become a promising approach, particularly throughconvolutional neural networks (CNN), streamlining medical image analysis byeliminating manual feature extraction. This allows for direct featureextraction from images, ensuring high accuracy in results. Therefore, in ourpaper, we utilized the object detection method on X-ray angiography images toprecisely identify the location of coronary artery stenosis. As a result, thismodel enables automatic and real-time detection of stenosis locations,assisting in the crucial and sensitive decision-making process for healthcareprofessionals.</description><author>Hadis Keshavarz, Hossein Sadr</author><pubDate>Tue, 30 Apr 2024 15:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12135v2</guid></item><item><title>Perceptual Constancy Constrained Single Opinion Score Calibration for Image Quality Assessment</title><link>http://arxiv.org/abs/2404.19595v1</link><description>In this paper, we propose a highly efficient method to estimate an image'smean opinion score (MOS) from a single opinion score (SOS). Assuming that eachSOS is the observed sample of a normal distribution and the MOS is its unknownexpectation, the MOS inference is formulated as a maximum likelihood estimationproblem, where the perceptual correlation of pairwise images is considered inmodeling the likelihood of SOS. More specifically, by means of thequality-aware representations learned from the self-supervised backbone, weintroduce a learnable relative quality measure to predict the MOS differencebetween two images. Then, the current image's maximum likelihood estimationtowards MOS is represented by the sum of another reference image's estimatedMOS and their relative quality. Ideally, no matter which image is selected asthe reference, the MOS of the current image should remain unchanged, which istermed perceptual cons tancy constrained calibration (PC3). Finally, wealternatively optimize the relative quality measure's parameter and the currentimage's estimated MOS via backpropagation and Newton's method respectively.Experiments show that the proposed method is efficient in calibrating thebiased SOS and significantly improves IQA model learning when only SOSs areavailable.</description><author>Lei Wang, Desen Yuan</author><pubDate>Tue, 30 Apr 2024 15:42:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19595v1</guid></item><item><title>Improving Language Model Reasoning with Self-motivated Learning</title><link>http://arxiv.org/abs/2404.07017v3</link><description>Large-scale high-quality training data is important for improving theperformance of models. After trained with data that has rationales (reasoningsteps), models gain reasoning capability. However, the dataset withhigh-quality rationales is relatively scarce due to the high annotation cost.To address this issue, we propose \textit{Self-motivated Learning} framework.The framework motivates the model itself to automatically generate rationaleson existing datasets. Based on the inherent rank from correctness acrossmultiple rationales, the model learns to generate better rationales, leading tohigher reasoning capability. Specifically, we train a reward model with therank to evaluate the quality of rationales, and improve the performance ofreasoning through reinforcement learning. Experiment results of Llama2 7B onmultiple reasoning datasets show that our method significantly improves thereasoning ability of models, even outperforming text-davinci-002 in somedatasets.</description><author>Yunlong Feng, Yang Xu, Libo Qin, Yasheng Wang, Wanxiang Che</author><pubDate>Tue, 30 Apr 2024 15:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07017v3</guid></item><item><title>Fast and Accurate Unknown Object Instance Segmentation through Error-Informed Refinement</title><link>http://arxiv.org/abs/2306.16132v2</link><description>Accurate perception of unknown objects is essential for autonomous robots,particularly when manipulating novel items in unstructured environments.However, existing unknown object instance segmentation (UOIS) methods oftenhave over-segmentation and under-segmentation problems, resulting in inaccurateinstance boundaries and failures in subsequent robotic tasks such as graspingand placement. To address this challenge, this article introduces INSTA-BEER, afast and accurate model-agnostic refinement method that enhances the UOISperformance. The model adopts an error-informed refinement approach, whichfirst predicts pixel-wise errors in the initial segmentation and then refinesthe segmentation guided by these error estimates. We introduce the quad-metricboundary error, which quantifies pixel-wise true positives, true negatives,false positives, and false negatives at the boundaries of object instances,effectively capturing both fine-grained and instance-level segmentation errors.Additionally, the Error Guidance Fusion (EGF) module explicitly integrateserror information into the refinement process, further improving segmentationquality. In comprehensive evaluations conducted on three widely used benchmarkdatasets, INSTA-BEER outperformed state-of-the-art models in both accuracy andinference time. Moreover, a real-world robotic experiment demonstrated thepractical applicability of our method in improving the performance of targetobject grasping tasks in cluttered environments.</description><author>Seunghyeok Back, Sangbeom Lee, Kangmin Kim, Joosoon Lee, Sungho Shin, Jemo Maeng, Kyoobin Lee</author><pubDate>Tue, 30 Apr 2024 15:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16132v2</guid></item><item><title>Contrastive Preference Learning: Learning from Human Feedback without RL</title><link>http://arxiv.org/abs/2310.13639v3</link><description>Reinforcement Learning from Human Feedback (RLHF) has emerged as a popularparadigm for aligning models with human intent. Typically RLHF algorithmsoperate in two phases: first, use human preferences to learn a reward functionand second, align the model by optimizing the learned reward via reinforcementlearning (RL). This paradigm assumes that human preferences are distributedaccording to reward, but recent work suggests that they instead follow theregret under the user's optimal policy. Thus, learning a reward function fromfeedback is not only based on a flawed assumption of human preference, but alsoleads to unwieldy optimization challenges that stem from policy gradients orbootstrapping in the RL phase. Because of these optimization challenges,contemporary RLHF methods restrict themselves to contextual bandit settings(e.g., as in large language models) or limit observation dimensionality (e.g.,state-based robotics). We overcome these limitations by introducing a newfamily of algorithms for optimizing behavior from human feedback using theregret-based model of human preferences. Using the principle of maximumentropy, we derive Contrastive Preference Learning (CPL), an algorithm forlearning optimal policies from preferences without learning reward functions,circumventing the need for RL. CPL is fully off-policy, uses only a simplecontrastive objective, and can be applied to arbitrary MDPs. This enables CPLto elegantly scale to high-dimensional and sequential RLHF problems while beingsimpler than prior methods.</description><author>Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh</author><pubDate>Tue, 30 Apr 2024 15:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13639v3</guid></item><item><title>Towards Interactively Improving ML Data Preparation Code via "Shadow Pipelines"</title><link>http://arxiv.org/abs/2404.19591v1</link><description>Data scientists develop ML pipelines in an iterative manner: they repeatedlyscreen a pipeline for potential issues, debug it, and then revise and improveits code according to their findings. However, this manual process is tediousand error-prone. Therefore, we propose to support data scientists during thisdevelopment cycle with automatically derived interactive suggestions forpipeline improvements. We discuss our vision to generate these suggestions withso-called shadow pipelines, hidden variants of the original pipeline thatmodify it to auto-detect potential issues, try out modifications forimprovements, and suggest and explain these modifications to the user. Weenvision to apply incremental view maintenance-based optimisations to ensurelow-latency computation and maintenance of the shadow pipelines. We conductpreliminary experiments to showcase the feasibility of our envisioned approachand the potential benefits of our proposed optimisations.</description><author>Stefan Grafberger, Paul Groth, Sebastian Schelter</author><pubDate>Tue, 30 Apr 2024 15:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19591v1</guid></item><item><title>Control randomisation approach for policy gradient and application to reinforcement learning in optimal switching</title><link>http://arxiv.org/abs/2404.17939v2</link><description>We propose a comprehensive framework for policy gradient methods tailored tocontinuous time reinforcement learning. This is based on the connection betweenstochastic control problems and randomised problems, enabling applicationsacross various classes of Markovian continuous time control problems, beyonddiffusion models, including e.g. regular, impulse and optimalstopping/switching problems. By utilizing change of measure in the controlrandomisation technique, we derive a new policy gradient representation forthese randomised problems, featuring parametrised intensity policies. Wefurther develop actor-critic algorithms specifically designed to addressgeneral Markovian stochastic control issues. Our framework is demonstratedthrough its application to optimal switching problems, with two numerical casestudies in the energy sector focusing on real options.</description><author>Robert Denkert, Huyên Pham, Xavier Warin</author><pubDate>Tue, 30 Apr 2024 15:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17939v2</guid></item><item><title>Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?</title><link>http://arxiv.org/abs/2402.03305v2</link><description>Diffusion models are capable of impressive feats of image generation withuncommon juxtapositions such as astronauts riding horses on the moon withproperly placed shadows. These outputs indicate the ability to performcompositional generalization, but how do the models do so? We performcontrolled experiments on conditional DDPMs learning to generate 2D sphericalGaussian bumps centered at specified $x$- and $y$-positions. Our results showthat the emergence of semantically meaningful latent representations is key toachieving high performance. En route to successful performance over learning,the model traverses three distinct phases of latent representations: (phase A)no latent structure, (phase B) a 2D manifold of disordered states, and (phaseC) a 2D ordered manifold. Corresponding to each of these phases, we identifyqualitatively different generation behaviors: 1) multiple bumps are generated,2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump isgenerated at the correct $x$ and y location. Furthermore, we show that evenunder imbalanced datasets where features ($x$- versus $y$-positions) arerepresented with skewed frequencies, the learning process for $x$ and $y$ iscoupled rather than factorized, demonstrating that simple vanilla-flavoreddiffusion models cannot learn efficient representations in which localizationin $x$ and $y$ are factorized into separate 1D tasks. These findings suggestthe need for future work to find inductive biases that will push generativemodels to discover and exploit factorizable independent structures in theirinputs, which will be required to vault these models into more data-efficientregimes.</description><author>Qiyao Liang, Ziming Liu, Ila Fiete</author><pubDate>Tue, 30 Apr 2024 15:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03305v2</guid></item><item><title>AI techniques for near real-time monitoring of contaminants in coastal waters on board future Phisat-2 mission</title><link>http://arxiv.org/abs/2404.19586v1</link><description>Differently from conventional procedures, the proposed solution advocates fora groundbreaking paradigm in water quality monitoring through the integrationof satellite Remote Sensing (RS) data, Artificial Intelligence (AI) techniques,and onboard processing. The objective is to offer nearly real-time detection ofcontaminants in coastal waters addressing a significant gap in the existingliterature. Moreover, the expected outcomes include substantial advancements inenvironmental monitoring, public health protection, and resource conservation.The specific focus of our study is on the estimation of Turbidity and pHparameters, for their implications on human and aquatic health. Nevertheless,the designed framework can be extended to include other parameters of interestin the water environment and beyond. Originating from our participation in theEuropean Space Agency (ESA) OrbitalAI Challenge, this article describes thedistinctive opportunities and issues for the contaminants monitoring on thePhisat-2 mission. The specific characteristics of this mission, with the toolsmade available, will be presented, with the methodology proposed by the authorsfor the onboard monitoring of water contaminants in near real-time. Preliminarypromising results are discussed and in progress and future work introduced.</description><author>Francesca Razzano, Pietro Di Stasio, Francesco Mauro, Gabriele Meoni, Marco Esposito, Gilda Schirinzi, Silvia L. Ullo</author><pubDate>Tue, 30 Apr 2024 15:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19586v1</guid></item><item><title>PoseAnimate: Zero-shot high fidelity pose controllable character animation</title><link>http://arxiv.org/abs/2404.13680v2</link><description>Image-to-video(I2V) generation aims to create a video sequence from a singleimage, which requires high temporal coherence and visual fidelity with thesource image.However, existing approaches suffer from character appearanceinconsistency and poor preservation of fine details. Moreover, they require alarge amount of video data for training, which can be computationallydemanding.To address these limitations,we propose PoseAnimate, a novelzero-shot I2V framework for character animation.PoseAnimate contains three keycomponents: 1) Pose-Aware Control Module (PACM) incorporates diverse posesignals into conditional embeddings, to preserve character-independent contentand maintain precise alignment of actions.2) Dual Consistency Attention Module(DCAM) enhances temporal consistency, and retains character identity andintricate background details.3) Mask-Guided Decoupling Module (MGDM) refinesdistinct feature perception, improving animation fidelity by decoupling thecharacter and background.We also propose a Pose Alignment Transition Algorithm(PATA) to ensure smooth action transition.Extensive experiment resultsdemonstrate that our approach outperforms the state-of-the-art training-basedmethods in terms of character consistency and detail fidelity. Moreover, itmaintains a high level of temporal coherence throughout the generatedanimations.</description><author>Bingwen Zhu, Fanyi Wang, Tianyi Lu, Peng Liu, Jingwen Su, Jinxiu Liu, Yanhao Zhang, Zuxuan Wu, Yu-Gang Jiang, Guo-Jun Qi</author><pubDate>Tue, 30 Apr 2024 15:24:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13680v2</guid></item><item><title>Leveraging Label Information for Stealthy Data Stealing in Vertical Federated Learning</title><link>http://arxiv.org/abs/2404.19582v1</link><description>We develop DMAVFL, a novel attack strategy that evades current detectionmechanisms. The key idea is to integrate a discriminator with auxiliaryclassifier that takes a full advantage of the label information (which wascompletely ignored in previous attacks): on one hand, label information helpsto better characterize embeddings of samples from distinct classes, yielding animproved reconstruction performance; on the other hand, computing maliciousgradients with label information better mimics the honest training, making themalicious gradients indistinguishable from the honest ones, and the attack muchmore stealthy. Our comprehensive experiments demonstrate that DMAVFLsignificantly outperforms existing attacks, and successfully circumvents SOTAdefenses for malicious attacks. Additional ablation studies and evaluations onother defenses further underscore the robustness and effectiveness of DMAVFL.</description><author>Duanyi Yao, Songze Li, Xueluan Gong, Sizai Hou, Gaoning Pan</author><pubDate>Tue, 30 Apr 2024 15:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19582v1</guid></item><item><title>Automatic Cardiac Pathology Recognition in Echocardiography Images Using Higher Order Dynamic Mode Decomposition and a Vision Transformer for Small Datasets</title><link>http://arxiv.org/abs/2404.19579v1</link><description>Heart diseases are the main international cause of human defunction.According to the WHO, nearly 18 million people decease each year because ofheart diseases. Also considering the increase of medical data, much pressure isput on the health industry to develop systems for early and accurate heartdisease recognition. In this work, an automatic cardiac pathology recognitionsystem based on a novel deep learning framework is proposed, which analyses inreal-time echocardiography video sequences. The system works in two stages. Thefirst one transforms the data included in a database of echocardiographysequences into a machine-learning-compatible collection of annotated imageswhich can be used in the training stage of any kind of machine learning-basedframework, and more specifically with deep learning. This includes the use ofthe Higher Order Dynamic Mode Decomposition (HODMD) algorithm, for the firsttime to the authors' knowledge, for both data augmentation and featureextraction in the medical field. The second stage is focused on building andtraining a Vision Transformer (ViT), barely explored in the related literature.The ViT is adapted for an effective training from scratch, even with smalldatasets. The designed neural network analyses images from an echocardiographysequence to predict the heart state. The results obtained show the superiorityof the proposed system and the efficacy of the HODMD algorithm, evenoutperforming pretrained Convolutional Neural Networks (CNNs), which are so farthe method of choice in the literature.</description><author>Andrés Bell-Navas, Nourelhouda Groun, María Villalba-Orero, Enrique Lara-Pezzi, Jesús Garicano-Mena, Soledad Le Clainche</author><pubDate>Tue, 30 Apr 2024 15:16:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19579v1</guid></item><item><title>ComposerX: Multi-Agent Symbolic Music Composition with LLMs</title><link>http://arxiv.org/abs/2404.18081v2</link><description>Music composition represents the creative side of humanity, and itself is acomplex task that requires abilities to understand and generate informationwith long dependency and harmony constraints. While demonstrating impressivecapabilities in STEM subjects, current LLMs easily fail in this task,generating ill-written music even when equipped with modern techniques likeIn-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs'potential in music composition by leveraging their reasoning ability and thelarge knowledge base in music history and theory, we propose ComposerX, anagent-based symbolic music generation framework. We find that applying amulti-agent approach significantly improves the music composition quality ofGPT-4. The results demonstrate that ComposerX is capable of producing coherentpolyphonic music compositions with captivating melodies, while adhering to userinstructions.</description><author>Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan, Ge Zhang, Hanfeng Lin, Yizhi Li, Yinghao Ma, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenwu Wang, Guangyu Xia, Wei Xue, Yike Guo</author><pubDate>Tue, 30 Apr 2024 15:14:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18081v2</guid></item><item><title>Adversarial Example Soups: Improving Transferability and Stealthiness for Free</title><link>http://arxiv.org/abs/2402.18370v2</link><description>Transferable adversarial examples cause practical security risks since theycan mislead a target model without knowing its internal knowledge. Aconventional recipe for maximizing transferability is to keep only the optimaladversarial example from all those obtained in the optimization pipeline. Inthis paper, for the first time, we question this convention and demonstratethat those discarded, sub-optimal adversarial examples can be reused to boosttransferability. Specifically, we propose ``Adversarial Example Soups'' (AES),with AES-tune for averaging discarded adversarial examples in hyperparametertuning and AES-rand for stability testing. In addition, our AES is inspired by``model soups'', which averages weights of multiple fine-tuned models forimproved accuracy without increasing inference time. Extensive experimentsvalidate the global effectiveness of our AES, boosting 10 state-of-the-arttransfer attacks and their combinations by up to 13% against 10 diverse(defensive) target models. We also show the possibility of generalizing AES toother types, e.g., directly averaging multiple in-the-wild adversarial examplesthat yield comparable success. A promising byproduct of AES is the improvedstealthiness of adversarial examples since the perturbation variances arenaturally reduced.</description><author>Bo Yang, Hengwei Zhang, Jindong Wang, Yulong Yang, Chenhao Lin, Chao Shen, Zhengyu Zhao</author><pubDate>Tue, 30 Apr 2024 15:13:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18370v2</guid></item><item><title>Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation</title><link>http://arxiv.org/abs/2404.17489v2</link><description>Contrastive learning is a model pre-training technique by first creatingsimilar views of the original data, and then encouraging the data and itscorresponding views to be close in the embedding space. Contrastive learninghas witnessed success in image and natural language data, thanks to thedomain-specific augmentation techniques that are both intuitive and effective.Nonetheless, in tabular domain, the predominant augmentation technique forcreating views is through corrupting tabular entries via swapping values, whichis not as sound or effective. We propose a simple yet powerful improvement tothis augmentation technique: corrupting tabular data conditioned on classidentity. Specifically, when corrupting a specific tabular entry from an anchorrow, instead of randomly sampling a value in the same feature column from theentire table uniformly, we only sample from rows that are identified to bewithin the same class as the anchor row. We assume the semi-supervised learningsetting, and adopt the pseudo labeling technique for obtaining class identitiesover all table rows. We also explore the novel idea of selecting features to becorrupted based on feature correlation structures. Extensive experiments showthat the proposed approach consistently outperforms the conventional corruptionmethod for tabular data classification tasks. Our code is available athttps://github.com/willtop/Tabular-Class-Conditioned-SSL.</description><author>Wei Cui, Rasa Hosseinzadeh, Junwei Ma, Tongzi Wu, Yi Sui, Keyvan Golestan</author><pubDate>Tue, 30 Apr 2024 15:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17489v2</guid></item><item><title>A Spatio-Temporal based Frame Indexing Algorithm for QoS Improvement in Live Low-Motion Video Streaming</title><link>http://arxiv.org/abs/2404.19574v1</link><description>Real-time video life streaming of events over a network continued to gainmore popularity among the populace. However, there is need to ensure thejudicious utilization of allocated bandwidth without compromising the Qualityof Service (QoS) of the system. In this regard, this paper presents an approachbased on spatio-temporal frame indexing that detects and eliminate redundancywithin and across captured frame, prior transmission from the server toclients. The standard and local low motion videos were the two scenariosconsidered in evaluating the performance of the proposed algorithm. Resultsobtained showed that the proposed approach achieved an improvement of 5.13%,15.8% and 5%, 15.6% improvement in terms of the buffer size and compressionratio. Though with a tradeoff of the frame-built time, where both the standardand local frame indexing outperforms the proposed scheme with 10.8% and 8.71%respectively.</description><author>Adewale Emmanuel Adedokun, Muhammed Bashir Abdulrazak, Muyideen Momoh Omuya, Habeeb BelloSalau, Bashir Olaniyi Sadiq</author><pubDate>Tue, 30 Apr 2024 15:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19574v1</guid></item><item><title>War Elephants: Rethinking Combat AI and Human Oversight</title><link>http://arxiv.org/abs/2404.19573v1</link><description>This paper explores the changes that pervasive AI is having on the nature ofcombat. We look beyond the substitution of AI for experts to an approach wherecomplementary human and machine abilities are blended. Using historical andmodern examples, we show how autonomous weapons systems can be effectivelymanaged by teams of human "AI Operators" combined with AI/ML "Proxy Operators."By basing our approach on the principles of complementation, we provide for aflexible and dynamic approach to managing lethal autonomous systems. Weconclude by presenting a path to achieving an integrated vision ofmachine-speed combat where the battlefield AI is operated by AI Operators thatwatch for patterns of behavior within battlefield to assess the performance oflethal autonomous systems. This approach enables the development of combatsystems that are likely to be more ethical, operate at machine speed, and arecapable of responding to a broader range of dynamic battlefield conditions thanany purely autonomous AI system could support.</description><author>Philip Feldman, Aaron Dant, Harry Dreany</author><pubDate>Tue, 30 Apr 2024 15:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19573v1</guid></item><item><title>Inductive biases in deep learning models for weather prediction</title><link>http://arxiv.org/abs/2304.04664v2</link><description>Deep learning has gained immense popularity in the Earth sciences as itenables us to formulate purely data-driven models of complex Earth systemprocesses. Deep learning-based weather prediction (DLWP) models have madesignificant progress in the last few years, achieving forecast skillscomparable to established numerical weather prediction models withcomparatively lesser computational costs. In order to train accurate, reliable,and tractable DLWP models with several millions of parameters, the model designneeds to incorporate suitable inductive biases that encode structuralassumptions about the data and the modelled processes. When chosenappropriately, these biases enable faster learning and better generalisation tounseen data. Although inductive biases play a crucial role in successful DLWPmodels, they are often not stated explicitly and their contribution to modelperformance remains unclear. Here, we review and analyse the inductive biasesof state-of-the-art DLWP models with respect to five key design elements: dataselection, learning objective, loss function, architecture, and optimisationmethod. We identify the most important inductive biases and highlight potentialavenues towards more efficient and probabilistic DLWP models.</description><author>Jannik Thuemmel, Matthias Karlbauer, Sebastian Otte, Christiane Zarfl, Georg Martius, Nicole Ludwig, Thomas Scholten, Ulrich Friedrich, Volker Wulfmeyer, Bedartha Goswami, Martin V. Butz</author><pubDate>Tue, 30 Apr 2024 15:05:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04664v2</guid></item><item><title>IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training</title><link>http://arxiv.org/abs/2310.07355v2</link><description>In the field of medical Vision-Language Pre-training (VLP), significantefforts have been devoted to deriving text and image features from bothclinical reports and associated medical images. However, most existing methodsmay have overlooked the opportunity in leveraging the inherent hierarchicalstructure of clinical reports, which are generally split into `findings' fordescriptive content and `impressions' for conclusive observation. Instead ofutilizing this rich, structured format, current medical VLP approaches oftensimplify the report into either a unified entity or fragmented tokens. In thiswork, we propose a novel clinical prior guided VLP framework named IMITATE tolearn the structure information from medical reports with hierarchicalvision-language alignment. The framework derives multi-level visual featuresfrom the chest X-ray (CXR) images and separately aligns these features with thedescriptive and the conclusive text encoded in the hierarchical medical report.Furthermore, a new clinical-informed contrastive loss is introduced forcross-modal learning, which accounts for clinical prior knowledge informulating sample correlations in contrastive learning. The proposed model,IMITATE, outperforms baseline VLP methods across six different datasets,spanning five medical imaging downstream tasks. Comprehensive experimentalresults highlight the advantages of integrating the hierarchical structure ofmedical reports for vision-language alignment.</description><author>Che Liu, Sibo Cheng, Miaojing Shi, Anand Shah, Wenjia Bai, Rossella Arcucci</author><pubDate>Tue, 30 Apr 2024 15:02:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07355v2</guid></item><item><title>MUTE-Reco: MUTual Information Assisted Ensemble Feature RECOmmender System for Healthcare Prognosis</title><link>http://arxiv.org/abs/2209.13836v2</link><description>Purpose: Health recommenders act as important decision support systems,aiding patients and medical professionals in taking actions that lead topatients' well-being. These systems extract the information which may be ofparticular relevance to the end-user, helping them in making appropriatedecisions. The present study proposes a feature recommender that identifies andrecommends the most important risk factors for healthcare prognosis. Methods: A novel mutual information and ensemble-based feature rankingapproach (termed as, MUTE-Reco) considering the rank of features obtained fromeight popular feature selection methods, is proposed. Results: To establish the effectiveness of the proposed method, theexperiment has been conducted on four benchmark datasets of diverse diseases(clear cell renal cell carcinoma (ccRCC), chronic kidney disease, Indian liverpatient, and cervical cancer risk factors). The performance of the proposedrecommender is compared with four state-of-the-art methods using recommendersystems' performance metrics like average precision@K, precision@K, recall@K,F1@K, reciprocal rank@K. Experimental results show that the model built withthe recommended features can attain a higher accuracy (96.6% and 98.6% usingsupport vector machine and neural network, respectively) for classifyingdifferent stages of ccRCC with a reduced feature set as compared to existingmethods. Moreover, the top two features recommended using the proposed methodwith ccRCC, viz. size of tumor and metastasis status, are medically validatedfrom the existing TNM system. Results are also found to be superior for theother three datasets. Conclusion: The proposed recommender, MUTE-Reco, can identify and recommendrisk factors that have the most discriminating power for detecting diseases.</description><author>Abhishek Dey, Debayan Goswami, Rahul Roy, Susmita Ghosh, Yu Shrike Zhang, Jonathan H. Chan</author><pubDate>Tue, 30 Apr 2024 15:01:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.13836v2</guid></item><item><title>Conditioning Generative Latent Optimization for Sparse-View CT Image Reconstruction</title><link>http://arxiv.org/abs/2307.16670v3</link><description>Computed Tomography (CT) is a prominent example of Imaging Inverse Problemhighlighting the unrivaled performances of data-driven methods in degradedmeasurements setups like sparse X-ray projections. Although a significantproportion of deep learning approaches benefit from large supervised datasets,they cannot generalize to new experimental setups. In contrast, fullyunsupervised techniques, most notably using score-based generative models, haverecently demonstrated similar or better performances compared to supervisedapproaches while being flexible at test time. However, their use cases arelimited as they need considerable amounts of training data to have goodgeneralization properties. Another unsupervised approach taking advantage ofthe implicit natural bias of deep convolutional networks, Deep Image Prior, hasrecently been adapted to solve sparse CT by reparameterizing the reconstructionproblem. Although this methodology does not require any training dataset, itenforces a weaker prior on the reconstructions when compared to data-drivenmethods. To fill the gap between these two strategies, we propose anunsupervised conditional approach to the Generative Latent Optimizationframework (cGLO). Similarly to DIP, without any training dataset, cGLO benefitsfrom the structural bias of a decoder network. However, the prior is furtherreinforced as the effect of a likelihood objective shared between multipleslices being reconstructed simultaneously through the same decoder network. Inaddition, the parameters of the decoder may be initialized on an unsupervised,and eventually very small, training dataset to enhance the reconstruction. Theresulting approach is tested on full-dose sparse-view CT using multipletraining dataset sizes and varying numbers of viewing angles.</description><author>Thomas Braure, Delphine Lazaro, David Hateau, Vincent Brandon, Kévin Ginsburger</author><pubDate>Tue, 30 Apr 2024 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16670v3</guid></item></channel></rss>