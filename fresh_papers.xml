<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 17 Dec 2024 13:00:22 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization</title><link>http://arxiv.org/abs/2412.12098v1</link><description>Reinforcement learning (RL) algorithms aim to balance exploiting the currentbest strategy with exploring new options that could lead to higher rewards.Most common RL algorithms use undirected exploration, i.e., select randomsequences of actions. Exploration can also be directed using intrinsic rewards,such as curiosity or model epistemic uncertainty. However, effectivelybalancing task and intrinsic rewards is challenging and often task-dependent.In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic andextrinsic exploration. MaxInfoRL steers exploration towards informativetransitions, by maximizing intrinsic rewards such as the information gain aboutthe underlying task. When combined with Boltzmann exploration, this approachnaturally trades off maximization of the value function with that of theentropy over states, rewards, and actions. We show that our approach achievessublinear regret in the simplified setting of multi-armed bandits. We thenapply this general formulation to a variety of off-policy model-free RL methodsfor continuous state-action spaces, yielding novel algorithms that achievesuperior performance across hard exploration problems and complex scenariossuch as visual control tasks.</description><author>Bhavya Sukhija, Stelian Coros, Andreas Krause, Pieter Abbeel, Carmelo Sferrazza</author><pubDate>Mon, 16 Dec 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12098v1</guid></item><item><title>PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting</title><link>http://arxiv.org/abs/2412.12096v1</link><description>With the advent of portable 360{\deg} cameras, panorama has gainedsignificant attention in applications like virtual reality (VR), virtual tours,robotics, and autonomous driving. As a result, wide-baseline panorama viewsynthesis has emerged as a vital task, where high resolution, fast inference,and memory efficiency are essential. Nevertheless, existing methods aretypically constrained to lower resolutions (512 $\times$ 1024) due to demandingmemory and computational requirements. In this paper, we present PanSplat, ageneralizable, feed-forward approach that efficiently supports resolution up to4K (2048 $\times$ 4096). Our approach features a tailored spherical 3D Gaussianpyramid with a Fibonacci lattice arrangement, enhancing image quality whilereducing information redundancy. To accommodate the demands of high resolution,we propose a pipeline that integrates a hierarchical spherical cost volume andGaussian heads with local operations, enabling two-step deferredbackpropagation for memory-efficient training on a single A100 GPU. Experimentsdemonstrate that PanSplat achieves state-of-the-art results with superiorefficiency and image quality across both synthetic and real-world datasets.Code will be available at \url{https://github.com/chengzhag/PanSplat}.</description><author>Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai</author><pubDate>Mon, 16 Dec 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12096v1</guid></item><item><title>Causal Diffusion Transformers for Generative Modeling</title><link>http://arxiv.org/abs/2412.12095v1</link><description>We introduce Causal Diffusion as the autoregressive (AR) counterpart ofDiffusion models. It is a next-token(s) forecasting framework that is friendlyto both discrete and continuous modalities and compatible with existingnext-token prediction models like LLaMA and GPT. While recent works attempt tocombine diffusion with AR models, we show that introducing sequentialfactorization to a diffusion model can substantially improve its performanceand enables a smooth transition between AR and diffusion generation modes.Hence, we propose CausalFusion - a decoder-only transformer thatdual-factorizes data across sequential tokens and diffusion noise levels,leading to state-of-the-art results on the ImageNet generation benchmark whilealso enjoying the AR advantage of generating an arbitrary number of tokens forin-context reasoning. We further demonstrate CausalFusion's multimodalcapabilities through a joint image generation and captioning model, andshowcase CausalFusion's ability for zero-shot in-context image manipulations.We hope that this work could provide the community with a fresh perspective ontraining multimodal models over discrete and continuous data.</description><author>Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, Haoqi Fan</author><pubDate>Mon, 16 Dec 2024 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12095v1</guid></item><item><title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title><link>http://arxiv.org/abs/2412.12094v1</link><description>Large Language Models (LLMs) have exhibited exceptional performance across aspectrum of natural language processing tasks. However, their substantial sizespose considerable challenges, particularly in computational demands andinference speed, due to their quadratic complexity. In this work, we haveidentified a key pattern: certain seemingly meaningless special tokens (i.e.,separators) contribute disproportionately to attention scores compared tosemantically meaningful tokens. This observation suggests that information ofthe segments between these separator tokens can be effectively condensed intothe separator tokens themselves without significant information loss. Guided bythis insight, we introduce SepLLM, a plug-and-play framework that acceleratesinference by compressing these segments and eliminating redundant tokens.Additionally, we implement efficient kernels for training acceleration.Experimental results across training-free, training-from-scratch, andpost-training settings demonstrate SepLLM's effectiveness. Notably, using theLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on theGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, instreaming settings, SepLLM effectively processes sequences of up to 4 milliontokens or more while maintaining consistent language modeling capabilities.</description><author>Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang</author><pubDate>Mon, 16 Dec 2024 18:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12094v1</guid></item><item><title>CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</title><link>http://arxiv.org/abs/2412.12093v1</link><description>Reconstructing photorealistic and dynamic portrait avatars from images isessential to many applications including advertising, visual effects, andvirtual reality. Depending on the application, avatar reconstruction involvesdifferent capture setups and constraints $-$ for example, visual effectsstudios use camera arrays to capture hundreds of reference images, whilecontent creators may seek to animate a single portrait image downloaded fromthe internet. As such, there is a large and heterogeneous ecosystem of methodsfor avatar reconstruction. Techniques based on multi-view stereo or neuralrendering achieve the highest quality results, but require hundreds ofreference images. Recent generative models produce convincing avatars from asingle reference image, but visual fidelity yet lags behind multi-viewtechniques. Here, we present CAP4D: an approach that uses a morphablemulti-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portraitavatars from any number of reference images (i.e., one to 100) and animate andrender them in real time. Our approach demonstrates state-of-the-artperformance for single-, few-, and multi-image 4D portrait avatarreconstruction, and takes steps to bridge the gap in visual fidelity betweensingle-image and multi-view reconstruction techniques.</description><author>Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell</author><pubDate>Mon, 16 Dec 2024 18:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12093v1</guid></item><item><title>No More Tuning: Prioritized Multi-Task Learning with Lagrangian Differential Multiplier Methods</title><link>http://arxiv.org/abs/2412.12092v1</link><description>Given the ubiquity of multi-task in practical systems, Multi-Task Learning(MTL) has found widespread application across diverse domains. In real-worldscenarios, these tasks often have different priorities. For instance, In websearch, relevance is often prioritized over other metrics, such asclick-through rates or user engagement. Existing frameworks pay insufficientattention to the prioritization among different tasks, which typically adjusttask-specific loss function weights to differentiate task priorities. However,this approach encounters challenges as the number of tasks grows, leading toexponential increases in hyper-parameter tuning complexity. Furthermore, thesimultaneous optimization of multiple objectives can negatively impact theperformance of high-priority tasks due to interference from lower-prioritytasks. In this paper, we introduce a novel multi-task learning framework employingLagrangian Differential Multiplier Methods for step-wise multi-taskoptimization. It is designed to boost the performance of high-priority taskswithout interference from other tasks. Its primary advantage lies in itsability to automatically optimize multiple objectives without requiringbalancing hyper-parameters for different tasks, thereby eliminating the needfor manual tuning. Additionally, we provide theoretical analysis demonstratingthat our method ensures optimization guarantees, enhancing the reliability ofthe process. We demonstrate its effectiveness through experiments on multiplepublic datasets and its application in Taobao search, a large-scale industrialsearch ranking system, resulting in significant improvements across variousbusiness metrics.</description><author>Zhengxing Cheng, Yuheng Huang, Zhixuan Zhang, Dan Ou, Qingwen Liu</author><pubDate>Mon, 16 Dec 2024 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12092v1</guid></item><item><title>Wonderland: Navigating 3D Scenes from a Single Image</title><link>http://arxiv.org/abs/2412.12091v1</link><description>This paper addresses a challenging question: How can we efficiently createhigh-quality, wide-scope 3D scenes from a single arbitrary image? Existingmethods face several constraints, such as requiring multi-view data,time-consuming per-scene optimization, low visual quality in backgrounds, anddistorted reconstructions in unseen areas. We propose a novel pipeline toovercome these limitations. Specifically, we introduce a large-scalereconstruction model that uses latents from a video diffusion model to predict3D Gaussian Splattings for the scenes in a feed-forward manner. The videodiffusion model is designed to create videos precisely following specifiedcamera trajectories, allowing it to generate compressed video latents thatcontain multi-view information while maintaining 3D consistency. We train the3D reconstruction model to operate on the video latent space with a progressivetraining strategy, enabling the efficient generation of high-quality,wide-scope, and generic 3D scenes. Extensive evaluations across variousdatasets demonstrate that our model significantly outperforms existing methodsfor single-view 3D scene generation, particularly with out-of-domain images.For the first time, we demonstrate that a 3D reconstruction model can beeffectively built upon the latent space of a diffusion model to realizeefficient 3D scene generation.</description><author>Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren</author><pubDate>Mon, 16 Dec 2024 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12091v1</guid></item><item><title>Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation</title><link>http://arxiv.org/abs/2412.12089v1</link><description>Recent advances in GPU-based parallel simulation have enabled practitionersto collect large amounts of data and train complex control policies using deepreinforcement learning (RL), on commodity GPUs. However, such successes for RLin robotics have been limited to tasks sufficiently simulated by fastrigid-body dynamics. Simulation techniques for soft bodies are comparativelyseveral orders of magnitude slower, thereby limiting the use of RL due tosample complexity requirements. To address this challenge, this paper presentsboth a novel RL algorithm and a simulation platform to enable scaling RL ontasks involving rigid bodies and deformables. We introduce Soft Analytic PolicyOptimization (SAPO), a maximum entropy first-order model-based actor-critic RLalgorithm, which uses first-order analytic gradients from differentiablesimulation to train a stochastic actor to maximize expected return and entropy.Alongside our approach, we develop Rewarped, a parallel differentiablemultiphysics simulation platform that supports simulating various materialsbeyond rigid bodies. We re-implement challenging manipulation and locomotiontasks in Rewarped, and show that SAPO outperforms baselines over a range oftasks that involve interaction between rigid bodies, articulations, anddeformables.</description><author>Eliot Xing, Vernon Luk, Jean Oh</author><pubDate>Mon, 16 Dec 2024 18:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12089v1</guid></item><item><title>Instruction-based Image Manipulation by Watching How Things Move</title><link>http://arxiv.org/abs/2412.12087v1</link><description>This paper introduces a novel dataset construction pipeline that samplespairs of frames from videos and uses multimodal large language models (MLLMs)to generate editing instructions for training instruction-based imagemanipulation models. Video frames inherently preserve the identity of subjectsand scenes, ensuring consistent content preservation during editing.Additionally, video data captures diverse, natural dynamics-such as non-rigidsubject motion and complex camera movements-that are difficult to modelotherwise, making it an ideal source for scalable dataset construction. Usingthis approach, we create a new dataset to train InstructMove, a model capableof instruction-based complex manipulations that are difficult to achieve withsynthetically generated datasets. Our model demonstrates state-of-the-artperformance in tasks such as adjusting subject poses, rearranging elements, andaltering camera perspectives.</description><author>Mingdeng Cao, Xuaner Zhang, Yinqiang Zheng, Zhihao Xia</author><pubDate>Mon, 16 Dec 2024 18:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12087v1</guid></item><item><title>Probing the Mid-level Vision Capabilities of Self-Supervised Learning</title><link>http://arxiv.org/abs/2411.17474v2</link><description>Mid-level vision capabilities - such as generic object localization and 3Dgeometric understanding - are not only fundamental to human vision but are alsocrucial for many real-world applications of computer vision. These abilitiesemerge with minimal supervision during the early stages of human visualdevelopment. Despite their significance, current self-supervised learning (SSL)approaches are primarily designed and evaluated for high-level recognitiontasks, leaving their mid-level vision capabilities largely unexamined. In this study, we introduce a suite of benchmark protocols to systematicallyassess mid-level vision capabilities and present a comprehensive, controlledevaluation of 22 prominent SSL models across 8 mid-level vision tasks. Ourexperiments reveal a weak correlation between mid-level and high-level taskperformance. We also identify several SSL methods with highly imbalancedperformance across mid-level and high-level capabilities, as well as some thatexcel in both. Additionally, we investigate key factors contributing tomid-level vision performance, such as pretraining objectives and networkarchitectures. Our study provides a holistic and timely view of what SSL modelshave learned, complementing existing research that primarily focuses onhigh-level vision tasks. We hope our findings guide future SSL research tobenchmark models not only on high-level vision tasks but on mid-level as well.</description><author>Xuweiyi Chen, Markus Marks, Zezhou Cheng</author><pubDate>Mon, 16 Dec 2024 18:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17474v2</guid></item><item><title>Understanding Language Model Circuits through Knowledge Editing</title><link>http://arxiv.org/abs/2406.17241v3</link><description>Recent advances in language model interpretability have identified circuits,critical subnetworks that replicate model behaviors, yet how knowledge isstructured within these crucial subnetworks remains opaque. To gain anunderstanding toward the knowledge in the circuits, we conduct systematicknowledge editing experiments on the circuits of the GPT-2 language model. Ouranalysis reveals intriguing patterns in how circuits respond to editingattempts, the extent of knowledge distribution across network components, andthe architectural composition of knowledge-bearing circuits. These findingsoffer insights into the complex relationship between model circuits andknowledge representation, deepening the understanding of how information isorganized within language models. Our findings offer novel insights into the``meanings'' of the circuits, and introduce directions for furtherinterpretability and safety research of language models.</description><author>Huaizhi Ge, Frank Rudzicz, Zining Zhu</author><pubDate>Mon, 16 Dec 2024 18:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17241v3</guid></item><item><title>IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations</title><link>http://arxiv.org/abs/2412.12083v1</link><description>Capturing geometric and material information from images remains afundamental challenge in computer vision and graphics. Traditionaloptimization-based methods often require hours of computational time toreconstruct geometry, material properties, and environmental lighting fromdense multi-view inputs, while still struggling with inherent ambiguitiesbetween lighting and material. On the other hand, learning-based approachesleverage rich material priors from existing 3D object datasets but facechallenges with maintaining multi-view consistency. In this paper, we introduceIDArb, a diffusion-based model designed to perform intrinsic decomposition onan arbitrary number of images under varying illuminations. Our method achievesaccurate and multi-view consistent estimation on surface normals and materialproperties. This is made possible through a novel cross-view, cross-domainattention module and an illumination-augmented, view-adaptive trainingstrategy. Additionally, we introduce ARB-Objaverse, a new dataset that provideslarge-scale multi-view intrinsic data and renderings under diverse lightingconditions, supporting robust training. Extensive experiments demonstrate thatIDArb outperforms state-of-the-art methods both qualitatively andquantitatively. Moreover, our approach facilitates a range of downstream tasks,including single-image relighting, photometric stereo, and 3D reconstruction,highlighting its broad applications in realistic 3D content creation.</description><author>Zhibing Li, Tong Wu, Jing Tan, Mengchen Zhang, Jiaqi Wang, Dahua Lin</author><pubDate>Mon, 16 Dec 2024 18:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12083v1</guid></item><item><title>UniLoc: Towards Universal Place Recognition Using Any Single Modality</title><link>http://arxiv.org/abs/2412.12079v1</link><description>To date, most place recognition methods focus on single-modality retrieval.While they perform well in specific environments, cross-modal methods offergreater flexibility by allowing seamless switching between map and querysources. It also promises to reduce computation requirements by having aunified model, and achieving greater sample efficiency by sharing parameters.In this work, we develop a universal solution to place recognition, UniLoc,that works with any single query modality (natural language, image, or pointcloud). UniLoc leverages recent advances in large-scale contrastive learning,and learns by matching hierarchically at two levels: instance-level matchingand scene-level matching. Specifically, we propose a novel Self-Attention basedPooling (SAP) module to evaluate the importance of instance descriptors whenaggregated into a place-level descriptor. Experiments on the KITTI-360 datasetdemonstrate the benefits of cross-modality for place recognition, achievingsuperior performance in cross-modal settings and competitive results also foruni-modal scenarios. Our project page is publicly available athttps://yan-xia.github.io/projects/UniLoc/.</description><author>Yan Xia, Zhendong Li, Yun-Jin Li, Letian Shi, Hu Cao, João F. Henriques, Daniel Cremers</author><pubDate>Mon, 16 Dec 2024 18:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12079v1</guid></item><item><title>CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology</title><link>http://arxiv.org/abs/2412.12077v1</link><description>The emergence of large multimodal models (LMMs) has brought significantadvancements to pathology. Previous research has primarily focused onseparately training patch-level and whole-slide image (WSI)-level models,limiting the integration of learned knowledge across patches and WSIs, andresulting in redundant models. In this work, we introduce CPath-Omni, the first15-billion-parameter LMM designed to unify both patch and WSI level imageanalysis, consolidating a variety of tasks at both levels, includingclassification, visual question answering, captioning, and visual referringprompting. Extensive experiments demonstrate that CPath-Omni achievesstate-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42datasets, outperforming or matching task-specific models trained for individualtasks. Additionally, we develop a specialized pathology CLIP-based visualprocessor for CPath-Omni, CPath-CLIP, which, for the first time, integratesdifferent vision models and incorporates a large language model as a textencoder to build a more powerful CLIP model, which achieves SOTA performance onnine zero-shot and four few-shot datasets. Our findings highlight CPath-Omni'sability to unify diverse pathology tasks, demonstrating its potential tostreamline and advance the field of foundation model in pathology.</description><author>Yuxuan Sun, Yixuan Si, Chenglu Zhu, Xuan Gong, Kai Zhang, Pingyi Chen, Ye Zhang, Zhongyi Shui, Tao Lin, Lin Yang</author><pubDate>Mon, 16 Dec 2024 18:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12077v1</guid></item><item><title>CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding</title><link>http://arxiv.org/abs/2412.12075v1</link><description>Most existing video understanding benchmarks for multimodal large languagemodels (MLLMs) focus only on short videos. The limited number of benchmarks forlong video understanding often rely solely on multiple-choice questions (MCQs).However, because of the inherent limitation of MCQ-based evaluation and theincreasing reasoning ability of MLLMs, models can give the current answerpurely by combining short video understanding with elimination, withoutgenuinely understanding the video content. To address this gap, we introduceCG-Bench, a novel benchmark designed for clue-grounded question answering inlong videos. CG-Bench emphasizes the model's ability to retrieve relevant cluesfor questions, enhancing evaluation credibility. It features 1,219 manuallycurated videos categorized by a granular system with 14 primary categories, 171secondary categories, and 638 tertiary categories, making it the largestbenchmark for long video analysis. The benchmark includes 12,129 QA pairs inthree major question types: perception, reasoning, and hallucination.Compensating the drawbacks of pure MCQ-based evaluation, we design two novelclue-based evaluation methods: clue-grounded white box and black boxevaluations, to assess whether the model generates answers based on the correctunderstanding of the video. We evaluate multiple closed-source and open-sourceMLLMs on CG-Bench. Results indicate that current models significantlyunderperform in understanding long videos compared to short ones, and asignificant gap exists between open-source and commercial models. We hopeCG-Bench can advance the development of more trustworthy and capable MLLMs forlong video understanding. All annotations and video data are released athttps://cg-bench.github.io/leaderboard/.</description><author>Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, Limin Wang</author><pubDate>Mon, 16 Dec 2024 18:46:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12075v1</guid></item><item><title>Extrapolating Jet Radiation with Autoregressive Transformers</title><link>http://arxiv.org/abs/2412.12074v1</link><description>Generative networks are an exciting tool for fast LHC event generation.Usually, they are used to generate configurations with a fixed number ofparticles. Autoregressive transformers allow us to generate events withvariable numbers of particles, very much in line with the physics of QCD jetradiation. We show how they can learn a factorized likelihood for jet radiationand extrapolate in terms of the number of generated jets. For thisextrapolation, bootstrapping training data and training with modifications ofthe likelihood loss can be used.</description><author>Anja Butter, François Charton, Javier Mariño Villadamigo, Ayodele Ore, Tilman Plehn, Jonas Spinner</author><pubDate>Mon, 16 Dec 2024 18:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12074v1</guid></item><item><title>Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats</title><link>http://arxiv.org/abs/2412.12072v1</link><description>WARNING: This paper contains content that maybe upsetting or offensive tosome readers. Dog whistles are coded expressions with dual meanings: oneintended for the general public (outgroup) and another that conveys a specificmessage to an intended audience (ingroup). Often, these expressions are used toconvey controversial political opinions while maintaining plausible deniabilityand slip by content moderation filters. Identification of dog whistles relieson curated lexicons, which have trouble keeping up to date. We introduce\textbf{FETCH!}, a task for finding novel dog whistles in massive social mediacorpora. We find that state-of-the-art systems fail to achieve meaningfulresults across three distinct social media case studies. We present\textbf{EarShot}, a novel system that combines the strengths of vectordatabases and Large Language Models (LLMs) to efficiently and effectivelyidentify new dog whistles.</description><author>Kuleen Sasse, Carlos Aguirre, Isabel Cachola, Sharon Levy, Mark Dredze</author><pubDate>Mon, 16 Dec 2024 18:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12072v1</guid></item><item><title>SPADE: Spectroscopic Photoacoustic Denoising using an Analytical and Data-free Enhancement Framework</title><link>http://arxiv.org/abs/2412.12068v1</link><description>Spectroscopic photoacoustic (sPA) imaging uses multiple wavelengths todifferentiate chromophores based on their unique optical absorption spectra.This technique has been widely applied in areas such as vascular mapping, tumordetection, and therapeutic monitoring. However, sPA imaging is highlysusceptible to noise, leading to poor signal-to-noise ratio (SNR) andcompromised image quality. Traditional denoising techniques like frameaveraging, though effective in improving SNR, can be impractical for dynamicimaging scenarios due to reduced frame rates. Advanced methods, includinglearning-based approaches and analytical algorithms, have demonstrated promisebut often require extensive training data and parameter tuning, limiting theiradaptability for real-time clinical use. In this work, we propose a sPAdenoising using a tuning-free analytical and data-free enhancement (SPADE)framework for denoising sPA images. This framework integrates a data-freelearning-based method with an efficient BM3D-based analytical approach whilepreserves spectral linearity, providing noise reduction and ensuring thatfunctional information is maintained. The SPADE framework was validated throughsimulation, phantom, ex vivo, and in vivo experiments. Results demonstratedthat SPADE improved SNR and preserved spectral information, outperformingconventional methods, especially in challenging imaging conditions. SPADEpresents a promising solution for enhancing sPA imaging quality in clinicalapplications where noise reduction and spectral preservation are critical.</description><author>Fangzhou Lin, Shang Gao, Yichuan Tang, Xihan Ma, Ryo Murakami, Ziming Zhang, John D. Obayemic, Winston W. Soboyejo, Haichong K. Zhang</author><pubDate>Mon, 16 Dec 2024 18:42:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12068v1</guid></item><item><title>Revelations: A Decidable Class of POMDPs with Omega-Regular Objectives</title><link>http://arxiv.org/abs/2412.12063v1</link><description>Partially observable Markov decision processes (POMDPs) form a prominentmodel for uncertainty in sequential decision making. We are interested inconstructing algorithms with theoretical guarantees to determine whether theagent has a strategy ensuring a given specification with probability 1. Thiswell-studied problem is known to be undecidable already for very simpleomega-regular objectives, because of the difficulty of reasoning on uncertainevents. We introduce a revelation mechanism which restricts information loss byrequiring that almost surely the agent has eventually full information of thecurrent state. Our main technical results are to construct exact algorithms fortwo classes of POMDPs called weakly and strongly revealing. Importantly, thedecidable cases reduce to the analysis of a finite belief-support Markovdecision process. This yields a conceptually simple and exact algorithm for alarge class of POMDPs.</description><author>Marius Belly, Nathanaël Fijalkow, Hugo Gimbert, Florian Horn, Guillermo A. Pérez, Pierre Vandenhove</author><pubDate>Mon, 16 Dec 2024 18:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12063v1</guid></item><item><title>Semi-automated analysis of audio-recorded lessons: The case of teachers' engaging messages</title><link>http://arxiv.org/abs/2412.12062v1</link><description>Engaging messages delivered by teachers are a key aspect of the classroomdiscourse that influences student outcomes. However, improving thiscommunication is challenging due to difficulties in obtaining observations.This study presents a methodology for efficiently extracting actualobservations of engaging messages from audio-recorded lessons. We collected2,477 audio-recorded lessons from 75 teachers over two academic years. Usingautomatic transcription and keyword-based filtering analysis, we identified andclassified engaging messages. This method reduced the information to beanalysed by 90%, optimising the time and resources required compared totraditional manual coding. Subsequent descriptive analysis revealed that themost used messages emphasised the future benefits of participating in schoolactivities. In addition, the use of engaging messages decreased as the academicyear progressed. This study offers insights for researchers seeking to extractinformation from teachers' discourse in naturalistic settings and providesuseful information for designing interventions to improve teachers'communication strategies.</description><author>Samuel Falcon, Carmen Alvarez-Alvarez, Jaime Leon</author><pubDate>Mon, 16 Dec 2024 18:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12062v1</guid></item><item><title>Virtual Agent-Based Communication Skills Training to Facilitate Health Persuasion Among Peers</title><link>http://arxiv.org/abs/2412.12061v1</link><description>Many laypeople are motivated to improve the health behavior of their familyor friends but do not know where to start, especially if the health behavior ispotentially stigmatizing or controversial. We present an approach that usesvirtual agents to coach community-based volunteers in health counselingtechniques, such as motivational interviewing, and allows them to practicethese skills in role-playing scenarios. We use this approach in a virtualagent-based system to increase COVID-19 vaccination by empowering users toinfluence their social network. In a between-subjects comparative design study,we test the effects of agent system interactivity and role-playingfunctionality on counseling outcomes, with participants evaluated bystandardized patients and objective judges. We find that all versions areeffective at producing peer counselors who score adequately on a standardizedmeasure of counseling competence, and that participants were significantly moresatisfied with interactive virtual agents compared to passive viewing of thetraining material. We discuss design implications for interpersonal skillstraining systems based on our findings.</description><author>Farnaz Nouraei, Keith Rebello, Mina Fallah, Prasanth Murali, Haley Matuszak, Valerie Jap, Andrea Parker, Michael Paasche-Orlow, Timothy Bickmore</author><pubDate>Mon, 16 Dec 2024 18:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12061v1</guid></item><item><title>Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained Weight Importance Assessment</title><link>http://arxiv.org/abs/2403.10799v4</link><description>Structured pruning for large language models (LLMs) has garnered significantacademic interest due to its ability to efficiently compress and accelerateLLMs by eliminating redundant weight groups at a coarse-grained granularity.Current structured pruning methods for LLMs typically depend on a singulargranularity for assessing weight importance, resulting in notable performancedegradation in downstream tasks. Intriguingly, our empirical investigationsreveal that utilizing unstructured pruning, which achieves better performanceretention by pruning weights at a finer granularity, \emph{i.e.}, individualweights, yields significantly varied sparse LLM structures when juxtaposed tostructured pruning. This suggests that evaluating both holistic and individualassessment for weight importance is essential for LLM pruning. Building on thisinsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),a novel method that merges fine-grained and coarse-grained evaluations ofweight importance for the pruning of LLMs. Leveraging an attention mechanism,HyWIA adaptively determines the optimal blend of granularity in weightimportance assessments in an end-to-end pruning manner. Extensive experimentson LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarksdemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIAsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\% inaccuracy across seven downstream tasks when pruning LLaMA-7B by 50\%.</description><author>Jun Liu, Zhenglun Kong, Pu Zhao, Changdi Yang, Hao Tang, Xuan Shen, Geng Yuan, Wei Niu, Wenbin Zhang, Xue Lin, Dong Huang, Yanzhi Wang</author><pubDate>Mon, 16 Dec 2024 18:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10799v4</guid></item><item><title>Differentially Private Prototypes for Imbalanced Transfer Learning</title><link>http://arxiv.org/abs/2406.08039v2</link><description>Machine learning (ML) models have been shown to leak private information fromtheir training datasets. Differential Privacy (DP), typically implementedthrough the differential private stochastic gradient descent algorithm(DP-SGD), has become the standard solution to bound leakage from the models.Despite recent improvements, DP-SGD-based approaches for private learning stillusually struggle in the high privacy ($\varepsilon\le1)$ and low data regimes,and when the private training datasets are imbalanced. To overcome theselimitations, we propose Differentially Private Prototype Learning (DPPL) as anew paradigm for private transfer learning. DPPL leverages publicly pre-trainedencoders to extract features from private data and generates DP prototypes thatrepresent each private class in the embedding space and can be publiclyreleased for inference. Since our DP prototypes can be obtained from only a fewprivate training data points and without iterative noise addition, they offerhigh-utility predictions and strong privacy guarantees even under the notion of\textit{pure DP}. We additionally show that privacy-utility trade-offs can befurther improved when leveraging the public data beyond pre-training of theencoder: in particular, we can privately sample our DP prototypes from thepublicly available data points used to train the encoder. Our experimentalevaluation with four state-of-the-art encoders, four vision datasets, and underdifferent data and imbalancedness regimes demonstrate DPPL's high performanceunder strong privacy guarantees in challenging private learning setups</description><author>Dariush Wahdany, Matthew Jagielski, Adam Dziedzic, Franziska Boenisch</author><pubDate>Mon, 16 Dec 2024 18:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08039v2</guid></item><item><title>The State of Robot Motion Generation</title><link>http://arxiv.org/abs/2410.12172v2</link><description>This paper reviews the large spectrum of methods for generating robot motionproposed over the 50 years of robotics research culminating in recentdevelopments. It crosses the boundaries of methodologies, typically notsurveyed together, from those that operate over explicit models to those thatlearn implicit ones. The paper discusses the current state-of-the-art as wellas properties of varying methodologies, highlighting opportunities forintegration.</description><author>Kostas E. Bekris, Joe Doerr, Patrick Meng, Sumanth Tangirala</author><pubDate>Mon, 16 Dec 2024 18:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12172v2</guid></item><item><title>MGH Radiology Llama: A Llama 3 70B Model for Radiology</title><link>http://arxiv.org/abs/2408.11848v2</link><description>In recent years, the field of radiology has increasingly harnessed the powerof artificial intelligence (AI) to enhance diagnostic accuracy, streamlineworkflows, and improve patient care. Large language models (LLMs) have emergedas particularly promising tools, offering significant potential in assistingradiologists with report generation, clinical decision support, and patientcommunication. This paper presents an advanced radiology-focused large languagemodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,building upon previous domain-specific models like Radiology-GPT andRadiology-Llama2. Leveraging a unique and comprehensive dataset fromMassachusetts General Hospital, comprising over 6.5 million de-identifiedmedical reports across various imaging modalities, the model demonstratessignificant improvements in generating accurate and clinically relevantradiology impressions given the corresponding findings. Our evaluation,incorporating both traditional metrics and a GPT-4-based assessment, highlightsthe enhanced performance of this work over general-purpose LLMs.</description><author>Yucheng Shi, Peng Shu, Zhengliang Liu, Zihao Wu, Quanzheng Li, Tianming Liu, Ninghao Liu, Xiang Li</author><pubDate>Mon, 16 Dec 2024 18:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11848v2</guid></item><item><title>Scaling laws for nonlinear dynamical models of articulatory control</title><link>http://arxiv.org/abs/2411.12720v2</link><description>Dynamical theories of speech use computational models of articulatory controlto generate quantitative predictions and advance understanding of speechdynamics. The addition of a nonlinear restoring force to task dynamic models isa significant improvement over linear models, but nonlinearity introduceschallenges with parameterization and interpretability. We illustrate theseproblems through numerical simulations and introduce solutions in the form ofscaling laws. We apply the scaling laws to a cubic model and show how theyfacilitate interpretable simulations of articulatory dynamics, and can betheoretically interpreted as imposing physical and cognitive constraints onmodels of speech movement dynamics.</description><author>Sam Kirkham</author><pubDate>Mon, 16 Dec 2024 18:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12720v2</guid></item><item><title>Exploring Semantic Consistency and Style Diversity for Domain Generalized Semantic Segmentation</title><link>http://arxiv.org/abs/2412.12050v1</link><description>Domain Generalized Semantic Segmentation (DGSS) seeks to utilize sourcedomain data exclusively to enhance the generalization of semantic segmentationacross unknown target domains. Prevailing studies predominantly concentrate onfeature normalization and domain randomization, these approaches exhibitsignificant limitations. Feature normalization-based methods tend to confusesemantic features in the process of constraining the feature spacedistribution, resulting in classification misjudgment. Domainrandomization-based methods frequently incorporate domain-irrelevant noise dueto the uncontrollability of style transformations, resulting in segmentationambiguity. To address these challenges, we introduce a novel framework, namedSCSD for Semantic Consistency prediction and Style Diversity generalization. Itcomprises three pivotal components: Firstly, a Semantic Query Booster isdesigned to enhance the semantic awareness and discrimination capabilities ofobject queries in the mask decoder, enabling cross-domain semantic consistencyprediction. Secondly, we develop a Text-Driven Style Transform module thatutilizes domain difference text embeddings to controllably guide the styletransformation of image features, thereby increasing inter-domain stylediversity. Lastly, to prevent the collapse of similar domain feature spaces, weintroduce a Style Synergy Optimization mechanism that fortifies the separationof inter-domain features and the aggregation of intra-domain features bysynergistically weighting style contrastive loss and style aggregation loss.Extensive experiments demonstrate that the proposed SCSD significantlyoutperforms existing state-of-theart methods. Notably, SCSD trained on GTAVachieved an average of 49.11 mIoU on the four unseen domain datasets,surpassing the previous state-of-the-art method by +4.08 mIoU. Code isavailable at https://github.com/nhw649/SCSD.</description><author>Hongwei Niu, Linhuang Xie, Jianghang Lin, Shengchuan Zhang</author><pubDate>Mon, 16 Dec 2024 18:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12050v1</guid></item><item><title>Bilevel Learning with Inexact Stochastic Gradients</title><link>http://arxiv.org/abs/2412.12049v1</link><description>Bilevel learning has gained prominence in machine learning, inverse problems,and imaging applications, including hyperparameter optimization, learningdata-adaptive regularizers, and optimizing forward operators. The large-scalenature of these problems has led to the development of inexact andcomputationally efficient methods. Existing adaptive methods predominantly relyon deterministic formulations, while stochastic approaches often adopt adoubly-stochastic framework with impractical variance assumptions, enforces afixed number of lower-level iterations, and requires extensive tuning. In thiswork, we focus on bilevel learning with strongly convex lower-level problemsand a nonconvex sum-of-functions in the upper-level. Stochasticity arises fromdata sampling in the upper-level which leads to inexact stochastichypergradients. We establish their connection to state-of-the-art stochasticoptimization theory for nonconvex objectives. Furthermore, we prove theconvergence of inexact stochastic bilevel optimization under mild assumptions.Our empirical results highlight significant speed-ups and improvedgeneralization in imaging tasks such as image denoising and deblurring incomparison with adaptive deterministic bilevel methods.</description><author>Mohammad Sadegh Salehi, Subhadip Mukherjee, Lindon Roberts, Matthias J. Ehrhardt</author><pubDate>Mon, 16 Dec 2024 18:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12049v1</guid></item><item><title>A LoRA is Worth a Thousand Pictures</title><link>http://arxiv.org/abs/2412.12048v1</link><description>Recent advances in diffusion models and parameter-efficient fine-tuning(PEFT) have made text-to-image generation and customization widely accessible,with Low Rank Adaptation (LoRA) able to replicate an artist's style or subjectusing minimal data and computation. In this paper, we examine the relationshipbetween LoRA weights and artistic styles, demonstrating that LoRA weights alonecan serve as an effective descriptor of style, without the need for additionalimage generation or knowledge of the original training set. Our findings showthat LoRA weights yield better performance in clustering of artistic stylescompared to traditional pre-trained features, such as CLIP and DINO, withstrong structural similarities between LoRA-based and conventional image-basedembeddings observed both qualitatively and quantitatively. We identify variousretrieval scenarios for the growing collection of customized models and showthat our approach enables more accurate retrieval in real-world settings whereknowledge of the training images is unavailable and additional generation isrequired. We conclude with a discussion on potential future applications, suchas zero-shot LoRA fine-tuning and model attribution.</description><author>Chenxi Liu, Towaki Takikawa, Alec Jacobson</author><pubDate>Mon, 16 Dec 2024 18:18:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12048v1</guid></item><item><title>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation</title><link>http://arxiv.org/abs/2412.08628v2</link><description>Open-vocabulary panoptic segmentation aims to segment and classify everythingin diverse scenes across an unbounded vocabulary. Existing methods typicallyemploy two-stage or single-stage framework. The two-stage framework involvescropping the image multiple times using masks generated by a mask generator,followed by feature extraction, while the single-stage framework relies on aheavyweight mask decoder to make up for the lack of spatial positioninformation through self-attention and cross-attention in multiple stackedTransformer blocks. Both methods incur substantial computational overhead,thereby hindering the efficiency of model inference. To fill the gap inefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, andspatialaware framework designed for open-vocabulary panoptic segmentation.Specifically, EOV-Seg innovates in two aspects. First, a Vocabulary-AwareSelection (VAS) module is proposed to improve the semantic comprehension ofvisual aggregated features and alleviate the feature interaction burden on themask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),which efficiently utilizes the spatial awareness capabilities of ViT-based CLIPbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabularypanoptic segmentation framework towards efficiency, which runs faster andachieves competitive performance compared with state-of-the-art methods.Specifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 timesfaster than state-of-theart methods. Especially, equipped with ResNet50backbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090GPU. Code is available at https://github.com/nhw649/EOV-Seg.</description><author>Hongwei Niu, Jie Hu, Jianghang Lin, Guannan Jiang, Shengchuan Zhang</author><pubDate>Mon, 16 Dec 2024 18:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08628v2</guid></item><item><title>Artificial Intelligence in Traffic Systems</title><link>http://arxiv.org/abs/2412.12046v1</link><description>Existing research on AI-based traffic management systems, utilizingtechniques such as fuzzy logic, reinforcement learning, deep neural networks,and evolutionary algorithms, demonstrates the potential of AI to transform thetraffic landscape. This article endeavors to review the topics where AI andtraffic management intersect. It comprises areas like AI-powered traffic signalcontrol systems, automatic distance and velocity recognition (for instance, inautonomous vehicles, hereafter AVs), smart parking systems, and IntelligentTraffic Management Systems (ITMS), which use data captured in real-time to keeptrack of traffic conditions, and traffic-related law enforcement andsurveillance using AI. AI applications in traffic management cover a wide rangeof spheres. The spheres comprise, inter alia, streamlining traffic signaltimings, predicting traffic bottlenecks in specific areas, detecting potentialaccidents and road hazards, managing incidents accurately, advancing publictransportation systems, development of innovative driver assistance systems,and minimizing environmental impact through simplified routes and reducedemissions. The benefits of AI in traffic management are also diverse. Theycomprise improved management of traffic data, sounder route decisionautomation, easier and speedier identification and resolution of vehicularissues through monitoring the condition of individual vehicles, decreasedtraffic snarls and mishaps, superior resource utilization, alleviated stress oftraffic management manpower, greater on-road safety, and better emergencyresponse time.</description><author>Ritwik Raj Saxena</author><pubDate>Mon, 16 Dec 2024 18:15:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12046v1</guid></item><item><title>Qsco: A Quantum Scoring Module for Open-set Supervised Anomaly Detection</title><link>http://arxiv.org/abs/2405.16368v2</link><description>Open set anomaly detection (OSAD) is a crucial task that aims to identifyabnormal patterns or behaviors in data sets, especially when the anomaliesobserved during training do not represent all possible classes of anomalies.The recent advances in quantum computing in handling complex data structuresand improving machine learning models herald a paradigm shift in anomalydetection methodologies. This study proposes a Quantum Scoring Module (Qsco),embedding quantum variational circuits into neural networks to enhance themodel's processing capabilities in handling uncertainty and unlabeled data.Extensive experiments conducted across eight real-world anomaly detectiondatasets demonstrate our model's superior performance in detecting anomaliesacross varied settings and reveal that integrating quantum simulators does notresult in prohibitive time complexities. Our study validates the feasibility ofquantum-enhanced anomaly detection methods in practical applications.</description><author>Yifeng Peng, Xinyi Li, Zhiding Liang, Ying Wang</author><pubDate>Mon, 16 Dec 2024 18:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16368v2</guid></item><item><title>The Impact of AI Assistance on Radiology Reporting: A Pilot Study Using Simulated AI Draft Reports</title><link>http://arxiv.org/abs/2412.12042v1</link><description>Radiologists face increasing workload pressures amid growing imaging volumes,creating risks of burnout and delayed reporting times. While artificialintelligence (AI) based automated radiology report generation shows promise forreporting workflow optimization, evidence of its real-world impact on clinicalaccuracy and efficiency remains limited. This study evaluated the effect ofdraft reports on radiology reporting workflows by conducting a three readermulti-case study comparing standard versus AI-assisted reporting workflows. Inboth workflows, radiologists reviewed the cases and modified either a standardtemplate (standard workflow) or an AI-generated draft report (AI-assistedworkflow) to create the final report. For controlled evaluation, we used GPT-4to generate simulated AI drafts and deliberately introduced 1-3 errors in halfthe cases to mimic real AI system performance. The AI-assisted workflowsignificantly reduced average reporting time from 573 to 435 seconds (p=0.003),without a statistically significant difference in clinically significant errorsbetween workflows. These findings suggest that AI-generated drafts canmeaningfully accelerate radiology reporting while maintaining diagnosticaccuracy, offering a practical solution to address mounting workload challengesin clinical practice.</description><author>Julián N. Acosta, Siddhant Dogra, Subathra Adithan, Kay Wu, Michael Moritz, Stephen Kwak, Pranav Rajpurkar</author><pubDate>Mon, 16 Dec 2024 18:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12042v1</guid></item><item><title>How Private are Language Models in Abstractive Summarization?</title><link>http://arxiv.org/abs/2412.12040v1</link><description>Language models (LMs) have shown outstanding performance in textsummarization including sensitive domains such as medicine and law. In thesesettings, it is important that personally identifying information (PII)included in the source document should not leak in the summary. Prior effortshave mostly focused on studying how LMs may inadvertently elicit PII fromtraining data. However, to what extent LMs can provide privacy-preservingsummaries given a non-private source document remains under-explored. In thispaper, we perform a comprehensive study across two closed- and threeopen-weight LMs of different sizes and families. We experiment with promptingand fine-tuning strategies for privacy-preservation across a range ofsummarization datasets across three domains. Our extensive quantitative andqualitative analysis including human evaluation shows that LMs often cannotprevent PII leakage on their summaries and that current widely-used metricscannot capture context dependent privacy risks.</description><author>Anthony Hughes, Nikolaos Aletras, Ning Ma</author><pubDate>Mon, 16 Dec 2024 18:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12040v1</guid></item><item><title>Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection</title><link>http://arxiv.org/abs/2412.12039v1</link><description>Despite their remarkable success, large language models (LLMs) have shownlimited ability on applied tasks such as vulnerability detection. Weinvestigate various prompting strategies for vulnerability detection and, aspart of this exploration, propose a prompting strategy that integrates naturallanguage descriptions of vulnerabilities with a contrastive chain-of-thoughtreasoning approach, augmented using contrastive samples from a syntheticdataset. Our study highlights the potential of LLMs to detect vulnerabilitiesby integrating natural language descriptions, contrastive reasoning, andsynthetic examples into a comprehensive prompting framework. Our results showthat this approach can enhance LLM understanding of vulnerabilities. On ahigh-quality vulnerability detection dataset such as SVEN, our promptingstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,11%, and 14%, respectively.</description><author>Ira Ceka, Feitong Qiao, Anik Dey, Aastha Valechia, Gail Kaiser, Baishakhi Ray</author><pubDate>Mon, 16 Dec 2024 18:08:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12039v1</guid></item><item><title>LLMs for Cold-Start Cutting Plane Separator Configuration</title><link>http://arxiv.org/abs/2412.12038v1</link><description>Mixed integer linear programming (MILP) solvers ship with a staggering numberof parameters that are challenging to select a priori for all but expertoptimization users, but can have an outsized impact on the performance of theMILP solver. Existing machine learning (ML) approaches to configure solversrequire training ML models by solving thousands of related MILP instances,generalize poorly to new problem sizes, and often require implementing complexML pipelines and custom solver interfaces that can be difficult to integrateinto existing optimization workflows. In this paper, we introduce a newLLM-based framework to configure which cutting plane separators to use for agiven MILP problem with little to no training data based on characteristics ofthe instance, such as a natural language description of the problem and theassociated LaTeX formulation. We augment these LLMs with descriptions ofcutting plane separators available in a given solver, grounded by summarizingthe existing research literature on separators. While individual solverconfigurations have a large variance in performance, we present a novelensembling strategy that clusters and aggregates configurations to create asmall portfolio of high-performing configurations. Our LLM-based methodologyrequires no custom solver interface, can find a high-performing configurationby solving only a small number of MILPs, and can generate the configurationwith simple API calls that run in under a second. Numerical results show ourapproach is competitive with existing configuration approaches on a suite ofclassic combinatorial optimization problems and real-world datasets with only afraction of the training data and computation time.</description><author>Connor Lawless, Yingxi Li, Anders Wikum, Madeleine Udell, Ellen Vitercik</author><pubDate>Mon, 16 Dec 2024 18:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12038v1</guid></item><item><title>LeARN: Learnable and Adaptive Representations for Nonlinear Dynamics in System Identification</title><link>http://arxiv.org/abs/2412.12036v1</link><description>System identification, the process of deriving mathematical models ofdynamical systems from observed input-output data, has undergone a paradigmshift with the advent of learning-based methods. Addressing the intricatechallenges of data-driven discovery in nonlinear dynamical systems, thesemethods have garnered significant attention. Among them, Sparse Identificationof Nonlinear Dynamics (SINDy) has emerged as a transformative approach,distilling complex dynamical behaviors into interpretable linear combinationsof basis functions. However, SINDy relies on domain-specific expertise toconstruct its foundational "library" of basis functions, which limits itsadaptability and universality. In this work, we introduce a nonlinear systemidentification framework called LeARN that transcends the need for prior domainknowledge by learning the library of basis functions directly from data. Toenhance adaptability to evolving system dynamics under varying noiseconditions, we employ a novel meta-learning-based system identificationapproach that uses a lightweight deep neural network (DNN) to dynamicallyrefine these basis functions. This not only captures intricate system behaviorsbut also adapts seamlessly to new dynamical regimes. We validate our frameworkon the Neural Fly dataset, showcasing its robust adaptation and generalizationcapabilities. Despite its simplicity, our LeARN achieves competitive dynamicalerror performance compared to SINDy. This work presents a step toward theautonomous discovery of dynamical systems, paving the way for a future wheremachine learning uncovers the governing principles of complex systems withoutrequiring extensive domain-specific interventions.</description><author>Arunabh Singh, Joyjit Mukherjee</author><pubDate>Mon, 16 Dec 2024 18:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12036v1</guid></item><item><title>Thermodynamics-informed graph neural networks for real-time simulation of digital human twins</title><link>http://arxiv.org/abs/2412.12034v1</link><description>The growing importance of real-time simulation in the medical field hasexposed the limitations and bottlenecks inherent in the digital representationof complex biological systems. This paper presents a novel methodology aimed atadvancing current lines of research in soft tissue simulation. The proposedapproach introduces a hybrid model that integrates the geometric bias of graphneural networks with the physical bias derived from the imposition of ametriplectic structure as soft and hard constrains in the architecture, beingable to simulate hepatic tissue with dissipative properties. This approachprovides an efficient solution capable of generating predictions at highfeedback rate while maintaining a remarkable generalization ability forpreviously unseen anatomies. This makes these features particularly relevant inthe context of precision medicine and haptic rendering. Based on the adopted methodologies, we propose a model that predicts humanliver responses to traction and compression loads in as little as 7.3milliseconds for optimized configurations and as fast as 1.65 milliseconds inthe most efficient cases, all in the forward pass. The model achieves relativeposition errors below 0.15\%, with stress tensor and velocity estimationsmaintaining relative errors under 7\%. This demonstrates the robustness of theapproach developed, which is capable of handling diverse load states andanatomies effectively. This work highlights the feasibility of integratingreal-time simulation with patient-specific geometries through deep learning,paving the way for more robust digital human twins in medical applications.</description><author>Lucas Tesán, David González, Pedro Martins, Elías Cueto</author><pubDate>Mon, 16 Dec 2024 18:01:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12034v1</guid></item><item><title>Merging Text Transformer Models from Different Initializations</title><link>http://arxiv.org/abs/2403.00986v3</link><description>Recent work on permutation-based model merging has shown impressive low- orzero-barrier mode connectivity between models from completely differentinitializations. However, this line of work has not yet extended to theTransformer architecture, despite its dominant popularity in the languagedomain. Therefore, in this work, we investigate the extent to which separateTransformer minima learn similar features, and propose a model mergingtechnique to investigate the relationship between these minima in the losslandscape. The specifics of the architecture, like its residual connections,multi-headed attention, and discrete, sequential input, require specificinterventions in order to compute model permutations that remain within thesame functional equivalence class. In merging these models with our method, weconsistently find lower loss barriers between minima compared to modelaveraging, across models trained on a masked-language modeling task orfine-tuned on a language understanding benchmark. Our results show that theminima of these models are less sharp and isolated than previously understood,and provide a basis for future work on merging separately trained Transformermodels.</description><author>Neha Verma, Maha Elbayad</author><pubDate>Mon, 16 Dec 2024 18:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00986v3</guid></item><item><title>FSFM: A Generalizable Face Security Foundation Model via Self-Supervised Facial Representation Learning</title><link>http://arxiv.org/abs/2412.12032v1</link><description>This work asks: with abundant, unlabeled real faces, how to learn a robustand transferable facial representation that boosts various face security taskswith respect to generalization performance? We make the first attempt andpropose a self-supervised pretraining framework to learn fundamentalrepresentations of real face images, FSFM, that leverages the synergy betweenmasked image modeling (MIM) and instance discrimination (ID). We explorevarious facial masking strategies for MIM and present a simple yet powerfulCRFR-P masking, which explicitly forces the model to capture meaningfulintra-region consistency and challenging inter-region coherency. Furthermore,we devise the ID network that naturally couples with MIM to establishunderlying local-to-global correspondence via tailored self-distillation. Thesethree learning objectives, namely 3C, empower encoding both local features andglobal semantics of real faces. After pretraining, a vanilla ViT serves as auniversal vision foundation model for downstream face security tasks:cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseendiffusion facial forgery detection. Extensive experiments on 10 public datasetsdemonstrate that our model transfers better than supervised pretraining, visualand facial self-supervised learning arts, and even outperforms task-specializedSOTA methods.</description><author>Gaojian Wang, Feng Lin, Tong Wu, Zhenguang Liu, Zhongjie Ba, Kui Ren</author><pubDate>Mon, 16 Dec 2024 17:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12032v1</guid></item><item><title>RepFace: Refining Closed-Set Noise with Progressive Label Correction for Face Recognition</title><link>http://arxiv.org/abs/2412.12031v1</link><description>Face recognition has made remarkable strides, driven by the expanding scaleof datasets, advancements in various backbone and discriminative losses.However, face recognition performance is heavily affected by the label noise,especially closed-set noise. While numerous studies have focused on handlinglabel noise, addressing closed-set noise still poses challenges. This paperidentifies this challenge as training isn't robust to noise at the early-stagetraining, and necessitating an appropriate learning strategy for samples withlow confidence, which are often misclassified as closed-set noise in latertraining phases. To address these issues, we propose a new framework tostabilize the training at early stages and split the samples into clean,ambiguous and noisy groups which are devised with separate training strategies.Initially, we employ generated auxiliary closed-set noisy samples to enable themodel to identify noisy data at the early stages of training. Subsequently, weintroduce how samples are split into clean, ambiguous and noisy groups by theirsimilarity to the positive and nearest negative centers. Then we perform labelfusion for ambiguous samples by incorporating accumulated model predictions.Finally, we apply label smoothing within the closed set, adjusting the label toa point between the nearest negative class and the initially assigned label.Extensive experiments validate the effectiveness of our method on mainstreamface datasets, achieving state-of-the-art results. The code will be releasedupon acceptance.</description><author>Jie Zhang, Xun Gong, Zhonglin Sun</author><pubDate>Mon, 16 Dec 2024 17:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12031v1</guid></item><item><title>Memory-Reduced Meta-Learning with Guaranteed Convergence</title><link>http://arxiv.org/abs/2412.12030v1</link><description>The optimization-based meta-learning approach is gaining increased tractionbecause of its unique ability to quickly adapt to a new task using only smallamounts of data. However, existing optimization-based meta-learning approaches,such as MAML, ANIL and their variants, generally employ backpropagation forupper-level gradient estimation, which requires using historical lower-levelparameters/gradients and thus increases computational and memory overhead ineach iteration. In this paper, we propose a meta-learning algorithm that canavoid using historical parameters/gradients and significantly reduce memorycosts in each iteration compared to existing optimization-based meta-learningapproaches. In addition to memory reduction, we prove that our proposedalgorithm converges sublinearly with the iteration number of upper-leveloptimization, and the convergence error decays sublinearly with the batch sizeof sampled tasks. In the specific case in terms of deterministic meta-learning,we also prove that our proposed algorithm converges to an exact solution.Moreover, we quantify that the computational complexity of the algorithm is onthe order of $\mathcal{O}(\epsilon^{-1})$, which matches existing convergenceresults on meta-learning even without using any historicalparameters/gradients. Experimental results on meta-learning benchmarks confirmthe efficacy of our proposed algorithm.</description><author>Honglin Yang, Ji Ma, Xiao Yu</author><pubDate>Mon, 16 Dec 2024 17:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12030v1</guid></item><item><title>BrushEdit: All-In-One Image Inpainting and Editing</title><link>http://arxiv.org/abs/2412.10316v2</link><description>Image editing has advanced significantly with the development of diffusionmodels using both inversion-based and instruction-based methods. However,current inversion-based approaches struggle with big modifications (e.g.,adding or removing objects) due to the structured nature of inversion noise,which hinders substantial changes. Meanwhile, instruction-based methods oftenconstrain users to black-box operations, limiting direct interaction forspecifying editing regions and intensity. To address these limitations, wepropose BrushEdit, a novel inpainting-based instruction-guided image editingparadigm, which leverages multimodal large language models (MLLMs) and imageinpainting models to enable autonomous, user-friendly, and interactivefree-form instruction editing. Specifically, we devise a system enablingfree-form instruction editing by integrating MLLMs and a dual-branch imageinpainting model in an agent-cooperative framework to perform editing categoryclassification, main object identification, mask acquisition, and editing areainpainting. Extensive experiments show that our framework effectively combinesMLLMs and inpainting models, achieving superior performance across sevenmetrics including mask region preservation and editing effect coherence.</description><author>Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Ying Shan, Yuexian Zou, Qiang Xu</author><pubDate>Mon, 16 Dec 2024 17:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10316v2</guid></item><item><title>Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down Maps</title><link>http://arxiv.org/abs/2412.12024v1</link><description>Learning navigation capabilities in different environments has long been oneof the major challenges in decision-making. In this work, we focus on zero-shotnavigation ability using given abstract $2$-D top-down maps. Like humannavigation by reading a paper map, the agent reads the map as an image whennavigating in a novel layout, after learning to navigate on a set of trainingmaps. We propose a model-based reinforcement learning approach for thismulti-task learning problem, where it jointly learns a hypermodel that takestop-down maps as input and predicts the weights of the transition network. Weuse the DeepMind Lab environment and customize layouts using generated maps.Our method can adapt better to novel environments in zero-shot and is morerobust to noise.</description><author>Linfeng Zhao, Lawson L. S. Wong</author><pubDate>Mon, 16 Dec 2024 17:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12024v1</guid></item><item><title>COSMo: CLIP Talks on Open-Set Multi-Target Domain Adaptation</title><link>http://arxiv.org/abs/2409.00397v2</link><description>Multi-Target Domain Adaptation (MTDA) entails learning domain-invariantinformation from a single source domain and applying it to multiple unlabeledtarget domains. Yet, existing MTDA methods predominantly focus on addressingdomain shifts within visual features, often overlooking semantic features andstruggling to handle unknown classes, resulting in what is known as Open-Set(OS) MTDA. While large-scale vision-language foundation models like CLIP showpromise, their potential for MTDA remains largely unexplored. This paperintroduces COSMo, a novel method that learns domain-agnostic prompts throughsource domain-guided prompt learning to tackle the MTDA problem in the promptspace. By leveraging a domain-specific bias network and separate prompts forknown and unknown classes, COSMo effectively adapts across domain and classshifts. To the best of our knowledge, COSMo is the first method to addressOpen-Set Multi-Target DA (OSMTDA), offering a more realistic representation ofreal-world scenarios and addressing the challenges of both open-set andmulti-target DA. COSMo demonstrates an average improvement of $5.1\%$ acrossthree challenging datasets: Mini-DomainNet, Office-31, and Office-Home,compared to other related DA methods adapted to operate within the OSMTDAsetting. Code is available at: https://github.com/munish30monga/COSMo</description><author>Munish Monga, Sachin Kumar Giroh, Ankit Jha, Mainak Singha, Biplab Banerjee, Jocelyn Chanussot</author><pubDate>Mon, 16 Dec 2024 17:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00397v2</guid></item><item><title>Deep-learning-based identification of individual motion characteristics from upper-limb trajectories towards disorder stage evaluation</title><link>http://arxiv.org/abs/2412.12016v1</link><description>The identification of individual movement characteristics sets the foundationfor the assessment of personal rehabilitation progress and can providediagnostic information on levels and stages of movement disorders. This workpresents a preliminary study for differentiating individual motion patternsusing a dataset of 3D upper-limb transport trajectories measured in task-space.Identifying individuals by deep time series learning can be a key step toabstracting individual motion properties. In this study, a classificationaccuracy of about 95% is reached for a subset of nine, and about 78% for thefull set of 31 individuals. This provides insights into the separability ofpatient attributes by exerting a simple standardized task to be transferred toportable systems.</description><author>Tim Sziburis, Susanne Blex, Tobias Glasmachers, Ioannis Iossifidis</author><pubDate>Mon, 16 Dec 2024 17:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12016v1</guid></item><item><title>Generalization Analysis for Deep Contrastive Representation Learning</title><link>http://arxiv.org/abs/2412.12014v1</link><description>In this paper, we present generalization bounds for the unsupervised risk inthe Deep Contrastive Representation Learning framework, which employs deepneural networks as representation functions. We approach this problem from twoangles. On the one hand, we derive a parameter-counting bound that scales withthe overall size of the neural networks. On the other hand, we provide anorm-based bound that scales with the norms of neural networks' weightmatrices. Ignoring logarithmic factors, the bounds are independent of $k$, thesize of the tuples provided for contrastive learning. To the best of ourknowledge, this property is only shared by one other work, which employed adifferent proof strategy and suffers from very strong exponential dependence onthe depth of the network which is due to a use of the peeling technique. Ourresults circumvent this by leveraging powerful results on covering numbers withrespect to uniform norms over samples. In addition, we utilize lossaugmentation techniques to further reduce the dependency on matrix norms andthe implicit dependence on network depth. In fact, our techniques allow us toproduce many bounds for the contrastive learning setting with similararchitectural dependencies as in the study of the sample complexity of ordinaryloss functions, thereby bridging the gap between the learning theories ofcontrastive learning and DNNs.</description><author>Nong Minh Hieu, Antoine Ledent, Yunwen Lei, Cheng Yeaw Ku</author><pubDate>Mon, 16 Dec 2024 17:40:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12014v1</guid></item><item><title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title><link>http://arxiv.org/abs/2403.05530v5</link><description>In this report, we introduce the Gemini 1.5 family of models, representingthe next generation of highly compute-efficient multimodal models capable ofrecalling and reasoning over fine-grained information from millions of tokensof context, including multiple long documents and hours of video and audio. Thefamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceedsthe February version on the great majority of capabilities and benchmarks; (2)Gemini 1.5 Flash, a more lightweight variant designed for efficiency withminimal regression in quality. Gemini 1.5 models achieve near-perfect recall onlong-context retrieval tasks across modalities, improve the state-of-the-art inlong-document QA, long-video QA and long-context ASR, and match or surpassGemini 1.0 Ultra's state-of-the-art performance across a broad set ofbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we findcontinued improvement in next-token prediction and near-perfect retrieval(&gt;99%) up to at least 10M tokens, a generational leap over existing models suchas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-worlduse cases, such as Gemini 1.5 collaborating with professionals on completingtheir tasks achieving 26 to 75% time savings across 10 different jobcategories, as well as surprising new capabilities of large language models atthe frontier; when given a grammar manual for Kalamang, a language with fewerthan 200 speakers worldwide, the model learns to translate English to Kalamangat a similar level to a person who learned from the same content.</description><author>Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, Fred Alcober, Roy Frostig, Mark Omernick, Lexi Walker, Cosmin Paduraru, Christina Sorokin, Andrea Tacchetti, Colin Gaffney, Samira Daruki, Olcan Sercinoglu, Zach Gleicher, Juliette Love, Paul Voigtlaender, Rohan Jain, Gabriela Surita, Kareem Mohamed, Rory Blevins, Junwhan Ahn, Tao Zhu, Kornraphop Kawintiranon, Orhan Firat, Yiming Gu, Yujing Zhang, Matthew Rahtz, Manaal Faruqui, Natalie Clay, Justin Gilmer, JD Co-Reyes, Ivo Penchev, Rui Zhu, Nobuyuki Morioka, Kevin Hui, Krishna Haridasan, Victor Campos, Mahdis Mahdieh, Mandy Guo, Samer Hassan, Kevin Kilgour, Arpi Vezer, Heng-Tze Cheng, Raoul de Liedekerke, Si</author><pubDate>Mon, 16 Dec 2024 17:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05530v5</guid></item><item><title>SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval</title><link>http://arxiv.org/abs/2412.12009v1</link><description>We introduce Speech Information Retrieval (SIR), a new long-context task forSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-samplebenchmark testing models' ability to extract critical details fromapproximately 90-second spoken inputs. While current Speech LLMs excel atshort-form tasks, they struggle with the computational and representationaldemands of longer audio sequences. To address this limitation, we proposeSpeechPrune, a training-free token pruning strategy that uses speech-textsimilarity and approximated attention scores to efficiently discard irrelevanttokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to47% over the original model and the random pruning model at a pruning rate of20%, respectively. SpeechPrune can maintain network performance even at apruning level of 80%. This approach highlights the potential of token-levelpruning for efficient and scalable long-form speech understanding.</description><author>Yueqian Lin, Yuzhe Fu, Jingyang Zhang, Yudong Liu, Jianyi Zhang, Jingwei Sun, Hai "Helen" Li, Yiran Chen</author><pubDate>Mon, 16 Dec 2024 17:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12009v1</guid></item><item><title>RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?</title><link>http://arxiv.org/abs/2404.14397v2</link><description>Large language models (LLMs) and small language models (SLMs) are beingadopted at remarkable speed, although their safety still remains a seriousconcern. With the advent of multilingual S/LLMs, the question now becomes amatter of scale: can we expand multilingual safety evaluations of these modelswith the same velocity at which they are deployed? To this end, we introduceRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts andoutputs in 28 languages. RTP-LX follows participatory design practices, and aportion of the corpus is especially designed to detect culturally-specifictoxic language. We evaluate 10 S/LLMs on their ability to detect toxic contentin a culturally-sensitive, multilingual scenario. We find that, although theytypically score acceptably in terms of accuracy, they have low agreement withhuman judges when scoring holistically the toxicity of a prompt; and havedifficulty discerning harm in context-dependent scenarios, particularly withsubtle-yet-harmful content (e.g. microaggressions, bias). We release thisdataset to contribute to further reduce harmful uses of these models andimprove their safe deployment.</description><author>Adrian de Wynter, Ishaan Watts, Tua Wongsangaroonsri, Minghui Zhang, Noura Farra, Nektar Ege Altıntoprak, Lena Baur, Samantha Claudet, Pavel Gajdusek, Can Gören, Qilong Gu, Anna Kaminska, Tomasz Kaminski, Ruby Kuo, Akiko Kyuba, Jongho Lee, Kartik Mathur, Petter Merok, Ivana Milovanović, Nani Paananen, Vesa-Matti Paananen, Anna Pavlenko, Bruno Pereira Vidal, Luciano Strika, Yueh Tsao, Davide Turcato, Oleksandr Vakhno, Judit Velcsov, Anna Vickers, Stéphanie Visser, Herdyan Widarmanto, Andrey Zaikin, Si-Qing Chen</author><pubDate>Mon, 16 Dec 2024 17:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14397v2</guid></item><item><title>Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A Novel Weighted Retrieval-Augmented Generation Paradigm</title><link>http://arxiv.org/abs/2412.12006v1</link><description>Technical troubleshooting in enterprise environments often involvesnavigating diverse, heterogeneous data sources to resolve complex issueseffectively. This paper presents a novel agentic AI solution built on aWeighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprisetechnical troubleshooting. By dynamically weighting retrieval sources such asproduct manuals, internal knowledge bases, FAQs, and troubleshooting guidesbased on query context, the framework prioritizes the most relevant data. Forinstance, it gives precedence to product manuals for SKU-specific queries whileincorporating general FAQs for broader issues. The system employs FAISS forefficient dense vector search, coupled with a dynamic aggregation mechanism toseamlessly integrate results from multiple sources. A Llama-basedself-evaluator ensures the contextual accuracy and confidence of the generatedresponses before delivering them. This iterative cycle of retrieval andvalidation enhances precision, diversity, and reliability in responsegeneration. Preliminary evaluations on large enterprise datasets demonstratethe framework's efficacy in improving troubleshooting accuracy, reducingresolution times, and adapting to varied technical challenges. Future researchaims to enhance the framework by integrating advanced conversational AIcapabilities, enabling more interactive and intuitive troubleshootingexperiences. Efforts will also focus on refining the dynamic weightingmechanism through reinforcement learning to further optimize the relevance andprecision of retrieved information. By incorporating these advancements, theproposed framework is poised to evolve into a comprehensive, autonomous AIsolution, redefining technical service workflows across enterprise settings.</description><author>Rajat Khanda</author><pubDate>Mon, 16 Dec 2024 17:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12006v1</guid></item><item><title>The Open Source Advantage in Large Language Models (LLMs)</title><link>http://arxiv.org/abs/2412.12004v1</link><description>Large language models (LLMs) mark a key shift in natural language processing(NLP), having advanced text generation, translation, and domain-specificreasoning. Closed-source models like GPT-4, powered by proprietary datasets andextensive computational resources, lead with state-of-the-art performancetoday. However, they face criticism for their "black box" nature and forlimiting accessibility in a manner that hinders reproducibility and equitableAI development. By contrast, open-source initiatives like LLaMA and BLOOMprioritize democratization through community-driven development andcomputational efficiency. These models have significantly reduced performancegaps, particularly in linguistic diversity and domain-specific applications,while providing accessible tools for global researchers and developers.Notably, both paradigms rely on foundational architectural innovations, such asthe Transformer framework by Vaswani et al. (2017). Closed-source models excelby scaling effectively, while open-source models adapt to real-worldapplications in underrepresented languages and domains. Techniques likeLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-sourcemodels to achieve competitive results despite limited resources. To be sure,the tension between closed-source and open-source approaches underscores abroader debate on transparency versus proprietary control in AI. Ethicalconsiderations further highlight this divide. Closed-source systems restrictexternal scrutiny, while open-source models promote reproducibility andcollaboration but lack standardized auditing documentation frameworks tomitigate biases. Hybrid approaches that leverage the strengths of bothparadigms are likely to shape the future of LLM innovation, ensuringaccessibility, competitive technical performance, and ethical deployment.</description><author>Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser</author><pubDate>Mon, 16 Dec 2024 17:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12004v1</guid></item><item><title>LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts</title><link>http://arxiv.org/abs/2412.12001v1</link><description>Drafting radiology reports is a complex task requiring flexibility, whereradiologists tail content to available information and particular clinicaldemands. However, most current radiology report generation (RRG) models areconstrained to a fixed task paradigm, such as predicting the full ``finding''section from a single image, inherently involving a mismatch between inputs andoutputs. The trained models lack the flexibility for diverse inputs and couldgenerate harmful, input-agnostic hallucinations. To bridge the gap betweencurrent RRG models and the clinical demands in practice, we first develop adata generation pipeline to create a new MIMIC-RG4 dataset, which considersfour common radiology report drafting scenarios and has perfectly correspondedinput and output. Secondly, we propose a novel large language model (LLM) basedRRG framework, namely LLM-RG4, which utilizes LLM's flexibleinstruction-following capabilities and extensive general knowledge. We furtherdevelop an adaptive token fusion module that offers flexibility to handlediverse scenarios with different input combinations, while minimizing theadditional computational burden associated with increased input volumes.Besides, we propose a token-level loss weighting strategy to direct the model'sattention towards positive and uncertain descriptions. Experimental resultsdemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinicalefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXRdatasets. We quantitatively demonstrate that our model has minimalinput-agnostic hallucinations, whereas current open-source models commonlysuffer from this problem.</description><author>Zhuhao Wang, Yihua Sun, Zihan Li, Xuan Yang, Fang Chen, Hongen Liao</author><pubDate>Mon, 16 Dec 2024 17:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12001v1</guid></item><item><title>CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception</title><link>http://arxiv.org/abs/2412.12000v1</link><description>Collaborative Perception (CP) has shown a promising technique for autonomousdriving, where multiple connected and autonomous vehicles (CAVs) share theirperception information to enhance the overall perception performance and expandthe perception range. However, in CP, ego CAV needs to receive messages fromits collaborators, which makes it easy to be attacked by malicious agents. Forexample, a malicious agent can send harmful information to the ego CAV tomislead it. To address this critical issue, we propose a novel method,\textbf{CP-Guard}, a tailored defense mechanism for CP that can be deployed byeach agent to accurately detect and eliminate malicious agents in itscollaboration network. Our key idea is to enable CP to reach a consensus ratherthan a conflict against the ego CAV's perception results. Based on this idea,we first develop a probability-agnostic sample consensus (PASAC) method toeffectively sample a subset of the collaborators and verify the consensuswithout prior probabilities of malicious agents. Furthermore, we define acollaborative consistency loss (CCLoss) to capture the discrepancy between theego CAV and its collaborators, which is used as a verification criterion forconsensus. Finally, we conduct extensive experiments in collaborative bird'seye view (BEV) tasks and our results demonstrate the effectiveness of ourCP-Guard.</description><author>Senkang Hu, Yihang Tao, Guowen Xu, Yiqin Deng, Xianhao Chen, Yuguang Fang, Sam Kwong</author><pubDate>Mon, 16 Dec 2024 17:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12000v1</guid></item><item><title>SAMIC: Segment Anything with In-Context Spatial Prompt Engineering</title><link>http://arxiv.org/abs/2412.11998v1</link><description>Few-shot segmentation is the problem of learning to identify specific typesof objects (e.g., airplanes) in images from a small set of labeled referenceimages. The current state of the art is driven by resource-intensiveconstruction of models for every new domain-specific application. Such modelsmust be trained on enormous labeled datasets of unrelated objects (e.g., cars,trains, animals) so that their ``knowledge'' can be transferred to new types ofobjects. In this paper, we show how to leverage existing vision foundationmodels (VFMs) to reduce the incremental cost of creating few-shot segmentationmodels for new domains. Specifically, we introduce SAMIC, a small network thatlearns how to prompt VFMs in order to segment new types of objects indomain-specific applications. SAMIC enables any task to be approached as afew-shot learning problem. At 2.6 million parameters, it is 94% smaller thanthe leading models (e.g., having ResNet 101 backbone with 45+ millionparameters). Even using 1/5th of the training data provided by one-shotbenchmarks, SAMIC is competitive with, or sets the state of the art, on avariety of few-shot and semantic segmentation datasets including COCO-$20^i$,Pascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10.</description><author>Savinay Nagendra, Kashif Rashid, Chaopeng Shen, Daniel Kifer</author><pubDate>Mon, 16 Dec 2024 17:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11998v1</guid></item><item><title>Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support</title><link>http://arxiv.org/abs/2412.11995v1</link><description>Caregivers (i.e., parents and members of a child's caring community) areunderappreciated stakeholders in learning analytics. Although caregiverinvolvement can enhance student academic outcomes, many obstacles hinderinvolvement, most notably knowledge gaps with respect to modern schoolcurricula. An emerging topic of interest in learning analytics is hybridtutoring, which includes instructional and motivational support. Caregiversassert similar roles in homework, yet it is unknown how learning analytics cansupport them. Our past work with caregivers suggested that conversationalsupport is a promising method of providing caregivers with the guidance neededto effectively support student learning. We developed a system that providesinstructional support to caregivers through conversational recommendationsgenerated by a Large Language Model (LLM). Addressing known instructionallimitations of LLMs, we use instructional intelligence from tutoring systemswhile conducting prompt engineering experiments with the open-source Llama 3LLM. This LLM generated message recommendations for caregivers supporting theirchild's math practice via chat. Few-shot prompting and combining real-timeproblem-solving context from tutoring systems with examples of tutoringpractices yielded desirable message recommendations. These recommendations wereevaluated with ten middle school caregivers, who valued recommendationsfacilitating content-level support and student metacognition throughself-explanation. We contribute insights into how tutoring systems can best bemerged with LLMs to support hybrid tutoring settings through conversationalassistance, facilitating effective caregiver involvement in tutoring systems.</description><author>Devika Venugopalan, Ziwen Yan, Conrad Borchers, Jionghao Lin, Vincent Aleven</author><pubDate>Mon, 16 Dec 2024 17:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11995v1</guid></item><item><title>Fairness Shields: Safeguarding against Biased Decision Makers</title><link>http://arxiv.org/abs/2412.11994v1</link><description>As AI-based decision-makers increasingly influence human lives, it is agrowing concern that their decisions are often unfair or biased with respect topeople's sensitive attributes, such as gender and race. Most existing biasprevention measures provide probabilistic fairness guarantees in the long run,and it is possible that the decisions are biased on specific instances of shortdecision sequences. We introduce fairness shielding, where a symbolicdecision-maker -- the fairness shield -- continuously monitors the sequence ofdecisions of another deployed black-box decision-maker, and makes interventionsso that a given fairness criterion is met while the total intervention costsare minimized. We present four different algorithms for computing fairnessshields, among which one guarantees fairness over fixed horizons, and threeguarantee fairness periodically after fixed intervals. Given a distributionover future decisions and their intervention costs, our algorithms solvedifferent instances of bounded-horizon optimal control problems with differentlevels of computational costs and optimality guarantees. Our empiricalevaluation demonstrates the effectiveness of these shields in ensuring fairnesswhile maintaining cost efficiency across various scenarios.</description><author>Filip Cano, Thomas A. Henzinger, Bettina Könighofer, Konstantin Kueffner, Kaushik Mallik</author><pubDate>Mon, 16 Dec 2024 17:21:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11994v1</guid></item><item><title>Habit Coach: Customising RAG-based chatbots to support behavior change</title><link>http://arxiv.org/abs/2411.19229v2</link><description>This paper presents the iterative development of Habit Coach, a GPT-basedchatbot designed to support users in habit change through personalizedinteraction. Employing a user-centered design approach, we developed thechatbot using a Retrieval-Augmented Generation (RAG) system, which enablesbehavior personalization without retraining the underlying language model(GPT-4). The system leverages document retrieval and specialized prompts totailor interactions, drawing from Cognitive Behavioral Therapy (CBT) andnarrative therapy techniques. A key challenge in the development process wasthe difficulty of translating declarative knowledge into effective interactionbehaviors. In the initial phase, the chatbot was provided with declarativeknowledge about CBT via reference textbooks and high-level conversationalgoals. However, this approach resulted in imprecise and inefficient behavior,as the GPT model struggled to convert static information into dynamic andcontextually appropriate interactions. This highlighted the limitations ofrelying solely on declarative knowledge to guide chatbot behavior, particularlyin nuanced, therapeutic conversations. Over four iterations, we addressed thisissue by gradually transitioning towards procedural knowledge, refining thechatbot's interaction strategies, and improving its overall effectiveness. Inthe final evaluation, 5 participants engaged with the chatbot over fiveconsecutive days, receiving individualized CBT interventions. The Self-ReportHabit Index (SRHI) was used to measure habit strength before and after theintervention, revealing a reduction in habit strength post-intervention. Theseresults underscore the importance of procedural knowledge in driving effective,personalized behavior change support in RAG-based systems.</description><author>Arian Fooroogh Mand Arabi, Cansu Koyuturk, Michael O'Mahony, Raffaella Calati, Dimitri Ognibene</author><pubDate>Mon, 16 Dec 2024 17:16:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19229v2</guid></item><item><title>ExecRepoBench: Multi-level Executable Code Completion Evaluation</title><link>http://arxiv.org/abs/2412.11990v1</link><description>Code completion has become an essential tool for daily software development.Existing evaluation benchmarks often employ static methods that do not fullycapture the dynamic nature of real-world coding environments and facesignificant challenges, including limited context length, reliance onsuperficial evaluation metrics, and potential overfitting to training datasets.In this work, we introduce a novel framework for enhancing code completion insoftware development through the creation of a repository-level benchmarkExecRepoBench and the instruction corpora Repo-Instruct, aim at improving thefunctionality of open-source large language models (LLMs) in real-world codingscenarios that involve complex interdependencies across multiple files.ExecRepoBench includes 1.2K samples from active Python repositories. Plus, wepresent a multi-level grammar-based completion methodology conditioned on theabstract syntax tree to mask code fragments at various logical units (e.g.statements, expressions, and functions). Then, we fine-tune the open-source LLMwith 7B parameters on Repo-Instruct to produce a strong code completionbaseline model Qwen2.5-Coder-Instruct-C based on the open-source model.Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks,including MultiPL-E and ExecRepoBench, which consistently outperforms priorbaselines across all programming languages. The deployment of \ourmethod{} canbe used as a high-performance, local service for programmingdevelopment\footnote{\url{https://execrepobench.github.io/}}.</description><author>Jian Yang, Jiajun Zhang, Jiaxi Yang, Ke Jin, Lei Zhang, Qiyao Peng, Ken Deng, Yibo Miao, Tianyu Liu, Zeyu Cui, Binyuan Hui, Junyang Lin</author><pubDate>Mon, 16 Dec 2024 17:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11990v1</guid></item><item><title>EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion</title><link>http://arxiv.org/abs/2411.16726v2</link><description>Diffusion models have revolutionized the field of talking head generation,yet still face challenges in expressiveness, controllability, and stability inlong-time generation. In this research, we propose an EmotiveTalk framework toaddress these issues. Firstly, to realize better control over the generation oflip movement and facial expression, a Vision-guided Audio InformationDecoupling (V-AID) approach is designed to generate audio-based decoupledrepresentations aligned with lip movements and expression. Specifically, toachieve alignment between audio and facial expression representation spaces, wepresent a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module withinV-AID to generate expression-related representations under multi-source emotioncondition constraints. Then we propose a well-designed Emotional Talking HeadDiffusion (ETHD) backbone to efficiently generate highly expressive talkinghead videos, which contains an Expression Decoupling Injection (EDI) module toautomatically decouple the expressions from reference portraits whileintegrating the target expression information, achieving more expressivegeneration performance. Experimental results show that EmotiveTalk can generateexpressive talking head videos, ensuring the promised controllability ofemotions and stability during long-time generation, yielding state-of-the-artperformance compared to existing methods.</description><author>Haotian Wang, Yuzhe Weng, Yueyan Li, Zilu Guo, Jun Du, Shutong Niu, Jiefeng Ma, Shan He, Xiaoyan Wu, Qiming Hu, Bing Yin, Cong Liu, Qingfeng Liu</author><pubDate>Mon, 16 Dec 2024 17:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16726v2</guid></item><item><title>SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with a GAN-Inspired Approach to Synthetic Dataset Generation</title><link>http://arxiv.org/abs/2412.11988v1</link><description>Consider the problem: ``If one man and one woman can produce one child in oneyear, how many children will be produced by one woman and three men in 0.5years?" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,and Gemini Flash frequently answer "0.5," which does not make sense. Whilethese models sometimes acknowledge the unrealistic nature of the question, inmany cases (8 out of 10 trials), they provide the nonsensical answer of "0.5child." Additionally, temporal variation has been observed: if an LLM answerscorrectly once (by recognizing the faulty nature of the question), subsequentresponses are more likely to also reflect this understanding. However, this isinconsistent. These types of questions have motivated us to develop a dataset of sciencequestions, SciFaultyQA, where the questions themselves are intentionallyfaulty. We observed that LLMs often proceed to answer these flawed questionswithout recognizing their inherent issues, producing results that are logicallyor scientifically invalid. By analyzing such patterns, we developed a novelmethod for generating synthetic datasets to evaluate and benchmark theperformance of various LLMs in identifying these flawed questions. We have alsodeveloped novel approaches to reduce the errors.</description><author>Debarshi Kundu</author><pubDate>Mon, 16 Dec 2024 17:11:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11988v1</guid></item><item><title>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</title><link>http://arxiv.org/abs/2409.02920v2</link><description>In the rapidly advancing field of robotics, dual-arm coordination and complexobject manipulation are essential capabilities for developing advancedautonomous systems. However, the scarcity of diverse, high-qualitydemonstration data and real-world-aligned evaluation benchmarks severely limitssuch development. To address this, we introduce RoboTwin, a generative digitaltwin framework that uses 3D generative foundation models and large languagemodels to produce diverse expert datasets and provide a real-world-alignedevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin createsvaried digital twins of objects from single 2D images, generating realistic andinteractive scenarios. It also introduces a spatial relation-aware codegeneration framework that combines object annotations with large languagemodels to break down tasks, determine spatial constraints, and generate preciserobotic movement code. Our framework offers a comprehensive benchmark with bothsimulated and real-world data, enabling standardized evaluation and betteralignment between simulated training and real-world performance. We validatedour approach using the open-source COBOT Magic Robot platform. Policiespre-trained on RoboTwin-generated data and fine-tuned with limited real-worldsamples improve the success rate of over 70% for single-arm tasks and over 40%for dual-arm tasks compared to models trained solely on real-world data. Thissignificant improvement demonstrates RoboTwin's potential to enhance thedevelopment and evaluation of dual-arm robotic manipulation systems. ProjectPage: https://robotwin-benchmark.github.io/early-version/.</description><author>Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen, Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie, Ping Luo</author><pubDate>Mon, 16 Dec 2024 17:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02920v2</guid></item><item><title>Speak &amp; Improve Corpus 2025: an L2 English Speech Corpus for Language Assessment and Feedback</title><link>http://arxiv.org/abs/2412.11986v1</link><description>We introduce the Speak \&amp; Improve Corpus 2025, a dataset of L2 learnerEnglish data with holistic scores and language error annotation, collected fromopen (spontaneous) speaking tests on the Speak \&amp; Improve learning platformhttps://speakandimprove.com . The aim of the corpus release is to address amajor challenge to developing L2 spoken language processing systems, the lackof publicly available data with high-quality annotations. It is being madeavailable for non-commercial use on the ELiT website. In designing this corpuswe have sought to make it cover a wide-range of speaker attributes, from theirL1 to their speaking ability, as well as providing manual annotations. Thisenables a range of language-learning tasks to be examined, such as assessingspeaking proficiency or providing feedback on grammatical errors in a learner'sspeech. Additionally, the data supports research into the underlying technologyrequired for these tasks including automatic speech recognition (ASR) of lowresource L2 learner English, disfluency detection or spoken grammatical errorcorrection (GEC). The corpus consists of around 340 hours of L2 Englishlearners audio with holistic scores, and a subset of audio annotated withtranscriptions and error labels.</description><author>Kate Knill, Diane Nicholls, Mark J. F. Gales, Mengjie Qian, Pawel Stroinski</author><pubDate>Mon, 16 Dec 2024 17:07:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11986v1</guid></item><item><title>Speak &amp; Improve Challenge 2025: Tasks and Baseline Systems</title><link>http://arxiv.org/abs/2412.11985v1</link><description>This paper presents the "Speak &amp; Improve Challenge 2025: Spoken LanguageAssessment and Feedback" -- a challenge associated with the ISCA SLaTE 2025Workshop. The goal of the challenge is to advance research on spoken languageassessment and feedback, with tasks associated with both the underlyingtechnology and language learning feedback. Linked with the challenge, the Speak&amp; Improve (S&amp;I) Corpus 2025 is being pre-released, a dataset of L2 learnerEnglish data with holistic scores and language error annotation, collected fromopen (spontaneous) speaking tests on the Speak &amp; Improve learning platform. Thecorpus consists of 340 hours of audio data from second language Englishlearners with holistic scores, and a 60-hour subset with manual transcriptionsand error labels. The Challenge has four shared tasks: Automatic SpeechRecognition (ASR), Spoken Language Assessment (SLA), Spoken Grammatical ErrorCorrection (SGEC), and Spoken Grammatical Error Correction Feedback (SGECF).Each of these tasks has a closed track where a predetermined set of models anddata sources are allowed to be used, and an open track where any publicresource may be used. Challenge participants may do one or more of the tasks.This paper describes the challenge, the S&amp;I Corpus 2025, and the baselinesystems released for the Challenge.</description><author>Mengjie Qian, Kate Knill, Stefano Banno, Siyuan Tang, Penny Karanasou, Mark J. F. Gales, Diane Nicholls</author><pubDate>Mon, 16 Dec 2024 17:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11985v1</guid></item><item><title>Cost-Effective Label-free Node Classification with LLMs</title><link>http://arxiv.org/abs/2412.11983v1</link><description>Graph neural networks (GNNs) have emerged as go-to models for nodeclassification in graph data due to their powerful abilities in fusing graphstructures and attributes. However, such models strongly rely on adequatehigh-quality labeled data for training, which are expensive to acquire inpractice. With the advent of large language models (LLMs), a promising way isto leverage their superb zero-shot capabilities and massive knowledge for nodelabeling. Despite promising results reported, this methodology either demandsconsiderable queries to LLMs, or suffers from compromised performance caused bynoisy labels produced by LLMs. To remedy these issues, this work presents Cella, an active self-trainingframework that integrates LLMs into GNNs in a cost-effective manner. The designrecipe of Cella is to iteratively identify small sets of "critical" samplesusing GNNs and extract informative pseudo-labels for them with both LLMs andGNNs as additional supervision signals to enhance model training. Particularly,Cella includes three major components: (i) an effective active node selectionstrategy for initial annotations; (ii) a judicious sample selection scheme tosift out the "critical" nodes based on label disharmonicity and entropy; and(iii) a label refinement module combining LLMs and GNNs with rewired topology.Our extensive experiments over five benchmark text-attributed graph datasetsdemonstrate that Cella significantly outperforms the state of the arts underthe same query budget to LLMs in terms of label-free node classification. Inparticular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an8.08% conspicuous improvement in accuracy over the state-of-the-art at a costof less than one cent.</description><author>Taiyan Zhang, Renchi Yang, Mingyu Yan, Xiaochun Ye, Dongrui Fan, Yurui Lai</author><pubDate>Mon, 16 Dec 2024 17:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11983v1</guid></item><item><title>Echo State network for coarsening dynamics of charge density waves</title><link>http://arxiv.org/abs/2412.11982v1</link><description>An echo state network (ESN) is a type of reservoir computer that uses arecurrent neural network with a sparsely connected hidden layer. Compared withother recurrent neural networks, one great advantage of ESN is the simplicityof its training process. Yet, despite the seemingly restricted learnableparameters, ESN has been shown to successfully capture the spatial-temporaldynamics of complex patterns. Here we build an ESN to model the coarseningdynamics of charge-density waves (CDW) in a semi-classical Holstein model,which exhibits a checkerboard electron density modulation at half-fillingstabilized by a commensurate lattice distortion. The inputs to the ESN arelocal CDW order-parameters in a finite neighborhood centered around a givensite, while the output is the predicted CDW order of the center site at thenext time step. Special care is taken in the design of couplings between hiddenlayer and input nodes to ensure lattice symmetries are properly incorporatedinto the ESN model. Since the model predictions depend only on CDWconfigurations of a finite domain, the ESN is scalable and transferrable in thesense that a model trained on dataset from a small system can be directlyapplied to dynamical simulations on larger lattices. Our work opens a newavenue for efficient dynamical modeling of pattern formations in functionalelectron materials.</description><author>Clement Dinh, Yunhao Fan, Gia-Wei Chern</author><pubDate>Mon, 16 Dec 2024 17:04:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11982v1</guid></item><item><title>Industrial-scale Prediction of Cement Clinker Phases using Machine Learning</title><link>http://arxiv.org/abs/2412.11981v1</link><description>Cement production, exceeding 4.1 billion tonnes and contributing 2.4 tonnesof CO2 annually, faces critical challenges in quality control and processoptimization. While traditional process models for cement manufacturing areconfined to steady-state conditions with limited predictive capability formineralogical phases, modern plants operate under dynamic conditions thatdemand real-time quality assessment. Here, exploiting a comprehensive two-yearoperational dataset from an industrial cement plant, we present a machinelearning framework that accurately predicts clinker mineralogy from processdata. Our model achieves unprecedented prediction accuracy for major clinkerphases while requiring minimal input parameters, demonstrating robustperformance under varying operating conditions. Through post-hoc explainablealgorithms, we interpret the hierarchical relationships between clinker oxidesand phase formation, providing insights into the functioning of an otherwiseblack-box model. This digital twin framework can potentially enable real-timeoptimization of cement production, thereby providing a route toward reducingmaterial waste and ensuring quality while reducing the associated emissionsunder real plant conditions. Our approach represents a significant advancementin industrial process control, offering a scalable solution for sustainablecement manufacturing.</description><author>Sheikh Junaid Fayaz, Nestor Montiel-Bohorquez, Shashank Bishnoi, Matteo Romano, Manuele Gatti, N. M. Anoop Krishnan</author><pubDate>Mon, 16 Dec 2024 17:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11981v1</guid></item><item><title>AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws</title><link>http://arxiv.org/abs/2412.11979v1</link><description>Neural scaling laws are observed in a range of domains, to date with no clearunderstanding of why they occur. Recent theories suggest that loss power lawsarise from Zipf's law, a power law observed in domains like natural language.One theory suggests that language scaling laws emerge when Zipf-distributedtask quanta are learned in descending order of frequency. In this paper weexamine power-law scaling in AlphaZero, a reinforcement learning algorithm,using a theory of language-model scaling. We find that game states in trainingand inference data scale with Zipf's law, which is known to arise from the treestructure of the environment, and examine the correlation between scaling-lawand Zipf's-law exponents. In agreement with quanta scaling theory, we find thatagents optimize state loss in descending order of frequency, even though thisorder scales inversely with modelling complexity. We also find that inversescaling, the failure of models to improve with size, is correlated with unusualZipf curves where end-game states are among the most frequent states. We showevidence that larger models shift their focus to these less-important states,sacrificing their understanding of important early-game states.</description><author>Oren Neumann, Claudius Gros</author><pubDate>Mon, 16 Dec 2024 16:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11979v1</guid></item><item><title>Speech Foundation Models and Crowdsourcing for Efficient, High-Quality Data Collection</title><link>http://arxiv.org/abs/2412.11978v1</link><description>While crowdsourcing is an established solution for facilitating and scalingthe collection of speech data, the involvement of non-experts necessitatesprotocols to ensure final data quality. To reduce the costs of these essentialcontrols, this paper investigates the use of Speech Foundation Models (SFMs) toautomate the validation process, examining for the first time the cost/qualitytrade-off in data acquisition. Experiments conducted on French, German, andKorean data demonstrate that SFM-based validation has the potential to reducereliance on human validation, resulting in an estimated cost saving of over40.0% without degrading final data quality. These findings open newopportunities for more efficient, cost-effective, and scalable speech dataacquisition.</description><author>Beomseok Lee, Marco Gaido, Ioan Calapodescu, Laurent Besacier, Matteo Negri</author><pubDate>Mon, 16 Dec 2024 16:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11978v1</guid></item><item><title>Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning</title><link>http://arxiv.org/abs/2412.11974v1</link><description>Traditional reinforcement learning-based robotic control methods are oftentask-specific and fail to generalize across diverse environments or unseenobjects and instructions. Visual Language Models (VLMs) demonstrate strongscene understanding and planning capabilities but lack the ability to generateactionable policies tailored to specific robotic embodiments. To address this,Visual-Language-Action (VLA) models have emerged, yet they face challenges inlong-horizon spatial reasoning and grounded task planning. In this work, wepropose the Embodied Multimodal Action Model with Grounded Chain of Thought andLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructedhierarchical embodiment dataset based on BridgeV2, containing 60,000 robotmanipulation trajectories auto-annotated with grounded task reasoning andspatial guidance. Additionally, we introduce a trajectory segmentation strategybased on gripper states and motion trajectories, which can help mitigatehallucination in grounding subtask reasoning generation. Experimental resultsdemonstrate that Emma-X achieves superior performance over competitivebaselines, particularly in real-world robotic tasks requiring spatialreasoning.</description><author>Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, U-Xuan Tan, Deepanway Ghosal, Soujanya Poria</author><pubDate>Mon, 16 Dec 2024 16:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11974v1</guid></item><item><title>DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images</title><link>http://arxiv.org/abs/2409.06694v2</link><description>Cancer is a complex disease characterized by uncontrolled cell growth. T cellreceptors (TCRs), crucial proteins in the immune system, play a key role inrecognizing antigens, including those associated with cancer. Recentadvancements in sequencing technologies have facilitated comprehensiveprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activityand enabling TCR-based immunotherapies. However, analyzing these intricatebiomolecules necessitates efficient representations that capture theirstructural and functional information. T-cell protein sequences pose uniquechallenges due to their relatively smaller lengths compared to otherbiomolecules. An image-based representation approach becomes a preferred choicefor efficient embeddings, allowing for the preservation of essential detailsand enabling comprehensive analysis of T-cell protein sequences. In this paper,we propose to generate images from the protein sequences using the idea ofChaos Game Representation (CGR) using the Kaleidoscopic images approach. ThisDeep Learning Assisted Analysis of Protein Sequences Using Chaos EnhancedKaleidoscopic Images (called DANCE) provides a unique way to visualize proteinsequences by recursively applying chaos game rules around a central seed point.we perform the classification of the T cell receptors (TCRs) protein sequencesin terms of their respective target cancer cells, as TCRs are known for theirimmune response against cancer disease. The TCR sequences are converted intoimages using the DANCE method. We employ deep-learning vision models to performthe classification to obtain insights into the relationship between the visualpatterns observed in the generated kaleidoscopic images and the underlyingprotein properties. By combining CGR-based image generation with deep learningclassification, this study opens novel possibilities in the protein analysisdomain.</description><author>Taslim Murad, Prakash Chourasia, Sarwan Ali, Imdad Ullah Khan, Murray Patterson</author><pubDate>Mon, 16 Dec 2024 16:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06694v2</guid></item><item><title>Neural general circulation models optimized to predict satellite-based precipitation observations</title><link>http://arxiv.org/abs/2412.11973v1</link><description>Climate models struggle to accurately simulate precipitation, particularlyextremes and the diurnal cycle. Here, we present a hybrid model that is traineddirectly on satellite-based precipitation observations. Our model runs at2.8$^\circ$ resolution and is built on the differentiable NeuralGCM framework.The model demonstrates significant improvements over existing generalcirculation models, the ERA5 reanalysis, and a global cloud-resolving model insimulating precipitation. Our approach yields reduced biases, a more realisticprecipitation distribution, improved representation of extremes, and a moreaccurate diurnal cycle. Furthermore, it outperforms the mid-range precipitationforecast of the ECMWF ensemble. This advance paves the way for more reliablesimulations of current climate and demonstrates how training on observationscan be used to directly improve GCMs.</description><author>Janni Yuval, Ian Langmore, Dmitrii Kochkov, Stephan Hoyer</author><pubDate>Mon, 16 Dec 2024 16:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11973v1</guid></item><item><title>Controllable Shadow Generation with Single-Step Diffusion Models from Synthetic Data</title><link>http://arxiv.org/abs/2412.11972v1</link><description>Realistic shadow generation is a critical component for high-quality imagecompositing and visual effects, yet existing methods suffer from certainlimitations: Physics-based approaches require a 3D scene geometry, which isoften unavailable, while learning-based techniques struggle with control andvisual artifacts. We introduce a novel method for fast, controllable, andbackground-free shadow generation for 2D object images. We create a largesynthetic dataset using a 3D rendering engine to train a diffusion model forcontrollable shadow generation, generating shadow maps for diverse light sourceparameters. Through extensive ablation studies, we find that rectified flowobjective achieves high-quality results with just a single sampling stepenabling real-time applications. Furthermore, our experiments demonstrate thatthe model generalizes well to real-world images. To facilitate further researchin evaluating quality and controllability in shadow generation, we release anew public benchmark containing a diverse set of object images and shadow mapsin various settings. The project page is available athttps://gojasper.github.io/controllable-shadow-generation-project/</description><author>Onur Tasar, Clément Chadebec, Benjamin Aubin</author><pubDate>Mon, 16 Dec 2024 16:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11972v1</guid></item><item><title>Multiplex Dirichlet stochastic block model for clustering multidimensional compositional networks</title><link>http://arxiv.org/abs/2412.11971v1</link><description>Network data often represent multiple types of relations, which can alsodenote exchanged quantities, and are typically encompassed in a weightedmultiplex. Such data frequently exhibit clustering structures, however,traditional clustering methods are not well-suited for multiplex networks.Additionally, standard methods treat edge weights in their raw form,potentially biasing clustering towards a node's total weight capacity ratherthan reflecting cluster-related interaction patterns. To address this, wepropose transforming edge weights into a compositional format, enabling theanalysis of connection strengths in relative terms and removing the impact ofnodes' total weights. We introduce a multiplex Dirichlet stochastic block modeldesigned for multiplex networks with compositional layers. This model accountsfor sparse compositional networks and enables joint clustering across differenttypes of interactions. We validate the model through a simulation study andapply it to the international export data from the Food and AgricultureOrganization of the United Nations.</description><author>Iuliia Promskaia, Adrian O'Hagan, Michael Fop</author><pubDate>Mon, 16 Dec 2024 16:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11971v1</guid></item><item><title>DARWIN 1.5: Large Language Models as Materials Science Adapted Learners</title><link>http://arxiv.org/abs/2412.11970v1</link><description>Materials discovery and design aim to find components and structures withdesirable properties over highly complex and diverse search spaces. Traditionalsolutions, such as high-throughput simulations and machine learning (ML), oftenrely on complex descriptors, which hinder generalizability and transferabilityacross tasks. Moreover, these descriptors may deviate from experimental datadue to inevitable defects and purity issues in the real world, which may reducetheir effectiveness in practical applications. To address these challenges, wepropose Darwin 1.5, an open-source large language model (LLM) tailored formaterials science. By leveraging natural language as input, Darwin eliminatesthe need for task-specific descriptors and enables a flexible, unified approachto material property prediction and discovery. We employ a two-stage trainingstrategy combining question-answering (QA) fine-tuning with multi-task learning(MTL) to inject domain-specific knowledge in various modalities and facilitatecross-task knowledge transfer. Through our strategic approach, we achieved asignificant enhancement in the prediction accuracy of LLMs, with a maximumimprovement of 60\% compared to LLaMA-7B base models. It further outperformstraditional machine learning models on various tasks in material science,showcasing the potential of LLMs to provide a more versatile and scalablefoundation model for materials discovery and design.</description><author>Tong Xie, Yuwei Wan, Yixuan Liu, Yuchen Zeng, Wenjie Zhang, Chunyu Kit, Dongzhan Zhou, Bram Hoex</author><pubDate>Mon, 16 Dec 2024 16:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11970v1</guid></item><item><title>A Digital twin for Diesel Engines: Operator-infused PINNs with Transfer Learning for Engine Health Monitoring</title><link>http://arxiv.org/abs/2412.11967v1</link><description>Improving diesel engine efficiency and emission reduction have been criticalresearch topics. Recent government regulations have shifted this focus toanother important area related to engine health and performance monitoring.Although the advancements in the use of deep learning methods for systemmonitoring have shown promising results in this direction, designing efficientmethods suitable for field systems remains an open research challenge. Theobjective of this study is to develop a computationally efficient neuralnetwork-based approach for identifying unknown parameters of a mean valuediesel engine model to facilitate physics-based health monitoring andmaintenance forecasting. We propose a hybrid method combining physics informedneural networks, PINNs, and a deep neural operator, DeepONet to predict unknownparameters and gas flow dynamics in a diesel engine. The operator networkpredicts independent actuator dynamics learnt through offline training, therebyreducing the PINNs online computational cost. To address PINNs need forretraining with changing input scenarios, we propose two transfer learning (TL)strategies. The first strategy involves multi-stage transfer learning forparameter identification. While this method is computationally efficient ascompared to online PINN training, improvements are required to meet fieldrequirements. The second TL strategy focuses solely on training the outputweights and biases of a subset of multi-head networks pretrained on a largerdataset, substantially reducing computation time during online prediction. Wealso evaluate our model for epistemic and aleatoric uncertainty byincorporating dropout in pretrained networks and Gaussian noise in the trainingdataset. This strategy offers a tailored, computationally inexpensive, andphysics-based approach for parameter identification in diesel engine subsystems.</description><author>Kamaljyoti Nath, Varun Kumar, Daniel J. Smith, George Em Karniadakis</author><pubDate>Mon, 16 Dec 2024 16:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11967v1</guid></item><item><title>GSDiff: Synthesizing Vector Floorplans via Geometry-enhanced Structural Graph Generation</title><link>http://arxiv.org/abs/2408.16258v2</link><description>Automating architectural floorplan design is vital for housing and interiordesign, offering a faster, cost-effective alternative to manual sketches byarchitects. However, existing methods, including rule-based and learning-basedapproaches, face challenges in design complexity and constrained generationwith extensive post-processing, and tend to obvious geometric inconsistenciessuch as misalignment, overlap, and gaps. In this work, we propose a novelgenerative framework for vector floorplan design via structural graphgeneration, called GSDiff, focusing on wall junction generation and wallsegment prediction to capture both geometric and semantic aspects of structuralgraphs. To improve the geometric rationality of generated structural graphs, wepropose two innovative geometry enhancement methods. In wall junctiongeneration, we propose a novel alignment loss function to improve geometricconsistency. In wall segment prediction, we propose a random self-supervisionmethod to enhance the model's perception of the overall geometric structure,thereby promoting the generation of reasonable geometric structures. Employingthe diffusion model and the Transformer model, as well as the geometryenhancement strategies, our framework can generate wall junctions, wallsegments and room polygons with structural and semantic information, resultingin structural graphs that accurately represent floorplans. Extensiveexperiments show that the proposed method surpasses existing techniques,enabling free generation and constrained generation, marking a shift towardsstructure generation in architectural design. Code and data are available athttps://github.com/SizheHu/GSDiff.</description><author>Sizhe Hu, Wenming Wu, Yuntao Wang, Benzhu Xu, Liping Zheng</author><pubDate>Mon, 16 Dec 2024 16:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16258v2</guid></item><item><title>Inferring Functionality of Attention Heads from their Parameters</title><link>http://arxiv.org/abs/2412.11965v1</link><description>Attention heads are one of the building blocks of large language models(LLMs). Prior work on investigating their operation mostly focused on analyzingtheir behavior during inference for specific circuits or tasks. In this work,we seek a comprehensive mapping of the operations they implement in a model. Wepropose MAPS (Mapping Attention head ParameterS), an efficient framework thatinfers the functionality of attention heads from their parameters, without anymodel training or inference. We showcase the utility of MAPS for answering twotypes of questions: (a) given a predefined operation, mapping how stronglyheads across the model implement it, and (b) given an attention head, inferringits salient functionality. Evaluating MAPS on 20 operations across 6 popularLLMs shows its estimations correlate with the head's outputs during inferenceand are causally linked to the model's predictions. Moreover, its mappingsreveal attention heads of certain operations that were overlooked in previousstudies, and valuable insights on function universality and architecture biasesin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPSto characterize the salient operations of a given head. Our pipeline producesplausible operation descriptions for most heads, as assessed by human judgment,while revealing diverse operations.</description><author>Amit Elhelo, Mor Geva</author><pubDate>Mon, 16 Dec 2024 16:45:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11965v1</guid></item><item><title>BetaExplainer: A Probabilistic Method to Explain Graph Neural Networks</title><link>http://arxiv.org/abs/2412.11964v1</link><description>Graph neural networks (GNNs) are powerful tools for conducting inference ongraph data but are often seen as "black boxes" due to difficulty in extractingmeaningful subnetworks driving predictive performance. Many interpretable GNNmethods exist, but they cannot quantify uncertainty in edge weights and sufferin predictive accuracy when applied to challenging graph structures. In thiswork, we proposed BetaExplainer which addresses these issues by using asparsity-inducing prior to mask unimportant edges during model training. Toevaluate our approach, we examine various simulated data sets with diversereal-world characteristics. Not only does this implementation provide a notionof edge importance uncertainty, it also improves upon evaluation metrics forchallenging datasets compared to state-of-the art explainer methods.</description><author>Whitney Sloneker, Shalin Patel, Michael Wang, Lorin Crawford, Ritambhara Singh</author><pubDate>Mon, 16 Dec 2024 16:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11964v1</guid></item><item><title>When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations</title><link>http://arxiv.org/abs/2411.12701v2</link><description>Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,where triggers embedded in poisoned samples can maliciously alter LLMs'behaviors. In this paper, we move beyond attacking LLMs and instead examinebackdoor attacks through the novel lens of natural language explanations.Specifically, we leverage LLMs' generative capabilities to producehuman-readable explanations for their decisions, enabling direct comparisonsbetween explanations for clean and poisoned samples. Our results show thatbackdoored models produce coherent explanations for clean inputs but diverseand logically flawed explanations for poisoned data, a pattern consistentacross classification and generation tasks for different backdoor attacks.Further analysis reveals key insights into the explanation generation process.At the token level, explanation tokens associated with poisoned samples onlyappear in the final few transformer layers. At the sentence level, attentiondynamics indicate that poisoned inputs shift attention away from the originalinput context during explanation generation. These findings enhance ourunderstanding of backdoor mechanisms in LLMs and present a promising frameworkfor detecting vulnerabilities through explainability.</description><author>Huaizhi Ge, Yiming Li, Qifan Wang, Yongfeng Zhang, Ruixiang Tang</author><pubDate>Mon, 16 Dec 2024 16:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12701v2</guid></item><item><title>Gramian Multimodal Representation Learning and Alignment</title><link>http://arxiv.org/abs/2412.11959v1</link><description>Human perception integrates multiple modalities, such as vision, hearing, andlanguage, into a unified understanding of the surrounding reality. While recentmultimodal models have achieved significant progress by aligning pairs ofmodalities via contrastive learning, their solutions are unsuitable whenscaling to multiple modalities. These models typically align each modality to adesignated anchor without ensuring the alignment of all modalities with eachother, leading to suboptimal performance in tasks requiring a jointunderstanding of multiple modalities. In this paper, we structurally rethinkthe pairwise conventional approach to multimodal learning and we present thenovel Gramian Representation Alignment Measure (GRAM), which overcomes theabove-mentioned limitations. GRAM learns and then aligns $n$ modalitiesdirectly in the higher-dimensional space in which modality embeddings lie byminimizing the Gramian volume of the $k$-dimensional parallelotope spanned bythe modality vectors, ensuring the geometric alignment of all modalitiessimultaneously. GRAM can replace cosine similarity in any downstream method,holding for 2 to $n$ modality and providing more meaningful alignment withrespect to previous similarity measures. The novel GRAM-based contrastive lossfunction enhances the alignment of multimodal models in the higher-dimensionalembedding space, leading to new state-of-the-art performance in downstreamtasks such as video-audio-text retrieval and audio-video classification. Theproject page, the code, and the pretrained models are available athttps://ispamm.github.io/GRAM/.</description><author>Giordano Cicchetti, Eleonora Grassucci, Luigi Sigillo, Danilo Comminiello</author><pubDate>Mon, 16 Dec 2024 16:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11959v1</guid></item><item><title>TurboAttention: Efficient Attention Approximation For High Throughputs LLMs</title><link>http://arxiv.org/abs/2412.08585v2</link><description>Large language model (LLM) inference demands significant amount ofcomputation and memory, especially in the key attention mechanism. Whiletechniques, such as quantization and acceleration algorithms, likeFlashAttention, have improved efficiency of the overall inference, they addressdifferent aspects of the problem: quantization focuses on weight-activationoperations, while FlashAttention improves execution but requires high-precisionformats. Recent Key-value (KV) cache quantization reduces memory bandwidth butstill needs floating-point dequantization for attention operation. We present TurboAttention, a comprehensive approach to enable quantizedexecution of attention that simultaneously addresses both memory andcomputational efficiency. Our solution introduces two key innovations: FlashQ,a headwise attention quantization technique that enables both compression of KVcache and quantized execution of activation-activation multiplication, andSparsity-based Softmax Approximation (SAS), which eliminates the need fordequantization to FP32 during exponentiation operation in attention.Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedupin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37xmaximum throughput over the FP16 baseline while outperforming state-of-the-artquantization and compression techniques across various datasets and models.</description><author>Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan</author><pubDate>Mon, 16 Dec 2024 16:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08585v2</guid></item><item><title>Reliable Breast Cancer Molecular Subtype Prediction based on uncertainty-aware Bayesian Deep Learning by Mammography</title><link>http://arxiv.org/abs/2412.11953v1</link><description>Breast cancer is a heterogeneous disease with different molecular subtypes,clinical behavior, treatment responses as well as survival outcomes. Thedevelopment of a reliable, accurate, available and inexpensive method topredict the molecular subtypes using medical images plays an important role inthe diagnosis and prognosis of breast cancer. Recently, deep learning methodshave shown good performance in the breast cancer classification tasks usingvarious medical images. Despite all that success, classical deep learningcannot deliver the predictive uncertainty. The uncertainty represents thevalidity of the predictions.Therefore, the high predicted uncertainty mightcause a negative effect in the accurate diagnosis of breast cancer molecularsubtypes. To overcome this, uncertainty quantification methods are used todetermine the predictive uncertainty. Accordingly, in this study, we proposedan uncertainty-aware Bayesian deep learning model using the full mammogramimages. In addition, to increase the performance of the multi-class molecularsubtype classification task, we proposed a novel hierarchical classificationstrategy, named the two-stage classification strategy. The separate AUC of theproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,luminal and triple-negative classes, respectively. The proposed model not onlyhas a comparable performance to other studies in the field of breast cancermolecular subtypes prediction, even using full mammography images, but it isalso more reliable, due to quantify the predictive uncertainty.</description><author>Mohaddeseh Chegini, Ali Mahloojifar</author><pubDate>Mon, 16 Dec 2024 16:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11953v1</guid></item><item><title>Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided Self-Supervised Learning</title><link>http://arxiv.org/abs/2412.11952v1</link><description>Image Aesthetic Assessment (IAA) is a vital and intricate task that entailsanalyzing and assessing an image's aesthetic values, and identifying itshighlights and areas for improvement. Traditional methods of IAA oftenconcentrate on a single aesthetic task and suffer from inadequate labeleddatasets, thus impairing in-depth aesthetic comprehension. Despite efforts toovercome this challenge through the application of Multi-modal Large LanguageModels (MLLMs), such models remain underdeveloped for IAA purposes. To addressthis, we propose a comprehensive aesthetic MLLM capable of nuanced aestheticinsight. Central to our approach is an innovative multi-scale text-guidedself-supervised learning technique. This technique features a multi-scalefeature alignment module and capitalizes on a wealth of unlabeled data in aself-supervised manner to structurally and functionally enhance aestheticability. The empirical evidence indicates that accompanied with extensiveinstruct-tuning, our model sets new state-of-the-art benchmarks across multipletasks, including aesthetic scoring, aesthetic commenting, and personalizedimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learningcapabilities in the emerging task of aesthetic suggesting. Furthermore, forpersonalized image aesthetic assessment, we harness the potential of in-contextlearning and showcase its inherent advantages.</description><author>Yuti Liu, Shice Liu, Junyuan Gao, Pengtao Jiang, Hao Zhang, Jinwei Chen, Bo Li</author><pubDate>Mon, 16 Dec 2024 16:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11952v1</guid></item><item><title>The Impact of Generalization Techniques on the Interplay Among Privacy, Utility, and Fairness in Image Classification</title><link>http://arxiv.org/abs/2412.11951v1</link><description>This study investigates the trade-offs between fairness, privacy, and utilityin image classification using machine learning (ML). Recent research suggeststhat generalization techniques can improve the balance between privacy andutility. One focus of this work is sharpness-aware training (SAT) and itsintegration with differential privacy (DP-SAT) to further improve this balance.Additionally, we examine fairness in both private and non-private learningmodels trained on datasets with synthetic and real-world biases. We alsomeasure the privacy risks involved in these scenarios by performing membershipinference attacks (MIAs) and explore the consequences of eliminatinghigh-privacy risk samples, termed outliers. Moreover, we introduce a newmetric, named \emph{harmonic score}, which combines accuracy, privacy, andfairness into a single measure. Through empirical analysis using generalization techniques, we achieve anaccuracy of 81.11\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\%reported by De et al. (2022). Moreover, our experiments show that memorizationof training samples can begin before the overfitting point, and generalizationtechniques do not guarantee the prevention of this memorization. Our analysisof synthetic biases shows that generalization techniques can amplify model biasin both private and non-private models. Additionally, our results indicate thatincreased bias in training data leads to reduced accuracy, greatervulnerability to privacy attacks, and higher model bias. We validate thesefindings with the CelebA dataset, demonstrating that similar trends persistwith real-world attribute imbalances. Finally, our experiments show thatremoving outlier data decreases accuracy and further amplifies model bias.</description><author>Ahmad Hassanpour, Amir Zarei, Khawla Mallat, Anderson Santana de Oliveira, Bian Yang</author><pubDate>Mon, 16 Dec 2024 16:35:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11951v1</guid></item><item><title>Asynchronous Distributed Gaussian Process Regression for Online Learning and Dynamical Systems: Complementary Document</title><link>http://arxiv.org/abs/2412.11950v1</link><description>This is a complementary document for the paper titled "AsynchronousDistributed Gaussian Process Regression for Online Learning and DynamicalSystems".</description><author>Zewen Yang, Xiaobing Dai, Sandra Hirche</author><pubDate>Mon, 16 Dec 2024 16:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11950v1</guid></item><item><title>Fast and Interpretable Mortality Risk Scores for Critical Care Patients</title><link>http://arxiv.org/abs/2311.13015v2</link><description>Prediction of mortality in intensive care unit (ICU) patients typicallyrelies on black box models (that are unacceptable for use in hospitals) orhand-tuned interpretable models (that might lead to the loss in performance).We aim to bridge the gap between these two categories by building on moderninterpretable ML techniques to design interpretable mortality risk scores thatare as accurate as black boxes. We developed a new algorithm, GroupFasterRisk,which has several important benefits: it uses both hard and soft directsparsity regularization, it incorporates group sparsity to allow more cohesivemodels, it allows for monotonicity constraint to include domain knowledge, andit produces many equally-good models, which allows domain experts to chooseamong them. For evaluation, we leveraged the largest existing public ICUmonitoring datasets (MIMIC III and eICU). Models produced by GroupFasterRiskoutperformed OASIS and SAPS II scores and performed similarly to APACHE IV/IVawhile using at most a third of the parameters. For patients withsepsis/septicemia, acute myocardial infarction, heart failure, and acute kidneyfailure, GroupFasterRisk models outperformed OASIS and SOFA. Finally, differentmortality prediction ML approaches performed better based on variables selectedby GroupFasterRisk as compared to OASIS variables. GroupFasterRisk's modelsperformed better than risk scores currently used in hospitals, and on par withblack box ML models, while being orders of magnitude sparser. BecauseGroupFasterRisk produces a variety of risk scores, it allows design flexibility- the key enabler of practical model creation. GroupFasterRisk is a fast,accessible, and flexible procedure that allows learning a diverse set of sparserisk scores for mortality prediction.</description><author>Chloe Qinyu Zhu, Muhang Tian, Lesia Semenova, Jiachang Liu, Jack Xu, Joseph Scarpa, Cynthia Rudin</author><pubDate>Mon, 16 Dec 2024 16:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13015v2</guid></item><item><title>Coconut Palm Tree Counting on Drone Images with Deep Object Detection and Synthetic Training Data</title><link>http://arxiv.org/abs/2412.11949v1</link><description>Drones have revolutionized various domains, including agriculture. Recentadvances in deep learning have propelled among other things object detection incomputer vision. This study utilized YOLO, a real-time object detector, toidentify and count coconut palm trees in Ghanaian farm drone footage. The farmpresented has lost track of its trees due to different planting phases. Whilemanual counting would be very tedious and error-prone, accurately determiningthe number of trees is crucial for efficient planning and management ofagricultural processes, especially for optimizing yields and predictingproduction. We assessed YOLO for palm detection within a semi-automatedframework, evaluated accuracy augmentations, and pondered its potential forfarmers. Data was captured in September 2022 via drones. To optimize YOLO withscarce data, synthetic images were created for model training and validation.The YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), wasadapted using tailored data. Trees from footage were repositioned on syntheticimages, with testing on distinct authentic images. In our experiments, weadjusted hyperparameters, improving YOLO's mean average precision (mAP). Wealso tested various altitudes to determine the best drone height. From aninitial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of syntheticimages in agricultural scenarios.</description><author>Tobias Rohe, Barbara Böhm, Michael Kölle, Jonas Stein, Robert Müller, Claudia Linnhoff-Popien</author><pubDate>Mon, 16 Dec 2024 16:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11949v1</guid></item><item><title>OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews</title><link>http://arxiv.org/abs/2412.11948v1</link><description>We present OpenReviewer, an open-source system for generating high-qualitypeer reviews of machine learning and AI conference papers. At its core isLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tunedon 79,000 expert reviews from top ML conferences. Given a PDF paper submissionand review template as input, OpenReviewer extracts the full text, includingtechnical content like equations and tables, and generates a structured reviewfollowing conference-specific guidelines. Our evaluation on 400 test papersshows that OpenReviewer produces significantly more critical and realisticreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While otherLLMs tend toward overly positive assessments, OpenReviewer's recommendationsclosely match the distribution of human reviewer ratings. The system providesauthors with rapid, constructive feedback to improve their manuscripts beforesubmission, though it is not intended to replace human peer review.OpenReviewer is available as an online demo and open-source tool.</description><author>Maximilian Idahl, Zahra Ahmadi</author><pubDate>Mon, 16 Dec 2024 16:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11948v1</guid></item><item><title>Risk and cross validation in ridge regression with correlated samples</title><link>http://arxiv.org/abs/2408.04607v3</link><description>Recent years have seen substantial advances in our understanding ofhigh-dimensional ridge regression, but existing theories assume that trainingexamples are independent. By leveraging techniques from random matrix theoryand free probability, we provide sharp asymptotics for the in- andout-of-sample risks of ridge regression when the data points have arbitrarycorrelations. We demonstrate that in this setting, the generalized crossvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.However, in the case where the noise residuals have the same correlations asthe data points, one can modify the GCV to yield an efficiently-computableunbiased estimator that concentrates in the high-dimensional limit, which wedub CorrGCV. We further extend our asymptotic analysis to the case where thetest point has nontrivial correlations with the training set, a setting oftenencountered in time series forecasting. Assuming knowledge of the correlationstructure of the time series, this again yields an extension of the GCVestimator, and sharply characterizes the degree to which such test points yieldan overly optimistic prediction of long-time risk. We validate the predictionsof our theory across a variety of high dimensional data.</description><author>Alexander Atanasov, Jacob A. Zavatone-Veth, Cengiz Pehlevan</author><pubDate>Mon, 16 Dec 2024 16:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04607v3</guid></item><item><title>autrainer: A Modular and Extensible Deep Learning Toolkit for Computer Audition Tasks</title><link>http://arxiv.org/abs/2412.11943v1</link><description>This work introduces the key operating principles for autrainer, our new deeplearning training framework for computer audition tasks. autrainer is aPyTorch-based toolkit that allows for rapid, reproducible, and easilyextensible training on a variety of different computer audition tasks.Concretely, autrainer offers low-code training and supports a wide range ofneural networks as well as preprocessing routines. In this work, we present anoverview of its inner workings and key capabilities.</description><author>Simon Rampp, Andreas Triantafyllopoulos, Manuel Milling, Björn W. Schuller</author><pubDate>Mon, 16 Dec 2024 16:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11943v1</guid></item><item><title>The Impact of Token Granularity on the Predictive Power of Language Model Surprisal</title><link>http://arxiv.org/abs/2412.11940v1</link><description>Word-by-word language model surprisal is often used to model the incrementalprocessing of human readers, which raises questions about how various choicesin language modeling influence its predictive power. One factor that has beenoverlooked in cognitive modeling is the granularity of subword tokens, whichexplicitly encodes information about word length and frequency, and ultimatelyinfluences the quality of vector representations that are learned. This paperpresents experiments that manipulate the token granularity and evaluate itsimpact on the ability of surprisal to account for processing difficulty ofnaturalistic text and garden-path constructions. Experiments with naturalisticreading times reveal a substantial influence of token granularity on surprisal,with tokens defined by a vocabulary size of 8,000 resulting in surprisal thatis most predictive. In contrast, on garden-path constructions, language modelstrained on coarser-grained tokens generally assigned higher surprisal tocritical regions, suggesting their increased sensitivity to syntax. Takentogether, these results suggest a large role of token granularity on thequality of language model surprisal for cognitive modeling.</description><author>Byung-Doh Oh, William Schuler</author><pubDate>Mon, 16 Dec 2024 16:24:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11940v1</guid></item><item><title>SEAGraph: Unveiling the Whole Story of Paper Review Comments</title><link>http://arxiv.org/abs/2412.11939v1</link><description>Peer review, as a cornerstone of scientific research, ensures the integrityand quality of scholarly work by providing authors with objective feedback forrefinement. However, in the traditional peer review process, authors oftenreceive vague or insufficiently detailed feedback, which provides limitedassistance and leads to a more time-consuming review cycle. If authors canidentify some specific weaknesses in their paper, they can not only address thereviewer's concerns but also improve their work. This raises the criticalquestion of how to enhance authors' comprehension of review comments. In thispaper, we present SEAGraph, a novel framework developed to clarify reviewcomments by uncovering the underlying intentions behind them. We construct twotypes of graphs for each paper: the semantic mind graph, which captures theauthor's thought process, and the hierarchical background graph, whichdelineates the research domains related to the paper. A retrieval method isthen designed to extract relevant content from both graphs, facilitatingcoherent explanations for the review comments. Extensive experiments show thatSEAGraph excels in review comment understanding tasks, offering significantbenefits to authors.</description><author>Jianxiang Yu, Jiaqi Tan, Zichen Ding, Jiapeng Zhu, Jiahao Li, Yao Cheng, Qier Cui, Yunshi Lan, Xiang Li</author><pubDate>Mon, 16 Dec 2024 16:24:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11939v1</guid></item><item><title>Are the Latent Representations of Foundation Models for Pathology Invariant to Rotation?</title><link>http://arxiv.org/abs/2412.11938v1</link><description>Self-supervised foundation models for digital pathology encode small patchesfrom H\&amp;E whole slide images into latent representations used for downstreamtasks. However, the invariance of these representations to patch rotationremains unexplored. This study investigates the rotational invariance of latentrepresentations across twelve foundation models by quantifying the alignmentbetween non-rotated and rotated patches using mutual $k$-nearest neighbours andcosine distance. Models that incorporated rotation augmentation duringself-supervised training exhibited significantly greater invariance torotations. We hypothesise that the absence of rotational inductive bias in thetransformer architecture necessitates rotation augmentation during training toachieve learned invariance. Code:https://github.com/MatousE/rot-invariance-analysis.</description><author>Matouš Elphick, Samra Turajlic, Guang Yang</author><pubDate>Mon, 16 Dec 2024 16:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11938v1</guid></item><item><title>Precise Length Control in Large Language Models</title><link>http://arxiv.org/abs/2412.11937v1</link><description>Large Language Models (LLMs) are increasingly used in production systems,powering applications such as chatbots, summarization, and question answering.Despite their success, controlling the length of their response remains asignificant challenge, particularly for tasks requiring structured outputs orspecific levels of detail. In this work, we propose a method to adaptpre-trained decoder-only LLMs for precise control of response length. Ourapproach incorporates a secondary length-difference positional encoding (LDPE)into the input embeddings, which counts down to a user-set response terminationlength. Fine-tuning with LDPE allows the model to learn to terminate responsescoherently at the desired length, achieving mean token errors of less than 3tokens. We also introduce Max New Tokens++, an extension that enables flexibleupper-bound length control, rather than an exact target. Experimental resultson tasks such as question answering and document summarization demonstrate thatour method enables precise length control without compromising responsequality.</description><author>Bradley Butcher, Michael O'Keefe, James Titchener</author><pubDate>Mon, 16 Dec 2024 16:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11937v1</guid></item><item><title>A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method &amp; Challenges</title><link>http://arxiv.org/abs/2412.11936v1</link><description>Mathematical reasoning, a core aspect of human cognition, is vital acrossmany domains, from educational problem-solving to scientific advancements. Asartificial general intelligence (AGI) progresses, integrating large languagemodels (LLMs) with mathematical reasoning tasks is becoming increasinglysignificant. This survey provides the first comprehensive analysis ofmathematical reasoning in the era of multimodal large language models (MLLMs).We review over 200 studies published since 2021, and examine thestate-of-the-art developments in Math-LLMs, with a focus on multimodalsettings. We categorize the field into three dimensions: benchmarks,methodologies, and challenges. In particular, we explore multimodalmathematical reasoning pipeline, as well as the role of (M)LLMs and theassociated methodologies. Finally, we identify five major challenges hinderingthe realization of AGI in this domain, offering insights into the futuredirection for enhancing multimodal reasoning capabilities. This survey servesas a critical resource for the research community in advancing the capabilitiesof LLMs to tackle complex multimodal reasoning tasks.</description><author>Yibo Yan, Jiamin Su, Jianxiang He, Fangteng Fu, Xu Zheng, Yuanhuiyi Lyu, Kun Wang, Shen Wang, Qingsong Wen, Xuming Hu</author><pubDate>Mon, 16 Dec 2024 16:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11936v1</guid></item><item><title>Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent</title><link>http://arxiv.org/abs/2410.16658v2</link><description>Adsorption energy is a key reactivity descriptor in catalysis, enablingefficient screening for optimal catalysts. However, determining adsorptionenergy typically requires evaluating numerous adsorbate-catalystconfigurations. Current algorithmic approaches rely on exhaustive enumerationof adsorption sites and configurations, which makes the process computationallyintensive and does not inherently guarantee the identification of the globalminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model(LLM) agent designed to efficiently identify system-specific stable adsorptionconfigurations corresponding to the global minimum adsorption energy.Adsorb-Agent leverages its built-in knowledge and emergent reasoningcapabilities to strategically explore adsorption configurations likely to holdadsorption energy. By reducing the reliance on exhaustive sampling, itsignificantly decreases the number of initial configurations required whileimproving the accuracy of adsorption energy predictions. We evaluateAdsorb-Agent's performance across twenty representative systems encompassing arange of complexities. The Adsorb-Agent successfully identifies comparableadsorption energies for 83.7% of the systems and achieves lower energies,closer to the actual global minimum, for 35% of the systems, while requiringsignificantly fewer initial configurations than conventional methods. Itscapability is particularly evident in complex systems, where it identifieslower adsorption energies for 46.7% of systems involving intermetallic surfacesand 66.7% of systems with large adsorbate molecules. These results demonstratethe potential of Adsorb-Agent to accelerate catalyst discovery by reducingcomputational costs and improving the reliability of adsorption energypredictions.</description><author>Janghoon Ock, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani</author><pubDate>Mon, 16 Dec 2024 16:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16658v2</guid></item><item><title>Stepwise Reasoning Error Disruption Attack of LLMs</title><link>http://arxiv.org/abs/2412.11934v1</link><description>Large language models (LLMs) have made remarkable strides in complexreasoning tasks, but their safety and robustness in reasoning processes remainunderexplored. Existing attacks on LLM reasoning are constrained by specificsettings or lack of imperceptibility, limiting their feasibility andgeneralizability. To address these challenges, we propose the StepwiserEasoning Error Disruption (SEED) attack, which subtly injects errors intoprior reasoning steps to mislead the model into producing incorrect subsequentreasoning and final answers. Unlike previous methods, SEED is compatible withzero-shot and few-shot settings, maintains the natural reasoning flow, andensures covert execution without modifying the instruction. Extensiveexperiments on four datasets across four different models demonstrate SEED'seffectiveness, revealing the vulnerabilities of LLMs to disruptions inreasoning processes. These findings underscore the need for greater attentionto the robustness of LLM reasoning to ensure safety in practical applications.</description><author>Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu</author><pubDate>Mon, 16 Dec 2024 16:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11934v1</guid></item><item><title>Two-Timescale Critic-Actor for Average Reward MDPs with Function Approximation</title><link>http://arxiv.org/abs/2402.01371v3</link><description>Several recent works have focused on carrying out non-asymptotic convergenceanalyses for AC algorithms. Recently, a two-timescale critic-actor algorithmhas been presented for the discounted cost setting in the look-up table casewhere the timescales of the actor and the critic are reversed and onlyasymptotic convergence shown. In our work, we present the first two-timescalecritic-actor algorithm with function approximation in the long-run averagereward setting and present the first finite-time non-asymptotic as well asasymptotic convergence analysis for such a scheme. We obtain optimal learningrates and prove that our algorithm achieves a sample complexity of{$\mathcal{\tilde{O}}(\epsilon^{-(2+\delta)})$ with $\delta &gt;0$ arbitrarilyclose to zero,} for the mean squared error of the critic to be upper bounded by$\epsilon$ which is better than the one obtained for two-timescale AC in asimilar setting. A notable feature of our analysis is that we present theasymptotic convergence analysis of our scheme in addition to the finite-timebounds that we obtain and show the almost sure asymptotic convergence of the(slower) critic recursion to the attractor of an associated differentialinclusion with actor parameters corresponding to local maxima of a perturbedaverage reward objective. We also show the results of numerical experiments onthree benchmark settings and observe that our critic-actor algorithm performsthe best amongst all algorithms.</description><author>Prashansa Panda, Shalabh Bhatnagar</author><pubDate>Mon, 16 Dec 2024 16:17:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01371v3</guid></item><item><title>Speeding Up the NSGA-II With a Simple Tie-Breaking Rule</title><link>http://arxiv.org/abs/2412.11931v1</link><description>The non-dominated sorting genetic algorithm~II (NSGA-II) is the most popularmulti-objective optimization heuristic. Recent mathematical runtime analyseshave detected two shortcomings in discrete search spaces, namely, that theNSGA-II has difficulties with more than two objectives and that it is verysensitive to the choice of the population size. To overcome these difficulties,we analyze a simple tie-breaking rule in the selection of the next population.Similar rules have been proposed before, but have found only little acceptance.We prove the effectiveness of our tie-breaking rule via mathematical runtimeanalyses on the classic OneMinMax, LeadingOnesTrailingZeros, andOneJumpZeroJump benchmarks. We prove that this modified NSGA-II can optimizethe three benchmarks efficiently also for many objectives, in contrast to theexponential lower runtime bound previously shown for OneMinMax with three ormore objectives. For the bi-objective problems, we show runtime guarantees thatdo not increase when moderately increasing the population size over the minimumadmissible size. For example, for the OneJumpZeroJump problem withrepresentation length $n$ and gap parameter $k$, we show a runtime guarantee of$O(\max\{n^{k+1},Nn\})$ function evaluations when the population size is atleast four times the size of the Pareto front. For population sizes larger thanthe minimal choice $N = \Theta(n)$, this result improves considerably over the$\Theta(Nn^k)$ runtime of the classic NSGA-II.</description><author>Benjamin Doerr, Tudor Ivan, Martin S. Krejca</author><pubDate>Mon, 16 Dec 2024 16:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11931v1</guid></item></channel></rss>