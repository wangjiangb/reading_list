<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 09 Apr 2024 06:01:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Finding Visual Task Vectors</title><link>http://arxiv.org/abs/2404.05729v1</link><description>Visual Prompting is a technique for teaching models to perform a visual taskvia in-context examples, without any additional training. In this work, weanalyze the activations of MAE-VQGAN, a recent Visual Prompting model, and findtask vectors, activations that encode task-specific information. Equipped withthis insight, we demonstrate that it is possible to identify the task vectorsand use them to guide the network towards performing different tasks withoutproviding any input-output examples. To find task vectors, we compute theaverage intermediate activations per task and use the REINFORCE algorithm tosearch for the subset of task vectors. The resulting task vectors guide themodel towards performing a task better than the original model without the needfor input-output examples.</description><author>Alberto Hojel, Yutong Bai, Trevor Darrell, Amir Globerson, Amir Bar</author><pubDate>Mon, 08 Apr 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05729v1</guid></item><item><title>A Large-Scale Exploration of $Î¼$-Transfer</title><link>http://arxiv.org/abs/2404.05728v1</link><description>Large neural network models have become a mainstay of natural languageprocessing and computer vision, yet their initialization and learning rates areset in a largely heuristic fashion, potentially varying from paper to paper andone model size to the next. The $\mu$-Parameterization ($\mu$P) offers apotential solution to these challenges, yielding scaling rules for modelinitialization and learning rates, and reportedly enabling zero-shothyperparameter transfer from small to large models in a variety of cases. Despite the evident promise, the $\mu$P scaling rules are not yet widelyadopted, perhaps due to higher implementation complexity, many variations, orcomplex theoretical background. This work investigates $\mu$P empirically,focusing on the ubiquitous transformer architecture, and aims to answer asimple question: does $\mu$-Transfer yield optimal learning rates in practice?From models with 2M to 10B parameters, we show that $\mu$-Transfer works asintended for the majority of important cases, but also identify some surprisingcases where it may not.</description><author>Lucas Lingle</author><pubDate>Mon, 08 Apr 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05728v1</guid></item><item><title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</title><link>http://arxiv.org/abs/2404.05726v1</link><description>With the success of large language models (LLMs), integrating the visionmodel into LLMs to build vision-language foundation models has gained much moreinterest recently. However, existing LLM-based large multimodal models (e.g.,Video-LLaMA, VideoChat) can only take in a limited number of frames for shortvideo understanding. In this study, we mainly focus on designing an efficientand effective model for long-term video understanding. Instead of trying toprocess more frames simultaneously like most existing work, we propose toprocess videos in an online manner and store past video information in a memorybank. This allows our model to reference historical video content for long-termanalysis without exceeding LLMs' context length constraints or GPU memorylimits. Our memory bank can be seamlessly integrated into current multimodalLLMs in an off-the-shelf manner. We conduct extensive experiments on variousvideo understanding tasks, such as long-video understanding, video questionanswering, and video captioning, and our model can achieve state-of-the-artperformances across multiple datasets. Code available athttps://boheumd.github.io/MA-LMM/.</description><author>Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim</author><pubDate>Mon, 08 Apr 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05726v1</guid></item><item><title>Predicting Overtakes in Trucks Using CAN Data</title><link>http://arxiv.org/abs/2404.05723v1</link><description>Safe overtakes in trucks are crucial to prevent accidents, reduce congestion,and ensure efficient traffic flow, making early prediction essential for timelyand informed driving decisions. Accordingly, we investigate the detection oftruck overtakes from CAN data. Three classifiers, Artificial Neural Networks(ANN), Random Forest, and Support Vector Machines (SVM), are employed for thetask. Our analysis covers up to 10 seconds before the overtaking event, usingan overlapping sliding window of 1 second to extract CAN features. We observethat the prediction scores of the overtake class tend to increase as weapproach the overtake trigger, while the no-overtake class remain stable oroscillates depending on the classifier. Thus, the best accuracy is achievedwhen approaching the trigger, making early overtaking prediction challenging.The classifiers show good accuracy in classifying overtakes (Recall/TPR &gt; 93%),but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90%and below 60% for one SVM variant). We further combine two classifiers (RandomForest and linear SVM) by averaging their output scores. The fusion is observedto improve no-overtake classification (TNR &gt; 92%) at the expense of reducingovertake accuracy (TPR). However, the latter is kept above 91% near theovertake trigger. Therefore, the fusion balances TPR and TNR, providing moreconsistent performance than individual classifiers.</description><author>Talha Hanif Butt, Prayag Tiwari, Fernando Alonso-Fernandez</author><pubDate>Mon, 08 Apr 2024 18:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05723v1</guid></item><item><title>Language-Independent Representations Improve Zero-Shot Summarization</title><link>http://arxiv.org/abs/2404.05720v1</link><description>Finetuning pretrained models on downstream generation tasks often leads tocatastrophic forgetting in zero-shot conditions. In this work, we focus onsummarization and tackle the problem through the lens of language-independentrepresentations. After training on monolingual summarization, we performzero-shot transfer to new languages or language pairs. We first show naivelyfinetuned models are highly language-specific in both output behavior andinternal representations, resulting in poor zero-shot performance. Next, wepropose query-key (QK) finetuning to decouple task-specific knowledge from thepretrained language generation abilities. Then, after showing downsides of thestandard adversarial language classifier, we propose a balanced variant thatmore directly enforces language-agnostic representations. Moreover, ourqualitative analyses show removing source language identity correlates tozero-shot summarization performance. Our code is openly available.</description><author>Vladimir Solovyev, Danni Liu, Jan Niehues</author><pubDate>Mon, 08 Apr 2024 18:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05720v1</guid></item><item><title>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</title><link>http://arxiv.org/abs/2404.05719v1</link><description>Recent advancements in multimodal large language models (MLLMs) have beennoteworthy, yet, these general-domain MLLMs often fall short in their abilityto comprehend and interact effectively with user interface (UI) screens. Inthis paper, we present Ferret-UI, a new MLLM tailored for enhancedunderstanding of mobile UI screens, equipped with referring, grounding, andreasoning capabilities. Given that UI screens typically exhibit a moreelongated aspect ratio and contain smaller objects of interest (e.g., icons,texts) than natural images, we incorporate "any resolution" on top of Ferret tomagnify details and leverage enhanced visual features. Specifically, eachscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,horizontal division for portrait screens and vertical division for landscapescreens). Both sub-images are encoded separately before being sent to LLMs. Wemeticulously gather training samples from an extensive range of elementary UItasks, such as icon recognition, find text, and widget listing. These samplesare formatted for instruction-following with region annotations to facilitateprecise referring and grounding. To augment the model's reasoning ability, wefurther compile a dataset for advanced tasks, including detailed description,perception/interaction conversations, and function inference. After training onthe curated datasets, Ferret-UI exhibits outstanding comprehension of UIscreens and the capability to execute open-ended instructions. For modelevaluation, we establish a comprehensive benchmark encompassing all theaforementioned tasks. Ferret-UI excels not only beyond most open-source UIMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.</description><author>Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, Zhe Gan</author><pubDate>Mon, 08 Apr 2024 18:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05719v1</guid></item><item><title>SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing</title><link>http://arxiv.org/abs/2404.05717v1</link><description>Effective editing of personal content holds a pivotal role in enablingindividuals to express their creativity, weaving captivating narratives withintheir visual stories, and elevate the overall quality and impact of theirvisual content. Therefore, in this work, we introduce SwapAnything, a novelframework that can swap any objects in an image with personalized conceptsgiven by the reference, while keeping the context unchanged. Compared withexisting methods for personalized subject swapping, SwapAnything has threeunique advantages: (1) precise control of arbitrary objects and parts ratherthan the main subject, (2) more faithful preservation of context pixels, (3)better adaptation of the personalized concept to the image. First, we proposetargeted variable swapping to apply region control over latent feature maps andswap masked variables for faithful context preservation and initial semanticconcept swapping. Then, we introduce appearance adaptation, to seamlessly adaptthe semantic concept into the original image in terms of target location,shape, style, and content during the image generation process. Extensiveresults on both human and automatic evaluation demonstrate significantimprovements of our approach over baseline methods on personalized swapping.Furthermore, SwapAnything shows its precise and faithful swapping abilitiesacross single object, multiple objects, partial object, and cross-domainswapping tasks. SwapAnything also achieves great performance on text-basedswapping and tasks beyond swapping such as object insertion.</description><author>Jing Gu, Yilin Wang, Nanxuan Zhao, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Xin Eric Wang</author><pubDate>Mon, 08 Apr 2024 18:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05717v1</guid></item><item><title>Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)</title><link>http://arxiv.org/abs/2403.09680v2</link><description>This paper proposes a machine learning pre-sort stage to traditionalsupervised learning using Tsetlin Machines. Initially, K data-points areidentified from the dataset using an expedited genetic algorithm to solve themaximum dispersion problem. These are then used as the initial placement to runthe K-Medoid clustering algorithm. Finally, an expedited genetic algorithm isused to align K independent Tsetlin Machines by maximising hamming distance.For MNIST level classification problems, results demonstrate up to 10%improvement in accuracy, approx. 383X reduction in training time and approx.86X reduction in inference time.</description><author>Jordan Morris</author><pubDate>Mon, 08 Apr 2024 18:51:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09680v2</guid></item><item><title>Energy-Calibrated VAE with Test Time Free Lunch</title><link>http://arxiv.org/abs/2311.04071v4</link><description>In this paper, we propose a novel generative model that utilizes aconditional Energy-Based Model (EBM) for enhancing Variational Autoencoder(VAE), termed Energy-Calibrated VAE (EC-VAE). Specifically, VAEs often sufferfrom blurry generated samples due to the lack of a tailored training on thesamples generated in the generative direction. On the other hand, EBMs cangenerate high-quality samples but require expensive Markov Chain Monte Carlo(MCMC) sampling. To address these issues, we introduce a conditional EBM forcalibrating the generative direction of VAE during training, without requiringit for the generation at test time. In particular, we train EC-VAE upon boththe input data and the calibrated samples with adaptive weight to enhanceefficacy while avoiding MCMC sampling at test time. Furthermore, we extend thecalibration idea of EC-VAE to variational learning and normalizing flows, andapply EC-VAE to an additional application of zero-shot image restoration vianeural transport prior and range-null theory. We evaluate the proposed methodwith two applications, including image generation and zero-shot imagerestoration, and the experimental results show that our method achievescompetitive performance over single-step non-adversarial generation. Our codeis available at https://github.com/DJ-LYH/EC-VAE.</description><author>Yihong Luo, Siya Qiu, Xingjian Tao, Yujun Cai, Jing Tang</author><pubDate>Mon, 08 Apr 2024 18:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04071v4</guid></item><item><title>Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation</title><link>http://arxiv.org/abs/2402.15781v2</link><description>This paper analyzes multi-step TD-learning algorithms within the `deadlytriad' scenario, characterized by linear function approximation, off-policylearning, and bootstrapping. In particular, we prove that n-step TD-learningalgorithms converge to a solution as the sampling horizon n increasessufficiently. The paper is divided into two parts. In the first part, wecomprehensively examine the fundamental properties of their model-baseddeterministic counterparts, including projected value iteration, gradientdescent algorithms, and the control theoretic approach, which can be viewed asprototype deterministic algorithms whose analysis plays a pivotal role inunderstanding and developing their model-free reinforcement learningcounterparts. In particular, we prove that these algorithms converge tomeaningful solutions when n is sufficiently large. Based on these findings, twon-step TD-learning algorithms are proposed and analyzed, which can be seen asthe model-free reinforcement learning counterparts of the gradient and controltheoretic algorithms.</description><author>Donghwan Lee</author><pubDate>Mon, 08 Apr 2024 18:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15781v2</guid></item><item><title>Learning 3D-Aware GANs from Unposed Images with Template Feature Field</title><link>http://arxiv.org/abs/2404.05705v1</link><description>Collecting accurate camera poses of training images has been shown to wellserve the learning of 3D-aware generative adversarial networks (GANs) yet canbe quite expensive in practice. This work targets learning 3D-aware GANs fromunposed images, for which we propose to perform on-the-fly pose estimation oftraining images with a learned template feature field (TeFF). Concretely, inaddition to a generative radiance field as in previous approaches, we ask thegenerator to also learn a field from 2D semantic features while sharing thedensity from the radiance field. Such a framework allows us to acquire acanonical 3D feature template leveraging the dataset mean discovered by thegenerative model, and further efficiently estimate the pose parameters on realdata. Experimental results on various challenging datasets demonstrate thesuperiority of our approach over state-of-the-art alternatives from both thequalitative and the quantitative perspectives.</description><author>Xinya Chen, Hanlei Guo, Yanrui Bin, Shangzhan Zhang, Yuanbo Yang, Yue Wang, Yujun Shen, Yiyi Liao</author><pubDate>Mon, 08 Apr 2024 18:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05705v1</guid></item><item><title>Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer</title><link>http://arxiv.org/abs/2404.05695v1</link><description>Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based onNvidia Isaac Gym, designed to train locomotion skills for humanoid robots,emphasizing zero-shot transfer from simulation to the real-world environment.Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujocothat allows users to verify the trained policies in different physicalsimulations to ensure the robustness and generalization of the policies. Thisframework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) andXBot-L (1.65-meter tall humanoid robot) in a real-world environment withzero-shot sim-to-real transfer. The project website and source code can befound at: https://sites.google.com/view/humanoid-gym/.</description><author>Xinyang Gu, Yen-Jen Wang, Jianyu Chen</author><pubDate>Mon, 08 Apr 2024 18:26:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05695v1</guid></item><item><title>Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding</title><link>http://arxiv.org/abs/2404.05694v1</link><description>Recent advances in natural language processing (NLP) can be largelyattributed to the advent of pre-trained language models such as BERT andRoBERTa. While these models demonstrate remarkable performance on generaldatasets, they can struggle in specialized domains such as medicine, whereunique domain-specific terminologies, domain-specific abbreviations, andvarying document structures are common. This paper explores strategies foradapting these models to domain-specific requirements, primarily throughcontinuous pre-training on domain-specific data. We pre-trained several Germanmedical language models on 2.4B tokens derived from translated public Englishmedical data and 3B tokens of German clinical data. The resulting models wereevaluated on various German downstream tasks, including named entityrecognition (NER), multi-label classification, and extractive questionanswering. Our results suggest that models augmented by clinical andtranslation-based pre-training typically outperform general domain models inmedical contexts. We conclude that continuous pre-training has demonstrated theability to match or even exceed the performance of clinical models trained fromscratch. Furthermore, pre-training on clinical data or leveraging translatedtexts have proven to be reliable methods for domain adaptation in medical NLPtasks.</description><author>Ahmad Idrissi-Yaghir, Amin Dada, Henning SchÃ¤fer, Kamyar Arzideh, Giulia Baldini, Jan Trienes, Max Hasin, Jeanette Bewersdorff, Cynthia S. Schmidt, Marie Bauer, Kaleb E. Smith, Jiang Bian, Yonghui Wu, JÃ¶rg SchlÃ¶tterer, Torsten Zesch, Peter A. Horn, Christin Seifert, Felix Nensa, Jens Kleesiek, Christoph M. Friedrich</author><pubDate>Mon, 08 Apr 2024 18:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05694v1</guid></item><item><title>Evaluating the Efficacy of Cut-and-Paste Data Augmentation in Semantic Segmentation for Satellite Imagery</title><link>http://arxiv.org/abs/2404.05693v1</link><description>Satellite imagery is crucial for tasks like environmental monitoring andurban planning. Typically, it relies on semantic segmentation or Land Use LandCover (LULC) classification to categorize each pixel. Despite the advancementsbrought about by Deep Neural Networks (DNNs), their performance in segmentationtasks is hindered by challenges such as limited availability of labeled data,class imbalance and the inherent variability and complexity of satelliteimages. In order to mitigate those issues, our study explores the effectivenessof a Cut-and-Paste augmentation technique for semantic segmentation insatellite images. We adapt this augmentation, which usually requires labeledinstances, to the case of semantic segmentation. By leveraging the connectedcomponents in the semantic segmentation labels, we extract instances that arethen randomly pasted during training. Using the DynamicEarthNet dataset and aU-Net model for evaluation, we found that this augmentation significantlyenhances the mIoU score on the test set from 37.9 to 44.1. This findinghighlights the potential of the Cut-and-Paste augmentation to improve thegeneralization capabilities of semantic segmentation models in satelliteimagery.</description><author>Ionut M. Motoi, Leonardo Saraceni, Daniele Nardi, Thomas A. Ciarfuglia</author><pubDate>Mon, 08 Apr 2024 18:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05693v1</guid></item><item><title>Evaluating Mathematical Reasoning Beyond Accuracy</title><link>http://arxiv.org/abs/2404.05692v1</link><description>The leaderboard of Large Language Models (LLMs) in mathematical tasks hasbeen continuously updated. However, the majority of evaluations focus solely onthe final results, neglecting the quality of the intermediate steps. Thisoversight can mask underlying problems, such as logical errors or unnecessarysteps in the reasoning process. To measure reasoning beyond final-answeraccuracy, we introduce ReasonEval, a new methodology for evaluating the qualityof reasoning steps. ReasonEval employs $\textit{validity}$ and$\textit{redundancy}$ to characterize the reasoning quality, as well asaccompanying LLMs to assess them automatically. Instantiated by base modelsthat possess strong mathematical knowledge and trained with high-qualitylabeled data, ReasonEval achieves state-of-the-art performance on human-labeleddatasets and can accurately detect different types of errors generated byperturbation. When applied to evaluate LLMs specialized in math, we find thatan increase in final-answer accuracy does not necessarily guarantee animprovement in the overall quality of the reasoning steps for challengingmathematical problems. Additionally, we observe that ReasonEval can play asignificant role in data selection. We release the best-performing model,meta-evaluation script, and all evaluation results athttps://github.com/GAIR-NLP/ReasonEval.</description><author>Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu</author><pubDate>Mon, 08 Apr 2024 18:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05692v1</guid></item><item><title>Automated discovery of symbolic laws governing skill acquisition from naturally occurring data</title><link>http://arxiv.org/abs/2404.05689v1</link><description>Skill acquisition is a key area of research in cognitive psychology as itencompasses multiple psychological processes. The laws discovered underexperimental paradigms are controversial and lack generalizability. This paperaims to unearth the laws of skill learning from large-scale training log data.A two-stage algorithm was developed to tackle the issues of unobservablecognitive states and algorithmic explosion in searching. Initially a deeplearning model is employed to determine the learner's cognitive state andassess the feature importance. Subsequently, symbolic regression algorithms areutilized to parse the neural network model into algebraic equations. Theexperimental results of simulated data demonstrate that the proposed algorithmcan accurately restore various preset laws within a certain range of noise, incontinues feedback setting. Application of proposed method to Lumosity trainingdata demonstrates superior performance compared to traditional and latestmodels in terms of fitness. The results indicate the discovery of two new formsof skill acquisition laws, while some previous findings have been reaffirmed.</description><author>Sannyuya Liu, Qing Li, Xiaoxuan Shen, Jianwen Sun, Zongkai Yang</author><pubDate>Mon, 08 Apr 2024 18:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05689v1</guid></item><item><title>David and Goliath: An Empirical Evaluation of Attacks and Defenses for QNNs at the Deep Edge</title><link>http://arxiv.org/abs/2404.05688v1</link><description>ML is shifting from the cloud to the edge. Edge computing reduces the surfaceexposing private data and enables reliable throughput guarantees in real-timeapplications. Of the panoply of devices deployed at the edge,resource-constrained MCUs, e.g., Arm Cortex-M, are more prevalent, orders ofmagnitude cheaper, and less power-hungry than application processors or GPUs.Thus, enabling intelligence at the deep edge is the zeitgeist, with researchersfocusing on unveiling novel approaches to deploy ANNs on these constraineddevices. Quantization is a well-established technique that has proved effectivein enabling the deployment of neural networks on MCUs; however, it is still anopen question to understand the robustness of QNNs in the face of adversarialexamples. To fill this gap, we empirically evaluate the effectiveness of attacks anddefenses from (full-precision) ANNs on (constrained) QNNs. Our evaluationincludes three QNNs targeting TinyML applications, ten attacks, and sixdefenses. With this study, we draw a set of interesting findings. First,quantization increases the point distance to the decision boundary and leadsthe gradient estimated by some attacks to explode or vanish. Second,quantization can act as a noise attenuator or amplifier, depending on the noisemagnitude, and causes gradient misalignment. Regarding adversarial defenses, weconclude that input pre-processing defenses show impressive results on smallperturbations; however, they fall short as the perturbation increases. At thesame time, train-based defenses increase the average point distance to thedecision boundary, which holds after quantization. However, we argue thattrain-based defenses still need to smooth the quantization-shift and gradientmisalignment phenomenons to counteract adversarial example transferability toQNNs. All artifacts are open-sourced to enable independent validation ofresults.</description><author>Miguel Costa, Sandro Pinto</author><pubDate>Mon, 08 Apr 2024 18:14:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05688v1</guid></item><item><title>Retrieval-Augmented Open-Vocabulary Object Detection</title><link>http://arxiv.org/abs/2404.05687v1</link><description>Open-vocabulary object detection (OVD) has been studied with Vision-LanguageModels (VLMs) to detect novel objects beyond the pre-trained categories.Previous approaches improve the generalization ability to expand the knowledgeof the detector, using 'positive' pseudo-labels with additional 'class' names,e.g., sock, iPod, and alligator. To extend the previous methods in two aspects,we propose Retrieval-Augmented Losses and visual Features (RALF). Our methodretrieves related 'negative' classes and augments loss functions. Also, visualfeatures are augmented with 'verbalized concepts' of classes, e.g., worn on thefeet, handheld music player, and sharp teeth. Specifically, RALF consists oftwo modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visualFeatures (RAF). RAL constitutes two losses reflecting the semantic similaritywith negative vocabularies. In addition, RAF augments visual features with theverbalized concepts from a large language model (LLM). Our experimentsdemonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. Weachieve improvement up to 3.4 box AP$_{50}^{\text{N}}$ on novel categories ofthe COCO dataset and 3.6 mask AP$_{\text{r}}$ gains on the LVIS dataset. Codeis available at https://github.com/mlvlab/RALF .</description><author>Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim</author><pubDate>Mon, 08 Apr 2024 18:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05687v1</guid></item><item><title>SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation</title><link>http://arxiv.org/abs/2404.05680v1</link><description>While recent advances in 3D-aware Generative Adversarial Networks (GANs) haveaided the development of near-frontal view human face synthesis, the challengeof comprehensively synthesizing a full 3D head viewable from all angles stillpersists. Although PanoHead proves the possibilities of using a large-scaledataset with images of both frontal and back views for full-head synthesis, itoften causes artifacts for back views. Based on our in-depth analysis, we foundthe reasons are mainly twofold. First, from network architecture perspective,we found each plane in the utilized tri-plane/tri-grid representation spacetends to confuse the features from both sides, causing "mirroring" artifacts(e.g., the glasses appear in the back). Second, from data supervision aspect,we found that existing discriminator training in 3D GANs mainly focuses on thequality of the rendered image itself, and does not care much about itsplausibility with the perspective from which it was rendered. This makes itpossible to generate "face" in non-frontal views, due to its easiness to foolthe discriminator. In response, we propose SphereHead, a novel tri-planerepresentation in the spherical coordinate system that fits the human head'sgeometric characteristics and efficiently mitigates many of the generatedartifacts. We further introduce a view-image consistency loss for thediscriminator to emphasize the correspondence of the camera parameters and theimages. The combination of these efforts results in visually superior outcomeswith significantly fewer artifacts. Our code and dataset are publicly availableat https://lhyfst.github.io/spherehead.</description><author>Heyuan Li, Ce Chen, Tianhao Shi, Yuda Qiu, Sizhe An, Guanying Chen, Xiaoguang Han</author><pubDate>Mon, 08 Apr 2024 17:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05680v1</guid></item><item><title>Flexible Fairness Learning via Inverse Conditional Permutation</title><link>http://arxiv.org/abs/2404.05678v1</link><description>Equalized odds, as a popular notion of algorithmic fairness, aims to ensurethat sensitive variables, such as race and gender, do not unfairly influencethe algorithm prediction when conditioning on the true outcome. Despite rapidadvancements, most of the current research focuses on the violation ofequalized odds caused by one sensitive attribute, leaving the challenge ofsimultaneously accounting for multiple attributes under-addressed. We addressthis gap by introducing a fairness learning approach that integratesadversarial learning with a novel inverse conditional permutation. Thisapproach effectively and flexibly handles multiple sensitive attributes,potentially of mixed data types. The efficacy and flexibility of our method aredemonstrated through both simulation studies and empirical analysis ofreal-world datasets.</description><author>Yuheng Lai, Leying Guan</author><pubDate>Mon, 08 Apr 2024 17:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05678v1</guid></item><item><title>Deep Internal Learning: Deep Learning from a Single Input</title><link>http://arxiv.org/abs/2312.07425v2</link><description>Deep learning, in general, focuses on training a neural network from largelabeled datasets. Yet, in many cases there is value in training a network justfrom the input at hand. This is particularly relevant in many signal and imageprocessing problems where training data is scarce and diversity is large on theone hand, and on the other, there is a lot of structure in the data that can beexploited. Using this information is the key to deep internal-learningstrategies, which may involve training a network from scratch using a singleinput or adapting an already trained network to a provided input example atinference time. This survey paper aims at covering deep internal-learningtechniques that have been proposed in the past few years for these twoimportant directions. While our main focus will be on image processingproblems, most of the approaches that we survey are derived for general signals(vectors with recurring patterns that can be distinguished from noise) and aretherefore applicable to other modalities.</description><author>Tom Tirer, Raja Giryes, Se Young Chun, Yonina C. Eldar</author><pubDate>Mon, 08 Apr 2024 17:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07425v2</guid></item><item><title>Normalizing Flows on the Product Space of SO(3) Manifolds for Probabilistic Human Pose Modeling</title><link>http://arxiv.org/abs/2404.05675v1</link><description>Normalizing flows have proven their efficacy for density estimation inEuclidean space, but their application to rotational representations, crucialin various domains such as robotics or human pose modeling, remainsunderexplored. Probabilistic models of the human pose can benefit fromapproaches that rigorously consider the rotational nature of human joints. Forthis purpose, we introduce HuProSO3, a normalizing flow model that operates ona high-dimensional product space of SO(3) manifolds, modeling the jointdistribution for human joints with three degrees of freedom. HuProSO3'sadvantage over state-of-the-art approaches is demonstrated through its superiormodeling accuracy in three different applications and its capability toevaluate the exact likelihood. This work not only addresses the technicalchallenge of learning densities on SO(3) manifolds, but it also has broaderimplications for domains where the probabilistic regression of correlated 3Drotations is of importance.</description><author>Olaf DÃ¼nkel, Tim Salzmann, Florian Pfaff</author><pubDate>Mon, 08 Apr 2024 17:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05675v1</guid></item><item><title>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</title><link>http://arxiv.org/abs/2404.05674v1</link><description>In this paper, we present MoMA: an open-vocabulary, training-freepersonalized image model that boasts flexible zero-shot capabilities. Asfoundational text-to-image models rapidly evolve, the demand for robustimage-to-image translation grows. Addressing this need, MoMA specializes insubject-driven personalized image generation. Utilizing an open-source,Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role asboth a feature extractor and a generator. This approach effectively synergizesreference image and text prompt information to produce valuable image features,facilitating an image diffusion model. To better leverage the generatedfeatures, we further introduce a novel self-attention shortcut method thatefficiently transfers image features to an image diffusion model, improving theresemblance of the target object in generated images. Remarkably, as atuning-free plug-and-play module, our model requires only a single referenceimage and outperforms existing methods in generating images with high detailfidelity, enhanced identity-preservation and prompt faithfulness. Our work isopen-source, thereby providing universal access to these advancements.</description><author>Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang</author><pubDate>Mon, 08 Apr 2024 17:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05674v1</guid></item><item><title>CoReS: Orchestrating the Dance of Reasoning and Segmentation</title><link>http://arxiv.org/abs/2404.05673v1</link><description>The reasoning segmentation task, which demands a nuanced comprehension ofintricate queries to accurately pinpoint object regions, is attractingincreasing attention. However, Multi-modal Large Language Models (MLLM) oftenfind it difficult to accurately localize the objects described in complexreasoning contexts. We believe that the act of reasoning segmentation shouldmirror the cognitive stages of human visual search, where each step is aprogressive refinement of thought toward the final object. Thus we introducethe Chains of Reasoning and Segmenting (CoReS) and find this top-down visualhierarchy indeed enhances the visual search process. Specifically, we propose adual-chain structure that generates multi-modal, chain-like outputs to aid thesegmentation process. Furthermore, to steer the MLLM's outputs into thisintended hierarchy, we incorporate in-context inputs as guidance. Extensiveexperiments demonstrate the superior performance of our CoReS, which surpassesthe state-of-the-art method by 7.1\% on the ReasonSeg dataset. The code will bereleased at https://github.com/baoxiaoyi/CoReS.</description><author>Xiaoyi Bao, Siyang Sun, Shuailei Ma, Kecheng Zheng, Yuxin Guo, Guosheng Zhao, Yun Zheng, Xingang Wang</author><pubDate>Mon, 08 Apr 2024 17:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05673v1</guid></item><item><title>New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance</title><link>http://arxiv.org/abs/2311.17929v7</link><description>This research examines the polycentric governance of digital assets inblockchain-based Decentralized Autonomous Organizations (DAOs). It offers atheoretical framework and addresses a critical challenge facing decentralizedgovernance by developing a method to identify Sybils, or spurious identities.Sybils pose significant organizational sustainability threats to DAOs andother, commons-based online communities, and threat models are identified. Theexperimental method uses an autoencoder architecture and graph deep learningtechniques to identify Sybil activity in a DAO governance dataset(snapshot.org). Specifically, a Graph Convolutional Neural Network (GCNN)learned voting behaviours and a fast vector clustering algorithm usedhigh-dimensional embeddings to identify similar nodes in a graph. The resultsreveal that deep learning can effectively identify Sybils, reducing the votinggraph by 2-5%. This research underscores the importance of Sybil resistance inDAOs, identifies challenges and opportunities for forensics and analysis ofanonymous networks, and offers a novel perspective on decentralized governance,informing future policy, regulation, and governance practices.</description><author>Quinn DuPont</author><pubDate>Mon, 08 Apr 2024 17:53:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17929v7</guid></item><item><title>NAF-DPM: A Nonlinear Activation-Free Diffusion Probabilistic Model for Document Enhancement</title><link>http://arxiv.org/abs/2404.05669v1</link><description>Real-world documents may suffer various forms of degradation, often resultingin lower accuracy in optical character recognition (OCR) systems. Therefore, acrucial preprocessing step is essential to eliminate noise while preservingtext and key features of documents. In this paper, we propose NAF-DPM, a novelgenerative framework based on a diffusion probabilistic model (DPM) designed torestore the original quality of degraded documents. While DPMs are recognizedfor their high-quality generated images, they are also known for their largeinference time. To mitigate this problem we provide the DPM with an efficientnonlinear activation-free (NAF) network and we employ as a sampler a fastsolver of ordinary differential equations, which can converge in a fewiterations. To better preserve text characters, we introduce an additionaldifferentiable module based on convolutional recurrent neural networks,simulating the behavior of an OCR system during training. Experiments conductedon various datasets showcase the superiority of our approach, achievingstate-of-the-art performance in terms of pixel-level and perceptual similaritymetrics. Furthermore, the results demonstrate a notable character errorreduction made by OCR systems when transcribing real-world document imagesenhanced by our framework. Code and pre-trained models are available athttps://github.com/ispamm/NAF-DPM.</description><author>Giordano Cicchetti, Danilo Comminiello</author><pubDate>Mon, 08 Apr 2024 17:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05669v1</guid></item><item><title>AlignZeg: Mitigating Objective Misalignment for Zero-shot Semantic Segmentation</title><link>http://arxiv.org/abs/2404.05667v1</link><description>A serious issue that harms the performance of zero-shot visual recognition isnamed objective misalignment, i.e., the learning objective prioritizesimproving the recognition accuracy of seen classes rather than unseen classes,while the latter is the true target to pursue. This issue becomes moresignificant in zero-shot image segmentation because the stronger (i.e.,pixel-level) supervision brings a larger gap between seen and unseen classes.To mitigate it, we propose a novel architecture named AlignZeg, which embodiesa comprehensive improvement of the segmentation pipeline, including proposalextraction, classification, and correction, to better fit the goal of zero-shotsegmentation. (1) Mutually-Refined Proposal Extraction. AlignZeg harnesses amutual interaction between mask queries and visual features, facilitatingdetailed class-agnostic mask proposal extraction. (2) Generalization-EnhancedProposal Classification. AlignZeg introduces synthetic data and incorporatesmultiple background prototypes to allocate a more generalizable feature space.(3) Predictive Bias Correction. During the inference stage, AlignZeg uses aclass indicator to find potential unseen class proposals followed by aprediction postprocess to correct the prediction bias. Experiments demonstratethat AlignZeg markedly enhances zero-shot semantic segmentation, as shown by anaverage 3.8% increase in hIoU, primarily attributed to a 7.1% improvement inidentifying unseen classes, and we further validate that the improvement comesfrom alleviating the objective misalignment issue.</description><author>Jiannan Ge, Lingxi Xie, Hongtao Xie, Pandeng Li, Xiaopeng Zhang, Yongdong Zhang, Qi Tian</author><pubDate>Mon, 08 Apr 2024 17:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05667v1</guid></item><item><title>YaART: Yet Another ART Rendering Technology</title><link>http://arxiv.org/abs/2404.05666v1</link><description>In the rapidly progressing field of generative models, the development ofefficient and high-fidelity text-to-image diffusion systems represents asignificant frontier. This study introduces YaART, a novel production-gradetext-to-image cascaded diffusion model aligned to human preferences usingReinforcement Learning from Human Feedback (RLHF). During the development ofYaART, we especially focus on the choices of the model and training datasetsizes, the aspects that were not systematically investigated for text-to-imagecascaded diffusion models before. In particular, we comprehensively analyze howthese choices affect both the efficiency of the training process and thequality of the generated images, which are highly important in practice.Furthermore, we demonstrate that models trained on smaller datasets ofhigher-quality images can successfully compete with those trained on largerdatasets, establishing a more efficient scenario of diffusion models training.From the quality perspective, YaART is consistently preferred by users overmany existing state-of-the-art models.</description><author>Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</author><pubDate>Mon, 08 Apr 2024 17:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05666v1</guid></item><item><title>BinaryDM: Towards Accurate Binarization of Diffusion Model</title><link>http://arxiv.org/abs/2404.05662v1</link><description>With the advancement of diffusion models (DMs) and the substantiallyincreased computational requirements, quantization emerges as a practicalsolution to obtain compact and efficient low-bit DMs. However, the highlydiscrete representation leads to severe accuracy degradation, hindering thequantization of diffusion models to ultra-low bit-widths. In this paper, wepropose BinaryDM, a novel accurate quantization-aware training approach to pushthe weights of diffusion models towards the limit of 1-bit. Firstly, we presenta Learnable Multi-basis Binarizer (LMB) to recover the representationsgenerated by the binarized DM, which improves the information in details ofrepresentations crucial to the DM. Secondly, a Low-rank RepresentationMimicking (LRM) is applied to enhance the binarization-aware optimization ofthe DM, alleviating the optimization direction ambiguity caused by fine-grainedalignment. Moreover, a progressive initialization strategy is applied totraining DMs to avoid convergence difficulties. Comprehensive experimentsdemonstrate that BinaryDM achieves significant accuracy and efficiency gainscompared to SOTA quantization methods of DMs under ultra-low bit-widths. As thefirst binarization method for diffusion models, BinaryDM achieves impressive16.0 times FLOPs and 27.1 times storage savings with 1-bit weight and 4-bitactivation, showcasing its substantial advantages and potential for deployingDMs on resource-limited scenarios.</description><author>Xingyu Zheng, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, Xianglong Liu</author><pubDate>Mon, 08 Apr 2024 17:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05662v1</guid></item><item><title>Automatic Controllable Colorization via Imagination</title><link>http://arxiv.org/abs/2404.05661v1</link><description>We propose a framework for automatic colorization that allows for iterativeediting and modifications. The core of our framework lies in an imaginationmodule: by understanding the content within a grayscale image, we utilize apre-trained image generation model to generate multiple images that contain thesame content. These images serve as references for coloring, mimicking theprocess of human experts. As the synthesized images can be imperfect ordifferent from the original grayscale image, we propose a Reference RefinementModule to select the optimal reference composition. Unlike most previousend-to-end automatic colorization algorithms, our framework allows foriterative and localized modifications of the colorization results because weexplicitly model the coloring samples. Extensive experiments demonstrate thesuperiority of our framework over existing automatic colorization algorithms ineditability and flexibility. Project page:https://xy-cong.github.io/imagine-colorization.</description><author>Xiaoyan Cong, Yue Wu, Qifeng Chen, Chenyang Lei</author><pubDate>Mon, 08 Apr 2024 17:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05661v1</guid></item><item><title>VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain</title><link>http://arxiv.org/abs/2404.05659v1</link><description>Due to privacy restrictions, there's a shortage of publicly available speechrecognition datasets in the medical domain. In this work, we present VietMed -a Vietnamese speech recognition dataset in the medical domain comprising 16h oflabeled medical speech, 1000h of unlabeled medical speech and 1200h ofunlabeled general-domain speech. To our best knowledge, VietMed is by far theworld's largest public medical speech recognition dataset in 7 aspects: totalduration, number of speakers, diseases, recording conditions, speaker roles,unique medical terms and accents. VietMed is also by far the largest publicVietnamese speech dataset in terms of total duration. Additionally, we are thefirst to present a medical ASR dataset covering all ICD-10 disease groups andall accents within a country. Moreover, we release the first public large-scalepre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along withthe first public large-scale fine-tuned models for medical ASR. Even withoutany medical data in unsupervised pre-training, our best pre-trained modelXLSR-53-Viet generalizes very well to the medical domain by outperformingstate-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relativereduction of more than 40%). All code, data and models are made publiclyavailable here: https://github.com/leduckhai/MultiMed.</description><author>Khai Le-Duc</author><pubDate>Mon, 08 Apr 2024 17:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05659v1</guid></item><item><title>MLP Can Be A Good Transformer Learner</title><link>http://arxiv.org/abs/2404.05657v1</link><description>Self-attention mechanism is the key of the Transformer but often criticizedfor its computation demands. Previous token pruning works motivate theirmethods from the view of computation redundancy but still need to load the fullnetwork and require same memory costs. This paper introduces a novel strategythat simplifies vision transformers and reduces computational load through theselective removal of non-essential attention layers, guided by entropyconsiderations. We identify that regarding the attention layer in bottomblocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicitthe same entropy quantity. Meanwhile, the accompanied MLPs are under-exploitedsince they exhibit smaller feature entropy compared to those MLPs in the topblocks. Therefore, we propose to integrate the uninformative attention layersinto their subsequent counterparts by degenerating them into identical mapping,yielding only MLP in certain transformer blocks. Experimental results onImageNet-1k show that the proposed method can remove 40% attention layer ofDeiT-B, improving throughput and memory bound without performance compromise.Code is available at https://github.com/sihaoevery/lambda_vit.</description><author>Sihao Lin, Pumeng Lyu, Dongrui Liu, Tao Tang, Xiaodan Liang, Andy Song, Xiaojun Chang</author><pubDate>Mon, 08 Apr 2024 17:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05657v1</guid></item><item><title>Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid Framework</title><link>http://arxiv.org/abs/2404.05656v1</link><description>Industry-wide nuclear power plant operating experience is a critical sourceof raw data for performing parameter estimations in reliability and riskmodels. Much operating experience information pertains to failure events and isstored as reports containing unstructured data, such as narratives. Eventreports are essential for understanding how failures are initiated andpropagated, including the numerous causal relations involved. Causal relationextraction using deep learning represents a significant frontier in the fieldof natural language processing (NLP), and is crucial since it enables theinterpretation of intricate narratives and connections contained within vastamounts of written information. This paper proposed a hybrid framework forcausality detection and extraction from nuclear licensee event reports. Themain contributions include: (1) we compiled an LER corpus with 20,129 textsamples for causality analysis, (2) developed an interactive tool for labelingcause effect pairs, (3) built a deep-learning-based approach for causalrelation detection, and (4) developed a knowledge based cause-effect extractionapproach.</description><author>Sohag Rahman, Sai Zhang, Min Xian, Shoukun Sun, Fei Xu, Zhegang Ma</author><pubDate>Mon, 08 Apr 2024 17:39:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05656v1</guid></item><item><title>Spectral Convergence of Simplicial Complex Signals</title><link>http://arxiv.org/abs/2309.07169v3</link><description>Topological signal processing (TSP) utilizes simplicial complexes to modelstructures with higher order than vertices and edges. In this paper, we studythe transferability of TSP via a generalized higher-order version of graphon,known as complexon. We recall the notion of a complexon as the limit of asimplicial complex sequence [1]. Inspired by the graphon shift operator andmessage-passing neural network, we construct a marginal complexon and complexonshift operator (CSO) according to components of all possible dimensions fromthe complexon. We investigate the CSO's eigenvalues and eigenvectors and relatethem to a new family of weighted adjacency matrices. We prove that when asimplicial complex signal sequence converges to a complexon signal, theeigenvalues, eigenspaces, and Fourier transform of the corresponding CSOsconverge to that of the limit complexon signal. This conclusion is furtherverified by two numerical experiments. These results hint at learningtransferability on large simplicial complexes or simplicial complex sequences,which generalize the graphon signal processing framework.</description><author>Purui Zhang, Xingchao Jian, Feng Ji, Wee Peng Tay, Bihan Wen</author><pubDate>Mon, 08 Apr 2024 17:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07169v3</guid></item><item><title>Resistive Memory-based Neural Differential Equation Solver for Score-based Diffusion Model</title><link>http://arxiv.org/abs/2404.05648v1</link><description>Human brains image complicated scenes when reading a novel. Replicating thisimagination is one of the ultimate goals of AI-Generated Content (AIGC).However, current AIGC methods, such as score-based diffusion, are stilldeficient in terms of rapidity and efficiency. This deficiency is rooted in thedifference between the brain and digital computers. Digital computers havephysically separated storage and processing units, resulting in frequent datatransfers during iterative calculations, incurring large time and energyoverheads. This issue is further intensified by the conversion of inherentlycontinuous and analog generation dynamics, which can be formulated by neuraldifferential equations, into discrete and digital operations. Inspired by thebrain, we propose a time-continuous and analog in-memory neural differentialequation solver for score-based diffusion, employing emerging resistive memory.The integration of storage and computation within resistive memory synapsessurmount the von Neumann bottleneck, benefiting the generative speed and energyefficiency. The closed-loop feedback integrator is time-continuous, analog, andcompact, physically implementing an infinite-depth neural network. Moreover,the software-hardware co-design is intrinsically robust to analog noise. Weexperimentally validate our solution with 180 nm resistive memory in-memorycomputing macros. Demonstrating equivalent generative quality to the softwarebaseline, our system achieved remarkable enhancements in generative speed forboth unconditional and conditional generation tasks, by factors of 64.8 and156.5, respectively. Moreover, it accomplished reductions in energy consumptionby factors of 5.2 and 4.1. Our approach heralds a new horizon for hardwaresolutions in edge computing for generative AI applications.</description><author>Jichang Yang, Hegan Chen, Jia Chen, Songqi Wang, Shaocong Wang, Yifei Yu, Xi Chen, Bo Wang, Xinyuan Zhang, Binbin Cui, Yi Li, Ning Lin, Meng Xu, Yi Li, Xiaoxin Xu, Xiaojuan Qi, Zhongrui Wang, Xumeng Zhang, Dashan Shang, Han Wang, Qi Liu, Kwang-Ting Cheng, Ming Liu</author><pubDate>Mon, 08 Apr 2024 17:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05648v1</guid></item><item><title>Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model</title><link>http://arxiv.org/abs/2404.04167v2</link><description>In this study, we introduce CT-LLM, a 2B large language model (LLM) thatillustrates a pivotal shift towards prioritizing the Chinese language indeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from theconventional methodology by primarily incorporating Chinese textual data,utilizing an extensive corpus of 1,200 billion tokens, including 800 billionChinese tokens, 300 billion English tokens, and 100 billion code tokens. Thisstrategic composition facilitates the model's exceptional proficiency inunderstanding and processing Chinese, a capability further enhanced throughalignment techniques. Demonstrating remarkable performance on the CHC-Bench,CT-LLM excels in Chinese language tasks, and showcases its adeptness in Englishthrough SFT. This research challenges the prevailing paradigm of training LLMspredominantly on English corpora and then adapting them to other languages,broadening the horizons for LLM training methodologies. By open-sourcing thefull process of training a Chinese LLM, including a detailed data processingprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to fosterfurther exploration and innovation in both academia and industry, paving theway for more inclusive and versatile language models.</description><author>Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang</author><pubDate>Mon, 08 Apr 2024 17:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04167v2</guid></item><item><title>3D-COCO: extension of MS-COCO dataset for image detection and 3D reconstruction modules</title><link>http://arxiv.org/abs/2404.05641v1</link><description>We introduce 3D-COCO, an extension of the original MS-COCO dataset providing3D models and 2D-3D alignment annotations. 3D-COCO was designed to achievecomputer vision tasks such as 3D reconstruction or image detection configurablewith textual, 2D image, and 3D CAD model queries. We complete the existingMS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse. Byusing an IoU-based method, we match each MS-COCO annotation with the best 3Dmodels to provide a 2D-3D alignment. The open-source nature of 3D-COCO is apremiere that should pave the way for new research on 3D-related topics. Thedataset and its source codes is available athttps://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/</description><author>Maxence Bideaux, Alice Phe, Mohamed Chaouch, Bertrand Luvison, Quoc-Cuong Pham</author><pubDate>Mon, 08 Apr 2024 17:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05641v1</guid></item><item><title>Investigating the Impact of Quantization on Adversarial Robustness</title><link>http://arxiv.org/abs/2404.05639v1</link><description>Quantization is a promising technique for reducing the bit-width of deepmodels to improve their runtime performance and storage efficiency, and thusbecomes a fundamental step for deployment. In real-world scenarios, quantizedmodels are often faced with adversarial attacks which cause the model to makeincorrect inferences by introducing slight perturbations. However, recentstudies have paid less attention to the impact of quantization on the modelrobustness. More surprisingly, existing studies on this topic even presentinconsistent conclusions, which prompted our in-depth investigation. In thispaper, we conduct a first-time analysis of the impact of the quantizationpipeline components that can incorporate robust optimization under the settingsof Post-Training Quantization and Quantization-Aware Training. Through ourdetailed analysis, we discovered that this inconsistency arises from the use ofdifferent pipelines in different studies, specifically regarding whether robustoptimization is performed and at which quantization stage it occurs. Ourresearch findings contribute insights into deploying more secure and robustquantized networks, assisting practitioners in reference for scenarios withhigh-security requirements and limited resources.</description><author>Qun Li, Yuan Meng, Chen Tang, Jiacheng Jiang, Zhi Wang</author><pubDate>Mon, 08 Apr 2024 17:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05639v1</guid></item><item><title>FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization</title><link>http://arxiv.org/abs/2403.06908v2</link><description>3D Gaussian splatting has achieved very impressive performance in real-timenovel view synthesis. However, it often suffers from over-reconstruction duringGaussian densification where high-variance image regions are covered by a fewlarge Gaussians only, leading to blur and artifacts in the rendered images. Wedesign a progressive frequency regularization (FreGS) technique to tackle theover-reconstruction issue within the frequency space. Specifically, FreGSperforms coarse-to-fine Gaussian densification by exploiting low-to-highfrequency components that can be easily extracted with low-pass and high-passfilters in the Fourier space. By minimizing the discrepancy between thefrequency spectrum of the rendered image and the corresponding ground truth, itachieves high-quality Gaussian densification and alleviates theover-reconstruction of Gaussian splatting effectively. Experiments overmultiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples andDeep Blending) show that FreGS achieves superior novel view synthesis andoutperforms the state-of-the-art consistently.</description><author>Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing</author><pubDate>Mon, 08 Apr 2024 17:16:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06908v2</guid></item><item><title>WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology</title><link>http://arxiv.org/abs/2403.15238v2</link><description>Deep learning enables the modelling of high-resolution histopathologywhole-slide images (WSI). Weakly supervised learning of tile-level data istypically applied for tasks where labels only exist on the patient or WSI level(e.g. patient outcomes or histological grading). In this context, there is aneed for improved spatial interpretability of predictions from such models. Wepropose a novel method, Wsi rEgion sElection aPproach (WEEP), for modelinterpretation. It provides a principled yet straightforward way to establishthe spatial area of WSI required for assigning a particular prediction label.We demonstrate WEEP on a binary classification task in the area of breastcancer computational pathology. WEEP is easy to implement, is directlyconnected to the model-based decision process, and offers information relevantto both research and diagnostic applications.</description><author>Abhinav Sharma, Bojing Liu, Mattias Rantalainen</author><pubDate>Mon, 08 Apr 2024 17:14:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15238v2</guid></item><item><title>Fighting crime with Transformers: Empirical analysis of address parsing methods in payment data</title><link>http://arxiv.org/abs/2404.05632v1</link><description>In the financial industry, identifying the location of parties involved inpayments is a major challenge in the context of various regulatoryrequirements. For this purpose address parsing entails extracting fields suchas street, postal code, or country from free text message attributes. Whilepayment processing platforms are updating their standards with more structuredformats such as SWIFT with ISO 20022, address parsing remains essential for aconsiderable volume of messages. With the emergence of Transformers andGenerative Large Language Models (LLM), we explore the performance ofstate-of-the-art solutions given the constraint of processing a vast amount ofdaily data. This paper also aims to show the need for training robust modelscapable of dealing with real-world noisy transactional data. Our resultssuggest that a well fine-tuned Transformer model using early-stoppingsignificantly outperforms other approaches. Nevertheless, generative LLMsdemonstrate strong zero-shot performance and warrant further investigations.</description><author>Haitham Hammami, Louis Baligand, Bojan Petrovski</author><pubDate>Mon, 08 Apr 2024 17:04:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05632v1</guid></item><item><title>Learning a Category-level Object Pose Estimator without Pose Annotations</title><link>http://arxiv.org/abs/2404.05626v1</link><description>3D object pose estimation is a challenging task. Previous works alwaysrequire thousands of object images with annotated poses for learning the 3Dpose correspondence, which is laborious and time-consuming for labeling. Inthis paper, we propose to learn a category-level 3D object pose estimatorwithout pose annotations. Instead of using manually annotated images, weleverage diffusion models (e.g., Zero-1-to-3) to generate a set of images undercontrolled pose differences and propose to learn our object pose estimator withthose images. Directly using the original diffusion model leads to images withnoisy poses and artifacts. To tackle this issue, firstly, we exploit an imageencoder, which is learned from a specially designed contrastive pose learning,to filter the unreasonable details and extract image feature maps.Additionally, we propose a novel learning strategy that allows the model tolearn object poses from those generated image sets without knowing thealignment of their canonical poses. Experimental results show that our methodhas the capability of category-level object pose estimation from a single shotsetting (as pose definition), while significantly outperforming otherstate-of-the-art methods on the few-shot category-level object pose estimationbenchmarks.</description><author>Fengrui Tian, Yaoyao Liu, Adam Kortylewski, Yueqi Duan, Shaoyi Du, Alan Yuille, Angtian Wang</author><pubDate>Mon, 08 Apr 2024 16:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05626v1</guid></item><item><title>Deep Feature Statistics Mapping for Generalized Screen Content Image Quality Assessment</title><link>http://arxiv.org/abs/2209.05321v3</link><description>The statistical regularities of natural images, referred to as natural scenestatistics, play an important role in no-reference image quality assessment.However, it has been widely acknowledged that screen content images (SCIs),which are typically computer generated, do not hold such statistics. Here wemake the first attempt to learn the statistics of SCIs, based upon which thequality of SCIs can be effectively determined. The underlying mechanism of theproposed approach is based upon the mild assumption that the SCIs, which arenot physically acquired, still obey certain statistics that could be understoodin a learning fashion. We empirically show that the statistics deviation couldbe effectively leveraged in quality assessment, and the proposed method issuperior when evaluated in different settings. Extensive experimental resultsdemonstrate the Deep Feature Statistics based SCI Quality Assessment (DFSS-IQA)model delivers promising performance compared with existing NR-IQA models andshows a high generalization capability in the cross-dataset settings. Theimplementation of our method is publicly available athttps://github.com/Baoliang93/DFSS-IQA.</description><author>Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang, Sam Kwong</author><pubDate>Mon, 08 Apr 2024 16:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05321v3</guid></item><item><title>LTNER: Large Language Model Tagging for Named Entity Recognition with Contextualized Entity Marking</title><link>http://arxiv.org/abs/2404.05624v1</link><description>The use of LLMs for natural language processing has become a popular trend inthe past two years, driven by their formidable capacity for contextcomprehension and learning, which has inspired a wave of research fromacademics and industry professionals. However, for certain NLP tasks, such asNER, the performance of LLMs still falls short when compared to supervisedlearning methods. In our research, we developed a NER processing frameworkcalled LTNER that incorporates a revolutionary Contextualized Entity MarkingGen Method. By leveraging the cost-effective GPT-3.5 coupled with contextlearning that does not require additional training, we significantly improvedthe accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 datasetincreased from the initial 85.9% to 91.9%, approaching the performance ofsupervised fine-tuning. This outcome has led to a deeper understanding of thepotential of LLMs.</description><author>Faren Yan, Peng Yu, Xin Chen</author><pubDate>Mon, 08 Apr 2024 16:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05624v1</guid></item><item><title>AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced Datasets</title><link>http://arxiv.org/abs/2404.05623v1</link><description>Active learning for imbalanced classification tasks is challenging as theminority classes naturally occur rarely. Gathering a large pool of unlabelleddata is thus essential to capture minority instances. Standard pool-basedactive learning is computationally expensive on large pools and often reacheslow accuracy by overfitting the initial decision boundary, thus failing toexplore the input space and find minority instances. To address these issues wepropose AnchorAL. At each iteration, AnchorAL chooses class-specific instancesfrom the labelled set, or anchors, and retrieves the most similar unlabelledinstances from the pool. This resulting subpool is then used for activelearning. Using a small, fixed-sized subpool AnchorAL allows scaling any activelearning strategy to large pools. By dynamically selecting different anchors ateach iteration it promotes class balance and prevents overfitting the initialdecision boundary, thus promoting the discovery of new clusters of minorityinstances. Experiments across different classification tasks, active learningstrategies, and model architectures AnchorAL is (i) faster, often reducingruntime from hours to minutes, (ii) trains more performant models, (iii) andreturns more balanced datasets than competing methods.</description><author>Pietro Lesci, Andreas Vlachos</author><pubDate>Mon, 08 Apr 2024 16:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05623v1</guid></item><item><title>How to Evaluate Entity Resolution Systems: An Entity-Centric Framework with Application to Inventor Name Disambiguation</title><link>http://arxiv.org/abs/2404.05622v1</link><description>Entity resolution (record linkage, microclustering) systems are notoriouslydifficult to evaluate. Looking for a needle in a haystack, traditionalevaluation methods use sophisticated, application-specific sampling schemes tofind matching pairs of records among an immense number of non-matches. Wepropose an alternative that facilitates the creation of representative,reusable benchmark data sets without necessitating complex sampling schemes.These benchmark data sets can then be used for model training and a variety ofevaluation tasks. Specifically, we propose an entity-centric data labelingmethodology that integrates with a unified framework for monitoring summarystatistics, estimating key performance metrics such as cluster and pairwiseprecision and recall, and analyzing root causes for errors. We validate theframework in an application to inventor name disambiguation and throughsimulation studies. Software: https://github.com/OlivierBinette/er-evaluation/</description><author>Olivier Binette, Youngsoo Baek, Siddharth Engineer, Christina Jones, Abel Dasylva, Jerome P. Reiter</author><pubDate>Mon, 08 Apr 2024 16:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05622v1</guid></item><item><title>Explainable Traffic Flow Prediction with Large Language Models</title><link>http://arxiv.org/abs/2404.02937v2</link><description>Traffic flow prediction is crucial for urban planning, transportationmanagement, and infrastructure development. However, achieving both accuracyand interpretability in prediction models remains challenging due to thecomplexity of traffic data and the inherent opacity of deep learningmethodologies. In this paper, we propose a novel approach, Traffic FlowPrediction LLM (TF-LLM), which leverages large language models (LLMs) togenerate interpretable traffic flow predictions. By transferring multi-modaltraffic data into natural language descriptions, TF-LLM captures complexspatial-temporal patterns and external factors such as weather conditions,Points of Interest (PoIs), date, and holidays. We fine-tune the LLM frameworkusing language-based instructions to align with spatial-temporal traffic flowdata. Our comprehensive multi-modal traffic flow dataset (CATraffic) inCalifornia enables the evaluation of TF-LLM against state-of-the-art deeplearning baselines. Results demonstrate TF-LLM's competitive accuracy whileproviding intuitive and interpretable predictions. We discuss thespatial-temporal and input dependencies for explainable future flowforecasting, showcasing TF-LLM's potential for diverse city prediction tasks.This paper contributes to advancing explainable traffic prediction models andlays a foundation for future exploration of LLM applications in transportation.</description><author>Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhu, Hao, Yang</author><pubDate>Mon, 08 Apr 2024 16:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02937v2</guid></item><item><title>Towards Domain-agnostic Depth Completion</title><link>http://arxiv.org/abs/2207.14466v2</link><description>Existing depth completion methods are often targeted at a specific sparsedepth type and generalize poorly across task domains. We present a method tocomplete sparse/semi-dense, noisy, and potentially low-resolution depth mapsobtained by various range sensors, including those in modern mobile phones, orby multi-view reconstruction algorithms. Our method leverages a data-drivenprior in the form of a single image depth prediction network trained onlarge-scale datasets, the output of which is used as an input to our model. Wepropose an effective training scheme where we simulate various sparsitypatterns in typical task domains. In addition, we design two new benchmarks toevaluate the generalizability and the robustness of depth completion methods.Our simple method shows superior cross-domain generalization ability againststate-of-the-art depth completion methods, introducing a practical solution tohigh-quality depth capture on a mobile device. The code is available at:https://github.com/YvanYin/FillDepth.</description><author>Guangkai Xu, Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Simon Chen, Jia-Wang Bian</author><pubDate>Mon, 08 Apr 2024 16:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.14466v2</guid></item><item><title>MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning</title><link>http://arxiv.org/abs/2404.05621v1</link><description>While excellent in transfer learning, Vision-Language models (VLMs) come withhigh computational costs due to their large number of parameters. To addressthis issue, removing parameters via model pruning is a viable solution.However, existing techniques for VLMs are task-specific, and thus requirepruning the network from scratch for each new task of interest. In this work,we explore a new direction: Task-Agnostic Vision-Language Pruning (TA-VLP).Given a pretrained VLM, the goal is to find a unique pruned counterparttransferable to multiple unknown downstream tasks. In this challenging setting,the transferable representations already encoded in the pretrained model are akey aspect to preserve. Thus, we propose Multimodal Flow Pruning (MULTIFLOW), afirst, gradient-free, pruning framework for TA-VLP where: (i) the importance ofa parameter is expressed in terms of its magnitude and its information flow, byincorporating the saliency of the neurons it connects; and (ii) pruning isdriven by the emergent (multimodal) distribution of the VLM parameters afterpretraining. We benchmark eight state-of-the-art pruning algorithms in thecontext of TA-VLP, experimenting with two VLMs, three vision-language tasks,and three pruning ratios. Our experimental results show that MULTIFLOWoutperforms recent sophisticated, combinatorial competitors in the vastmajority of the cases, paving the way towards addressing TA-VLP. The code ispublicly available at https://github.com/FarinaMatteo/multiflow.</description><author>Matteo Farina, Massimiliano Mancini, Elia Cunegatti, Gaowen Liu, Giovanni Iacca, Elisa Ricci</author><pubDate>Mon, 08 Apr 2024 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05621v1</guid></item><item><title>Understanding the Learning Dynamics of Alignment with Human Feedback</title><link>http://arxiv.org/abs/2403.18742v3</link><description>Aligning large language models (LLMs) with human intentions has become acritical task for safely deploying models in real-world systems. While existingalignment approaches have seen empirical success, theoretically understandinghow these methods affect model behavior remains an open question. Our workprovides an initial attempt to theoretically analyze the learning dynamics ofhuman preference alignment. We formally show how the distribution of preferencedatasets influences the rate of model updates and provide rigorous guaranteeson the training accuracy. Our theory also reveals an intricate phenomenon wherethe optimization is prone to prioritizing certain behaviors with higherpreference distinguishability. We empirically validate our findings oncontemporary LLMs and alignment tasks, reinforcing our theoretical insights andshedding light on considerations for future alignment approaches. Disclaimer:This paper contains potentially offensive text; reader discretion is advised.</description><author>Shawn Im, Yixuan Li</author><pubDate>Mon, 08 Apr 2024 16:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18742v3</guid></item><item><title>Intention-Conditioned Long-Term Human Egocentric Action Forecasting</title><link>http://arxiv.org/abs/2207.12080v4</link><description>To anticipate how a human would act in the future, it is essential tounderstand the human intention since it guides the human towards a certaingoal. In this paper, we propose a hierarchical architecture which assumes asequence of human action (low-level) can be driven from the human intention(high-level). Based on this, we deal with Long-Term Action Anticipation task inegocentric videos. Our framework first extracts two level of human informationover the N observed videos human actions through a Hierarchical Multi-task MLPMixer (H3M). Then, we condition the uncertainty of the future through anIntention-Conditioned Variational Auto-Encoder (I-CVAE) that generates K stablepredictions of the next Z=20 actions that the observed human might perform. Byleveraging human intention as high-level information, we claim that our modelis able to anticipate more time-consistent actions in the long-term, thusimproving the results over baseline methods in EGO4D Challenge. This workranked first in both CVPR@2022 and ECVV@2022 EGO4D LTA Challenge by providingmore plausible anticipated sequences, improving the anticipation of nouns andoverall actions. Webpage: https://evm7.github.io/icvae-page/</description><author>Esteve Valls Mascaro, Hyemin Ahn, Dongheui Lee</author><pubDate>Mon, 08 Apr 2024 16:50:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.12080v4</guid></item><item><title>Robust Human Motion Forecasting using Transformer-based Model</title><link>http://arxiv.org/abs/2302.08274v3</link><description>Comprehending human motion is a fundamental challenge for developingHuman-Robot Collaborative applications. Computer vision researchers haveaddressed this field by only focusing on reducing error in predictions, but nottaking into account the requirements to facilitate its implementation inrobots. In this paper, we propose a new model based on Transformer thatsimultaneously deals with the real time 3D human motion forecasting in theshort and long term. Our 2-Channel Transformer (2CH-TR) is able to efficientlyexploit the spatio-temporal information of a shortly observed sequence (400ms)and generates a competitive accuracy against the current state-of-the-art.2CH-TR stands out for the efficient performance of the Transformer, beinglighter and faster than its competitors. In addition, our model is tested inconditions where the human motion is severely occluded, demonstrating itsrobustness in reconstructing and predicting 3D human motion in a highly noisyenvironment. Our experiment results show that the proposed 2CH-TR outperformsthe ST-Transformer, which is another state-of-the-art model based on theTransformer, in terms of reconstruction and prediction under the sameconditions of input prefix. Our model reduces in 8.89% the mean squared errorof ST-Transformer in short-term prediction, and 2.57% in long-term predictionin Human3.6M dataset with 400ms input prefix. Webpage:https://evm7.github.io/2CHTR-page/</description><author>Esteve Valls Mascaro, Shuo Ma, Hyemin Ahn, Dongheui Lee</author><pubDate>Mon, 08 Apr 2024 16:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08274v3</guid></item><item><title>A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis</title><link>http://arxiv.org/abs/2308.07301v2</link><description>The synthesis of human motion has traditionally been addressed throughtask-dependent models that focus on specific challenges, such as predictingfuture motions or filling in intermediate poses conditioned on known key-poses.In this paper, we present a novel task-independent model called UNIMASK-M,which can effectively address these challenges using a unified architecture.Our model obtains comparable or better performance than the state-of-the-art ineach field. Inspired by Vision Transformers (ViTs), our UNIMASK-M modeldecomposes a human pose into body parts to leverage the spatio-temporalrelationships existing in human motion. Moreover, we reformulate variouspose-conditioned motion synthesis tasks as a reconstruction problem withdifferent masking patterns given as input. By explicitly informing our modelabout the masked joints, our UNIMASK-M becomes more robust to occlusions.Experimental results show that our model successfully forecasts human motion onthe Human3.6M dataset. Moreover, it achieves state-of-the-art results in motioninbetweening on the LaFAN1 dataset, particularly in long transition periods.More information can be found on the project websitehttps://evm7.github.io/UNIMASKM-page/</description><author>Esteve Valls Mascaro, Hyemin Ahn, Dongheui Lee</author><pubDate>Mon, 08 Apr 2024 16:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07301v2</guid></item><item><title>HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs</title><link>http://arxiv.org/abs/2309.16524v2</link><description>Robots are becoming increasingly integrated into our lives, assisting us invarious tasks. To ensure effective collaboration between humans and robots, itis essential that they understand our intentions and anticipate our actions. Inthis paper, we propose a Human-Object Interaction (HOI) anticipation frameworkfor collaborative robots. We propose an efficient and robust transformer-basedmodel to detect and anticipate HOIs from videos. This enhanced anticipationempowers robots to proactively assist humans, resulting in more efficient andintuitive collaborations. Our model outperforms state-of-the-art results in HOIdetection and anticipation in VidHOI dataset with an increase of 1.76% and1.04% in mAP respectively while being 15.4 times faster. We showcase theeffectiveness of our approach through experimental results in a real robot,demonstrating that the robot's ability to anticipate HOIs is key for betterHuman-Robot Interaction. More information can be found on our project webpage:https://evm7.github.io/HOI4ABOT_page/</description><author>Esteve Valls Mascaro, Daniel Sliwowski, Dongheui Lee</author><pubDate>Mon, 08 Apr 2024 16:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16524v2</guid></item><item><title>ImitationNet: Unsupervised Human-to-Robot Motion Retargeting via Shared Latent Space</title><link>http://arxiv.org/abs/2309.05310v3</link><description>This paper introduces a novel deep-learning approach for human-to-robotmotion retargeting, enabling robots to mimic human poses accurately. Contraryto prior deep-learning-based works, our method does not require pairedhuman-to-robot data, which facilitates its translation to new robots. First, weconstruct a shared latent space between humans and robots via adaptivecontrastive learning that takes advantage of a proposed cross-domain similaritymetric between the human and robot poses. Additionally, we propose aconsistency term to build a common latent space that captures the similarity ofthe poses with precision while allowing direct robot motion control from thelatent space. For instance, we can generate in-between motion through simplelinear interpolation between two projected human poses. We conduct acomprehensive evaluation of robot control from diverse modalities (i.e., texts,RGB videos, and key poses), which facilitates robot control for non-expertusers. Our model outperforms existing works regarding human-to-robotretargeting in terms of efficiency and precision. Finally, we implemented ourmethod in a real robot with self-collision avoidance through a whole-bodycontroller to showcase the effectiveness of our approach. More information onour website https://evm7.github.io/UnsH2R/</description><author>Yashuai Yan, Esteve Valls Mascaro, Dongheui Lee</author><pubDate>Mon, 08 Apr 2024 16:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05310v3</guid></item><item><title>Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction</title><link>http://arxiv.org/abs/2402.04768v2</link><description>Integrating robots into populated environments is a complex challenge thatrequires an understanding of human social dynamics. In this work, we propose tomodel social motion forecasting in a shared human-robot representation space,which facilitates us to synthesize robot motions that interact with humans insocial scenarios despite not observing any robot in the motion training. Wedevelop a transformer-based architecture called ECHO, which operates in theaforementioned shared space to predict the future motions of the agentsencountered in social scenarios. Contrary to prior works, we reformulate thesocial motion problem as the refinement of the predicted individual motionsbased on the surrounding agents, which facilitates the training while allowingfor single-motion forecasting when only one human is in the scene. We evaluateour model in multi-person and human-robot motion forecasting tasks and obtainstate-of-the-art performance by a large margin while being efficient andperforming in real-time. Additionally, our qualitative results showcase theeffectiveness of our approach in generating human-robot interaction behaviorsthat can be controlled via text commands. Webpage: https://evm7.github.io/ECHO/</description><author>Esteve Valls Mascaro, Yashuai Yan, Dongheui Lee</author><pubDate>Mon, 08 Apr 2024 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04768v2</guid></item><item><title>Deep Representation Learning for Multi-functional Degradation Modeling of Community-dwelling Aging Population</title><link>http://arxiv.org/abs/2404.05613v1</link><description>As the aging population grows, particularly for the baby boomer generation,the United States is witnessing a significant increase in the elderlypopulation experiencing multifunctional disabilities. These disabilities,stemming from a variety of chronic diseases, injuries, and impairments, presenta complex challenge due to their multidimensional nature, encompassing bothphysical and cognitive aspects. Traditional methods often use univariateregression-based methods to model and predict single degradation conditions andassume population homogeneity, which is inadequate to address the complexityand diversity of aging-related degradation. This study introduces a novelframework for multi-functional degradation modeling that captures themultidimensional (e.g., physical and cognitive) and heterogeneous nature ofelderly disabilities. Utilizing deep learning, our approach predicts healthdegradation scores and uncovers latent heterogeneity from elderly healthhistories, offering both efficient estimation and explainable insights into thediverse effects and causes of aging-related degradation. A real-case studydemonstrates the effectiveness and marks a pivotal contribution to accuratelymodeling the intricate dynamics of elderly degradation, and addresses thehealthcare challenges in the aging population.</description><author>Suiyao Chen, Xinyi Liu, Yulei Li, Jing Wu, Handong Yao</author><pubDate>Mon, 08 Apr 2024 16:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05613v1</guid></item><item><title>Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects</title><link>http://arxiv.org/abs/2306.10125v4</link><description>Self-supervised learning (SSL) has recently achieved impressive performanceon various time series tasks. The most prominent advantage of SSL is that itreduces the dependence on labeled data. Based on the pre-training andfine-tuning strategy, even a small amount of labeled data can achieve highperformance. Compared with many published self-supervised surveys on computervision and natural language processing, a comprehensive survey for time seriesSSL is still missing. To fill this gap, we review current state-of-the-art SSLmethods for time series data in this article. To this end, we firstcomprehensively review existing surveys related to SSL and time series, andthen provide a new taxonomy of existing time series SSL methods by summarizingthem from three perspectives: generative-based, contrastive-based, andadversarial-based. These methods are further divided into ten subcategorieswith detailed reviews and discussions about their key intuitions, mainframeworks, advantages and disadvantages. To facilitate the experiments andvalidation of time series SSL methods, we also summarize datasets commonly usedin time series forecasting, classification, anomaly detection, and clusteringtasks. Finally, we present the future directions of SSL for time seriesanalysis.</description><author>Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, Shirui Pan</author><pubDate>Mon, 08 Apr 2024 16:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10125v4</guid></item><item><title>Faithful and Robust Local Interpretability for Textual Predictions</title><link>http://arxiv.org/abs/2311.01605v2</link><description>Interpretability is essential for machine learning models to be trusted anddeployed in critical domains. However, existing methods for interpreting textmodels are often complex, lack mathematical foundations, and their performanceis not guaranteed. In this paper, we propose FRED (Faithful and RobustExplainer for textual Documents), a novel method for interpreting predictionsover text. FRED offers three key insights to explain a model prediction: (1) itidentifies the minimal set of words in a document whose removal has thestrongest influence on the prediction, (2) it assigns an importance score toeach token, reflecting its influence on the model's output, and (3) it providescounterfactual explanations by generating examples similar to the originaldocument, but leading to a different prediction. We establish the reliabilityof FRED through formal definitions and theoretical analyses on interpretableclassifiers. Additionally, our empirical evaluation against state-of-the-artmethods demonstrates the effectiveness of FRED in providing insights into textmodels.</description><author>Gianluigi Lopardo, Frederic Precioso, Damien Garreau</author><pubDate>Mon, 08 Apr 2024 16:35:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01605v2</guid></item><item><title>Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective</title><link>http://arxiv.org/abs/2312.10401v3</link><description>Graph contrastive learning is a general learning paradigm excelling atcapturing invariant information from diverse perturbations in graphs. Recentworks focus on exploring the structural rationale from graphs, therebyincreasing the discriminability of the invariant information. However, suchmethods may incur in the mis-learning of graph models towards theinterpretability of graphs, and thus the learned noisy and task-agnosticinformation interferes with the prediction of graphs. To this end, with thepurpose of exploring the intrinsic rationale of graphs, we accordingly proposeto capture the dimensional rationale from graphs, which has not receivedsufficient attention in the literature. The conducted exploratory experimentsattest to the feasibility of the aforementioned roadmap. To elucidate theinnate mechanism behind the performance improvement arising from thedimensional rationale, we rethink the dimensional rationale in graphcontrastive learning from a causal perspective and further formalize thecausality among the variables in the pre-training stage to build thecorresponding structural causal model. On the basis of the understanding of thestructural causal model, we propose the dimensional rationale-aware graphcontrastive learning approach, which introduces a learnable dimensionalrationale acquiring network and a redundancy reduction constraint. Thelearnable dimensional rationale acquiring network is updated by leveraging abi-level meta-learning technique, and the redundancy reduction constraintdisentangles the redundant features through a decorrelation process duringlearning. Empirically, compared with state-of-the-art methods, our method canyield significant performance boosts on various benchmarks with respect todiscriminability and transferability. The code implementation of our method isavailable at https://github.com/ByronJi/DRGCL.</description><author>Qirui Ji, Jiangmeng Li, Jie Hu, Rui Wang, Changwen Zheng, Fanjiang Xu</author><pubDate>Mon, 08 Apr 2024 16:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10401v3</guid></item><item><title>A Training-Free Plug-and-Play Watermark Framework for Stable Diffusion</title><link>http://arxiv.org/abs/2404.05607v1</link><description>Nowadays, the family of Stable Diffusion (SD) models has gained prominencefor its high quality outputs and scalability. This has also raised securityconcerns on social media, as malicious users can create and disseminate harmfulcontent. Existing approaches involve training components or entire SDs to embeda watermark in generated images for traceability and responsibilityattribution. However, in the era of AI-generated content (AIGC), the rapiditeration of SDs renders retraining with watermark models costly. To addressthis, we propose a training-free plug-and-play watermark framework for SDs.Without modifying any components of SDs, we embed diverse watermarks in thelatent space, adapting to the denoising process. Our experimental findingsreveal that our method effectively harmonizes image quality and watermarkinvisibility. Furthermore, it performs robustly under various attacks. We alsohave validated that our method is generalized to multiple versions of SDs, evenwithout retraining the watermark model.</description><author>Guokai Zhang, Lanjun Wang, Yuting Su, An-An Liu</author><pubDate>Mon, 08 Apr 2024 16:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05607v1</guid></item><item><title>Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations</title><link>http://arxiv.org/abs/2309.05575v3</link><description>Anisotropic diffusion processes with a diffusion tensor are important inimage analysis, physics, and engineering. However, their numericalapproximation has a strong impact on dissipative artefacts and deviations fromrotation invariance. In this work, we study a large family of finite differencediscretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropicdiffusion into four 1-D diffusions. The resulting stencil class involves onefree parameter and covers a wide range of existing discretisations. Itcomprises the full stencil family of Weickert et al. (2013) and shows thattheir two parameters contain redundancy. Furthermore, we establish a bound onthe spectral norm of the matrix corresponding to the stencil. This gives timestep size limits that guarantee stability of an explicit scheme in theEuclidean norm. Our directional splitting also allows a very naturaltranslation of the explicit scheme into ResNet blocks. Employing neural networklibraries enables simple and highly efficient parallel implementations on GPUs.</description><author>Karl Schrader, Joachim Weickert, Michael Krause</author><pubDate>Mon, 08 Apr 2024 16:26:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05575v3</guid></item><item><title>Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction</title><link>http://arxiv.org/abs/2404.05606v1</link><description>Face meshes in consistent topology serve as the foundation for manyface-related applications, such as 3DMM constrained face reconstruction andexpression retargeting. Traditional methods commonly acquire topology uniformedface meshes by two separate steps: multi-view stereo (MVS) to reconstructshapes followed by non-rigid registration to align topology, but struggles withhandling noise and non-lambertian surfaces. Recently neural volume renderingtechniques have been rapidly evolved and shown great advantages in 3Dreconstruction or novel view synthesis. Our goal is to leverage the superiorityof neural volume rendering into multi-view reconstruction of face mesh withconsistent topology. We propose a mesh volume rendering method that enablesdirectly optimizing mesh geometry while preserving topology, and learningimplicit features to model complex facial appearance from multi-view images.The key innovation lies in spreading sparse mesh features into the surroundingspace to simulate radiance field required for volume rendering, whichfacilitates backpropagation of gradients from images to mesh geometry andimplicit appearance features. Our proposed feature spreading module exhibitsdeformation invariance, enabling photorealistic rendering seamlessly after meshediting. We conduct experiments on multi-view face image dataset to evaluatethe reconstruction and implement an application for photorealistic rendering ofanimated face mesh.</description><author>Yating Wang, Ran Yi, Ke Fan, Jinkun Hao, Jiangbo Lu, Lizhuang Ma</author><pubDate>Mon, 08 Apr 2024 16:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05606v1</guid></item><item><title>Graph Neural Networks Automated Design and Deployment on Device-Edge Co-Inference Systems</title><link>http://arxiv.org/abs/2404.05605v1</link><description>The key to device-edge co-inference paradigm is to partition models intocomputation-friendly and computation-intensive parts across the device and theedge, respectively. However, for Graph Neural Networks (GNNs), we find thatsimply partitioning without altering their structures can hardly achieve thefull potential of the co-inference paradigm due to variouscomputational-communication overheads of GNN operations over heterogeneousdevices. We present GCoDE, the first automatic framework for GNN thatinnovatively Co-designs the architecture search and the mapping of eachoperation on Device-Edge hierarchies. GCoDE abstracts the device communicationprocess into an explicit operation and fuses the search of architecture and theoperations mapping in a unified space for joint-optimization. Also, theperformance-awareness approach, utilized in the constraint-based search processof GCoDE, enables effective evaluation of architecture efficiency in diverseheterogeneous systems. We implement the co-inference engine and runtimedispatcher in GCoDE to enhance the deployment efficiency. Experimental resultsshow that GCoDE can achieve up to $44.9\times$ speedup and $98.2\%$ energyreduction compared to existing approaches across various applications andsystem configurations.</description><author>Ao Zhou, Jianlei Yang, Tong Qiao, Yingjie Qi, Zhi Yang, Weisheng Zhao, Chunming Hu</author><pubDate>Mon, 08 Apr 2024 16:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05605v1</guid></item><item><title>Technical Report: The Graph Spectral Token -- Enhancing Graph Transformers with Spectral Information</title><link>http://arxiv.org/abs/2404.05604v1</link><description>Graph Transformers have emerged as a powerful alternative to Message-PassingGraph Neural Networks (MP-GNNs) to address limitations such as over-squashingof information exchange. However, incorporating graph inductive bias intotransformer architectures remains a significant challenge. In this report, wepropose the Graph Spectral Token, a novel approach to directly encode graphspectral information, which captures the global structure of the graph, intothe transformer architecture. By parameterizing the auxiliary [CLS] token andleaving other tokens representing graph nodes, our method seamlessly integratesspectral information into the learning process. We benchmark the effectivenessof our approach by enhancing two existing graph transformers, GraphTrans andSubFormer. The improved GraphTrans, dubbed GraphTrans-Spec, achieves over 10%improvements on large graph benchmark datasets while maintaining efficiencycomparable to MP-GNNs. SubFormer-Spec demonstrates strong performance acrossvarious datasets.</description><author>Zihan Pengmei, Zimu Li</author><pubDate>Mon, 08 Apr 2024 16:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05604v1</guid></item><item><title>Learning Mutually Informed Representations for Characters and Subwords</title><link>http://arxiv.org/abs/2311.07853v2</link><description>Most pretrained language models rely on subword tokenization, which processestext as a sequence of subword tokens. However, different granularities of text,such as characters, subwords, and words, can contain different kinds ofinformation. Previous studies have shown that incorporating multiple inputgranularities improves model generalization, yet very few of them outputsuseful representations for each granularity. In this paper, we introduce theentanglement model, aiming to combine character and subword language models.Inspired by vision-language models, our model treats characters and subwords asseparate modalities, and it generates mutually informed representations forboth granularities as output. We evaluate our model on text classification,named entity recognition, POS-tagging, and character-level sequence labeling(intraword code-switching). Notably, the entanglement model outperforms itsbackbone language models, particularly in the presence of noisy texts andlow-resource languages. Furthermore, the entanglement model even outperformslarger pre-trained models on all English sequence labeling tasks andclassification tasks. We make our code publically available.</description><author>Yilin Wang, Xinyi Hu, Matthew R. Gormley</author><pubDate>Mon, 08 Apr 2024 16:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07853v2</guid></item><item><title>Self-Explainable Affordance Learning with Embodied Caption</title><link>http://arxiv.org/abs/2404.05603v1</link><description>In the field of visual affordance learning, previous methods mainly usedabundant images or videos that delineate human behavior patterns to identifyaction possibility regions for object manipulation, with a variety ofapplications in robotic tasks. However, they encounter a main challenge ofaction ambiguity, illustrated by the vagueness like whether to beat or carry adrum, and the complexities involved in processing intricate scenes. Moreover,it is important for human intervention to rectify robot errors in time. Toaddress these issues, we introduce Self-Explainable Affordance learning (SEA)with embodied caption. This innovation enables robots to articulate theirintentions and bridge the gap between explainable vision-language caption andvisual affordance learning. Due to a lack of appropriate dataset, we unveil apioneering dataset and metrics tailored for this task, which integrates images,heatmaps, and embodied captions. Furthermore, we propose a novel model toeffectively combine affordance grounding with self-explanation in a simple butefficient manner. Extensive quantitative and qualitative experimentsdemonstrate our method's effectiveness.</description><author>Zhipeng Zhang, Zhimin Wei, Guolei Sun, Peng Wang, Luc Van Gool</author><pubDate>Mon, 08 Apr 2024 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05603v1</guid></item><item><title>SpeechAlign: Aligning Speech Generation to Human Preferences</title><link>http://arxiv.org/abs/2404.05600v1</link><description>Speech language models have significantly advanced in generating realisticspeech, with neural codec language models standing out. However, theintegration of human feedback to align speech outputs to human preferences isoften neglected. This paper addresses this gap by first analyzing thedistribution gap in codec language models, highlighting how it leads todiscrepancies between the training and inference phases, which negativelyaffects performance. Then we explore leveraging learning from human feedback tobridge the distribution gap. We introduce SpeechAlign, an iterativeself-improvement strategy that aligns speech language models to humanpreferences. SpeechAlign involves constructing a preference codec datasetcontrasting golden codec tokens against synthetic tokens, followed bypreference optimization to improve the codec language model. This cycle ofimprovement is carried out iteratively to steadily convert weak models tostrong ones. Through both subjective and objective evaluations, we show thatSpeechAlign can bridge the distribution gap and facilitating continuousself-improvement of the speech language model. Moreover, SpeechAlign exhibitsrobust generalization capabilities and works for smaller models. Code andmodels will be available at https://github.com/0nutation/SpeechGPT.</description><author>Dong Zhang, Zhaowei Li, Shimin Li, Xin Zhang, Pengyu Wang, Yaqian Zhou, Xipeng Qiu</author><pubDate>Mon, 08 Apr 2024 16:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05600v1</guid></item><item><title>Neural Embedding Compression For Efficient Multi-Task Earth Observation Modelling</title><link>http://arxiv.org/abs/2403.17886v3</link><description>As repositories of large scale data in earth observation (EO) have grown, sohave transfer and storage costs for model training and inference, expendingsignificant resources. We introduce Neural Embedding Compression (NEC), basedon the transfer of compressed embeddings to data consumers instead of raw data.We adapt foundation models (FM) through learned neural compression to generatemulti-task embeddings while navigating the tradeoff between compression rateand embedding utility. We update only a small fraction of the FM parameters(10%) for a short training period (1% of the iterations of pre-training). Weevaluate NEC on two EO tasks: scene classification and semantic segmentation.Compared with applying traditional compression to the raw data, NEC achievessimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%compression, performance drops by only 5% on the scene classification task.Overall, NEC is a data-efficient yet performant approach for multi-task EOmodelling.</description><author>Carlos Gomes, Thomas Brunschwiler</author><pubDate>Mon, 08 Apr 2024 16:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17886v3</guid></item><item><title>DRCT: Saving Image Super-resolution away from Information Bottleneck</title><link>http://arxiv.org/abs/2404.00722v3</link><description>In recent years, Vision Transformer-based applications to low-level visiontasks have achieved widespread success. Unlike CNN-based models, Transformersare more adept at capturing long-range dependencies, enabling thereconstruction of images utilizing information from non-local areas. In thedomain of super-resolution, Swin-transformer-based approaches have becomemainstream due to their capacity to capture global spatial information andtheir shifting-window attention mechanism that facilitates the interchange ofinformation between different windows. Many researchers have enhanced imagequality and network efficiency by expanding the receptive field or designingcomplex networks, yielding commendable results. However, we observed thatspatial information tends to diminish during the forward propagation processdue to increased depth, leading to a loss of spatial information and,consequently, limiting the model's potential. To address this, we propose theDense-residual-connected Transformer (DRCT), aimed at mitigating the loss ofspatial information through dense-residual connections between layers, therebyunleashing the model's potential and enhancing performance. Experiment resultsindicate that our approach is not only straightforward but also achievesremarkable efficiency, surpassing state-of-the-art methods and performingcommendably at NTIRE2024.</description><author>Chih-Chung Hsu, Chia-Ming Lee, Yi-Shiuan Chou</author><pubDate>Mon, 08 Apr 2024 16:15:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00722v3</guid></item><item><title>UniFL: Improve Stable Diffusion via Unified Feedback Learning</title><link>http://arxiv.org/abs/2404.05595v1</link><description>Diffusion models have revolutionized the field of image generation, leadingto the proliferation of high-quality models and diverse downstreamapplications. However, despite these significant advancements, the currentcompetitive solutions still suffer from several limitations, including inferiorvisual quality, a lack of aesthetic appeal, and inefficient inference, withouta comprehensive solution in sight. To address these challenges, we presentUniFL, a unified framework that leverages feedback learning to enhancediffusion models comprehensively. UniFL stands out as a universal, effective,and generalizable solution applicable to various diffusion models, such asSD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptualfeedback learning, which enhances visual quality; decoupled feedback learning,which improves aesthetic appeal; and adversarial feedback learning, whichoptimizes inference speed. In-depth experiments and extensive user studiesvalidate the superior performance of our proposed method in enhancing both thequality of generated models and their acceleration. For instance, UniFLsurpasses ImageReward by 17% user preference in terms of generation quality andoutperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, wehave verified the efficacy of our approach in downstream tasks, including Lora,ControlNet, and AnimateDiff.</description><author>Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, Lean Fu, Guanbin Li</author><pubDate>Mon, 08 Apr 2024 16:14:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05595v1</guid></item><item><title>MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering</title><link>http://arxiv.org/abs/2404.05590v1</link><description>Large Language Models (LLMs) have the potential of facilitating thedevelopment of Artificial Intelligence technology to assist medical experts forinteractive decision support, which has been demonstrated by their competitiveperformances in Medical QA. However, while impressive, the required quality barfor medical applications remains far from being achieved. Currently, LLMsremain challenged by outdated knowledge and by their tendency to generatehallucinated content. Furthermore, most benchmarks to assess medical knowledgelack reference gold explanations which means that it is not possible toevaluate the reasoning of LLMs predictions. Finally, the situation isparticularly grim if we consider benchmarking LLMs for languages other thanEnglish which remains, as far as we know, a totally neglected topic. In orderto address these shortcomings, in this paper we present MedExpQA, the firstmultilingual benchmark based on medical exams to evaluate LLMs in MedicalQuestion Answering. To the best of our knowledge, MedExpQA includes for thefirst time reference gold explanations written by medical doctors which can beleveraged to establish various gold-based upper-bounds for comparison with LLMsperformance. Comprehensive multilingual experimentation using both the goldreference explanations and Retrieval Augmented Generation (RAG) approaches showthat performance of LLMs still has large room for improvement, especially forlanguages other than English. Furthermore, and despite using state-of-the-artRAG methods, our results also demonstrate the difficulty of obtaining andintegrating readily available medical knowledge that may positively impactresults on downstream evaluations for Medical Question Answering. So far thebenchmark is available in four languages, but we hope that this work mayencourage further development to other languages.</description><author>IÃ±igo Alonso, Maite Oronoz, Rodrigo Agerri</author><pubDate>Mon, 08 Apr 2024 16:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05590v1</guid></item><item><title>Towards an end-to-end artificial intelligence driven global weather forecasting system</title><link>http://arxiv.org/abs/2312.12462v3</link><description>The weather forecasting system is important for science and society, andsignificant achievements have been made in applying artificial intelligence(AI) to medium-range weather forecasting. However, existing AI-based weatherforecasting models rely on analysis or reanalysis products from traditionalnumerical weather prediction (NWP) systems as initial conditions for makingpredictions. Initial states are typically generated by traditional dataassimilation components, which are computational expensive and time-consuming.Here we present an AI-based data assimilation model, i.e., Adas, for globalweather variables. By introducing the confidence matrix, Adas employs gatedconvolution to handle sparse observations and gated cross-attention forcapturing the interactions between the background and observations. Further, wecombine Adas with the advanced AI-based forecasting model (i.e., FengWu) toconstruct the first end-to-end AI-based global weather forecasting system:FengWu-Adas. We demonstrate that Adas can assimilate global observations toproduce high-quality analysis, enabling the system operate stably for longterm. Moreover, we are the first to apply the methods to real-world scenarios,which is more challenging and has considerable practical application potential.We have also achieved the forecasts based on the analyses generated by AI witha skillful forecast lead time exceeding that of the IFS for the first time.</description><author>Kun Chen, Lei Bai, Fenghua Ling, Peng Ye, Tao Chen, Jing-Jia Luo, Hao Chen, Yi Xiao, Kang Chen, Tao Han, Wanli Ouyang</author><pubDate>Mon, 08 Apr 2024 16:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12462v3</guid></item><item><title>Enhancing Software Related Information Extraction with Generative Language Models through Single-Choice Question Answering</title><link>http://arxiv.org/abs/2404.05587v1</link><description>This paper describes our participation in the Shared Task on SoftwareMentions Disambiguation (SOMD), with a focus on improving relation extractionin scholarly texts through Generative Language Models (GLMs) usingsingle-choice question-answering. The methodology prioritises the use ofin-context learning capabilities of GLMs to extract software-related entitiesand their descriptive attributes, such as distributive information. Ourapproach uses Retrieval-Augmented Generation (RAG) techniques and GLMs forNamed Entity Recognition (NER) and Attributive NER to identify relationshipsbetween extracted software entities, providing a structured solution foranalysing software citations in academic literature. The paper provides adetailed description of our approach, demonstrating how using GLMs in asingle-choice QA paradigm can greatly enhance IE methodologies. Ourparticipation in the SOMD shared task highlights the importance of precisesoftware citation practices and showcases our system's ability to overcome thechallenges of disambiguating and extracting relationships between softwarementions. This sets the groundwork for future research and development in thisfield.</description><author>Wolfgang Otto, Sharmila Upadhyaya, Stefan Dietze</author><pubDate>Mon, 08 Apr 2024 16:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05587v1</guid></item><item><title>Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning</title><link>http://arxiv.org/abs/2305.14970v2</link><description>Event temporal reasoning aims at identifying the temporal relations betweentwo or more events from narratives. However, knowledge conflicts arise whenthere is a mismatch between the actual temporal relations of events in thecontext and the prior knowledge or biases learned by the model. In this paper,we propose to detect knowledge-conflict examples in event temporal reasoningusing bias indicators, which include event relation prior bias, tense bias,narrative bias, and dependency bias. We define conflict examples as those whereevent relations are opposite to biased or prior relations. To mitigateevent-related knowledge conflicts, we introduce a Counterfactual DataAugmentation (CDA) based method that can be applied to both Pre-trainedLanguage Models (PLMs) and Large Language Models (LLMs) either as additionaltraining data or demonstrations for In-Context Learning. Experiments suggestboth PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning,and CDA has the potential for reducing hallucination and improving modelperformance.</description><author>Tianqing Fang, Zhaowei Wang, Wenxuan Zhou, Hongming Zhang, Yangqiu Song, Muhao Chen</author><pubDate>Mon, 08 Apr 2024 15:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14970v2</guid></item><item><title>Neural Cellular Automata for Lightweight, Robust and Explainable Classification of White Blood Cell Images</title><link>http://arxiv.org/abs/2404.05584v1</link><description>Diagnosis of hematological malignancies depends on accurate identification ofwhite blood cells in peripheral blood smears. Deep learning techniques areemerging as a viable solution to scale and optimize this process by automaticidentification of cells in laboratories. However, these techniques face severalchallenges such as limited generalizability, sensitivity to domain shifts andlack of explainability. Here, we are introducing a novel approach based onneural cellular automata (NCA) for white blood cell classification. We test ourapproach on three datasets of white blood cell images and show that we achievecompetitive performance compared to conventional methods. Our NCA-based methodis significantly smaller in terms of parameters and exhibits robustness todomain shifts. Furthermore, the architecture is inherently explainable,providing insights into the decision process for each classification, helpingexperts understand and validate model predictions. Results demonstrate that NCAnot only can be used for image classification, but also address key challengesof conventional methods, indicating a high potential for applicability inclinical practice.</description><author>Michael Deutges, Ario Sadafi, Nassir Navab, Carsten Marr</author><pubDate>Mon, 08 Apr 2024 15:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05584v1</guid></item><item><title>Towards More General Video-based Deepfake Detection through Facial Feature Guided Adaptation for Foundation Model</title><link>http://arxiv.org/abs/2404.05583v1</link><description>With the rise of deep learning, generative models have enabled the creationof highly realistic synthetic images, presenting challenges due to theirpotential misuse. While research in Deepfake detection has grown rapidly inresponse, many detection methods struggle with unseen Deepfakes generated bynew synthesis techniques. To address this generalisation challenge, we proposea novel Deepfake detection approach by adapting rich information encoded insidethe Foundation Models with rich information encoded inside, specifically usingthe image encoder from CLIP which has demonstrated strong zero-shot capabilityfor downstream tasks. Inspired by the recent advances of parameter efficientfine-tuning, we propose a novel side-network-based decoder to extract spatialand temporal cues from the given video clip, with the promotion of the FacialComponent Guidance (FCG) to guidencourage the spatial feature to includefeatures of key facial parts for more robust and general Deepfake detection.Through extensive cross-dataset evaluations, our approach exhibits superioreffectiveness in identifying unseen Deepfake samples, achieving notableperformance improvementsuccess even with limited training samples andmanipulation types. Our model secures an average performance enhancement of0.9% AUROC in cross-dataset assessments comparing with state-of-the-artmethods, especiallytablishing a significant lead of achieving 4.4% improvementon the challenging DFDC dataset.</description><author>Yue-Hua Han, Tai-Ming Huang, Shu-Tzu Lo, Po-Han Huang, Kai-Lung Hua, Jun-Cheng Chen</author><pubDate>Mon, 08 Apr 2024 15:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05583v1</guid></item><item><title>Deep Clustering Survival Machines with Interpretable Expert Distributions</title><link>http://arxiv.org/abs/2301.11826v4</link><description>Conventional survival analysis methods are typically ineffective tocharacterize heterogeneity in the population while such information can be usedto assist predictive modeling. In this study, we propose a hybrid survivalanalysis method, referred to as deep clustering survival machines, thatcombines the discriminative and generative mechanisms. Similar to the mixturemodels, we assume that the timing information of survival data is generativelydescribed by a mixture of certain numbers of parametric distributions, i.e.,expert distributions. We learn weights of the expert distributions forindividual instances according to their features discriminatively such thateach instance's survival information can be characterized by a weightedcombination of the learned constant expert distributions. This method alsofacilitates interpretable subgrouping/clustering of all instances according totheir associated expert distributions. Extensive experiments on both real andsynthetic datasets have demonstrated that the method is capable of obtainingpromising clustering results and competitive time-to-event predictingperformance.</description><author>Bojian Hou, Hongming Li, Zhicheng Jiao, Zhen Zhou, Hao Zheng, Yong Fan</author><pubDate>Mon, 08 Apr 2024 15:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11826v4</guid></item><item><title>Responsible Visual Editing</title><link>http://arxiv.org/abs/2404.05580v1</link><description>With recent advancements in visual synthesis, there is a growing risk ofencountering images with detrimental effects, such as hate, discrimination, orprivacy violations. The research on transforming harmful images intoresponsible ones remains unexplored. In this paper, we formulate a new task,responsible visual editing, which entails modifying specific concepts within animage to render it more responsible while minimizing changes. However, theconcept that needs to be edited is often abstract, making it challenging tolocate what needs to be modified and plan how to modify it. To tackle thesechallenges, we propose a Cognitive Editor (CoEditor) that harnesses the largemultimodal model through a two-stage cognitive process: (1) a perceptualcognitive process to focus on what needs to be modified and (2) a behavioralcognitive process to strategize how to modify. To mitigate the negativeimplications of harmful images on research, we create a transparent and publicdataset, AltBear, which expresses harmful information using teddy bears insteadof humans. Experiments demonstrate that CoEditor can effectively comprehendabstract concepts within complex scenes and significantly surpass theperformance of baseline models for responsible visual editing. We find that theAltBear dataset corresponds well to the harmful content found in real images,offering a consistent experimental evaluation, thereby providing a saferbenchmark for future research. Moreover, CoEditor also shows great results ingeneral editing. We release our code and dataset athttps://github.com/kodenii/Responsible-Visual-Editing.</description><author>Minheng Ni, Yeli Shen, Lei Zhang, Wangmeng Zuo</author><pubDate>Mon, 08 Apr 2024 15:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05580v1</guid></item><item><title>Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation</title><link>http://arxiv.org/abs/2403.19826v2</link><description>In the domain of computer vision, semantic segmentation emerges as afundamental application within machine learning, wherein individual pixels ofan image are classified into distinct semantic categories. This task transcendstraditional accuracy metrics by incorporating uncertainty quantification, acritical measure for assessing the reliability of each segmentation prediction.Such quantification is instrumental in facilitating informed decision-making,particularly in applications where precision is paramount. Within this nuancedframework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty)has been developed as a specialized tool for evaluating entropy-baseduncertainty in image segmentation tasks. However, our investigation identifiesthree core deficiencies within the PAvPU framework and proposes robustsolutions aimed at refining the metric. By addressing these issues, we aim toenhance the reliability and applicability of uncertainty quantification,especially in scenarios that demand high levels of safety and accuracy, thuscontributing to the advancement of semantic segmentation methodologies incritical applications.</description><author>Qitian Ma, Shyam Nanda Rai, Carlo Masone, Tatiana Tommasi</author><pubDate>Mon, 08 Apr 2024 15:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19826v2</guid></item><item><title>Robust Data Pruning: Uncovering and Overcoming Implicit Bias</title><link>http://arxiv.org/abs/2404.05579v1</link><description>In the era of exceptionally data-hungry models, careful selection of thetraining data is essential to mitigate the extensive costs of deep learning.Data pruning offers a solution by removing redundant or uninformative samplesfrom the dataset, which yields faster convergence and improved neural scalinglaws. However, little is known about its impact on classification bias of thetrained models. We conduct the first systematic study of this effect and revealthat existing data pruning algorithms can produce highly biased classifiers. Atthe same time, we argue that random data pruning with appropriate class ratioshas potential to improve the worst-class performance. We propose a"fairness-aware" approach to pruning and empirically demonstrate itsperformance on standard computer vision benchmarks. In sharp contrast toexisting algorithms, our proposed method continues improving robustness at atolerable drop of average performance as we prune more from the datasets. Wepresent theoretical analysis of the classification risk in a mixture ofGaussians to further motivate our algorithm and support our findings.</description><author>Artem Vysogorets, Kartik Ahuja, Julia Kempe</author><pubDate>Mon, 08 Apr 2024 15:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05579v1</guid></item><item><title>Social-MAE: Social Masked Autoencoder for Multi-person Motion Representation Learning</title><link>http://arxiv.org/abs/2404.05578v1</link><description>For a complete comprehension of multi-person scenes, it is essential to gobeyond basic tasks like detection and tracking. Higher-level tasks, such asunderstanding the interactions and social activities among individuals, arealso crucial. Progress towards models that can fully understand scenesinvolving multiple people is hindered by a lack of sufficient annotated datafor such high-level tasks. To address this challenge, we introduce Social-MAE,a simple yet effective transformer-based masked autoencoder framework formulti-person human motion data. The framework uses masked modeling to pre-trainthe encoder to reconstruct masked human joint trajectories, enabling it tolearn generalizable and data efficient representations of motion in humancrowded scenes. Social-MAE comprises a transformer as the MAE encoder and alighter-weight transformer as the MAE decoder which operates on multi-personjoints' trajectory in the frequency domain. After the reconstruction task, theMAE decoder is replaced with a task-specific decoder and the model isfine-tuned end-to-end for a variety of high-level social tasks. Our proposedmodel combined with our pre-training approach achieves the state-of-the-artresults on various high-level social tasks, including multi-person poseforecasting, social grouping, and social action understanding. Theseimprovements are demonstrated across four popular multi-person datasetsencompassing both human 2D and 3D body pose.</description><author>Mahsa Ehsanpour, Ian Reid, Hamid Rezatofighi</author><pubDate>Mon, 08 Apr 2024 15:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05578v1</guid></item><item><title>Dynamic Backtracking in GFlowNet: Enhancing Decision Steps with Reward-Dependent Adjustment Mechanisms</title><link>http://arxiv.org/abs/2404.05576v1</link><description>Generative Flow Networks (GFlowNets) are probabilistic models predicated onMarkov flows, employing specific amortization algorithms to learn stochasticpolicies that generate compositional substances including biomolecules,chemical materials, and more. Demonstrating formidable prowess in generatinghigh-performance biochemical molecules, GFlowNets accelerate the discovery ofscientific substances, effectively circumventing the time-consuming,labor-intensive, and costly shortcomings intrinsic to conventional materialdiscovery. However, previous work often struggles to accumulate exploratoryexperience and is prone to becoming disoriented within expansive samplingspaces. Attempts to address this issue, such as LS-GFN, are limited to localgreedy searches and lack broader global adjustments. This paper introduces anovel GFlowNet variant, the Dynamic Backtracking GFN (DB-GFN), which enhancesthe adaptability of decision-making steps through a reward-based dynamicbacktracking mechanism. DB-GFN permits backtracking during the networkconstruction process according to the current state's reward value, thuscorrecting disadvantageous decisions and exploring alternative pathways duringthe exploration process. Applied to generative tasks of biochemical moleculesand genetic material sequences, DB-GFN surpasses existing GFlowNet models andtraditional reinforcement learning methods in terms of sample quality,exploration sample quantity, and training convergence speed. Furthermore, theorthogonal nature of DB-GFN suggests its potential as a powerful tool forfuture improvements in GFN networks, with the promise of integrating with otherstrategies to achieve more efficient search performance.</description><author>Shuai Guo, Jielei Chu, Lei Zhu, Tianrui Li</author><pubDate>Mon, 08 Apr 2024 15:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05576v1</guid></item><item><title>FaaF: Facts as a Function for the evaluation of generated text</title><link>http://arxiv.org/abs/2403.03888v2</link><description>The demand for accurate and efficient verification of information in textsgenerated by large language models (LMs) is at an all-time high, but remainsunresolved. Recent efforts have focused on extracting and verifying atomicfacts from these texts via prompting LM evaluators. However, we demonstratethat this method of prompting is unreliable when faced with incomplete orinaccurate reference information. We introduce Facts as a Function (FaaF), anew approach to the fact verification task that leverages the function-callingcapabilities of LMs. FaaF significantly enhances the ability of LMs to identifyunsupported facts in texts, while also improving efficiency and significantlylowering costs compared to prompt-based methods. Additionally, we propose aframework for evaluating factual recall in Retrieval Augmented Generation (RAG)systems, which we employ to compare prompt-based and FaaF methods using variousLMs under challenging conditions.</description><author>Vasileios Katranidis, Gabor Barany</author><pubDate>Mon, 08 Apr 2024 15:49:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03888v2</guid></item><item><title>Fact-Checking Generative AI: Ontology-Driven Biological Graphs for Disease-Gene Link Verification</title><link>http://arxiv.org/abs/2308.03929v4</link><description>Since the launch of various generative AI tools, scientists have beenstriving to evaluate their capabilities and contents, in the hope ofestablishing trust in their generative abilities. Regulations and guidelinesare emerging to verify generated contents and identify novel uses. we aspire todemonstrate how ChatGPT claims are checked computationally using the rigor ofnetwork models. We aim to achieve fact-checking of the knowledge embedded inbiological graphs that were contrived from ChatGPT contents at the aggregatelevel. We adopted a biological networks approach that enables the systematicinterrogation of ChatGPT's linked entities. We designed an ontology-drivenfact-checking algorithm that compares biological graphs constructed fromapproximately 200,000 PubMed abstracts with counterparts constructed from adataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250randomly selected records a ChatGPT dataset of 1000 "simulated" articles , thefact-checking link accuracy ranged from 70% to 86%. This study demonstratedhigh accuracy of aggregate disease-gene links relationships found inChatGPT-generated texts.</description><author>Ahmed Abdeen Hamed, Byung Suk Lee, Alessandro Crimi, Magdalena M. Misiak</author><pubDate>Mon, 08 Apr 2024 15:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03929v4</guid></item><item><title>360Â°REA: Towards A Reusable Experience Accumulation with 360Â° Assessment for Multi-Agent System</title><link>http://arxiv.org/abs/2404.05569v1</link><description>Large language model agents have demonstrated remarkable advancements acrossvarious complex tasks. Recent works focus on optimizing the agent team oremploying self-reflection to iteratively solve complex tasks. Since theseagents are all based on the same LLM, only conducting self-evaluation orremoving underperforming agents does not substantively enhance the capabilityof the agents. We argue that a comprehensive evaluation and accumulatingexperience from evaluation feedback is an effective approach to improvingsystem performance. In this paper, we propose Reusable Experience Accumulationwith 360{\deg} Assessment (360{\deg}REA), a hierarchical multi-agent frameworkinspired by corporate organizational practices. The framework employs a novel360{\deg} performance assessment method for multi-perspective performanceevaluation with fine-grained assessment. To enhance the capability of agents inaddressing complex tasks, we introduce dual-level experience pool for agents toaccumulate experience through fine-grained assessment. Extensive experiments oncomplex task datasets demonstrate the effectiveness of 360{\deg}REA.</description><author>Shen Gao, Hao Li, Zhengliang Shi, Chengrui Huang, Quan Tu, Zhiliang Tian, Minlie Huang, Shuo Shang</author><pubDate>Mon, 08 Apr 2024 15:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05569v1</guid></item><item><title>MESA: Matching Everything by Segmenting Anything</title><link>http://arxiv.org/abs/2401.16741v2</link><description>Feature matching is a crucial task in the field of computer vision, whichinvolves finding correspondences between images. Previous studies achieveremarkable performance using learning-based feature comparison. However, thepervasive presence of matching redundancy between images gives rise tounnecessary and error-prone computations in these methods, imposing limitationson their accuracy. To address this issue, we propose MESA, a novel approach toestablish precise area (or region) matches for efficient matching redundancyreduction. MESA first leverages the advanced image understanding capability ofSAM, a state-of-the-art foundation model for image segmentation, to obtainimage areas with implicit semantic. Then, a multi-relational graph is proposedto model the spatial structure of these areas and construct their scalehierarchy. Based on graphical models derived from the graph, the area matchingis reformulated as an energy minimization task and effectively resolved.Extensive experiments demonstrate that MESA yields substantial precisionimprovement for multiple point matchers in indoor and outdoor downstream tasks,e.g. +13.61% for DKM in indoor pose estimation.</description><author>Yesheng Zhang, Xu Zhao</author><pubDate>Mon, 08 Apr 2024 15:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16741v2</guid></item><item><title>Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models</title><link>http://arxiv.org/abs/2404.05567v1</link><description>Mixture-of-Experts (MoE) language models can reduce computational costs by2-4$\times$ compared to dense models without sacrificing performance, makingthem more efficient in computation-bounded scenarios. However, MoE modelsgenerally require 2-4$\times$ times more parameters to achieve comparableperformance to a dense model, which incurs larger GPU memory requirements andmakes MoE models less efficient in I/O-bounded scenarios like autoregressivegeneration. In this work, we propose a hybrid dense training and sparseinference framework for MoE models (DS-MoE) which achieves strong computationand parameter efficiency by employing dense computation across all expertsduring training and sparse computation during inference. Our experiments ontraining LLMs demonstrate that our DS-MoE models are more parameter-efficientthan standard sparse MoEs and are on par with dense models in terms of totalparameter size and performance while being computationally cheaper (activating30-40% of the model's parameters). Performance tests using vLLM show that ourDS-MoE-6B model runs up to $1.86\times$ faster than similar dense models likeMistral-7B, and between $1.50\times$ and $1.71\times$ faster than comparableMoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.</description><author>Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, Rameswar Panda</author><pubDate>Mon, 08 Apr 2024 15:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05567v1</guid></item><item><title>DPHMs: Diffusion Parametric Head Models for Depth-based Tracking</title><link>http://arxiv.org/abs/2312.01068v2</link><description>We introduce Diffusion Parametric Head Models (DPHMs), a generative modelthat enables robust volumetric head reconstruction and tracking from monoculardepth sequences. While recent volumetric head models, such as NPHMs, can nowexcel in representing high-fidelity head geometries, tracking andreconstructing heads from real-world single-view depth sequences remains verychallenging, as the fitting to partial and noisy observations isunderconstrained. To tackle these challenges, we propose a latentdiffusion-based prior to regularize volumetric head reconstruction andtracking. This prior-based regularizer effectively constrains the identity andexpression codes to lie on the underlying latent manifold which representsplausible head shapes. To evaluate the effectiveness of the diffusion-basedprior, we collect a dataset of monocular Kinect sequences consisting of variouscomplex facial expression motions and rapid transitions. We compare our methodto state-of-the-art tracking methods and demonstrate improved head identityreconstruction as well as robust expression tracking.</description><author>Jiapeng Tang, Angela Dai, Yinyu Nie, Lev Markhasin, Justus Thies, Matthias Niessner</author><pubDate>Mon, 08 Apr 2024 15:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01068v2</guid></item><item><title>Chinese Sequence Labeling with Semi-Supervised Boundary-Aware Language Model Pre-training</title><link>http://arxiv.org/abs/2404.05560v1</link><description>Chinese sequence labeling tasks are heavily reliant on accurate word boundarydemarcation. Although current pre-trained language models (PLMs) have achievedsubstantial gains on these tasks, they rarely explicitly incorporate boundaryinformation into the modeling process. An exception to this is BABERT, whichincorporates unsupervised statistical boundary information into Chinese BERT'spre-training objectives. Building upon this approach, we input supervisedhigh-quality boundary information to enhance BABERT's learning, developing asemi-supervised boundary-aware PLM. To assess PLMs' ability to encodeboundaries, we introduce a novel ``Boundary Information Metric'' that is bothsimple and effective. This metric allows comparison of different PLMs withouttask-specific fine-tuning. Experimental results on Chinese sequence labelingdatasets demonstrate that the improved BABERT variant outperforms the vanillaversion, not only on these tasks but also more broadly across a range ofChinese natural language understanding tasks. Additionally, our proposed metricoffers a convenient and accurate means of evaluating PLMs' boundary awareness.</description><author>Longhui Zhang, Dingkun Long, Meishan Zhang, Yanzhao Zhang, Pengjun Xie, Min Zhang</author><pubDate>Mon, 08 Apr 2024 15:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05560v1</guid></item><item><title>TIM: A Time Interval Machine for Audio-Visual Action Recognition</title><link>http://arxiv.org/abs/2404.05559v1</link><description>Diverse actions give rise to rich audio-visual signals in long videos. Recentworks showcase that the two modalities of audio and video exhibit differenttemporal extents of events and distinct labels. We address the interplaybetween the two modalities in long videos by explicitly modelling the temporalextents of audio and visual events. We propose the Time Interval Machine (TIM)where a modality-specific time interval poses as a query to a transformerencoder that ingests a long video input. The encoder then attends to thespecified interval, as well as the surrounding context in both modalities, inorder to recognise the ongoing action. We test TIM on three long audio-visual video datasets: EPIC-KITCHENS,Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. OnEPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantlylarger pre-training by 2.9% top-1 action recognition accuracy. Additionally, weshow that TIM can be adapted for action detection, using dense multi-scaleinterval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, andshowing strong performance on the Perception Test. Our ablations show thecritical role of integrating the two modalities and modelling their timeintervals in achieving this performance. Code and models at:https://github.com/JacobChalk/TIM</description><author>Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen</author><pubDate>Mon, 08 Apr 2024 15:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05559v1</guid></item><item><title>A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers</title><link>http://arxiv.org/abs/2305.12563v2</link><description>This paper proposes a methodology for generating and perturbing detailedderivations of equations at scale, aided by a symbolic engine, to evaluate thegeneralisability of Transformers to out-of-distribution mathematical reasoningproblems. Instantiating the framework in the context of sequence classificationtasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tunedBERT models, exploring the relationship between specific operators andgeneralisation failure via the perturbation of reasoning aspects such assymmetry and variable surface forms. Surprisingly, our empirical evaluationreveals that the average in-distribution performance of fine-tuned modelssurpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoningcan reduce their performance by up to 80 F1 points. Overall, the resultssuggest that the in-distribution performance of smaller open-source models maypotentially rival GPT by incorporating appropriately structured derivationdependencies during training, and highlight a shared weakness between BERT andGPT involving a relative inability to decode indirect references tomathematical entities. We release the full codebase, constructed datasets, andfine-tuned models to encourage future progress in the field.</description><author>Jordan Meadows, Marco Valentino, Damien Teney, Andre Freitas</author><pubDate>Mon, 08 Apr 2024 15:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12563v2</guid></item><item><title>On the Convergence of Continual Learning with Adaptive Methods</title><link>http://arxiv.org/abs/2404.05555v1</link><description>One of the objectives of continual learning is to prevent catastrophicforgetting in learning multiple tasks sequentially, and the existing solutionshave been driven by the conceptualization of the plasticity-stability dilemma.However, the convergence of continual learning for each sequential task is lessstudied so far. In this paper, we provide a convergence analysis ofmemory-based continual learning with stochastic gradient descent and empiricalevidence that training current tasks causes the cumulative degradation ofprevious tasks. We propose an adaptive method for nonconvex continual learning(NCCL), which adjusts step sizes of both previous and current tasks with thegradients. The proposed method can achieve the same convergence rate as the SGDmethod when the catastrophic forgetting term which we define in the paper issuppressed at each iteration. Further, we demonstrate that the proposedalgorithm improves the performance of continual learning over existing methodsfor several image classification tasks.</description><author>Seungyub Han, Yeongmo Kim, Taehyun Cho, Jungwoo Lee</author><pubDate>Mon, 08 Apr 2024 15:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05555v1</guid></item><item><title>SepVAE: a contrastive VAE to separate pathological patterns from healthy ones</title><link>http://arxiv.org/abs/2307.06206v2</link><description>Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders(VAEs) that aims at separating the common factors of variation between abackground dataset (BG) (i.e., healthy subjects) and a target dataset (TG)(i.e., patients) from the ones that only exist in the target dataset. To do so,these methods separate the latent space into a set of salient features (i.e.,proper to the target dataset) and a set of common features (i.e., exist in bothdatasets). Currently, all models fail to prevent the sharing of informationbetween latent spaces effectively and to capture all salient factors ofvariation. To this end, we introduce two crucial regularization losses: adisentangling term between common and salient representations and aclassification term between background and target samples in the salient space.We show a better performance than previous CA-VAEs methods on three medicalapplications and a natural images dataset (CelebA). Code and datasets areavailable on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.</description><author>Robin Louiset, Edouard Duchesnay, Antoine Grigis, Benoit Dufumier, Pietro Gori</author><pubDate>Mon, 08 Apr 2024 15:26:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06206v2</guid></item><item><title>Alljoined -- A dataset for EEG-to-Image decoding</title><link>http://arxiv.org/abs/2404.05553v1</link><description>We present Alljoined, a dataset built specifically for EEG-to-Image decoding.Recognizing that an extensive and unbiased sampling of neural responses tovisual stimuli is crucial for image reconstruction efforts, we collected datafrom 8 participants looking at 10,000 natural images each. We have currentlygathered 46,080 epochs of brain responses recorded with a 64-channel EEGheadset. The dataset combines response-based stimulus timing, repetitionbetween blocks and sessions, and diverse image classes with the goal ofimproving signal quality. For transparency, we also provide data qualityscores. We publicly release the dataset and all code athttps://linktr.ee/alljoined1.</description><author>Jonathan Xu, Bruno Aristimunha, Max Emanuel Feucht, Emma Qian, Charles Liu, Tazik Shahjahan, Martyna Spyra, Steven Zifan Zhang, Nicholas Short, Jioh Kim, Paula Perdomo, Ricky Renfeng Mao, Yashvir Sabharwal, Michael Ahedor Moaz Shoura, Adrian Nestor</author><pubDate>Mon, 08 Apr 2024 15:21:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05553v1</guid></item><item><title>Evaluating Interventional Reasoning Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2404.05545v1</link><description>Numerous decision-making tasks require estimating causal effects underinterventions on different parts of a system. As practitioners consider usinglarge language models (LLMs) to automate decisions, studying their causalreasoning capabilities becomes crucial. A recent line of work evaluates LLMsability to retrieve commonsense causal facts, but these evaluations do notsufficiently assess how LLMs reason about interventions. Motivated by the rolethat interventions play in causal inference, in this paper, we conductempirical analyses to evaluate whether LLMs can accurately update theirknowledge of a data-generating process in response to an intervention. Wecreate benchmarks that span diverse causal graphs (e.g., confounding,mediation) and variable types, and enable a study of intervention-basedreasoning. These benchmarks allow us to isolate the ability of LLMs toaccurately predict changes resulting from their ability to memorize facts orfind other shortcuts. Our analysis on four LLMs highlights that while GPT- 4models show promising accuracy at predicting the intervention effects, theyremain sensitive to distracting factors in the prompts.</description><author>Tejas Kasetty, Divyat Mahajan, Gintare Karolina Dziugaite, Alexandre Drouin, Dhanya Sridhar</author><pubDate>Mon, 08 Apr 2024 15:15:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05545v1</guid></item><item><title>Stitching Satellites to the Edge: Pervasive and Efficient Federated LEO Satellite Learning</title><link>http://arxiv.org/abs/2401.15541v2</link><description>In the ambitious realm of space AI, the integration of federated learning(FL) with low Earth orbit (LEO) satellite constellations holds immense promise.However, many challenges persist in terms of feasibility, learning efficiency,and convergence. These hurdles stem from the bottleneck in communication,characterized by sporadic and irregular connectivity between LEO satellites andground stations, coupled with the limited computation capability of satelliteedge computing (SEC). This paper proposes a novel FL-SEC framework thatempowers LEO satellites to execute large-scale machine learning (ML) tasksonboard efficiently. Its key components include i) personalized learning viadivide-and-conquer, which identifies and eliminates redundant satellite imagesand converts complex multi-class classification problems to simple binaryclassification, enabling rapid and energy-efficient training of lightweight MLmodels suitable for IoT/edge devices on satellites; ii) orbital modelretraining, which generates an aggregated "orbital model" per orbit andretrains it before sending to the ground station, significantly reducing therequired communication rounds. We conducted experiments using Jetson Nano, anedge device closely mimicking the limited compute on LEO satellites, and a realsatellite dataset. The results underscore the effectiveness of our approach,highlighting SEC's ability to run lightweight ML models on real andhigh-resolution satellite imagery. Our approach dramatically reduces FLconvergence time by nearly 30 times, and satellite energy consumption down toas low as 1.38 watts, all while maintaining an exceptional accuracy of up to96%.</description><author>Mohamed Elmahallawy, Tie Luo</author><pubDate>Mon, 08 Apr 2024 15:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15541v2</guid></item><item><title>SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v4</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, previous GCN-based methods rely onelaborate human priors excessively and construct complex feature aggregationmechanisms, which limits the generalizability and effectiveness of networks. Tosolve these problems, we propose a novel Spatial Topology Gating Unit (STGU),an MLP-based variant without extra priors, to capture the co-occurrencetopology features that encode the spatial dependency across all joints. InSTGU, to learn the point-wise topology features, a new gate-based featureinteraction mechanism is introduced to activate the features point-to-point bythe attention map generated from the input sample. Based on the STGU, wepropose the first MLP-based model, SiT-MLP, for skeleton-based actionrecognition in this work. Compared with previous methods on three large-scaledatasets, SiT-MLP achieves competitive performance. In addition, SiT-MLPreduces the parameters significantly with favorable results. The code will beavailable at https://github.com/BUPTSJZhang/SiT?MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Mon, 08 Apr 2024 15:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v4</guid></item><item><title>OPSD: an Offensive Persian Social media Dataset and its baseline evaluations</title><link>http://arxiv.org/abs/2404.05540v1</link><description>The proliferation of hate speech and offensive comments on social media hasbecome increasingly prevalent due to user activities. Such comments can havedetrimental effects on individuals' psychological well-being and socialbehavior. While numerous datasets in the English language exist in this domain,few equivalent resources are available for Persian language. To address thisgap, this paper introduces two offensive datasets. The first dataset comprisesannotations provided by domain experts, while the second consists of a largecollection of unlabeled data obtained through web crawling for unsupervisedlearning purposes. To ensure the quality of the former dataset, a meticulousthree-stage labeling process was conducted, and kappa measures were computed toassess inter-annotator agreement. Furthermore, experiments were performed onthe dataset using state-of-the-art language models, both with and withoutemploying masked language modeling techniques, as well as machine learningalgorithms, in order to establish the baselines for the dataset usingcontemporary cutting-edge approaches. The obtained F1-scores for thethree-class and two-class versions of the dataset were 76.9% and 89.9% forXLM-RoBERTa, respectively.</description><author>Mehran Safayani, Amir Sartipi, Amir Hossein Ahmadi, Parniyan Jalali, Amir Hossein Mansouri, Mohammad Bisheh-Niasar, Zahra Pourbahman</author><pubDate>Mon, 08 Apr 2024 15:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05540v1</guid></item><item><title>Cell-Free Multi-User MIMO Equalization via In-Context Learning</title><link>http://arxiv.org/abs/2404.05538v1</link><description>Large pre-trained sequence models, such as transformers, excel as few-shotlearners capable of in-context learning (ICL). In ICL, a model is trained toadapt its operation to a new task based on limited contextual information,typically in the form of a few training examples for the given task. Previouswork has explored the use of ICL for channel equalization in single-usermulti-input and multiple-output (MIMO) systems. In this work, we demonstratethat ICL can be also used to tackle the problem of multi-user equalization incell-free MIMO systems with limited fronthaul capacity. In this scenario, atask is defined by channel statistics, signal-to-noise ratio, and modulationschemes. The context encompasses the users' pilot sequences, the correspondingquantized received signals, and the current received data signal. Differentprompt design strategies are proposed and evaluated that encompass alsolarge-scale fading and modulation information. Experiments demonstrate thatICL-based equalization provides estimates with lower mean squared error ascompared to the linear minimum mean squared error equalizer, especially in thepresence of limited fronthaul capacity and pilot contamination.</description><author>Matteo Zecchin, Kai Zu, Osvaldo Simeone</author><pubDate>Mon, 08 Apr 2024 15:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05538v1</guid></item></channel></rss>