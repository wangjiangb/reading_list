<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 07 May 2023 17:43:46 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl</title><link>http://arxiv.org/abs/2305.01582v3</link><description>PySR is an open-source library for practical symbolic regression, a type ofmachine learning which aims to discover human-interpretable symbolic models.PySR was developed to democratize and popularize symbolic regression for thesciences, and is built on a high-performance distributed back-end, a flexiblesearch algorithm, and interfaces with several deep learning packages. PySR'sinternal search algorithm is a multi-population evolutionary algorithm, whichconsists of a unique evolve-simplify-optimize loop, designed for optimizationof unknown scalar constants in newly-discovered empirical expressions. PySR'sbackend is the extremely optimized Julia library SymbolicRegression.jl, whichcan be used directly from Julia. It is capable of fusing user-defined operatorsinto SIMD kernels at runtime, performing automatic differentiation, anddistributing populations of expressions to thousands of cores across a cluster.In describing this software, we also introduce a new benchmark,"EmpiricalBench," to quantify the applicability of symbolic regressionalgorithms in science. This benchmark measures recovery of historical empiricalequations from original and synthetic datasets.</description><author>Miles Cranmer</author><pubDate>Fri, 05 May 2023 18:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01582v3</guid></item><item><title>Cuttlefish: Low-Rank Model Training without All the Tuning</title><link>http://arxiv.org/abs/2305.02538v2</link><description>Recent research has shown that training low-rank neural networks caneffectively reduce the total number of trainable parameters without sacrificingpredictive accuracy, resulting in end-to-end speedups. However, low-rank modeltraining necessitates adjusting several additional factorizationhyperparameters, such as the rank of the factorization at each layer. In thispaper, we tackle this challenge by introducing Cuttlefish, an automatedlow-rank training approach that eliminates the need for tuning factorizationhyperparameters. Cuttlefish leverages the observation that after a few epochsof full-rank training, the stable rank (i.e., an approximation of the truerank) of each layer stabilizes at a constant value. Cuttlefish switches fromfull-rank to low-rank training once the stable ranks of all layers haveconverged, setting the dimension of each factorization to its correspondingstable rank. Our results show that Cuttlefish generates models up to 5.6 timessmaller than full-rank models, and attains up to a 1.2 times faster end-to-endtraining process while preserving comparable accuracy. Moreover, Cuttlefishoutperforms state-of-the-art low-rank model training methods and otherprominent baselines. The source code for our implementation can be found at:https://github.com/hwang595/Cuttlefish.</description><author>Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos</author><pubDate>Fri, 05 May 2023 17:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02538v2</guid></item><item><title>Leveraging gradient-derived metrics for data selection and valuation in differentially private training</title><link>http://arxiv.org/abs/2305.02942v2</link><description>Obtaining high-quality data for collaborative training of machine learningmodels can be a challenging task due to A) the regulatory concerns and B) lackof incentive to participate. The first issue can be addressed through the useof privacy enhancing technologies (PET), one of the most frequently used onebeing differentially private (DP) training. The second challenge can beaddressed by identifying which data points can be beneficial for model trainingand rewarding data owners for sharing this data. However, DP in deep learningtypically adversely affects atypical (often informative) data samples, makingit difficult to assess the usefulness of individual contributions. In this workwe investigate how to leverage gradient information to identify trainingsamples of interest in private training settings. We show that there existtechniques which are able to provide the clients with the tools for principleddata selection even in strictest privacy settings.</description><author>Dmitrii Usynin, Daniel Rueckert, Georgios Kaissis</author><pubDate>Fri, 05 May 2023 10:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02942v2</guid></item><item><title>Transforming Visual Scene Graphs to Image Captions</title><link>http://arxiv.org/abs/2305.02177v3</link><description>We propose to Transform Scene Graphs (TSG) into more descriptive captions. InTSG, we apply multi-head attention (MHA) to design the Graph Neural Network(GNN) for embedding scene graphs. After embedding, different graph embeddingscontain diverse specific knowledge for generating the words with differentpart-of-speech, e.g., object/attribute embedding is good for generatingnouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-baseddecoder, where each expert is built on MHA, for discriminating the graphembeddings to generate different kinds of words. Since both the encoder anddecoder are built based on the MHA, as a result, we construct a homogeneousencoder-decoder unlike the previous heterogeneous ones which usually applyFully-Connected-based GNN and LSTM-based decoder. The homogeneous architectureenables us to unify the training configuration of the whole model instead ofspecifying different training strategies for diverse sub-networks as in theheterogeneous pipeline, which releases the training difficulty. Extensiveexperiments on the MS-COCO captioning benchmark validate the effectiveness ofour TSG. The code is in: https://anonymous.4open.science/r/ACL23_TSG.</description><author>Xu Yang, Jiawei Peng, Zihua Wang, Haiyang Xu, Qinghao Ye, Chenliang Li, Ming Yan, Fei Huang, Zhangzikang Li, Yu Zhang</author><pubDate>Fri, 05 May 2023 06:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02177v3</guid></item><item><title>Hierarchical Transformer for Scalable Graph Learning</title><link>http://arxiv.org/abs/2305.02866v2</link><description>Graph Transformer is gaining increasing attention in the field of machinelearning and has demonstrated state-of-the-art performance on benchmarks forgraph representation learning. However, as current implementations of GraphTransformer primarily focus on learning representations of small-scale graphs,the quadratic complexity of the global self-attention mechanism presents achallenge for full-batch training when applied to larger graphs. Additionally,conventional sampling-based methods fail to capture necessary high-levelcontextual information, resulting in a significant loss of performance. In thispaper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as asolution to these challenges. HSGT successfully scales the Transformerarchitecture to node representation learning tasks on large-scale graphs, whilemaintaining high performance. By utilizing graph hierarchies constructedthrough coarsening techniques, HSGT efficiently updates and stores multi-scaleinformation in node embeddings at different levels. Together withsampling-based training methods, HSGT effectively captures and aggregatesmulti-level information on the hierarchical graph using only Transformerblocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-artperformance on large-scale benchmarks with graphs containing millions of nodeswith high efficiency.</description><author>Wenhao Zhu, Tianyu Wen, Guojie Song, Xiaojun Ma, Liang Wang</author><pubDate>Fri, 05 May 2023 06:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02866v2</guid></item><item><title>Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner</title><link>http://arxiv.org/abs/2305.02470v2</link><description>Most existing exoplanets are discovered using validation techniques ratherthan being confirmed by complementary observations. These techniques generate ascore that is typically the probability of the transit signal being anexoplanet (y(x)=exoplanet) given some information related to that signal(represented by x). Except for the validation technique in Rowe et al. (2014)that uses multiplicity information to generate these probability scores, theexisting validation techniques ignore the multiplicity boost information. Inthis work, we introduce a framework with the following premise: given anexisting transit signal vetter (classifier), improve its performance usingmultiplicity information. We apply this framework to several existingclassifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin etal. 2017), AstroNet (Shallue &amp; Vanderburg 2018), ExoNet (Ansdel et al. 2018),GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), tosupport our claim that this framework is able to improve the performance of agiven classifier. We then use the proposed multiplicity boost framework forExoMiner V1.2, which addresses some of the shortcomings of the originalExoMiner classifier (Valizadegan et al. 2022), and validate 69 new exoplanetsfor systems with multiple KOIs from the Kepler catalog.</description><author>Hamed Valizadegan, Miguel J. S. Martinho, Jon M. Jenkins, Douglas A. Caldwell, Joseph D. Twicken, Stephen T. Bryson</author><pubDate>Fri, 05 May 2023 05:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02470v2</guid></item><item><title>Scanpath Prediction in Panoramic Videos via Expected Code Length Minimization</title><link>http://arxiv.org/abs/2305.02536v2</link><description>Predicting human scanpaths when exploring panoramic videos is a challengingtask due to the spherical geometry and the multimodality of the input, and theinherent uncertainty and diversity of the output. Most previous methods fail togive a complete treatment of these characteristics, and thus are prone toerrors. In this paper, we present a simple new criterion for scanpathprediction based on principles from lossy data compression. This criterionsuggests minimizing the expected code length of quantized scanpaths in atraining set, which corresponds to fitting a discrete conditional probabilitymodel via maximum likelihood. Specifically, the probability model isconditioned on two modalities: a viewport sequence as the deformation-reducedvisual input and a set of relative historical scanpaths projected ontorespective viewports as the aligned path input. The probability model isparameterized by a product of discretized Gaussian mixture models to capturethe uncertainty and the diversity of scanpaths from different users. Mostimportantly, the training of the probability model does not rely on thespecification of "ground-truth" scanpaths for imitation learning. We alsointroduce a proportional-integral-derivative (PID) controller-based sampler togenerate realistic human-like scanpaths from the learned probability model.Experimental results demonstrate that our method consistently produces betterquantitative scanpath results in terms of prediction accuracy (by comparing tothe assumed "ground-truths") and perceptual realism (through machinediscrimination) over a wide range of prediction horizons. We additionallyverify the perceptual realism improvement via a formal psychophysicalexperiment and the generalization improvement on several unseen panoramic videodatasets.</description><author>Mu Li, Kanglong Fan, Kede Ma</author><pubDate>Fri, 05 May 2023 04:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02536v2</guid></item><item><title>Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results</title><link>http://arxiv.org/abs/2304.05753v3</link><description>Face anti-spoofing (FAS) is an essential mechanism for safeguarding theintegrity of automated face recognition systems. Despite substantialadvancements, the generalization of existing approaches to real-worldapplications remains challenging. This limitation can be attributed to thescarcity and lack of diversity in publicly available FAS datasets, which oftenleads to overfitting during training or saturation during testing. In terms ofquantity, the number of spoof subjects is a critical determinant. Most datasetscomprise fewer than 2,000 subjects. With regard to diversity, the majority ofdatasets consist of spoof samples collected in controlled environments usingrepetitive, mechanical processes. This data collection methodology results inhomogenized samples and a dearth of scenario diversity. To address theseshortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, alarge-scale, diverse FAS dataset collected in unconstrained settings. Ourdataset encompasses 853,729 images of 321,751 spoof subjects and 529,571 imagesof 148,169 live subjects, representing a substantial increase in quantity.Moreover, our dataset incorporates spoof data obtained from the internet,spanning a wide array of scenarios and various commercial sensors, including 17presentation attacks (PAs) that encompass both 2D and 3D forms. This novel datacollection strategy markedly enhances FAS data diversity. Leveraging the WFASdataset and Protocol 1 (Known-Type), we host the Wild Face Anti-SpoofingChallenge at the CVPR2023 workshop. Additionally, we meticulously evaluaterepresentative methods using Protocol 1 and Protocol 2 (Unknown-Type). Throughan in-depth examination of the challenge outcomes and benchmark baselines, weprovide insightful analyses and propose potential avenues for future research.The dataset is released under Insightface.</description><author>Dong Wang, Jia Guo, Qiqi Shao, Haochi He, Zhian Chen, Chuanbao Xiao, Ajian Liu, Sergio Escalera, Hugo Jair Escalante, Zhen Lei, Jun Wan, Jiankang Deng</author><pubDate>Fri, 05 May 2023 02:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05753v3</guid></item><item><title>Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge</title><link>http://arxiv.org/abs/2305.02459v2</link><description>While transformer-based systems have enabled greater accuracies with fewertraining examples, data acquisition obstacles still persist for rare-classtasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Activelearning has in general been proposed to alleviate such challenges, but choiceof selection strategy, the criteria by which rare-class examples are chosen,has not been systematically evaluated. Further, transformers enable iterativetransfer-learning approaches. We propose and investigate transfer- and activelearning solutions to the rare class problem of dissonance detection throughutilizing models trained on closely related tasks and the evaluation ofacquisition strategies, including a proposed probability-of-rare-class (PRC)approach. We perform these experiments for a specific rare class problem:collecting language samples of cognitive dissonance from social media. We findthat PRC is a simple and effective strategy to guide annotations and ultimatelyimprove model accuracy while transfer-learning in a specific order can improvethe cold-start performance of the learner but does not benefit iterations ofactive learning.</description><author>Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian Luhmann, H. Andrew Schwartz</author><pubDate>Fri, 05 May 2023 01:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02459v2</guid></item><item><title>ZipIt! Merging Models from Different Tasks without Training</title><link>http://arxiv.org/abs/2305.03053v1</link><description>Typical deep visual recognition models are capable of performing the one taskthey were trained on. In this paper, we tackle the extremely difficult problemof combining completely distinct models with different initializations, eachsolving a separate task, into one multi-task model without any additionaltraining. Prior work in model merging permutes one model to the space of theother then adds them together. While this works for models trained on the sametask, we find that this fails to account for the differences in models trainedon disjoint tasks. Thus, we introduce "ZipIt!", a general method for mergingtwo arbitrary models of the same architecture that incorporates two simplestrategies. First, in order to account for features that aren't shared betweenmodels, we expand the model merging problem to additionally allow for mergingfeatures within each model by defining a general "zip" operation. Second, weadd support for partially zipping the models up until a specified layer,naturally creating a multi-head model. We find that these two changes combinedaccount for a staggering 20-60% improvement over prior work, making the mergingof models trained on disjoint tasks feasible.</description><author>George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, Judy Hoffman</author><pubDate>Thu, 04 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03053v1</guid></item><item><title>Tracking through Containers and Occluders in the Wild</title><link>http://arxiv.org/abs/2305.03052v1</link><description>Tracking objects with persistence in cluttered and dynamic environmentsremains a difficult challenge for computer vision systems. In this paper, weintroduce $\textbf{TCOW}$, a new benchmark and model for visual trackingthrough heavy occlusion and containment. We set up a task where the goal is to,given a video sequence, segment both the projected extent of the target object,as well as the surrounding container or occluder whenever one exists. To studythis task, we create a mixture of synthetic and annotated real datasets tosupport both supervised learning and structured evaluation of model performanceunder various forms of task variation, such as moving or nested containment. Weevaluate two recent transformer-based video models and find that while they canbe surprisingly capable of tracking targets under certain settings of taskvariation, there remains a considerable performance gap before we can claim atracking model to have acquired a true notion of object permanence.</description><author>Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, Carl Vondrick</author><pubDate>Thu, 04 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03052v1</guid></item><item><title>Controllable Visual-Tactile Synthesis</title><link>http://arxiv.org/abs/2305.03051v1</link><description>Deep generative models have various content creation applications such asgraphic design, e-commerce, and virtual Try-on. However, current works mainlyfocus on synthesizing realistic visual outputs, often ignoring other sensorymodalities, such as touch, which limits physical interaction with users. Inthis work, we leverage deep generative models to create a multi-sensoryexperience where users can touch and see the synthesized object when slidingtheir fingers on a haptic surface. The main challenges lie in the significantscale discrepancy between vision and touch sensing and the lack of explicitmapping from touch sensing data to a haptic rendering device. To bridge thisgap, we collect high-resolution tactile data with a GelSight sensor and createa new visuotactile clothing dataset. We then develop a conditional generativemodel that synthesizes both visual and tactile outputs from a single sketch. Weevaluate our method regarding image quality and tactile rendering accuracy.Finally, we introduce a pipeline to render high-quality visual and tactileoutputs on an electroadhesion-based haptic device for an immersive experience,allowing for challenging materials and editable sketch inputs.</description><author>Ruihan Gao, Wenzhen Yuan, Jun-Yan Zhu</author><pubDate>Thu, 04 May 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03051v1</guid></item><item><title>NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds</title><link>http://arxiv.org/abs/2305.03049v1</link><description>This paper proposes NeuralEditor that enables neural radiance fields (NeRFs)natively editable for general shape editing tasks. Despite their impressiveresults on novel-view synthesis, it remains a fundamental challenge for NeRFsto edit the shape of the scene. Our key insight is to exploit the explicitpoint cloud representation as the underlying structure to construct NeRFs,inspired by the intuitive interpretation of NeRF rendering as a process thatprojects or "plots" the associated 3D point cloud to a 2D image plane. To thisend, NeuralEditor introduces a novel rendering scheme based on deterministicintegration within K-D tree-guided density-adaptive voxels, which produces bothhigh-quality rendering results and precise point clouds through optimization.NeuralEditor then performs shape editing via mapping associated points betweenpoint clouds. Extensive evaluation shows that NeuralEditor achievesstate-of-the-art performance in both shape deformation and scene morphingtasks. Notably, NeuralEditor supports both zero-shot inference and furtherfine-tuning over the edited scene. Our code, benchmark, and demo video areavailable at https://immortalco.github.io/NeuralEditor.</description><author>Jun-Kun Chen, Jipeng Lyu, Yu-Xiong Wang</author><pubDate>Thu, 04 May 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03049v1</guid></item><item><title>Personalize Segment Anything Model with One Shot</title><link>http://arxiv.org/abs/2305.03048v1</link><description>Driven by large-data pre-training, Segment Anything Model (SAM) has beendemonstrated as a powerful and promptable framework, revolutionizing thesegmentation models. Despite the generality, customizing SAM for specificvisual concepts without man-powered prompting is under explored, e.g.,automatically segmenting your pet dog in different images. In this paper, wepropose a training-free Personalization approach for SAM, termed as PerSAM.Given only a single image with a reference mask, PerSAM first localizes thetarget concept by a location prior, and segments it within other images orvideos via three techniques: target-guided attention, target-semanticprompting, and cascaded post-refinement. In this way, we effectively adapt SAMfor private use without any training. To further alleviate the mask ambiguity,we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing theentire SAM, we introduce two learnable weights for multi-scale masks, onlytraining 2 parameters within 10 seconds for improved performance. Todemonstrate our efficacy, we construct a new segmentation dataset, PerSeg, forpersonalized evaluation, and test our methods on video object segmentation withcompetitive performance. Besides, our approach can also enhance DreamBooth topersonalize Stable Diffusion for text-to-image generation, which discards thebackground disturbance for better target appearance learning. Code is releasedat https://github.com/ZrrSkywalker/Personalize-SAM</description><author>Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li</author><pubDate>Thu, 04 May 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03048v1</guid></item><item><title>Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision</title><link>http://arxiv.org/abs/2305.03047v1</link><description>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervisedfine-tuning (SFT) with human annotations and reinforcement learning from humanfeedback (RLHF) to align the output of large language models (LLMs) with humanintentions, ensuring they are helpful, ethical, and reliable. However, thisdependence can significantly constrain the true potential of AI-assistantagents due to the high cost of obtaining human supervision and the relatedissues on quality, reliability, diversity, self-consistency, and undesirablebiases. To address these challenges, we propose a novel approach calledSELF-ALIGN, which combines principle-driven reasoning and the generative powerof LLMs for the self-alignment of AI agents with minimal human supervision. Ourapproach encompasses four stages: first, we use an LLM to generate syntheticprompts, and a topic-guided method to augment the prompt diversity; second, weuse a small set of human-written principles for AI models to follow, and guidethe LLM through in-context learning from demonstrations (of principlesapplication) to produce helpful, ethical, and reliable responses to user'squeries; third, we fine-tune the original LLM with the high-qualityself-aligned responses so that the resulting model can generate desirableresponses for each query directly without the principle set and thedemonstrations anymore; and finally, we offer a refinement step to address theissues of overly-brief or indirect responses. Applying SELF-ALIGN to theLLaMA-65b base language model, we develop an AI assistant named Dromedary. Withfewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16generic principles, and 5 exemplars for in-context learning). Dromedarysignificantly surpasses the performance of several state-of-the-art AI systems,including Text-Davinci-003 and Alpaca, on benchmark datasets with varioussettings.</description><author>Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</author><pubDate>Thu, 04 May 2023 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03047v1</guid></item><item><title>OctFormer: Octree-based Transformers for 3D Point Clouds</title><link>http://arxiv.org/abs/2305.03045v1</link><description>OctFormer can not only serve as a general and effective backbone for 3D pointcloud segmentation and object detection but also have linear complexity and isscalable for large-scale point clouds. The key challenge in applyingtransformers to point clouds is reducing the quadratic, thus overwhelming,computation complexity of attentions. To combat this issue, several worksdivide point clouds into non-overlapping windows and constrain attentions ineach local window. However, the point number in each window varies greatly,impeding the efficient execution on GPU. Observing that attentions are robustto the shapes of local windows, we propose a novel octree attention, whichleverages sorted shuffled keys of octrees to partition point clouds into localwindows containing a fixed number of points while permitting shapes of windowsto change freely. And we also introduce dilated octree attention to expand thereceptive field further. Our octree attention can be implemented in 10 lines ofcode with open-sourced libraries and runs 17 times faster than other pointcloud attentions when the point number exceeds 200k. Built upon the octreeattention, OctFormer can be easily scaled up and achieves state-of-the-artperformances on a series of 3D segmentation and detection benchmarks,surpassing previous sparse-voxel-based CNNs and point cloud transformers interms of both efficiency and effectiveness. Notably, on the challengingScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 inmIoU. Our code and trained models are available athttps://wang-ps.github.io/octformer.</description><author>Peng-Shuai Wang</author><pubDate>Thu, 04 May 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03045v1</guid></item><item><title>Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization</title><link>http://arxiv.org/abs/2305.03043v1</link><description>There is a growing demand for the accessible creation of high-quality 3Davatars that are animatable and customizable. Although 3D morphable modelsprovide intuitive control for editing and animation, and robustness forsingle-view face reconstruction, they cannot easily capture geometric andappearance details. Methods based on neural implicit representations, such assigned distance functions (SDF) or neural radiance fields, approachphoto-realism, but are difficult to animate and do not generalize well tounseen data. To tackle this problem, we propose a novel method for constructingimplicit 3D morphable face models that are both generalizable and intuitive forediting. Trained from a collection of high-quality 3D scans, our face model isparameterized by geometry, expression, and texture latent codes with a learnedSDF and explicit UV texture parameterization. Once trained, we can reconstructan avatar from a single in-the-wild image by leveraging the learned prior toproject the image into the latent space of our model. Our implicit morphableface models can be used to render an avatar from novel views, animate facialexpressions by modifying expression codes, and edit textures by directlypainting on the learned UV-texture maps. We demonstrate quantitatively andqualitatively that our method improves upon photo-realism, geometry, andexpression accuracy compared to state-of-the-art methods.</description><author>Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis</author><pubDate>Thu, 04 May 2023 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03043v1</guid></item><item><title>Are VAEs Bad at Reconstructing Molecular Graphs?</title><link>http://arxiv.org/abs/2305.03041v1</link><description>Many contemporary generative models of molecules are variationalauto-encoders of molecular graphs. One term in their training loss pertains toreconstructing the input, yet reconstruction capabilities of state-of-the-artmodels have not yet been thoroughly compared on a large and chemically diversedataset. In this work, we show that when several state-of-the-art generativemodels are evaluated under the same conditions, their reconstruction accuracyis surprisingly low, worse than what was previously reported on seeminglyharder datasets. However, we show that improving reconstruction does notdirectly lead to better sampling or optimization performance. Failedreconstructions from the MoLeR model are usually similar to the inputs,assembling the same motifs in a different way, and possess similar chemicalproperties such as solubility. Finally, we show that the input molecule and itsfailed reconstruction are usually mapped by the different encoders tostatistically distinguishable posterior distributions, hinting that posteriorcollapse may not fully explain why VAEs are bad at reconstructing moleculargraphs.</description><author>Hagen Muenkler, Hubert Misztela, Michal Pikusa, Marwin Segler, Nadine Schneider, Krzysztof Maziarz</author><pubDate>Thu, 04 May 2023 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03041v1</guid></item><item><title>TUVF: Learning Generalizable Texture UV Radiance Fields</title><link>http://arxiv.org/abs/2305.03040v1</link><description>Textures are a vital aspect of creating visually appealing and realistic 3Dmodels. In this paper, we study the problem of generating high-fidelity texturegiven shapes of 3D assets, which has been relatively less explored comparedwith generic 3D shape modeling. Our goal is to facilitate a controllabletexture generation process, such that one texture code can correspond to aparticular appearance style independent of any input shapes from a category. Weintroduce Texture UV Radiance Fields (TUVF) that generate textures in alearnable UV sphere space rather than directly on the 3D shape. This allows thetexture to be disentangled from the underlying shape and transferable to othershapes that share the same UV space, i.e., from the same category. We integratethe UV sphere space with the radiance field, which provides a more efficientand accurate representation of textures than traditional texture maps. Weperform our experiments on real-world object datasets where we achieve not onlyrealistic synthesis but also substantial improvements over state-of-the-arts ontexture controlling and editing. Project Page: https://www.anjiecheng.me/TUVF</description><author>An-Chieh Cheng, Xueting Li, Sifei Liu, Xiaolong Wang</author><pubDate>Thu, 04 May 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03040v1</guid></item><item><title>SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks</title><link>http://arxiv.org/abs/2305.03039v1</link><description>Computational notebooks such as Jupyter Notebook have become data scientists'de facto programming environments. Many visualization researchers andpractitioners have developed interactive visualization tools that supportnotebooks. However, little is known about the appropriate design of visualanalytics (VA) tools in notebooks. To bridge this critical research gap, weinvestigate the design strategies in this space by analyzing 159 notebook VAtools and their users' feedback. Our analysis encompasses 62 systems fromacademic papers and 103 systems sourced from a pool of 55k notebooks containinginteractive visualizations that we obtain via scraping 8.6 million notebooks onGitHub. We also examine findings from 15 user studies and user feedback in 379GitHub issues. Through this work, we identify unique design opportunities andconsiderations for future notebook VA tools, such as using and manipulatingmultimodal data in notebooks as well as balancing the degree ofvisualization-notebook integration. Finally, we develop SuperNOVA, anopen-source interactive tool to help researchers explore existing notebook VAtools and search for related work.</description><author>Zijie J. Wang, David Munechika, Seongmin Lee, Duen Horng Chau</author><pubDate>Thu, 04 May 2023 18:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03039v1</guid></item><item><title>Learning Hand-Held Object Reconstruction from In-The-Wild Videos</title><link>http://arxiv.org/abs/2305.03036v1</link><description>Prior works for reconstructing hand-held objects from a single image rely ondirect 3D shape supervision which is challenging to gather in real world atscale. Consequently, these approaches do not generalize well when presentedwith novel objects in in-the-wild settings. While 3D supervision is a majorbottleneck, there is an abundance of in-the-wild raw video data showinghand-object interactions. In this paper, we automatically extract 3Dsupervision (via multiview 2D supervision) from such raw video data to scale upthe learning of models for hand-held object reconstruction. This requirestackling two key challenges: unknown camera pose and occlusion. For the former,we use hand pose (predicted from existing techniques, e.g. FrankMocap) as aproxy for object pose. For the latter, we learn data-driven 3D shape priorsusing synthetic objects from the ObMan dataset. We use these indirect 3D cuesto train occupancy networks that predict the 3D shape of objects from a singleRGB image. Our experiments on the MOW and HO3D datasets show the effectivenessof these supervisory signals at predicting the 3D shape for real-worldhand-held objects without any direct real-world 3D supervision.</description><author>Aditya Prakash, Matthew Chang, Matthew Jin, Saurabh Gupta</author><pubDate>Thu, 04 May 2023 18:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03036v1</guid></item><item><title>Contrastive Mean Teacher for Domain Adaptive Object Detectors</title><link>http://arxiv.org/abs/2305.03034v1</link><description>Object detectors often suffer from the domain gap between training (sourcedomain) and real-world applications (target domain). Mean-teacher self-trainingis a powerful paradigm in unsupervised domain adaptation for object detection,but it struggles with low-quality pseudo-labels. In this work, we identify theintriguing alignment and synergy between mean-teacher self-training andcontrastive learning. Motivated by this, we propose Contrastive Mean Teacher(CMT) -- a unified, general-purpose framework with the two paradigms naturallyintegrated to maximize beneficial learning signals. Instead of usingpseudo-labels solely for final predictions, our strategy extracts object-levelfeatures using pseudo-labels and optimizes them via contrastive learning,without requiring labels in the target domain. When combined with recentmean-teacher self-training methods, CMT leads to new state-of-the-arttarget-domain performance: 51.9% mAP on Foggy Cityscapes, outperforming thepreviously best by 2.1% mAP. Notably, CMT can stabilize performance and providemore significant gains as pseudo-label noise increases.</description><author>Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Thu, 04 May 2023 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03034v1</guid></item><item><title>What changes when you randomly choose BPE merge operations? Not much</title><link>http://arxiv.org/abs/2305.03029v1</link><description>We introduce three simple randomized variants of byte pair encoding (BPE) andexplore whether randomizing the selection of merge operations substantiallyaffects a downstream machine translation task. We focus on translation intomorphologically rich languages, hypothesizing that this task may showsensitivity to the method of choosing subwords. Analysis using a Bayesianlinear model indicates that two of the variants perform nearlyindistinguishably compared to standard BPE while the other degrades performanceless than we anticipated. We conclude that although standard BPE is widelyused, there exists an interesting universe of potential variations on it worthinvestigating. Our code is available at: https://github.com/bltlab/random-bpe.</description><author>Jonne Sälevä, Constantine Lignos</author><pubDate>Thu, 04 May 2023 18:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03029v1</guid></item><item><title>NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads</title><link>http://arxiv.org/abs/2305.03027v1</link><description>We focus on reconstructing high-fidelity radiance fields of human heads,capturing their animations over time, and synthesizing re-renderings from novelviewpoints at arbitrary time steps. To this end, we propose a new multi-viewcapture setup composed of 16 calibrated machine vision cameras that recordtime-synchronized images at 7.1 MP resolution and 73 frames per second. Withour setup, we collect a new dataset of over 4700 high-resolution,high-framerate sequences of more than 220 human heads, from which we introducea new human head reconstruction benchmark. The recorded sequences cover a widerange of facial dynamics, including head motions, natural expressions,emotions, and spoken language. In order to reconstruct high-fidelity humanheads, we propose Dynamic Neural Radiance Fields using Hash Ensembles(NeRSemble). We represent scene dynamics by combining a deformation field andan ensemble of 3D multi-resolution hash encodings. The deformation field allowsfor precise modeling of simple scene movements, while the ensemble of hashencodings helps to represent complex dynamics. As a result, we obtain radiancefield representations of human heads that capture motion over time andfacilitate re-rendering of arbitrary novel viewpoints. In a series ofexperiments, we explore the design choices of our method and demonstrate thatour approach outperforms state-of-the-art dynamic radiance field approaches bya significant margin.</description><author>Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nießner</author><pubDate>Thu, 04 May 2023 18:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03027v1</guid></item><item><title>Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models</title><link>http://arxiv.org/abs/2305.03025v1</link><description>This project focuses on enhancing open-source large language models throughinstruction-tuning and providing comprehensive evaluations of theirperformance. We explore how various training data factors, such as quantity,quality, and linguistic distribution, influence the performance ofinstruction-tuned models trained on publicly accessible high-qualityinstruction datasets for both English and Chinese languages. Our goal is tosupplement evaluation with quantitative analyses, providing valuable insightsfor the continued advancement of open-source chat models. Our model, data, andcode are publicly available for others to use and build upon.</description><author>Fangkai Jiao, Bosheng Ding, Tianze Luo, Zhanfeng Mo</author><pubDate>Thu, 04 May 2023 18:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03025v1</guid></item><item><title>Tutorial and Practice in Linear Programming: Optimization Problems in Supply Chain and Transport Logistics</title><link>http://arxiv.org/abs/2211.07345v2</link><description>This tutorial is an andragogical guide for students and practitioners seekingto understand the fundamentals and practice of linear programming. Theexercises demonstrate how to solve classical optimization problems with anemphasis on spatial analysis in supply chain management and transportlogistics. All exercises display the Python programs and optimization librariesused to solve them. The first chapter introduces key concepts in linearprogramming and contributes a new cognitive framework to help students andpractitioners set up each optimization problem. The cognitive frameworkorganizes the decision variables, constraints, the objective function, andvariable bounds in a format for direct application to optimization software.The second chapter introduces two types of mobility optimization problems(shortest path in a network and minimum cost tour) in the context of deliveryand service planning logistics. The third chapter introduces four types ofspatial optimization problems (neighborhood coverage, flow capturing, zoneheterogeneity, service coverage) and contributes a workflow to visualize theoptimized solutions in maps. The workflow creates decision variables from mapsby using the free geographic information systems (GIS) programs QGIS and GeoDA.The fourth chapter introduces three types of spatial logistical problems(spatial distribution, flow maximization, warehouse location optimization) anddemonstrates how to scale the cognitive framework in software to reachsolutions. The final chapter summarizes lessons learned and provides insightsabout how students and practitioners can modify the Phyton programs and GISworkflows to solve their own optimization problem and visualize the results.</description><author>Raj Bridgelall</author><pubDate>Thu, 04 May 2023 18:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07345v2</guid></item><item><title>Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study</title><link>http://arxiv.org/abs/2305.03017v1</link><description>The study of code example recommendation has been conducted extensively inthe past and recently in order to assist developers in their softwaredevelopment tasks. This is because developers often spend significant timesearching for relevant code examples on the internet, utilizing open-sourceprojects and informal documentation. For finding useful code examples, informaldocumentation, such as Stack Overflow discussions and forums, can beinvaluable. We have focused our research on Stack Overflow, which is a popularresource for discussing different topics among software developers. Forincreasing the quality of the recommended code examples, we have collected andrecommended the best code examples in the Java programming language. We haveutilized BERT in our approach, which is a Large Language Model (LLM) for textrepresentation that can effectively extract semantic information from textualdata. Our first step involved using BERT to convert code examples intonumerical vectors. Subsequently, we applied LSH to identify Approximate NearestNeighbors (ANN). Our research involved the implementation of two variants ofthis approach, namely the Random Hyperplane-based LSH and the Query-Aware LSH.Our study compared two algorithms using four parameters: HitRate, MeanReciprocal Rank (MRR), Average Execution Time, and Relevance. The results ofour analysis revealed that the Query- Aware (QA) approach outperformed theRandom Hyperplane-based (RH) approach in terms of HitRate. Specifically, the QAapproach achieved a HitRate improvement of 20% to 35% for query pairs comparedto the RH approach. Creating hashing tables and assigning data samples tobuckets using the QA approach is at least four times faster than the RHapproach. The QA approach returns code examples within milliseconds, while ittakes several seconds (sec) for the RH approach to recommend code examples.</description><author>Sajjad Rahmani, AmirHossein Naghshzan, Latifa Guerrouj</author><pubDate>Thu, 04 May 2023 18:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03017v1</guid></item><item><title>Sigmoid Loss for Language Image Pre-Training</title><link>http://arxiv.org/abs/2303.15343v3</link><description>We propose a simple pairwise sigmoid loss for image-text pre-training. Unlikestandard contrastive learning with softmax normalization, the sigmoid lossoperates solely on image-text pairs and does not require a global view of thepairwise similarities for normalization. The sigmoid loss simultaneously allowsfurther scaling up the batch size, while also performing better at smallerbatch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4kbatch size and a Large LiT model at 20k batch size, the latter achieves 84.5%ImageNet zero-shot accuracy in two days. This disentanglement of the batch sizefrom the loss further allows us to study the impact of examples vs pairs andnegative to positive ratio. Finally, we push the batch size to the extreme, upto one million, and find that the benefits of growing batch size quicklydiminish, with a more reasonable batch size of 32k being sufficient. We hopeour research motivates further explorations in improving the quality andefficiency of language-image pre-training.</description><author>Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer</author><pubDate>Thu, 04 May 2023 18:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15343v3</guid></item><item><title>Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence</title><link>http://arxiv.org/abs/2305.03010v1</link><description>Sentence-level representations are beneficial for various natural languageprocessing tasks. It is commonly believed that vector representations cancapture rich linguistic properties. Currently, large language models (LMs)achieve state-of-the-art performance on sentence embedding. However, somerecent works suggest that vector representations from LMs can cause informationleakage. In this work, we further investigate the information leakage issue andpropose a generative embedding inversion attack (GEIA) that aims to reconstructinput sequences based only on their sentence embeddings. Given the black-boxaccess to a language model, we treat sentence embeddings as initial tokens'representations and train or fine-tune a powerful decoder model to decode thewhole sequences directly. We conduct extensive experiments to demonstrate thatour generative inversion attack outperforms previous embedding inversionattacks in classification metrics and generates coherent and contextuallysimilar sentences as the original inputs.</description><author>Haoran Li, Mingshi Xu, Yangqiu Song</author><pubDate>Thu, 04 May 2023 18:31:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03010v1</guid></item><item><title>NatCS: Eliciting Natural Customer Support Dialogues</title><link>http://arxiv.org/abs/2305.03007v1</link><description>Despite growing interest in applications based on natural customer supportconversations, there exist remarkably few publicly available datasets thatreflect the expected characteristics of conversations in these settings.Existing task-oriented dialogue datasets, which were collected to benchmarkdialogue systems mainly in written human-to-bot settings, are notrepresentative of real customer support conversations and do not providerealistic benchmarks for systems that are applied to natural data. To addressthis gap, we introduce NatCS, a multi-domain collection of spoken customerservice conversations. We describe our process for collecting syntheticconversations between customers and agents based on natural language phenomenaobserved in real conversations. Compared to previous dialogue datasets, theconversations collected with our approach are more representative of realhuman-to-human conversations along multiple metrics. Finally, we demonstratepotential uses of NatCS, including dialogue act classification and intentinduction from conversations as potential applications, showing that dialogueact annotations in NatCS provide more effective training data for modeling realconversations compared to existing synthetic written datasets. We publiclyrelease NatCS to facilitate research in natural dialog systems</description><author>James Gung, Emily Moeng, Wesley Rose, Arshit Gupta, Yi Zhang, Saab Mansour</author><pubDate>Thu, 04 May 2023 18:25:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03007v1</guid></item><item><title>Evaluating Post-hoc Interpretability with Intrinsic Interpretability</title><link>http://arxiv.org/abs/2305.03002v1</link><description>Despite Convolutional Neural Networks having reached human-level performancein some medical tasks, their clinical use has been hindered by their lack ofinterpretability. Two major interpretability strategies have been proposed totackle this problem: post-hoc methods and intrinsic methods. Although there areseveral post-hoc methods to interpret DL models, there is significant variationbetween the explanations provided by each method, and it a difficult tovalidate them due to the lack of ground-truth. To address this challenge, weadapted the intrinsical interpretable ProtoPNet for the context ofhistopathology imaging and compared the attribution maps produced by it and thesaliency maps made by post-hoc methods. To evaluate the similarity betweensaliency map methods and attribution maps we adapted 10 saliency metrics fromthe saliency model literature, and used the breast cancer metastases detectiondataset PatchCamelyon with 327,680 patches of histopathological images ofsentinel lymph node sections to validate the proposed approach. Overall,SmoothGrad and Occlusion were found to have a statistically bigger overlap withProtoPNet while Deconvolution and Lime have been found to have the least.</description><author>José Pereira Amorim, Pedro Henriques Abreu, João Santos, Henning Müller</author><pubDate>Thu, 04 May 2023 18:20:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03002v1</guid></item><item><title>OSDaR23: Open Sensor Data for Rail 2023</title><link>http://arxiv.org/abs/2305.03001v1</link><description>For driverless train operation on mainline railways, several tasks need to beimplemented by technical systems. One of the most challenging tasks is tomonitor the train's driveway and its surroundings for potential obstacles dueto long braking distances. Machine learning algorithms can be used to analyzedata from vision sensors such as infrared (IR) and visual (RGB) cameras,lidars, and radars to detect objects. Such algorithms require large amounts ofannotated data from objects in the rail environment that may pose potentialobstacles, as well as rail-specific objects such as tracks or catenary poles,as training data. However, only very few datasets are publicly available andthese available datasets typically involve only a limited number of sensors.Datasets and trained models from other domains, such as automotive, are usefulbut insufficient for object detection in the railway context. Therefore, thispublication presents OSDaR23, a multi-sensor dataset of 21 sequences capturedin Hamburg, Germany, in September 2021. The sensor setup consisted of multiplecalibrated and synchronized IR/RGB cameras, lidars, a radar, and position andacceleration sensors front-mounted on a railway vehicle. In addition to rawdata, the dataset contains 204091 polyline, polygonal, rectangle and cuboidannotations for 20 different object classes. This dataset can also be used fortasks going beyond collision prediction, which are listed in this paper.</description><author>Rustam Tagiew, Martin Köppel, Karsten Schwalbe, Patrick Denzler, Philipp Neumaier, Tobias Klockau, Martin Boekhoff, Pavel Klasek, Roman Tilly</author><pubDate>Thu, 04 May 2023 18:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03001v1</guid></item><item><title>NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers</title><link>http://arxiv.org/abs/2304.09116v2</link><description>Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wilddatasets is important to capture the diversity in human speech such as speakeridentities, prosodies, and styles (e.g., singing). Current large TTS systemsusually quantize speech into discrete tokens and use language models togenerate these tokens one by one, which suffer from unstable prosody, wordskipping/repeating issue, and poor voice quality. In this paper, we developNaturalSpeech 2, a TTS system that leverages a neural audio codec with residualvector quantizers to get the quantized latent vectors and uses a diffusionmodel to generate these latent vectors conditioned on text input. To enhancethe zero-shot capability that is important to achieve diverse speech synthesis,we design a speech prompting mechanism to facilitate in-context learning in thediffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 tolarge-scale datasets with 44K hours of speech and singing data and evaluate itsvoice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTSsystems by a large margin in terms of prosody/timbre similarity, robustness,and voice quality in a zero-shot setting, and performs novel zero-shot singingsynthesis with only a speech prompt. Audio samples are available athttps://speechresearch.github.io/naturalspeech2.</description><author>Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, Jiang Bian</author><pubDate>Thu, 04 May 2023 18:08:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09116v2</guid></item><item><title>A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus</title><link>http://arxiv.org/abs/2305.02170v2</link><description>We present a pipeline for a statistical textual exploration, offering astylometry-based explanation and statistical validation of a hypothesizedpartition of a text. Given a parameterization of the text, our pipeline: (1)detects literary features yielding the optimal overlap between the hypothesizedand unsupervised partitions, (2) performs a hypothesis-testing analysis toquantify the statistical significance of the optimal overlap, while conservingimplicit correlations between units of text that are more likely to be grouped,and (3) extracts and quantifies the importance of features most responsible forthe classification, estimates their statistical stability and cluster-wiseabundance. We apply our pipeline to the first two books in the Bible, where onestylistic component stands out in the eyes of biblical scholars, namely, thePriestly component. We identify and explore statistically significant stylisticdifferences between the Priestly and non-Priestly components.</description><author>Gideon Yoffe, Axel Bühler, Nachum Dershowitz, Israel Finkelstein, Eli Piasetzky, Thomas Römer, Barak Sober</author><pubDate>Thu, 04 May 2023 18:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02170v2</guid></item><item><title>When Do Neural Nets Outperform Boosted Trees on Tabular Data?</title><link>http://arxiv.org/abs/2305.02997v1</link><description>Tabular data is one of the most commonly used types of data in machinelearning. Despite recent advances in neural nets (NNs) for tabular data, thereis still an active discussion on whether or not NNs generally outperformgradient-boosted decision trees (GBDTs) on tabular data, with several recentworks arguing either that GBDTs consistently outperform NNs on tabular data, orvice versa. In this work, we take a step back and ask, 'does it matter?' Weconduct the largest tabular data analysis to date, by comparing 19 algorithmsacross 176 datasets, and we find that the 'NN vs. GBDT' debate isoveremphasized: for a surprisingly high number of datasets, either theperformance difference between GBDTs and NNs is negligible, or lighthyperparameter tuning on a GBDT is more important than selecting the bestalgorithm. Next, we analyze 965 metafeatures to determine what properties of adataset make NNs or GBDTs better-suited to perform well. For example, we findthat GBDTs are much better than NNs at handling skewed feature distributions,heavy-tailed feature distributions, and other forms of dataset irregularities.Our insights act as a guide for practitioners to decide whether or not theyneed to run a neural net to reach top performance on their dataset. Ourcodebase and all raw results are available athttps://github.com/naszilla/tabzilla.</description><author>Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, Colin White</author><pubDate>Thu, 04 May 2023 18:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02997v1</guid></item><item><title>Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders</title><link>http://arxiv.org/abs/2305.02996v1</link><description>Cross-encoder models, which jointly encode and score a query-item pair, aretypically prohibitively expensive for k-nearest neighbor search. Consequently,k-NN search is performed not with a cross-encoder, but with a heuristicretrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent workproposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization toproduce an embedding space for efficient vector-based search that directlyapproximates the cross-encoder without the need for dual-encoders. ANNCURdefines this shared query-item embedding space by scoring the test queryagainst anchor items which are sampled uniformly at random. While thisminimizes average approximation error over all items, unsuitably highapproximation error on top-k items remains and leads to poor recall of top-k(and especially top-1) items. Increasing the number of anchor items is astraightforward way of improving the approximation error and hence k-NN recallof ANNCUR but at the cost of increased inference latency. In this paper, wepropose a new method for adaptively choosing anchor items that minimizes theapproximation error for the practically important top-k neighbors for a querywith minimal computational overhead. Our proposed method incrementally selectsa suitable set of anchor items for a given test query over several rounds,using anchors chosen in previous rounds to inform selection of more anchoritems. Empirically, our method consistently improves k-NN recall as compared toboth ANNCUR and the widely-used dual-encoder-based retrieve-and-rerankapproach.</description><author>Nishant Yadav, Nicholas Monath, Manzil Zaheer, Andrew McCallum</author><pubDate>Thu, 04 May 2023 18:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02996v1</guid></item><item><title>On the nonlinear correlation of ML performance between data subpopulations</title><link>http://arxiv.org/abs/2305.02995v1</link><description>Understanding the performance of machine learning (ML) models across diversedata distributions is critically important for reliable applications. Despiterecent empirical studies positing a near-perfect linear correlation betweenin-distribution (ID) and out-of-distribution (OOD) accuracies, we empiricallydemonstrate that this correlation is more nuanced under subpopulation shifts.Through rigorous experimentation and analysis across a variety of datasets,models, and training epochs, we demonstrate that OOD performance often has anonlinear correlation with ID performance in subpopulation shifts. Ourfindings, which contrast previous studies that have posited a linearcorrelation in model performance during distribution shifts, reveal a "moonshape" correlation (parabolic uptrend curve) between the test performance onthe majority subpopulation and the minority subpopulation. This non-trivialnonlinear correlation holds across model architectures, hyperparameters,training durations, and the imbalance between subpopulations. Furthermore, wefound that the nonlinearity of this "moon shape" is causally influenced by thedegree of spurious correlations in the training data. Our controlledexperiments show that stronger spurious correlation in the training datacreates more nonlinear performance correlation. We provide complementaryexperimental and theoretical analyses for this phenomenon, and discuss itsimplications for ML reliability and fairness. Our work highlights theimportance of understanding the nonlinear effects of model improvement onperformance in different subpopulations, and has the potential to inform thedevelopment of more equitable and responsible machine learning models.</description><author>Weixin Liang, Yining Mao, Yongchan Kwon, Xinyu Yang, James Zou</author><pubDate>Thu, 04 May 2023 18:00:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02995v1</guid></item><item><title>SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data</title><link>http://arxiv.org/abs/2305.02993v1</link><description>This paper describes the results of SemEval 2023 task 7 -- Multi-EvidenceNatural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2tasks, a Natural Language Inference (NLI) task, and an evidence selection taskon clinical trial data. The proposed challenges require multi-hop biomedicaland numerical reasoning, which are of significant importance to the developmentof systems capable of large-scale interpretation and retrieval of medicalevidence, to provide personalized evidence-based care. Task 1, the entailment task, received 643 submissions from 40 participants,and Task 2, the evidence selection task, received 364 submissions from 23participants. The tasks are challenging, with the majority of submitted systemsfailing to significantly outperform the majority class baseline on theentailment task, and we observe significantly better performance on theevidence selection task than on the entailment task. Increasing the number ofmodel parameters leads to a direct increase in performance, far moresignificant than the effect of biomedical pre-training. Future works couldexplore the limitations of large models for generalization and numericalinference, and investigate methods to augment clinical datasets to allow formore rigorous testing and to facilitate fine-tuning. We envisage that the dataset, models, and results of this task will be usefulto the biomedical NLI and evidence retrieval communities. The dataset,competition leaderboard, and website are publicly available.</description><author>Maël Jullien, Marco Valentino, Hannah Frost, Paul O'Regan, Donal Landers, André Freitas</author><pubDate>Thu, 04 May 2023 17:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02993v1</guid></item><item><title>Machine Learning Benchmarks for the Classification of Equivalent Circuit Models from Electrochemical Impedance Spectra</title><link>http://arxiv.org/abs/2302.03362v2</link><description>Analysis of Electrochemical Impedance Spectroscopy (EIS) data forelectrochemical systems often consists of defining an Equivalent Circuit Model(ECM) using expert knowledge and then optimizing the model parameters todeconvolute various resistance, capacitive, inductive, or diffusion responses.For small data sets, this procedure can be conducted manually; however, it isnot feasible to manually define a proper ECM for extensive data sets with awide range of EIS responses. Automatic identification of an ECM wouldsubstantially accelerate the analysis of large sets of EIS data. We showcasemachine learning methods to classify the ECMs of 9,300 impedance spectraprovided by QuantumScape for the BatteryDEV hackathon. The best-performingapproach is a gradient-boosted tree model utilizing a library to automaticallygenerate features, followed by a random forest model using the raw spectraldata. A convolutional neural network using boolean images of Nyquistrepresentations is presented as an alternative, although it achieves a loweraccuracy. We publish the data and open source the associated code. Theapproaches described in this article can serve as benchmarks for furtherstudies. A key remaining challenge is the identifiability of the labels,underlined by the model performances and the comparison of misclassifiedspectra.</description><author>Joachim Schaeffer, Paul Gasper, Esteban Garcia-Tamayo, Raymond Gasper, Masaki Adachi, Juan Pablo Gaviria-Cardona, Simon Montoya-Bedoya, Anoushka Bhutani, Andrew Schiek, Rhys Goodall, Rolf Findeisen, Richard D. Braatz, Simon Engelke</author><pubDate>Thu, 04 May 2023 17:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03362v2</guid></item><item><title>An Empirical Study of Remote Sensing Pretraining</title><link>http://arxiv.org/abs/2204.02825v4</link><description>Deep learning has largely reshaped remote sensing (RS) research for aerialimage understanding and made a great success. Nevertheless, most of theexisting deep models are initialized with the ImageNet pretrained weights.Since natural images inevitably present a large domain gap relative to aerialimages, probably limiting the finetuning performance on downstream aerial scenetasks. This issue motivates us to conduct an empirical study of remote sensingpretraining (RSP) on aerial images. To this end, we train different networksfrom scratch with the help of the largest RS scene recognition dataset up tonow -- MillionAID, to obtain a series of RS pretrained backbones, includingboth convolutional neural networks (CNN) and vision transformers such as Swinand ViTAE, which have shown promising performance on computer vision tasks.Then, we investigate the impact of RSP on representative downstream tasksincluding scene recognition, semantic segmentation, object detection, andchange detection using these CNN and vision transformer backbones. Empiricalstudy shows that RSP can help deliver distinctive performances in scenerecognition tasks and in perceiving RS related semantics such as "Bridge" and"Airplane". We also find that, although RSP mitigates the data discrepancies oftraditional ImageNet pretraining on RS images, it may still suffer from taskdiscrepancies, where downstream tasks require different representations fromscene recognition tasks. These findings call for further research efforts onboth large-scale pretraining datasets and effective pretraining methods. Thecodes and pretrained models will be released athttps://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing.</description><author>Di Wang, Jing Zhang, Bo Du, Gui-Song Xia, Dacheng Tao</author><pubDate>Thu, 04 May 2023 17:53:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.02825v4</guid></item><item><title>Adversarially-Guided Portrait Matting</title><link>http://arxiv.org/abs/2305.02981v1</link><description>We present a method for generating alpha mattes using a limited data source.We pretrain a novel transformerbased model (StyleMatte) on portrait datasets.We utilize this model to provide image-mask pairs for the StyleGAN3- basednetwork (StyleMatteGAN). This network is trained unsupervisedly and generatespreviously unseen imagemask training pairs that are fed back to StyleMatte. Wedemonstrate that the performance of the matte pulling network improves duringthis cycle and obtains top results on the used datasets. Furthermore,StyleMatteGAN provides high-resolution, privacy-preserving portraits with alphamattes, making it suitable for various image composition tasks. Our code isavailable at https://github.com/chroneus/stylematte</description><author>Sergej Chicherin, Karen Efremyan</author><pubDate>Thu, 04 May 2023 17:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02981v1</guid></item><item><title>Reasoning with Language Model Prompting: A Survey</title><link>http://arxiv.org/abs/2212.09597v2</link><description>Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updatedperiodically).</description><author>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen</author><pubDate>Thu, 04 May 2023 17:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09597v2</guid></item><item><title>Attention-Based Generative Neural Image Compression on Solar Dynamics Observatory</title><link>http://arxiv.org/abs/2210.06478v2</link><description>NASA's Solar Dynamics Observatory (SDO) mission gathers 1.4 terabytes of dataeach day from its geosynchronous orbit in space. SDO data includes images ofthe Sun captured at different wavelengths, with the primary scientific goal ofunderstanding the dynamic processes governing the Sun. Recently, end-to-endoptimized artificial neural networks (ANN) have shown great potential inperforming image compression. ANN-based compression schemes have outperformedconventional hand-engineered algorithms for lossy and lossless imagecompression. We have designed an ad-hoc ANN-based image compression scheme toreduce the amount of data needed to be stored and retrieved on space missionsstudying solar dynamics. In this work, we propose an attention module to makeuse of both local and non-local attention mechanisms in an adversariallytrained neural image compression network. We have also demonstrated thesuperior perceptual quality of this neural image compressor. Our proposedalgorithm for compressing images downloaded from the SDO spacecraft performsbetter in rate-distortion trade-off than the popular currently-in-use imagecompression codecs such as JPEG and JPEG2000. In addition we have shown thatthe proposed method outperforms state-of-the art lossy transform codingcompression codec, i.e., BPG.</description><author>Ali Zafari, Atefeh Khoshkhahtinat, Piyush M. Mehta, Nasser M. Nasrabadi, Barbara J. Thompson, Daniel da Silva, Michael S. F. Kirk</author><pubDate>Thu, 04 May 2023 17:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06478v2</guid></item><item><title>Masked Trajectory Models for Prediction, Representation, and Control</title><link>http://arxiv.org/abs/2305.02968v1</link><description>We introduce Masked Trajectory Models (MTM) as a generic abstraction forsequential decision making. MTM takes a trajectory, such as a state-actionsequence, and aims to reconstruct the trajectory conditioned on random subsetsof the same trajectory. By training with a highly randomized masking pattern,MTM learns versatile networks that can take on different roles or capabilities,by simply choosing appropriate masks at inference time. For example, the sameMTM network can be used as a forward dynamics model, inverse dynamics model, oreven an offline RL agent. Through extensive experiments in several continuouscontrol tasks, we show that the same MTM network -- i.e. same weights -- canmatch or outperform specialized networks trained for the aforementionedcapabilities. Additionally, we find that state representations learned by MTMcan significantly accelerate the learning speed of traditional RL algorithms.Finally, in offline RL benchmarks, we find that MTM is competitive withspecialized offline RL algorithms, despite MTM being a generic self-supervisedlearning method without any explicit RL components. Code is available athttps://github.com/facebookresearch/mtm</description><author>Philipp Wu, Arjun Majumdar, Kevin Stone, Yixin Lin, Igor Mordatch, Pieter Abbeel, Aravind Rajeswaran</author><pubDate>Thu, 04 May 2023 17:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02968v1</guid></item><item><title>ExeKGLib: Knowledge Graphs-Empowered Machine Learning Analytics</title><link>http://arxiv.org/abs/2305.02966v1</link><description>Many machine learning (ML) libraries are accessible online for MLpractitioners. Typical ML pipelines are complex and consist of a series ofsteps, each of them invoking several ML libraries. In this demo paper, wepresent ExeKGLib, a Python library that allows users with coding skills andminimal ML knowledge to build ML pipelines. ExeKGLib relies on knowledge graphsto improve the transparency and reusability of the built ML workflows, and toensure that they are executable. We demonstrate the usage of ExeKGLib andcompare it with conventional ML code to show its benefits.</description><author>Antonis Klironomos, Baifan Zhou, Zhipeng Tan, Zhuoxun Zheng, Gad-Elrab Mohamed, Heiko Paulheim, Evgeny Kharlamov</author><pubDate>Thu, 04 May 2023 17:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02966v1</guid></item><item><title>FUSegNet: A Deep Convolutional Neural Network for Foot Ulcer Segmentation</title><link>http://arxiv.org/abs/2305.02961v1</link><description>This paper presents FUSegNet, a new model for foot ulcer segmentation indiabetes patients, which uses the pre-trained EfficientNet-b7 as a backbone toaddress the issue of limited training samples. A modified spatial and channelsqueeze-and-excitation (scSE) module called parallel scSE or P-scSE is proposedthat combines additive and max-out scSE. A new arrangement is introduced forthe module by fusing it in the middle of each decoder stage. As the top decoderstage carries a limited number of feature maps, max-out scSE is bypassed thereto form a shorted P-scSE. A set of augmentations, comprising geometric,morphological, and intensity-based augmentations, is applied before feeding thedata into the network. The proposed model is first evaluated on a publiclyavailable chronic wound dataset where it achieves a data-based dice score of92.70%, which is the highest score among the reported approaches. The modeloutperforms other scSE-based UNet models in terms of Pratt's figure of merits(PFOM) scores in most categories, which evaluates the accuracy of edgelocalization. The model is then tested in the MICCAI 2021 FUSeg challenge,where a variation of FUSegNet called x-FUSegNet is submitted. The x-FUSegNetmodel, which takes the average of outputs obtained by FUSegNet using 5-foldcross-validation, achieves a dice score of 89.23%, placing it at the top of theFUSeg Challenge leaderboard. The source code for the model is available onhttps://github.com/mrinal054/FUSegNet.</description><author>Mrinal Kanti Dhar, Taiyu Zhang, Yash Patel, Zeyun Yu</author><pubDate>Thu, 04 May 2023 17:07:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02961v1</guid></item><item><title>Majorizing Measures, Codes, and Information</title><link>http://arxiv.org/abs/2305.02960v1</link><description>The majorizing measure theorem of Fernique and Talagrand is a fundamentalresult in the theory of random processes. It relates the boundedness of randomprocesses indexed by elements of a metric space to complexity measures arisingfrom certain multiscale combinatorial structures, such as packing and coveringtrees. This paper builds on the ideas first outlined in a little-noticedpreprint of Andreas Maurer to present an information-theoretic perspective onthe majorizing measure theorem, according to which the boundedness of randomprocesses is phrased in terms of the existence of efficient variable-lengthcodes for the elements of the indexing metric space.</description><author>Yifeng Chu, Maxim Raginsky</author><pubDate>Thu, 04 May 2023 17:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02960v1</guid></item><item><title>A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search</title><link>http://arxiv.org/abs/2107.08484v2</link><description>In this work, we propose a novel evolutionary algorithm for neuralarchitecture search, applicable to global search spaces. The algorithm'sarchitectural representation organizes the topology in multiple hierarchicalmodules, while the design process exploits this representation, in order toexplore the search space. We also employ a curation system, which promotes theutilization of well performing sub-structures to subsequent generations. Weapply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of$93.2\%$ and $94.8\%$ respectively in a relatively small number of generations.</description><author>Aristeidis Christoforidis, George Kyriakides, Konstantinos Margaritis</author><pubDate>Thu, 04 May 2023 17:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.08484v2</guid></item><item><title>FineEHR: Refine Clinical Note Representations to Improve Mortality Prediction</title><link>http://arxiv.org/abs/2304.11794v2</link><description>Monitoring the health status of patients in the Intensive Care Unit (ICU) isa critical aspect of providing superior care and treatment. The availability oflarge-scale electronic health records (EHR) provides machine learning modelswith an abundance of clinical text and vital sign data, enabling them to makehighly accurate predictions. Despite the emergence of advanced Natural LanguageProcessing (NLP) algorithms for clinical note analysis, the complex textualstructure and noise present in raw clinical data have posed significantchallenges. Coarse embedding approaches without domain-specific refinement havelimited the accuracy of these algorithms. To address this issue, we proposeFINEEHR, a system that utilizes two representation learning techniques, namelymetric learning and fine-tuning, to refine clinical note embeddings, whileleveraging the intrinsic correlations among different health statuses and notecategories. We evaluate the performance of FINEEHR using two metrics, namelyArea Under the Curve (AUC) and AUC-PR, on a real-world MIMIC III dataset. Ourexperimental results demonstrate that both refinement approaches improveprediction accuracy, and their combination yields the best results. Moreover,our proposed method outperforms prior works, with an AUC improvement of over10%, achieving an average AUC of 96.04% and an average AUC-PR of 96.48% acrossvarious classifiers.</description><author>Jun Wu, Xuesong Ye, Chengjie Mou, Weinan Dai</author><pubDate>Thu, 04 May 2023 17:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11794v2</guid></item><item><title>Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality</title><link>http://arxiv.org/abs/2305.02955v1</link><description>In recommender system or crowdsourcing applications of online learning, ahuman's preferences or abilities are often a function of the algorithm's recentactions. Motivated by this, a significant line of work has formalized settingswhere an action's loss is a function of the number of times that action wasrecently played in the prior $m$ timesteps, where $m$ corresponds to a bound onhuman memory capacity. To more faithfully capture decay of human memory withtime, we introduce the Weighted Tallying Bandit (WTB), which generalizes thissetting by requiring that an action's loss is a function of a \emph{weighted}summation of the number of times that arm was played in the last $m$ timesteps.This WTB setting is intractable without further assumption. So we study itunder Repeated Exposure Optimality (REO), a condition motivated by theliterature on human physiology, which requires the existence of an action thatwhen repetitively played will eventually yield smaller loss than any othersequence of actions. We study the minimization of the complete policy regret(CPR), which is the strongest notion of regret, in WTB under REO. Since $m$ istypically unknown, we assume we only have access to an upper bound $M$ on $m$.We show that for problems with $K$ actions and horizon $T$, a simplemodification of the successive elimination algorithm has $O \left( \sqrt{KT} +(m+M)K \right)$ CPR. Interestingly, upto an additive (in lieu ofmutliplicative) factor in $(m+M)K$, this recovers the classical guarantee forthe simpler stochastic multi-armed bandit with traditional regret. Weadditionally show that in our setting, any algorithm will suffer additive CPRof $\Omega \left( mK + M \right)$, demonstrating our result is nearly optimal.Our algorithm is computationally efficient, and we experimentally demonstrateits practicality and superiority over natural baselines.</description><author>Dhruv Malik, Conor Igoe, Yuanzhi Li, Aarti Singh</author><pubDate>Thu, 04 May 2023 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02955v1</guid></item><item><title>Rethinking Population-assisted Off-policy Reinforcement Learning</title><link>http://arxiv.org/abs/2305.02949v1</link><description>While off-policy reinforcement learning (RL) algorithms are sample efficientdue to gradient-based updates and data reuse in the replay buffer, theystruggle with convergence to local optima due to limited exploration. On theother hand, population-based algorithms offer a natural exploration strategy,but their heuristic black-box operators are inefficient. Recent algorithms haveintegrated these two methods, connecting them through a shared replay buffer.However, the effect of using diverse data from population optimizationiterations on off-policy RL algorithms has not been thoroughly investigated. Inthis paper, we first analyze the use of off-policy RL algorithms in combinationwith population-based algorithms, showing that the use of population data couldintroduce an overlooked error and harm performance. To test this, we propose auniform and scalable training design and conduct experiments on our tailoredframework in robot locomotion tasks from the OpenAI gym. Our resultssubstantiate that using population data in off-policy RL can cause instabilityduring training and even degrade performance. To remedy this issue, we furtherpropose a double replay buffer design that provides more on-policy data andshow its effectiveness through experiments. Our results offer practicalinsights for training these hybrid methods.</description><author>Bowen Zheng, Ran Cheng</author><pubDate>Thu, 04 May 2023 16:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02949v1</guid></item><item><title>SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers</title><link>http://arxiv.org/abs/2212.10325v3</link><description>Diffusion model, a new generative modelling paradigm, has achieved greatsuccess in image, audio, and video generation. However, considering thediscrete categorical nature of text, it is not trivial to extend continuousdiffusion models to natural language, and text diffusion models are lessstudied. Sequence-to-sequence text generation is one of the essential naturallanguage processing topics. In this work, we apply diffusion models to approachsequence-to-sequence text generation, and explore whether the superioritygeneration performance of diffusion model can transfer to natural languagedomain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequencegeneration. SeqDiffuSeq uses an encoder-decoder Transformers architecture tomodel denoising function. In order to improve generation quality, SeqDiffuSeqcombines the self-conditioning technique and a newly proposed adaptive noiseschedule technique. The adaptive noise schedule has the difficulty of denoisingevenly distributed across time steps, and considers exclusive noise schedulesfor tokens at different positional order. Experiment results illustrate thegood performance on sequence-to-sequence generation in terms of text qualityand inference time.</description><author>Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang</author><pubDate>Thu, 04 May 2023 16:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10325v3</guid></item><item><title>ECOLA: Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations</title><link>http://arxiv.org/abs/2203.09590v5</link><description>Since conventional knowledge embedding models cannot take full advantage ofthe abundant textual information, there have been extensive research efforts inenhancing knowledge embedding using texts. However, existing enhancementapproaches cannot apply to temporal knowledge graphs (tKGs), which containtime-dependent event knowledge with complex temporal dynamics. Specifically,existing enhancement approaches often assume knowledge embedding istime-independent. In contrast, the entity embedding in tKG models usuallyevolves, which poses the challenge of aligning temporally relevant texts withentities. To this end, we propose to study enhancing temporal knowledgeembedding with textual data in this paper. As an approach to this task, wepropose Enhanced Temporal Knowledge Embeddings with Contextualized LanguageRepresentations (ECOLA), which takes the temporal aspect into account andinjects textual information into temporal knowledge embedding. To evaluateECOLA, we introduce three new datasets for training and evaluating ECOLA.Extensive experiments show that ECOLA significantly enhances temporal KGembedding models with up to 287% relative improvements regarding Hits@1 on thelink prediction task. The code and models are publicly available onhttps://anonymous.4open.science/r/ECOLA.</description><author>Zhen Han, Ruotong Liao, Jindong Gu, Yao Zhang, Zifeng Ding, Yujia Gu, Heinz Köppl, Hinrich Schütze, Volker Tresp</author><pubDate>Thu, 04 May 2023 16:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.09590v5</guid></item><item><title>Leveraging gradient-derived metrics for data selection and valuation in differentially private training</title><link>http://arxiv.org/abs/2305.02942v1</link><description>Obtaining high-quality data for collaborative training of machine learningmodels can be a challenging task due to A) the regulatory concerns and B) lackof incentive to participate. The first issue can be addressed through the useof privacy enhancing technologies (PET), one of the most frequently used onebeing differentially private (DP) training. The second challenge can beaddressed by identifying which data points can be beneficial for model trainingand rewarding data owners for sharing this data. However, DP in deep learningtypically adversely affects atypical (often informative) data samples, makingit difficult to assess the usefulness of individual contributions. In this workwe investigate how to leverage gradient information to identify trainingsamples of interest in private training settings. We show that there existtechniques which are able to provide the clients with the tools for principleddata selection even in strictest privacy settings.</description><author>Dmitrii Usynin, Daniel Rueckert, Giorgios Kaissis</author><pubDate>Thu, 04 May 2023 16:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02942v1</guid></item><item><title>End-to-end spoken language understanding using joint CTC loss and self-supervised, pretrained acoustic encoders</title><link>http://arxiv.org/abs/2305.02937v1</link><description>It is challenging to extract semantic meanings directly from audio signals inspoken language understanding (SLU), due to the lack of textual information.Popular end-to-end (E2E) SLU models utilize sequence-to-sequence automaticspeech recognition (ASR) models to extract textual embeddings as input to infersemantics, which, however, require computationally expensive auto-regressivedecoding. In this work, we leverage self-supervised acoustic encodersfine-tuned with Connectionist Temporal Classification (CTC) to extract textualembeddings and use joint CTC and SLU losses for utterance-level SLU tasks.Experiments show that our model achieves 4% absolute improvement over the thestate-of-the-art (SOTA) dialogue act classification model on the DSTC2 datasetand 1.3% absolute improvement over the SOTA SLU model on the SLURP dataset.</description><author>Jixuan Wang, Martin Radfar, Kai Wei, Clement Chung</author><pubDate>Thu, 04 May 2023 16:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02937v1</guid></item><item><title>Image Captioners Sometimes Tell More Than Images They See</title><link>http://arxiv.org/abs/2305.02932v1</link><description>Image captioning, a.k.a. "image-to-text," which generates descriptive textfrom given images, has been rapidly developing throughout the era of deeplearning. To what extent is the information in the original image preserved inthe descriptive text generated by an image captioner? To answer that question,we have performed experiments involving the classification of images fromdescriptive text alone, without referring to the images at all, and comparedresults with those from standard image-based classifiers. We have evaluateseveral image captioning models with respect to a disaster image classificationtask, CrisisNLP, and show that descriptive text classifiers can sometimesachieve higher accuracy than standard image-based classifiers. Further, we showthat fusing an image-based classifier with a descriptive text classifier canprovide improvement in accuracy.</description><author>Honori Udo, Takafumi Koshinaka</author><pubDate>Thu, 04 May 2023 16:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02932v1</guid></item><item><title>Piecewise Normalizing Flows</title><link>http://arxiv.org/abs/2305.02930v1</link><description>Normalizing flows are an established approach for modelling complexprobability densities through invertible transformations from a basedistribution. However, the accuracy with which the target distribution can becaptured by the normalizing flow is strongly influenced by the topology of thebase distribution. A mismatch between the topology of the target and the basecan result in a poor performance, as is the case for multi-modal problems. Anumber of different works have attempted to modify the topology of the basedistribution to better match the target, either through the use of GaussianMixture Models [Izmailov et al., 2020, Ardizzone et al., 2020, Hagemann andNeumayer, 2021] or learned accept/reject sampling [Stimper et al., 2022]. Weintroduce piecewise normalizing flows which divide the target distribution intoclusters, with topologies that better match the standard normal basedistribution, and train a series of flows to model complex multi-modal targets.The piecewise nature of the flows can be exploited to significantly reduce thecomputational cost of training through parallelization. We demonstrate theperformance of the piecewise flows using standard benchmarks and compare theaccuracy of the flows to the approach taken in Stimper et al., 2022 formodelling multi-modal distributions.</description><author>Harry Bevins, Will Handley</author><pubDate>Thu, 04 May 2023 16:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02930v1</guid></item><item><title>Forward-Forward Contrastive Learning</title><link>http://arxiv.org/abs/2305.02927v1</link><description>Medical image classification is one of the most important tasks forcomputer-aided diagnosis. Deep learning models, particularly convolutionalneural networks, have been successfully used for disease classification frommedical images, facilitated by automated feature learning. However, the diverseimaging modalities and clinical pathology make it challenging to constructgeneralized and robust classifications. Towards improving the modelperformance, we propose a novel pretraining approach, namely Forward ForwardContrastive Learning (FFCL), which leverages the Forward-Forward Algorithm in acontrastive learning framework--both locally and globally. Our experimentalresults on the chest X-ray dataset indicate that the proposed FFCL achievessuperior performance (3.69% accuracy over ImageNet pretrained ResNet-18) overexisting pretraining models in the pneumonia classification task. Moreover,extensive ablation experiments support the particular local and globalcontrastive pretraining design in FFCL.</description><author>Md. Atik Ahamed, Jin Chen, Abdullah-Al-Zubaer Imran</author><pubDate>Thu, 04 May 2023 16:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02927v1</guid></item><item><title>Interpreting Vision and Language Generative Models with Semantic Visual Priors</title><link>http://arxiv.org/abs/2304.14986v2</link><description>When applied to Image-to-text models, interpretability methods often providetoken-by-token explanations namely, they compute a visual explanation for eachtoken of the generated sequence. Those explanations are expensive to computeand unable to comprehensively explain the model's output. Therefore, thesemodels often require some sort of approximation that eventually leads tomisleading explanations. We develop a framework based on SHAP, that allows forgenerating comprehensive, meaningful explanations leveraging the meaningrepresentation of the output sequence as a whole. Moreover, by exploitingsemantic priors in the visual backbone, we extract an arbitrary number offeatures that allows the efficient computation of Shapley values on large-scalemodels, generating at the same time highly meaningful visual explanations. Wedemonstrate that our method generates semantically more expressive explanationsthan traditional methods at a lower compute cost and that it can be generalizedover other explainability methods.</description><author>Michele Cafagna, Lina M. Rojas-Barahona, Kees van Deemter, Albert Gatt</author><pubDate>Thu, 04 May 2023 16:25:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14986v2</guid></item><item><title>Vertex Nomination in Richly Attributed Networks</title><link>http://arxiv.org/abs/2005.02151v3</link><description>Vertex nomination is a lightly-supervised network information retrieval taskin which vertices of interest in one graph are used to query a second graph todiscover vertices of interest in the second graph. Similar to other informationretrieval tasks, the output of a vertex nomination scheme is a ranked list ofthe vertices in the second graph, with the heretofore unknown vertices ofinterest ideally concentrating at the top of the list. Vertex nominationschemes provide a useful suite of tools for efficiently mining complex networksfor pertinent information. In this paper, we explore, both theoretically andpractically, the dual roles of content (i.e., edge and vertex attributes) andcontext (i.e., network topology) in vertex nomination. We provide necessary andsufficient conditions under which vertex nomination schemes that leverage bothcontent and context outperform schemes that leverage only content or contextseparately. While the joint utility of both content and context has beendemonstrated empirically in the literature, the framework presented in thispaper provides a novel theoretical basis for understanding the potentialcomplementary roles of network features and topology.</description><author>Keith Levin, Carey E. Priebe, Vince Lyzinski</author><pubDate>Thu, 04 May 2023 16:21:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.02151v3</guid></item><item><title>Recent Advances in the Foundations and Applications of Unbiased Learning to Rank</title><link>http://arxiv.org/abs/2305.02914v1</link><description>Since its inception, the field of unbiased learning to rank (ULTR) hasremained very active and has seen several impactful advancements in recentyears. This tutorial provides both an introduction to the core concepts of thefield and an overview of recent advancements in its foundations along withseveral applications of its methods. The tutorial is divided into four parts:Firstly, we give an overview of the different forms of bias that can beaddressed with ULTR methods. Secondly, we present a comprehensive discussion ofthe latest estimation techniques in the ULTR field. Thirdly, we surveypublished results of ULTR in real-world applications. Fourthly, we discuss theconnection between ULTR and fairness in ranking. We end by briefly reflectingon the future of ULTR research and its applications. This tutorial is intendedto benefit both researchers and industry practitioners who are interested indeveloping new ULTR solutions or utilizing them in real-world applications.</description><author>Shashank Gupta, Philipp Hager, Jin Huang, Ali Vardasbi, Harrie Oosterhuis</author><pubDate>Thu, 04 May 2023 16:19:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02914v1</guid></item><item><title>UPDExplainer: an Interpretable Transformer-based Framework for Urban Physical Disorder Detection Using Street View Imagery</title><link>http://arxiv.org/abs/2305.02911v1</link><description>Urban Physical Disorder (UPD), such as old or abandoned buildings, brokensidewalks, litter, and graffiti, has a negative impact on residents' quality oflife. They can also increase crime rates, cause social disorder, and pose apublic health risk. Currently, there is a lack of efficient and reliablemethods for detecting and understanding UPD. To bridge this gap, we proposeUPDExplainer, an interpretable transformer-based framework for UPD detection.We first develop a UPD detection model based on the Swin Transformerarchitecture, which leverages readily accessible street view images to learndiscriminative representations. In order to provide clear and comprehensibleevidence and analysis, we subsequently introduce a UPD factor identificationand ranking module that combines visual explanation maps with semanticsegmentation maps. This novel integrated approach enables us to identify theexact objects within street view images that are responsible for physicaldisorders and gain insights into the underlying causes. Experimental results onthe re-annotated Place Pulse 2.0 dataset demonstrate promising detectionperformance of the proposed method, with an accuracy of 79.9%. For acomprehensive evaluation of the method's ranking performance, we report themean Average Precision (mAP), R-Precision (RPrec), and Normalized DiscountedCumulative Gain (NDCG), with success rates of 75.51%, 80.61%, and 82.58%,respectively. We also present a case study of detecting and ranking physicaldisorders in the southern region of downtown Los Angeles, California, todemonstrate the practicality and effectiveness of our framework.</description><author>Chuanbo Hu, Shan Jia, Fan Zhang, Changjiang Xiao, Mindi Ruan, Jacob Thrasher, Xin Li</author><pubDate>Thu, 04 May 2023 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02911v1</guid></item><item><title>Aligning Bird-Eye View Representation of Point Cloud Sequences using Scene Flow</title><link>http://arxiv.org/abs/2305.02909v1</link><description>Low-resolution point clouds are challenging for object detection methods dueto their sparsity. Densifying the present point cloud by concatenating it withits predecessors is a popular solution to this challenge. Such concatenation ispossible thanks to the removal of ego vehicle motion using its odometry. Thismethod is called Ego Motion Compensation (EMC). Thanks to the added points, EMCsignificantly improves the performance of single-frame detectors. However, itsuffers from the shadow effect that manifests in dynamic objects' pointsscattering along their trajectories. This effect results in a misalignmentbetween feature maps and objects' locations, thus limiting performanceimprovement to stationary and slow-moving objects only. Scene flow allowsaligning point clouds in 3D space, thus naturally resolving the misalignment infeature spaces. By observing that scene flow computation shares severalcomponents with 3D object detection pipelines, we develop a plug-in module thatenables single-frame detectors to compute scene flow to rectify their Bird-EyeView representation. Experiments on the NuScenes dataset show that our moduleleads to a significant increase (up to 16%) in the Average Precision of largevehicles, which interestingly demonstrates the most severe shadow effect. Thecode is published at https://github.com/quan-dao/pc-corrector.</description><author>Minh-Quan Dao, Vincent Frémont, Elwan Héry</author><pubDate>Thu, 04 May 2023 16:16:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02909v1</guid></item><item><title>Single Node Injection Label Specificity Attack on Graph Neural Networks via Reinforcement Learning</title><link>http://arxiv.org/abs/2305.02901v1</link><description>Graph neural networks (GNNs) have achieved remarkable success in variousreal-world applications. However, recent studies highlight the vulnerability ofGNNs to malicious perturbations. Previous adversaries primarily focus on graphmodifications or node injections to existing graphs, yielding promising resultsbut with notable limitations. Graph modification attack~(GMA) requiresmanipulation of the original graph, which is often impractical, while graphinjection attack~(GIA) necessitates training a surrogate model in the black-boxsetting, leading to significant performance degradation due to divergencebetween the surrogate architecture and the actual victim model. Furthermore,most methods concentrate on a single attack goal and lack a generalizableadversary to develop distinct attack strategies for diverse goals, thuslimiting precise control over victim model behavior in real-world scenarios. Toaddress these issues, we present a gradient-free generalizable adversary thatinjects a single malicious node to manipulate the classification result of atarget node in the black-box evasion setting. We propose Gradient-freeGeneralizable Single Node Injection Attack, namely G$^2$-SNIA, a reinforcementlearning framework employing Proximal Policy Optimization. By directly queryingthe victim model, G$^2$-SNIA learns patterns from exploration to achievediverse attack goals with extremely limited attack budgets. Throughcomprehensive experiments over three acknowledged benchmark datasets and fourprominent GNNs in the most challenging and realistic scenario, we demonstratethe superior performance of our proposed G$^2$-SNIA over the existingstate-of-the-art baselines. Moreover, by comparing G$^2$-SNIA with multiplewhite-box evasion baselines, we confirm its capacity to generate solutionscomparable to those of the best adversaries.</description><author>Dayuan Chen, Jian Zhang, Yuqian Lv, Jinhuan Wang, Hongjie Ni, Shanqing Yu, Zhen Wang, Qi Xuan</author><pubDate>Thu, 04 May 2023 16:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02901v1</guid></item><item><title>Additive Class Distinction Maps using Branched-GANs</title><link>http://arxiv.org/abs/2305.02899v1</link><description>We present a new model, training procedure and architecture to create precisemaps of distinction between two classes of images. The objective is tocomprehend, in pixel-wise resolution, the unique characteristics of a class.These maps can facilitate self-supervised segmentation and objectdetection inaddition to new capabilities in explainable AI (XAI). Our proposed architectureis based on image decomposition, where the output is the sum of multiplegenerative networks (branched-GANs). The distinction between classes isisolated in a dedicated branch. This approach allows clear, precise andinterpretable visualization of the unique characteristics of each class. Weshow how our generic method can be used in several modalities for varioustasks, such as MRI brain tumor extraction, isolating cars in aerial photographyand obtaining feminine and masculine face features. This is a preliminaryreport of our initial findings and results.</description><author>Elnatan Kadar, Jonathan Brokman, Guy Gilboa</author><pubDate>Thu, 04 May 2023 16:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02899v1</guid></item><item><title>Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning</title><link>http://arxiv.org/abs/2301.11916v2</link><description>In recent years, pre-trained large language models have demonstratedremarkable efficiency in achieving an inference-time few-shot learningcapability known as in-context learning. However, existing literature hashighlighted the sensitivity of this capability to the selection of few-shotdemonstrations. The underlying mechanisms by which this capability arises fromregular language model pretraining objectives remain poorly understood. In thisstudy, we aim to examine the in-context learning phenomenon through a Bayesianlens, viewing large language models as topic models that implicitly infertask-related information from demonstrations. On this premise, we propose analgorithm for selecting optimal demonstrations from a set of annotated data anddemonstrate a significant 12.5% improvement relative to the random selectionbaseline, averaged over eight GPT2 and GPT3 models on eight differentreal-world text classification datasets. Our empirical findings support ourhypothesis that large language models implicitly infer a latent conceptvariable.</description><author>Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang</author><pubDate>Thu, 04 May 2023 16:09:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11916v2</guid></item><item><title>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</title><link>http://arxiv.org/abs/2305.02897v1</link><description>Emergent chain-of-thought (CoT) reasoning capabilities promise to improveperformance and explainability of large language models (LLMs). However,uncertainties remain about how prompting strategies formulated for previousmodel generations generalize to new model generations and different datasets.In this small-scale study we compare the performance of a range of zero-shotprompts for inducing CoT reasoning across six recently released LLMs(davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Coherecommand-xlarge) on a mixture of six question-answering datasets, includingdatasets from scientific and medical domains. We find that a CoT prompt thatwas previously discovered through automated prompt discovery shows robustperformance across experimental conditions and produces best results whenapplied to the state-of-the-art model GPT-4.</description><author>Konstantin Hebenstreit, Robert Praas, Louis P Kiesewetter, Matthias Samwald</author><pubDate>Thu, 04 May 2023 16:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02897v1</guid></item><item><title>Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions</title><link>http://arxiv.org/abs/2004.08318v5</link><description>We study causal inference under case-control and case-population sampling.Specifically, we focus on the binary-outcome and binary-treatment case, wherethe parameters of interest are causal relative and attributable risks definedvia the potential outcome framework. It is shown that strong ignorability isnot always as powerful as it is under random sampling and that certainmonotonicity assumptions yield comparable results in terms of sharp identifiedintervals. Specifically, the usual odds ratio is shown to be a sharp identifiedupper bound on causal relative risk under the monotone treatment response andmonotone treatment selection assumptions. We offer algorithms for inference onthe causal parameters that are aggregated over the true population distributionof the covariates. We show the usefulness of our approach by studying threeempirical examples: the benefit of attending private school for entering aprestigious university in Pakistan; the relationship between staying in schooland getting involved with drug-trafficking gangs in Brazil; and the linkbetween physicians' hours and size of the group practice in the United States.</description><author>Sung Jae Jun, Sokbae Lee</author><pubDate>Thu, 04 May 2023 16:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2004.08318v5</guid></item><item><title>FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization</title><link>http://arxiv.org/abs/2305.02894v1</link><description>Federated learning is an important framework in modern machine learning thatseeks to integrate the training of learning models from multiple users, eachuser having their own local data set, in a way that is sensitive to dataprivacy and to communication loss constraints. In clustered federated learning,one assumes an additional unknown group structure among users, and the goal isto train models that are useful for each group, rather than simply training asingle global model for all users. In this paper, we propose a novel solutionto the problem of clustered federated learning that is inspired by ideas inconsensus-based optimization (CBO). Our new CBO-type method is based on asystem of interacting particles that is oblivious to group memberships. Ourmodel is motivated by rigorous mathematical reasoning, including a mean fieldanalysis describing the large number of particles limit of our particle system,as well as convergence guarantees for the simultaneous global optimization ofgeneral non-convex objective functions (corresponding to the loss functions ofeach cluster of users) in the mean-field regime. Experimental resultsdemonstrate the efficacy of our FedCBO algorithm compared to otherstate-of-the-art methods and help validate our methodological and theoreticalwork.</description><author>Jose A. Carrillo, Nicolas Garcia Trillos, Sixu Li, Yuhua Zhu</author><pubDate>Thu, 04 May 2023 16:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02894v1</guid></item><item><title>APR: Online Distant Point Cloud Registration Through Aggregated Point Cloud Reconstruction</title><link>http://arxiv.org/abs/2305.02893v1</link><description>For many driving safety applications, it is of great importance to accuratelyregister LiDAR point clouds generated on distant moving vehicles. However, suchpoint clouds have extremely different point density and sensor perspective onthe same object, making registration on such point clouds very hard. In thispaper, we propose a novel feature extraction framework, called APR, for onlinedistant point cloud registration. Specifically, APR leverages an autoencoderdesign, where the autoencoder reconstructs a denser aggregated point cloud withseveral frames instead of the original single input point cloud. Our designforces the encoder to extract features with rich local geometry informationbased on one single input point cloud. Such features are then used for onlinedistant point cloud registration. We conduct extensive experiments againststate-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets.Results show that APR outperforms all other extractors by a large margin,increasing average registration recall of SOTA extractors by 7.1% on LoKITTIand 4.6% on LoNuScenes.</description><author>Quan Liu, Yunsong Zhou, Hongzi Zhu, Shan Chang, Minyi Guo</author><pubDate>Thu, 04 May 2023 16:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02893v1</guid></item><item><title>Incorporating Background Knowledge in Symbolic Regression using a Computer Algebra System</title><link>http://arxiv.org/abs/2301.11919v2</link><description>Symbolic Regression (SR) can generate interpretable, concise expressions thatfit a given dataset, allowing for more human understanding of the structurethan black-box approaches. The addition of background knowledge (in the form ofsymbolic mathematical constraints) allows for the generation of expressionsthat are meaningful with respect to theory while also being consistent withdata. We specifically examine the addition of constraints to traditionalgenetic algorithm (GA) based SR (PySR) as well as a Markov-chain Monte Carlo(MCMC) based Bayesian SR architecture (Bayesian Machine Scientist), and applythese to rediscovering adsorption equations from experimental, historicaldatasets. We find that, while hard constraints prevent GA and MCMC SR fromsearching, soft constraints can lead to improved performance both in terms ofsearch effectiveness and model meaningfulness, with computational costsincreasing by about an order-of-magnitude. If the constraints do not correlatewell with the dataset or expected models, they can hinder the search ofexpressions. We find Bayesian SR is better these constraints (as the Bayesianprior) than by modifying the fitness function in the GA</description><author>Charles Fox, Neil Tran, Nikki Nacion, Samiha Sharlin, Tyler R. Josephson</author><pubDate>Thu, 04 May 2023 15:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11919v2</guid></item><item><title>Neural Generalization of Multiple Kernel Learning</title><link>http://arxiv.org/abs/2102.13337v2</link><description>Multiple Kernel Learning is a conventional way to learn the kernel functionin kernel-based methods. MKL algorithms enhance the performance of kernelmethods. However, these methods have a lower complexity compared to deeplearning models and are inferior to these models in terms of recognitionaccuracy. Deep learning models can learn complex functions by applyingnonlinear transformations to data through several layers. In this paper, weshow that a typical MKL algorithm can be interpreted as a one-layer neuralnetwork with linear activation functions. By this interpretation, we propose aNeural Generalization of Multiple Kernel Learning (NGMKL), which extends theconventional multiple kernel learning framework to a multi-layer neural networkwith nonlinear activation functions. Our experiments on several benchmarks showthat the proposed method improves the complexity of MKL algorithms and leads tohigher recognition accuracy.</description><author>Ahmad Navid Ghanizadeh, Kamaledin Ghiasi-Shirazi, Reza Monsefi, Mohammadreza Qaraei</author><pubDate>Thu, 04 May 2023 15:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.13337v2</guid></item><item><title>Neuromorphic Sensing for Yawn Detection in Driver Drowsiness</title><link>http://arxiv.org/abs/2305.02888v1</link><description>Driver monitoring systems (DMS) are a key component of vehicular safety andessential for the transition from semiautonomous to fully autonomous driving. Akey task for DMS is to ascertain the cognitive state of a driver and todetermine their level of tiredness. Neuromorphic vision systems, based on eventcamera technology, provide advanced sensing of facial characteristics, inparticular the behavior of a driver's eyes. This research explores thepotential to extend neuromorphic sensing techniques to analyze the entirefacial region, detecting yawning behaviors that give a complimentary indicatorof tiredness. A neuromorphic dataset is constructed from 952 video clips (481yawns, 471 not-yawns) captured with an RGB color camera, with 37 subjects. Atotal of 95200 neuromorphic image frames are generated from this video datausing a video-to-event converter. From these data 21 subjects were selected toprovide a training dataset, 8 subjects were used for validation data, and theremaining 8 subjects were reserved for an "unseen" test dataset. An additional12300 frames were generated from event simulations of a public dataset to testagainst other methods. A CNN with self-attention and a recurrent head wasdesigned, trained, and tested with these data. Respective precision and recallscores of 95.9 percent and 94.7 percent were achieved on our test set, and 89.9percent and 91 percent on the simulated public test set, demonstrating thefeasibility to add yawn detection as a sensing component of a neuromorphic DMS.</description><author>Paul Kielty, Mehdi Sefidgar Dilmaghani, Cian Ryan, Joe Lemley, Peter Corcoran</author><pubDate>Thu, 04 May 2023 15:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02888v1</guid></item><item><title>Few-shot In-context Learning for Knowledge Base Question Answering</title><link>http://arxiv.org/abs/2305.01750v2</link><description>Question answering over knowledge bases is considered a difficult problem dueto the challenge of generalizing to a wide variety of possible natural languagequestions. Additionally, the heterogeneity of knowledge base schema itemsbetween different knowledge bases often necessitates specialized training fordifferent knowledge base question-answering (KBQA) datasets. To handlequestions over diverse KBQA datasets with a unified training-free framework, wepropose KB-BINDER, which for the first time enables few-shot in-contextlearning over KBQA tasks. Firstly, KB-BINDER leverages large language modelslike Codex to generate logical forms as the draft for a specific question byimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledgebase to bind the generated draft to an executable one with BM25 score matching.The experimental results on four public heterogeneous KBQA datasets show thatKB-BINDER can achieve a strong performance with only a few in-contextdemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can evenoutperform the state-of-the-art trained models. On GrailQA and WebQSP, ourmodel is also on par with other fully-trained models. We believe KB-BINDER canserve as an important baseline for future research. Our code is available athttps://github.com/ltl3A87/KB-BINDER.</description><author>Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, Wenhu Chen</author><pubDate>Thu, 04 May 2023 15:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01750v2</guid></item><item><title>Input Layer Binarization with Bit-Plane Encoding</title><link>http://arxiv.org/abs/2305.02885v1</link><description>Binary Neural Networks (BNNs) use 1-bit weights and activations toefficiently execute deep convolutional neural networks on edge devices.Nevertheless, the binarization of the first layer is conventionally excluded,as it leads to a large accuracy loss. The few works addressing the first layerbinarization, typically increase the number of input channels to enhance datarepresentation; such data expansion raises the amount of operations needed andit is feasible only on systems with enough computational resources. In thiswork, we present a new method to binarize the first layer using directly the8-bit representation of input data; we exploit the standard bit-planes encodingto extract features bit-wise (using depth-wise convolutions); after are-weighting stage, features are fused again. The resulting model is fullybinarized and our first layer binarization approach is model independent. Theconcept is evaluated on three classification datasets (CIFAR10, SVHN andCIFAR100) for different model architectures (VGG and ResNet) and, the proposedtechnique outperforms state of the art methods both in accuracy and BMACsreduction.</description><author>Lorenzo Vorabbi, Davide Maltoni, Stefano Santi</author><pubDate>Thu, 04 May 2023 15:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02885v1</guid></item><item><title>AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions</title><link>http://arxiv.org/abs/2112.00246v6</link><description>Perceiving and interacting with 3D articulated objects, such as cabinets,doors, and faucets, pose particular challenges for future home-assistant robotsperforming daily tasks in human environments. Besides parsing the articulatedparts and joint parameters, researchers recently advocate learning manipulationaffordance over the input shape geometry which is more task-aware andgeometrically fine-grained. However, taking only passive observations asinputs, these methods ignore many hidden but important kinematic constraints(e.g., joint location and limits) and dynamic factors (e.g., joint friction andrestitution), therefore losing significant accuracy for test cases with suchuncertainties. In this paper, we propose a novel framework, named AdaAfford,that learns to perform very few test-time interactions for quickly adapting theaffordance priors to more accurate instance-specific posteriors. We conductlarge-scale experiments using the PartNet-Mobility dataset and prove that oursystem performs better than baselines.</description><author>Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas Guibas, Hao Dong</author><pubDate>Thu, 04 May 2023 15:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.00246v6</guid></item><item><title>Global Performance Guarantees for Neural Network Models of AC Power Flow</title><link>http://arxiv.org/abs/2211.07125v2</link><description>Machine learning can generate black-box surrogate models which are bothextremely fast and highly accurate. Rigorously verifying the accuracy of theseblack-box models, however, is computationally challenging. When it comes topower systems, learning AC power flow is the cornerstone of any machinelearning surrogate model wishing to drastically accelerate computations,whether it is for optimization, control, or dynamics. This paper develops forthe first time, to our knowledge, a tractable neural network verificationprocedure which incorporates the ground truth of the non-linear AC power flowequations to determine worst-case neural network performance. Our approach,termed Sequential Targeted Tightening (STT), leverages a loosely convexifiedreformulation of the original verification problem, which is a mixed integerquadratic program (MIQP). Using the sequential addition of targeted cuts, weiteratively tighten our formulation until either the solution is sufficientlytight or a satisfactory performance guarantee has been generated. Afterlearning neural network models of the 14, 57, 118, and 200-bus PGLib testcases, we compare the performance guarantees generated by our STT procedurewith ones generated by a state-of-the-art MIQP solver, Gurobi 9.5. We show thatSTT often generates performance guarantees which are orders of magnitudetighter than the MIQP upper bound.</description><author>Samuel Chevalier, Spyros Chatzivasileiadis</author><pubDate>Thu, 04 May 2023 15:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07125v2</guid></item><item><title>Simple Noisy Environment Augmentation for Reinforcement Learning</title><link>http://arxiv.org/abs/2305.02882v1</link><description>Data augmentation is a widely used technique for improving model performancein machine learning, particularly in computer vision and natural languageprocessing. Recently, there has been increasing interest in applyingaugmentation techniques to reinforcement learning (RL) problems, with a focuson image-based augmentation. In this paper, we explore a set of genericwrappers designed to augment RL environments with noise and encourage agentexploration and improve training data diversity which are applicable to a broadspectrum of RL algorithms and environments. Specifically, we concentrate onaugmentations concerning states, rewards, and transition dynamics and introducetwo novel augmentation techniques. In addition, we introduce a noise ratehyperparameter for control over the frequency of noise injection. We presentexperimental results on the impact of these wrappers on return using threepopular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), andProximal Policy Optimization (PPO), across five MuJoCo environments. To supportthe choice of augmentation technique in practice, we also present analysis thatexplores the performance these techniques across environments. Lastly, wepublish the wrappers in our noisyenv repository for use with gym environments.</description><author>Raad Khraishi, Ramin Okhrati</author><pubDate>Thu, 04 May 2023 15:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02882v1</guid></item><item><title>Trainability barriers and opportunities in quantum generative modeling</title><link>http://arxiv.org/abs/2305.02881v1</link><description>Quantum generative models, in providing inherently efficient samplingstrategies, show promise for achieving a near-term advantage on quantumhardware. Nonetheless, important questions remain regarding their scalability.In this work, we investigate the barriers to the trainability of quantumgenerative models posed by barren plateaus and exponential loss concentration.We explore the interplay between explicit and implicit models and losses, andshow that using implicit generative models (such as quantum circuit-basedmodels) with explicit losses (such as the KL divergence) leads to a new flavourof barren plateau. In contrast, the Maximum Mean Discrepancy (MMD), which is apopular example of an implicit loss, can be viewed as the expectation value ofan observable that is either low-bodied and trainable, or global anduntrainable depending on the choice of kernel. However, in parallel, wehighlight that the low-bodied losses required for trainability cannot ingeneral distinguish high-order correlations, leading to a fundamental tensionbetween exponential concentration and the emergence of spurious minima. Wefurther propose a new local quantum fidelity-type loss which, by leveragingquantum circuits to estimate the quality of the encoded distribution, is bothfaithful and enjoys trainability guarantees. Finally, we compare theperformance of different loss functions for modelling real-world data from theHigh-Energy-Physics domain and confirm the trends predicted by our theoreticalresults.</description><author>Manuel S. Rudolph, Sacha Lerch, Supanut Thanasilp, Oriel Kiss, Sofia Vallecorsa, Michele Grossi, Zoë Holmes</author><pubDate>Thu, 04 May 2023 15:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02881v1</guid></item><item><title>DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data</title><link>http://arxiv.org/abs/2303.04201v2</link><description>Determining causal effects of interventions onto outcomes from real-world,observational (non-randomized) data, e.g., treatment repurposing usingelectronic health records, is challenging due to underlying bias. Causal deeplearning has improved over traditional techniques for estimating individualizedtreatment effects (ITE). We present the Doubly Robust VariationalInformation-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generativeframework that combines two joint models of treatment and outcome, ensuring anunbiased ITE estimation even when one of the two is misspecified. DR-VIDALintegrates: (i) a variational autoencoder (VAE) to factorize confounders intolatent variables according to causal assumptions; (ii) an information-theoreticgenerative adversarial network (Info-GAN) to generate counterfactuals; (iii) adoubly robust block incorporating treatment propensities for outcomepredictions. On synthetic and real-world datasets (Infant Health andDevelopment Program, Twin Birth Registry, and National Supported Work Program),DR-VIDAL achieves better performance than other non-generative and generativemethods. In conclusion, DR-VIDAL uniquely fuses causal assumptions, VAE,Info-GAN, and doubly robustness into a comprehensive, performant framework.Code is available at: https://github.com/Shantanu48114860/DR-VIDAL-AMIA-22under MIT license.</description><author>Shantanu Ghosh, Zheng Feng, Jiang Bian, Kevin Butler, Mattia Prosperi</author><pubDate>Thu, 04 May 2023 15:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04201v2</guid></item><item><title>The AI generation gap: Are Gen Z students more interested in adopting generative AI such as ChatGPT in teaching and learning than their Gen X and Millennial Generation teachers?</title><link>http://arxiv.org/abs/2305.02878v1</link><description>This study aimed to explore the experiences, perceptions, knowledge,concerns, and intentions of Gen Z students with Gen X and Gen Y teachersregarding the use of generative AI (GenAI) in higher education. A sample ofstudents and teachers were recruited to investigate the above using a surveyconsisting of both open and closed questions. The findings showed that Gen Zparticipants were generally optimistic about the potential benefits of GenAI,including enhanced productivity, efficiency, and personalized learning, andexpressed intentions to use GenAI for various educational purposes. Gen X andGen Y teachers acknowledged the potential benefits of GenAI but expressedheightened concerns about overreliance, ethical and pedagogical implications,emphasizing the need for proper guidelines and policies to ensure responsibleuse of the technology. The study highlighted the importance of combiningtechnology with traditional teaching methods to provide a more effectivelearning experience. Implications of the findings include the need to developevidence-based guidelines and policies for GenAI integration, foster criticalthinking and digital literacy skills among students, and promote responsibleuse of GenAI technologies in higher education.</description><author>Cecilia Ka Yuk Chan, Katherine K. W. Lee</author><pubDate>Thu, 04 May 2023 15:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02878v1</guid></item><item><title>Learning to Learn and Sample BRDFs</title><link>http://arxiv.org/abs/2210.03510v2</link><description>We propose a method to accelerate the joint process of physically acquiringand learning neural Bi-directional Reflectance Distribution Function (BRDF)models. While BRDF learning alone can be accelerated by meta-learning,acquisition remains slow as it relies on a mechanical process. We show thatmeta-learning can be extended to optimize the physical sampling pattern, too.After our method has been meta-trained for a set of fully-sampled BRDFs, it isable to quickly train on new BRDFs with up to five orders of magnitude fewerphysical acquisition samples at similar quality. Our approach also extends toother linear and non-linear BRDF models, which we show in an extensiveevaluation.</description><author>Chen Liu, Michael Fischer, Tobias Ritschel</author><pubDate>Thu, 04 May 2023 15:42:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03510v2</guid></item><item><title>The Role of Cross-Silo Federated Learning in Facilitating Data Sharing in the Agri-Food Sector</title><link>http://arxiv.org/abs/2104.07468v2</link><description>Data sharing remains a major hindering factor when it comes to adoptingemerging AI technologies in general, but particularly in the agri-food sector.Protectiveness of data is natural in this setting; data is a precious commodityfor data owners, which if used properly can provide them with useful insightson operations and processes leading to a competitive advantage. Unfortunately,novel AI technologies often require large amounts of training data in order toperform well, something that in many scenarios is unrealistic. However, recentmachine learning advances, e.g. federated learning and privacy-preservingtechnologies, can offer a solution to this issue via providing theinfrastructure and underpinning technologies needed to use data from varioussources to train models without ever sharing the raw data themselves. In thispaper, we propose a technical solution based on federated learning that usesdecentralized data, (i.e. data that are not exchanged or shared but remain withthe owners) to develop a cross-silo machine learning model that facilitatesdata sharing across supply chains. We focus our data sharing proposition onimproving production optimization through soybean yield prediction, and providepotential use-cases that such methods can assist in other problem settings. Ourresults demonstrate that our approach not only performs better than each of themodels trained on an individual data source, but also that data sharing in theagri-food sector can be enabled via alternatives to data exchange, whilst alsohelping to adopt emerging machine learning technologies to boost productivity.</description><author>Aiden Durrant, Milan Markovic, David Matthews, David May, Jessica Enright, Georgios Leontidis</author><pubDate>Thu, 04 May 2023 15:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.07468v2</guid></item><item><title>Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration</title><link>http://arxiv.org/abs/2302.04250v2</link><description>To generalize across tasks, an agent should acquire knowledge from past tasksthat facilitate adaptation and exploration in future tasks. We focus on theproblem of in-context adaptation and exploration, where an agent only relies oncontext, i.e., history of states, actions and/or rewards, rather thangradient-based updates. Posterior sampling (extension of Thompson sampling) isa promising approach, but it requires Bayesian inference and dynamicprogramming, which often involve unknowns (e.g., a prior) and costlycomputations. To address these difficulties, we use a transformer to learn aninference process from training tasks and consider a hypothesis space ofpartial models, represented as small Markov decision processes that are cheapfor dynamic programming. In our version of the Symbolic Alchemy benchmark, ourmethod's adaptation speed and exploration-exploitation balance approach thoseof an exact posterior sampling oracle. We also show that even though partialmodels exclude relevant information from the environment, they can neverthelesslead to good policies.</description><author>Chentian Jiang, Nan Rosemary Ke, Hado van Hasselt</author><pubDate>Thu, 04 May 2023 15:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04250v2</guid></item><item><title>PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying Fused Chinese Painting and Calligraphy</title><link>http://arxiv.org/abs/2301.13082v4</link><description>AI-Generated Content (AIGC) has recently gained a surge in popularity,powered by its high efficiency and consistency in production, and itscapability of being customized and diversified. The cross-modality nature ofthe representation learning mechanism in most AIGC technology allows for morefreedom and flexibility in exploring new types of art that would be impossiblein the past. Inspired by the pictogram subset of Chinese characters, weproposed PaCaNet, a CycleGAN-based pipeline for producing novel artworks thatfuse two different art types, traditional Chinese painting and calligraphy. Inan effort to produce stable and diversified output, we adopted three maintechnical innovations: 1. Using one-shot learning to increase the creativity ofpre-trained models and diversify the content of the fused images. 2.Controlling the preference over generated Chinese calligraphy by freezingrandomly sampled parameters in pre-trained models. 3. Using a regularizationmethod to encourage the models to produce images similar to Chinese paintings.Furthermore, we conducted a systematic study to explore the performance ofPaCaNet in diversifying fused Chinese painting and calligraphy, which showedsatisfying results. In conclusion, we provide a new direction of creating artsby fusing the visual information in paintings and the stroke features inChinese calligraphy. Our approach creates a unique aesthetic experience rootedin the origination of Chinese hieroglyph characters. It is also a uniqueopportunity to delve deeper into traditional artwork and, in doing so, tocreate a meaningful impact on preserving and revitalizing traditional heritage.</description><author>Zuhao Yang, Huajun Bai, Zhang Luo, Yang Xu, Wei Pang, Yue Wang, Yisheng Yuan, Yingfang Yuan</author><pubDate>Thu, 04 May 2023 15:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13082v4</guid></item><item><title>Towards Complex Document Understanding By Discrete Reasoning</title><link>http://arxiv.org/abs/2207.11871v3</link><description>Document Visual Question Answering (VQA) aims to understand visually-richdocuments to answer questions in natural language, which is an emergingresearch topic for both Natural Language Processing and Computer Vision. Inthis work, we introduce a new Document VQA dataset, named TAT-DQA, whichconsists of 3,067 document pages comprising semi-structured table(s) andunstructured text as well as 16,558 question-answer pairs by extending theTAT-QA dataset. These documents are sampled from real-world financial reportsand contain lots of numbers, which means discrete reasoning capability isdemanded to answer questions on this dataset. Based on TAT-DQA, we furtherdevelop a novel model named MHST that takes into account the information inmulti-modalities, including text, layout and visual image, to intelligentlyaddress different types of questions with corresponding strategies, i.e.,extraction or reasoning. Extensive experiments show that the MHST modelsignificantly outperforms the baseline methods, demonstrating itseffectiveness. However, the performance still lags far behind that of experthumans. We expect that our new TAT-DQA dataset would facilitate the research ondeep understanding of visually-rich documents combining vision and language,especially for scenarios that require discrete reasoning. Also, we hope theproposed model would inspire researchers to design more advanced Document VQAmodels in future. Our dataset will be publicly available for non-commercial useat https://nextplusplus.github.io/TAT-DQA/.</description><author>Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, Tat-Seng Chua</author><pubDate>Thu, 04 May 2023 15:30:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.11871v3</guid></item><item><title>2x Faster Language Model Pre-training via Masked Structural Growth</title><link>http://arxiv.org/abs/2305.02869v1</link><description>Acceleration of large language model pre-training is a critical issue inpresent NLP research. In this paper, we focus on speeding up pre-training byprogressively growing from a small Transformer structure to a large one. Thereare two main research problems related to progressive growth: growth scheduleand growth operator. For growth schedule, existing work has exploredmulti-stage expansion of depth and feedforward layers. However, the impact ofeach dimension on the schedule's efficiency is still an open question. Forgrowth operator, existing work relies on the initialization of new weights toinherit knowledge, and achieve only non-strict function preservation, limitingfurther optimization of training dynamics. To address these issues, we proposeMasked Structural Growth (MSG), including growth schedules involving allpossible dimensions and strictly function-preserving growth operators that isindependent of the initialization of new weights. Experiments show that MSG issignificantly faster than related work: we achieve a speed-up of 80% forBert-base and 120% for Bert-large pre-training. Moreover, MSG is able toimprove fine-tuning performances at the same time.</description><author>Yiqun Yao, Zheng Zhang, Jing Li, Yequan Wang</author><pubDate>Thu, 04 May 2023 15:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02869v1</guid></item><item><title>ISP Distillation</title><link>http://arxiv.org/abs/2101.10203v3</link><description>Nowadays, many of the images captured are `observed' by machines only and notby humans, e.g., in autonomous systems. High-level machine vision models, suchas object recognition or semantic segmentation, assume images are transformedinto some canonical image space by the camera \ans{Image Signal Processor(ISP)}. However, the camera ISP is optimized for producing visually pleasingimages for human observers and not for machines. Therefore, one may spare theISP compute time and apply vision models directly to RAW images. Yet, it hasbeen shown that training such models directly on RAW images results in aperformance drop. To mitigate this drop, we use a RAW and RGB image pairsdataset, which can be easily acquired with no human labeling. We then train amodel that is applied directly to the RAW data by using knowledge distillationsuch that the model predictions for RAW images will be aligned with thepredictions of an off-the-shelf pre-trained model for processed RGB images. Ourexperiments show that our performance on RAW images for object classificationand semantic segmentation is significantly better than models trained onlabeled RAW images. It also reasonably matches the predictions of a pre-trainedmodel on processed RGB images, while saving the ISP compute overhead.</description><author>Eli Schwartz, Alex Bronstein, Raja Giryes</author><pubDate>Thu, 04 May 2023 15:27:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2101.10203v3</guid></item><item><title>Hierarchical Transformer for Scalable Graph Learning</title><link>http://arxiv.org/abs/2305.02866v1</link><description>Graph Transformer is gaining increasing attention in the field of machinelearning and has demonstrated state-of-the-art performance on benchmarks forgraph representation learning. However, as current implementations of GraphTransformer primarily focus on learning representations of small-scale graphs,the quadratic complexity of the global self-attention mechanism presents achallenge for full-batch training when applied to larger graphs. Additionally,conventional sampling-based methods fail to capture necessary high-levelcontextual information, resulting in a significant loss of performance. In thispaper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as asolution to these challenges. HSGT successfully scales the Transformerarchitecture to node representation learning tasks on large-scale graphs, whilemaintaining high performance. By utilizing graph hierarchies constructedthrough coarsening techniques, HSGT efficiently updates and stores multi-scaleinformation in node embeddings at different levels. Together withsampling-based training methods, HSGT effectively captures and aggregatesmulti-level information on the hierarchical graph using only Transformerblocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-artperformance on large-scale benchmarks with graphs containing millions of nodeswith high efficiency.</description><author>Wenhao Zhu, Tianyu Wen, Guojie Song, Xiaojun Ma, Liang Wang</author><pubDate>Thu, 04 May 2023 15:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02866v1</guid></item><item><title>CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing</title><link>http://arxiv.org/abs/2305.02865v1</link><description>Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics,is getting increasing attention for its detrimental effect on thegeneralization ability of NLU models. Existing works focus on eliminatingdataset bias by down-weighting problematic data in the training process, whichinduce the omission of valid feature information while mitigating bias. In thiswork, We analyze the causes of dataset bias from the perspective of causalinference and propose CausalAPM, a generalizable literal disentanglingframework to ameliorate the bias problem from feature granularity. The proposedapproach projects literal and semantic information into independent featuresubspaces, and constrains the involvement of literal information in subsequentpredictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, andQQP) demonstrate that our proposed framework significantly improves the OODgeneralization performance while maintaining ID performance.</description><author>Songyang Gao, Shihan Dou, Junjie Shan, Qi Zhang, Xuanjing Huang</author><pubDate>Thu, 04 May 2023 15:22:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02865v1</guid></item><item><title>ReMask: A Robust Information-Masking Approach for Domain Counterfactual Generation</title><link>http://arxiv.org/abs/2305.02858v1</link><description>Domain shift is a big challenge in NLP, thus, many approaches resort tolearning domain-invariant features to mitigate the inference phase domainshift. Such methods, however, fail to leverage the domain-specific nuancesrelevant to the task at hand. To avoid such drawbacks, domain counterfactualgeneration aims to transform a text from the source domain to a given targetdomain. However, due to the limited availability of data, such frequency-basedmethods often miss and lead to some valid and spurious domain-tokenassociations. Hence, we employ a three-step domain obfuscation approach thatinvolves frequency and attention norm-based masking, to mask domain-specificcues, and unmasking to regain the domain generic context. Our experimentsempirically show that the counterfactual samples sourced from our masked textlead to improved domain transfer on 10 out of 12 domain sentimentclassification settings, with an average of 2% accuracy improvement over thestate-of-the-art for unsupervised domain adaptation (UDA). Further, our modeloutperforms the state-of-the-art by achieving 1.4% average accuracy improvementin the adversarial domain adaptation (ADA) setting. Moreover, our model alsoshows its domain adaptation efficacy on a large multi-domain intentclassification dataset where it attains state-of-the-art results. We releasethe codes publicly at \url{https://github.com/declare-lab/remask}.</description><author>Pengfei Hong, Rishabh Bhardwaj, Navonil Majumdar, Somak Aditya, Soujanya Poria</author><pubDate>Thu, 04 May 2023 15:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02858v1</guid></item><item><title>Maximum Causal Entropy Inverse Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2305.02857v1</link><description>When deploying artificial agents in real-world environments where theyinteract with humans, it is crucial that their behavior is aligned with thevalues, social norms or other requirements of that environment. However, manyenvironments have implicit constraints that are difficult to specify andtransfer to a learning agent. To address this challenge, we propose a novelmethod that utilizes the principle of maximum causal entropy to learnconstraints and an optimal policy that adheres to these constraints, usingdemonstrations of agents that abide by the constraints. We prove convergence ina tabular setting and provide an approximation which scales to complexenvironments. We evaluate the effectiveness of the learned policy by assessingthe reward received and the number of constraint violations, and we evaluatethe learned cost function based on its transferability to other agents. Ourmethod has been shown to outperform state-of-the-art approaches across avariety of tasks and environments, and it is able to handle problems withstochastic dynamics and a continuous state-action space.</description><author>Mattijs Baert, Pietro Mazzaglia, Sam Leroux, Pieter Simoens</author><pubDate>Thu, 04 May 2023 15:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02857v1</guid></item><item><title>Improving Contrastive Learning of Sentence Embeddings from AI Feedback</title><link>http://arxiv.org/abs/2305.01918v2</link><description>Contrastive learning has become a popular approach in natural languageprocessing, particularly for the learning of sentence embeddings. However, thediscrete nature of natural language makes it difficult to ensure the quality ofpositive and negative sample pairs generated through data augmentation methods.Although supervised contrastive learning can produce more accurate sample pairswith human feedback labels, it still lacks fine-grained training signals. Inthis paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning ofsentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Ourmethod utilizes AI feedback from large pre-trained language models (LLMs) toconstruct sample pairs with fine-grained sample similarity scores to improvecontrastive learning. Besides, we combine human feedback and AI feedback toprovide better supervision signals for supervised contrastive learning ofsentence embeddings. Experimental results show that our method achievesstate-of-the-art performance on several semantic textual similarity (STS) andtransfer learning tasks compared to other unsupervised and supervisedcontrastive learning methods.</description><author>Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang Li, Xipeng Qiu</author><pubDate>Thu, 04 May 2023 15:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01918v2</guid></item><item><title>MedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation</title><link>http://arxiv.org/abs/2211.07302v2</link><description>Separation of multiple singing voices into each voice is a rarely studiedarea in music source separation research. The absence of a benchmark datasethas hindered its progress. In this paper, we present an evaluation dataset andprovide baseline studies for multiple singing voices separation. First, weintroduce MedleyVox, an evaluation dataset for multiple singing voicesseparation. We specify the problem definition in this dataset by categorizingit into i) unison, ii) duet, iii) main vs. rest, and iv) N-singing separation.Second, to overcome the absence of existing multi-singing datasets for atraining purpose, we present a strategy for construction of multiple singingmixtures using various single-singing datasets. Third, we propose the improvedsuper-resolution network (iSRNet), which greatly enhances initial estimates ofseparation networks. Jointly trained with the Conv-TasNet and the multi-singingmixture construction strategy, the proposed iSRNet achieved comparableperformance to ideal time-frequency masks on duet and unison subsets ofMedleyVox. Audio samples, the dataset, and codes are available on our website(https://github.com/jeonchangbin49/MedleyVox).</description><author>Chang-Bin Jeon, Hyeongi Moon, Keunwoo Choi, Ben Sangbae Chon, Kyogu Lee</author><pubDate>Thu, 04 May 2023 15:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07302v2</guid></item><item><title>Marginal Thresholding in Noisy Image Segmentation</title><link>http://arxiv.org/abs/2304.04116v2</link><description>This work presents a study on label noise in medical image segmentation byconsidering a noise model based on Gaussian field deformations. Such noise isof interest because it yields realistic looking segmentations and because it isunbiased in the sense that the expected deformation is the identity mapping.Efficient methods for sampling and closed form solutions for the marginalprobabilities are provided. Moreover, theoretically optimal solutions to theloss functions cross-entropy and soft-Dice are studied and it is shown how theydiverge as the level of noise increases. Based on recent work on loss functioncharacterization, it is shown that optimal solutions to soft-Dice can berecovered by thresholding solutions to cross-entropy with a particular a prioriunknown threshold that efficiently can be computed. This raises the questionwhether the decrease in performance seen when using cross-entropy as comparedto soft-Dice is caused by using the wrong threshold. The hypothesis isvalidated in 5-fold studies on three organ segmentation problems from theTotalSegmentor data set, using 4 different strengths of noise. The results showthat changing the threshold leads the performance of cross-entropy to go fromsystematically worse than soft-Dice to similar or better results thansoft-Dice.</description><author>Marcus Nordström, Henrik Hult, Atsuto Maki</author><pubDate>Thu, 04 May 2023 15:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04116v2</guid></item><item><title>DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics</title><link>http://arxiv.org/abs/2210.02438v3</link><description>We introduce the first work to explore web-scale diffusion models forrobotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by firstinferring a text description of those objects, then generating an imagerepresenting a natural, human-like arrangement of those objects, and finallyphysically arranging the objects according to that goal image. We show thatthis is possible zero-shot using DALL-E, without needing any further examplearrangements, data collection, or training. DALL-E-Bot is fully autonomous andis not restricted to a pre-defined set of objects or scenes, thanks to DALL-E'sweb-scale pre-training. Encouraging real-world results, with both human studiesand objective metrics, show that integrating web-scale diffusion models intorobotics pipelines is a promising direction for scalable, unsupervised robotlearning.</description><author>Ivan Kapelyukh, Vitalis Vosylius, Edward Johns</author><pubDate>Thu, 04 May 2023 15:11:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.02438v3</guid></item><item><title>Impossibility of Depth Reduction in Explainable Clustering</title><link>http://arxiv.org/abs/2305.02850v1</link><description>Over the last few years Explainable Clustering has gathered a lot ofattention. Dasgupta et al. [ICML'20] initiated the study of explainable k-meansand k-median clustering problems where the explanation is captured by athreshold decision tree which partitions the space at each node using axisparallel hyperplanes. Recently, Laber et al. [Pattern Recognition'23] made acase to consider the depth of the decision tree as an additional complexitymeasure of interest. In this work, we prove that even when the input points are in the Euclideanplane, then any depth reduction in the explanation incurs unbounded loss in thek-means and k-median cost. Formally, we show that there exists a data set X inthe Euclidean plane, for which there is a decision tree of depth k-1 whosek-means/k-median cost matches the optimal clustering cost of X, but everydecision tree of depth less than k-1 has unbounded cost w.r.t. the optimal costof clustering. We extend our results to the k-center objective as well, albeitwith weaker guarantees.</description><author>Chengyuan Deng, Surya Teja Gavva, Karthik C. S., Parth Patel, Adarsh Srinivasan</author><pubDate>Thu, 04 May 2023 15:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02850v1</guid></item><item><title>MEDIC: A Multimodal Empathy Dataset in Counseling</title><link>http://arxiv.org/abs/2305.02842v1</link><description>Although empathic interaction between counselor and client is fundamental tosuccess in the psychotherapeutic process, there are currently few datasets toaid a computational approach to empathy understanding. In this paper, weconstruct a multimodal empathy dataset collected from face-to-facepsychological counseling sessions. The dataset consists of 771 video clips. Wealso propose three labels (i.e., expression of experience, emotional reaction,and cognitive reaction) to describe the degree of empathy between counselorsand their clients. Expression of experience describes whether the client hasexpressed experiences that can trigger empathy, and emotional and cognitivereactions indicate the counselor's empathic reactions. As an elementaryassessment of the usability of the constructed multimodal empathy dataset, aninterrater reliability analysis of annotators' subjective evaluations for videoclips is conducted using the intraclass correlation coefficient and Fleiss'Kappa. Results prove that our data annotation is reliable. Furthermore, weconduct empathy prediction using three typical methods, including the tensorfusion network, the sentimental words aware fusion network, and a simpleconcatenation model. The experimental results show that empathy can be wellpredicted on our dataset. Our dataset is available for research purposes.</description><author>Zhou'an_Zhu, Xin Li, Jicai Pan, Yufei Xiao, Yanan Chang, Feiyi Zheng, Shangfei Wang</author><pubDate>Thu, 04 May 2023 15:02:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02842v1</guid></item><item><title>CrAM: A Compression-Aware Minimizer</title><link>http://arxiv.org/abs/2207.14200v4</link><description>Deep neural networks (DNNs) often have to be compressed, via pruning and/orquantization, before they can be deployed in practical settings. In this workwe propose a new compression-aware minimizer dubbed CrAM that modifies theoptimization step in a principled way, in order to produce models whose localloss behavior is stable under compression operations such as pruning. Thus,dense models trained via CrAM should be compressible post-training, in a singlestep, without significant accuracy loss. Experimental results on standardbenchmarks, such as residual networks for ImageNet classification and BERTmodels for language modelling, show that CrAM produces dense models that can bemore accurate than the standard SGD/Adam-based baselines, but which are stableunder weight pruning: specifically, we can prune models in one-shot to 70-80%sparsity with almost no accuracy loss, and to 90% with reasonable ($\sim 1\%$)accuracy loss, which is competitive with gradual compression methods.Additionally, CrAM can produce sparse models which perform well for transferlearning, and it also works for semi-structured 2:4 pruning patterns supportedby GPU hardware. The code for reproducing the results is available athttps://github.com/IST-DASLab/CrAM .</description><author>Alexandra Peste, Adrian Vladu, Eldar Kurtic, Christoph H. Lampert, Dan Alistarh</author><pubDate>Thu, 04 May 2023 14:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.14200v4</guid></item><item><title>Noisy Image Segmentation With Soft-Dice</title><link>http://arxiv.org/abs/2304.00801v3</link><description>This paper presents a study on the soft-Dice loss, one of the most popularloss functions in medical image segmentation, for situations where noise ispresent in target labels. In particular, the set of optimal solutions arecharacterized and sharp bounds on the volume bias of these solutions areprovided. It is further shown that a sequence of soft segmentations convergingto optimal soft-Dice also converges to optimal Dice when converted to hardsegmentations using thresholding. This is an important result because soft-Diceis often used as a proxy for maximizing the Dice metric. Finally, experimentsconfirming the theoretical results are provided.</description><author>Marcus Nordström, Henrik Hult, Atsuto Maki, Fredrik Löfman</author><pubDate>Thu, 04 May 2023 14:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00801v3</guid></item></channel></rss>