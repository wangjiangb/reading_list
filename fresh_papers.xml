<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 17 Jul 2023 06:00:17 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Expressive Monotonic Neural Networks</title><link>http://arxiv.org/abs/2307.07512v1</link><description>The monotonic dependence of the outputs of a neural network on some of itsinputs is a crucial inductive bias in many scenarios where domain knowledgedictates such behavior. This is especially important for interpretability andfairness considerations. In a broader context, scenarios in which monotonicityis important can be found in finance, medicine, physics, and other disciplines.It is thus desirable to build neural network architectures that implement thisinductive bias provably. In this work, we propose a weight-constrainedarchitecture with a single residual connection to achieve exact monotonicdependence in any subset of the inputs. The weight constraint scheme directlycontrols the Lipschitz constant of the neural network and thus provides theadditional benefit of robustness. Compared to currently existing techniquesused for monotonicity, our method is simpler in implementation and in theoryfoundations, has negligible computational overhead, is guaranteed to producemonotonic dependence, and is highly expressive. We show how the algorithm isused to train powerful, robust, and interpretable discriminators that achievecompetitive performance compared to current state-of-the-art methods acrossvarious benchmarks, from social applications to the classification of thedecays of subatomic particles produced at the CERN Large Hadron Collider.</description><author>Ouail Kitouni, Niklas Nolte, Michael Williams</author><pubDate>Fri, 14 Jul 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07512v1</guid></item><item><title>NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis</title><link>http://arxiv.org/abs/2307.07511v1</link><description>We address the problem of generating realistic 3D motions of humansinteracting with objects in a scene. Our key idea is to create a neuralinteraction field attached to a specific object, which outputs the distance tothe valid interaction manifold given a human pose as input. This interactionfield guides the sampling of an object-conditioned human motion diffusionmodel, so as to encourage plausible contacts and affordance semantics. Tosupport interactions with scarcely available data, we propose an automatedsynthetic data pipeline. For this, we seed a pre-trained motion model, whichhas priors for the basics of human movement, with interaction-specific anchorposes extracted from limited motion capture data. Using our guided diffusionmodel trained on generated synthetic data, we synthesize realistic motions forsitting and lifting with several objects, outperforming alternative approachesin terms of motion quality and successful action completion. We call ourframework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.</description><author>Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, Leonidas Guibas</author><pubDate>Fri, 14 Jul 2023 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07511v1</guid></item><item><title>MGit: A Model Versioning and Management System</title><link>http://arxiv.org/abs/2307.07507v1</link><description>Models derived from other models are extremely common in machine learning(ML) today. For example, transfer learning is used to create task-specificmodels from "pre-trained" models through finetuning. This has led to anecosystem where models are related to each other, sharing structure and ofteneven parameter values. However, it is hard to manage these model derivatives:the storage overhead of storing all derived models quickly becomes onerous,prompting users to get rid of intermediate models that might be useful forfurther analysis. Additionally, undesired behaviors in models are hard to trackdown (e.g., is a bug inherited from an upstream model?). In this paper, wepropose a model versioning and management system called MGit that makes iteasier to store, test, update, and collaborate on model derivatives. MGitintroduces a lineage graph that records provenance and versioning informationbetween models, optimizations to efficiently store model parameters, as well asabstractions over this lineage graph that facilitate relevant testing, updatingand collaboration functionality. MGit is able to reduce the lineage graph'sstorage footprint by up to 7x and automatically update downstream models inresponse to updates to upstream models.</description><author>Wei Hao, Daniel Mendoza, Rafael da Silva, Deepak Narayanan, Amar Phanishaye</author><pubDate>Fri, 14 Jul 2023 18:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07507v1</guid></item><item><title>Unpacking the Black Box: Regulating Algorithmic Decisions</title><link>http://arxiv.org/abs/2110.03443v2</link><description>We show how to optimally regulate prediction algorithms in a world where anagent uses complex 'black-box' prediction functions to make decisions such aslending, medical testing, or hiring, and where a principal is limited in howmuch she can learn about the agent's black-box model. We show that limitingagents to prediction functions that are simple enough to be fully transparentis inefficient as long as the misalignment is limited and first-best predictionfunctions are sufficiently complex. Algorithmic audits can improve welfare, butthe gains depend on the design of the audit tools. Tools that focus onminimizing overall information loss, the focus of many explainer tools, willgenerally be inefficient since they focus on explaining the average behavior ofthe prediction function. Targeted tools that focus on the source of incentivemisalignment, e.g., excess false positives or racial disparities, can providesecond-best solutions. We provide empirical support for our theoreticalfindings using an application in consumer lending, where we document thatcomplex models regulated based on context-specific explanation tools outperformsimple, fully transparent models. This gain from complex models represents aPareto improvement across our empirical applications that are preferred both bythe lender and from the perspective of the financial regulator.</description><author>Laura Blattner, Scott Nelson, Jann Spiess</author><pubDate>Fri, 14 Jul 2023 18:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.03443v2</guid></item><item><title>Brain Tumor Detection using Convolutional Neural Networks with Skip Connections</title><link>http://arxiv.org/abs/2307.07503v1</link><description>In this paper, we present different architectures of Convolutional NeuralNetworks (CNN) to analyze and classify the brain tumors into benign andmalignant types using the Magnetic Resonance Imaging (MRI) technique. DifferentCNN architecture optimization techniques such as widening and deepening of thenetwork and adding skip connections are applied to improve the accuracy of thenetwork. Results show that a subset of these techniques can judiciously be usedto outperform a baseline CNN model used for the same purpose.</description><author>Aupam Hamran, Marzieh Vaeztourshizi, Amirhossein Esmaili, Massoud Pedram</author><pubDate>Fri, 14 Jul 2023 18:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07503v1</guid></item><item><title>TALL: Thumbnail Layout for Deepfake Video Detection</title><link>http://arxiv.org/abs/2307.07494v1</link><description>The growing threats of deepfakes to society and cybersecurity have raisedenormous public concerns, and increasing efforts have been devoted to thiscritical topic of deepfake video detection. Existing video methods achieve goodperformance but are computationally intensive. This paper introduces a simpleyet effective strategy named Thumbnail Layout (TALL), which transforms a videoclip into a pre-defined layout to realize the preservation of spatial andtemporal dependencies. Specifically, consecutive frames are masked in a fixedposition in each frame to improve generalization, then resized to sub-imagesand rearranged into a pre-defined layout as the thumbnail. TALL ismodel-agnostic and extremely simple by only modifying a few lines of code.Inspired by the success of vision transformers, we incorporate TALL into SwinTransformer, forming an efficient and effective method TALL-Swin. Extensiveexperiments on intra-dataset and cross-dataset validate the validity andsuperiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\%$ AUC on thechallenging cross-dataset task, FaceForensics++ $\to$ Celeb-DF. The code isavailable at https://github.com/rainy-xu/TALL4Deepfake.</description><author>Yuting Xu, Jian Liang, Gengyun Jia, Ziming Yang, Yanhao Zhang, Ran He</author><pubDate>Fri, 14 Jul 2023 18:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07494v1</guid></item><item><title>PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation</title><link>http://arxiv.org/abs/2307.07489v1</link><description>Unsupervised domain adaptation (UDA) has witnessed remarkable advancements inimproving the accuracy of models for unlabeled target domains. However, thecalibration of predictive uncertainty in the target domain, a crucial aspect ofthe safe deployment of UDA models, has received limited attention. Theconventional in-domain calibration method, \textit{temperature scaling}(TempScal), encounters challenges due to domain distribution shifts and theabsence of labeled target domain data. Recent approaches have employedimportance-weighting techniques to estimate the target-optimal temperaturebased on re-weighted labeled source data. Nonetheless, these methods requiresource data and suffer from unreliable density estimates under severe domainshifts, rendering them unsuitable for source-free UDA settings. To overcomethese limitations, we propose PseudoCal, a source-free calibration method thatexclusively relies on unlabeled target data. Unlike previous approaches thattreat UDA calibration as a \textit{covariate shift} problem, we consider it asan unsupervised calibration problem specific to the target domain. Motivated bythe factorization of the negative log-likelihood (NLL) objective in TempScal,we generate a labeled pseudo-target set that captures the structure of the realtarget. By doing so, we transform the unsupervised calibration problem into asupervised one, enabling us to effectively address it using widely-usedin-domain methods like TempScal. Finally, we thoroughly evaluate thecalibration performance of PseudoCal by conducting extensive experiments on 10UDA methods, considering both traditional UDA settings and recent source-freeUDA scenarios. The experimental results consistently demonstrate the superiorperformance of PseudoCal, exhibiting significantly reduced calibration errorcompared to existing calibration methods.</description><author>Dapeng Hu, Jian Liang, Xinchao Wang, Chuan-Sheng Foo</author><pubDate>Fri, 14 Jul 2023 18:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07489v1</guid></item><item><title>DreamTeacher: Pretraining Image Backbones with Deep Generative Models</title><link>http://arxiv.org/abs/2307.07487v1</link><description>In this work, we introduce a self-supervised feature representation learningframework DreamTeacher that utilizes generative networks for pre-trainingdownstream image backbones. We propose to distill knowledge from a trainedgenerative model into standard image backbones that have been well engineeredfor specific perception tasks. We investigate two types of knowledgedistillation: 1) distilling learned generative features onto target imagebackbones as an alternative to pretraining these backbones on large labeleddatasets such as ImageNet, and 2) distilling labels obtained from generativenetworks with task heads onto logits of target backbones. We perform extensiveanalyses on multiple generative models, dense prediction benchmarks, andseveral pre-training regimes. We empirically find that our DreamTeachersignificantly outperforms existing self-supervised representation learningapproaches across the board. Unsupervised ImageNet pre-training withDreamTeacher leads to significant improvements over ImageNet classificationpre-training on downstream datasets, showcasing generative models, anddiffusion generative models specifically, as a promising approach torepresentation learning on large, diverse datasets without requiring manualannotation.</description><author>Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, Sanja Fidler</author><pubDate>Fri, 14 Jul 2023 18:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07487v1</guid></item><item><title>Multimodal Distillation for Egocentric Action Recognition</title><link>http://arxiv.org/abs/2307.07483v1</link><description>The focal point of egocentric video understanding is modelling hand-objectinteractions. Standard models, e.g. CNNs or Vision Transformers, which receiveRGB frames as input perform well. However, their performance improves furtherby employing additional input modalities that provide complementary cues, suchas object detections, optical flow, audio, etc. The added complexity of themodality-specific modules, on the other hand, makes these models impracticalfor deployment. The goal of this work is to retain the performance of such amultimodal approach, while using only the RGB frames as input at inferencetime. We demonstrate that for egocentric action recognition on theEpic-Kitchens and the Something-Something datasets, students which are taughtby multimodal teachers tend to be more accurate and better calibrated thanarchitecturally equivalent models trained on ground truth labels in a unimodalor multimodal fashion. We further adopt a principled multimodal knowledgedistillation framework, allowing us to deal with issues which occur whenapplying multimodal knowledge distillation in a naive manner. Lastly, wedemonstrate the achieved reduction in computational complexity, and show thatour approach maintains higher performance with the reduction of the number ofinput views.</description><author>Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars</author><pubDate>Fri, 14 Jul 2023 18:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07483v1</guid></item><item><title>Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based Tumor Classification</title><link>http://arxiv.org/abs/2307.07482v1</link><description>Whole slide image (WSI) assessment is a challenging and crucial step incancer diagnosis and treatment planning. WSIs require high magnifications tofacilitate sub-cellular analysis. Precise annotations for patch- or evenpixel-level classifications in the context of gigapixel WSIs are tedious toacquire and require domain experts. Coarse-grained labels, on the other hand,are easily accessible, which makes WSI classification an ideal use case formultiple instance learning (MIL). In our work, we propose a novelembedding-based Dual-Query MIL pipeline (DQ-MIL). We contribute to both theembedding and aggregation steps. Since all-purpose visual featurerepresentations are not yet available, embedding models are currently limitedin terms of generalizability. With our work, we explore the potential ofdynamic meta-embedding based on cutting-edge self-supervised pre-trained modelsin the context of MIL. Moreover, we propose a new MIL architecture capable ofcombining MIL-attention with correlated self-attention. The Dual-QueryPerceiver design of our approach allows us to leverage the concept ofself-distillation and to combine the advantages of a small model in the contextof a low data regime with the rich feature representation of a larger model. Wedemonstrate the superior performance of our approach on three histopathologicaldatasets, where we show improvement of up to 10% over state-of-the-artapproaches.</description><author>Simon Holdenried-Krafft, Peter Somers, Ivonne A. Montes-Majarro, Diana Silimon, Cristina Tarín, Falko Fend, Hendrik P. A. Lensch</author><pubDate>Fri, 14 Jul 2023 18:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07482v1</guid></item><item><title>ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning</title><link>http://arxiv.org/abs/2305.01486v3</link><description>In this paper, we introduce a framework ARBEx, a novel attentive featureextraction framework driven by Vision Transformer with reliability balancing tocope against poor class distributions, bias, and uncertainty in the facialexpression learning (FEL) task. We reinforce several data pre-processing andrefinement methods along with a window-based cross-attention ViT to squeeze thebest of the data. We also employ learnable anchor points in the embedding spacewith label distributions and multi-head self-attention mechanism to optimizeperformance against weak predictions with reliability balancing, which is astrategy that leverages anchor points, attention scores, and confidence valuesto enhance the resilience of label predictions. To ensure correct labelclassification and improve the models' discriminative power, we introduceanchor loss, which encourages large margins between anchor points.Additionally, the multi-head self-attention mechanism, which is also trainable,plays an integral role in identifying accurate labels. This approach providescritical elements for improving the reliability of predictions and has asubstantial positive effect on final prediction capabilities. Our adaptivemodel can be integrated with any deep neural network to forestall challenges invarious recognition tasks. Our strategy outperforms current state-of-the-artmethodologies, according to extensive experiments conducted in a variety ofcontexts.</description><author>Azmine Toushik Wasi, Karlo Šerbetar, Raima Islam, Taki Hasan Rafi, Dong-Kyu Chae</author><pubDate>Fri, 14 Jul 2023 18:02:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01486v3</guid></item><item><title>Population Expansion for Training Language Models with Private Federated Learning</title><link>http://arxiv.org/abs/2307.07477v1</link><description>Federated learning (FL) combined with differential privacy (DP) offersmachine learning (ML) training with distributed devices and with a formalprivacy guarantee. With a large population of devices, FL with DP produces aperformant model in a timely manner. However, for applications with a smallerpopulation, not only does the model utility degrade as the DP noise isinversely proportional to population, but also the training latency increasessince waiting for enough clients to become available from a smaller pool isslower. In this work, we thus propose expanding the population based on domainadaptation techniques to speed up the training and improves the final modelquality when training with small populations. We empirically demonstrate thatour techniques can improve the utility by 13% to 30% on real-world languagemodeling datasets.</description><author>Tatsuki Koga, Congzheng Song, Martin Pelikan, Mona Chitnis</author><pubDate>Fri, 14 Jul 2023 17:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07477v1</guid></item><item><title>Edit Distance based RL for RNNT decoding</title><link>http://arxiv.org/abs/2306.01789v2</link><description>RNN-T is currently considered the industry standard in ASR due to itsexceptional WERs in various benchmark tests and its ability to support seamlessstreaming and longform transcription. However, its biggest drawback lies in thesignificant discrepancy between its training and inference objectives. Duringtraining, RNN-T maximizes all alignment probabilities by teacher forcing, whileduring inference, it uses beam search which may not necessarily find themaximum probable alignment. Additionally, RNN-T's inability to experiencemistakes during teacher forcing training makes it more problematic when amistake occurs in inference. To address this issue, this paper proposes aReinforcement Learning method that minimizes the gap between training andinference time. Our Edit Distance based RL (EDRL) approach computes rewardsbased on the edit distance, and trains the network at every action level. Theproposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-Tmodel.</description><author>Dongseong Hwang, Changwan Ryu, Khe Chai Sim</author><pubDate>Fri, 14 Jul 2023 17:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01789v2</guid></item><item><title>Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition</title><link>http://arxiv.org/abs/2307.07469v1</link><description>Recognizing interactive action plays an important role in human-robotinteraction and collaboration. Previous methods use late fusion andco-attention mechanism to capture interactive relations, which have limitedlearning capability or inefficiency to adapt to more interacting entities. Withassumption that priors of each entity are already known, they also lackevaluations on a more general setting addressing the diversity of subjects. Toaddress these problems, we propose an Interactive Spatiotemporal TokenAttention Network (ISTA-Net), which simultaneously model spatial, temporal, andinteractive relations. Specifically, our network contains a tokenizer topartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way torepresent motions of multiple diverse entities. By extending the entitydimension, ISTs provide better interactive representations. To jointly learnalong three dimensions in ISTs, multi-head self-attention blocks integratedwith 3D convolutions are designed to capture inter-token correlations. Whenmodeling correlations, a strict entity ordering is usually irrelevant forrecognizing interactive actions. To this end, Entity Rearrangement is proposedto eliminate the orderliness in ISTs for interchangeable entities. Extensiveexperiments on four datasets verify the effectiveness of ISTA-Net byoutperforming state-of-the-art methods. Our code is publicly available athttps://github.com/Necolizer/ISTA-Net</description><author>Yuhang Wen, Zixuan Tang, Yunsheng Pang, Beichen Ding, Mengyuan Liu</author><pubDate>Fri, 14 Jul 2023 17:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07469v1</guid></item><item><title>A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization</title><link>http://arxiv.org/abs/2307.01946v2</link><description>The electrocardiogram (ECG) is an accurate and widely available tool fordiagnosing cardiovascular diseases. ECGs have been recorded in printed formatsfor decades and their digitization holds great potential for training machinelearning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are atrisk of deterioration and scanning printed ECGs alone is insufficient, as MLmodels require ECG time-series data. Therefore, the digitization and conversionof paper ECG archives into time-series data is of utmost importance. Deeplearning models for image processing show promise in this regard. However, thescarcity of ECG archives with reference time-series is a challenge. Dataaugmentation techniques utilizing \textit{digital twins} present a potentialsolution. We introduce a novel method for generating synthetic ECG images on standardpaper-like ECG backgrounds with realistic artifacts. Distortions includinghandwritten text artifacts, wrinkles, creases and perspective transforms areapplied to the generated images, without personally identifiable information.As a use case, we generated an ECG image dataset of 21,801 records from the12-lead PhysioNet PTB-XL ECG time-series dataset. A deep ECG image digitizationmodel was built and trained on the synthetic dataset, and was employed toconvert the synthetic images to time-series data for evaluation. Thesignal-to-noise ratio (SNR) was calculated to assess the image digitizationquality vs the ground truth ECG time-series. The results show an average signalrecovery SNR of 27$\pm$2.8\,dB, demonstrating the significance of the proposedsynthetic ECG image dataset for training deep learning models. The codebase isavailable as an open-access toolbox for ECG research.</description><author>Kshama Kodthalu Shivashankara, Afagh Mehri Shervedani, Reza Sameni</author><pubDate>Fri, 14 Jul 2023 17:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01946v2</guid></item><item><title>Structured Pruning of Neural Networks for Constraints Learning</title><link>http://arxiv.org/abs/2307.07457v1</link><description>In recent years, the integration of Machine Learning (ML) models withOperation Research (OR) tools has gained popularity across diverseapplications, including cancer treatment, algorithmic configuration, andchemical process optimization. In this domain, the combination of ML and ORoften relies on representing the ML model output using Mixed IntegerProgramming (MIP) formulations. Numerous studies in the literature havedeveloped such formulations for many ML predictors, with a particular emphasison Artificial Neural Networks (ANNs) due to their significant interest in manyapplications. However, ANNs frequently contain a large number of parameters,resulting in MIP formulations that are impractical to solve, thereby impedingscalability. In fact, the ML community has already introduced severaltechniques to reduce the parameter count of ANNs without compromising theirperformance, since the substantial size of modern ANNs presents challenges forML applications as it significantly impacts computational efforts duringtraining and necessitates significant memory resources for storage. In thispaper, we showcase the effectiveness of pruning, one of these techniques, whenapplied to ANNs prior to their integration into MIPs. By pruning the ANN, weachieve significant improvements in the speed of the solution process. Wediscuss why pruning is more suitable in this context compared to other MLcompression techniques, and we identify the most appropriate pruningstrategies. To highlight the potential of this approach, we conduct experimentsusing feed-forward neural networks with multiple layers to constructadversarial examples. Our results demonstrate that pruning offers remarkablereductions in solution times without hindering the quality of the finaldecision, enabling the resolution of previously unsolvable instances.</description><author>Matteo Cacciola, Antonio Frangioni, Andrea Lodi</author><pubDate>Fri, 14 Jul 2023 17:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07457v1</guid></item><item><title>Generative adversarial networks for data-scarce spectral applications</title><link>http://arxiv.org/abs/2307.07454v1</link><description>Generative adversarial networks (GANs) are one of the most robust andversatile techniques in the field of generative artificial intelligence. Inthis work, we report on an application of GANs in the domain of syntheticspectral data generation, offering a solution to the scarcity of data found invarious scientific contexts. We demonstrate the proposed approach by applyingit to an illustrative problem within the realm of near-field radiative heattransfer involving a multilayered hyperbolic metamaterial. We find that asuccessful generation of spectral data requires two modifications toconventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoidmode collapse, and, (ii) the conditioning of WGANs to obtain accurate labelsfor the generated data. We show that a simple feed-forward neural network(FFNN), when augmented with data generated by a CWGAN, enhances significantlyits performance under conditions of limited data availability, demonstratingthe intrinsic value of CWGAN data augmentation beyond simply providing largerdatasets. In addition, we show that CWGANs can act as a surrogate model withimproved performance in the low-data regime with respect to simple FFNNs.Overall, this work highlights the potential of generative machine learningalgorithms in scientific applications beyond image generation and optimization.</description><author>Juan José García-Esteban, Juan Carlos Cuevas, Jorge Bravo-Abad</author><pubDate>Fri, 14 Jul 2023 17:27:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07454v1</guid></item><item><title>Differentially Private Clustering in Data Streams</title><link>http://arxiv.org/abs/2307.07449v1</link><description>The streaming model is an abstraction of computing over massive data streams,which is a popular way of dealing with large-scale modern data analysis. Inthis model, there is a stream of data points, one after the other. A streamingalgorithm is only allowed one pass over the data stream, and the goal is toperform some analysis during the stream while using as small space as possible. Clustering problems (such as $k$-means and $k$-median) are fundamentalunsupervised machine learning primitives, and streaming clustering algorithmshave been extensively studied in the past. However, since data privacy becomesa central concern in many real-world applications, non-private clusteringalgorithms are not applicable in many scenarios. In this work, we provide the first differentially private streamingalgorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclideandata points over a stream with length at most $T$ using $poly(k,d,\log(T))$space to achieve a {\it constant} multiplicative error and a$poly(k,d,\log(T))$ additive error. In particular, we present a differentiallyprivate streaming clustering framework which only requires an offline DPcoreset algorithm as a blackbox. By plugging in existing DP coreset results viaGhazi, Kumar, Manurangsi 2020 and Kaplan, Stemmer 2018, we achieve (1) a$(1+\gamma)$-multiplicative approximation with$\tilde{O}_\gamma(poly(k,d,\log(T)))$ space for any $\gamma&gt;0$, and theadditive error is $poly(k,d,\log(T))$ or (2) an $O(1)$-multiplicativeapproximation with $\tilde{O}(k \cdot poly(d,\log(T)))$ space and$poly(k,d,\log(T))$ additive error. In addition, our algorithmic framework is also differentially private underthe continual release setting, i.e., the union of outputs of our algorithms atevery timestamp is always differentially private.</description><author>Alessandro Epasto, Tamalika Mukherjee, Peilin Zhong</author><pubDate>Fri, 14 Jul 2023 17:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07449v1</guid></item><item><title>I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors</title><link>http://arxiv.org/abs/2305.14724v2</link><description>Visual metaphors are powerful rhetorical devices used to persuade orcommunicate creative ideas through images. Similar to linguistic metaphors,they convey meaning implicitly through symbolism and juxtaposition of thesymbols. We propose a new task of generating visual metaphors from linguisticmetaphors. This is a challenging task for diffusion-based text-to-image models,such as DALL$\cdot$E 2, since it requires the ability to model implicit meaningand compositionality. We propose to solve the task through the collaborationbetween Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3(davinci-002) with Chain-of-Thought prompting generates text that represents avisual elaboration of the linguistic metaphor containing the implicit meaningand relevant objects, which is then used as input to the diffusion-basedtext-to-image models.Using a human-AI collaboration framework, where humansinteract both with the LLM and the top-performing diffusion model, we create ahigh-quality dataset containing 6,476 visual metaphors for 1,540 linguisticmetaphors and their associated visual elaborations. Evaluation by professionalillustrators shows the promise of LLM-Diffusion Model collaboration for thistask . To evaluate the utility of our Human-AI collaboration framework and thequality of our dataset, we perform both an intrinsic human-based evaluation andan extrinsic evaluation using visual entailment as a downstream task.</description><author>Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan</author><pubDate>Fri, 14 Jul 2023 17:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14724v2</guid></item><item><title>Can Large Language Models Empower Molecular Property Prediction?</title><link>http://arxiv.org/abs/2307.07443v1</link><description>Molecular property prediction has gained significant attention due to itstransformative potential in multiple scientific disciplines. Conventionally, amolecule graph can be represented either as a graph-structured data or a SMILEStext. Recently, the rapid development of Large Language Models (LLMs) hasrevolutionized the field of NLP. Although it is natural to utilize LLMs toassist in understanding molecules represented by SMILES, the exploration of howLLMs will impact molecular property prediction is still in its early stage. Inthis work, we advance towards this objective through two perspectives:zero/few-shot molecular classification, and using the new explanationsgenerated by LLMs as representations of molecules. To be specific, we firstprompt LLMs to do in-context molecular classification and evaluate theirperformance. After that, we employ LLMs to generate semantically enrichedexplanations for the original SMILES and then leverage that to fine-tune asmall-scale LM model for multiple downstream tasks. The experimental resultshighlight the superiority of text explanations as molecular representationsacross multiple benchmark datasets, and confirm the immense potential of LLMsin molecular property prediction tasks. Codes are available at\url{https://github.com/ChnQ/LLM4Mol}.</description><author>Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu</author><pubDate>Fri, 14 Jul 2023 17:06:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07443v1</guid></item><item><title>Atlas-Based Interpretable Age Prediction</title><link>http://arxiv.org/abs/2307.07439v1</link><description>Age prediction is an important part of medical assessments and research. Itcan aid in detecting diseases as well as abnormal ageing by highlighting thediscrepancy between chronological and biological age. To gain a comprehensiveunderstanding of age-related changes observed in various body parts, weinvestigate them on a larger scale by using whole-body images. We utilise theGrad-CAM interpretability method to determine the body areas most predictive ofa person's age. We expand our analysis beyond individual subjects by employingregistration techniques to generate population-wide interpretability maps.Furthermore, we set state-of-the-art whole-body age prediction with a modelthat achieves a mean absolute error of 2.76 years. Our findings reveal threeprimary areas of interest: the spine, the autochthonous back muscles, and thecardiac region, which exhibits the highest importance.</description><author>Sophie Starck, Yadunandan Vivekanand Kini, Jessica Johanna Maria Ritter, Rickmer Braren, Daniel Rueckert, Tamara Mueller</author><pubDate>Fri, 14 Jul 2023 17:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07439v1</guid></item><item><title>Towards spoken dialect identification of Irish</title><link>http://arxiv.org/abs/2307.07436v1</link><description>The Irish language is rich in its diversity of dialects and accents. Thiscompounds the difficulty of creating a speech recognition system for thelow-resource language, as such a system must contend with a high degree ofvariability with limited corpora. A recent study investigating dialect bias inIrish ASR found that balanced training corpora gave rise to unequal dialectperformance, with performance for the Ulster dialect being consistently worsethan for the Connacht or Munster dialects. Motivated by this, the presentexperiments investigate spoken dialect identification of Irish, with a view toincorporating such a system into the speech recognition pipeline. Two acousticclassification models are tested, XLS-R and ECAPA-TDNN, in conjunction with atext-based classifier using a pretrained Irish-language BERT model. TheECAPA-TDNN, particularly a model pretrained for language identification on theVoxLingua107 dataset, performed best overall, with an accuracy of 73%. This wasfurther improved to 76% by fusing the model's outputs with the text-basedmodel. The Ulster dialect was most accurately identified, with an accuracy of94%, however the model struggled to disambiguate between the Connacht andMunster dialects, suggesting a more nuanced approach may be necessary torobustly distinguish between the dialects of Irish.</description><author>Liam Lonergan, Mengjie Qian, Neasa Ní Chiaráin, Christer Gobl, Ailbhe Ní Chasaide</author><pubDate>Fri, 14 Jul 2023 17:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07436v1</guid></item><item><title>Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers</title><link>http://arxiv.org/abs/2303.16464v2</link><description>The generalization performance of deep neural networks with regard to theoptimization algorithm is one of the major concerns in machine learning. Thisperformance can be affected by various factors. In this paper, we theoreticallyprove that the Lipschitz constant of a loss function is an important factor todiminish the generalization error of the output model obtained by Adam orAdamW. The results can be used as a guideline for choosing the loss functionwhen the optimization algorithm is Adam or AdamW. In addition, to evaluate thetheoretical bound in a practical setting, we choose the human age estimationproblem in computer vision. For assessing the generalization better, thetraining and test datasets are drawn from different distributions. Ourexperimental evaluation shows that the loss function with a lower Lipschitzconstant and maximum value improves the generalization of the model trained byAdam or AdamW.</description><author>Mohammad Lashkari, Amin Gheibi</author><pubDate>Fri, 14 Jul 2023 17:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16464v2</guid></item><item><title>Dink-Net: Neural Clustering on Large Graphs</title><link>http://arxiv.org/abs/2305.18405v3</link><description>Deep graph clustering, which aims to group the nodes of a graph into disjointclusters with deep neural networks, has achieved promising progress in recentyears. However, the existing methods fail to scale to the large graph withmillion nodes. To solve this problem, a scalable deep graph clustering method(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, bydiscriminating nodes, whether being corrupted by augmentations, representationsare learned in a self-supervised manner. Meanwhile, the cluster centres areinitialized as learnable neural parameters. Subsequently, the clusteringdistribution is optimized by minimizing the proposed cluster dilation loss andcluster shrink loss in an adversarial manner. By these settings, we unify thetwo-step clustering, i.e., representation learning and clustering optimization,into an end-to-end framework, guiding the network to learn clustering-friendlyfeatures. Besides, Dink-Net scales well to large graphs since the designed lossfunctions adopt the mini-batch data to optimize the clustering distributioneven without performance drops. Both experimental results and theoreticalanalyses demonstrate the superiority of our method. Compared to the runner-up,Dink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111million nodes and 1.6 billion edges. The source code is released athttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,and datasets) of deep graph clustering is shared athttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering.</description><author>Yue Liu, Ke Liang, Jun Xia, Sihang Zhou, Xihong Yang, Xinwang Liu, Stan Z. Li</author><pubDate>Fri, 14 Jul 2023 17:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18405v3</guid></item><item><title>Combining multitemporal optical and SAR data for LAI imputation with BiLSTM network</title><link>http://arxiv.org/abs/2307.07434v1</link><description>The Leaf Area Index (LAI) is vital for predicting winter wheat yield.Acquisition of crop conditions via Sentinel-2 remote sensing images can behindered by persistent clouds, affecting yield predictions. Synthetic ApertureRadar (SAR) provides all-weather imagery, and the ratio between its cross- andco-polarized channels (C-band) shows a high correlation with time series LAIover winter wheat regions. This study evaluates the use of time seriesSentinel-1 VH/VV for LAI imputation, aiming to increase spatial-temporaldensity. We utilize a bidirectional LSTM (BiLSTM) network to impute time seriesLAI and use half mean squared error for each time step as the loss function. Wetrained models on data from southern Germany and the North China Plain usingonly LAI data generated by Sentinel-1 VH/VV and Sentinel-2. Experimentalresults show BiLSTM outperforms traditional regression methods, capturingnonlinear dynamics between multiple time series. It proves robust in variousgrowing conditions and is effective even with limited Sentinel-2 images.BiLSTM's performance surpasses that of LSTM, particularly over the senescenceperiod. Therefore, BiLSTM can be used to impute LAI with time-series Sentinel-1VH/VV and Sentinel-2 data, and this method could be applied to othertime-series imputation issues.</description><author>W. Zhao, F. Yin, H. Ma, Q. Wu, J. Gomez-Dans, P. Lewis</author><pubDate>Fri, 14 Jul 2023 16:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07434v1</guid></item><item><title>UNITE: A Unified Benchmark for Text-to-SQL Evaluation</title><link>http://arxiv.org/abs/2305.16265v3</link><description>A practical text-to-SQL system should generalize well on a wide variety ofnatural language questions, unseen database schemas, and novel SQL querystructures. To comprehensively evaluate text-to-SQL systems, we introduce aUNIfied benchmark for Text-to-SQL Evaluation (UNITE). It is composed ofpublicly available text-to-SQL datasets, containing natural language questionsfrom more than 12 domains, SQL queries from more than 3.9K patterns, and 29Kdatabases. Compared to the widely used Spider benchmark, we introduce$\sim$120K additional examples and a threefold increase in SQL patterns, suchas comparative and boolean questions. We conduct a systematic study of sixstate-of-the-art (SOTA) text-to-SQL parsers on our new benchmark and show that:1) Codex performs surprisingly well on out-of-domain datasets; 2) speciallydesigned decoding methods (e.g. constrained beam search) can improveperformance for both in-domain and out-of-domain settings; 3) explicitlymodeling the relationship between questions and schemas further improves theSeq2Seq models. More importantly, our benchmark presents key challenges towardscompositional generalization and robustness issues -- which these SOTA modelscannot address well. Our code and data processing script are available athttps://github.com/awslabs/unified-text2sql-benchmark</description><author>Wuwei Lan, Zhiguo Wang, Anuj Chauhan, Henghui Zhu, Alexander Li, Jiang Guo, Sheng Zhang, Chung-Wei Hang, Joseph Lilien, Yiqun Hu, Lin Pan, Mingwen Dong, Jun Wang, Jiarong Jiang, Stephen Ash, Vittorio Castelli, Patrick Ng, Bing Xiang</author><pubDate>Fri, 14 Jul 2023 16:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16265v3</guid></item><item><title>On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach</title><link>http://arxiv.org/abs/2010.01079v6</link><description>We analyze statistical discrimination in hiring markets using a multi-armedbandit model. Myopic firms face workers arriving with heterogeneous observablecharacteristics. The association between the worker's skill and characteristicsis unknown ex ante; thus, firms need to learn it. Laissez-faire causesperpetual underestimation: minority workers are rarely hired, and therefore,the underestimation tends to persist. Even a marginal imbalance in thepopulation ratio frequently results in perpetual underestimation. We proposetwo policy solutions: a novel subsidy rule (the hybrid mechanism) and theRooney Rule. Our results indicate that temporary affirmative actionseffectively alleviate discrimination stemming from insufficient data.</description><author>Junpei Komiyama, Shunya Noda</author><pubDate>Fri, 14 Jul 2023 16:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2010.01079v6</guid></item><item><title>Exploiting Counter-Examples for Active Learning with Partial labels</title><link>http://arxiv.org/abs/2307.07413v1</link><description>This paper studies a new problem, \emph{active learning with partial labels}(ALPL). In this setting, an oracle annotates the query samples with partiallabels, relaxing the oracle from the demanding accurate labeling process. Toaddress ALPL, we first build an intuitive baseline that can be seamlesslyincorporated into existing AL frameworks. Though effective, this baseline isstill susceptible to the \emph{overfitting}, and falls short of therepresentative partial-label-based samples during the query process. Drawinginspiration from human inference in cognitive science, where accurateinferences can be explicitly derived from \emph{counter-examples} (CEs), ourobjective is to leverage this human-like learning pattern to tackle the\emph{overfitting} while enhancing the process of selecting representativesamples in ALPL. Specifically, we construct CEs by reversing the partial labelsfor each instance, and then we propose a simple but effective WorseNet todirectly learn from this complementary pattern. By leveraging the distributiongap between WorseNet and the predictor, this adversarial evaluation mannercould enhance both the performance of the predictor itself and the sampleselection process, allowing the predictor to capture more accurate patterns inthe data. Experimental results on five real-world datasets and four benchmarkdatasets show that our proposed method achieves comprehensive improvements overten representative AL frameworks, highlighting the superiority of WorseNet. Thesource code will be available at \url{https://github.com/Ferenas/APLL}.</description><author>Fei Zhang, Yunjie Ye, Lei Feng, Zhongwen Rao, Jieming Zhu, Marcus Kalander, Chen Gong, Jianye Hao, Bo Han</author><pubDate>Fri, 14 Jul 2023 16:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07413v1</guid></item><item><title>HuCurl: Human-induced Curriculum Discovery</title><link>http://arxiv.org/abs/2307.07412v1</link><description>We introduce the problem of curriculum discovery and describe a curriculumlearning framework capable of discovering effective curricula in a curriculumspace based on prior knowledge about sample difficulty. Using annotationentropy and loss as measures of difficulty, we show that (i): thetop-performing discovered curricula for a given model and dataset are oftennon-monotonic as opposed to monotonic curricula in existing literature, (ii):the prevailing easy-to-hard or hard-to-easy transition curricula are often atthe risk of underperforming, and (iii): the curricula discovered for smallerdatasets and models perform well on larger datasets and models respectively.The proposed framework encompasses some of the existing curriculum learningapproaches and can discover curricula that outperform them across several NLPtasks.</description><author>Mohamed Elgaar, Hadi Amiri</author><pubDate>Fri, 14 Jul 2023 16:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07412v1</guid></item><item><title>Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs</title><link>http://arxiv.org/abs/2305.03642v2</link><description>Results from Randomized Controlled Trials (RCTs) establish the comparativeeffectiveness of interventions, and are in turn critical inputs forevidence-based care. However, results from RCTs are presented in (oftenunstructured) natural language articles describing the design, execution, andoutcomes of trials; clinicians must manually extract findings pertaining tointerventions and outcomes of interest from such articles. This onerous manualprocess has motivated work on (semi-)automating extraction of structuredevidence from trial reports. In this work we propose and evaluate atext-to-text model built on instruction-tuned Large Language Models (LLMs) tojointly extract Interventions, Outcomes, and Comparators (ICO elements) fromclinical abstracts, and infer the associated results reported. Manual (expert)and automated evaluations indicate that framing evidence extraction as aconditional generation task and fine-tuning LLMs for this purpose realizesconsiderable ($\sim$20 point absolute F1 score) gains over the previous SOTA.We perform ablations and error analyses to assess aspects that contribute tomodel performance, and to highlight potential directions for furtherimprovements. We apply our model to a collection of published RCTs throughmid-2022, and release a searchable database of structured findings:bit.ly/joint-relations-extraction-mlhc</description><author>Somin Wadhwa, Jay DeYoung, Benjamin Nye, Silvio Amir, Byron C. Wallace</author><pubDate>Fri, 14 Jul 2023 16:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03642v2</guid></item><item><title>Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise</title><link>http://arxiv.org/abs/2307.07406v1</link><description>We propose an improved convergence analysis technique that characterizes thedistributed learning paradigm of federated learning (FL) with imperfect/noisyuplink and downlink communications. Such imperfect communication scenariosarise in the practical deployment of FL in emerging communication systems andprotocols. The analysis developed in this paper demonstrates, for the firsttime, that there is an asymmetry in the detrimental effects of uplink anddownlink communications in FL. In particular, the adverse effect of thedownlink noise is more severe on the convergence of FL algorithms. Using thisinsight, we propose improved Signal-to-Noise (SNR) control strategies that,discarding the negligible higher-order terms, lead to a similar convergencerate for FL as in the case of a perfect, noise-free communication channel whileincurring significantly less power resources compared to existing solutions. Inparticular, we establish that to maintain the $O(\frac{1}{\sqrt{K}})$ rate ofconvergence like in the case of noise-free FL, we need to scale down the uplinkand downlink noise by $\Omega({\sqrt{k}})$ and $\Omega({k})$ respectively,where $k$ denotes the communication round, $k=1,\dots, K$. Our theoreticalresult is further characterized by two major benefits: firstly, it does notassume the somewhat unrealistic assumption of bounded client dissimilarity, andsecondly, it only requires smooth non-convex loss functions, a function classbetter suited for modern machine learning and deep learning models. We alsoperform extensive empirical analysis to verify the validity of our theoreticalfindings.</description><author>Antesh Upadhyay, Abolfazl Hashemi</author><pubDate>Fri, 14 Jul 2023 16:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07406v1</guid></item><item><title>Performance of $\ell_1$ Regularization for Sparse Convex Optimization</title><link>http://arxiv.org/abs/2307.07405v1</link><description>Despite widespread adoption in practice, guarantees for the LASSO and GroupLASSO are strikingly lacking in settings beyond statistical problems, and thesealgorithms are usually considered to be a heuristic in the context of sparseconvex optimization on deterministic inputs. We give the first recoveryguarantees for the Group LASSO for sparse convex optimization withvector-valued features. We show that if a sufficiently large Group LASSOregularization is applied when minimizing a strictly convex function $l$, thenthe minimizer is a sparse vector supported on vector-valued features with thelargest $\ell_2$ norm of the gradient. Thus, repeating this procedure selectsthe same set of features as the Orthogonal Matching Pursuit algorithm, whichadmits recovery guarantees for any function $l$ with restricted strongconvexity and smoothness via weak submodularity arguments. This answers openquestions of Tibshirani et al. and Yasuda et al. Our result is the first totheoretically explain the empirical success of the Group LASSO for convexfunctions under general input instances assuming only restricted strongconvexity and smoothness. Our result also generalizes provable guarantees forthe Sequential Attention algorithm, which is a feature selection algorithminspired by the attention mechanism proposed by Yasuda et al. As an application of our result, we give new results for the column subsetselection problem, which is well-studied when the loss is the Frobenius norm orother entrywise matrix losses. We give the first result for general lossfunctions for this problem that requires only restricted strong convexity andsmoothness.</description><author>Kyriakos Axiotis, Taisuke Yasuda</author><pubDate>Fri, 14 Jul 2023 16:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07405v1</guid></item><item><title>PC-Droid: Faster diffusion and improved quality for particle cloud generation</title><link>http://arxiv.org/abs/2307.06836v2</link><description>Building on the success of PC-JeDi we introduce PC-Droid, a substantiallyimproved diffusion model for the generation of jet particle clouds. Byleveraging a new diffusion formulation, studying more recent integrationsolvers, and training on all jet types simultaneously, we are able to achievestate-of-the-art performance for all types of jets across all evaluationmetrics. We study the trade-off between generation speed and quality bycomparing two attention based architectures, as well as the potential ofconsistency distillation to reduce the number of diffusion steps. Both thefaster architecture and consistency models demonstrate performance surpassingmany competing models, with generation time up to two orders of magnitudefaster than PC-JeDi.</description><author>Matthew Leigh, Debajyoti Sengupta, John Andrew Raine, Guillaume Quétant, Tobias Golling</author><pubDate>Fri, 14 Jul 2023 16:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06836v2</guid></item><item><title>Parametric Information Maximization for Generalized Category Discovery</title><link>http://arxiv.org/abs/2212.00334v3</link><description>We introduce a Parametric Information Maximization (PIM) model for theGeneralized Category Discovery (GCD) problem. Specifically, we propose abi-level optimization formulation, which explores a parameterized family ofobjective functions, each evaluating a weighted mutual information between thefeatures and the latent labels, subject to supervision constraints from thelabeled samples. Our formulation mitigates the class-balance bias encoded instandard information maximization approaches, thereby handling effectively bothshort-tailed and long-tailed data sets. We report extensive experiments andcomparisons demonstrating that our PIM model consistently sets newstate-of-the-art performances in GCD across six different datasets, more sowhen dealing with challenging fine-grained problems.</description><author>Florent Chiaroni, Jose Dolz, Ziko Imtiaz Masud, Amar Mitiche, Ismail Ben Ayed</author><pubDate>Fri, 14 Jul 2023 16:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00334v3</guid></item><item><title>Improving Zero-Shot Generalization for CLIP with Synthesized Prompts</title><link>http://arxiv.org/abs/2307.07397v1</link><description>With the growing interest in pretrained vision-language models like CLIP,recent research has focused on adapting these models to downstream tasks.Despite achieving promising results, most existing methods require labeled datafor all classes, which may not hold in real-world applications due to the longtail and Zipf's law. For example, some classes may lack labeled data entirely,such as emerging concepts. To address this problem, we propose a plug-and-playgenerative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed\textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods.Specifically, we follow variational autoencoders to introduce a generator thatreconstructs the visual features by inputting the synthesized prompts and thecorresponding class names to the textual encoder of CLIP. In this manner, weeasily obtain the synthesized features for the remaining label-only classes.Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeledand synthesized features. Extensive experiments on base-to-new generalization,cross-dataset transfer learning, and generalized zero-shot learning demonstratethe superiority of our approach. The code is available at\url{https://github.com/mrflogs/SHIP}.</description><author>Zhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang, Tieniu Tan</author><pubDate>Fri, 14 Jul 2023 16:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07397v1</guid></item><item><title>Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations</title><link>http://arxiv.org/abs/2307.07396v1</link><description>Finding (bi-)clusters in bipartite graphs is a popular data analysisapproach. Analysts typically want to visualize the clusters, which is simple aslong as the clusters are disjoint. However, many modern algorithms findoverlapping clusters, making visualization more complicated. In this paper, westudy the problem of visualizing \emph{a given clustering} of overlappingclusters in bipartite graphs and the related problem of visualizing BooleanMatrix Factorizations. We conceptualize three different objectives that anygood visualization should satisfy: (1) proximity of cluster elements, (2) largeconsecutive areas of elements from the same cluster, and (3) largeuninterrupted areas in the visualization, regardless of the cluster membership.We provide objective functions that capture these goals and algorithms thatoptimize these objective functions. Interestingly, in experiments on real-worlddatasets, we find that the best trade-off between these competing goals isachieved by a novel heuristic, which locally aims to place rows and columnswith similar cluster membership next to each other.</description><author>Thibault Marette, Pauli Miettinen, Stefan Neumann</author><pubDate>Fri, 14 Jul 2023 16:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07396v1</guid></item><item><title>L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning</title><link>http://arxiv.org/abs/2307.07393v1</link><description>The ubiquity of camera-enabled devices has led to large amounts of unlabeledimage data being produced at the edge. The integration of self-supervisedlearning (SSL) and federated learning (FL) into one coherent system canpotentially offer data privacy guarantees while also advancing the quality androbustness of the learned visual representations without needing to move dataaround. However, client bias and divergence during FL aggregation caused bydata heterogeneity limits the performance of learned visual representations ondownstream tasks. In this paper, we propose a new aggregation strategy termedLayer-wise Divergence Aware Weight Aggregation (L-DAWA) to mitigate theinfluence of client bias and divergence during FL aggregation. The proposedmethod aggregates weights at the layer-level according to the measure ofangular divergence between the clients' model and the global model. Extensiveexperiments with cross-silo and cross-device settings on CIFAR-10/100 and TinyImageNet datasets demonstrate that our methods are effective and obtain newSOTA performance on both contrastive and non-contrastive SSL approaches.</description><author>Yasar Abbas Ur Rehman, Yan Gao, Pedro Porto Buarque de Gusmão, Mina Alibeigi, Jiajun Shen, Nicholas D. Lane</author><pubDate>Fri, 14 Jul 2023 16:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07393v1</guid></item><item><title>Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach</title><link>http://arxiv.org/abs/2307.07392v1</link><description>With the increasing need for text summarization techniques that are bothefficient and accurate, it becomes crucial to explore avenues that enhance thequality and precision of pre-trained models specifically tailored forsummarizing Bengali texts. When it comes to text summarization tasks, there arenumerous pre-trained transformer models at one's disposal. Consequently, itbecomes quite a challenge to discern the most informative and relevant summaryfor a given text among the various options generated by these pre-trainedsummarization models. This paper aims to identify the most accurate andinformative summary for a given text by utilizing a simple but effectiveranking-based approach that compares the output of four different pre-trainedBengali text summarization models. The process begins by carrying outpreprocessing of the input text that involves eliminating unnecessary elementssuch as special characters and punctuation marks. Next, we utilize fourpre-trained summarization models to generate summaries, followed by applying atext ranking algorithm to identify the most suitable summary. Ultimately, thesummary with the highest ranking score is chosen as the final one. To evaluatethe effectiveness of this approach, the generated summaries are comparedagainst human-annotated summaries using standard NLG metrics such as BLEU,ROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that byleveraging the strengths of each pre-trained transformer model and combiningthem using a ranking-based approach, our methodology significantly improves theaccuracy and effectiveness of the Bengali text summarization.</description><author>G. M. Shahariar, Tonmoy Talukder, Rafin Alam Khan Sotez, Md. Tanvir Rouf Shawon</author><pubDate>Fri, 14 Jul 2023 16:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07392v1</guid></item><item><title>Approximating the Shapley Value without Marginal Contributions</title><link>http://arxiv.org/abs/2302.00736v3</link><description>The Shapley value is arguably the most popular approach for assigning ameaningful contribution value to players in a cooperative game, which hasrecently been used intensively in explainable artificial intelligence. Themeaningfulness is due to axiomatic properties that only the Shapley valuesatisfies, which, however, comes at the expense of an exact computation growingexponentially with the number of agents. Accordingly, a number of works aredevoted to the efficient approximation of the Shapley values, most of themrevolve around the notion of an agent's marginal contribution. In this paper,we propose with SVARM and Stratified SVARM two parameter-free anddomain-independent approximation algorithms based on a representation of theShapley value detached from the notion of marginal contributions. We proveunmatched theoretical guarantees regarding their approximation quality andprovide empirical results including synthetic games as well as commonexplainability use cases comparing ourselves with state-of-the-art methods.</description><author>Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, Eyke Hüllermeier</author><pubDate>Fri, 14 Jul 2023 16:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00736v3</guid></item><item><title>Learning Sparse Neural Networks with Identity Layers</title><link>http://arxiv.org/abs/2307.07389v1</link><description>The sparsity of Deep Neural Networks is well investigated to maximize theperformance and reduce the size of overparameterized networks as possible.Existing methods focus on pruning parameters in the training process by usingthresholds and metrics. Meanwhile, feature similarity between different layershas not been discussed sufficiently before, which could be rigorously proved tobe highly correlated to the network sparsity in this paper. Inspired byinterlayer feature similarity in overparameterized models, we investigate theintrinsic link between network sparsity and interlayer feature similarity.Specifically, we prove that reducing interlayer feature similarity based onCentered Kernel Alignment (CKA) improves the sparsity of the network by usinginformation bottleneck theory. Applying such theory, we propose a plug-and-playCKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR,which utilizes CKA to reduce feature similarity between layers and increasenetwork sparsity. In other words, layers of our sparse network tend to havetheir own identity compared to each other. Experimentally, we plug the proposedCKA-SR into the training process of sparse network training methods and findthat CKA-SR consistently improves the performance of several State-Of-The-Artsparse training methods, especially at extremely high sparsity. Code isincluded in the supplementary materials.</description><author>Mingjian Ni, Guangyao Chen, Xiawu Zheng, Peixi Peng, Li Yuan, Yonghong Tian</author><pubDate>Fri, 14 Jul 2023 15:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07389v1</guid></item><item><title>Open-Vocabulary Affordance Detection in 3D Point Clouds</title><link>http://arxiv.org/abs/2303.02401v3</link><description>Affordance detection is a challenging problem with a wide variety of roboticapplications. Traditional affordance detection methods are limited to apredefined set of affordance labels, hence potentially restricting theadaptability of intelligent robots in complex and dynamic environments. In thispaper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,which is capable of detecting an unbounded number of affordances in 3D pointclouds. By simultaneously learning the affordance text and the point feature,OpenAD successfully exploits the semantic relationships between affordances.Therefore, our proposed method enables zero-shot detection and can be able todetect previously unseen affordances without a single annotation example.Intensive experimental results show that OpenAD works effectively on a widerange of affordance detection setups and outperforms other baselines by a largemargin. Additionally, we demonstrate the practicality of the proposed OpenAD inreal-world robotic applications with a fast inference speed (~100ms). Ourproject is available at https://openad2023.github.io.</description><author>Toan Nguyen, Minh Nhat Vu, An Vuong, Dzung Nguyen, Thieu Vo, Ngan Le, Anh Nguyen</author><pubDate>Fri, 14 Jul 2023 15:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02401v3</guid></item><item><title>Hypothesis Transfer Learning with Surrogate Classification Losses: Generalization Bounds through Algorithmic Stability</title><link>http://arxiv.org/abs/2305.19694v2</link><description>Hypothesis transfer learning (HTL) contrasts domain adaptation by allowingfor a previous task leverage, named the source, into a new one, the target,without requiring access to the source data. Indeed, HTL relies only on ahypothesis learnt from such source data, relieving the hurdle of expansive datastorage and providing great practical benefits. Hence, HTL is highly beneficialfor real-world applications relying on big data. The analysis of such a methodfrom a theoretical perspective faces multiple challenges, particularly inclassification tasks. This paper deals with this problem by studying thelearning theory of HTL through algorithmic stability, an attractive theoreticalframework for machine learning algorithms analysis. In particular, we areinterested in the statistical behaviour of the regularized empirical riskminimizers in the case of binary classification. Our stability analysisprovides learning guarantees under mild assumptions. Consequently, we deriveseveral complexity-free generalization bounds for essential statisticalquantities like the training error, the excess risk and cross-validationestimates. These refined bounds allow understanding the benefits of transferlearning and comparing the behaviour of standard losses in different scenarios,leading to valuable insights for practitioners.</description><author>Anass Aghbalou, Guillaume Staerman</author><pubDate>Fri, 14 Jul 2023 15:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19694v2</guid></item><item><title>Higher-order topological kernels via quantum computation</title><link>http://arxiv.org/abs/2307.07383v1</link><description>Topological data analysis (TDA) has emerged as a powerful tool for extractingmeaningful insights from complex data. TDA enhances the analysis of objects byembedding them into a simplicial complex and extracting useful globalproperties such as the Betti numbers, i.e. the number of multidimensionalholes, which can be used to define kernel methods that are easily integratedwith existing machine-learning algorithms. These kernel methods have foundbroad applications, as they rely on powerful mathematical frameworks whichprovide theoretical guarantees on their performance. However, the computationof higher-dimensional Betti numbers can be prohibitively expensive on classicalhardware, while quantum algorithms can approximate them in polynomial time inthe instance size. In this work, we propose a quantum approach to definingtopological kernels, which is based on constructing Betti curves, i.e.topological fingerprint of filtrations with increasing order. We exhibit aworking prototype of our approach implemented on a noiseless simulator and showits robustness by means of some empirical results suggesting that topologicalapproaches may offer an advantage in quantum machine learning.</description><author>Massimiliano Incudini, Francesco Martini, Alessandra Di Pierro</author><pubDate>Fri, 14 Jul 2023 15:48:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07383v1</guid></item><item><title>Composition-contrastive Learning for Sentence Embeddings</title><link>http://arxiv.org/abs/2307.07380v1</link><description>Vector representations of natural language are ubiquitous in searchapplications. Recently, various methods based on contrastive learning have beenproposed to learn textual representations from unlabelled data; by maximizingalignment between minimally-perturbed embeddings of the same text, andencouraging a uniform distribution of embeddings across a broader corpus.Differently, we propose maximizing alignment between texts and a composition oftheir phrasal constituents. We consider several realizations of this objectiveand elaborate the impact on representations in each case. Experimental resultson semantic textual similarity tasks show improvements over baselines that arecomparable with state-of-the-art approaches. Moreover, this work is the firstto do so without incurring costs in auxiliary training objectives or additionalnetwork parameters.</description><author>Sachin J. Chanchani, Ruihong Huang</author><pubDate>Fri, 14 Jul 2023 15:39:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07380v1</guid></item><item><title>Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing</title><link>http://arxiv.org/abs/2307.07378v1</link><description>The development of computer vision and in-situ monitoring using visualsensors allows the collection of large datasets from the additive manufacturing(AM) process. Such datasets could be used with machine learning techniques toimprove the quality of AM. This paper examines two scenarios: first, usingconvolutional neural networks (CNNs) to accurately classify defects in an imagedataset from AM and second, applying active learning techniques to thedeveloped classification model. This allows the construction of ahuman-in-the-loop mechanism to reduce the size of the data required to trainand generate training data.</description><author>Xiao Liu, Alessandra Mileo, Alan F. Smeaton</author><pubDate>Fri, 14 Jul 2023 15:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07378v1</guid></item><item><title>AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes</title><link>http://arxiv.org/abs/2307.07370v1</link><description>Image captioning is a significant field across computer vision and naturallanguage processing. We propose and present AIC-AB NET, a novelAttribute-Information-Combined Attention-Based Network that combines spatialattention architecture and text attributes in an encoder-decoder. For captiongeneration, adaptive spatial attention determines which image region bestrepresents the image and whether to attend to the visual features or the visualsentinel. Text attribute information is synchronously fed into the decoder tohelp image recognition and reduce uncertainty. We have tested and evaluated ourAICAB NET on the MS COCO dataset and a new proposed Fashion dataset. TheFashion dataset is employed as a benchmark of single-object images. The resultsshow the superior performance of the proposed model compared to thestate-of-the-art baseline and ablated models on both the images from MSCOCO andour single-object images. Our AIC-AB NET outperforms the baseline adaptiveattention network by 0.017 (CIDEr score) on the MS COCO dataset and 0.095(CIDEr score) on the Fashion dataset.</description><author>Guoyun Tu, Ying Liu, Vladimir Vlassov</author><pubDate>Fri, 14 Jul 2023 15:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07370v1</guid></item><item><title>Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis</title><link>http://arxiv.org/abs/2302.14460v2</link><description>Appendicitis is among the most frequent reasons for pediatric abdominalsurgeries. With recent advances in machine learning, data-driven decisionsupport could help clinicians diagnose and manage patients while reducing thenumber of non-critical surgeries. Previous decision support systems forappendicitis focused on clinical, laboratory, scoring and computed tomographydata, mainly ignoring abdominal ultrasound, a noninvasive and readily availablediagnostic modality. To this end, we developed and validated interpretablemachine learning models for predicting the diagnosis, management and severityof suspected appendicitis using ultrasound images. Our models were trained on adataset comprising 579 pediatric patients with 1709 ultrasound imagesaccompanied by clinical and laboratory data. Our methodological contribution isthe generalization of concept bottleneck models to prediction problems withmultiple views and incomplete concept sets. Notably, such models lendthemselves to interpretation and interaction via high-level conceptsunderstandable to clinicians without sacrificing performance or requiringtime-consuming image annotation when deployed.</description><author>Ričards Marcinkevičs, Patricia Reis Wolfertstetter, Ugne Klimiene, Kieran Chin-Cheong, Alyssia Paschke, Julia Zerres, Markus Denzinger, David Niederberger, Sven Wellmann, Ece Ozkan, Christian Knorr, Julia E. Vogt</author><pubDate>Fri, 14 Jul 2023 15:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14460v2</guid></item><item><title>Optimizing Data Augmentation Policy Through Random Unidimensional Search</title><link>http://arxiv.org/abs/2106.08756v4</link><description>It is no secret amongst deep learning researchers that finding the optimaldata augmentation strategy during training can mean the difference betweenstate-of-the-art performance and a run-of-the-mill result. To that end, thecommunity has seen many efforts to automate the process of finding the perfectaugmentation procedure for any task at hand. Unfortunately, even recentcutting-edge methods bring massive computational overhead, requiring as many as100 full model trainings to settle on an ideal configuration. We show how toachieve equivalent performance using just 6 trainings with RandomUnidimensional Augmentation. Source code is available athttps://github.com/fastestimator/RUA/tree/v1.0</description><author>Xiaomeng Dong, Michael Potter, Gaurav Kumar, Yun-Chan Tsai, V. Ratna Saripalli, Theodore Trafalis</author><pubDate>Fri, 14 Jul 2023 15:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.08756v4</guid></item><item><title>Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow</title><link>http://arxiv.org/abs/2307.07367v1</link><description>Large language models like ChatGPT efficiently provide users with informationabout various topics, presenting a potential substitute for searching the weband asking people for help online. But since users interact privately with themodel, these models may drastically reduce the amount of publicly availablehuman-generated data and knowledge resources. This substitution can present asignificant problem in securing training data for future models. In this work,we investigate how the release of ChatGPT changed human-generated open data onthe web by analyzing the activity on Stack Overflow, the leading online Q\&amp;Aplatform for computer programming. We find that relative to its Russian andChinese counterparts, where access to ChatGPT is limited, and to similar forumsfor mathematics, where ChatGPT is less capable, activity on Stack Overflowsignificantly decreased. A difference-in-differences model estimates a 16\%decrease in weekly posts on Stack Overflow. This effect increases in magnitudeover time, and is larger for posts related to the most widely used programminglanguages. Posts made after ChatGPT get similar voting scores than before,suggesting that ChatGPT is not merely displacing duplicate or low-qualitycontent. These results suggest that more users are adopting large languagemodels to answer questions and they are better substitutes for Stack Overflowfor languages for which they have more training data. Using models like ChatGPTmay be more efficient for solving certain programming problems, but itswidespread adoption and the resulting shift away from public exchange on theweb will limit the open data people and models can learn from in the future.</description><author>Maria del Rio-Chanona, Nadzeya Laurentsyeva, Johannes Wachs</author><pubDate>Fri, 14 Jul 2023 15:22:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07367v1</guid></item><item><title>Using Linear Regression for Iteratively Training Neural Networks</title><link>http://arxiv.org/abs/2307.05189v2</link><description>We present a simple linear regression based approach for learning the weightsand biases of a neural network, as an alternative to standard gradient basedbackpropagation. The present work is exploratory in nature, and we restrict thedescription and experiments to (i) simple feedforward neural networks, (ii)scalar (single output) regression problems, and (iii) invertible activationfunctions. However, the approach is intended to be extensible to larger, morecomplex architectures. The key idea is the observation that the input to everyneuron in a neural network is a linear combination of the activations ofneurons in the previous layer, as well as the parameters (weights and biases)of the layer. If we are able to compute the ideal total input values to everyneuron by working backwards from the output, we can formulate the learningproblem as a linear least squares problem which iterates between updating theparameters and the activation values. We present an explicit algorithm thatimplements this idea, and we show that (at least for small problems) theapproach is more stable and faster than gradient-based methods.</description><author>Harshad Khadilkar</author><pubDate>Fri, 14 Jul 2023 15:13:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05189v2</guid></item><item><title>A scoping review on multimodal deep learning in biomedical images and texts</title><link>http://arxiv.org/abs/2307.07362v1</link><description>Computer-assisted diagnostic and prognostic systems of the future should becapable of simultaneously processing multimodal data. Multimodal deep learning(MDL), which involves the integration of multiple sources of data, such asimages and text, has the potential to revolutionize the analysis andinterpretation of biomedical data. However, it only caught researchers'attention recently. To this end, there is a critical need to conduct asystematic review on this topic, identify the limitations of current work, andexplore future directions. In this scoping review, we aim to provide acomprehensive overview of the current state of the field and identify keyconcepts, types of studies, and research gaps with a focus on biomedical imagesand texts joint learning, mainly because these two were the most commonlyavailable data types in MDL research. This study reviewed the current uses ofmultimodal deep learning on five tasks: (1) Report generation, (2) Visualquestion answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,and (5) Semantic segmentation. Our results highlight the diverse applicationsand potential of MDL and suggest directions for future research in the field.We hope our review will facilitate the collaboration of natural languageprocessing (NLP) and medical imaging communities and support the nextgeneration of decision-making and computer-assisted diagnostic systemdevelopment.</description><author>Zhaoyi Sun, Mingquan Lin, Qingqing Zhu, Qianqian Xie, Fei Wang, Zhiyong Lu, Yifan Peng</author><pubDate>Fri, 14 Jul 2023 15:08:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07362v1</guid></item><item><title>Gloss Attention for Gloss-free Sign Language Translation</title><link>http://arxiv.org/abs/2307.07361v1</link><description>Most sign language translation (SLT) methods to date require the use of glossannotations to provide additional supervision information, however, theacquisition of gloss is not easy. To solve this problem, we first perform ananalysis of existing models to confirm how gloss annotations make SLT easier.We find that it can provide two aspects of information for the model, 1) it canhelp the model implicitly learn the location of semantic boundaries incontinuous sign language videos, 2) it can help the model understand the signlanguage video globally. We then propose \emph{gloss attention}, which enablesthe model to keep its attention within video segments that have the samesemantics locally, just as gloss helps existing models do. Furthermore, wetransfer the knowledge of sentence-to-sentence similarity from the naturallanguage model to our gloss attention SLT network (GASLT) to help it understandsign language videos at the sentence level. Experimental results on multiplelarge-scale sign language datasets show that our proposed GASLT modelsignificantly outperforms existing methods. Our code is provided in\url{https://github.com/YinAoXiong/GASLT}.</description><author>Aoxiong Yin, Tianyun Zhong, Li Tang, Weike Jin, Tao Jin, Zhou Zhao</author><pubDate>Fri, 14 Jul 2023 15:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07361v1</guid></item><item><title>A testing-based approach to assess the clusterability of categorical data</title><link>http://arxiv.org/abs/2307.07346v1</link><description>The objective of clusterability evaluation is to check whether a clusteringstructure exists within the data set. As a crucial yet often-overlooked issuein cluster analysis, it is essential to conduct such a test before applying anyclustering algorithm. If a data set is unclusterable, any subsequent clusteringanalysis would not yield valid results. Despite its importance, the majority ofexisting studies focus on numerical data, leaving the clusterability evaluationissue for categorical data as an open problem. Here we present TestCat, atesting-based approach to assess the clusterability of categorical data interms of an analytical $p$-value. The key idea underlying TestCat is thatclusterable categorical data possess many strongly correlated attribute pairsand hence the sum of chi-squared statistics of all attribute pairs is employedas the test statistic for $p$-value calculation. We apply our method to a setof benchmark categorical data sets, showing that TestCat outperforms thosesolutions based on existing clusterability evaluation methods for numeric data.To the best of our knowledge, our work provides the first way to effectivelyrecognize the clusterability of categorical data in a statistically soundmanner.</description><author>Lianyu Hu, Junjie Dong, Mudi Jiang, Yan Liu, Zengyou He</author><pubDate>Fri, 14 Jul 2023 14:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07346v1</guid></item><item><title>Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks</title><link>http://arxiv.org/abs/2307.07344v1</link><description>This paper proposes a novel approach to integrating partial differentialequation (PDE)-based evolution models into neural networks through a new typeof regularization. Specifically, we propose inverse evolution layers (IELs)based on evolution equations. These layers can achieve specific regularizationobjectives and endow neural networks' outputs with corresponding properties ofthe evolution models. Moreover, IELs are straightforward to construct andimplement, and can be easily designed for various physical evolutions andneural networks. Additionally, the design process for these layers can provideneural networks with intuitive and mathematical interpretability, thusenhancing the transparency and explainability of the approach. To demonstratethe effectiveness, efficiency, and simplicity of our approach, we present anexample of endowing semantic segmentation models with the smoothness propertybased on the heat diffusion model. To achieve this goal, we designheat-diffusion IELs and apply them to address the challenge of semanticsegmentation with noisy labels. The experimental results demonstrate that theheat-diffusion IELs can effectively mitigate the overfitting problem caused bynoisy labels.</description><author>Chaoyu Liu, Zhonghua Qiao, Chao Li, Carola-Bibiane Schönlieb</author><pubDate>Fri, 14 Jul 2023 14:47:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07344v1</guid></item><item><title>MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters</title><link>http://arxiv.org/abs/2307.07343v1</link><description>The selection of model's parameters plays an important role in theapplication of support vector classification (SVC). The commonly used method ofselecting model's parameters is the k-fold cross validation with grid search(CV). It is extremely time-consuming because it needs to train a large numberof SVC models. In this paper, a new method is proposed to train SVC with theselection of model's parameters. Firstly, training SVC with the selection ofmodel's parameters is modeled as a minimax optimization problem(MaxMin-L2-SVC-NCH), in which the minimization problem is an optimizationproblem of finding the closest points between two normal convex hulls(L2-SVC-NCH) while the maximization problem is an optimization problem offinding the optimal model's parameters. A lower time complexity can be expectedin MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm isthen proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by aprojected gradient algorithm (PGA) while the maximization problem is solved bya gradient ascent algorithm with dynamic learning rate. To demonstrate theadvantages of the PGA in solving L2-SVC-NCH, we carry out a comparison of thePGA and the famous sequential minimal optimization (SMO) algorithm after a SMOalgorithm and some KKT conditions for L2-SVC-NCH are provided. It is revealedthat the SMO algorithm is a special case of the PGA. Thus, the PGA can providemore flexibility. The comparative experiments between MaxMin-L2-SVC-NCH and theclassical parameter selection models on public datasets show thatMaxMin-L2-SVC-NCH greatly reduces the number of models to be trained and thetest accuracy is not lost to the classical models. It indicates thatMaxMin-L2-SVC-NCH performs better than the other models. We strongly recommendMaxMin-L2-SVC-NCH as a preferred model for SVC task.</description><author>Linkai Luo, Qiaoling Yang, Hong Peng, Yiding Wang, Ziyang Chen</author><pubDate>Fri, 14 Jul 2023 14:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07343v1</guid></item><item><title>PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting</title><link>http://arxiv.org/abs/2307.07341v1</link><description>Vision-language (VL) Pre-training (VLP) has shown to well generalize VLmodels over a wide range of VL downstream tasks, especially for cross-modalretrieval. However, it hinges on a huge amount of image-text pairs, whichrequires tedious and costly curation. On the contrary, weakly-supervised VLP(W-VLP) explores means with object tags generated by a pre-trained objectdetector (OD) from images. Yet, they still require paired information, i.e.images and object-level annotations, as supervision to train an OD. To further reduce the amount of supervision, we propose Prompts-in-The-Loop(PiTL) that prompts knowledge from large language models (LLMs) to describeimages. Concretely, given a category label of an image, e.g. refinery, theknowledge, e.g. a refinery could be seen with large storage tanks, pipework,and ..., extracted by LLMs is used as the language counterpart. The knowledgesupplements, e.g. the common relations among entities most likely appearing ina scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of14K categories from ImageNet21K with PiTL. Empirically, the VL modelspre-trained with PiTL-generated pairs are strongly favored over other W-VLPworks on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with lesssupervision. The results reveal the effectiveness of PiTL-generated pairs forVLP.</description><author>Zixin Guo, Tzu-Jui Julius Wang, Selen Pehlivan, Abduljalil Radman, Jorma Laaksonen</author><pubDate>Fri, 14 Jul 2023 14:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07341v1</guid></item><item><title>Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers</title><link>http://arxiv.org/abs/2306.13804v3</link><description>Despite the recent progress in speech emotion recognition (SER),state-of-the-art systems are unable to achieve improved performance incross-language settings. In this paper, we propose a Multimodal Dual AttentionTransformer (MDAT) model to improve cross-language SER. Our model utilisespre-trained models for multimodal feature extraction and is equipped with adual attention mechanism including graph attention and co-attention to capturecomplex dependencies across different modalities and achieve improvedcross-language SER results using minimal target language data. In addition, ourmodel also exploits a transformer encoder layer for high-level featurerepresentation to improve emotion classification accuracy. In this way, MDATperforms refinement of feature representation at various stages and providesemotional salient features to the classification layer. This novel approachalso ensures the preservation of modality-specific emotional information whileenhancing cross-modality and cross-language interactions. We assess our model'sperformance on four publicly available SER datasets and establish its superioreffectiveness compared to recent approaches and baseline models.</description><author>Syed Aun Muhammad Zaidi, Siddique Latif, Junaid Qadir</author><pubDate>Fri, 14 Jul 2023 14:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13804v3</guid></item><item><title>Risk Controlled Image Retrieval</title><link>http://arxiv.org/abs/2307.07336v1</link><description>Most image retrieval research focuses on improving predictive performance,but they may fall short in scenarios where the reliability of the prediction iscrucial. Though uncertainty quantification can help by assessing uncertaintyfor query and database images, this method can provide only a heuristicestimate rather than an guarantee. To address these limitations, we presentRisk Controlled Image Retrieval (RCIR), which generates retrieval sets that areguaranteed to contain the ground truth samples with a predefined probability.RCIR can be easily plugged into any image retrieval method, agnostic to datadistribution and model selection. To the best of our knowledge, this is thefirst work that provides coverage guarantees for image retrieval. The validityand efficiency of RCIR is demonstrated on four real-world image retrievaldatasets, including the Stanford CAR-196 (Krause et al. 2013), CUB-200 (Wah etal. 2011), the Pittsburgh dataset (Torii et al. 2013) and the ChestX-Detdataset (Lian et al. 2021).</description><author>Kaiwen Cai, Chris Xiaoxuan Lu, Xingyu Zhao, Xiaowei Huang</author><pubDate>Fri, 14 Jul 2023 14:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07336v1</guid></item><item><title>14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon</title><link>http://arxiv.org/abs/2306.06283v4</link><description>Large-language models (LLMs) such as GPT-4 caught the interest of manyscientists. Recent studies suggested that these models could be useful inchemistry and materials science. To explore these possibilities, we organized ahackathon. This article chronicles the projects built as part of this hackathon.Participants employed LLMs for various applications, including predictingproperties of molecules and materials, designing novel interfaces for tools,extracting knowledge from unstructured data, and developing new educationalapplications. The diverse topics and the fact that working prototypes could be generated inless than two days highlight that LLMs will profoundly impact the future of ourfields. The rich collection of ideas and projects also indicates that theapplications of LLMs are not limited to materials science and chemistry butoffer potential benefits to a wide range of scientific disciplines.</description><author>Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D. Bocarsly, Andres M Bran, Stefan Bringuier, L. Catherine Brinson, Kamal Choudhary, Defne Circi, Sam Cox, Wibe A. de Jong, Matthew L. Evans, Nicolas Gastellu, Jerome Genzling, María Victoria Gil, Ankur K. Gupta, Zhi Hong, Alishba Imran, Sabine Kruschwitz, Anne Labarre, Jakub Lála, Tao Liu, Steven Ma, Sauradeep Majumdar, Garrett W. Merz, Nicolas Moitessier, Elias Moubarak, Beatriz Mouriño, Brenden Pelkie, Michael Pieler, Mayk Caldas Ramos, Bojana Ranković, Samuel G. Rodriques, Jacob N. Sanders, Philippe Schwaller, Marcus Schwarting, Jiale Shi, Berend Smit, Ben E. Smith, Joren Van Herck, Christoph Völker, Logan Ward, Sean Warren, Benjamin Weiser, Sylvester Zhang, Xiaoqi Zhang, Ghezal Ahmad Zia, Aristana Scourtas</author><pubDate>Fri, 14 Jul 2023 14:24:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06283v4</guid></item><item><title>SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes</title><link>http://arxiv.org/abs/2307.07333v1</link><description>In this work, we present SynTable, a unified and flexible Python-baseddataset generator built using NVIDIA's Isaac Sim Replicator Composer forgenerating high-quality synthetic datasets for unseen object amodal instancesegmentation of cluttered tabletop scenes. Our dataset generation tool canrender a complex 3D scene containing object meshes, materials, textures,lighting, and backgrounds. Metadata, such as modal and amodal instancesegmentation masks, occlusion masks, depth maps, bounding boxes, and materialproperties, can be generated to automatically annotate the scene according tothe users' requirements. Our tool eliminates the need for manual labeling inthe dataset generation process while ensuring the quality and accuracy of thedataset. In this work, we discuss our design goals, framework architecture, andthe performance of our tool. We demonstrate the use of a sample datasetgenerated using SynTable by ray tracing for training a state-of-the-art model,UOAIS-Net. The results show significantly improved performance in Sim-to-Realtransfer when evaluated on the OSD-Amodal dataset. We offer this tool as anopen-source, easy-to-use, photorealistic dataset generator for advancingresearch in deep learning and synthetic data generation.</description><author>Zhili Ng, Haozhe Wang, Zhengshen Zhang, Francis Tay Eng Hock, Marcelo H. Ang Jr</author><pubDate>Fri, 14 Jul 2023 14:24:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07333v1</guid></item><item><title>How Different Is Stereotypical Bias Across Languages?</title><link>http://arxiv.org/abs/2307.07331v1</link><description>Recent studies have demonstrated how to assess the stereotypical bias inpre-trained English language models. In this work, we extend this branch ofresearch in multiple different dimensions by systematically investigating (a)mono- and multilingual models of (b) different underlying architectures withrespect to their bias in (c) multiple different languages. To that end, we makeuse of the English StereoSet data set (Nadeem et al., 2021), which wesemi-automatically translate into German, French, Spanish, and Turkish. We findthat it is of major importance to conduct this type of analysis in amultilingual setting, as our experiments show a much more nuanced picture aswell as notable differences from the English-only analysis. The main takeawaysfrom our analysis are that mGPT-2 (partly) shows surprising anti-stereotypicalbehavior across languages, English (monolingual) models exhibit the strongestbias, and the stereotypes reflected in the data set are least present inTurkish models. Finally, we release our codebase alongside the translated datasets and practical guidelines for the semi-automatic translation to encourage afurther extension of our work to other languages.</description><author>Ibrahim Tolga Öztürk, Rostislav Nedelchev, Christian Heumann, Esteban Garces Arias, Marius Roger, Bernd Bischl, Matthias Aßenmacher</author><pubDate>Fri, 14 Jul 2023 14:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07331v1</guid></item><item><title>Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy</title><link>http://arxiv.org/abs/2307.07328v1</link><description>Data-poisoning based backdoor attacks aim to insert backdoor into models bymanipulating training datasets without controlling the training process of thetarget model. Existing attack methods mainly focus on designing triggers orfusion strategies between triggers and benign samples. However, they oftenrandomly select samples to be poisoned, disregarding the varying importance ofeach poisoning sample in terms of backdoor injection. A recent selectionstrategy filters a fixed-size poisoning sample pool by recording forgettingevents, but it fails to consider the remaining samples outside the pool from aglobal perspective. Moreover, computing forgetting events requires significantadditional computing resources. Therefore, how to efficiently and effectivelyselect poisoning samples from the entire dataset is an urgent problem inbackdoor attacks.To address it, firstly, we introduce a poisoning mask into theregular backdoor training loss. We suppose that a backdoored model trainingwith hard poisoning samples has a more backdoor effect on easy ones, which canbe implemented by hindering the normal training process (\ie, maximizing loss\wrt mask). To further integrate it with normal training process, we thenpropose a learnable poisoning sample selection strategy to learn the masktogether with the model parameters through a min-max optimization.Specifically,the outer loop aims to achieve the backdoor attack goal by minimizing the lossbased on the selected samples, while the inner loop selects hard poisoningsamples that impede this goal by maximizing the loss. After several rounds ofadversarial training, we finally select effective poisoning samples with highcontribution. Extensive experiments on benchmark datasets demonstrate theeffectiveness and efficiency of our approach in boosting backdoor attackperformance.</description><author>Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu</author><pubDate>Fri, 14 Jul 2023 14:12:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07328v1</guid></item><item><title>Privacy-preserving machine learning with tensor networks</title><link>http://arxiv.org/abs/2202.12319v2</link><description>Tensor networks, widely used for providing efficient representations oflow-energy states of local quantum many-body systems, have been recentlyproposed as machine learning architectures which could present advantages withrespect to traditional ones. In this work we show that tensor networkarchitectures have especially prospective properties for privacy-preservingmachine learning, which is important in tasks such as the processing of medicalrecords. First, we describe a new privacy vulnerability that is present infeedforward neural networks, illustrating it in synthetic and real-worlddatasets. Then, we develop well-defined conditions to guarantee robustness tosuch vulnerability, which involve the characterization of models equivalentunder gauge symmetry. We rigorously prove that such conditions are satisfied bytensor-network architectures. In doing so, we define a novel canonical form formatrix product states, which has a high degree of regularity and fixes theresidual gauge that is left in the canonical forms based on singular valuedecompositions. We supplement the analytical findings with practical exampleswhere matrix product states are trained on datasets of medical records, whichshow large reductions on the probability of an attacker extracting informationabout the training dataset from the model's parameters. Given the growingexpertise in training tensor-network architectures, these results imply thatone may not have to be forced to make a choice between accuracy in predictionand ensuring the privacy of the information processed.</description><author>Alejandro Pozas-Kerstjens, Senaida Hernández-Santana, José Ramón Pareja Monturiol, Marco Castrillón López, Giannicola Scarpa, Carlos E. González-Guillén, David Pérez-García</author><pubDate>Fri, 14 Jul 2023 14:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.12319v2</guid></item><item><title>Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications</title><link>http://arxiv.org/abs/2307.07325v1</link><description>The representation learning of speech, without textual resources, is an areaof significant interest for many low resource speech applications. In thispaper, we describe an approach to self-supervised representation learning fromraw audio using a hidden unit clustering (HUC) framework. The input to themodel consists of audio samples that are windowed and processed with 1-Dconvolutional layers. The learned "time-frequency" representations from theconvolutional neural network (CNN) module are further processed with long shortterm memory (LSTM) layers which generate a contextual vector representation forevery windowed segment. The HUC framework, allowing the categorization of therepresentations into a small number of phoneme-like units, is used to train themodel for learning semantically rich speech representations. The targetsconsist of phoneme-like pseudo labels for each audio segment and these aregenerated with an iterative k-means algorithm. We explore techniques thatimprove the speaker invariance of the learned representations and illustratethe effectiveness of the proposed approach on two settings, i) completelyunsupervised speech applications on the sub-tasks described as part of theZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition(ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindidataset. In these experiments, we achieve state-of-art results for variousZeroSpeech tasks. Further, on the ASR experiments, the HUC representations areshown to improve significantly over other established benchmarks based onWav2vec, HuBERT and Best-RQ.</description><author>Varun Krishna, Tarun Sai, Sriram Ganapathy</author><pubDate>Fri, 14 Jul 2023 14:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07325v1</guid></item><item><title>A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming</title><link>http://arxiv.org/abs/2307.07322v1</link><description>The current cut selection algorithm used in mixed-integer programming solvershas remained largely unchanged since its creation. In this paper, we propose aset of new cut scoring measures, cut filtering techniques, and stoppingcriteria, extending the current state-of-the-art algorithm and obtaining a 4\%performance improvement for SCIP over the MIPLIB 2017 benchmark set.</description><author>Mark Turner, Timo Berthold, Mathieu Besançon</author><pubDate>Fri, 14 Jul 2023 13:57:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07322v1</guid></item><item><title>Adaptive Linear Estimating Equations</title><link>http://arxiv.org/abs/2307.07320v1</link><description>Sequential data collection has emerged as a widely adopted technique forenhancing the efficiency of data gathering processes. Despite its advantages,such data collection mechanism often introduces complexities to the statisticalinference procedure. For instance, the ordinary least squares (OLS) estimatorin an adaptive linear regression model can exhibit non-normal asymptoticbehavior, posing challenges for accurate inference and interpretation. In thispaper, we propose a general method for constructing debiased estimator whichremedies this issue. It makes use of the idea of adaptive linear estimatingequations, and we establish theoretical guarantees of asymptotic normality,supplemented by discussions on achieving near-optimal asymptotic variance. Asalient feature of our estimator is that in the context of multi-armed bandits,our estimator retains the non-asymptotic performance of the least squareestimator while obtaining asymptotic normality property. Consequently, thiswork helps connect two fruitful paradigms of adaptive inference: a)non-asymptotic inference using concentration inequalities and b) asymptoticinference via asymptotic normality.</description><author>Mufang Ying, Koulik Khamaru, Cun-Hui Zhang</author><pubDate>Fri, 14 Jul 2023 13:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07320v1</guid></item><item><title>Hybrid moderation in the newsroom: Recommending featured posts to content moderators</title><link>http://arxiv.org/abs/2307.07317v1</link><description>Online news outlets are grappling with the moderation of user-generatedcontent within their comment section. We present a recommender system based onranking class probabilities to support and empower the moderator in choosingfeatured posts, a time-consuming task. By combining user and textual contentfeatures we obtain an optimal classification F1-score of 0.44 on the test set.Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set ofvalidation articles. As an expert evaluation, content moderators assessed theoutput of a random selection of articles by choosing comments to feature basedon the recommendations, which resulted in a NDCG score of 0.83. We concludethat first, adding text features yields the best score and second, whilechoosing featured content remains somewhat subjective, content moderators foundsuitable comments in all but one evaluated recommendations. We end the paper byanalyzing our best-performing model, a step towards transparency andexplainability in hybrid content moderation.</description><author>Cedric Waterschoot, Antal van den Bosch</author><pubDate>Fri, 14 Jul 2023 13:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07317v1</guid></item><item><title>HEAL-SWIN: A Vision Transformer On The Sphere</title><link>http://arxiv.org/abs/2307.07313v1</link><description>High-resolution wide-angle fisheye images are becoming more and moreimportant for robotics applications such as autonomous driving. However, usingordinary convolutional neural networks or vision transformers on this data isproblematic due to projection and distortion losses introduced when projectingto a rectangular grid on the plane. We introduce the HEAL-SWIN transformer,which combines the highly uniform Hierarchical Equal Area iso-LatitudePixelation (HEALPix) grid used in astrophysics and cosmology with theHierarchical Shifted-Window (SWIN) transformer to yield an efficient andflexible model capable of training on high-resolution, distortion-freespherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is usedto perform the patching and windowing operations of the SWIN transformer,resulting in a one-dimensional representation of the spherical data withminimal computational overhead. We demonstrate the superior performance of ourmodel for semantic segmentation and depth regression tasks on both syntheticand real automotive datasets. Our code is available athttps://github.com/JanEGerken/HEAL-SWIN.</description><author>Oscar Carlsson, Jan E. Gerken, Hampus Linander, Heiner Spieß, Fredrik Ohlsson, Christoffer Petersson, Daniel Persson</author><pubDate>Fri, 14 Jul 2023 13:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07313v1</guid></item><item><title>Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs</title><link>http://arxiv.org/abs/2307.07312v1</link><description>In any system that uses structured knowledge graph (KG) data as itsunderlying knowledge representation, KG-to-text generation is a useful tool forturning parts of the graph data into text that can be understood by humans.Recent work has shown that models that make use of pretraining on large amountsof text data can perform well on the KG-to-text task even with relatively smallsets of training data on the specific graph-to-text task. In this paper, webuild on this concept by using large language models to perform zero-shotgeneration based on nothing but the model's understanding of the triplestructure from what it can read. We show that ChatGPT achieves nearstate-of-the-art performance on some measures of the WebNLG 2020 challenge, butfalls behind on others. Additionally, we compare factual, counter-factual andfictional statements, and show that there is a significant connection betweenwhat the LLM already knows about the data it is parsing and the quality of theoutput text.</description><author>Agnes Axelsson, Gabriel Skantze</author><pubDate>Fri, 14 Jul 2023 13:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07312v1</guid></item><item><title>LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields</title><link>http://arxiv.org/abs/2304.10406v2</link><description>We introduce a new task, novel view synthesis for LiDAR sensors. Whiletraditional model-based LiDAR simulators with style-transfer neural networkscan be applied to render novel views, they fall short of producing accurate andrealistic LiDAR patterns because the renderers rely on explicit 3Dreconstruction and exploit game engines, that ignore important attributes ofLiDAR points. We address this challenge by formulating, to the best of ourknowledge, the first differentiable end-to-end LiDAR rendering framework,LiDAR-NeRF, leveraging a neural radiance field (NeRF) to facilitate the jointlearning of geometry and the attributes of 3D points. However, simply employingNeRF cannot achieve satisfactory results, as it only focuses on learningindividual pixels while ignoring local information, especially at low textureareas, resulting in poor geometry. To this end, we have taken steps to addressthis issue by introducing a structural regularization method to preserve localstructural details. To evaluate the effectiveness of our approach, we establishan object-centric multi-view LiDAR dataset, dubbed NeRF-MVL. It containsobservations of objects from 9 categories seen from 360-degree viewpointscaptured with multiple LiDAR sensors. Our extensive experiments on thescene-level KITTI-360 dataset, and on our object-level NeRF-MVL show that ourLiDAR-NeRF surpasses the model-based algorithms significantly.</description><author>Tang Tao, Longfei Gao, Guangrun Wang, Yixing Lao, Peng Chen, Hengshuang Zhao, Dayang Hao, Xiaodan Liang, Mathieu Salzmann, Kaicheng Yu</author><pubDate>Fri, 14 Jul 2023 13:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10406v2</guid></item><item><title>$Φ$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation</title><link>http://arxiv.org/abs/2209.15609v2</link><description>Incorporating unstructured data into physical models is a challenging problemthat is emerging in data assimilation. Traditional approaches focus onwell-defined observation operators whose functional forms are typically assumedto be known. This prevents these methods from achieving a consistent model-datasynthesis in configurations where the mapping from data-space to model-space isunknown. To address these shortcomings, in this paper we develop aphysics-informed dynamical variational autoencoder ($\Phi$-DVAE) to embeddiverse data streams into time-evolving physical systems described bydifferential equations. Our approach combines a standard, possibly nonlinear,filter for the latent state-space model and a VAE, to assimilate theunstructured data into the latent dynamical system. Unstructured data, in ourexample systems, comes in the form of video data and velocity fieldmeasurements, however the methodology is suitably generic to allow forarbitrary unknown observation operators. A variational Bayesian framework isused for the joint estimation of the encoding, latent states, and unknownsystem parameters. To demonstrate the method, we provide case studies with theLorenz-63 ordinary differential equation, and the advection and Korteweg-deVries partial differential equations. Our results, with synthetic data, showthat $\Phi$-DVAE provides a data efficient dynamics encoding methodology whichis competitive with standard approaches. Unknown parameters are recovered withuncertainty quantification, and unseen data are accurately predicted.</description><author>Alex Glyn-Davies, Connor Duffin, Ö. Deniz Akyildiz, Mark Girolami</author><pubDate>Fri, 14 Jul 2023 13:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.15609v2</guid></item><item><title>C3: Zero-shot Text-to-SQL with ChatGPT</title><link>http://arxiv.org/abs/2307.07306v1</link><description>This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,which achieves 82.3\% in terms of execution accuracy on the holdout test set ofSpider and becomes the state-of-the-art zero-shot Text-to-SQL method on theSpider Challenge. C3 consists of three key components: Clear Prompting (CP),Calibration with Hints (CH), and Consistent Output (CO), which arecorresponding to the model input, model bias and model output respectively. Itprovides a systematic treatment for zero-shot Text-to-SQL. Extensiveexperiments have been conducted to verify the effectiveness and efficiency ofour proposed method.</description><author>Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, lu Chen, Jinshu Lin, Dongfang Lou</author><pubDate>Fri, 14 Jul 2023 13:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07306v1</guid></item><item><title>Solving higher-order Lane-Emden-Fowler type equations using physics-informed neural networks: benchmark tests comparing soft and hard constraints</title><link>http://arxiv.org/abs/2307.07302v1</link><description>In this paper, numerical methods using Physics-Informed Neural Networks(PINNs) are presented with the aim to solve higher-order ordinary differentialequations (ODEs). Indeed, this deep-learning technique is successfully appliedfor solving different classes of singular ODEs, namely the well knownsecond-order Lane-Emden equations, third order-order Emden-Fowler equations,and fourth-order Lane-Emden-Fowler equations. Two variants of PINNs techniqueare considered and compared. First, a minimization procedure is used toconstrain the total loss function of the neural network, in which the equationresidual is considered with some weight to form a physics-based loss and addedto the training data loss that contains the initial/boundary conditions.Second, a specific choice of trial solutions ensuring these conditions as hardconstraints is done in order to satisfy the differential equation, contrary tothe first variant based on training data where the constraints appear as softones. Advantages and drawbacks of PINNs variants are highlighted.</description><author>Hubert Baty</author><pubDate>Fri, 14 Jul 2023 13:27:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07302v1</guid></item><item><title>Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains</title><link>http://arxiv.org/abs/2307.04378v2</link><description>Diabetic Retinopathy (DR) is a common complication of diabetes and a leadingcause of blindness worldwide. Early and accurate grading of its severity iscrucial for disease management. Although deep learning has shown greatpotential for automated DR grading, its real-world deployment is stillchallenging due to distribution shifts among source and target domains, knownas the domain generalization problem. Existing works have mainly attributed theperformance degradation to limited domain shifts caused by simple visualdiscrepancies, which cannot handle complex real-world scenarios. Instead, wepresent preliminary evidence suggesting the existence of three-foldgeneralization issues: visual and degradation style shifts, diagnostic patterndiversity, and data imbalance. To tackle these issues, we propose a novelunified framework named Generalizable Diabetic Retinopathy Grading Network(GDRNet). GDRNet consists of three vital components: fundus visual-artifactaugmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), anddomain-class-aware re-balancing (DCR). FundusAug generates realistic augmentedimages via visual transformation and image degradation, while DahLoss jointlyleverages pixel-level consistency and image-level semantics to capture thediverse diagnostic patterns and build generalizable feature representations.Moreover, DCR mitigates the data imbalance from a domain-class view and avoidsundesired over-emphasis on rare domain-class pairs. Finally, we design apublicly available benchmark for fair evaluations. Extensive comparisonexperiments against advanced methods and exhaustive ablation studiesdemonstrate the effectiveness and generalization ability of GDRNet.</description><author>Haoxuan Che, Yuhan Cheng, Haibo Jin, Hao Chen</author><pubDate>Fri, 14 Jul 2023 13:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04378v2</guid></item><item><title>3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks</title><link>http://arxiv.org/abs/2307.07298v1</link><description>Myocardial infarction (MI) is one of the most prevalent cardiovasculardiseases with associated clinical decision-making typically based onsingle-valued imaging biomarkers. However, such metrics only approximate thecomplex 3D structure and physiology of the heart and hence hinder a betterunderstanding and prediction of MI outcomes. In this work, we investigate theutility of complete 3D cardiac shapes in the form of point clouds for animproved detection of MI events. To this end, we propose a fully automaticmulti-step pipeline consisting of a 3D cardiac surface reconstruction stepfollowed by a point cloud classification network. Our method utilizes recentadvances in geometric deep learning on point clouds to enable direct andefficient multi-scale learning on high-resolution surface models of the cardiacanatomy. We evaluate our approach on 1068 UK Biobank subjects for the tasks ofprevalent MI detection and incident MI prediction and find improvements of ~13%and ~5% respectively over clinical benchmarks. Furthermore, we analyze the roleof each ventricle and cardiac phase for 3D shape-based MI detection and conducta visual analysis of the morphological and physiological patterns typicallyassociated with MI outcomes.</description><author>Marcel Beetz, Yilong Yang, Abhirup Banerjee, Lei Li, Vicente Grau</author><pubDate>Fri, 14 Jul 2023 13:21:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07298v1</guid></item><item><title>Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment</title><link>http://arxiv.org/abs/2307.07296v1</link><description>Active Simultaneous Localisation and Mapping (SLAM) is a critical problem inautonomous robotics, enabling robots to navigate to new regions while buildingan accurate model of their surroundings. Visual SLAM is a popular techniquethat uses virtual elements to enhance the experience. However, existingfrontier-based exploration strategies can lead to a non-optimal path inscenarios where there are multiple frontiers with similar distance. This issuecan impact the efficiency and accuracy of Visual SLAM, which is crucial for awide range of robotic applications, such as search and rescue, exploration, andmapping. To address this issue, this research combines both an existingVisual-Graph SLAM known as ExploreORB with reinforcement learning. The proposedalgorithm allows the robot to learn and optimize exploration routes through areward-based system to create an accurate map of the environment with properfrontier selection. Frontier-based exploration is used to detect unexploredareas, while reinforcement learning optimizes the robot's movement by assigningrewards for optimal frontier points. Graph SLAM is then used to integrate therobot's sensory data and build an accurate map of the environment. The proposedalgorithm aims to improve the efficiency and accuracy of ExploreORB byoptimizing the exploration process of frontiers to build a more accurate map.To evaluate the effectiveness of the proposed approach, experiments will beconducted in various virtual environments using Gazebo, a robot simulationsoftware. Results of these experiments will be compared with existing methodsto demonstrate the potential of the proposed approach as an optimal solutionfor SLAM in autonomous robotics.</description><author>Kenji Leong</author><pubDate>Fri, 14 Jul 2023 13:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07296v1</guid></item><item><title>Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?</title><link>http://arxiv.org/abs/2307.07295v1</link><description>ASR systems are generally built for the spoken 'standard', and theirperformance declines for non-standard dialects/varieties. This is a problem fora language like Irish, where there is no single spoken standard, but ratherthree major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As adiagnostic to quantify the effect of the speaker's dialect on recognitionperformance, 12 ASR systems were trained, firstly using baselinedialect-balanced training corpora, and then using modified versions of thebaseline corpora, where dialect-specific materials were either subtracted oradded. Results indicate that dialect-balanced corpora do not yield a similarperformance across the dialects: the Ul dialect consistently underperforms,whereas Mu yields lowest WERs. There is a close relationship between Co and Mudialects, but one that is not symmetrical. These results will guide futurecorpus collection and system building strategies to optimise for cross-dialectperformance equity.</description><author>Liam Lonergan, Mengjie Qian, Neasa Ní Chiaráin, Christer Gobl, Ailbhe Ní Chasaide</author><pubDate>Fri, 14 Jul 2023 13:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07295v1</guid></item><item><title>DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion</title><link>http://arxiv.org/abs/2303.14863v2</link><description>We propose a new formulation of temporal action detection (TAD) withdenoising diffusion, DiffTAD in short. Taking as input random temporalproposals, it can yield action proposals accurately given an untrimmed longvideo. This presents a generative modeling perspective, against previousdiscriminative learning manners. This capability is achieved by first diffusingthe ground-truth proposals to random ones (i.e., the forward/noising process)and then learning to reverse the noising process (i.e., the backward/denoisingprocess). Concretely, we establish the denoising process in the Transformerdecoder (e.g., DETR) by introducing a temporal location query design withfaster convergence in training. We further propose a cross-step selectiveconditioning algorithm for inference acceleration. Extensive evaluations onActivityNet and THUMOS show that our DiffTAD achieves top performance comparedto previous art alternatives. The code will be made available athttps://github.com/sauradip/DiffusionTAD.</description><author>Sauradip Nag, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang</author><pubDate>Fri, 14 Jul 2023 13:06:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14863v2</guid></item><item><title>Sampling-Priors-Augmented Deep Unfolding Network for Robust Video Compressive Sensing</title><link>http://arxiv.org/abs/2307.07291v1</link><description>Video Compressed Sensing (VCS) aims to reconstruct multiple frames from onesingle captured measurement, thus achieving high-speed scene recording with alow-frame-rate sensor. Although there have been impressive advances in VCSrecently, those state-of-the-art (SOTA) methods also significantly increasemodel complexity and suffer from poor generality and robustness, which meansthat those networks need to be retrained to accommodate the new system. Suchlimitations hinder the real-time imaging and practical deployment of models. Inthis work, we propose a Sampling-Priors-Augmented Deep Unfolding Network(SPA-DUN) for efficient and robust VCS reconstruction. Under theoptimization-inspired deep unfolding framework, a lightweight and efficientU-net is exploited to downsize the model while improving overall performance.Moreover, the prior knowledge from the sampling model is utilized todynamically modulate the network features to enable single SPA-DUN to handlearbitrary sampling settings, augmenting interpretability and generality.Extensive experiments on both simulation and real datasets demonstrate thatSPA-DUN is not only applicable for various sampling settings with one singlemodel but also achieves SOTA performance with incredible efficiency.</description><author>Yuhao Huang, Gangrong Qu, Youran Ge</author><pubDate>Fri, 14 Jul 2023 13:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07291v1</guid></item><item><title>Implicit Neural Feature Fusion Function for Multispectral and Hyperspectral Image Fusion</title><link>http://arxiv.org/abs/2307.07288v1</link><description>Multispectral and Hyperspectral Image Fusion (MHIF) is a practical task thataims to fuse a high-resolution multispectral image (HR-MSI) and alow-resolution hyperspectral image (LR-HSI) of the same scene to obtain ahigh-resolution hyperspectral image (HR-HSI). Benefiting from powerfulinductive bias capability, CNN-based methods have achieved great success in theMHIF task. However, they lack certain interpretability and require convolutionstructures be stacked to enhance performance. Recently, Implicit NeuralRepresentation (INR) has achieved good performance and interpretability in 2Dtasks due to its ability to locally interpolate samples and utilize multimodalcontent such as pixels and coordinates. Although INR-based approaches showpromise, they require extra construction of high-frequency information(\emph{e.g.,} positional encoding). In this paper, inspired by previous work ofMHIF task, we realize that HR-MSI could serve as a high-frequency detailauxiliary input, leading us to propose a novel INR-based hyperspectral fusionfunction named Implicit Neural Feature Fusion Function (INF). As an elaboratestructure, it solves the MHIF task and addresses deficiencies in the INR-basedapproaches. Specifically, our INF designs a Dual High-Frequency Fusion (DHFF)structure that obtains high-frequency information twice from HR-MSI and LR-HSI,then subtly fuses them with coordinate information. Moreover, the proposed INFincorporates a parameter-free method named INR with cosine similarity (INR-CS)that uses cosine similarity to generate local weights through feature vectors.Based on INF, we construct an Implicit Neural Fusion Network (INFN) thatachieves state-of-the-art performance for MHIF tasks of two public datasets,\emph{i.e.,} CAVE and Harvard. The code will soon be made available on GitHub.</description><author>ShangQi Deng, RuoCheng Wu, Liang-Jian Deng, Ran Ran, Tai-Xiang Jiang</author><pubDate>Fri, 14 Jul 2023 12:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07288v1</guid></item><item><title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</title><link>http://arxiv.org/abs/2307.07286v1</link><description>One-shot skeleton action recognition, which aims to learn a skeleton actionrecognition model with a single training sample, has attracted increasinginterest due to the challenge of collecting and annotating large-scale skeletonaction data. However, most existing studies match skeleton sequences bycomparing their feature vectors directly which neglects spatial structures andtemporal orders of skeleton data. This paper presents a novel one-shot skeletonaction recognition technique that handles skeleton action recognition viamulti-scale spatial-temporal feature matching. We represent skeleton data atmultiple spatial and temporal scales and achieve optimal feature matching fromtwo perspectives. The first is multi-scale matching which captures thescale-wise semantic relevance of skeleton data at multiple spatial and temporalscales simultaneously. The second is cross-scale matching which handlesdifferent motion magnitudes and speeds by capturing sample-wise relevanceacross multiple scales. Extensive experiments over three large-scale datasets(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superiorone-shot skeleton action recognition, and it outperforms the state-of-the-artconsistently by large margins.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</author><pubDate>Fri, 14 Jul 2023 12:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07286v1</guid></item><item><title>A Data Mining Approach for Detecting Collusion in Unproctored Online Exams</title><link>http://arxiv.org/abs/2302.07014v3</link><description>Due to the precautionary measures during the COVID-19 pandemic manyuniversities offered unproctored take-home exams. We propose methods to detectpotential collusion between students and apply our approach on event log datafrom take-home exams during the pandemic. We find groups of students withsuspiciously similar exams. In addition, we compare our findings to a proctoredcontrol group. By this, we establish a rule of thumb for evaluating which casesare "outstandingly similar", i.e., suspicious cases.</description><author>Janine Langerbein, Till Massing, Jens Klenke, Natalie Reckmann, Michael Striewe, Michael Goedicke, Christoph Hanck</author><pubDate>Fri, 14 Jul 2023 12:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07014v3</guid></item><item><title>Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm</title><link>http://arxiv.org/abs/2211.12271v3</link><description>The $k$-means algorithm is a prevalent clustering method due to itssimplicity, effectiveness, and speed. However, its main disadvantage is itshigh sensitivity to the initial positions of the cluster centers. The global$k$-means is a deterministic algorithm proposed to tackle the randominitialization problem of k-means but its well-known that requires highcomputational cost. It partitions the data to $K$ clusters by solving all$k$-means sub-problems incrementally for all $k=1,\ldots, K$. For each $k$cluster problem, the method executes the $k$-means algorithm $N$ times, where$N$ is the number of datapoints. In this paper, we propose the \emph{global$k$-means\texttt{++}} clustering algorithm, which is an effective way ofacquiring quality clustering solutions akin to those of global $k$-means with areduced computational load. This is achieved by exploiting the center selectionprobability that is effectively used in the $k$-means\texttt{++} algorithm. Theproposed method has been tested and compared in various benchmark datasetsyielding very satisfactory results in terms of clustering quality and executionspeed.</description><author>Georgios Vardakas, Aristidis Likas</author><pubDate>Fri, 14 Jul 2023 12:39:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12271v3</guid></item><item><title>Cloud Detection in Multispectral Satellite Images Using Support Vector Machines With Quantum Kernels</title><link>http://arxiv.org/abs/2307.07281v1</link><description>Support vector machines (SVMs) are a well-established classifier effectivelydeployed in an array of pattern recognition and classification tasks. In thiswork, we consider extending classic SVMs with quantum kernels and applying themto satellite data analysis. The design and implementation of SVMs with quantumkernels (hybrid SVMs) is presented. It consists of the Quantum KernelEstimation (QKE) procedure combined with a classic SVM training routine. Thepixel data are mapped to the Hilbert space using ZZ-feature maps acting on theparameterized ansatz state. The parameters are optimized to maximize the kerneltarget alignment. We approach the problem of cloud detection in satellite imagedata, which is one of the pivotal steps in both on-the-ground and on-boardsatellite image analysis processing chains. The experiments performed over thebenchmark Landsat-8 multispectral dataset revealed that the simulated hybridSVM successfully classifies satellite images with accuracy on par with classicSVMs.</description><author>Artur Miroszewski, Jakub Mielczarek, Filip Szczepanek, Grzegorz Czelusta, Bartosz Grabowski, Bertrand Le Saux, Jakub Nalepa</author><pubDate>Fri, 14 Jul 2023 12:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07281v1</guid></item><item><title>Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition</title><link>http://arxiv.org/abs/2307.07280v1</link><description>While Automatic Speech Recognition (ASR) models have shown significantadvances with the introduction of unsupervised or self-supervised trainingtechniques, these improvements are still only limited to a subsection oflanguages and speakers. Transfer learning enables the adaptation of large-scalemultilingual models to not only low-resource languages but also to morespecific speaker groups. However, fine-tuning on data from new domains isusually accompanied by a decrease in performance on the original domain.Therefore, in our experiments, we examine how well the performance oflarge-scale ASR models can be approximated for smaller domains, with our owndataset of German Senior Voice Commands (SVC-de), and how much of the generalspeech recognition performance can be preserved by selectively freezing partsof the model during training. To further increase the robustness of the ASRmodel to vocabulary and speakers outside of the fine-tuned domain, we applyExperience Replay for continual learning. By adding only a fraction of datafrom the original domain, we are able to reach Word-Error-Rates (WERs) below5\% on the new domain, while stabilizing performance for general speechrecognition at acceptable WERs.</description><author>Theresa Pekarek Rosin, Stefan Wermter</author><pubDate>Fri, 14 Jul 2023 12:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07280v1</guid></item><item><title>Are words equally surprising in audio and audio-visual comprehension?</title><link>http://arxiv.org/abs/2307.07277v1</link><description>We report a controlled study investigating the effect of visual information(i.e., seeing the speaker) on spoken language comprehension. We compare the ERPsignature (N400) associated with each word in audio-only and audio-visualpresentations of the same verbal stimuli. We assess the extent to whichsurprisal measures (which quantify the predictability of words in their lexicalcontext) are generated on the basis of different types of language models(specifically n-gram and Transformer models) that predict N400 responses foreach word. Our results indicate that cognitive effort differs significantlybetween multimodal and unimodal settings. In addition, our findings suggestthat while Transformer-based models, which have access to a larger lexicalcontext, provide a better fit in the audio-only setting, 2-gram language modelsare more effective in the multimodal setting. This highlights the significantimpact of local lexical context on cognitive processing in a multimodalenvironment.</description><author>Pranava Madhyastha, Ye Zhang, Gabriella Vigliocco</author><pubDate>Fri, 14 Jul 2023 12:17:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07277v1</guid></item><item><title>CodeQueries: A Dataset of Semantic Queries over Code</title><link>http://arxiv.org/abs/2209.08372v2</link><description>Developers often have questions about semantic aspects of code they areworking on, e.g., "Is there a class whose parent classes declare a conflictingattribute?". Answering them requires understanding code semantics such asattributes and inheritance relation of classes. An answer to such a questionshould identify code spans constituting the answer (e.g., the declaration ofthe subclass) as well as supporting facts (e.g., the definitions of theconflicting attributes). The existing work on question-answering over code hasconsidered yes/no questions or method-level context. We contribute a labeleddataset, called CodeQueries, of semantic queries over Python code. Compared tothe existing datasets, in CodeQueries, the queries are about code semantics,the context is file level and the answers are code spans. We curate the datasetbased on queries supported by a widely-used static analysis tool, CodeQL, andinclude both positive and negative examples, and queries requiring single-hopand multi-hop reasoning. To assess the value of our dataset, we evaluate baseline neural approaches.We study a large language model (GPT3.5-Turbo) in zero-shot and few-shotsettings on a subset of CodeQueries. We also evaluate a BERT style model(CuBERT) with fine-tuning. We find that these models achieve limited success onCodeQueries. CodeQueries is thus a challenging dataset to test the ability ofneural models, to understand code semantics, in the extractivequestion-answering setting.</description><author>Surya Prakash Sahu, Madhurima Mandal, Shikhar Bharadwaj, Aditya Kanade, Petros Maniatis, Shirish Shevade</author><pubDate>Fri, 14 Jul 2023 12:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08372v2</guid></item><item><title>Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation</title><link>http://arxiv.org/abs/2307.07269v1</link><description>It is imperative to ensure the robustness of deep learning models in criticalapplications such as, healthcare. While recent advances in deep learning haveimproved the performance of volumetric medical image segmentation models, thesemodels cannot be deployed for real-world applications immediately due to theirvulnerability to adversarial attacks. We present a 3D frequency domainadversarial attack for volumetric medical image segmentation models anddemonstrate its advantages over conventional input or voxel domain attacks.Using our proposed attack, we introduce a novel frequency domain adversarialtraining approach for optimizing a robust model against voxel and frequencydomain attacks. Moreover, we propose frequency consistency loss to regulate ourfrequency domain adversarial training that achieves a better tradeoff betweenmodel's performance on clean and adversarial samples. Code is publiclyavailable at https://github.com/asif-hanif/vafa.</description><author>Asif Hanif, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Fri, 14 Jul 2023 11:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07269v1</guid></item><item><title>AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023</title><link>http://arxiv.org/abs/2307.07265v1</link><description>This report presents the technical details of our submission to the 2023Epic-Kitchen EPIC-SOUNDS Audio-Based Interaction Recognition Challenge. Thetask is to learn the mapping from audio samples to their corresponding actionlabels. To achieve this goal, we propose a simple yet effective single-streamCNN-based architecture called AudioInceptionNeXt that operates on thetime-frequency log-mel-spectrogram of the audio samples. Motivated by thedesign of the InceptionNeXt, we propose parallel multi-scale depthwiseseparable convolutional kernels in the AudioInceptionNeXt block, which enablethe model to learn the time and frequency information more effectively. Thelarge-scale separable kernels capture the long duration of activities and theglobal frequency semantic information, while the small-scale separable kernelscapture the short duration of activities and local details of frequencyinformation. Our approach achieved 55.43% of top-1 accuracy on the challengetest set, ranked as 1st on the public leaderboard. Codes are availableanonymously at https://github.com/StevenLauHKHK/AudioInceptionNeXt.git.</description><author>Kin Wai Lau, Yasar Abbas Ur Rehman, Yuyang Xie, Lan Ma</author><pubDate>Fri, 14 Jul 2023 11:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07265v1</guid></item><item><title>On Interpolating Experts and Multi-Armed Bandits</title><link>http://arxiv.org/abs/2307.07264v1</link><description>Learning with expert advice and multi-armed bandit are two classic onlinedecision problems which differ on how the information is observed in each roundof the game. We study a family of problems interpolating the two. For a vector$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MABindicates that the arms are partitioned into $K$ groups and the $i$-th groupcontains $m_i$ arms. Once an arm is pulled, the losses of all arms in the samegroup are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MABand design an optimal PAC algorithm for its pure exploration version,$\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss withas few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MABis $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum numberof pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Bothour upper bounds and lower bounds for $\mathbf{m}$-MAB can be extended to amore general setting, namely the bandit with graph feedback, in terms of theclique cover and related graph parameters. As consequences, we obtained tightminimax regret bounds for several families of feedback graphs.</description><author>Houshuang Chen, Yuchen He, Chihao Zhang</author><pubDate>Fri, 14 Jul 2023 11:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07264v1</guid></item><item><title>MorphPiece : Moving away from Statistical Language Representation</title><link>http://arxiv.org/abs/2307.07262v1</link><description>Tokenization is a critical part of modern NLP pipelines. However,contemporary tokenizers for Large Language Models are based on statisticalanalysis of text corpora, without much consideration to the linguisticfeatures. We propose a linguistically motivated tokenization scheme,MorphPiece, which is based partly on morphological segmentation of theunderlying text. A GPT-style causal language model trained on this tokenizer(called MorphGPT) shows superior convergence compared to the same architecturetrained on a standard BPE tokenizer. Specifically we get Language Modelingperformance comparable to a 6 times larger model. Additionally, we evaluateMorphGPT on a variety of NLP tasks in supervised and unsupervised settings andfind superior performance across the board, compared to GPT-2 model.</description><author>Haris Jabbar</author><pubDate>Fri, 14 Jul 2023 11:35:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07262v1</guid></item><item><title>A Dynamic Points Removal Benchmark in Point Cloud Maps</title><link>http://arxiv.org/abs/2307.07260v1</link><description>In the field of robotics, the point cloud has become an essential maprepresentation. From the perspective of downstream tasks like localization andglobal path planning, points corresponding to dynamic objects will adverselyaffect their performance. Existing methods for removing dynamic points in pointclouds often lack clarity in comparative evaluations and comprehensiveanalysis. Therefore, we propose an easy-to-extend unified benchmarkingframework for evaluating techniques for removing dynamic points in maps. Itincludes refactored state-of-art methods and novel metrics to analyze thelimitations of these approaches. This enables researchers to dive deep into theunderlying reasons behind these limitations. The benchmark makes use of severaldatasets with different sensor types. All the code and datasets related to ourstudy are publicly available for further development and utilization.</description><author>Qingwen Zhang, Daniel Duberg, Ruoyu Geng, Mingkai Jia, Lujia Wang, Patric Jensfelt</author><pubDate>Fri, 14 Jul 2023 11:21:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07260v1</guid></item><item><title>Improving BERT with Hybrid Pooling Network and Drop Mask</title><link>http://arxiv.org/abs/2307.07258v1</link><description>Transformer-based pre-trained language models, such as BERT, achieve greatsuccess in various natural language understanding tasks. Prior research foundthat BERT captures a rich hierarchy of linguistic information at differentlayers. However, the vanilla BERT uses the same self-attention mechanism foreach layer to model the different contextual features. In this paper, wepropose a HybridBERT model which combines self-attention and pooling networksto encode different contextual features in each layer. Additionally, we proposea simple DropMask method to address the mismatch between pre-training andfine-tuning caused by excessive use of special mask tokens during MaskedLanguage Modeling pre-training. Experiments show that HybridBERT outperformsBERT in pre-training with lower loss, faster training speed (8% relative),lower memory cost (13% relative), and also in transfer learning with 1.5%relative higher accuracies on downstream tasks. Additionally, DropMask improvesaccuracies of BERT on downstream tasks across various masking rates.</description><author>Qian Chen, Wen Wang, Qinglin Zhang, Chong Deng, Ma Yukun, Siqi Zheng</author><pubDate>Fri, 14 Jul 2023 11:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07258v1</guid></item><item><title>The Re-Label Method For Data-Centric Machine Learning</title><link>http://arxiv.org/abs/2302.04391v4</link><description>In industry deep learning application, our manually labeled data has acertain number of noisy data. To solve this problem and achieve more than 90score in dev dataset, we present a simple method to find the noisy data andre-label the noisy data by human, given the model predictions as references inhuman labeling. In this paper, we illustrate our idea for a broad set of deeplearning tasks, includes classification, sequence tagging, object detection,sequence generation, click-through rate prediction. The experimental resultsand human evaluation results verify our idea.</description><author>Tong Guo</author><pubDate>Fri, 14 Jul 2023 11:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04391v4</guid></item><item><title>Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems</title><link>http://arxiv.org/abs/2307.07255v1</link><description>Sharing ideas through communication with peers is the primary mode of humaninteraction. Consequently, extensive research has been conducted in the area ofconversational AI, leading to an increase in the availability and diversity ofconversational tasks, datasets, and methods. However, with numerous tasks beingexplored simultaneously, the current landscape of conversational AI becomesfragmented. Therefore, initiating a well-thought-out model for a dialogue agentcan pose significant challenges for a practitioner. Towards highlighting thecritical ingredients needed for a practitioner to design a dialogue agent fromscratch, the current study provides a comprehensive overview of the primarycharacteristics of a dialogue agent, the supporting tasks, their correspondingopen-domain datasets, and the methods used to benchmark these datasets. Weobserve that different methods have been used to tackle distinct dialoguetasks. However, building separate models for each task is costly and does notleverage the correlation among the several tasks of a dialogue agent. As aresult, recent trends suggest a shift towards building unified foundationmodels. To this end, we propose UNIT, a UNified dIalogue dataseT constructedfrom conversations of existing datasets for different dialogue tasks capturingthe nuances for each of them. We also examine the evaluation strategies used tomeasure the performance of dialogue agents and highlight the scope for futureresearch in the area of conversational AI.</description><author>Shivani Kumar, Sumit Bhatia, Milan Aggarwal, Tanmoy Chakraborty</author><pubDate>Fri, 14 Jul 2023 11:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07255v1</guid></item><item><title>cOOpD: Reformulating COPD classification on chest CT scans as anomaly detection using contrastive representations</title><link>http://arxiv.org/abs/2307.07254v1</link><description>Classification of heterogeneous diseases is challenging due to theircomplexity, variability of symptoms and imaging findings. Chronic ObstructivePulmonary Disease (COPD) is a prime example, being underdiagnosed despite beingthe third leading cause of death. Its sparse, diffuse and heterogeneousappearance on computed tomography challenges supervised binary classification.We reformulate COPD binary classification as an anomaly detection task,proposing cOOpD: heterogeneous pathological regions are detected asOut-of-Distribution (OOD) from normal homogeneous lung regions. To this end, welearn representations of unlabeled lung regions employing a self-supervisedcontrastive pretext model, potentially capturing specific characteristics ofdiseased and healthy unlabeled regions. A generative model then learns thedistribution of healthy representations and identifies abnormalities (stemmingfrom COPD) as deviations. Patient-level scores are obtained by aggregatingregion OOD scores. We show that cOOpD achieves the best performance on twopublic datasets, with an increase of 8.2% and 7.7% in terms of AUROC comparedto the previous supervised state-of-the-art. Additionally, cOOpD yieldswell-interpretable spatial anomaly maps and patient-level scores which we showto be of additional value in identifying individuals in the early stage ofprogression. Experiments in artificially designed real-world prevalencesettings further support that anomaly detection is a powerful way of tacklingCOPD classification.</description><author>Silvia D. Almeida, Carsten T. Lüth, Tobias Norajitra, Tassilo Wald, Marco Nolden, Paul F. Jaeger, Claus P. Heussel, Jürgen Biederer, Oliver Weinheimer, Klaus Maier-Hein</author><pubDate>Fri, 14 Jul 2023 11:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07254v1</guid></item><item><title>Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning</title><link>http://arxiv.org/abs/2307.07250v1</link><description>Adversarial examples derived from deliberately crafted perturbations onvisual inputs can easily harm decision process of deep neural networks. Toprevent potential threats, various adversarial training-based defense methodshave grown rapidly and become a de facto standard approach for robustness.Despite recent competitive achievements, we observe that adversarialvulnerability varies across targets and certain vulnerabilities remainprevalent. Intriguingly, such peculiar phenomenon cannot be relieved even withdeeper architectures and advanced defense methods. To address this issue, inthis paper, we introduce a causal approach called Adversarial Double MachineLearning (ADML), which allows us to quantify the degree of adversarialvulnerability for network predictions and capture the effect of treatments onoutcome of interests. ADML can directly estimate causal parameter ofadversarial perturbations per se and mitigate negative effects that canpotentially damage robustness, bridging a causal perspective into theadversarial vulnerability. Through extensive experiments on various CNN andTransformer architectures, we corroborate that ADML improves adversarialrobustness with large margins and relieve the empirical observation.</description><author>Byung-Kwan Lee, Junho Kim, Yong Man Ro</author><pubDate>Fri, 14 Jul 2023 10:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07250v1</guid></item><item><title>The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios</title><link>http://arxiv.org/abs/2306.13734v2</link><description>The CHiME challenges have played a significant role in the development andevaluation of robust automatic speech recognition (ASR) systems. We introducethe CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This taskcomprises joint ASR and diarization in far-field settings with multiple, andpossibly heterogeneous, recording devices. Different from previous challenges,we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. Thegoal is for participants to devise a single system that can generalize acrossdifferent array geometries and use cases with no a-priori information. Anotherdeparture from earlier CHiME iterations is that participants are allowed to useopen-source pre-trained models and datasets. In this paper, we describe thechallenge design, motivation, and fundamental research questions in detail. Wealso present the baseline system, which is fully array-topology agnostic andfeatures multi-channel diarization, channel selection, guided source separationand a robust ASR model that leverages self-supervised speech representations(SSLR).</description><author>Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola Garcia, Matthew Maciejewski, Yoshiki Masuyama, Zhong-Qiu Wang, Stefano Squartini, Sanjeev Khudanpur</author><pubDate>Fri, 14 Jul 2023 10:45:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13734v2</guid></item><item><title>Rigorous Runtime Analysis of Diversity Optimization with GSEMO on OneMinMax</title><link>http://arxiv.org/abs/2307.07248v1</link><description>The evolutionary diversity optimization aims at finding a diverse set ofsolutions which satisfy some constraint on their fitness. In the context ofmulti-objective optimization this constraint can require solutions to bePareto-optimal. In this paper we study how the GSEMO algorithm with additionaldiversity-enhancing heuristic optimizes a diversity of its population on abi-objective benchmark problem OneMinMax, for which all solutions arePareto-optimal. We provide a rigorous runtime analysis of the last step of the optimization,when the algorithm starts with a population with a second-best diversity, andprove that it finds a population with optimal diversity in expected time$O(n^2)$, when the problem size $n$ is odd. For reaching our goal, we analysethe random walk of the population, which reflects the frequency of changes inthe population and their outcomes.</description><author>Denis Antipov, Aneta Neumann, Frank Neumann</author><pubDate>Fri, 14 Jul 2023 10:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07248v1</guid></item><item><title>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</title><link>http://arxiv.org/abs/2307.05300v2</link><description>Human intelligence thrives on the concept of cognitive synergy, wherecollaboration and information integration among different cognitive processesyield superior outcomes compared to individual cognitive processes inisolation. Although Large Language Models (LLMs) have demonstrated promisingperformance as general task-solving agents, they still struggle with tasks thatrequire intensive domain knowledge and complex reasoning. In this work, wepropose Solo Performance Prompting (SPP), which transforms a single LLM into acognitive synergist by engaging in multi-turn self-collaboration with multiplepersonas. A cognitive synergist refers to an intelligent agent thatcollaborates with multiple minds, combining their individual strengths andknowledge, to enhance problem-solving and overall performance in complex tasks.By dynamically identifying and simulating different personas based on taskinputs, SPP unleashes the potential of cognitive synergy in LLMs. We havediscovered that assigning multiple, fine-grained personas in LLMs elicitsbetter problem-solving abilities compared to using a single or fixed number ofpersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,Codenames Collaborative, and Logic Grid Puzzle, encompassing bothknowledge-intensive and reasoning-intensive types. Unlike previous works, suchas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPPeffectively elicits internal knowledge acquisition abilities, reduceshallucination, and maintains strong reasoning capabilities. Code, data, andprompts can be found at:https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.</description><author>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji</author><pubDate>Fri, 14 Jul 2023 10:38:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05300v2</guid></item></channel></rss>