<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 11 Feb 2025 13:00:14 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>EVEv2: Improved Baselines for Encoder-Free Vision-Language Models</title><link>http://arxiv.org/abs/2502.06788v1</link><description>Existing encoder-free vision-language models (VLMs) are rapidly narrowing theperformance gap with their encoder-based counterparts, highlighting thepromising potential for unified multimodal systems with structural simplicityand efficient deployment. We systematically clarify the performance gap betweenVLMs using pre-trained vision encoders, discrete tokenizers, and minimalistvisual layers from scratch, deeply excavating the under-examinedcharacteristics of encoder-free VLMs. We develop efficient strategies forencoder-free VLMs that rival mainstream encoder-based ones. After an in-depthinvestigation, we launch EVEv2.0, a new and improved family of encoder-freeVLMs. We show that: (i) Properly decomposing and hierarchically associatingvision and language within a unified model reduces interference betweenmodalities. (ii) A well-designed training strategy enables effectiveoptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0represents a thorough study for developing a decoder-only architecture acrossmodalities, demonstrating superior data efficiency and strong vision-reasoningcapability. Code is publicly available at: https://github.com/baaivision/EVE.</description><author>Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, Xinlong Wang</author><pubDate>Mon, 10 Feb 2025 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06788v1</guid></item><item><title>Visual Agentic AI for Spatial Reasoning with a Dynamic API</title><link>http://arxiv.org/abs/2502.06787v1</link><description>Visual reasoning -- the ability to interpret the visual world -- is crucialfor embodied agents that operate within three-dimensional scenes. Progress inAI has led to vision and language models capable of answering questions fromimages. However, their performance declines when tasked with 3D spatialreasoning. To tackle the complexity of such reasoning problems, we introduce anagentic program synthesis approach where LLM agents collaboratively generate aPythonic API with new functions to solve common subproblems. Our methodovercomes limitations of prior approaches that rely on a static, human-definedAPI, allowing it to handle a wider range of queries. To assess AI capabilitiesfor 3D understanding, we introduce a new benchmark of queries involvingmultiple steps of grounding and inference. We show that our method outperformsprior zero-shot models for visual reasoning in 3D and empirically validate theeffectiveness of our agentic framework for 3D spatial reasoning tasks. Projectwebsite: https://glab-caltech.github.io/vadar/</description><author>Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari</author><pubDate>Mon, 10 Feb 2025 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06787v1</guid></item><item><title>Matryoshka Quantization</title><link>http://arxiv.org/abs/2502.06786v1</link><description>Quantizing model weights is critical for reducing the communication andinference costs of large models. However, quantizing models -- especially tolow precisions like int4 or int2 -- requires a trade-off in model quality;int2, in particular, is known to severely degrade model quality. Consequently,practitioners are often forced to maintain multiple models with differentquantization levels or serve a single model that best satisfies thequality-latency trade-off. On the other hand, integer data types, such as int8,inherently possess a nested (Matryoshka) structure where smaller bit-widthintegers, like int4 or int2, are nested within the most significant bits. Thispaper proposes Matryoshka Quantization (MatQuant), a novel multi-scalequantization technique that addresses the challenge of needing multiplequantized models. It allows training and maintaining just one model, which canthen be served at different precision levels. Furthermore, due to theco-training and co-distillation regularization provided by MatQuant, the int2precision models extracted by MatQuant can be up to $10\%$ more accurate thanstandard int2 quantization (using techniques like QAT or OmniQuant). Thisrepresents significant progress in model quantization, demonstrated by the factthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is moreaccurate than an int8 FFN-quantized Gemma-2 2B model.</description><author>Pranav Nair, Puranjay Datta, Jeff Dean, Prateek Jain, Aditya Kusupati</author><pubDate>Mon, 10 Feb 2025 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06786v1</guid></item><item><title>DeepCrossAttention: Supercharging Transformer Residual Connections</title><link>http://arxiv.org/abs/2502.06785v1</link><description>Transformer networks have achieved remarkable success across diverse domains,leveraging a variety of architectural innovations, including residualconnections. However, traditional residual connections, which simply sum theoutputs of previous layers, can dilute crucial information. This workintroduces DeepCrossAttention (DCA), an approach that enhances residuallearning in transformers. DCA employs learnable, input-dependent weights todynamically combine layer outputs, enabling the model to selectively focus onthe most relevant information in any of the previous layers. Furthermore, DCAincorporates depth-wise cross-attention, allowing for richer interactionsbetween layers at different depths. Our language modeling experiments show thatDCA achieves improved perplexity for a given training time. Moreover, DCAobtains the same model quality up to 3x faster while adding a negligible numberof parameters. Theoretical analysis confirms that DCA provides an improvedtrade-off between accuracy and model size when the ratio of collective layerranks to the ambient dimension falls below a critical threshold.</description><author>Mike Heddes, Adel Javanmard, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni, Vahab Mirrokni</author><pubDate>Mon, 10 Feb 2025 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06785v1</guid></item><item><title>RelGNN: Composite Message Passing for Relational Deep Learning</title><link>http://arxiv.org/abs/2502.06784v1</link><description>Predictive tasks on relational databases are critical in real-worldapplications spanning e-commerce, healthcare, and social media. To addressthese tasks effectively, Relational Deep Learning (RDL) encodes relational dataas graphs, enabling Graph Neural Networks (GNNs) to exploit relationalstructures for improved predictions. However, existing heterogeneous GNNs oftenoverlook the intrinsic structural properties of relational databases, leadingto modeling inefficiencies. Here we introduce RelGNN, a novel GNN frameworkspecifically designed to capture the unique characteristics of relationaldatabases. At the core of our approach is the introduction of atomic routes,which are sequences of nodes forming high-order tripartite structures. Buildingupon these atomic routes, RelGNN designs new composite message passingmechanisms between heterogeneous nodes, allowing direct single-hop interactionsbetween them. This approach avoids redundant aggregations and mitigatesinformation entanglement, ultimately leading to more efficient and accuratepredictive modeling. RelGNN is evaluated on 30 diverse real-world tasks fromRelBench (Fey et al., 2024), and consistently achieves state-of-the-artaccuracy with up to 25% improvement.</description><author>Tianlang Chen, Charilaos Kanatsoulis, Jure Leskovec</author><pubDate>Mon, 10 Feb 2025 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06784v1</guid></item><item><title>Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT</title><link>http://arxiv.org/abs/2502.06782v1</link><description>Recent advancements have established Diffusion Transformers (DiTs) as adominant framework in generative modeling. Building on this success,Lumina-Next achieves exceptional performance in the generation ofphotorealistic images with Next-DiT. However, its potential for videogeneration remains largely untapped, with significant challenges in modelingthe spatiotemporal complexity inherent to video data. To address this, weintroduce Lumina-Video, a framework that leverages the strengths of Next-DiTwhile introducing tailored solutions for video synthesis. Lumina-Videoincorporates a Multi-scale Next-DiT architecture, which jointly learns multiplepatchifications to enhance both efficiency and flexibility. By incorporatingthe motion score as an explicit condition, Lumina-Video also enables directcontrol of generated videos' dynamic degree. Combined with a progressivetraining scheme with increasingly higher resolution and FPS, and a multi-sourcetraining scheme with mixed natural and synthetic data, Lumina-Video achievesremarkable aesthetic quality and motion smoothness at high training andinference efficiency. We additionally propose Lumina-V2A, a video-to-audiomodel based on Next-DiT, to create synchronized sounds for generated videos.Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.</description><author>Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, Peng Gao</author><pubDate>Mon, 10 Feb 2025 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06782v1</guid></item><item><title>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning</title><link>http://arxiv.org/abs/2502.06781v1</link><description>Reasoning abilities, especially those for solving complex math problems, arecrucial components of general intelligence. Recent advances by proprietarycompanies, such as o-series models of OpenAI, have made remarkable progress onreasoning tasks. However, the complete technical details remain unrevealed, andthe techniques that are believed certainly to be adopted are only reinforcementlearning (RL) and the long chain of thoughts. This paper proposes a new RLframework, termed OREAL, to pursue the performance limit that can be achievedthrough \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement\textbf{L}earning for mathematical reasoning tasks, where only binary outcomerewards are easily accessible. We theoretically prove that behavior cloning onpositive trajectories from best-of-N (BoN) sampling is sufficient to learn theKL-regularized optimal policy in binary feedback environments. This formulationfurther implies that the rewards of negative samples should be reshaped toensure the gradient consistency between positive and negative samples. Toalleviate the long-existing difficulties brought by sparse rewards in RL, whichare even exacerbated by the partial correctness of the long chain of thoughtfor reasoning tasks, we further apply a token-level reward model to sampleimportant tokens in reasoning trajectories for learning. With OREAL, for thefirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,being on par with 32B models. OREAL-32B also surpasses previous 32B modelstrained by distillation with 95.0 pass@1 accuracy on MATH-500. Ourinvestigation also indicates the importance of initial policy models andtraining queries for RL. Code, models, and data will be released to benefitfuture research\footnote{https://github.com/InternLM/OREAL}.</description><author>Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, Kai Chen</author><pubDate>Mon, 10 Feb 2025 18:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06781v1</guid></item><item><title>Ensemble of Large Language Models for Curated Labeling and Rating of Free-text Data</title><link>http://arxiv.org/abs/2501.08413v2</link><description>Free-text responses are commonly collected in psychological studies,providing rich qualitative insights that quantitative measures may not capture.Labeling curated topics of research interest in free-text data by multipletrained human coders is typically labor-intensive and time-consuming. Thoughlarge language models (LLMs) excel in language processing, LLM-assistedlabeling techniques relying on closed-source LLMs cannot be directly applied tofree-text data, without explicit consent for external use. In this study, we propose a framework of assembling locally-deployable LLMsto enhance the labeling of predetermined topics in free-text data under privacyconstraints. Analogous to annotation by multiple human raters, this frameworkleverages the heterogeneity of diverse open-source LLMs. The ensemble approachseeks a balance between the agreement and disagreement across LLMs, guided by arelevancy scoring methodology that utilizes embedding distances between topicdescriptions and LLMs' reasoning. We evaluated the ensemble approach using bothpublicly accessible Reddit data from eating disorder related forums, andfree-text responses from eating disorder patients, both complemented by humanannotations. We found that: (1) there is heterogeneity in the performance of labelingamong same-sized LLMs, with some showing low sensitivity but high precision,while others exhibit high sensitivity but low precision. (2) Compared toindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimalprecision-sensitivity trade-off in predicting human annotations. (3) Therelevancy scores across LLMs showed greater agreement than dichotomous labels,indicating that the relevancy scoring method effectively mitigates theheterogeneity in LLMs' labeling.</description><author>Jiaxing Qiu, Dongliang Guo, Natalie Papini, Noelle Peace, Cheri A. Levinson, Teague R. Henry</author><pubDate>Mon, 10 Feb 2025 18:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08413v2</guid></item><item><title>KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission for Visual Classification</title><link>http://arxiv.org/abs/2502.06779v1</link><description>Fine-tuning pre-trained vision models for specific tasks is a common practicein computer vision. However, this process becomes more expensive as models growlarger. Recently, parameter-efficient fine-tuning (PEFT) methods have emergedas a popular solution to improve training efficiency and reduce storage needsby tuning additional low-rank modules within pre-trained backbones. Despitetheir advantages, they struggle with limited representation capabilities andmisalignment with pre-trained intermediate features. To address these issues,we introduce an innovative Multi-Kernel Kronecker Adaptation with Re-ScalingTransmission (KARST) for various recognition tasks. Specifically, itsmulti-kernel design extends Kronecker projections horizontally and separatesadaptation matrices into multiple complementary spaces, reducing parameterdependency and creating more compact subspaces. Besides, it incorporates extralearnable re-scaling factors to better align with pre-trained featuredistributions, allowing for more flexible and balanced feature aggregation.Extensive experiments validate that our KARST outperforms other PEFTcounterparts with a negligible inference cost due to its re-parameterizationcharacteristics. Code is publicly available at:https://github.com/Lucenova/KARST.</description><author>Yue Zhu, Haiwen Diao, Shang Gao, Long Chen, Huchuan Lu</author><pubDate>Mon, 10 Feb 2025 18:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06779v1</guid></item><item><title>Guided and Variance-Corrected Fusion with One-shot Style Alignment for Large-Content Image Generation</title><link>http://arxiv.org/abs/2412.12771v2</link><description>Producing large images using small diffusion models is gaining increasingpopularity, as the cost of training large models could be prohibitive. A commonapproach involves jointly generating a series of overlapped image patches andobtaining large images by merging adjacent patches. However, results fromexisting methods often exhibit noticeable artifacts, e.g., seams andinconsistent objects and styles. To address the issues, we proposed GuidedFusion (GF), which mitigates the negative impact from distant image regions byapplying a weighted average to the overlapping regions. Moreover, we proposedVariance-Corrected Fusion (VCF), which corrects data variance atpost-averaging, generating more accurate fusion for the Denoising DiffusionProbabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA),which generates a coherent style for large images by adjusting the initialinput noise without adding extra computational burden. Extensive experimentsdemonstrated that the proposed fusion methods improved the quality of thegenerated image significantly. The proposed method can be widely applied as aplug-and-play module to enhance other fusion-based methods for large imagegeneration. Code: https://github.com/TitorX/GVCFDiffusion</description><author>Shoukun Sun, Min Xian, Tiankai Yao, Fei Xu, Luca Capriotti</author><pubDate>Mon, 10 Feb 2025 18:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12771v2</guid></item><item><title>Learning an Optimal Assortment Policy under Observational Data</title><link>http://arxiv.org/abs/2502.06777v1</link><description>We study the fundamental problem of offline assortment optimization under theMultinomial Logit (MNL) model, where sellers must determine the optimal subsetof the products to offer based solely on historical customer choice data. Whilemost existing approaches to learning-based assortment optimization focus on theonline learning of the optimal assortment through repeated interactions withcustomers, such exploration can be costly or even impractical in manyreal-world settings. In this paper, we consider the offline learning paradigmand investigate the minimal data requirements for efficient offline assortmentoptimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), analgorithm that combines rank-breaking with pessimistic estimation. We provethat PRB is nearly minimax optimal by establishing the tight suboptimalityupper bound and a nearly matching lower bound. This further shows that "optimalitem coverage" - where each item in the optimal assortment appears sufficientlyoften in the historical data - is both sufficient and necessary for efficientoffline learning. This significantly relaxes the previous requirement ofobserving the complete optimal assortment in the data. Our results providefundamental insights into the data requirements for offline assortmentoptimization under the MNL model.</description><author>Yuxuan Han, Han Zhong, Miao Lu, Jose Blanchet, Zhengyuan Zhou</author><pubDate>Mon, 10 Feb 2025 18:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06777v1</guid></item><item><title>Grounding Text-to-Image Diffusion Models for Controlled High-Quality Image Generation</title><link>http://arxiv.org/abs/2501.09194v2</link><description>Text-to-image (T2I) generative diffusion models have demonstrated outstandingperformance in synthesizing diverse, high-quality visuals from text captions.Several layout-to-image models have been developed to control the generationprocess by utilizing a wide range of layouts, such as segmentation maps, edges,and human keypoints. In this work, we propose ObjectDiffusion, a model thatconditions T2I diffusion models on semantic and spatial grounding information,enabling the precise rendering and placement of desired objects in specificlocations defined by bounding boxes. To achieve this, we make substantialmodifications to the network architecture introduced in ControlNet to integrateit with the grounding method proposed in GLIGEN. We fine-tune ObjectDiffusionon the COCO2017 training dataset and evaluate it on the COCO2017 validationdataset. Our model improves the precision and quality of controllable imagegeneration, achieving an AP$_{\text{50}}$ of 46.6, an AR of 44.5, and an FID of19.8, outperforming the current SOTA model trained on open-source datasetsacross all three metrics. ObjectDiffusion demonstrates a distinctive capabilityin synthesizing diverse, high-quality, high-fidelity images that seamlesslyconform to the semantic and spatial control layout. Evaluated in qualitativeand quantitative tests, ObjectDiffusion exhibits remarkable groundingcapabilities in closed-set and open-set vocabulary settings across a widevariety of contexts. The qualitative assessment verifies the ability ofObjectDiffusion to generate multiple detailed objects in varying sizes, forms,and locations.</description><author>Ahmad Süleyman, Göksel Biricik</author><pubDate>Mon, 10 Feb 2025 18:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09194v2</guid></item><item><title>Towards Internet-Scale Training For Agents</title><link>http://arxiv.org/abs/2502.06776v1</link><description>The predominant approach for training web navigation agents gathers humandemonstrations for a set of popular websites and hand-written tasks, but it isbecoming clear that human data are an inefficient resource. We develop apipeline to facilitate Internet-scale training for agents without laborioushuman annotations. In the first stage, an LLM generates tasks for 150k diversewebsites. In the next stage, LLM agents complete tasks and producetrajectories. In the final stage, an LLM reviews the trajectories and judgestheir success. Language models are competitive with human annotators, detectingand filtering out harmful content with an accuracy of 97%, generating feasibletasks with an 89% rate, and judging successful trajectories with an 82.6%accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% oftasks for 150k sites. Training on the data generated by our pipeline iscompetitive with training on human demonstrations. In data-limited settingsderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and+122.1% respectively for agents trained on mixtures of data from our pipeline,and human data. When training agents with all available human data from thesebenchmarks, agents fail to generalize to diverse real sites, and adding ourdata improves their generalization by +149.0% for WebLINX and +156.3% forMind2Web. Code will be available at: data-for-agents.github.io.</description><author>Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, Ruslan Salakhutdinov</author><pubDate>Mon, 10 Feb 2025 18:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06776v1</guid></item><item><title>Enhancing Performance of Explainable AI Models with Constrained Concept Refinement</title><link>http://arxiv.org/abs/2502.06775v1</link><description>The trade-off between accuracy and interpretability has long been a challengein machine learning (ML). This tension is particularly significant for emerginginterpretable-by-design methods, which aim to redesign ML algorithms fortrustworthy interpretability but often sacrifice accuracy in the process. Inthis paper, we address this gap by investigating the impact of deviations inconcept representations-an essential component of interpretable models-onprediction performance and propose a novel framework to mitigate these effects.The framework builds on the principle of optimizing concept embeddings underconstraints that preserve interpretability. Using a generative model as atest-bed, we rigorously prove that our algorithm achieves zero loss whileprogressively enhancing the interpretability of the resulting model.Additionally, we evaluate the practical performance of our proposed frameworkin generating explainable predictions for image classification tasks acrossvarious benchmarks. Compared to existing explainable methods, our approach notonly improves prediction accuracy while preserving model interpretabilityacross various large-scale benchmarks but also achieves this with significantlylower computational cost.</description><author>Geyu Liang, Senne Michielssen, Salar Fattahi</author><pubDate>Mon, 10 Feb 2025 18:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06775v1</guid></item><item><title>When Witnesses Defend: A Witness Graph Topological Layer for Adversarial Graph Learning</title><link>http://arxiv.org/abs/2409.14161v3</link><description>Capitalizing on the intuitive premise that shape characteristics are morerobust to perturbations, we bridge adversarial graph learning with the emergingtools from computational topology, namely, persistent homology representationsof graphs. We introduce the concept of witness complex to adversarial analysison graphs, which allows us to focus only on the salient shape characteristicsof graphs, yielded by the subset of the most essential nodes (i.e., landmarks),with minimal loss of topological information on the whole graph. The remainingnodes are then used as witnesses, governing which higher-order graphsubstructures are incorporated into the learning process. Armed with thewitness mechanism, we design Witness Graph Topological Layer (WGTL), whichsystematically integrates both local and global topological graph featurerepresentations, the impact of which is, in turn, automatically controlled bythe robust regularized topological loss. Given the attacker's budget, we derivethe important stability guarantees of both local and global topology encodingsand the associated robust topological loss. We illustrate the versatility andefficiency of WGTL by its integration with five GNNs and three existingnon-topological defense mechanisms. Our extensive experiments across sixdatasets demonstrate that WGTL boosts the robustness of GNNs across a range ofperturbations and against a range of adversarial attacks. Our datasets andsource codes are available at https://github.com/toggled/WGTL.</description><author>Naheed Anjum Arafat, Debabrota Basu, Yulia Gel, Yuzhou Chen</author><pubDate>Mon, 10 Feb 2025 18:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14161v3</guid></item><item><title>ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural Projection</title><link>http://arxiv.org/abs/2502.06774v1</link><description>Ensuring neural networks adhere to domain-specific constraints is crucial foraddressing safety and ethical concerns while also enhancing predictionaccuracy. Despite the nonlinear nature of most real-world tasks, existingmethods are predominantly limited to affine or convex constraints. We introduceENFORCE, a neural network architecture that guarantees predictions to satisfynonlinear constraints exactly. ENFORCE is trained with standard unconstrainedgradient-based optimizers (e.g., Adam) and leverages autodifferentiation andlocal neural projections to enforce any $\mathcal{C}^1$ constraint to arbitrarytolerance $\epsilon$. We build an adaptive-depth neural projection (AdaNP)module that dynamically adjusts its complexity to suit the specific problem andthe required tolerance levels. ENFORCE guarantees satisfaction of equalityconstraints that are nonlinear in both inputs and outputs of the neural networkwith minimal (and adjustable) computational cost.</description><author>Giacomo Lastrucci, Artur M. Schweidtmann</author><pubDate>Mon, 10 Feb 2025 18:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06774v1</guid></item><item><title>On the Emergence of Thinking in LLMs I: Searching for the Right Intuition</title><link>http://arxiv.org/abs/2502.06773v1</link><description>Recent AI advancements, such as OpenAI's new models, are transforming LLMsinto LRMs (Large Reasoning Models) that perform reasoning during inference,taking extra time and compute for higher-quality outputs. We aim to uncover thealgorithmic framework for training LRMs. Methods like self-consistency, PRM,and AlphaZero suggest reasoning as guided search. We ask: what is the simplest,most scalable way to enable search in LLMs? We propose a post-training framework called Reinforcement Learning viaSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning withhuman or synthetic demonstrations of the reasoning process, (2) using anexploration reward signal to encourage diverse and efficient reasoningbehaviors, and (3) RL training with an outcome verifier to ensure correctnesswhile preventing reward hacking. Our key innovation is to decouple explorationand correctness signals during PPO training, carefully balancing them toimprove performance and efficiency. Empirical studies in the math domain show that RLSP improves reasoning. Onthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% dueto RLSP. However, a more important finding of this work is that the modelstrained using RLSP, even with the simplest exploration reward that encouragesthe model to take more intermediate steps, showed several emergent behaviorssuch as backtracking, exploration of ideas, and verification. These findingsdemonstrate that RLSP framework might be enough to enable emergence of complexreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to whyRLSP search strategy is more suitable for LLMs inspired by a remarkable resultthat says CoT provably increases computational power of LLMs, which grows asthe number of steps in CoT \cite{li2024chain,merrill2023expresssive}.</description><author>Guanghao Ye, Khiem Duc Pham, Xinzhi Zhang, Sivakanth Gopi, Baolin Peng, Beibin Li, Janardhan Kulkarni, Huseyin A. Inan</author><pubDate>Mon, 10 Feb 2025 18:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06773v1</guid></item><item><title>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</title><link>http://arxiv.org/abs/2502.06772v1</link><description>We present that hierarchical LLM reasoning via scaling thought templates caneffectively optimize the reasoning search space and outperform the mathematicalreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.We train our ReasonFlux-32B model with only 8 GPUs and introduces threeinnovations: (i) a structured and generic thought template library, containingaround 500 high-level thought templates capable of generalizing to similar orrelevant reasoning problems; (ii) performing hierarchical reinforcementlearning on a sequence of thought templates instead of long CoTs, optimizing abase LLM to plan out an optimal template trajectory for gradually handlingcomplex problems; (iii) a brand new inference scaling system that enableshierarchical LLM reasoning by adaptively scaling thought templates at inferencetime. With a template trajectory containing sequential thought templates, ourReasonFlux-32B significantly advances math reasoning capabilities tostate-of-the-art levels. Notably, on the MATH benchmark, it achieves anaccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:https://github.com/Gen-Verse/ReasonFlux</description><author>Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</author><pubDate>Mon, 10 Feb 2025 18:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06772v1</guid></item><item><title>Unsupervised Particle Tracking with Neuromorphic Computing</title><link>http://arxiv.org/abs/2502.06771v1</link><description>We study the application of a neural network architecture for identifyingcharged particle trajectories via unsupervised learning of delays and synapticweights using a spike-time-dependent plasticity rule. In the considered model,the neurons receive time-encoded information on the position of particle hitsin a tracking detector for a particle collider, modeled according to thegeometry of the Compact Muon Solenoid Phase II detector. We show how a spikingneural network is capable of successfully identifying in a completelyunsupervised way the signal left by charged particles in the presence ofconspicuous noise from accidental or combinatorial hits. These results open theway to applications of neuromorphic computing to particle tracking, motivatingfurther studies into its potential for real-time, low-power particle trackingin future high-energy physics experiments.</description><author>Emanuele Coradin, Fabio Cufino, Muhammad Awais, Tommaso Dorigo, Enrico Lupi, Eleonora Porcu, Jinu Raj, Fredrik Sandin, Mia Tosi</author><pubDate>Mon, 10 Feb 2025 18:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06771v1</guid></item><item><title>Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions</title><link>http://arxiv.org/abs/2502.06768v1</link><description>In recent years, masked diffusion models (MDMs) have emerged as a promisingalternative approach for generative modeling over discrete domains. Compared toautoregressive models (ARMs), MDMs trade off complexity at training time withflexibility at inference time. At training time, they must learn to solve anexponentially large number of infilling problems, but at inference time, theycan decode tokens in essentially arbitrary order. In this work, we closelyexamine these two competing effects. On the training front, we theoreticallyand empirically demonstrate that MDMs indeed train on computationallyintractable subproblems compared to their autoregressive counterparts. On theinference front, we show that a suitable strategy for adaptively choosing thetoken decoding order significantly enhances the capabilities of MDMs, allowingthem to sidestep hard subproblems. On logic puzzles like Sudoku, we show thatadaptive inference can boost solving accuracy in pretrained MDMs from $&lt;7$% to$\approx 90$%, even outperforming ARMs with $7\times$ as many parameters andthat were explicitly trained via teacher forcing to learn the right order ofdecoding.</description><author>Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, Sitan Chen</author><pubDate>Mon, 10 Feb 2025 18:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06768v1</guid></item><item><title>Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs</title><link>http://arxiv.org/abs/2502.06766v1</link><description>There is growing demand for performing inference with hundreds of thousandsof input tokens on trained transformer models. Inference at this extreme scaledemands significant computational resources, hindering the application oftransformers at long contexts on commodity (i.e not data center scale)hardware. To address the inference time costs associated with runningself-attention based transformer language models on long contexts and enabletheir adoption on widely available hardware, we propose a tunable mechanismthat reduces the cost of the forward pass by attending to only the mostrelevant tokens at every generation step using a top-k selection mechanism. Weshowcase the efficiency gains afforded by our method by performing inference oncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Ourexperiments reveal that models are capable of handling the sparsity induced bythe reduced number of keys and values. By attending to less than 2% of inputtokens, we achieve over 95% of model performance on common long contextbenchmarks (LM-Eval, AlpacaEval, and RULER).</description><author>Ryan Synk, Monte Hoover, John Kirchenbauer, Neel Jain, Alex Stein, Manli Shu, Josue Melendez Sanchez, Ramani Duraiswami, Tom Goldstein</author><pubDate>Mon, 10 Feb 2025 18:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06766v1</guid></item><item><title>Are all models wrong? Fundamental limits in distribution-free empirical model falsification</title><link>http://arxiv.org/abs/2502.06765v1</link><description>In statistics and machine learning, when we train a fitted model on availabledata, we typically want to ensure that we are searching within a model classthat contains at least one accurate model -- that is, we would like to ensurean upper bound on the model class risk (the lowest possible risk that can beattained by any model in the class). However, it is also of interest toestablish lower bounds on the model class risk, for instance so that we candetermine whether our fitted model is at least approximately optimal within theclass, or, so that we can decide whether the model class is unsuitable for theparticular task at hand. Particularly in the setting of interpolation learningwhere machine learning models are trained to reach zero error on the trainingdata, we might ask if, at the very least, a positive lower bound on the modelclass risk is possible -- or are we unable to detect that "all models arewrong"? In this work, we answer these questions in a distribution-free settingby establishing a model-agnostic, fundamental hardness result for the problemof constructing a lower bound on the best test error achievable over a modelclass, and examine its implications on specific model classes such astree-based methods and linear regression.</description><author>Manuel M. Müller, Yuetian Luo, Rina Foygel Barber</author><pubDate>Mon, 10 Feb 2025 18:44:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06765v1</guid></item><item><title>History-Guided Video Diffusion</title><link>http://arxiv.org/abs/2502.06764v1</link><description>Classifier-free guidance (CFG) is a key technique for improving conditionalgeneration in diffusion models, enabling more accurate control while enhancingsample quality. It is natural to extend this technique to video diffusion,which generates video conditioned on a variable number of context frames,collectively referred to as history. However, we find two key challenges toguiding with variable-length history: architectures that only supportfixed-size conditioning, and the empirical observation that CFG-style historydropout performs poorly. To address this, we propose the Diffusion ForcingTransformer (DFoT), a video diffusion architecture and theoretically groundedtraining objective that jointly enable conditioning on a flexible number ofhistory frames. We then introduce History Guidance, a family of guidancemethods uniquely enabled by DFoT. We show that its simplest form, vanillahistory guidance, already significantly improves video generation quality andtemporal consistency. A more advanced method, history guidance across time andfrequency further enhances motion dynamics, enables compositionalgeneralization to out-of-distribution history, and can stably roll outextremely long videos. Website: https://boyuan.space/history-guidance</description><author>Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, Vincent Sitzmann</author><pubDate>Mon, 10 Feb 2025 18:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06764v1</guid></item><item><title>Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts</title><link>http://arxiv.org/abs/2410.10626v2</link><description>Adapting medical Large Language Models to local languages can reduce barriersto accessing healthcare services, but data scarcity remains a significantchallenge, particularly for low-resource languages. To address this, we firstconstruct a high-quality medical dataset and conduct analysis to ensure itsquality. In order to leverage the generalization capability of multilingualLLMs to efficiently scale to more resource-constrained languages, we explorethe internal information flow of LLMs from a multilingual perspective usingMixture of Experts (MoE) modularity. Technically, we propose a novel MoErouting method that employs language-specific experts and cross-lingualrouting. Inspired by circuit theory, our routing analysis revealed a Spread Outin the End information flow mechanism: while earlier layers concentratecross-lingual information flow, the later layers exhibit language-specificdivergence. This insight directly led to the development of the Post-MoEarchitecture, which applies sparse routing only in the later layers whilemaintaining dense others. Experimental results demonstrate that this approachenhances the generalization of multilingual models to other languages whilepreserving interpretability. Finally, to efficiently scale the model to 50languages, we introduce the concept of language family experts, drawing onlinguistic priors, which enables scaling the number of languages without addingadditional parameters.</description><author>Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, Benyou Wang</author><pubDate>Mon, 10 Feb 2025 18:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10626v2</guid></item><item><title>When, Where and Why to Average Weights?</title><link>http://arxiv.org/abs/2502.06761v1</link><description>Averaging checkpoints along the training trajectory is a simple yet powerfulapproach to improve the generalization performance of Machine Learning modelsand reduce training time. Motivated by these potential gains, and in an effortto fairly and thoroughly benchmark this technique, we present an extensiveevaluation of averaging techniques in modern Deep Learning, which we performusing AlgoPerf \citep{dahl_benchmarking_2023}, a large-scale benchmark foroptimization algorithms. We investigate whether weight averaging can reducetraining time, improve generalization, and replace learning rate decay, assuggested by recent literature. Our evaluation across seven architectures anddatasets reveals that averaging significantly accelerates training and yieldsconsiderable efficiency gains, at the price of a minimal implementation andmemory cost, while mildly improving generalization across all consideredworkloads. Finally, we explore the relationship between averaging and learningrate annealing and show how to optimally combine the two to achieve the bestperformances.</description><author>Niccolò Ajroldi, Antonio Orvieto, Jonas Geiping</author><pubDate>Mon, 10 Feb 2025 18:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06761v1</guid></item><item><title>ACECODER: Acing Coder RL via Automated Test-Case Synthesis</title><link>http://arxiv.org/abs/2502.01718v3</link><description>Most progress in recent coder models has been driven by supervisedfine-tuning (SFT), while the potential of reinforcement learning (RL) remainslargely unexplored, primarily due to the lack of reliable reward data/model inthe code domain. In this paper, we address this challenge by leveragingautomated large-scale test-case synthesis to enhance code model training.Specifically, we design a pipeline that generates extensive (question,test-cases) pairs from existing code data. Using these test cases, we constructpreference pairs based on pass rates over sampled programs to train rewardmodels with Bradley-Terry loss. It shows an average of 10-point improvement forLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins throughbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.Furthermore, we conduct reinforcement learning with both reward models andtest-case pass rewards, leading to consistent improvements across HumanEval,MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-styletraining to start from Qwen2.5-Coder-base directly and show that our RLtraining can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\%for merely 80 optimization steps. We believe our results highlight the hugepotential of reinforcement learning in coder models.</description><author>Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, Wenhu Chen</author><pubDate>Mon, 10 Feb 2025 18:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.01718v3</guid></item><item><title>SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning</title><link>http://arxiv.org/abs/2409.17755v2</link><description>This paper addresses a challenging interactive task learning scenario we callrearrangement under unawareness: to manipulate a rigid-body environment in acontext where the agent is unaware of a concept that is key to solving theinstructed task. We propose SECURE, an interactive task learning frameworkdesigned to solve such problems. It uses embodied conversation to fix itsdeficient domain model -- through dialogue, the agent discovers and then learnsto exploit unforeseen possibilities. In particular, SECURE learns from theuser's embodied corrective feedback when it makes a mistake, and it makesstrategic dialogue decisions to reveal useful evidence about novel concepts forsolving the instructed task. Together, these abilities allow the agent togeneralise to subsequent tasks using newly acquired knowledge. We demonstratethat learning to solve rearrangement under unawareness is more data efficientwhen the agent is semantics-aware -- that is, during both learning andinference it augments the evidence from the user's embodied conversation withits logical consequences, stemming from semantic analysis.</description><author>Rimvydas Rubavicius, Peter David Fagan, Alex Lascarides, Subramanian Ramamoorthy</author><pubDate>Mon, 10 Feb 2025 18:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17755v2</guid></item><item><title>Rationalization Models for Text-to-SQL</title><link>http://arxiv.org/abs/2502.06759v1</link><description>We introduce a framework for generating Chain-of-Thought (CoT) rationales toenhance text-to-SQL model fine-tuning. These rationales consist of intermediateSQL statements and explanations, serving as incremental steps towardconstructing the final SQL query. The process begins with manually annotating asmall set of examples, which are then used to prompt a large language model inan iterative, dynamic few-shot knowledge distillation procedure from a teachermodel. A rationalization model is subsequently trained on the validateddecomposed queries, enabling extensive synthetic CoT annotations fortext-to-SQL datasets. To evaluate the approach, we fine-tune small languagemodels with and without these rationales on the BIRD dataset. Results indicatethat step-by-step query generation improves execution accuracy, especially formoderately and highly complex queries, while also enhancing explainability.</description><author>Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Shankar Subramanian</author><pubDate>Mon, 10 Feb 2025 18:38:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06759v1</guid></item><item><title>Emotion estimation from video footage with LSTM</title><link>http://arxiv.org/abs/2501.13432v2</link><description>Emotion estimation in general is a field that has been studied for a longtime, and several approaches exist using machine learning. in this paper, wepresent an LSTM model, that processes the blend-shapes produced by the libraryMediaPipe, for a face detected in a live stream of a camera, to estimate themain emotion from the facial expressions, this model is trained on the FER2013dataset and delivers a result of 71% accuracy and 62% f1-score which meets theaccuracy benchmark of the FER2013 dataset, with significantly reducedcomputation costs.https://github.com/Samir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm</description><author>Samer Attrah</author><pubDate>Mon, 10 Feb 2025 18:37:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13432v2</guid></item><item><title>LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering</title><link>http://arxiv.org/abs/2406.06621v2</link><description>We present LinkQ, a system that leverages a large language model (LLM) tofacilitate knowledge graph (KG) query construction through natural languagequestion-answering. Traditional approaches often require detailed knowledge ofa graph querying language, limiting the ability for users -- even experts -- toacquire valuable insights from KGs. LinkQ simplifies this process byimplementing a multistep protocol in which the LLM interprets a user'squestion, then systematically converts it into a well-formed query. LinkQ helpsusers iteratively refine any open-ended questions into precise ones, supportingboth targeted and exploratory analysis. Further, LinkQ guards against the LLMhallucinating outputs by ensuring users' questions are only ever answered fromground truth KG data. We demonstrate the efficacy of LinkQ through aqualitative study with five KG practitioners. Our results indicate thatpractitioners find LinkQ effective for KG question-answering, and desire futureLLM-assisted exploratory data analysis systems.</description><author>Harry Li, Gabriel Appleby, Ashley Suh</author><pubDate>Mon, 10 Feb 2025 18:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06621v2</guid></item><item><title>SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement</title><link>http://arxiv.org/abs/2502.06756v1</link><description>In this paper, we explore a principal way to enhance the quality of widelypre-existing coarse masks, enabling them to serve as reliable training data forsegmentation models to reduce the annotation cost. In contrast to priorrefinement techniques that are tailored to specific models or tasks in aclose-world manner, we propose SAMRefiner, a universal and efficient approachby adapting SAM to the mask refinement task. The core technique of our model isthe noise-tolerant prompting scheme. Specifically, we introduce a multi-promptexcavation strategy to mine diverse input prompts for SAM (i.e.,distance-guided points, context-aware elastic bounding boxes, andGaussian-style masks) from initial coarse masks. These prompts can collaboratewith each other to mitigate the effect of defects in coarse masks. Inparticular, considering the difficulty of SAM to handle the multi-object casein semantic segmentation, we introduce a split-then-merge (STM) pipeline.Additionally, we extend our method to SAMRefiner++ by introducing an additionalIoU adaption step to further boost the performance of the generic SAMRefiner onthe target dataset. This step is self-boosted and requires no additionalannotation. The proposed framework is versatile and can flexibly cooperate withexisting segmentation methods. We evaluate our mask framework on a wide rangeof benchmarks under different settings, demonstrating better accuracy andefficiency. SAMRefiner holds significant potential to expedite the evolution ofrefinement tools. Our code is available athttps://github.com/linyq2117/SAMRefiner.</description><author>Yuqi Lin, Hengjia Li, Wenqi Shao, Zheng Yang, Jun Zhao, Xiaofei He, Ping Luo, Kaipeng Zhang</author><pubDate>Mon, 10 Feb 2025 18:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06756v1</guid></item><item><title>Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models</title><link>http://arxiv.org/abs/2502.06755v1</link><description>To truly understand vision models, we must not only interpret their learnedfeatures but also validate these interpretations through controlledexperiments. Current approaches either provide interpretable features withoutthe ability to test their causal influence, or enable model editing withoutinterpretable controls. We present a unified framework using sparseautoencoders (SAEs) that bridges this gap, allowing us to discoverhuman-interpretable visual features and precisely manipulate them to testhypotheses about model behavior. By applying our method to state-of-the-artvision models, we reveal key differences in the semantic abstractions learnedby models with different pre-training objectives. We then demonstrate thepractical usage of our framework through controlled interventions acrossmultiple vision tasks. We show that SAEs can reliably identify and manipulateinterpretable visual features without model re-training, providing a powerfultool for understanding and controlling vision model behavior. We provide code,demos and models on our project website: https://osu-nlp-group.github.io/SAE-V.</description><author>Samuel Stevens, Wei-Lun Chao, Tanya Berger-Wolf, Yu Su</author><pubDate>Mon, 10 Feb 2025 18:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06755v1</guid></item><item><title>Case for a unified surrogate modelling framework in the age of AI</title><link>http://arxiv.org/abs/2502.06753v1</link><description>Surrogate models are widely used in natural sciences, engineering, andmachine learning to approximate complex systems and reduce computational costs.However, the current landscape lacks standardisation across key stages of thepipeline, including data collection, sampling design, model class selection,evaluation metrics, and downstream task performance analysis. Thisfragmentation limits reproducibility, reliability, and cross-domainapplicability. The issue has only been exacerbated by the AI revolution and anew suite of surrogate model classes that it offers. In this position paper, weargue for the urgent need for a unified framework to guide the development andevaluation of surrogate models. We outline essential steps for constructing acomprehensive pipeline and discuss alternative perspectives, such as thebenefits of domain-specific frameworks. By advocating for a standardisedapproach, this paper seeks to improve the reliability of surrogate modelling,foster cross-disciplinary knowledge transfer, and, as a result, acceleratescientific progress.</description><author>Elizaveta Semenova</author><pubDate>Mon, 10 Feb 2025 18:31:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06753v1</guid></item><item><title>Private Federated Learning In Real World Application -- A Case Study</title><link>http://arxiv.org/abs/2502.04565v2</link><description>This paper presents an implementation of machine learning model trainingusing private federated learning (PFL) on edge devices. We introduce a novelframework that uses PFL to address the challenge of training a model usingusers' private data. The framework ensures that user data remain on individualdevices, with only essential model updates transmitted to a central server foraggregation with privacy guarantees. We detail the architecture of our appselection model, which incorporates a neural network with attention mechanismsand ambiguity handling through uncertainty management. Experiments conductedthrough off-line simulations and on device training demonstrate the feasibilityof our approach in real-world scenarios. Our results show the potential of PFLto improve the accuracy of an app selection model by adapting to changes inuser behavior over time, while adhering to privacy standards. The insightsgained from this study are important for industries looking to implement PFL,offering a robust strategy for training a predictive model directly on edgedevices while ensuring user data privacy.</description><author>An Ji, Bortik Bandyopadhyay, Congzheng Song, Natarajan Krishnaswami, Prabal Vashisht, Rigel Smiroldo, Isabel Litton, Sayantan Mahinder, Mona Chitnis, Andrew W Hill</author><pubDate>Mon, 10 Feb 2025 18:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04565v2</guid></item><item><title>What makes a good feedforward computational graph?</title><link>http://arxiv.org/abs/2502.06751v1</link><description>As implied by the plethora of literature on graph rewiring, the choice ofcomputational graph employed by a neural network can make a significant impacton its downstream performance. Certain effects related to the computationalgraph, such as under-reaching and over-squashing, may even render the modelincapable of learning certain functions. Most of these effects have only beenthoroughly studied in the domain of undirected graphs; however, recent yearshave seen a significant rise in interest in feedforward computational graphs:directed graphs without any back edges. In this paper, we study the desirableproperties of a feedforward computational graph, discovering two importantcomplementary measures: fidelity and mixing time, and evaluating a few popularchoices of graphs through the lens of these measures. Our study is backed byboth theoretical analyses of the metrics' asymptotic behaviour for variousgraphs, as well as correlating these metrics to the performance of trainedneural network models using the corresponding graphs.</description><author>Alex Vitvitskyi, João G. M. Araújo, Marc Lackenby, Petar Veličković</author><pubDate>Mon, 10 Feb 2025 18:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06751v1</guid></item><item><title>Tamper-Resistant Safeguards for Open-Weight LLMs</title><link>http://arxiv.org/abs/2408.00761v4</link><description>Rapid advances in the capabilities of large language models (LLMs) haveraised widespread concerns regarding their potential for malicious use.Open-weight LLMs present unique challenges, as existing safeguards lackrobustness to tampering attacks that modify model weights. For example, recentworks have demonstrated that refusal and unlearning safeguards can be triviallyremoved with a few steps of fine-tuning. These vulnerabilities necessitate newapproaches for enabling the safe release of open-weight LLMs. We develop amethod, called TAR, for building tamper-resistant safeguards into open-weightLLMs such that adversaries cannot remove the safeguards even after hundreds ofsteps of fine-tuning. In extensive evaluations and red teaming analyses, wefind that our method greatly improves tamper-resistance while preserving benigncapabilities. Our results demonstrate that progress on tamper-resistance ispossible, opening up a promising new avenue to improve the safety and securityof open-weight LLMs.</description><author>Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, Mantas Mazeika</author><pubDate>Mon, 10 Feb 2025 18:26:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00761v4</guid></item><item><title>Accelerating Data Processing and Benchmarking of AI Models for Pathology</title><link>http://arxiv.org/abs/2502.06750v1</link><description>Advances in foundation modeling have reshaped computational pathology.However, the increasing number of available models and lack of standardizedbenchmarks make it increasingly complex to assess their strengths, limitations,and potential for further development. To address these challenges, weintroduce a new suite of software tools for whole-slide image processing,foundation model benchmarking, and curated publicly available tasks. Weanticipate that these resources will promote transparency, reproducibility, andcontinued progress in the field.</description><author>Andrew Zhang, Guillaume Jaume, Anurag Vaidya, Tong Ding, Faisal Mahmood</author><pubDate>Mon, 10 Feb 2025 18:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06750v1</guid></item><item><title>Diverse Preference Optimization</title><link>http://arxiv.org/abs/2501.18101v3</link><description>Post-training of language models, either through reinforcement learning,preference optimization or supervised finetuning, tends to sharpen the outputprobability distribution and reduce the diversity of generated responses. Thisis particularly a problem for creative generative tasks where varied responsesare desired. In this work we introduce Diverse Preference Optimization (DivPO),an optimization method which learns to generate much more diverse responsesthan standard pipelines, while maintaining the quality of the generations. InDivPO, preference pairs are selected by first considering a pool of responses,and a measure of diversity among them, and selecting chosen examples as beingmore rare but high quality, while rejected examples are more common, but lowquality. DivPO results in generating 45.6% more diverse persona attributes, andan 74.6% increase in story diversity, while maintaining similar win rates asstandard baselines.</description><author>Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, Ilia Kulikov</author><pubDate>Mon, 10 Feb 2025 18:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18101v3</guid></item><item><title>Incentivizing Desirable Effort Profiles in Strategic Classification: The Role of Causality and Uncertainty</title><link>http://arxiv.org/abs/2502.06749v1</link><description>We study strategic classification in binary decision-making settings whereagents can modify their features in order to improve their classificationoutcomes. Importantly, our work considers the causal structure across differentfeatures, acknowledging that effort in a given feature may affect otherfeatures. The main goal of our work is to understand \emph{when and how muchagent effort is invested towards desirable features}, and how this isinfluenced by the deployed classifier, the causal structure of the agent'sfeatures, their ability to modify them, and the information available to theagent about the classifier and the feature causal graph. In the complete information case, when agents know the classifier and thecausal structure of the problem, we derive conditions ensuring that rationalagents focus on features favored by the principal. We show that designingclassifiers to induce desirable behavior is generally non-convex, thoughtractable in special cases. We also extend our analysis to settings whereagents have incomplete information about the classifier or the causal graph.While optimal effort selection is again a non-convex problem under generaluncertainty, we highlight special cases of partial uncertainty where thisselection problem becomes tractable. Our results indicate that uncertaintydrives agents to favor features with higher expected importance and lowervariance, potentially misaligning with principal preferences. Finally,numerical experiments based on a cardiovascular disease risk study illustratehow to incentivize desirable modifications under uncertainty.</description><author>Valia Efthymiou, Chara Podimata, Diptangshu Sen, Juba Ziani</author><pubDate>Mon, 10 Feb 2025 18:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06749v1</guid></item><item><title>Wandering around: A bioinspired approach to visual attention through object motion sensitivity</title><link>http://arxiv.org/abs/2502.06747v1</link><description>Active vision enables dynamic visual perception, offering an alternative tostatic feedforward architectures in computer vision, which rely on largedatasets and high computational resources. Biological selective attentionmechanisms allow agents to focus on salient Regions of Interest (ROIs),reducing computational demand while maintaining real-time responsiveness.Event-based cameras, inspired by the mammalian retina, enhance this capabilityby capturing asynchronous scene changes enabling efficient low-latencyprocessing. To distinguish moving objects while the event-based camera is inmotion the agent requires an object motion segmentation mechanism to accuratelydetect targets and center them in the visual field (fovea). Integratingevent-based sensors with neuromorphic algorithms represents a paradigm shift,using Spiking Neural Networks to parallelize computation and adapt to dynamicenvironments. This work presents a Spiking Convolutional Neural Networkbioinspired attention system for selective attention through object motionsensitivity. The system generates events via fixational eye movements using aDynamic Vision Sensor integrated into the Speck neuromorphic hardware, mountedon a Pan-Tilt unit, to identify the ROI and saccade toward it. The system,characterized using ideal gratings and benchmarked against the Event CameraMotion Segmentation Dataset, reaches a mean IoU of 82.2% and a mean SSIM of 96%in multi-object motion segmentation. The detection of salient objects reaches88.8% accuracy in office scenarios and 89.8% in low-light conditions on theEvent-Assisted Low-Light Video Object Segmentation Dataset. A real-timedemonstrator shows the system's 0.12 s response to dynamic scenes. Itslearning-free design ensures robustness across perceptual scenes, making it areliable foundation for real-time robotic applications serving as a basis formore complex architectures.</description><author>Giulia D Angelo, Victoria Clerico, Chiara Bartolozzi, Matej Hoffmann, P. Michael Furlong, Alexander Hadjiivanov</author><pubDate>Mon, 10 Feb 2025 18:16:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06747v1</guid></item><item><title>DPD-NeuralEngine: A 22-nm 6.6-TOPS/W/mm$^2$ Recurrent Neural Network Accelerator for Wideband Power Amplifier Digital Pre-Distortion</title><link>http://arxiv.org/abs/2410.11766v2</link><description>The increasing adoption of Deep Neural Network (DNN)-based DigitalPre-distortion (DPD) in modern communication systems necessitates efficienthardware implementations. This paper presents DPD-NeuralEngine, an ultra-fast,tiny-area, and power-efficient DPD accelerator based on a Gated Recurrent Unit(GRU) neural network (NN). Leveraging a co-designed software and hardwareapproach, our 22 nm CMOS implementation operates at 2 GHz, capable ofprocessing I/Q signals up to 250 MSps. Experimental results demonstrate athroughput of 256.5 GOPS and power efficiency of 1.32 TOPS/W with DPDlinearization performance measured in Adjacent Channel Power Ratio (ACPR) of-45.3 dBc and Error Vector Magnitude (EVM) of -39.8 dB. To our knowledge, thiswork represents the first AI-based DPD application-specific integrated circuit(ASIC) accelerator, achieving a power-area efficiency (PAE) of 6.6TOPS/W/mm$^2$.</description><author>Ang Li, Haolin Wu, Yizhuo Wu, Qinyu Chen, Leo C. N. de Vreede, Chang Gao</author><pubDate>Mon, 10 Feb 2025 18:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11766v2</guid></item><item><title>Gradient Multi-Normalization for Stateless and Scalable LLM Training</title><link>http://arxiv.org/abs/2502.06742v1</link><description>Training large language models (LLMs) typically relies on adaptive optimizerslike Adam (Kingma &amp; Ba, 2015) which store additional state information toaccelerate convergence but incur significant memory overhead. Recent efforts,such as SWAN (Ma et al., 2024) address this by eliminating the need foroptimizer states while achieving performance comparable to Adam via amulti-step preprocessing procedure applied to instantaneous gradients.Motivated by the success of SWAN, we introduce a novel framework for designingstateless optimizers that normalizes stochastic gradients according to multiplenorms. To achieve this, we propose a simple alternating scheme to enforce thenormalization of gradients w.r.t these norms. We show that our procedure canproduce, up to an arbitrary precision, a fixed-point of the problem, and thatSWAN is a particular instance of our approach with carefully chosen norms,providing a deeper understanding of its design. However, SWAN's computationallyexpensive whitening/orthogonalization step limit its practicality for largeLMs. Using our principled perspective, we develop of a more efficient,scalable, and practical stateless optimizer. Our algorithm relaxes theproperties of SWAN, significantly reducing its computational cost whileretaining its memory efficiency, making it applicable to training large-scalemodels. Experiments on pre-training LLaMA models with up to 1 billionparameters demonstrate a 3X speedup over Adam with significantly reduced memoryrequirements, outperforming other memory-efficient baselines.</description><author>Meyer Scetbon, Chao Ma, Wenbo Gong, Edward Meeds</author><pubDate>Mon, 10 Feb 2025 18:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06742v1</guid></item><item><title>ViSIR: Vision Transformer Single Image Reconstruction Method for Earth System Models</title><link>http://arxiv.org/abs/2502.06741v1</link><description>Purpose: Earth system models (ESMs) integrate the interactions of theatmosphere, ocean, land, ice, and biosphere to estimate the state of regionaland global climate under a wide variety of conditions. The ESMs are highlycomplex, and thus, deep neural network architectures are used to model thecomplexity and store the down-sampled data. In this paper, we propose theVision Transformer Sinusoidal Representation Networks (ViSIR) to improve thesingle image SR (SR) reconstruction task for the ESM data. Methods: ViSIR combines the SR capability of Vision Transformers (ViT) withthe high-frequency detail preservation of the Sinusoidal Representation Network(SIREN) to address the spectral bias observed in SR tasks. Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, andSR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for threedifferent measurements. Conclusion: The proposed ViSIR is evaluated and compared withstate-of-the-art methods. The results show that the proposed algorithm isoutperforming other methods in terms of Mean Square Error(MSE),Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity IndexMeasure(SSIM).</description><author>Ehsan Zeraatkar, Salah Faroughi, Jelena Tesic</author><pubDate>Mon, 10 Feb 2025 18:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06741v1</guid></item><item><title>CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning</title><link>http://arxiv.org/abs/2410.11062v2</link><description>This paper presents CleanUMamba, a time-domain neural network architecturedesigned for real-time causal audio denoising directly applied to rawwaveforms. CleanUMamba leverages a U-Net encoder-decoder structure,incorporating the Mamba state-space model in the bottleneck layer. By replacingconventional self-attention and LSTM mechanisms with Mamba, our architectureoffers superior denoising performance while maintaining a constant memoryfootprint, enabling streaming operation. To enhance efficiency, we appliedstructured channel pruning, achieving an 8X reduction in model size withoutcompromising audio quality. Our model demonstrates strong results in theInterspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMambaachieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and468M MACs, matching or outperforming larger models in real-time performance.Code will be available at: https://github.com/lab-emi/CleanUMamba</description><author>Sjoerd Groot, Qinyu Chen, Jan C. van Gemert, Chang Gao</author><pubDate>Mon, 10 Feb 2025 18:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11062v2</guid></item><item><title>A note on the physical interpretation of neural PDE's</title><link>http://arxiv.org/abs/2502.06739v1</link><description>We highlight a formal and substantial analogy between Machine Learning (ML)algorithms and discrete dynamical systems (DDS) in relaxation form. The analogyoffers a transparent interpretation of the weights in terms of physicalinformation-propagation processes and identifies the model function of theforward ML step with the local attractor of the corresponding discretedynamics. Besides improving the explainability of current ML applications, thisanalogy may also facilitate the development of a new class ML algorithms with areduced number of weights.</description><author>Sauro Succi</author><pubDate>Mon, 10 Feb 2025 18:07:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06739v1</guid></item><item><title>Resurrecting saturated LLM benchmarks with adversarial encoding</title><link>http://arxiv.org/abs/2502.06738v1</link><description>Recent work showed that small changes in benchmark questions can reduce LLMs'reasoning and recall. We explore two such changes: pairing questions and addingmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. Wefind that for more capable models, these predictably reduce performance,essentially heightening the performance ceiling of a benchmark and unsaturatingit again. We suggest this approach can resurrect old benchmarks.</description><author>Igor Ivanov, Dmitrii Volkov</author><pubDate>Mon, 10 Feb 2025 18:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06738v1</guid></item><item><title>VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data</title><link>http://arxiv.org/abs/2502.06737v1</link><description>Process Reward Models (PRMs) have proven effective at enhancing mathematicalreasoning for Large Language Models (LLMs) by leveraging increasedinference-time computation. However, they are predominantly trained onmathematical data and their generalizability to non-mathematical domains hasnot been rigorously studied. In response, this work first shows that currentPRMs have poor performance in other domains. To address this limitation, weintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning datagenerated using our novel data generation and annotation method. VersaPRMachieves consistent performance gains across diverse domains. For instance, inthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a7.9% performance gain over the majority voting baseline -- surpassingQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community byopen-sourcing all data, code and models for VersaPRM.</description><author>Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, Hyung Il Koo, Kannan Ramchandran, Dimitris Papailiopoulos, Kangwook Lee</author><pubDate>Mon, 10 Feb 2025 18:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06737v1</guid></item><item><title>Low-power Spike-based Wearable Analytics on RRAM Crossbars</title><link>http://arxiv.org/abs/2502.06736v1</link><description>This work introduces a spike-based wearable analytics system utilizingSpiking Neural Networks (SNNs) deployed on an In-memory Computing engine basedon RRAM crossbars, which are known for their compactness and energy-efficiency.Given the hardware constraints and noise characteristics of the underlying RRAMcrossbars, we propose online adaptation of pre-trained SNNs in real-time usingDirect Feedback Alignment (DFA) against traditional backpropagation (BP).Direct Feedback Alignment (DFA) learning, that allows layer-parallel gradientcomputations, acts as a fast, energy &amp; area-efficient method for onlineadaptation of SNNs on RRAM crossbars, unleashing better algorithmic performanceagainst those adapted using BP. Through extensive simulations using ourin-house hardware evaluation engine called DFA_Sim, we find that DFA achievesupto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1xreduction in latency compared to BP, while delivering upto 7.55% higherinference accuracy on human activity recognition (HAR) tasks.</description><author>Abhiroop Bhattacharjee, Jinquan Shi, Wei-Chen Chen, Xinxin Wang, Priyadarshini Panda</author><pubDate>Mon, 10 Feb 2025 18:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06736v1</guid></item><item><title>Regularized Q-Learning with Linear Function Approximation</title><link>http://arxiv.org/abs/2401.15196v3</link><description>Regularized Markov Decision Processes serve as models of sequential decisionmaking under uncertainty wherein the decision maker has limited informationprocessing capacity and/or aversion to model ambiguity. With functionalapproximation, the convergence properties of learning algorithms forregularized MDPs (e.g. soft Q-learning) are not well understood because thecomposition of the regularized Bellman operator and a projection onto the spanof basis vectors is not a contraction with respect to any norm. In this paper,we consider a bi-level optimization formulation of regularized Q-learning withlinear functional approximation. The {\em lower} level optimization problemaims to identify a value function approximation that satisfies Bellman'srecursive optimality condition and the {\em upper} level aims to find theprojection onto the span of basis vectors. This formulation motivates asingle-loop algorithm with finite time convergence guarantees. The algorithmoperates on two time-scales: updates to the projection of state-action valuesare `slow' in that they are implemented with a step size that is smaller thanthe one used for `faster' updates of approximate solutions to Bellman'srecursive optimality equation. We show that, under certain assumptions, theproposed algorithm converges to a stationary point in the presence of Markoviannoise. In addition, we provide a performance guarantee for the policies derivedfrom the proposed algorithm.</description><author>Jiachen Xi, Alfredo Garcia, Petar Momcilovic</author><pubDate>Mon, 10 Feb 2025 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15196v3</guid></item><item><title>Enhancing Pneumonia Diagnosis and Severity Assessment through Deep Learning: A Comprehensive Approach Integrating CNN Classification and Infection Segmentation</title><link>http://arxiv.org/abs/2502.06735v1</link><description>Lung disease poses a substantial global health challenge, with pneumoniabeing a prevalent concern. This research focuses on leveraging deep learningtechniques to detect and assess pneumonia, addressing two interconnectedobjectives. Initially, Convolutional Neural Network (CNN) models are introducedfor pneumonia classification, emphasizing the necessity of comprehensivediagnostic assessments considering COVID-19. Subsequently, the study advocatesfor the utilization of deep learning-based segmentation to determine theseverity of infection. This dual-pronged approach offers valuable insights formedical professionals, facilitating a more nuanced understanding and effectivetreatment of pneumonia. Integrating deep learning aims to elevate the accuracyand efficiency of pneumonia detection, thereby contributing to enhancedhealthcare outcomes on a global scale.</description><author>S Kumar Reddy Mallidi</author><pubDate>Mon, 10 Feb 2025 17:58:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06735v1</guid></item><item><title>Señorita-2M: A High-Quality Instruction-based Dataset for General Video Editing by Video Specialists</title><link>http://arxiv.org/abs/2502.06734v1</link><description>Recent advancements in video generation have spurred the development of videoediting techniques, which can be divided into inversion-based and end-to-endmethods. However, current video editing methods still suffer from severalchallenges. Inversion-based methods, though training-free and flexible, aretime-consuming during inference, struggle with fine-grained editinginstructions, and produce artifacts and jitter. On the other hand, end-to-endmethods, which rely on edited video pairs for training, offer faster inferencespeeds but often produce poor editing results due to a lack of high-qualitytraining video pairs. In this paper, to close the gap in end-to-end methods, weintroduce Se\~norita-2M, a high-quality video editing dataset. Se\~norita-2Mconsists of approximately 2 millions of video editing pairs. It is built bycrafting four high-quality, specialized video editing models, each crafted andtrained by our team to achieve state-of-the-art editing results. We alsopropose a filtering pipeline to eliminate poorly edited video pairs.Furthermore, we explore common video editing architectures to identify the mosteffective structure based on current pre-trained generative model. Extensiveexperiments show that our dataset can help to yield remarkably high-qualityvideo editing results. More details are available athttps://senorita.github.io.</description><author>Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, Kam-Fai Wong</author><pubDate>Mon, 10 Feb 2025 17:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06734v1</guid></item><item><title>A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition</title><link>http://arxiv.org/abs/2403.14318v2</link><description>Convolutional neural networks (CNNs) and their variations have showneffectiveness in facial expression recognition (FER). However, they facechallenges when dealing with high computational complexity and multi-view headposes in real-world scenarios. We introduce a lightweight attentional networkincorporating multi-scale feature fusion (LANMSFF) to tackle these issues. Forthe first challenge, we carefully design a lightweight network. We address thesecond challenge by presenting two novel components, namely mass attention(MassAtt) and point wise feature selection (PWFS) blocks. The MassAtt blocksimultaneously generates channel and spatial attention maps to recalibratefeature maps by emphasizing important features while suppressing irrelevantones. In addition, the PWFS block employs a feature selection mechanism thatdiscards less meaningful features prior to the fusion process. This mechanismdistinguishes it from previous methods that directly fuse multi-scale features.Our proposed approach achieved results comparable to state-of-the-art methodsin terms of parameter count and robustness to pose variation, with accuracyrates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets.The code for LANMSFF is available at https://github.com/AE-1129/LANMSFF.</description><author>Ali Ezati, Mohammadreza Dezyani, Rajib Rana, Roozbeh Rajabi, Ahmad Ayatollahi</author><pubDate>Mon, 10 Feb 2025 17:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14318v2</guid></item><item><title>Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining</title><link>http://arxiv.org/abs/2502.06733v1</link><description>Pretraining large language models (LLMs) on vast and heterogeneous datasetsis crucial for achieving state-of-the-art performance across diverse downstreamtasks. However, current training paradigms treat all samples equally,overlooking the importance or relevance of individual samples throughout thetraining process. Existing reweighting strategies, which primarily focus ongroup-level data importance, fail to leverage fine-grained instance-levelinformation and do not adapt dynamically to individual sample importance astraining progresses. In this paper, we introduce novel algorithms for dynamic,instance-level data reweighting aimed at improving both the efficiency andeffectiveness of LLM pretraining. Our methods adjust the weight of eachtraining sample based on its loss value in an online fashion, allowing themodel to dynamically focus on more informative or important samples at thecurrent training stage. In particular, our framework allows us tosystematically devise reweighting strategies deprioritizing redundant oruninformative data, which we find tend to work best. Furthermore, we develop anew theoretical framework for analyzing the impact of loss-based reweighting onthe convergence of gradient-based optimization, providing the first formalcharacterization of how these strategies affect convergence bounds. Weempirically validate our approach across a spectrum of tasks, from pretraining7B and 1.4B parameter LLMs to smaller-scale language models and linearregression problems, demonstrating that our loss-based reweighting approach canlead to faster convergence and significantly improved performance.</description><author>Daouda Sow, Herbert Woisetschläger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang</author><pubDate>Mon, 10 Feb 2025 17:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06733v1</guid></item><item><title>FlexDeMo: Decoupled Momentum Optimization for Fully and Hybrid Sharded Training</title><link>http://arxiv.org/abs/2502.06728v1</link><description>Training large neural network models requires extensive computationalresources, often distributed across several nodes and accelerators. Recentfindings suggest that it may be sufficient to only exchange the fast movingcomponents of the gradients, while accumulating momentum locally (DecoupledMomentum, or DeMo). However, when considering larger models that do not fit ona single accelerate, the exchange of gradient information and the integrationof DeMo needs to be reconsidered. Here, we propose employing a hybrid strategy,FlexDeMo, whereby nodes fully synchronize locally between different GPUs andinter-node communication is improved through only using the fast-movingcomponents. This effectively combines previous hybrid sharding strategies withthe advantages of decoupled momentum. Our experimental results show thatFlexDeMo is on par with AdamW in terms of validation loss, demonstrating itsviability.</description><author>Mogens Henrik From, Jacob Nielsen, Lukas Galke, Peter Schneider-Kamp</author><pubDate>Mon, 10 Feb 2025 17:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06728v1</guid></item><item><title>Application of Artificial Intelligence (AI) in Civil Engineering</title><link>http://arxiv.org/abs/2502.06727v1</link><description>Hard computing generally deals with precise data, which provides idealsolutions to problems. However, in the civil engineering field, amongst otherdisciplines, that is not always the case as real-world systems are continuouslychanging. Here lies the need to explore soft computing methods and artificialintelligence to solve civil engineering shortcomings. The integration ofadvanced computational models, including Artificial Neural Networks (ANNs),Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, hasrevolutionized the domain of civil engineering. These models have significantlyadvanced diverse sub-fields by offering innovative solutions and improvedanalysis capabilities. Sub-fields such as: slope stability analysis, bearingcapacity, water quality and treatment, transportation systems, air quality,structural materials, etc. ANNs predict non-linearities and provide accurateestimates. Fuzzy logic uses an efficient decision-making process to provide amore precise assessment of systems. Lastly, while GAs optimizes models (basedon evolutionary processes) for better outcomes, probabilistic reasoning lowerstheir statistical uncertainties.</description><author>Temitope Funmilayo Awolusi, Bernard Chukwuemeka Finbarrs-Ezema, Isaac Munachimdinamma Chukwudulue, Marc Azab</author><pubDate>Mon, 10 Feb 2025 17:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06727v1</guid></item><item><title>Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2502.06719v1</link><description>In this paper, we establish non-asymptotic convergence rates in the centrallimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradientdescent (SGD). Our analysis builds on the result of the Gaussian approximationfor nonlinear statistics of independent random variables of Shao and Zhang(2022). Using this result, we prove the non-asymptotic validity of themultiplier bootstrap for constructing the confidence sets for the optimalsolution of an optimization problem. In particular, our approach avoids theneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,which allows us to derive approximation rates in convex distance of order up to$1/\sqrt{n}$.</description><author>Marina Sheshukova, Sergey Samsonov, Denis Belomestny, Eric Moulines, Qi-Man Shao, Zhuo-Song Zhang, Alexey Naumov</author><pubDate>Mon, 10 Feb 2025 17:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06719v1</guid></item><item><title>Learning Musical Representations for Music Performance Question Answering</title><link>http://arxiv.org/abs/2502.06710v1</link><description>Music performances are representative scenarios for audio-visual modeling.Unlike common scenarios with sparse audio, music performances continuouslyinvolve dense audio signals throughout. While existing multimodal learningmethods on the audio-video QA demonstrate impressive capabilities in generalscenarios, they are incapable of dealing with fundamental problems within themusic performances: they underexplore the interaction between the multimodalsignals in performance and fail to consider the distinctive characteristics ofinstruments and music. Therefore, existing methods tend to answer questionsregarding musical performances inaccurately. To bridge the above research gaps,(i) given the intricate multimodal interconnectivity inherent to music data,our primary backbone is designed to incorporate multimodal interactions withinthe context of music; (ii) to enable the model to learn music characteristics,we annotate and release rhythmic and music sources in the current musicdatasets; (iii) for time-aware audio-visual modeling, we align the model'smusic predictions with the temporal dimension. Our experiments showstate-of-the-art effects on the Music AVQA datasets. Our code is available athttps://github.com/xid32/Amuse.</description><author>Xingjian Diao, Chunhui Zhang, Tingxuan Wu, Ming Cheng, Zhongyu Ouyang, Weiyi Wu, Jiang Gui</author><pubDate>Mon, 10 Feb 2025 17:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06710v1</guid></item><item><title>TEMSET-24K: Densely Annotated Dataset for Indexing Multipart Endoscopic Videos using Surgical Timeline Segmentation</title><link>http://arxiv.org/abs/2502.06708v1</link><description>Indexing endoscopic surgical videos is vital in surgical data science,forming the basis for systematic retrospective analysis and clinicalperformance evaluation. Despite its significance, current video analytics relyon manual indexing, a time-consuming process. Advances in computer vision,particularly deep learning, offer automation potential, yet progress is limitedby the lack of publicly available, densely annotated surgical datasets. Toaddress this, we present TEMSET-24K, an open-source dataset comprising 24,306trans-anal endoscopic microsurgery (TEMS) video micro-clips. Each clip ismeticulously annotated by clinical experts using a novel hierarchical labelingtaxonomy encompassing phase, task, and action triplets, capturing intricatesurgical workflows. To validate this dataset, we benchmarked deep learningmodels, including transformer-based architectures. Our in silico evaluationdemonstrates high accuracy (up to 0.99) and F1 scores (up to 0.99) for keyphases like Setup and Suturing. The STALNet model, tested with ConvNeXt, ViT,and SWIN V2 encoders, consistently segmented well-represented phases.TEMSET-24K provides a critical benchmark, propelling state-of-the-art solutionsin surgical data science.</description><author>Muhammad Bilal, Mahmood Alam, Deepa Bapu, Stephan Korsgen, Neeraj Lal, Simon Bach, Amir M Hajivanand, Muhammed Ali, Kamran Soomro, Iqbal Qasim, Paweł Capik, Aslam Khan, Zaheer Khan, Hunaid Vohra, Massimo Caputo, Andrew Beggs, Adnan Qayyum, Junaid Qadir, Shazad Ashraf</author><pubDate>Mon, 10 Feb 2025 17:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06708v1</guid></item><item><title>GHOST: Gaussian Hypothesis Open-Set Technique</title><link>http://arxiv.org/abs/2502.03359v2</link><description>Evaluations of large-scale recognition methods typically focus on overallperformance. While this approach is common, it often fails to provide insightsinto performance across individual classes, which can lead to fairness issuesand misrepresentation. Addressing these gaps is crucial for accuratelyassessing how well methods handle novel or unseen classes and ensuring a fairevaluation. To address fairness in Open-Set Recognition (OSR), we demonstratethat per-class performance can vary dramatically. We introduce GaussianHypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithmthat models deep features using class-wise multivariate Gaussian distributionswith diagonal covariance matrices. We apply Z-score normalization to logits tomitigate the impact of feature magnitudes that deviate from the model'sexpectations, thereby reducing the likelihood of the network assigning a highscore to an unknown sample. We evaluate GHOST across multiple ImageNet-1Kpre-trained deep networks and test it with four different unknown datasets.Using standard metrics such as AUOSCR, AUROC and FPR95, we achievestatistically significant improvements, advancing the state-of-the-art inlarge-scale OSR. Source code is provided online.</description><author>Ryan Rabinowitz, Steve Cruz, Manuel Günther, Terrance E. Boult</author><pubDate>Mon, 10 Feb 2025 17:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.03359v2</guid></item><item><title>RSAttAE: An Information-Aware Attention-based Autoencoder Recommender System</title><link>http://arxiv.org/abs/2502.06705v1</link><description>Recommender systems play a crucial role in modern life, including informationretrieval, the pharmaceutical industry, retail, and entertainment. Theentertainment sector, in particular, attracts significant attention andgenerates substantial profits. This work proposes a new method for predictingunknown user-movie ratings to enhance customer satisfaction. To achieve this,we utilize the MovieLens 100K dataset. Our approach introduces anattention-based autoencoder to create meaningful representations and theXGBoost method for rating predictions. The results demonstrate that ourproposal outperforms most of the existing state-of-the-art methods.Availability: github.com/ComputationIASBS/RecommSys</description><author>Amirhossein Dadashzadeh Taromi, Sina Heydari, Mohsen Hooshmand, Majid Ramezani</author><pubDate>Mon, 10 Feb 2025 17:33:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06705v1</guid></item><item><title>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</title><link>http://arxiv.org/abs/2502.06703v1</link><description>Test-Time Scaling (TTS) is an important method for improving the performanceof Large Language Models (LLMs) by using additional computation during theinference phase. However, current studies do not systematically analyze howpolicy models, Process Reward Models (PRMs), and problem difficulty influenceTTS. This lack of analysis limits the understanding and practical use of TTSmethods. In this paper, we focus on two core questions: (1) What is the optimalapproach to scale test-time computation across different policy models, PRMs,and problem difficulty levels? (2) To what extent can extended computationimprove the performance of LLMs on complex tasks, and can smaller languagemodels outperform larger ones through this approach? Through comprehensiveexperiments on MATH-500 and challenging AIME24 tasks, we have the followingobservations: (1) The compute-optimal TTS strategy is highly dependent on thechoice of policy model, PRM, and problem difficulty. (2) With ourcompute-optimal TTS strategy, extremely small policy models can outperformlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLMsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higherinference efficiency. These findings show the significance of adapting TTSstrategies to the specific characteristics of each task and model and indicatethat TTS is a promising approach for enhancing the reasoning abilities of LLMs.</description><author>Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou</author><pubDate>Mon, 10 Feb 2025 17:30:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06703v1</guid></item><item><title>Exploring Audio Editing Features as User-Centric Privacy Defenses Against Large Language Model(LLM) Based Emotion Inference Attacks</title><link>http://arxiv.org/abs/2501.18727v2</link><description>The rapid proliferation of speech-enabled technologies, including virtualassistants, video conferencing platforms, and wearable devices, has raisedsignificant privacy concerns, particularly regarding the inference of sensitiveemotional information from audio data. Existing privacy-preserving methodsoften compromise usability and security, limiting their adoption in practicalscenarios. This paper introduces a novel, user-centric approach that leveragesfamiliar audio editing techniques, specifically pitch and tempo manipulation,to protect emotional privacy without sacrificing usability. By analyzingpopular audio editing applications on Android and iOS platforms, we identifiedthese features as both widely available and usable. We rigorously evaluatedtheir effectiveness against a threat model, considering adversarial attacksfrom diverse sources, including Deep Neural Networks (DNNs), Large LanguageModels (LLMs), and and reversibility testing. Our experiments, conducted onthree distinct datasets, demonstrate that pitch and tempo manipulationeffectively obfuscates emotional data. Additionally, we explore the designprinciples for lightweight, on-device implementation to ensure broadapplicability across various devices and platforms.</description><author>Mohd. Farhan Israk Soumik, W. K. M. Mithsara, Abdur R. Shahid, Ahmed Imteaj</author><pubDate>Mon, 10 Feb 2025 17:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18727v2</guid></item><item><title>MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval</title><link>http://arxiv.org/abs/2410.11619v2</link><description>Efficiently retrieving and synthesizing information from large-scalemultimodal collections has become a critical challenge. However, existing videoretrieval datasets suffer from scope limitations, primarily focusing onmatching descriptive but vague queries with small collections of professionallyedited, English-centric videos. To address this gap, we introduce$\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric videoretrieval benchmark featuring a collection of more than 218,000 news videos and3,906 queries targeting specific world events. These queries specificallytarget information found in the visual content, audio, embedded text, and textmetadata of the videos, requiring systems leverage all these sources to succeedat the task. Preliminary results show that state-of-the-art vision-languagemodels struggle significantly with this task, and while alternative approachesshow promise, they are still insufficient to adequately address this problem.These findings underscore the need for more robust multimodal retrievalsystems, as effective video retrieval is a crucial step towards multimodalcontent understanding and generation.</description><author>Reno Kriz, Kate Sanders, David Etter, Kenton Murray, Cameron Carpenter, Kelly Van Ochten, Hannah Recknor, Jimena Guallar-Blasco, Alexander Martin, Ronald Colaianni, Nolan King, Eugene Yang, Benjamin Van Durme</author><pubDate>Mon, 10 Feb 2025 17:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11619v2</guid></item><item><title>Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models</title><link>http://arxiv.org/abs/2501.13629v2</link><description>We introduce Sigma, an efficient large language model specialized for thesystem domain, empowered by a novel architecture including DiffQKV attention,and pre-trained on our meticulously collected system domain data. DiffQKVattention significantly enhances the inference efficiency of Sigma byoptimizing the Query (Q), Key (K), and Value (V) components in the attentionmechanism differentially, based on their varying impacts on the modelperformance and efficiency indicators. Specifically, we (1) conduct extensiveexperiments that demonstrate the model's varying sensitivity to the compressionof K and V components, leading to the development of differentially compressedKV, and (2) propose augmented Q to expand the Q head dimension, which enhancesthe model's representation capacity with minimal impacts on the inferencespeed. Rigorous theoretical and empirical analyses reveal that DiffQKVattention significantly enhances efficiency, achieving up to a 33.36%improvement in inference speed over the conventional grouped-query attention(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from varioussources, including 19.5B system domain data that we carefully collect and 1Ttokens of synthesized and rewritten data. In general domains, Sigma achievescomparable performance to other state-of-arts models. In the system domain, weintroduce the first comprehensive benchmark AIMicius, where Sigma demonstratesremarkable performance across all tasks, significantly outperforming GPT-4 withan absolute improvement up to 52.5%.</description><author>Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong, Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang, Kailai Yang, Yu Yan, Xiao Liang, Shuai Lu, Yiming Huang, Zheheng Luo, Lei Qu, Xuan Feng, Yaoxiang Wang, Yuqing Xia, Feiyang Chen, Yuting Jiang, Yasen Hu, Hao Ni, Binyang Li, Guoshuai Zhao, Jui-Hao Chiang, Zhongxin Guo, Chen Lin, Kun Kuang, Wenjie Li, Yelong Shen, Jian Jiao, Peng Cheng, Mao Yang</author><pubDate>Mon, 10 Feb 2025 17:19:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13629v2</guid></item><item><title>Optimal Visual Search with Highly Heuristic Decision Rules</title><link>http://arxiv.org/abs/2409.12124v3</link><description>Visual search is a fundamental natural task for humans and other animals. Weinvestigated the decision processes humans use in covert (single-fixation)search with briefly presented displays having well-separated potential targetlocations. Performance was compared with the Bayesian-optimal decision processunder the assumption that the information from the different potential targetlocations is statistically independent. Surprisingly, humans performed slightlybetter than optimal, despite humans' substantial loss of sensitivity in thefovea (foveal neglect), and the implausibility of the human brain replicatingthe optimal computations. We show that three factors can quantitatively explainthese seemingly paradoxical results. Most importantly, simple and fixedheuristic decision rules reach near optimal search performance. Secondly,foveal neglect primarily affects only the central potential target location.Finally, spatially correlated neural noise can cause search performance toexceed that predicted for independent noise. These findings have broadimplications for understanding visual search tasks and other identificationtasks in humans and other animals.</description><author>Anqi Zhang, Wilson S. Geisler</author><pubDate>Mon, 10 Feb 2025 17:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12124v3</guid></item><item><title>FairDropout: Using Example-Tied Dropout to Enhance Generalization of Minority Groups</title><link>http://arxiv.org/abs/2502.06695v1</link><description>Deep learning models frequently exploit spurious features in training data toachieve low training error, often resulting in poor generalization when facedwith shifted testing distributions. To address this issue, various methods fromimbalanced learning, representation learning, and classifier recalibration havebeen proposed to enhance the robustness of deep neural networks againstspurious correlations. In this paper, we observe that models trained withempirical risk minimization tend to generalize well for examples from themajority groups while memorizing instances from minority groups. Building onrecent findings that show memorization can be localized to a limited number ofneurons, we apply example-tied dropout as a method we term FairDropout, aimedat redirecting this memorization to specific neurons that we subsequently dropout during inference. We empirically evaluate FairDropout using thesubpopulation benchmark suite encompassing vision, language, and healthcaretasks, demonstrating that it significantly reduces reliance on spuriouscorrelations, and outperforms state-of-the-art methods.</description><author>Geraldin Nanfack, Eugene Belilovsky</author><pubDate>Mon, 10 Feb 2025 17:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06695v1</guid></item><item><title>Adaptive Reconstruction for Graph Neural Networks</title><link>http://arxiv.org/abs/2406.17281v2</link><description>Graph Neural Networks (GNNs) have become fundamental in semi-supervisedlearning for graph representation, leveraging their ability to capture complexnode relationships. A recent trend in GNN research focuses on \textbf{adaptivek-hop structure learning}, moving beyond fixed-hop aggregation to more flexibleand dynamic neighborhood selection. While GAMLP \cite{Zhang_2022} employsseparate MLP layers for each k-hop domain and ImprovingTE\cite{Yao2023ImprovingTE} enhances this by injecting contextualizedsubstructure information, these methods still rely heavily on predefinedsampling strategies, which may limit their ability to generalize and maintainstable accuracy. To address these limitations, we propose an \textbf{adaptivereconstruction framework} that dynamically refines k-hop structure learning.Inspired by "coreset selection" \cite{guo2022deepcore}, our approach adaptively\textbf{reconstructs} node neighborhoods to optimize message passing, ensuringmore \textbf{effective and context-aware information flow} across the graph. Tofurther enhance structural robustness, we introduce two key modules: the\textbf{Distance Recomputator} and the \textbf{Topology Reconstructor}(\textcolor{blue}{DRTR}). The Distance Recomputator \textbf{reassesses andrecalibrates} node distances based on adaptive graph properties, leading to\textbf{improved node embeddings} that better reflect latent relationships.Meanwhile, the Topology Reconstructor \textbf{dynamically refines local graphstructures}, enabling the model to \textbf{adapt to evolving graph topologies}and mitigate the impact of noise and mislabeled data. Empirical evaluationsdemonstrate that our \textbf{adaptive reconstruction framework} achieves\textbf{significant improvements} over existing k-hop-based models, providingmore \textbf{stable and accurate} performance in various graph learningbenchmarks.</description><author>Dong Liu</author><pubDate>Mon, 10 Feb 2025 17:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17281v2</guid></item><item><title>Recent Advances, Applications and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2024 Symposium</title><link>http://arxiv.org/abs/2502.06693v1</link><description>The fourth Machine Learning for Health (ML4H) symposium was held in person onDecember 15th and 16th, 2024, in the traditional, ancestral, and uncededterritories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver,British Columbia, Canada. The symposium included research roundtable sessionsto foster discussions between participants and senior researchers on timely andrelevant topics for the ML4H community. The organization of the researchroundtables at the conference involved 13 senior and 27 junior chairs across 13tables. Each roundtable session included an invited senior chair (withsubstantial experience in the field), junior chairs (responsible forfacilitating the discussion), and attendees from diverse backgrounds with aninterest in the session's topic.</description><author>Amin Adibi, Xu Cao, Zongliang Ji, Jivat Neet Kaur, Winston Chen, Elizabeth Healey, Brighton Nuwagira, Wenqian Ye, Geoffrey Woollard, Maxwell A Xu, Hejie Cui, Johnny Xi, Trenton Chang, Vasiliki Bikia, Nicole Zhang, Ayush Noori, Yuan Xia, Md. Belal Hossain, Hanna A. Frank, Alina Peluso, Yuan Pu, Shannon Zejiang Shen, John Wu, Adibvafa Fallahpour, Sazan Mahbub, Ross Duncan, Yuwei Zhang, Yurui Cao, Zuheng Xu, Michael Craig, Rahul G. Krishnan, Rahmatollah Beheshti, James M. Rehg, Mohammad Ehsanul Karim, Megan Coffee, Leo Anthony Celi, Jason Alan Fries, Mohsen Sadatsafavi, Dennis Shung, Shannon McWeeney, Jessica Dafflon, Sarah Jabbour</author><pubDate>Mon, 10 Feb 2025 17:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06693v1</guid></item><item><title>Multi-label Scandinavian Language Identification (SLIDE)</title><link>http://arxiv.org/abs/2502.06692v1</link><description>Identifying closely related languages at sentence level is difficult, inparticular because it is often impossible to assign a sentence to a singlelanguage. In this paper, we focus on multi-label sentence-level Scandinavianlanguage identification (LID) for Danish, Norwegian Bokm\r{a}l, NorwegianNynorsk, and Swedish. We present the Scandinavian Language Identification andEvaluation, SLIDE, a manually curated multi-label evaluation dataset and asuite of LID models with varying speed-accuracy tradeoffs. We demonstrate thatthe ability to identify multiple languages simultaneously is necessary for anyaccurate LID method, and present a novel approach to training such multi-labelLID models.</description><author>Mariia Fedorova, Jonas Sebulon Frydenberg, Victoria Handford, Victoria Ovedie Chruickshank Langø, Solveig Helene Willoch, Marthe Løken Midtgaard, Yves Scherrer, Petter Mæhlum, David Samuel</author><pubDate>Mon, 10 Feb 2025 17:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06692v1</guid></item><item><title>CoverUp: Coverage-Guided LLM-Based Test Generation</title><link>http://arxiv.org/abs/2403.16218v3</link><description>Testing is an essential part of software development. Test generation toolsattempt to automate the otherwise labor-intensive task of test creation, butgenerating high-coverage tests remains challenging. This paper proposesCoverUp, a novel approach to driving the generation of high-coverage Pythonregression tests. CoverUp combines coverage analysis, code context, andfeedback in prompts that iteratively guide the LLM to generate tests thatimprove line and branch coverage. We evaluate our prototype CoverUpimplementation across a benchmark of challenging code derived from open-sourcePython projects and show that CoverUp substantially improves on the state ofthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,CoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).Compared to MuTAP, a mutation- and LLM-based test generator, CoverUp achievesan overall line+branch coverage of 90% (vs. 77%). We also demonstrate thatCoverUp's performance stems not only from the LLM used but from the combinedeffectiveness of its components.</description><author>Juan Altmayer Pizzorno, Emery D. Berger</author><pubDate>Mon, 10 Feb 2025 17:15:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16218v3</guid></item><item><title>Neumann eigenmaps for landmark embedding</title><link>http://arxiv.org/abs/2502.06689v1</link><description>We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing thestandard diffusion map embedding using landmarks, i.e distinguished sampleswithin the dataset. By interpreting these landmarks as a subgraph of the largerdata graph, NeuMaps are obtained via the eigendecomposition of a renormalizedNeumann Laplacian. We show that NeuMaps offer two key advantages: (1) theyprovide a computationally efficient embedding that accurately recovers thediffusion distance associated with the reflecting random walk on the subgraph,and (2) they naturally incorporate the Nystr\"om extension within the diffusionmap framework through the discrete Neumann boundary condition. Through examplesin digit classification and molecular dynamics, we demonstrate that NeuMaps notonly improve upon existing landmark-based embedding methods but also enhancethe stability of diffusion map embeddings to the removal of highly significantpoints.</description><author>Shashank Sule, Wojciech Czaja</author><pubDate>Mon, 10 Feb 2025 17:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06689v1</guid></item><item><title>No Trick, No Treat: Pursuits and Challenges Towards Simulation-free Training of Neural Samplers</title><link>http://arxiv.org/abs/2502.06685v1</link><description>We consider the sampling problem, where the aim is to draw samples from adistribution whose density is known only up to a normalization constant. Recentbreakthroughs in generative modeling to approximate a high-dimensional datadistribution have sparked significant interest in developing neuralnetwork-based methods for this challenging problem. However, neural samplerstypically incur heavy computational overhead due to simulating trajectoriesduring training. This motivates the pursuit of simulation-free trainingprocedures of neural samplers. In this work, we propose an elegant modificationto previous methods, which allows simulation-free training with the help of atime-dependent normalizing flow. However, it ultimately suffers from severemode collapse. On closer inspection, we find that nearly all successful neuralsamplers rely on Langevin preconditioning to avoid mode collapsing. Wesystematically analyze several popular methods with various objective functionsand demonstrate that, in the absence of Langevin preconditioning, most of themfail to adequately cover even a simple target. Finally, we draw attention to astrong baseline by combining the state-of-the-art MCMC method, ParallelTempering (PT), with an additional generative model to shed light on futureexplorations of neural samplers.</description><author>Jiajun He, Yuanqi Du, Francisco Vargas, Dinghuai Zhang, Shreyas Padhy, RuiKang OuYang, Carla Gomes, José Miguel Hernández-Lobato</author><pubDate>Mon, 10 Feb 2025 17:13:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06685v1</guid></item><item><title>EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks</title><link>http://arxiv.org/abs/2502.06684v1</link><description>Recent foundational models for tabular data, such as TabPFN, havedemonstrated remarkable effectiveness in adapting to new tasks throughin-context learning. However, these models overlook a crucial equivarianceproperty: the arbitrary ordering of target dimensions should not influencemodel predictions. In this study, we identify this oversight as a source ofincompressible error, termed the equivariance gap, which introduces instabilityin predictions. To mitigate these issues, we propose a novel model designed topreserve equivariance across output dimensions. Our experimental resultsindicate that our proposed model not only addresses these pitfalls effectivelybut also achieves competitive benchmark performance.</description><author>Michael Arbel, David Salinas, Frank Hutter</author><pubDate>Mon, 10 Feb 2025 17:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06684v1</guid></item><item><title>LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection</title><link>http://arxiv.org/abs/2502.01678v2</link><description>Electroencephalogram (EEG) provides a non-invasive, highly accessible, andcost-effective solution for Alzheimer's Disease (AD) detection. However,existing methods, whether based on manual feature extraction or deep learning,face two major challenges: the lack of large-scale datasets for robust featurelearning and evaluation, and poor detection performance due to inter-subjectvariations. To address these challenges, we curate an EEG-AD corpus containing813 subjects, which forms the world's largest EEG-AD dataset to the best of ourknowledge. Using this unique dataset, we propose LEAD, the first largefoundation model for EEG-based AD detection. Our method encompasses an entirepipeline, from data selection and preprocessing to self-supervised contrastivepretraining, fine-tuning, and key setups such as subject-independent evaluationand majority voting for subject-level detection. We pre-train the model on 11EEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervisedpre-training design includes sample-level and subject-level contrasting toextract useful general EEG features. Fine-tuning is performed on 5channel-aligned datasets together. The backbone encoder incorporates temporaland channel embeddings to capture features across both temporal and spatialdimensions. Our method demonstrates outstanding AD detection performance,achieving up to a 9.86% increase in F1 score at the sample-level and up to a9.31% at the subject-level compared to state-of-the-art methods. The results ofour model strongly confirm the effectiveness of contrastive pre-training andchannel-aligned unified fine-tuning for addressing inter-subject variation. Thesource code is at https://github.com/DL4mHealth/LEAD.</description><author>Yihe Wang, Nan Huang, Nadia Mammone, Marco Cecchi, Xiang Zhang</author><pubDate>Mon, 10 Feb 2025 17:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.01678v2</guid></item><item><title>Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene</title><link>http://arxiv.org/abs/2502.06682v1</link><description>Self-driving cars relying solely on ego-centric perception face limitationsin sensing, often failing to detect occluded, faraway objects. Collaborativeautonomous driving (CAV) seems like a promising direction, but collecting datafor development is non-trivial. It requires placing multiple sensor-equippedagents in a real-world driving scene, simultaneously! As such, existingdatasets are limited in locations and agents. We introduce a novel surrogate tothe rescue, which is to generate realistic perception from different viewpointsin a driving scene, conditioned on a real-world sample - the ego-car's sensorydata. This surrogate has huge potential: it could potentially turn any ego-cardataset into a collaborative driving one to scale up the development of CAV. Wepresent the very first solution, using a combination of simulated collaborativedata and real ego-car data. Our method, Transfer Your Perspective (TYP), learnsa conditioned diffusion model whose output samples are not only realistic butalso consistent in both semantics and layouts with the given ego-car data.Empirical results demonstrate TYP's effectiveness in aiding in a CAV setting.In particular, TYP enables us to (pre-)train collaborative perceptionalgorithms like early and late fusion with little or no real-worldcollaborative data, greatly facilitating downstream CAV applications.</description><author>Tai-Yu Pan, Sooyoung Jeon, Mengdi Fan, Jinsu Yoo, Zhenyang Feng, Mark Campbell, Kilian Q. Weinberger, Bharath Hariharan, Wei-Lun Chao</author><pubDate>Mon, 10 Feb 2025 17:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06682v1</guid></item><item><title>CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis</title><link>http://arxiv.org/abs/2502.06681v1</link><description>Person re-identification (Re-ID) is a key challenge in computer vision,requiring the matching of individuals across different cameras, locations, andtime periods. While most research focuses on short-term scenarios with minimalappearance changes, real-world applications demand robust Re-ID systems capableof handling long-term scenarios, where persons' appearances can changesignificantly due to variations in clothing and physical characteristics. Inthis paper, we present CHIRLA, Comprehensive High-resolution Identification andRe-identification for Large-scale Analysis, a novel dataset specificallydesigned for long-term person Re-ID. CHIRLA consists of recordings fromstrategically placed cameras over a seven-month period, capturing significantvariations in both temporal and appearance attributes, including controlledchanges in participants' clothing and physical features. The dataset includes22 individuals, four connected indoor environments, and seven cameras. Wecollected more than five hours of video that we semi-automatically labeled togenerate around one million bounding boxes with identity annotations. Byintroducing this comprehensive benchmark, we aim to facilitate the developmentand evaluation of Re-ID algorithms that can reliably perform in challenging,long-term real-world scenarios.</description><author>Bessie Dominguez-Dager, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla</author><pubDate>Mon, 10 Feb 2025 17:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06681v1</guid></item><item><title>Conformalized Strategy-Proof Auctions</title><link>http://arxiv.org/abs/2405.12016v4</link><description>Auctions are key for maximizing sellers' revenue and ensuring truthfulbidding among buyers. Recently, an approach known as differentiable economicsbased on machine learning (ML) has shown promise in learning powerful auctionmechanisms for multiple items and participants. However, this approach has noguarantee of strategy-proofness at test time. Strategy-proofness is crucial asit ensures that buyers are incentivized to bid their true valuations, leadingto optimal and fair auction outcomes without the risk of manipulation. In thiswork, we propose a formulation of statistical strategy-proofness auctionmechanism, ensuring that the probability of regret exceeding a predefinedthreshold is strictly controlled. Building upon conformal predictiontechniques, we develop an auction acceptance rule that leverages regretpredictions to guarantee that the data-driven auction mechanism meets thestatistical strategy-proofness requirement with high probability. Our approachrepresents a practical middle-ground between two extremes: forcing zero-regretat the cost of significant revenue loss, and naively using ML to constructauctions with the hope of attaining low regret at test time. Numericalexperiments demonstrate the necessity of the proposed method, the validity ofour theoretical result, and its applicability.</description><author>Roy Maor Lotan, Inbal Talgam-Cohen, Yaniv Romano</author><pubDate>Mon, 10 Feb 2025 17:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12016v4</guid></item><item><title>Quantile Multi-Armed Bandits with 1-bit Feedback</title><link>http://arxiv.org/abs/2502.06678v1</link><description>In this paper, we study a variant of best-arm identification involvingelements of risk sensitivity and communication constraints. Specifically, thegoal of the learner is to identify the arm with the highest quantile reward,while the communication from an agent (who observes rewards) and the learner(who chooses actions) is restricted to only one bit of feedback per arm pull.We propose an algorithm that utilizes noisy binary search as a subroutine,allowing the learner to estimate quantile rewards through 1-bit feedback. Wederive an instance-dependent upper bound on the sample complexity of ouralgorithm and provide an algorithm-independent lower bound for specificinstances, with the two matching to within logarithmic factors under mildconditions, or even to within constant factors in certain low error probabilityscaling regimes. The lower bound is applicable even in the absence ofcommunication constraints, and thus we conclude that restricting to 1-bitfeedback has a minimal impact on the scaling of the sample complexity.</description><author>Ivan Lau, Jonathan Scarlett</author><pubDate>Mon, 10 Feb 2025 17:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06678v1</guid></item><item><title>RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and Service Provider Management in Multi-Domain Networks</title><link>http://arxiv.org/abs/2502.06674v1</link><description>The emergence of the fifth generation (5G) technology has transformed mobilenetworks into multi-service environments, necessitating efficient networkslicing to meet diverse Service Level Agreements (SLAs). SLA decompositionacross multiple network domains, each potentially managed by different serviceproviders, poses a significant challenge due to limited visibility intoreal-time underlying domain conditions. This paper introduces Risk-AwareIterated Local Search (RAILS), a novel risk model-driven meta-heuristicframework designed to jointly address SLA decomposition and service providerselection in multi-domain networks. By integrating online risk modeling withiterated local search principles, RAILS effectively navigates the complexoptimization landscape, utilizing historical feedback from domain controllers.We formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP)problem and prove its NP-hardness. Extensive simulations demonstrate that RAILSachieves near-optimal performance, offering an efficient, real-time solutionfor adaptive SLA management in modern multi-domain networks.</description><author>Cyril Shih-Huan Hsu, Chrysa Papagianni, Paola Grosso</author><pubDate>Mon, 10 Feb 2025 17:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06674v1</guid></item><item><title>Covariates-Adjusted Mixed-Membership Estimation: A Novel Network Model with Optimal Guarantees</title><link>http://arxiv.org/abs/2502.06671v1</link><description>This paper addresses the problem of mixed-membership estimation in networks,where the goal is to efficiently estimate the latent mixed-membership structurefrom the observed network. Recognizing the widespread availability and valuableinformation carried by node covariates, we propose a novel network model thatincorporates both community information, as represented by the Degree-CorrectedMixed Membership (DCMM) model, and node covariate similarities to determineconnections. We investigate the regularized maximum likelihood estimation (MLE) for thismodel and demonstrate that our approach achieves optimal estimation accuracyfor both the similarity matrix and the mixed-membership, in terms of both theFrobenius norm and the entrywise loss. Since directly analyzing the originalconvex optimization problem is intractable, we employ nonconvex optimization tofacilitate the analysis. A key contribution of our work is identifying acrucial assumption that bridges the gap between convex and nonconvex solutions,enabling the transfer of statistical guarantees from the nonconvex approach toits convex counterpart. Importantly, our analysis extends beyond the MLE lossand the mean squared error (MSE) used in matrix completion problems,generalizing to all the convex loss functions. Consequently, our analysistechniques extend to a broader set of applications, including ranking problemsbased on pairwise comparisons. Finally, simulation experiments validate our theoretical findings, andreal-world data analyses confirm the practical relevance of our model.</description><author>Jianqing Fan, Jiawei Ge, Jikai Hou</author><pubDate>Mon, 10 Feb 2025 16:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06671v1</guid></item><item><title>Predicting Molecular Ground-State Conformation via Conformation Optimization</title><link>http://arxiv.org/abs/2410.09795v3</link><description>Predicting molecular ground-state conformation (i.e., energy-minimizedconformation) is crucial for many chemical applications such as moleculardocking and property prediction. Classic energy-based simulation istime-consuming when solving this problem while existing learning-based methodshave advantages in computational efficiency but sacrifice accuracy andinterpretability. In this work, we propose a novel and effective method tobridge the energy-based simulation and the learning-based strategy, whichdesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, calledWGFormer, for molecular ground-state conformation prediction. Specifically, ourmethod tackles this task within an auto-encoding framework, which encodeslow-quality conformations by the proposed WGFormer and decodes correspondingground-state conformations by an MLP. The architecture of WGFormer correspondsto Wasserstein gradient flows -- it optimizes molecular conformations byminimizing an energy function defined on the latent mixture models of atoms,thereby significantly improving performance and interpretability. Extensiveexperiments show that our method consistently outperforms state-of-the-artcompetitors, providing a new and insightful paradigm to predict molecularground-state conformation.</description><author>Fanmeng Wang, Minjie Cheng, Hongteng Xu</author><pubDate>Mon, 10 Feb 2025 16:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09795v3</guid></item><item><title>Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations</title><link>http://arxiv.org/abs/2502.06669v1</link><description>Significant improvements have been observed in the zero-shot capabilities ofthe Large Language Models (LLMs). Due to their high sensitivity to input,research has increasingly focused on enhancing LLMs' performance via direct andsimple prompt engineering rather than intricate domain adaptation. Studiessuggest that LLMs exhibit emotional intelligence, and both positive andnegative emotions can potentially enhance task performances. However, priorinteraction prompts have predominantly concentrated on a single stimulus type,neglecting to compare different stimulus effects, examine the influence ofvarying task difficulties, or explore underlying mechanisms. This paper,inspired by the positive correlation between self-efficacy and task performancewithin the social cognitive theory, introduces Verbal Efficacy Stimulations(VES). Our VES comprises three types of verbal prompts: encouraging,provocative, and critical, addressing six aspects such as helpfulness andcompetence. And we further categorize task difficulty, aiming to extensivelyinvestigate how distinct VES influence the self-efficacy and task achievementsof language models at varied levels of difficulty. The experimental resultsshow that the three types of VES improve the performance of LLMs on most tasks,and the most effective VES varies for different models. In extensiveexperiments, we have obtained some findings consistent with psychologicaltheories, providing novel insights for future research.</description><author>Rui Chen, Tailai Peng, Xinran Xie, Dekun Lin, Zhe Cui, Zheng Chen</author><pubDate>Mon, 10 Feb 2025 16:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06669v1</guid></item><item><title>Automatic Evaluation of Healthcare LLMs Beyond Question-Answering</title><link>http://arxiv.org/abs/2502.06666v1</link><description>Current Large Language Models (LLMs) benchmarks are often based on open-endedor close-ended QA evaluations, avoiding the requirement of human labor.Close-ended measurements evaluate the factuality of responses but lackexpressiveness. Open-ended capture the model's capacity to produce discourseresponses but are harder to assess for correctness. These two approaches arecommonly used, either independently or together, though their relationshipremains poorly understood. This work is focused on the healthcare domain, whereboth factuality and discourse matter greatly. It introduces a comprehensive,multi-axis suite for healthcare LLM evaluation, exploring correlations betweenopen and close benchmarks and metrics. Findings include blind spots andoverlaps in current methodologies. As an updated sanity check, we release a newmedical benchmark--CareQA--, with both open and closed variants. Finally, wepropose a novel metric for open-ended evaluations --Relaxed Perplexity-- tomitigate the identified limitations.</description><author>Anna Arias-Duart, Pablo Agustin Martin-Torres, Daniel Hinjos, Pablo Bernabeu-Perez, Lucia Urcelay Ganzabal, Marta Gonzalez Mallo, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Sergio Alvarez-Napagao, Dario Garcia-Gasulla</author><pubDate>Mon, 10 Feb 2025 16:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06666v1</guid></item><item><title>A Family of Distributions of Random Subsets for Controlling Positive and Negative Dependence</title><link>http://arxiv.org/abs/2408.01022v2</link><description>Positive and negative dependence are fundamental concepts that characterizethe attractive and repulsive behavior of random subsets. Although someprobabilistic models are known to exhibit positive or negative dependence, itis challenging to seamlessly bridge them with a practicable probabilisticmodel. In this study, we introduce a new family of distributions, named thediscrete kernel point process (DKPP), which includes determinantal pointprocesses and parts of Boltzmann machines. We also develop some computationalmethods for probabilistic operations and inference with DKPPs, such ascalculating marginal and conditional probabilities and learning the parameters.Our numerical experiments demonstrate the controllability of positive andnegative dependence and the effectiveness of the computational methods forDKPPs.</description><author>Takahiro Kawashima, Hideitsu Hino</author><pubDate>Mon, 10 Feb 2025 16:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01022v2</guid></item><item><title>Evaluation of Deep Audio Representations for Hearables</title><link>http://arxiv.org/abs/2502.06664v1</link><description>Effectively steering hearable devices requires understanding the acousticenvironment around the user. In the computational analysis of sound scenes,foundation models have emerged as the state of the art to producehigh-performance, robust, multi-purpose audio representations. We introduce andrelease Deep Evaluation of Audio Representations (DEAR), the first dataset andbenchmark to evaluate the efficacy of foundation models in capturing essentialacoustic properties for hearables. The dataset includes 1,158 audio tracks,each 30 seconds long, created by spatially mixing proprietary monologues withcommercial, high-quality recordings of everyday acoustic scenes. Our benchmarkencompasses eight tasks that assess the general context, speech sources, andtechnical acoustic properties of the audio scenes. Through our evaluation offour general-purpose audio representation models, we demonstrate that the BEATsmodel significantly surpasses its counterparts. This superiority underscoresthe advantage of models trained on diverse audio collections, confirming theirapplicability to a wide array of auditory tasks, including encoding theenvironment properties necessary for hearable steering. The DEAR dataset andassociated code are available at https://dear-dataset.github.io.</description><author>Fabian Gröger, Pascal Baumann, Ludovic Amruthalingam, Laurent Simon, Ruksana Giurda, Simone Lionetti</author><pubDate>Mon, 10 Feb 2025 16:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06664v1</guid></item><item><title>EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models</title><link>http://arxiv.org/abs/2502.06663v1</link><description>Modern large language models (LLMs) driven by scaling laws, achieveintelligence emergency in large model sizes. Recently, the increasing concernsabout cloud costs, latency, and privacy make it an urgent requirement todevelop compact edge language models. Distinguished from direct pretrainingthat bounded by the scaling law, this work proposes the pruning-awarepretraining, focusing on retaining performance of much larger optimized models.It features following characteristics: 1) Data-scalable: we introduce minimalparameter groups in LLM and continuously optimize structural pruning, extendingpost-training pruning methods like LLM-Pruner and SparseGPT into thepretraining phase. 2) Architecture-agnostic: the LLM architecture isauto-designed using saliency-driven pruning, which is the first time to exceedSoTA human-designed LLMs in modern pretraining. We reveal that it achievestop-quality edge language models, termed EfficientLLM, by scaling up LLMcompression and extending its boundary. EfficientLLM significantly outperformsSoTA baselines with $100M \sim 1B$ parameters, such as MobileLLM, SmolLM,Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the firstattempt, EfficientLLM bridges the performance gap between traditional LLMcompression and direct pretraining methods, and we will fully open source athttps://github.com/Xingrun-Xing2/EfficientLLM.</description><author>Xingrun Xing, Zheng Liu, Shitao Xiao, Boyan Gao, Yiming Liang, Wanpeng Zhang, Haokun Lin, Guoqi Li, Jiajun Zhang</author><pubDate>Mon, 10 Feb 2025 16:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06663v1</guid></item><item><title>iLOCO: Distribution-Free Inference for Feature Interactions</title><link>http://arxiv.org/abs/2502.06661v1</link><description>Feature importance measures are widely studied and are essential forunderstanding model behavior, guiding feature selection, and enhancinginterpretability. However, many machine learning fitted models involve complex,higher-order interactions between features. Existing feature importance metricsfail to capture these higher-order effects while existing interaction metricsoften suffer from limited applicability or excessive computation; no methodsexist to conduct statistical inference for feature interactions. To bridge thisgap, we first propose a new model-agnostic metric, interactionLeave-One-Covariate-Out iLOCO, for measuring the importance of higher-orderfeature interactions. Next, we leverage recent advances in LOCO inference todevelop distribution-free and assumption-light confidence intervals for ouriLOCO metric. To address computational challenges, we also introduce anensemble learning method for calculating the iLOCO metric and confidenceintervals that we show is both computationally and statistically efficient. Wevalidate our iLOCO metric and our confidence intervals on both synthetic andreal data sets, showing that our approach outperforms existing methods andprovides the first inferential approach to detecting feature interactions.</description><author>Camille Little, Lili Zheng, Genevera Allen</author><pubDate>Mon, 10 Feb 2025 16:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06661v1</guid></item><item><title>Generalizable Implicit Motion Modeling for Video Frame Interpolation</title><link>http://arxiv.org/abs/2407.08680v5</link><description>Motion modeling is critical in flow-based Video Frame Interpolation (VFI).Existing paradigms either consider linear combinations of bidirectional flowsor directly predict bilateral flows for given timestamps without exploringfavorable motion priors, thus lacking the capability of effectively modelingspatiotemporal dynamics in real-world videos. To address this limitation, inthis study, we introduce Generalizable Implicit Motion Modeling (GIMM), a noveland effective approach to motion modeling for VFI. Specifically, to enable GIMMas an effective motion modeling paradigm, we design a motion encoding pipelineto model spatiotemporal motion latent from bidirectional flows extracted frompre-trained flow estimators, effectively representing input-specific motionpriors. Then, we implicitly predict arbitrary-timestep optical flows within twoadjacent input frames via an adaptive coordinate-based neural network, withspatiotemporal coordinates and motion latent as inputs. Our GIMM can be easilyintegrated with existing flow-based VFI works by supplying accurately modeledmotion. We show that GIMM performs better than the current state of the art onstandard VFI benchmarks.</description><author>Zujin Guo, Wei Li, Chen Change Loy</author><pubDate>Mon, 10 Feb 2025 16:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08680v5</guid></item><item><title>Who Taught You That? Tracing Teachers in Model Distillation</title><link>http://arxiv.org/abs/2502.06659v1</link><description>Model distillation -- using outputs from a large teacher model to teach asmall student model -- is a practical means of creating efficient models for aparticular task. We ask: Can we identify a students' teacher based on itsoutputs? Such "footprints" left by teacher LLMs would be interesting artifacts.Beyond this, reliable teacher inference may have practical implications asactors seek to distill specific capabilities of massive proprietary LLMs intodeployed smaller LMs, potentially violating terms of service. We considerpractical task distillation targets including summarization, questionanswering, and instruction-following. We assume a finite set of candidateteacher models, which we treat as blackboxes. We design discriminative modelsthat operate over lexical features. We find that $n$-gram similarity alone isunreliable for identifying teachers, but part-of-speech (PoS) templatespreferred by student models mimic those of their teachers.</description><author>Somin Wadhwa, Chantal Shaib, Silvio Amir, Byron C. Wallace</author><pubDate>Mon, 10 Feb 2025 16:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06659v1</guid></item><item><title>Generating Samples to Question Trained Models</title><link>http://arxiv.org/abs/2502.06658v1</link><description>There is a growing need for investigating how machine learning modelsoperate. With this work, we aim to understand trained machine learning modelsby questioning their data preferences. We propose a mathematical framework thatallows us to probe trained models and identify their preferred samples invarious scenarios including prediction-risky, parameter-sensitive, ormodel-contrastive samples. To showcase our framework, we pose these queries toa range of models trained on a range of classification and regression tasks,and receive answers in the form of generated data.</description><author>E. Mehmet Kıral, Nurşen Aydın, Ş. İlker Birbil</author><pubDate>Mon, 10 Feb 2025 16:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06658v1</guid></item><item><title>A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management</title><link>http://arxiv.org/abs/2502.06656v1</link><description>The recent development of powerful AI systems has highlighted the need forrobust risk management frameworks in the AI industry. Although companies havebegun to implement safety frameworks, current approaches often lack thesystematic rigor found in other high-risk industries. This paper presents acomprehensive risk management framework for the development of frontier AI thatbridges this gap by integrating established risk management principles withemerging AI-specific practices. The framework consists of four key components:(1) risk identification (through literature review, open-ended red-teaming, andrisk modeling), (2) risk analysis and evaluation using quantitative metrics andclearly defined thresholds, (3) risk treatment through mitigation measures suchas containment, deployment controls, and assurance processes, and (4) riskgovernance establishing clear organizational structures and accountability.Drawing from best practices in mature industries such as aviation or nuclearpower, while accounting for AI's unique challenges, this framework provides AIdevelopers with actionable guidelines for implementing robust risk management.The paper details how each component should be implemented throughout thelife-cycle of the AI system - from planning through deployment - and emphasizesthe importance and feasibility of conducting risk management work prior to thefinal training run to minimize the burden associated with it.</description><author>Simeon Campos, Henry Papadatos, Fabien Roger, Chloé Touzet, Malcolm Murray, Otter Quarks</author><pubDate>Mon, 10 Feb 2025 16:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06656v1</guid></item><item><title>Unbiased Evaluation of Large Language Models from a Causal Perspective</title><link>http://arxiv.org/abs/2502.06655v1</link><description>Benchmark contamination has become a significant concern in the LLMevaluation community. Previous Agents-as-an-Evaluator address this issue byinvolving agents in the generation of questions. Despite their success, thebiases in Agents-as-an-Evaluator methods remain largely unexplored. In thispaper, we present a theoretical formulation of evaluation bias, providingvaluable insights into designing unbiased evaluation protocols. Furthermore, weidentify two type of bias in Agents-as-an-Evaluator through carefully designedprobing tasks on a minimal Agents-as-an-Evaluator setup. To address theseissues, we propose the Unbiased Evaluator, an evaluation protocol that deliversa more comprehensive, unbiased, and interpretable assessment of LLMs.Extensiveexperiments reveal significant room for improvement in current LLMs.Additionally, we demonstrate that the Unbiased Evaluator not only offers strongevidence of benchmark contamination but also provides interpretable evaluationresults.</description><author>Meilin Chen, Jian Tian, Liang Ma, Di Xie, Weijie Chen, Jiang Zhu</author><pubDate>Mon, 10 Feb 2025 16:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06655v1</guid></item><item><title>In-Context Learning (and Unlearning) of Length Biases</title><link>http://arxiv.org/abs/2502.06653v1</link><description>Large language models have demonstrated strong capabilities to learnin-context, where exemplar input-output pairings are appended to the prompt fordemonstration. However, existing work has demonstrated the ability of models tolearn lexical and label biases in-context, which negatively impacts bothperformance and robustness of models. The impact of other statistical databiases remains under-explored, which this work aims to address. We specificallyinvestigate the impact of length biases on in-context learning. We demonstratethat models do learn length biases in the context window for their predictions,and further empirically analyze the factors that modulate the level of biasexhibited by the model. In addition, we show that learning length informationin-context can be used to counter the length bias that has been encoded inmodels (e.g., via fine-tuning). This reveals the power of in-context learningin debiasing model prediction behaviors without the need for costly parameterupdates.</description><author>Stephanie Schoch, Yangfeng Ji</author><pubDate>Mon, 10 Feb 2025 16:43:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06653v1</guid></item><item><title>DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications</title><link>http://arxiv.org/abs/2409.19020v3</link><description>The scarcity of domain-specific dialogue datasets limits the development ofdialogue systems across applications. Existing research is constrained bygeneral or niche datasets that lack sufficient scale for training dialoguesystems. To address this gap, we introduce DiaSynth - a synthetic dialoguegeneration framework capable of generating high-quality, contextually richdialogues across a wide range of domains. Unlike existing frameworks, DiaSynthuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning togenerate dynamic, domain-specific dialogues with simulated personas and diverseconversational features. We perform our experiments by generating syntheticdata using different LLMs and few-shot examples from DialogSum and SAMSum. Thepretrained language models fine-tuned on the synthetic data outperform the basemodels by 16.47% on dialogue summarization, while the comparison between modelsfine-tuned on in-domain data and synthetic data shows that the synthetic datais able to capture 90.48% of the performance distribution of the in-domain dataon dialogue summarization. The quality of the data generated also increases aswe increase the size of LLM from 3B to 8B. These results validate DiaSynth'spotential as a robust alternative to traditional data collection methods. Weopen source the code and data generated for future research.</description><author>Sathya Krishnan Suresh, Wu Mengjun, Tushar Pranav, Eng Siong Chng</author><pubDate>Mon, 10 Feb 2025 16:42:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19020v3</guid></item><item><title>Transparent NLP: Using RAG and LLM Alignment for Privacy Q&amp;A</title><link>http://arxiv.org/abs/2502.06652v1</link><description>The transparency principle of the General Data Protection Regulation (GDPR)requires data processing information to be clear, precise, and accessible.While language models show promise in this context, their probabilistic naturecomplicates truthfulness and comprehensibility. This paper examines state-of-the-art Retrieval Augmented Generation (RAG)systems enhanced with alignment techniques to fulfill GDPR obligations. Weevaluate RAG systems incorporating an alignment module like RewindableAuto-regressive Inference (RAIN) and our proposed multidimensional extension,MultiRAIN, using a Privacy Q&amp;A dataset. Responses are optimized for precisenessand comprehensibility and are assessed through 21 metrics, includingdeterministic and large language model-based evaluations. Our results show that RAG systems with an alignment module outperformbaseline RAG systems on most metrics, though none fully match human answers.Principal component analysis of the results reveals complex interactionsbetween metrics, highlighting the need to refine metrics. This study provides afoundation for integrating advanced natural language processing systems intolegal compliance frameworks.</description><author>Anna Leschanowsky, Zahra Kolagar, Erion Çano, Ivan Habernal, Dara Hallinan, Emanuël A. P. Habets, Birgit Popp</author><pubDate>Mon, 10 Feb 2025 16:42:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06652v1</guid></item><item><title>Prototype Contrastive Consistency Learning for Semi-Supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2502.06650v1</link><description>Medical image segmentation is a crucial task in medical image analysis, butit can be very challenging especially when there are less labeled data but withlarge unlabeled data. Contrastive learning has proven to be effective formedical image segmentation in semi-supervised learning by constructingcontrastive samples from partial pixels. However, although previous contrastivelearning methods can mine semantic information from partial pixels withinimages, they ignore the whole context information of unlabeled images, which isvery important to precise segmentation. In order to solve this problem, wepropose a novel prototype contrastive learning method called PrototypeContrastive Consistency Segmentation (PCCS) for semi-supervised medical imagesegmentation. The core idea is to enforce the prototypes of the same semanticclass to be closer and push the prototypes in different semantic classes faraway from each other. Specifically, we construct a signed distance map and anuncertainty map from unlabeled images. The signed distance map is used toconstruct prototypes for contrastive learning, and then we estimate theprototype uncertainty from the uncertainty map as trade-off among prototypes.In order to obtain better prototypes, based on the student-teacherarchitecture, a new mechanism named prototype updating prototype is designed toassist in updating the prototypes for contrastive learning. In addition, wepropose an uncertainty-consistency loss to mine more reliable information fromunlabeled data. Extensive experiments on medical image segmentation demonstratethat PCCS achieves better segmentation performance than the state-of-the-artmethods. The code is available at https://github.com/comphsh/PCCS.</description><author>Shihuan He, Zhihui Lai, Ruxin Wang, Heng Kong</author><pubDate>Mon, 10 Feb 2025 16:40:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06650v1</guid></item><item><title>Estimation of Food Intake Quantity Using Inertial Signals from Smartwatches</title><link>http://arxiv.org/abs/2502.06649v1</link><description>Accurate monitoring of eating behavior is crucial for managing obesity andeating disorders such as bulimia nervosa. At the same time, existing methodsrely on multiple and/or specialized sensors, greatly harming adherence andultimately, the quality and continuity of data. This paper introduces a novelapproach for estimating the weight of a bite, from a commercial smartwatch. Ourpublicly-available dataset contains smartwatch inertial data from tenparticipants, with manually annotated start and end times of each bite alongwith their corresponding weights from a smart scale, under semi-controlledconditions. The proposed method combines extracted behavioral features such asthe time required to load the utensil with food, with statistical features ofinertial signals, that serve as input to a Support Vector Regression model toestimate bite weights. Under a leave-one-subject-out cross-validation scheme,our approach achieves a mean absolute error (MAE) of 3.99 grams per bite. Tocontextualize this performance, we introduce the improvement metric, thatmeasures the relative MAE difference compared to a baseline model. Our methoddemonstrates a 17.41% improvement, while the adapted state-of-the art methodshows a -28.89% performance against that same baseline. The results presentedin this work establish the feasibility of extracting meaningful bite weightestimates from commercial smartwatch inertial sensors alone, laying thegroundwork for future accessible, non-invasive dietary monitoring systems.</description><author>Ioannis Levi, Konstantinos Kyritsis, Vasileios Papapanagiotou, Georgios Tsakiridis, Anastasios Delopoulos</author><pubDate>Mon, 10 Feb 2025 16:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06649v1</guid></item><item><title>The 2021 Tokyo Olympics Multilingual News Article Dataset</title><link>http://arxiv.org/abs/2502.06648v1</link><description>In this paper, we introduce a dataset of multilingual news articles coveringthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, andpublished between July 1, 2021, and August 14, 2021. These articles are writtenin nine languages from different language families and in different scripts. Tocreate the dataset, the raw news articles were first retrieved via a servicethat collects and analyzes news articles. Then, the articles were grouped usingan online clustering algorithm, with each group containing articles reportingon the same sub-event. Finally, the groups were manually annotated andevaluated. The development of this dataset aims to provide a resource forevaluating the performance of multilingual news clustering algorithms, forwhich limited datasets are available. It can also be used to analyze thedynamics and events of the 2021 Tokyo Olympics from different perspectives. Thedataset is available in CSV format and can be accessed from the CLARIN.SIrepository.</description><author>Erik Novak, Erik Calcina, Dunja Mladenić, Marko Grobelnik</author><pubDate>Mon, 10 Feb 2025 16:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06648v1</guid></item><item><title>Koopman-Equivariant Gaussian Processes</title><link>http://arxiv.org/abs/2502.06645v1</link><description>Credible forecasting and representation learning of dynamical systems are ofever-increasing importance for reliable decision-making. To that end, wepropose a family of Gaussian processes (GP) for dynamical systems with lineartime-invariant responses, which are nonlinear only in initial conditions. Thislinearity allows us to tractably quantify forecasting and representationaluncertainty, simultaneously alleviating the challenge of computing thedistribution of trajectories from a GP-based dynamical system and enabling anew probabilistic treatment of learning Koopman operator representations. Usinga trajectory-based equivariance -- which we refer to as \textit{Koopmanequivariance} -- we obtain a GP model with enhanced generalizationcapabilities. To allow for large-scale regression, we equip our framework withvariational inference based on suitable inducing points. Experimentsdemonstrate on-par and often better forecasting performance compared tokernel-based methods for learning dynamical systems.</description><author>Petar Bevanda, Max Beier, Armin Lederer, Alexandre Capone, Stefan Sosnowski, Sandra Hirche</author><pubDate>Mon, 10 Feb 2025 16:35:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06645v1</guid></item><item><title>MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing</title><link>http://arxiv.org/abs/2502.06643v1</link><description>Mixture-of-Experts (MoE) model architecture has emerged as a promisingsolution for scaling transformer models efficiently, offering sparse activationthat reduces computational costs while increasing model capacity. However, asMoE models scale, they need to be distributed across GPU devices, thus facecritical performance bottlenecks due to their large memory footprint. Expertparallelism distributes experts across GPUs, however, faces key challengesincluding an unbalanced token routing and expert activation, resulting incommunication tail latency and processing inefficiencies. While existingsolutions address some of these issues, they fail to resolve the dualchallenges of load imbalance and communication skew. The imbalance in tokenprocessing load across experts causes uneven processing times on differentGPUs, while communication skew between GPUs leads to unbalanced inter-GPU datatransfers. These factors degrade the performance of MoE models by increasingtail latency and reducing overall throughput. To address these limitations, wepropose an Integer Linear Programming (ILP) formulation to optimize expertplacement by jointly considering token load, communication, and computationcosts. We exploit the property that there is a token routing dependency acrosslayers, where tokens routed to a specific expert in one layer are likely to berouted to a limited set of experts in the subsequent layer. Our solution,MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPUtoken routing costs and balances token processing across devices, therebyreducing tail latency and end-to-end execution time. Experimental resultsdemonstrate 9.3% and 17.5% of end-to-end speedups for single-node andmulti-node inference respectively, showcasing the potential of our ILP-basedoptimization for offering expert parallel solutions for next-generation MoEs.</description><author>Seokjin Go, Divya Mahajan</author><pubDate>Mon, 10 Feb 2025 16:34:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06643v1</guid></item></channel></rss>