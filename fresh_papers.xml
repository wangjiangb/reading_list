<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 02 Jun 2024 06:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</title><link>http://arxiv.org/abs/2405.20343v1</link><description>In this work, we introduce Unique3D, a novel image-to-3D framework forefficiently generating high-quality 3D meshes from single-view images,featuring state-of-the-art generation fidelity and strong generalizability.Previous methods based on Score Distillation Sampling (SDS) can producediversified 3D results by distilling 3D knowledge from large 2D diffusionmodels, but they usually suffer from long per-case optimization time withinconsistent issues. Recent works address the problem and generate better 3Dresults either by finetuning a multi-view diffusion model or training a fastfeed-forward model. However, they still lack intricate textures and complexgeometries due to inconsistency and limited generated resolution. Tosimultaneously achieve high fidelity, consistency, and efficiency in singleimage-to-3D, we propose a novel framework Unique3D that includes a multi-viewdiffusion model with a corresponding normal diffusion model to generatemulti-view images with their normal maps, a multi-level upscale process toprogressively improve the resolution of generated orthographic multi-views, aswell as an instant and consistent mesh reconstruction algorithm called ISOMER,which fully integrates the color and geometric priors into mesh results.Extensive experiments demonstrate that our Unique3D significantly outperformsother image-to-3D baselines in terms of geometric and textural details.</description><author>Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, Kaisheng Ma</author><pubDate>Thu, 30 May 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20343v1</guid></item><item><title>From Zero to Hero: Cold-Start Anomaly Detection</title><link>http://arxiv.org/abs/2405.20341v1</link><description>When first deploying an anomaly detection system, e.g., to detectout-of-scope queries in chatbots, there are no observed data, makingdata-driven approaches ineffective. Zero-shot anomaly detection methods offer asolution to such "cold-start" cases, but unfortunately they are often notaccurate enough. This paper studies the realistic but underexplored cold-startsetting where an anomaly detection model is initialized using zero-shotguidance, but subsequently receives a small number of contaminated observations(namely, that may include anomalies). The goal is to make efficient use of boththe zero-shot guidance and the observations. We propose ColdFusion, a methodthat effectively adapts the zero-shot anomaly detector to contaminatedobservations. To support future development of this new setting, we propose anevaluation suite consisting of evaluation protocols and metrics.</description><author>Tal Reiss, George Kour, Naama Zwerdling, Ateret Anaby-Tavor, Yedid Hoshen</author><pubDate>Thu, 30 May 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20341v1</guid></item><item><title>MotionLLM: Understanding Human Behaviors from Human Motions and Videos</title><link>http://arxiv.org/abs/2405.20340v1</link><description>This study delves into the realm of multi-modality (i.e., video and motionmodalities) human behavior understanding by leveraging the powerfulcapabilities of Large Language Models (LLMs). Diverging from recent LLMsdesigned for video-only or motion-only understanding, we argue thatunderstanding human behavior necessitates joint modeling from both videos andmotion sequences (e.g., SMPL sequences) to capture nuanced body part dynamicsand semantics effectively. In light of this, we present MotionLLM, astraightforward yet effective framework for human motion understanding,captioning, and reasoning. Specifically, MotionLLM adopts a unifiedvideo-motion training strategy that leverages the complementary advantages ofexisting coarse video-text data and fine-grained motion-text data to glean richspatial-temporal insights. Furthermore, we collect a substantial dataset,MoVid, comprising diverse videos, motions, captions, and instructions.Additionally, we propose the MoVid-Bench, with carefully manual annotations,for better evaluation of human behavior understanding on video and motion.Extensive experiments show the superiority of MotionLLM in the caption,spatial-temporal comprehension, and reasoning ability.</description><author>Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, Lei Zhang</author><pubDate>Thu, 30 May 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20340v1</guid></item><item><title>Visual Perception by Large Language Model's Weights</title><link>http://arxiv.org/abs/2405.20339v1</link><description>Existing Multimodal Large Language Models (MLLMs) follow the paradigm thatperceives visual information by aligning visual features with the input spaceof Large Language Models (LLMs), and concatenating visual tokens with texttokens to form a unified sequence input for LLMs. These methods demonstratepromising results on various vision-language tasks but are limited by the highcomputational effort due to the extended input sequence resulting from theinvolvement of visual tokens. In this paper, instead of input space alignment,we propose a novel parameter space alignment paradigm that represents visualinformation as model weights. For each input image, we use a vision encoder toextract visual features, convert features into perceptual weights, and mergethe perceptual weights with LLM's weights. In this way, the input of LLM doesnot require visual tokens, which reduces the length of the input sequence andgreatly improves efficiency. Following this paradigm, we propose VLoRA with theperceptual weights generator. The perceptual weights generator is designed toconvert visual features to perceptual weights with low-rank property,exhibiting a form similar to LoRA. The experimental results show that our VLoRAachieves comparable performance on various benchmarks for MLLMs, whilesignificantly reducing the computational costs for both training and inference.The code and models will be made open-source.</description><author>Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, Xiaoyan Sun</author><pubDate>Thu, 30 May 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20339v1</guid></item><item><title>OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving</title><link>http://arxiv.org/abs/2405.20337v1</link><description>Understanding the evolution of 3D scenes is important for effectiveautonomous driving. While conventional methods mode scene development with themotion of individual instances, world models emerge as a generative frameworkto describe the general scene dynamics. However, most existing methods adopt anautoregressive framework to perform next-token prediction, which suffer frominefficiency in modeling long-term temporal evolutions. To address this, wepropose a diffusion-based 4D occupancy generation model, OccSora, to simulatethe development of the 3D world for autonomous driving. We employ a 4D scenetokenizer to obtain compact discrete spatial-temporal representations for 4Doccupancy input and achieve high-quality reconstruction for long-sequenceoccupancy videos. We then learn a diffusion transformer on the spatial-temporalrepresentations and generate 4D occupancy conditioned on a trajectory prompt.We conduct extensive experiments on the widely used nuScenes dataset with Occ3Doccupancy annotations. OccSora can generate 16s-videos with authentic 3D layoutand temporal consistency, demonstrating its ability to understand the spatialand temporal distributions of driving scenes. With trajectory-aware 4Dgeneration, OccSora has the potential to serve as a world simulator for thedecision-making of autonomous driving. Code is available at:https://github.com/wzzheng/OccSora.</description><author>Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, Jiwen Lu</author><pubDate>Thu, 30 May 2024 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20337v1</guid></item><item><title>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</title><link>http://arxiv.org/abs/2405.20336v1</link><description>In this work, we introduce a challenging task for simultaneously generating3D holistic body motions and singing vocals directly from textual lyricsinputs, advancing beyond existing works that typically address these twomodalities in isolation. To facilitate this, we first collect the RapVersedataset, a large dataset containing synchronous rapping vocals, lyrics, andhigh-quality 3D holistic body meshes. With the RapVerse dataset, we investigatethe extent to which scaling autoregressive multimodal transformers acrosslanguage, audio, and motion can enhance the coherent and realistic generationof vocals and whole-body human motions. For modality unification, avector-quantized variational autoencoder is employed to encode whole-bodymotion sequences into discrete motion tokens, while a vocal-to-unit model isleveraged to obtain quantized audio tokens preserving content, prosodicinformation, and singer identity. By jointly performing transformer modeling onthese three modalities in a unified way, our framework ensures a seamless andrealistic blend of vocals and human motions. Extensive experiments demonstratethat our unified generation framework not only produces coherent and realisticsinging vocals alongside human motions directly from textual inputs but alsorivals the performance of specialized single-modality generation systems,establishing new benchmarks for joint vocal-motion generation. The project pageis available for research purposes at https://vis-www.cs.umass.edu/RapVerse.</description><author>Jiaben Chen, Xin Yan, Yihang Chen, Siyuan Cen, Qinwei Ma, Haoyu Zhen, Kaizhi Qian, Lie Lu, Chuang Gan</author><pubDate>Thu, 30 May 2024 18:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20336v1</guid></item><item><title>Xwin-LM: Strong and Scalable Alignment Practice for LLMs</title><link>http://arxiv.org/abs/2405.20335v1</link><description>In this work, we present Xwin-LM, a comprehensive suite of alignmentmethodologies for large language models (LLMs). This suite encompasses severalkey techniques, including supervised finetuning (SFT), reward modeling (RM),rejection sampling finetuning (RS), and direct preference optimization (DPO).The key components are as follows: (1) Xwin-LM-SFT, models initially finetunedwith high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turnpreference dataset meticulously annotated using GPT-4; (3) Xwin-RM, rewardmodels trained on Xwin-Pair, developed at scales of 7B, 13B, and 70Bparameters; (4) Xwin-Set, a multiwise preference dataset in which each promptis linked to 64 unique responses generated by Xwin-LM-SFT and scored byXwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responsesfrom Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using theDPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrateconsistent and significant improvements across the pipeline, demonstrating thestrength and scalability of Xwin-LM. The repositoryhttps://github.com/Xwin-LM/Xwin-LM will be continually updated to fostercommunity research.</description><author>Bolin Ni, JingCheng Hu, Yixuan Wei, Houwen Peng, Zheng Zhang, Gaofeng Meng, Han Hu</author><pubDate>Thu, 30 May 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20335v1</guid></item><item><title>VividDream: Generating 3D Scene with Ambient Dynamics</title><link>http://arxiv.org/abs/2405.20334v1</link><description>We introduce VividDream, a method for generating explorable 4D scenes withambient dynamics from a single input image or text prompt. VividDream firstexpands an input image into a static 3D point cloud through iterativeinpainting and geometry merging. An ensemble of animated videos is thengenerated using video diffusion models with quality refinement techniques andconditioned on renderings of the static 3D scene from the sampled cameratrajectories. We then optimize a canonical 4D scene representation using ananimated video ensemble, with per-video motion embeddings and visibility masksto mitigate inconsistencies. The resulting 4D scene enables free-viewexploration of a 3D scene with plausible ambient scene dynamics. Experimentsdemonstrate that VividDream can provide human viewers with compelling 4Dexperiences generated based on diverse real images and text prompts.</description><author>Yao-Chih Lee, Yi-Ting Chen, Andrew Wang, Ting-Hsuan Liao, Brandon Y. Feng, Jia-Bin Huang</author><pubDate>Thu, 30 May 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20334v1</guid></item><item><title>SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical Videos</title><link>http://arxiv.org/abs/2405.20333v1</link><description>Accurate tool tracking is essential for the success of computer-assistedintervention. Previous efforts often modeled tool trajectories rigidly,overlooking the dynamic nature of surgical procedures, especially trackingscenarios like out-of-body and out-of-camera views. Addressing this limitation,the new CholecTrack20 dataset provides detailed labels that account formultiple tool trajectories in three perspectives: (1) intraoperative, (2)intracorporeal, and (3) visibility, representing the different types oftemporal duration of tool tracks. These fine-grained labels enhance trackingflexibility but also increase the task complexity. Re-identifying tools afterocclusion or re-insertion into the body remains challenging due to high visualsimilarity, especially among tools of the same category. This work recognizesthe critical role of the tool operators in distinguishing tool track instances,especially those belonging to the same tool category. The operators'information are however not explicitly captured in surgical videos. Wetherefore propose SurgiTrack, a novel deep learning method that leveragesYOLOv7 for precise tool detection and employs an attention mechanism to modelthe originating direction of the tools, as a proxy to their operators, for toolre-identification. To handle diverse tool trajectory perspectives, SurgiTrackemploys a harmonizing bipartite matching graph, minimizing conflicts andensuring accurate tool identity association. Experimental results onCholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselinesand state-of-the-art methods with real-time inference capability. This worksets a new standard in surgical tool tracking, providing dynamic trajectoriesfor more adaptable and precise assistance in minimally invasive surgeries.</description><author>Chinedu Innocent Nwoye, Nicolas Padoy</author><pubDate>Thu, 30 May 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20333v1</guid></item><item><title>CoSy: Evaluating Textual Explanations of Neurons</title><link>http://arxiv.org/abs/2405.20331v1</link><description>A crucial aspect of understanding the complex nature of Deep Neural Networks(DNNs) is the ability to explain learned concepts within their latentrepresentations. While various methods exist to connect neurons to textualdescriptions of human-understandable concepts, evaluating the quality of theseexplanation methods presents a major challenge in the field due to a lack ofunified, general-purpose quantitative evaluation. In this work, we introduceCoSy (Concept Synthesis) -- a novel, architecture-agnostic framework toevaluate the quality of textual explanations for latent neurons. Given textualexplanations, our proposed framework leverages a generative model conditionedon textual input to create data points representing the textual explanation.Then, the neuron's response to these explanation data points is compared withthe response to control data points, providing a quality estimate of the givenexplanation. We ensure the reliability of our proposed framework in a series ofmeta-evaluation experiments and demonstrate practical value through insightsfrom benchmarking various concept-based textual explanation methods forComputer Vision tasks, showing that tested explanation methods significantlydiffer in quality.</description><author>Laura Kopf, Philine Lou Bommer, Anna Hedström, Sebastian Lapuschkin, Marina M. -C. Höhne, Kirill Bykov</author><pubDate>Thu, 30 May 2024 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20331v1</guid></item><item><title>4DHands: Reconstructing Interactive Hands in 4D with Transformers</title><link>http://arxiv.org/abs/2405.20330v1</link><description>In this paper, we introduce 4DHands, a robust approach to recoveringinteractive hand meshes and their relative movement from monocular inputs. Ourapproach addresses two major limitations of previous methods: lacking a unifiedsolution for handling various hand image inputs and neglecting the positionalrelationship of two hands within images. To overcome these challenges, wedevelop a transformer-based architecture with novel tokenization and featurefusion strategies. Specifically, we propose a Relation-aware Two-HandTokenization (RAT) method to embed positional relation information into thehand tokens. In this way, our network can handle both single-hand and two-handinputs and explicitly leverage relative hand positions, facilitating thereconstruction of intricate hand interactions in real-world scenarios. As suchtokenization indicates the relative relationship of two hands, it also supportsmore effective feature fusion. To this end, we further develop aSpatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4Dwith attention and decode them into 3D hand meshes and relative temporalmovements. The efficacy of our approach is validated on several benchmarkdatasets. The results on in-the-wild videos and real-world scenariosdemonstrate the superior performances of our approach for interactive handreconstruction. More video results can be found on the project page:https://4dhands.github.io.</description><author>Dixuan Lin, Yuxiang Zhang, Mengcheng Li, Yebin Liu, Wei Jing, Qi Yan, Qianying Wang, Hongwen Zhang</author><pubDate>Thu, 30 May 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20330v1</guid></item><item><title>GECO: Generative Image-to-3D within a SECOnd</title><link>http://arxiv.org/abs/2405.20327v1</link><description>3D generation has seen remarkable progress in recent years. Existingtechniques, such as score distillation methods, produce notable results butrequire extensive per-scene optimization, impacting time efficiency.Alternatively, reconstruction-based approaches prioritize efficiency butcompromise quality due to their limited handling of uncertainty. We introduceGECO, a novel method for high-quality 3D generative modeling that operateswithin a second. Our approach addresses the prevalent issues of uncertainty andinefficiency in current methods through a two-stage approach. In the initialstage, we train a single-step multi-view generative model with scoredistillation. Then, a second-stage distillation is applied to address thechallenge of view inconsistency from the multi-view prediction. This two-stageprocess ensures a balanced approach to 3D generation, optimizing both qualityand efficiency. Our comprehensive experiments demonstrate that GECO achieveshigh-quality image-to-3D generation with an unprecedented level of efficiency.</description><author>Chen Wang, Jiatao Gu, Xiaoxiao Long, Yuan Liu, Lingjie Liu</author><pubDate>Thu, 30 May 2024 18:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20327v1</guid></item><item><title>MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion</title><link>http://arxiv.org/abs/2405.20325v1</link><description>Despite impressive advancements in diffusion-based video editing models inaltering video attributes, there has been limited exploration into modifyingmotion information while preserving the original protagonist's appearance andbackground. In this paper, we propose MotionFollower, a lightweightscore-guided diffusion model for video motion editing. To introduce conditionalcontrols to the denoising process, MotionFollower leverages two of our proposedlightweight signal controllers, one for poses and the other for appearances,both of which consist of convolution blocks without involving heavy attentioncalculations. Further, we design a score guidance principle based on atwo-branch architecture, including the reconstruction and editing branches,which significantly enhance the modeling capability of texture details andcomplicated backgrounds. Concretely, we enforce several consistencyregularizers and losses during the score estimation. The resulting gradientsthus inject appropriate guidance to the intermediate latents, forcing the modelto preserve the original background details and protagonists' appearanceswithout interfering with the motion modification. Experiments demonstrate thecompetitive motion editing ability of MotionFollower qualitatively andquantitatively. Compared with MotionEditor, the most advanced motion editingmodel, MotionFollower achieves an approximately 80% reduction in GPU memorywhile delivering superior motion editing performance and exclusively supportinglarge camera movements and actions.</description><author>Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Thu, 30 May 2024 18:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20325v1</guid></item><item><title>Don't drop your samples! Coherence-aware training benefits Conditional diffusion</title><link>http://arxiv.org/abs/2405.20324v1</link><description>Conditional diffusion models are powerful generative models that can leveragevarious types of conditional information, such as class labels, segmentationmasks, or text captions. However, in many real-world scenarios, conditionalinformation may be noisy or unreliable due to human annotation errors or weakalignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), anovel method that integrates coherence in conditional information intodiffusion models, allowing them to learn from noisy annotations withoutdiscarding data. We assume that each data point has an associated coherencescore that reflects the quality of the conditional information. We thencondition the diffusion model on both the conditional information and thecoherence score. In this way, the model learns to ignore or discount theconditioning when the coherence is low. We show that CAD is theoretically soundand empirically effective on various conditional generation tasks. Moreover, weshow that leveraging coherence generates realistic and diverse samples thatrespect conditional information better than models trained on cleaned datasetswhere samples with low coherence have been discarded.</description><author>Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, David Picard</author><pubDate>Thu, 30 May 2024 18:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20324v1</guid></item><item><title>$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving</title><link>http://arxiv.org/abs/2405.20323v1</link><description>Photorealistic 3D reconstruction of street scenes is a critical technique fordeveloping real-world simulators for autonomous driving. Despite the efficacyof Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting(3DGS) emerges as a promising direction due to its faster speed and moreexplicit representation. However, most existing street 3DGS methods requiretracked 3D vehicle bounding boxes to decompose the static and dynamic elementsfor effective reconstruction, limiting their applications for in-the-wildscenarios. To facilitate efficient 3D scene reconstruction without costlyannotations, we propose a self-supervised street Gaussian($\textit{S}^3$Gaussian) method to decompose dynamic and static elements from4D consistency. We represent each scene with 3D Gaussians to preserve theexplicitness and further accompany them with a spatial-temporal field networkto compactly model the 4D dynamics. We conduct extensive experiments on thechallenging Waymo-Open dataset to evaluate the effectiveness of our method. Our$\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamicscenes and achieves the best performance without using 3D annotations. Code isavailable at: https://github.com/nnanhuang/S3Gaussian/.</description><author>Nan Huang, Xiaobao Wei, Wenzhao Zheng, Pengju An, Ming Lu, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Shanghang Zhang</author><pubDate>Thu, 30 May 2024 18:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20323v1</guid></item><item><title>Vision-based Manipulation from Single Human Video with Open-World Object Graphs</title><link>http://arxiv.org/abs/2405.20321v1</link><description>We present an object-centric approach to empower robots to learn vision-basedmanipulation skills from human videos. We investigate the problem of imitatingrobot manipulation from a single human video in the open-world setting, where arobot must learn to manipulate novel objects from one video demonstration. Weintroduce ORION, an algorithm that tackles the problem by extracting anobject-centric manipulation plan from a single RGB-D video and deriving apolicy that conditions on the extracted plan. Our method enables the robot tolearn from videos captured by daily mobile devices such as an iPad andgeneralize the policies to deployment environments with varying visualbackgrounds, camera angles, spatial layouts, and novel object instances. Wesystematically evaluate our method on both short-horizon and long-horizontasks, demonstrating the efficacy of ORION in learning from a single humanvideo in the open world. Videos can be found in the project websitehttps://ut-austin-rpl.github.io/ORION-release.</description><author>Yifeng Zhu, Arisrei Lim, Peter Stone, Yuke Zhu</author><pubDate>Thu, 30 May 2024 18:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20321v1</guid></item><item><title>Improving the Training of Rectified Flows</title><link>http://arxiv.org/abs/2405.20320v1</link><description>Diffusion models have shown great promise for image and video generation, butsampling from state-of-the-art models requires expensive numerical integrationof a generative ODE. One approach for tackling this problem is rectified flows,which iteratively learn smooth ODE paths that are less susceptible totruncation error. However, rectified flows still require a relatively largenumber of function evaluations (NFEs). In this work, we propose improvedtechniques for training rectified flows, allowing them to compete withknowledge distillation methods even in the low NFE setting. Our main insight isthat under realistic settings, a single iteration of the Reflow algorithm fortraining rectified flows is sufficient to learn nearly straight trajectories;hence, the current practice of using multiple Reflow iterations is unnecessary.We thus propose techniques to improve one-round training of rectified flows,including a U-shaped timestep distribution and LPIPS-Huber premetric. Withthese techniques, we improve the FID of the previous 2-rectified flow by up to72% in the 1 NFE setting on CIFAR-10. On ImageNet 64$\times$64, our improvedrectified flow outperforms the state-of-the-art distillation methods such asconsistency distillation and progressive distillation in both one-step andtwo-step settings and rivals the performance of improved consistency training(iCT) in FID. Code is available at https://github.com/sangyun884/rfpp.</description><author>Sangyun Lee, Zinan Lin, Giulia Fanti</author><pubDate>Thu, 30 May 2024 18:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20320v1</guid></item><item><title>ParSEL: Parameterized Shape Editing with Language</title><link>http://arxiv.org/abs/2405.20319v1</link><description>The ability to edit 3D assets from natural language presents a compellingparadigm to aid in the democratization of 3D content creation. However, whilenatural language is often effective at communicating general intent, it ispoorly suited for specifying precise manipulation. To address this gap, weintroduce ParSEL, a system that enables controllable editing of high-quality 3Dassets from natural language. Given a segmented 3D mesh and an editing request,ParSEL produces a parameterized editing program. Adjusting the programparameters allows users to explore shape variations with a precise control overthe magnitudes of edits. To infer editing programs which align with an inputedit request, we leverage the abilities of large-language models (LLMs).However, while we find that LLMs excel at identifying initial edit operations,they often fail to infer complete editing programs, and produce outputs thatviolate shape semantics. To overcome this issue, we introduce Analytical EditPropagation (AEP), an algorithm which extends a seed edit with additionaloperations until a complete editing program has been formed. Unlike priormethods, AEP searches for analytical editing operations compatible with a rangeof possible user edits through the integration of computer algebra systems forgeometric analysis. Experimentally we demonstrate ParSEL's effectiveness inenabling controllable editing of 3D objects through natural language requestsover alternative system designs.</description><author>Aditya Ganeshan, Ryan Y. Huang, Xianghao Xu, R. Kenny Jones, Daniel Ritchie</author><pubDate>Thu, 30 May 2024 18:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20319v1</guid></item><item><title>CausalQuest: Collecting Natural Causal Questions for AI Agents</title><link>http://arxiv.org/abs/2405.20318v1</link><description>Humans have an innate drive to seek out causality. Whether fuelled bycuriosity or specific goals, we constantly question why things happen, how theyare interconnected, and many other related phenomena. To develop AI agentscapable of addressing this natural human quest for causality, we urgently needa comprehensive dataset of natural causal questions. Unfortunately, existingdatasets either contain only artificially-crafted questions that do not reflectreal AI usage scenarios or have limited coverage of questions from specificsources. To address this gap, we present CausalQuest, a dataset of 13,500naturally occurring questions sourced from social networks, search engines, andAI assistants. We formalize the definition of causal questions and establish ataxonomy for finer-grained classification. Through a combined effort of humanannotators and large language models (LLMs), we carefully label the dataset. Wefind that 42% of the questions humans ask are indeed causal, with the majorityseeking to understand the causes behind given effects. Using this dataset, wetrain efficient classifiers (up to 2.85B parameters) for the binary task ofidentifying causal questions, achieving high performance with F1 scores of upto 0.877. We conclude with a rich set of future research directions that canbuild upon our data and models.</description><author>Roberto Ceraolo, Dmitrii Kharlapenko, Amélie Reymond, Rada Mihalcea, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin</author><pubDate>Thu, 30 May 2024 18:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20318v1</guid></item><item><title>Recurrent Drafter for Fast Speculative Decoding in Large Language Models</title><link>http://arxiv.org/abs/2403.09919v3</link><description>In this paper, we introduce an improved approach of speculative decodingaimed at enhancing the efficiency of serving large language models. Our methodcapitalizes on the strengths of two established techniques: the classictwo-model speculative decoding approach, and the more recent single-modelapproach, Medusa. Drawing inspiration from Medusa, our approach adopts asingle-model strategy for speculative decoding. However, our methoddistinguishes itself by employing a single, lightweight draft head with arecurrent dependency design, akin in essence to the small, draft model uses inclassic speculative decoding, but without the complexities of the fulltransformer architecture. And because of the recurrent dependency, we can usebeam search to swiftly filter out undesired candidates with the draft head. Theoutcome is a method that combines the simplicity of single-model design andavoids the need to create a data-dependent tree attention structure only forinference in Medusa. We empirically demonstrate the effectiveness of theproposed method on several popular open source language models, along with acomprehensive analysis of the trade-offs involved in adopting this approach.</description><author>Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng</author><pubDate>Thu, 30 May 2024 18:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09919v3</guid></item><item><title>ANAH: Analytical Annotation of Hallucinations in Large Language Models</title><link>http://arxiv.org/abs/2405.20315v1</link><description>Reducing the `$\textit{hallucination}$' problem of Large Language Models(LLMs) is crucial for their wide applications. A comprehensive and fine-grainedmeasurement of the hallucination is the first key step for the governance ofthis issue but is under-explored in the community. Thus, we present$\textbf{ANAH}$, a bilingual dataset that offers $\textbf{AN}$alytical$\textbf{A}$nnotation of $\textbf{H}$allucinations in LLMs within GenerativeQuestion Answering. Each answer sentence in our dataset undergoes rigorousannotation, involving the retrieval of a reference fragment, the judgment ofthe hallucination type, and the correction of hallucinated content. ANAHconsists of ~12k sentence-level annotations for ~4.3k LLM responses coveringover 700 topics, constructed by a human-in-the-loop pipeline. Thanks to thefine granularity of the hallucination annotations, we can quantitativelyconfirm that the hallucinations of LLMs progressively accumulate in the answerand use ANAH to train and evaluate hallucination annotators. We conductextensive experiments on studying generative and discriminative annotators andshow that, although current open-source LLMs have difficulties in fine-grainedhallucination annotation, the generative annotator trained with ANAH cansurpass all open-source LLMs and GPT-3.5, obtain performance competitive withGPT-4, and exhibits better generalization ability on unseen questions.</description><author>Ziwei Ji, Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen</author><pubDate>Thu, 30 May 2024 18:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20315v1</guid></item><item><title>S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs</title><link>http://arxiv.org/abs/2405.20314v1</link><description>Speculative decoding (SD) has attracted a significant amount of researchattention due to the substantial speedup it can achieve for LLM inference.However, despite the high speedups they offer, speculative decoding methodsoften achieve optimal performance on high-end devices or with a substantial GPUmemory overhead. Given limited memory and the necessity of quantization, ahigh-performing model on a high-end GPU can slow down by up to 7 times. To thisend, we propose Skippy Simultaneous Speculative Decoding (or S3D), acost-effective self-speculative SD method based on simultaneous multi-tokendecoding and mid-layer skipping. When compared against recent effectiveopen-source SD systems, our method has achieved one of the topperformance-memory ratios while requiring minimal architecture changes andtraining data. Leveraging our memory efficiency, we created a smaller yet moreeffective SD model based on Phi-3. It is 1.4 to 2 times faster than thequantized EAGLE model and operates in half-precision while using less VRAM.</description><author>Wei Zhong, Manasa Bharadwaj</author><pubDate>Thu, 30 May 2024 18:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20314v1</guid></item><item><title>Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss</title><link>http://arxiv.org/abs/2402.05928v2</link><description>In this work, we study statistical learning with dependent ($\beta$-mixing)data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p}\|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by thesearch for a sharp noise interaction term, or variance proxy, in learning withdependent data. Absent any realizability assumption, typical non-asymptoticresults exhibit variance proxies that are deflated multiplicatively by themixing time of the underlying covariates process. We show that whenever thetopologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class$\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class:$\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- theempirical risk minimizer achieves a rate that only depends on the complexity ofthe class and second order statistics in its leading term. Our result holdswhether the problem is realizable or not and we refer to this as a \emph{nearmixing-free rate}, since direct dependence on mixing is relegated to anadditive higher order term. We arrive at our result by combining the abovenotion of a weakly sub-Gaussian class with mixed tail generic chaining. Thiscombination allows us to compute sharp, instance-optimal rates for a wide rangeof problems. Examples that satisfy our framework include sub-Gaussian linearregression, more general smoothly parameterized function classes, finitehypothesis classes, and bounded smoothness classes.</description><author>Ingvar Ziemann, Stephen Tu, George J. Pappas, Nikolai Matni</author><pubDate>Thu, 30 May 2024 18:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05928v2</guid></item><item><title>Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation</title><link>http://arxiv.org/abs/2405.20313v1</link><description>Proteins are essential for almost all biological processes and derive theirdiverse functions from complex 3D structures, which are in turn determined bytheir amino acid sequences. In this paper, we exploit the rich biologicalinductive bias of amino acid sequences and introduce FoldFlow-2, a novelsequence-conditioned SE(3)-equivariant flow matching model for proteinstructure generation. FoldFlow-2 presents substantial new architecturalfeatures over the previous FoldFlow family of models including a protein largelanguage model to encode sequence, a new multi-modal fusion trunk that combinesstructure and sequence representations, and a geometric transformer baseddecoder. To increase diversity and novelty of generated samples -- crucial forde-novo drug design -- we train FoldFlow-2 at scale on a new dataset that is anorder of magnitude larger than PDB datasets of prior works, containing bothknown proteins in PDB and high-quality synthetic structures achieved throughfiltering. We further demonstrate the ability to align FoldFlow-2 to arbitraryrewards, e.g. increasing secondary structures diversity, by introducing aReinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow-2outperforms previous state-of-the-art protein structure-based generativemodels, improving over RFDiffusion in terms of unconditional generation acrossall metrics including designability, diversity, and novelty across all proteinlengths, as well as exhibiting generalization on the task of equilibriumconformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow-2makes progress on challenging conditional design tasks such as designingscaffolds for the VHH nanobody.</description><author>Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, Alexander Tong, Avishek Joey Bose</author><pubDate>Thu, 30 May 2024 18:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20313v1</guid></item><item><title>A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D Reconstruction</title><link>http://arxiv.org/abs/2405.20310v1</link><description>Learning 3D scene representation from a single-view image is a long-standingfundamental problem in computer vision, with the inherent ambiguity inpredicting contents unseen from the input view. Built on the recently proposed3D Gaussian Splatting (3DGS), the Splatter Image method has made promisingprogress on fast single-image novel view synthesis via learning a single 3DGaussian for each pixel based on the U-Net feature map of an input image.However, it has limited expressive power to represent occluded components thatare not observable in the input view. To address this problem, this paperpresents a Hierarchical Splatter Image method in which a pixel is worth morethan one 3D Gaussians. Specifically, each pixel is represented by a parent 3D Gaussian and a small number of child3D Gaussians. Parent 3D Gaussians are learned as done in the vanilla SplatterImage. Child 3D Gaussians are learned via a lightweight Multi-Layer Perceptron(MLP) which takes as input the projected image features of a parent 3D Gaussianand the embedding of a target camera view. Both parent and child 3D Gaussiansare learned end-to-end in a stage-wise way. The joint condition of input imagefeatures from eyes of the parent Gaussians and the target camera positionfacilitates learning to allocate child Gaussians to ``see the unseen'',recovering the occluded details that are often missed by parent Gaussians. In experiments, the proposed method is tested on the ShapeNet-SRN and CO3Ddatasets with state-of-the-art performance obtained, especially showingpromising capabilities of reconstructing occluded contents in the input view.</description><author>Jianghao Shen, Tianfu Wu</author><pubDate>Thu, 30 May 2024 18:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20310v1</guid></item><item><title>Large Language Models Can Self-Improve At Web Agent Tasks</title><link>http://arxiv.org/abs/2405.20309v1</link><description>Training models to act as agents that can effectively navigate and performactions in a complex environment, such as a web browser, has typically beenchallenging due to lack of training data. Large language models (LLMs) haverecently demonstrated some capability to navigate novel environments as agentsin a zero-shot or few-shot fashion, purely guided by natural languageinstructions as prompts. Recent research has also demonstrated LLMs have thecapability to exceed their base performance through self-improvement, i.e.fine-tuning on data generated by the model itself. In this work, we explore theextent to which LLMs can self-improve their performance as agents inlong-horizon tasks in a complex environment using the WebArena benchmark. InWebArena, an agent must autonomously navigate and perform actions on web pagesto achieve a specified objective. We explore fine-tuning on three distinctsynthetic training data mixtures and achieve a 31\% improvement in taskcompletion rate over the base model on the WebArena benchmark through aself-improvement procedure. We additionally contribute novel evaluation metricsfor assessing the performance, robustness, capabilities, and quality oftrajectories of our fine-tuned agent models to a greater degree than simple,aggregate-level benchmark scores currently used to measure self-improvement.</description><author>Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, Sepp Hochreiter</author><pubDate>Thu, 30 May 2024 18:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20309v1</guid></item><item><title>Contextual Position Encoding: Learning to Count What's Important</title><link>http://arxiv.org/abs/2405.18719v2</link><description>The attention mechanism is a critical component of Large Language Models(LLMs) that allows tokens in a sequence to interact with each other, but isorder-invariant. Incorporating position encoding (PE) makes it possible toaddress by position, such as attending to the i-th token. However, current PEmethods use token counts to derive position, and thus cannot generalize tohigher levels of abstraction, such as attending to the i-th sentence. In thispaper, we propose a new position encoding method, Contextual Position Encoding(CoPE), that allows positions to be conditioned on context by incrementingposition only on certain tokens determined by the model. This allows moregeneral position addressing such as attending to the $i$-th particular word,noun, or sentence. We show that CoPE can solve the selective copy, counting andFlip-Flop tasks where popular position embeddings fail, and improves perplexityon language modeling and coding tasks.</description><author>Olga Golovneva, Tianlu Wang, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Thu, 30 May 2024 18:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18719v2</guid></item><item><title>Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2209.15210v5</link><description>Most existing methods for unsupervised domain adaptation (UDA) rely on ashared network to extract domain-invariant features. However, when facingmultiple source domains, optimizing such a network involves updating theparameters of the entire network, making it both computationally expensive andchallenging, particularly when coupled with min-max objectives. Inspired byrecent advances in prompt learning that adapts high-capacity models fordownstream tasks in a computationally economic way, we introduce Multi-PromptAlignment (MPA), a simple yet efficient framework for multi-source UDA. Given asource and target domain pair, MPA first trains an individual prompt tominimize the domain gap through a contrastive loss. Then, MPA denoises thelearned prompts through an auto-encoding process and aligns them by maximizingthe agreement of all the reconstructed prompts. Moreover, we show that theresulting subspace acquired from the auto-encoding process can easilygeneralize to a streamlined set of target domains, making our method moreefficient for practical usage. Extensive experiments show that MPA achievesstate-of-the-art results on three popular datasets with an impressive averageaccuracy of 54.1% on DomainNet.</description><author>Haoran Chen, Xintong Han, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Thu, 30 May 2024 18:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.15210v5</guid></item><item><title>Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models</title><link>http://arxiv.org/abs/2405.20305v1</link><description>We introduce PlausiVL, a large video-language model for anticipating actionsequences that are plausible in the real-world. While significant efforts havebeen made towards anticipating future actions, prior approaches do not takeinto account the aspect of plausibility in an action sequence. To address thislimitation, we explore the generative capability of a large video-languagemodel in our work and further, develop the understanding of plausibility in anaction sequence by introducing two objective functions, a counterfactual-basedplausible action sequence learning loss and a long-horizon action repetitionloss. We utilize temporal logical constraints as well as verb-noun action pairlogical constraints to create implausible/counterfactual action sequences anduse them to train the model with plausible action sequence learning loss. Thisloss helps the model to differentiate between plausible and not plausibleaction sequences and also helps the model to learn implicit temporal cuescrucial for the task of action anticipation. The long-horizon action repetitionloss puts a higher penalty on the actions that are more prone to repetitionover a longer temporal window. With this penalization, the model is able togenerate diverse, plausible action sequences. We evaluate our approach on twolarge-scale datasets, Ego4D and EPIC-Kitchens-100, and show improvements on thetask of action anticipation.</description><author>Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee</author><pubDate>Thu, 30 May 2024 18:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20305v1</guid></item><item><title>Group Robust Preference Optimization in Reward-free RLHF</title><link>http://arxiv.org/abs/2405.20304v1</link><description>Adapting large language models (LLMs) for specific tasks usually involvesfine-tuning through reinforcement learning with human feedback (RLHF) onpreference data. While these data often come from diverse labelers' groups(e.g., different demographics, ethnicities, company teams, etc.), traditionalRLHF approaches adopt a "one-size-fits-all" approach, i.e., theyindiscriminately assume and optimize a single preference model, thus not beingrobust to unique characteristics and needs of the various groups. To addressthis limitation, we propose a novel Group Robust Preference Optimization (GRPO)method to align LLMs to individual groups' preferences robustly. Our approachbuilds upon reward-free direct preference optimization methods, but unlikeprevious approaches, it seeks a robust policy which maximizes the worst-casegroup performance. To achieve this, GRPO adaptively and sequentially weightsthe importance of different groups, prioritizing groups with worse cumulativeloss. We theoretically study the feasibility of GRPO and analyze itsconvergence for the log-linear policy class. By fine-tuning LLMs with GRPOusing diverse group-based global opinion data, we significantly improvedperformance for the worst-performing groups, reduced loss imbalances acrossgroups, and improved probability accuracies compared to non-robust baselines.</description><author>Shyam Sundhar Ramesh, Yifan Hu, Iason Chaimalas, Viraj Mehta, Pier Giuseppe Sessa, Haitham Bou Ammar, Ilija Bogunovic</author><pubDate>Thu, 30 May 2024 18:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20304v1</guid></item><item><title>Scaling White-Box Transformers for Vision</title><link>http://arxiv.org/abs/2405.20299v1</link><description>CRATE, a white-box transformer architecture designed to learn compressed andsparse representations, offers an intriguing alternative to standard visiontransformers (ViTs) due to its inherent mathematical interpretability. Despiteextensive investigations into the scaling behaviors of language and visiontransformers, the scalability of CRATE remains an open question which thispaper aims to address. Specifically, we propose CRATE-$\alpha$, featuringstrategic yet minimal modifications to the sparse coding block in the CRATEarchitecture design, and a light training recipe designed to improve thescalability of CRATE. Through extensive experiments, we demonstrate thatCRATE-$\alpha$ can effectively scale with larger model sizes and datasets. Forexample, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-Bmodel accuracy on ImageNet classification by 3.7%, achieving an accuracy of83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains anImageNet classification accuracy of 85.1%. More notably, these modelperformance improvements are achieved while preserving, and potentially evenenhancing the interpretability of learned CRATE models, as we demonstratethrough showing that the learned token representations of increasingly largertrained CRATE-$\alpha$ models yield increasingly higher-quality unsupervisedobject segmentation of images. The project page ishttps://rayjryang.github.io/CRATE-alpha/.</description><author>Jinrui Yang, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, Cihang Xie</author><pubDate>Thu, 30 May 2024 18:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20299v1</guid></item><item><title>You Need to Pay Better Attention: Rethinking the Mathematics of Attention Mechanism</title><link>http://arxiv.org/abs/2403.01643v2</link><description>Scaled Dot Product Attention (SDPA) is the backbone of many moderndeep-learning models. It is so versatile that it has been used in naturallanguage, vision, and multi-modal domains with very little change compared toits original formulation. This paper discusses why the current formulation isinefficient by delving into the mathematical details of the attentionmechanism. We propose three improvements to mitigate these inefficiencies,thereby, introducing three enhanced attention mechanisms: Optimised, Efficient,and Super Attention. Optimised and Efficient Attention have one and two matrixmultiplications fewer per head, respectively, and 25% and 50% fewer parameters,respectively, than standard SDPA, but perform similarly to standard SDPA inboth vision and natural language tasks. They can be used in all applicationswhere SDPA is used while offering smaller model sizes and faster training andinference without noticeable loss in performance. Super Attention introduces anew linear transformation on the values, transforming them from the left. Itoutperforms standard SPDA on vision and natural language tasks by up to 17%while having one fewer matrix multiplication per head and 25% fewer parametersthan standard SDPA. Consequently, it is also faster than standard SDPA. SuperAttention is ideal in applications where the attention layer's context lengthis fixed, such as Vision Transformers. In addition to providing mathematicalreasoning, we evaluate the presented attention mechanisms on several datasetsincluding MNIST, CIFAR100, ImageNet, IMDB Movie Reviews, and Amazon Reviewsdatasets, as well as combined Europarl and Anki English-Spanish datasets forneural machine translation.</description><author>Mehran Hosseini, Peyman Hosseini</author><pubDate>Thu, 30 May 2024 18:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01643v2</guid></item><item><title>Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness</title><link>http://arxiv.org/abs/2405.20291v1</link><description>The security threat of backdoor attacks is a central concern for deep neuralnetworks (DNNs). Recently, without poisoned data, unlearning models with cleandata and then learning a pruning mask have contributed to backdoor defense.Additionally, vanilla fine-tuning with those clean data can help recover thelost clean accuracy. However, the behavior of clean unlearning is stillunder-explored, and vanilla fine-tuning unintentionally induces back thebackdoor effect. In this work, we first investigate model unlearning from theperspective of weight changes and gradient norms, and find two interestingobservations in the backdoored model: 1) the weight changes between poison andclean unlearning are positively correlated, making it possible for us toidentify the backdoored-related neurons without using poisoned data; 2) theneurons of the backdoored model are more active (i.e., larger changes ingradient norm) than those in the clean model, suggesting the need to suppressthe gradient norm during fine-tuning. Then, we propose an effective two-stagedefense method. In the first stage, an efficient Neuron Weight Change(NWC)-based Backdoor Reinitialization is proposed based on observation 1). Inthe second stage, based on observation 2), we design an Activeness-AwareFine-Tuning to replace the vanilla fine-tuning. Extensive experiments,involving eight backdoor attacks on three benchmark datasets, demonstrate thesuperior performance of our proposed method compared to recent state-of-the-artbackdoor defense approaches.</description><author>Weilin Lin, Li Liu, Shaokui Wei, Jianze Li, Hui Xiong</author><pubDate>Thu, 30 May 2024 18:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20291v1</guid></item><item><title>DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation</title><link>http://arxiv.org/abs/2405.20289v1</link><description>Controllable music generation methods are critical for human-centeredAI-based music creation, but are currently limited by speed, quality, andcontrol design trade-offs. Diffusion Inference-Time T-optimization (DITTO), inparticular, offers state-of-the-art results, but is over 10x slower thanreal-time, limiting practical use. We propose Distilled DiffusionInference-Time T -Optimization (or DITTO-2), a new method to speed upinference-time optimization-based control and unlock faster-than-real-timegeneration for a wide-variety of applications such as music inpainting,outpainting, intensity, melody, and musical structure control. Our method worksby (1) distilling a pre-trained diffusion model for fast sampling via anefficient, modified consistency or consistency trajectory distillation process(2) performing inference-time optimization using our distilled model withone-step sampling as an efficient surrogate optimization task and (3) running afinal multi-step sampling generation (decoding) using our estimated noiselatents for best-quality, fast, controllable generation. Through thoroughevaluation, we find our method not only speeds up generation over 10-20x, butsimultaneously improves control adherence and generation quality all at once.Furthermore, we apply our approach to a new application of maximizing textadherence (CLAP score) and show we can convert an unconditional diffusion modelwithout text inputs into a model that yields state-of-the-art text control.Sound examples can be found at https://ditto-music.github.io/ditto2/.</description><author>Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas Bryan</author><pubDate>Thu, 30 May 2024 18:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20289v1</guid></item><item><title>Flexible SE(2) graph neural networks with applications to PDE surrogates</title><link>http://arxiv.org/abs/2405.20287v1</link><description>This paper presents a novel approach for constructing graph neural networksequivariant to 2D rotations and translations and leveraging them as PDEsurrogates on non-gridded domains. We show that aligning the representationswith the principal axis allows us to sidestep many constraints while preservingSE(2) equivariance. By applying our model as a surrogate for fluid flowsimulations and conducting thorough benchmarks against non-equivariant models,we demonstrate significant gains in terms of both data efficiency and accuracy.</description><author>Maria Bånkestad, Olof Mogren, Aleksis Pirinen</author><pubDate>Thu, 30 May 2024 18:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20287v1</guid></item><item><title>Who Writes the Review, Human or AI?</title><link>http://arxiv.org/abs/2405.20285v1</link><description>With the increasing use of Artificial Intelligence in Natural LanguageProcessing, concerns have been raised regarding the detection of AI-generatedtext in various domains. This study aims to investigate this issue by proposinga methodology to accurately distinguish AI-generated and human-written bookreviews. Our approach utilizes transfer learning, enabling the model toidentify generated text across different topics while improving its ability todetect variations in writing style and vocabulary. To evaluate theeffectiveness of the proposed methodology, we developed a dataset consisting ofreal book reviews and AI-generated reviews using the recently proposed Vicunaopen-source language model. The experimental results demonstrate that it isfeasible to detect the original source of text, achieving an accuracy rate of96.86%. Our efforts are oriented toward the exploration of the capabilities andlimitations of Large Language Models in the context of text identification.Expanding our knowledge in these aspects will be valuable for effectivelynavigating similar models in the future and ensuring the integrity andauthenticity of human-generated content.</description><author>Panagiotis C. Theocharopoulos, Spiros V. Georgakopoulos, Sotiris K. Tasoulis, Vassilis P. Plagianakos</author><pubDate>Thu, 30 May 2024 18:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20285v1</guid></item><item><title>From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models</title><link>http://arxiv.org/abs/2403.03893v3</link><description>To date, toxicity mitigation in language models has almost entirely beenfocused on single-language settings. As language models embrace multilingualcapabilities, it's crucial our safety measures keep pace. Recognizing thisresearch gap, our approach expands the scope of conventional toxicitymitigation to address the complexities presented by multiple languages. In theabsence of sufficient annotated datasets across languages, we employ translateddata to evaluate and enhance our mitigation techniques. We also comparefinetuning mitigation approaches against retrieval-augmented techniques underboth static and continual toxicity mitigation scenarios. This allows us toexamine the effects of translation quality and the cross-lingual transfer ontoxicity mitigation. We also explore how model size and data quantity affectthe success of these mitigation efforts. Covering nine languages, our studyrepresents a broad array of linguistic families and levels of resourceavailability, ranging from high to mid-resource languages. Throughcomprehensive experiments, we provide insights into the complexities ofmultilingual toxicity mitigation, offering valuable insights and paving the wayfor future research in this increasingly important field. Code and data areavailable at https://github.com/for-ai/goodtriever.</description><author>Luiza Pozzobon, Patrick Lewis, Sara Hooker, Beyza Ermis</author><pubDate>Thu, 30 May 2024 18:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03893v3</guid></item><item><title>Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains</title><link>http://arxiv.org/abs/2402.05140v2</link><description>Large Language Models (LLMs) have demonstrated remarkable proficiency inunderstanding and generating natural language. However, their capabilities wanein highly specialized domains underrepresented in the pretraining corpus, suchas physical and biomedical sciences. This work explores how to repurposegeneral LLMs into effective task solvers for specialized domains. We introducea novel, model-agnostic framework for learning custom input tags, which areparameterized as continuous vectors appended to the LLM's embedding layer, tocondition the LLM. We design two types of input tags: domain tags are used todelimit specialized representations (e.g., chemical formulas) and providedomain-relevant context; function tags are used to represent specific functions(e.g., predicting molecular properties) and compress function-solvinginstructions. We develop a three-stage protocol to learn these tags usingauxiliary data and domain knowledge. By explicitly disentangling task domainsfrom task functions, our method enables zero-shot generalization to unseenproblems through diverse combinations of the input tags. It also boosts LLM'sperformance in various specialized domains, such as predicting protein orchemical properties and modeling drug-target interactions, outperforming expertmodels tailored to these tasks.</description><author>Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, Nicolo Fusi</author><pubDate>Thu, 30 May 2024 18:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05140v2</guid></item><item><title>TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes</title><link>http://arxiv.org/abs/2405.20283v1</link><description>We present TetSphere splatting, an explicit, Lagrangian representation forreconstructing 3D shapes with high-quality geometry. In contrast toconventional object reconstruction methods which predominantly use Eulerianrepresentations, including both neural implicit (e.g., NeRF, NeuS) and explicitrepresentations (e.g., DMTet), and often struggle with high computationaldemands and suboptimal mesh quality, TetSphere splatting utilizes an underusedbut highly effective geometric primitive -- tetrahedral meshes. This approachdirectly yields superior mesh quality without relying on neural networks orpost-processing. It deforms multiple initial tetrahedral spheres to accuratelyreconstruct the 3D shape through a combination of differentiable rendering andgeometric energy optimization, resulting in significant computationalefficiency. Serving as a robust and versatile geometry representation,Tet-Sphere splatting seamlessly integrates into diverse applications, includingsingle-view 3D reconstruction, image-/text-to-3D content generation.Experimental results demonstrate that TetSphere splatting outperforms existingrepresentations, delivering faster optimization speed, enhanced mesh quality,and reliable preservation of thin structures.</description><author>Minghao Guo, Bohan Wang, Kaiming He, Wojciech Matusik</author><pubDate>Thu, 30 May 2024 18:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20283v1</guid></item><item><title>SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified Flow</title><link>http://arxiv.org/abs/2405.20282v1</link><description>Semantic segmentation and semantic image synthesis are two representativetasks in visual perception and generation. While existing methods consider themas two distinct tasks, we propose a unified diffusion-based framework (SemFlow)and model them as a pair of reverse problems. Specifically, motivated byrectified flow theory, we train an ordinary differential equation (ODE) modelto transport between the distributions of real images and semantic masks. Asthe training object is symmetric, samples belonging to the two distributions,images and semantic masks, can be effortlessly transferred reversibly. Forsemantic segmentation, our approach solves the contradiction between therandomness of diffusion outputs and the uniqueness of segmentation results. Forimage synthesis, we propose a finite perturbation approach to enhance thediversity of generated results without changing the semantic categories.Experiments show that our SemFlow achieves competitive results on semanticsegmentation and semantic image synthesis tasks. We hope this simple frameworkwill motivate people to rethink the unification of low-level and high-levelvision. Project page: https://github.com/wang-chaoyang/SemFlow.</description><author>Chaoyang Wang, Xiangtai Li, Lu Qi, Henghui Ding, Yunhai Tong, Ming-Hsuan Yang</author><pubDate>Thu, 30 May 2024 18:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20282v1</guid></item><item><title>CV-VAE: A Compatible Video VAE for Latent Generative Video Models</title><link>http://arxiv.org/abs/2405.20279v1</link><description>Spatio-temporal compression of videos, utilizing networks such as VariationalAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous othervideo generative models. For instance, many LLM-like video models learn thedistribution of discrete tokens derived from 3D VAEs within the VQVAEframework, while most diffusion-based video models capture the distribution ofcontinuous latent extracted by 2D VAEs without quantization. The temporalcompression is simply realized by uniform frame sampling which results inunsmooth motion between consecutive frames. Currently, there lacks of acommonly used continuous video (3D) VAE for latent diffusion-based video modelsin the research community. Moreover, since current diffusion-based approachesare often implemented using pre-trained text-to-image (T2I) models, directlytraining a video VAE without considering the compatibility with existing T2Imodels will result in a latent space gap between them, which will take hugecomputational resources for training to bridge the gap even with the T2I modelsas initialization. To address this issue, we propose a method for training avideo VAE of latent video models, namely CV-VAE, whose latent space iscompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion(SD). The compatibility is achieved by the proposed novel latent spaceregularization, which involves formulating a regularization loss using theimage VAE. Benefiting from the latent space compatibility, video models can betrained seamlessly from pre-trained T2I or video models in a trulyspatio-temporally compressed latent space, rather than simply sampling videoframes at equal intervals. With our CV-VAE, existing video models can generatefour times more frames with minimal finetuning. Extensive experiments areconducted to demonstrate the effectiveness of the proposed video VAE.</description><author>Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, Ying Shan</author><pubDate>Thu, 30 May 2024 18:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20279v1</guid></item><item><title>Length independent generalization bounds for deep SSM architectures with stability constraints</title><link>http://arxiv.org/abs/2405.20278v1</link><description>Many state-of-the-art models trained on long-range sequences, for example S4,S5 or LRU, are made of sequential blocks combining State-Space Models (SSMs)with neural networks. In this paper we provide a PAC bound that holds for thesekind of architectures with stable SSM blocks and does not depend on the lengthof the input sequence. Imposing stability of the SSM blocks is a standardpractice in the literature, and it is known to help performance. Our resultsprovide a theoretical justification for the use of stable SSM blocks as theproposed PAC bound decreases as the degree of stability of the SSM blocksincreases.</description><author>Dániel Rácz, Mihály Petreczky, Bálint Daróczy</author><pubDate>Thu, 30 May 2024 18:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20278v1</guid></item><item><title>Unveiling Linguistic Regions in Large Language Models</title><link>http://arxiv.org/abs/2402.14700v3</link><description>Large Language Models (LLMs) have demonstrated considerable cross-lingualalignment and generalization ability. Current research primarily focuses onimproving LLMs' cross-lingual generalization capabilities. However, there isstill a lack of research on the intrinsic mechanisms of how LLMs achievecross-lingual alignment. From the perspective of region partitioning, thispaper conducts several investigations on the linguistic competence of LLMs. Wediscover a core region in LLMs that corresponds to linguistic competence,accounting for approximately 1% of the total model parameters. Removing thiscore region by setting parameters to zero results in a significant performancedecrease across 30 different languages. Furthermore, this core region exhibitssignificant dimensional dependence, perturbations to even a single parameter onspecific dimensions leading to a loss of linguistic competence. Moreover, wediscover that distinct monolingual regions exist for different languages, anddisruption to these specific regions substantially reduces the LLMs'proficiency in those corresponding languages. Our research also indicates thatfreezing the core linguistic region during further pre-training can mitigatethe issue of catastrophic forgetting (CF), a common phenomenon observed duringfurther pre-training of LLMs. Overall, exploring the LLMs' functional regionsprovides insights into the foundation of their intelligence.</description><author>Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang</author><pubDate>Thu, 30 May 2024 18:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14700v3</guid></item><item><title>Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2210.16299v4</link><description>A key challenge in solving the deterministic inverse reinforcement learning(IRL) problem online and in real-time is the existence of multiple solutions.Nonuniqueness necessitates the study of the notion of equivalent solutions,i.e., solutions that result in a different cost functional but same feedbackmatrix, and convergence to such solutions. While offline algorithms that resultin convergence to equivalent solutions have been developed in the literature,online, real-time techniques that address nonuniqueness are not available. Inthis paper, a regularized history stack observer that converges toapproximately equivalent solutions of the IRL problem is developed. Noveldata-richness conditions are developed to facilitate the analysis andsimulation results are provided to demonstrate the effectiveness of thedeveloped technique.</description><author>Jared Town, Zachary Morrison, Rushikesh Kamalapurkar</author><pubDate>Thu, 30 May 2024 18:31:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16299v4</guid></item><item><title>A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts</title><link>http://arxiv.org/abs/2405.16646v3</link><description>The sparsely gated mixture of experts (MoE) architecture sends differentinputs to different subnetworks, i.e., experts, through trainable routers. MoEreduces the training computation significantly for large models, but itsdeployment can be still memory or computation expensive for some downstreamtasks. Model pruning is a popular approach to reduce inference computation, butits application in MoE architecture is largely unexplored. To the best of ourknowledge, this paper provides the first provably efficient technique forpruning experts in finetuned MoE models. We theoretically prove thatprioritizing the pruning of the experts with a smaller change of the routers l2norm from the pretrained model guarantees the preservation of test accuracy,while significantly reducing the model size and the computational requirements.Although our theoretical analysis is centered on binary classification tasks onsimplified MoE architecture, our expert pruning method is verified on largevision MoE models such as VMoE and E3MoE finetuned on benchmark datasets suchas CIFAR10, CIFAR100, and ImageNet.</description><author>Mohammed Nowaz Rabbani Chowdhury, Meng Wang, Kaoutar El Maghraoui, Naigang Wang, Pin-Yu Chen, Christopher Carothers</author><pubDate>Thu, 30 May 2024 18:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16646v3</guid></item><item><title>ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection</title><link>http://arxiv.org/abs/2405.20274v1</link><description>Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansionand diversity due to various shared tasks spanning several languages and fieldsand organized via SemEval workshops and Germeval. Nonetheless, a fewshortcomings still need to be addressed, such as the lack of low-resourcelanguage evaluations and the emphasis on sentence-level analysis. To thoroughlyassess ABSA techniques in the context of complete reviews, this researchpresents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST).ROAST seeks to close the gap between sentence-level and text-level ABSA byidentifying every ABSA constituent at the review level. We extend the availabledatasets to enable ROAST, addressing the drawbacks noted in previous researchby incorporating low-resource languages, numerous languages, and a variety oftopics. Through this effort, ABSA research will be able to cover more groundand get a deeper comprehension of the task and its practical application in avariety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA).</description><author>Siva Uday Sampreeth Chebolu, Franck Dernoncourt, Nedim Lipka, Thamar Solorio</author><pubDate>Thu, 30 May 2024 18:29:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20274v1</guid></item><item><title>Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable</title><link>http://arxiv.org/abs/2405.20272v1</link><description>Machine unlearning is motivated by desire for data autonomy: a person canrequest to have their data's influence removed from deployed models, and thosemodels should be updated as if they were retrained without the person's data.We show that, counter-intuitively, these updates expose individuals tohigh-accuracy reconstruction attacks which allow the attacker to recover theirdata in its entirety, even when the original models are so simple that privacyrisk might not otherwise have been a concern. We show how to mount anear-perfect attack on the deleted data point from linear regression models. Wethen generalize our attack to other loss functions and architectures, andempirically demonstrate the effectiveness of our attacks across a wide range ofdatasets (capturing both tabular and image data). Our work highlights thatprivacy risk is significant even for extremely simple model classes whenindividuals can request deletion of their data from the model.</description><author>Martin Bertran, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu</author><pubDate>Thu, 30 May 2024 18:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20272v1</guid></item><item><title>ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections</title><link>http://arxiv.org/abs/2405.20271v1</link><description>Parameter-efficient finetuning (PEFT) has become ubiquitous to adaptfoundation models to downstream task requirements while retaining theirgeneralization ability. However, the amount of additionally introducedparameters and compute for successful adaptation and hyperparameter searchescan explode quickly, especially when deployed at scale to serve numerousindividual requests. To ensure effective, parameter-efficient, andhyperparameter-robust adaptation, we propose the ETHER transformation family,which performs Efficient fineTuning via HypErplane Reflections. By design,ETHER transformations require a minimal number of parameters, are less likelyto deteriorate model performance, and exhibit robustness to hyperparameter andlearning rate choices. In particular, we introduce ETHER and its relaxationETHER+, which match or outperform existing PEFT methods with significantlyfewer parameters ($\sim$$10$-$100$ times lower than LoRA or OFT) acrossmultiple image synthesis and natural language tasks without exhaustivehyperparameter tuning. Finally, we investigate the recent emphasis onHyperspherical Energy retention for adaptation and raise questions on itspractical utility. The code is available at https://github.com/mwbini/ether.</description><author>Massimo Bini, Karsten Roth, Zeynep Akata, Anna Khoreva</author><pubDate>Thu, 30 May 2024 18:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20271v1</guid></item><item><title>Oja's Algorithm for Sparse PCA</title><link>http://arxiv.org/abs/2402.07240v3</link><description>Oja's algorithm for streaming Principal Component Analysis (PCA) for $n$datapoints in a $d$ dimensional space achieves the same sin-squared error$O(r_\mathsf{eff}/n)$ as the offline algorithm in $O(d)$ space and $O(nd)$ timeand a single pass through the datapoints. Here $r_\mathsf{eff}$ is theeffective rank (ratio of the trace and the principal eigenvalue of thepopulation covariance matrix $\Sigma$). Under this computational budget, weconsider the problem of sparse PCA, where the principal eigenvector of $\Sigma$is $s$-sparse, and $r_\mathsf{eff}$ can be large. In this setting, to ourknowledge, \textit{there are no known single-pass algorithms} that achieve theminimax error bound in $O(d)$ space and $O(nd)$ time without either requiringstrong initialization conditions or assuming further structure (e.g., spiked)of the covariance matrix. We show that a simple single-pass procedure thatthresholds the output of Oja's algorithm (the Oja vector) can achieve theminimax error bound under some regularity conditions in $O(d)$ space and$O(nd)$ time as long as $r_\mathsf{eff}=O(n/\log n)$. We present a nontrivialand novel analysis of the entries of the unnormalized Oja vector, whichinvolves the projection of a product of independent random matrices on a randominitial vector. This is completely different from previous analyses of Oja'salgorithm and matrix products, which have been done when the $r_\mathsf{eff}$is bounded.</description><author>Syamantak Kumar, Purnamrita Sarkar</author><pubDate>Thu, 30 May 2024 18:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07240v3</guid></item><item><title>IsraParlTweet: The Israeli Parliamentary and Twitter Resource</title><link>http://arxiv.org/abs/2405.20269v1</link><description>We introduce IsraParlTweet, a new linked corpus of Hebrew-languageparliamentary discussions from the Knesset (Israeli Parliament) between theyears 1992-2023 and Twitter posts made by Members of the Knesset between theyears 2008-2023, containing a total of 294.5 million Hebrew tokens. In additionto raw text, the corpus contains comprehensive metadata on speakers and Knessetsessions as well as several linguistic annotations. As a result, IsraParlTweetcan be used to conduct a wide variety of quantitative and qualitative analysesand provide valuable insights into political discourse in Israel.</description><author>Guy Mor-Lan, Effi Levi, Tamir Sheafer, Shaul R. Shenhav</author><pubDate>Thu, 30 May 2024 18:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20269v1</guid></item><item><title>Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions</title><link>http://arxiv.org/abs/2405.20267v1</link><description>As LLMs evolve on a daily basis, there is an urgent need for a trustworthyevaluation method that can provide robust evaluation results in a timelyfashion. Currently, as static benchmarks are prone to contamination concerns,users tend to trust human voting platforms, such as Chatbot Arena. However,human annotations require extensive manual efforts. To provide an automatic,robust, and trustworthy evaluation framework, we innovatively propose theAuto-Arena of LLMs, which automates the entire evaluation process with LLMagents. Firstly, an examiner LLM devises queries. Then, a pair of candidateLLMs engage in a multi-round peer-battle around the query, during which theLLM's true performance gaps become visible. Finally, a committee of LLM judgescollectively discuss and determine the winner, which alleviates bias andpromotes fairness. In our extensive experiment on the 17 newest LLMs,Auto-Arena shows the highest correlation with human preferences, providing apromising alternative to human evaluation platforms.</description><author>Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, Lidong Bing</author><pubDate>Thu, 30 May 2024 18:19:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20267v1</guid></item><item><title>MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</title><link>http://arxiv.org/abs/2405.19327v2</link><description>Large Language Models (LLMs) have made great strides in recent years toachieve unprecedented performance across different tasks. However, due tocommercial interest, the most competitive models like GPT, Gemini, and Claudehave been gated behind proprietary interfaces without disclosing the trainingdetails. Recently, many institutions have open-sourced several strong LLMs likeLLaMA-3, comparable to existing closed-source LLMs. However, only the model'sweights are provided with most details (e.g., intermediate checkpoints,pre-training corpus, and training code, etc.) being undisclosed. To improve thetransparency of LLMs, the research community has formed to open-source trulyopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-trainingcorpus and training code) are being provided. These models have greatlyadvanced the scientific study of these large models including their strengths,weaknesses, biases and risks. However, we observe that the existing truly openLLMs on reasoning, knowledge, and coding tasks are still inferior to existingstate-of-the-art LLMs with similar model sizes. To this end, we open-sourceMAP-Neo, a highly capable and transparent bilingual language model with 7Bparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is thefirst fully open-sourced bilingual LLM with comparable performance compared toexisting state-of-the-art LLMs. Moreover, we open-source all details toreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaningpipeline, checkpoints, and well-optimized training/evaluation framework areprovided. Finally, we hope our MAP-Neo will enhance and strengthen the openresearch community and inspire more innovations and creativities to facilitatethe further improvements of LLMs.</description><author>Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen</author><pubDate>Thu, 30 May 2024 18:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19327v2</guid></item><item><title>How to Leverage Diverse Demonstrations in Offline Imitation Learning</title><link>http://arxiv.org/abs/2405.17476v3</link><description>Offline Imitation Learning (IL) with imperfect demonstrations has garneredincreasing attention owing to the scarcity of expert data in many real-worlddomains. A fundamental problem in this scenario is how to extract positivebehaviors from noisy data. In general, current approaches to the problem selectdata building on state-action similarity to given expert demonstrations,neglecting precious information in (potentially abundant) $\textit{diverse}$state-actions that deviate from expert ones. In this paper, we introduce asimple yet effective data selection method that identifies positive behaviorsbased on their resultant states -- a more informative criterion enablingexplicit utilization of dynamics information and effective extraction of bothexpert and beneficial diverse behaviors. Further, we devise a lightweightbehavior cloning algorithm capable of leveraging the expert and selected datacorrectly. In the experiments, we evaluate our method on a suite of complex andhigh-dimensional offline IL benchmarks, including continuous-control andvision-based tasks. The results demonstrate that our method achievesstate-of-the-art performance, outperforming existing methods on$\textbf{20/21}$ benchmarks, typically by $\textbf{2-5x}$, while maintaining acomparable runtime to Behavior Cloning ($\texttt{BC}$).</description><author>Sheng Yue, Jiani Liu, Xingyuan Hua, Ju Ren, Sen Lin, Junshan Zhang, Yaoxue Zhang</author><pubDate>Thu, 30 May 2024 18:15:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17476v3</guid></item><item><title>Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning</title><link>http://arxiv.org/abs/2402.01057v2</link><description>In this paper, we focus on single-demonstration imitation learning (IL), apractical approach for real-world applications where acquiring multiple expertdemonstrations is costly or infeasible and the ground truth reward function isnot available. In contrast to typical IL settings with multiple demonstrations,single-demonstration IL involves an agent having access to only one experttrajectory. We highlight the issue of sparse reward signals in this setting andpropose to mitigate this issue through our proposed TransitionDiscriminator-based IL (TDIL) method. TDIL is an IRL method designed to addressreward sparsity by introducing a denser surrogate reward function thatconsiders environmental dynamics. This surrogate reward function encourages theagent to navigate towards states that are proximal to expert states. Inpractice, TDIL trains a transition discriminator to differentiate between validand non-valid transitions in a given environment to compute the surrogaterewards. The experiments demonstrate that TDIL outperforms existing ILapproaches and achieves expert-level performance in the single-demonstration ILsetting across five widely adopted MuJoCo benchmarks as well as the "AdroitDoor" robotic environment.</description><author>Chia-Cheng Chiang, Li-Cheng Lan, Wei-Fang Sun, Chien Feng, Cho-Jui Hsieh, Chun-Yi Lee</author><pubDate>Thu, 30 May 2024 18:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01057v2</guid></item><item><title>Absolute Policy Optimization</title><link>http://arxiv.org/abs/2310.13230v5</link><description>In recent years, trust region on-policy reinforcement learning has achievedimpressive results in addressing complex control tasks and gaming scenarios.However, contemporary state-of-the-art algorithms within this categoryprimarily emphasize improvement in expected performance, lacking the ability tocontrol over the worst-case performance outcomes. To address this limitation,we introduce a novel objective function, optimizing which leads to guaranteedmonotonic improvement in the lower probability bound of performance with highconfidence. Building upon this groundbreaking theoretical advancement, wefurther introduce a practical solution called Absolute Policy Optimization(APO). Our experiments demonstrate the effectiveness of our approach acrosschallenging continuous control benchmark tasks and extend its applicability tomastering Atari games. Our findings reveal that APO as well as its efficientvariation Proximal Absolute Policy Optimization (PAPO) significantlyoutperforms state-of-the-art policy gradient algorithms, resulting insubstantial improvements in worst-case performance, as well as expectedperformance.</description><author>Weiye Zhao, Feihan Li, Yifan Sun, Rui Chen, Tianhao Wei, Changliu Liu</author><pubDate>Thu, 30 May 2024 18:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13230v5</guid></item><item><title>OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning</title><link>http://arxiv.org/abs/2405.17477v3</link><description>In this paper, we study offline-to-online Imitation Learning (IL) thatpretrains an imitation policy from static demonstration data, followed by fastfinetuning with minimal environmental interaction. We find the na\"ivecombination of existing offline IL and online IL methods tends to behave poorlyin this context, because the initial discriminator (often used in online IL)operates randomly and discordantly against the policy initialization, leadingto misguided policy optimization and $\textit{unlearning}$ of pretrainingknowledge. To overcome this challenge, we propose a principledoffline-to-online IL method, named $\texttt{OLLIE}$, that simultaneously learnsa near-expert policy initialization along with an $\textit{aligneddiscriminator initialization}$, which can be seamlessly integrated into onlineIL, achieving smooth and fast finetuning. Empirically, $\texttt{OLLIE}$consistently and significantly outperforms the baseline methods in$\textbf{20}$ challenging tasks, from continuous control to vision-baseddomains, in terms of performance, demonstration efficiency, and convergencespeed. This work may serve as a foundation for further exploration ofpretraining and finetuning in the context of IL.</description><author>Sheng Yue, Xingyuan Hua, Ju Ren, Sen Lin, Junshan Zhang, Yaoxue Zhang</author><pubDate>Thu, 30 May 2024 18:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17477v3</guid></item><item><title>Formalizing and Benchmarking Prompt Injection Attacks and Defenses</title><link>http://arxiv.org/abs/2310.12815v2</link><description>A prompt injection attack aims to inject malicious instruction/data into theinput of an LLM-Integrated Application such that it produces results as anattacker desires. Existing works are limited to case studies. As a result, theliterature lacks a systematic understanding of prompt injection attacks andtheir defenses. We aim to bridge the gap in this work. In particular, wepropose a framework to formalize prompt injection attacks. Existing attacks arespecial cases in our framework. Moreover, based on our framework, we design anew attack by combining existing ones. Using our framework, we conduct asystematic evaluation on 5 prompt injection attacks and 10 defenses with 10LLMs and 7 tasks. Our work provides a common benchmark for quantitativelyevaluating future prompt injection attacks and defenses. To facilitate researchon this topic, we make our platform public athttps://github.com/liu00222/Open-Prompt-Injection.</description><author>Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong</author><pubDate>Thu, 30 May 2024 18:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12815v2</guid></item><item><title>FaceMixup: Enhancing Facial Expression Recognition through Mixed Face Regularization</title><link>http://arxiv.org/abs/2405.20259v1</link><description>The proliferation of deep learning solutions and the scarcity of largeannotated datasets pose significant challenges in real-world applications.Various strategies have been explored to overcome this challenge, with dataaugmentation (DA) approaches emerging as prominent solutions. DA approachesinvolve generating additional examples by transforming existing labeled data,thereby enriching the dataset and helping deep learning models achieve improvedgeneralization without succumbing to overfitting. In real applications, wheresolutions based on deep learning are widely used, there is facial expressionrecognition (FER), which plays an essential role in human communication,improving a range of knowledge areas (e.g., medicine, security, and marketing).In this paper, we propose a simple and comprehensive face data augmentationapproach based on mixed face component regularization that outperforms theclassical DA approaches from the literature, including the MixAugment which isa specific approach for the target task in two well-known FER datasets existingin the literature.</description><author>Fabio A. Faria, Mateus M. Souza, Raoni F. da S. Teixeira, Mauricio P. Segundo</author><pubDate>Thu, 30 May 2024 18:09:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20259v1</guid></item><item><title>Evaluating Large Language Model Biases in Persona-Steered Generation</title><link>http://arxiv.org/abs/2405.20253v1</link><description>The task of persona-steered text generation requires large language models(LLMs) to generate text that reflects the distribution of views that anindividual fitting a persona could have. People have multifaceted personas, butprior work on bias in LLM-generated opinions has only explored multiple-choicesettings or one-dimensional personas. We define an incongruous persona as apersona with multiple traits where one trait makes its other traits less likelyin human survey data, e.g. political liberals who support increased militaryspending. We find that LLMs are 9.7% less steerable towards incongruouspersonas than congruous ones, sometimes generating the stereotypical stanceassociated with its demographic rather than the target stance. Models that weevaluate that are fine-tuned with Reinforcement Learning from Human Feedback(RLHF) are more steerable, especially towards stances associated with politicalliberals and women, but present significantly less diverse views of personas.We also find variance in LLM steerability that cannot be predicted frommultiple-choice opinion evaluation. Our results show the importance ofevaluating models in open-ended text generation, as it can surface new LLMopinion biases. Moreover, such a setup can shed light on our ability to steermodels toward a richer and more diverse range of viewpoints.</description><author>Andy Liu, Mona Diab, Daniel Fried</author><pubDate>Thu, 30 May 2024 18:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20253v1</guid></item><item><title>Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization</title><link>http://arxiv.org/abs/2405.20252v1</link><description>Large language models (LLMs) have shown great progress in responding to userquestions, allowing for a multitude of diverse applications. Yet, the qualityof LLM outputs heavily depends on the prompt design, where a good prompt mightenable the LLM to answer a very challenging question correctly. Therefore,recent works have developed many strategies for improving the prompt, includingboth manual crafting and in-domain optimization. However, their efficacy inunrestricted scenarios remains questionable, as the former depends on humandesign for specific questions and the latter usually generalizes poorly tounseen scenarios. To address these problems, we give LLMs the freedom to designthe best prompts according to themselves. Specifically, we include a hierarchyof LLMs, first constructing a prompt with precise instructions and accuratewording in a hierarchical manner, and then using this prompt to generate thefinal answer to the user query. We term this pipeline Hierarchical Multi-AgentWorkflow, or HMAW. In contrast with prior works, HMAW imposes no humanrestriction and requires no training, and is completely task-agnostic whilecapable of adjusting to the nuances of the underlying task. Through bothquantitative and qualitative experiments across multiple benchmarks, we verifythat despite its simplicity, the proposed approach can create detailed andsuitable prompts, further boosting the performance of current LLMs.</description><author>Yuchi Liu, Jaskirat Singh, Gaowen Liu, Ali Payani, Liang Zheng</author><pubDate>Thu, 30 May 2024 18:05:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20252v1</guid></item><item><title>Entropy annealing for policy mirror descent in continuous time and space</title><link>http://arxiv.org/abs/2405.20250v1</link><description>Entropy regularization has been extensively used in policy optimizationalgorithms to regularize the optimization landscape and accelerate convergence;however, it comes at the cost of introducing an additional regularization bias.This work quantifies the impact of entropy regularization on the convergence ofpolicy gradient methods for stochastic exit time control problems. We analyze acontinuous-time policy mirror descent dynamics, which updates the policy basedon the gradient of an entropy-regularized value function and adjusts thestrength of entropy regularization as the algorithm progresses. We prove thatwith a fixed entropy level, the dynamics converges exponentially to the optimalsolution of the regularized problem. We further show that when the entropylevel decays at suitable polynomial rates, the annealed flow converges to thesolution of the unregularized problem at a rate of $\mathcal O(1/S)$ fordiscrete action spaces and, under suitable conditions, at a rate of $\mathcalO(1/\sqrt{S})$ for general action spaces, with $S$ being the gradient flowtime. This paper explains how entropy regularization improves policyoptimization, even with the true gradient, from the perspective of convergencerate.</description><author>Deven Sethi, David Šiška, Yufei Zhang</author><pubDate>Thu, 30 May 2024 18:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20250v1</guid></item><item><title>WW-FL: Secure and Private Large-Scale Federated Learning</title><link>http://arxiv.org/abs/2302.09904v3</link><description>Federated learning (FL) is an efficient approach for large-scale distributedmachine learning that promises data privacy by keeping training data on clientdevices. However, recent research has uncovered vulnerabilities in FL,impacting both security and privacy through poisoning attacks and the potentialdisclosure of sensitive information in individual model updates as well as theaggregated global model. This paper explores the inadequacies of existing FLprotection measures when applied independently, and the challenges of creatingeffective compositions. Addressing these issues, we propose WW-FL, an innovative framework thatcombines secure multi-party computation (MPC) with hierarchical FL to guaranteedata and global model privacy. One notable feature of WW-FL is its capabilityto prevent malicious clients from directly poisoning model parameters,confining them to less destructive data poisoning attacks. We furthermoreprovide a PyTorch-based FL implementation integrated with Meta's CrypTen MPCframework to systematically measure the performance and robustness of WW-FL.Our extensive evaluation demonstrates that WW-FL is a promising solution forsecure and private large-scale federated learning.</description><author>Felix Marx, Thomas Schneider, Ajith Suresh, Tobias Wehrle, Christian Weinert, Hossein Yalame</author><pubDate>Thu, 30 May 2024 18:00:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09904v3</guid></item><item><title>Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration</title><link>http://arxiv.org/abs/2404.12715v2</link><description>Large language models (LLMs) exhibit complementary strengths in varioustasks, motivating the research of LLM ensembling. However, existing workfocuses on training an extra reward model or fusion model to select or combineall candidate answers, posing a great challenge to the generalization on unseendata distributions. Besides, prior methods use textual responses ascommunication media, ignoring the valuable information in the internalrepresentations. In this work, we propose a training-free ensemble frameworkDeePEn, fusing the informative probability distributions yielded by differentLLMs at each decoding step. Unfortunately, the vocabulary discrepancy betweenheterogeneous LLMs directly makes averaging the distributions unfeasible due tothe token misalignment. To address this challenge, DeePEn maps the probabilitydistribution of each model from its own probability space to a universalrelative space based on the relative representation theory, and performsaggregation. Next, we devise a search-based inverse transformation to transformthe aggregated result back to the probability space of one of the ensemblingLLMs (main model), in order to determine the next token. We conduct extensiveexperiments on ensembles of different number of LLMs, ensembles of LLMs withdifferent architectures, and ensembles between the LLM and the specialistmodel. Experimental results show that (i) DeePEn achieves consistentimprovements across six benchmarks covering subject examination, reasoning, andknowledge, (ii) a well-performing specialist model can benefit from a lesseffective LLM through distribution fusion, and (iii) DeePEn has complementarystrengths with other ensemble methods such as voting.</description><author>Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Bing Qin, Ting Liu</author><pubDate>Thu, 30 May 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12715v2</guid></item><item><title>On the Last-Iterate Convergence of Shuffling Gradient Methods</title><link>http://arxiv.org/abs/2403.07723v2</link><description>Shuffling gradient methods are widely implemented in practice, particularlyincluding three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO),and Incremental Gradient (IG). Compared to the empirical success, thetheoretical guarantee of shuffling gradient methods was not well-understood fora long time. Until recently, the convergence rates had just been establishedfor the average iterate for convex functions and the last iterate for stronglyconvex problems (using squared distance as the metric). However, when using thefunction value gap as the convergence criterion, existing theories cannotinterpret the good performance of the last iterate in different settings (e.g.,constrained optimization). To bridge this gap between practice and theory, weprove the first last-iterate convergence rates for shuffling gradient methodswith respect to the objective value even without strong convexity. Our newresults either (nearly) match the existing last-iterate lower bounds or are asfast as the previous best upper bounds for the average iterate.</description><author>Zijian Liu, Zhengyuan Zhou</author><pubDate>Thu, 30 May 2024 17:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07723v2</guid></item><item><title>KerasCV and KerasNLP: Vision and Language Power-Ups</title><link>http://arxiv.org/abs/2405.20247v1</link><description>We present the Keras domain packages KerasCV and KerasNLP, extensions of theKeras API for Computer Vision and Natural Language Processing workflows,capable of running on either JAX, TensorFlow, or PyTorch. These domain packagesare designed to enable fast experimentation, with a focus on ease-of-use andperformance. We adopt a modular, layered design: at the library's lowest levelof abstraction, we provide building blocks for creating models and datapreprocessing pipelines, and at the library's highest level of abstraction, weprovide pretrained ``task" models for popular architectures such as StableDiffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, T5, etc. Task models havebuilt-in preprocessing, pretrained weights, and can be fine-tuned on rawinputs. To enable efficient training, we support XLA compilation for allmodels, and run all preprocessing via a compiled graph of TensorFlow operationsusing the tf.data API. The libraries are fully open-source (Apache 2.0 license)and available on GitHub.</description><author>Matthew Watson, Divyashree Shivakumar Sreepathihalli, Francois Chollet, Martin Gorner, Kiranbir Sodhia, Ramesh Sampath, Tirth Patel, Haifeng Jin, Neel Kovelamudi, Gabriel Rasskin, Samaneh Saadat, Luke Wood, Chen Qian, Jonathan Bischof, Ian Stenbit</author><pubDate>Thu, 30 May 2024 17:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20247v1</guid></item><item><title>Image Deraining with Frequency-Enhanced State Space Model</title><link>http://arxiv.org/abs/2405.16470v2</link><description>Removing rain artifacts in images is recognized as a significant issue. Inthis field, deep learning-based approaches, such as convolutional neuralnetworks (CNNs) and Transformers, have succeeded. Recently, State Space Models(SSMs) have exhibited superior performance across various tasks in both naturallanguage processing and image processing due to their ability to modellong-range dependencies. This study introduces SSM to rain removal and proposesa Deraining Frequency-Enhanced State Space Model (DFSSM). To effectively removerain streaks, which produce high-intensity frequency components in specificdirections, we employ frequency domain processing concurrently with SSM.Additionally, we develop a novel mixed-scale gated-convolutional block, whichuses convolutions with multiple kernel sizes to capture various scaledegradations effectively and integrates a gating mechanism to manage the flowof information. Finally, experiments on synthetic and real-world rainy imagedatasets show that our method surpasses state-of-the-art methods.</description><author>Shugo Yamashita, Masaaki Ikehara</author><pubDate>Thu, 30 May 2024 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16470v2</guid></item><item><title>Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use</title><link>http://arxiv.org/abs/2405.20245v1</link><description>Business Document Information Extraction (BDIE) is the problem oftransforming a blob of unstructured information (raw text, scanned documents,etc.) into a structured format that downstream systems can parse and use. Ithas two main tasks: Key-Information Extraction (KIE) and Line Items Recognition(LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem,where the tools are these downstream systems. We then present RetrievalAugmented Structured Generation (RASG), a novel general framework for BDIE thatachieves state of the art (SOTA) results on both KIE and LIR tasks on BDIEbenchmarks. The contributions of this paper are threefold: (1) We show, with ablationbenchmarks, that Large Language Models (LLMs) with RASG are already competitivewith or surpasses current SOTA Large Multimodal Models (LMMs) without RASG onBDIE benchmarks. (2) We propose a new metric class for Line Items Recognition,General Line Items Recognition Metric (GLIRM), that is more aligned withpractical BDIE use cases compared to existing metrics, such as ANLS*, DocILE,and GriTS. (3) We provide a heuristic algorithm for backcalculating boundingboxes of predicted line items and tables without the need for vision encoders.Finally, we claim that, while LMMs might sometimes offer marginal performancebenefits, LLMs + RASG is oftentimes superior given real-world applications andconstraints of BDIE.</description><author>Franz Louis Cesista, Rui Aguiar, Jason Kim, Paolo Acilo</author><pubDate>Thu, 30 May 2024 17:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20245v1</guid></item><item><title>Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk</title><link>http://arxiv.org/abs/2208.07590v3</link><description>Risk assessment for extreme events requires accurate estimation of highquantiles that go beyond the range of historical observations. When the riskdepends on the values of observed predictors, regression techniques are used tointerpolate in the predictor space. We propose the EQRN model that combinestools from neural networks and extreme value theory into a method capable ofextrapolation in the presence of complex predictor dependence. Neural networkscan naturally incorporate additional structure in the data. We develop arecurrent version of EQRN that is able to capture complex sequential dependencein time series. We apply this method to forecast flood risk in the Swiss Aarecatchment. It exploits information from multiple covariates in space and timeto provide one-day-ahead predictions of return levels and exceedanceprobabilities. This output complements the static return level from atraditional extreme value analysis, and the predictions are able to adapt todistributional shifts as experienced in a changing climate. Our model can helpauthorities to manage flooding more effectively and to minimize theirdisastrous impacts through early warning systems.</description><author>Olivier C. Pasche, Sebastian Engelke</author><pubDate>Thu, 30 May 2024 17:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07590v3</guid></item><item><title>EMS: Efficient and Effective Massively Multilingual Sentence Embedding Learning</title><link>http://arxiv.org/abs/2205.15744v2</link><description>Massively multilingual sentence representation models, e.g., LASER,SBERT-distill, and LaBSE, help significantly improve cross-lingual downstreamtasks. However, the use of a large amount of data or inefficient modelarchitectures results in heavy computation to train a new model according toour preferred languages and domains. To resolve this issue, we introduceefficient and effective massively multilingual sentence embedding (EMS), usingcross-lingual token-level reconstruction (XTR) and sentence-level contrastivelearning as training objectives. Compared with related studies, the proposedmodel can be efficiently trained using significantly fewer parallel sentencesand GPU computation resources. Empirical results showed that the proposed modelsignificantly yields better or comparable results with regard to cross-lingualsentence retrieval, zero-shot cross-lingual genre classification, and sentimentclassification. Ablative analyses demonstrated the efficiency and effectivenessof each component of the proposed model. We release the codes for modeltraining and the EMS pre-trained sentence embedding model, which supports 62languages ( https://github.com/Mao-KU/EMS ).</description><author>Zhuoyuan Mao, Chenhui Chu, Sadao Kurohashi</author><pubDate>Thu, 30 May 2024 17:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.15744v2</guid></item><item><title>Training-efficient density quantum machine learning</title><link>http://arxiv.org/abs/2405.20237v1</link><description>Quantum machine learning requires powerful, flexible and efficientlytrainable models to be successful in solving challenging problems. In thiswork, we present density quantum neural networks, a learning modelincorporating randomisation over a set of trainable unitaries. These modelsgeneralise quantum neural networks using parameterised quantum circuits, andallow a trade-off between expressibility and efficient trainability,particularly on quantum hardware. We demonstrate the flexibility of theformalism by applying it to two recently proposed model families. The first arecommuting-block quantum neural networks (QNNs) which are efficiently trainablebut may be limited in expressibility. The second are orthogonal (Hamming-weightpreserving) quantum neural networks which provide well-defined andinterpretable transformations on data but are challenging to train at scale onquantum devices. Density commuting QNNs improve capacity with minimal gradientcomplexity overhead, and density orthogonal neural networks admit aquadratic-to-constant gradient query advantage with minimal to no performanceloss. We conduct numerical experiments on synthetic translationally invariantdata and MNIST image data with hyperparameter optimisation to support ourfindings. Finally, we discuss the connection to post-variational quantum neuralnetworks, measurement-based quantum machine learning and the dropout mechanism.</description><author>Brian Coyle, El Amine Cherrat, Nishant Jain, Natansh Mathur, Snehal Raj, Skander Kazdaghli, Iordanis Kerenidis</author><pubDate>Thu, 30 May 2024 17:40:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20237v1</guid></item><item><title>Disentangling and Mitigating the Impact of Task Similarity for Continual Learning</title><link>http://arxiv.org/abs/2405.20236v1</link><description>Continual learning of partially similar tasks poses a challenge forartificial neural networks, as task similarity presents both an opportunity forknowledge transfer and a risk of interference and catastrophic forgetting.However, it remains unclear how task similarity in input features and readoutpatterns influences knowledge transfer and forgetting, as well as how theyinteract with common algorithms for continual learning. Here, we develop alinear teacher-student model with latent structure and show analytically thathigh input feature similarity coupled with low readout similarity iscatastrophic for both knowledge transfer and retention. Conversely, theopposite scenario is relatively benign. Our analysis further reveals thattask-dependent activity gating improves knowledge retention at the expense oftransfer, while task-dependent plasticity gating does not affect eitherretention or transfer performance at the over-parameterized limit. In contrast,weight regularization based on the Fisher information metric significantlyimproves retention, regardless of task similarity, without compromisingtransfer performance. Nevertheless, its diagonal approximation andregularization in the Euclidean space are much less robust against tasksimilarity. We demonstrate consistent results in a permuted MNIST task withlatent variables. Overall, this work provides insights into when continuallearning is difficult and how to mitigate it.</description><author>Naoki Hiratani</author><pubDate>Thu, 30 May 2024 17:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20236v1</guid></item><item><title>Context Injection Attacks on Large Language Models</title><link>http://arxiv.org/abs/2405.20234v1</link><description>Large Language Models (LLMs) such as ChatGPT and Llama-2 have becomeprevalent in real-world applications, exhibiting impressive text generationperformance. LLMs are fundamentally developed from a scenario where the inputdata remains static and lacks a clear structure. To behave interactively overtime, LLM-based chat systems must integrate additional contextual information(i.e., chat history) into their inputs, following a pre-defined structure. Thispaper identifies how such integration can expose LLMs to misleading contextfrom untrusted sources and fail to differentiate between system and userinputs, allowing users to inject context. We present a systematic methodologyfor conducting context injection attacks aimed at eliciting disallowedresponses by introducing fabricated context. This could lead to illegalactions, inappropriate content, or technology misuse. Our context fabricationstrategies, acceptance elicitation and word anonymization, effectively createmisleading contexts that can be structured with attacker-customized prompttemplates, achieving injection through malicious user messages. Comprehensiveevaluations on real-world LLMs such as ChatGPT and Llama-2 confirm the efficacyof the proposed attack with success rates reaching 97%. We also discusspotential countermeasures that can be adopted for attack detection anddeveloping more secure models. Our findings provide insights into thechallenges associated with the real-world deployment of LLMs for interactiveand structured data scenarios.</description><author>Cheng'an Wei, Kai Chen, Yue Zhao, Yujia Gong, Lu Xiang, Shenchen Zhu</author><pubDate>Thu, 30 May 2024 17:36:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20234v1</guid></item><item><title>Grokfast: Accelerated Grokking by Amplifying Slow Gradients</title><link>http://arxiv.org/abs/2405.20233v1</link><description>One puzzling artifact in machine learning dubbed grokking is where delayedgeneralization is achieved tenfolds of iterations after near perfectoverfitting to the training data. Focusing on the long delay itself on behalfof machine learning practitioners, our goal is to accelerate generalization ofa model under grokking phenomenon. By regarding a series of gradients of aparameter over training iterations as a random signal over time, we canspectrally decompose the parameter trajectories under gradient descent into twocomponents: the fast-varying, overfitting-yielding component and theslow-varying, generalization-inducing component. This analysis allows us toaccelerate the grokking phenomenon more than $\times 50$ with only a few linesof code that amplifies the slow-varying components of gradients. Theexperiments show that our algorithm applies to diverse tasks involving images,languages, and graphs, enabling practical availability of this peculiarartifact of sudden generalization. Our code is available at\url{https://github.com/ironjr/grokfast}.</description><author>Jaerin Lee, Bong Gyun Kang, Kihoon Kim, Kyoung Mu Lee</author><pubDate>Thu, 30 May 2024 17:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20233v1</guid></item><item><title>The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof</title><link>http://arxiv.org/abs/2405.20231v1</link><description>Many algorithms and observed phenomena in deep learning appear to be affectedby parameter symmetries -- transformations of neural network parameters that donot change the underlying neural network function. These include linear modeconnectivity, model merging, Bayesian neural network inference, metanetworks,and several other characteristics of optimization or loss-landscapes. However,theoretical analysis of the relationship between parameter space symmetries andthese phenomena is difficult. In this work, we empirically investigate theimpact of neural parameter symmetries by introducing new neural networkarchitectures that have reduced parameter space symmetries. We develop twomethods, with some provable guarantees, of modifying standard neural networksto reduce parameter space symmetries. With these new methods, we conduct acomprehensive experimental study consisting of multiple tasks aimed atassessing the effect of removing parameter symmetries. Our experiments revealseveral interesting observations on the empirical impact of parametersymmetries; for instance, we observe linear mode connectivity between ournetworks without alignment of weight spaces, and we find that our networksallow for faster and more effective Bayesian neural network training.</description><author>Derek Lim, Moe Putterman, Robin Walters, Haggai Maron, Stefanie Jegelka</author><pubDate>Thu, 30 May 2024 17:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20231v1</guid></item><item><title>Finding Outliers in Gaussian Model-Based Clustering</title><link>http://arxiv.org/abs/1907.01136v6</link><description>Clustering, or unsupervised classification, is a task often plagued byoutliers. Yet there is a paucity of work on handling outliers in clustering.Outlier identification algorithms tend to fall into three broad categories:outlier inclusion, outlier trimming, and post hoc outlier identificationmethods, with the former two often requiring pre-specification of the number ofoutliers. The fact that sample squared Mahalanobis distance is beta-distributedis used to derive an approximate distribution for the log-likelihoods of subsetfinite Gaussian mixture models. An algorithm is then proposed that removes theleast plausible points according to the subset log-likelihoods, which aredeemed outliers, until the subset log-likelihoods adhere to the referencedistribution. This results in a trimming method, called OCLUST, that inherentlyestimates the number of outliers.</description><author>Katharine M. Clark, Paul D. McNicholas</author><pubDate>Thu, 30 May 2024 17:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1907.01136v6</guid></item><item><title>Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</title><link>http://arxiv.org/abs/2402.14800v2</link><description>A pivotal advancement in the progress of large language models (LLMs) is theemergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,MoE LLMs can achieve higher performance with fewer parameters, but it is stillhard to deploy them due to their immense parameter sizes. Different fromprevious weight pruning methods that rely on specifically designed hardware,this paper mainly aims to enhance the deployment efficiency of MoE LLMs byintroducing plug-and-play expert-level sparsification techniques. Specifically,we propose, for the first time to our best knowledge, post-training approachesfor task-agnostic and task-specific expert pruning and skipping of MoE LLMs,tailored to improve deployment efficiency while maintaining model performanceacross a wide range of tasks. Extensive experiments show that our proposedmethods can simultaneously reduce model sizes and increase the inference speed,while maintaining satisfactory performance. Data and code will be available athttps://github.com/Lucky-Lance/Expert_Sparsity.</description><author>Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li</author><pubDate>Thu, 30 May 2024 17:24:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14800v2</guid></item><item><title>MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model</title><link>http://arxiv.org/abs/2405.20222v1</link><description>We present MOFA-Video, an advanced controllable image animation method thatgenerates video from the given image using various additional controllablesignals (such as human landmarks reference, manual trajectories, and anothereven provided video) or their combinations. This is different from previousmethods which only can work on a specific motion domain or show weak controlabilities with diffusion prior. To achieve our goal, we design severaldomain-aware motion field adapters (\ie, MOFA-Adapters) to control thegenerated motions in the video generation pipeline. For MOFA-Adapters, weconsider the temporal motion consistency of the video and generate the densemotion flow from the given sparse control conditions first, and then, themulti-scale features of the given image are wrapped as a guided feature forstable video diffusion generation. We naively train two motion adapters for themanual trajectories and the human landmarks individually since they bothcontain sparse information about the control. After training, the MOFA-Adaptersin different domains can also work together for more controllable videogeneration.</description><author>Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, Yinqiang Zheng</author><pubDate>Thu, 30 May 2024 17:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20222v1</guid></item><item><title>Croissant: A Metadata Format for ML-Ready Datasets</title><link>http://arxiv.org/abs/2403.19546v2</link><description>Data is a critical resource for Machine Learning (ML), yet working with dataremains a key friction point. This paper introduces Croissant, a metadataformat for datasets that simplifies how data is used by ML tools andframeworks. Croissant makes datasets more discoverable, portable andinteroperable, thereby addressing significant challenges in ML data managementand responsible AI. Croissant is already supported by several popular datasetrepositories, spanning hundreds of thousands of datasets, ready to be loadedinto the most popular ML frameworks.</description><author>Mubashara Akhtar, Omar Benjelloun, Costanza Conforti, Pieter Gijsbers, Joan Giner-Miguelez, Nitisha Jain, Michael Kuchnik, Quentin Lhoest, Pierre Marcenac, Manil Maskey, Peter Mattson, Luis Oala, Pierre Ruyssen, Rajat Shinde, Elena Simperl, Goeffry Thomas, Slava Tykhonov, Joaquin Vanschoren, Jos van der Velde, Steffen Vogler, Carole-Jean Wu</author><pubDate>Thu, 30 May 2024 17:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19546v2</guid></item><item><title>ESG-FTSE: A corpus of news articles with ESG relevance labels and use cases</title><link>http://arxiv.org/abs/2405.20218v1</link><description>We present ESG-FTSE, the first corpus comprised of news articles withEnvironmental, Social and Governance (ESG) relevance annotations. In recentyears, investors and regulators have pushed ESG investing to the mainstream dueto the urgency of climate change. This has led to the rise of ESG scores toevaluate an investment's credentials as socially responsible. While demand forESG scores is high, their quality varies wildly. Quantitative techniques can beapplied to improve ESG scores, thus, responsible investing. To contribute toresource building for ESG and financial text mining, we pioneer the ESG-FTSEcorpus. We further present the first of its kind ESG annotation schema. It hasthree levels: a binary classification (relevant versus irrelevant newsarticles), ESG classification (ESG-related news articles), and target company.Both supervised and unsupervised learning experiments for ESG relevancedetection were conducted to demonstrate that the corpus can be used indifferent settings to derive accurate ESG predictions. Keywords: corpusannotation, ESG labels, annotation schema, news article, natural languageprocessing</description><author>Mariya Pavlova, Bernard Casey, Miaosen Wang</author><pubDate>Thu, 30 May 2024 17:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20218v1</guid></item><item><title>Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback</title><link>http://arxiv.org/abs/2405.20216v1</link><description>The generation of high-quality human images through text-to-image (T2I)methods is a significant yet challenging task. Distinct from general imagegeneration, human image synthesis must satisfy stringent criteria related tohuman pose, anatomy, and alignment with textual prompts, making it particularlydifficult to achieve realistic results. Recent advancements in T2I generationbased on diffusion models have shown promise, yet challenges remain in meetinghuman-specific preferences. In this paper, we introduce a novel approachtailored specifically for human image generation utilizing Direct PreferenceOptimization (DPO). Specifically, we introduce an efficient method forconstructing a specialized DPO dataset for training human image generationmodels without the need for costly human feedback. We also propose a modifiedloss function that enhances the DPO training process by minimizing artifactsand improving image fidelity. Our method demonstrates its versatility andeffectiveness in generating human images, including personalized text-to-imagegeneration. Through comprehensive evaluations, we show that our approachsignificantly advances the state of human image generation, achieving superiorresults in terms of natural anatomies, poses, and text-image alignment.</description><author>Sanghyeon Na, Yonggyu Kim, Hyunjoon Lee</author><pubDate>Thu, 30 May 2024 17:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20216v1</guid></item><item><title>TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models</title><link>http://arxiv.org/abs/2405.20215v1</link><description>Mainstream approaches to aligning large language models (LLMs) heavily relyon human preference data, particularly when models require periodic updates.The standard process for iterative alignment of LLMs involves collecting newhuman feedback for each update. However, the data collection process is costlyand challenging to scale. To address this issue, we introduce the "TS-Align"framework, which fine-tunes a policy model using pairwise feedback dataautomatically mined from its outputs. This automatic mining process isefficiently accomplished through the collaboration between a large-scaleteacher model and a small-scale student model. The policy fine-tuning processcan be iteratively repeated using on-policy generations within our proposedteacher-student collaborative framework. Through extensive experiments, wedemonstrate that our final aligned policy outperforms the base policy modelwith an average win rate of 69.7% across seven conversational orinstruction-following datasets. Furthermore, we show that the rankingcapability of the teacher is effectively distilled into the student through ourpipeline, resulting in a small-scale yet effective reward model for policymodel alignment.</description><author>Chen Zhang, Chengguang Tang, Dading Chong, Ke Shi, Guohua Tang, Feng Jiang, Haizhou Li</author><pubDate>Thu, 30 May 2024 17:17:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20215v1</guid></item><item><title>Abstract Weighted Based Gradual Semantics in Argumentation Theory</title><link>http://arxiv.org/abs/2401.11472v2</link><description>Weighted gradual semantics provide an acceptability degree to each argumentrepresenting the strength of the argument, computed based on factors includingbackground evidence for the argument, and taking into account interactionsbetween this argument and others. We introduce four important problems linkinggradual semantics and acceptability degrees. First, we reexamine the inverseproblem, seeking to identify the argument weights of the argumentationframework which lead to a specific final acceptability degree. Second, we askwhether the function mapping between argument weights and acceptability degreesis injective or a homeomorphism onto its image. Third, we ask whether argumentweights can be found when preferences, rather than acceptability degrees forarguments are considered. Fourth, we consider the topology of the space ofvalid acceptability degrees, asking whether "gaps" exist in this space. Whiledifferent gradual semantics have been proposed in the literature, in thispaper, we identify a large family of weighted gradual semantics, calledabstract weighted based gradual semantics. These generalise many of theexisting semantics while maintaining desirable properties such as convergenceto a unique fixed point. We also show that a sub-family of the weighted gradualsemantics, called abstract weighted (L^p,\lambda,\mu)-based gradual semanticsand which include well-known semantics, solve all four of the aforementionedproblems.</description><author>Assaf Libman, Nir Oren, Bruno Yun</author><pubDate>Thu, 30 May 2024 17:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11472v2</guid></item><item><title>PostDoc: Generating Poster from a Long Multimodal Document Using Deep Submodular Optimization</title><link>http://arxiv.org/abs/2405.20213v1</link><description>A poster from a long input document can be considered as a one-pageeasy-to-read multimodal (text and images) summary presented on a nice templatewith good design elements. Automatic transformation of a long document into aposter is a very less studied but challenging task. It involves contentsummarization of the input document followed by template generation andharmonization. In this work, we propose a novel deep submodular function whichcan be trained on ground truth summaries to extract multimodal content from thedocument and explicitly ensures good coverage, diversity and alignment of textand images. Then, we use an LLM based paraphraser and propose to generate atemplate with various design aspects conditioned on the input content. We showthe merits of our approach through extensive automated and human evaluations.</description><author>Vijay Jaisankar, Sambaran Bandyopadhyay, Kalp Vyas, Varre Chaitanya, Shwetha Somasundaram</author><pubDate>Thu, 30 May 2024 17:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20213v1</guid></item><item><title>Jina CLIP: Your CLIP Model Is Also Your Text Retriever</title><link>http://arxiv.org/abs/2405.20204v1</link><description>Contrastive Language-Image Pretraining (CLIP) is widely used to train modelsto align images and texts in a common embedding space by mapping them tofixed-sized vectors. These models are key to multimodal information retrievaland related tasks. However, CLIP models generally underperform in text-onlytasks compared to specialized text models. This creates inefficiencies forinformation retrieval systems that keep separate embeddings and models fortext-only and multimodal tasks. We propose a novel, multi-task contrastivetraining method to address this issue, which we use to train the jina-clip-v1model to achieve the state-of-the-art performance on both text-image andtext-text retrieval tasks.</description><author>Andreas Koukounas, Georgios Mastrapas, Michael Günther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martínez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, Han Xiao</author><pubDate>Thu, 30 May 2024 17:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20204v1</guid></item><item><title>One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments</title><link>http://arxiv.org/abs/2405.20202v1</link><description>Large Language Models (LLMs) have advanced rapidly but face significantmemory demands. While quantization has shown promise for LLMs, current methodstypically require lengthy training to alleviate the performance degradationfrom quantization loss. However, deploying LLMs across diverse scenarios withdifferent resource constraints, e.g., servers and personal computers, requiresrepeated training per application, which amplifies the lengthy trainingproblem. Given that, it is advantageous to train a once-for-all (OFA) supernetcapable of yielding diverse optimal subnets for downstream applications throughone-shot training. Nonetheless, the scale of current language models impedesefficiency and amplifies interference from weight sharing between subnets. Wemake an initial attempt to extend the once-for-all framework to large languagemodels. Specifically, we decouple shared weights to eliminate the interferenceand incorporate Low-Rank adapters for training efficiency. Furthermore, weobserve the imbalance allocation of training resources from the traditionaluniform sampling. A non-parametric scheduler is introduced to adjust thesampling rate for each quantization configuration, achieving a more balancedallocation among subnets with varying demands. We validate the approach onLLaMA2 families, and downstream evaluation confirms our ability to maintainhigh performance while significantly reducing deployment time faced withmultiple scenarios.</description><author>Ke Yi, Yuhui Xu, Heng Chang, Chen Tang, Yuan Meng, Tong Zhang, Jia Li</author><pubDate>Thu, 30 May 2024 17:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20202v1</guid></item><item><title>AnalogCoder: Analog Circuit Design via Training-Free Code Generation</title><link>http://arxiv.org/abs/2405.14918v2</link><description>Analog circuit design is a significant task in modern chip technology,focusing on the selection of component types, connectivity, and parameters toensure proper circuit functionality. Despite advances made by Large LanguageModels (LLMs) in digital circuit design, the complexity and scarcity of data inanalog circuitry pose significant challenges. To mitigate these issues, weintroduce AnalogCoder, the first training-free LLM agent for designing analogcircuits through Python code generation. Firstly, AnalogCoder incorporates afeedback-enhanced flow with tailored domain-specific prompts, enabling theautomated and self-correcting design of analog circuits with a high successrate. Secondly, it proposes a circuit tool library to archive successfuldesigns as reusable modular sub-circuits, simplifying composite circuitcreation. Thirdly, extensive experiments on a benchmark designed to cover awide range of analog circuit tasks show that AnalogCoder outperforms otherLLM-based methods. It has successfully designed 20 circuits, 5 more thanstandard GPT-4o. We believe AnalogCoder can significantly improve thelabor-intensive chip design process, enabling non-experts to design analogcircuits efficiently.</description><author>Yao Lai, Sungyoung Lee, Guojin Chen, Souradip Poddar, Mengkang Hu, David Z. Pan, Ping Luo</author><pubDate>Thu, 30 May 2024 17:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14918v2</guid></item><item><title>Unified Explanations in Machine Learning Models: A Perturbation Approach</title><link>http://arxiv.org/abs/2405.20200v1</link><description>A high-velocity paradigm shift towards Explainable Artificial Intelligence(XAI) has emerged in recent years. Highly complex Machine Learning (ML) modelshave flourished in many tasks of intelligence, and the questions have startedto shift away from traditional metrics of validity towards something deeper:What is this model telling me about my data, and how is it arriving at theseconclusions? Inconsistencies between XAI and modeling techniques can have theundesirable effect of casting doubt upon the efficacy of these explainabilityapproaches. To address these problems, we propose a systematic,perturbation-based analysis against a popular, model-agnostic method in XAI,SHapley Additive exPlanations (Shap). We devise algorithms to generate relativefeature importance in settings of dynamic inference amongst a suite of popularmachine learning and deep learning methods, and metrics that allow us toquantify how well explanations generated under the static case hold. We proposea taxonomy for feature importance methodology, measure alignment, and observequantifiable similarity amongst explanation models across several datasets.</description><author>Jacob Dineen, Don Kridel, Daniel Dolk, David Castillo</author><pubDate>Thu, 30 May 2024 17:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20200v1</guid></item><item><title>Occam Gradient Descent</title><link>http://arxiv.org/abs/2405.20194v1</link><description>Deep learning neural network models must be large enough to adapt to theirproblem domain, while small enough to avoid overfitting training data duringgradient descent. To balance these competing demands, overprovisioned deeplearning models such as transformers are trained for a single epoch on largedata sets, and hence inefficient with both computing resources and trainingdata. In response to these inefficiencies, we exploit learning theory to deriveOccam Gradient Descent, an algorithm that interleaves adaptive reduction ofmodel size to minimize generalization error, with gradient descent on modelweights to minimize fitting error. In contrast, traditional gradient descentgreedily minimizes fitting error without regard to generalization error. Ouralgorithm simultaneously descends the space of weights and topological size ofany neural network without modification, and is effective in our experiments inoutperforming traditional gradient descent with or without post-train pruningin accuracy, compute and model compression.</description><author>B. N. Kausik</author><pubDate>Thu, 30 May 2024 16:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20194v1</guid></item><item><title>TAIA: Large Language Models are Out-of-Distribution Data Learners</title><link>http://arxiv.org/abs/2405.20192v1</link><description>Fine-tuning on task-specific question-answer pairs is a predominant methodfor enhancing the performance of instruction-tuned large language models (LLMs)on downstream tasks. However, in certain specialized domains, such ashealthcare or harmless content generation, it is nearly impossible to obtain alarge volume of high-quality data that matches the downstream distribution. Toimprove the performance of LLMs in data-scarce domains with domain-mismatcheddata, we re-evaluated the Transformer architecture and discovered that not allparameter updates during fine-tuning contribute positively to downstreamperformance. Our analysis reveals that within the self-attention andfeed-forward networks, only the fine-tuned attention parameters areparticularly beneficial when the training set's distribution does not fullyalign with the test set. Based on this insight, we propose an effectiveinference-time intervention method: \uline{T}raining \uline{A}ll parameters but\uline{I}nferring with only \uline{A}ttention (\trainallInfAttn). Weempirically validate \trainallInfAttn using two general instruction-tuningdatasets and evaluate it on seven downstream tasks involving math, reasoning,and knowledge understanding across LLMs of different parameter sizes andfine-tuning techniques. Our comprehensive experiments demonstrate that\trainallInfAttn achieves superior improvements compared to both the fullyfine-tuned model and the base model in most scenarios, with significantperformance gains. The high tolerance of \trainallInfAttn to data mismatchesmakes it resistant to jailbreaking tuning and enhances specialized tasks usinggeneral data.</description><author>Shuyang Jiang, Yusheng Liao, Ya Zhang, Yu Wang, Yanfeng Wang</author><pubDate>Thu, 30 May 2024 16:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20192v1</guid></item><item><title>Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory</title><link>http://arxiv.org/abs/2405.20189v1</link><description>In this work, we describe our approach to developing an intelligent androbust social robotic system for the Nadine social robot platform. We achievethis by integrating Large Language Models (LLMs) and skilfully leveraging thepowerful reasoning and instruction-following capabilities of these types ofmodels to achieve advanced human-like affective and cognitive capabilities.This approach is novel compared to the current state-of-the-art LLM-basedagents which do not implement human-like long-term memory or sophisticatedemotional appraisal. The naturalness of social robots, consisting of multiplemodules, highly depends on the performance and capabilities of each componentof the system and the seamless integration of the components. We built a socialrobot system that enables generating appropriate behaviours through multimodalinput processing, bringing episodic memories accordingly to the recogniseduser, and simulating the emotional states of the robot induced by theinteraction with the human partner. In particular, we introduce an LLM-agentframe for social robots, SoR-ReAct, serving as a core component for theinteraction module in our system. This design has brought forth the advancementof social robots and aims to increase the quality of human-robot interaction.</description><author>Hangyeol Kang, Maher Ben Moussa, Nadia Magnenat-Thalmann</author><pubDate>Thu, 30 May 2024 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20189v1</guid></item><item><title>SPARE: Symmetrized Point-to-Plane Distance for Robust Non-Rigid Registration</title><link>http://arxiv.org/abs/2405.20188v1</link><description>Existing optimization-based methods for non-rigid registration typicallyminimize an alignment error metric based on the point-to-point orpoint-to-plane distance between corresponding point pairs on the source surfaceand target surface. However, these metrics can result in slow convergence or aloss of detail. In this paper, we propose SPARE, a novel formulation thatutilizes a symmetrized point-to-plane distance for robust non-rigidregistration. The symmetrized point-to-plane distance relies on both thepositions and normals of the corresponding points, resulting in a more accurateapproximation of the underlying geometry and can achieve higher accuracy thanexisting methods. To solve this optimization problem efficiently, we propose analternating minimization solver using a majorization-minimization strategy.Moreover, for effective initialization of the solver, we incorporate adeformation graph-based coarse alignment that improves registration quality andefficiency. Extensive experiments show that the proposed method greatlyimproves the accuracy of non-rigid registration problems and maintainsrelatively high solution efficiency. The code is publicly available athttps://github.com/yaoyx689/spare.</description><author>Yuxin Yao, Bailin Deng, Junhui Hou, Juyong Zhang</author><pubDate>Thu, 30 May 2024 16:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20188v1</guid></item><item><title>A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models</title><link>http://arxiv.org/abs/2405.20183v1</link><description>This paper analyzes Large Language Models (LLMs) with regard to theirprogramming exercise generation capabilities. Through a survey study, wedefined the state of the art, extracted their strengths and weaknesses andfinally proposed an evaluation matrix, helping researchers and educators todecide which LLM is the best fitting for the programming exercise generationuse case. We also found that multiple LLMs are capable of producing usefulprogramming exercises. Nevertheless, there exist challenges like the ease withwhich LLMs might solve exercises generated by LLMs. This paper contributes tothe ongoing discourse on the integration of LLMs in education.</description><author>Eduard Frankford, Ingo Höhn, Clemens Sauerwein, Ruth Breu</author><pubDate>Thu, 30 May 2024 16:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20183v1</guid></item><item><title>Transformers and Slot Encoding for Sample Efficient Physical World Modelling</title><link>http://arxiv.org/abs/2405.20180v1</link><description>World modelling, i.e. building a representation of the rules that govern theworld so as to predict its evolution, is an essential ability for any agentinteracting with the physical world. Recent applications of the Transformerarchitecture to the problem of world modelling from video input show notableimprovements in sample efficiency. However, existing approaches tend to workonly at the image level thus disregarding that the environment is composed ofobjects interacting with each other. In this paper, we propose an architecturecombining Transformers for world modelling with the slot-attention paradigm, anapproach for learning representations of objects appearing in a scene. Wedescribe the resulting neural architecture and report experimental resultsshowing an improvement over the existing solutions in terms of sampleefficiency and a reduction of the variation of the performance over thetraining examples. The code for our architecture and experiments is availableat https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm</description><author>Francesco Petri, Luigi Asprino, Aldo Gangemi</author><pubDate>Thu, 30 May 2024 16:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20180v1</guid></item><item><title>Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs</title><link>http://arxiv.org/abs/2405.20179v1</link><description>Large language models (LLMs) have shown great promise at generating robotprograms from natural language given domain-specific robot applicationprogramming interfaces (APIs). However, the performance gap between proprietaryLLMs and smaller open-weight LLMs remains wide. This raises a question: Can wefine-tune smaller open-weight LLMs for generating domain-specific robotprograms to close the performance gap with proprietary LLMs? WhileSelf-Instruct is a promising solution by generating a diverse set of trainingdata, it cannot verify the correctness of these programs. In contrast, a robotsimulator with a well-defined world can identify execution errors but limitsthe diversity of programs that it can verify. In this work, we introduceRobo-Instruct, which brings the best of both worlds -- it promotes thediversity of Self-Instruct while providing the correctness of simulator-basedchecking. Robo-Instruct introduces RoboSim to synthesize a consistent worldstate on the fly by inferring properties relevant to the program being checked,and simulating actions accordingly. Furthermore, the instructions and programsgenerated by Self-Instruct may be subtly inconsistent -- such as the programmissing a step implied by the instruction. Robo-Instruct further addresses thiswith InstAlign, an instruction-program alignment procedure that revises thetask instruction to reflect the actual results of the generated program. Givena few seed task descriptions and the robot APIs, Robo-Instruct is capable ofgenerating a training dataset using only a small open-weight model. Thisdataset can then be used to fine-tune small open-weight language models,enabling them to match or even exceed the performance of several proprietaryLLMs, such as GPT-3.5-Turbo and Gemini-Pro.</description><author>Zichao Hu, Junyi Jessy Li, Arjun Guha, Joydeep Biswas</author><pubDate>Thu, 30 May 2024 16:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20179v1</guid></item><item><title>Non-intrusive data-driven model order reduction for circuits based on Hammerstein architectures</title><link>http://arxiv.org/abs/2405.20178v1</link><description>We demonstrate that data-driven system identification techniques can providea basis for effective, non-intrusive model order reduction (MOR) for commoncircuits that are key building blocks in microelectronics. Our approach ismotivated by the practical operation of these circuits and utilizes a canonicalHammerstein architecture. To demonstrate the approach we develop a parsimoniousHammerstein model for a non-linear CMOS differential amplifier. We train thismodel on a combination of direct current (DC) and transient Spice (Xyce)circuit simulation data using a novel sequential strategy to identify thestatic nonlinear and linear dynamical parts of the model. Simulation resultsshow that the Hammerstein model is an effective surrogate for the differentialamplifier circuit that accurately and efficiently reproduces its behavior overa wide range of operating points and input frequencies.</description><author>Joshua Hanson, Biliana Paskaleva, Pavel Bochev</author><pubDate>Thu, 30 May 2024 16:47:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20178v1</guid></item><item><title>Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources</title><link>http://arxiv.org/abs/2402.11505v2</link><description>Federated Learning (FL) has recently been applied to the parameter-efficientfine-tuning of Large Language Models (LLMs). While promising, it raisessignificant challenges due to the heterogeneous resources and datadistributions of clients. This study introduces FlexLoRA, a simple yeteffective aggregation scheme for LLM fine-tuning, which mitigates the ``bucketeffect'' in traditional FL that restricts the potential of clients with ampleresources by tying them to the capabilities of the least-resourcedparticipants. FlexLoRA allows for dynamic adjustment of local LoRA ranks,fostering the development of a global model imbued with broader, lesstask-specific knowledge. By synthesizing a full-size LoRA weight fromindividual client contributions and employing Singular Value Decomposition(SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous clientresources. Involving thousands of clients performing heterogeneous NLP tasksand client resources, our experiments validate the efficacy of FlexLoRA, withthe federated global model achieving consistently better improvement over SOTAFL methods in downstream NLP task performance across various heterogeneousdistributions. FlexLoRA's practicality is further underscored by ourtheoretical analysis and its seamless integration with existing LoRA-based FLmethods, offering a path toward cross-device, privacy-preserving federatedtuning for LLMs.</description><author>Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, Yaliang Li</author><pubDate>Thu, 30 May 2024 16:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11505v2</guid></item><item><title>InstructionCP: A fast approach to transfer Large Language Models into target language</title><link>http://arxiv.org/abs/2405.20175v1</link><description>The rapid development of large language models (LLMs) in recent years haslargely focused on English, resulting in models that respond exclusively inEnglish. To adapt these models to other languages, continual pre-training (CP)is often employed, followed by supervised fine-tuning (SFT) to maintainconversational abilities. However, CP and SFT can reduce a model's ability tofilter harmful content. We propose Instruction Continual Pre-training (InsCP),which integrates instruction tags into the CP process to prevent loss ofconversational proficiency while acquiring new languages. Our experimentsdemonstrate that InsCP retains conversational and Reinforcement Learning fromHuman Feedback (RLHF) abilities. Empirical evaluations on language alignment,reliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably,this approach requires only 0.1 billion tokens of high-qualityinstruction-following data, thereby reducing resource consumption.</description><author>Kuang-Ming Chen, Hung-yi Lee</author><pubDate>Thu, 30 May 2024 16:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20175v1</guid></item><item><title>Tropical Expressivity of Neural Networks</title><link>http://arxiv.org/abs/2405.20174v1</link><description>We propose an algebraic geometric framework to study the expressivity oflinear activation neural networks. A particular quantity that has been activelystudied in the field of deep learning is the number of linear regions, whichgives an estimate of the information capacity of the architecture. To study andevaluate information capacity and expressivity, we work in the setting oftropical geometry -- a combinatorial and polyhedral variant of algebraicgeometry -- where there are known connections between tropical rational mapsand feedforward neural networks. Our work builds on and expands this connectionto capitalize on the rich theory of tropical geometry to characterize and studyvarious architectural aspects of neural networks. Our contributions arethreefold: we provide a novel tropical geometric approach to selecting samplingdomains among linear regions; an algebraic result allowing for a guidedrestriction of the sampling domain for network architectures with symmetries;and an open source library to analyze neural networks as tropical Puiseuxrational maps. We provide a comprehensive set of proof-of-concept numericalexperiments demonstrating the breadth of neural network architectures to whichtropical geometric theory can be applied to reveal insights on expressivitycharacteristics of a network. Our work provides the foundations for theadaptation of both theory and existing software from computational tropicalgeometry and symbolic computation to deep learning.</description><author>Shiv Bhatia, Yueqi Cao, Paul Lezeau, Anthea Monod</author><pubDate>Thu, 30 May 2024 16:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20174v1</guid></item><item><title>Machine Unlearning of Pre-trained Large Language Models</title><link>http://arxiv.org/abs/2402.15159v3</link><description>This study investigates the concept of the `right to be forgotten' within thecontext of large language models (LLMs). We explore machine unlearning as apivotal solution, with a focus on pre-trained models--a notablyunder-researched area. Our research delineates a comprehensive framework formachine unlearning in pre-trained LLMs, encompassing a critical analysis ofseven diverse unlearning methods. Through rigorous evaluation using curateddatasets from arXiv, books, and GitHub, we establish a robust benchmark forunlearning performance, demonstrating that these methods are over $10^5$ timesmore computationally efficient than retraining. Our results show thatintegrating gradient ascent with gradient descent on in-distribution dataimproves hyperparameter robustness. We also provide detailed guidelines forefficient hyperparameter tuning in the unlearning process. Our findings advancethe discourse on ethical AI practices, offering substantive insights into themechanics of machine unlearning for pre-trained LLMs and underscoring thepotential for responsible AI development.</description><author>Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue</author><pubDate>Thu, 30 May 2024 16:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15159v3</guid></item><item><title>Cheap Talking Algorithms</title><link>http://arxiv.org/abs/2310.07867v4</link><description>We simulate behaviour of two independent reinforcement learning algorithmsplaying the Crawford and Sobel (1982) game of strategic informationtransmission. We adopt memoryless algorithms to capture learning in a staticgame where a large population interacts anonymously. We show that sender andreceiver converge to Nash equilibrium play. The level of informativeness of thesender's cheap talk decreases as the bias increases and, at intermediate levelof the bias, it matches the level predicted by the Pareto optimal equilibriumor by the second best one. Conclusions are robust to alternative specificationsof the learning hyperparameters and of the game.</description><author>Daniele Condorelli, Massimiliano Furlan</author><pubDate>Thu, 30 May 2024 16:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07867v4</guid></item></channel></rss>