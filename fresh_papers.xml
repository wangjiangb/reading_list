<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 07 Aug 2023 06:00:44 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</title><link>http://arxiv.org/abs/2308.02490v1</link><description>We propose MM-Vet, an evaluation benchmark that examines large multimodalmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown variousintriguing abilities, such as solving math problems written on the blackboard,reasoning about events and celebrities in news images, and explaining visualjokes. Rapid model advancements pose challenges to evaluation benchmarkdevelopment. Problems include: (1) How to systematically structure and evaluatethe complicated multimodal tasks; (2) How to design evaluation metrics thatwork well across question and answer types; and (3) How to give model insightsbeyond a simple performance ranking. To this end, we present MM-Vet, designedbased on the insight that the intriguing ability to solve complicated tasks isoften achieved by a generalist model being able to integrate different corevision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities andexamines the 16 integrations of interest derived from the capabilitycombination. For evaluation metrics, we propose an LLM-based evaluator foropen-ended outputs. The evaluator enables the evaluation across differentquestion types and answer styles, resulting in a unified scoring metric. Weevaluate representative LMMs on MM-Vet, providing insights into thecapabilities of different LMM system paradigms and models. Code and data areavailable at https://github.com/yuweihao/MM-Vet.</description><author>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang</author><pubDate>Fri, 04 Aug 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02490v1</guid></item><item><title>Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP</title><link>http://arxiv.org/abs/2308.02487v1</link><description>Open-vocabulary segmentation is a challenging task requiring segmenting andrecognizing objects from an open set of categories. One way to address thischallenge is to leverage multi-modal models, such as CLIP, to provide image andtext features in a shared embedding space, which bridges the gap betweenclosed-vocabulary and open-vocabulary recognition. Hence, existing methodsoften adopt a two-stage framework to tackle the problem, where the inputs firstgo through a mask generator and then through the CLIP model along with thepredicted masks. This process involves extracting features from images multipletimes, which can be ineffective and inefficient. By contrast, we propose tobuild everything into a single-stage framework using a shared FrozenConvolutional CLIP backbone, which not only significantly simplifies thecurrent two-stage pipeline, but also remarkably yields a better accuracy-costtrade-off. The proposed FC-CLIP, benefits from the following observations: thefrozen CLIP backbone maintains the ability of open-vocabulary classificationand can also serve as a strong mask generator, and the convolutional CLIPgeneralizes well to a larger input resolution than the one used duringcontrastive image-text pretraining. When training on COCO panoptic data onlyand testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2mIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoUon ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,respectively. Additionally, the training and testing time of FC-CLIP is 7.5xand 6.6x significantly faster than the same prior art, while using 5.9x fewerparameters. FC-CLIP also sets a new state-of-the-art performance across variousopen-vocabulary semantic segmentation datasets. Code athttps://github.com/bytedance/fc-clip</description><author>Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, Liang-Chieh Chen</author><pubDate>Fri, 04 Aug 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02487v1</guid></item><item><title>Adapting the NICT-JLE Corpus for Disfluency Detection Models</title><link>http://arxiv.org/abs/2308.02482v1</link><description>The detection of disfluencies such as hesitations, repetitions and falsestarts commonly found in speech is a widely studied area of research. With astandardised process for evaluation using the Switchboard Corpus, modelperformance can be easily compared across approaches. This is not the case fordisfluency detection research on learner speech, however, where such datasetshave restricted access policies, making comparison and subsequent developmentof improved models more challenging. To address this issue, this paperdescribes the adaptation of the NICT-JLE corpus, containing approximately 300hours of English learners' oral proficiency tests, to a format that is suitablefor disfluency detection model training and evaluation. Points of differencebetween the NICT-JLE and Switchboard corpora are explored, followed by adetailed overview of adaptations to the tag set and meta-features of theNICT-JLE corpus. The result of this work provides a standardised train, heldoutand test set for use in future research on disfluency detection for learnerspeech.</description><author>Lucy Skidmore, Roger K. Moore</author><pubDate>Fri, 04 Aug 2023 18:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02482v1</guid></item><item><title>Inference-Based Quantum Sensing</title><link>http://arxiv.org/abs/2206.09919v2</link><description>In a standard Quantum Sensing (QS) task one aims at estimating an unknownparameter $\theta$, encoded into an $n$-qubit probe state, via measurements ofthe system. The success of this task hinges on the ability to correlate changesin the parameter to changes in the system response $\mathcal{R}(\theta)$ (i.e.,changes in the measurement outcomes). For simple cases the form of$\mathcal{R}(\theta)$ is known, but the same cannot be said for realisticscenarios, as no general closed-form expression exists. In this work we presentan inference-based scheme for QS. We show that, for a general class of unitaryfamilies of encoding, $\mathcal{R}(\theta)$ can be fully characterized by onlymeasuring the system response at $2n+1$ parameters. This allows us to infer thevalue of an unknown parameter given the measured response, as well as todetermine the sensitivity of the scheme, which characterizes its overallperformance. We show that inference error is, with high probability, smallerthan $\delta$, if one measures the system response with a number of shots thatscales only as $\Omega(\log^3(n)/\delta^2)$. Furthermore, the frameworkpresented can be broadly applied as it remains valid for arbitrary probe statesand measurement schemes, and, even holds in the presence of quantum noise. Wealso discuss how to extend our results beyond unitary families. Finally, toshowcase our method we implement it for a QS task on real quantum hardware, andin numerical simulations.</description><author>C. Huerta Alderete, Max Hunter Gordon, Frederic Sauvage, Akira Sone, Andrew T. Sornborger, Patrick J. Coles, M. Cerezo</author><pubDate>Fri, 04 Aug 2023 18:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.09919v2</guid></item><item><title>Grounded Image Text Matching with Mismatched Relation Reasoning</title><link>http://arxiv.org/abs/2308.01236v2</link><description>This paper introduces Grounded Image Text Matching with Mismatched Relation(GITM-MR), a novel visual-linguistic joint task that evaluates the relationunderstanding capabilities of transformer-based pre-trained models. GITM-MRrequires a model to first determine if an expression describes an image, thenlocalize referred objects or ground the mismatched parts of the text. Weprovide a benchmark for evaluating pre-trained models on this task, with afocus on the challenging settings of limited data and out-of-distributionsentence lengths. Our evaluation demonstrates that pre-trained models lack dataefficiency and length generalization ability. To address this, we propose theRelation-sensitive Correspondence Reasoning Network (RCRN), which incorporatesrelation-aware reasoning via bi-directional message propagation guided bylanguage structure. RCRN can be interpreted as a modular program and deliversstrong performance in both length generalization and data efficiency.</description><author>Yu Wu, Yana Wei, Haozhe Wang, Yongfei Liu, Sibei Yang, Xuming He</author><pubDate>Fri, 04 Aug 2023 18:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01236v2</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v3</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation. The github repository can be foundhere https://github.com/IntelLabs/baa-ngp.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Fri, 04 Aug 2023 18:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v3</guid></item><item><title>Enhancing Clinical Support for Breast Cancer with Deep Learning Models using Synthetic Correlated Diffusion Imaging</title><link>http://arxiv.org/abs/2211.05308v2</link><description>Breast cancer is the second most common type of cancer in women in Canada andthe United States, representing over 25\% of all new female cancer cases. Assuch, there has been immense research and progress on improving screening andclinical support for breast cancer. In this paper, we investigate enhancingclinical support for breast cancer with deep learning models using a newlyintroduced magnetic resonance imaging (MRI) modality called syntheticcorrelated diffusion imaging (CDI$^s$). More specifically, we leverage avolumetric convolutional neural network to learn volumetric deep radiomicfeatures from a pre-treatment cohort and construct a predictor based on thelearnt features for grade and post-treatment response prediction. As the firststudy to learn CDI$^s$-centric radiomic sequences within a deep learningperspective for clinical decision support, we evaluated the proposed approachusing the ACRIN-6698 study against those learnt using gold-standard imagingmodalities. We find that the proposed approach can achieve better performancefor both grade and post-treatment response prediction and thus may be a usefultool to aid oncologists in improving recommendation of treatment of patients.Subsequently, the approach to leverage volumetric deep radiomic features forbreast cancer can be further extended to other applications of CDI$^s$ in thecancer domain to further improve clinical support.</description><author>Chi-en Amy Tai, Hayden Gunraj, Nedim Hodzic, Nic Flanagan, Ali Sabri, Alexander Wong</author><pubDate>Fri, 04 Aug 2023 18:13:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05308v2</guid></item><item><title>GraphCast: Learning skillful medium-range global weather forecasting</title><link>http://arxiv.org/abs/2212.12794v2</link><description>Global medium-range weather forecasting is critical to decision-making acrossmany social and economic domains. Traditional numerical weather prediction usesincreased compute resources to improve forecast accuracy, but cannot directlyuse historical weather data to improve the underlying model. We introduce amachine learning-based method called "GraphCast", which can be trained directlyfrom reanalysis data. It predicts hundreds of weather variables, over 10 daysat 0.25 degree resolution globally, in under one minute. We show that GraphCastsignificantly outperforms the most accurate operational deterministic systemson 90% of 1380 verification targets, and its forecasts support better severeevent prediction, including tropical cyclones, atmospheric rivers, and extremetemperatures. GraphCast is a key advance in accurate and efficient weatherforecasting, and helps realize the promise of machine learning for modelingcomplex dynamical systems.</description><author>Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, Peter Battaglia</author><pubDate>Fri, 04 Aug 2023 18:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12794v2</guid></item><item><title>BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks</title><link>http://arxiv.org/abs/2308.02465v1</link><description>Federated learning enables collaborative training of machine learning modelsby keeping the raw data of the involved workers private. One of its mainobjectives is to improve the models' privacy, security, and scalability.Vertical Federated Learning (VFL) offers an efficient cross-silo setting wherea few parties collaboratively train a model without sharing the same features.In such a scenario, classification labels are commonly considered sensitiveinformation held exclusively by one (active) party, while other (passive)parties use only their local information. Recent works have uncovered importantflaws of VFL, leading to possible label inference attacks under the assumptionthat the attacker has some, even limited, background knowledge on the relationbetween labels and data. In this work, we are the first (to the best of ourknowledge) to investigate label inference attacks on VFL using azero-background knowledge strategy. To concretely formulate our proposal, wefocus on Graph Neural Networks (GNNs) as a target model for the underlying VFL.In particular, we refer to node classification tasks, which are widely studied,and GNNs have shown promising results. Our proposed attack, BlindSage, providesimpressive results in the experiments, achieving nearly 100% accuracy in mostcases. Even when the attacker has no information about the used architecture orthe number of classes, the accuracy remained above 85% in most instances.Finally, we observe that well-known defenses cannot mitigate our attack withoutaffecting the model's performance on the main classification task.</description><author>Marco Arazzi, Mauro Conti, Stefanos Koffas, Marina Krcek, Antonino Nocera, Stjepan Picek, Jing Xu</author><pubDate>Fri, 04 Aug 2023 18:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02465v1</guid></item><item><title>Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing</title><link>http://arxiv.org/abs/2308.02464v1</link><description>Recurrent neural networks (RNNs) are known to be universal approximators ofdynamic systems under fairly mild and general assumptions, making them goodtools to process temporal information. However, RNNs usually suffer from theissues of vanishing and exploding gradients in the standard RNN training.Reservoir computing (RC), a special RNN where the recurrent weights arerandomized and left untrained, has been introduced to overcome these issues andhas demonstrated superior empirical performance in fields as diverse as naturallanguage processing and wireless communications especially in scenarios wheretraining samples are extremely limited. On the contrary, the theoreticalgrounding to support this observed performance has not been fully developed atthe same pace. In this work, we show that RNNs can provide universalapproximation of linear time-invariant (LTI) systems. Specifically, we showthat RC can universally approximate a general LTI system. We present a clearsignal processing interpretation of RC and utilize this understanding in theproblem of simulating a generic LTI system through RC. Under this setup, weanalytically characterize the optimal probability distribution function forgenerating the recurrent weights of the underlying RNN of the RC. We provideextensive numerical evaluations to validate the optimality of the derivedoptimum distribution of the recurrent weights of the RC for the LTI systemsimulation problem. Our work results in clear signal processing-based modelinterpretability of RC and provides theoretical explanation for the power ofrandomness in setting instead of training RC's recurrent weights. It furtherprovides a complete optimum analytical characterization for the untrainedrecurrent weights, marking an important step towards explainable machinelearning (XML) which is extremely important for applications where trainingsamples are limited.</description><author>Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu</author><pubDate>Fri, 04 Aug 2023 18:04:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02464v1</guid></item><item><title>Towards Generalist Foundation Model for Radiology</title><link>http://arxiv.org/abs/2308.02463v1</link><description>In this study, we aim to initiate the development of Radiology FoundationModel, termed as RadFM.We consider the construction of foundational models fromthe perspectives of data, model design, and evaluation thoroughly. Ourcontribution can be concluded as follows: (i), we construct a large-scaleMedical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.To the best of our knowledge, this is the first multi-modal dataset containing3D medical scans. (ii), We propose an architecture that enables visuallyconditioned generative pre-training, allowing for the integration of text inputinterleaved with 2D or 3D medical scans to generate response for diverseradiologic tasks. The model was initially pre-trained on MedMD and subsequentlydomain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,containing 3M radiologic visual-language pairs. (iii), we propose a newevaluation benchmark that comprises five tasks, aiming to comprehensivelyassess the capability of foundation models in handling practical clinicalproblems. Our experimental results confirm that RadFM significantly outperformsexisting multi-modal foundation models. The codes, data, and model checkpointwill all be made publicly available to promote further research and developmentin the field.</description><author>Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Fri, 04 Aug 2023 18:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02463v1</guid></item><item><title>Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning</title><link>http://arxiv.org/abs/2308.02462v1</link><description>One predominant challenge in additive manufacturing (AM) is to achievespecific material properties by manipulating manufacturing process parametersduring the runtime. Such manipulation tends to increase the computational loadimposed on existing simulation tools employed in AM. The goal of the presentwork is to construct a fast and accurate reduced-order model (ROM) for an AMmodel developed within the Multiphysics Object-Oriented Simulation Environment(MOOSE) framework, ultimately reducing the time/cost of AM control andoptimization processes. Our adoption of the operator learning (OL) approachenabled us to learn a family of differential equations produced by alteringprocess variables in the laser's Gaussian point heat source. More specifically,we used the Fourier neural operator (FNO) and deep operator network (DeepONet)to develop ROMs for time-dependent responses. Furthermore, we benchmarked theperformance of these OL methods against a conventional deep neural network(DNN)-based ROM. Ultimately, we found that OL methods offer comparableperformance and, in terms of accuracy and generalizability, even outperform DNNat predicting scalar model responses. The DNN-based ROM afforded the fastesttraining time. Furthermore, all the ROMs were faster than the original MOOSEmodel yet still provided accurate predictions. FNO had a smaller meanprediction error than DeepONet, with a larger variance for time-dependentresponses. Unlike DNN, both FNO and DeepONet were able to simulate time seriesdata without the need for dimensionality reduction techniques. The present workcan help facilitate the AM optimization process by enabling faster execution ofsimulation tools while still preserving evaluation accuracy.</description><author>Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</author><pubDate>Fri, 04 Aug 2023 18:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02462v1</guid></item><item><title>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration</title><link>http://arxiv.org/abs/2308.02459v1</link><description>Developing robot controllers capable of achieving dexterous nonprehensilemanipulation, such as pushing an object on a table, is challenging. Theunderactuated and hybrid-dynamics nature of the problem, further complicated bythe uncertainty resulting from the frictional interactions, requiressophisticated control behaviors. Reinforcement Learning (RL) is a powerfulframework for developing such robot controllers. However, previous RLliterature addressing the nonprehensile pushing task achieves low accuracy,non-smooth trajectories, and only simple motions, i.e. without rotation of themanipulated object. We conjecture that previously used unimodal explorationstrategies fail to capture the inherent hybrid-dynamics of the task, arisingfrom the different possible contact interaction modes between the robot and theobject, such as sticking, sliding, and separation. In this work, we propose amultimodal exploration approach through categorical distributions, whichenables us to train planar pushing RL policies for arbitrary starting andtarget object poses, i.e. positions and orientations, and with improvedaccuracy. We show that the learned policies are robust to external disturbancesand observation noise, and scale to tasks with multiple pushers. Furthermore,we validate the transferability of the learned policies, trained entirely insimulation, to a physical robot hardware using the KUKA iiwa robot arm. See oursupplemental video: https://youtu.be/vTdva1mgrk4.</description><author>Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar</author><pubDate>Fri, 04 Aug 2023 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02459v1</guid></item><item><title>A Survey on Temporal Knowledge Graph Completion: Taxonomy, Progress, and Prospects</title><link>http://arxiv.org/abs/2308.02457v1</link><description>Temporal characteristics are prominently evident in a substantial volume ofknowledge, which underscores the pivotal role of Temporal Knowledge Graphs(TKGs) in both academia and industry. However, TKGs often suffer fromincompleteness for three main reasons: the continuous emergence of newknowledge, the weakness of the algorithm for extracting structured informationfrom unstructured data, and the lack of information in the source dataset.Thus, the task of Temporal Knowledge Graph Completion (TKGC) has attractedincreasing attention, aiming to predict missing items based on the availableinformation. In this paper, we provide a comprehensive review of TKGC methodsand their details. Specifically, this paper mainly consists of threecomponents, namely, 1)Background, which covers the preliminaries of TKGCmethods, loss functions required for training, as well as the dataset andevaluation protocol; 2)Interpolation, that estimates and predicts the missingelements or set of elements through the relevant available information. Itfurther categorizes related TKGC methods based on how to process temporalinformation; 3)Extrapolation, which typically focuses on continuous TKGs andpredicts future events, and then classifies all extrapolation methods based onthe algorithms they utilize. We further pinpoint the challenges and discussfuture research directions of TKGC.</description><author>Jiapu Wang, Boyue Wang, Meikang Qiu, Shirui Pan, Bo Xiong, Heng Liu, Linhao Luo, Tengfei Liu, Yongli Hu, Baocai Yin, Wen Gao</author><pubDate>Fri, 04 Aug 2023 17:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02457v1</guid></item><item><title>Federated Deep Learning for Intrusion Detection in IoT Networks</title><link>http://arxiv.org/abs/2306.02715v3</link><description>The vast increase of Internet of Things (IoT) technologies and theever-evolving attack vectors have increased cyber-security risks dramatically.A common approach to implementing AI-based Intrusion Detection systems (IDSs)in distributed IoT systems is in a centralised manner. However, this approachmay violate data privacy and prohibit IDS scalability. Therefore, intrusiondetection solutions in IoT ecosystems need to move towards a decentraliseddirection. Federated Learning (FL) has attracted significant interest in recentyears due to its ability to perform collaborative learning while preservingdata confidentiality and locality. Nevertheless, most FL-based IDS for IoTsystems are designed under unrealistic data distribution conditions. To thatend, we design an experiment representative of the real world and evaluate theperformance of an FL-based IDS. For our experiments, we rely on TON-IoT, arealistic IoT network traffic dataset, associating each IP address with asingle FL client. Additionally, we explore pre-training and investigate variousaggregation methods to mitigate the impact of data heterogeneity. Lastly, webenchmark our approach against a centralised solution. The comparison showsthat the heterogeneous nature of the data has a considerable negative impact onthe model's performance when trained in a distributed manner. However, in thecase of a pre-trained initial global FL model, we demonstrate a performanceimprovement of over 20% (F1-score) compared to a randomly initiated globalmodel.</description><author>Othmane Belarbi, Theodoros Spyridopoulos, Eirini Anthi, Ioannis Mavromatis, Pietro Carnelli, Aftab Khan</author><pubDate>Fri, 04 Aug 2023 17:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02715v3</guid></item><item><title>Generative Modelling of Lévy Area for High Order SDE Simulation</title><link>http://arxiv.org/abs/2308.02452v1</link><description>It is well known that, when numerically simulating solutions to SDEs,achieving a strong convergence rate better than O(\sqrt{h}) (where h is thestep size) requires the use of certain iterated integrals of Brownian motion,commonly referred to as its "L\'{e}vy areas". However, these stochasticintegrals are difficult to simulate due to their non-Gaussian nature and for ad-dimensional Brownian motion with d &gt; 2, no fast almost-exact samplingalgorithm is known. In this paper, we propose L\'{e}vyGAN, a deep-learning-based model forgenerating approximate samples of L\'{e}vy area conditional on a Brownianincrement. Due to our "Bridge-flipping" operation, the output samples match alljoint and conditional odd moments exactly. Our generator employs a tailoredGNN-inspired architecture, which enforces the correct dependency structurebetween the output distribution and the conditioning variable. Furthermore, weincorporate a mathematically principled characteristic-function baseddiscriminator. Lastly, we introduce a novel training mechanism termed"Chen-training", which circumvents the need for expensive-to-generate trainingdata-sets. This new training procedure is underpinned by our two maintheoretical results. For 4-dimensional Brownian motion, we show that L\'{e}vyGAN exhibitsstate-of-the-art performance across several metrics which measure both thejoint and marginal distributions. We conclude with a numerical experiment onthe log-Heston model, a popular SDE in mathematical finance, demonstrating thathigh-quality synthetic L\'{e}vy area can lead to high order weak convergenceand variance reduction when using multilevel Monte Carlo (MLMC).</description><author>Andraž Jelinčič, Jiajie Tao, William F. Turner, Thomas Cass, James Foster, Hao Ni</author><pubDate>Fri, 04 Aug 2023 17:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02452v1</guid></item><item><title>Pruning a neural network using Bayesian inference</title><link>http://arxiv.org/abs/2308.02451v1</link><description>Neural network pruning is a highly effective technique aimed at reducing thecomputational and memory demands of large neural networks. In this researchpaper, we present a novel approach to pruning neural networks utilizingBayesian inference, which can seamlessly integrate into the training procedure.Our proposed method leverages the posterior probabilities of the neural networkprior to and following pruning, enabling the calculation of Bayes factors. Thecalculated Bayes factors guide the iterative pruning. Through comprehensiveevaluations conducted on multiple benchmarks, we demonstrate that our methodachieves desired levels of sparsity while maintaining competitive accuracy.</description><author>Sunil Mathew, Daniel B. Rowe</author><pubDate>Fri, 04 Aug 2023 17:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02451v1</guid></item><item><title>DiSProD: Differentiable Symbolic Propagation of Distributions for Planning</title><link>http://arxiv.org/abs/2302.01491v4</link><description>The paper introduces DiSProD, an online planner developed for environmentswith probabilistic transitions in continuous state and action spaces. DiSProDbuilds a symbolic graph that captures the distribution of future trajectories,conditioned on a given policy, using independence assumptions and approximatepropagation of distributions. The symbolic graph provides a differentiablerepresentation of the policy's value, enabling efficient gradient-basedoptimization for long-horizon search. The propagation of approximatedistributions can be seen as an aggregation of many trajectories, making itwell-suited for dealing with sparse rewards and stochastic environments. Anextensive experimental evaluation compares DiSProD to state-of-the-art plannersin discrete-time planning and real-time control of robotic systems. Theproposed method improves over existing planners in handling stochasticenvironments, sensitivity to search depth, sparsity of rewards, and largeaction spaces. Additional real-world experiments demonstrate that DiSProD cancontrol ground vehicles and surface vessels to successfully navigate aroundobstacles.</description><author>Palash Chatterjee, Ashutosh Chapagain, Weizhe Chen, Roni Khardon</author><pubDate>Fri, 04 Aug 2023 17:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01491v4</guid></item><item><title>From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence</title><link>http://arxiv.org/abs/2308.02448v1</link><description>In 2020, the U.S. Department of Defense officially disclosed a set of ethicalprinciples to guide the use of Artificial Intelligence (AI) technologies onfuture battlefields. Despite stark differences, there are core similaritiesbetween the military and medical service. Warriors on battlefields often facelife-altering circumstances that require quick decision-making. Medicalproviders experience similar challenges in a rapidly changing healthcareenvironment, such as in the emergency department or during surgery treating alife-threatening condition. Generative AI, an emerging technology designed toefficiently generate valuable information, holds great promise. As computingpower becomes more accessible and the abundance of health data, such aselectronic health records, electrocardiograms, and medical images, increases,it is inevitable that healthcare will be revolutionized by this technology.Recently, generative AI has captivated the research community, leading todebates about its application in healthcare, mainly due to concerns abouttransparency and related issues. Meanwhile, concerns about the potentialexacerbation of health disparities due to modeling biases have raised notableethical concerns regarding the use of this technology in healthcare. However,the ethical principles for generative AI in healthcare have been understudied,and decision-makers often fail to consider the significance of generative AI.In this paper, we propose GREAT PLEA ethical principles, encompassinggovernance, reliability, equity, accountability, traceability, privacy,lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim toproactively address the ethical dilemmas and challenges posed by theintegration of generative AI in healthcare.</description><author>David Oniani, Jordan Hilsman, Yifan Peng, COL, Ronald K. Poropatich, COL Jeremy C. Pamplin, LTC Gary L. Legault, Yanshan Wang</author><pubDate>Fri, 04 Aug 2023 17:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02448v1</guid></item><item><title>Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation</title><link>http://arxiv.org/abs/2303.17910v2</link><description>Benefiting from the sequence-level knowledge distillation, theNon-Autoregressive Transformer (NAT) achieves great success in neural machinetranslation tasks. However, existing knowledge distillation has side effects,such as propagating errors from the teacher to NAT students, which may limitfurther improvements of NAT models and are rarely discussed in existingresearch. In this paper, we introduce selective knowledge distillation byintroducing an NAT evaluator to select NAT-friendly targets that are of highquality and easy to learn. In addition, we introduce a simple yet effectiveprogressive distillation method to boost NAT performance. Experiment results onmultiple WMT language directions and several representative NAT models showthat our approach can realize a flexible trade-off between the quality andcomplexity of training data for NAT models, achieving strong performances.Further analysis shows that distilling only 5% of the raw translations can helpan NAT outperform its counterpart trained on raw data by about 2.4 BLEU.</description><author>Min Liu, Yu Bao, Chengqi Zhao, Shujian Huang</author><pubDate>Fri, 04 Aug 2023 17:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17910v2</guid></item><item><title>Beating Backdoor Attack at Its Own Game</title><link>http://arxiv.org/abs/2307.15539v3</link><description>Deep neural networks (DNNs) are vulnerable to backdoor attack, which does notaffect the network's performance on clean data but would manipulate the networkbehavior once a trigger pattern is added. Existing defense methods have greatlyreduced attack success rate, but their prediction accuracy on clean data stilllags behind a clean model by a large margin. Inspired by the stealthiness andeffectiveness of backdoor attack, we propose a simple but highly effectivedefense framework which injects non-adversarial backdoors targeting poisonedsamples. Following the general steps in backdoor attack, we detect a small setof suspected samples and then apply a poisoning strategy to them. Thenon-adversarial backdoor, once triggered, suppresses the attacker's backdoor onpoisoned data, but has limited influence on clean data. The defense can becarried out during data preprocessing, without any modification to the standardend-to-end training pipeline. We conduct extensive experiments on multiplebenchmarks with different architectures and representative attacks. Resultsdemonstrate that our method achieves state-of-the-art defense effectivenesswith by far the lowest performance drop on clean data. Considering thesurprising defense ability displayed by our framework, we call for moreattention to utilizing backdoor for backdoor defense. Code is available athttps://github.com/damianliumin/non-adversarial_backdoor.</description><author>Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue</author><pubDate>Fri, 04 Aug 2023 17:16:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15539v3</guid></item><item><title>Adaptive Preferential Attached kNN Graph With Distribution-Awareness</title><link>http://arxiv.org/abs/2308.02442v1</link><description>Graph-based kNN algorithms have garnered widespread popularity for machinelearning tasks, due to their simplicity and effectiveness. However, theconventional kNN graph's reliance on a fixed value of k can hinder itsperformance, especially in scenarios involving complex data distributions.Moreover, like other classification models, the presence of ambiguous samplesalong decision boundaries often presents a challenge, as they are more prone toincorrect classification. To address these issues, we propose the PreferentialAttached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN withdistribution-based graph construction. By incorporating distributioninformation, paNNG can significantly improve performance for ambiguous samplesby "pulling" them towards their original classes and hence enable enhancedoverall accuracy and generalization capability. Through rigorous evaluations ondiverse benchmark datasets, paNNG outperforms state-of-the-art algorithms,showcasing its adaptability and efficacy across various real-world scenarios.</description><author>Shaojie Min, Ji Liu</author><pubDate>Fri, 04 Aug 2023 17:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02442v1</guid></item><item><title>A State-Space Perspective on Modelling and Inference for Online Skill Rating</title><link>http://arxiv.org/abs/2308.02414v1</link><description>This paper offers a comprehensive review of the main methodologies used forskill rating in competitive sports. We advocate for a state-space modelperspective, wherein players' skills are represented as time-varying, and matchresults serve as the sole observed quantities. The state-space modelperspective facilitates the decoupling of modeling and inference, enabling amore focused approach highlighting model assumptions, while also fostering thedevelopment of general-purpose inference tools. We explore the essential stepsinvolved in constructing a state-space model for skill rating before turning toa discussion on the three stages of inference: filtering, smoothing andparameter estimation. Throughout, we examine the computational challenges ofscaling up to high-dimensional scenarios involving numerous players andmatches, highlighting approximations and reductions used to address thesechallenges effectively. We provide concise summaries of popular methodsdocumented in the literature, along with their inferential paradigms andintroduce new approaches to skill rating inference based on sequential MonteCarlo and finite state-spaces. We close with numerical experimentsdemonstrating a practical workflow on real data across different sports.</description><author>Samuel Duffield, Samuel Power, Lorenzo Rimella</author><pubDate>Fri, 04 Aug 2023 17:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02414v1</guid></item><item><title>Mitigating the Bias of Centered Objects in Common Datasets</title><link>http://arxiv.org/abs/2112.09195v3</link><description>Convolutional networks are considered shift invariant, but it wasdemonstrated that their response may vary according to the exact location ofthe objects. In this paper we will demonstrate that most commonly investigateddatasets have a bias, where objects are over-represented at the center of theimage during training. This bias and the boundary condition of these networkscan have a significant effect on the performance of these architectures andtheir accuracy drops significantly as an object approaches the boundary. Wewill also demonstrate how this effect can be mitigated with data augmentationtechniques.</description><author>Gergely Szabo, Andras Horvath</author><pubDate>Fri, 04 Aug 2023 17:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.09195v3</guid></item><item><title>Video Background Music Generation: Dataset, Method and Evaluation</title><link>http://arxiv.org/abs/2211.11248v2</link><description>Music is essential when editing videos, but selecting music manually isdifficult and time-consuming. Thus, we seek to automatically generatebackground music tracks given video input. This is a challenging task since itrequires music-video datasets, efficient architectures for video-to-musicgeneration, and reasonable metrics, none of which currently exist. To closethis gap, we introduce a complete recipe including dataset, benchmark model,and evaluation metric for video background music generation. We present SymMV,a video and symbolic music dataset with various musical annotations. To thebest of our knowledge, it is the first video-music dataset with rich musicalannotations. We also propose a benchmark video background music generationframework named V-MusProd, which utilizes music priors of chords, melody, andaccompaniment along with video-music relations of semantic, color, and motionfeatures. To address the lack of objective metrics for video-musiccorrespondence, we design a retrieval-based metric VMCP built upon a powerfulvideo-music representation learning model. Experiments show that with ourdataset, V-MusProd outperforms the state-of-the-art method in both musicquality and correspondence with videos. We believe our dataset, benchmarkmodel, and evaluation metric will boost the development of video backgroundmusic generation. Our dataset and code are available athttps://github.com/zhuole1025/SymMV.</description><author>Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, Si Liu</author><pubDate>Fri, 04 Aug 2023 16:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11248v2</guid></item><item><title>A Bi-variant Variational Model for Diffeomorphic Image Registration with Relaxed Jacobian Determinant Constraints</title><link>http://arxiv.org/abs/2308.02393v1</link><description>Diffeomorphic registration has become a powerful approach for seeking asmooth and invertible spatial transformation between two coordinate systemswhich have been measured via the template and reference images. While thepointwise volume-preserving constraint is effective for some problems, it istoo stringent for many other problems especially when the local deformationsare relatively large, because it may lead to a poor large-deformation forenforcing local matching.In this paper, we propose a novel bi-variantdiffeomorphic image registration model with the soft constraint of Jacobianequation, which allows local deformations to shrink and grow in a flexiblerange.The Jacobian determinant of the transformation is explicitly controlledby optimizing the relaxation function. To prevent deformation folding andenhance the smoothness of deformation, we not only impose a positivityconstraint in optimizing the relaxation function, but also employ a regularizerto ensure the smoothness of the relaxation function.Furthermore, the positivityconstraint ensures that is as close to one as possible, which helps to obtain avolume-preserving transformation on average.We further analyze the existence ofthe minimizer for the variational model and propose a penalty splitting methodwith a multilevel strategy to solve this model. Numerical experiments show thatthe proposed algorithm is convergent, and the positivity constraint can controlthe range of relative volume and not compromise registration accuracy.Moreover, the proposed model produces diffeomorphic maps for large deformation,and achieves better performance compared to the several existing registrationmodels.</description><author>Yanyan Li, Ke Chen, Chong Chen, Jianping Zhang</author><pubDate>Fri, 04 Aug 2023 16:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02393v1</guid></item><item><title>Mitigating Label Biases for In-context Learning</title><link>http://arxiv.org/abs/2305.19148v3</link><description>Various design settings for in-context learning (ICL), such as the choice andorder of the in-context examples, can bias a model toward a particularprediction without being reflective of an understanding of the task. While manystudies discuss these design choices, there have been few systematicinvestigations into categorizing them and mitigating their impact. In thiswork, we define a typology for three types of label biases in ICL for textclassification: vanilla-label bias, context-label bias, and domain-label bias(which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fallshort of addressing all three types of biases. Specifically, domain-label biasrestricts LLMs to random-level performance on many tasks regardless of thechoice of in-context examples. To mitigate the effect of these biases, wepropose a simple bias calibration method that estimates a language model'slabel bias using random in-domain words from the task corpus. After controllingfor this estimated bias when making predictions, our novel domain-contextcalibration significantly improves the ICL performance of GPT-J and GPT-3 on awide range of tasks. The gain is substantial on tasks with large domain-labelbias (up to 37% in Macro-F1). Furthermore, our results generalize to modelswith different scales, pretraining methods, and manually-designed taskinstructions, showing the prevalence of label biases in ICL.</description><author>Yu Fei, Yifan Hou, Zeming Chen, Antoine Bosselut</author><pubDate>Fri, 04 Aug 2023 16:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19148v3</guid></item><item><title>Learning Optimal Admission Control in Partially Observable Queueing Networks</title><link>http://arxiv.org/abs/2308.02391v1</link><description>We present an efficient reinforcement learning algorithm that learns theoptimal admission control policy in a partially observable queueing network.Specifically, only the arrival and departure times from the network areobservable, and optimality refers to the average holding/rejection cost ininfinite horizon. While reinforcement learning in Partially Observable Markov DecisionProcesses (POMDP) is prohibitively expensive in general, we show that ouralgorithm has a regret that only depends sub-linearly on the maximal number ofjobs in the network, $S$. In particular, in contrast with existing regretanalyses, our regret bound does not depend on the diameter of the underlyingMarkov Decision Process (MDP), which in most queueing systems is at leastexponential in $S$. The novelty of our approach is to leverage Norton's equivalent theorem forclosed product-form queueing networks and an efficient reinforcement learningalgorithm for MDPs with the structure of birth-and-death processes.</description><author>Jonatha Anselmi, Bruno Gaujal, Louis-Sébastien Rebuffi</author><pubDate>Fri, 04 Aug 2023 16:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02391v1</guid></item><item><title>Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics</title><link>http://arxiv.org/abs/2308.02382v1</link><description>Survival analysis is a fundamental tool in medicine, modeling the time untilan event of interest occurs in a population. However, in real-worldapplications, survival data are often incomplete, censored, distributed, andconfidential, especially in healthcare settings where privacy is critical. Thescarcity of data can severely limit the scalability of survival models todistributed applications that rely on large data pools. Federated learning is apromising technique that enables machine learning models to be trained onmultiple datasets without compromising user privacy, making it particularlywell-suited for addressing the challenges of survival data and large-scalesurvival applications. Despite significant developments in federated learningfor classification and regression, many directions remain unexplored in thecontext of survival analysis. In this work, we propose an extension of theFederated Survival Forest algorithm, called FedSurF++. This federated ensemblemethod constructs random survival forests in heterogeneous federations.Specifically, we investigate several new tree sampling methods from clientforests and compare the results with state-of-the-art survival models based onneural networks. The key advantage of FedSurF++ is its ability to achievecomparable performance to existing methods while requiring only a singlecommunication round to complete. The extensive empirical investigation resultsin a significant improvement from the algorithmic and privacy preservationperspectives, making the original FedSurF algorithm more efficient, robust, andprivate. We also present results on two real-world datasets demonstrating thesuccess of FedSurF++ in real-world healthcare studies. Our results underscorethe potential of FedSurF++ to improve the scalability and effectiveness ofsurvival analysis in distributed settings while preserving user privacy.</description><author>Alberto Archetti, Francesca Ieva, Matteo Matteucci</author><pubDate>Fri, 04 Aug 2023 16:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02382v1</guid></item><item><title>Classifying Causal Structures: Ascertaining when Classical Correlations are Constrained by Inequalities</title><link>http://arxiv.org/abs/2308.02380v1</link><description>The classical causal relations between a set of variables, some observed andsome latent, can induce both equality constraints (typically conditionalindependences) as well as inequality constraints (Instrumental and Bellinequalities being prototypical examples) on their compatible distribution overthe observed variables. Enumerating a causal structure's implied inequalityconstraints is generally far more difficult than enumerating its equalities.Furthermore, only inequality constraints ever admit violation by quantumcorrelations. For both those reasons, it is important to classify causalscenarios into those which impose inequality constraints versus those which donot. Here we develop methods for detecting such scenarios by appealing tod-separation, e-separation, and incompatible supports. Many (perhaps all?)scenarios with exclusively equality constraints can be detected via a conditionarticulated by Henson, Lal and Pusey (HLP). Considering all scenarios with upto 4 observed variables, which number in the thousands, we are able to resolveall but three causal scenarios, providing evidence that the HLP condition is,in fact, exhaustive.</description><author>Shashaank Khanna, Marina Maciel Ansanelli, Matthew F. Pusey, Elie Wolfe</author><pubDate>Fri, 04 Aug 2023 16:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02380v1</guid></item><item><title>A closer look at the training dynamics of knowledge distillation</title><link>http://arxiv.org/abs/2303.11098v3</link><description>In this paper we revisit the efficacy of knowledge distillation as a functionmatching and metric learning problem. In doing so we verify three importantdesign decisions, namely the normalisation, soft maximum function, andprojection layers as key ingredients. We theoretically show that the projectorimplicitly encodes information on past examples, enabling relational gradientsfor the student. We then show that the normalisation of representations istightly coupled with the training dynamics of this projector, which can have alarge impact on the students performance. Finally, we show that a simple softmaximum function can be used to address any significant capacity gap problems.Experimental results on various benchmark datasets demonstrate that using theseinsights can lead to superior or comparable performance to state-of-the-artknowledge distillation techniques, despite being much more computationallyefficient. In particular, we obtain these results across image classification(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficultdistillation objectives, such as training data efficient transformers, wherebywe attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.</description><author>Roy Miles, Krystian Mikolajczyk</author><pubDate>Fri, 04 Aug 2023 16:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11098v3</guid></item><item><title>Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training</title><link>http://arxiv.org/abs/2305.05237v2</link><description>New roads are being constructed all the time. However, the capabilities ofprevious deep forecasting models to generalize to new roads not seen in thetraining data (unseen roads) are rarely explored. In this paper, we introduce anovel setup called a spatio-temporal (ST) split to evaluate the models'capabilities to generalize to unseen roads. In this setup, the models aretrained on data from a sample of roads, but tested on roads not seen in thetraining data. Moreover, we also present a novel framework called SpatialContrastive Pre-Training (SCPT) where we introduce a spatial encoder module toextract latent features from unseen roads during inference time. This spatialencoder is pre-trained using contrastive learning. During inference, thespatial encoder only requires two days of traffic data on the new roads anddoes not require any re-training. We also show that the output from the spatialencoder can be used effectively to infer latent node embeddings on unseen roadsduring inference time. The SCPT framework also incorporates a new layer, namedthe spatially gated addition (SGA) layer, to effectively combine the latentfeatures from the output of the spatial encoder to existing backbones.Additionally, since there is limited data on the unseen roads, we argue that itis better to decouple traffic signals to trivial-to-capture periodic signalsand difficult-to-capture Markovian signals, and for the spatial encoder to onlylearn the Markovian signals. Finally, we empirically evaluated SCPT using theST split setup on four real-world datasets. The results showed that adding SCPTto a backbone consistently improves forecasting performance on unseen roads.More importantly, the improvements are greater when forecasting further intothe future. The codes are available on GitHub:\burl{https://github.com/cruiseresearchgroup/forecasting-on-new-roads}.</description><author>Arian Prabowo, Wei Shao, Hao Xue, Piotr Koniusz, Flora D. Salim</author><pubDate>Fri, 04 Aug 2023 16:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05237v2</guid></item><item><title>A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data</title><link>http://arxiv.org/abs/2308.02370v1</link><description>Traffic signals play an important role in transportation by enabling trafficflow management, and ensuring safety at intersections. In addition, knowing thetraffic signal phase and timing data can allow optimal vehicle routing for timeand energy efficiency, eco-driving, and the accurate simulation of signalizedroad networks. In this paper, we present a machine learning (ML) method forestimating traffic signal timing information from vehicle probe data. To theauthors best knowledge, very few works have presented ML techniques fordetermining traffic signal timing parameters from vehicle probe data. In thiswork, we develop an Extreme Gradient Boosting (XGBoost) model to estimatesignal cycle lengths and a neural network model to determine the correspondingred times per phase from probe data. The green times are then be derived fromthe cycle length and red times. Our results show an error of less than 0.56 secfor cycle length, and red times predictions within 7.2 sec error on average.</description><author>Juliette Ugirumurera, Joseph Severino, Erik A. Bensen, Qichao Wang, Jane Macfarlane</author><pubDate>Fri, 04 Aug 2023 16:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02370v1</guid></item><item><title>X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance</title><link>http://arxiv.org/abs/2303.15764v2</link><description>Text-driven 3D stylization is a complex and crucial task in the fields ofcomputer vision (CV) and computer graphics (CG), aimed at transforming a baremesh to fit a target text. Prior methods adopt text-independent multilayerperceptrons (MLPs) to predict the attributes of the target mesh with thesupervision of CLIP loss. However, such text-independent architecture lackstextual guidance during predicting attributes, thus leading to unsatisfactorystylization and slow convergence. To address these limitations, we presentX-Mesh, an innovative text-driven 3D stylization framework that incorporates anovel Text-guided Dynamic Attention Module (TDAM). The TDAM dynamicallyintegrates the guidance of the target text by utilizing text-relevant spatialand channel-wise attentions during vertex feature extraction, resulting in moreaccurate attribute prediction and faster convergence speed. Furthermore,existing works lack standard benchmarks and automated metrics for evaluation,often relying on subjective and non-reproducible user studies to assess thequality of stylized 3D assets. To overcome this limitation, we introduce a newstandard text-mesh benchmark, namely MIT-30, and two automated metrics, whichwill enable future research to achieve fair and objective comparisons. Ourextensive qualitative and quantitative experiments demonstrate that X-Meshoutperforms previous state-of-the-art methods.</description><author>Yiwei Ma, Xiaioqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, Rongrong Ji</author><pubDate>Fri, 04 Aug 2023 16:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15764v2</guid></item><item><title>Universal Defensive Underpainting Patch: Making Your Text Invisible to Optical Character Recognition</title><link>http://arxiv.org/abs/2308.02369v1</link><description>Optical Character Recognition (OCR) enables automatic text extraction fromscanned or digitized text images, but it also makes it easy to pirate valuableor sensitive text from these images. Previous methods to prevent OCR piracy bydistorting characters in text images are impractical in real-world scenarios,as pirates can capture arbitrary portions of the text images, rendering thedefenses ineffective. In this work, we propose a novel and effective defensemechanism termed the Universal Defensive Underpainting Patch (UDUP) thatmodifies the underpainting of text images instead of the characters. UDUP iscreated through an iterative optimization process to craft a small, fixed-sizedefensive patch that can generate non-overlapping underpainting for text imagesof any size. Experimental results show that UDUP effectively defends againstunauthorized OCR under the setting of any screenshot range or complex imagebackground. It is agnostic to the content, size, colors, and languages ofcharacters, and is robust to typical image operations such as scaling andcompressing. In addition, the transferability of UDUP is demonstrated byevading several off-the-shelf OCRs. The code is available athttps://github.com/QRICKDD/UDUP.</description><author>JiaCheng Deng, Li Dong, Jiahao Chen, Diqun Yan, Rangding Wang, Dengpan Ye, Lingchen Zhao, Jinyu Tian</author><pubDate>Fri, 04 Aug 2023 16:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02369v1</guid></item><item><title>Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise</title><link>http://arxiv.org/abs/2305.13498v2</link><description>This article aims to investigate the impact of noise on parameter fitting foran Ornstein-Uhlenbeck process, focusing on the effects of multiplicative andthermal noise on the accuracy of signal separation. To address these issues, wepropose algorithms and methods that can effectively distinguish between thermaland multiplicative noise and improve the precision of parameter estimation foroptimal data analysis. Specifically, we explore the impact of bothmultiplicative and thermal noise on the obfuscation of the actual signal andpropose methods to resolve them. Firstly, we present an algorithm that caneffectively separate thermal noise with comparable performance to HamiltonMonte Carlo (HMC) but with significantly improved speed. Subsequently, weanalyze multiplicative noise and demonstrate that HMC is insufficient forisolating thermal and multiplicative noise. However, we show that, withadditional knowledge of the ratio between thermal and multiplicative noise, wecan accurately distinguish between the two types of noise when provided with asufficiently large sampling rate or an amplitude of multiplicative noisesmaller than thermal noise. This finding results in a situation that initiallyseems counterintuitive. When multiplicative noise dominates the noise spectrum,we can successfully estimate the parameters for such systems after addingadditional white noise to shift the noise balance.</description><author>Simon Carter, Helmut H. Strey</author><pubDate>Fri, 04 Aug 2023 16:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13498v2</guid></item><item><title>Fine-grained Species Recognition with Privileged Pooling: Better Sample Efficiency Through Supervised Attention</title><link>http://arxiv.org/abs/2003.09168v4</link><description>We propose a scheme for supervised image classification that uses privilegedinformation, in the form of keypoint annotations for the training data, tolearn strong models from small and/or biased training sets. Our main motivationis the recognition of animal species for ecological applications such asbiodiversity modelling, which is challenging because of long-tailed speciesdistributions due to rare species, and strong dataset biases such as repetitivescene background in camera traps. To counteract these challenges, we propose avisual attention mechanism that is supervised via keypoint annotations thathighlight important object parts. This privileged information, implemented as anovel privileged pooling operation, is only required during training and helpsthe model to focus on regions that are discriminative. In experiments withthree different animal species datasets, we show that deep networks withprivileged pooling can use small training sets more efficiently and generalizebetter.</description><author>Andres C. Rodriguez, Stefano D'Aronco, Konrad Schindler, Jan Dirk Wegner</author><pubDate>Fri, 04 Aug 2023 15:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.09168v4</guid></item><item><title>Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation</title><link>http://arxiv.org/abs/2308.02363v1</link><description>Deep learning models usually require sufficient training data to achieve highaccuracy, but obtaining labeled data can be time-consuming and labor-intensive.Here we introduce a template-based training method to train a 3D U-Net modelfrom scratch using only one population-averaged brain MRI template and itsassociated segmentation label. The process incorporated visual perceptionaugmentation to enhance the model's robustness in handling diverse image inputsand mitigating overfitting. Leveraging this approach, we trained 3D U-Netmodels for mouse, rat, marmoset, rhesus, and human brain MRI to achievesegmentation tasks such as skull-stripping, brain segmentation, and tissueprobability mapping. This tool effectively addresses the limited availabilityof training data and holds significant potential for expanding deep learningapplications in image analysis, providing researchers with a unified solutionto train deep neural networks with only one image sample.</description><author>Fang-Cheng Yeh</author><pubDate>Fri, 04 Aug 2023 15:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02363v1</guid></item><item><title>Intensity-free Integral-based Learning of Marked Temporal Point Processes</title><link>http://arxiv.org/abs/2308.02360v1</link><description>In the marked temporal point processes (MTPP), a core problem is toparameterize the conditional joint PDF (probability distribution function)$p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history.The majority of existing studies predefine intensity functions. Their utilityis challenged by specifying the intensity function's proper form, which iscritical to balance expressiveness and processing efficiency. Recently, thereare studies moving away from predefining the intensity function -- one models$p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal pointprocesses (TPPs), which do not consider marks. This study aims to develophigh-fidelity $p^*(m,t)$ for discrete events where the event marks are eithercategorical or numeric in a multi-dimensional continuous space. We propose asolution framework IFIB (\underline{I}ntensity-\underline{f}ree\underline{I}ntegral-\underline{b}ased process) that models conditional jointPDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifiesthe process to compel the essential mathematical restrictions. We show thedesired properties of IFIB and the superior experimental results of IFIB onreal-world and synthetic datasets. The code is available at\url{https://github.com/StepinSilence/IFIB}.</description><author>Sishun Liu, Ke Deng, Jenny Zhang, Yongli Ren</author><pubDate>Fri, 04 Aug 2023 15:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02360v1</guid></item><item><title>Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text</title><link>http://arxiv.org/abs/2308.02357v1</link><description>The recent advances in large language models (LLM) and foundation models withemergent capabilities have been shown to improve the performance of many NLPtasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMscan be used for KG construction or completion while existing KGs can be usedfor different tasks such as making LLM outputs explainable or fact-checking inNeuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark toevaluate the capabilities of language models to generate KGs from naturallanguage text guided by an ontology. Given an input ontology and a set ofsentences, the task is to extract facts from the text while complying with thegiven ontology (concepts, relations, domain/range constraints) and beingfaithful to the input sentences. We provide two datasets (i) Wikidata-TekGenwith 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19ontologies and 4,860 sentences. We define seven evaluation metrics to measurefact extraction performance, ontology conformance, and hallucinations by LLMs.Furthermore, we provide results for two baseline models, Vicuna-13B andAlpaca-LoRA-13B using automatic prompt generation from test cases. The baselineresults show that there is room for improvement using both Semantic Web andNatural Language Processing techniques.</description><author>Nandana Mihindukulasooriya, Sanju Tiwari, Carlos F. Enguix, Kusum Lata</author><pubDate>Fri, 04 Aug 2023 15:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02357v1</guid></item><item><title>T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images</title><link>http://arxiv.org/abs/2308.02356v1</link><description>Remote sensing image change detection aims to identify the differencesbetween images acquired at different times in the same area. It is widely usedin land management, environmental monitoring, disaster assessment and otherfields. Currently, most change detection methods are based on Siamese networkstructure or early fusion structure. Siamese structure focuses on extractingobject features at different times but lacks attention to change information,which leads to false alarms and missed detections. Early fusion (EF) structurefocuses on extracting features after the fusion of images of different phasesbut ignores the significance of object features at different times fordetecting change details, making it difficult to accurately discern the edgesof changed objects. To address these issues and obtain more accurate results,we propose a novel network, Triplet UNet(T-UNet), based on a three-branchencoder, which is capable to simultaneously extract the object features and thechange features between the pre- and post-time-phase images through tripletencoder. To effectively interact and fuse the features extracted from the threebranches of triplet encoder, we propose a multi-branch spatial-spectralcross-attention module (MBSSCA). In the decoder stage, we introduce the channelattention mechanism (CAM) and spatial attention mechanism (SAM) to fully mineand integrate detailed textures information at the shallow layer and semanticlocalization information at the deep layer.</description><author>Huan Zhong, Chen Wu</author><pubDate>Fri, 04 Aug 2023 15:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02356v1</guid></item><item><title>Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes</title><link>http://arxiv.org/abs/2308.02353v1</link><description>We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE)methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leveragesinitial knowledge about the data distribution to search for validcounterfactuals while avoiding using information from potentially outdateddecision functions in subsequent time steps. Employing two graph autoencoders(GAEs), DyGRACE learns the representation of each class in a binaryclassification scenario. The GAEs minimise the reconstruction error between theoriginal graph and its learned representation during training. The methodinvolves (i) optimising a parametric density function (implemented as alogistic regression function) to identify counterfactuals by maximising thefactual autoencoder's reconstruction error, (ii) minimising the counterfactualautoencoder's error, and (iii) maximising the similarity between the factualand counterfactual graphs. This semi-supervised approach is independent of anunderlying black-box oracle. A logistic regression model is trained on a set ofgraph pairs to learn weights that aid in finding counterfactuals. At inference,for each unseen graph, the logistic regressor identifies the bestcounterfactual candidate using these learned weights, while the GAEs can beiteratively updated to represent the continual adaptation of the learned graphrepresentation over iterations. DyGRACE is quite effective and can act as adrift detector, identifying distributional drift based on differences inreconstruction errors between iterations. It avoids reliance on the oracle'spredictions in successive iterations, thereby increasing the efficiency ofcounterfactual discovery. DyGRACE, with its capacity for contrastive learningand drift detection, will offer new avenues for semi-supervised learning andexplanation generation.</description><author>Bardh Prenkaj, Mario Villaizan-Vallelado, Tobias Leemann, Gjergji Kasneci</author><pubDate>Fri, 04 Aug 2023 15:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02353v1</guid></item><item><title>A Parameter-efficient Multi-subject Model for Predicting fMRI Activity</title><link>http://arxiv.org/abs/2308.02351v1</link><description>This is the Algonauts 2023 submission report for team "BlobGPT". Our modelconsists of a multi-subject linear encoding head attached to a pretrained trunkmodel. The multi-subject head consists of three components: (1) a sharedmulti-layer feature projection, (2) shared plus subject-specific low-dimensionlinear transformations, and (3) a shared PCA fMRI embedding. In this report, weexplain these components in more detail and present some experimental results.Our code is available at https://github.com/cmi-dair/algonauts23.</description><author>Connor Lane, Gregory Kiar</author><pubDate>Fri, 04 Aug 2023 15:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02351v1</guid></item><item><title>RobustMQ: Benchmarking Robustness of Quantized Models</title><link>http://arxiv.org/abs/2308.02350v1</link><description>Quantization has emerged as an essential technique for deploying deep neuralnetworks (DNNs) on devices with limited resources. However, quantized modelsexhibit vulnerabilities when exposed to various noises in real-worldapplications. Despite the importance of evaluating the impact of quantizationon robustness, existing research on this topic is limited and often disregardsestablished principles of robustness evaluation, resulting in incomplete andinconclusive findings. To address this gap, we thoroughly evaluated therobustness of quantized models against various noises (adversarial attacks,natural corruptions, and systematic noises) on ImageNet. The comprehensiveevaluation results empirically provide valuable insights into the robustness ofquantized models in various scenarios, for example: (1) quantized modelsexhibit higher adversarial robustness than their floating-point counterparts,but are more vulnerable to natural corruptions and systematic noises; (2) ingeneral, increasing the quantization bit-width results in a decrease inadversarial robustness, an increase in natural robustness, and an increase insystematic robustness; (3) among corruption methods, \textit{impulse noise} and\textit{glass blur} are the most harmful to quantized models, while\textit{brightness} has the least impact; (4) among systematic noises, the\textit{nearest neighbor interpolation} has the highest impact, while bilinearinterpolation, cubic interpolation, and area interpolation are the three leastharmful. Our research contributes to advancing the robust quantization ofmodels and their deployment in real-world scenarios.</description><author>Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu</author><pubDate>Fri, 04 Aug 2023 15:37:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02350v1</guid></item><item><title>Data-driven modeling of Landau damping by physics-informed neural networks</title><link>http://arxiv.org/abs/2211.01021v3</link><description>Kinetic approaches are generally accurate in dealing with microscale plasmaphysics problems but are computationally expensive for large-scale ormultiscale systems. One of the long-standing problems in plasma physics is theintegration of kinetic physics into fluid models, which is often achievedthrough sophisticated analytical closure terms. In this paper, we successfullyconstruct a multi-moment fluid model with an implicit fluid closure included inthe neural network using machine learning. The multi-moment fluid model istrained with a small fraction of sparsely sampled data from kinetic simulationsof Landau damping, using the physics-informed neural network (PINN) and thegradient-enhanced physics-informed neural network (gPINN). The multi-momentfluid model constructed using either PINN or gPINN reproduces the timeevolution of the electric field energy, including its damping rate, and theplasma dynamics from the kinetic simulations. In addition, we introduce avariant of the gPINN architecture, namely, gPINN$p$ to capture the Landaudamping process. Instead of including the gradients of all the equationresiduals, gPINN$p$ only adds the gradient of the pressure equation residual asone additional constraint. Among the three approaches, the gPINN$p$-constructedmulti-moment fluid model offers the most accurate results. This work shedslight on the accurate and efficient modeling of large-scale systems, which canbe extended to complex multiscale laboratory, space, and astrophysical plasmaphysics problems.</description><author>Yilan Qin, Jiayu Ma, Mingle Jiang, Chuanfei Dong, Haiyang Fu, Liang Wang, Wenjie Cheng, Yaqiu Jin</author><pubDate>Fri, 04 Aug 2023 15:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01021v3</guid></item><item><title>Exploiting Multiple Abstractions in Episodic RL via Reward Shaping</title><link>http://arxiv.org/abs/2303.00516v2</link><description>One major limitation to the applicability of Reinforcement Learning (RL) tomany practical domains is the large number of samples required to learn anoptimal policy. To address this problem and improve learning efficiency, weconsider a linear hierarchy of abstraction layers of the Markov DecisionProcess (MDP) underlying the target domain. Each layer is an MDP representing acoarser model of the one immediately below in the hierarchy. In this work, wepropose a novel form of Reward Shaping where the solution obtained at theabstract level is used to offer rewards to the more concrete MDP, in such a waythat the abstract solution guides the learning in the more complex domain. Incontrast with other works in Hierarchical RL, our technique has fewrequirements in the design of the abstract models and it is also tolerant tomodeling errors, thus making the proposed approach practical. We formallyanalyze the relationship between the abstract models and the explorationheuristic induced in the lower-level domain. Moreover, we prove that the methodguarantees optimal convergence and we demonstrate its effectivenessexperimentally.</description><author>Roberto Cipollone, Giuseppe De Giacomo, Marco Favorito, Luca Iocchi, Fabio Patrizi</author><pubDate>Fri, 04 Aug 2023 15:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00516v2</guid></item><item><title>Stability and Generalization of Hypergraph Collaborative Networks</title><link>http://arxiv.org/abs/2308.02347v1</link><description>Graph neural networks have been shown to be very effective in utilizingpairwise relationships across samples. Recently, there have been severalsuccessful proposals to generalize graph neural networks to hypergraph neuralnetworks to exploit more complex relationships. In particular, the hypergraphcollaborative networks yield superior results compared to other hypergraphneural networks for various semi-supervised learning tasks. The collaborativenetwork can provide high quality vertex embeddings and hyperedge embeddingstogether by formulating them as a joint optimization problem and by using theirconsistency in reconstructing the given hypergraph. In this paper, we aim toestablish the algorithmic stability of the core layer of the collaborativenetwork and provide generalization guarantees. The analysis sheds light on thedesign of hypergraph filters in collaborative networks, for instance, how thedata and hypergraph filters should be scaled to achieve uniform stability ofthe learning process. Some experimental results on real-world datasets arepresented to illustrate the theory.</description><author>Michael Ng, Hanrui Wu, Andy Yip</author><pubDate>Fri, 04 Aug 2023 15:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02347v1</guid></item><item><title>Class Incremental Learning with Self-Supervised Pre-Training and Prototype Learning</title><link>http://arxiv.org/abs/2308.02346v1</link><description>Deep Neural Network (DNN) has achieved great success on datasets of closedclass set. However, new classes, like new categories of social media topics,are continuously added to the real world, making it necessary to incrementallylearn. This is hard for DNN because it tends to focus on fitting to new classeswhile ignoring old classes, a phenomenon known as catastrophic forgetting.State-of-the-art methods rely on knowledge distillation and data replaytechniques but still have limitations. In this work, we analyze the causes ofcatastrophic forgetting in class incremental learning, which owes to threefactors: representation drift, representation confusion, and classifierdistortion. Based on this view, we propose a two-stage learning framework witha fixed encoder and an incrementally updated prototype classifier. The encoderis trained with self-supervised learning to generate a feature space with highintrinsic dimensionality, thus improving its transferability and generality.The classifier incrementally learns new prototypes while retaining theprototypes of previously learned data, which is crucial in preserving thedecision boundary.Our method does not rely on preserved samples of old classes,is thus a non-exemplar based CIL method. Experiments on public datasets showthat our method can significantly outperform state-of-the-art exemplar-basedmethods when they reserved 5 examplers per class, under the incremental settingof 10 phases, by 18.24% on CIFAR-100 and 9.37% on ImageNet100.</description><author>Wenzhuo Liu, Xinjian Wu, Fei Zhu, Mingming Yu, Chuang Wang, Cheng-Lin Liu</author><pubDate>Fri, 04 Aug 2023 15:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02346v1</guid></item><item><title>Learning Networks from Gaussian Graphical Models and Gaussian Free Fields</title><link>http://arxiv.org/abs/2308.02344v1</link><description>We investigate the problem of estimating the structure of a weighted networkfrom repeated measurements of a Gaussian Graphical Model (GGM) on the network.In this vein, we consider GGMs whose covariance structures align with thegeometry of the weighted network on which they are based. Such GGMs have beenof longstanding interest in statistical physics, and are referred to as theGaussian Free Field (GFF). In recent years, they have attracted considerableinterest in the machine learning and theoretical computer science. In thiswork, we propose a novel estimator for the weighted network (equivalently, itsLaplacian) from repeated measurements of a GFF on the network, based on theFourier analytic properties of the Gaussian distribution. In this pursuit, ourapproach exploits complex-valued statistics constructed from observed data,that are of interest on their own right. We demonstrate the effectiveness ofour estimator with concrete recovery guarantees and bounds on the requiredsample complexity. In particular, we show that the proposed statistic achievesthe parametric rate of estimation for fixed network size. In the setting ofnetworks growing with sample size, our results show that for Erdos-Renyi randomgraphs $G(d,p)$ above the connectivity threshold, we demonstrate that networkrecovery takes place with high probability as soon as the sample size $n$satisfies $n \gg d^4 \log d \cdot p^{-2}$.</description><author>Subhro Ghosh, Soumendu Sundar Mukherjee, Hoang-Son Tran, Ujan Gangopadhyay</author><pubDate>Fri, 04 Aug 2023 15:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02344v1</guid></item><item><title>SVCNet: Scribble-based Video Colorization Network with Temporal Aggregation</title><link>http://arxiv.org/abs/2303.11591v2</link><description>In this paper, we propose a scribble-based video colorization network withtemporal aggregation called SVCNet. It can colorize monochrome videos based ondifferent user-given color scribbles. It addresses three common issues in thescribble-based video colorization area: colorization vividness, temporalconsistency, and color bleeding. To improve the colorization quality andstrengthen the temporal consistency, we adopt two sequential sub-networks inSVCNet for precise colorization and temporal smoothing, respectively. The firststage includes a pyramid feature encoder to incorporate color scribbles with agrayscale frame, and a semantic feature encoder to extract semantics. Thesecond stage finetunes the output from the first stage by aggregating theinformation of neighboring colorized frames (as short-range connections) andthe first colorized frame (as a long-range connection). To alleviate the colorbleeding artifacts, we learn video colorization and segmentationsimultaneously. Furthermore, we set the majority of operations on a fixed smallimage resolution and use a Super-resolution Module at the tail of SVCNet torecover original sizes. It allows the SVCNet to fit different image resolutionsat the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvobenchmarks. The experimental results demonstrate that SVCNet produces bothhigher-quality and more temporally consistent videos than other well-knownvideo colorization approaches. The codes and models can be found athttps://github.com/zhaoyuzhi/SVCNet.</description><author>Yuzhi Zhao, Lai-Man Po, Kangcheng Liu, Xuehui Wang, Wing-Yin Yu, Pengfei Xian, Yujia Zhang, Mengyang Liu</author><pubDate>Fri, 04 Aug 2023 15:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11591v2</guid></item><item><title>Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?</title><link>http://arxiv.org/abs/2304.10224v2</link><description>Point cloud based 3D deep model has wide applications in many applicationssuch as autonomous driving, house robot, and so on. Inspired by the recentprompt learning in natural language processing, this work proposes a novelMulti-view Vision-Prompt Fusion Network (MvNet) for few-shot 3D point cloudclassification. MvNet investigates the possibility of leveraging theoff-the-shelf 2D pre-trained models to achieve the few-shot classification,which can alleviate the over-dependence issue of the existing baseline modelstowards the large-scale annotated 3D point cloud data. Specifically, MvNetfirst encodes a 3D point cloud into multi-view image features for a number ofdifferent views. Then, a novel multi-view prompt fusion module is developed toeffectively fuse information from different views to bridge the gap between 3Dpoint cloud data and 2D pre-trained models. A set of 2D image prompts can thenbe derived to better describe the suitable prior knowledge for a large-scalepre-trained image model for few-shot 3D point cloud classification. Extensiveexperiments on ModelNet, ScanObjectNN, and ShapeNet datasets demonstrate thatMvNet achieves new state-of-the-art performance for 3D few-shot point cloudimage classification. The source code of this work will be available soon.</description><author>Haoyang Peng, Baopu Li, Bo Zhang, Xin Chen, Tao Chen, Hongyuan Zhu</author><pubDate>Fri, 04 Aug 2023 10:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10224v2</guid></item><item><title>Balanced Classification: A Unified Framework for Long-Tailed Object Detection</title><link>http://arxiv.org/abs/2308.02213v1</link><description>Conventional detectors suffer from performance degradation when dealing withlong-tailed data due to a classification bias towards the majority headcategories. In this paper, we contend that the learning bias originates fromtwo factors: 1) the unequal competition arising from the imbalanceddistribution of foreground categories, and 2) the lack of sample diversity intail categories. To tackle these issues, we introduce a unified frameworkcalled BAlanced CLassification (BACL), which enables adaptive rectification ofinequalities caused by disparities in category distribution and dynamicintensification of sample diversities in a synchronized manner. Specifically, anovel foreground classification balance loss (FCBL) is developed to amelioratethe domination of head categories and shift attention todifficult-to-differentiate categories by introducing pairwise class-awaremargins and auto-adjusted weight terms, respectively. This loss prevents theover-suppression of tail categories in the context of unequal competition.Moreover, we propose a dynamic feature hallucination module (FHM), whichenhances the representation of tail categories in the feature space bysynthesizing hallucinated samples to introduce additional data variances. Inthis divide-and-conquer approach, BACL sets a new state-of-the-art on thechallenging LVIS benchmark with a decoupled training pipeline, surpassingvanilla Faster R-CNN with ResNet-50-FPN by 5.8% AP and 16.1% AP for overall andtail categories. Extensive experiments demonstrate that BACL consistentlyachieves performance improvements across various datasets with differentbackbones and architectures. Code and models are available athttps://github.com/Tianhao-Qi/BACL.</description><author>Tianhao Qi, Hongtao Xie, Pandeng Li, Jiannan Ge, Yongdong Zhang</author><pubDate>Fri, 04 Aug 2023 10:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02213v1</guid></item><item><title>ENCODE: Encoding NetFlows for Network Anomaly Detection</title><link>http://arxiv.org/abs/2207.03890v2</link><description>NetFlow data is a popular network log format used by many network analystsand researchers. The advantages of using NetFlow over deep packet inspectionare that it is easier to collect and process, and it is less privacy intrusive.Many works have used machine learning to detect network attacks using NetFlowdata. The first step for these machine learning pipelines is to pre-process thedata before it is given to the machine learning algorithm. Many approachesexist to pre-process NetFlow data; however, these simply apply existing methodsto the data, not considering the specific properties of network data. We arguethat for data originating from software systems, such as NetFlow or softwarelogs, similarities in frequency and contexts of feature values are moreimportant than similarities in the value itself. In this work, we propose anencoding algorithm that directly takes the frequency and the context of thefeature values into account when the data is being processed. Different typesof network behaviours can be clustered using this encoding, thus aiding theprocess of detecting anomalies within the network. We train several machinelearning models for anomaly detection using the data that has been encoded withour encoding algorithm. We evaluate the effectiveness of our encoding on a newdataset that we created for network attacks on Kubernetes clusters and twowell-known public NetFlow datasets. We empirically demonstrate that the machinelearning models benefit from using our encoding for anomaly detection.</description><author>Clinton Cao, Annibale Panichella, Sicco Verwer, Agathe Blaise, Filippo Rebecchi</author><pubDate>Fri, 04 Aug 2023 10:03:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.03890v2</guid></item><item><title>Towards Personalized Prompt-Model Retrieval for Generative Recommendation</title><link>http://arxiv.org/abs/2308.02205v1</link><description>Recommender Systems are built to retrieve relevant items to satisfy users'information needs. The candidate corpus usually consists of a finite set ofitems that are ready to be served, such as videos, products, or articles. Withrecent advances in Generative AI such as GPT and Diffusion models, a new formof recommendation task is yet to be explored where items are to be created bygenerative models with personalized prompts. Taking image generation as anexample, with a single prompt from the user and access to a generative model,it is possible to generate hundreds of new images in a few minutes. How shallwe attain personalization in the presence of "infinite" items? In thispreliminary study, we propose a two-stage framework, namely Prompt-ModelRetrieval and Generated Item Ranking, to approach this new task formulation. Werelease GEMRec-18K, a prompt-model interaction dataset with 18K imagesgenerated by 200 publicly-available generative models paired with a diverse setof 90 textual prompts. Our findings demonstrate the promise of generative modelrecommendation as a novel personalization problem and the limitations ofexisting evaluation metrics. We highlight future directions for the RecSyscommunity to advance towards generative recommender systems. Our code anddataset are available at https://github.com/MAPS-research/GEMRec.</description><author>Yuanhe Guo, Haoming Liu, Hongyi Wen</author><pubDate>Fri, 04 Aug 2023 09:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02205v1</guid></item><item><title>Explore Spatio-temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline</title><link>http://arxiv.org/abs/2206.11459v2</link><description>We endeavor on a rarely explored task named Insubstantial Object Detection(IOD), which aims to localize the object with following characteristics: (1)amorphous shape with indistinct boundary; (2) similarity to surroundings; (3)absence in color. Accordingly, it is far more challenging to distinguishinsubstantial objects in a single static frame and the collaborativerepresentation of spatial and temporal information is crucial. Thus, weconstruct an IOD-Video dataset comprised of 600 videos (141,017 frames)covering various distances, sizes, visibility, and scenes captured by differentspectral ranges. In addition, we develop a spatio-temporal aggregationframework for IOD, in which different backbones are deployed and aspatio-temporal aggregation loss (STAloss) is elaborately designed to leveragethe consistency along the time axis. Experiments conducted on IOD-Video datasetdemonstrate that spatio-temporal aggregation can significantly improve theperformance of IOD. We hope our work will attract further researches into thisvaluable yet challenging task. The code will be available at:\url{https://github.com/CalayZhou/IOD-Video}.</description><author>Kailai Zhou, Yibo Wang, Tao Lv, Yunqian Li, Linsen Chen, Qiu Shen, Xun Cao</author><pubDate>Fri, 04 Aug 2023 09:43:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11459v2</guid></item><item><title>Benchmarking Visual-Inertial Deep Multimodal Fusion for Relative Pose Regression and Odometry-aided Absolute Pose Regression</title><link>http://arxiv.org/abs/2208.00919v3</link><description>Visual-inertial localization is a key problem in computer vision and roboticsapplications such as virtual reality, self-driving cars, and aerial vehicles.The goal is to estimate an accurate pose of an object when either theenvironment or the dynamics are known. Absolute pose regression (APR)techniques directly regress the absolute pose from an image input in a knownscene using convolutional and spatio-temporal networks. Odometry methodsperform relative pose regression (RPR) that predicts the relative pose from aknown object dynamic (visual or inertial inputs). The localization task can beimproved by retrieving information from both data sources for a cross-modalsetup, which is a challenging problem due to contradictory tasks. In this work,we conduct a benchmark to evaluate deep multimodal fusion based on pose graphoptimization and attention networks. Auxiliary and Bayesian learning areutilized for the APR task. We show accuracy improvements for the APR-RPR taskand for the RPR-RPR task for aerial vehicles and hand-held devices. We conductexperiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate anovel industry dataset.</description><author>Felix Ott, Nisha Lakshmana Raichur, David Rügamer, Tobias Feigl, Heiko Neumann, Bernd Bischl, Christopher Mutschler</author><pubDate>Fri, 04 Aug 2023 09:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.00919v3</guid></item><item><title>Reconstructing Three-Dimensional Models of Interacting Humans</title><link>http://arxiv.org/abs/2308.01854v2</link><description>Understanding 3d human interactions is fundamental for fine-grained sceneanalysis and behavioural modeling. However, most of the existing models predictincorrect, lifeless 3d estimates, that miss the subtle human contactaspects--the essence of the event--and are of little use for detailedbehavioral understanding. This paper addresses such issues with severalcontributions: (1) we introduce models for interaction signature estimation(ISP) encompassing contact detection, segmentation, and 3d contact signatureprediction; (2) we show how such components can be leveraged to ensure contactconsistency during 3d reconstruction; (3) we construct several large datasetsfor learning and evaluating 3d contact prediction and reconstruction methods;specifically, we introduce CHI3D, a lab-based accurate 3d motion capturedataset with 631 sequences containing $2,525$ contact events, $728,664$ groundtruth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with$14,081$ processed pairs of people, and $81,233$ facet-level surfacecorrespondences. Finally, (4) we propose methodology for recovering theground-truth pose and shape of interacting people in a controlled setup and (5)annotate all 3d interaction motions in CHI3D with textual descriptions. Motiondata in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) ismade available for research purposes at \url{https://ci3d.imar.ro}, togetherwith an evaluation server and a public benchmark.</description><author>Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, Cristian Sminchisescu</author><pubDate>Fri, 04 Aug 2023 09:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01854v2</guid></item><item><title>A Survey of Spanish Clinical Language Models</title><link>http://arxiv.org/abs/2308.02199v1</link><description>This survey focuses in encoder Language Models for solving tasks in theclinical domain in the Spanish language. We review the contributions of 17corpora focused mainly in clinical tasks, then list the most relevant SpanishLanguage Models and Spanish Clinical Language models. We perform a thoroughcomparison of these models by benchmarking them over a curated subset of theavailable corpora, in order to find the best-performing ones; in total morethan 3000 models were fine-tuned for this study. All the tested corpora and thebest models are made publically available in an accessible way, so that theresults can be reproduced by independent teams or challenged in the future whennew Spanish Clinical Language models are created.</description><author>Guillem García Subies, Álvaro Barbero Jiménez, Paloma Martínez Fernández</author><pubDate>Fri, 04 Aug 2023 09:33:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02199v1</guid></item><item><title>Paired Competing Neurons Improving STDP Supervised Local Learning In Spiking Neural Networks</title><link>http://arxiv.org/abs/2308.02194v1</link><description>Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardwarehas the potential to significantly reduce the high energy consumption ofArtificial Neural Networks (ANNs) training on modern computers. The biologicalplausibility of SNNs allows them to benefit from bio-inspired plasticity rules,such as Spike Timing-Dependent Plasticity (STDP). STDP offers gradient-free andunsupervised local learning, which can be easily implemented on neuromorphichardware. However, relying solely on unsupervised STDP to performclassification tasks is not enough. In this paper, we propose StabilizedSupervised STDP (S2-STDP), a supervised STDP learning rule to train theclassification layer of an SNN equipped with unsupervised STDP. S2-STDPintegrates error-modulated weight updates that align neuron spikes with desiredtimestamps derived from the average firing time within the layer. Then, weintroduce a training architecture called Paired Competing Neurons (PCN) tofurther enhance the learning capabilities of our classification layer trainedwith S2-STDP. PCN associates each class with paired neurons and encouragesneuron specialization through intra-class competition. We evaluated ourproposed methods on image recognition datasets, including MNIST, Fashion-MNIST,and CIFAR-10. Results showed that our methods outperform current supervisedSTDP-based state of the art, for comparable architectures and numbers ofneurons. Also, the use of PCN enhances the performance of S2-STDP, regardlessof the configuration, and without introducing any hyperparameters.Furtheranalysis demonstrated that our methods exhibited improved hyperparameterrobustness, which reduces the need for tuning.</description><author>Gaspard Goupy, Pierre Tirilly, Ioan Marius Bilasco</author><pubDate>Fri, 04 Aug 2023 09:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02194v1</guid></item><item><title>Explaining Relation Classification Models with Semantic Extents</title><link>http://arxiv.org/abs/2308.02193v1</link><description>In recent years, the development of large pretrained language models, such asBERT and GPT, significantly improved information extraction systems on varioustasks, including relation classification. State-of-the-art systems are highlyaccurate on scientific benchmarks. A lack of explainability is currently acomplicating factor in many real-world applications. Comprehensible systems arenecessary to prevent biased, counterintuitive, or harmful decisions. We introduce semantic extents, a concept to analyze decision patterns for therelation classification task. Semantic extents are the most influential partsof texts concerning classification decisions. Our definition allows similarprocedures to determine semantic extents for humans and models. We provide anannotation tool and a software framework to determine semantic extents forhumans and models conveniently and reproducibly. Comparing both reveals thatmodels tend to learn shortcut patterns from data. These patterns are hard todetect with current interpretability methods, such as input reductions. Ourapproach can help detect and eliminate spurious decision patterns during modeldevelopment. Semantic extents can increase the reliability and security ofnatural language processing systems. Semantic extents are an essential step inenabling applications in critical areas like healthcare or finance. Moreover,our work opens new research directions for developing methods to explain deeplearning models.</description><author>Lars Klöser, Andre Büsgen, Philipp Kohl, Bodo Kraft, Albert Zündorf</author><pubDate>Fri, 04 Aug 2023 09:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02193v1</guid></item><item><title>ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View Stereo</title><link>http://arxiv.org/abs/2308.02191v1</link><description>Compared to the multi-stage self-supervised multi-view stereo (MVS) method,the end-to-end (E2E) approach has received more attention due to its conciseand efficient training pipeline. Recent E2E self-supervised MVS approaches haveintegrated third-party models (such as optical flow models, semanticsegmentation models, NeRF models, etc.) to provide additional consistencyconstraints, which grows GPU memory consumption and complicates the model'sstructure and training pipeline. In this work, we propose an efficientframework for end-to-end self-supervised MVS, dubbed ES-MVSNet. To alleviatethe high memory consumption of current E2E self-supervised MVS frameworks, wepresent a memory-efficient architecture that reduces memory usage by 43%without compromising model performance. Furthermore, with the novel design ofasymmetric view selection policy and region-aware depth consistency, we achievestate-of-the-art performance among E2E self-supervised MVS methods, withoutrelying on third-party models for additional consistency signals. Extensiveexperiments on DTU and Tanks&amp;Temples benchmarks demonstrate that the proposedES-MVSNet approach achieves state-of-the-art performance among E2Eself-supervised MVS methods and competitive performance to many supervised andmulti-stage self-supervised methods.</description><author>Qiang Zhou, Chaohui Yu, Jingliang Li, Yuang Liu, Jing Wang, Zhibin Wang</author><pubDate>Fri, 04 Aug 2023 09:16:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02191v1</guid></item><item><title>Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition</title><link>http://arxiv.org/abs/2308.02190v1</link><description>Cross-corpus speech emotion recognition (SER) seeks to generalize the abilityof inferring speech emotion from a well-labeled corpus to an unlabeled one,which is a rather challenging task due to the significant discrepancy betweentwo corpora. Existing methods, typically based on unsupervised domainadaptation (UDA), struggle to learn corpus-invariant features by globaldistribution alignment, but unfortunately, the resulting features are mixedwith corpus-specific features or not class-discriminative. To tackle thesechallenges, we propose a novel Emotion Decoupling aNd Alignment learningframework (EMO-DNA) for cross-corpus SER, a novel UDA method to learnemotion-relevant corpus-invariant features. The novelties of EMO-DNA aretwo-fold: contrastive emotion decoupling and dual-level emotion alignment. Onone hand, our contrastive emotion decoupling achieves decoupling learning via acontrastive decoupling loss to strengthen the separability of emotion-relevantfeatures from corpus-specific ones. On the other hand, our dual-level emotionalignment introduces an adaptive threshold pseudo-labeling to select confidenttarget samples for class-level alignment, and performs corpus-level alignmentto jointly guide model for learning class-discriminative corpus-invariantfeatures across corpora. Extensive experimental results demonstrate thesuperior performance of EMO-DNA over the state-of-the-art methods in severalcross-corpus scenarios. Source code is available athttps://github.com/Jiaxin-Ye/Emo-DNA.</description><author>Jiaxin Ye, Yujie Wei, Xin-Cheng Wen, Chenglong Ma, Zhizhong Huang, Kunhong Liu, Hongming Shan</author><pubDate>Fri, 04 Aug 2023 09:15:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02190v1</guid></item><item><title>Evaluating generation of chaotic time series by convolutional generative adversarial networks</title><link>http://arxiv.org/abs/2305.16729v2</link><description>To understand the ability and limitations of convolutional neural networks togenerate time series that mimic complex temporal signals, we trained agenerative adversarial network consisting of deep convolutional networks togenerate chaotic time series and used nonlinear time series analysis toevaluate the generated time series. A numerical measure of determinism and theLyapunov exponent, a measure of trajectory instability, showed that thegenerated time series well reproduce the chaotic properties of the originaltime series. However, error distribution analyses showed that large errorsappeared at a low but non-negligible rate. Such errors would not be expected ifthe distribution were assumed to be exponential.</description><author>Yuki Tanaka, Yutaka Yamaguti</author><pubDate>Fri, 04 Aug 2023 09:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16729v2</guid></item><item><title>From Fake to Hyperpartisan News Detection Using Domain Adaptation</title><link>http://arxiv.org/abs/2308.02185v1</link><description>Unsupervised Domain Adaptation (UDA) is a popular technique that aims toreduce the domain shift between two data distributions. It was successfullyapplied in computer vision and natural language processing. In the currentwork, we explore the effects of various unsupervised domain adaptationtechniques between two text classification tasks: fake and hyperpartisan newsdetection. We investigate the knowledge transfer from fake to hyperpartisannews detection without involving target labels during training. Thus, weevaluate UDA, cluster alignment with a teacher, and cross-domain contrastivelearning. Extensive experiments show that these techniques improve performance,while including data augmentation further enhances the results. In addition, wecombine clustering and topic modeling algorithms with UDA, resulting inimproved performances compared to the initial UDA setup.</description><author>Răzvan-Alexandru Smădu, Sebastian-Vasile Echim, Dumitru-Clementin Cercel, Iuliana Marin, Florin Pop</author><pubDate>Fri, 04 Aug 2023 08:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02185v1</guid></item><item><title>Synthetic outlier generation for anomaly detection in autonomous driving</title><link>http://arxiv.org/abs/2308.02184v1</link><description>Anomaly detection, or outlier detection, is a crucial task in various domainsto identify instances that significantly deviate from established patterns orthe majority of data. In the context of autonomous driving, the identificationof anomalies is particularly important to prevent safety-critical incidents, asdeep learning models often exhibit overconfidence in anomalous or outliersamples. In this study, we explore different strategies for training an imagesemantic segmentation model with an anomaly detection module. By introducingmodifications to the training stage of the state-of-the-art DenseHybrid model,we achieve significant performance improvements in anomaly detection. Moreover,we propose a simplified detector that achieves comparable results to ourmodified DenseHybrid approach, while also surpassing the performance of theoriginal DenseHybrid model. These findings demonstrate the efficacy of ourproposed strategies for enhancing anomaly detection in the context ofautonomous driving.</description><author>Martin Bikandi, Gorka Velez, Naiara Aginako, Itziar Irigoien</author><pubDate>Fri, 04 Aug 2023 08:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02184v1</guid></item><item><title>AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification</title><link>http://arxiv.org/abs/2308.02182v1</link><description>Deep learning (DL) has been successfully applied to encrypted network trafficclassification in experimental settings. However, in production use, it hasbeen shown that a DL classifier's performance inevitably decays over time.Re-training the model on newer datasets has been shown to only partiallyimprove its performance. Manually re-tuning the model architecture to meet theperformance expectations on newer datasets is time-consuming and requiresdomain expertise. We propose AutoML4ETC, a novel tool to automatically designefficient and high-performing neural architectures for encrypted trafficclassification. We define a novel, powerful search space tailored specificallyfor the near real-time classification of encrypted traffic using packet headerbytes. We show that with different search strategies over our search space,AutoML4ETC generates neural architectures that outperform the state-of-the-artencrypted traffic classifiers on several datasets, including public benchmarkdatasets and real-world TLS and QUIC traffic collected from the Orange mobilenetwork. In addition to being more accurate, AutoML4ETC's architectures aresignificantly more efficient and lighter in terms of the number of parameters.Finally, we make AutoML4ETC publicly available for future research.</description><author>Navid Malekghaini, Elham Akbari, Mohammad A. Salahuddin, Noura Limam, Raouf Boutaba, Bertrand Mathieu, Stephanie Moteau, Stephane Tuffin</author><pubDate>Fri, 04 Aug 2023 08:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02182v1</guid></item><item><title>Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology</title><link>http://arxiv.org/abs/2308.02180v1</link><description>Clinical trial matching is a key process in health delivery and discovery. Inpractice, it is plagued by overwhelming unstructured data and unscalable manualprocessing. In this paper, we conduct a systematic study on scaling clinicaltrial matching using large language models (LLMs), with oncology as the focusarea. Our study is grounded in a clinical trial matching system currently intest deployment at a large U.S. health network. Initial findings are promising:out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborateeligibility criteria of clinical trials and extract complex matching logic(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantiallyoutperform prior strong baselines and may serve as a preliminary solution tohelp triage patient-trial candidates with humans in the loop. Our study alsoreveals a few significant growth areas for applying LLMs to end-to-end clinicaltrial matching, such as context limitation and accuracy, especially instructuring patient information from longitudinal medical records.</description><author>Cliff Wong, Sheng Zheng, Yu Gu, Christine Moung, Jacob Abel, Naoto Usuyama, Roshanthi Weerasinghe, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</author><pubDate>Fri, 04 Aug 2023 08:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02180v1</guid></item><item><title>SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization</title><link>http://arxiv.org/abs/2303.13035v3</link><description>Electronic health records (EHRs) store an extensive array of patientinformation, encompassing medical histories, diagnoses, treatments, and testoutcomes. These records are crucial for enabling healthcare providers to makewell-informed decisions regarding patient care. Summarizing clinical notesfurther assists healthcare professionals in pinpointing potential health risksand making better-informed decisions. This process contributes to reducingerrors and enhancing patient outcomes by ensuring providers have access to themost pertinent and current patient data. Recent research has shown thatincorporating prompts with large language models (LLMs) substantially booststhe efficacy of summarization tasks. However, we show that this approach alsoleads to increased output variance, resulting in notably divergent outputs evenwhen prompts share similar meanings. To tackle this challenge, we introduce amodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs softprompts to diminish variance while preserving the advantages of prompt-basedsummarization. Experimental findings on multiple clinical note tasks and LLMsindicate that our method not only bolsters performance but also effectivelycurbs variance for various LLMs, providing a more uniform and dependablesolution for summarizing vital medical information.</description><author>Yu-Neng Chuang, Ruixiang Tang, Xiaoqian Jiang, Xia Hu</author><pubDate>Fri, 04 Aug 2023 08:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13035v3</guid></item><item><title>Scene-aware Human Pose Generation using Transformer</title><link>http://arxiv.org/abs/2308.02177v1</link><description>Affordance learning considers the interaction opportunities for an actor inthe scene and thus has wide application in scene understanding and intelligentrobotics. In this paper, we focus on contextual affordance learning, i.e.,using affordance as context to generate a reasonable human pose in a scene.Existing scene-aware human pose generation methods could be divided into twocategories depending on whether using pose templates. Our proposed methodbelongs to the template-based category, which benefits from the representativepose templates. Moreover, inspired by recent transformer-based methods, weassociate each query embedding with a pose template, and use the interactionbetween query embeddings and scene feature map to effectively predict the scaleand offsets for each pose template. In addition, we employ knowledgedistillation to facilitate the offset learning given the predicted scale.Comprehensive experiments on Sitcom dataset demonstrate the effectiveness ofour method.</description><author>Jieteng Yao, Junjie Chen, Li Niu, Bin Sheng</author><pubDate>Fri, 04 Aug 2023 08:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02177v1</guid></item><item><title>Efficient Labelling of Affective Video Datasets via Few-Shot &amp; Multi-Task Contrastive Learning</title><link>http://arxiv.org/abs/2308.02173v1</link><description>Whilst deep learning techniques have achieved excellent emotion prediction,they still require large amounts of labelled training data, which are (a)onerous and tedious to compile, and (b) prone to errors and biases. We proposeMulti-Task Contrastive Learning for Affect Representation (\textbf{MT-CLAR})for few-shot affect inference. MT-CLAR combines multi-task learning with aSiamese network trained via contrastive learning to infer from a pair ofexpressive facial images (a) the (dis)similarity between the facialexpressions, and (b) the difference in valence and arousal levels of the twofaces. We further extend the image-based MT-CLAR framework for automated videolabelling where, given one or a few labelled video frames (termed\textit{support-set}), MT-CLAR labels the remainder of the video for valenceand arousal. Experiments are performed on the AFEW-VA dataset with multiplesupport-set configurations; moreover, supervised learning on representationslearnt via MT-CLAR are used for valence, arousal and categorical emotionprediction on the AffectNet and AFEW-VA datasets. The results show that valenceand arousal predictions via MT-CLAR are very comparable to the state-of-the-art(SOTA), and we significantly outperform SOTA with a support-set $\approx$6\%the size of the video dataset.</description><author>Ravikiran Parameshwara, Ibrahim Radwan, Akshay Asthana, Iman Abbasnejad, Ramanathan Subramanian, Roland Goecke</author><pubDate>Fri, 04 Aug 2023 08:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02173v1</guid></item><item><title>You talk what you read: Understanding News Comment Behavior by Dispositional and Situational Attribution</title><link>http://arxiv.org/abs/2308.02168v1</link><description>Many news comment mining studies are based on the assumption that comment isexplicitly linked to the corresponding news. In this paper, we observed thatusers' comments are also heavily influenced by their individual characteristicsembodied by the interaction history. Therefore, we position to understand newscomment behavior by considering both the dispositional factors from newsinteraction history, and the situational factors from corresponding news. Athree-part encoder-decoder framework is proposed to model the generativeprocess of news comment. The resultant dispositional and situationalattribution contributes to understanding user focus and opinions, which arevalidated in applications of reader-aware news summarization and newsaspect-opinion forecasting.</description><author>Yuhang Wang, Yuxiang Zhang, Dongyuan Lu, Jitao Sang</author><pubDate>Fri, 04 Aug 2023 08:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02168v1</guid></item><item><title>SE(3) symmetry lets graph neural networks learn arterial velocity estimation from small datasets</title><link>http://arxiv.org/abs/2302.08780v3</link><description>Hemodynamic velocity fields in coronary arteries could be the basis ofvaluable biomarkers for diagnosis, prognosis and treatment planning incardiovascular disease. Velocity fields are typically obtained frompatient-specific 3D artery models via computational fluid dynamics (CFD).However, CFD simulation requires meticulous setup by experts and istime-intensive, which hinders large-scale acceptance in clinical practice. Toaddress this, we propose graph neural networks (GNN) as an efficient black-boxsurrogate method to estimate 3D velocity fields mapped to the vertices oftetrahedral meshes of the artery lumen. We train these GNNs on synthetic arterymodels and CFD-based ground truth velocity fields. Once the GNN is trained,velocity estimates in a new and unseen artery can be obtained with 36-foldspeed-up compared to CFD. We demonstrate how to construct an SE(3)-equivariantGNN that is independent of the spatial orientation of the input mesh and showhow this reduces the necessary amount of training data compared to a baselineneural network.</description><author>Julian Suk, Christoph Brune, Jelmer M. Wolterink</author><pubDate>Fri, 04 Aug 2023 07:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08780v3</guid></item><item><title>Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling</title><link>http://arxiv.org/abs/2308.02165v1</link><description>The crystal diffusion variational autoencoder (CDVAE) is a machine learningmodel that leverages score matching to generate realistic crystal structuresthat preserve crystal symmetry. In this study, we leverage novel diffusionprobabilistic (DP) models to denoise atomic coordinates rather than adoptingthe standard score matching approach in CDVAE. Our proposed DP-CDVAE model canreconstruct and generate crystal structures whose qualities are statisticallycomparable to those of the original CDVAE. Furthermore, notably, when comparingthe carbon structures generated by the DP-CDVAE model with relaxed structuresobtained from density functional theory calculations, we find that the DP-CDVAEgenerated structures are remarkably closer to their respective ground states.The energy differences between these structures and the true ground states are,on average, 68.1 meV/atom lower than those generated by the original CDVAE.This significant improvement in the energy accuracy highlights theeffectiveness of the DP-CDVAE model in generating crystal structures thatbetter represent their ground-state configurations.</description><author>Teerachote Pakornchote, Natthaphon Choomphon-anomakhun, Sorrjit Arrerut, Chayanon Atthapak, Sakarn Khamkaeo, Thiparat Chotibut, Thiti Bovornratanaraks</author><pubDate>Fri, 04 Aug 2023 07:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02165v1</guid></item><item><title>Learning Referring Video Object Segmentation from Weak Annotation</title><link>http://arxiv.org/abs/2308.02162v1</link><description>Referring video object segmentation (RVOS) is a task that aims to segment thetarget object in all video frames based on a sentence describing the object.Previous RVOS methods have achieved significant performance withdensely-annotated datasets, whose construction is expensive and time-consuming.To relieve the burden of data annotation while maintaining sufficientsupervision for segmentation, we propose a new annotation scheme, in which welabel the frame where the object first appears with a mask and use boundingboxes for the subsequent frames. Based on this scheme, we propose a method tolearn from this weak annotation. Specifically, we design a cross framesegmentation method, which uses the language-guided dynamic filters tothoroughly leverage the valuable mask annotation and bounding boxes. We furtherdevelop a bi-level contrastive learning method to encourage the model to learndiscriminative representation at the pixel level. Extensive experiments andablative analyses show that our method is able to achieve competitiveperformance without the demand of dense mask annotation. The code will beavailable at https://github.com/wangbo-zhao/WRVOS/.</description><author>Wangbo Zhao, Kepan Nan, Songyang Zhang, Kai Chen, Dahua Lin, Yang You</author><pubDate>Fri, 04 Aug 2023 07:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02162v1</guid></item><item><title>M2Former: Multi-Scale Patch Selection for Fine-Grained Visual Recognition</title><link>http://arxiv.org/abs/2308.02161v1</link><description>Recently, vision Transformers (ViTs) have been actively applied tofine-grained visual recognition (FGVR). ViT can effectively model theinterdependencies between patch-divided object regions through an inherentself-attention mechanism. In addition, patch selection is used with ViT toremove redundant patch information and highlight the most discriminative objectpatches. However, existing ViT-based FGVR models are limited to single-scaleprocessing, and their fixed receptive fields hinder representational richnessand exacerbate vulnerability to scale variability. Therefore, we proposemulti-scale patch selection (MSPS) to improve the multi-scale capabilities ofexisting ViT-based models. Specifically, MSPS selects salient patches ofdifferent scales at different stages of a multi-scale vision Transformer(MS-ViT). In addition, we introduce class token transfer (CTT) and multi-scalecross-attention (MSCA) to model cross-scale interactions between selectedmulti-scale patches and fully reflect them in model decisions. Compared toprevious single-scale patch selection (SSPS), our proposed MSPS encouragesricher object representations based on feature hierarchy and consistentlyimproves performance from small-sized to large-sized objects. As a result, wepropose M2Former, which outperforms CNN-/ViT-based models on several widelyused FGVR benchmarks.</description><author>Jiyong Moon, Junseok Lee, Yunju Lee, Seongsik Park</author><pubDate>Fri, 04 Aug 2023 07:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02161v1</guid></item><item><title>Speaker Diarization of Scripted Audiovisual Content</title><link>http://arxiv.org/abs/2308.02160v1</link><description>The media localization industry usually requires a verbatim script of thefinal film or TV production in order to create subtitles or dubbing scripts ina foreign language. In particular, the verbatim script (i.e. as-broadcastscript) must be structured into a sequence of dialogue lines each includingtime codes, speaker name and transcript. Current speech recognition technologyalleviates the transcription step. However, state-of-the-art speakerdiarization models still fall short on TV shows for two main reasons: (i) theirinability to track a large number of speakers, (ii) their low accuracy indetecting frequent speaker changes. To mitigate this problem, we present anovel approach to leverage production scripts used during the shooting process,to extract pseudo-labeled data for the speaker diarization task. We propose anovel semi-supervised approach and demonstrate improvements of 51.7% relativeto two unsupervised baseline models on our metrics on a 66 show test set.</description><author>Yogesh Virkar, Brian Thompson, Rohit Paturi, Sundararajan Srinivasan, Marcello Federico</author><pubDate>Fri, 04 Aug 2023 07:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02160v1</guid></item><item><title>CTP-Net: Character Texture Perception Network for Document Image Forgery Localization</title><link>http://arxiv.org/abs/2308.02158v1</link><description>Due to the progression of information technology in recent years, documentimages have been widely disseminated in social networks. With the help ofpowerful image editing tools, document images are easily forged without leavingvisible manipulation traces, which leads to severe issues if significantinformation is falsified for malicious use. Therefore, the research of documentimage forensics is worth further exploring. In a document image, the characterwith specific semantic information is most vulnerable to tampering, for whichcapturing the forgery traces of the character is the key to localizing theforged region in document images. Considering both character and imagetextures, in this paper, we propose a Character Texture Perception Network(CTP-Net) to localize the forgery of document images. Based on opticalcharacter recognition, a Character Texture Stream (CTS) is designed to capturefeatures of text areas that are essential components of a document image.Meanwhile, texture features of the whole document image are exploited by anImage Texture Stream (ITS). Combining the features extracted from the CTS andthe ITS, the CTP-Net can reveal more subtle forgery traces from documentimages. To overcome the challenge caused by the lack of fake document images,we design a data generation strategy that is utilized to construct a FakeChinese Trademark dataset (FCTM). Through a series of experiments, we show thatthe proposed CTP-Net is able to capture tampering traces in document images,especially in text regions. Experimental results demonstrate that CTP-Net canlocalize multi-scale forged areas in document images and outperform thestate-of-the-art forgery localization methods.</description><author>Xin Liao, Siliang Chen, Jiaxin Chen, Tianyi Wang, Xiehua Li</author><pubDate>Fri, 04 Aug 2023 07:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02158v1</guid></item><item><title>Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling</title><link>http://arxiv.org/abs/2308.02157v1</link><description>Efficient differential equation solvers have significantly reduced thesampling time of diffusion models (DMs) while retaining high sampling quality.Among these solvers, exponential integrators (EI) have gained prominence bydemonstrating state-of-the-art performance. However, existing high-orderEI-based sampling algorithms rely on degenerate EI solvers, resulting ininferior error bounds and reduced accuracy in contrast to the theoreticallyanticipated results under optimal settings. This situation makes the samplingquality extremely vulnerable to seemingly innocuous design choices such astimestep schedules. For example, an inefficient timestep scheduler mightnecessitate twice the number of steps to achieve a quality comparable to thatobtained through carefully optimized timesteps. To address this issue, wereevaluate the design of high-order differential solvers for DMs. Through athorough order analysis, we reveal that the degeneration of existing high-orderEI solvers can be attributed to the absence of essential order conditions. Byreformulating the differential equations in DMs and capitalizing on the theoryof exponential integrators, we propose refined EI solvers that fulfill all theorder conditions, which we designate as Refined Exponential Solver (RES).Utilizing these improved solvers, RES exhibits more favorable error boundstheoretically and achieves superior sampling efficiency and stability inpractical applications. For instance, a simple switch from the single-stepDPM-Solver++ to our order-satisfied RES solver when Number of FunctionEvaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\%$and FID improvement of $25.4\%$ (16.77 vs 12.51) on a pre-trained ImageNetdiffusion model.</description><author>Qinsheng Zhang, Jiaming Song, Yongxin Chen</author><pubDate>Fri, 04 Aug 2023 07:30:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02157v1</guid></item><item><title>Competing for Shareable Arms in Multi-Player Multi-Armed Bandits</title><link>http://arxiv.org/abs/2305.19158v2</link><description>Competitions for shareable and limited resources have long been studied withstrategic agents. In reality, agents often have to learn and maximize therewards of the resources at the same time. To design an individualizedcompeting policy, we model the competition between agents in a novelmulti-player multi-armed bandit (MPMAB) setting where players are selfish andaim to maximize their own rewards. In addition, when several players pull thesame arm, we assume that these players averagely share the arms' rewards byexpectation. Under this setting, we first analyze the Nash equilibrium whenarms' rewards are known. Subsequently, we propose a novel Selfish MPMAB withAveraging Allocation (SMAA) approach based on the equilibrium. We theoreticallydemonstrate that SMAA could achieve a good regret guarantee for each playerwhen all players follow the algorithm. Additionally, we establish that nosingle selfish player can significantly increase their rewards throughdeviation, nor can they detrimentally affect other players' rewards withoutincurring substantial losses for themselves. We finally validate theeffectiveness of the method in extensive synthetic experiments.</description><author>Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, Peng Cui</author><pubDate>Fri, 04 Aug 2023 07:29:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19158v2</guid></item><item><title>SDDM: Score-Decomposed Diffusion Models on Manifolds for Unpaired Image-to-Image Translation</title><link>http://arxiv.org/abs/2308.02154v1</link><description>Recent score-based diffusion models (SBDMs) show promising results inunpaired image-to-image translation (I2I). However, existing methods, eitherenergy-based or statistically-based, provide no explicit form of the interferedintermediate generative distributions. This work presents a newscore-decomposed diffusion model (SDDM) on manifolds to explicitly optimize thetangled distributions during image generation. SDDM derives manifolds to makethe distributions of adjacent time steps separable and decompose the scorefunction or energy guidance into an image ``denoising" part and a content``refinement" part. To refine the image in the same noise level, we equalizethe refinement parts of the score function and energy guidance, which permitsmulti-objective optimization on the manifold. We also leverage the blockadaptive instance normalization module to construct manifolds with lowerdimensions but still concentrated with the perturbed reference image. SDDMoutperforms existing SBDM-based methods with much fewer diffusion steps onseveral I2I benchmarks.</description><author>Shikun Sun, Longhui Wei, Junliang Xing, Jia Jia, Qi Tian</author><pubDate>Fri, 04 Aug 2023 07:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02154v1</guid></item><item><title>Robust Self-Supervised Extrinsic Self-Calibration</title><link>http://arxiv.org/abs/2308.02153v1</link><description>Autonomous vehicles and robots need to operate over a wide variety ofscenarios in order to complete tasks efficiently and safely. Multi-cameraself-supervised monocular depth estimation from videos is a promising way toreason about the environment, as it generates metrically scaled geometricpredictions from visual data without requiring additional sensors. However,most works assume well-calibrated extrinsics to fully leverage thismulti-camera setup, even though accurate and efficient calibration is still achallenging problem. In this work, we introduce a novel method for extrinsiccalibration that builds upon the principles of self-supervised monocular depthand ego-motion learning. Our proposed curriculum learning strategy usesmonocular depth and pose estimators with velocity supervision to estimateextrinsics, and then jointly learns extrinsic calibration along with depth andpose for a set of overlapping cameras rigidly attached to a moving vehicle.Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that ourmethod enables self-calibration in various scenes robustly and efficientlycompared to a traditional vision-based pose estimation pipeline. Furthermore,we demonstrate the benefits of extrinsics self-calibration as a way to improvedepth prediction via joint optimization.</description><author>Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus</author><pubDate>Fri, 04 Aug 2023 07:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02153v1</guid></item><item><title>Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution</title><link>http://arxiv.org/abs/2307.00925v3</link><description>Semantic similarity measures are widely used in natural language processingto catalyze various computer-related tasks. However, no single semanticsimilarity measure is the most appropriate for all tasks, and researchers oftenuse ensemble strategies to ensure performance. This research work proposes amethod for automatically designing semantic similarity ensembles. In fact, ourproposed method uses grammatical evolution, for the first time, toautomatically select and aggregate measures from a pool of candidates to createan ensemble that maximizes correlation to human judgment. The method isevaluated on several benchmark datasets and compared to state-of-the-artensembles, showing that it can significantly improve similarity assessmentaccuracy and outperform existing methods in some cases. As a result, ourresearch demonstrates the potential of using grammatical evolution toautomatically compare text and prove the benefits of using ensembles forsemantic similarity tasks. The source code that illustrates our approach can bedownloaded from https://github.com/jorge-martinez-gil/sesige.</description><author>Jorge Martinez-Gil</author><pubDate>Fri, 04 Aug 2023 07:15:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00925v3</guid></item><item><title>Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</title><link>http://arxiv.org/abs/2308.02151v1</link><description>Recent months have seen the emergence of a powerful new trend in which largelanguage models (LLMs) are augmented to become autonomous language agentscapable of performing objective oriented multi-step tasks on their own, ratherthan merely responding to queries from human users. Most existing languageagents, however, are not optimized using environment-specific rewards. Althoughsome agents enable iterative refinement through verbal feedback, they do notreason and plan in ways that are compatible with gradient-based learning fromrewards. This paper introduces a principled framework for reinforcing largelanguage agents by learning a retrospective model, which automatically tunesthe language agent prompts from environment feedback through policy gradient.Specifically, our proposed agent architecture learns from rewards acrossmultiple environments and tasks, for fine-tuning a pre-trained language modelwhich refines the language agent prompt by summarizing the root cause of priorfailed attempts and proposing action plans. Experimental results on varioustasks demonstrate that the language agents improve over time and that ourapproach considerably outperforms baselines that do not properly leveragegradients from the environment. This demonstrates that using policy gradientoptimization to improve language agents, for which we believe our work is oneof the first, seems promising and can be applied to optimize other models inthe agent architecture to enhance agent performances over time.</description><author>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</author><pubDate>Fri, 04 Aug 2023 07:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02151v1</guid></item><item><title>How Informative is the Approximation Error from Tensor Decomposition for Neural Network Compression?</title><link>http://arxiv.org/abs/2305.05318v2</link><description>Tensor decompositions have been successfully applied to compress neuralnetworks. The compression algorithms using tensor decompositions commonlyminimize the approximation error on the weights. Recent work assumes theapproximation error on the weights is a proxy for the performance of the modelto compress multiple layers and fine-tune the compressed model. Surprisingly,little research has systematically evaluated which approximation errors can beused to make choices regarding the layer, tensor decomposition method, andlevel of compression. To close this gap, we perform an experimental study totest if this assumption holds across different layers and types ofdecompositions, and what the effect of fine-tuning is. We include theapproximation error on the features resulting from a compressed layer in ouranalysis to test if this provides a better proxy, as it explicitly takes thedata into account. We find the approximation error on the weights has apositive correlation with the performance error, before as well as afterfine-tuning. Basing the approximation error on the features does not improvethe correlation significantly. While scaling the approximation error commonlyis used to account for the different sizes of layers, the average correlationacross layers is smaller than across all choices (i.e. layers, decompositions,and level of compression) before fine-tuning. When calculating the correlationacross the different decompositions, the average rank correlation is largerthan across all choices. This means multiple decompositions can be consideredfor compression and the approximation error can be used to choose between them.</description><author>Jetze T. Schuurmans, Kim Batselier, Julian F. P. Kooij</author><pubDate>Fri, 04 Aug 2023 07:11:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05318v2</guid></item><item><title>G3Detector: General GPT-Generated Text Detector</title><link>http://arxiv.org/abs/2305.12680v2</link><description>The burgeoning progress in the field of Large Language Models (LLMs) heraldssignificant benefits due to their unparalleled capacities. However, it iscritical to acknowledge the potential misuse of these models, which could giverise to a spectrum of social and ethical dilemmas. Despite numerous precedingefforts centered around distinguishing synthetic text, most existing detectionsystems fail to identify data synthesized by the latest LLMs, such as ChatGPTand GPT-4. In response to this challenge, we introduce an unpretentious yetpotent detection approach proficient in identifying synthetic text across awide array of fields. Moreover, our detector demonstrates outstandingperformance uniformly across various model architectures and decodingstrategies. It also possesses the capability to identify text generatedutilizing a potent detection-evasion technique. Our comprehensive researchunderlines our commitment to boosting the robustness and efficiency ofmachine-generated text detection mechanisms, particularly in the context ofswiftly progressing and increasingly adaptive AI technologies.</description><author>Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, Pontus Stenetorp</author><pubDate>Fri, 04 Aug 2023 07:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12680v2</guid></item><item><title>Bridging Language and Geometric Primitives for Zero-shot Point Cloud Segmentation</title><link>http://arxiv.org/abs/2210.09923v2</link><description>We investigate transductive zero-shot point cloud semantic segmentation,where the network is trained on seen objects and able to segment unseenobjects. The 3D geometric elements are essential cues to imply a novel 3Dobject type. However, previous methods neglect the fine-grained relationshipbetween the language and the 3D geometric elements. To this end, we propose anovel framework to learn the geometric primitives shared in seen and unseencategories' objects and employ a fine-grained alignment between language andthe learned geometric primitives. Therefore, guided by language, the networkrecognizes the novel objects represented with geometric primitives.Specifically, we formulate a novel point visual representation, the similarityvector of the point's feature to the learnable prototypes, where the prototypesautomatically encode geometric primitives via back-propagation. Besides, wepropose a novel Unknown-aware InfoNCE Loss to fine-grained align the visualrepresentation with language. Extensive experiments show that our methodsignificantly outperforms other state-of-the-art methods in the harmonicmean-intersection-over-union (hIoU), with the improvement of 17.8\%, 30.4\%,9.2\% and 7.9\% on S3DIS, ScanNet, SemanticKITTI and nuScenes datasets,respectively. Codes are available(https://github.com/runnanchen/Zero-Shot-Point-Cloud-Segmentation)</description><author>Runnan Chen, Xinge Zhu, Nenglun Chen, Wei Li, Yuexin Ma, Ruigang Yang, Wenping Wang</author><pubDate>Fri, 04 Aug 2023 06:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.09923v2</guid></item><item><title>Optimization on Pareto sets: On a theory of multi-objective optimization</title><link>http://arxiv.org/abs/2308.02145v1</link><description>In multi-objective optimization, a single decision vector must balance thetrade-offs between many objectives. Solutions achieving an optimal trade-offare said to be Pareto optimal: these are decision vectors for which improvingany one objective must come at a cost to another. But as the set of Paretooptimal vectors can be very large, we further consider a more practicallysignificant Pareto-constrained optimization problem, where the goal is tooptimize a preference function constrained to the Pareto set. We investigate local methods for solving this constrained optimizationproblem, which poses significant challenges because the constraint set is (i)implicitly defined, and (ii) generally non-convex and non-smooth, even when theobjectives are. We define notions of optimality and stationarity, and providean algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ tostationarity when the objectives are strongly convex and Lipschitz smooth.</description><author>Abhishek Roy, Geelon So, Yi-An Ma</author><pubDate>Fri, 04 Aug 2023 06:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02145v1</guid></item><item><title>Tweet Insights: A Visualization Platform to Extract Temporal Insights from Twitter</title><link>http://arxiv.org/abs/2308.02142v1</link><description>This paper introduces a large collection of time series data derived fromTwitter, postprocessed using word embedding techniques, as well as specializedfine-tuned language models. This data comprises the past five years andcaptures changes in n-gram frequency, similarity, sentiment and topicdistribution. The interface built on top of this data enables temporal analysisfor detecting and characterizing shifts in meaning, including complementaryinformation to trending metrics, such as sentiment and topic association overtime. We release an online demo for easy experimentation, and we share code andthe underlying aggregated data for future work. In this paper, we also discussthree case studies unlocked thanks to our platform, showcasing its potentialfor temporal linguistic analysis.</description><author>Daniel Loureiro, Kiamehr Rezaee, Talayeh Riahi, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, Jose Camacho-Collados</author><pubDate>Fri, 04 Aug 2023 06:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02142v1</guid></item><item><title>AI4GCC-Team -- Below Sea Level: Score and Real World Relevance</title><link>http://arxiv.org/abs/2307.13892v2</link><description>As our submission for track three of the AI for Global Climate Cooperation(AI4GCC) competition, we propose a negotiation protocol for use in the RICE-Nclimate-economic simulation. Our proposal seeks to address the challenges ofcarbon leakage through methods inspired by the Carbon Border AdjustmentMechanism (CBAM) and Climate Clubs (CC). We demonstrate the effectiveness ofour approach by comparing simulated outcomes to representative concentrationpathways (RCP) and shared socioeconomic pathways (SSP). Our protocol results ina temperature rise comparable to RCP 3.4/4.5 and SSP 2. Furthermore, we providean analysis of our protocol's World Trade Organization compliance,administrative and political feasibility, and ethical concerns. We recognizethat our proposal risks hurting the least developing countries, and we suggestspecific corrective measures to avoid exacerbating existing inequalities, suchas technology sharing and wealth redistribution. Future research should improvethe RICE-N tariff mechanism and implement actions allowing for theaforementioned corrective measures.</description><author>Phillip Wozny, Bram Renting, Robert Loftin, Claudia Wieners, Erman Acar</author><pubDate>Fri, 04 Aug 2023 06:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13892v2</guid></item><item><title>Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting</title><link>http://arxiv.org/abs/2307.15299v2</link><description>Accurate load forecasting plays a vital role in numerous sectors, butaccurately capturing the complex dynamics of dynamic power systems remains achallenge for traditional statistical models. For these reasons, time-seriesmodels (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonlydeployed and often experience higher success. In this paper, we analyze theefficacy of the recently developed Transformer-based Neural Network model inLoad forecasting. Transformer models have the potential to improve Loadforecasting because of their ability to learn long-range dependencies derivedfrom their Attention Mechanism. We apply several metaheuristics namelyDifferential Evolution to find the optimal hyperparameters of theTransformer-based Neural Network to produce accurate forecasts. DifferentialEvolution provides scalable, robust, global solutions to non-differentiable,multi-objective, or constrained optimization problems. Our work compares theproposed Transformer based Neural Network model integrated with differentmetaheuristic algorithms by their performance in Load forecasting based onnumerical metrics such as Mean Squared Error (MSE) and Mean Absolute PercentageError (MAPE). Our findings demonstrate the potential of metaheuristic-enhancedTransformer-based Neural Network models in Load forecasting accuracy andprovide optimal hyperparameters for each model.</description><author>Anuvab Sen, Arul Rhik Mazumder, Udayon Sen</author><pubDate>Fri, 04 Aug 2023 06:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15299v2</guid></item><item><title>Improved Algorithms for Bandit with Graph Feedback via Regret Decomposition</title><link>http://arxiv.org/abs/2205.15076v2</link><description>The problem of bandit with graph feedback generalizes both the multi-armedbandit (MAB) problem and the learning with expert advice problem by encoding ina directed graph how the loss vector can be observed in each round of the game.The mini-max regret is closely related to the structure of the feedback graphand their connection is far from being fully understood. We propose a newalgorithmic framework for the problem based on a partition of the feedbackgraph. Our analysis reveals the interplay between various parts of the graph bydecomposing the regret to the sum of the regret caused by small parts and theregret caused by their interaction. As a result, our algorithm can be viewed asan interpolation and generalization of the optimal algorithms for MAB andlearning with expert advice. Our framework unifies previous algorithms for bothstrongly observable graphs and weakly observable graphs, resulting in improvedand optimal regret bounds on a wide range of graph families including graphs ofbounded degree and strongly observable graphs with a few corrupted arms.</description><author>Yuchen He, Chihao Zhang</author><pubDate>Fri, 04 Aug 2023 06:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.15076v2</guid></item><item><title>Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks</title><link>http://arxiv.org/abs/2308.02137v1</link><description>In recent years, the concept of introducing physics to machine learning hasbecome widely popular. Most physics-inclusive ML-techniques however are stilllimited to a single geometry or a set of parametrizable geometries. Thus, thereremains the need to train a new model for a new geometry, even if it is onlyslightly modified. With this work we introduce a technique with which it ispossible to learn approximate solutions to the steady-state Navier--Stokesequations in varying geometries without the need of parametrization. Thistechnique is based on a combination of a U-Net-like CNN and well establisheddiscretization methods from the field of the finite difference method.Theresults of our physics-aware CNN are compared to a state-of-the-art data-basedapproach. Additionally, it is also shown how our approach performs whencombined with the data-based approach.</description><author>Viktor Grimm, Alexander Heinlein, Axel Klawonn</author><pubDate>Fri, 04 Aug 2023 06:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02137v1</guid></item><item><title>Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs</title><link>http://arxiv.org/abs/2212.09034v4</link><description>Graph neural networks (GNNs), as the de-facto model class for representationlearning on graphs, are built upon the multi-layer perceptrons (MLP)architecture with additional message passing layers to allow features to flowacross nodes. While conventional wisdom commonly attributes the success of GNNsto their advanced expressivity, we conjecture that this is not the main causeof GNNs' superiority in node-level prediction tasks. This paper pinpoints themajor source of GNNs' performance gain to their intrinsic generalizationcapability, by introducing an intermediate model class dubbed asP(ropagational)MLP, which is identical to standard MLP in training, but thenadopts GNN's architecture in testing. Intriguingly, we observe that PMLPsconsistently perform on par with (or even exceed) their GNN counterparts, whilebeing much more efficient in training. This finding sheds new insights intounderstanding the learning behavior of GNNs, and can be used as an analytictool for dissecting various GNN-related research problems. As an initial stepto analyze the inherent generalizability of GNNs, we show the essentialdifference between MLP and PMLP at infinite-width limit lies in the NTK featuremap in the post-training stage. Moreover, by examining their extrapolationbehavior, we find that though many GNNs and their PMLP counterparts cannotextrapolate non-linear functions for extremely out-of-distribution samples,they have greater potential to generalize to testing samples near the trainingdata range as natural advantages of GNN architectures.</description><author>Chenxiao Yang, Qitian Wu, Jiahua Wang, Junchi Yan</author><pubDate>Fri, 04 Aug 2023 06:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09034v4</guid></item><item><title>On Interpolating Experts and Multi-Armed Bandits</title><link>http://arxiv.org/abs/2307.07264v2</link><description>Learning with expert advice and multi-armed bandit are two classic onlinedecision problems which differ on how the information is observed in each roundof the game. We study a family of problems interpolating the two. For a vector$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MABindicates that the arms are partitioned into $K$ groups and the $i$-th groupcontains $m_i$ arms. Once an arm is pulled, the losses of all arms in the samegroup are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MABand design an optimal PAC algorithm for its pure exploration version,$\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss withas few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MABis $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum numberof pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Bothour upper bounds and lower bounds for $\mathbf{m}$-MAB can be extended to amore general setting, namely the bandit with graph feedback, in terms of theclique cover and related graph parameters. As consequences, we obtained tightminimax regret bounds for several families of feedback graphs.</description><author>Houshuang Chen, Yuchen He, Chihao Zhang</author><pubDate>Fri, 04 Aug 2023 06:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07264v2</guid></item><item><title>LeCo: Lightweight Compression via Learning Serial Correlations</title><link>http://arxiv.org/abs/2306.15374v2</link><description>Lightweight data compression is a key technique that allows column stores toexhibit superior performance for analytical queries. Despite a comprehensivestudy on dictionary-based encodings to approach Shannon's entropy, few priorworks have systematically exploited the serial correlation in a column forcompression. In this paper, we propose LeCo (i.e., Learned Compression), aframework that uses machine learning to remove the serial redundancy in a valuesequence automatically to achieve an outstanding compression ratio anddecompression performance simultaneously. LeCo presents a general approach tothis end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR),Delta Encoding, and Run-Length Encoding (RLE) special cases under ourframework. Our microbenchmark with three synthetic and six real-world data setsshows that a prototype of LeCo achieves a Pareto improvement on bothcompression ratio and random access speed over the existing solutions. Whenintegrating LeCo into widely-used applications, we observe up to 3.9x speed upin filter-scanning a Parquet file and a 16% increase in Rocksdb's throughput.</description><author>Yihao Liu, Xinyu Zeng, Huanchen Zhang</author><pubDate>Fri, 04 Aug 2023 06:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15374v2</guid></item><item><title>Ridge Estimation with Nonlinear Transformations</title><link>http://arxiv.org/abs/2306.05722v2</link><description>Ridge estimation is an important manifold learning technique. The goal ofthis paper is to examine the effects of nonlinear transformations on the ridgesets. The main result proves the inclusion relationship between ridges:$\cR(f\circ p)\subseteq \cR(p)$, provided that the transformation $f$ isstrictly increasing and concave on the range of the function $p$. Additionally,given an underlying true manifold $\cM$, we show that the Hausdorff distancebetween $\cR(f\circ p)$ and its projection onto $\cM$ is smaller than theHausdorff distance between $\cR(p)$ and the corresponding projection. Thismotivates us to apply an increasing and concave transformation before the ridgeestimation. In specific, we show that the power transformations$f^{q}(y)=y^q/q,-\infty&lt;q\leq 1$ are increasing and concave on $\RR_+$, andthus we can use such power transformations when $p$ is strictly positive.Numerical experiments demonstrate the advantages of the proposed methods.</description><author>Zheng Zhai, Hengchao Chen, Zhigang Yao</author><pubDate>Fri, 04 Aug 2023 05:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05722v2</guid></item><item><title>Damage Vision Mining Opportunity for Imbalanced Anomaly Detection</title><link>http://arxiv.org/abs/2307.12676v2</link><description>In past decade, previous balanced datasets have been used to advancealgorithms for classification, object detection, semantic segmentation, andanomaly detection in industrial applications. Specifically, for condition-basedmaintenance, automating visual inspection is crucial to ensure high quality.Deterioration prognostic attempts to optimize the fine decision process forpredictive maintenance and proactive repair. In civil infrastructure and livingenvironment, damage data mining cannot avoid the imbalanced data issue becauseof rare unseen events and high quality status by improved operations. Forvisual inspection, deteriorated class acquired from the surface of concrete andsteel components are occasionally imbalanced. From numerous related surveys, wesummarize that imbalanced data problems can be categorized into four types; 1)missing range of target and label valuables, 2) majority-minority classimbalance, 3) foreground-background of spatial imbalance, 4) long-tailed classof pixel-wise imbalance. Since 2015, there has been many imbalanced studiesusing deep learning approaches that includes regression, image classification,object detection, semantic segmentation. However, anomaly detection forimbalanced data is not yet well known. In the study, we highlight one-classanomaly detection application whether anomalous class or not, and demonstrateclear examples on imbalanced vision datasets: blood smear, lung infection,wooden, concrete deterioration, and disaster damage. We provide key results ondamage vision mining advantage, hypothesizing that the more effective range ofpositive ratio, the higher accuracy gain of anomaly detection application.Finally, the applicability of the damage learning methods, limitations, andfuture works are mentioned.</description><author>Takato Yasuno</author><pubDate>Fri, 04 Aug 2023 05:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12676v2</guid></item><item><title>MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior</title><link>http://arxiv.org/abs/2308.01568v2</link><description>In recent years, many deep learning-based methods have been proposed totackle the problem of optical flow estimation and achieved promising results.However, they hardly consider that most videos are compressed and thus ignorethe pre-computed information in compressed video streams. Motion vectors, oneof the compression information, record the motion of the video frames. They canbe directly extracted from the compression code stream without computationalcost and serve as a solid prior for optical flow estimation. Therefore, wepropose an optical flow model, MVFlow, which uses motion vectors to improve thespeed and accuracy of optical flow estimation for compressed videos. In detail,MVFlow includes a key Motion-Vector Converting Module, which ensures that themotion vectors can be transformed into the same domain of optical flow and thenbe utilized fully by the flow estimation module. Meanwhile, we construct fouroptical flow datasets for compressed videos containing frames and motionvectors in pairs. The experimental results demonstrate the superiority of ourproposed MVFlow, which can reduce the AEPE by 1.09 compared to existing modelsor save 52% time to achieve similar accuracy to existing models.</description><author>Shili Zhou, Xuhao Jiang, Weimin Tan, Ruian He, Bo Yan</author><pubDate>Fri, 04 Aug 2023 05:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01568v2</guid></item><item><title>Semantics-guided Transformer-based Sensor Fusion for Improved Waypoint Prediction</title><link>http://arxiv.org/abs/2308.02126v1</link><description>Sensor fusion approaches for intelligent self-driving agents remain key todriving scene understanding given visual global contexts acquired from inputsensors. Specifically, for the local waypoint prediction task, single-modalitynetworks are still limited by strong dependency on the sensitivity of the inputsensor, and thus recent works promote the use of multiple sensors in fusion infeature level. While it is well known that multiple data modalities promotemutual contextual exchange, deployment to practical driving scenarios requiresglobal 3D scene understanding in real-time with minimal computations, thusplacing greater significance on training strategies given a limited number ofpractically usable sensors. In this light, we exploit carefully selectedauxiliary tasks that are highly correlated with the target task of interest(e.g., traffic light recognition and semantic segmentation) by fusing auxiliarytask features and also using auxiliary heads for waypoint prediction based onimitation learning. Our multi-task feature fusion augments and improves thebase network, TransFuser, by significant margins for safer and more completeroad navigation in CARLA simulator as validated on the Town05 Benchmark throughextensive experiments.</description><author>Hwan-Soo Choi, Jongoh Jeong, Young Hoo Cho, Kuk-Jin Yoon, Jong-Hwan Kim</author><pubDate>Fri, 04 Aug 2023 04:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02126v1</guid></item><item><title>Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning</title><link>http://arxiv.org/abs/2303.12745v2</link><description>Deception detection in conversations is a challenging yet important task,having pivotal applications in many fields such as credibility assessment inbusiness, multimedia anti-frauds, and custom security. Despite this, deceptiondetection research is hindered by the lack of high-quality deception datasets,as well as the difficulties of learning multimodal features effectively. Toaddress this issue, we introduce DOLOS\footnote {The name ``DOLOS" comes fromGreek mythology.}, the largest gameshow deception detection dataset with richdeceptive conversations. DOLOS includes 1,675 video clips featuring 213subjects, and it has been labeled with audio-visual feature annotations. Weprovide train-test, duration, and gender protocols to investigate the impact ofdifferent factors. We benchmark our dataset on previously proposed deceptiondetection approaches. To further improve the performance by fine-tuning fewerparameters, we propose Parameter-Efficient Crossmodal Learning (PECL), where aUniform Temporal Adapter (UT-Adapter) explores temporal attention intransformer-based architectures, and a crossmodal fusion module, Plug-inAudio-Visual Fusion (PAVF), combines crossmodal information from audio-visualfeatures. Based on the rich fine-grained audio-visual annotations on DOLOS, wealso exploit multi-task learning to enhance performance by concurrentlypredicting deception and audio-visual features. Experimental resultsdemonstrate the desired quality of the DOLOS dataset and the effectiveness ofthe PECL. The DOLOS dataset and the source codes are available athttps://github.com/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning/tree/main.</description><author>Xiaobao Guo, Nithish Muthuchamy Selvaraj, Zitong Yu, Adams Wai-Kin Kong, Bingquan Shen, Alex Kot</author><pubDate>Fri, 04 Aug 2023 04:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12745v2</guid></item></channel></rss>