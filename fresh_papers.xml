<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 08 Oct 2024 01:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Estimating Body and Hand Motion in an Ego-sensed World</title><link>http://arxiv.org/abs/2410.03665v1</link><description>We present EgoAllo, a system for human motion estimation from a head-mounteddevice. Using only egocentric SLAM poses and images, EgoAllo guides samplingfrom a conditional diffusion model to estimate 3D body pose, height, and handparameters that capture the wearer's actions in the allocentric coordinateframe of the scene. To achieve this, our key insight is in representation: wepropose spatial and temporal invariance criteria for improving modelperformance, from which we derive a head motion conditioning parameterizationthat improves estimation by up to 18%. We also show how the bodies estimated byour system can improve the hands: the resulting kinematic and temporalconstraints result in over 40% lower hand estimation errors compared to noisymonocular estimates. Project page: https://egoallo.github.io/</description><author>Brent Yi, Vickie Ye, Maya Zheng, Lea MÃ¼ller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa</author><pubDate>Fri, 04 Oct 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03665v1</guid></item><item><title>Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models</title><link>http://arxiv.org/abs/2410.03663v1</link><description>Large language models (LLMs) have exhibited complex reasoning abilities bygenerating question rationales and demonstrated exceptional performance innatural language processing (NLP) tasks. However, these reasoning capabilitiesgenerally emerge in models with tens of billions of parameters, creatingsignificant computational challenges for real-world deployment. Recent researchhas concentrated on improving open-source smaller models through knowledgedistillation (KD) from commercial LLMs. Nevertheless, most of these studiesrely solely on the responses from one single LLM as the gold rationale fortraining. In this paper, we introduce a novel Mistake-Aware Peer-ReviewDistillation (MAPD) approach: 1) Instead of merely obtaining gold rationalesfrom teachers, our method asks teachers to identify and explain the student'smistakes, providing customized instruction learning data. 2) We design asimulated peer-review process between teacher LLMs, which selects only thegenerated rationales above the acceptance threshold. This reduces the chance ofteachers guessing correctly with flawed rationale, improving instructional dataquality. Comprehensive experiments and analysis on mathematical, commonsense,and logical reasoning tasks demonstrate the effectiveness of our method.</description><author>Zhuochun Li, Yuelyu Ji, Rui Meng, Daqing He</author><pubDate>Fri, 04 Oct 2024 17:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03663v1</guid></item><item><title>System 2 reasoning capabilities are nigh</title><link>http://arxiv.org/abs/2410.03662v1</link><description>In recent years, machine learning models have made strides towards human-likereasoning capabilities from several directions. In this work, we review thecurrent state of the literature and describe the remaining steps to achieve aneural model which can perform System 2 reasoning analogous to a human. Weargue that if current models are insufficient to be classed as performingreasoning, there remains very little additional progress needed to attain thatgoal.</description><author>Scott C. Lowe</author><pubDate>Fri, 04 Oct 2024 17:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03662v1</guid></item><item><title>Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models</title><link>http://arxiv.org/abs/2410.03659v1</link><description>Large Vision-Language Models (LVLMs) have demonstrated impressivecapabilities for capturing and reasoning over multimodal inputs. However, thesemodels are prone to parametric knowledge conflicts, which arise frominconsistencies of represented knowledge between their vision and languagecomponents. In this paper, we formally define the problem of$\textbf{cross-modality parametric knowledge conflict}$ and present asystematic approach to detect, interpret, and mitigate them. We introduce apipeline that identifies conflicts between visual and textual answers, showinga persistently high conflict rate across modalities in recent LVLMs regardlessof the model size. We further investigate how these conflicts interfere withthe inference process and propose a contrastive metric to discern theconflicting samples from the others. Building on these insights, we develop anovel dynamic contrastive decoding method that removes undesirable logitsinferred from the less confident modality components based on answerconfidence. For models that do not provide logits, we also introduce twoprompt-based strategies to mitigate the conflicts. Our methods achievepromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.Specifically, using LLaVA-34B, our proposed dynamic contrastive decodingimproves an average accuracy of 2.24%.</description><author>Tinghui Zhu, Qin Liu, Fei Wang, Zhengzhong Tu, Muhao Chen</author><pubDate>Fri, 04 Oct 2024 17:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03659v1</guid></item><item><title>RAFT: Realistic Attacks to Fool Text Detectors</title><link>http://arxiv.org/abs/2410.03658v1</link><description>Large language models (LLMs) have exhibited remarkable fluency across varioustasks. However, their unethical applications, such as disseminatingdisinformation, have become a growing concern. Although recent works haveproposed a number of LLM detection methods, their robustness and reliabilityremain unclear. In this paper, we present RAFT: a grammar error-free black-boxattack against existing LLM detectors. In contrast to previous attacks forlanguage models, our method exploits the transferability of LLM embeddings atthe word-level while preserving the original text quality. We leverage anauxiliary embedding to greedily select candidate words to perturb against thetarget detector. Experiments reveal that our attack effectively compromises alldetectors in the study across various domains by up to 99%, and aretransferable across source models. Manual human evaluation studies show ourattacks are realistic and indistinguishable from original human-written text.We also show that examples generated by RAFT can be used to train adversariallyrobust detectors. Our work shows that current LLM detectors are notadversarially robust, underscoring the urgent need for more resilient detectionmechanisms.</description><author>James Wang, Ran Li, Junfeng Yang, Chengzhi Mao</author><pubDate>Fri, 04 Oct 2024 17:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03658v1</guid></item><item><title>DiffusionPID: Interpreting Diffusion via Partial Information Decomposition</title><link>http://arxiv.org/abs/2406.05191v3</link><description>Text-to-image diffusion models have made significant progress in generatingnaturalistic images from textual inputs, and demonstrate the capacity to learnand represent complex visual-semantic relationships. While these diffusionmodels have achieved remarkable success, the underlying mechanisms drivingtheir performance are not yet fully accounted for, with many unansweredquestions surrounding what they learn, how they represent visual-semanticrelationships, and why they sometimes fail to generalize. Our work presentsDiffusion Partial Information Decomposition (DiffusionPID), a novel techniquethat applies information-theoretic principles to decompose the input textprompt into its elementary components, enabling a detailed examination of howindividual tokens and their interactions shape the generated image. Weintroduce a formal approach to analyze the uniqueness, redundancy, and synergyterms by applying PID to the denoising model at both the image and pixel level.This approach enables us to characterize how individual tokens and theirinteractions affect the model output. We first present a fine-grained analysisof characteristics utilized by the model to uniquely localize specificconcepts, we then apply our approach in bias analysis and show it can recovergender and ethnicity biases. Finally, we use our method to visuallycharacterize word ambiguity and similarity from the model's perspective andillustrate the efficacy of our method for prompt intervention. Our results showthat PID is a potent tool for evaluating and diagnosing text-to-image diffusionmodels.</description><author>Shaurya Dewan, Rushikesh Zawar, Prakanshul Saxena, Yingshan Chang, Andrew Luo, Yonatan Bisk</author><pubDate>Fri, 04 Oct 2024 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05191v3</guid></item><item><title>Geometric Representation Condition Improves Equivariant Molecule Generation</title><link>http://arxiv.org/abs/2410.03655v1</link><description>Recent advancements in molecular generative models have demonstratedsubstantial potential in accelerating scientific discovery, particularly indrug design. However, these models often face challenges in generatinghigh-quality molecules, especially in conditional scenarios where specificmolecular properties must be satisfied. In this work, we introduce GeoRCG, ageneral framework to enhance the performance of molecular generative models byintegrating geometric representation conditions. We decompose the moleculegeneration process into two stages: first, generating an informative geometricrepresentation; second, generating a molecule conditioned on therepresentation. Compared to directly generating a molecule, the relativelyeasy-to-generate representation in the first-stage guides the second-stagegeneration to reach a high-quality molecule in a more goal-oriented and muchfaster way. Leveraging EDM as the base generator, we observe significantquality improvements in unconditional molecule generation on the widely-usedQM9 and GEOM-DRUG datasets. More notably, in the challenging conditionalmolecular generation task, our framework achieves an average 31\% performanceimprovement over state-of-the-art approaches, highlighting the superiority ofconditioning on semantically rich geometric representations over conditioningon individual property values as in previous approaches. Furthermore, we showthat, with such representation guidance, the number of diffusion steps can bereduced to as small as 100 while maintaining superior generation quality thanthat achieved with 1,000 steps, thereby significantly accelerating thegeneration process.</description><author>Zian Li, Cai Zhou, Xiyuan Wang, Xingang Peng, Muhan Zhang</author><pubDate>Fri, 04 Oct 2024 17:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03655v1</guid></item><item><title>Learning Humanoid Locomotion over Challenging Terrain</title><link>http://arxiv.org/abs/2410.03654v1</link><description>Humanoid robots can, in principle, use their legs to go almost anywhere.Developing controllers capable of traversing diverse terrains, however, remainsa considerable challenge. Classical controllers are hard to generalize broadlywhile the learning-based methods have primarily focused on gentle terrains.Here, we present a learning-based approach for blind humanoid locomotioncapable of traversing challenging natural and man-made terrain. Our method usesa transformer model to predict the next action based on the history ofproprioceptive observations and actions. The model is first pre-trained on adataset of flat-ground trajectories with sequence modeling, and then fine-tunedon uneven terrain using reinforcement learning. We evaluate our model on a realhumanoid robot across a variety of terrains, including rough, deformable, andsloped surfaces. The model demonstrates robust performance, in-contextadaptation, and emergent terrain representations. In real-world case studies,our humanoid robot successfully traversed over 4 miles of hiking trails inBerkeley and climbed some of the steepest streets in San Francisco.</description><author>Ilija Radosavovic, Sarthak Kamat, Trevor Darrell, Jitendra Malik</author><pubDate>Fri, 04 Oct 2024 17:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03654v1</guid></item><item><title>Minimax-optimal trust-aware multi-armed bandits</title><link>http://arxiv.org/abs/2410.03651v1</link><description>Multi-armed bandit (MAB) algorithms have achieved significant success insequential decision-making applications, under the premise that humansperfectly implement the recommended policy. However, existing methods oftenoverlook the crucial factor of human trust in learning algorithms. When trustis lacking, humans may deviate from the recommended policy, leading toundesired learning performance. Motivated by this gap, we study the trust-awareMAB problem by integrating a dynamic trust model into the standard MABframework. Specifically, it assumes that the recommended and actuallyimplemented policy differs depending on human trust, which in turn evolves withthe quality of the recommended policy. We establish the minimax regret in thepresence of the trust issue and demonstrate the suboptimality of vanilla MABalgorithms such as the upper confidence bound (UCB) algorithm. To overcome thislimitation, we introduce a novel two-stage trust-aware procedure that provablyattains near-optimal statistical guarantees. A simulation study is conducted toillustrate the benefits of our proposed algorithm when dealing with the trustissue.</description><author>Changxiao Cai, Jiacheng Zhang</author><pubDate>Fri, 04 Oct 2024 17:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03651v1</guid></item><item><title>GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs</title><link>http://arxiv.org/abs/2410.03645v1</link><description>Robotic simulation today remains challenging to scale up due to the humanefforts required to create diverse simulation tasks and scenes.Simulation-trained policies also face scalability issues as many sim-to-realmethods focus on a single task. To address these challenges, this work proposesGenSim2, a scalable framework that leverages coding LLMs with multi-modal andreasoning capabilities for complex and realistic simulation task creation,including long-horizon tasks with articulated objects. To automaticallygenerate demonstration data for these tasks at scale, we propose planning andRL solvers that generalize within object categories. The pipeline can generatedata for up to 100 articulated tasks with 200 objects and reduce the requiredhuman efforts. To utilize such data, we propose an effective multi-tasklanguage-conditioned policy architecture, dubbed proprioceptive point-cloudtransformer (PPT), that learns from the generated demonstrations and exhibitsstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and thepolicy architecture, we show a promising usage of GenSim2 that the generateddata can be used for zero-shot transfer or co-train with real-world collecteddata, which enhances the policy performance by 20% compared with trainingexclusively on limited real data.</description><author>Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, Lirui Wang</author><pubDate>Fri, 04 Oct 2024 17:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03645v1</guid></item><item><title>Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and Hindsight Modular Reflections for Task Planning with LLMs</title><link>http://arxiv.org/abs/2408.06520v2</link><description>Large Language Models (LLMs) have demonstrated remarkable abilities invarious language tasks, making them promising candidates for decision-making inrobotics. Inspired by Hierarchical Reinforcement Learning (HRL), we proposeRetrieval-Augmented in-context reinforcement Learning (RAHL), a novel frameworkthat decomposes complex tasks into sub-tasks using an LLM-based high-levelpolicy, in which a complex task is decomposed into sub-tasks by a high-levelpolicy on-the-fly. The sub-tasks, defined by goals, are assigned to thelow-level policy to complete. To improve the agent's performance inmulti-episode execution, we propose Hindsight Modular Reflection (HMR), where,instead of reflecting on the full trajectory, we let the agent reflect onshorter sub-trajectories to improve reflection efficiency. We evaluated thedecision-making ability of the proposed RAHL in three benchmarkenvironments--ALFWorld, Webshop, and HotpotQA. The results show that RAHL canachieve an improvement in performance in 9%, 42%, and 10% in 5 episodes ofexecution in strong baselines. Furthermore, we also implemented RAHL on theBoston Dynamics SPOT robot. The experiment shows that the robot can scan theenvironment, find entrances, and navigate to new rooms controlled by the LLMpolicy.</description><author>Chuanneng Sun, Songjun Huang, Dario Pompili</author><pubDate>Fri, 04 Oct 2024 17:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06520v2</guid></item><item><title>Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need</title><link>http://arxiv.org/abs/2410.03644v1</link><description>Traditional unlearnable strategies have been proposed to prevent unauthorizedusers from training on the 2D image data. With more 3D point cloud datacontaining sensitivity information, unauthorized usage of this new type datahas also become a serious concern. To address this, we propose the firstintegral unlearnable framework for 3D point clouds including two processes: (i)we propose an unlearnable data protection scheme, involving a class-wisesetting established by a category-adaptive allocation strategy andmulti-transformations assigned to samples; (ii) we propose a data restorationscheme that utilizes class-wise inverse matrix transformation, thus enablingauthorized-only training for unlearnable data. This restoration process is apractical issue overlooked in most existing unlearnable literature, \ie, evenauthorized users struggle to gain knowledge from 3D unlearnable data. Boththeoretical and empirical results (including 6 datasets, 16 models, and 2tasks) demonstrate the effectiveness of our proposed unlearnable framework. Ourcode is available at \url{https://github.com/CGCL-codes/UnlearnablePC}</description><author>Xianlong Wang, Minghui Li, Wei Liu, Hangtao Zhang, Shengshan Hu, Yechao Zhang, Ziqi Zhou, Hai Jin</author><pubDate>Fri, 04 Oct 2024 17:49:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03644v1</guid></item><item><title>Aligning LLMs with Individual Preferences via Interaction</title><link>http://arxiv.org/abs/2410.03642v1</link><description>As large language models (LLMs) demonstrate increasingly advancedcapabilities, aligning their behaviors with human values and preferencesbecomes crucial for their wide adoption. While previous research focuses ongeneral alignment to principles such as helpfulness, harmlessness, and honesty,the need to account for individual and diverse preferences has been largelyoverlooked, potentially undermining customized human experiences. To addressthis gap, we train LLMs that can ''interact to align'', essentially cultivatingthe meta-skill of LLMs to implicitly infer the unspoken personalizedpreferences of the current user through multi-turn conversations, and thendynamically align their following behaviors and responses to these inferredpreferences. Our approach involves establishing a diverse pool of 3,310distinct user personas by initially creating seed examples, which are thenexpanded through iterative self-generation and filtering. Guided by distinctuser personas, we leverage multi-LLM collaboration to develop a multi-turnpreference dataset containing 3K+ multi-turn conversations in tree structures.Finally, we apply supervised fine-tuning and reinforcement learning to enhanceLLMs using this dataset. For evaluation, we establish the ALOE (ALign WithCustOmized PrEferences) benchmark, consisting of 100 carefully selectedexamples and well-designed metrics to measure the customized alignmentperformance during conversations. Experimental results demonstrate theeffectiveness of our method in enabling dynamic, personalized alignment viainteraction.</description><author>Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, Heng Ji</author><pubDate>Fri, 04 Oct 2024 17:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03642v1</guid></item><item><title>Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices</title><link>http://arxiv.org/abs/2410.02117v2</link><description>Dense linear layers are the dominant computational bottleneck in large neuralnetworks, presenting a critical need for more efficient alternatives. Previousefforts focused on a small number of hand-crafted structured matrices andneglected to investigate whether these structures can surpass dense layers interms of compute-optimal scaling laws when both the model size and trainingexamples are optimally allocated. In this work, we present a unifying frameworkthat enables searching among all linear operators expressible via an Einsteinsummation. This framework encompasses many previously proposed structures, suchas low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,along with many novel structures. To analyze the framework, we develop ataxonomy of all such operators based on their computational and algebraicproperties and show that differences in the compute-optimal scaling laws aremostly governed by a small number of variables that we introduce. Namely, asmall $\omega$ (which measures parameter sharing) and large $\psi$ (whichmeasures the rank) reliably led to better scaling laws. Guided by the insightthat full-rank structures that maximize parameters per unit of compute performthe best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architectureobtained by sparsifying computation in the BTT structure. In contrast to thestandard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoEin every single linear layer of the model, including the projection matrices inthe attention blocks. We find BTT-MoE provides a substantial compute-efficiencygain over dense layers and standard MoE.</description><author>Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen, Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson</author><pubDate>Fri, 04 Oct 2024 17:47:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02117v2</guid></item><item><title>Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models</title><link>http://arxiv.org/abs/2410.03640v1</link><description>Membership inference attacks (MIAs) on diffusion models have emerged aspotential evidence of unauthorized data usage in training pre-trained diffusionmodels. These attacks aim to detect the presence of specific images in trainingdatasets of diffusion models. Our study delves into the evaluation ofstate-of-the-art MIAs on diffusion models and reveals critical flaws and overlyoptimistic performance estimates in existing MIA evaluation. We introduceCopyMark, a more realistic MIA benchmark that distinguishes itself through thesupport for pre-trained diffusion models, unbiased datasets, and fairevaluation pipelines. Through extensive experiments, we demonstrate that theeffectiveness of current MIA methods significantly degrades under these morepractical conditions. Based on our results, we alert that MIA, in its currentstate, is not a reliable approach for identifying unauthorized data usage inpre-trained diffusion models. To the best of our knowledge, we are the first todiscover the performance overestimation of MIAs on diffusion models and presenta unified benchmark for more realistic evaluation. Our code is available onGitHub: \url{https://github.com/caradryanl/CopyMark}.</description><author>Chumeng Liang, Jiaxuan You</author><pubDate>Fri, 04 Oct 2024 17:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03640v1</guid></item><item><title>Scattering Spectra Models for Physics</title><link>http://arxiv.org/abs/2306.17210v2</link><description>Physicists routinely need probabilistic models for a number of tasks such asparameter inference or the generation of new realizations of a field.Establishing such models for highly non-Gaussian fields is a challenge,especially when the number of samples is limited. In this paper, we introducescattering spectra models for stationary fields and we show that they provideaccurate and robust statistical descriptions of a wide range of fieldsencountered in physics. These models are based on covariances of scatteringcoefficients, i.e. wavelet decomposition of a field coupled with a point-wisemodulus. After introducing useful dimension reductions taking advantage of theregularity of a field under rotation and scaling, we validate these models onvarious multi-scale physical fields and demonstrate that they reproducestandard statistics, including spatial moments up to 4th order. Thesescattering spectra provide us with a low-dimensional structured representationthat captures key properties encountered in a wide range of physical fields.These generic models can be used for data exploration, classification,parameter inference, symmetry detection, and component separation.</description><author>Sihao Cheng, Rudy Morel, Erwan Allys, Brice MÃ©nard, StÃ©phane Mallat</author><pubDate>Fri, 04 Oct 2024 17:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17210v2</guid></item><item><title>Conditional Enzyme Generation Using Protein Language Models with Adapters</title><link>http://arxiv.org/abs/2410.03634v1</link><description>The conditional generation of proteins with desired functions and/orproperties is a key goal for generative models. Existing methods based onprompting of language models can generate proteins conditioned on a targetfunctionality, such as a desired enzyme family. However, these methods arelimited to simple, tokenized conditioning and have not been shown to generalizeto unseen functions. In this study, we propose ProCALM (Protein ConditionallyAdapted Language Model), an approach for the conditional generation of proteinsusing adapters to protein language models. Our specific implementation ofProCALM involves finetuning ProGen2 to incorporate conditioning representationsof enzyme function and taxonomy. ProCALM matches existing methods atconditionally generating sequences from target enzyme families. Impressively,it can also generate within the joint distribution of enzymatic function andtaxonomy, and it can generalize to rare and unseen enzyme families andtaxonomies. Overall, ProCALM is a flexible and computationally efficientapproach, and we expect that it can be extended to a wide range of generativelanguage models.</description><author>Jason Yang, Aadyot Bhatnagar, Jeffrey A. Ruffolo, Ali Madani</author><pubDate>Fri, 04 Oct 2024 17:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03634v1</guid></item><item><title>Robust Offline Imitation Learning from Diverse Auxiliary Data</title><link>http://arxiv.org/abs/2410.03626v1</link><description>Offline imitation learning enables learning a policy solely from a set ofexpert demonstrations, without any environment interaction. To alleviate theissue of distribution shift arising due to the small amount of expert data,recent works incorporate large numbers of auxiliary demonstrations alongsidethe expert data. However, the performance of these approaches rely onassumptions about the quality and composition of the auxiliary data. However,they are rarely successful when those assumptions do not hold. To address thislimitation, we propose Robust Offline Imitation from Diverse Auxiliary Data(ROIDA). ROIDA first identifies high-quality transitions from the entireauxiliary dataset using a learned reward function. These high-reward samplesare combined with the expert demonstrations for weighted behavioral cloning.For lower-quality samples, ROIDA applies temporal difference learning to steerthe policy towards high-reward states, improving long-term returns. Thistwo-pronged approach enables our framework to effectively leverage both highand low-quality data without any assumptions. Extensive experiments validatethat ROIDA achieves robust and consistent performance across multiple auxiliarydatasets with diverse ratios of expert and non-expert demonstrations. ROIDAeffectively leverages unlabeled auxiliary data, outperforming prior methodsreliant on specific data assumptions.</description><author>Udita Ghosh, Dripta S. Raychaudhuri, Jiachen Li, Konstantinos Karydis, Amit K. Roy-Chowdhury</author><pubDate>Fri, 04 Oct 2024 17:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03626v1</guid></item><item><title>HyperCMR: Enhanced Multi-Contrast CMR Reconstruction with Eagle Loss</title><link>http://arxiv.org/abs/2410.03624v1</link><description>Accelerating image acquisition for cardiac magnetic resonance imaging (CMRI)is a critical task. CMRxRecon2024 challenge aims to set the state of the artfor multi-contrast CMR reconstruction. This paper presents HyperCMR, a novelframework designed to accelerate the reconstruction of multi-contrast cardiacmagnetic resonance (CMR) images. HyperCMR enhances the existing PromptMR modelby incorporating advanced loss functions, notably the innovative Eagle Loss,which is specifically designed to recover missing high-frequency information inundersampled k-space. Extensive experiments conducted on the CMRxRecon2024challenge dataset demonstrate that HyperCMR consistently outperforms thebaseline across multiple evaluation metrics, achieving superior SSIM and PSNRscores.</description><author>Ruru Xu, Caner Ãzer, Ilkay Oksuz</author><pubDate>Fri, 04 Oct 2024 17:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03624v1</guid></item><item><title>Training Language Models to Self-Correct via Reinforcement Learning</title><link>http://arxiv.org/abs/2409.12917v2</link><description>Self-correction is a highly desirable capability of large language models(LLMs), yet it has consistently been found to be largely ineffective in modernLLMs. Current methods for training self-correction typically depend on eithermultiple models, a more advanced model, or additional forms of supervision. Toaddress these shortcomings, we develop a multi-turn online reinforcementlearning (RL) approach, SCoRe, that significantly improves an LLM'sself-correction ability using entirely self-generated data. To build SCoRe, wefirst show that variants of supervised fine-tuning (SFT) on offlinemodel-generated correction traces are often insufficient for instillingself-correction behavior. In particular, we observe that training via SFT fallsprey to either a distribution mismatch between mistakes made by thedata-collection policy and the model's own responses, or to behavior collapse,where learning implicitly prefers only a certain mode of correction behaviorthat is often not effective at self-correction on test problems. SCoReaddresses these challenges by training under the model's own distribution ofself-generated correction traces and using appropriate regularization to steerthe learning process into learning a self-correction behavior that is effectiveat test time as opposed to fitting high-reward responses for a given prompt.This regularization process includes an initial phase of multi-turn RL on abase model to generate a policy initialization that is less susceptible tocollapse, followed by using a reward bonus to amplify self-correction. WithGemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achievesstate-of-the-art self-correction performance, improving the base models'self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.</description><author>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, Aleksandra Faust</author><pubDate>Fri, 04 Oct 2024 17:28:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12917v2</guid></item><item><title>SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales</title><link>http://arxiv.org/abs/2405.20974v3</link><description>Large language models (LLMs) often generate inaccurate or fabricatedinformation and generally fail to indicate their confidence, which limits theirbroader applications. Previous work elicits confidence from LLMs by direct orself-consistency prompting, or constructing specific datasets for supervisedfinetuning. The prompting-based approaches have inferior performance, and thetraining-based approaches are limited to binary or inaccurate group-levelconfidence estimates. In this work, we present the advanced SaySelf, a trainingframework that teaches LLMs to express more accurate fine-grained confidenceestimates. In addition, beyond the confidence scores, SaySelf initiates theprocess of directing LLMs to produce self-reflective rationales that clearlyidentify gaps in their parametric knowledge and explain their uncertainty. Thisis achieved by using an LLM to automatically summarize the uncertainties inspecific knowledge via natural language. The summarization is based on theanalysis of the inconsistency in multiple sampled reasoning chains, and theresulting data is utilized for supervised fine-tuning. Moreover, we utilizereinforcement learning with a meticulously crafted reward function to calibratethe confidence estimates, motivating LLMs to deliver accurate, high-confidencepredictions and to penalize overconfidence in erroneous outputs. Experimentalresults in both in-distribution and out-of-distribution datasets demonstratethe effectiveness of SaySelf in reducing the confidence calibration error andmaintaining the task performance. We show that the generated self-reflectiverationales are reasonable and can further contribute to the calibration. Thecode is made public at https://github.com/xu1868/SaySelf.</description><author>Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao</author><pubDate>Fri, 04 Oct 2024 17:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20974v3</guid></item><item><title>Wrapper Boxes: Faithful Attribution of Model Predictions to Training Data</title><link>http://arxiv.org/abs/2311.08644v3</link><description>Can we preserve the accuracy of neural models while also providing faithfulexplanations of model decisions to training data? We propose a "wrapper box''pipeline: training a neural model as usual and then using its learned featurerepresentation in classic, interpretable models to perform prediction. Acrossseven language models of varying sizes, including four large language models(LLMs), two datasets at different scales, three classic models, and fourevaluation metrics, we first show that the predictive performance of wrapperclassic models is largely comparable to the original neural models. Because classic models are transparent, each model decision is determined bya known set of training examples that can be directly shown to users. Ourpipeline thus preserves the predictive performance of neural language modelswhile faithfully attributing classic model decisions to training data. Amongother use cases, such attribution enables model decisions to be contested basedon responsible training instances. Compared to prior work, our approachachieves higher coverage and correctness in identifying which training data toremove to change a model decision. To reproduce findings, our source code isonline at: https://github.com/SamSoup/WrapperBox.</description><author>Yiheng Su, Junyi Jessy Li, Matthew Lease</author><pubDate>Fri, 04 Oct 2024 17:23:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08644v3</guid></item><item><title>A Global Medical Data Security and Privacy Preserving Standards Identification Framework for Electronic Healthcare Consumers</title><link>http://arxiv.org/abs/2410.03621v1</link><description>Electronic Health Records (EHR) are crucial for the success of digitalhealthcare, with a focus on putting consumers at the center of thistransformation. However, the digitalization of healthcare records brings alongsecurity and privacy risks for personal data. The major concern is thatdifferent countries have varying standards for the security and privacy ofmedical data. This paper proposed a novel and comprehensive framework tostandardize these rules globally, bringing them together on a common platform.To support this proposal, the study reviews existing literature to understandthe research interest in this issue. It also examines six key laws andstandards related to security and privacy, identifying twenty concepts. Theproposed framework utilized K-means clustering to categorize these concepts andidentify five key factors. Finally, an Ordinal Priority Approach is applied todetermine the preferred implementation of these factors in the context of EHRs.The proposed study provides a descriptive then prescriptive framework for theimplementation of privacy and security in the context of electronic healthrecords. Therefore, the findings of the proposed framework are useful forprofessionals and policymakers in improving the security and privacy associatedwith EHRs.</description><author>Vinaytosh Mishra, Kishu Gupta, Deepika Saxena, Ashutosh Kumar Singh</author><pubDate>Fri, 04 Oct 2024 17:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03621v1</guid></item><item><title>Open-World Reinforcement Learning over Long Short-Term Imagination</title><link>http://arxiv.org/abs/2410.03618v1</link><description>Training visual reinforcement learning agents in a high-dimensional openworld presents significant challenges. While various model-based methods haveimproved sample efficiency by learning interactive world models, these agentstend to be "short-sighted", as they are typically trained on short snippets ofimagined experiences. We argue that the primary obstacle in open-worlddecision-making is improving the efficiency of off-policy exploration across anextensive state space. In this paper, we present LS-Imagine, which extends theimagination horizon within a limited number of state transition steps, enablingthe agent to explore behaviors that potentially lead to promising long-termfeedback. The foundation of our approach is to build a long short-term worldmodel. To achieve this, we simulate goal-conditioned jumpy state transitionsand compute corresponding affordance maps by zooming in on specific areaswithin single images. This facilitates the integration of direct long-termvalues into behavior learning. Our method demonstrates significant improvementsover state-of-the-art techniques in MineDojo.</description><author>Jiajian Li, Qi Wang, Yunbo Wang, Xin Jin, Yang Li, Wenjun Zeng, Xiaokang Yang</author><pubDate>Fri, 04 Oct 2024 17:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03618v1</guid></item><item><title>What Matters for Model Merging at Scale?</title><link>http://arxiv.org/abs/2410.03617v1</link><description>Model merging aims to combine multiple expert models into a more capablesingle model, offering benefits such as reduced storage and serving costs,improved generalization, and support for decentralized model development.Despite its promise, previous studies have primarily focused on merging a fewsmall models. This leaves many unanswered questions about the effect of scalingmodel size and how it interplays with other key factors -- like the base modelquality and number of expert models -- , to affect the merged model'sperformance. This work systematically evaluates the utility of model merging atscale, examining the impact of these different factors. We experiment withmerging fully fine-tuned models using 4 popular merging methods -- Averaging,Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64Bparameters and merging up to 8 different expert models. We evaluate the mergedmodels on both held-in tasks, i.e., the expert's training tasks, and zero-shotgeneralization to unseen held-out tasks. Our experiments provide several newinsights about model merging at scale and the interplay between differentfactors. First, we find that merging is more effective when experts are createdfrom strong base models, i.e., models with good zero-shot performance. Second,larger models facilitate easier merging. Third merging consistently improvesgeneralization capabilities. Notably, when merging 8 large expert models, themerged models often generalize better compared to the multitask trained models.Fourth, we can better merge more expert models when working with larger models.Fifth, different merging methods behave very similarly at larger scales.Overall, our findings shed light on some interesting properties of modelmerging while also highlighting some limitations. We hope that this study willserve as a reference point on large-scale merging for upcoming research.</description><author>Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, Tsendsuren Munkhdalai</author><pubDate>Fri, 04 Oct 2024 17:17:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03617v1</guid></item><item><title>Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation</title><link>http://arxiv.org/abs/2410.03613v1</link><description>As large language models (LLMs) increasingly integrate into every aspect ofour work and daily lives, there are growing concerns about user privacy, whichpush the trend toward local deployment of these models. There are a number oflightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally onsmartphones, providing users with greater control over their personal data. Asa rapidly emerging application, we are concerned about their performance oncommercial-off-the-shelf mobile devices. To fully understand the currentlandscape of LLM deployment on mobile platforms, we conduct a comprehensivemeasurement study on mobile devices. We evaluate both metrics that affect userexperience, including token throughput, latency, and battery consumption, aswell as factors critical to developers, such as resource utilization, DVFSstrategies, and inference engines. In addition, we provide a detailed analysisof how these hardware capabilities and system dynamics affect on-device LLMperformance, which may help developers identify and address bottlenecks formobile LLM applications. We also provide comprehensive comparisons across themobile system-on-chips (SoCs) from major vendors, highlighting theirperformance differences in handling LLM workloads. We hope that this study canprovide insights for both the development of on-device LLMs and the design forfuture mobile system architecture.</description><author>Jie Xiao, Qianyi Huang, Xu Chen, Chen Tian</author><pubDate>Fri, 04 Oct 2024 17:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03613v1</guid></item><item><title>LInK: Learning Joint Representations of Design and Performance Spaces through Contrastive Learning for Mechanism Synthesis</title><link>http://arxiv.org/abs/2405.20592v2</link><description>In this paper, we introduce LInK, a novel framework that integratescontrastive learning of performance and design space with optimizationtechniques for solving complex inverse problems in engineering design withdiscrete and continuous variables. We focus on the path synthesis problem forplanar linkage mechanisms. By leveraging a multimodal andtransformation-invariant contrastive learning framework, LInK learns a jointrepresentation that captures complex physics and design representations ofmechanisms, enabling rapid retrieval from a vast dataset of over 10 millionmechanisms. This approach improves precision through the warm start of ahierarchical unconstrained nonlinear optimization algorithm, combining therobustness of traditional optimization with the speed and adaptability ofmodern deep learning methods. Our results on an existing benchmark demonstratethat LInK outperforms existing methods with 28 times less error compared to astate of the art approach while taking 20 times less time on an existingbenchmark. Moreover, we introduce a significantly more challenging benchmark,named LINK ABC, which involves synthesizing linkages that trace thetrajectories of English capital alphabets, an inverse design benchmark taskthat existing methods struggle with due to large nonlinearities and tinyfeasible space. Our results demonstrate that LInK not only advances the fieldof mechanism design but also broadens the applicability of contrastive learningand optimization to other areas of engineering. The code and data are publiclyavailable at https://github.com/ahnobari/LInK.</description><author>Amin Heyrani Nobari, Akash Srivastava, Dan Gutfreund, Kai Xu, Faez Ahmed</author><pubDate>Fri, 04 Oct 2024 17:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20592v2</guid></item><item><title>AID: Attention Interpolation of Text-to-Image Diffusion</title><link>http://arxiv.org/abs/2403.17924v3</link><description>Conditional diffusion models can create unseen images in various settings,aiding image interpolation. Interpolation in latent spaces is well-studied, butinterpolation with specific conditions like text or poses is less understood.Simple approaches, such as linear interpolation in the space of conditions,often result in images that lack consistency, smoothness, and fidelity. To thatend, we introduce a novel training-free technique named Attention Interpolationvia Diffusion (AID). Our key contributions include 1) proposing an inner/outerinterpolated attention layer; 2) fusing the interpolated attention withself-attention to boost fidelity; and 3) applying beta distribution toselection to increase smoothness. We also present a variant, Prompt-guidedAttention Interpolation via Diffusion (PAID), that considers interpolation as acondition-dependent generative process. This method enables the creation of newimages with greater consistency, smoothness, and efficiency, and offers controlover the exact path of interpolation. Our approach demonstrates effectivenessfor conceptual and spatial interpolation. Code and demo are available athttps://github.com/QY-H00/attention-interpolation-diffusion.</description><author>Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao</author><pubDate>Fri, 04 Oct 2024 17:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17924v3</guid></item><item><title>TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation</title><link>http://arxiv.org/abs/2410.03608v1</link><description>Given the widespread adoption and usage of Large Language Models (LLMs), itis crucial to have flexible and interpretable evaluations of theirinstruction-following ability. Preference judgments between model outputs havebecome the de facto evaluation standard, despite distilling complex,multi-faceted preferences into a single ranking. Furthermore, as humanannotation is slow and costly, LLMs are increasingly used to make thesejudgments, at the expense of reliability and interpretability. In this work, wepropose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,interpretable evaluation protocol that structures evaluations withLLM-generated, instruction-specific checklists. We first show that, given aninstruction, LLMs can reliably produce high-quality, tailored evaluationchecklists that decompose the instruction into a series of YES/NO questions.Each question asks whether a candidate response meets a specific requirement ofthe instruction. We demonstrate that using TICK leads to a significant increase(46.4% $\to$ 52.2%) in the frequency of exact agreements between LLM judgementsand human preferences, as compared to having an LLM directly score an output.We then show that STICK (Self-TICK) can be used to improve generation qualityacross multiple benchmarks via self-refinement and Best-of-N selection. STICKself-refinement on LiveBench reasoning tasks leads to an absolute gain of$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absoluteimprovement on the real-world instruction dataset, WildBench. In light of this,structured, multi-faceted self-improvement is shown to be a promising way tofurther advance LLM capabilities. Finally, by providing LLM-generatedchecklists to human evaluators tasked with directly scoring LLM responses toWildBench instructions, we notably increase inter-annotator agreement (0.194$\to$ 0.256).</description><author>Jonathan Cook, Tim RocktÃ¤schel, Jakob Foerster, Dennis Aumiller, Alex Wang</author><pubDate>Fri, 04 Oct 2024 17:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03608v1</guid></item><item><title>Language Model Empowered Spatio-Temporal Forecasting via Physics-Aware Reprogramming</title><link>http://arxiv.org/abs/2408.14505v2</link><description>Spatio-temporal forecasting is pivotal in numerous real-world applications,including transportation planning, energy management, and climate monitoring.In this work, we aim to harness the reasoning and generalization abilities ofPre-trained Language Models (PLMs) for more effective spatio-temporalforecasting, particularly in data-scarce scenarios. However, recent studiesuncover that PLMs, which are primarily trained on textual data, often falterwhen tasked with modeling the intricate correlations in numerical time series,thereby limiting their effectiveness in comprehending spatio-temporal data. Tobridge the gap, we propose RePST, a physics-aware PLM reprogramming frameworktailored for spatio-temporal forecasting. Specifically, we first propose aphysics-aware decomposer that adaptively disentangles spatially correlated timeseries into interpretable sub-components, which facilitates PLM to understandsophisticated spatio-temporal dynamics via a divide-and-conquer strategy.Moreover, we propose a selective discrete reprogramming scheme, whichintroduces an expanded spatio-temporal vocabulary space to projectspatio-temporal series into discrete representations. This scheme minimizes theinformation loss during reprogramming and enriches the representations derivedby PLMs. Extensive experiments on real-world datasets show that the proposedRePST outperforms twelve state-of-the-art baseline methods, particularly indata-scarce scenarios, highlighting the effectiveness and superiorgeneralization capabilities of PLMs for spatio-temporal forecasting.</description><author>Hao Wang, Jindong Han, Wei Fan, Hao Liu</author><pubDate>Fri, 04 Oct 2024 17:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14505v2</guid></item><item><title>Exploring gauge-fixing conditions with gradient-based optimization</title><link>http://arxiv.org/abs/2410.03602v1</link><description>Lattice gauge fixing is required to compute gauge-variant quantities, forexample those used in RI-MOM renormalization schemes or as objects ofcomparison for model calculations. Recently, gauge-variant quantities have alsobeen found to be more amenable to signal-to-noise optimization using contourdeformations. These applications motivate systematic parameterization andexploration of gauge-fixing schemes. This work introduces a differentiableparameterization of gauge fixing which is broad enough to cover Landau gauge,Coulomb gauge, and maximal tree gauges. The adjoint state method allowsgradient-based optimization to select gauge-fixing schemes that minimize anarbitrary target loss function.</description><author>William Detmold, Gurtej Kanwar, Yin Lin, Phiala E. Shanahan, Michael L. Wagman</author><pubDate>Fri, 04 Oct 2024 17:01:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03602v1</guid></item><item><title>How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework</title><link>http://arxiv.org/abs/2410.03601v1</link><description>Discrete diffusion models have gained increasing attention for their abilityto model complex distributions with tractable sampling and inference. However,the error analysis for discrete diffusion models remains less well-understood.In this work, we propose a comprehensive framework for the error analysis ofdiscrete diffusion models based on L\'evy-type stochastic integrals. Bygeneralizing the Poisson random measure to that with a time-independent andstate-dependent intensity, we rigorously establish a stochastic integralformulation of discrete diffusion models and provide the corresponding changeof measure theorems that are intriguingly analogous to It\^o integrals andGirsanov's theorem for their continuous counterparts. Our framework unifies andstrengthens the current theoretical results on discrete diffusion models andobtains the first error bound for the $\tau$-leaping scheme in KL divergence.With error sources clearly identified, our analysis gives new insight into themathematical properties of discrete diffusion models and offers guidance forthe design of efficient and accurate algorithms for real-world discretediffusion model applications.</description><author>Yinuo Ren, Haoxuan Chen, Grant M. Rotskoff, Lexing Ying</author><pubDate>Fri, 04 Oct 2024 16:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03601v1</guid></item><item><title>Efficiently Identifying Watermarked Segments in Mixed-Source Texts</title><link>http://arxiv.org/abs/2410.03600v1</link><description>Text watermarks in large language models (LLMs) are increasingly used todetect synthetic text, mitigating misuse cases like fake news and academicdishonesty. While existing watermarking detection techniques primarily focus onclassifying entire documents as watermarked or not, they often neglect thecommon scenario of identifying individual watermark segments within longer,mixed-source documents. Drawing inspiration from plagiarism detection systems,we propose two novel methods for partial watermark detection. First, we developa geometry cover detection framework aimed at determining whether there is awatermark segment in long text. Second, we introduce an adaptive onlinelearning algorithm to pinpoint the precise location of watermark segmentswithin the text. Evaluated on three popular watermarking techniques(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieveshigh accuracy, significantly outperforming baseline methods. Moreover, ourframework is adaptable to other watermarking techniques, offering new insightsfor precise watermark detection.</description><author>Xuandong Zhao, Chenwen Liao, Yu-Xiang Wang, Lei Li</author><pubDate>Fri, 04 Oct 2024 16:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03600v1</guid></item><item><title>Improving Statistical Significance in Human Evaluation of Automatic Metrics via Soft Pairwise Accuracy</title><link>http://arxiv.org/abs/2409.09598v2</link><description>Selecting an automatic metric that best emulates human annotators is oftennon-trivial, because there is no clear definition of "best emulates." Ameta-metric is required to compare the human judgments to the automatic metricscores, and metric rankings depend on the choice of meta-metric. We proposeSoft Pairwise Accuracy (SPA), a new meta-metric that builds on PairwiseAccuracy (PA) but incorporates the statistical significance of both the humanjudgments and the metric scores. We show that SPA is more stable than PA withrespect to changes in the number of systems/segments used for evaluation. Wealso show that PA can only assign a small set of distinct output values tometrics, and this results in many metrics being artificially assigned the exactsame PA score. We demonstrate that SPA fixes this issue. Finally, we show thatSPA is more discriminative than PA, producing more statistically significantcomparisons between metrics. SPA was selected as the official system-levelmetric for the 2024 WMT Metrics Shared Task.</description><author>Brian Thompson, Nitika Mathur, Daniel Deutsch, Huda Khayrallah</author><pubDate>Fri, 04 Oct 2024 16:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09598v2</guid></item><item><title>MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making</title><link>http://arxiv.org/abs/2404.15155v2</link><description>Foundation models are becoming valuable tools in medicine. Yet despite theirpromise, the best way to leverage Large Language Models (LLMs) in complexmedical tasks remains an open question. We introduce a novel multi-agentframework, named Medical Decision-making Agents (MDAgents) that helps addressthis gap by automatically assigning a collaboration structure to a team ofLLMs. The assigned solo or group collaboration structure is tailored to themedical task at hand, emulating real-world medical decision-making processesadapted to tasks of varying complexities. We evaluate our framework andbaseline methods using state-of-the-art LLMs across a suite of real-worldmedical knowledge and medical diagnosis benchmarks. MDAgents achieved the bestperformance in seven out of ten benchmarks on tasks requiring an understandingof medical knowledge and multi-modal reasoning, showing a significantimprovement of up to 6.5% (p &lt; 0.05) compared to previous methods' bestperformances. Ablation studies reveal that MDAgents effectively determinesmedical complexity to optimize for efficiency and accuracy across diversemedical tasks. Notably, the combination of moderator review and externalmedical knowledge in group collaboration resulted in an average accuracyimprovement of 11.8%. Our code can be found athttps://github.com/mitmedialab/MDAgents.</description><author>Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, Hae Won Park</author><pubDate>Fri, 04 Oct 2024 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15155v2</guid></item><item><title>SiMilarity-Enhanced Homophily for Multi-View Heterophilous Graph Clustering</title><link>http://arxiv.org/abs/2410.03596v1</link><description>With the increasing prevalence of graph-structured data, multi-view graphclustering has been widely used in various downstream applications. Existingapproaches primarily rely on a unified message passing mechanism, whichsignificantly enhances clustering performance. Nevertheless, this mechanismlimits its applicability to heterophilous situations, as it is fundamentallypredicated on the assumption of homophily, i.e., the connected nodes oftenbelong to the same class. In reality, this assumption does not always hold; amoderately or even mildly homophilous graph is more common than a fullyhomophilous one due to inevitable heterophilous information in the graph. Toaddress this issue, in this paper, we propose a novel SiMilarity-enhancedHomophily for Multi-view Heterophilous Graph Clustering (SMHGC) approach. Byanalyzing the relationship between similarity and graph homophily, we proposeto enhance the homophily by introducing three similarity terms, i.e., neighborpattern similarity, node feature similarity, and multi-view global similarity,in a label-free manner. Then, a consensus-based inter- and intra-view fusionparadigm is proposed to fuse the improved homophilous graph from differentviews and utilize them for clustering. The state-of-the-art experimentalresults on both multi-view heterophilous and homophilous datasets collectivelydemonstrate the strong capacity of similarity for unsupervised multi-viewheterophilous graph learning. Additionally, the consistent performance acrosssemi-synthetic datasets with varying levels of homophily serves as furtherevidence of SMHGC's resilience to heterophily.</description><author>Jianpeng Chen, Yawen Ling, Yazhou Ren, Zichen Wen, Tianyi Wu, Shufei Zhang, Lifang He</author><pubDate>Fri, 04 Oct 2024 16:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03596v1</guid></item><item><title>Understanding Reasoning in Chain-of-Thought from the Hopfieldian View</title><link>http://arxiv.org/abs/2410.03595v1</link><description>Large Language Models have demonstrated remarkable abilities across varioustasks, with Chain-of-Thought (CoT) prompting emerging as a key technique toenhance reasoning capabilities. However, existing research primarily focuses onimproving performance, lacking a comprehensive framework to explain andunderstand the fundamental factors behind CoT's success. To bridge this gap, weintroduce a novel perspective grounded in the Hopfieldian view of cognition incognitive neuroscience. We establish a connection between CoT reasoning and keycognitive elements such as stimuli, actions, neural populations, andrepresentation spaces. From our view, we can understand the reasoning processas the movement between these representation spaces. Building on this insight,we develop a method for localizing reasoning errors in the response of CoTs.Moreover, we propose the Representation-of-Thought (RoT) framework, whichleverages the robustness of low-dimensional representation spaces to enhancethe robustness of the reasoning process in CoTs. Experimental resultsdemonstrate that RoT improves the robustness and interpretability of CoTreasoning while offering fine-grained control over the reasoning process.</description><author>Lijie Hu, Liang Liu, Shu Yang, Xin Chen, Zhen Tan, Muhammad Asif Ali, Mengdi Li, Di Wang</author><pubDate>Fri, 04 Oct 2024 16:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03595v1</guid></item><item><title>Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments</title><link>http://arxiv.org/abs/2410.03594v1</link><description>Prior works formulate the extraction of event-specific arguments as a spanextraction problem, where event arguments are explicit -- i.e. assumed to becontiguous spans of text in a document. In this study, we revisit thisdefinition of Event Extraction (EE) by introducing two key argument types thatcannot be modeled by existing EE frameworks. First, implicit arguments areevent arguments which are not explicitly mentioned in the text, but can beinferred through context. Second, scattered arguments are event arguments thatare composed of information scattered throughout the text. These two argumenttypes are crucial to elicit the full breadth of information required for properevent modeling. To support the extraction of explicit, implicit, and scattered arguments, wedevelop a novel dataset, DiscourseEE, which includes 7,464 argument annotationsfrom online health discourse. Notably, 51.2% of the arguments are implicit, and17.4% are scattered, making DiscourseEE a unique corpus for complex eventextraction. Additionally, we formulate argument extraction as a text generationproblem to facilitate the extraction of complex argument types. We provide acomprehensive evaluation of state-of-the-art models and highlight critical openchallenges in generative event extraction. Our data and codebase are availableat https://omar-sharif03.github.io/DiscourseEE.</description><author>Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah M. Preum</author><pubDate>Fri, 04 Oct 2024 16:54:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03594v1</guid></item><item><title>Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness</title><link>http://arxiv.org/abs/2407.11229v2</link><description>Chart question answering (CQA) is a crucial area of Visual LanguageUnderstanding. However, the robustness and consistency of current VisualLanguage Models (VLMs) in this field remain under-explored. This paperevaluates state-of-the-art VLMs on comprehensive datasets, developedspecifically for this study, encompassing diverse question categories and chartformats. We investigate two key aspects: 1) the models' ability to handlevarying levels of chart and question complexity, and 2) their robustness acrossdifferent visual representations of the same underlying data. Our analysisreveals significant performance variations based on question and chart types,highlighting both strengths and weaknesses of current models. Additionally, weidentify areas for improvement and propose future research directions to buildmore robust and reliable CQA systems. This study sheds light on the limitationsof current models and paves the way for future advancements in the field.</description><author>Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, Dan Roth</author><pubDate>Fri, 04 Oct 2024 16:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11229v2</guid></item><item><title>Variational Bayes Gaussian Splatting</title><link>http://arxiv.org/abs/2410.03592v1</link><description>Recently, 3D Gaussian Splatting has emerged as a promising approach formodeling 3D scenes using mixtures of Gaussians. The predominant optimizationmethod for these models relies on backpropagating gradients through adifferentiable rendering pipeline, which struggles with catastrophic forgettingwhen dealing with continuous streams of data. To address this limitation, wepropose Variational Bayes Gaussian Splatting (VBGS), a novel approach thatframes training a Gaussian splat as variational inference over modelparameters. By leveraging the conjugacy properties of multivariate Gaussians,we derive a closed-form variational update rule, allowing efficient updatesfrom partial, sequential observations without the need for replay buffers. Ourexperiments show that VBGS not only matches state-of-the-art performance onstatic datasets, but also enables continual learning from sequentially streamed2D and 3D data, drastically improving performance in this setting.</description><author>Toon Van de Maele, Ozan Catal, Alexander Tschantz, Christopher L. Buckley, Tim Verbelen</author><pubDate>Fri, 04 Oct 2024 16:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03592v1</guid></item><item><title>Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding</title><link>http://arxiv.org/abs/2312.06149v4</link><description>Large Language Models (LLMs) have demonstrated a powerful ability for textgeneration. However, achieving optimal results with a given prompt orinstruction can be challenging, especially for billion-sized models.Additionally, undesired behaviors such as toxicity or hallucinations canmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength inmitigating these issues, there is still no guarantee of complete prevention. Inthis work, we propose formalizing text generation as a future-constrainedgeneration problem to minimize undesirable behaviors and enforce faithfulnessto instructions. The estimation of future constraint satisfaction, accomplishedusing LLMs, guides the text generation process. Our extensive experimentsdemonstrate the effectiveness of the proposed approach across three distincttext generation tasks: keyword-constrained generation (Lin et al., 2020),toxicity reduction (Gehman et al., 2020), and factual correctness inquestion-answering (Gao et al., 2023).</description><author>Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, Yingbo Zhou</author><pubDate>Fri, 04 Oct 2024 16:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06149v4</guid></item><item><title>Training Over a Distribution of Hyperparameters for Enhanced Performance and Adaptability on Imbalanced Classification</title><link>http://arxiv.org/abs/2410.03588v1</link><description>Although binary classification is a well-studied problem, training reliableclassifiers under severe class imbalance remains a challenge. Recent techniquesmitigate the ill effects of imbalance on training by modifying the lossfunctions or optimization methods. We observe that different hyperparametervalues on these loss functions perform better at different recall values. Wepropose to exploit this fact by training one model over a distribution ofhyperparameter values--instead of a single value--via Loss Conditional Training(LCT). Experiments show that training over a distribution of hyperparametersnot only approximates the performance of several models but actually improvesthe overall performance of models on both CIFAR and real medical imagingapplications, such as melanoma and diabetic retinopathy detection. Furthermore,training models with LCT is more efficient because some hyperparameter tuningcan be conducted after training to meet individual needs without needing toretrain from scratch.</description><author>Kelsey Lieberman, Swarna Kamlam Ravindran, Shuai Yuan, Carlo Tomasi</author><pubDate>Fri, 04 Oct 2024 16:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03588v1</guid></item><item><title>Why Would You Suggest That? Human Trust in Language Model Responses</title><link>http://arxiv.org/abs/2406.02018v2</link><description>The emergence of Large Language Models (LLMs) has revealed a growing need forhuman-AI collaboration, especially in creative decision-making scenarios wheretrust and reliance are paramount. Through human studies and model evaluationson the open-ended News Headline Generation task from the LaMP benchmark, weanalyze how the framing and presence of explanations affect user trust andmodel performance. Overall, we provide evidence that adding an explanation inthe model response to justify its reasoning significantly increasesself-reported user trust in the model when the user has the opportunity tocompare various responses. Position and faithfulness of these explanations arealso important factors. However, these gains disappear when users are shownresponses independently, suggesting that humans trust all model responses,including deceptive ones, equitably when they are shown in isolation. Ourfindings urge future research to delve deeper into the nuanced evaluation oftrust in human-machine teaming systems.</description><author>Manasi Sharma, Ho Chit Siu, Rohan Paleja, Jaime D. PeÃ±a</author><pubDate>Fri, 04 Oct 2024 16:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02018v2</guid></item><item><title>Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise</title><link>http://arxiv.org/abs/2311.14900v3</link><description>Recently, research on denoising diffusion models has expanded its applicationto the field of image restoration. Traditional diffusion-based imagerestoration methods utilize degraded images as conditional input to effectivelyguide the reverse generation process, without modifying the original denoisingdiffusion process. However, since the degraded images already includelow-frequency information, starting from Gaussian white noise will result inincreased sampling steps. We propose Resfusion, a general framework thatincorporates the residual term into the diffusion forward process, starting thereverse process directly from the noisy degraded images. The form of ourinference process is consistent with the DDPM. We introduced a weightedresidual noise, named resnoise, as the prediction target and explicitly providethe quantitative relationship between the residual term and the noise term inresnoise. By leveraging a smooth equivalence transformation, Resfusiondetermine the optimal acceleration step and maintains the integrity of existingnoise schedules, unifying the training and inference processes. Theexperimental results demonstrate that Resfusion exhibits competitiveperformance on ISTD dataset, LOL dataset and Raindrop dataset with only fivesampling steps. Furthermore, Resfusion can be easily applied to imagegeneration and emerges with strong versatility. Our code and model areavailable at https://github.com/nkicsl/Resfusion.</description><author>Zhenning Shi, Haoshuai Zheng, Chen Xu, Changsheng Dong, Bin Pan, Xueshuo Xie, Along He, Tao Li, Huazhu Fu</author><pubDate>Fri, 04 Oct 2024 16:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14900v3</guid></item><item><title>Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG</title><link>http://arxiv.org/abs/2406.13069v3</link><description>How novel are texts generated by language models (LMs) relative to theirtraining corpora? In this work, we investigate the extent to which modern LMsgenerate $n$-grams from their training data, evaluating both (i) theprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, theproportion of $n$-grams generated by an LM that did not appear in the trainingdata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram searchover a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, anovel search tool inspired by indexing of genomic data. We compare the noveltyof LM-generated text to human-written text and explore factors that affectgeneration novelty, focusing on the Pythia models. We find that, for $n &gt; 4$,LM-generated text is less novel than human-written text, though it is morenovel for smaller $n$. Larger LMs and more constrained decoding strategies bothdecrease novelty. Finally, we show that LMs complete $n$-grams with lower lossif they are more frequent in the training data. Overall, our results revealfactors influencing the novelty of LM-generated text, and we release Rusty-DAWGto facilitate further pretraining data research.</description><author>William Merrill, Noah A. Smith, Yanai Elazar</author><pubDate>Fri, 04 Oct 2024 16:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13069v3</guid></item><item><title>Navigable Graphs for High-Dimensional Nearest Neighbor Search: Constructions and Limits</title><link>http://arxiv.org/abs/2405.18680v2</link><description>There has been significant recent interest in graph-based nearest neighborsearch methods, many of which are centered on the construction of navigablegraphs over high-dimensional point sets. A graph is navigable if we cansuccessfully move from any starting node to any target node using a greedyrouting strategy where we always move to the neighbor that is closest to thedestination according to a given distance function. The complete graph isnavigable for any point set, but the important question for applications is ifsparser graphs can be constructed. While this question is fairly wellunderstood in low-dimensions, we establish some of the first upper and lowerbounds for high-dimensional point sets. First, we give a simple and efficientway to construct a navigable graph with average degree $O(\sqrt{n \log n })$for any set of $n$ points, in any dimension, for any distance function. Wecompliment this result with a nearly matching lower bound: even under theEuclidean metric in $O(\log n)$ dimensions, a random point set has no navigablegraph with average degree $O(n^{\alpha})$ for any $\alpha &lt; 1/2$. Our lowerbound relies on sharp anti-concentration bounds for binomial random variables,which we use to show that the near-neighborhoods of a set of random points donot overlap significantly, forcing any navigable graph to have many edges.</description><author>Haya Diwan, Jinrui Gou, Cameron Musco, Christopher Musco, Torsten Suel</author><pubDate>Fri, 04 Oct 2024 16:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18680v2</guid></item><item><title>Nonstationary Sparse Spectral Permanental Process</title><link>http://arxiv.org/abs/2410.03581v1</link><description>Existing permanental processes often impose constraints on kernel types orstationarity, limiting the model's expressiveness. To overcome theselimitations, we propose a novel approach utilizing the sparse spectralrepresentation of nonstationary kernels. This technique relaxes the constraintson kernel types and stationarity, allowing for more flexible modeling whilereducing computational complexity to the linear level. Additionally, weintroduce a deep kernel variant by hierarchically stacking multiple spectralfeature mappings, further enhancing the model's expressiveness to capturecomplex patterns in data. Experimental results on both synthetic and real-worlddatasets demonstrate the effectiveness of our approach, particularly inscenarios with pronounced data nonstationarity. Additionally, ablation studiesare conducted to provide insights into the impact of various hyperparameters onmodel performance.</description><author>Zicheng Sun, Yixuan Zhang, Zenan Ling, Xuhui Fan, Feng Zhou</author><pubDate>Fri, 04 Oct 2024 16:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03581v1</guid></item><item><title>Data Diet: Can Trimming PET/CT Datasets Enhance Lesion Segmentation?</title><link>http://arxiv.org/abs/2409.13548v3</link><description>In this work, we describe our approach to compete in the autoPET3 datacentrictrack. While conventional wisdom suggests that larger datasets lead to bettermodel performance, recent studies indicate that excluding certain trainingsamples can enhance model accuracy. We find that in the autoPETIII dataset, amodel that is trained on the entire dataset exhibits undesirablecharacteristics by producing a large number of false positives particularly forPSMA-PETs. We counteract this by removing the easiest samples from the trainingdataset as measured by the model loss before retraining from scratch. Using theproposed approach we manage to drive down the false negative volume and improveupon the baseline model in both false negative volume and dice score on thepreliminary test set. Code and pre-trained models are available atgithub.com/alexanderjaus/autopet3_datadiet.</description><author>Alexander Jaus, Simon ReiÃ, Jens Klesiek, Rainer Stiefelhagen</author><pubDate>Fri, 04 Oct 2024 16:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13548v3</guid></item><item><title>A Multi-model Approach for Video Data Retrieval in Autonomous Vehicle Development</title><link>http://arxiv.org/abs/2410.03580v1</link><description>Autonomous driving software generates enormous amounts of data every second,which software development organizations save for future analysis and testingin the form of logs. However, given the vast size of this data, locatingspecific scenarios within a collection of vehicle logs can be challenging.Writing the correct SQL queries to find these scenarios requires engineers tohave a strong background in SQL and the specific databases in question, furthercomplicating the search process. This paper presents and evaluates a pipelinethat allows searching for specific scenarios in log collections using naturallanguage descriptions instead of SQL. The generated descriptions were evaluatedby engineers working with vehicle logs at the Zenseact on a scale from 1 to 5.Our approach achieved a mean score of 3.3, demonstrating the potential of usinga multi-model architecture to improve the software development workflow. Wealso present an interface that can visualize the query process and visualizethe results.</description><author>Jesper Knapp, Klas Moberg, Yuchuan Jin, Simin Sun, Miroslaw Staron</author><pubDate>Fri, 04 Oct 2024 16:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03580v1</guid></item><item><title>Learning 3D Perception from Others' Predictions</title><link>http://arxiv.org/abs/2410.02646v2</link><description>Accurate 3D object detection in real-world environments requires a hugeamount of annotated data with high quality. Acquiring such data is tedious andexpensive, and often needs repeated effort when a new sensor is adopted or whenthe detector is deployed in a new environment. We investigate a new scenario toconstruct 3D object detectors: learning from the predictions of a nearby unitthat is equipped with an accurate detector. For example, when a self-drivingcar enters a new area, it may learn from other traffic participants whosedetectors have been optimized for that area. This setting is label-efficient,sensor-agnostic, and communication-efficient: nearby units only need to sharethe predictions with the ego agent (e.g., car). Naively using the receivedpredictions as ground-truths to train the detector for the ego car, however,leads to inferior performance. We systematically study the problem and identifyviewpoint mismatches and mislocalization (due to synchronization and GPSerrors) as the main causes, which unavoidably result in false positives, falsenegatives, and inaccurate pseudo labels. We propose a distance-basedcurriculum, first learning from closer units with similar viewpoints andsubsequently improving the quality of other units' predictions viaself-training. We further demonstrate that an effective pseudo label refinementmodule can be trained with a handful of annotated data, largely reducing thedata quantity necessary to train an object detector. We validate our approachon the recently released real-world collaborative driving dataset, usingreference cars' predictions as pseudo labels for the ego car. Extensiveexperiments including several scenarios (e.g., different sensors, detectors,and domains) demonstrate the effectiveness of our approach towardlabel-efficient learning of 3D perception from other units' predictions.</description><author>Jinsu Yoo, Zhenyang Feng, Tai-Yu Pan, Yihong Sun, Cheng Perng Phoo, Xiangyu Chen, Mark Campbell, Kilian Q. Weinberger, Bharath Hariharan, Wei-Lun Chao</author><pubDate>Fri, 04 Oct 2024 16:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02646v2</guid></item><item><title>Lessons Learned from a Unifying Empirical Study of Parameter-Efficient Transfer Learning (PETL) in Visual Recognition</title><link>http://arxiv.org/abs/2409.16434v3</link><description>Parameter-efficient transfer learning (PETL) has attracted significantattention lately, due to the increasing size of pre-trained models and the needto fine-tune (FT) them for superior downstream performance. This community-wideenthusiasm has sparked a plethora of approaches. Nevertheless, a systematicstudy to understand their performance and suitable application scenarios islacking, leaving questions like when to apply PETL and which approach to uselargely unanswered. In this paper, we conduct a unifying empirical study ofrepresentative PETL methods in the context of Vision Transformers. Wesystematically tune their hyper-parameters to fairly compare their accuracy ondownstream tasks. Our study not only offers a valuable user guide but alsounveils several new insights. First, if tuned carefully, different PETL methodscan obtain similar accuracy in the low-shot benchmark VTAB-1K. This includessimple methods like FT the bias terms that were reported inferior. Second,though with similar accuracy, we find that PETL methods make different mistakesand high-confidence predictions, likely due to their different inductivebiases. Such an inconsistency (or complementariness) opens up the opportunityfor ensemble methods, and we make preliminary attempts at this. Third, goingbeyond the commonly used low-shot tasks, we find that PETL is also useful inmany-shot regimes -- it achieves comparable and sometimes better accuracy thanfull FT, using much fewer learnable parameters. Last but not least, weinvestigate PETL's ability to preserve a pre-trained model's robustness todistribution shifts (e.g., a CLIP backbone). Perhaps not surprisingly, PETLmethods outperform full FT alone. However, with weight-space ensembles, thefully fine-tuned model can better balance target (i.e., downstream)distribution and distribution shift performance, suggesting a future researchdirection for PETL.</description><author>Zheda Mai, Ping Zhang, Cheng-Hao Tu, Hong-You Chen, Li Zhang, Wei-Lun Chao</author><pubDate>Fri, 04 Oct 2024 16:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16434v3</guid></item><item><title>Semi-Supervised Manifold Learning with Complexity Decoupled Chart Autoencoders</title><link>http://arxiv.org/abs/2208.10570v2</link><description>Autoencoding is a popular method in representation learning. Conventionalautoencoders employ symmetric encoding-decoding procedures and a simpleEuclidean latent space to detect hidden low-dimensional structures in anunsupervised way. Some modern approaches to novel data generation such asgenerative adversarial networks askew this symmetry, but still employ a pair ofmassive networks--one to generate the image and another to judge the imagesquality based on priors learned from a training set. This work introduces achart autoencoder with an asymmetric encoding-decoding process that canincorporate additional semi-supervised information such as class labels.Besides enhancing the capability for handling data with complicated topologicaland geometric structures, the proposed model can successfully differentiatenearby but disjoint manifolds and intersecting manifolds with only a smallamount of supervision. Moreover, this model only requires a low-complexityencoding operation, such as a locally defined linear projection. We discuss theapproximation power of such networks and derive a bound that essentiallydepends on the intrinsic dimension of the data manifold rather than thedimension of ambient space. Next we incorporate bounds for the sampling rate oftraining data need to faithfully represent a given data manifold. We presentnumerical experiments that verify that the proposed model can effectivelymanage data with multi-class nearby but disjoint manifolds of differentclasses, overlapping manifolds, and manifolds with non-trivial topology.Finally, we conclude with some experiments on computer vision and moleculardynamics problems which showcase the efficacy of our methods on real-worlddata.</description><author>Stefan C. Schonsheck, Scott Mahan, Timo Klock, Alexander Cloninger, Rongjie Lai</author><pubDate>Fri, 04 Oct 2024 16:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10570v2</guid></item><item><title>Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models</title><link>http://arxiv.org/abs/2410.03577v1</link><description>Despite their impressive capabilities, Multimodal Large Language Models(MLLMs) are susceptible to hallucinations, especially assertively fabricatingcontent not present in the visual inputs. To address the aforementionedchallenge, we follow a common cognitive process - when one's initial memory ofcritical on-sight details fades, it is intuitive to look at them a second timeto seek a factual and accurate answer. Therefore, we introduce Memory-spaceVisual Retracing (MemVR), a novel hallucination mitigation paradigm thatwithout the need for external knowledge retrieval or additional fine-tuning. Inparticular, we treat visual prompts as supplementary evidence to be reinjectedinto MLLMs via Feed Forward Network (FFN) as key-value memory, when the modelis uncertain or even amnesic about question-relevant visual memories.Comprehensive experimental evaluations demonstrate that MemVR significantlymitigates hallucination issues across various MLLMs and excels in generalbenchmarks without incurring added time overhead, thus emphasizing itspotential for widespread applicability.</description><author>Xin Zou, Yizhou Wang, Yibo Yan, Sirui Huang, Kening Zheng, Junkai Chen, Chang Tang, Xuming Hu</author><pubDate>Fri, 04 Oct 2024 16:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03577v1</guid></item><item><title>Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&amp;A</title><link>http://arxiv.org/abs/2402.13213v2</link><description>We study 14 large language models (LLMs) fine-tuned for chat and find thattheir maximum softmax probabilities (MSPs) are consistently miscalibrated onmultiple-choice Q&amp;A. However, those MSPs might still encode useful uncertaintyinformation. Specifically, we hypothesized that wrong answers would beassociated with smaller MSPs compared to correct answers. Via rigororousstatistical testing, we show that this hypothesis holds for models whichperform well on the underlying Q&amp;A task. We also find a strong directioncorrelation between Q&amp;A accuracy and MSP correctness prediction, while findingno correlation between Q&amp;A accuracy and calibration error. This suggests thatwithin the current fine-tuning paradigm, we can expect correctness predictionbut not calibration to improve as LLM capabilities progress. To demonstrate theutility of correctness prediction, we show that when models have the option toabstain, performance can be improved by selectively abstaining based on the MSPof the initial model response, using only a small amount of labeled data tochoose the MSP threshold.</description><author>Benjamin Plaut, Nguyen X. Khanh, Tu Trinh</author><pubDate>Fri, 04 Oct 2024 16:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13213v2</guid></item><item><title>Table Question Answering for Low-resourced Indic Languages</title><link>http://arxiv.org/abs/2410.03576v1</link><description>TableQA is the task of answering questions over tables of structuredinformation, returning individual cells or tables as output. TableQA researchhas focused primarily on high-resource languages, leaving medium- andlow-resource languages with little progress due to scarcity of annotated dataand neural models. We address this gap by introducing a fully automaticlarge-scale tableQA data generation process for low-resource languages withlimited budget. We incorporate our data generation method on two Indiclanguages, Bengali and Hindi, which have no tableQA datasets or models. TableQAmodels trained on our large-scale datasets outperform state-of-the-art LLMs. Wefurther study the trained models on different aspects, including mathematicalreasoning capabilities and zero-shot cross-lingual transfer. Our work is thefirst on low-resource tableQA focusing on scalable data generation andevaluation procedures. Our proposed data generation method can be applied toany low-resource language with a web presence. We release datasets, models, andcode (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).</description><author>Vaishali Pal, Evangelos Kanoulas, Andrew Yates, Maarten de Rijke</author><pubDate>Fri, 04 Oct 2024 16:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03576v1</guid></item><item><title>HyResPINNs: Adaptive Hybrid Residual Networks for Learning Optimal Combinations of Neural and RBF Components for Physics-Informed Modeling</title><link>http://arxiv.org/abs/2410.03573v1</link><description>Physics-informed neural networks (PINNs) are an increasingly popular class oftechniques for the numerical solution of partial differential equations (PDEs),where neural networks are trained using loss functions regularized by relevantPDE terms to enforce physical constraints. We present a new class of PINNscalled HyResPINNs, which augment traditional PINNs with adaptive hybridresidual blocks that combine the outputs of a standard neural network and aradial basis function (RBF) network. A key feature of our method is theinclusion of adaptive combination parameters within each residual block, whichdynamically learn to weigh the contributions of the neural network and RBFnetwork outputs. Additionally, adaptive connections between residual blocksallow for flexible information flow throughout the network. We show thatHyResPINNs are more robust to training point locations and neural networkarchitectures than traditional PINNs. Moreover, HyResPINNs offer orders ofmagnitude greater accuracy than competing methods on certain problems, withonly modest increases in training costs. We demonstrate the strengths of ourapproach on challenging PDEs, including the Allen-Cahn equation and theDarcy-Flow equation. Our results suggest that HyResPINNs effectively bridge thegap between traditional numerical methods and modern machine learning-basedsolvers.</description><author>Madison Cooley, Robert M. Kirby, Shandian Zhe, Varun Shankar</author><pubDate>Fri, 04 Oct 2024 16:21:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03573v1</guid></item><item><title>Teaching Transformers Modular Arithmetic at Scale</title><link>http://arxiv.org/abs/2410.03569v1</link><description>Modular addition is, on its face, a simple operation: given $N$ elements in$\mathbb{Z}_q$, compute their sum modulo $q$. Yet, scalable machine learningsolutions to this problem remain elusive: prior work trains ML models that sum$N \le 6$ elements mod $q \le 1000$. Promising applications of ML models forcryptanalysis-which often involve modular arithmetic with large $N$ and$q$-motivate reconsideration of this problem. This work proposes three changesto the modular addition model training pipeline: more diverse training data, anangular embedding, and a custom loss function. With these changes, wedemonstrate success with our approach for $N = 256, q = 3329$, a case which isinteresting for cryptographic applications, and a significant increase in $N$and $q$ over prior work. These techniques also generalize to other modulararithmetic problems, motivating future work.</description><author>Eshika Saxena, Alberto Alfarano, Emily Wenger, Kristin Lauter</author><pubDate>Fri, 04 Oct 2024 16:19:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03569v1</guid></item><item><title>Towards Linguistically-Aware and Language-Independent Tokenization for Large Language Models (LLMs)</title><link>http://arxiv.org/abs/2410.03568v1</link><description>This paper presents a comprehensive study on the tokenization techniquesemployed by state-of-the-art large language models (LLMs) and theirimplications on the cost and availability of services across differentlanguages, especially low resource languages. The analysis considers multipleLLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_baseembeddings), and DaVinci (employing r50k_base embeddings), as well as thewidely used BERT base tokenizer. The study evaluates the tokenizationvariability observed across these models and investigates the challenges oflinguistic representation in subword tokenization. The research underscores theimportance of fostering linguistically-aware development practices, especiallyfor languages that are traditionally under-resourced. Moreover, this paperintroduces case studies that highlight the real-world implications oftokenization choices, particularly in the context of electronic health record(EHR) systems. This research aims to promote generalizable Internationalization(I18N) practices in the development of AI services in this domain and beyond,with a strong emphasis on inclusivity, particularly for languages traditionallyunderrepresented in AI applications.</description><author>Abrar Rahman, Garry Bowlin, Binit Mohanty, Sean McGunigal</author><pubDate>Fri, 04 Oct 2024 16:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03568v1</guid></item><item><title>Training on more Reachable Tasks for Generalisation in Reinforcement Learning</title><link>http://arxiv.org/abs/2410.03565v1</link><description>In multi-task reinforcement learning, agents train on a fixed set of tasksand have to generalise to new ones. Recent work has shown that increasedexploration improves this generalisation, but it remains unclear why exactlythat is. In this paper, we introduce the concept of reachability in multi-taskreinforcement learning and show that an initial exploration phase increases thenumber of reachable tasks the agent is trained on. This, and not the increasedexploration, is responsible for the improved generalisation, even tounreachable tasks. Inspired by this, we propose a novel method Explore-Go thatimplements such an exploration phase at the beginning of each episode.Explore-Go only modifies the way experience is collected and can be used withmost existing on-policy or off-policy reinforcement learning algorithms. Wedemonstrate the effectiveness of our method when combined with some popularalgorithms and show an increase in generalisation performance across severalenvironments.</description><author>Max Weltevrede, Caroline Horsch, Matthijs T. J. Spaan, Wendelin BÃ¶hmer</author><pubDate>Fri, 04 Oct 2024 16:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03565v1</guid></item><item><title>Universal Functional Regression with Neural Operator Flows</title><link>http://arxiv.org/abs/2404.02986v2</link><description>Regression on function spaces is typically limited to models with Gaussianprocess priors. We introduce the notion of universal functional regression, inwhich we aim to learn a prior distribution over non-Gaussian function spacesthat remains mathematically tractable for functional regression. To do this, wedevelop Neural Operator Flows (OpFlow), an infinite-dimensional extension ofnormalizing flows. OpFlow is an invertible operator that maps the (potentiallyunknown) data function space into a Gaussian process, allowing for exactlikelihood estimation of functional point evaluations. OpFlow enables robustand accurate uncertainty quantification via drawing posterior samples of theGaussian process and subsequently mapping them into the data function space. Weempirically study the performance of OpFlow on regression and generation taskswith data generated from Gaussian processes with known posterior forms andnon-Gaussian processes, as well as real-world earthquake seismograms with anunknown closed-form distribution.</description><author>Yaozhong Shi, Angela F. Gao, Zachary E. Ross, Kamyar Azizzadenesheli</author><pubDate>Fri, 04 Oct 2024 16:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02986v2</guid></item><item><title>MetaKP: On-Demand Keyphrase Generation</title><link>http://arxiv.org/abs/2407.00191v2</link><description>Traditional keyphrase prediction methods predict a single set of keyphrasesper document, failing to cater to the diverse needs of users and downstreamapplications. To bridge the gap, we introduce on-demand keyphrase generation, anovel paradigm that requires keyphrases that conform to specific high-levelgoals or intents. For this task, we present MetaKP, a large-scale benchmarkcomprising four datasets, 7500 documents, and 3760 goals across news andbiomedical domains with human-annotated keyphrases. Leveraging MetaKP, wedesign both supervised and unsupervised methods, including a multi-taskfine-tuning approach and a self-consistency prompting method with largelanguage models. The results highlight the challenges of supervisedfine-tuning, whose performance is not robust to distribution shifts. Bycontrast, the proposed self-consistency prompting approach greatly improves theperformance of large language models, enabling GPT-4o to achieve 0.548 SemF1,surpassing the performance of a fully fine-tuned BART-base model. Finally, wedemonstrate the potential of our method to serve as a general NLPinfrastructure, exemplified by its application in epidemic event detection fromsocial media.</description><author>Di Wu, Xiaoxian Shen, Kai-Wei Chang</author><pubDate>Fri, 04 Oct 2024 16:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00191v2</guid></item><item><title>VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models</title><link>http://arxiv.org/abs/2410.00741v2</link><description>Contrastive Language-Image Pre-training (CLIP) has been widely studied andapplied in numerous applications. However, the emphasis on brief summary textsduring pre-training prevents CLIP from understanding long descriptions. Thisissue is particularly acute regarding videos given that videos often containabundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtraLength) model, which aims to unleash the long-description understandingcapability of video CLIP models. Firstly, we establish an automatic datacollection system and gather a large-scale VILD pre-training dataset with VIdeoand Long-Description pairs. Then, we propose Text-similarity-guided PrimaryComponent Matching (TPCM) to better learn the distribution of feature spacewhile expanding the long description capability. We also introduce two newtasks namely Detail-aware Description Ranking (DDR) and Hallucination-awareDescription Ranking (HDR) for further understanding improvement. Finally, weconstruct a Long Video Description Ranking (LVDR) benchmark for evaluating thelong-description capability more comprehensively. Extensive experimentalresults on widely-used text-video retrieval benchmarks with both short and longdescriptions and our LVDR benchmark can fully demonstrate the effectiveness ofour method.</description><author>Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, Lianwen Jin</author><pubDate>Fri, 04 Oct 2024 16:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00741v2</guid></item><item><title>N-Agent Ad Hoc Teamwork</title><link>http://arxiv.org/abs/2404.10740v3</link><description>Current approaches to learning cooperative multi-agent behaviors assumerelatively restrictive settings. In standard fully cooperative multi-agentreinforcement learning, the learning algorithm controls $\textit{all}$ agentsin the scenario, while in ad hoc teamwork, the learning algorithm usuallyassumes control over only a $\textit{single}$ agent in the scenario. However,many cooperative settings in the real world are much less restrictive. Forexample, in an autonomous driving scenario, a company might train its cars withthe same learning algorithm, yet once on the road, these cars must cooperatewith cars from another company. Towards expanding the class of scenarios thatcooperative learning methods may optimally address, we introduce $N$-agent adhoc teamwork (NAHT), where a set of autonomous agents must interact andcooperate with dynamically varying numbers and types of teammates. This paperformalizes the problem, and proposes the Policy Optimization with AgentModelling (POAM) algorithm. POAM is a policy gradient, multi-agentreinforcement learning approach to the NAHT problem, that enables adaptation todiverse teammate behaviors by learning representations of teammate behaviors.Empirical evaluation on tasks from the multi-agent particle environment andStarCraft II shows that POAM improves cooperative task returns compared tobaseline approaches, and enables out-of-distribution generalization to unseenteammates.</description><author>Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, Peter Stone</author><pubDate>Fri, 04 Oct 2024 16:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10740v3</guid></item><item><title>fÃ¦rdXel: An Expert System for Danish Traffic Law</title><link>http://arxiv.org/abs/2410.03560v1</link><description>We present f{\ae}rdXel, a tool for symbolic reasoning in the domain of Danishtraffic law. f{\ae}rdXel combines techniques from logic programming with anovel interface that allows users to navigate through its reasoning process,thereby ensuring the system's trustworthiness. A preliminary empiricalevaluation indicates that this work is seen as very promising, and has thepotential to become a foundation for real-world AI tools supportingprofessionals in the Danish legal sector.</description><author>LuÃ­s Cruz-Filipe, Jonas Vistrup</author><pubDate>Fri, 04 Oct 2024 16:07:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03560v1</guid></item><item><title>Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy</title><link>http://arxiv.org/abs/2402.12821v3</link><description>Factual inconsistencies pose a significant hurdle for the faithfulsummarization by generative models. While a major direction to enhanceinconsistency detection is to derive stronger Natural Language Inference (NLI)models, we propose an orthogonal aspect that underscores the importance ofincorporating task-specific taxonomy into the inference. To this end, weconsolidate key error types of inconsistent facts in summaries, and incorporatethem to facilitate both the zero-shot and supervised paradigms of LLMs.Extensive experiments on ten datasets of five distinct domains suggest that,zero-shot LLM inference could benefit from the explicit solution space depictedby the error type taxonomy, and achieves state-of-the-art performance overall,surpassing specialized non-LLM baselines, as well as recent LLM baselines. Wefurther distill models that fuse the taxonomy into parameters through ourdesigned prompt completions and supervised training strategies, efficientlysubstituting state-of-the-art zero-shot inference with much larger LLMs.</description><author>Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, Fei Liu</author><pubDate>Fri, 04 Oct 2024 16:07:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12821v3</guid></item><item><title>Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features</title><link>http://arxiv.org/abs/2410.03558v1</link><description>Diffusion models are initially designed for image generation. Recent researchshows that the internal signals within their backbones, named activations, canalso serve as dense features for various discriminative tasks such as semanticsegmentation. Given numerous activations, selecting a small yet effectivesubset poses a fundamental problem. To this end, the early study of this fieldperforms a large-scale quantitative comparison of the discriminative ability ofthe activations. However, we find that many potential activations have not beenevaluated, such as the queries and keys used to compute attention scores.Moreover, recent advancements in diffusion architectures bring many newactivations, such as those within embedded ViT modules. Both combined,activation selection remains unresolved but overlooked. To tackle this issue,this paper takes a further step with a much broader range of activationsevaluated. Considering the significant increase in activations, a full-scalequantitative comparison is no longer operational. Instead, we seek tounderstand the properties of these activations, such that the activations thatare clearly inferior can be filtered out in advance via simple qualitativeevaluation. After careful analysis, we discover three properties universalamong diffusion models, enabling this study to go beyond specific models. Ontop of this, we present effective feature selection solutions for severalpopular diffusion models. Finally, the experiments across multiplediscriminative tasks validate the superiority of our method over the SOTAcompetitors. Our code is available athttps://github.com/Darkbblue/generic-diffusion-feature.</description><author>Benyuan Meng, Qianqian Xu, Zitai Wang, Xiaochun Cao, Qingming Huang</author><pubDate>Fri, 04 Oct 2024 16:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03558v1</guid></item><item><title>Enhancing Autonomous Navigation by Imaging Hidden Objects using Single-Photon LiDAR</title><link>http://arxiv.org/abs/2410.03555v1</link><description>Robust autonomous navigation in environments with limited visibility remainsa critical challenge in robotics. We present a novel approach that leveragesNon-Line-of-Sight (NLOS) sensing using single-photon LiDAR to improvevisibility and enhance autonomous navigation. Our method enables mobile robotsto "see around corners" by utilizing multi-bounce light information,effectively expanding their perceptual range without additional infrastructure.We propose a three-module pipeline: (1) Sensing, which captures multi-bouncehistograms using SPAD-based LiDAR; (2) Perception, which estimates occupancymaps of hidden regions from these histograms using a convolutional neuralnetwork; and (3) Control, which allows a robot to follow safe paths based onthe estimated occupancy. We evaluate our approach through simulations andreal-world experiments on a mobile robot navigating an L-shaped corridor withhidden obstacles. Our work represents the first experimental demonstration ofNLOS imaging for autonomous navigation, paving the way for safer and moreefficient robotic systems operating in complex environments. We also contributea novel dynamics-integrated transient rendering framework for simulating NLOSscenarios, facilitating future research in this domain.</description><author>Aaron Young, Nevindu M. Batagoda, Harry Zhang, Akshat Dave, Adithya Pediredla, Dan Negrut, Ramesh Raskar</author><pubDate>Fri, 04 Oct 2024 16:03:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03555v1</guid></item><item><title>Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding</title><link>http://arxiv.org/abs/2410.03553v1</link><description>Proteins, as essential biomolecules, play a central role in biologicalprocesses, including metabolic reactions and DNA replication. Accurateprediction of their properties and functions is crucial in biologicalapplications. Recent development of protein language models (pLMs) withsupervised fine tuning provides a promising solution to this problem. However,the fine-tuned model is tailored for particular downstream prediction task, andachieving general-purpose protein understanding remains a challenge. In thispaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)framework to bridge this gap. Our approach integrates a noval structure-awaremodule into pLMs to inform them with structural knowledge, and then connectsthese enhanced pLMs to large language models (LLMs) to generate understandingof proteins. In this framework, we propose a novel two-stage instruction tuningpipeline that first establishes a basic understanding of proteins throughcaption-based instructions and then refines this understanding using a mixtureof experts (MoEs) to learn more complex properties and functional informationwith the same amount of activated parameters. Moreover, we construct thelargest and most comprehensive protein instruction dataset to date, whichallows us to train and evaluate the general-purpose protein understandingmodel. Extensive experimental results on open-ended generation and closed-setanswer tasks demonstrate the superior performance of SEPIT over bothclosed-source general LLMs and open-source LLMs trained with protein knowledge.</description><author>Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang</author><pubDate>Fri, 04 Oct 2024 16:02:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03553v1</guid></item><item><title>Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research</title><link>http://arxiv.org/abs/2410.03545v1</link><description>Research in natural language processing (NLP) for Computational SocialScience (CSS) heavily relies on data from social media platforms. This dataplays a crucial role in the development of models for analysingsocio-linguistic phenomena within online communities. In this work, we conductan in-depth examination of 20 datasets extensively used in NLP for CSS tocomprehensively examine data quality. Our analysis reveals that social mediadatasets exhibit varying levels of data duplication. Consequently, this givesrise to challenges like label inconsistencies and data leakage, compromisingthe reliability of models. Our findings also suggest that data duplication hasan impact on the current claims of state-of-the-art performance, potentiallyleading to an overestimation of model effectiveness in real-world scenarios.Finally, we propose new protocols and best practices for improving datasetdevelopment from social media data and its usage.</description><author>Yida Mu, Mali Jin, Xingyi Song, Nikolaos Aletras</author><pubDate>Fri, 04 Oct 2024 15:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03545v1</guid></item><item><title>Re-examining Sexism and Misogyny Classification with Annotator Attitudes</title><link>http://arxiv.org/abs/2410.03543v1</link><description>Gender-Based Violence (GBV) is an increasing problem online, but existingdatasets fail to capture the plurality of possible annotator perspectives orensure the representation of affected groups. We revisit two important stagesin the moderation pipeline for GBV: (1) manual data labelling; and (2)automated classification. For (1), we examine two datasets to investigate therelationship between annotator identities and attitudes and the responses theygive to two GBV labelling tasks. To this end, we collect demographic andattitudinal information from crowd-sourced annotators using three validatedsurveys from Social Psychology. We find that higher Right Wing Authoritarianismscores are associated with a higher propensity to label text as sexist, whilefor Social Dominance Orientation and Neosexist Attitudes, higher scores areassociated with a negative tendency to do so. For (2), we conductclassification experiments using Large Language Models and five promptingstrategies, including infusing prompts with annotator information. We find: (i)annotator attitudes affect the ability of classifiers to predict their labels;(ii) including attitudinal information can boost performance when we usewell-structured brief annotator descriptions; and (iii) models struggle toreflect the increased complexity and imbalanced classes of the new label sets.</description><author>Aiqi Jiang, Nikolas Vitsakis, Tanvi Dinkar, Gavin Abercrombie, Ioannis Konstas</author><pubDate>Fri, 04 Oct 2024 15:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03543v1</guid></item><item><title>Ward: Provable RAG Dataset Inference via LLM Watermarks</title><link>http://arxiv.org/abs/2410.03537v1</link><description>Retrieval-Augmented Generation (RAG) improves LLMs by enabling them toincorporate external data during generation. This raises concerns for dataowners regarding unauthorized use of their content in RAG systems. Despite itsimportance, the challenge of detecting such unauthorized usage remainsunderexplored, with existing datasets and methodologies from adjacent fieldsbeing ill-suited for its study. In this work, we take several steps to bridgethis gap. First, we formalize this problem as (black-box) RAG Dataset Inference(RAG-DI). To facilitate research on this challenge, we further introduce anovel dataset specifically designed for benchmarking RAG-DI methods underrealistic conditions, and propose a set of baseline approaches. Building onthis foundation, we introduce Ward, a RAG-DI method based on LLM watermarksthat enables data owners to obtain rigorous statistical guarantees regardingthe usage of their dataset in a RAG system. In our experimental evaluation, weshow that Ward consistently outperforms all baselines across many challengingsettings, achieving higher accuracy, superior query efficiency and robustness.Our work provides a foundation for future studies of RAG-DI and highlights LLMwatermarks as a promising approach to this problem.</description><author>Nikola JovanoviÄ, Robin Staab, Maximilian Baader, Martin Vechev</author><pubDate>Fri, 04 Oct 2024 15:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03537v1</guid></item><item><title>Explainable Earth Surface Forecasting under Extreme Events</title><link>http://arxiv.org/abs/2410.01770v2</link><description>With climate change-related extreme events on the rise, high dimensionalEarth observation data presents a unique opportunity for forecasting andunderstanding impacts on ecosystems. This is, however, impeded by thecomplexity of processing, visualizing, modeling, and explaining this data. Toshowcase how this challenge can be met, here we train a convolutional longshort-term memory-based architecture on the novel DeepExtremeCubes dataset.DeepExtremeCubes includes around 40,000 long-term Sentinel-2 minicubes (January2016-October 2022) worldwide, along with labeled extreme events, meteorologicaldata, vegetation land cover, and topography map, sampled from locationsaffected by extreme climate events and surrounding areas. When predictingfuture reflectances and vegetation impacts through kernel normalized differencevegetation index, the model achieved an R$^2$ score of 0.9055 in the test set.Explainable artificial intelligence was used to analyze the model's predictionsduring the October 2020 Central South America compound heatwave and droughtevent. We chose the same area exactly one year before the event ascounterfactual, finding that the average temperature and surface pressure aregenerally the best predictors under normal conditions. In contrast, minimumanomalies of evaporation and surface latent heat flux take the lead during theevent. A change of regime is also observed in the attributions before theevent, which might help assess how long the event was brewing before happening.The code to replicate all experiments and figures in this paper is publiclyavailable at https://github.com/DeepExtremes/txyXAI</description><author>Oscar J. Pellicer-Valero, Miguel-Ãngel FernÃ¡ndez-Torres, Chaonan Ji, Miguel D. Mahecha, Gustau Camps-Valls</author><pubDate>Fri, 04 Oct 2024 15:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01770v2</guid></item><item><title>NRGBoost: Energy-Based Generative Boosted Trees</title><link>http://arxiv.org/abs/2410.03535v1</link><description>Despite the rise to dominance of deep learning in unstructured data domains,tree-based methods such as Random Forests (RF) and Gradient Boosted DecisionTrees (GBDT) are still the workhorses for handling discriminative tasks ontabular data. We explore generative extensions of these popular algorithms witha focus on explicitly modeling the data density (up to a normalizationconstant), thus enabling other applications besides sampling. As our maincontribution we propose an energy-based generative boosting algorithm that isanalogous to the second order boosting implemented in popular packages likeXGBoost. We show that, despite producing a generative model capable of handlinginference tasks over any input variable, our proposed algorithm can achievesimilar discriminative performance to GBDT on a number of real world tabulardatasets, outperforming alternative generative approaches. At the same time, weshow that it is also competitive with neural network based models for sampling.</description><author>JoÃ£o Bravo</author><pubDate>Fri, 04 Oct 2024 15:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03535v1</guid></item><item><title>MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction</title><link>http://arxiv.org/abs/2410.03531v1</link><description>Unsupervised rationale extraction aims to extract text snippets to supportmodel predictions without explicit rationale annotation. Researchers have mademany efforts to solve this task. Previous works often encode each aspectindependently, which may limit their ability to capture meaningful internalcorrelations between aspects. While there has been significant work onmitigating spurious correlations, our approach focuses on leveraging thebeneficial internal correlations to improve multi-aspect rationale extraction.In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explainand predict multiple aspects simultaneously. Concretely, we propose aMulti-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion toencode multiple text chunks simultaneously. Furthermore, multiple specialtokens are prepended in front of the text with each corresponding to onecertain aspect. Finally, multi-task training is deployed to reduce the trainingoverhead. Experimental results on two unsupervised rationale extractionbenchmarks show that MARE achieves state-of-the-art performance. Ablationstudies further demonstrate the effectiveness of our method. Our codes havebeen available at https://github.com/CSU-NLP-Group/MARE.</description><author>Han Jiang, Junwen Duan, Zhe Qu, Jianxin Wang</author><pubDate>Fri, 04 Oct 2024 15:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03531v1</guid></item><item><title>PRF: Parallel Resonate and Fire Neuron for Long Sequence Learning in Spiking Neural Networks</title><link>http://arxiv.org/abs/2410.03530v1</link><description>Recently, there is growing demand for effective and efficient long sequencemodeling, with State Space Models (SSMs) proving to be effective for longsequence tasks. To further reduce energy consumption, SSMs can be adapted toSpiking Neural Networks (SNNs) using spiking functions. However, currentspiking-formalized SSMs approaches still rely on float-point matrix-vectormultiplication during inference, undermining SNNs' energy advantage. In thiswork, we address the efficiency and performance challenges of long sequencelearning in SNNs simultaneously. First, we propose a decoupled reset method forparallel spiking neuron training, reducing the typical Leaky Integrate-and-Fire(LIF) model's training time from $O(L^2)$ to $O(L\log L)$, effectively speedingup the training by $6.57 \times$ to $16.50 \times$ on sequence lengths $1,024$to $32,768$. To our best knowledge, this is the first time that parallelcomputation with a reset mechanism is implemented achieving equivalence to itssequential counterpart. Secondly, to capture long-range dependencies, wepropose a Parallel Resonate and Fire (PRF) neuron, which leverages anoscillating membrane potential driven by a resonate mechanism from adifferentiable reset function in the complex domain. The PRF enables efficientlong sequence learning while maintaining parallel training. Finally, wedemonstrate that the proposed spike-driven architecture using PRF achievesperformance comparable to Structured SSMs (S4), with two orders of magnitudereduction in energy consumption, outperforming Transformer on Long Range Arenatasks.</description><author>Yulong Huang, Zunchang Liu, Changchun Feng, Xiaopeng Lin, Hongwei Ren, Haotian Fu, Yue Zhou, Hong Xing, Bojun Cheng</author><pubDate>Fri, 04 Oct 2024 15:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03530v1</guid></item><item><title>No Need to Talk: Asynchronous Mixture of Language Models</title><link>http://arxiv.org/abs/2410.03529v1</link><description>We introduce SmallTalk LM, an innovative method for training a mixture oflanguage models in an almost asynchronous manner. Each model of the mixturespecializes in distinct parts of the data distribution, without the need ofhigh-bandwidth communication between the nodes training each model. Atinference, a lightweight router directs a given sequence to a single expert,according to a short prefix. This inference scheme naturally uses a fraction ofthe parameters from the overall mixture model. Our experiments on languagemodeling demonstrate tha SmallTalk LM achieves significantly lower perplexitythan dense model baselines for the same total training FLOPs and an almostidentical inference cost. Finally, in our downstream evaluations we outperformthe dense baseline on $75\%$ of the tasks.</description><author>Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan Collobert</author><pubDate>Fri, 04 Oct 2024 15:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03529v1</guid></item><item><title>Branches: A Fast Dynamic Programming and Branch &amp; Bound Algorithm for Optimal Decision Trees</title><link>http://arxiv.org/abs/2406.02175v3</link><description>Decision Tree (DT) Learning is a fundamental problem in Interpretable MachineLearning, yet it poses a formidable optimisation challenge. Despite numerousefforts dating back to the early 1990's, practical algorithms have onlyrecently emerged, primarily leveraging Dynamic Programming (DP) and Branch &amp;Bound (B&amp;B) techniques. These methods fall into two categories: algorithms likeDL8.5, MurTree and STreeD utilise an efficient DP strategy but lack effectivebounds for pruning the search space; while algorithms like OSDT and GOSDTemploy more efficient pruning bounds but at the expense of a less refined DPstrategy. We introduce Branches, a new algorithm that combines the strengths ofboth approaches. Using DP and B&amp;B with a novel analytical bound for efficientpruning, Branches offers both speed and sparsity optimisation. Unlike othermethods, it also handles non-binary features. Theoretical analysis shows itslower complexity compared to existing methods, and empirical results confirmthat Branches outperforms the state-of-the-art in speed, iterations, andoptimality.</description><author>Ayman Chaouki, Jesse Read, Albert Bifet</author><pubDate>Fri, 04 Oct 2024 15:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02175v3</guid></item><item><title>Steering Large Language Models between Code Execution and Textual Reasoning</title><link>http://arxiv.org/abs/2410.03524v1</link><description>While a lot of recent research focuses on enhancing the textual reasoningcapabilities of Large Language Models (LLMs) by optimizing the multi-agentframework or reasoning chains, several benchmark tasks can be solved with 100%success through direct coding, which is more scalable and avoids thecomputational overhead associated with textual iterating and searching. Textualreasoning has inherent limitations in solving tasks with challenges in math,logics, optimization, and searching, which is unlikely to be solved by simplyscaling up the model and data size. The recently released OpenAI GPT CodeInterpreter and multi-agent frameworks such as AutoGen have demonstratedremarkable proficiency of integrating code generation and execution to solvecomplex tasks using LLMs. However, based on our experiments on 7 existingpopular methods for steering code/text generation in both single- andmulti-turn settings with 14 tasks and 6 types of LLMs (including the newO1-preview), currently there is no optimal method to correctly steer LLMs towrite code when needed. We discover some interesting patterns on when modelsuse code vs. textual reasoning with the evolution to task complexity and modelsizes, which even result in an astonishingly inverse scaling law. We alsodiscover that results from LLM written code are not always better than usingtextual reasoning, even if the task could be solved through code. To mitigatethe above issues, we propose three methods to better steer LLM code/textgeneration and achieve a notable improvement. The costs of token lengths andruntime are thoroughly discussed for all the methods. We believe the problem ofsteering LLM code/text generation is critical for future research and has muchspace for further improvement. Project Page, Datasets, and Codes are availableat https://yongchao98.github.io/CodeSteer/.</description><author>Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, Chi Wang</author><pubDate>Fri, 04 Oct 2024 15:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03524v1</guid></item><item><title>A Probabilistic Perspective on Unlearning and Alignment for Large Language Models</title><link>http://arxiv.org/abs/2410.03523v1</link><description>Comprehensive evaluation of Large Language Models (LLMs) is an open researchproblem. Existing evaluations rely on deterministic point estimates generatedvia greedy decoding. However, we find that deterministic evaluations fail tocapture the whole output distribution of a model, yielding inaccurateestimations of model capabilities. This is particularly problematic in criticalcontexts such as unlearning and alignment, where precise model evaluations arecrucial. To remedy this, we introduce the first formal probabilistic evaluationframework in LLMs. Namely, we derive novel metrics with high-probabilityguarantees concerning the output distribution of a model. Our metrics areapplication-independent and allow practitioners to make more reliable estimatesabout model capabilities before deployment. Through a case study focused onunlearning, we reveal that deterministic evaluations falsely indicatesuccessful unlearning, whereas our probabilistic evaluations demonstrate thatmost if not all of the supposedly unlearned information remains accessible inthese models. Additionally, we propose a novel unlearning loss based on entropyoptimization and adaptive temperature scaling, which significantly improvesunlearning in probabilistic settings on recent benchmarks. Our proposed shiftfrom point estimates to probabilistic evaluations of output distributionsrepresents an important step toward comprehensive evaluations of LLMs.https://github.com/yascho/probabilistic-unlearning</description><author>Yan Scholten, Stephan GÃ¼nnemann, Leo Schwinn</author><pubDate>Fri, 04 Oct 2024 15:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03523v1</guid></item><item><title>Graph Cuts with Arbitrary Size Constraints Through Optimal Transport</title><link>http://arxiv.org/abs/2402.04732v2</link><description>A common way of partitioning graphs is through minimum cuts. One drawback ofclassical minimum cut methods is that they tend to produce small groups, whichis why more balanced variants such as normalized and ratio cuts have seen moresuccess. However, we believe that with these variants, the balance constraintscan be too restrictive for some applications like for clustering of imbalanceddatasets, while not being restrictive enough for when searching for perfectlybalanced partitions. Here, we propose a new graph cut algorithm forpartitioning graphs under arbitrary size constraints. We formulate the graphcut problem as a Gromov-Wasserstein with a concave regularizer problem. We thenpropose to solve it using an accelerated proximal GD algorithm which guaranteesglobal convergence to a critical point, results in sparse solutions and onlyincurs an additional ratio of $\mathcal{O}(\log(n))$ compared to the classicalspectral clustering algorithm but was seen to be more efficient.</description><author>Chakib Fettal, Lazhar Labiod, Mohamed Nadif</author><pubDate>Fri, 04 Oct 2024 15:39:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04732v2</guid></item><item><title>Buckle Up: Robustifying LLMs at Every Customization Stage via Data Curation</title><link>http://arxiv.org/abs/2410.02220v2</link><description>Large language models (LLMs) are extensively adapted for downstreamapplications through a process known as "customization," with fine-tuning beinga common method for integrating domain-specific expertise. However, recentstudies have revealed a vulnerability that tuning LLMs with malicious samplescan compromise their robustness and amplify harmful content, an attack known as"jailbreaking." To mitigate such attack, we propose an effective defensiveframework utilizing data curation to revise commonsense texts and enhance theirsafety implication from the perspective of LLMs. The curated texts can mitigatejailbreaking attacks at every stage of the customization process: beforecustomization to immunize LLMs against future jailbreak attempts, duringcustomization to neutralize jailbreaking risks, or after customization torestore the compromised models. Since the curated data strengthens LLMs throughthe standard fine-tuning workflow, we do not introduce additional modulesduring LLM inference, thereby preserving the original customization process.Experimental results demonstrate a substantial reduction in jailbreakingeffects, with up to a 100% success in generating responsible responses.Notably, our method is effective even with commonsense texts, which are oftenmore readily available than safety-relevant data. With the every-stagedefensive framework and supporting experimental performance, this workrepresents a significant advancement in mitigating jailbreaking risks andensuring the secure customization of LLMs.</description><author>Xiaoqun Liu, Jiacheng Liang, Luoxi Tang, Chenyu You, Muchao Ye, Zhaohan Xi</author><pubDate>Fri, 04 Oct 2024 15:39:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02220v2</guid></item><item><title>Improving Online Bagging for Complex Imbalanced Data Stream</title><link>http://arxiv.org/abs/2410.03519v1</link><description>Learning classifiers from imbalanced and concept drifting data streams isstill a challenge. Most of the current proposals focus on taking into accountchanges in the global imbalance ratio only and ignore the local difficultyfactors, such as the minority class decomposition into sub-concepts and thepresence of unsafe types of examples (borderline or rare ones). As the abovefactors present in the stream may deteriorate the performance of popular onlineclassifiers, we propose extensions of resampling online bagging, namelyNeighbourhood Undersampling or Oversampling Online Bagging to take betteraccount of the presence of unsafe minority examples. The performedcomputational experiments with synthetic complex imbalanced data streams haveshown their advantage over earlier variants of online bagging resamplingensembles.</description><author>Bartosz Przybyl, Jerzy Stefanowski</author><pubDate>Fri, 04 Oct 2024 15:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03519v1</guid></item><item><title>Fine-Grained Expressive Power of Weisfeiler-Leman: A Homomorphism Counting Perspective</title><link>http://arxiv.org/abs/2410.03517v1</link><description>The ability of graph neural networks (GNNs) to count homomorphisms hasrecently been proposed as a practical and fine-grained measure of theirexpressive power. Although several existing works have investigated thehomomorphism counting power of certain GNN families, a simple and unifiedframework for analyzing the problem is absent. In this paper, we first propose\emph{generalized folklore Weisfeiler-Leman (GFWL)} algorithms as a flexibledesign basis for expressive GNNs, and then provide a theoretical framework toalgorithmically determine the homomorphism counting power of an arbitrary classof GNN within the GFWL design space. As the considered design space is largeenough to accommodate almost all known powerful GNNs, our result greatlyextends all existing works, and may find its application in the automation ofGNN model design.</description><author>Junru Zhou, Muhan Zhang</author><pubDate>Fri, 04 Oct 2024 15:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03517v1</guid></item><item><title>A Survey on Time-Series Pre-Trained Models</title><link>http://arxiv.org/abs/2305.10716v2</link><description>Time-Series Mining (TSM) is an important research area since it shows greatpotential in practical applications. Deep learning models that rely on massivelabeled data have been utilized for TSM successfully. However, constructing alarge-scale well-labeled dataset is difficult due to data annotation costs.Recently, pre-trained models have gradually attracted attention in the timeseries domain due to their remarkable performance in computer vision andnatural language processing. In this survey, we provide a comprehensive reviewof Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding,applying, and studying TS-PTMs. Specifically, we first briefly introduce thetypical deep learning models employed in TSM. Then, we give an overview ofTS-PTMs according to the pre-training techniques. The main categories weexplore include supervised, unsupervised, and self-supervised TS-PTMs. Further,extensive experiments involving 27 methods, 434 datasets, and 679 transferlearning scenarios are conducted to analyze the advantages and disadvantages oftransfer learning strategies, Transformer-based models, and representativeTS-PTMs. Finally, we point out some potential directions of TS-PTMs for futurework.</description><author>Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong Yu, James T. Kwok</author><pubDate>Fri, 04 Oct 2024 15:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10716v2</guid></item><item><title>To Err Is Human, but Llamas Can Learn It Too</title><link>http://arxiv.org/abs/2403.05493v2</link><description>This study explores enhancing grammatical error correction (GEC) throughartificial error generation (AEG) using language models (LMs). Specifically, wefine-tune Llama 2-based LMs for error generation and find that this approachyields synthetic errors akin to human errors. Next, we train GEC Llama modelswith the help of these artificial errors and outperform previousstate-of-the-art error correction models, with gains ranging between 0.8 and 6F0.5 points across all tested languages (German, Ukrainian, and Estonian).Moreover, we demonstrate that generating errors by fine-tuning smallersequence-to-sequence models and prompting large commercial LMs (GPT-3.5 andGPT-4) also results in synthetic errors beneficially affecting error generationmodels.</description><author>Agnes Luhtaru, Taido Purason, Martin Vainikko, Maksym Del, Mark Fishel</author><pubDate>Fri, 04 Oct 2024 15:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05493v2</guid></item><item><title>A Unified Theory of Quantum Neural Network Loss Landscapes</title><link>http://arxiv.org/abs/2408.11901v2</link><description>Classical neural networks with random initialization famously behave asGaussian processes in the limit of many neurons, which allows one to completelycharacterize their training and generalization behavior. No such generalunderstanding exists for quantum neural networks (QNNs), which -- outside ofcertain special cases -- are known to not behave as Gaussian processes whenrandomly initialized. We here prove that QNNs and their first two derivativesinstead generally form what we call "Wishart processes," where certainalgebraic properties of the network determine the hyperparameters of theprocess. This Wishart process description allows us to, for the first time:give necessary and sufficient conditions for a QNN architecture to have aGaussian process limit; calculate the full gradient distribution, generalizingpreviously known barren plateau results; and calculate the local minimadistribution of algebraically constrained QNNs. Our unified framework suggestsa certain simple operational definition for the "trainability" of a given QNNmodel using a newly introduced, experimentally accessible quantity we call the"degrees of freedom" of the network architecture.</description><author>Eric R. Anschuetz</author><pubDate>Fri, 04 Oct 2024 15:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11901v2</guid></item><item><title>Linear combinations of Gaussian latents in generative models: interpolation and beyond</title><link>http://arxiv.org/abs/2408.08558v2</link><description>Sampling from generative models has become a crucial tool for applicationslike data synthesis and augmentation. Diffusion, Flow Matching and ContinuousNormalizing Flows have shown effectiveness across various modalities, and relyon Gaussian latent variables for generation. For search-based or creativeapplications that require additional control over the generation process, ithas become common to manipulate the latent variable directly. However, existingapproaches for performing such manipulations (e.g. interpolation or forminglow-dimensional representations) only work well in special cases or are networkor data-modality specific. We propose Combination of Gaussian variables (COG)as a general purpose interpolation method that is easy to implement yetoutperforms recent sophisticated methods. Moreover, COG naturally addresses thebroader task of forming general linear combinations of latent variables,allowing the construction of subspaces of the latent space, dramaticallysimplifying the creation of expressive low-dimensional spaces ofhigh-dimensional objects.</description><author>Erik Bodin, Carl Henrik Ek, Henry Moss</author><pubDate>Fri, 04 Oct 2024 15:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08558v2</guid></item><item><title>Stabilized Neural Prediction of Potential Outcomes in Continuous Time</title><link>http://arxiv.org/abs/2410.03514v1</link><description>Patient trajectories from electronic health records are widely used topredict potential outcomes of treatments over time, which then allows topersonalize care. Yet, existing neural methods for this purpose have a keylimitation: while some adjust for time-varying confounding, these methodsassume that the time series are recorded in discrete time. In other words, theyare constrained to settings where measurements and treatments are conducted atfixed time steps, even though this is unrealistic in medical practice. In thiswork, we aim to predict potential outcomes in continuous time. The latter is ofdirect practical relevance because it allows for modeling patient trajectorieswhere measurements and treatments take place at arbitrary, irregulartimestamps. We thus propose a new method called stabilized continuous timeinverse propensity network (SCIP-Net). For this, we further derive stabilizedinverse propensity weights for robust prediction of the potential outcomes. Tothe best of our knowledge, our SCIP-Net is the first neural method thatperforms proper adjustments for time-varying confounding in continuous time.</description><author>Konstantin Hess, Stefan Feuerriegel</author><pubDate>Fri, 04 Oct 2024 15:29:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03514v1</guid></item><item><title>Infinite Limits of Multi-head Transformer Dynamics</title><link>http://arxiv.org/abs/2405.15712v2</link><description>In this work, we analyze various scaling limits of the training dynamics oftransformer models in the feature learning regime. We identify the set ofparameterizations that admit well-defined infinite width and depth limits,allowing the attention layers to update throughout training--a relevant notionof feature learning in these models. We then use tools from dynamical meanfield theory (DMFT) to analyze various infinite limits (infinite key/querydimension, infinite heads, and infinite depth) which have different statisticaldescriptions depending on which infinite limit is taken and how attentionlayers are scaled. We provide numerical evidence of convergence to the limitsand discuss how the parameterization qualitatively influences learned features.</description><author>Blake Bordelon, Hamza Tahir Chaudhry, Cengiz Pehlevan</author><pubDate>Fri, 04 Oct 2024 15:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15712v2</guid></item><item><title>Towards Efficient Hyperdimensional Computing Using Photonics</title><link>http://arxiv.org/abs/2311.17801v2</link><description>Over the past few years, silicon photonics-based computing has emerged as apromising alternative to CMOS-based computing for Deep Neural Networks (DNN).Unfortunately, the non-linear operations and the high-precision requirements ofDNNs make it extremely challenging to design efficient silicon photonics-basedsystems for DNN inference and training. Hyperdimensional Computing (HDC) is anemerging, brain-inspired machine learning technique that enjoys severaladvantages over existing DNNs, including being lightweight, requiringlow-precision operands, and being robust to noise introduced by thenonidealities in the hardware. For HDC, computing in-memory (CiM) approacheshave been widely used, as CiM reduces the data transfer cost if the operandscan fit into the memory. However, inefficient multi-bit operations, high writelatency, and low endurance make CiM ill-suited for HDC. On the other hand, theexisting electro-photonic DNN accelerators are inefficient for HDC because theyare specifically optimized for matrix multiplication in DNNs and consume a lotof power with high-precision data converters. In this paper, we argue that photonic computing and HDC complement each otherbetter than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,the first-ever electro-photonic accelerator for HDC training and inference,supporting the basic, record-based, and graph encoding schemes. Evaluating withpopular datasets, we show that our accelerator can achieve two to five ordersof magnitude lower EDP than the state-of-the-art electro-photonic DNNaccelerators for implementing HDC training and inference. PhotoHDC alsoachieves four orders of magnitude lower energy-delay product than CiM-basedaccelerators for both HDC training and inference.</description><author>Farbin Fayza, Cansu Demirkiran, Hanning Chen, Che-Kai Liu, Avi Mohan, Hamza Errahmouni, Sanggeon Yun, Mohsen Imani, David Zhang, Darius Bunandar, Ajay Joshi</author><pubDate>Fri, 04 Oct 2024 15:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17801v2</guid></item><item><title>Authentication by Location Tracking in Underwater Acoustic Networks</title><link>http://arxiv.org/abs/2410.03511v1</link><description>Physical layer message authentication in underwater acoustic networks (UWANs)leverages the characteristics of the underwater acoustic channel (UWAC) as afingerprint of the transmitting device. However, as the device moves its UWACchanges, and the authentication mechanism must track such variations. In thispaper, we propose a context-based authentication mechanism operating in twosteps: first, we estimate the position of the underwater device, then wepredict its future position based on the previously estimated ones. To checkthe authenticity of the transmission, we compare the estimated and thepredicted position. The location is estimated using a convolutional neuralnetwork taking as input the sample covariance matrix of the estimated UWACs.The prediction uses either a Kalman filter or a recurrent neural network (RNN).The authentication check is performed on the squared error between thepredicted and estimated positions. The solution based on the Kalman filteroutperforms that built on the RNN when the device moves according to acorrelated Gauss-Markov mobility model, which reproduces a typical underwatermotion.</description><author>Gianmaria Ventura, Francesco Ardizzon, Stefano Tomasin</author><pubDate>Fri, 04 Oct 2024 15:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03511v1</guid></item><item><title>Avoiding Catastrophe in Online Learning by Asking for Help</title><link>http://arxiv.org/abs/2402.08062v3</link><description>Most learning algorithms with formal regret guarantees assume that no mistakeis irreparable and essentially rely on trying all possible behaviors. Thisapproach is problematic when some mistakes are \emph{catastrophic}, i.e.,irreparable. We propose an online learning problem where the goal is tominimize the chance of catastrophe. Specifically, we assume that the payoff ineach round represents the chance of avoiding catastrophe that round and aim tomaximize the product of payoffs (the overall chance of avoiding catastrophe)while allowing a limited number of queries to a mentor. We first show that ingeneral, any algorithm either constantly queries the mentor or is nearlyguaranteed to cause catastrophe. However, in settings where the mentor policyclass is learnable in the standard online learning model, we provide analgorithm whose regret and rate of querying the mentor both approach 0 as thetime horizon grows. Conceptually, if a policy class is learnable in the absenceof catastrophic risk, it is learnable in the presence of catastrophic risk ifthe agent can ask for help.</description><author>Benjamin Plaut, Hanlin Zhu, Stuart Russell</author><pubDate>Fri, 04 Oct 2024 15:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08062v3</guid></item><item><title>"Seeing the Big through the Small": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?</title><link>http://arxiv.org/abs/2406.17600v2</link><description>Human label variation (HLV) is a valuable source of information that ariseswhen multiple human annotators provide different labels for valid reasons. InNatural Language Inference (NLI) earlier approaches to capturing HLV involveeither collecting annotations from many crowd workers to represent humanjudgment distribution (HJD) or use expert linguists to provide detailedexplanations for their chosen labels. While the former method provides denserHJD information, obtaining it is resource-intensive. In contrast, the latteroffers richer textual information but it is challenging to scale up to manyhuman judges. Besides, large language models (LLMs) are increasingly used asevaluators ("LLM judges") but with mixed results, and few works aim to studyHJDs. This study proposes to exploit LLMs to approximate HJDs using a smallnumber of expert labels and explanations. Our experiments show that a fewexplanations significantly improve LLMs' ability to approximate HJDs with andwithout explicit labels, thereby providing a solution to scale up annotationsfor HJD. However, fine-tuning smaller soft-label aware models with theLLM-generated model judgment distributions (MJDs) presents partiallyinconsistent results: while similar in distance, their resulting fine-tunedmodels and visualized distributions differ substantially. We show theimportance of complementing instance-level distance measures with aglobal-level shape metric and visualization to more effectively evaluate MJDsagainst human judgment distributions.</description><author>Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank</author><pubDate>Fri, 04 Oct 2024 15:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17600v2</guid></item><item><title>Classification-Denoising Networks</title><link>http://arxiv.org/abs/2410.03505v1</link><description>Image classification and denoising suffer from complementary issues of lackof robustness or partially ignoring conditioning information. We argue thatthey can be alleviated by unifying both tasks through a model of the jointprobability of (noisy) images and class labels. Classification is performedwith a forward pass followed by conditioning. Using the Tweedie-Miyasawaformula, we evaluate the denoising function with the score, which can becomputed by marginalization and back-propagation. The training objective isthen a combination of cross-entropy loss and denoising score matching lossintegrated over noise levels. Numerical experiments on CIFAR-10 and ImageNetshow competitive classification and denoising performance compared to referencedeep convolutional classifiers/denoisers, and significantly improves efficiencycompared to previous joint approaches. Our model shows an increased robustnessto adversarial perturbations compared to a standard discriminative classifier,and allows for a novel interpretation of adversarial gradients as a differenceof denoisers.</description><author>Louis Thiry, Florentin Guth</author><pubDate>Fri, 04 Oct 2024 15:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03505v1</guid></item><item><title>CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios</title><link>http://arxiv.org/abs/2410.03502v1</link><description>With the proliferation of Large Language Models (LLMs) in diverse domains,there is a particular need for unified evaluation standards in clinical medicalscenarios, where models need to be examined very thoroughly. We presentCliMedBench, a comprehensive benchmark with 14 expert-guided core clinicalscenarios specifically designed to assess the medical ability of LLMs across 7pivot dimensions. It comprises 33,735 questions derived from real-world medicalreports of top-tier tertiary hospitals and authentic examination exercises. Thereliability of this benchmark has been confirmed in several ways. Subsequentexperiments with existing LLMs have led to the following findings: (i) Chinesemedical LLMs underperform on this benchmark, especially where medical reasoningand factual consistency are vital, underscoring the need for advances inclinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMsdemonstrate substantial potential in medical clinics, while the limited inputcapacity of many medical LLMs hinders their practical use. These findingsreveal both the strengths and limitations of LLMs in clinical scenarios andoffer critical insights for medical research.</description><author>Zetian Ouyang, Yishuai Qiu, Linlin Wang, Gerard de Melo, Ya Zhang, Yanfeng Wang, Liang He</author><pubDate>Fri, 04 Oct 2024 15:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03502v1</guid></item><item><title>A SMART Mnemonic Sounds like "Glue Tonic": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick</title><link>http://arxiv.org/abs/2406.15352v2</link><description>Keyword mnemonics are memorable explanations that link new terms to simplerkeywords. Prior work generates mnemonics for students, but they do not trainmodels using mnemonics students prefer and aid learning. We build SMART, amnemonic generator trained on feedback from real students learning new terms.To train SMART, we first fine-tune LLaMA-2 on a curated set of user-writtenmnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonicsgenerated by SMART in a flashcard app to find preferences on mnemonics studentsfavor. We gather 2684 preferences from 45 students across two types: expressed(inferred from ratings) and observed (inferred from student learning), yieldingthree key findings. First, expressed and observed preferences disagree; whatstudents think is helpful does not always capture what is truly helpful.Second, Bayesian models can synthesize complementary data from multiplepreference types into a single effectiveness signal. SMART is tuned via DirectPreference Optimization on this signal, which resolves ties and missing labelsin the typical method of pairwise comparisons, augmenting data for LLM outputquality gains. Third, mnemonic experts assess SMART as matching GPT-4 at muchlower deployment costs, showing the utility of capturing diverse studentfeedback to align LLMs in education.</description><author>Nishant Balepur, Matthew Shu, Alexander Hoyle, Alison Robey, Shi Feng, Seraphina Goldfarb-Tarrant, Jordan Boyd-Graber</author><pubDate>Fri, 04 Oct 2024 15:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15352v2</guid></item><item><title>Flow of Reasoning:Training LLMs for Divergent Problem Solving with Minimal Examples</title><link>http://arxiv.org/abs/2406.05673v3</link><description>The ability to generate diverse solutions to a given problem is a hallmark ofhuman creativity. This divergent reasoning is also crucial for machines,enhancing their robustness and enabling them to assist humans in manyapplications such as scientific discovery. However, existing approaches tomulti-step reasoning with large language models (LLMs) have mostly focused onlyon reasoning accuracy, without further discovering more diverse validsolutions. For example, supervised fine-tuning can improve LLM reasoningquality, but requires extensive supervised data to capture the full range ofpossible solutions. Reinforcement learning aims to find limited highest-rewardsolutions while neglecting the solution diversity. To fill this gap, we proposeFlow of Reasoning (FoR), an efficient diversity-seeking LLM finetuning methodaimed at improving reasoning quality and diversity with minimal data. FoRformulates multi-step LLM reasoning as a Markovian flow on a DAG-structuredreasoning graph. This formulation allows us to incorporate and adapt principledGFlowNet approaches, for finetuning LLMs to sample diverse reasoning paths withprobabilities proportional to the (unnormalized) reward of target problems.Extensive experiments show that, with limited training examples (e.g., 15examples), FoR enables the discovery of diverse, creative, high-qualitysolutions, greatly outperforming a wide range of existing inference andtraining methods across five challenging puzzle-solving tasks, includingBlocksWorld (embodied reasoning), Game24 (math puzzle solving), Rubik's Cube(spatial reasoning), 1D-ARC (abstraction reasoning), and PrOntoQA (logicalreasoning). Code is available at https://github.com/Yu-Fangxu/FoR.</description><author>Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin</author><pubDate>Fri, 04 Oct 2024 15:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05673v3</guid></item><item><title>FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator</title><link>http://arxiv.org/abs/2410.03499v1</link><description>Federated Learning (FL) facilitates data privacy by enabling collaborativein-situ training across decentralized clients. Despite its inherent advantages,FL faces significant challenges of performance and convergence when dealingwith data that is not independently and identically distributed (non-i.i.d.).While previous research has primarily addressed the issue of skewed labeldistribution across clients, this study focuses on the less explored challengeof multi-domain FL, where client data originates from distinct domains withvarying feature distributions. We introduce a novel method designed to addressthese challenges FedStein: Enhancing Multi-Domain Federated Learning Throughthe James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)estimates of batch normalization (BN) statistics across clients, whilemaintaining local BN parameters. The non-BN layer parameters are exchanged viastandard FL techniques. Extensive experiments conducted across three datasetsand multiple models demonstrate that FedStein surpasses existing methods suchas FedAvg and FedBN, with accuracy improvements exceeding 14% in certaindomains leading to enhanced domain generalization. The code is available athttps://github.com/sunnyinAI/FedStein</description><author>Sunny Gupta, Nikita Jangid, Amit Sethi</author><pubDate>Fri, 04 Oct 2024 15:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03499v1</guid></item><item><title>Collaborative and Efficient Personalization with Mixtures of Adaptors</title><link>http://arxiv.org/abs/2410.03497v1</link><description>Non-iid data is prevalent in real-world federated learning problems. Dataheterogeneity can come in different types in terms of distribution shifts. Inthis work, we are interested in the heterogeneity that comes from conceptshifts, i.e., shifts in the prediction across clients. In particular, weconsider multi-task learning, where we want the model to adapt to the task ofthe client. We propose a parameter-efficient framework to tackle this issue,where each client learns to mix between parameter-efficient adaptors accordingto its task. We use Low-Rank Adaptors (LoRAs) as the backbone and extend itsconcept to other types of layers. We call our framework Federated Low-RankAdaptive Learning (FLoRAL). This framework is not an algorithm but rather amodel parameterization for a multi-task learning objective, so it can work ontop of any algorithm that optimizes this objective, which includes manyalgorithms from the literature. FLoRAL is memory-efficient, and clients arepersonalized with small states (e.g., one number per adaptor) as the adaptorsthemselves are federated. Hence, personalization is--in this sense--federatedas well. Even though clients can personalize more freely by training an adaptorlocally, we show that collaborative and efficient training of adaptors ispossible and performs better. We also show that FLoRAL can outperform anensemble of full models with optimal cluster assignment, which demonstrates thebenefits of federated personalization and the robustness of FLoRAL tooverfitting. We show promising experimental results on synthetic datasets,real-world federated multi-task problems such as MNIST, CIFAR-10, andCIFAR-100. We also provide a theoretical analysis of local SGD on a relaxedobjective and discuss the effects of aggregation mismatch on convergence.</description><author>Abdulla Jasem Almansoori, Samuel HorvÃ¡th, Martin TakÃ¡Ä</author><pubDate>Fri, 04 Oct 2024 15:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03497v1</guid></item><item><title>Jailbreaking as a Reward Misspecification Problem</title><link>http://arxiv.org/abs/2406.14393v3</link><description>The widespread adoption of large language models (LLMs) has raised concernsabout their safety and reliability, particularly regarding their vulnerabilityto adversarial attacks. In this paper, we propose a novel perspective thatattributes this vulnerability to reward misspecification during the alignmentprocess. This misspecification occurs when the reward function fails toaccurately capture the intended behavior, leading to misaligned model outputs.We introduce a metric ReGap to quantify the extent of reward misspecificationand demonstrate its effectiveness and robustness in detecting harmful backdoorprompts. Building upon these insights, we present ReMiss, a system forautomated red teaming that generates adversarial prompts in areward-misspecified space. ReMiss achieves state-of-the-art attack successrates on the AdvBench benchmark against various target aligned LLMs whilepreserving the human readability of the generated prompts. Furthermore, theseattacks on open-source models demonstrate high transferability to closed-sourcemodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailedanalysis highlights the unique advantages of the proposed rewardmisspecification objective compared to previous methods, offering new insightsfor improving LLM safety and robustness.</description><author>Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong</author><pubDate>Fri, 04 Oct 2024 15:10:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14393v3</guid></item></channel></rss>