<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 02 Dec 2024 13:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs</title><link>http://arxiv.org/abs/2411.19951v1</link><description>The success of Multimodal Large Language Models (MLLMs) in the image domainhas garnered wide attention from the research community. Drawing on previoussuccessful experiences, researchers have recently explored extending thesuccess to the video understanding realms. Apart from training from scratch, anefficient way is to utilize the pre-trained image-LLMs, leading to twomainstream approaches, i.e. zero-shot inference and further fine-tuning withvideo data. In this work, our study of these approaches harvests an effectivedata augmentation method. We first make a deeper inspection of the zero-shotinference way and identify two limitations, i.e. limited generalization andlack of temporal understanding capabilities. Thus, we further investigate thefine-tuning approach and find a low learning efficiency when simply using allthe video data samples, which can be attributed to a lack of instructiondiversity. Aiming at this issue, we develop a method called T2Vid to synthesizevideo-like samples to enrich the instruction diversity in the training corpus.Integrating these data enables a simple and efficient training scheme, whichachieves performance comparable to or even superior to using full videodatasets by training with just 15% the sample size. Meanwhile, we find that theproposed scheme can boost the performance of long video understanding withouttraining with long video samples. We hope our study will spark more thinkingabout using MLLMs for video understanding and curation of high-quality data.The code is released at https://github.com/xjtupanda/T2Vid.</description><author>Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, Enhong Chen</author><pubDate>Fri, 29 Nov 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19951v1</guid></item><item><title>AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos</title><link>http://arxiv.org/abs/2411.19950v1</link><description>We introduce AlphaTablets, a novel and generic representation of 3D planesthat features continuous 3D surface and precise boundary delineation. Byrepresenting 3D planes as rectangles with alpha channels, AlphaTablets combinethe advantages of current 2D and 3D plane representations, enabling accurate,consistent and flexible modeling of 3D planes. We derive differentiablerasterization on top of AlphaTablets to efficiently render 3D planes intoimages, and propose a novel bottom-up pipeline for 3D planar reconstructionfrom monocular videos. Starting with 2D superpixels and geometric cues frompre-trained models, we initialize 3D planes as AlphaTablets and optimize themvia differentiable rendering. An effective merging scheme is introduced tofacilitate the growth and refinement of AlphaTablets. Through iterativeoptimization and merging, we reconstruct complete and accurate 3D planes withsolid surfaces and clear boundaries. Extensive experiments on the ScanNetdataset demonstrate state-of-the-art performance in 3D planar reconstruction,underscoring the great potential of AlphaTablets as a generic 3D planerepresentation for various applications. Project page is available at:https://hyzcluster.github.io/alphatablets</description><author>Yuze He, Wang Zhao, Shaohui Liu, Yubin Hu, Yushi Bai, Yu-Hui Wen, Yong-Jin Liu</author><pubDate>Fri, 29 Nov 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19950v1</guid></item><item><title>DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation</title><link>http://arxiv.org/abs/2411.19946v1</link><description>Recent advances in dataset distillation have led to solutions in two maindirections. The conventional batch-to-batch matching mechanism is ideal forsmall-scale datasets and includes bi-level optimization methods on models andsyntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods likedistribution matching, gradient matching, and weight trajectory matching.Conversely, batch-to-global matching typifies decoupled methods, which areparticularly advantageous for large-scale datasets. This approach has garneredsubstantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD,and CDA. A primary challenge with the second approach is the lack of diversityamong syntheses within each class since samples are optimized independently andthe same global supervision signals are reused across different syntheticimages. In this study, we propose a new Diversity-driven EarlyLate Training(DELT) scheme to enhance the diversity of images in batch-to-global matchingwith less computation. Our approach is conceptually simple yet effective, itpartitions predefined IPC samples into smaller subtasks and employs localoptimizations to distill each subset into distributions from distinct phases,reducing the uniformity induced by the unified optimization process. Thesedistilled images from the subtasks demonstrate effective generalization whenapplied to the entire task. We conduct extensive experiments on CIFAR,Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms theprevious state-of-the-art by 2$\sim$5% on average across different datasets andIPCs (images per class), increasing diversity per class by more than 5% whilereducing synthesis time by up to 39.3% for enhancing the training efficiency.Code is available at: https://github.com/VILA-Lab/DELT.</description><author>Zhiqiang Shen, Ammar Sherif, Zeyuan Yin, Shitong Shao</author><pubDate>Fri, 29 Nov 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19946v1</guid></item><item><title>Reanimating Images using Neural Representations of Dynamic Stimuli</title><link>http://arxiv.org/abs/2406.02659v2</link><description>While computer vision models have made incredible strides in static imagerecognition, they still do not match human performance in tasks that requirethe understanding of complex, dynamic motion. This is notably true forreal-world scenarios where embodied agents face complex and motion-richenvironments. Our approach leverages state-of-the-art video diffusion models todecouple static image representation from motion generation, enabling us toutilize fMRI brain activity for a deeper understanding of human responses todynamic visual stimuli. Conversely, we also demonstrate that information aboutthe brain's representation of motion can enhance the prediction of optical flowin artificial systems. Our novel approach leads to four main findings: (1)Visual motion, represented as fine-grained, object-level resolution opticalflow, can be decoded from brain activity generated by participants viewingvideo stimuli; (2) Video encoders outperform image-based models in predictingvideo-driven brain activity; (3) Brain-decoded motion signals enable realisticvideo reanimation based only on the initial frame of the video; and (4) Weextend prior work to achieve full video decoding from video-driven brainactivity. This framework advances our understanding of how the brain representsspatial and temporal information in dynamic visual scenes. Our findingsdemonstrate the potential of combining brain imaging with video diffusionmodels for developing more robust and biologically-inspired computer visionsystems. We show additional decoding and encoding examples on this site:https://sites.google.com/view/neural-dynamics/home.</description><author>Jacob Yeung, Andrew F. Luo, Gabriel Sarch, Margaret M. Henderson, Deva Ramanan, Michael J. Tarr</author><pubDate>Fri, 29 Nov 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02659v2</guid></item><item><title>Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability</title><link>http://arxiv.org/abs/2411.19943v1</link><description>Large Language Models (LLMs) have exhibited remarkable performance onreasoning tasks. They utilize autoregressive token generation to constructreasoning trajectories, enabling the development of a coherent chain ofthought. In this work, we explore the impact of individual tokens on the finaloutcomes of reasoning tasks. We identify the existence of ``critical tokens''that lead to incorrect reasoning trajectories in LLMs. Specifically, we findthat LLMs tend to produce positive outcomes when forced to decode other tokensinstead of critical tokens. Motivated by this observation, we propose a novelapproach - cDPO - designed to automatically recognize and conduct token-levelrewards for the critical tokens during the alignment process. Specifically, wedevelop a contrastive estimation approach to automatically identify criticaltokens. It is achieved by comparing the generation likelihood of positive andnegative models. To achieve this, we separately fine-tune the positive andnegative models on various reasoning trajectories, consequently, they arecapable of identifying identify critical tokens within incorrect trajectoriesthat contribute to erroneous outcomes. Moreover, to further align the modelwith the critical token information during the alignment process, we extend theconventional DPO algorithms to token-level DPO and utilize the differentiallikelihood from the aforementioned positive and negative model as importantweight for token-level DPO learning.Experimental results on GSM8K and MATH500benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math(7B) demonstrate the effectiveness of the propsoed approach cDPO.</description><author>Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu</author><pubDate>Fri, 29 Nov 2024 18:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19943v1</guid></item><item><title>Free-form Generation Enhances Challenging Clothed Human Modeling</title><link>http://arxiv.org/abs/2411.19942v1</link><description>Achieving realistic animated human avatars requires accurate modeling ofpose-dependent clothing deformations. Existing learning-based methods heavilyrely on the Linear Blend Skinning (LBS) of minimally-clothed human models likeSMPL to model deformation. However, these methods struggle to handle looseclothing, such as long dresses, where the canonicalization process becomesill-defined when the clothing is far from the body, leading to disjointed andfragmented results. To overcome this limitation, we propose a novel hybridframework to model challenging clothed humans. Our core idea is to usededicated strategies to model different regions, depending on whether they areclose to or distant from the body. Specifically, we segment the human body intothree categories: unclothed, deformed, and generated. We simply replicateunclothed regions that require no deformation. For deformed regions close tothe body, we leverage LBS to handle the deformation. As for the generatedregions, which correspond to loose clothing areas, we introduce a novelfree-form, part-aware generator to model them, as they are less affected bymovements. This free-form generation paradigm brings enhanced flexibility andexpressiveness to our hybrid framework, enabling it to capture the intricategeometric details of challenging loose clothing, such as skirts and dresses.Experimental results on the benchmark dataset featuring loose clothingdemonstrate that our method achieves state-of-the-art performance with superiorvisual fidelity and realism, particularly in the most challenging cases.</description><author>Hang Ye, Xiaoxuan Ma, Hai Ci, Wentao Zhu, Yizhou Wang</author><pubDate>Fri, 29 Nov 2024 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19942v1</guid></item><item><title>Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark</title><link>http://arxiv.org/abs/2411.19941v1</link><description>Following the successful 2023 edition, we organised the Second PerceptionTest challenge as a half-day workshop alongside the IEEE/CVF EuropeanConference on Computer Vision (ECCV) 2024, with the goal of benchmarkingstate-of-the-art video models and measuring the progress since last year usingthe Perception Test benchmark. This year, the challenge had seven tracks (upfrom six last year) and covered low-level and high-level tasks, with languageand non-language interfaces, across video, audio, and text modalities; theadditional track covered hour-long video understanding and introduced a novelvideo QA benchmark 1h-walk VQA. Overall, the tasks in the different trackswere: object tracking, point tracking, temporal action localisation, temporalsound localisation, multiple-choice video question-answering, grounded videoquestion-answering, and hour-long video question-answering. We summarise inthis report the challenge tasks and results, and introduce in detail the novelhour-long video QA benchmark 1h-walk VQA.</description><author>Joseph Heyward, João Carreira, Dima Damen, Andrew Zisserman, Viorica Pătrăucean</author><pubDate>Fri, 29 Nov 2024 18:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19941v1</guid></item><item><title>Diffeomorphic Latent Neural Operators for Data-Efficient Learning of Solutions to Partial Differential Equations</title><link>http://arxiv.org/abs/2411.18014v2</link><description>A computed approximation of the solution operator to a system of partialdifferential equations (PDEs) is needed in various areas of science andengineering. Neural operators have been shown to be quite effective atpredicting these solution generators after training on high-fidelity groundtruth data (e.g. numerical simulations). However, in order to generalize wellto unseen spatial domains, neural operators must be trained on an extensiveamount of geometrically varying data samples that may not be feasible toacquire or simulate in certain contexts (e.g., patient-specific medical data,large-scale computationally intensive simulations.) We propose that in order tolearn a PDE solution operator that can generalize across multiple domainswithout needing to sample enough data expressive enough for all possiblegeometries, we can train instead a latent neural operator on just a few groundtruth solution fields diffeomorphically mapped from different geometric/spatialdomains to a fixed reference configuration. Furthermore, the form of thesolutions is dependent on the choice of mapping to and from the referencedomain. We emphasize that preserving properties of the differential operatorwhen constructing these mappings can significantly reduce the data requirementfor achieving an accurate model due to the regularity of the solution fieldsthat the latent neural operator is training on. We provide motivating numericalexperimentation that demonstrates an extreme case of this consideration byexploiting the conformal invariance of the Laplacian</description><author>Zan Ahmad, Shiyi Chen, Minglang Yin, Avisha Kumar, Nicolas Charon, Natalia Trayanova, Mauro Maggioni</author><pubDate>Fri, 29 Nov 2024 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18014v2</guid></item><item><title>VLSBench: Unveiling Visual Leakage in Multimodal Safety</title><link>http://arxiv.org/abs/2411.19939v1</link><description>Safety concerns of Multimodal large language models (MLLMs) have graduallybecome an important problem in various applications. Surprisingly, previousworks indicate a counter-intuitive phenomenon that using textual unlearning toalign MLLMs achieves comparable safety performances with MLLMs trained withimage-text pairs. To explain such a counter-intuitive phenomenon, we discover avisual safety information leakage (VSIL) problem in existing multimodal safetybenchmarks, i.e., the potentially risky and sensitive content in the image hasbeen revealed in the textual query. In this way, MLLMs can easily refuse thesesensitive text-image queries according to textual queries. However, image-textpairs without VSIL are common in real-world scenarios and are overlooked byexisting multimodal safety benchmarks. To this end, we construct multimodalvisual leakless safety benchmark (VLSBench) preventing visual safety leakagefrom image to textual query with 2.4k image-text pairs. Experimental resultsindicate that VLSBench poses a significant challenge to both open-source andclose-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.This study demonstrates that textual alignment is enough for multimodal safetyscenarios with VSIL, while multimodal alignment is a more promising solutionfor multimodal safety scenarios without VSIL. Please see our code and data at:http://hxhcreate.github.io/VLSBench</description><author>Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao</author><pubDate>Fri, 29 Nov 2024 18:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19939v1</guid></item><item><title>An Operator Splitting View of Federated Learning</title><link>http://arxiv.org/abs/2108.05974v2</link><description>Over the past few years, the federated learning ($\texttt{FL}$) community haswitnessed a proliferation of new $\texttt{FL}$ algorithms. However, ourunderstating of the theory of $\texttt{FL}$ is still fragmented, and athorough, formal comparison of these algorithms remains elusive. Motivated bythis gap, we show that many of the existing $\texttt{FL}$ algorithms can beunderstood from an operator splitting point of view. This unification allows usto compare different algorithms with ease, to refine previous convergenceresults and to uncover new algorithmic variants. In particular, our analysisreveals the vital role played by the step size in $\texttt{FL}$ algorithms. Theunification also leads to a streamlined and economic way to accelerate$\texttt{FL}$ algorithms, without incurring any communication overhead. Weperform numerical experiments on both convex and nonconvex models to validateour findings.</description><author>Saber Malekmohammadi, Kiarash Shaloudegi, Zeou Hu, Yaoliang Yu</author><pubDate>Fri, 29 Nov 2024 18:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.05974v2</guid></item><item><title>MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds</title><link>http://arxiv.org/abs/2405.17421v2</link><description>We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction systemdesigned to reconstruct and synthesize novel views of dynamic scenes frommonocular videos captured casually in the wild. To address such a challengingand ill-posed inverse problem, we leverage prior knowledge from foundationalvision models and lift the video data to a novel Motion Scaffold (MoSca)representation, which compactly and smoothly encodes the underlyingmotions/deformations. The scene geometry and appearance are then disentangledfrom the deformation field and are encoded by globally fusing the Gaussiansanchored onto the MoSca and optimized via Gaussian Splatting. Additionally,camera focal length and poses can be solved using bundle adjustment without theneed of any other pose estimation tools. Experiments demonstratestate-of-the-art performance on dynamic rendering benchmarks and itseffectiveness on real videos.</description><author>Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, Kostas Daniilidis</author><pubDate>Fri, 29 Nov 2024 18:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17421v2</guid></item><item><title>Gradient Routing: Masking Gradients to Localize Computation in Neural Networks</title><link>http://arxiv.org/abs/2410.04332v2</link><description>Neural networks are trained primarily based on their inputs and outputs,without regard for their internal mechanisms. These neglected mechanismsdetermine properties that are critical for safety, like (i) transparency; (ii)the absence of sensitive information or harmful capabilities; and (iii)reliable generalization of goals beyond the training distribution. To addressthis shortcoming, we introduce gradient routing, a training method thatisolates capabilities to specific subregions of a neural network. Gradientrouting applies data-dependent, weighted masks to gradients duringbackpropagation. These masks are supplied by the user in order to configurewhich parameters are updated by which data points. We show that gradientrouting can be used to (1) learn representations which are partitioned in aninterpretable way; (2) enable robust unlearning via ablation of a pre-specifiednetwork subregion; and (3) achieve scalable oversight of a reinforcementlearner by localizing modules responsible for different behaviors. Throughout,we find that gradient routing localizes capabilities even when applied to alimited, ad-hoc subset of the data. We conclude that the approach holds promisefor challenging, real-world applications where quality data are scarce.</description><author>Alex Cloud, Jacob Goldman-Wetzler, Evžen Wybitul, Joseph Miller, Alexander Matt Turner</author><pubDate>Fri, 29 Nov 2024 18:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.04332v2</guid></item><item><title>On the consistency of hyper-parameter selection in value-based deep reinforcement learning</title><link>http://arxiv.org/abs/2406.17523v3</link><description>Deep reinforcement learning (deep RL) has achieved tremendous success onvarious domains through a combination of algorithmic design and carefulselection of hyper-parameters. Algorithmic improvements are often the result ofiterative enhancements built upon prior approaches, while hyper-parameterchoices are typically inherited from previous methods or fine-tunedspecifically for the proposed technique. Despite their crucial impact onperformance, hyper-parameter choices are frequently overshadowed by algorithmicadvancements. This paper conducts an extensive empirical study focusing on thereliability of hyper-parameter selection for value-based deep reinforcementlearning agents, including the introduction of a new score to quantify theconsistency and reliability of various hyper-parameters. Our findings not onlyhelp establish which hyper-parameters are most critical to tune, but also helpclarify which tunings remain consistent across different training regimes.</description><author>Johan Obando-Ceron, João G. M. Araújo, Aaron Courville, Pablo Samuel Castro</author><pubDate>Fri, 29 Nov 2024 18:51:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17523v3</guid></item><item><title>Transfer Learning for High-dimensional Quantile Regression with Distribution Shift</title><link>http://arxiv.org/abs/2411.19933v1</link><description>Information from related source studies can often enhance the findings of atarget study. However, the distribution shift between target and source studiescan severely impact the efficiency of knowledge transfer. In thehigh-dimensional regression setting, existing transfer approaches mainly focuson the parameter shift. In this paper, we focus on the high-dimensionalquantile regression with knowledge transfer under three types of distributionshift: parameter shift, covariate shift, and residual shift. We propose a noveltransferable set and a new transfer framework to address the above threediscrepancies. Non-asymptotic estimation error bounds and source detectionconsistency are established to validate the availability and superiority of ourmethod in the presence of distribution shift. Additionally, an orthogonaldebiased approach is proposed for statistical inference with knowledgetransfer, leading to sharper asymptotic results. Extensive simulation resultsas well as real data applications further demonstrate the effectiveness of ourproposed procedure.</description><author>Ruiqi Bai, Yijiao Zhang, Hanbo Yang, Zhongyi Zhu</author><pubDate>Fri, 29 Nov 2024 18:49:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19933v1</guid></item><item><title>On Domain-Specific Post-Training for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2411.19930v1</link><description>Recent years have witnessed the rapid development of general multimodal largelanguage models (MLLMs). However, adapting general MLLMs to specific domains,such as scientific fields and industrial applications, remains less explored.This paper systematically investigates domain adaptation of MLLMs throughpost-training, focusing on data synthesis, training pipelines, and taskevaluation. (1) Data Synthesis: Using open-source models, we develop a visualinstruction synthesizer that effectively generates diverse visual instructiontasks from domain-specific image-caption pairs. Our synthetic tasks surpassthose generated by manual rules, GPT-4, and GPT-4V in enhancing thedomain-specific performance of MLLMs. (2) Training Pipeline: While thetwo-stage training--initially on image-caption pairs followed by visualinstruction tasks--is commonly adopted for developing general MLLMs, we apply asingle-stage training pipeline to enhance task diversity for domain-specificpost-training. (3) Task Evaluation: We conduct experiments in two domains,biomedicine and food, by post-training MLLMs of different sources and scales(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLMperformance on various domain-specific tasks. To support further research inMLLM domain adaptation, we will open-source our implementations.</description><author>Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang</author><pubDate>Fri, 29 Nov 2024 18:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19930v1</guid></item><item><title>Scalable Out-of-distribution Robustness in the Presence of Unobserved Confounders</title><link>http://arxiv.org/abs/2411.19923v1</link><description>We consider the task of out-of-distribution (OOD) generalization, where thedistribution shift is due to an unobserved confounder ($Z$) affecting both thecovariates ($X$) and the labels ($Y$). In this setting, traditional assumptionsof covariate and label shift are unsuitable due to the confounding, whichintroduces heterogeneity in the predictor, i.e., $\hat{Y} = f_Z(X)$. OODgeneralization differs from traditional domain adaptation by not assumingaccess to the covariate distribution ($X^\text{te}$) of the test samples duringtraining. These conditions create a challenging scenario for OOD robustness:(a) $Z^\text{tr}$ is an unobserved confounder during training, (b)$P^\text{te}{Z} \neq P^\text{tr}{Z}$, (c) $X^\text{te}$ is unavailable duringtraining, and (d) the posterior predictive distribution depends on$P^\text{te}(Z)$, i.e., $\hat{Y} = E_{P^\text{te}(Z)}[f_Z(X)]$. In general,accurate predictions are unattainable in this scenario, and existing literaturehas proposed complex predictors based on identifiability assumptions thatrequire multiple additional variables. Our work investigates a set ofidentifiability assumptions that tremendously simplify the predictor, whoseresulting elegant simplicity outperforms existing approaches.</description><author>Parjanya Prashant, Seyedeh Baharan Khatami, Bruno Ribeiro, Babak Salimi</author><pubDate>Fri, 29 Nov 2024 18:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19923v1</guid></item><item><title>Dynamic EEG-fMRI mapping: Revealing the relationship between brain connectivity and cognitive state</title><link>http://arxiv.org/abs/2411.19922v1</link><description>This study investigated the dynamic connectivity patterns between EEG andfMRI modalities, contributing to our understanding of brain networkinteractions. By employing a comprehensive approach that integrated static anddynamic analyses of EEG-fMRI data, we were able to uncover distinctconnectivity states and characterize their temporal fluctuations. The resultsrevealed modular organization within the intrinsic connectivity networks (ICNs)of the brain, highlighting the significant roles of sensory systems and thedefault mode network. The use of a sliding window technique allowed us toassess how functional connectivity varies over time, further elucidating thetransient nature of brain connectivity. Additionally, our findings align withprevious literature, reinforcing the notion that cognitive states can beeffectively identified through short-duration data, specifically within the30-60 second timeframe. The established relationships between connectivitystrength and cognitive processes, particularly during different visual states,underscore the relevance of our approach for future research into braindynamics. Overall, this study not only enhances our understanding of theinterplay between EEG and fMRI signals but also paves the way for furtherexploration into the neural correlates of cognitive functions and theirimplications in clinical settings. Future research should focus on refiningthese methodologies and exploring their applications in various cognitive andclinical contexts.</description><author>Guiran Liu, Binrong Zhu</author><pubDate>Fri, 29 Nov 2024 18:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19922v1</guid></item><item><title>SIMS: Simulating Human-Scene Interactions with Real World Script Planning</title><link>http://arxiv.org/abs/2411.19921v1</link><description>Simulating long-term human-scene interaction is a challenging yet fascinatingtask. Previous works have not effectively addressed the generation of long-termhuman scene interactions with detailed narratives for physics-based animation.This paper introduces a novel framework for the planning and controlling oflong-horizon physical plausible human-scene interaction. On the one hand, filmsand shows with stylish human locomotions or interactions with scenes areabundantly available on the internet, providing a rich source of data forscript planning. On the other hand, Large Language Models (LLMs) can understandand generate logical storylines. This motivates us to marry the two by using an LLM-based pipeline to extractscripts from videos, and then employ LLMs to imitate and create new scripts,capturing complex, time-series human behaviors and interactions withenvironments. By leveraging this, we utilize a dual-aware policy that achievesboth language comprehension and scene understanding to guide character motionswithin contextual and spatial constraints. To facilitate training andevaluation, we contribute a comprehensive planning dataset containing diversemotion sequences extracted from real-world videos and expand them with largelanguage models. We also collect and re-annotate motion clips from existingkinematic datasets to enable our policy learn diverse skills. Extensiveexperiments demonstrate the effectiveness of our framework in versatile taskexecution and its generalization ability to various scenarios, showingremarkably enhanced performance compared with existing methods. Our code anddata will be publicly available soon.</description><author>Wenjia Wang, Liang Pan, Zhiyang Dou, Zhouyingcheng Liao, Yuke Lou, Lei Yang, Jingbo Wang, Taku Komura</author><pubDate>Fri, 29 Nov 2024 18:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19921v1</guid></item><item><title>Geometry of fibers of the multiplication map of deep linear neural networks</title><link>http://arxiv.org/abs/2411.19920v1</link><description>We study the geometry of the algebraic set of tuples of composable matriceswhich multiply to a fixed matrix, using tools from the theory of quiverrepresentations. In particular, we determine its codimension $C$ and the number$\theta$ of its top-dimensional irreducible components. Our solution ispresented in three forms: a Poincar\'e series in equivariant cohomology, aquadratic integer program, and an explicit formula. In the course of the proof,we establish a surprising property: $C$ and $\theta$ are invariant underarbitrary permutations of the dimension vector. We also show that the reallog-canonical threshold of the function taking a tuple to the square Frobeniusnorm of its product is $C/2$. These results are motivated by the study of deeplinear neural networks in machine learning and Bayesian statistics (singularlearning theory) and show that deep linear networks are in a certain sense``mildly singular".</description><author>SImon Pepin Lehalleur, Richárd Rimányi</author><pubDate>Fri, 29 Nov 2024 18:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19920v1</guid></item><item><title>Handling irresolvable conflicts in the Semantic Web: an RDF-based conflict-tolerant version of the Deontic Traditional Scheme</title><link>http://arxiv.org/abs/2411.19918v1</link><description>This paper presents a new ontology that implements the well-known DeonticTraditional Scheme in RDFs and SPARQL, fit to handle irresolvable conflicts,i.e., situations in which two or more statements prescribe conflictingobligations, prohibitions, or permissions, with none of them being "stronger"than the other one(s). In our view, this paper marks a significant advancementin standard theoretical research in formal Deontic Logic. Most contemporaryapproaches in this field are confined to the propositional level, mainly focuson the notion of obligation, and lack implementations. The proposed frameworkis encoded in RDF, which is not only a first-order language but also the mostwidely used knowledge representation language, as it forms the foundation ofthe Semantic Web. Moreover, the proposed computational ontology formalizes alldeontic modalities defined in the Deontic Traditional Scheme, withoutspecifically focusing on obligations, and offers constructs to model and reasonwith various types of irresolvable conflicts, violations, and the interactionbetween deontic modalities and contextual constraints in a given state ofaffairs. To the best of our knowledge, no existing approach in the literatureaddresses all these aspects within a unified integrated framework. All examplespresented and discussed in this paper, together with Java code and clearinstructions to re-execute them locally, are available athttps://github.com/liviorobaldo/conflict-tolerantDeonticTraditionalScheme</description><author>Livio Robaldo, Gianluca Pozzato</author><pubDate>Fri, 29 Nov 2024 18:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19918v1</guid></item><item><title>Analysing Multiscale Clusterings with Persistent Homology</title><link>http://arxiv.org/abs/2305.04281v3</link><description>In many applications in data clustering, it is desirable to find not just asingle partition into clusters but a sequence of partitions describing the dataat different scales (or levels of coarseness). A natural problem then is toanalyse and compare the (not necessarily hierarchical) sequences of partitionsthat underpin multiscale descriptions of data. Here, we introduce theMultiscale Clustering Filtration (MCF), a well-defined and stable filtration ofabstract simplicial complexes that encodes arbitrary patterns of clusterassignments across scales of increasing coarseness. We show that thezero-dimensional persistent homology of the MCF measures the degree ofhierarchy in the sequence of partitions, and the higher-dimensional persistenthomology tracks the emergence and resolution of conflicts between clusterassignments across the sequence of partitions. To broaden the theoreticalfoundations of the MCF, we also provide an equivalent construction via a nervecomplex filtration, and we show that in the hierarchical case, the MCF reducesto a Vietoris-Rips filtration of an ultrametric space. We then use numericalexperiments to illustrate how the MCF can serve to characterise multiscaleclusterings of synthetic data from stochastic block models.</description><author>Dominik J. Schindler, Mauricio Barahona</author><pubDate>Fri, 29 Nov 2024 18:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04281v3</guid></item><item><title>DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2411.17660v2</link><description>Recent progress in scene synthesis makes standalone SLAM systems purely basedon optimizing hyperprimitives with a Rendering objective possible. However, thetracking performance still lacks behind traditional and end-to-end SLAMsystems. An optimal trade-off between robustness, speed and accuracy has notyet been reached, especially for monocular video. In this paper, we introduce aSLAM system based on an end-to-end Tracker and extend it with a Renderer basedon recent 3D Gaussian Splatting techniques. Our framework \textbf{DroidSplat}achieves both SotA tracking and rendering results on common SLAM benchmarks. Weimplemented multiple building blocks of modern SLAM systems to run in parallel,allowing for fast inference on common consumer GPU's. Recent progress inmonocular depth prediction and camera calibration allows our system to achievestrong results even on in-the-wild data without known camera intrinsics. Codewill be available at \url{https://github.com/ChenHoy/DROID-Splat}.</description><author>Christian Homeyer, Leon Begiristain, Christoph Schnörr</author><pubDate>Fri, 29 Nov 2024 18:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17660v2</guid></item><item><title>Quantifying the synthetic and real domain gap in aerial scene understanding</title><link>http://arxiv.org/abs/2411.19913v1</link><description>Quantifying the gap between synthetic and real-world imagery is essential forimproving both transformer-based models - that rely on large volumes of data -and datasets, especially in underexplored domains like aerial sceneunderstanding where the potential impact is significant. This paper introducesa novel methodology for scene complexity assessment using Multi-Model ConsensusMetric (MMCM) and depth-based structural metrics, enabling a robust evaluationof perceptual and structural disparities between domains. Our experimentalanalysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes)datasets, demonstrates that real-world scenes generally exhibit higherconsensus among state-of-the-art vision transformers, while synthetic scenesshow greater variability and challenge model adaptability. The resultsunderline the inherent complexities and domain gaps, emphasizing the need forenhanced simulation fidelity and model generalization. This work providescritical insights into the interplay between domain characteristics and modelperformance, offering a pathway for improved domain adaptation strategies inaerial scene understanding.</description><author>Alina Marcu</author><pubDate>Fri, 29 Nov 2024 18:18:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19913v1</guid></item><item><title>Another look at inference after prediction</title><link>http://arxiv.org/abs/2411.19908v1</link><description>Prediction-based (PB) inference is increasingly used in applications wherethe outcome of interest is difficult to obtain, but its predictors are readilyavailable. Unlike traditional inference, PB inference performs statisticalinference using a partially observed outcome and a set of covariates byleveraging a prediction of the outcome generated from a machine learning (ML)model. Motwani and Witten (2023) recently revisited two innovative PB inferenceapproaches for ordinary least squares. They found that the method proposed byWang et al. (2020) yields a consistent estimator for the association ofinterest when the ML model perfectly captures the underlying regressionfunction. Conversely, the prediction-powered inference (PPI) method proposed byAngelopoulos et al. (2023) yields valid inference regardless of the model'saccuracy. In this paper, we study the statistical efficiency of the PPIestimator. Our analysis reveals that a more efficient estimator, proposed 25years ago by Chen and Chen (2000), can be obtained by simply adding a weight tothe PPI estimator. We also contextualize PB inference with methods from theeconomics and statistics literature dating back to the 1960s. Our extensivetheoretical and numerical analyses indicate that the Chen and Chen (CC)estimator offers a balance between robustness to ML model specification andstatistical efficiency, making it the preferred choice for use in practice.</description><author>Jessica Gronsbell, Jianhui Gao, Yaqi Shi, Zachary R. McCaw, David Cheng</author><pubDate>Fri, 29 Nov 2024 18:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19908v1</guid></item><item><title>Classical and Quantum Algorithms for the Deterministic L-system Inductive Inference Problem</title><link>http://arxiv.org/abs/2411.19906v1</link><description>L-systems can be made to model and create simulations of many biologicalprocesses, such as plant development. Finding an L-system for a given processis typically solved by hand, by experts, in a hugely time-consuming process. Itwould be significant if this could be done automatically from data, such asfrom sequences of images. In this paper, we are interested in inferring aparticular type of L-system, deterministic context-free L-system (D0L-system)from a sequence of strings. We introduce the characteristic graph of a sequenceof strings, which we then utilize to translate our problem (inferringD0L-system) in polynomial time into the maximum independent set problem (MIS)and the SAT problem. After that, we offer a classical exact algorithm and anapproximate quantum algorithm for the problem.</description><author>Ali Lotfi, Ian McQuillan, Steven Rayan</author><pubDate>Fri, 29 Nov 2024 18:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19906v1</guid></item><item><title>$C^{3}$-NeRF: Modeling Multiple Scenes via Conditional-cum-Continual Neural Radiance Fields</title><link>http://arxiv.org/abs/2411.19903v1</link><description>Neural radiance fields (NeRF) have exhibited highly photorealistic renderingof novel views through per-scene optimization over a single 3D scene. With thegrowing popularity of NeRF and its variants, they have become ubiquitous andhave been identified as efficient 3D resources. However, they are still farfrom being scalable since a separate model needs to be stored for each scene,and the training time increases linearly with every newly added scene.Surprisingly, the idea of encoding multiple 3D scenes into a single NeRF modelis heavily under-explored. In this work, we propose a novelconditional-cum-continual framework, called $C^{3}$-NeRF, to accommodatemultiple scenes into the parameters of a single neural radiance field. Unlikeconventional approaches that leverage feature extractors and pre-trained priorsfor scene conditioning, we use simple pseudo-scene labels to model multiplescenes in NeRF. Interestingly, we observe the framework is also inherentlycontinual (via generative replay) with minimal, if not no, forgetting of thepreviously learned scenes. Consequently, the proposed framework adapts tomultiple new scenes without necessarily accessing the old data. Throughextensive qualitative and quantitative evaluation using synthetic and realdatasets, we demonstrate the inherent capacity of the NeRF model to accommodatemultiple scenes with high-quality novel-view renderings without addingadditional parameters. We provide implementation details and dynamicvisualizations of our results in the supplementary file.</description><author>Prajwal Singh, Ashish Tiwari, Gautam Vashishtha, Shanmuganathan Raman</author><pubDate>Fri, 29 Nov 2024 18:05:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19903v1</guid></item><item><title>Noncommutative Model Selection for Data Clustering and Dimension Reduction Using Relative von Neumann Entropy</title><link>http://arxiv.org/abs/2411.19902v1</link><description>We propose a pair of completely data-driven algorithms for unsupervisedclassification and dimension reduction, and we empirically study theirperformance on a number of data sets, both simulated data in three-dimensionsand images from the COIL-20 data set. The algorithms take as input a set ofpoints sampled from a uniform distribution supported on a metric space, thelatter embedded in an ambient metric space, and they output a clustering orreduction of dimension of the data. They work by constructing a natural familyof graphs from the data and selecting the graph which maximizes the relativevon Neumann entropy of certain normalized heat operators constructed from thegraphs. Once the appropriate graph is selected, the eigenvectors of the graphLaplacian may be used to reduce the dimension of the data, and clusters in thedata may be identified with the kernel of the associated graph Laplacian.Notably, these algorithms do not require information about the size of aneighborhood or the desired number of clusters as input, in contrast to popularalgorithms such as $k$-means, and even more modern spectral methods such asLaplacian eigenmaps, among others. In our computational experiments, our clustering algorithm outperforms$k$-means clustering on data sets with non-trivial geometry and topology, inparticular data whose clusters are not concentrated around a specific point,and our dimension reduction algorithm is shown to work well in several simpleexamples.</description><author>Araceli Guzmán-Tristán, Antonio Rieser</author><pubDate>Fri, 29 Nov 2024 18:04:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19902v1</guid></item><item><title>Efficient quantum-enhanced classical simulation for patches of quantum landscapes</title><link>http://arxiv.org/abs/2411.19896v1</link><description>Understanding the capabilities of classical simulation methods is key toidentifying where quantum computers are advantageous. Not only does this ensurethat quantum computers are used only where necessary, but also one canpotentially identify subroutines that can be offloaded onto a classical device.In this work, we show that it is always possible to generate a classicalsurrogate of a sub-region (dubbed a "patch") of an expectation landscapeproduced by a parameterized quantum circuit. That is, we provide aquantum-enhanced classical algorithm which, after simple measurements on aquantum device, allows one to classically simulate approximate expectationvalues of a subregion of a landscape. We provide time and sample complexityguarantees for a range of families of circuits of interest, and furthernumerically demonstrate our simulation algorithms on an exactly verifiablesimulation of a Hamiltonian variational ansatz and long-time dynamicssimulation on a 127-qubit heavy-hex topology.</description><author>Sacha Lerch, Ricard Puig, Manuel S. Rudolph, Armando Angrisani, Tyson Jones, M. Cerezo, Supanut Thanasilp, Zoë Holmes</author><pubDate>Fri, 29 Nov 2024 18:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19896v1</guid></item><item><title>GuardSplat: Robust and Efficient Watermarking for 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2411.19895v1</link><description>3D Gaussian Splatting (3DGS) has recently created impressive assets forvarious applications. However, the copyright of these assets is not wellprotected as existing watermarking methods are not suited for 3DGS consideringsecurity, capacity, and invisibility. Besides, these methods often requirehours or even days for optimization, limiting the application scenarios. Inthis paper, we propose GuardSplat, an innovative and efficient framework thateffectively protects the copyright of 3DGS assets. Specifically, 1) We firstpropose a CLIP-guided Message Decoupling Optimization module for training themessage decoder, leveraging CLIP's aligning capability and rich representationsto achieve a high extraction accuracy with minimal optimization costs,presenting exceptional capability and efficiency. 2) Then, we propose aSpherical-harmonic-aware (SH-aware) Message Embedding module tailored for 3DGS,which employs a set of SH offsets to seamlessly embed the message into the SHfeatures of each 3D Gaussian while maintaining the original 3D structure. Itenables the 3DGS assets to be watermarked with minimal fidelity trade-offs andprevents malicious users from removing the messages from the model files,meeting the demands for invisibility and security. 3) We further propose anAnti-distortion Message Extraction module to improve robustness against variousvisual distortions. Extensive experiments demonstrate that GuardSplatoutperforms the state-of-the-art methods and achieves fast optimization speed.</description><author>Zixuan Chen, Guangcong Wang, Jiahao Zhu, Jianhuang Lai, Xiaohua Xie</author><pubDate>Fri, 29 Nov 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19895v1</guid></item><item><title>Noncommutative Model Selection and the Data-Driven Estimation of Real Cohomology Groups</title><link>http://arxiv.org/abs/2411.19894v1</link><description>We propose three completely data-driven methods for estimating the realcohomology groups $H^k (X ; \mathbb{R})$ of a compact metric-measure space $(X,d_X, \mu_X)$ embedded in a metric-measure space $(Y,d_Y,\mu_Y)$, given a finiteset of points $S$ sampled from a uniform distrbution $\mu_X$ on $X$, possiblycorrupted with noise from $Y$. We present the results of several computationalexperiments in the case that $X$ is embedded in $\mathbb{R}^n$, where two ofthe three algorithms performed well.</description><author>Araceli Guzmán-Tristán, Antonio Rieser, Eduardo Velázquez-Richards</author><pubDate>Fri, 29 Nov 2024 17:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19894v1</guid></item><item><title>FlowCLAS: Enhancing Normalizing Flow Via Contrastive Learning For Anomaly Segmentation</title><link>http://arxiv.org/abs/2411.19888v1</link><description>Anomaly segmentation is a valuable computer vision task for safety-criticalapplications that need to be aware of unexpected events. Currentstate-of-the-art (SOTA) scene-level anomaly segmentation approaches rely ondiverse inlier class labels during training, limiting their ability to leveragevast unlabeled datasets and pre-trained vision encoders. These methods mayunderperform in domains with reduced color diversity and limited objectclasses. Conversely, existing unsupervised methods struggle with anomalysegmentation with the diverse scenes of less restricted domains. To addressthese challenges, we introduce FlowCLAS, a novel self-supervised framework thatutilizes vision foundation models to extract rich features and employs anormalizing flow network to learn their density distribution. We enhance themodel's discriminative power by incorporating Outlier Exposure and contrastivelearning in the latent space. FlowCLAS significantly outperforms all existingmethods on the ALLO anomaly segmentation benchmark for space robotics anddemonstrates competitive results on multiple road anomaly segmentationbenchmarks for autonomous driving, including Fishyscapes Lost&amp;Found and RoadAnomaly. These results highlight FlowCLAS's effectiveness in addressing theunique challenges of space anomaly segmentation while retaining SOTAperformance in the autonomous driving domain without reliance on inliersegmentation labels.</description><author>Chang Won Lee, Selina Leveugle, Svetlana Stolpner, Chris Langley, Paul Grouchy, Jonathan Kelly, Steven L. Waslander</author><pubDate>Fri, 29 Nov 2024 17:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19888v1</guid></item><item><title>PDDLFuse: A Tool for Generating Diverse Planning Domains</title><link>http://arxiv.org/abs/2411.19886v1</link><description>Various real-world challenges require planning algorithms that can adapt to abroad range of domains. Traditionally, the creation of planning domains hasrelied heavily on human implementation, which limits the scale and diversity ofavailable domains. While recent advancements have leveraged generative AItechnologies such as large language models (LLMs) for domain creation, theseefforts have predominantly focused on translating existing domains from naturallanguage descriptions rather than generating novel ones. In contrast, theconcept of domain randomization, which has been highly effective inreinforcement learning, enhances performance and generalizability by trainingon a diverse array of randomized new domains. Inspired by this success, ourtool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language(PDDL). PDDLFuse is designed to generate new, diverse planning domains that canbe used to validate new planners or test foundational planning models. We havedeveloped methods to adjust the domain generators parameters to modulate thedifficulty of the domains it generates. This adaptability is crucial asexisting domain-independent planners often struggle with more complex problems.Initial tests indicate that PDDLFuse efficiently creates intricate and varieddomains, representing a significant advancement over traditional domaingeneration methods and making a contribution towards planning research.</description><author>Vedant Khandelwal, Amit Sheth, Forest Agostinelli</author><pubDate>Fri, 29 Nov 2024 17:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19886v1</guid></item><item><title>Open source Differentiable ODE Solving Infrastructure</title><link>http://arxiv.org/abs/2411.19882v1</link><description>Ordinary Differential Equations (ODEs) are widely used in physics, chemistry,and biology to model dynamic systems, including reaction kinetics, populationdynamics, and biological processes. In this work, we integrate GPU-acceleratedODE solvers into the open-source DeepChem framework, making these tools easilyaccessible. These solvers support multiple numerical methods and are fullydifferentiable, enabling easy integration into more complex differentiableprograms. We demonstrate the capabilities of our implementation throughexperiments on Lotka-Volterra predator-prey dynamics, pharmacokineticcompartment models, neural ODEs, and solving PDEs using reaction-diffusionequations. Our solvers achieved high accuracy with mean squared errors rangingfrom $10^{-4}$ to $10^{-6}$ and showed scalability in solving large systemswith up to 100 compartments.</description><author>Rakshit Kr. Singh, Aaron Rock Menezes, Rida Irfan, Bharath Ramsundar</author><pubDate>Fri, 29 Nov 2024 17:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19882v1</guid></item><item><title>LUMIA: Linear probing for Unimodal and MultiModal Membership Inference A!acks leveraging internal LLM states</title><link>http://arxiv.org/abs/2411.19876v1</link><description>Large Language Models (LLMs) are increasingly used in a variety ofapplications, but concerns around membership inference have grown in parallel.Previous efforts focus on black-to-grey-box models, thus neglecting thepotential benefit from internal LLM information. To address this, we proposethe use of Linear Probes (LPs) as a method to detect Membership InferenceAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbedLUMIA, applies LPs layer-by-layer to get fine-grained data on the model innerworkings. We test this method across several model architectures, sizes anddatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIAachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previoustechniques. Remarkably, LUMIA reaches AUC&gt;60% in 65.33% of cases -- anincrement of 46.80% against the state of the art. Furthermore, our approachreveals key insights, such as the model layers where MIAs are most detectable.In multimodal models, LPs indicate that visual inputs can significantlycontribute to detect MIAs -- AUC&gt;60% is reached in 85.90% of experiments.</description><author>Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro</author><pubDate>Fri, 29 Nov 2024 17:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19876v1</guid></item><item><title>Unsupervised Speaker Diarization in Distributed IoT Networks Using Federated Learning</title><link>http://arxiv.org/abs/2404.10842v2</link><description>This paper presents a computationally efficient and distributed speakerdiarization framework for networked IoT-style audio devices. The work proposesa Federated Learning model which can identify the participants in aconversation without the requirement of a large audio database for training. Anunsupervised online update mechanism is proposed for the Federated Learningmodel which depends on cosine similarity of speaker embeddings. Moreover, theproposed diarization system solves the problem of speaker change detection via.unsupervised segmentation techniques using Hotelling's t-squared Statistic andBayesian Information Criterion. In this new approach, speaker change detectionis biased around detected quasi-silences, which reduces the severity of thetrade-off between the missed detection and false detection rates. Additionally,the computational overhead due to frame-by-frame identification of speakers isreduced via. unsupervised clustering of speech segments. The resultsdemonstrate the effectiveness of the proposed training method in the presenceof non-IID speech data. It also shows a considerable improvement in thereduction of false and missed detection at the segmentation stage, whilereducing the computational overhead. Improved accuracy and reducedcomputational cost makes the mechanism suitable for real-time speakerdiarization across a distributed IoT audio network.</description><author>Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas</author><pubDate>Fri, 29 Nov 2024 17:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10842v2</guid></item><item><title>Enhanced anomaly detection in well log data through the application of ensemble GANs</title><link>http://arxiv.org/abs/2411.19875v1</link><description>Although generative adversarial networks (GANs) have shown significantsuccess in modeling data distributions for image datasets, their application tostructured or tabular data, such as well logs, remains relativelyunderexplored. This study extends the ensemble GANs (EGANs) framework tocapture the distribution of well log data and detect anomalies that falloutside of these distributions. The proposed approach compares the performanceof traditional methods, such as Gaussian mixture models (GMMs), with EGANs indetecting anomalies outside the expected data distributions. For the gamma ray(GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76,outperforming GMM's precision of 0.38 and F1 score of 0.54. Similarly, fortravel time (DT), EGANs achieved a precision of 0.70 and F1 score of 0.79,surpassing GMM 0.56 and 0.71. In the neutron porosity (NPHI) dataset, EGANsrecorded a precision of 0.53 and F1 score of 0.68, outshining GMM 0.47 and0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52and an F1 score of 0.67, slightly outperforming GMM, which yielded a precisionof 0.50 and an F1 score of 0.65. This work's novelty lies in applying EGANs forwell log data analysis, showcasing their ability to learn data patterns andidentify anomalies that deviate from them. This approach offers more reliableanomaly detection compared to traditional methods like GMM. The findingshighlight the potential of EGANs in enhancing anomaly detection for well logdata, delivering significant implications for optimizing drilling strategiesand reservoir management through more accurate, data-driven insights intosubsurface characterization.</description><author>Abdulrahman Al-Fakih, A. Koeshidayatullah, Tapan Mukerji, SanLinn I. Kaka</author><pubDate>Fri, 29 Nov 2024 17:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19875v1</guid></item><item><title>SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts</title><link>http://arxiv.org/abs/2408.01537v3</link><description>Self-driving vehicles rely on multimodal motion forecasts to effectivelyinteract with their environment and plan safe maneuvers. We introduceSceneMotion, an attention-based model for forecasting scene-wide motion modesof multiple traffic agents. Our model transforms local agent-centric embeddingsinto scene-wide forecasts using a novel latent context module. This modulelearns a scene-wide latent space from multiple agent-centric embeddings,enabling joint forecasting and interaction modeling. The competitiveperformance in the Waymo Open Interaction Prediction Challenge demonstrates theeffectiveness of our approach. Moreover, we cluster future waypoints in timeand space to quantify the interaction between agents. We merge all modes andanalyze each mode independently to determine which clusters are resolvedthrough interaction or result in conflict. Our implementation is available at:https://github.com/kit-mrt/future-motion</description><author>Royden Wagner, Ömer Sahin Tas, Marlon Steiner, Fabian Konstantinidis, Hendrik Königshof, Marvin Klemp, Carlos Fernandez, Christoph Stiller</author><pubDate>Fri, 29 Nov 2024 17:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01537v3</guid></item><item><title>DeMo: Decoupled Momentum Optimization</title><link>http://arxiv.org/abs/2411.19870v1</link><description>Training large neural networks typically requires sharing gradients betweenaccelerators through specialized high-speed interconnects. Drawing from thesignal processing principles of frequency decomposition and energy compaction,we demonstrate that synchronizing full optimizer states and model parametersduring training is unnecessary. By decoupling momentum updates and allowingcontrolled divergence in optimizer states across accelerators, we achieveimproved convergence compared to state-of-the-art optimizers. We introduce{\textbf{De}}coupled {\textbf{Mo}}mentum (DeMo), a fused optimizer and dataparallel algorithm that reduces inter-accelerator communication requirements byseveral orders of magnitude. This enables training of large neural networkseven with limited network bandwidth and heterogeneous hardware. Our method istopology-agnostic and architecture-independent and supports scalableclock-synchronous distributed training with negligible compute and memoryoverhead. Empirical results show that models trained with DeMo match or exceedthe performance of equivalent models trained with AdamW, while eliminating theneed for high-speed interconnects when pre-training large scale foundationmodels. An open source reference PyTorch implementation is published on GitHubat https://github.com/bloc97/DeMo</description><author>Bowen Peng, Jeffrey Quesnelle, Diederik P. Kingma</author><pubDate>Fri, 29 Nov 2024 17:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19870v1</guid></item><item><title>AIDetx: a compression-based method for identification of machine-learning generated text</title><link>http://arxiv.org/abs/2411.19869v1</link><description>This paper introduces AIDetx, a novel method for detecting machine-generatedtext using data compression techniques. Traditional approaches, such as deeplearning classifiers, often suffer from high computational costs and limitedinterpretability. To address these limitations, we propose a compression-basedclassification framework that leverages finite-context models (FCMs). AIDetxconstructs distinct compression models for human-written and AI-generated text,classifying new inputs based on which model achieves a higher compressionratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scoresexceeding 97% and 99%, respectively, highlighting its high accuracy. Comparedto current methods, such as large language models (LLMs), AIDetx offers a moreinterpretable and computationally efficient solution, significantly reducingboth training time and hardware requirements (e.g., no GPUs needed). The fullimplementation is publicly available at https://github.com/AIDetx/AIDetx.</description><author>Leonardo Almeida, Pedro Rodrigues, Diogo Magalhães, Armando J. Pinho, Diogo Pratas</author><pubDate>Fri, 29 Nov 2024 17:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19869v1</guid></item><item><title>Reverse Thinking Makes LLMs Stronger Reasoners</title><link>http://arxiv.org/abs/2411.19865v1</link><description>Reverse thinking plays a crucial role in human reasoning. Humans can reasonnot only from a problem to a solution but also in reverse, i.e., start from thesolution and reason towards the problem. This often enhances overall reasoningperformance as it enables consistency checks between their forward and backwardthinking. To enable Large Language Models (LLMs) to perform reverse thinking,we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of dataaugmentation and learning objectives. In RevThink, we augment the dataset bycollecting structured forward-backward reasoning from a teacher model,consisting of: (1) the original question, (2) forward reasoning, (3) backwardquestion, and (4) backward reasoning. We then employ three objectives to traina smaller student model in a multi-task learning fashion: (a) generate forwardreasoning from a question, (b) generate a backward question from a question,and (c) generate backward reasoning from the backward question. Experimentsacross 12 datasets covering commonsense, math, and logical reasoning show anaverage 13.53% improvement over the student model's zero-shot performance and a6.84% improvement over the strongest knowledge distillation baselines.Moreover, our method demonstrates sample efficiency -- using only 10% of thecorrect forward reasoning from the training data, it outperforms a standardfine-tuning method trained on 10x more forward reasoning. RevThink alsoexhibits strong generalization to out-of-distribution held-out datasets.</description><author>Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, Tomas Pfister</author><pubDate>Fri, 29 Nov 2024 17:27:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19865v1</guid></item><item><title>Multi-label Sequential Sentence Classification via Large Language Model</title><link>http://arxiv.org/abs/2411.15623v2</link><description>Sequential sentence classification (SSC) in scientific publications iscrucial for supporting downstream tasks such as fine-grained informationretrieval and extractive summarization. However, current SSC methods areconstrained by model size, sequence length, and single-label setting. Toaddress these limitations, this paper proposes LLM-SSC, a large language model(LLM)-based framework for both single- and multi-label SSC tasks. Unlikeprevious approaches that employ small- or medium-sized language models, theproposed framework utilizes LLMs to generate SSC labels through designedprompts, which enhance task understanding by incorporating demonstrations and aquery to describe the prediction target. We also present a multi-labelcontrastive learning loss with auto-weighting scheme, enabling the multi-labelclassification task. To support our multi-label SSC analysis, we introduce andrelease a new dataset, biorc800, which mainly contains unstructured abstractsin the biomedical domain with manual annotations. Experiments demonstrateLLM-SSC's strong performance in SSC under both in-context learning andtask-specific tuning settings. We release biorc800 and our code at:https://github.com/ScienceNLP-Lab/LLM-SSC.</description><author>Mengfei Lan, Lecheng Zheng, Shufan Ming, Halil Kilicoglu</author><pubDate>Fri, 29 Nov 2024 17:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15623v2</guid></item><item><title>SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2411.19860v1</link><description>In this work, we present SpaRC, a novel Sparse fusion transformer for 3Dperception that integrates multi-view image semantics with Radar and Camerapoint features. The fusion of radar and camera modalities has emerged as anefficient perception paradigm for autonomous driving systems. Whileconventional approaches utilize dense Bird's Eye View (BEV)-based architecturesfor depth estimation, contemporary query-based transformers excel incamera-only detection through object-centric methodology. However, thesequery-based approaches exhibit limitations in false positive detections andlocalization precision due to implicit depth modeling. We address thesechallenges through three key contributions: (1) sparse frustum fusion (SFF) forcross-modal feature alignment, (2) range-adaptive radar aggregation (RAR) forprecise object localization, and (3) local self-attention (LSA) for focusedquery aggregation. In contrast to existing methods requiring computationallyintensive BEV-grid rendering, SpaRC operates directly on encoded pointfeatures, yielding substantial improvements in efficiency and accuracy.Empirical evaluations on the nuScenes and TruckScenes benchmarks demonstratethat SpaRC significantly outperforms existing dense BEV-based and sparsequery-based detectors. Our method achieves state-of-the-art performance metricsof 67.1 NDS and 63.1 AMOTA. The code and pretrained models are available athttps://github.com/phi-wol/sparc.</description><author>Philipp Wolters, Johannes Gilg, Torben Teepe, Fabian Herzog, Felix Fent, Gerhard Rigoll</author><pubDate>Fri, 29 Nov 2024 17:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19860v1</guid></item><item><title>Bias-inducing geometries: an exactly solvable data model with fairness implications</title><link>http://arxiv.org/abs/2205.15935v4</link><description>Machine learning (ML) may be oblivious to human bias but it is not immune toits perpetuation. Marginalisation and iniquitous group representation are oftentraceable in the very data used for training, and may be reflected or evenenhanced by the learning models. In the present work, we aim at clarifying therole played by data geometry in the emergence of ML bias. We introduce anexactly solvable high-dimensional model of data imbalance, where parametriccontrol over the many bias-inducing factors allows for an extensive explorationof the bias inheritance mechanism. Through the tools of statistical physics, weanalytically characterise the typical properties of learning models trained inthis synthetic framework and obtain exact predictions for the observables thatare commonly employed for fairness assessment. Despite the simplicity of thedata model, we retrace and unpack typical unfairness behaviour observed onreal-world datasets. We also obtain a detailed analytical characterisation of aclass of bias mitigation strategies. We first consider a basic loss-reweighingscheme, which allows for an implicit minimisation of different unfairnessmetrics, and quantify the incompatibilities between some existing fairnesscriteria. Then, we consider a novel mitigation strategy based on a matchedinference approach, consisting in the introduction of coupled learning models.Our theoretical analysis of this approach shows that the coupled strategy canstrike superior fairness-accuracy trade-offs.</description><author>Stefano Sarao Mannelli, Federica Gerace, Negar Rostamzadeh, Luca Saglietti</author><pubDate>Fri, 29 Nov 2024 17:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.15935v4</guid></item><item><title>What fifty-one years of Linguistics and Artificial Intelligence research tell us about their correlation: A scientometric review</title><link>http://arxiv.org/abs/2411.19858v1</link><description>There is a strong correlation between linguistics and artificial intelligence(AI), best manifested by deep learning language models. This study provides athorough scientometric analysis of this correlation, synthesizing theintellectual production during 51 years, from 1974 to 2024. It involves 5750Web of Science-indexed articles published in 2124 journals, which are writtenby 20835 authors belonging to 13773 research centers in 794 countries. Twopowerful software, viz., CiteSpace and VOSviewer, were used to generate mappingvisualizations of the intellectual landscape, trending issues and (re)emerginghotspots. The results indicate that in the 1980s and 1990s, linguistics and AIresearch was not robust, characterized by unstable publication over time. Ithas, however, witnessed a remarkable increase of publication since then,reaching 1478 articles in 2023, and 546 articles in January-March timespan in2024, involving emerging issues and hotspots, addressing new horizons, newtopics, and launching new applications and powerful deep learning languagemodels including ChatGPT.</description><author>Mohammed Q. Shormani</author><pubDate>Fri, 29 Nov 2024 17:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19858v1</guid></item><item><title>Artificial intelligence contribution to translation industry: looking back and forward</title><link>http://arxiv.org/abs/2411.19855v1</link><description>This study provides a comprehensive analysis of artificial intelligence (AI)contribution to translation industry (ACTI) research, synthesizing it overforty-one years from 1980-2024. 13220 articles were retrieved from threesources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz.,scientometric and thematic, focusing on cluster, subject categories, keywords,burstness, centrality and research centers as for the former. For the latter,we thematically review 18 articles, selected purposefully from the articlesinvolved, centering on purpose, approach, findings, and contribution to ACTIfuture directions. The findings reveal that in the past AI contribution totranslation industry was not rigorous, resulting in rule-based machinetranslation and statistical machine translation whose output was notsatisfactory. However, the more AI develops, the more machine translationdevelops, incorporating Neural Networking Algorithms and (Deep) LanguageLearning Models like ChatGPT whose translation output has developedconsiderably. However, much rigorous research is still needed to overcomeseveral problems encountering translation industry, specifically concerninglow-source languages, multi-dialectical and free word order languages, andcultural and religious registers.</description><author>Mohammed Q. Shormani</author><pubDate>Fri, 29 Nov 2024 17:10:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19855v1</guid></item><item><title>Towards Class-wise Robustness Analysis</title><link>http://arxiv.org/abs/2411.19853v1</link><description>While being very successful in solving many downstream tasks, the applicationof deep neural networks is limited in real-life scenarios because of theirsusceptibility to domain shifts such as common corruptions, and adversarialattacks. The existence of adversarial examples and data corruptionsignificantly reduces the performance of deep classification models.Researchers have made strides in developing robust neural architectures tobolster decisions of deep classifiers. However, most of these works rely oneffective adversarial training methods, and predominantly focus on overallmodel robustness, disregarding class-wise differences in robustness, which arecritical. Exploiting weakly robust classes is a potential avenue for attackersto fool the image recognition models. Therefore, this study investigatesclass-to-class biases across adversarially trained robust classification modelsto understand their latent space structures and analyze their strong and weakclass-wise properties. We further assess the robustness of classes againstcommon corruptions and adversarial attacks, recognizing that classvulnerability extends beyond the number of correct classifications for aspecific class. We find that the number of false positives of classes asspecific target classes significantly impacts their vulnerability to attacks.Through our analysis on the Class False Positive Score, we assess a fairevaluation of how susceptible each class is to misclassification.</description><author>Tejaswini Medi, Julia Grabinski, Margret Keuper</author><pubDate>Fri, 29 Nov 2024 17:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19853v1</guid></item><item><title>A Visual-inertial Localization Algorithm using Opportunistic Visual Beacons and Dead-Reckoning for GNSS-Denied Large-scale Applications</title><link>http://arxiv.org/abs/2411.19845v1</link><description>With the development of smart cities, the demand for continuous pedestriannavigation in large-scale urban environments has significantly increased. Whileglobal navigation satellite systems (GNSS) provide low-cost and reliablepositioning services, they are often hindered in complex urban canyonenvironments. Thus, exploring opportunistic signals for positioning in urbanareas has become a key solution. Augmented reality (AR) allows pedestrians toacquire real-time visual information. Accordingly, we propose a low-costvisual-inertial positioning solution. This method comprises a lightweightmulti-scale group convolution (MSGC)-based visual place recognition (VPR)neural network, a pedestrian dead reckoning (PDR) algorithm, and avisual/inertial fusion approach based on a Kalman filter with gross errorsuppression. The VPR serves as a conditional observation to the Kalman filter,effectively correcting the errors accumulated through the PDR method. Thisenables the entire algorithm to ensure the reliability of long-term positioningin GNSS-denied areas. Extensive experimental results demonstrate that ourmethod maintains stable positioning during large-scale movements. Compared tothe lightweight MobileNetV3-based VPR method, our proposed VPR solutionimproves Recall@1 by at least 3\% on two public datasets while reducing thenumber of parameters by 63.37\%. It also achieves performance that iscomparable to the VGG16-based method. The VPR-PDR algorithm improveslocalization accuracy by more than 40\% compared to the original PDR.</description><author>Liqiang Zhang Ye Tian Dongyan Wei</author><pubDate>Fri, 29 Nov 2024 17:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19845v1</guid></item><item><title>A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics</title><link>http://arxiv.org/abs/2410.18868v2</link><description>By incorporating physical consistency as inductive bias, deep neural networksdisplay increased generalization capabilities and data efficiency in learningnonlinear dynamic models. However, the complexity of these models generallyincreases with the system dimensionality, requiring larger datasets, morecomplex deep networks, and significant computational effort. We propose a novelgeometric network architecture to learn physically-consistent reduced-orderdynamic parameters that accurately describe the original high-dimensionalsystem behavior. This is achieved by building on recent advances in model-orderreduction and by adopting a Riemannian perspective to jointly learn anon-linear structure-preserving latent space and the associated low-dimensionaldynamics. Our approach enables accurate long-term predictions of thehigh-dimensional dynamics of rigid and deformable systems with increased dataefficiency by inferring interpretable and physically plausible reducedLagrangian models.</description><author>Katharina Friedl, Noémie Jaquier, Jens Lundell, Tamim Asfour, Danica Kragic</author><pubDate>Fri, 29 Nov 2024 17:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18868v2</guid></item><item><title>Scaling Transformers for Low-Bitrate High-Quality Speech Coding</title><link>http://arxiv.org/abs/2411.19842v1</link><description>The tokenization of speech with neural audio codec models is a vital part ofmodern AI pipelines for the generation or understanding of speech, alone or ina multimodal context. Traditionally such tokenization models have concentratedon low parameter-count architectures using only components with stronginductive biases. In this work we show that by scaling a transformerarchitecture with large parameter count to this problem, and applying aflexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible toreach state-of-the-art speech quality at extremely low bit-rates of $400$ or$700$ bits-per-second. The trained models strongly out-perform existingbaselines in both objective and subjective tests.</description><author>Julian D Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, Xubo Liu</author><pubDate>Fri, 29 Nov 2024 16:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19842v1</guid></item><item><title>Parallel Stacked Aggregated Network for Voice Authentication in IoT-Enabled Smart Devices</title><link>http://arxiv.org/abs/2411.19841v1</link><description>Voice authentication on IoT-enabled smart devices has gained prominence inrecent years due to increasing concerns over user privacy and security. Thecurrent authentication systems are vulnerable to different voice-spoofingattacks (e.g., replay, voice cloning, and audio deepfakes) that mimiclegitimate voices to deceive authentication systems and enable fraudulentactivities (e.g., impersonation, unauthorized access, financial fraud, etc.).Existing solutions are often designed to tackle a single type of attack,leading to compromised performance against unseen attacks. On the other hand,existing unified voice anti-spoofing solutions, not designed specifically forIoT, possess complex architectures and thus cannot be deployed on IoT-enabledsmart devices. Additionally, most of these unified solutions exhibitsignificant performance issues, including higher equal error rates or loweraccuracy for specific attacks. To overcome these issues, we present theparallel stacked aggregation network (PSA-Net), a lightweight frameworkdesigned as an anti-spoofing defense system for voice-controlled smart IoTdevices. The PSA-Net processes raw audios directly and eliminates the need fordataset-dependent handcrafted features or pre-computed spectrograms.Furthermore, PSA-Net employs a split-transform-aggregate approach, whichinvolves the segmentation of utterances, the extraction of intrinsicdifferentiable embeddings through convolutions, and the aggregation of them todistinguish legitimate from spoofed audios. In contrast to existing deepResnet-oriented solutions, we incorporate cardinality as an additionaldimension in our network, which enhances the PSA-Net ability to generalizeacross diverse attacks. The results show that the PSA-Net achieves moreconsistent performance for different attacks that exist in currentanti-spoofing solutions.</description><author>Awais Khan, Ijaz Ul Haq, Khalid Mahmood Malik</author><pubDate>Fri, 29 Nov 2024 16:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19841v1</guid></item><item><title>Multiscale spatiotemporal heterogeneity analysis of bike-sharing system's self-loop phenomenon: Evidence from Shanghai</title><link>http://arxiv.org/abs/2411.17555v2</link><description>Bike-sharing is an environmentally friendly shared mobility mode, but itsself-loop phenomenon, where bikes are returned to the same station afterseveral time usage, significantly impacts equity in accessing its services.Therefore, this study conducts a multiscale analysis with a spatialautoregressive model and double machine learning framework to assesssocioeconomic features and geospatial location's impact on the self-loopphenomenon at metro stations and street scales. The results reveal thatbike-sharing self-loop intensity exhibits significant spatial lag effect atstreet scale and is positively associated with residential land use. Marginaltreatment effects of residential land use is higher on streets with middle-agedresidents, high fixed employment, and low car ownership. The multimodal publictransit condition reveals significant positive marginal treatment effects atboth scales. To enhance bike-sharing cooperation, we advocate augmentingbicycle availability in areas with high metro usage and low bus coverage,alongside implementing adaptable redistribution strategies.</description><author>Yichen Wang, Qing Yu, Yancun Song</author><pubDate>Fri, 29 Nov 2024 16:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17555v2</guid></item><item><title>Feedback-driven object detection and iterative model improvement</title><link>http://arxiv.org/abs/2411.19835v1</link><description>Automated object detection has become increasingly valuable across diverseapplications, yet efficient, high-quality annotation remains a persistentchallenge. In this paper, we present the development and evaluation of aplatform designed to interactively improve object detection models. Theplatform allows uploading and annotating images as well as fine-tuning objectdetection models. Users can then manually review and refine annotations,further creating improved snapshots that are used for automatic objectdetection on subsequent image uploads - a process we refer to as semi-automaticannotation resulting in a significant gain in annotation efficiency. Whereas iterative refinement of model results to speed up annotation hasbecome common practice, we are the first to quantitatively evaluate itsbenefits with respect to time, effort, and interaction savings. Ourexperimental results show clear evidence for a significant time reduction of upto 53% for semi-automatic compared to manual annotation. Importantly, theseefficiency gains did not compromise annotation quality, while matching oroccasionally even exceeding the accuracy of manual annotations. These findingsdemonstrate the potential of our lightweight annotation platform for creatinghigh-quality object detection datasets and provide best practices to guidefuture development of annotation platforms. The platform is open-source, with the frontend and backend repositoriesavailable on GitHub.</description><author>Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner</author><pubDate>Fri, 29 Nov 2024 16:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19835v1</guid></item><item><title>Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation</title><link>http://arxiv.org/abs/2411.19832v1</link><description>The detection of sensitive content in large datasets is crucial for ensuringthat shared and analysed data is free from harmful material. However, currentmoderation tools, such as external APIs, suffer from limitations incustomisation, accuracy across diverse sensitive categories, and privacyconcerns. Additionally, existing datasets and open-source models focuspredominantly on toxic language, leaving gaps in detecting other sensitivecategories such as substance abuse or self-harm. In this paper, we put forwarda unified dataset tailored for social media content moderation across sixsensitive categories: conflictual language, profanity, sexually explicitmaterial, drug-related content, self-harm, and spam. By collecting andannotating data with consistent retrieval strategies and guidelines, we addressthe shortcomings of previous focalised research. Our analysis demonstrates thatfine-tuning large language models (LLMs) on this novel dataset yieldssignificant improvements in detection performance compared to openoff-the-shelf models such as LLaMA, and even proprietary OpenAI models, whichunderperform by 10-15% overall. This limitation is even more pronounced onpopular moderation APIs, which cannot be easily tailored to specific sensitivecontent categories, among others.</description><author>Dimosthenis Antypas, Indira Sen, Carla Perez-Almendros, Jose Camacho-Collados, Francesco Barbieri</author><pubDate>Fri, 29 Nov 2024 16:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19832v1</guid></item><item><title>Imagined Speech and Visual Imagery as Intuitive Paradigms for Brain-Computer Interfaces</title><link>http://arxiv.org/abs/2411.09400v2</link><description>Brain-computer interfaces (BCIs) have shown promise in enabling communicationfor individuals with motor impairments. Recent advancements likebrain-to-speech technology aim to reconstruct speech from neural activity.However, decoding communication-related paradigms, such as imagined speech andvisual imagery, using non-invasive techniques remains challenging. This studyanalyzes brain dynamics in these two paradigms by examining neuralsynchronization and functional connectivity through phase-locking values (PLV)in EEG data from 16 participants. Results show that visual imagery produceshigher PLV values in visual cortex, engaging spatial networks, while imaginedspeech demonstrates consistent synchronization, primarily engaginglanguage-related regions. These findings suggest that imagined speech issuitable for language-driven BCI applications, while visual imagery cancomplement BCI systems for users with speech impairments. Personalizedcalibration is crucial for optimizing BCI performance.</description><author>Seo-Hyun Lee, Ji-Ha Park, Deok-Seon Kim</author><pubDate>Fri, 29 Nov 2024 16:34:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09400v2</guid></item><item><title>SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive Tokens</title><link>http://arxiv.org/abs/2411.19824v1</link><description>We propose a one-stage framework for real-time multi-person 3D human meshestimation from a single RGB image. While current one-stage methods, whichfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance withhigh-resolution inputs, we observe that this particularly benefits theestimation of individuals in smaller scales of the image (e.g., those far fromthe camera), but at the cost of significantly increased computation overhead.To address this, we introduce scale-adaptive tokens that are dynamicallyadjusted based on the relative scale of each individual in the image within theDETR framework. Specifically, individuals in smaller scales are processed athigher resolutions, larger ones at lower resolutions, and background regionsare further distilled. These scale-adaptive tokens more efficiently encode theimage features, facilitating subsequent decoding to regress the human mesh,while allowing the model to allocate computational resources more effectivelyand focus on more challenging cases. Experiments show that our method preservesthe accuracy benefits of high-resolution processing while substantiallyreducing computational cost, achieving real-time inference with performancecomparable to SOTA methods.</description><author>Chi Su, Xiaoxuan Ma, Jiajun Su, Yizhou Wang</author><pubDate>Fri, 29 Nov 2024 16:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19824v1</guid></item><item><title>ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust Machine Learning</title><link>http://arxiv.org/abs/2308.04964v3</link><description>Many Web Application Firewalls (WAFs) leverage the OWASP Core Rule Set (CRS)to block incoming malicious requests. The CRS consists of different sets ofrules designed by domain experts to detect well-known web attack patterns. Boththe set of rules to be used and the weights used to combine them are manuallydefined, yielding four different default configurations of the CRS. In thiswork, we focus on the detection of SQL injection (SQLi) attacks, and show thatthe manual configurations of the CRS typically yield a suboptimal trade-offbetween detection and false alarm rates. Furthermore, we show that theseconfigurations are not robust to adversarial SQLi attacks, i.e.,carefully-crafted attacks that iteratively refine the malicious SQLi payload byquerying the target WAF to bypass detection. To overcome these limitations, wepropose (i) using machine learning to automate the selection of the set ofrules to be combined along with their weights, i.e., customizing the CRSconfiguration based on the monitored web services; and (ii) leveragingadversarial training to significantly improve its robustness to adversarialSQLi manipulations. Our experiments, conducted using the well-known open-sourceModSecurity WAF equipped with the CRS rules, show that our approach, namedModSec-AdvLearn, can (i) increase the detection rate up to 30%, while retainingnegligible false alarm rates and discarding up to 50% of the CRS rules; and(ii) improve robustness against adversarial SQLi attacks up to 85%, marking asignificant stride toward designing more effective and robust WAFs. We releaseour open-source code at https://github.com/pralab/modsec-advlearn.</description><author>Biagio Montaruli, Giuseppe Floris, Christian Scano, Luca Demetrio, Andrea Valenza, Luca Compagna, Davide Ariu, Luca Piras, Davide Balzarotti, Battista Biggio</author><pubDate>Fri, 29 Nov 2024 16:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04964v3</guid></item><item><title>Hybrid Architecture for Real-Time Video Anomaly Detection: Integrating Spatial and Temporal Analysis</title><link>http://arxiv.org/abs/2410.15909v3</link><description>In this paper, we propose a new architecture for real-time anomaly detectionin video data, inspired by human behavior combining spatial and temporalanalyses. This approach uses two distinct models: (i) for temporal analysis, arecurrent convolutional network (CNN + RNN) is employed, associating VGG19 anda GRU to process video sequences; (ii) regarding spatial analysis, it isperformed using YOLOv7 to analyze individual images. These two analyses can becarried out either in parallel, with a final prediction that combines theresults of both analysis, or in series, where the spatial analysis enriches thedata before the temporal analysis. Some experimentations are been made tocompare these two architectural configurations with each other, and evaluatethe effectiveness of our hybrid approach in video anomaly detection.</description><author>Fabien Poirier</author><pubDate>Fri, 29 Nov 2024 16:32:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15909v3</guid></item><item><title>SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for Incomplete Multimodal Learning in Conversational Emotion Recognition</title><link>http://arxiv.org/abs/2411.19822v1</link><description>Multimodal Emotion Recognition in Conversations (MERC) aims to classifyutterance emotions using textual, auditory, and visual modal features. Mostexisting MERC methods assume each utterance has complete modalities,overlooking the common issue of incomplete modalities in real-world scenarios.Recently, graph neural networks (GNNs) have achieved notable results inIncomplete Multimodal Emotion Recognition in Conversations (IMERC). However,traditional GNNs focus on binary relationships between nodes, limiting theirability to capture more complex, higher-order information. Moreover, repeatedmessage passing can cause over-smoothing, reducing their capacity to preserveessential high-frequency details. To address these issues, we propose aSpectral Domain Reconstruction Graph Neural Network (SDR-GNN) for incompletemultimodal learning in conversational emotion recognition. SDR-GNN constructsan utterance semantic interaction graph using a sliding window based on bothspeaker and context relationships to model emotional dependencies. To capturehigher-order and high-frequency information, SDR-GNN utilizes weightedrelationship aggregation, ensuring consistent semantic feature extractionacross utterances. Additionally, it performs multi-frequency aggregation in thespectral domain, enabling efficient recovery of incomplete modalities byextracting both high- and low-frequency information. Finally, multi-headattention is applied to fuse and optimize features for emotion recognition.Extensive experiments on various real-world datasets demonstrate that ourapproach is effective in incomplete multimodal learning and outperforms currentstate-of-the-art methods.</description><author>Fangze Fu, Wei Ai, Fan Yang, Yuntao Shou, Tao Meng, Keqin Li</author><pubDate>Fri, 29 Nov 2024 16:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19822v1</guid></item><item><title>Integrated Artificial Neurons from Metal Halide Perovskites</title><link>http://arxiv.org/abs/2411.19820v1</link><description>Hardware neural networks could perform certain computational tasks orders ofmagnitude more energy-efficiently than conventional computers. Artificialneurons are a key component of these networks and are currently implementedwith electronic circuits based on capacitors and transistors. However,artificial neurons based on memristive devices are a promising alternative,owing to their potentially smaller size and inherent stochasticity. But despitetheir promise, demonstrations of memristive artificial neurons have so far beenlimited. Here we demonstrate a fully on-chip artificial neuron based onmicroscale electrodes and halide perovskite semiconductors as the active layer.By connecting a halide perovskite memristive device in series with a capacitor,the device demonstrates stochastic leaky integrate-and-fire behavior, with anenergy consumption of 20 to 60 pJ per spike, lower than that of a biologicalneuron. We simulate populations of our neuron and show that the stochasticfiring allows the detection of sub-threshold inputs. The neuron can easily beintegrated with previously-demonstrated halide perovskite artificial synapsesin energy-efficient neural networks.</description><author>Jeroen J. de Boer, Bruno Ehrler</author><pubDate>Fri, 29 Nov 2024 16:30:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19820v1</guid></item><item><title>GradAlign for Training-free Model Performance Inference</title><link>http://arxiv.org/abs/2411.19819v1</link><description>Architecture plays an important role in deciding the performance of deepneural networks. However, the search for the optimal architecture is oftenhindered by the vast search space, making it a time-intensive process.Recently, a novel approach known as training-free neural architecture search(NAS) has emerged, aiming to discover the ideal architecture withoutnecessitating extensive training. Training-free NAS leverages variousindicators for architecture selection, including metrics such as the count oflinear regions, the density of per-sample losses, and the stability of thefinite-width Neural Tangent Kernel (NTK) matrix. Despite the competitiveempirical performance of current training-free NAS techniques, they suffer fromcertain limitations, including inconsistent performance and a lack of deepunderstanding. In this paper, we introduce GradAlign, a simple yet effectivemethod designed for inferring model performance without the need for training.At its core, GradAlign quantifies the extent of conflicts within per-samplegradients during initialization, as substantial conflicts hinder modelconvergence and ultimately result in worse performance. We evaluate GradAlignagainst established training-free NAS methods using standard NAS benchmarks,showing a better overall performance. Moreover, we show that the widely adoptedmetric of linear region count may not suffice as a dependable criterion forselecting network architectures during at initialization.</description><author>Yuxuan Li, Yunhui Guo</author><pubDate>Fri, 29 Nov 2024 16:27:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19819v1</guid></item><item><title>Robust Stochastically-Descending Unrolled Networks</title><link>http://arxiv.org/abs/2312.15788v2</link><description>Deep unrolling, or unfolding, is an emerging learning-to-optimize method thatunrolls a truncated iterative algorithm in the layers of a trainable neuralnetwork. However, the convergence guarantees and generalizability of theunrolled networks are still open theoretical problems. To tackle theseproblems, we provide deep unrolled architectures with a stochastic descentnature by imposing descending constraints during training. The descendingconstraints are forced layer by layer to ensure that each unrolled layer takes,on average, a descent step toward the optimum during training. We theoreticallyprove that the sequence constructed by the outputs of the unrolled layers isthen guaranteed to converge for unseen problems, assuming no distribution shiftbetween training and test problems. We also show that standard unrolling isbrittle to perturbations, and our imposed constraints provide the unrollednetworks with robustness to additive noise and perturbations. We numericallyassess unrolled architectures trained under the proposed constraints in twodifferent applications, including the sparse coding using learnable iterativeshrinkage and thresholding algorithm (LISTA) and image inpainting usingproximal generative flow (GLOW-Prox), and demonstrate the performance androbustness benefits of the proposed method.</description><author>Samar Hadou, Navid NaderiAlizadeh, Alejandro Ribeiro</author><pubDate>Fri, 29 Nov 2024 16:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15788v2</guid></item><item><title>Gaussian multi-target filtering with target dynamics driven by a stochastic differential equation</title><link>http://arxiv.org/abs/2411.19814v1</link><description>This paper proposes multi-target filtering algorithms in which targetdynamics are given in continuous time and measurements are obtained at discretetime instants. In particular, targets appear according to a Poisson pointprocess (PPP) in time with a given Gaussian spatial distribution, targets moveaccording to a general time-invariant linear stochastic differential equation,and the life span of each target is modelled with an exponential distribution.For this multi-target dynamic model, we derive the distribution of the set ofnew born targets and calculate closed-form expressions for the best fittingmean and covariance of each target at its time of birth by minimising theKullback-Leibler divergence via moment matching. This yields a novel Gaussiancontinuous-discrete Poisson multi-Bernoulli mixture (PMBM) filter, and itsapproximations based on Poisson multi-Bernoulli and probability hypothesisdensity filtering. These continuous-discrete multi-target filters are alsoextended to target dynamics driven by nonlinear stochastic differentialequations.</description><author>Ángel F. García-Fernández, Simo Särkkä</author><pubDate>Fri, 29 Nov 2024 16:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19814v1</guid></item><item><title>Recent Advances of Foundation Language Models-based Continual Learning: A Survey</title><link>http://arxiv.org/abs/2405.18653v2</link><description>Recently, foundation language models (LMs) have marked significantachievements in the domains of natural language processing (NLP) and computervision (CV). Unlike traditional neural network models, foundation LMs obtain agreat ability for transfer learning by acquiring rich commonsense knowledgethrough pre-training on extensive unsupervised datasets with a vast number ofparameters. However, they still can not emulate human-like continuous learningdue to catastrophic forgetting. Consequently, various continual learning(CL)-based methodologies have been developed to refine LMs, enabling them toadapt to new tasks without forgetting previous knowledge. However, a systematictaxonomy of existing approaches and a comparison of their performance are stilllacking, which is the gap that our survey aims to fill. We delve into acomprehensive review, summarization, and classification of the existingliterature on CL-based approaches applied to foundation language models, suchas pre-trained language models (PLMs), large language models (LLMs) andvision-language models (VLMs). We divide these studies into offline CL andonline CL, which consist of traditional methods, parameter-efficient-basedmethods, instruction tuning-based methods and continual pre-training methods.Offline CL encompasses domain-incremental learning, task-incremental learning,and class-incremental learning, while online CL is subdivided into hard taskboundary and blurry task boundary settings. Additionally, we outline thetypical datasets and metrics employed in CL research and provide a detailedanalysis of the challenges and future work for LMs-based continual learning.</description><author>Yutao Yang, Jie Zhou, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Yuan Xie, Liang He</author><pubDate>Fri, 29 Nov 2024 16:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18653v2</guid></item><item><title>Think Beyond Size: Adaptive Prompting for More Effective Reasoning</title><link>http://arxiv.org/abs/2410.08130v2</link><description>Pretrained large language models (LLMs) are increasingly utilized across awide range of natural language processing (NLP) tasks due to their impressivecapabilities as few-shot learners. Recent techniques, such as chain-of-thought(CoT) prompting, have significantly advanced multi-step reasoning byintroducing step-by-step decomposition, achieving state-of-the-art results oncomplex reasoning benchmarks. However, these approaches often rely on staticprompting templates that do not adapt to task complexity or errors during thereasoning process. In this work, we introduce Adaptive Prompting, a dynamic anditerative framework designed to enhance reasoning by incorporating real-timeadjustments to prompt structures and validation mechanisms.Experimental resultsdemonstrate that Adaptive Prompting significantly improves performance ondiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,MultiArith), logical reasoning and commonsense tasks, achieving substantialaccuracy gains compared to static prompting baselines. By integrating guidedprompts, intermediate validation, and self-corrective steps, our approachenables smaller models to achieve competitive performance with largercounterparts, such as GPT-4, while maintaining computational efficiency. Theframework achieves this without requiring fine-tuning or task-specific trainingdata, highlighting the untapped potential of iterative reasoning methods.</description><author>Kamesh R</author><pubDate>Fri, 29 Nov 2024 16:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08130v2</guid></item><item><title>Q-learning-based Model-free Safety Filter</title><link>http://arxiv.org/abs/2411.19809v1</link><description>Ensuring safety via safety filters in real-world robotics presentssignificant challenges, particularly when the system dynamics is complex orunavailable. To handle this issue, learning-based safety filters recentlygained popularity, which can be classified as model-based and model-freemethods. Existing model-based approaches requires various assumptions on systemmodel (e.g., control-affine), which limits their application in complexsystems, and existing model-free approaches need substantial modifications tostandard RL algorithms and lack versatility. This paper proposes a simple,plugin-and-play, and effective model-free safety filter learning framework. Weintroduce a novel reward formulation and use Q-learning to learn Q-valuefunctions to safeguard arbitrary task specific nominal policies via filteringout their potentially unsafe actions. The threshold used in the filteringprocess is supported by our theoretical analysis. Due to its model-free natureand simplicity, our framework can be seamlessly integrated with various RLalgorithms. We validate the proposed approach through simulations on doubleintegrator and Dubin's car systems and demonstrate its effectiveness inreal-world experiments with a soft robotic limb.</description><author>Guo Ning Sue, Yogita Choudhary, Richard Desatnik, Carmel Majidi, John Dolan, Guanya Shi</author><pubDate>Fri, 29 Nov 2024 16:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19809v1</guid></item><item><title>Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures</title><link>http://arxiv.org/abs/2411.19806v1</link><description>In this paper, we tackle the task of musical stem retrieval. Given a musicalmix, it consists in retrieving a stem that would fit with it, i.e., that wouldsound pleasant if played together. To do so, we introduce a new method based onJoint-Embedding Predictive Architectures, where an encoder and a predictor arejointly trained to produce latent representations of a context and predictlatent representations of a target. In particular, we design our predictor tobe conditioned on arbitrary instruments, enabling our model to performzero-shot stem retrieval. In addition, we discover that pretraining the encoderusing contrastive learning drastically improves the model's performance. We validate the retrieval performances of our model using the MUSDB18 andMoisesDB datasets. We show that it significantly outperforms previous baselineson both datasets, showcasing its ability to support more or less precise (andpossibly unseen) conditioning. We also evaluate the learned embeddings on abeat tracking task, demonstrating that they retain temporal structure and localinformation.</description><author>Alain Riou, Antonin Gagneré, Gaëtan Hadjeres, Stefan Lattner, Geoffroy Peeters</author><pubDate>Fri, 29 Nov 2024 16:11:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19806v1</guid></item><item><title>Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2411.19804v1</link><description>Integrating multiple (sub-)systems is essential to create advancedInformation Systems (ISs). Difficulties mainly arise when integrating dynamicenvironments across the IS lifecycle. A traditional approach is a registry thatprovides the API documentation of the systems' endpoints. Large Language Models(LLMs) have shown to be capable of automatically creating system integrations(e.g., as service composition) based on this documentation but require conciseinput due to input token limitations, especially regarding comprehensive APIdescriptions. Currently, it is unknown how best to preprocess these APIdescriptions. Within this work, we (i) analyze the usage of Retrieval AugmentedGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,of OpenAPIs to reduce the input token length while preserving the most relevantinformation. To further reduce the input token length for the compositionprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent thatonly receives a summary of the most relevant endpoints and retrieves details ondemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,first, for the different chunking possibilities and parameters measuring theendpoint retrieval recall, precision, and F1 score. Then, we assess theDiscovery Agent using the same test set. With our prototype, we demonstrate howto successfully employ RAG for endpoint discovery to reduce the token count.While revealing high values for recall, precision, and F1, further research isnecessary to retrieve all requisite endpoints. Our experiments show that forpreprocessing, LLM-based and format-specific approaches outperform na\"ivechunking methods. Relying on an agent further enhances these results as theagent splits the tasks into multiple fine granular subtasks, improving theoverall RAG performance in the token count, precision, and F1 score.</description><author>Robin D. Pesl, Jerin G. Mathew, Massimo Mecella, Marco Aiello</author><pubDate>Fri, 29 Nov 2024 16:09:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19804v1</guid></item><item><title>Efficient Text-driven Motion Generation via Latent Consistency Training</title><link>http://arxiv.org/abs/2405.02791v3</link><description>Text-driven human motion generation based on diffusion strategies establishesa reliable foundation for multimodal applications in human-computerinteractions. However, existing advances face significant efficiency challengesdue to the substantial computational overhead of iteratively solving fornonlinear reverse diffusion trajectories during the inference phase. To thisend, we propose the motion latent consistency training framework (MLCT), whichprecomputes reverse diffusion trajectories from raw data in the training phaseand enables few-step or single-step inference via self-consistency constraintsin the inference phase. Specifically, a motion autoencoder with quantizationconstraints is first proposed for constructing concise and bounded solutiondistributions for motion diffusion processes. Subsequently, a classifier-freeguidance format is constructed via an additional unconditional loss function toaccomplish the precomputation of conditional diffusion trajectories in thetraining phase. Finally, a clustering guidance module based on theK-nearest-neighbor algorithm is developed for the chain-conduction optimizationmechanism of self-consistency constraints, which provides additional referencesof solution distributions at a small query cost. By combining theseenhancements, we achieve stable and consistency training in non-pixel modalityand latent representation spaces. Benchmark experiments demonstrate that ourmethod significantly outperforms traditional consistency distillation methodswith reduced training cost and enhances the consistency model to performcomparably to state-of-the-art models with lower inference costs.</description><author>Mengxian Hu, Minghao Zhu, Xun Zhou, Qingqing Yan, Shu Li, Chengju Liu, Qijun Chen</author><pubDate>Fri, 29 Nov 2024 16:03:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02791v3</guid></item><item><title>INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</title><link>http://arxiv.org/abs/2411.19799v1</link><description>The performance differential of large language models (LLM) between languageshinders their effective deployment in many regions, inhibiting the potentialeconomic and societal value of generative AI tools in many communities.However, the development of functional LLMs in many languages (\ie,multilingual LLMs) is bottlenecked by the lack of high-quality evaluationresources in languages other than English. Moreover, current practices inmultilingual benchmark construction often translate English resources, ignoringthe regional and cultural knowledge of the environments in which multilingualsystems would be used. In this work, we construct an evaluation suite of197,243 QA pairs from local exam sources to measure the capabilities ofmultilingual LLMs in a variety of regional contexts. Our novel resource,INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across44 written languages that evaluates multilingual LLMs for performance in theactual language environments where they would be deployed.</description><author>Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sh</author><pubDate>Fri, 29 Nov 2024 16:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19799v1</guid></item><item><title>Statistical learning theory and Occam's razor: The core argument</title><link>http://arxiv.org/abs/2312.13842v2</link><description>Statistical learning theory is often associated with the principle of Occam'srazor, which recommends a simplicity preference in inductive inference. Thispaper distills the core argument for simplicity obtainable from statisticallearning theory, built on the theory's central learning guarantee for themethod of empirical risk minimization. This core "means-ends" argument is thata simpler hypothesis class or inductive model is better because it has betterlearning guarantees; however, these guarantees are model-relative and so thetheoretical push towards simplicity is checked by our prior knowledge.</description><author>Tom F. Sterkenburg</author><pubDate>Fri, 29 Nov 2024 16:02:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13842v2</guid></item><item><title>What Is Fairness? On the Role of Protected Attributes and Fictitious Worlds</title><link>http://arxiv.org/abs/2205.09622v6</link><description>A growing body of literature in fairness-aware machine learning (fairML) aimsto mitigate machine learning (ML)-related unfairness in automateddecision-making (ADM) by defining metrics that measure fairness of an ML modeland by proposing methods to ensure that trained ML models achieve low scores onthese metrics. However, the underlying concept of fairness, i.e., the questionof what fairness is, is rarely discussed, leaving a significant gap betweencenturies of philosophical discussion and the recent adoption of the concept inthe ML community. In this work, we try to bridge this gap by formalizing aconsistent concept of fairness and by translating the philosophicalconsiderations into a formal framework for the training and evaluation of MLmodels in ADM systems. We argue that fairness problems can arise even withoutthe presence of protected attributes (PAs), and point out that fairness andpredictive performance are not irreconcilable opposites, but that the latter isnecessary to achieve the former. Furthermore, we argue why and how causalconsiderations are necessary when assessing fairness in the presence of PAs byproposing a fictitious, normatively desired (FiND) world in which PAs have nocausal effects. In practice, this FiND world must be approximated by a warpedworld in which the causal effects of the PAs are removed from the real-worlddata. Finally, we achieve greater linguistic clarity in the discussion offairML. We outline algorithms for practical applications and presentillustrative experiments on COMPAS data.</description><author>Ludwig Bothmann, Kristina Peters, Bernd Bischl</author><pubDate>Fri, 29 Nov 2024 16:01:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.09622v6</guid></item><item><title>Rethinking the initialization of Momentum in Federated Learning with Heterogeneous Data</title><link>http://arxiv.org/abs/2411.19798v1</link><description>Data Heterogeneity is a major challenge of Federated Learning performance.Recently, momentum based optimization techniques have beed proved to beeffective in mitigating the heterogeneity issue. Along with the model updates,the momentum updates are transmitted to the server side and aggregated.Therefore, the local training initialized with a global momentum is guided bythe global history of the gradients. However, we spot a problem in thetraditional cumulation of the momentum which is suboptimal in the FederatedLearning systems. The momentum used to weight less on the historical gradientsand more on the recent gradients. This however, will engage more biased localgradients in the end of the local training. In this work, we propose a new wayto calculate the estimated momentum used in local initialization. The proposedmethod is named as Reversed Momentum Federated Learning (RMFL). The key idea isto assign exponentially decayed weights to the gradients with the time goingforward, which is on the contrary to the traditional momentum cumulation. Theeffectiveness of RMFL is evaluated on three popular benchmark datasets withdifferent heterogeneity levels.</description><author>Chenguang Xiao, Shuo Wang</author><pubDate>Fri, 29 Nov 2024 16:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19798v1</guid></item><item><title>Aggregating Nearest Sharp Features via Hybrid Transformers for Video Deblurring</title><link>http://arxiv.org/abs/2309.07054v2</link><description>Video deblurring methods, aiming at recovering consecutive sharp frames froma given blurry video, usually assume that the input video suffers fromconsecutively blurry frames. However, in real-world scenarios captured bymodern imaging devices, sharp frames often interspersed within the video,providing temporally nearest sharp features that can aid in the restoration ofblurry frames. In this work, we propose a video deblurring method thatleverages both neighboring frames and existing sharp frames using hybridTransformers for feature aggregation. Specifically, we first train a blur-awaredetector to distinguish between sharp and blurry frames. Then, a window-basedlocal Transformer is employed for exploiting features from neighboring frames,where cross attention is beneficial for aggregating features from neighboringframes without explicit spatial alignment. To aggregate nearest sharp featuresfrom detected sharp frames, we utilize a global Transformer with multi-scalematching capability. Moreover, our method can easily be extended toevent-driven video deblurring by incorporating an event fusion module into theglobal Transformer. Extensive experiments on benchmark datasets demonstratethat our proposed method outperforms state-of-the-art video deblurring methodsas well as event-driven video deblurring methods in terms of quantitativemetrics and visual quality. The source code and trained models are available athttps://github.com/shangwei5/STGTN.</description><author>Wei Shang, Dongwei Ren, Yi Yang, Wangmeng Zuo</author><pubDate>Fri, 29 Nov 2024 15:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07054v2</guid></item><item><title>Tractable Agreement Protocols</title><link>http://arxiv.org/abs/2411.19791v1</link><description>We present an efficient reduction that converts any machine learningalgorithm into an interactive protocol, enabling collaboration with anotherparty (e.g., a human) to achieve consensus on predictions and improve accuracy.This approach imposes calibration conditions on each party, which arecomputationally and statistically tractable relaxations of Bayesianrationality. These conditions are sensible even in prior-free settings,representing a significant generalization of Aumann's classic "agreementtheorem." In our protocol, the model first provides a prediction. The human thenresponds by either agreeing or offering feedback. The model updates its stateand revises its prediction, while the human may adjust their beliefs. Thisiterative process continues until the two parties reach agreement. Initially,we study a setting that extends Aumann's Agreement Theorem, where parties aimto agree on a one-dimensional expectation by iteratively sharing their currentestimates. Here, we recover the convergence theorem of Aaronson'05 under weakerassumptions. We then address the case where parties hold beliefs overdistributions with d outcomes, exploring two feedback mechanisms. The firstinvolves vector-valued estimates of predictions, while the second adopts adecision-theoretic approach: the human, needing to take an action from a finiteset based on utility, communicates their utility-maximizing action at eachround. In this setup, the number of rounds until agreement remains independentof d. Finally, we generalize to scenarios with more than two parties, wherecomputational complexity scales linearly with the number of participants. Ourprotocols rely on simple, efficient conditions and produce predictions thatsurpass the accuracy of any individual party's alone.</description><author>Natalie Collina, Surbhi Goel, Varun Gupta, Aaron Roth</author><pubDate>Fri, 29 Nov 2024 15:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19791v1</guid></item><item><title>A Survey on Multimodal Large Language Models</title><link>http://arxiv.org/abs/2306.13549v4</link><description>Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V hasbeen a new rising research hotspot, which uses powerful Large Language Models(LLMs) as a brain to perform multimodal tasks. The surprising emergentcapabilities of MLLM, such as writing stories based on images and OCR-free mathreasoning, are rare in traditional multimodal methods, suggesting a potentialpath to artificial general intelligence. To this end, both academia andindustry have endeavored to develop MLLMs that can compete with or even betterthan GPT-4V, pushing the limit of research at a surprising speed. In thispaper, we aim to trace and summarize the recent progress of MLLMs. First ofall, we present the basic formulation of MLLM and delineate its relatedconcepts, including architecture, training strategy and data, as well asevaluation. Then, we introduce research topics about how MLLMs can be extendedto support more granularity, modalities, languages, and scenarios. We continuewith multimodal hallucination and extended techniques, including Multimodal ICL(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Toconclude the paper, we discuss existing challenges and point out promisingresearch directions. In light of the fact that the era of MLLM has only justbegun, we will keep updating this survey and hope it can inspire more research.An associated GitHub link collecting the latest papers is available athttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.</description><author>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen</author><pubDate>Fri, 29 Nov 2024 15:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13549v4</guid></item><item><title>Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models</title><link>http://arxiv.org/abs/2404.02837v2</link><description>This paper reveals the phenomenon of parameter heterogeneity in largelanguage models (LLMs). We find that a small subset of "cherry" parametersexhibit a disproportionately large influence on model performance, while thevast majority of parameters have minimal impact. This heterogeneity is found tobe prevalent across different model families, scales, and types. Motivated bythis observation, we propose CherryQ, a novel quantization method that unifiesthe optimization of mixed-precision parameters. CherryQ identifies andpreserves the critical cherry parameters in high precision while aggressivelyquantizing the remaining parameters to low precision. Extensive experimentsdemonstrate the effectiveness of CherryQ. CherryQ outperforms existingquantization approaches in terms of perplexity and downstream task performance.Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performancecompared to their 16-bit counterparts.</description><author>Wanyun Cui, Qianle Wang</author><pubDate>Fri, 29 Nov 2024 15:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02837v2</guid></item><item><title>CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives</title><link>http://arxiv.org/abs/2411.19787v1</link><description>Grounding the instruction in the environment is a key step in solvinglanguage-guided goal-reaching reinforcement learning problems. In automatedreinforcement learning, a key concern is to enhance the model's ability togeneralize across various tasks and environments. In goal-reaching scenarios,the agent must comprehend the different parts of the instructions within theenvironmental context in order to complete the overall task successfully. Inthis work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as anew framework to solve this problem using auxiliary loss functions inspired byvideo-text retrieval literature and a novel method called instruction tracking,which automatically keeps track of progress in an environment. The results ofour experiments suggest superior sample efficiency and systematicgeneralization for this framework in multi-modal reinforcement learningproblems. Our code base is available here.</description><author>Armin Saghafian, Amirmohammad Izadi, Negin Hashemi Dijujin, Mahdieh Soleymani Baghshah</author><pubDate>Fri, 29 Nov 2024 15:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19787v1</guid></item><item><title>MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks</title><link>http://arxiv.org/abs/2411.19786v1</link><description>Recently, human motion analysis has experienced great improvement due toinspiring generative models such as the denoising diffusion model and largelanguage model. While the existing approaches mainly focus on generatingmotions with textual descriptions and overlook the reciprocal task. In thispaper, we present~\textbf{MoTe}, a unified multi-modal model that could handlediverse tasks by learning the marginal, conditional, and joint distributions ofmotion and text simultaneously. MoTe enables us to handle the pairedtext-motion generation, motion captioning, and text-driven motion generation bysimply modifying the input context. Specifically, MoTe is composed of threecomponents: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), andMoti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained forextracting latent embeddings, and subsequently reconstructing the motionsequences and textual descriptions from the extracted embeddings, respectively.MTDM, on the other hand, performs an iterative denoising process on the inputcontext to handle diverse tasks. Experimental results on the benchmark datasetsdemonstrate the superior performance of our proposed method on text-to-motiongeneration and competitive performance on motion captioning.</description><author>Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu</author><pubDate>Fri, 29 Nov 2024 15:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19786v1</guid></item><item><title>Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models</title><link>http://arxiv.org/abs/2410.09432v2</link><description>Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuningof foundation models. However, applying LoRA in federated learningenvironments, where data is distributed across multiple clients, presentsunique challenges. Existing methods rely on traditional federated averaging ofLoRA adapters, resulting in inexact updates. To address this, we proposeFederated Exact LoRA, or FedExLoRA, which adds a residual error term to thepretrained frozen weight matrix. Our approach achieves exact updates withminimal computational and communication overhead, preserving LoRA's efficiency.We evaluate the method on various models across arithmetic reasoning,commonsense reasoning, natural language understanding and natural languagegeneration tasks, showing consistent performance gains over state-of-the-artmethods across multiple settings. Through extensive analysis, we quantify thatthe deviations in updates from the ideal solution are significant, highlightingthe need for exact aggregation. Our method's simplicity, efficiency, and broadapplicability position it as a promising solution for accurate and effectivefederated fine-tuning of foundation models. Our code is publicly available athttps://github.com/RaghavSinghal10/fedex-lora.</description><author>Raghav Singhal, Kaustubh Ponkshe, Praneeth Vepakomma</author><pubDate>Fri, 29 Nov 2024 15:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09432v2</guid></item><item><title>Learning Local Control Barrier Functions for Hybrid Systems</title><link>http://arxiv.org/abs/2401.14907v2</link><description>Hybrid dynamical systems are ubiquitous as practical robotic applicationsoften involve both continuous states and discrete switchings. Safety is aprimary concern for hybrid robotic systems. Existing safety-critical controlapproaches for hybrid systems are either computationally inefficient,detrimental to system performance, or limited to small-scale systems. To amendthese drawbacks, in this paper, we propose a learning-enabled approach toconstruct local Control Barrier Functions (CBFs) to guarantee the safety of awide class of nonlinear hybrid dynamical systems. The end result is a safeneural CBF-based switching controller. Our approach is computationallyefficient, minimally invasive to any reference controller, and applicable tolarge-scale systems. We empirically evaluate our framework and demonstrate itsefficacy and flexibility through two robotic examples including ahigh-dimensional autonomous racing case, against other CBF-based approaches andmodel predictive control.</description><author>Shuo Yang, Yu Chen, Xiang Yin, George J. Pappas, Rahul Mangharam</author><pubDate>Fri, 29 Nov 2024 15:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14907v2</guid></item><item><title>Image segmentation of treated and untreated tumor spheroids by Fully Convolutional Networks</title><link>http://arxiv.org/abs/2405.01105v2</link><description>Multicellular tumor spheroids (MCTS) are advanced cell culture systems forassessing the impact of combinatorial radio(chemo)therapy. They exhibittherapeutically relevant in-vivo-like characteristics from 3D cell-cell andcell-matrix interactions to radial pathophysiological gradients related toproliferative activity and nutrient/oxygen supply, altering cellularradioresponse. State-of-the-art assays quantify long-term curative endpointsbased on collected brightfield image time series from large treated spheroidpopulations per irradiation dose and treatment arm. Here, spheroid controlprobabilities are documented analogous to in-vivo tumor control probabilitiesbased on Kaplan-Meier curves. This analyses require laborious spheroidsegmentation of up to 100.000 images per treatment arm to extract relevantstructural information from the images, e.g., diameter, area, volume andcircularity. While several image analysis algorithms are available for spheroidsegmentation, they all focus on compact MCTS with clearly distinguishable outerrim throughout growth. However, treated MCTS may partly be detached anddestroyed and are usually obscured by dead cell debris. We successfully traintwo Fully Convolutional Networks, UNet and HRNet, and optimize theirhyperparameters to develop an automatic segmentation for both untreated andtreated MCTS. We systematically validate the automatic segmentation on larger,independent data sets of spheroids derived from two human head-and-neck cancercell lines. We find an excellent overlap between manual and automaticsegmentation for most images, quantified by Jaccard indices at around 90%. Forimages with smaller overlap of the segmentations, we demonstrate that thiserror is comparable to the variations across segmentations from differentbiological experts, suggesting that these images represent biologically unclearor ambiguous cases.</description><author>Matthias Streller, Soňa Michlíková, Willy Ciecior, Katharina Lönnecke, Leoni A. Kunz-Schughart, Steffen Lange, Anja Voss-Böhme</author><pubDate>Fri, 29 Nov 2024 15:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01105v2</guid></item><item><title>Machine learning force-field model for kinetic Monte Carlo simulations of itinerant Ising magnets</title><link>http://arxiv.org/abs/2411.19780v1</link><description>We present a scalable machine learning (ML) framework for large-scale kineticMonte Carlo (kMC) simulations of itinerant electron Ising systems. As theeffective interactions between Ising spins in such itinerant magnets aremediated by conducting electrons, the calculation of energy change due to alocal spin update requires solving an electronic structure problem. Suchrepeated electronic structure calculations could be overwhelmingly prohibitivefor large systems. Assuming the locality principle, a convolutional neuralnetwork (CNN) model is developed to directly predict the effective local fieldand the corresponding energy change associated with a given spin update basedon Ising configuration in a finite neighborhood. As the kernel size of the CNNis fixed at a constant, the model can be directly scalable to kMC simulationsof large lattices. Our approach is reminiscent of the ML force-field modelswidely used in first-principles molecular dynamics simulations. Applying our MLframework to a square-lattice double-exchange Ising model, we uncover unusualcoarsening of ferromagnetic domains at low temperatures. Our work highlightsthe potential of ML methods for large-scale modeling of similar itinerantsystems with discrete dynamical variables.</description><author>Alexa Tyberg, Yunhao Fan, Gia-Wei Chern</author><pubDate>Fri, 29 Nov 2024 15:35:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19780v1</guid></item><item><title>P2PFormer: A Primitive-to-polygon Method for Regular Building Contour Extraction from Remote Sensing Images</title><link>http://arxiv.org/abs/2406.02930v2</link><description>Extracting building contours from remote sensing imagery is a significantchallenge due to buildings' complex and diverse shapes, occlusions, and noise.Existing methods often struggle with irregular contours, rounded corners, andredundancy points, necessitating extensive post-processing to produce regularpolygonal building contours. To address these challenges, we introduce a novel,streamlined pipeline that generates regular building contours withoutpost-processing. Our approach begins with the segmentation of generic geometricprimitives (which can include vertices, lines, and corners), followed by theprediction of their sequence. This allows for the direct construction ofregular building contours by sequentially connecting the segmented primitives.Building on this pipeline, we developed P2PFormer, which utilizes atransformer-based architecture to segment geometric primitives and predicttheir order. To enhance the segmentation of primitives, we introduce a uniquerepresentation called group queries. This representation comprises a set ofqueries and a singular query position, which improve the focus on multiplemidpoints of primitives and their efficient linkage. Furthermore, we propose aninnovative implicit update strategy for the query position embedding aimed atsharpening the focus of queries on the correct positions and, consequently,enhancing the quality of primitive segmentation. Our experiments demonstratethat P2PFormer achieves new state-of-the-art performance on the WHU, CrowdAI,and WHU-Mix datasets, surpassing the previous SOTA PolyWorld by a margin of 2.7AP and 6.5 AP75 on the largest CrowdAI dataset</description><author>Tao Zhang, Shiqing Wei, Yikang Zhou, Muying Luo, Wenling You, Shunping Ji</author><pubDate>Fri, 29 Nov 2024 15:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02930v2</guid></item><item><title>VideoDirector: Precise Video Editing via Text-to-Video Models</title><link>http://arxiv.org/abs/2411.17592v2</link><description>Despite the typical inversion-then-editing paradigm using text-to-image (T2I)models has demonstrated promising results, directly extending it totext-to-video (T2V) models still suffers severe artifacts such as colorflickering and content distortion. Consequently, current video editing methodsprimarily rely on T2I models, which inherently lack temporal-coherencegenerative ability, often resulting in inferior editing results. In this paper,we attribute the failure of the typical editing paradigm to: 1) TightlySpatial-temporal Coupling. The vanilla pivotal-based inversion strategystruggles to disentangle spatial-temporal information in the video diffusionmodel; 2) Complicated Spatial-temporal Layout. The vanilla cross-attentioncontrol is deficient in preserving the unedited content. To address theselimitations, we propose a spatial-temporal decoupled guidance (STDG) andmulti-frame null-text optimization strategy to provide pivotal temporal cuesfor more precise pivotal inversion. Furthermore, we introduce a self-attentioncontrol strategy to maintain higher fidelity for precise partial contentediting. Experimental results demonstrate that our method (termedVideoDirector) effectively harnesses the powerful temporal generationcapabilities of T2V models, producing edited videos with state-of-the-artperformance in accuracy, motion smoothness, realism, and fidelity to uneditedcontent.</description><author>Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, Yulan Guo</author><pubDate>Fri, 29 Nov 2024 15:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17592v2</guid></item><item><title>PerLA: Perceptive 3D Language Assistant</title><link>http://arxiv.org/abs/2411.19774v1</link><description>Enabling Large Language Models (LLMs) to understand the 3D physical world isan emerging yet challenging research direction. Current strategies forprocessing point clouds typically downsample the scene or divide it intosmaller parts for separate analysis. However, both approaches risk losing keylocal details or global contextual information. In this paper, we introducePerLA, a 3D language assistant designed to be more perceptive to both detailsand context, making visual representations more informative for the LLM. PerLAcaptures high-resolution (local) details in parallel from different point cloudareas and integrates them with (global) context obtained from alower-resolution whole point cloud. We present a novel algorithm that preservespoint cloud locality through the Hilbert curve and effectively aggregateslocal-to-global information via cross-attention and a graph neural network.Lastly, we introduce a novel loss for local representation consensus to promotetraining stability. PerLA outperforms state-of-the-art 3D language assistants,with gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 onScanRefer and +3.88 on Nr3D for densecaptioning.\url{https://gfmei.github.io/PerLA/}</description><author>Guofeng Mei, Wei Lin, Luigi Riz, Yujiao Wu, Fabio Poiesi, Yiming Wang</author><pubDate>Fri, 29 Nov 2024 15:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19774v1</guid></item><item><title>LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos</title><link>http://arxiv.org/abs/2411.19772v1</link><description>Despite impressive advancements in video understanding, most efforts remainlimited to coarse-grained or visual-only video tasks. However, real-worldvideos encompass omni-modal information (vision, audio, and speech) with aseries of events forming a cohesive storyline. The lack of multi-modal videodata with fine-grained event annotations and the high cost of manual labelingare major obstacles to comprehensive omni-modality video perception. To addressthis gap, we propose an automatic pipeline consisting of high-qualitymulti-modal video filtering, semantically coherent omni-modal event boundarydetection, and cross-modal correlation-aware event captioning. In this way, wepresent LongVALE, the first-ever Vision-Audio-Language Event understandingbenchmark comprising 105K omni-modal events with precise temporal boundariesand detailed relation-aware captions within 8.4K high-quality long videos.Further, we build a baseline that leverages LongVALE to enable video largelanguage models (LLMs) for omni-modality fine-grained temporal videounderstanding for the first time. Extensive experiments demonstrate theeffectiveness and great potential of LongVALE in advancing comprehensivemulti-modal video understanding.</description><author>Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng</author><pubDate>Fri, 29 Nov 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19772v1</guid></item><item><title>Noro: A Noise-Robust One-shot Voice Conversion System with Hidden Speaker Representation Capabilities</title><link>http://arxiv.org/abs/2411.19770v1</link><description>One-shot voice conversion (VC) aims to alter the timbre of speech from asource speaker to match that of a target speaker using just a single referencespeech from the target, while preserving the semantic content of the originalsource speech. Despite advancements in one-shot VC, its effectiveness decreasesin real-world scenarios where reference speeches, often sourced from theinternet, contain various disturbances like background noise. To address thisissue, we introduce Noro, a Noise Robust One-shot VC system. Noro featuresinnovative components tailored for VC using noisy reference speeches, includinga dual-branch reference encoding module and a noise-agnostic contrastivespeaker loss. Experimental results demonstrate that Noro outperforms ourbaseline system in both clean and noisy scenarios, highlighting its efficacyfor real-world applications. Additionally, we investigate the hidden speakerrepresentation capabilities of our baseline system by repurposing its referenceencoder as a speaker encoder. The results shows that it is competitive withseveral advanced self-supervised learning models for speaker representationunder the SUPERB settings, highlighting the potential for advancing speakerrepresentation learning through one-shot VC task.</description><author>Haorui He, Yuchen Song, Yuancheng Wang, Haoyang Li, Xueyao Zhang, Li Wang, Gongping Huang, Eng Siong Chng, Zhizheng Wu</author><pubDate>Fri, 29 Nov 2024 15:18:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19770v1</guid></item><item><title>Riemannian Denoising Score Matching for Molecular Structure Optimization with Accurate Energy</title><link>http://arxiv.org/abs/2411.19769v1</link><description>This study introduces a modified score matching method aimed at generatingmolecular structures with high energy accuracy. The denoising process of scorematching or diffusion models mirrors molecular structure optimization, wherescores act like physical force fields that guide particles toward equilibriumstates. To achieve energetically accurate structures, it can be advantageous tohave the score closely approximate the gradient of the actual potential energysurface. Unlike conventional methods that simply design the target score basedon structural differences in Euclidean space, we propose a Riemannian scorematching approach. This method represents molecular structures on a manifolddefined by physics-informed internal coordinates to efficiently mimic theenergy landscape, and performs noising and denoising within this space. Ourmethod has been evaluated by refining several types of starting structures onthe QM9 and GEOM datasets, demonstrating that the proposed Riemannian scorematching method significantly improves the accuracy of the generated molecularstructures, attaining chemical accuracy. The implications of this study extendto various applications in computational chemistry, offering a robust tool foraccurate molecular structure prediction.</description><author>Jeheon Woo, Seonghwan Kim, Jun Hyeong Kim, Woo Youn Kim</author><pubDate>Fri, 29 Nov 2024 15:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19769v1</guid></item><item><title>Stock Price Prediction using Multi-Faceted Information based on Deep Recurrent Neural Networks</title><link>http://arxiv.org/abs/2411.19766v1</link><description>Accurate prediction of stock market trends is crucial for informed investmentdecisions and effective portfolio management, ultimately leading to enhancedwealth creation and risk mitigation. This study proposes a novel approach forpredicting stock prices in the stock market by integrating Convolutional NeuralNetworks (CNN) and Long Short-Term Memory (LSTM) networks, using sentimentanalysis of social network data and candlestick data (price). The proposedmethodology consists of two primary components: sentiment analysis of socialnetwork and candlestick data. By amalgamating candlestick data with insightsgleaned from Twitter, this approach facilitates a more detailed and accurateexamination of market trends and patterns, ultimately leading to more effectivestock price predictions. Additionally, a Random Forest algorithm is used toclassify tweets as either positive or negative, allowing for a more subtle andinformed assessment of market sentiment. This study uses CNN and LSTM networksto predict stock prices. The CNN extracts short-term features, while the LSTMmodels long-term dependencies. The integration of both networks enables a morecomprehensive analysis of market trends and patterns, leading to more accuratestock price predictions.</description><author>Lida Shahbandari, Elahe Moradi, Mohammad Manthouri</author><pubDate>Fri, 29 Nov 2024 15:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19766v1</guid></item><item><title>Risk-Sensitive Reinforcement Learning with Exponential Criteria</title><link>http://arxiv.org/abs/2212.09010v6</link><description>While reinforcement learning has shown experimental success in a number ofapplications, it is known to be sensitive to noise and perturbations in theparameters of the system, leading to high variance in the total reward amongstdifferent episodes in slightly different environments. To introduce robustness,as well as sample efficiency, risk-sensitive reinforcement learning methods arebeing thoroughly studied. In this work, we provide a definition of robustreinforcement learning policies and formulate a risk-sensitive reinforcementlearning problem to approximate them, by solving an optimization problem withrespect to a modified objective based on exponential criteria. In particular,we study a model-free risk-sensitive variation of the widely-used Monte CarloPolicy Gradient algorithm and introduce a novel risk-sensitive onlineActor-Critic algorithm based on solving a multiplicative Bellman equation usingstochastic approximation updates. Analytical results suggest that the use ofexponential criteria generalizes commonly used ad-hoc regularizationapproaches, improves sample efficiency, and introduces robustness with respectto perturbations in the model parameters and the environment. Theimplementation, performance, and robustness properties of the proposed methodsare evaluated in simulated experiments.</description><author>Erfaun Noorani, Christos Mavridis, John Baras</author><pubDate>Fri, 29 Nov 2024 15:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09010v6</guid></item><item><title>Forecasting Foreign Exchange Market Prices Using Technical Indicators with Deep Learning and Attention Mechanism</title><link>http://arxiv.org/abs/2411.19763v1</link><description>Accurate prediction of price behavior in the foreign exchange market iscrucial. This paper proposes a novel approach that leverages technicalindicators and deep neural networks. The proposed architecture consists of aLong Short-Term Memory (LSTM) and Convolutional Neural Network (CNN), andattention mechanism. Initially, trend and oscillation technical indicators areemployed to extract statistical features from Forex currency pair data,providing insights into price trends, market volatility, relative pricestrength, and overbought and oversold conditions. Subsequently, the LSTM andCNN networks are utilized in parallel to predict future price movements,leveraging the strengths of both recurrent and convolutional architectures. TheLSTM network captures long-term dependencies and temporal patterns in the data,while the CNN network extracts local patterns. The outputs of the parallel LSTMand CNN networks are then fed into an attention mechanism, which learns toweigh the importance of each feature and temporal dependency, generating acontext-aware representation of the input data. The attention-weighted outputis then used to predict future price movements, enabling the model to focus onthe most relevant features and temporal dependencies. Through a comprehensiveevaluation of the proposed approach on multiple Forex currency pairs, wedemonstrate its effectiveness in predicting price behavior and outperformingbenchmark models.</description><author>Sahabeh Saadati, Mohammad Manthouri</author><pubDate>Fri, 29 Nov 2024 15:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19763v1</guid></item><item><title>LaVIDE: A Language-Vision Discriminator for Detecting Changes in Satellite Image with Map References</title><link>http://arxiv.org/abs/2411.19758v1</link><description>Change detection, which typically relies on the comparison of bi-temporalimages, is significantly hindered when only a single image is available.Comparing a single image with an existing map, such as OpenStreetMap, which iscontinuously updated through crowd-sourcing, offers a viable solution to thischallenge. Unlike images that carry low-level visual details of ground objects,maps convey high-level categorical information. This discrepancy in abstractionlevels complicates the alignment and comparison of the two data types. In thispaper, we propose a \textbf{La}nguage-\textbf{VI}sion \textbf{D}iscriminatorfor d\textbf{E}tecting changes in satellite image with map references, namely\ours{}, which leverages language to bridge the information gap between mapsand images. Specifically, \ours{} formulates change detection as the problem of``{\textit Does the pixel belong to [class]?}'', aligning maps and imageswithin the feature space of the language-vision model to associate high-levelmap categories with low-level image details. Moreover, we build amixture-of-experts discriminative module, which compares linguistic featuresfrom maps with visual features from images across various semanticperspectives, achieving comprehensive semantic comparison for change detection.Extensive evaluation on four benchmark datasets demonstrates that \ours{} caneffectively detect changes in satellite image with map references,outperforming state-of-the-art change detection algorithms, e.g., with gains ofabout $13.8$\% on the DynamicEarthNet dataset and $4.3$\% on the SECONDdataset.</description><author>Shuguo Jiang, Fang Xu, Sen Jia, Gui-Song Xia</author><pubDate>Fri, 29 Nov 2024 15:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19758v1</guid></item><item><title>Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models</title><link>http://arxiv.org/abs/2411.19757v1</link><description>Fine-tuning foundation models often compromises their robustness todistribution shifts. To remedy this, most robust fine-tuning methods aim topreserve the pre-trained features. However, not all pre-trained features arerobust and those methods are largely indifferent to which ones to preserve. Wepropose dual risk minimization (DRM), which combines empirical riskminimization with worst-case risk minimization, to better preserve the corefeatures of downstream tasks. In particular, we utilize core-featuredescriptions generated by LLMs to induce core-based zero-shot predictions whichthen serve as proxies to estimate the worst-case risk. DRM balances two crucialaspects of model robustness: expected performance and worst-case performance,establishing a new state of the art on various real-world benchmarks. DRMsignificantly improves the out-of-distribution performance of CLIP ViT-L/14@336on ImageNet (75.9 to 77.1), WILDS-iWildCam (47.1 to 51.8), and WILDS-FMoW (50.7to 53.1); opening up new avenues for robust fine-tuning. Our code is availableat https://github.com/vaynexie/DRM .</description><author>Kaican Li, Weiyan Xie, Yongxiang Huang, Didan Deng, Lanqing Hong, Zhenguo Li, Ricardo Silva, Nevin L. Zhang</author><pubDate>Fri, 29 Nov 2024 15:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19757v1</guid></item><item><title>DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering</title><link>http://arxiv.org/abs/2411.19756v1</link><description>Gaussian splatting enables fast novel view synthesis in static 3Denvironments. However, reconstructing real-world environments remainschallenging as distractors or occluders break the multi-view consistencyassumption required for accurate 3D reconstruction. Most existing methods relyon external semantic information from pre-trained models, introducingadditional computational overhead as pre-processing steps or duringoptimization. In this work, we propose a novel method, DeSplat, that directlyseparates distractors and static scene elements purely based on volumerendering of Gaussian primitives. We initialize Gaussians within each cameraview for reconstructing the view-specific distractors to separately model thestatic 3D scene and distractors in the alpha compositing stages. DeSplat yieldsan explicit scene separation of static elements and distractors, achievingcomparable results to prior distractor-free approaches without sacrificingrendering speed. We demonstrate DeSplat's effectiveness on three benchmark datasets for distractor-free novel view synthesis. See the project website athttps://aaltoml.github.io/desplat/.</description><author>Yihao Wang, Marcus Klasson, Matias Turkulainen, Shuzhe Wang, Juho Kannala, Arno Solin</author><pubDate>Fri, 29 Nov 2024 15:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19756v1</guid></item><item><title>Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation</title><link>http://arxiv.org/abs/2402.07808v3</link><description>Scientific modeling applications often require estimating a distribution ofparameters consistent with a dataset of observations - an inference task alsoknown as source distribution estimation. This problem can be ill-posed,however, since many different source distributions might produce the samedistribution of data-consistent simulations. To make a principled choice amongmany equally valid sources, we propose an approach which targets the maximumentropy distribution, i.e., prioritizes retaining as much uncertainty aspossible. Our method is purely sample-based - leveraging the Sliced-Wassersteindistance to measure the discrepancy between the dataset and simulations - andthus suitable for simulators with intractable likelihoods. We benchmark ourmethod on several tasks, and show that it can recover source distributions withsubstantially higher entropy than recent source estimation methods, withoutsacrificing the fidelity of the simulations. Finally, to demonstrate theutility of our approach, we infer source distributions for parameters of theHodgkin-Huxley model from experimental datasets with thousands of single-neuronmeasurements. In summary, we propose a principled method for inferring sourcedistributions of scientific simulator parameters while retaining as muchuncertainty as possible.</description><author>Julius Vetter, Guy Moss, Cornelius Schröder, Richard Gao, Jakob H. Macke</author><pubDate>Fri, 29 Nov 2024 14:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07808v3</guid></item><item><title>Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification in Gastrointestinal Endoscopy</title><link>http://arxiv.org/abs/2410.21302v3</link><description>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE)diagnostics by offering a non-invasive method for capturing detailed images ofthe gastrointestinal tract, enabling early disease detection. However, itspotential is limited by the sheer volume of images generated during the imagingprocedure, which can take anywhere from 6-8 hours and often produce up to 1million images, necessitating automated analysis. Additionally, the variabilityof these images, combined with the need for expert annotations and the scarcityof large, high-quality labeled datasets, constrains the effectiveness ofcurrent medical image analysis models. To address this, we introduce a novellarge GIE dataset, called EndoExtend24, created by merging ten existing publicand private datasets, ensuring patient integrity across splits. EndoExtend24includes over 226,000 labeled images, as well as dynamic class mappings, whichallow unified training across datasets with differing labeling granularity,supporting up to 123 distinct pathological findings. Further, we propose toleverage domain adaptive pre-training of foundation models trained withself-supervision on generic image data, to adapt them to the task of GIEmedical image diagnosis. Specifically, the EVA-02 model, which is based on theViT architecture and trained on ImageNet-22k with masked image modeling (usingEVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset toachieve domain adaptation, and finally trained on the Capsule Endoscopy 2024Challenge dataset. Our model demonstrates robust performance, securing thirdplace in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762and a balanced accuracy of 37.1% on the test set. These results emphasize theeffectiveness of our domain-adaptive pre-training approach and the enrichedEndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics.</description><author>Marcel Roth, Micha V. Nowak, Adrian Krenzer, Frank Puppe</author><pubDate>Fri, 29 Nov 2024 14:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21302v3</guid></item><item><title>A Comprehensive Content Verification System for ensuring Digital Integrity in the Age of Deep Fakes</title><link>http://arxiv.org/abs/2411.19750v1</link><description>In an era marked by the widespread sharing of digital content, the need for arobust content-integrity verification goes beyond the confines of individualsocial media platforms. While verified profiles (such as blue ticks onplatforms like Instagram and X) have become synonymous with credibility, thecontent they share often traverses a complex network of interconnectedplatforms, by means of re-sharing, re-posting, etc., leaving a void in theauthentication process of the content itself. With the advent of easilyaccessible AI tools (like DALL-E, Sora, and the tools that are explicitly builtfor generating deepfakes &amp; face swaps), the risk of misinformation throughsocial media platforms is growing exponentially. This paper discusses asolution, a Content Verification System, designed to authenticate images andvideos shared as posts or stories across the digital landscape. Going beyondthe limitations of blue ticks, this system empowers individuals and influencersto validate the authenticity of their digital footprint, safeguarding theirreputation in an interconnected world.</description><author>RaviKanth Kaja</author><pubDate>Fri, 29 Nov 2024 14:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19750v1</guid></item><item><title>A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining Off-Road, Diversity, and Directional Consistency Losses</title><link>http://arxiv.org/abs/2411.19747v1</link><description>Trajectory prediction is essential for the safety and efficiency of planningin autonomous vehicles. However, current models often fail to fully capturecomplex traffic rules and the complete range of potential vehicle movements.Addressing these limitations, this study introduces three novel loss functions:Offroad Loss, Direction Consistency Error, and Diversity Loss. These functionsare designed to keep predicted paths within driving area boundaries, alignedwith traffic directions, and cover a wider variety of plausible drivingscenarios. As all prediction modes should adhere to road rules and conditions,this work overcomes the shortcomings of traditional "winner takes all" trainingmethods by applying the loss functions to all prediction modes. These lossfunctions not only improve model training but can also serve as metrics forevaluating the realism and diversity of trajectory predictions. Extensivevalidation on the nuScenes and Argoverse 2 datasets with leading baselinemodels demonstrates that our approach not only maintains accuracy butsignificantly improves safety and robustness, reducing offroad errors onaverage by 47% on original and by 37% on attacked scenes. This work sets a newbenchmark for trajectory prediction in autonomous driving, offering substantialimprovements in navigating complex environments. Our code is available athttps://github.com/vita-epfl/stay-on-track .</description><author>Ahmad Rahimi, Alexandre Alahi</author><pubDate>Fri, 29 Nov 2024 14:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19747v1</guid></item><item><title>HVAC-DPT: A Decision Pretrained Transformer for HVAC Control</title><link>http://arxiv.org/abs/2411.19746v1</link><description>Building operations consume approximately 40% of global energy, with Heating,Ventilation, and Air Conditioning (HVAC) systems responsible for up to 50% ofthis consumption. As HVAC energy demands are expected to rise, optimisingsystem efficiency is crucial for reducing future energy use and mitigatingclimate change. Existing control strategies lack generalisation and requireextensive training and data, limiting their rapid deployment across diversebuildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformerusing in-context Reinforcement Learning (RL) for multi-zone HVAC control.HVAC-DPT frames HVAC control as a sequential prediction task, training a causaltransformer on interaction histories generated by diverse RL agents. Thisapproach enables HVAC-DPT to refine its policy in-context, without modifyingnetwork parameters, allowing for deployment across different buildings withoutthe need for additional training or data collection. HVAC-DPT reduces energyconsumption in unseen buildings by 45% compared to the baseline controller,offering a scalable and effective approach to mitigating the increasingenvironmental impact of HVAC systems.</description><author>Anaïs Berkes</author><pubDate>Fri, 29 Nov 2024 14:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19746v1</guid></item><item><title>Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries</title><link>http://arxiv.org/abs/2402.08349v3</link><description>Text-to-SQL systems (also known as NL-to-SQL systems) have become anincreasingly popular solution for bridging the gap between user capabilitiesand SQL-based data access. These systems translate user requests in naturallanguage to valid SQL statements for a specific database. Recent Text-to-SQLsystems have benefited from the rapid improvement of transformer-based languagemodels. However, while Text-to-SQL systems that incorporate such modelscontinuously reach new high scores on -- often synthetic -- benchmark datasets,a systematic exploration of their robustness towards different data models in areal-world, realistic scenario is notably missing. This paper provides thefirst in-depth evaluation of the data model robustness of Text-to-SQL systemsin practice based on a multi-year international project focused on Text-to-SQLinterfaces. Our evaluation is based on a real-world deployment of FootballDB, asystem that was deployed over a 9 month period in the context of the FIFA WorldCup 2022, during which about 6K natural language questions were asked andexecuted. All of our data is based on real user questions that were asked liveto the system. We manually labeled and translated a subset of these questionsfor three different data models. For each data model, we explore theperformance of representative Text-to-SQL systems and language models. Wefurther quantify the impact of training data size, pre-, and post-processingsteps as well as language model inference time. Our comprehensive evaluationsheds light on the design choices of real-world Text-to-SQL systems and theirimpact on moving from research prototypes to real deployments. Last, we providea new benchmark dataset to the community, which is the first to enable theevaluation of different data models for the same dataset and is substantiallymore challenging than most previous datasets in terms of query complexity.</description><author>Jonathan Fürst, Catherine Kosten, Farhad Nooralahzadeh, Yi Zhang, Kurt Stockinger</author><pubDate>Fri, 29 Nov 2024 14:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08349v3</guid></item></channel></rss>