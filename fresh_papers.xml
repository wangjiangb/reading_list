<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 04 Feb 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Low-Rank Adapting Models for Sparse Autoencoders</title><link>http://arxiv.org/abs/2501.19406v1</link><description>Sparse autoencoders (SAEs) decompose language model representations into asparse set of linear latent vectors. Recent works have improved SAEs usinglanguage model gradients, but these techniques require many expensive backwardpasses during training and still cause a significant increase in cross entropyloss when SAE reconstructions are inserted into the model. In this work, weimprove on these limitations by taking a fundamentally different approach: weuse low-rank adaptation (LoRA) to finetune the language model itself around apreviously trained SAE. We analyze our method across SAE sparsity, SAE width,language model size, LoRA rank, and model layer on the Gemma Scope family ofSAEs. In these settings, our method reduces the cross entropy loss gap by 30%to 55% when SAEs are inserted during the forward pass. We also find thatcompared to end-to-end (e2e) SAEs, our approach achieves the same downstreamcross entropy loss 3$\times$ to 20$\times$ faster on Gemma-2-2B and 2$\times$to 10$\times$ faster on Llama-3.2-1B. We further show that our techniqueimproves downstream metrics and can adapt multiple SAEs at once. Our resultsdemonstrate that improving model interpretability is not limited to post-hocSAE training; Pareto improvements can also be achieved by directly optimizingthe model itself.</description><author>Matthew Chen, Joshua Engels, Max Tegmark</author><pubDate>Fri, 31 Jan 2025 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19406v1</guid></item><item><title>Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach</title><link>http://arxiv.org/abs/2501.19403v1</link><description>Machine unlearning seeks to systematically remove specified data from atrained model, effectively achieving a state as though the data had never beenencountered during training. While metrics such as Unlearning Accuracy (UA) andMembership Inference Attack (MIA) provide a baseline for assessing unlearningperformance, they fall short of evaluating the completeness and reliability offorgetting. This is because the ground truth labels remain potential candidateswithin the scope of uncertainty quantification, leaving gaps in the evaluationof true forgetting. In this paper, we identify critical limitations in existingunlearning metrics and propose enhanced evaluation metrics inspired byconformal prediction. Our metrics can effectively capture the extent to whichground truth labels are excluded from the prediction set. Furthermore, weobserve that many existing machine unlearning methods do not achievesatisfactory forgetting performance when evaluated with our new metrics. Toaddress this, we propose an unlearning framework that integrates conformalprediction insights into Carlini &amp; Wagner adversarial attack loss. Extensiveexperiments on the image classification task demonstrate that our enhancedmetrics offer deeper insights into unlearning effectiveness, and that ourunlearning framework significantly improves the forgetting quality ofunlearning methods.</description><author>Yingdan Shi, Ren Wang</author><pubDate>Fri, 31 Jan 2025 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19403v1</guid></item><item><title>LLMs Are In-Context Bandit Reinforcement Learners</title><link>http://arxiv.org/abs/2410.05362v2</link><description>Large Language Models (LLMs) excel at in-context learning (ICL), a supervisedlearning technique that relies on adding annotated examples to the modelcontext. We investigate a contextual bandit version of in-context reinforcementlearning (ICRL), where models learn in-context, online, from external reward,instead of supervised data. We show that LLMs effectively demonstrate suchlearning, and provide a detailed study of the phenomena, experimenting withchallenging classification tasks and models of sizes from 500M to 70Bparameters. This includes identifying and addressing the instability of theprocess, demonstrating learning with both semantic and abstract labels, andshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, whilealso underscoring fundamental limitations in their implicit reasoning abouterrors.</description><author>Giovanni Monea, Antoine Bosselut, Kiant√© Brantley, Yoav Artzi</author><pubDate>Fri, 31 Jan 2025 18:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05362v2</guid></item><item><title>Diverse Preference Optimization</title><link>http://arxiv.org/abs/2501.18101v2</link><description>Post-training of language models, either through reinforcement learning,preference optimization or supervised finetuning, tends to sharpen the outputprobability distribution and reduce the diversity of generated responses. Thisis particularly a problem for creative generative tasks where varied responsesare desired. In this work we introduce Diverse Preference Optimization (DivPO),an optimization method which learns to generate much more diverse responsesthan standard pipelines, while maintaining the quality of the generations. InDivPO, preference pairs are selected by first considering a pool of responses,and a measure of diversity among them, and selecting chosen examples as beingmore rare but high quality, while rejected examples are more common, but lowquality. DivPO results in generating 45.6% more diverse persona attributes, andan 74.6% increase in story diversity, while maintaining similar win rates asstandard baselines.</description><author>Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, Ilia Kulikov</author><pubDate>Fri, 31 Jan 2025 18:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18101v2</guid></item><item><title>Detection Is All You Need: A Feasible Optimal Prior-Free Black-Box Approach For Piecewise Stationary Bandits</title><link>http://arxiv.org/abs/2501.19401v1</link><description>We study the problem of piecewise stationary bandits without prior knowledgeof the underlying non-stationarity. We propose the first $\textit{feasible}$black-box algorithm applicable to most common parametric bandit variants. Ourprocedure, termed Detection Augmented Bandit (DAB), is modular, accepting anystationary bandit algorithm as input and augmenting it with a change detector.DAB achieves optimal regret in the piecewise stationary setting under mildassumptions. Specifically, we prove that DAB attains the order-optimal regretbound of $\tilde{\mathcal{O}}(\sqrt{N_T T})$, where $N_T$ denotes the number ofchanges over the horizon $T$, if its input stationary bandit algorithm hasorder-optimal stationary regret guarantees. Applying DAB to differentparametric bandit settings, we recover recent state-of-the-art results.Notably, for self-concordant bandits, DAB achieves optimal dynamic regret,while previous works obtain suboptimal bounds and require knowledge on thenon-stationarity. In simulations on piecewise stationary environments, DABoutperforms existing approaches across varying number of changes.Interestingly, despite being theoretically designed for piecewise stationaryenvironments, DAB is also effective in simulations in drifting environments,outperforming existing methods designed specifically for this scenario.</description><author>Argyrios Gerogiannis, Yu-Han Huang, Subhonmesh Bose, Venugopal V. Veeravalli</author><pubDate>Fri, 31 Jan 2025 18:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19401v1</guid></item><item><title>Node Classification and Search on the Rubik's Cube Graph with GNNs</title><link>http://arxiv.org/abs/2501.18580v2</link><description>This study focuses on the application of deep geometric models to solve the3x3x3 Rubik's Cube. We begin by discussing the cube's graph representation anddefining distance as the model's optimization objective. The distanceapproximation task is reformulated as a node classification problem,effectively addressed using Graph Neural Networks (GNNs). After training themodel on a random subgraph, the predicted classes are used to construct aheuristic for $A^*$ search. We conclude with experiments comparing ourheuristic to that of the DeepCubeA model.</description><author>Alessandro Barro</author><pubDate>Fri, 31 Jan 2025 18:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18580v2</guid></item><item><title>Vintix: Action Model via In-Context Reinforcement Learning</title><link>http://arxiv.org/abs/2501.19400v1</link><description>In-Context Reinforcement Learning (ICRL) represents a promising paradigm fordeveloping generalist agents that learn at inference time throughtrial-and-error interactions, analogous to how large language models adaptcontextually, but with a focus on reward maximization. However, the scalabilityof ICRL beyond toy tasks and single-domain settings remains an open challenge.In this work, we present the first steps toward scaling ICRL by introducing afixed, cross-domain model capable of learning behaviors through in-contextreinforcement learning. Our results demonstrate that Algorithm Distillation, aframework designed to facilitate ICRL, offers a compelling and competitivealternative to expert distillation to construct versatile action models. Thesefindings highlight the potential of ICRL as a scalable approach for generalistdecision-making systems. Code to be released athttps://github.com/dunnolab/vintix</description><author>Andrey Polubarov, Nikita Lyubaykin, Alexander Derevyagin, Ilya Zisman, Denis Tarasov, Alexander Nikulin, Vladislav Kurenkov</author><pubDate>Fri, 31 Jan 2025 18:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19400v1</guid></item><item><title>Scalable-Softmax Is Superior for Attention</title><link>http://arxiv.org/abs/2501.19399v1</link><description>The maximum element of the vector output by the Softmax function approacheszero as the input vector size increases. Transformer-based language models relyon Softmax to compute attention scores, causing the attention distribution toflatten as the context size grows. This reduces the model's ability toprioritize key information effectively and potentially limits its lengthgeneralization. To address this problem, we propose Scalable-Softmax (SSMax),which replaces Softmax in scenarios where the input vector size varies. SSMaxcan be seamlessly integrated into existing Transformer-based architectures.Experimental results in language modeling show that models using SSMax not onlyachieve faster loss reduction during pretraining but also significantly improveperformance in long contexts and key information retrieval. Furthermore, ananalysis of attention scores reveals that SSMax enables the model to focusattention on key information even in long contexts. Additionally, althoughmodels that use SSMax from the beginning of pretraining achieve better lengthgeneralization, those that have already started pretraining can still gain someof this ability by replacing Softmax in the attention layers with SSMax, eitherduring or after pretraining.</description><author>Ken M. Nakanishi</author><pubDate>Fri, 31 Jan 2025 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19399v1</guid></item><item><title>Near-Optimal Algorithms for Group Distributionally Robust Optimization and Beyond</title><link>http://arxiv.org/abs/2212.13669v2</link><description>Distributionally robust optimization (DRO) can improve the robustness andfairness of learning methods. In this paper, we devise stochastic algorithmsfor a class of DRO problems including group DRO, subpopulation fairness, andempirical conditional value at risk (CVaR) optimization. Our new algorithmsachieve faster convergence rates than existing algorithms for multiple DROsettings. We also provide a new information-theoretic lower bound that impliesour bounds are tight for group DRO. Empirically, too, our algorithms outperformknown methods.</description><author>Tasuku Soma, Khashayar Gatmiry, Sharut Gupta, Stefanie Jegelka</author><pubDate>Fri, 31 Jan 2025 18:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.13669v2</guid></item><item><title>Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game</title><link>http://arxiv.org/abs/2501.19398v1</link><description>Large language model-based (LLM-based) agents have become common in settingsthat include non-cooperative parties. In such settings, agents' decision-makingneeds to conceal information from their adversaries, reveal information totheir cooperators, and infer information to identify the other agents'characteristics. To investigate whether LLMs have these information control anddecision-making capabilities, we make LLM agents play the language-basedhidden-identity game, The Chameleon. In the game, a group of non-chameleonagents who do not know each other aim to identify the chameleon agent withoutrevealing a secret. The game requires the aforementioned information controlcapabilities both as a chameleon and a non-chameleon. The empirical resultsshow that while non-chameleon LLM agents identify the chameleon, they fail toconceal the secret from the chameleon, and their winning probability is farfrom the levels of even trivial strategies. To formally explain this behavior,we give a theoretical analysis for a spectrum of strategies, from concealing torevealing, and provide bounds on the non-chameleons' winning probability. Basedon the empirical results and theoretical analysis of different strategies, wededuce that LLM-based non-chameleon agents reveal excessive information toagents of unknown identities. Our results point to a weakness of contemporaryLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategicinteractions.</description><author>Mustafa O. Karabag, Ufuk Topcu</author><pubDate>Fri, 31 Jan 2025 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19398v1</guid></item><item><title>Contextual Emotion Recognition using Large Vision Language Models</title><link>http://arxiv.org/abs/2405.08992v2</link><description>"How does the person in the bounding box feel?" Achieving human-levelrecognition of the apparent emotion of a person in real world situationsremains an unsolved task in computer vision. Facial expressions are not enough:body pose, contextual knowledge, and commonsense reasoning all contribute tohow humans perform this emotional theory of mind task. In this paper, weexamine two major approaches enabled by recent large vision language models: 1)image captioning followed by a language-only LLM, and 2) vision languagemodels, under zero-shot and fine-tuned setups. We evaluate the methods on theEmotions in Context (EMOTIC) dataset and demonstrate that a vision languagemodel, fine-tuned even on a small dataset, can significantly outperformtraditional baselines. The results of this work aim to help robots and agentsperform emotionally sensitive decision-making and interaction in the future.</description><author>Yasaman Etesam, √ñzge Nilay Yal√ßƒ±n, Chuxuan Zhang, Angelica Lim</author><pubDate>Fri, 31 Jan 2025 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08992v2</guid></item><item><title>SOAP: Improving and Stabilizing Shampoo using Adam</title><link>http://arxiv.org/abs/2409.11321v2</link><description>There is growing evidence of the effectiveness of Shampoo, a higher-orderpreconditioning method, over Adam in deep learning optimization tasks. However,Shampoo's drawbacks include additional hyperparameters and computationaloverhead when compared to Adam, which only updates running averages of first-and second-moment quantities. This work establishes a formal connection betweenShampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficientapproximation of Adam -- showing that Shampoo is equivalent to runningAdafactor in the eigenbasis of Shampoo's preconditioner. This insight leads tothe design of a simpler and computationally efficient algorithm:$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the$\textbf{P}$reconditioner's eigenbasis (SOAP). With regards to improving Shampoo's computational efficiency, the moststraightforward approach would be to simply compute Shampoo'seigendecomposition less frequently. Unfortunately, as our empirical resultsshow, this leads to performance degradation that worsens with this frequency.SOAP mitigates this degradation by continually updating the running average ofthe second moment, just as Adam does, but in the current (slowly changing)coordinate basis. Furthermore, since SOAP is equivalent to running Adam in arotated space, it introduces only one additional hyperparameter (thepreconditioning frequency) compared to Adam. We empirically evaluate SOAP onlanguage model pre-training with 360m and 660m sized models. In the large batchregime, SOAP reduces the number of iterations by over 40% and wall clock timeby over 35% compared to AdamW, with approximately 20% improvements in bothmetrics compared to Shampoo. An implementation of SOAP is available athttps://github.com/nikhilvyas/SOAP.</description><author>Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade</author><pubDate>Fri, 31 Jan 2025 18:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11321v2</guid></item><item><title>DarkIR: Robust Low-Light Image Restoration</title><link>http://arxiv.org/abs/2412.13443v2</link><description>Photography during night or in dark conditions typically suffers from noise,low light and blurring issues due to the dim environment and the common use oflong exposure. Although Deblurring and Low-light Image Enhancement (LLIE) arerelated under these conditions, most approaches in image restoration solvethese tasks separately. In this paper, we present an efficient and robustneural network for multi-task low-light image restoration. Instead of followingthe current tendency of Transformer-based models, we propose new attentionmechanisms to enhance the receptive field of efficient CNNs. Our method reducesthe computational costs in terms of parameters and MAC operations compared toprevious methods. Our model, DarkIR, achieves new state-of-the-art results onthe popular LOLBlur, LOLv2 and Real-LOLBlur datasets, being able to generalizeon real-world night and dark images. Code and models athttps://github.com/cidautai/DarkIR</description><author>Daniel Feijoo, Juan C. Benito, Alvaro Garcia, Marcos V. Conde</author><pubDate>Fri, 31 Jan 2025 18:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13443v2</guid></item><item><title>s1: Simple test-time scaling</title><link>http://arxiv.org/abs/2501.19393v1</link><description>Test-time scaling is a promising new approach to language modeling that usesextra test-time compute to improve performance. Recently, OpenAI's o1 modelshowed this capability but did not publicly share its methodology, leading tomany replication efforts. We seek the simplest approach to achieve test-timescaling and strong reasoning performance. First, we curate a small dataset s1Kof 1,000 questions paired with reasoning traces relying on three criteria wevalidate through ablations: difficulty, diversity, and quality. Second, wedevelop budget forcing to control test-time compute by forcefully terminatingthe model's thinking process or lengthening it by appending "Wait" multipletimes to the model's generation when it tries to end. This can lead the modelto double-check its answer, often fixing incorrect reasoning steps. Aftersupervised finetuning the Qwen2.5-32B-Instruct language model on s1K andequipping it with budget forcing, our model s1 exceeds o1-preview oncompetition math questions by up to 27% (MATH and AIME24). Further, scaling s1with budget forcing allows extrapolating beyond its performance withouttest-time intervention: from 50% to 57% on AIME24. Our model, data, and codeare open-source at https://github.com/simplescaling/s1.</description><author>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand√®s, Tatsunori Hashimoto</author><pubDate>Fri, 31 Jan 2025 18:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19393v1</guid></item><item><title>Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models</title><link>http://arxiv.org/abs/2501.19392v1</link><description>Efficient real-world deployments of large language models (LLMs) rely onKey-Value (KV) caching for processing and generating long outputs, reducing theneed for repetitive computation. For large contexts, Key-Value caches can takeup tens of gigabytes of device memory, as they store vector representations foreach token and layer. Recent work has shown that the cached vectors can becompressed through quantization, pruning or merging, but these techniques oftencompromise quality towards higher compression rates. In this work, we aim toimprove Key &amp; Value compression by exploiting two observations: 1) the inherentdependencies between keys and values across different layers, and 2)high-compression mechanisms for internal network states. We propose AQUA-KV, anadaptive quantization for Key-Value caches that relies on compact adapters toexploit existing dependencies between Keys and Values, and aims to "optimally"compress the information that cannot be predicted. AQUA-KV significantlyimproves compression rates, while maintaining high accuracy on state-of-the-artLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5bits per value with under $1\%$ relative error in perplexity and LongBenchscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on asingle GPU within 1-6 hours, even for 70B models.</description><author>Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh</author><pubDate>Fri, 31 Jan 2025 18:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19392v1</guid></item><item><title>Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2501.19389v1</link><description>Fine-tuning large language models (LLMs) on devices is attracting increasinginterest. Recent works have fused low-rank adaptation (LoRA) techniques withfederated fine-tuning to mitigate challenges associated with device model sizesand data scarcity. Still, the heterogeneity of computational resources remainsa critical bottleneck: while higher-rank modules generally enhance performance,varying device capabilities constrain LoRA's feasible rank range. Existingapproaches attempting to resolve this issue either lack analyticaljustification or impose additional computational overhead, leaving a wide gapfor an efficient and theoretically-grounded solution. To address thesechallenges, we propose federated sketching LoRA (FSLoRA), which leverages asketching mechanism to enable devices to selectively update submatrices ofglobal LoRA modules maintained by the server. By adjusting the sketchingratios, which determine the ranks of the submatrices on the devices, FSLoRAflexibly adapts to device-specific communication and computational constraints.We provide a rigorous convergence analysis of FSLoRA that characterizes how thesketching ratios affect the convergence rate. Through comprehensive experimentson multiple datasets and LLM models, we demonstrate FSLoRA's superiorperformance compared to various baselines.</description><author>Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Seyyedali Hosseinalipour, Christopher G. Brinton</author><pubDate>Fri, 31 Jan 2025 18:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19389v1</guid></item><item><title>Learning multivariate Gaussians with imperfect advice</title><link>http://arxiv.org/abs/2411.12700v3</link><description>We revisit the problem of distribution learning within the framework oflearning-augmented algorithms. In this setting, we explore the scenario where aprobability distribution is provided as potentially inaccurate advice on thetrue, unknown distribution. Our objective is to develop learning algorithmswhose sample complexity decreases as the quality of the advice improves,thereby surpassing standard learning lower bounds when the advice issufficiently accurate. Specifically, we demonstrate that this outcome is achievable for the problemof learning a multivariate Gaussian distribution $N(\boldsymbol{\mu},\boldsymbol{\Sigma})$ in the PAC learning setting. Classically, in theadvice-free setting, $\tilde{\Theta}(d^2/\varepsilon^2)$ samples are sufficientand worst case necessary to learn $d$-dimensional Gaussians up to TV distance$\varepsilon$ with constant probability. When we are additionally given aparameter $\tilde{\boldsymbol{\Sigma}}$ as advice, we show that$\tilde{O}(d^{2-\beta}/\varepsilon^2)$ samples suffices whenever $\|\tilde{\boldsymbol{\Sigma}}^{-1/2} \boldsymbol{\Sigma}\tilde{\boldsymbol{\Sigma}}^{-1/2} - \boldsymbol{I_d} \|_1 \leq \varepsilond^{1-\beta}$ (where $\|\cdot\|_1$ denotes the entrywise $\ell_1$ norm) for any$\beta &gt; 0$, yielding a polynomial improvement over the advice-free setting.</description><author>Arnab Bhattacharyya, Davin Choo, Philips George John, Themis Gouleakis</author><pubDate>Fri, 31 Jan 2025 18:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12700v3</guid></item><item><title>Multi-Frame Blind Manifold Deconvolution for Rotating Synthetic Aperture Imaging</title><link>http://arxiv.org/abs/2501.19386v1</link><description>Rotating synthetic aperture (RSA) imaging system captures images of thetarget scene at different rotation angles by rotating a rectangular aperture.Deblurring acquired RSA images plays a critical role in reconstructing a latentsharp image underlying the scene. In the past decade, the emergence of blindconvolution technology has revolutionised this field by its ability to modelcomplex features from acquired images. Most of the existing methods attempt tosolve the above ill-posed inverse problem through maximising a posterior. Despite this progress, researchers have paid limited attention to exploringlow-dimensional manifold structures of the latent image within ahigh-dimensional ambient-space. Here, we propose a novel method to process RSAimages using manifold fitting and penalisation in the content of multi-frameblind convolution. We develop fast algorithms for implementing the proposedprocedure. Simulation studies demonstrate that manifold-based deconvolution canoutperform conventional deconvolution algorithms in the sense that it cangenerate a sharper estimate of the latent image in terms of estimating pixelintensities and preserving structural details.</description><author>Dao Lin, Jian Zhang, Martin Benning</author><pubDate>Fri, 31 Jan 2025 18:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19386v1</guid></item><item><title>SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders</title><link>http://arxiv.org/abs/2501.18052v2</link><description>Diffusion models, while powerful, can inadvertently generate harmful orundesirable content, raising significant ethical and safety concerns. Recentmachine unlearning approaches offer potential solutions but often lacktransparency, making it difficult to understand the changes they introduce tothe base model. In this work, we introduce SAeUron, a novel method leveragingfeatures learned by sparse autoencoders (SAEs) to remove unwanted concepts intext-to-image diffusion models. First, we demonstrate that SAEs, trained in anunsupervised manner on activations from multiple denoising timesteps of thediffusion model, capture sparse and interpretable features corresponding tospecific concepts. Building on this, we propose a feature selection method thatenables precise interventions on model activations to block targeted contentwhile preserving overall performance. Evaluation with the competitiveUnlearnCanvas benchmark on object and style unlearning highlights SAeUron'sstate-of-the-art performance. Moreover, we show that with a single SAE, we canremove multiple concepts simultaneously and that in contrast to other methods,SAeUron mitigates the possibility of generating unwanted content, even underadversarial attack. Code and checkpoints are available at:https://github.com/cywinski/SAeUron.</description><author>Bartosz Cywi≈Ñski, Kamil Deja</author><pubDate>Fri, 31 Jan 2025 18:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18052v2</guid></item><item><title>Decoding-based Regression</title><link>http://arxiv.org/abs/2501.19383v1</link><description>Language models have recently been shown capable of performing regressiontasks wherein numeric predictions are represented as decoded strings. In thiswork, we provide theoretical grounds for this capability and furthermoreinvestigate the utility of causal auto-regressive sequence models when they areapplied to any feature representation. We find that, despite being trained inthe usual way - for next-token prediction via cross-entropy loss -decoding-based regression is as performant as traditional approaches fortabular regression tasks, while being flexible enough to capture arbitrarydistributions, such as in the task of density estimation.</description><author>Xingyou Song, Dara Bahri</author><pubDate>Fri, 31 Jan 2025 18:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19383v1</guid></item><item><title>LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks</title><link>http://arxiv.org/abs/2501.19382v1</link><description>In this paper, we propose a novel loop closure detection algorithm that usesgraph attention neural networks to encode semantic graphs to perform placerecognition and then use semantic registration to estimate the 6 DoF relativepose constraint. Our place recognition algorithm has two key modules, namely, asemantic graph encoder module and a graph comparison module. The semantic graphencoder employs graph attention networks to efficiently encode spatial,semantic and geometric information from the semantic graph of the input pointcloud. We then use self-attention mechanism in both node-embedding andgraph-embedding steps to create distinctive graph vectors. The graph vectors ofthe current scan and a keyframe scan are then compared in the graph comparisonmodule to identify a possible loop closure. Specifically, employing thedifference of the two graph vectors showed a significant improvement inperformance, as shown in ablation studies. Lastly, we implemented a semanticregistration algorithm that takes in loop closure candidate scans and estimatesthe relative 6 DoF pose constraint for the LiDAR SLAM system. Extensiveevaluation on public datasets shows that our model is more accurate and robust,achieving 13% improvement in maximum F1 score on the SemanticKITTI dataset,when compared to the baseline semantic graph algorithm. For the benefit of thecommunity, we open-source the complete implementation of our proposed algorithmand custom implementation of semantic registration athttps://github.com/crepuscularlight/SemanticLoopClosure</description><author>Liudi Yang, Ruben Mascaro, Ignacio Alzugaray, Sai Manoj Prakhya, Marco Karrer, Ziyuan Liu, Margarita Chli</author><pubDate>Fri, 31 Jan 2025 18:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19382v1</guid></item><item><title>Using gradient of Lagrangian function to compute efficient channels for the ideal observer</title><link>http://arxiv.org/abs/2501.19381v1</link><description>It is widely accepted that the Bayesian ideal observer (IO) should be used toguide the objective assessment and optimization of medical imaging systems. TheIO employs complete task-specific information to compute test statistics formaking inference decisions and performs optimally in signal detection tasks.However, the IO test statistic typically depends non-linearly on the image dataand cannot be analytically determined. The ideal linear observer, known as theHotelling observer (HO), can sometimes be used as a surrogate for the IO.However, when image data are high dimensional, HO computation can be difficult.Efficient channels that can extract task-relevant features have beeninvestigated to reduce the dimensionality of image data to approximate IO andHO performance. This work proposes a novel method for generating efficientchannels by use of the gradient of a Lagrangian-based loss function that wasdesigned to learn the HO. The generated channels are referred to as theLagrangian-gradient (L-grad) channels. Numerical studies are conducted thatconsider binary signal detection tasks involving various backgrounds andsignals. It is demonstrated that channelized HO (CHO) using L-grad channels canproduce significantly better signal detection performance compared to the CHOusing PLS channels. Moreover, it is shown that the proposed L-grad method canachieve significantly lower computation time compared to the PLS method.</description><author>Weimin Zhou</author><pubDate>Fri, 31 Jan 2025 18:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19381v1</guid></item><item><title>TableMaster: A Recipe to Advance Table Understanding with Language Models</title><link>http://arxiv.org/abs/2501.19378v1</link><description>Tables serve as a fundamental format for representing structured relationaldata. While current language models (LMs) excel at many text-based tasks, theystill face challenges in table understanding due to the complex characteristicsof tabular data, such as their structured nature. In this paper, we aim toenhance LMs for improved table understanding. We identify four key challenges:1) difficulty in locating target data, 2) deficiency in table semantics, 3)numerical inaccuracies in textual reasoning, and 4) semantic inflexibility insymbolic reasoning. To address these issues, we propose TableMaster, a recipeand comprehensive framework that integrates multiple solutions to overcomethese obstacles. TableMaster first extracts relevant table content andverbalizes it with enriched semantic context. Additionally, we introduceadaptive reasoning, a flexible approach that dynamically adjusts betweentextual and symbolic reasoning, tailoring the reasoning process to each query.Extensive analyses and experiments demonstrate our findings and theeffectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves anaccuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.</description><author>Lang Cao</author><pubDate>Fri, 31 Jan 2025 18:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19378v1</guid></item><item><title>SELMA: A Speech-Enabled Language Model for Virtual Assistant Interactions</title><link>http://arxiv.org/abs/2501.19377v1</link><description>In this work, we present and evaluate SELMA, a Speech-Enabled Language Modelfor virtual Assistant interactions that integrates audio and text as inputs toa Large Language Model (LLM). SELMA is designed to handle three primary and twoauxiliary tasks related to interactions with virtual assistants simultaneouslywithin a single end-to-end model. We employ low-rank adaptation modules forparameter-efficient training of both the audio encoder and the LLM.Additionally, we implement a feature pooling strategy enabling the system torecognize global patterns and improve accuracy on tasks less reliant onindividual sequence elements. Experimental results on Voice Trigger (VT)detection, Device-Directed Speech Detection (DDSD), and Automatic SpeechRecognition (ASR), demonstrate that our approach both simplifies the typicalinput processing pipeline of virtual assistants significantly and also improvesperformance compared to dedicated models for each individual task. SELMA yieldsrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%on DDSD, while also achieving word error rates close to the baseline.</description><author>Dominik Wagner, Alexander Churchill, Siddarth Sigtia, Erik Marchi</author><pubDate>Fri, 31 Jan 2025 18:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19377v1</guid></item><item><title>Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function</title><link>http://arxiv.org/abs/2501.19374v1</link><description>Recent advancements in data-driven weather forecasting models have delivereddeterministic models that outperform the leading operational forecast systemsbased on traditional, physics-based models. However, these data-driven modelsare typically trained with a mean squared error loss function, which causessmoothing of fine scales through a "double penalty" effect. We develop asimple, parameter-free modification to this loss function that avoids thisproblem by separating the loss attributable to decorrelation from the lossattributable to spectral amplitude errors. Fine-tuning the GraphCast model withthis new loss function results in sharp deterministic weather forecasts, anincrease of the model's effective resolution from 1,250km to 160km,improvements to ensemble spread, and improvements to predictions of tropicalcyclone strength and surface wind extremes.</description><author>Christopher Subich, Syed Zahid Husain, Leo Separovic, Jing Yang</author><pubDate>Fri, 31 Jan 2025 18:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19374v1</guid></item><item><title>Beyond Fixed Horizons: A Theoretical Framework for Adaptive Denoising Diffusions</title><link>http://arxiv.org/abs/2501.19373v1</link><description>We introduce a new class of generative diffusion models that, unlikeconventional denoising diffusion models, achieve a time-homogeneous structurefor both the noising and denoising processes, allowing the number of steps toadaptively adjust based on the noise level. This is accomplished byconditioning the forward process using Doob's $h$-transform, which terminatesthe process at a suitable sampling distribution at a random time. The model isparticularly well suited for generating data with lower intrinsic dimensions,as the termination criterion simplifies to a first-hitting rule. A key featureof the model is its adaptability to the target data, enabling a variety ofdownstream tasks using a pre-trained unconditional generative model. Thesetasks include natural conditioning through appropriate initialization of thedenoising process and classification of noisy data.</description><author>S√∂ren Christensen, Claudia Strauch, Lukas Trottner</author><pubDate>Fri, 31 Jan 2025 18:23:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19373v1</guid></item><item><title>Communication- and Computation-Efficient Distributed Submodular Optimization in Robot Mesh Networks</title><link>http://arxiv.org/abs/2407.10382v2</link><description>We provide a communication- and computation-efficient method for distributedsubmodular optimization in robot mesh networks. Submodularity is a property ofdiminishing returns that arises in active information gathering such asmapping, surveillance, and target tracking. Our method, Resource-Awaredistributed Greedy (RAG), introduces a new distributed optimization paradigmthat enables scalable and near-optimal action coordination. To this end, RAGrequires each robot to make decisions based only on information received fromand about their neighbors. In contrast, the current paradigms allow the relayof information about all robots across the network. As a result, RAG'sdecision-time scales linearly with the network size, while state-of-the-artnear-optimal submodular optimization algorithms scale cubically. We alsocharacterize how the designed mesh-network topology affects RAG's approximationperformance. Our analysis implies that sparser networks favor scalabilitywithout proportionally compromising approximation performance: while RAG'sdecision time scales linearly with network size, the gain in approximationperformance scales sublinearly. We demonstrate RAG's performance in simulatedscenarios of area detection with up to 45 robots, simulating realisticrobot-to-robot (r2r) communication speeds such as the 0.25 Mbps speed of theDigi XBee 3 Zigbee 3.0. In the simulations, RAG enables real-time planning, upto three orders of magnitude faster than competitive near-optimal algorithms,while also achieving superior mean coverage performance. To enable thesimulations, we extend the high-fidelity and photo-realistic simulator AirSimby integrating a scalable collaborative autonomy pipeline to tens of robots andsimulating r2r communication delays. Our code is available athttps://github.com/UM-iRaL/Resource-Aware-Coordination-AirSim.</description><author>Zirui Xu, Sandilya Sai Garimella, Vasileios Tzoumas</author><pubDate>Fri, 31 Jan 2025 18:22:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10382v2</guid></item><item><title>GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to Filter Language Model Pretraining Data</title><link>http://arxiv.org/abs/2410.02755v3</link><description>Large language models require vast amounts of high-quality training data, buteffective filtering of web-scale datasets remains a significant challenge. Thispaper demonstrates that GPT-4o is remarkably effective at identifyinghigh-quality training data, but its prohibitive cost makes it impractical atweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4oaccuracy at less than 1\% of the cost. SIEVE can perform up to 500 filteringoperations for the cost of one GPT-4o filtering call. The key to SIEVE is aseamless integration of GPT-4o and lightweight text classification models,using active learning to fine-tune these models in the background with a smallnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at atiny fraction of the cost. Through different filtering prompts, SIEVE canefficiently curate high quality data for general or specialized domains fromweb-scale corpora -- a valuable capability given the current scarcity ofhigh-quality domain-specific datasets. Extensive experiments using automaticand human evaluation metrics show that SIEVE and GPT-4o achieve similarperformance on five highly specific filtering prompts. In addition, whenperforming quality filtering on web crawl datasets, we demonstrate SIEVE canfurther improve over state-of-the-art quality filtering methods in theDataComp-LM challenge for selecting LLM pretraining data.</description><author>Jifan Zhang, Ziyue Luo, Jia Liu, Ness Shroff, Robert Nowak</author><pubDate>Fri, 31 Jan 2025 18:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02755v3</guid></item><item><title>CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation</title><link>http://arxiv.org/abs/2501.19364v1</link><description>Multivariate Time Series Imputation (MTSI) is crucial for many applications,such as healthcare monitoring and traffic management, where incomplete data cancompromise decision-making. Existing state-of-the-art methods, like DenoisingDiffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;however, they suffer from significant computational costs and are notablytime-consuming due to their iterative nature. In this work, we propose CoSTI,an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTIemploys Consistency Training to achieve comparable imputation quality to DDPMswhile drastically reducing inference times, making it more suitable forreal-time applications. We evaluate CoSTI across multiple datasets and missingdata scenarios, demonstrating up to a 98% reduction in imputation time withperformance on par with diffusion-based models. This work bridges the gapbetween efficiency and accuracy in generative imputation tasks, providing ascalable solution for handling missing data in critical spatio-temporalsystems.</description><author>Javier Sol√≠s-Garc√≠a, Bel√©n Vega-M√°rquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro</author><pubDate>Fri, 31 Jan 2025 18:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19364v1</guid></item><item><title>CORAL: Concept Drift Representation Learning for Co-evolving Time-series</title><link>http://arxiv.org/abs/2501.01480v3</link><description>In the realm of time series analysis, tackling the phenomenon of conceptdrift poses a significant challenge. Concept drift -- characterized by theevolving statistical properties of time series data, affects the reliabilityand accuracy of conventional analysis models. This is particularly evident inco-evolving scenarios where interactions among variables are crucial. Thispaper presents CORAL, a simple yet effective method that models time series asan evolving ecosystem to learn representations of concept drift. CORAL employsa kernel-induced self-representation learning to generate a representationmatrix, encapsulating the inherent dynamics of co-evolving time series. Thismatrix serves as a key tool for identification and adaptation to concept driftby observing its temporal variations. Furthermore, CORAL effectively identifiesprevailing patterns and offers insights into emerging trends through patternevolution analysis. Our empirical evaluation of CORAL across various datasetsdemonstrates its effectiveness in handling the complexities of concept drift.This approach introduces a novel perspective in the theoretical domain ofco-evolving time series analysis, enhancing adaptability and accuracy in theface of dynamic data environments, and can be easily integrated into most deeplearning backbones.</description><author>Kunpeng Xu, Lifei Chen, Shengrui Wang</author><pubDate>Fri, 31 Jan 2025 18:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01480v3</guid></item><item><title>Loss shaping enhances exact gradient learning with Eventprop in spiking neural networks</title><link>http://arxiv.org/abs/2212.01232v3</link><description>Event-based machine learning promises more energy-efficient AI on futureneuromorphic hardware. Here, we investigate how the recently discoveredEventprop algorithm for gradient descent on exact gradients in spiking neuralnetworks can be scaled up to challenging keyword recognition benchmarks. Weimplemented Eventprop in the GPU-enhanced Neural Networks framework and used itfor training recurrent spiking neural networks on the Spiking Heidelberg Digitsand Spiking Speech Commands datasets. We found that learning depended stronglyon the loss function and extended Eventprop to a wider class of loss functionsto enable effective training. We then tested a large number of dataaugmentations and regularisations as well as exploring different networkstructures; and heterogeneous and trainable timescales. We found that whencombined with two specific augmentations, the right regularisation and a delayline input, Eventprop networks with one recurrent layer achievedstate-of-the-art performance on Spiking Heidelberg Digits and good accuracy onSpiking Speech Commands. In comparison to a leading surrogate-gradient-basedSNN training method, our GeNN Eventprop implementation is 3X faster and uses 4Xless memory. This work is a significant step towards a low-power neuromorphicalternative to current machine learning paradigms.</description><author>Thomas Nowotny, James P. Turner, James C. Knight</author><pubDate>Fri, 31 Jan 2025 18:12:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01232v3</guid></item><item><title>We're Different, We're the Same: Creative Homogeneity Across LLMs</title><link>http://arxiv.org/abs/2501.19361v1</link><description>Numerous powerful large language models (LLMs) are now available for use aswriting support tools, idea generators, and beyond. Although these LLMs aremarketed as helpful creative assistants, several works have shown that using anLLM as a creative partner results in a narrower set of creative outputs.However, these studies only consider the effects of interacting with a singleLLM, begging the question of whether such narrowed creativity stems from usinga particular LLM -- which arguably has a limited range of outputs -- or fromusing LLMs in general as creative assistants. To study this question, we elicitcreative responses from humans and a broad set of LLMs using standardizedcreativity tests and compare the population-level diversity of responses. Wefind that LLM responses are much more similar to other LLM responses than humanresponses are to each other, even after controlling for response structure andother key variables. This finding of significant homogeneity in creativeoutputs across the LLMs we evaluate adds a new dimension to the ongoingconversation about creativity and LLMs. If today's LLMs behave similarly, usingthem as a creative partners -- regardless of the model used -- may drive allusers towards a limited set of "creative" outputs.</description><author>Emily Wenger, Yoed Kenett</author><pubDate>Fri, 31 Jan 2025 18:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19361v1</guid></item><item><title>Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians</title><link>http://arxiv.org/abs/2501.09009v2</link><description>The foundation model (FM) paradigm is transforming Machine Learning ForceFields (MLFFs), leveraging general-purpose representations and scalabletraining to perform a variety of computational chemistry tasks. Although MLFFFMs have begun to close the accuracy gap relative to first-principles methods,there is still a strong need for faster inference speed. Additionally, whileresearch is increasingly focused on general-purpose models which transferacross chemical space, practitioners typically only study a small subset ofsystems at a given time. This underscores the need for fast, specialized MLFFsrelevant to specific downstream applications, which preserve test-time physicalsoundness while maintaining train-time scalability. In this work, we introducea method for transferring general-purpose representations from MLFF foundationmodels to smaller, faster MLFFs specialized to specific regions of chemicalspace. We formulate our approach as a knowledge distillation procedure, wherethe smaller "student" MLFF is trained to match the Hessians of the energypredictions of the "teacher" foundation model. Our specialized MLFFs can be upto 20 $\times$ faster than the original foundation model, while retaining, andin some cases exceeding, its performance and that of undistilled models. Wealso show that distilling from a teacher model with a direct forceparameterization into a student model trained with conservative forces (i.e.,computed as derivatives of the potential energy) successfully leverages therepresentations from the large-scale teacher for improved accuracy, whilemaintaining energy conservation during test-time molecular dynamicssimulations. More broadly, our work suggests a new paradigm for MLFFdevelopment, in which foundation models are released along with smaller,specialized simulation "engines" for common chemical subsets.</description><author>Ishan Amin, Sanjeev Raja, Aditi Krishnapriyan</author><pubDate>Fri, 31 Jan 2025 18:12:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09009v2</guid></item><item><title>The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking</title><link>http://arxiv.org/abs/2501.19358v1</link><description>This work identifies the Energy Loss Phenomenon in Reinforcement Learningfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,energy loss in the final layer of a Large Language Model (LLM) graduallyincreases during the RL process, with an excessive increase in energy losscharacterizing reward hacking. Beyond empirical analysis, we further provide atheoretical foundation by proving that, under mild conditions, the increasedenergy loss reduces the upper bound of contextual relevance in LLMs, which is acritical aspect of reward hacking as the reduced contextual relevance typicallyindicates overfitting to reward model-favored patterns in RL. To address thisissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes theincrease in energy loss in the LLM's final layer during reward calculation toprevent excessive energy loss, thereby mitigating reward hacking. Wetheoretically show that EPPO can be conceptually interpreted as anentropy-regularized RL algorithm, which provides deeper insights into itseffectiveness. Extensive experiments across various LLMs and tasks demonstratethe commonality of the energy loss phenomenon, as well as the effectiveness of\texttt{EPPO} in mitigating reward hacking and improving RLHF performance.</description><author>Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao</author><pubDate>Fri, 31 Jan 2025 18:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19358v1</guid></item><item><title>Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SCICAP Challenge 2023</title><link>http://arxiv.org/abs/2501.19353v1</link><description>Since the SCICAP datasets launch in 2021, the research community has madesignificant progress in generating captions for scientific figures in scholarlyarticles. In 2023, the first SCICAP Challenge took place, inviting global teamsto use an expanded SCICAP dataset to develop models for captioning diversefigure types across various academic fields. At the same time, text generationmodels advanced quickly, with many powerful pre-trained large multimodal models(LMMs) emerging that showed impressive capabilities in variousvision-and-language tasks. This paper presents an overview of the first SCICAPChallenge and details the performance of various models on its data, capturinga snapshot of the fields state. We found that professional editorsoverwhelmingly preferred figure captions generated by GPT-4V over those fromall other models and even the original captions written by authors. Followingthis key finding, we conducted detailed analyses to answer this question: Haveadvanced LMMs solved the task of generating captions for scientific figures?</description><author>Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang</author><pubDate>Fri, 31 Jan 2025 18:02:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19353v1</guid></item><item><title>SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior</title><link>http://arxiv.org/abs/2410.16665v2</link><description>The ideal AI safety moderation system would be both structurallyinterpretable (so its decisions can be reliably explained) and steerable (toalign to safety standards and reflect a community's values), which currentsystems fall short on. To address this gap, we present SafetyAnalyst, a novelAI safety moderation framework. Given an AI behavior, SafetyAnalyst useschain-of-thought reasoning to analyze its potential consequences by creating astructured "harm-benefit tree," which enumerates harmful and beneficial actionsand effects the AI behavior may lead to, along with likelihood, severity, andimmediacy labels that describe potential impact on any stakeholders.SafetyAnalyst then aggregates all harmful and beneficial effects into aharmfulness score using fully interpretable weight parameters, which can bealigned to particular safety preferences. We applied this conceptual frameworkto develop, test, and release an open-source LLM prompt safety classificationsystem, distilled from 18.5 million harm-benefit features generated by frontierLLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, weshow that SafetyReporter (average F1=0.81) outperforms existing LLM safetymoderation systems (average F1$&lt;$0.72) on prompt safety classification, whileoffering the additional advantages of interpretability, transparency, andsteerability.</description><author>Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine</author><pubDate>Fri, 31 Jan 2025 18:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16665v2</guid></item><item><title>Neural Implicit Solution Formula for Efficiently Solving Hamilton-Jacobi Equations</title><link>http://arxiv.org/abs/2501.19351v1</link><description>This paper presents an implicit solution formula for the Hamilton-Jacobipartial differential equation (HJ PDE). The formula is derived using the methodof characteristics and is shown to coincide with the Hopf and Lax formulas inthe case where either the Hamiltonian or the initial function is convex. Itprovides a simple and efficient numerical approach for computing the viscositysolution of HJ PDEs, bypassing the need for the Legendre transform of theHamiltonian or the initial condition, and the explicit computation ofindividual characteristic trajectories. A deep learning-based methodology isproposed to learn this implicit solution formula, leveraging the mesh-freenature of deep learning to ensure scalability for high-dimensional problems.Building upon this framework, an algorithm is developed that approximates thecharacteristic curves piecewise linearly for state-dependent Hamiltonians.Extensive experimental results demonstrate that the proposed method delivershighly accurate solutions, even for nonconvex Hamiltonians, and exhibitsremarkable scalability, achieving computational efficiency for problems up to40 dimensions.</description><author>Yesom Park, Stanley Osher</author><pubDate>Fri, 31 Jan 2025 17:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19351v1</guid></item><item><title>An All-digital 65-nm Tsetlin Machine Image Classification Accelerator with 8.6 nJ per MNIST Frame at 60.3k Frames per Second</title><link>http://arxiv.org/abs/2501.19347v1</link><description>We present an all-digital programmable machine learning accelerator chip forimage classification, underpinning on the Tsetlin machine (TM) principles. TheTM is a machine learning algorithm founded on propositional logic, utilizingsub-pattern recognition expressions called clauses. The accelerator implementsthe coalesced TM version with convolution, and classifies booleanized images of28$\times$28 pixels with 10 categories. A configuration with 128 clauses isused in a highly parallel architecture. Fast clause evaluation is obtained bykeeping all clause weights and Tsetlin automata (TA) action signals inregisters. The chip is implemented in a 65 nm low-leakage CMOS technology, andoccupies an active area of 2.7mm$^2$. At a clock frequency of 27.8 MHz, theaccelerator achieves 60.3k classifications per second, and consumes 8.6 nJ perclassification. The latency for classifying a single image is 25.4 $\mu$s whichincludes system timing overhead. The accelerator achieves 97.42%, 84.54% and82.55% test accuracies for the datasets MNIST, Fashion-MNIST andKuzushiji-MNIST, respectively, matching the TM software models.</description><author>Svein Anders Tunheim, Yujin Zheng, Lei Jiao, Rishad Shafik, Alex Yakovlev, Ole-Christoffer Granmo</author><pubDate>Fri, 31 Jan 2025 17:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19347v1</guid></item><item><title>Better Slow than Sorry: Introducing Positive Friction for Reliable Dialogue Systems</title><link>http://arxiv.org/abs/2501.17348v2</link><description>While theories of discourse and cognitive science have long recognized thevalue of unhurried pacing, recent dialogue research tends to minimize frictionin conversational systems. Yet, frictionless dialogue risks fosteringuncritical reliance on AI outputs, which can obscure implicit assumptions andlead to unintended consequences. To meet this challenge, we propose integratingpositive friction into conversational AI, which promotes user reflection ongoals, critical thinking on system response, and subsequent re-conditioning ofAI systems. We hypothesize systems can improve goal alignment, modeling of usermental states, and task success by deliberately slowing down conversations instrategic moments to ask questions, reveal assumptions, or pause. We present anontology of positive friction and collect expert human annotations onmulti-domain and embodied goal-oriented corpora. Experiments on these corpora,along with simulated interactions using state-of-the-art systems, suggestincorporating friction not only fosters accountable decision-making, but alsoenhances machine understanding of user beliefs and goals, and increases tasksuccess rates.</description><author>Mert ƒ∞nan, Anthony Sicilia, Suvodip Dey, Vardhan Dongre, Tejas Srinivasan, Jesse Thomason, G√∂khan T√ºr, Dilek Hakkani-T√ºr, Malihe Alikhani</author><pubDate>Fri, 31 Jan 2025 17:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.17348v2</guid></item><item><title>PUATE: Semiparametric Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</title><link>http://arxiv.org/abs/2501.19345v1</link><description>The estimation of average treatment effects (ATEs), defined as the differencein expected outcomes between treatment and control groups, is a central topicin causal inference. This study develops semiparametric efficient estimatorsfor ATE estimation in a setting where only a treatment group and an unknowngroup-comprising units for which it is unclear whether they received thetreatment or control-are observable. This scenario represents a variant oflearning from positive and unlabeled data (PU learning) and can be regarded asa special case of ATE estimation with missing data. For this setting, we derivesemiparametric efficiency bounds, which provide lower bounds on the asymptoticvariance of regular estimators. We then propose semiparametric efficient ATEestimators whose asymptotic variance aligns with these efficiency bounds. Ourfindings contribute to causal inference with missing data and weakly supervisedlearning.</description><author>Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi</author><pubDate>Fri, 31 Jan 2025 17:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19345v1</guid></item><item><title>Residual Connections Harm Generative Representation Learning</title><link>http://arxiv.org/abs/2404.10947v4</link><description>We show that introducing a weighting factor to reduce the influence ofidentity shortcuts in residual networks significantly enhances semantic featurelearning in generative representation learning frameworks, such as maskedautoencoders (MAEs) and diffusion models. Our modification notably improvesfeature quality, raising ImageNet-1K K-Nearest Neighbor accuracy from 27.4% to63.9% and linear probing accuracy from 67.8% to 72.7% for MAEs with a ViT-B/16backbone, while also enhancing generation quality in diffusion models. Thissignificant gap suggests that, while residual connection structure serves anessential role in facilitating gradient propagation, it may have a harmful sideeffect of reducing capacity for abstract learning by virtue of injecting anecho of shallower representations into deeper layers. We ameliorate thisdownside via a fixed formula for monotonically decreasing the contribution ofidentity connections as layer depth increases. Our design promotes the gradualdevelopment of feature abstractions, without impacting network trainability.Analyzing the representations learned by our modified residual networks, wefind correlation between low effective feature rank and downstream taskperformance.</description><author>Xiao Zhang, Ruoxi Jiang, William Gao, Rebecca Willett, Michael Maire</author><pubDate>Fri, 31 Jan 2025 17:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10947v4</guid></item><item><title>HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs</title><link>http://arxiv.org/abs/2312.14140v2</link><description>Current advances in human head modeling allow the generation ofplausible-looking 3D head models via neural representations, such as NeRFs andSDFs. Nevertheless, constructing complete high-fidelity head models withexplicitly controlled animation remains an issue. Furthermore, completing thehead geometry based on a partial observation, e.g., coming from a depth sensor,while preserving a high level of detail is often problematic for the existingmethods. We introduce a generative model for detailed 3D head meshes on top ofan articulated 3DMM, simultaneously allowing explicit animation and high-detailpreservation. Our method is trained in two stages. First, we register aparametric head model with vertex displacements to each mesh of the recentlyintroduced NPHM dataset of accurate 3D head scans. The estimated displacementsare baked into a hand-crafted UV layout. Second, we train a StyleGAN model togeneralize over the UV maps of displacements, which we later refer to asHeadCraft. The decomposition of the parametric model and high-quality vertexdisplacements allows us to animate the model and modify the regionssemantically. We demonstrate the results of unconditional sampling, fitting toa scan and editing. The project page is available athttps://seva100.github.io/headcraft.</description><author>Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner</author><pubDate>Fri, 31 Jan 2025 17:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14140v2</guid></item><item><title>Covering Multiple Objectives with a Small Set of Solutions Using Bayesian Optimization</title><link>http://arxiv.org/abs/2501.19342v1</link><description>In multi-objective black-box optimization, the goal is typically to findsolutions that optimize a set of T black-box objective functions, $f_1$, ...,$f_T$, simultaneously. Traditional approaches often seek a singlePareto-optimal set that balances trade-offs among all objectives. In this work,we introduce a novel problem setting that departs from this paradigm: finding asmaller set of K solutions, where K &lt; T, that collectively "covers" the Tobjectives. A set of solutions is defined as "covering" if, for each objective$f_1$, ..., $f_T$, there is at least one good solution. A motivating examplefor this problem setting occurs in drug design. For example, we may have Tpathogens and aim to identify a set of K &lt; T antibiotics such that at least oneantibiotic can be used to treat each pathogen. To address this problem, wepropose Multi-Objective Coverage Bayesian Optimization (MOCOBO), a principledalgorithm designed to efficiently find a covering set. We validate our approachthrough extensive experiments on challenging high-dimensional tasks, includingapplications in peptide and molecular design. Experiments demonstrate MOCOBO'sability to find high-performing covering sets of solutions. Additionally, weshow that the small sets of K &lt; T solutions found by MOCOBO can match or nearlymatch the performance of T individually optimized solutions for the sameobjectives. Our results highlight MOCOBO's potential to tackle complexmulti-objective problems in domains where finding at least one high-performingsolution for each objective is critical.</description><author>Natalie Maus, Kyurae Kim, Yimeng Zeng, Haydn Thomas Jones, Fangping Wan, Marcelo Der Torossian Torres, Cesar de la Fuente-Nunez, Jacob R. Gardner</author><pubDate>Fri, 31 Jan 2025 17:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19342v1</guid></item><item><title>PixelWorld: Towards Perceiving Everything as Pixels</title><link>http://arxiv.org/abs/2501.19339v1</link><description>Existing foundation models typically process visual input as pixels andtextual input as tokens, a paradigm that contrasts with human perception, whereboth modalities are processed in a unified manner. With the rise of embodiedand agentic AI, where inputs primarily come from camera pixels, the need for aunified perception framework becomes increasingly evident. In this paper, wepropose to unify all modalities (text, tables, code, diagrams, images, etc) aspixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introducePixelWorld, a novel evaluation suite that unifies all the mentioned modalitiesinto pixel space to gauge the existing models' performance. Our findings showthat (1) PEAP outperforms baseline with token-based input in multimodaldatasets, benefiting from unified input for better disambiguation, (2)significant declines in reasoning and coding capabilities across all modelswhen processing pixel-based input, underscoring the need to enhance foundationmodels' perceptual abilities, (3) larger models can maintain strong performanceon non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffersignificant performance degradation, (4) the attention pattern of PEAP ishighly aligned with text token input, (5) PEAP can be accelerated significantlyby exploiting the spatial sparsity. We conclude that the existing frontiermodels are competent in pixel perception, however, there is still headroom forimprovement. Our code, dataset will be released upon acceptance.</description><author>Zhiheng Lyu, Xueguang Ma, Wenhu Chen</author><pubDate>Fri, 31 Jan 2025 17:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19339v1</guid></item><item><title>FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing</title><link>http://arxiv.org/abs/2501.14713v2</link><description>The rapid proliferation of large language models (LLMs) in natural languageprocessing (NLP) has created a critical need for techniques that enableefficient deployment on memory-constrained devices without compromisingperformance. We present a method to prune LLMs that selectively prunes modelblocks based on an importance score and replaces them with a low-parameterreplacement strategy. Specifically, we propose a principled metric to replaceeach pruned block using a weight-sharing mechanism that leverages unprunedcounterparts from the model and block-specific low-rank adapters. Furthermore,we facilitate the learning of these replacement blocks with output featurenormalization and an adapter initialization scheme built on low-rank SVDreconstructions. Empirical evaluations demonstrate substantial performancegains over existing methods, achieving state-of-the-art performance on 5/6benchmarks for a compression rate of 30% and 6/6 benchmarks for a compressionrate of 40%. We also demonstrate that our approach can extend smaller models,boosting performance on 6/6 benchmarks using only ~0.3% tokens of extendedtraining with minimal additional parameter costs.</description><author>James Seale Smith, Chi-Heng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, Yen-Chang Hsu</author><pubDate>Fri, 31 Jan 2025 17:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.14713v2</guid></item><item><title>Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates</title><link>http://arxiv.org/abs/2501.19338v1</link><description>Developing new methods for the automated analysis of clinical fetal andneonatal MRI data is limited by the scarcity of annotated pathological datasetsand privacy concerns that often restrict data sharing, hindering theeffectiveness of deep learning models. We address this in two ways. First, weintroduce Fetal&amp;Neonatal-DDPM, a novel diffusion model framework designed togenerate high-quality synthetic pathological fetal and neonatal MRIs fromsemantic label images. Second, we enhance training data by modifying healthylabel images through morphological alterations to simulate conditions such asventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.By leveraging Fetal&amp;Neonatal-DDPM, we synthesize realistic pathological MRIsfrom these modified pathological label images. Radiologists rated the syntheticMRIs as significantly (p &lt; 0.05) superior in quality and diagnostic valuecompared to real MRIs, demonstrating features such as blood vessels and choroidplexus, and improved alignment with label annotations. Synthetic pathologicaldata enhanced state-of-the-art nnUNet segmentation performance, particularlyfor severe ventriculomegaly cases, with the greatest improvements achieved inventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscoresthe potential of generative AI as transformative tool for data augmentation,offering improved segmentation performance in pathological cases. Thisdevelopment represents a significant step towards improving analysis andsegmentation accuracy in prenatal imaging, and also offers new ways for dataanonymization through the generation of pathologic image data.</description><author>Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente Istv√°n L√°nczi, Andras Jakab</author><pubDate>Fri, 31 Jan 2025 17:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19338v1</guid></item><item><title>Homogeneity Bias as Differential Sampling Uncertainty in Language Models</title><link>http://arxiv.org/abs/2501.19337v1</link><description>Prior research show that Large Language Models (LLMs) and Vision-LanguageModels (VLMs) represent marginalized groups more homogeneously than dominantgroups. However, the mechanisms underlying this homogeneity bias remainrelatively unexplored. We propose that this bias emerges from systematicdifferences in the probability distributions from which tokens are sampled atinference-time. Analyzing three measures of uncertainty in token samplingdistributions-entropy, perplexity, and probability of differentiation-we findthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampledmore deterministically when generating texts about marginalized groups (i.e.,Black Americans and women) compared to their dominant group counterparts (i.e.,White Americans and men). While these findings may help explain homogeneitybias in certain models, the patterns did not replicate across all VLMs tested,suggesting multiple mechanisms may contribute to homogeneity bias in AI.</description><author>Messi H. J. Lee, Soyeon Jeon</author><pubDate>Fri, 31 Jan 2025 17:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19337v1</guid></item><item><title>FACTTRACK: Time-Aware World State Tracking in Story Outlines</title><link>http://arxiv.org/abs/2407.16347v2</link><description>While accurately detecting and correcting factual contradictions in languagemodel outputs has become increasingly important as their capabilities improve,doing so is highly challenging. We propose a novel method, FACTTRACK, fortracking atomic facts and addressing factual contradictions. Crucially,FACTTRACK also maintains time-aware validity intervals for each fact, allowingfor change over time. At a high level, FACTTRACK consists of a four-steppipeline to update a world state data structure for each new event: (1)decompose the event into directional atomic facts; (2) determine the validityinterval of each atomic fact using the world state; (3) detect contradictionswith existing facts in the world state; and finally (4) add new facts to theworld state and update existing atomic facts. When we apply FACTTRACK tocontradiction detection on structured story outlines, we find that FACTTRACKusing LLaMA2-7B-Chat substantially outperforms a fair baseline usingLLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline.Moreover, when using GPT4, FACTTRACK significantly outperforms the GPT4baseline.</description><author>Zhiheng Lyu, Kevin Yang, Lingpeng Kong, Daniel Klein</author><pubDate>Fri, 31 Jan 2025 17:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16347v2</guid></item><item><title>What is causal about causal models and representations?</title><link>http://arxiv.org/abs/2501.19335v1</link><description>Causal Bayesian networks are 'causal' models since they make predictionsabout interventional distributions. To connect such causal model predictions toreal-world outcomes, we must determine which actions in the world correspond towhich interventions in the model. For example, to interpret an action as anintervention on a treatment variable, the action will presumably have to a)change the distribution of treatment in a way that corresponds to theintervention, and b) not change other aspects, such as how the outcome dependson the treatment; while the marginal distributions of some variables may changeas an effect. We introduce a formal framework to make such requirements fordifferent interpretations of actions as interventions precise. We prove thatthe seemingly natural interpretation of actions as interventions is circular:Under this interpretation, every causal Bayesian network that correctly modelsthe observational distribution is trivially also interventionally valid, and noaction yields empirical data that could possibly falsify such a model. We provean impossibility result: No interpretation exists that is non-circular andsimultaneously satisfies a set of natural desiderata. Instead, we examinenon-circular interpretations that may violate some desiderata and show how thismay in turn enable the falsification of causal models. By rigorously examininghow a causal Bayesian network could be a 'causal' model of the world instead ofmerely a mathematical object, our formal framework contributes to theconceptual foundations of causal representation learning, causal discovery, andcausal abstraction, while also highlighting some limitations of existingapproaches.</description><author>Frederik Hytting J√∏rgensen, Luigi Gresele, Sebastian Weichwald</author><pubDate>Fri, 31 Jan 2025 17:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19335v1</guid></item><item><title>Wearable Accelerometer Foundation Models for Health via Knowledge Distillation</title><link>http://arxiv.org/abs/2412.11276v2</link><description>Modern wearable devices can conveniently record various biosignals in themany different environments of daily living, enabling a rich view of individualhealth. However, not all biosignals are the same: high-fidelity biosignals,such as photoplethysmogram (PPG), contain more physiological information, butrequire optical sensors with a high power footprint. Alternatively, alower-fidelity biosignal such as accelerometry has a significantly smallerpower footprint and is available in almost any wearable device. Whileaccelerometry is widely used for activity recognition and fitness, it is lessexplored for health biomarkers and diagnosis. Here, we show that anaccelerometry foundation model can predict a wide variety of health targets. Toachieve improved performance, we distill representational knowledge from PPGencoders to accelerometery encoders using 20 million minutes of unlabeled data,collected from ~172K participants in the Apple Heart and Movement Study underinformed consent. We observe strong cross-modal alignment on unseen data, e.g.,99.2% top-1 accuracy for retrieving PPG embeddings from accelerometryembeddings. We show that distilled accelerometry encoders have significantlymore informative representations compared to self-supervised or supervisedencoders trained directly on accelerometry data, observed by at least 23%-49%improved performance for predicting heart rate and heart rate variability. Wealso show that distilled accelerometry encoders are readily predictive of awide array of downstream health targets, i.e., they are generalist foundationmodels. We believe accelerometry foundation models for health may unlock newopportunities for developing digital biomarkers from any wearable device.</description><author>Salar Abbaspourazad, Anshuman Mishra, Joseph Futoma, Andrew C. Miller, Ian Shapiro</author><pubDate>Fri, 31 Jan 2025 17:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11276v2</guid></item><item><title>The Value of Prediction in Identifying the Worst-Off</title><link>http://arxiv.org/abs/2501.19334v1</link><description>Machine learning is increasingly used in government programs to identify andsupport the most vulnerable individuals, prioritizing assistance for those atgreatest risk over optimizing aggregate outcomes. This paper examines thewelfare impacts of prediction in equity-driven contexts, and how they compareto other policy levers, such as expanding bureaucratic capacity. Throughmathematical models and a real-world case study on long-term unemploymentamongst German residents, we develop a comprehensive understanding of therelative effectiveness of prediction in surfacing the worst-off. Our findingsprovide clear analytical frameworks and practical, data-driven tools thatempower policymakers to make principled decisions when designing these systems.</description><author>Unai Fischer-Abaigar, Christoph Kern, Juan Carlos Perdomo</author><pubDate>Fri, 31 Jan 2025 17:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19334v1</guid></item><item><title>Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection</title><link>http://arxiv.org/abs/2411.15633v2</link><description>AI-generated images (AIGIs), such as natural or face images, have becomeincreasingly realistic and indistinguishable, making their detection a criticaland pressing challenge. In this paper, we start from a new perspective toexcavate the reason behind the failure generalization in AIGI detection, namedthe \textit{asymmetry phenomenon}, where a naively trained detector tends tofavor overfitting to the limited and monotonous fake patterns, causing thefeature space to become highly constrained and low-ranked, which is provedseriously limiting the expressivity and generalization. One potential remedy isincorporating the pre-trained knowledge within the vision foundation models(higher-ranked) to expand the feature space, alleviating the model'soverfitting to fake. To this end, we employ Singular Value Decomposition (SVD)to decompose the original feature space into two orthogonal subspaces. Byfreezing the principal components and adapting only the remained components, wepreserve the pre-trained knowledge while learning forgery-related patterns.Compared to existing full-parameters and LoRA-based tuning methods, weexplicitly ensure orthogonality enabling the higher rank of the whole featurespace, effectively minimizing overfitting and enhancing generalization.Extensive experiments with our deep analysis on both deepfake and syntheticimage detection benchmarks demonstrate superior generalization performance indetection.</description><author>Zhiyuan Yan, Jiangming Wang, Peng Jin, Ke-Yue Zhang, Chengchun Liu, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, Li Yuan</author><pubDate>Fri, 31 Jan 2025 17:31:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15633v2</guid></item><item><title>Consistent Video Colorization via Palette Guidance</title><link>http://arxiv.org/abs/2501.19331v1</link><description>Colorization is a traditional computer vision task and it plays an importantrole in many time-consuming tasks, such as old film restoration. Existingmethods suffer from unsaturated color and temporally inconsistency. In thispaper, we propose a novel pipeline to overcome the challenges. We regard thecolorization task as a generative task and introduce Stable Video Diffusion(SVD) as our base model. We design a palette-based color guider to assist themodel in generating vivid and consistent colors. The color context introducedby the palette not only provides guidance for color generation, but alsoenhances the stability of the generated colors through a unified color contextacross multiple sequences. Experiments demonstrate that the proposed method canprovide vivid and stable colors for videos, surpassing previous methods.</description><author>Han Wang, Yuang Zhang, Yuhong Zhang, Lingxiao Lu, Li Song</author><pubDate>Fri, 31 Jan 2025 17:31:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19331v1</guid></item><item><title>Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient</title><link>http://arxiv.org/abs/2410.08893v2</link><description>Model-based reinforcement learning (RL) offers a solution to the datainefficiency that plagues most model-free RL algorithms. However, learning arobust world model often demands complex and deep architectures, which areexpensive to compute and train. Within the world model, dynamics models areparticularly crucial for accurate predictions, and various dynamics-modelarchitectures have been explored, each with its own set of challenges.Currently, recurrent neural network (RNN) based world models face issues suchas vanishing gradients and difficulty in capturing long-term dependencieseffectively. In contrast, use of transformers suffers from the well-knownissues of self-attention mechanisms, where both memory and computationalcomplexity scale as $O(n^2)$, with $n$ representing the sequence length. To address these challenges we propose a state space model (SSM) based worldmodel, specifically based on Mamba, that achieves $O(n)$ memory andcomputational complexity while effectively capturing long-term dependencies andfacilitating the use of longer training sequences efficiently. We alsointroduce a novel sampling method to mitigate the suboptimality caused by anincorrect world model in the early stages of training, combining it with theaforementioned technique to achieve a normalised score comparable to otherstate-of-the-art model-based RL algorithms using only a 7 million trainableparameter world model. This model is accessible and can be trained on anoff-the-shelf laptop. Our code is available athttps://github.com/realwenlongwang/Drama.git</description><author>Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill</author><pubDate>Fri, 31 Jan 2025 17:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08893v2</guid></item><item><title>CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control</title><link>http://arxiv.org/abs/2501.06006v2</link><description>We propose a method for generating fly-through videos of a scene, from asingle image and a given camera trajectory. We build upon an image-to-videolatent diffusion model. We condition its UNet denoiser on the cameratrajectory, using four techniques. (1) We condition the UNet's temporal blockson raw camera extrinsics, similar to MotionCtrl. (2) We use images containingcamera rays and directions, similar to CameraCtrl. (3) We reproject the initialimage to subsequent frames and use the resulting video as a condition. (4) Weuse 2D&lt;=&gt;3D transformers to introduce a global 3D representation, whichimplicitly conditions on the camera poses. We combine all conditions in aContolNet-style architecture. We then propose a metric that evaluates overallvideo quality and the ability to preserve details with view changes, which weuse to analyze the trade-offs of individual and combined conditions. Finally,we identify an optimal combination of conditions. We calibrate camera positionsin our datasets for scale consistency across scenes, and we train our sceneexploration model, CamCtrl3D, demonstrating state-of-theart results.</description><author>Stefan Popov, Amit Raj, Michael Krainin, Yuanzhen Li, William T. Freeman, Michael Rubinstein</author><pubDate>Fri, 31 Jan 2025 17:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06006v2</guid></item><item><title>Let Human Sketches Help: Empowering Challenging Image Segmentation Task with Freehand Sketches</title><link>http://arxiv.org/abs/2501.19329v1</link><description>Sketches, with their expressive potential, allow humans to convey the essenceof an object through even a rough contour. For the first time, we harness thisexpressive potential to improve segmentation performance in challenging taskslike camouflaged object detection (COD). Our approach introduces an innovativesketch-guided interactive segmentation framework, allowing users to intuitivelyannotate objects with freehand sketches (drawing a rough contour of the object)instead of the traditional bounding boxes or points used in classic interactivesegmentation models like SAM. We demonstrate that sketch input cansignificantly improve performance in existing iterative segmentation methods,outperforming text or bounding box annotations. Additionally, we introduce keymodifications to network architectures and a novel sketch augmentationtechnique to fully harness the power of sketch input and further boostsegmentation accuracy. Remarkably, our model' s output can be directly used totrain other neural networks, achieving results comparable to pixel-by-pixelannotations--while reducing annotation time by up to 120 times, which showsgreat potential in democratizing the annotation process and enabling modeltraining with less reliance on resource-intensive, laborious pixel-levelannotations. We also present KOSCamo+, the first freehand sketch dataset forcamouflaged object detection. The dataset, code, and the labeling tool will beopen sourced.</description><author>Ying Zang, Runlong Cao, Jianqi Zhang, Yidong Han, Ziyue Cao, Wenjun Hu, Didi Zhu, Lanyun Zhu, Zejian Li, Deyi Ji, Tianrun Chen</author><pubDate>Fri, 31 Jan 2025 17:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19329v1</guid></item><item><title>From Natural Language to Extensive-Form Game Representations</title><link>http://arxiv.org/abs/2501.17282v3</link><description>We introduce a framework for translating game descriptions in naturallanguage into extensive-form representations in game theory, leveraging LargeLanguage Models (LLMs) and in-context learning. Given the varying levels ofstrategic complexity in games, such as perfect versus imperfect information,directly applying in-context learning would be insufficient. To address this,we introduce a two-stage framework with specialized modules to enhancein-context learning, enabling it to divide and conquer the problem effectively.In the first stage, we tackle the challenge of imperfect information bydeveloping a module that identifies information sets along and thecorresponding partial tree structure. With this information, the second stageleverages in-context learning alongside a self-debugging module to produce acomplete extensive-form game tree represented using pygambit, the Python API ofa recognized game-theoretic analysis tool called Gambit. Using this pythonrepresentation enables the automation of tasks such as computing Nashequilibria directly from natural language descriptions. We evaluate theperformance of the full framework, as well as its individual components, usingvarious LLMs on games with different levels of strategic complexity. Ourexperimental results show that the framework significantly outperforms baselinemodels in generating accurate extensive-form games, with each module playing acritical role in its success.</description><author>Shilong Deng, Yongzhao Wang, Rahul Savani</author><pubDate>Fri, 31 Jan 2025 17:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.17282v3</guid></item><item><title>Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation</title><link>http://arxiv.org/abs/2501.19328v1</link><description>With the rise in global greenhouse gas emissions, accurate large-scale treecanopy height maps are essential for understanding forest structure, estimatingabove-ground biomass, and monitoring ecological disruptions. To this end, wepresent a novel approach to generate large-scale, high-resolution canopy heightmaps over time. Our model accurately predicts canopy height over multiple yearsgiven Sentinel-2 time series satellite data. Using GEDI LiDAR data as theground truth for training the model, we present the first 10m resolutiontemporal canopy height map of the European continent for the period 2019-2022.As part of this product, we also offer a detailed canopy height map for 2020,providing more precise estimates than previous studies. Our pipeline and theresulting temporal height map are publicly available, enabling comprehensivelarge-scale monitoring of forests and, hence, facilitating future research andecological analyses. For an interactive viewer, seehttps://europetreemap.projects.earthengine.app/view/temporalcanopyheight.</description><author>Jan Pauls, Max Zimmer, Berkant Turan, Sassan Saatchi, Philippe Ciais, Sebastian Pokutta, Fabian Gieseke</author><pubDate>Fri, 31 Jan 2025 17:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19328v1</guid></item><item><title>A Generic Hybrid Framework for 2D Visual Reconstruction</title><link>http://arxiv.org/abs/2501.19325v1</link><description>This paper presents a versatile hybrid framework for addressing 2D real-worldreconstruction tasks formulated as jigsaw puzzle problems (JPPs) with square,non-overlapping pieces. Our approach integrates a deep learning (DL)-basedcompatibility measure (CM) model that evaluates pairs of puzzle piecesholistically, rather than focusing solely on their adjacent edges astraditionally done. This DL-based CM is paired with an optimized geneticalgorithm (GA)-based solver, which iteratively searches for a global optimalarrangement using the pairwise CM scores of the puzzle pieces. Extensiveexperimental results highlight the framework's adaptability and robustnessacross multiple real-world domains. Notably, our unique hybrid methodologyachieves state-of-the-art (SOTA) results in reconstructing Portuguese tilepanels and large degraded puzzles with eroded boundaries.</description><author>Daniel Rika, Dror Sholomon, Eli David, Alexandre Pais, Nathan S. Netanyahu</author><pubDate>Fri, 31 Jan 2025 17:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19325v1</guid></item><item><title>Reward-Guided Speculative Decoding for Efficient LLM Reasoning</title><link>http://arxiv.org/abs/2501.19324v1</link><description>We introduce Reward-Guided Speculative Decoding (RSD), a novel frameworkaimed at improving the efficiency of inference in large language models (LLMs).RSD synergistically combines a lightweight draft model with a more powerfultarget model, incorporating a controlled bias to prioritize high-rewardoutputs, in contrast to existing speculative decoding methods that enforcestrict unbiasedness. RSD employs a process reward model to evaluateintermediate decoding steps and dynamically decide whether to invoke the targetmodel, optimizing the trade-off between computational cost and output quality.We theoretically demonstrate that a threshold-based mixture strategy achievesan optimal balance between resource utilization and performance. Extensiveevaluations on challenging reasoning benchmarks, including Olympiad-leveltasks, show that RSD delivers significant efficiency gains against decodingwith the target model only (up to 4.4x fewer FLOPs), while achievingsignificant better accuracy than parallel decoding method on average (up to+3.5). These results highlight RSD as a robust and cost-effective approach fordeploying LLMs in resource-intensive scenarios.</description><author>Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong</author><pubDate>Fri, 31 Jan 2025 17:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19324v1</guid></item><item><title>Language Bias in Self-Supervised Learning For Automatic Speech Recognition</title><link>http://arxiv.org/abs/2501.19321v1</link><description>Self-supervised learning (SSL) is used in deep learning to train on largedatasets without the need for expensive labelling of the data. Recently, largeAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL totrain on over one hundred different languages simultaneously. However, deeperinvestigation shows that the bulk of the training data for XLS-R comes from asmall number of languages. Biases learned through SSL have been shown to existin multiple domains, but language bias in multilingual SSL ASR has not beenthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis(LTH) to identify language-specific subnetworks within XLS-R and test theperformance of these subnetworks on a variety of different languages. We areable to show that when fine-tuning, XLS-R bypasses traditional linguisticknowledge and builds only on weights learned from the languages with thelargest data contribution to the pretraining data.</description><author>Edward Storey, Naomi Harte, Peter Bell</author><pubDate>Fri, 31 Jan 2025 17:16:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19321v1</guid></item><item><title>Random features and polynomial rules</title><link>http://arxiv.org/abs/2402.10164v2</link><description>Random features models play a distinguished role in the theory of deeplearning, describing the behavior of neural networks close to theirinfinite-width limit. In this work, we present a thorough analysis of thegeneralization performance of random features models for generic supervisedlearning problems with Gaussian data. Our approach, built with tools from thestatistical mechanics of disordered systems, maps the random features model toan equivalent polynomial model, and allows us to plot average generalizationcurves as functions of the two main control parameters of the problem: thenumber of random features $N$ and the size $P$ of the training set, bothassumed to scale as powers in the input dimension $D$. Our results extend thecase of proportional scaling between $N$, $P$ and $D$. They are in accordancewith rigorous bounds known for certain particular learning tasks and are inquantitative agreement with numerical experiments performed over many order ofmagnitudes of $N$ and $P$. We find good agreement also far from the asymptoticlimits where $D\to \infty$ and at least one between $P/D^K$, $N/D^L$ remainsfinite.</description><author>Fabi√°n Aguirre-L√≥pez, Silvio Franz, Mauro Pastore</author><pubDate>Fri, 31 Jan 2025 17:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10164v2</guid></item><item><title>Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping</title><link>http://arxiv.org/abs/2501.19319v1</link><description>Simultaneous Localization and Mapping (SLAM) is essential for precisesurgical interventions and robotic tasks in minimally invasive procedures.While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAMwith high-quality novel view synthesis and fast rendering, these systemsstruggle with accurate depth and surface reconstruction due to multi-viewinconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches betweenthe reconstructed frames. In this work, we present Endo-2DTAM, a real-timeendoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address thesechallenges. Endo-2DTAM incorporates a surface normal-aware pipeline, whichconsists of tracking, mapping, and bundle adjustment modules for geometricallyaccurate reconstruction. Our robust tracking module combines point-to-point andpoint-to-plane distance metrics, while the mapping module utilizes normalconsistency and depth distortion to enhance surface reconstruction quality. Wealso introduce a pose-consistent strategy for efficient and geometricallycoherent keyframe sampling. Extensive experiments on public endoscopic datasetsdemonstrate that Endo-2DTAM achieves an RMSE of $1.87\pm 0.63$ mm for depthreconstruction of surgical scenes while maintaining computationally efficienttracking, high-quality visual appearance, and real-time rendering. Our codewill be released at github.com/lastbasket/Endo-2DTAM.</description><author>Yiming Huang, Beilei Cui, Long Bai, Zhen Chen, Jinlin Wu, Zhen Li, Hongbin Liu, Hongliang Ren</author><pubDate>Fri, 31 Jan 2025 17:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19319v1</guid></item><item><title>MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems</title><link>http://arxiv.org/abs/2501.19318v1</link><description>While large language models (LLMs) have shown promising capabilities aszero-shot planners for embodied agents, their inability to learn fromexperience and build persistent mental models limits their robustness incomplex open-world environments like Minecraft. We introduce MINDSTORES, anexperience-augmented planning framework that enables embodied agents to buildand leverage mental models through natural interaction with their environment.Drawing inspiration from how humans construct and refine cognitive mentalmodels, our approach extends existing zero-shot LLM planning by maintaining adatabase of past experiences that informs future planning iterations. The keyinnovation is representing accumulated experiences as natural languageembeddings of (state, task, plan, outcome) tuples, which can then beefficiently retrieved and reasoned over by an LLM planner to generate insightsand guide plan refinement for novel states and tasks. Through extensiveexperiments in the MineDojo environment, a simulation environment for agents inMinecraft that provides low-level controls for Minecraft, we find thatMINDSTORES learns and applies its knowledge significantly better than existingmemory-based LLM planners while maintaining the flexibility and generalizationbenefits of zero-shot approaches, representing an important step toward morecapable embodied AI systems that can learn continuously through naturalexperience.</description><author>Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou</author><pubDate>Fri, 31 Jan 2025 17:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19318v1</guid></item><item><title>LLM-based Affective Text Generation Quality Based on Different Quantization Values</title><link>http://arxiv.org/abs/2501.19317v1</link><description>Large language models exhibit a remarkable capacity in language generationand comprehension. These advances enable AI systems to produce more human-likeand emotionally engaging text. However, these models rely on a large number ofparameters, requiring significant computational resources for training andinference. In some scenarios, accessing these resources can be challenging(e.g., budget or hardware limitations). Techniques like reducing precision bitscan make models more memory-efficient, reducing the computational resourcesneeded, at the cost of reduced accuracy. This paper addresses the trade-offbetween different quantization values, GPU RAM utilization, and text quality inaffective text generation (e.g., "I really enjoy running in the snow-coveredforest"). To evaluate, we use an emotion classifier and ten seed prompts togenerate affective text. We test three setups of precision bits (8, 16, and 32)across five open-weight language models from two different families. Ourfindings demonstrate that bit reductions lead to memory savings, achieving areduction of 76%. However, this optimization comes with a trade-off, leading toa decrease of up to 10 pp in F1 score for larger models and an increase of 10pp for smaller models, along with roughly double the inference time. In termsof text quality, larger models at lower quantization levels generallyoutperform smaller, higher-precision models -- while requiring similar memory.</description><author>Yarik Menchaca Resendiz, Roman Klinger</author><pubDate>Fri, 31 Jan 2025 17:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19317v1</guid></item><item><title>Reverse Probing: Evaluating Knowledge Transfer via Finetuned Task Embeddings for Coreference Resolution</title><link>http://arxiv.org/abs/2501.19316v1</link><description>In this work, we reimagine classical probing to evaluate knowledge transferfrom simple source to more complex target tasks. Instead of probing frozenrepresentations from a complex source task on diverse simple target probingtasks (as usually done in probing), we explore the effectiveness of embeddingsfrom multiple simple source tasks on a single target task. We selectcoreference resolution, a linguistically complex problem requiring contextualunderstanding, as focus target task, and test the usefulness of embeddings fromcomparably simpler tasks tasks such as paraphrase detection, named entityrecognition, and relation extraction. Through systematic experiments, weevaluate the impact of individual and combined task embeddings. Our findings reveal that task embeddings vary significantly in utility forcoreference resolution, with semantic similarity tasks (e.g., paraphrasedetection) proving most beneficial. Additionally, representations fromintermediate layers of fine-tuned models often outperform those from finallayers. Combining embeddings from multiple tasks consistently improvesperformance, with attention-based aggregation yielding substantial gains. Theseinsights shed light on relationships between task-specific representations andtheir adaptability to complex downstream tasks, encouraging further explorationof embedding-level task transfer.</description><author>Tatiana Anikina, Arne Binder, David Harbecke, Stalin Varanasi, Leonhard Hennig, Simon Ostermann, Sebastian M√∂ller, Josef van Genabith</author><pubDate>Fri, 31 Jan 2025 17:12:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19316v1</guid></item><item><title>Towards Universal Certified Robustness with Multi-Norm Training</title><link>http://arxiv.org/abs/2410.03000v2</link><description>Existing certified training methods can only train models to be robustagainst a certain perturbation type (e.g. $l_\infty$ or $l_2$). However, an$l_\infty$ certifiably robust model may not be certifiably robust against $l_2$perturbation (and vice versa) and also has low robustness against otherperturbations (e.g. geometric and patch transformation). By constructing atheoretical framework to analyze and mitigate the tradeoff, we propose thefirst multi-norm certified training framework \textbf{CURE}, consisting ofseveral multi-norm certified training methods, to attain better \emph{unionrobustness} when training from scratch or fine-tuning a pre-trained certifiedmodel. Inspired by our theoretical findings, we devise bound alignment andconnect natural training with certified training for better union robustness.Compared with SOTA-certified training, \textbf{CURE} improves union robustnessto $32.0\%$ on MNIST, $25.8\%$ on CIFAR-10, and $10.6\%$ on TinyImagenet acrossdifferent epsilon values. It leads to better generalization on a diverse set ofchallenging unseen geometric and patch perturbations to $6.8\%$ and $16.0\%$ onCIFAR-10. Overall, our contributions pave a path towards \textit{universalcertified robustness}.</description><author>Enyi Jiang, David S. Cheung, Gagandeep Singh</author><pubDate>Fri, 31 Jan 2025 17:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03000v2</guid></item><item><title>An Efficient Approach for Machine Translation on Low-resource Languages: A Case Study in Vietnamese-Chinese</title><link>http://arxiv.org/abs/2501.19314v1</link><description>Despite the rise of recent neural networks in machine translation, thosenetworks do not work well if the training data is insufficient. In this paper,we proposed an approach for machine translation in low-resource languages suchas Vietnamese-Chinese. Our proposed method leveraged the power of themultilingual pre-trained language model (mBART) and both Vietnamese and Chinesemonolingual corpus. Firstly, we built an early bird machine translation modelusing the bilingual training dataset. Secondly, we used TF-IDF technique toselect sentences from the monolingual corpus which are the most related todomains of the parallel dataset. Finally, the first model was used tosynthesize the augmented training data from the selected monolingual corpus forthe translation model. Our proposed scheme showed that it outperformed 8%compared to the transformer model. The augmented dataset also pushed the modelperformance.</description><author>Tran Ngoc Son, Nguyen Anh Tu, Nguyen Minh Tri</author><pubDate>Fri, 31 Jan 2025 17:11:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19314v1</guid></item><item><title>Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment</title><link>http://arxiv.org/abs/2501.19309v1</link><description>The performance of large language models (LLMs) is closely linked to theirunderlying size, leading to ever-growing networks and hence slower inference.Speculative decoding has been proposed as a technique to accelerateautoregressive generation, leveraging a fast draft model to propose candidatetokens, which are then verified in parallel based on their likelihood under thetarget model. While this approach guarantees to reproduce the target output, itincurs a substantial penalty: many high-quality draft tokens are rejected, evenwhen they represent objectively valid continuations. Indeed, we show that evenpowerful draft models such as GPT-4o, as well as human text cannot achieve highacceptance rates under the standard verification scheme. This severely limitsthe speedup potential of current speculative decoding methods, as an earlyrejection becomes overwhelmingly likely when solely relying on alignment ofdraft and target. We thus ask the following question: Can we adapt verification to recognizecorrect, but non-aligned replies? To this end, we draw inspiration from theLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answersin a versatile way. We carefully design a dataset to elicit the same capabilityin the target model by training a compact module on top of the embeddings toproduce ``judgements" of the current continuation. We showcase our strategy onthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x overLlama-405B, while maintaining its quality on a large range of benchmarks. Thesebenefits remain present even in optimized inference frameworks, where ourmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405Bon 2 and 8 H100s respectively.</description><author>Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Sch√∂nfeld, Ali Thabet, Jonas Kohler</author><pubDate>Fri, 31 Jan 2025 17:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19309v1</guid></item><item><title>Ontological analysis of proactive life event services</title><link>http://arxiv.org/abs/2501.19308v1</link><description>Life event service is a direct digital public service provided jointly byseveral governmental institutions so that a person can fulfill all theobligations and use all the rights that arise due to a particular event orsituation in personal life. Life event service consolidates several publicservices related to the same life event into one service for the serviceconsumer. This paper presents an ontological analysis of life event services,which is based on the works by Guarino, Guizzardi, Nardi, Wagner, and others.The purpose of the ontological analysis is to understand the meanings of lifeevent, proactive public service based on life event, and other related notions.This kind of ontological analysis is crucial because for implementing thehardware and software architectures of e-government and digital publicservices, it is essential to agree upon the precise meanings of the underlyingterms.</description><author>Kuldar Taveter</author><pubDate>Fri, 31 Jan 2025 17:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19308v1</guid></item><item><title>Anatomy Might Be All You Need: Forecasting What to Do During Surgery</title><link>http://arxiv.org/abs/2501.18011v2</link><description>Surgical guidance can be delivered in various ways. In neurosurgery, spatialguidance and orientation are predominantly achieved through neuronavigationsystems that reference pre-operative MRI scans. Recently, there has beengrowing interest in providing live guidance by analyzing video feeds from toolssuch as endoscopes. Existing approaches, including anatomy detection,orientation feedback, phase recognition, and visual question-answering,primarily focus on aiding surgeons in assessing the current surgical scene.This work aims to provide guidance on a finer scale, aiming to provide guidanceby forecasting the trajectory of the surgical instrument, essentiallyaddressing the question of what to do next. To address this task, we propose amodel that not only leverages the historical locations of surgical instrumentsbut also integrates anatomical features. Importantly, our work does not rely onexplicit ground truth labels for instrument trajectories. Instead, the groundtruth is generated by a detection model trained to detect both anatomicalstructures and instruments within surgical videos of a comprehensive datasetcontaining pituitary surgery videos. By analyzing the interaction betweenanatomy and instrument movements in these videos and forecasting futureinstrument movements, we show that anatomical features are a valuable asset inaddressing this challenging task. To the best of our knowledge, this work isthe first attempt to address this task for manually operated surgeries.</description><author>Gary Sarwin, Alessandro Carretta, Victor Staartjes, Matteo Zoli, Diego Mazzatenta, Luca Regli, Carlo Serra, Ender Konukoglu</author><pubDate>Fri, 31 Jan 2025 17:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18011v2</guid></item><item><title>SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling</title><link>http://arxiv.org/abs/2501.19306v1</link><description>Recent advancements in Large Language Models (LLMs) have created newopportunities to enhance performance on complex reasoning tasks by leveragingtest-time computation. However, conventional approaches such as repeatedsampling with majority voting or reward model scoring, often face diminishingreturns as test-time compute scales, in addition to requiring costlytask-specific reward model training. In this paper, we present Self-EnhancedTest-Time Scaling (SETS), a novel method that leverages the self-verificationand self-correction capabilities of recent advanced LLMs to overcome theselimitations. SETS integrates sampling, self-verification, and self-correctioninto a unified framework, enabling efficient and scalable test-time computationfor improved capabilities at complex tasks. Through extensive experiments onchallenging planning and reasoning benchmarks, compared to the alternatives, wedemonstrate that SETS achieves significant performance improvements and morefavorable test-time scaling laws.</description><author>Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan √ñ Arƒ±k</author><pubDate>Fri, 31 Jan 2025 17:03:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19306v1</guid></item><item><title>Potential and limitations of random Fourier features for dequantizing quantum machine learning</title><link>http://arxiv.org/abs/2309.11647v2</link><description>Quantum machine learning is arguably one of the most explored applications ofnear-term quantum devices. Much focus has been put on notions of variationalquantum machine learning where parameterized quantum circuits (PQCs) are usedas learning models. These PQC models have a rich structure which suggests thatthey might be amenable to efficient dequantization via random Fourier features(RFF). In this work, we establish necessary and sufficient conditions underwhich RFF does indeed provide an efficient dequantization of variationalquantum machine learning for regression. We build on these insights to makeconcrete suggestions for PQC architecture design, and to identify structureswhich are necessary for a regression problem to admit a potential quantumadvantage via PQC based optimization.</description><author>Ryan Sweke, Erik Recio, Sofiene Jerbi, Elies Gil-Fuster, Bryce Fuller, Jens Eisert, Johannes Jakob Meyer</author><pubDate>Fri, 31 Jan 2025 17:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11647v2</guid></item><item><title>FAN: Fourier Analysis Networks</title><link>http://arxiv.org/abs/2410.02675v3</link><description>Despite the remarkable successes of general-purpose neural networks, such asMLPs and Transformers, we find that they exhibit notable shortcomings inmodeling and reasoning about periodic phenomena, achieving only marginalperformance within the training domain and failing to generalize effectively toout-of-domain (OOD) scenarios. Periodicity is ubiquitous throughout nature andscience. Therefore, neural networks should be equipped with the essentialability to model and handle periodicity. In this work, we propose FAN, a novelgeneral-purpose neural network that offers broad applicability similar to MLPwhile effectively addressing periodicity modeling challenges. Periodicity isnaturally integrated into FAN's structure and computational processes byintroducing the Fourier Principle. Unlike existing Fourier-based networks,which possess particular periodicity modeling abilities but are typicallydesigned for specific tasks, our approach maintains the general-purposemodeling capability. Therefore, FAN can seamlessly replace MLP in various modelarchitectures with fewer parameters and FLOPs. Through extensive experiments,we demonstrate the superiority of FAN in periodicity modeling tasks and theeffectiveness and generalizability of FAN across a range of real-world tasks,e.g., symbolic formula representation, time series forecasting, languagemodeling, and image recognition.</description><author>Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jinliang Deng, Jing Su, Jun Zhang, Jingjing Xu</author><pubDate>Fri, 31 Jan 2025 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02675v3</guid></item><item><title>Beyond checkmate: exploring the creative chokepoints in AI text</title><link>http://arxiv.org/abs/2501.19301v1</link><description>Large Language Models (LLMs) have revolutionized Natural Language Processing(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.This rapid advancement has spurred research into various aspects of LLMs, theirtext generation &amp; reasoning capability, and potential misuse, fueling thenecessity for robust detection methods. While numerous prior research hasfocused on detecting LLM-generated text (AI text) and thus checkmating them,our study investigates a relatively unexplored territory: portraying thenuanced distinctions between human and AI texts across text segments. WhetherLLMs struggle with or excel at incorporating linguistic ingenuity acrossdifferent text segments carries substantial implications for determining theirpotential as effective creative assistants to humans. Through an analogy withthe structure of chess games-comprising opening, middle, and end games-weanalyze text segments (introduction, body, and conclusion) to determine wherethe most significant distinctions between human and AI texts exist. While AItexts can approximate the body segment better due to its increased length, acloser examination reveals a pronounced disparity, highlighting the importanceof this segment in AI text detection. Additionally, human texts exhibit highercross-segment differences compared to AI texts. Overall, our research can shedlight on the intricacies of human-AI text distinctions, offering novel insightsfor text detection and understanding.</description><author>Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee</author><pubDate>Fri, 31 Jan 2025 16:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19301v1</guid></item><item><title>Offline Learning for Combinatorial Multi-armed Bandits</title><link>http://arxiv.org/abs/2501.19300v1</link><description>The combinatorial multi-armed bandit (CMAB) is a fundamental sequentialdecision-making framework, extensively studied over the past decade. However,existing work primarily focuses on the online setting, overlooking thesubstantial costs of online interactions and the readily available offlinedatasets. To overcome these limitations, we introduce Off-CMAB, the firstoffline learning framework for CMAB. Central to our framework is thecombinatorial lower confidence bound (CLCB) algorithm, which combinespessimistic reward estimations with combinatorial solvers. To characterize thequality of offline datasets, we propose two novel data coverage conditions andprove that, under these conditions, CLCB achieves a near-optimal suboptimalitygap, matching the theoretical lower bound up to a logarithmic factor. Wevalidate Off-CMAB through practical applications, including learning to rank,large language model (LLM) caching, and social influence maximization, showingits ability to handle nonlinear reward functions, general feedback models, andout-of-distribution action samples that excludes optimal or even feasibleactions. Extensive experiments on synthetic and real-world datasets furtherhighlight the superior performance of CLCB.</description><author>Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee-Joe Wong, John C. S. Lui, Wei Chen</author><pubDate>Fri, 31 Jan 2025 16:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19300v1</guid></item><item><title>Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes</title><link>http://arxiv.org/abs/2501.19298v1</link><description>In recent years, as smart home systems have become more widespread, securityconcerns within these environments have become a growing threat. Currently,most smart home security solutions, such as anomaly detection and behaviorprediction models, are trained using fixed datasets that are precollected.However, the process of dataset collection is time-consuming and lacks theflexibility needed to adapt to the constantly evolving smart home environment.Additionally, the collection of personal data raises significant privacyconcerns for users. Lately, large language models (LLMs) have emerged as apowerful tool for a wide range of tasks across diverse application domains,thanks to their strong capabilities in natural language processing, reasoning,and problem-solving. In this paper, we propose an LLM-based synthetic datasetgeneration IoTGen framework to enhance the generalization of downstream smarthome intelligent models. By generating new synthetic datasets that reflectchanges in the environment, smart home intelligent models can be retrained toovercome the limitations of fixed and outdated data, allowing them to betteralign with the dynamic nature of real-world home environments. Specifically, wefirst propose a Structure Pattern Perception Compression (SPPC) method tailoredfor IoT behavior data, which preserves the most informative content in the datawhile significantly reducing token consumption. Then, we propose a systematicapproach to create prompts and implement data generation to automaticallygenerate IoT synthetic data with normative and reasonable properties, assistingtask models in adaptive training to improve generalization and real-worldperformance.</description><author>Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li</author><pubDate>Fri, 31 Jan 2025 16:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19298v1</guid></item><item><title>Analysis of LLMs vs Human Experts in Requirements Engineering</title><link>http://arxiv.org/abs/2501.19297v1</link><description>The majority of research around Large Language Models (LLM) application tosoftware development has been on the subject of code generation. There islittle literature on LLMs' impact on requirements engineering (RE), which dealswith the process of developing and verifying the system requirements. WithinRE, there is a subdiscipline of requirements elicitation, which is the practiceof discovering and documenting requirements for a system from users, customers,and other stakeholders. In this analysis, we compare LLM's ability to elicitrequirements of a software system, as compared to that of a human expert in atime-boxed and prompt-boxed study. We found LLM-generated requirements wereevaluated as more aligned (+1.12) than human-generated requirements with atrend of being more complete (+10.2%). Conversely, we found users tended tobelieve that solutions they perceived as more aligned had been generated byhuman experts. Furthermore, while LLM-generated documents scored higher andperformed at 720x the speed, their cost was, on average, only 0.06% that of ahuman expert. Overall, these findings indicate that LLMs will play anincreasingly important role in requirements engineering by improvingrequirements definitions, enabling more efficient resource allocation, andreducing overall project timelines.</description><author>Cory Hymel, Hiroe Johnson</author><pubDate>Fri, 31 Jan 2025 16:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19297v1</guid></item><item><title>Differentially Private In-context Learning via Sampling Few-shot Mixed with Zero-shot Outputs</title><link>http://arxiv.org/abs/2501.19287v1</link><description>In-context learning (ICL) has shown promising improvement in downstream taskadaptation of LLMs by augmenting prompts with relevant input-output examples(demonstrations). However, the ICL demonstrations can contain privacy-sensitiveinformation, which can be leaked and/or regurgitated by the LLM output.Differential Privacy (DP), a widely adopted privacy safeguard, has emerged tomitigate this privacy leakage, with recent work demonstrating strongprivacy-utility tradeoffs in classification tasks for ICL. However, generationtasks for ICL are challenging due to the high-dimensional output space ofopen-ended generation. To this end, we propose $\texttt{dps-mozo}$,Differentially Private Sampling by Mixing One-shot with Zero-shot Outputs, adecoding framework that generates DP text by sampling from the product ofmultiple one-shot outputs mixed with a zero-shot output. This mixingeffectively reduces the amount of information that can be leaked by eachdemonstration. By utilizing the inherent randomness in sampling from the mixeddistributions, we can achieve DP without adding noise, thereby improving theprivacy-utility tradeoff. Our experimental evaluations show $\texttt{dps-mozo}$can achieve a strong privacy guarantee, $\epsilon=2$, with minimal utilitydegradation compared to non-private few-shot learning, $\textbf{0.3}$% ROUGE-LF1 score decrease on the SAMSum dataset with Gemma 2 2B.</description><author>James Flemings, Haosheng Gan, Hongyi Li, Meisam Razaviyayn, Murali Annavaram</author><pubDate>Fri, 31 Jan 2025 16:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19287v1</guid></item><item><title>MVG-CRPS: A Robust Loss Function for Multivariate Probabilistic Forecasting</title><link>http://arxiv.org/abs/2410.09133v2</link><description>Multivariate Gaussian (MVG) distributions are central to modeling correlatedcontinuous variables in probabilistic forecasting. Neural forecasting modelstypically parameterize the mean vector and covariance matrix of thedistribution using neural networks, optimizing with the log-score (negativelog-likelihood) as the loss function. However, the sensitivity of the log-scoreto outliers can lead to significant errors in the presence of anomalies.Drawing on the continuous ranked probability score (CRPS) for univariatedistributions, we propose MVG-CRPS, a strictly proper scoring rule for MVGdistributions. MVG-CRPS admits a closed-form expression in terms of neuralnetwork outputs, thereby integrating seamlessly into deep learning frameworks.Experiments on real-world datasets across multivariate autoregressive andunivariate sequence-to-sequence (Seq2Seq) forecasting tasks show that MVG-CRPSimproves robustness, accuracy, and uncertainty quantification in probabilisticforecasting.</description><author>Vincent Zhihao Zheng, Lijun Sun</author><pubDate>Fri, 31 Jan 2025 16:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09133v2</guid></item><item><title>Contraction of Private Quantum Channels and Private Quantum Hypothesis Testing</title><link>http://arxiv.org/abs/2406.18651v2</link><description>A quantum generalized divergence by definition satisfies the data-processinginequality; as such, the relative decrease in such a divergence under theaction of a quantum channel is at most one. This relative decrease is formallyknown as the contraction coefficient of the channel and the divergence.Interestingly, there exist combinations of channels and divergences for whichthe contraction coefficient is strictly less than one. Furthermore,understanding the contraction coefficient is fundamental for the study ofstatistical tasks under privacy constraints. To this end, here we establishupper bounds on contraction coefficients for the hockey-stick divergence underprivacy constraints, where privacy is quantified with respect to the quantumlocal differential privacy (QLDP) framework, and we fully characterize thecontraction coefficient for the trace distance under privacy constraints. Withthe machinery developed, we also determine an upper bound on the contraction ofboth the Bures distance and quantum relative entropy relative to the normalizedtrace distance, under QLDP constraints. Next, we apply our findings toestablish bounds on the sample complexity of quantum hypothesis testing underprivacy constraints. Furthermore, we study various scenarios in which thesample complexity bounds are tight, while providing order-optimal quantumchannels that achieve those bounds. Lastly, we show how private quantumchannels provide fairness and Holevo information stability in quantum learningsettings.</description><author>Theshani Nuradha, Mark M. Wilde</author><pubDate>Fri, 31 Jan 2025 16:48:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18651v2</guid></item><item><title>OneBatchPAM: A Fast and Frugal K-Medoids Algorithm</title><link>http://arxiv.org/abs/2501.19285v1</link><description>This paper proposes a novel k-medoids approximation algorithm to handlelarge-scale datasets with reasonable computational time and memory complexity.We develop a local-search algorithm that iteratively improves the medoidselection based on the estimation of the k-medoids objective. A single batch ofsize m &lt;&lt; n provides the estimation, which reduces the required memory size andthe number of pairwise dissimilarities computations to O(mn), instead of O(n^2)compared to most k-medoids baselines. We obtain theoretical resultshighlighting that a batch of size m = O(log(n)) is sufficient to guarantee,with strong probability, the same performance as the original local-searchalgorithm. Multiple experiments conducted on real datasets of various sizes anddimensions show that our algorithm provides similar performances asstate-of-the-art methods such as FasterPAM and BanditPAM++ with a drasticallyreduced running time.</description><author>Antoine de Mathelin, Nicolas Enrique Cecchi, Fran√ßois Deheeger, Mathilde Mougeot, Nicolas Vayatis</author><pubDate>Fri, 31 Jan 2025 16:48:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19285v1</guid></item><item><title>Application of Generative Adversarial Network (GAN) for Synthetic Training Data Creation to improve performance of ANN Classifier for extracting Built-Up pixels from Landsat Satellite Imagery</title><link>http://arxiv.org/abs/2501.19283v1</link><description>Training a neural network for pixel based classification task using lowresolution Landsat images is difficult as the size of the training data isusually small due to less number of available pixels that represent a singleclass without any mixing with other classes. Due to this scarcity of trainingdata, neural network may not be able to attain expected level of accuracy. Thislimitation could be overcome using a generative network that aims to generatesynthetic data having the same distribution as the sample data with which it istrained. In this work, we have proposed a methodology for improving theperformance of ANN classifier to identify built-up pixels in the Landsat$7$image with the help of developing a simple GAN architecture that could generatesynthetic training pixels when trained using original set of sample built-uppixels. To ensure that the marginal and joint distributions of all the bandscorresponding to the generated and original set of pixels areindistinguishable, non-parametric Kolmogorov Smirnov Test and Ball Divergencebased Equality of Distributions Test have been performed respectively. It hasbeen observed that the overall accuracy and kappa coefficient of the ANN modelfor built-up classification have continuously improved from $0.9331$ to$0.9983$ and $0.8277$ to $0.9958$ respectively, with the inclusion of generatedsets of built-up pixels to the original one.</description><author>Amritendu Mukherjee, Dipanwita Sinha Mukherjee, Parthasarathy Ramachandran</author><pubDate>Fri, 31 Jan 2025 16:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19283v1</guid></item><item><title>UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models</title><link>http://arxiv.org/abs/2411.01703v2</link><description>Multimodal large language models (MLLMs) have revolutionized vision-languageunderstanding but remain vulnerable to multimodal jailbreak attacks, whereadversarial inputs are meticulously crafted to elicit harmful or inappropriateresponses. We propose UniGuard, a novel multimodal safety guardrail thatjointly considers the unimodal and cross-modal harmful signals. UniGuard trainsa multimodal guardrail to minimize the likelihood of generating harmfulresponses in a toxic corpus. The guardrail can be seamlessly applied to anyinput prompt during inference with minimal computational costs. Extensiveexperiments demonstrate the generalizability of UniGuard across multiplemodalities, attack strategies, and multiple state-of-the-art MLLMs, includingLLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robustdefense mechanism maintains the models' overall vision-language understandingcapabilities.</description><author>Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar</author><pubDate>Fri, 31 Jan 2025 16:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01703v2</guid></item><item><title>Addressing Quality Challenges in Deep Learning: The Role of MLOps and Domain Knowledge</title><link>http://arxiv.org/abs/2501.08402v2</link><description>Deep learning (DL) systems present unique challenges in software engineering,especially concerning quality attributes like correctness and resourceefficiency. While DL models excel in specific tasks, engineering DL systems isstill essential. The effort, cost, and potential diminishing returns ofcontinual improvements must be carefully evaluated, as software engineers oftenface the critical decision of when to stop refining a system relative to itsquality attributes. This experience paper explores the role of MLOps practices-- such as monitoring and experiment tracking -- in creating transparent andreproducible experimentation environments that enable teams to assess andjustify the impact of design decisions on quality attributes. Furthermore, wereport on experiences addressing the quality challenges by embedding domainknowledge into the design of a DL model and its integration within a largersystem. The findings offer actionable insights into the benefits of domainknowledge and MLOps and the strategic consideration of when to limit furtheroptimizations in DL projects to maximize overall system quality andreliability.</description><author>Santiago del Rey, Adri√† Medina, Xavier Franch, Silverio Mart√≠nez-Fern√°ndez</author><pubDate>Fri, 31 Jan 2025 16:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08402v2</guid></item><item><title>Statistical Physics of Deep Neural Networks: Generalization Capability, Beyond the Infinite Width, and Feature Learning</title><link>http://arxiv.org/abs/2501.19281v1</link><description>Deep Neural Networks (DNNs) excel at many tasks, often rivaling or surpassinghuman performance. Yet their internal processes remain elusive, frequentlydescribed as "black boxes." While performance can be refined experimentally,achieving a fundamental grasp of their inner workings is still a challenge. Statistical Mechanics has long tackled computational problems, and thisthesis applies physics-based insights to understand DNNs via threecomplementary approaches. First, by averaging over data, we derive an asymptotic bound ongeneralization that depends solely on the size of the last layer, rather thanon the total number of parameters -- revealing how deep architectures processinformation differently across layers. Second, adopting a data-dependent viewpoint, we explore a finite-widththermodynamic limit beyond the infinite-width regime. This leads to: (i) aclosed-form expression for the generalization error in a finite-widthone-hidden-layer network (regression task); (ii) an approximate partitionfunction for deeper architectures; and (iii) a link between deep networks inthis thermodynamic limit and Student's t-processes. Finally, from a task-explicit perspective, we present a preliminary analysisof how DNNs interact with a controlled dataset, investigating whether theytruly internalize its structure -- collapsing to the teacher -- or merelymemorize it. By understanding when a network must learn data structure ratherthan just memorize, it sheds light on fostering meaningful internalrepresentations. In essence, this thesis leverages the synergy between Statistical Physics andMachine Learning to illuminate the inner behavior of DNNs.</description><author>Sebastiano Ariosto</author><pubDate>Fri, 31 Jan 2025 16:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19281v1</guid></item><item><title>Referential communication in heterogeneous communities of pre-trained visual deep networks</title><link>http://arxiv.org/abs/2302.08913v6</link><description>As large pre-trained image-processing neural networks are being embedded inautonomous agents such as self-driving cars or robots, the question arises ofhow such systems can communicate with each other about the surrounding world,despite their different architectures and training regimes. As a first step inthis direction, we systematically explore the task of referential communicationin a community of heterogeneous state-of-the-art pre-trained visual networks,showing that they can develop, in a self-supervised way, a shared protocol torefer to a target object among a set of candidates. This shared protocol canalso be used, to some extent, to communicate about previously unseen objectcategories of different granularity. Moreover, a visual network that was notinitially part of an existing community can learn the community's protocol withremarkable ease. Finally, we study, both qualitatively and quantitatively, theproperties of the emergent protocol, providing some evidence that it iscapturing high-level semantic features of objects.</description><author>Mat√©o Mahaut, Francesca Franzon, Roberto Dess√¨, Marco Baroni</author><pubDate>Fri, 31 Jan 2025 16:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08913v6</guid></item><item><title>S-VOTE: Similarity-based Voting for Client Selection in Decentralized Federated Learning</title><link>http://arxiv.org/abs/2501.19279v1</link><description>Decentralized Federated Learning (DFL) enables collaborative,privacy-preserving model training without relying on a central server. Thisdecentralized approach reduces bottlenecks and eliminates single points offailure, enhancing scalability and resilience. However, DFL also introduceschallenges such as suboptimal models with non-IID data distributions, increasedcommunication overhead, and resource usage. Thus, this work proposes S-VOTE, avoting-based client selection mechanism that optimizes resource usage andenhances model performance in federations with non-IID data conditions. S-VOTEconsiders an adaptive strategy for spontaneous local training that addressesparticipation imbalance, allowing underutilized clients to contribute withoutsignificantly increasing resource costs. Extensive experiments on benchmarkdatasets demonstrate the S-VOTE effectiveness. More in detail, it achieveslower communication costs by up to 21%, 4-6% faster convergence, and improveslocal performance by 9-17% compared to baseline methods in some configurations,all while achieving a 14-24% energy consumption reduction. These resultshighlight the potential of S-VOTE to address DFL challenges in heterogeneousenvironments.</description><author>Pedro Miguel S√°nchez S√°nchez, Enrique Tom√°s Mart√≠nez Beltr√°n, Chao Feng, G√©r√¥me Bovet, Gregorio Mart√≠nez P√©rez, Alberto Huertas Celdr√°n</author><pubDate>Fri, 31 Jan 2025 16:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19279v1</guid></item><item><title>Pheromone-based Learning of Optimal Reasoning Paths</title><link>http://arxiv.org/abs/2501.19278v1</link><description>Large Language Models (LLMs) have demonstrated remarkable reasoningcapabilities through chain-of-thought prompting, yet discovering effectivereasoning methods for complex problems remains challenging due to the vastspace of possible intermediate steps. We introduce Ant ColonyOptimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combinesACO with LLMs to discover optimal reasoning paths for complex problemsefficiently. Drawing inspiration from Hebbian learning in neurological systems,our method employs a collection of distinctly fine-tuned LLM "ants" to traverseand lay pheromone trails through a centralized tree of thought, with each ant'smovement governed by a weighted combination of existing pheromone trails andits own specialized expertise. The algorithm evaluates complete reasoning pathsusing a mixture-of-experts-based scoring function, with pheromones reinforcingproductive reasoning paths across iterations. Experiments on three challengingreasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToTperforms significantly better than existing chain-of-thought optimizationapproaches, suggesting that incorporating biologically inspired collectivesearch mechanisms into LLM inference can substantially enhance reasoningcapabilities.</description><author>Anirudh Chari, Aditya Tiwari, Richard Lian, Suraj Reddy, Brian Zhou</author><pubDate>Fri, 31 Jan 2025 16:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19278v1</guid></item><item><title>On Pareto Optimality for the Multinomial Logistic Bandit</title><link>http://arxiv.org/abs/2501.19277v1</link><description>We provide a new online learning algorithm for tackling the Multinomial LogitBandit (MNL-Bandit) problem. Despite the challenges posed by the combinatorialnature of the MNL model, we develop a novel Upper Confidence Bound (UCB)-basedmethod that achieves Pareto optimality by balancing regret minimization andestimation error of the assortment revenues and the MNL parameters. We developtheoretical guarantees characterizing the tradeoff between regret andestimation error for the MNL-Bandit problem through information-theoreticbounds, and propose a modified UCB algorithm that incorporates forcedexploration to improve parameter estimation accuracy while maintaining lowregret. Our analysis sheds critical insights into how to optimally balance thecollected revenues and the treatment estimation in dynamic assortmentoptimization.</description><author>Jierui Zuo, Hanzhang Qin</author><pubDate>Fri, 31 Jan 2025 16:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19277v1</guid></item><item><title>Restoring balance: principled under/oversampling of data for optimal classification</title><link>http://arxiv.org/abs/2405.09535v2</link><description>Class imbalance in real-world data poses a common bottleneck for machinelearning tasks, since achieving good generalization on under-representedexamples is often challenging. Mitigation strategies, such as under oroversampling the data depending on their abundances, are routinely proposed andtested empirically, but how they should adapt to the data statistics remainspoorly understood. In this work, we determine exact analytical expressions ofthe generalization curves in the high-dimensional regime for linear classifiers(Support Vector Machines). We also provide a sharp prediction of the effects ofunder/oversampling strategies depending on class imbalance, first and secondmoments of the data, and the metrics of performance considered. We show thatmixed strategies involving under and oversampling of data lead to performanceimprovement. Through numerical experiments, we show the relevance of ourtheoretical predictions on real datasets, on deeper architectures and withsampling strategies based on unsupervised probabilistic models.</description><author>Emanuele Loffredo, Mauro Pastore, Simona Cocco, R√©mi Monasson</author><pubDate>Fri, 31 Jan 2025 16:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09535v2</guid></item><item><title>Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts on Social Media</title><link>http://arxiv.org/abs/2501.06274v2</link><description>The rise of misinformation and fake news in online political discourse posessignificant challenges to democratic processes and public engagement. Whiledebunking efforts aim to counteract misinformation and foster fact-baseddialogue, these discussions often involve language toxicity and emotionalpolarization. We examined over 86 million debunking tweets and more than 4million Reddit debunking comments to investigate the relationship betweenlanguage toxicity, pessimism, and social polarization in debunking efforts.Focusing on discussions of the 2016 and 2020 U.S. presidential elections andthe QAnon conspiracy theory, our analysis reveals three key findings: (1)peripheral participants (1-degree users) play a disproportionate role inshaping toxic discourse, driven by lower community accountability and emotionalexpression; (2) platform mechanisms significantly influence polarization, withTwitter amplifying partisan differences and Reddit fostering higher overalltoxicity due to its structured, community-driven interactions; and (3) anegative correlation exists between language toxicity and pessimism, withincreased interaction reducing toxicity, especially on Reddit. We show thatplatform architecture affects informational complexity of user interactions,with Twitter promoting concentrated, uniform discourse and Reddit encouragingdiverse, complex communication. Our findings highlight the importance of userengagement patterns, platform dynamics, and emotional expressions in shapingpolarization in debunking discourse. This study offers insights forpolicymakers and platform designers to mitigate harmful effects and promotehealthier online discussions, with implications for understandingmisinformation, hate speech, and political polarization in digitalenvironments.</description><author>Wentao Xu, Wenlu Fan, Shiqian Lu, Tenghao Li, Bin Wang</author><pubDate>Fri, 31 Jan 2025 16:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06274v2</guid></item><item><title>ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral Residuals</title><link>http://arxiv.org/abs/2501.00701v2</link><description>Analyzing long-term behaviors in high-dimensional nonlinear dynamical systemsremains challenging, with the Koopman operator framework providing a powerfulglobal linearization approach, though existing methods for approximating itsspectral components often suffer from theoretical limitations and reliance onpredefined dictionaries. While Residual Dynamic Mode Decomposition (ResDMD)introduced the spectral residual to assess the accuracy of Koopman operatorapproximation, its only filters precomputed spectra, which prevents it fromfully discovering the Koopman operator's complete spectral information (alimitation sometimes referred to as the 'spectral inclusion' problem). Weintroduce ResKoopNet (Residual-based Koopman-learning Network), a novel methodthat addresses this limitation by explicitly minimizing the spectral residualto compute Koopman eigenpairs, which can identify a more precise and completespectrum of the Koopman operator. This approach provides theoretical guaranteeswhile maintaining computational adaptability through a neural networkimplementation. Experiments on physical and biological systems demonstrateResKoopNet's superior accuracy in spectral approximation compared to existingmethods, particularly for systems with continuous spectra and high dimensional,which makes it as an effective tool for analyzing complex dynamical systems.</description><author>Yuanchao Xu, Kaidi Shao, Nikos Logothetis, Zhongwei Shen</author><pubDate>Fri, 31 Jan 2025 16:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.00701v2</guid></item><item><title>Deep Out-of-Distribution Uncertainty Quantification via Weight Entropy Maximization</title><link>http://arxiv.org/abs/2309.15704v2</link><description>This paper deals with uncertainty quantification and out-of-distributiondetection in deep learning using Bayesian and ensemble methods. It proposes apractical solution to the lack of prediction diversity observed recently forstandard approaches when used out-of-distribution (Ovadia et al., 2019; Liu etal., 2021). Considering that this issue is mainly related to a lack of weightdiversity, we claim that standard methods sample in "over-restricted" regionsof the weight space due to the use of "over-regularization" processes, such asweight decay and zero-mean centered Gaussian priors. We propose to solve theproblem by adopting the maximum entropy principle for the weight distribution,with the underlying idea to maximize the weight diversity. Under this paradigm,the epistemic uncertainty is described by the weight distribution of maximalentropy that produces neural networks "consistent" with the trainingobservations. Considering stochastic neural networks, a practical optimizationis derived to build such a distribution, defined as a trade-off between theaverage empirical risk and the weight distribution entropy. We develop a novelweight parameterization for the stochastic model, based on the singular valuedecomposition of the neural network's hidden representations, which enables alarge increase of the weight entropy for a small empirical risk penalization.We provide both theoretical and numerical results to assess the efficiency ofthe approach. In particular, the proposed algorithm appears in the top threebest methods in all configurations of an extensive out-of-distributiondetection benchmark including more than thirty competitors.</description><author>Antoine de Mathelin, Fran√ßois Deheeger, Mathilde Mougeot, Nicolas Vayatis</author><pubDate>Fri, 31 Jan 2025 16:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15704v2</guid></item><item><title>Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks</title><link>http://arxiv.org/abs/2501.19271v1</link><description>Concept-based explanation methods, such as concept bottleneck models (CBMs),aim to improve the interpretability of machine learning models by linking theirdecisions to human-understandable concepts, under the critical assumption thatsuch concepts can be accurately attributed to the network's feature space.However, this foundational assumption has not been rigorously validated, mainlybecause the field lacks standardised metrics and benchmarks to assess theexistence and spatial alignment of such concepts. To address this, we proposethree metrics: the concept global importance metric, the concept existencemetric, and the concept location metric, including a technique for visualisingconcept activations, i.e., concept activation mapping. We benchmark post-hocCBMs to illustrate their capabilities and challenges. Through qualitative andquantitative experiments, we demonstrate that, in many cases, even the mostimportant concepts determined by post-hoc CBMs are not present in input images;moreover, when they are present, their saliency maps fail to align with theexpected regions by either activating across an entire object or misidentifyingrelevant concept-specific regions. We analyse the root causes of theselimitations, such as the natural correlation of concepts. Our findingsunderscore the need for more careful application of concept-based explanationtechniques especially in settings where spatial interpretability is critical.</description><author>Halil Ibrahim Aysel, Xiaohao Cai, Adam Prugel-Bennett</author><pubDate>Fri, 31 Jan 2025 16:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19271v1</guid></item><item><title>Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way</title><link>http://arxiv.org/abs/2501.19270v1</link><description>Point cloud completion aims to recover the completed 3D shape of an objectfrom its partial observation caused by occlusion, sensor's limitation, noise,etc. When some key semantic information is lost in the incomplete point cloud,the neural network needs to infer the missing part based on the inputinformation. Intuitively we would apply an autoencoder architecture to solvethis kind of problem, which take the incomplete point cloud as input and issupervised by the ground truth. This process that develops model's imaginationfrom incomplete shape to complete shape is done automatically in the latentspace. But the knowledge for mapping from incomplete to complete still remainsdark and could be further explored. Motivated by the knowledge distillation'steacher-student learning strategy, we design a knowledge transfer way forcompleting 3d shape. In this work, we propose a novel View Distillation PointCompletion Network (VD-PCN), which solve the completion problem by a multi-viewdistillation way. The design methodology fully leverages the orderliness of 2dpixels, flexibleness of 2d processing and powerfulness of 2d network. Extensiveevaluations on PCN, ShapeNet55/34, and MVP datasets confirm the effectivenessof our design and knowledge transfer strategy, both quantitatively andqualitatively. Committed to facilitate ongoing research, we will make our codepublicly available.</description><author>Zhanpeng Luo, Linna Wang, Guangwu Qian, Li Lu</author><pubDate>Fri, 31 Jan 2025 16:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19270v1</guid></item><item><title>Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP</title><link>http://arxiv.org/abs/2407.09861v3</link><description>Natural Language Processing (NLP) research has traditionally beenpredominantly focused on English, driven by the availability of resources, thesize of the research community, and market demands. Recently, there has been anoticeable shift towards multilingualism in NLP, recognizing the need forinclusivity and effectiveness across diverse languages and cultures.Monolingual surveys have the potential to complement the broader trend towardsmultilingualism in NLP by providing foundational insights and resources,necessary for effectively addressing the linguistic diversity of globalcommunication. However, monolingual NLP surveys are extremely rare in theliterature. This study introduces a generalizable methodology for creatingsystematic and comprehensive monolingual NLP surveys, aimed at optimizing theprocess of constructing such surveys and thoroughly addressing a language's NLPsupport. Our approach integrates a structured search protocol to avoidselection bias and ensure reproducibility, an NLP task taxonomy to organize thesurveyed material coherently, and language resources (LRs) taxonomies toidentify potential benchmarks and highlight opportunities for improvingresource availability (e.g., through better maintenance or licensing). We applythis methodology to Greek NLP (2012-2023), providing a comprehensive overviewof its current state and challenges. We discuss the progress of Greek NLP andoutline the Greek LRs found, classified by availability and usability,assessing language support per NLP task. The presented systematic literaturereview of Greek NLP serves as an application of our method that showcases thebenefits of monolingual NLP surveys more broadly. Similar applications could beconsidered for the myriads of languages whose progress in NLP lags behind thatof well-supported languages.</description><author>Juli Bakagianni, Kanella Pouli, Maria Gavriilidou, John Pavlopoulos</author><pubDate>Fri, 31 Jan 2025 16:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09861v3</guid></item><item><title>Jackpot! Alignment as a Maximal Lottery</title><link>http://arxiv.org/abs/2501.19266v1</link><description>Reinforcement Learning from Human Feedback (RLHF), the standard for aligningLarge Language Models (LLMs) with human values, is known to fail to satisfyproperties that are intuitively desirable, such as respecting the preferencesof the majority \cite{ge2024axioms}. To overcome these issues, we propose theuse of a probabilistic Social Choice rule called \emph{maximal lotteries} as areplacement for RLHF. We show that a family of alignment techniques, namelyNash Learning from Human Feedback (NLHF) \cite{munos2023nash} and variants,approximate maximal lottery outcomes and thus inherit its beneficialproperties. We confirm experimentally that our proposed methodology handles situationsthat arise when working with preferences more robustly than standard RLHF,including supporting the preferences of the majority, providing principled waysof handling non-transitivities in the preference data, and robustness toirrelevant alternatives. This results in systems that better incorporate humanvalues and respect human intentions.</description><author>Roberto-Rafael Maura-Rivero, Marc Lanctot, Francesco Visin, Kate Larson</author><pubDate>Fri, 31 Jan 2025 16:26:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19266v1</guid></item><item><title>Efficient Event-based Delay Learning in Spiking Neural Networks</title><link>http://arxiv.org/abs/2501.07331v2</link><description>Spiking Neural Networks (SNNs) are attracting increased attention as a moreenergy-efficient alternative to traditional Artificial Neural Networks. Spikingneurons are stateful and intrinsically recurrent, making them well-suited forspatio-temporal tasks. However, this intrinsic memory is limited by synapticand membrane time constants. A powerful additional mechanism are delays. Inthis paper, we propose a novel event-based training method for SNNs withdelays, grounded in the EventProp formalism and enabling the calculation ofexact gradients with respect to weights and delays. Our method supportsmultiple spikes per neuron and, to our best knowledge, is the first delaylearning algorithm to be applied to recurrent SNNs. We evaluate our method on asimple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits andSpiking Speech Commands datasets, demonstrating that our algorithm can optimizedelays from suboptimal initial conditions and enhance classification accuracycompared to architectures without delays. Finally, we show that our approachuses less than half the memory of the current state-of-the-art delay-learningmethod and is up to 26x faster.</description><author>Bal√°zs M√©sz√°ros, James C. Knight, Thomas Nowotny</author><pubDate>Fri, 31 Jan 2025 16:26:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07331v2</guid></item><item><title>Medical Semantic Segmentation with Diffusion Pretrain</title><link>http://arxiv.org/abs/2501.19265v1</link><description>Recent advances in deep learning have shown that learning robust featurerepresentations is critical for the success of many computer vision tasks,including medical image segmentation. In particular, both transformer andconvolutional-based architectures have benefit from leveraging pretext tasksfor pretraining. However, the adoption of pretext tasks in 3D medical imaginghas been less explored and remains a challenge, especially in the context oflearning generalizable feature representations. We propose a novel pretraining strategy using diffusion models withanatomical guidance, tailored to the intricacies of 3D medical image data. Weintroduce an auxiliary diffusion process to pretrain a model that producegeneralizable feature representations, useful for a variety of downstreamsegmentation tasks. We employ an additional model that predicts 3D universalbody-part coordinates, providing guidance during the diffusion process andimproving spatial awareness in generated representations. This approach notonly aids in resolving localization inaccuracies but also enriches the model'sability to understand complex anatomical structures. Empirical validation on a 13-class organ segmentation task demonstrate theeffectiveness of our pretraining technique. It surpasses existing restorativepretraining methods in 3D medical image segmentation by $7.5\%$, and iscompetitive with the state-of-the-art contrastive pretraining approach,achieving an average Dice coefficient of 67.8 in a non-linear evaluationscenario.</description><author>David Li, Anvar Kurmukov, Mikhail Goncharov, Roman Sokolov, Mikhail Belyaev</author><pubDate>Fri, 31 Jan 2025 16:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19265v1</guid></item></channel></rss>