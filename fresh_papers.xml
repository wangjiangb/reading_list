<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 14 Oct 2024 13:00:23 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SceneCraft: Layout-Guided 3D Scene Generation</title><link>http://arxiv.org/abs/2410.09049v1</link><description>The creation of complex 3D scenes tailored to user specifications has been atedious and challenging task with traditional 3D modeling tools. Although somepioneering methods have achieved automatic text-to-3D generation, they aregenerally limited to small-scale scenes with restricted control over the shapeand texture. We introduce SceneCraft, a novel method for generating detailedindoor scenes that adhere to textual descriptions and spatial layoutpreferences provided by users. Central to our method is a rendering-basedtechnique, which converts 3D semantic layouts into multi-view 2D proxy maps.Furthermore, we design a semantic and depth conditioned diffusion model togenerate multi-view images, which are used to learn a neural radiance field(NeRF) as the final scene representation. Without the constraints of panoramaimage generation, we surpass previous methods in supporting complicated indoorspace generation beyond a single room, even as complicated as a wholemulti-bedroom apartment with irregular shapes and layouts. Through experimentalanalysis, we demonstrate that our method significantly outperforms existingapproaches in complex indoor scene generation with diverse textures, consistentgeometry, and realistic visual quality. Code and more results are available at:https://orangesodahub.github.io/SceneCraft</description><author>Xiuyu Yang, Yunze Man, Jun-Kun Chen, Yu-Xiong Wang</author><pubDate>Fri, 11 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09049v1</guid></item><item><title>Generative Verifiers: Reward Modeling as Next-Token Prediction</title><link>http://arxiv.org/abs/2408.15240v2</link><description>Verifiers or reward models are often used to enhance the reasoningperformance of large language models (LLMs). A common approach is the Best-of-Nmethod, where N candidate solutions generated by the LLM are ranked by averifier, and the best one is selected. While LLM-based verifiers are typicallytrained as discriminative classifiers to score solutions, they do not utilizethe text generation capabilities of pretrained LLMs. To overcome thislimitation, we instead propose training verifiers using the ubiquitousnext-token prediction objective, jointly on verification and solutiongeneration. Compared to standard verifiers, such generative verifiers (GenRM)can benefit from several advantages of LLMs: they integrate seamlessly withinstruction tuning, enable chain-of-thought reasoning, and can utilizeadditional test-time compute via majority voting for better verification. Wedemonstrate that GenRM outperforms discriminative, DPO verifiers, andLLM-as-a-Judge, resulting in a 16-40% improvement in the number of problemssolved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, wefind that training GenRM with synthetic verification rationales is sufficientto pick out subtle errors on math problems. Finally, we demonstrate thatgenerative verifiers scale favorably with model size and inference-timecompute.</description><author>Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal</author><pubDate>Fri, 11 Oct 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15240v2</guid></item><item><title>Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models</title><link>http://arxiv.org/abs/2410.09047v1</link><description>The safety alignment ability of Vision-Language Models (VLMs) is prone to bedegraded by the integration of the vision module compared to its LLM backbone.We investigate this phenomenon, dubbed as ''safety alignment degradation'' inthis paper, and show that the challenge arises from the representation gap thatemerges when introducing vision modality to VLMs. In particular, we show thatthe representations of multi-modal inputs shift away from that of text-onlyinputs which represent the distribution that the LLM backbone is optimized for.At the same time, the safety alignment capabilities, initially developed withinthe textual embedding space, do not successfully transfer to this newmulti-modal representation space. To reduce safety alignment degradation, weintroduce Cross-Modality Representation Manipulation (CMRM), an inference timerepresentation intervention method for recovering the safety alignment abilitythat is inherent in the LLM backbone of VLMs, while simultaneously preservingthe functional capabilities of VLMs. The empirical results show that ourframework significantly recovers the alignment ability that is inherited fromthe LLM backbone with minimal impact on the fluency and linguistic capabilitiesof pre-trained VLMs even without additional training. Specifically, the unsaferate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as3.15% with only inference-time intervention. WARNING: This paper contains examples of toxic or harmful language.</description><author>Qin Liu, Chao Shang, Ling Liu, Nikolaos Pappas, Jie Ma, Neha Anna John, Srikanth Doss, Lluis Marquez, Miguel Ballesteros, Yassine Benajiba</author><pubDate>Fri, 11 Oct 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09047v1</guid></item><item><title>Linear Convergence of Diffusion Models Under the Manifold Hypothesis</title><link>http://arxiv.org/abs/2410.09046v1</link><description>Score-matching generative models have proven successful at sampling fromcomplex high-dimensional data distributions. In many applications, thisdistribution is believed to concentrate on a much lower $d$-dimensionalmanifold embedded into $D$-dimensional space; this is known as the manifoldhypothesis. The current best-known convergence guarantees are either linear in$D$ or polynomial (superlinear) in $d$. The latter exploits a novel integrationscheme for the backward SDE. We take the best of both worlds and show that thenumber of steps diffusion models require in order to converge inKullback-Leibler~(KL) divergence is linear (up to logarithmic terms) in theintrinsic dimension $d$. Moreover, we show that this linear dependency issharp.</description><author>Peter Potaptchik, Iskander Azangulov, George Deligiannidis</author><pubDate>Fri, 11 Oct 2024 17:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09046v1</guid></item><item><title>MiRAGeNews: Multimodal Realistic AI-Generated News Detection</title><link>http://arxiv.org/abs/2410.09045v1</link><description>The proliferation of inflammatory or misleading "fake" news content hasbecome increasingly common in recent years. Simultaneously, it has becomeeasier than ever to use AI tools to generate photorealistic images depictingany scene imaginable. Combining these two -- AI-generated fake news content --is particularly potent and dangerous. To combat the spread of AI-generated fakenews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality realand AI-generated image-caption pairs from state-of-the-art generators. We findthat our dataset poses a significant challenge to humans (60% F-1) andstate-of-the-art multi-modal LLMs (&lt; 24% F-1). Using our dataset we train amulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-artbaselines on image-caption pairs from out-of-domain image generators and newspublishers. We release our code and data to aid future work on detectingAI-generated content.</description><author>Runsheng Huang, Liam Dugan, Yue Yang, Chris Callison-Burch</author><pubDate>Fri, 11 Oct 2024 17:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09045v1</guid></item><item><title>Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge Distillation Meets Explainable AI</title><link>http://arxiv.org/abs/2410.09043v1</link><description>In the evolving landscape of autonomous vehicles, ensuring robust in-vehiclenetwork (IVN) security is paramount. This paper introduces an advancedintrusion detection system (IDS) called KD-XVAE that uses a VariationalAutoencoder (VAE)-based knowledge distillation approach to enhance bothperformance and efficiency. Our model significantly reduces complexity,operating with just 1669 parameters and achieving an inference time of 0.3 msper batch, making it highly suitable for resource-constrained automotiveenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrateexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Scoreof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,Gear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 datasetfurther underscores its superiority over traditional machine learning models,achieving perfect detection metrics. We furthermore integrate Explainable AI(XAI) techniques to ensure transparency in the model's decisions. The VAEcompresses the original feature space into a latent space, on which thedistilled model is trained. SHAP(SHapley Additive exPlanations) values provideinsights into the importance of each latent dimension, mapped back to originalfeatures for intuitive understanding. Our paper advances the field byintegrating state-of-the-art techniques, addressing critical challenges in thedeployment of efficient, trustworthy, and reliable IDSes for autonomousvehicles, ensuring enhanced protection against emerging cyber threats.</description><author>Muhammet Anil Yagiz, Pedram MohajerAnsari, Mert D. Pese, Polat Goktas</author><pubDate>Fri, 11 Oct 2024 17:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09043v1</guid></item><item><title>AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation</title><link>http://arxiv.org/abs/2410.09040v1</link><description>This paper studies the vulnerabilities of transformer-based Large LanguageModels (LLMs) to jailbreaking attacks, focusing specifically on theoptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observea positive correlation between the effectiveness of attacks and the internalbehaviors of the models. For instance, attacks tend to be less effective whenmodels pay more attention to system prompts designed to ensure LLM safetyalignment. Building on this discovery, we introduce an enhanced method thatmanipulates models' attention scores to facilitate LLM jailbreaking, which weterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attackefficacy across diverse LLMs, achieving an average increase of ~7% in theLlama-2 series and ~10% in the Gemma series. Our strategy also demonstratesrobust attack transferability against both unseen harmful goals and black-boxLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-scorevisualization is more interpretable, allowing us to gain better insights intohow our targeted attention manipulation facilitates more effectivejailbreaking. We release the code athttps://github.com/UCSC-VLAA/AttnGCG-attack.</description><author>Zijun Wang, Haoqin Tu, Jieru Mei, Bingchen Zhao, Yisen Wang, Cihang Xie</author><pubDate>Fri, 11 Oct 2024 17:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09040v1</guid></item><item><title>Autonomous Underwater Robotic System for Aquaculture Applications</title><link>http://arxiv.org/abs/2308.14762v2</link><description>Aquaculture is a thriving food-producing sector producing over half of theglobal fish consumption. However, these aquafarms pose significant challengessuch as biofouling, vegetation, and holes within their net pens and have aprofound effect on the efficiency and sustainability of fish production.Currently, divers and/or remotely operated vehicles are deployed for inspectingand maintaining aquafarms; this approach is expensive and requires highlyskilled human operators. This work aims to develop a robotic-based automaticnet defect detection system for aquaculture net pens oriented to on- ROVprocessing and real-time detection of different aqua-net defects such asbiofouling, vegetation, net holes, and plastic. The proposed system integratesboth deep learning-based methods for aqua-net defect detection and feedbackcontrol law for the vehicle movement around the aqua-net to obtain a clearsequence of net images and inspect the status of the net via performing theinspection tasks. This work contributes to the area of aquaculture inspection,marine robotics, and deep learning aiming to reduce cost, improve quality, andease of operation.</description><author>Waseem Akram, Muhayyuddin Ahmed, Lakmal Seneviratne, Irfan Hussain</author><pubDate>Fri, 11 Oct 2024 17:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14762v2</guid></item><item><title>SimpleStrat: Diversifying Language Model Generation with Stratification</title><link>http://arxiv.org/abs/2410.09038v1</link><description>Generating diverse responses from large language models (LLMs) is crucial forapplications such as planning/search and synthetic data generation, wherediversity provides distinct answers across generations. Prior approaches relyon increasing temperature to increase diversity. However, contrary to popularbelief, we show not only does this approach produce lower quality individualgenerations as temperature increases, but it depends on model's next-tokenprobabilities being similar to the true distribution of answers. We propose\method{}, an alternative approach that uses the language model itself topartition the space into strata. At inference, a random stratum is selected anda sample drawn from within the strata. To measure diversity, we introduceCoverageQA, a dataset of underspecified questions with multiple equallyplausible answers, and assess diversity by measuring KL Divergence between theoutput distribution and uniform distribution over valid ground truth answers.As computing probability per response/solution for proprietary models isinfeasible, we measure recall on ground truth solutions. Our evaluation showusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36average reduction in KL Divergence compared to Llama 3.</description><author>Justin Wong, Yury Orlovskiy, Michael Luo, Sanjit A. Seshia, Joseph E. Gonzalez</author><pubDate>Fri, 11 Oct 2024 17:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09038v1</guid></item><item><title>Mentor-KD: Making Small Language Models Better Multi-step Reasoners</title><link>http://arxiv.org/abs/2410.09037v1</link><description>Large Language Models (LLMs) have displayed remarkable performances acrossvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,studies have proposed a Knowledge Distillation (KD) approach, reasoningdistillation, which transfers such reasoning ability of LLMs throughfine-tuning language models of multi-step rationales generated by LLM teachers.However, they have inadequately considered two challenges regardinginsufficient distillation sets from the LLM teacher model, in terms of 1) dataquality and 2) soft label provision. In this paper, we propose Mentor-KD, whicheffectively distills the multi-step reasoning capability of LLMs to smaller LMswhile addressing the aforementioned challenges. Specifically, we exploit amentor, intermediate-sized task-specific fine-tuned model, to augmentadditional CoT annotations and provide soft labels for the student model duringreasoning distillation. We conduct extensive experiments and confirmMentor-KD's effectiveness across various models and complex reasoning tasks.</description><author>Hojae Lee, Junho Kim, SangKeun Lee</author><pubDate>Fri, 11 Oct 2024 17:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09037v1</guid></item><item><title>PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents</title><link>http://arxiv.org/abs/2410.09034v1</link><description>Ptychography is an advanced computational imaging technique in X-ray andelectron microscopy. It has been widely adopted across scientific researchfields, including physics, chemistry, biology, and materials science, as wellas in industrial applications such as semiconductor characterization. Inpractice, obtaining high-quality ptychographic images requires simultaneousoptimization of numerous experimental and algorithmic parameters.Traditionally, parameter selection often relies on trial and error, leading tolow-throughput workflows and potential human bias. In this work, we develop the"Ptychographic Experiment and Analysis Robot" (PEAR), a framework thatleverages large language models (LLMs) to automate data analysis inptychography. To ensure high robustness and accuracy, PEAR employs multiple LLMagents for tasks including knowledge retrieval, code generation, parameterrecommendation, and image reasoning. Our study demonstrates that PEAR'smulti-agent design significantly improves the workflow success rate, even withsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports variousautomation levels and is designed to work with customized local knowledgebases, ensuring flexibility and adaptability across different researchenvironments.</description><author>Xiangyu Yin, Chuqiao Shi, Yimo Han, Yi Jiang</author><pubDate>Fri, 11 Oct 2024 17:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09034v1</guid></item><item><title>Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery</title><link>http://arxiv.org/abs/2410.09032v1</link><description>Millions of abandoned oil and gas wells are scattered across the world,leaching methane into the atmosphere and toxic compounds into the groundwater.Many of these locations are unknown, preventing the wells from being pluggedand their polluting effects averted. Remote sensing is a relatively unexploredtool for pinpointing abandoned wells at scale. We introduce the firstlarge-scale benchmark dataset for this problem, leveraging medium-resolutionmulti-spectral satellite imagery from Planet Labs. Our curated datasetcomprises over 213,000 wells (abandoned, suspended, and active) from Alberta, aregion with especially high well density, sourced from the Alberta EnergyRegulator and verified by domain experts. We evaluate baseline algorithms forwell detection and segmentation, showing the promise of computer visionapproaches but also significant room for improvement.</description><author>Pratinav Seth, Michelle Lin, Brefo Dwamena Yaw, Jade Boutot, Mary Kang, David Rolnick</author><pubDate>Fri, 11 Oct 2024 17:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09032v1</guid></item><item><title>Variance reduction combining pre-experiment and in-experiment data</title><link>http://arxiv.org/abs/2410.09027v1</link><description>Online controlled experiments (A/B testing) are essential in data-drivendecision-making for many companies. Increasing the sensitivity of theseexperiments, particularly with a fixed sample size, relies on reducing thevariance of the estimator for the average treatment effect (ATE). Existingmethods like CUPED and CUPAC use pre-experiment data to reduce variance, buttheir effectiveness depends on the correlation between the pre-experiment dataand the outcome. In contrast, in-experiment data is often more stronglycorrelated with the outcome and thus more informative. In this paper, weintroduce a novel method that combines both pre-experiment and in-experimentdata to achieve greater variance reduction than CUPED and CUPAC, withoutintroducing bias or additional computation complexity. We also establishasymptotic theory and provide consistent variance estimators for our method.Applying this method to multiple online experiments at Etsy, we reachsubstantial variance reduction over CUPAC with the inclusion of only a fewin-experiment covariates. These results highlight the potential of our approachto significantly improve experiment sensitivity and accelerate decision-making.</description><author>Zhexiao Lin, Pablo Crespo</author><pubDate>Fri, 11 Oct 2024 17:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09027v1</guid></item><item><title>IP-FL: Incentivized and Personalized Federated Learning</title><link>http://arxiv.org/abs/2304.07514v4</link><description>Existing incentive solutions for traditional Federated Learning (FL) focus onindividual contributions to a single global objective, neglecting the nuancesof clustered personalization with multiple cluster-level models and thenon-monetary incentives such as personalized model appeal for clients. In thispaper, we first propose to treat incentivization and personalization asinterrelated challenges and solve them with an incentive mechanism that fosterspersonalized learning. Additionally, current methods depend on an aggregatorfor client clustering, which is limited by a lack of access to clients'confidential information due to privacy constraints, leading to inaccurateclustering. To overcome this, we propose direct client involvement, allowingclients to indicate their cluster membership preferences based on datadistribution and incentive-driven feedback. Our approach enhances thepersonalized model appeal for self-aware clients with high-quality data leadingto their active and consistent participation. Our evaluation demonstratessignificant improvements in test accuracy (8-45%), personalized model appeal(3-38%), and participation rates (31-100%) over existing FL models, includingthose addressing data heterogeneity and personalization.</description><author>Ahmad Faraz Khan, Xinran Wang, Qi Le, Zain ul Abdeen, Azal Ahmad Khan, Haider Ali, Ming Jin, Jie Ding, Ali R. Butt, Ali Anwar</author><pubDate>Fri, 11 Oct 2024 17:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07514v4</guid></item><item><title>DeLLMa: Decision Making Under Uncertainty with Large Language Models</title><link>http://arxiv.org/abs/2402.02392v3</link><description>The potential of large language models (LLMs) as decision support tools isincreasingly being explored in fields such as business, engineering, andmedicine, which often face challenging tasks of decision-making underuncertainty. In this paper, we show that directly prompting LLMs on these typesof decision-making problems can yield poor results, especially as the problemcomplexity increases. To aid in these tasks, we propose DeLLMa (Decision-makingLarge Language Model assistant), a framework designed to enhancedecision-making accuracy in uncertain environments. DeLLMa involves amulti-step reasoning procedure that integrates recent best practices in scalinginference-time reasoning, drawing upon principles from decision theory andutility theory, to provide an accurate and human-auditable decision-makingprocess. We validate our procedure on multiple realistic decision-makingenvironments, demonstrating that DeLLMa can consistently enhance thedecision-making performance of leading language models, and achieve up to a 40%increase in accuracy over competing methods. Additionally, we show howperformance improves when scaling compute at test time, and carry out humanevaluations to benchmark components of DeLLMa.</description><author>Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger</author><pubDate>Fri, 11 Oct 2024 17:43:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02392v3</guid></item><item><title>Evaluating Copyright Takedown Methods for Language Models</title><link>http://arxiv.org/abs/2406.18664v4</link><description>Language models (LMs) derive their capabilities from extensive training ondiverse data, including potentially copyrighted material. These models canmemorize and generate content similar to their training data, posing potentialconcerns. Therefore, model creators are motivated to develop mitigation methodsthat prevent generating protected content. We term this procedure as copyrighttakedowns for LMs, noting the conceptual similarity to (but legal distinctionfrom) the DMCA takedown This paper introduces the first evaluation of thefeasibility and side effects of copyright takedowns for LMs. We proposeCoTaEval, an evaluation framework to assess the effectiveness of copyrighttakedown methods, the impact on the model's ability to retain uncopyrightablefactual knowledge from the training data whose recitation is embargoed, and howwell the model maintains its general utility and efficiency. We examine severalstrategies, including adding system prompts, decoding-time filteringinterventions, and unlearning approaches. Our findings indicate that no testedmethod excels across all metrics, showing significant room for research in thisunique problem setting and indicating potential unresolved challenges for livepolicy proposals.</description><author>Boyi Wei, Weijia Shi, Yangsibo Huang, Noah A. Smith, Chiyuan Zhang, Luke Zettlemoyer, Kai Li, Peter Henderson</author><pubDate>Fri, 11 Oct 2024 17:42:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18664v4</guid></item><item><title>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</title><link>http://arxiv.org/abs/2410.09024v1</link><description>The robustness of LLMs to jailbreak attacks, where users design prompts tocircumvent safety measures and misuse model capabilities, has been studiedprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- whichuse external tools and can execute multi-stage tasks -- may pose a greater riskif misused, but their robustness remains underexplored. To facilitate researchon LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmarkincludes a diverse set of 110 explicitly malicious agent tasks (440 withaugmentations), covering 11 harm categories including fraud, cybercrime, andharassment. In addition to measuring whether models refuse harmful agenticrequests, scoring well on AgentHarm requires jailbroken agents to maintaintheir capabilities following an attack to complete a multi-step task. Weevaluate a range of leading LLMs, and find (1) leading LLMs are surprisinglycompliant with malicious agent requests without jailbreaking, (2) simpleuniversal jailbreak templates can be adapted to effectively jailbreak agents,and (3) these jailbreaks enable coherent and malicious multi-step agentbehavior and retain model capabilities. We publicly release AgentHarm to enablesimple and reliable evaluation of attacks and defenses for LLM-based agents. Wepublicly release the benchmark athttps://huggingface.co/ai-safety-institute/AgentHarm.</description><author>Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies</author><pubDate>Fri, 11 Oct 2024 17:39:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09024v1</guid></item><item><title>MedMobile: A mobile-sized language model with expert-level clinical capabilities</title><link>http://arxiv.org/abs/2410.09019v1</link><description>Language models (LMs) have demonstrated expert-level reasoning and recallabilities in medicine. However, computational costs and privacy concerns aremounting barriers to wide-scale implementation. We introduce a parsimoniousadaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable ofrunning on a mobile device, for medical applications. We demonstrate thatMedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark forphysicians (~60%), and approaching the scores of models 100 times its size. Wesubsequently perform a careful set of ablations, and demonstrate that chain ofthought, ensembling, and fine-tuning lead to the greatest performance gains,while unexpectedly retrieval augmented generation fails to demonstratesignificant improvements</description><author>Krithik Vishwanath, Jaden Stryker, Anton Alaykin, Daniel Alexander Alber, Eric Karl Oermann</author><pubDate>Fri, 11 Oct 2024 17:32:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09019v1</guid></item><item><title>Parameter-Efficient Fine-Tuning of State Space Models</title><link>http://arxiv.org/abs/2410.09016v1</link><description>Deep State Space Models (SSMs), such as Mamba (Gu &amp; Dao, 2024), have emergedas powerful tools for language modeling, offering high performance withefficient inference and linear scaling in sequence length. However, theapplication of parameter-efficient fine-tuning (PEFT) methods to SSM-basedmodels remains largely unexplored. This paper aims to systematically study twokey questions: (i) How do existing PEFT methods perform on SSM-based models?(ii) Which modules are most effective for fine-tuning? We conduct an empiricalbenchmark of four basic PEFT methods on SSM-based models. Our findings revealthat prompt-based methods (e.g., prefix-tuning) are no longer effective, anempirical result further supported by theoretical analysis. In contrast, LoRAremains effective for SSM-based models. We further investigate the optimalapplication of LoRA within these models, demonstrating both theoretically andexperimentally that applying LoRA to linear projection matrices withoutmodifying SSM modules yields the best results, as LoRA is not effective attuning SSM modules. To further improve performance, we introduce LoRA withSelective Dimension tuning (SDLoRA), which selectively updates certain channelsand states on SSM modules while applying LoRA to linear projection matrices.Extensive experimental results show that this approach outperforms standardLoRA.</description><author>Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, Kangwook Lee</author><pubDate>Fri, 11 Oct 2024 17:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09016v1</guid></item><item><title>The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals</title><link>http://arxiv.org/abs/2410.09013v1</link><description>The glyphic writing system of Chinese incorporates information-rich visualfeatures in each character, such as radicals that provide hints about meaningor pronunciation. However, there has been no investigation into whethercontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) canharness these sub-character features in Chinese through prompting. In thisstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding ofvisual elements in Chinese characters, including radicals, compositionstructures, strokes, and stroke counts. Our results reveal that modelssurprisingly exhibit some, but still limited, knowledge of the visualinformation, regardless of whether images of characters are provided. To incitemodels' ability to use radicals, we further experiment with incorporatingradicals into the prompts for Chinese language understanding tasks. We observeconsistent improvement in Part-Of-Speech tagging when providing additionalinformation about radicals, suggesting the potential to enhance CLP byintegrating sub-character information.</description><author>Xiaofeng Wu, Karl Stratos, Wei Xu</author><pubDate>Fri, 11 Oct 2024 17:30:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09013v1</guid></item><item><title>Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models</title><link>http://arxiv.org/abs/2410.09012v1</link><description>Foundation models (FMs) such as large language models (LLMs) havesignificantly impacted many fields, including software engineering (SE). Theinteraction between SE and FMs has led to the integration of FMs into SEpractices (FM4SE) and the application of SE methodologies to FMs (SE4FM). Whileseveral literature surveys exist on academic contributions to these trends, weare the first to provide a practitioner's view. We analyze 155 FM4SE and 997SE4FM blog posts from leading technology companies, leveraging an FM-poweredsurveying approach to systematically label and summarize the discussedactivities and tasks. We observed that while code generation is the mostprominent FM4SE task, FMs are leveraged for many other SE activities such ascode understanding, summarization, and API recommendation. The majority of blogposts on SE4FM are about model deployment &amp; operation, and system architecture&amp; orchestration. Although the emphasis is on cloud deployments, there is agrowing interest in compressing FMs and deploying them on smaller devices suchas edge or mobile devices. We outline eight future research directions inspiredby our gained insights, aiming to bridge the gap between academic findings andreal-world applications. Our study not only enriches the body of knowledge onpractical applications of FM4SE and SE4FM but also demonstrates the utility ofFMs as a powerful and efficient approach in conducting literature surveyswithin technical and grey literature domains. Our dataset, results, code andused prompts can be found in our online replication package athttps://github.com/SAILResearch/fmse-blogs.</description><author>Hao Li, Cor-Paul Bezemer, Ahmed E. Hassan</author><pubDate>Fri, 11 Oct 2024 17:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09012v1</guid></item><item><title>CVAM-Pose: Conditional Variational Autoencoder for Multi-Object Monocular Pose Estimation</title><link>http://arxiv.org/abs/2410.09010v1</link><description>Estimating rigid objects' poses is one of the fundamental problems incomputer vision, with a range of applications across automation and augmentedreality. Most existing approaches adopt one network per object class strategy,depend heavily on objects' 3D models, depth data, and employ a time-consumingiterative refinement, which could be impractical for some applications. Thispaper presents a novel approach, CVAM-Pose, for multi-object monocular poseestimation that addresses these limitations. The CVAM-Pose method employs alabel-embedded conditional variational autoencoder network, to implicitlyabstract regularised representations of multiple objects in a singlelow-dimensional latent space. This autoencoding process uses only imagescaptured by a projective camera and is robust to objects' occlusion and sceneclutter. The classes of objects are one-hot encoded and embedded throughout thenetwork. The proposed label-embedded pose regression strategy interprets thelearnt latent space representations utilising continuous pose representations.Ablation tests and systematic evaluations demonstrate the scalability andefficiency of the CVAM-Pose method for multi-object scenarios. The proposedCVAM-Pose outperforms competing latent space approaches. For example, it isrespectively 25% and 20% better than AAE and Multi-Path methods, when evaluatedusing the $\mathrm{AR_{VSD}}$ metric on the Linemod-Occluded dataset. It alsoachieves results somewhat comparable to methods reliant on 3D models reportedin BOP challenges. Code available: https://github.com/JZhao12/CVAM-Pose</description><author>Jianyu Zhao, Wei Quan, Bogdan J. Matuszewski</author><pubDate>Fri, 11 Oct 2024 17:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09010v1</guid></item><item><title>Semantic Score Distillation Sampling for Compositional Text-to-3D Generation</title><link>http://arxiv.org/abs/2410.09009v1</link><description>Generating high-quality 3D assets from textual descriptions remains a pivotalchallenge in computer graphics and vision research. Due to the scarcity of 3Ddata, state-of-the-art approaches utilize pre-trained 2D diffusion priors,optimized through Score Distillation Sampling (SDS). Despite progress, craftingcomplex 3D scenes featuring multiple objects or intricate interactions is stilldifficult. To tackle this, recent methods have incorporated box or layoutguidance. However, these layout-guided compositional methods often struggle toprovide fine-grained control, as they are generally coarse and lackexpressiveness. To overcome these challenges, we introduce a novel SDSapproach, Semantic Score Distillation Sampling (SemanticSDS), designed toeffectively improve the expressiveness and accuracy of compositional text-to-3Dgeneration. Our approach integrates new semantic embeddings that maintainconsistency across different rendering views and clearly differentiate betweenvarious objects and parts. These embeddings are transformed into a semanticmap, which directs a region-specific SDS process, enabling precise optimizationand compositional generation. By leveraging explicit semantic guidance, ourmethod unlocks the compositional capabilities of existing pre-trained diffusionmodels, thereby achieving superior quality in 3D content generation,particularly for complex objects and scenes. Experimental results demonstratethat our SemanticSDS framework is highly effective for generatingstate-of-the-art complex 3D content. Code:https://github.com/YangLing0818/SemanticSDS-3D</description><author>Ling Yang, Zixiang Zhang, Junlin Han, Bohan Zeng, Runjia Li, Philip Torr, Wentao Zhang</author><pubDate>Fri, 11 Oct 2024 17:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09009v1</guid></item><item><title>SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights</title><link>http://arxiv.org/abs/2410.09008v1</link><description>Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shownsignificant improvements in various reasoning tasks. However, smaller modelssuch as Llama-3-8B and DeepSeekMath-Base still struggle with complexmathematical reasoning because they fail to effectively identify and correctreasoning errors. Recent reflection-based methods aim to address these issuesby enabling self-reflection and self-correction, but they still face challengesin independently detecting errors in their reasoning steps. To overcome theselimitations, we propose SuperCorrect, a novel two-stage framework that uses alarge teacher model to supervise and correct both the reasoning and reflectionprocesses of a smaller student model. In the first stage, we extracthierarchical high-level and detailed thought templates from the teacher modelto guide the student model in eliciting more fine-grained reasoning thoughts.In the second stage, we introduce cross-model collaborative direct preferenceoptimization (DPO) to enhance the self-correction abilities of the studentmodel by following the teacher's correction traces during training. Thiscross-model DPO approach teaches the student model to effectively locate andresolve erroneous thoughts with error-driven insights from the teacher model,breaking the bottleneck of its thoughts and acquiring new skills and knowledgeto tackle challenging problems. Extensive experiments consistently demonstrateour superiority over previous methods. Notably, our SuperCorrect-7B modelsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% andQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTAperformance among all 7B models. Code:https://github.com/YangLing0818/SuperCorrect-llm</description><author>Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan</author><pubDate>Fri, 11 Oct 2024 17:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09008v1</guid></item><item><title>Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra</title><link>http://arxiv.org/abs/2410.09005v1</link><description>Neural scaling laws describe how the performance of deep neural networksscales with key factors such as training data size, model complexity, andtraining time, often following power-law behaviors over multiple orders ofmagnitude. Despite their empirical observation, the theoretical understandingof these scaling laws remains limited. In this work, we employ techniques fromstatistical mechanics to analyze one-pass stochastic gradient descent within astudent-teacher framework, where both the student and teacher are two-layerneural networks. Our study primarily focuses on the generalization error andits behavior in response to data covariance matrices that exhibit power-lawspectra. For linear activation functions, we derive analytical expressions forthe generalization error, exploring different learning regimes and identifyingconditions under which power-law scaling emerges. Additionally, we extend ouranalysis to non-linear activation functions in the feature learning regime,investigating how power-law spectra in the data covariance matrix impactlearning dynamics. Importantly, we find that the length of the symmetricplateau depends on the number of distinct eigenvalues of the data covariancematrix and the number of hidden units, demonstrating how these plateaus behaveunder various configurations. In addition, our results reveal a transition fromexponential to power-law convergence in the specialized phase when the datacovariance matrix possesses a power-law spectrum. This work contributes to thetheoretical understanding of neural scaling laws and provides insights intooptimizing learning performance in practical scenarios involving complex datastructures.</description><author>Roman Worschech, Bernd Rosenow</author><pubDate>Fri, 11 Oct 2024 17:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09005v1</guid></item><item><title>Controlling Whisper: Universal Acoustic Adversarial Attacks to Control Speech Foundation Models</title><link>http://arxiv.org/abs/2407.04482v2</link><description>Speech enabled foundation models, either in the form of flexible speechrecognition based systems or audio-prompted large language models (LLMs), arebecoming increasingly popular. One of the interesting aspects of these modelsis their ability to perform tasks other than automatic speech recognition (ASR)using an appropriate prompt. For example, the OpenAI Whisper model can performboth speech transcription and speech translation. With the development ofaudio-prompted LLMs there is the potential for even greater control options. Inthis work we demonstrate that with this greater flexibility the systems can besusceptible to model-control adversarial attacks. Without any access to themodel prompt it is possible to modify the behaviour of the system byappropriately changing the audio input. To illustrate this risk, we demonstratethat it is possible to prepend a short universal adversarial acoustic segmentto any input speech signal to override the prompt setting of an ASR foundationmodel. Specifically, we successfully use a universal adversarial acousticsegment to control Whisper to always perform speech translation, despite beingset to perform speech transcription. Overall, this work demonstrates a new formof adversarial attack on multi-tasking speech enabled foundation models thatneeds to be considered prior to the deployment of this form of model.</description><author>Vyas Raina, Mark Gales</author><pubDate>Fri, 11 Oct 2024 17:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04482v2</guid></item><item><title>DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object Detection</title><link>http://arxiv.org/abs/2410.09004v1</link><description>Domain adaptive object detection (DAOD) aims to generalize detectors trainedon an annotated source domain to an unlabelled target domain. As thevisual-language models (VLMs) can provide essential general knowledge on unseenimages, freezing the visual encoder and inserting a domain-agnostic adapter canlearn domain-invariant knowledge for DAOD. However, the domain-agnostic adapteris inevitably biased to the source domain. It discards some beneficialknowledge discriminative on the unlabelled domain, i.e., domain-specificknowledge of the target domain. To solve the issue, we propose a novelDomain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point isexploiting domain-specific knowledge between the essential general knowledgeand domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter(DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter(DSA) for injecting the domain-specific knowledge from the informationdiscarded by the visual encoder. Comprehensive experiments over multiple DAODtasks show that DA-Ada can efficiently infer a domain-aware visual encoder forboosting domain adaptive object detection. Our code is available athttps://github.com/Therock90421/DA-Ada.</description><author>Haochen Li, Rui Zhang, Hantao Yao, Xin Zhang, Yifan Hao, Xinkai Song, Xiaqing Li, Yongwei Zhao, Ling Li, Yunji Chen</author><pubDate>Fri, 11 Oct 2024 17:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09004v1</guid></item><item><title>Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis</title><link>http://arxiv.org/abs/2409.05292v4</link><description>The world is currently experiencing an outbreak of mpox, which has beendeclared a Public Health Emergency of International Concern by WHO. No priorwork related to social media mining has focused on the development of a datasetof Instagram posts about the mpox outbreak. The work presented in this paperaims to address this research gap and makes two scientific contributions tothis field. First, it presents a multilingual dataset of 60,127 Instagram postsabout mpox, published between July 23, 2022, and September 5, 2024. Thedataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagramposts about mpox in 52 languages. For each of these posts, the Post ID, PostDescription, Date of publication, language, and translated version of the post(translation to English was performed using the Google Translate API) arepresented as separate attributes in the dataset. After developing this dataset,sentiment analysis, hate speech detection, and anxiety or stress detection wereperformed. This process included classifying each post into (i) one of thesentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, orneutral, (ii) hate or not hate, and (iii) anxiety/stress detected or noanxiety/stress detected. These results are presented as separate attributes inthe dataset. Second, this paper presents the results of performing sentimentanalysis, hate speech analysis, and anxiety or stress analysis. The variationof the sentiment classes - fear, surprise, joy, sadness, anger, disgust, andneutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and50.64%, respectively. In terms of hate speech detection, 95.75% of the postsdid not contain hate and the remaining 4.25% of the posts contained hate.Finally, 72.05% of the posts did not indicate any anxiety/stress, and theremaining 27.95% of the posts represented some form of anxiety/stress.</description><author>Nirmalya Thakur</author><pubDate>Fri, 11 Oct 2024 17:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05292v4</guid></item><item><title>LLM-Generated Black-box Explanations Can Be Adversarially Helpful</title><link>http://arxiv.org/abs/2405.06800v3</link><description>Large Language Models (LLMs) are becoming vital tools that help us solve andunderstand complex problems by acting as digital assistants. LLMs can generateconvincing explanations, even when only given the inputs and outputs of theseproblems, i.e., in a ``black-box'' approach. However, our research uncovers ahidden risk tied to this approach, which we call *adversarial helpfulness*.This happens when an LLM's explanations make a wrong answer look right,potentially leading people to trust incorrect solutions. In this paper, we showthat this issue affects not just humans, but also LLM evaluators. Diggingdeeper, we identify and examine key persuasive strategies employed by LLMs. Ourfindings reveal that these models employ strategies such as reframing thequestions, expressing an elevated level of confidence, and cherry-pickingevidence to paint misleading answers in a credible light. To examine if LLMsare able to navigate complex-structured knowledge when generating adversariallyhelpful explanations, we create a special task based on navigating throughgraphs. Most LLMs are not able to find alternative paths along simple graphs,indicating that their misleading explanations aren't produced by only logicaldeductions using complex knowledge. These findings shed light on thelimitations of the black-box explanation setting and allow us to provide adviceon the safe usage of LLMs.</description><author>Rohan Ajwani, Shashidhar Reddy Javaji, Frank Rudzicz, Zining Zhu</author><pubDate>Fri, 11 Oct 2024 17:16:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06800v3</guid></item><item><title>xTED: Cross-Domain Adaptation via Diffusion-Based Trajectory Editing</title><link>http://arxiv.org/abs/2409.08687v2</link><description>Reusing pre-collected data from different domains is an appealing solutionfor decision-making tasks that have insufficient data in the target domain butare relatively abundant in other related domains. Existing cross-domain policytransfer methods mostly aim at learning domain correspondences or correctionsto facilitate policy learning, such as learning domain/task-specificdiscriminators, representations, or policies. This design philosophy oftenresults in heavy model architectures or task/domain-specific modeling, lackingflexibility. This reality makes us wonder: can we directly bridge the domaingaps universally at the data level, instead of relying on complex downstreamcross-domain policy transfer models? In this study, we propose the Cross-DomainTrajectory EDiting (xTED) framework that employs a specially designed diffusionmodel for cross-domain trajectory adaptation. Our proposed model architectureeffectively captures the intricate dependencies among states, actions, andrewards, as well as the dynamics patterns within target data. By utilizing thepre-trained diffusion as a prior, source domain trajectories can be transformedto match with target domain properties while preserving original semanticinformation. This process implicitly corrects underlying domain gaps, enhancingstate realism and dynamics reliability in the source data, and allowingflexible incorporation with various downstream policy learning methods. Despiteits simplicity, xTED demonstrates superior performance in extensive simulationand real-robot experiments.</description><author>Haoyi Niu, Qimao Chen, Tenglong Liu, Jianxiong Li, Guyue Zhou, Yi Zhang, Jianming Hu, Xianyuan Zhan</author><pubDate>Fri, 11 Oct 2024 17:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08687v2</guid></item><item><title>A Feature Generator for Few-Shot Learning</title><link>http://arxiv.org/abs/2409.14141v2</link><description>Few-shot learning (FSL) aims to enable models to recognize novel objects orclasses with limited labelled data. Feature generators, which synthesize newdata points to augment limited datasets, have emerged as a promising solutionto this challenge. This paper investigates the effectiveness of featuregenerators in enhancing the embedding process for FSL tasks. To address theissue of inaccurate embeddings due to the scarcity of images per class, weintroduce a feature generator that creates visual features from class-leveltextual descriptions. By training the generator with a combination ofclassifier loss, discriminator loss, and distance loss between the generatedfeatures and true class embeddings, we ensure the generation of accuratesame-class features and enhance the overall feature representation. Our resultsshow a significant improvement in accuracy over baseline methods, with ourapproach outperforming the baseline model by 10% in 1-shot and around 5% in5-shot approaches. Additionally, both visual-only and visual + textualgenerators have also been tested in this paper. The code is publicly availableat https://github.com/heethanjan/Feature-Generator-for-FSL.</description><author>Heethanjan Kanagalingam, Thenukan Pathmanathan, Navaneethan Ketheeswaran, Mokeeshan Vathanakumar, Mohamed Afham, Ranga Rodrigo</author><pubDate>Fri, 11 Oct 2024 17:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14141v2</guid></item><item><title>CDAN: Convolutional dense attention-guided network for low-light image enhancement</title><link>http://arxiv.org/abs/2308.12902v3</link><description>Low-light images, characterized by inadequate illumination, pose challengesof diminished clarity, muted colors, and reduced details. Low-light imageenhancement, an essential task in computer vision, aims to rectify these issuesby improving brightness, contrast, and overall perceptual quality, therebyfacilitating accurate analysis and interpretation. This paper introduces theConvolutional Dense Attention-guided Network (CDAN), a novel solution forenhancing low-light images. CDAN integrates an autoencoder-based architecturewith convolutional and dense blocks, complemented by an attention mechanism andskip connections. This architecture ensures efficient information propagationand feature learning. Furthermore, a dedicated post-processing phase refinescolor balance and contrast. Our approach demonstrates notable progress comparedto state-of-the-art results in low-light image enhancement, showcasing itsrobustness across a wide range of challenging scenarios. Our model performsremarkably on benchmark datasets, effectively mitigating under-exposure andproficiently restoring textures and colors in diverse low-light scenarios. Thisachievement underscores CDAN's potential for diverse computer vision tasks,notably enabling robust object detection and recognition in challenginglow-light conditions.</description><author>Hossein Shakibania, Sina Raoufi, Hassan Khotanlou</author><pubDate>Fri, 11 Oct 2024 17:12:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12902v3</guid></item><item><title>Hierarchical Universal Value Function Approximators</title><link>http://arxiv.org/abs/2410.08997v1</link><description>There have been key advancements to building universal approximators formulti-goal collections of reinforcement learning value functions -- keyelements in estimating long-term returns of states in a parameterized manner.We extend this to hierarchical reinforcement learning, using the optionsframework, by introducing hierarchical universal value function approximators(H-UVFAs). This allows us to leverage the added benefits of scaling, planning,and generalization expected in temporal abstraction settings. We developsupervised and reinforcement learning methods for learning embeddings of thestates, goals, options, and actions in the two hierarchical value functions:$Q(s, g, o; \theta)$ and $Q(s, g, o, a; \theta)$. Finally we demonstrategeneralization of the HUVFAs and show they outperform corresponding UVFAs.</description><author>Rushiv Arora</author><pubDate>Fri, 11 Oct 2024 17:09:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08997v1</guid></item><item><title>Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference</title><link>http://arxiv.org/abs/2410.08996v1</link><description>We test whether replacing crowdsource workers with LLMs to write NaturalLanguage Inference (NLI) hypotheses similarly results in annotation artifacts.We recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 andMistral 7b, and train hypothesis-only classifiers to determine whetherLLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLIdatasets, BERT-based hypothesis-only classifiers achieve between 86-96%accuracy, indicating these datasets contain hypothesis-only artifacts. We alsofind frequent "give-aways" in LLM-generated hypotheses, e.g. the phrase"swimming in a pool" appears in more than 10,000 contradictions generated byGPT-4. Our analysis provides empirical evidence that well-attested biases inNLI can persist in LLM-generated data.</description><author>Grace Proebsting, Adam Poliak</author><pubDate>Fri, 11 Oct 2024 17:09:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08996v1</guid></item><item><title>Optimal Downsampling for Imbalanced Classification with Generalized Linear Models</title><link>http://arxiv.org/abs/2410.08994v1</link><description>Downsampling or under-sampling is a technique that is utilized in the contextof large and highly imbalanced classification models. We study optimaldownsampling for imbalanced classification using generalized linear models(GLMs). We propose a pseudo maximum likelihood estimator and study itsasymptotic normality in the context of increasingly imbalanced populationsrelative to an increasingly large sample size. We provide theoreticalguarantees for the introduced estimator. Additionally, we compute the optimaldownsampling rate using a criterion that balances statistical accuracy andcomputational efficiency. Our numerical experiments, conducted on bothsynthetic and empirical data, further validate our theoretical results, anddemonstrate that the introduced estimator outperforms commonly availablealternatives.</description><author>Yan Chen, Jose Blanchet, Krzysztof Dembczynski, Laura Fee Nern, Aaron Flores</author><pubDate>Fri, 11 Oct 2024 17:08:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08994v1</guid></item><item><title>The structure of the token space for large language models</title><link>http://arxiv.org/abs/2410.08993v1</link><description>Large language models encode the correlational structure present in naturallanguage by fitting segments of utterances (tokens) into a high dimensionalambient latent space upon which the models then operate. We assert that inorder to develop a foundational, first-principles understanding of the behaviorand limitations of large language models, it is crucial to understand thetopological and geometric structure of this token subspace. In this article, wepresent estimators for the dimension and Ricci scalar curvature of the tokensubspace, and apply it to three open source large language models of moderatesize: GPT2, LLEMMA7B, and MISTRAL7B. In all three models, using thesemeasurements, we find that the token subspace is not a manifold, but is insteada stratified manifold, where on each of the individual strata, the Riccicurvature is significantly negative. We additionally find that the dimensionand curvature correlate with generative fluency of the models, which suggestthat these findings have implications for model behavior.</description><author>Michael Robinson, Sourya Dey, Shauna Sweet</author><pubDate>Fri, 11 Oct 2024 17:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08993v1</guid></item><item><title>Science is Exploration: Computational Frontiers for Conceptual Metaphor Theory</title><link>http://arxiv.org/abs/2410.08991v1</link><description>Metaphors are everywhere. They appear extensively across all domains ofnatural language, from the most sophisticated poetry to seemingly dry academicprose. A significant body of research in the cognitive science of languageargues for the existence of conceptual metaphors, the systematic structuring ofone domain of experience in the language of another. Conceptual metaphors arenot simply rhetorical flourishes but are crucial evidence of the role ofanalogical reasoning in human cognition. In this paper, we ask whether LargeLanguage Models (LLMs) can accurately identify and explain the presence of suchconceptual metaphors in natural language data. Using a novel promptingtechnique based on metaphor annotation guidelines, we demonstrate that LLMs area promising tool for large-scale computational research on conceptualmetaphors. Further, we show that LLMs are able to apply procedural guidelinesdesigned for human annotators, displaying a surprising depth of linguisticknowledge.</description><author>Rebecca M. M. Hicke, Ross Deans Kristensen-McLachlan</author><pubDate>Fri, 11 Oct 2024 17:03:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08991v1</guid></item><item><title>An Ontology-based Approach Towards Traceable Behavior Specifications in Automated Driving</title><link>http://arxiv.org/abs/2409.06607v2</link><description>Vehicles in public traffic that are equipped with Automated Driving Systemsare subject to a number of expectations: Among other aspects, their behaviorshould be safe, conforming to the rules of the road and provide mobility totheir users. This poses challenges for the developers of such systems:Developers are responsible for specifying this behavior, for example, in termsof requirements at system design time. As we will discuss in the article, thisspecification always involves the need for assumptions and trade-offs. As aresult, insufficiencies in such a behavior specification can occur that canpotentially lead to unsafe system behavior. In order to support theidentification of specification insufficiencies, requirements and respectiveassumptions need to be made explicit. In this article, we propose the SemanticNorm Behavior Analysis as an ontology-based approach to specify the behaviorfor an Automated Driving System equipped vehicle. We use ontologies to formallyrepresent specified behavior for a targeted operational environment, and toestablish traceability between specified behavior and the addressed stakeholderneeds. Furthermore, we illustrate the application of the Semantic Norm BehaviorAnalysis in a German legal context with two example scenarios and evaluate ourresults. Our evaluation shows that the explicit documentation of assumptions inthe behavior specification supports both the identification of specificationinsufficiencies and their treatment. Therefore, this article providesrequirements, terminology and an according methodology to facilitateontology-based behavior specifications in automated driving.</description><author>Nayel Fabian Salem, Marcus Nolte, Veronica Haber, Till Menzel, Hans Steege, Robert Graubohm, Markus Maurer</author><pubDate>Fri, 11 Oct 2024 17:02:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06607v2</guid></item><item><title>SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning</title><link>http://arxiv.org/abs/2410.08989v1</link><description>Fine-tuning Large Language Models (LLMs) has proven effective for a varietyof downstream tasks. However, as LLMs grow in size, the memory demands forbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimizationmethods offer a memory-efficient alternative by using forward passes toestimate gradients, but the variance of gradient estimates typically scaleslinearly with the model's parameter dimension$\unicode{x2013}$a significantissue for LLMs. In this paper, we propose the random Subspace Zeroth-order(SubZero) optimization to address the challenges posed by LLMs' highdimensionality. We introduce a low-rank perturbation tailored for LLMs thatsignificantly reduces memory consumption while improving training performance.Additionally, we prove that our gradient estimation closely approximates thebackpropagation gradient, exhibits lower variance than traditional ZO methods,and ensures convergence when combined with SGD. Experimental results show thatSubZero enhances fine-tuning performance and achieves faster convergencecompared to standard ZO approaches like MeZO across various language modelingtasks.</description><author>Ziming Yu, Pan Zhou, Sike Wang, Jia Li, Hua Huang</author><pubDate>Fri, 11 Oct 2024 17:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08989v1</guid></item><item><title>Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware Perspective</title><link>http://arxiv.org/abs/2410.08985v1</link><description>Recently, Knowledge Graphs (KGs) have been successfully coupled with LargeLanguage Models (LLMs) to mitigate their hallucinations and enhance theirreasoning capability, such as in KG-based retrieval-augmented frameworks.However, current KG-LLM frameworks lack rigorous uncertainty estimation,limiting their reliable deployment in high-stakes applications. Directlyincorporating uncertainty quantification into KG-LLM frameworks presentschallenges due to their complex architectures and the intricate interactionsbetween the knowledge graph and language model components. To address this gap,we propose a new trustworthy KG-LLM framework, Uncertainty AwareKnowledge-Graph Reasoning (UAG), which incorporates uncertainty quantificationinto the KG-LLM framework. We design an uncertainty-aware multi-step reasoningframework that leverages conformal prediction to provide a theoreticalguarantee on the prediction set. To manage the error rate of the multi-stepprocess, we additionally introduce an error rate control module to adjust theerror rate within the individual components. Extensive experiments show thatour proposed UAG can achieve any pre-defined coverage rate while reducing theprediction set/interval size by 40% on average over the baselines.</description><author>Bo Ni, Yu Wang, Lu Cheng, Erik Blasch, Tyler Derr</author><pubDate>Fri, 11 Oct 2024 16:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08985v1</guid></item><item><title>DEL: Discrete Element Learner for Learning 3D Particle Dynamics with Neural Rendering</title><link>http://arxiv.org/abs/2410.08983v1</link><description>Learning-based simulators show great potential for simulating particledynamics when 3D groundtruth is available, but per-particle correspondences arenot always accessible. The development of neural rendering presents a newsolution to this field to learn 3D dynamics from 2D images by inverserendering. However, existing approaches still suffer from ill-posed naturesresulting from the 2D to 3D uncertainty, for example, specific 2D images cancorrespond with various 3D particle distributions. To mitigate suchuncertainty, we consider a conventional, mechanically interpretable frameworkas the physical priors and extend it to a learning-based version. In brief, weincorporate the learnable graph kernels into the classic Discrete ElementAnalysis (DEA) framework to implement a novel mechanics-integrated learningsystem. In this case, the graph network kernels are only used for approximatingsome specific mechanical operators in the DEA framework rather than the wholedynamics mapping. By integrating the strong physics priors, our methods caneffectively learn the dynamics of various materials from the partial 2Dobservations in a unified manner. Experiments show that our approachoutperforms other learned simulators by a large margin in this context and isrobust to different renderers, fewer training samples, and fewer camera views.</description><author>Jiaxu Wang, Jingkai Sun, Junhao He, Ziyi Zhang, Qiang Zhang, Mingyuan Sun, Renjing Xu</author><pubDate>Fri, 11 Oct 2024 16:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08983v1</guid></item><item><title>Hedging and Approximate Truthfulness in Traditional Forecasting Competitions</title><link>http://arxiv.org/abs/2409.19477v2</link><description>In forecasting competitions, the traditional mechanism scores the predictionsof each contestant against the outcome of each event, and the contestant withthe highest total score wins. While it is well-known that this traditionalmechanism can suffer from incentive issues, it is folklore that contestantswill still be roughly truthful as the number of events grows. Yet thus far theliterature lacks a formal analysis of this traditional mechanism. This papergives the first such analysis. We first demonstrate that the ''long-runtruthfulness'' folklore is false: even for arbitrary numbers of events, thebest forecaster can have an incentive to hedge, reporting more moderate beliefsto increase their win probability. On the positive side, however, we show thattwo contestants will be approximately truthful when they have sufficientuncertainty over the relative quality of their opponent and the outcomes of theevents, a case which may arise in practice.</description><author>Mary Monroe, Anish Thilagar, Melody Hsu, Rafael Frongillo</author><pubDate>Fri, 11 Oct 2024 16:56:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19477v2</guid></item><item><title>Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control</title><link>http://arxiv.org/abs/2410.08979v1</link><description>Reinforcement learning (RL) is rapidly reaching and surpassing human-levelcontrol capabilities. However, state-of-the-art RL algorithms often requiretimesteps and reaction times significantly faster than human capabilities,which is impractical in real-world settings and typically necessitatesspecialized hardware. Such speeds are difficult to achieve in the real worldand often requires specialized hardware. We introduce Sequence ReinforcementLearning (SRL), an RL algorithm designed to produce a sequence of actions for agiven input state, enabling effective control at lower decision frequencies.SRL addresses the challenges of learning action sequences by employing both amodel and an actor-critic architecture operating at different temporal scales.We propose a "temporal recall" mechanism, where the critic uses the model toestimate intermediate states between primitive actions, providing a learningsignal for each individual action within the sequence. Once training iscomplete, the actor can generate action sequences independently of the model,achieving model-free control at a slower frequency. We evaluate SRL on a suiteof continuous control tasks, demonstrating that it achieves performancecomparable to state-of-the-art algorithms while significantly reducing actorsample complexity. To better assess performance across varying decisionfrequencies, we introduce the Frequency-Averaged Score (FAS) metric. Ourresults show that SRL significantly outperforms traditional RL algorithms interms of FAS, making it particularly suitable for applications requiringvariable decision frequencies. Additionally, we compare SRL with model-basedonline planning, showing that SRL achieves superior FAS while leveraging thesame model during training that online planners use for planning.</description><author>Devdhar Patel, Hava Siegelmann</author><pubDate>Fri, 11 Oct 2024 16:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08979v1</guid></item><item><title>Online-to-PAC generalization bounds under graph-mixing dependencies</title><link>http://arxiv.org/abs/2410.08977v1</link><description>Traditional generalization results in statistical learning require a trainingdata set made of independently drawn examples. Most of the recent efforts torelax this independence assumption have considered either purely temporal(mixing) dependencies, or graph-dependencies, where non-adjacent verticescorrespond to independent random variables. Both approaches have their ownlimitations, the former requiring a temporal ordered structure, and the latterlacking a way to quantify the strength of inter-dependencies. In this work, webridge these two lines of work by proposing a framework where dependenciesdecay with graph distance. We derive generalization bounds leveraging theonline-to-PAC framework, by deriving a concentration result and introducing anonline learning framework incorporating the graph structure. The resultinghigh-probability generalization guarantees depend on both the mixing rate andthe graph's chromatic number.</description><author>Baptiste Abélès, Eugenio Clerico, Gergely Neu</author><pubDate>Fri, 11 Oct 2024 16:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08977v1</guid></item><item><title>Learning Representations of Instruments for Partial Identification of Treatment Effects</title><link>http://arxiv.org/abs/2410.08976v1</link><description>Reliable estimation of treatment effects from observational data is importantin many disciplines such as medicine. However, estimation is challenging whenunconfoundedness as a standard assumption in the causal inference literature isviolated. In this work, we leverage arbitrary (potentially high-dimensional)instruments to estimate bounds on the conditional average treatment effect(CATE). Our contributions are three-fold: (1) We propose a novel approach forpartial identification through a mapping of instruments to a discreterepresentation space so that we yield valid bounds on the CATE. This is crucialfor reliable decision-making in real-world applications. (2) We derive atwo-step procedure that learns tight bounds using a tailored neuralpartitioning of the latent instrument space. As a result, we avoid instabilityissues due to numerical approximations or adversarial training. Furthermore,our procedure aims to reduce the estimation variance in finite-sample settingsto yield more reliable estimates. (3) We show theoretically that our procedureobtains valid bounds while reducing estimation variance. We further performextensive experiments to demonstrate the effectiveness across various settings.Overall, our procedure offers a novel path for practitioners to make use ofpotentially high-dimensional instruments (e.g., as in Mendelian randomization).</description><author>Jonas Schweisthal, Dennis Frauen, Maresa Schröder, Konstantin Hess, Niki Kilbertus, Stefan Feuerriegel</author><pubDate>Fri, 11 Oct 2024 16:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08976v1</guid></item><item><title>UniGlyph: A Seven-Segment Script for Universal Language Representation</title><link>http://arxiv.org/abs/2410.08974v1</link><description>UniGlyph is a constructed language (conlang) designed to create a universaltransliteration system using a script derived from seven-segment characters.The goal of UniGlyph is to facilitate cross-language communication by offeringa flexible and consistent script that can represent a wide range of phoneticsounds. This paper explores the design of UniGlyph, detailing its scriptstructure, phonetic mapping, and transliteration rules. The system addressesimperfections in the International Phonetic Alphabet (IPA) and traditionalcharacter sets by providing a compact, versatile method to represent phoneticdiversity across languages. With pitch and length markers, UniGlyph ensuresaccurate phonetic representation while maintaining a small character set.Applications of UniGlyph include artificial intelligence integrations, such asnatural language processing and multilingual speech recognition, enhancingcommunication across different languages. Future expansions are discussed,including the addition of animal phonetic sounds, where unique scripts areassigned to different species, broadening the scope of UniGlyph beyond humancommunication. This study presents the challenges and solutions in developingsuch a universal script, demonstrating the potential of UniGlyph to bridgelinguistic gaps in cross-language communication, educational phonetics, andAI-driven applications.</description><author>G. V. Bency Sherin, A. Abijesh Euphrine, A. Lenora Moreen, L. Arun Jose</author><pubDate>Fri, 11 Oct 2024 16:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08974v1</guid></item><item><title>ALVIN: Active Learning Via INterpolation</title><link>http://arxiv.org/abs/2410.08972v1</link><description>Active Learning aims to minimize annotation effort by selecting the mostuseful instances from a pool of unlabeled data. However, typical activelearning methods overlook the presence of distinct example groups within aclass, whose prevalence may vary, e.g., in occupation classification datasetscertain demographics are disproportionately represented in specific classes.This oversight causes models to rely on shortcuts for predictions, i.e.,spurious correlations between input attributes and labels occurring inwell-represented groups. To address this issue, we propose Active Learning ViaINterpolation (ALVIN), which conducts intra-class interpolations betweenexamples from under-represented and well-represented groups to create anchors,i.e., artificial points situated between the example groups in therepresentation space. By selecting instances close to the anchors forannotation, ALVIN identifies informative examples exposing the model to regionsof the representation space that counteract the influence of shortcuts.Crucially, since the model considers these examples to be of high certainty,they are likely to be ignored by typical active learning methods. Experimentalresults on six datasets encompassing sentiment analysis, natural languageinference, and paraphrase detection demonstrate that ALVIN outperformsstate-of-the-art active learning methods in both in-distribution andout-of-distribution generalization.</description><author>Michalis Korakakis, Andreas Vlachos, Adrian Weller</author><pubDate>Fri, 11 Oct 2024 16:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08972v1</guid></item><item><title>Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures</title><link>http://arxiv.org/abs/2410.08971v1</link><description>In this paper, we propose an extension to Longformer Encoder-Decoder, apopular sparse transformer architecture. One common challenge with sparsetransformers is that they can struggle with encoding of long range context,such as connections between topics discussed at a beginning and end of adocument. A method to selectively increase global attention is proposed anddemonstrated for abstractive summarization tasks on several benchmark datasets. By prefixing the transcript with additional keywords and encoding globalattention on these keywords, improvement in zero-shot, few-shot, and fine-tunedcases is demonstrated for some benchmark data sets.</description><author>Evan Lucas, Dylan Kangas, Timothy C Havens</author><pubDate>Fri, 11 Oct 2024 16:41:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08971v1</guid></item><item><title>NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models</title><link>http://arxiv.org/abs/2410.08970v1</link><description>Hallucinations in Large Language Models (LLMs) remain a major obstacle,particularly in high-stakes applications where factual accuracy is critical.While representation editing and reading methods have made strides in reducinghallucinations, their heavy reliance on specialised tools and training onin-domain samples, makes them difficult to scale and prone to overfitting. Thislimits their accuracy gains and generalizability to diverse datasets. Thispaper presents a lightweight method, Norm Voting (NoVo), which harnesses theuntapped potential of attention head norms to dramatically enhance factualaccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins byautomatically selecting truth-correlated head norms with an efficient,inference-only algorithm using only 30 random samples, allowing NoVo toeffortlessly scale to diverse datasets. Afterwards, selected head norms areemployed in a simple voting algorithm, which yields significant gains inprediction accuracy. On TruthfulQA MC1, NoVo surpasses the currentstate-of-the-art and all previous methods by an astounding margin -- at least19 accuracy points. NoVo demonstrates exceptional generalization to 20 diversedatasets, with significant gains in over 90\% of them, far exceeding allcurrent representation editing and reading methods. NoVo also reveals promisinggains to finetuning strategies and building textual adversarial defence. NoVo'seffectiveness with head norms opens new frontiers in LLM interpretability,robustness and reliability.</description><author>Zheng Yi Ho, Siyuan Liang, Sen Zhang, Yibing Zhan, Dacheng Tao</author><pubDate>Fri, 11 Oct 2024 16:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08970v1</guid></item><item><title>Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements</title><link>http://arxiv.org/abs/2410.08968v1</link><description>The current paradigm for safety alignment of large language models (LLMs)follows a one-size-fits-all approach: the model refuses to interact with anycontent deemed unsafe by the model provider. This approach lacks flexibility inthe face of varying social norms across cultures and regions. In addition,users may have diverse safety needs, making a model with static safetystandards too restrictive to be useful, as well as too costly to be re-aligned. We propose Controllable Safety Alignment (CoSA), a framework designed toadapt models to diverse safety requirements without re-training. Instead ofaligning a fixed model, we align models to follow safety configs -- free-formnatural language descriptions of the desired safety behaviors -- that areprovided as part of the system prompt. To adjust model safety behavior,authorized users only need to modify such safety configs at inference time. Toenable that, we propose CoSAlign, a data-centric method for aligning LLMs toeasily adapt to diverse safety configs. Furthermore, we devise a novelcontrollability evaluation protocol that considers both helpfulness andconfigured safety, summarizing them into CoSA-Score, and construct CoSApien, ahuman-authored benchmark that consists of real-world LLM use cases with diversesafety requirements and corresponding evaluation prompts. We show that CoSAlign leads to substantial gains of controllability overstrong baselines including in-context alignment. Our framework encouragesbetter representation and adaptation to pluralistic human values in LLMs, andthereby increasing their practicality.</description><author>Jingyu Zhang, Ahmed Elgohary, Ahmed Magooda, Daniel Khashabi, Benjamin Van Durme</author><pubDate>Fri, 11 Oct 2024 16:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08968v1</guid></item><item><title>Language Imbalance Driven Rewarding for Multilingual Self-improving</title><link>http://arxiv.org/abs/2410.08964v1</link><description>Large Language Models (LLMs) have achieved state-of-the-art performanceacross numerous tasks. However, these advancements have predominantly benefited"first-class" languages such as English and Chinese, leaving many otherlanguages underrepresented. This imbalance, while limiting broaderapplications, generates a natural preference ranking between languages,offering an opportunity to bootstrap the multilingual capabilities of LLM in aself-improving manner. Thus, we propose $\textit{Language Imbalance DrivenRewarding}$, where the inherent imbalance between dominant and non-dominantlanguages within LLMs is leveraged as a reward signal. Iterative DPO trainingdemonstrates that this approach not only enhances LLM performance innon-dominant languages but also improves the dominant language's capacity,thereby yielding an iterative reward signal. Fine-tuningMeta-Llama-3-8B-Instruct over two iterations of this approach results incontinuous improvements in multilingual performance acrossinstruction-following and arithmetic reasoning tasks, evidenced by an averageimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%accuracy on the MGSM benchmark. This work serves as an initial exploration,paving the way for multilingual self-improvement of LLMs.</description><author>Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang</author><pubDate>Fri, 11 Oct 2024 16:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08964v1</guid></item><item><title>LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch</title><link>http://arxiv.org/abs/2409.02969v3</link><description>Multiobjective optimization problems (MOPs) are prevalent in machinelearning, with applications in multi-task learning, learning under fairness orrobustness constraints, etc. Instead of reducing multiple objective functionsinto a scalar objective, MOPs aim to optimize for the so-called Paretooptimality or Pareto set learning, which involves optimizing more than oneobjective function simultaneously, over models with thousands / millions ofparameters. Existing benchmark libraries for MOPs mainly focus on evolutionaryalgorithms, most of which are zeroth-order / meta-heuristic methods that do noteffectively utilize higher-order information from objectives and cannot scaleto large-scale models with thousands / millions of parameters. In light of theabove gap, this paper introduces LibMOON, the first multiobjective optimizationlibrary that supports state-of-the-art gradient-based methods, provides a fairbenchmark, and is open-sourced for the community.</description><author>Xiaoyuan Zhang, Liang Zhao, Yingying Yu, Xi Lin, Yifan Chen, Han Zhao, Qingfu Zhang</author><pubDate>Fri, 11 Oct 2024 16:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02969v3</guid></item><item><title>Scaling Instructable Agents Across Many Simulated Worlds</title><link>http://arxiv.org/abs/2404.10179v3</link><description>Building embodied AI systems that can follow arbitrary language instructionsin any 3D environment is a key challenge for creating general AI. Accomplishingthis goal requires learning to ground language in perception and embodiedactions, in order to accomplish complex tasks. The Scalable, Instructable,Multiworld Agent (SIMA) project tackles this by training agents to followfree-form instructions across a diverse range of virtual 3D environments,including curated research environments as well as open-ended, commercial videogames. Our goal is to develop an instructable agent that can accomplishanything a human can do in any simulated 3D environment. Our approach focuseson language-driven generality while imposing minimal assumptions. Our agentsinteract with environments in real-time using a generic, human-like interface:the inputs are image observations and language instructions and the outputs arekeyboard-and-mouse actions. This general approach is challenging, but it allowsagents to ground language across many visually complex and semantically richenvironments while also allowing us to readily run agents in new environments.In this paper we describe our motivation and goal, the initial progress we havemade, and promising preliminary results on several diverse researchenvironments and a variety of commercial video games.</description><author>SIMA Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Kshitij Gupta, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew A. Hudson, Jony Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Rory Lawton, Andrew Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph</author><pubDate>Fri, 11 Oct 2024 16:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10179v3</guid></item><item><title>Evaluating Federated Kolmogorov-Arnold Networks on Non-IID Data</title><link>http://arxiv.org/abs/2410.08961v1</link><description>Federated Kolmogorov-Arnold Networks (F-KANs) have already been proposed, buttheir assessment is at an initial stage. We present a comparison between KANs(using B-splines and Radial Basis Functions as activation functions) and Multi-Layer Perceptrons (MLPs) with a similar number of parameters for 100 rounds offederated learning in the MNIST classification task using non-IID partitionswith 100 clients. After 15 trials for each model, we show that the bestaccuracies achieved by MLPs can be achieved by Spline-KANs in half of the time(in rounds), with just a moderate increase in computing time.</description><author>Arthur Mendonça Sasse, Claudio Miceli de Farias</author><pubDate>Fri, 11 Oct 2024 16:30:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08961v1</guid></item><item><title>Interpretable Contrastive Monte Carlo Tree Search Reasoning</title><link>http://arxiv.org/abs/2410.01707v2</link><description>We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoningalgorithm for Large Language Models (LLMs), significantly improves bothreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLMreasoning works often overlooked its biggest drawback--slower speed compared toCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning onvarious tasks with limited quantitative analysis or ablation studies of itscomponents from reasoning interpretability perspective. 3. The reward model isthe most crucial component in MCTS, however previous work has rarely conductedin-depth study or improvement of MCTS's reward models. Thus, we conductedextensive ablation studies and quantitative analysis on components of MCTS,revealing the impact of each component on the MCTS reasoning performance ofLLMs. Building on this, (i) we designed a highly interpretable reward modelbased on the principle of contrastive decoding and (ii) achieved an averagespeed improvement of 51.9% per node using speculative decoding. Additionally,(iii) we improved UCT node selection strategy and backpropagation used inprevious works, resulting in significant performance improvement. Weoutperformed o1-mini by an average of 17.4% on the Blocksworld multi-stepreasoning dataset using Llama-3.1-70B with SC-MCTS*. Our code is available at\url{https://github.com/zitian-gao/SC-MCTS}.</description><author>Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, Lijie Wen</author><pubDate>Fri, 11 Oct 2024 16:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01707v2</guid></item><item><title>Fusing Echocardiography Images and Medical Records for Continuous Patient Stratification</title><link>http://arxiv.org/abs/2401.07796v2</link><description>Deep learning enables automatic and robust extraction of cardiac functiondescriptors from echocardiographic sequences, such as ejection fraction orstrain. These descriptors provide fine-grained information that physiciansconsider, in conjunction with more global variables from the clinical record,to assess patients' condition. Drawing on novel transformer models applied totabular data, we propose a method that considers all descriptors extracted frommedical records and echocardiograms to learn the representation of acardiovascular pathology with a difficult-to-characterize continuum, namelyhypertension. Our method first projects each variable into its ownrepresentation space using modality-specific approaches. These standardizedrepresentations of multimodal data are then fed to a transformer encoder, whichlearns to merge them into a comprehensive representation of the patient throughthe task of predicting a clinical rating. This stratification task isformulated as an ordinal classification to enforce a pathological continuum inthe representation space. We observe the major trends along this continuum on acohort of 239 hypertensive patients, providing unprecedented details in thedescription of hypertension's impact on various cardiac function descriptors.Our analysis shows that i) the XTab foundation model's architecture allows toreach outstanding performance (98% AUROC) even with limited data (less than 200training samples), ii) stratification across the population is reproduciblebetween trainings (within 3.6% MAE), and iii) patterns emerge in descriptors,some of which align with established physiological knowledge abouthypertension, while others could pave the way for a more comprehensiveunderstanding of this pathology.</description><author>Nathan Painchaud, Jérémie Stym-Popper, Pierre-Yves Courand, Nicolas Thome, Pierre-Marc Jodoin, Nicolas Duchateau, Olivier Bernard</author><pubDate>Fri, 11 Oct 2024 16:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07796v2</guid></item><item><title>Lifted Coefficient of Determination: Fast model-free prediction intervals and likelihood-free model comparison</title><link>http://arxiv.org/abs/2410.08958v1</link><description>We propose the $\textit{lifted linear model}$, and derive model-freeprediction intervals that become tighter as the correlation between predictionsand observations increases. These intervals motivate the $\textit{LiftedCoefficient of Determination}$, a model comparison criterion for arbitrary lossfunctions in prediction-based settings, e.g., regression, classification orcounts. We extend the prediction intervals to more general error distributions,and propose a fast model-free outlier detection algorithm for regression.Finally, we illustrate the framework via numerical experiments.</description><author>Daniel Salnikov, Kevin Michalewicz, Dan Leonte</author><pubDate>Fri, 11 Oct 2024 16:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08958v1</guid></item><item><title>Rapid Grassmannian Averaging with Chebyshev Polynomials</title><link>http://arxiv.org/abs/2410.08956v1</link><description>We propose new algorithms to efficiently average a collection of points on aGrassmannian manifold in both the centralized and decentralized settings.Grassmannian points are used ubiquitously in machine learning, computer vision,and signal processing to represent data through (often low-dimensional)subspaces. While averaging these points is crucial to many tasks (especially inthe decentralized setting), existing methods unfortunately remaincomputationally expensive due to the non-Euclidean geometry of the manifold.Our proposed algorithms, Rapid Grassmannian Averaging (RGrAv) and DecentralizedRapid Grassmannian Averaging (DRGrAv), overcome this challenge by leveragingthe spectral structure of the problem to rapidly compute an average using onlysmall matrix multiplications and QR factorizations. We provide a theoreticalguarantee of optimality and present numerical experiments which demonstratethat our algorithms outperform state-of-the-art methods in providing highaccuracy solutions in minimal time. Additional experiments showcase theversatility of our algorithms to tasks such as K-means clustering on videomotion data, establishing RGrAv and DRGrAv as powerful tools for genericGrassmannian averaging.</description><author>Brighton Ancelin, Alex Saad-Falcon, Kason Ancelin, Justin Romberg</author><pubDate>Fri, 11 Oct 2024 16:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08956v1</guid></item><item><title>Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs</title><link>http://arxiv.org/abs/2406.20086v3</link><description>LLMs process text as sequences of tokens that roughly correspond to words,where less common words are represented by multiple tokens. However, individualtokens are often semantically unrelated to the meanings of the words/conceptsthey comprise. For example, Llama-2-7b's tokenizer splits the word"northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of whichcorrespond to semantically meaningful units like "north" or "east." Similarly,the overall meanings of named entities like "Neil Young" and multi-wordexpressions like "break a leg" cannot be directly inferred from theirconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groupsof tokens into useful higher-level representations? In this work, we find thatlast token representations of named entities and multi-token words exhibit apronounced "erasure" effect, where information about previous and currenttokens is rapidly forgotten in early layers. Using this observation, we proposea method to "read out" the implicit vocabulary of an autoregressive LLM byexamining differences in token representations across layers, and presentresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this isthe first attempt to probe the implicit vocabulary of an LLM.</description><author>Sheridan Feucht, David Atkinson, Byron Wallace, David Bau</author><pubDate>Fri, 11 Oct 2024 16:20:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.20086v3</guid></item><item><title>PostMark: A Robust Blackbox Watermark for Large Language Models</title><link>http://arxiv.org/abs/2406.14517v2</link><description>The most effective techniques to detect LLM-generated text rely on insertinga detectable signature -- or watermark -- during the model's decoding process.Most existing watermarking methods require access to the underlying LLM'slogits, which LLM API providers are loath to share due to fears of modeldistillation. As such, these watermarks must be implemented independently byeach LLM provider. In this paper, we develop PostMark, a modular post-hocwatermarking procedure in which an input-dependent set of words (determined viaa semantic embedding) is inserted into the text after the decoding process hascompleted. Critically, PostMark does not require logit access, which means itcan be implemented by a third party. We also show that PostMark is more robustto paraphrasing attacks than existing watermarking methods: our experimentscover eight baseline algorithms, five base LLMs, and three datasets. Finally,we evaluate the impact of PostMark on text quality using both automated andhuman assessments, highlighting the trade-off between quality and robustness toparaphrasing. We release our code, outputs, and annotations athttps://github.com/lilakk/PostMark.</description><author>Yapei Chang, Kalpesh Krishna, Amir Houmansadr, John Wieting, Mohit Iyyer</author><pubDate>Fri, 11 Oct 2024 16:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14517v2</guid></item><item><title>Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow</title><link>http://arxiv.org/abs/2410.07303v2</link><description>Diffusion models have greatly improved visual generation but are hindered byslow generation speed due to the computationally intensive nature of solvinggenerative ODEs. Rectified flow, a widely recognized solution, improvesgeneration speed by straightening the ODE path. Its key components include: 1)using the diffusion form of flow-matching, 2) employing $\boldsymbolv$-prediction, and 3) performing rectification (a.k.a. reflow). In this paper,we argue that the success of rectification primarily lies in using a pretraineddiffusion model to obtain matched pairs of noise and samples, followed byretraining with these matched noise-sample pairs. Based on this, components 1)and 2) are unnecessary. Furthermore, we highlight that straightness is not anessential training target for rectification; rather, it is a specific case offlow-matching models. The more critical training target is to achieve afirst-order approximate ODE path, which is inherently curved for models likeDDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion,which generalizes the design space and application scope of rectification toencompass the broader category of diffusion models, rather than beingrestricted to flow-matching models. We validate our method on Stable Diffusionv1-5 and Stable Diffusion XL. Our method not only greatly simplifies thetraining procedure of rectified flow-based previous works (e.g., InstaFlow) butalso achieves superior performance with even lower training cost. Our code isavailable at https://github.com/G-U-N/Rectified-Diffusion.</description><author>Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, Hongsheng Li</author><pubDate>Fri, 11 Oct 2024 16:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07303v2</guid></item><item><title>On the Adversarial Transferability of Generalized "Skip Connections"</title><link>http://arxiv.org/abs/2410.08950v1</link><description>Skip connection is an essential ingredient for modern deep models to bedeeper and more powerful. Despite their huge success in normal scenarios(state-of-the-art classification performance on natural examples), weinvestigate and identify an interesting property of skip connections underadversarial scenarios, namely, the use of skip connections allows easiergeneration of highly transferable adversarial examples. Specifically, inResNet-like models (with skip connections), we find that using more gradientsfrom the skip connections rather than the residual modules according to a decayfactor during backpropagation allows one to craft adversarial examples withhigh transferability. The above method is termed as Skip Gradient Method (SGM).Although starting from ResNet-like models in vision domains, we further extendSGM to more advanced architectures, including Vision Transformers (ViTs) andmodels with length-varying paths and other domains, i.e. natural languageprocessing. We conduct comprehensive transfer attacks against various modelsincluding ResNets, Transformers, Inceptions, Neural Architecture Search, andLarge Language Models (LLMs). We show that employing SGM can greatly improvethe transferability of crafted attacks in almost all cases. Furthermore,considering the big complexity for practical use, we further demonstrate thatSGM can even improve the transferability on ensembles of models or targetedattacks and the stealthiness against current defenses. At last, we providetheoretical explanations and empirical insights on how SGM works. Our findingsnot only motivate new adversarial research into the architecturalcharacteristics of models but also open up further challenges for secure modelarchitecture design. Our code is available at https://github.com/mo666666/SGM.</description><author>Yisen Wang, Yichuan Mo, Dongxian Wu, Mingjie Li, Xingjun Ma, Zhouchen Lin</author><pubDate>Fri, 11 Oct 2024 16:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08950v1</guid></item><item><title>Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</title><link>http://arxiv.org/abs/2410.06820v2</link><description>Physics-informed deep learning often faces optimization challenges due to thecomplexity of solving partial differential equations (PDEs), which involveexploring large solution spaces, require numerous iterations, and can lead tounstable training. These challenges arise particularly from theill-conditioning of the optimization problem, caused by the differential termsin the loss function. To address these issues, we propose learning a solver,i.e., solving PDEs using a physics-informed iterative algorithm trained ondata. Our method learns to condition a gradient descent algorithm thatautomatically adapts to each PDE instance, significantly accelerating andstabilizing the optimization process and enabling faster convergence ofphysics-aware models. Furthermore, while traditional physics-informed methodssolve for a single PDE instance, our approach addresses parametric PDEs.Specifically, our method integrates the physical loss gradient with the PDEparameters to solve over a distribution of PDE parameters, includingcoefficients, initial conditions, or boundary conditions. We demonstrate theeffectiveness of our method through empirical experiments on multiple datasets,comparing training and test-time optimization performance.</description><author>Lise Le Boudec, Emmanuel de Bezenac, Louis Serrano, Ramon Daniel Regueiro-Espino, Yuan Yin, Patrick Gallinari</author><pubDate>Fri, 11 Oct 2024 16:17:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06820v2</guid></item><item><title>Transferable Belief Model on Quantum Circuits</title><link>http://arxiv.org/abs/2410.08949v1</link><description>The transferable belief model, as a semantic interpretation ofDempster-Shafer theory, enables agents to perform reasoning and decision makingin imprecise and incomplete environments. The model offers distinct semanticsfor handling unreliable testimonies, allowing for a more reasonable and generalprocess of belief transfer compared to the Bayesian approach. However, becauseboth the belief masses and the structure of focal sets must be considered whenupdating belief functions-leading to extra computational complexity duringreasoning-the transferable belief model has gradually lost favor amongresearchers in recent developments. In this paper, we implement thetransferable belief model on quantum circuits and demonstrate that belieffunctions offer a more concise and effective alternative to Bayesian approacheswithin the quantum computing framework. Furthermore, leveraging the uniquecharacteristics of quantum computing, we propose several novel belief transferapproaches. More broadly, this paper introduces a new perspective on basicinformation representation for quantum AI models, suggesting that belieffunctions are more suitable than Bayesian approach for handling uncertainty onquantum circuits.</description><author>Qianli Zhou, Hao Luo, Lipeng Pan, Yong Deng, Eloi Bosse</author><pubDate>Fri, 11 Oct 2024 16:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08949v1</guid></item><item><title>Meta-Transfer Learning Empowered Temporal Graph Networks for Cross-City Real Estate Appraisal</title><link>http://arxiv.org/abs/2410.08947v1</link><description>Real estate appraisal is important for a variety of endeavors such as realestate deals, investment analysis, and real property taxation. Recently, deeplearning has shown great promise for real estate appraisal by harnessingsubstantial online transaction data from web platforms. Nonetheless, deeplearning is data-hungry, and thus it may not be trivially applicable toenormous small cities with limited data. To this end, we propose Meta-TransferLearning Empowered Temporal Graph Networks (MetaTransfer) to transfer valuableknowledge from multiple data-rich metropolises to the data-scarce city toimprove valuation performance. Specifically, by modeling the ever-growing realestate transactions with associated residential communities as a temporal eventheterogeneous graph, we first design an Event-Triggered Temporal Graph Networkto model the irregular spatiotemporal correlations between evolving real estatetransactions. Besides, we formulate the city-wide real estate appraisal as amulti-task dynamic graph link label prediction problem, where the valuation ofeach community in a city is regarded as an individual task. AHypernetwork-Based Multi-Task Learning module is proposed to simultaneouslyfacilitate intra-city knowledge sharing between multiple communities andtask-specific parameters generation to accommodate the community-wise realestate price distribution. Furthermore, we propose a Tri-Level OptimizationBased Meta- Learning framework to adaptively re-weight training transactioninstances from multiple source cities to mitigate negative transfer, and thusimprove the cross-city knowledge transfer effectiveness. Finally, extensiveexperiments based on five real-world datasets demonstrate the significantsuperiority of MetaTransfer compared with eleven baseline algorithms.</description><author>Weijia Zhang, Jindong Han, Hao Liu, Wei Fan, Hao Wang, Hui Xiong</author><pubDate>Fri, 11 Oct 2024 16:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08947v1</guid></item><item><title>The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points</title><link>http://arxiv.org/abs/2410.08948v1</link><description>Social conventions are the foundation for social and economic life. Aslegions of AI agents increasingly interact with each other and with humans,their ability to form shared conventions will determine how effectively theywill coordinate behaviors, integrate into society and influence it. Here, weinvestigate the dynamics of conventions within populations of Large LanguageModel (LLM) agents using simulated interactions. First, we show that globallyaccepted social conventions can spontaneously arise from local interactionsbetween communicating LLMs. Second, we demonstrate how strong collective biasescan emerge during this process, even when individual agents appear to beunbiased. Third, we examine how minority groups of committed LLMs can drivesocial change by establishing new social conventions. We show that once theseminority groups reach a critical size, they can consistently overturnestablished behaviors. In all cases, contrasting the experimental results withpredictions from a minimal multi-agent model allows us to isolate the specificrole of LLM agents. Our results clarify how AI systems can autonomously developnorms without explicit programming and have implications for designing AIsystems that align with human values and societal goals.</description><author>Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli</author><pubDate>Fri, 11 Oct 2024 16:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08948v1</guid></item><item><title>CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes</title><link>http://arxiv.org/abs/2409.05166v3</link><description>We present CD-NGP, which is a fast and scalable representation for 3Dreconstruction and novel view synthesis in dynamic scenes. Inspired bycontinual learning, our method first segments input videos into multiplechunks, followed by training the model chunk by chunk, and finally, fusesfeatures of the first branch and subsequent branches. Experiments on theprevailing DyNeRF dataset demonstrate that our proposed novel representationreaches a great balance between memory consumption, model size, training speed,and rendering quality. Specifically, our method consumes $85\%$ less trainingmemory ($&lt;14$GB) than offline methods and requires significantly lowerstreaming bandwidth ($&lt;0.4$MB/frame) than other online alternatives.</description><author>Zhenhuan Liu, Shuai Liu, Zhiwei Ning, Jie Yang, Wei Liu</author><pubDate>Fri, 11 Oct 2024 16:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05166v3</guid></item><item><title>Mixed-type Distance Shrinkage and Selection for Clustering via Kernel Metric Learning</title><link>http://arxiv.org/abs/2306.01890v3</link><description>Distance-based clustering and classification are widely used in variousfields to group mixed numeric and categorical data. In many algorithms, apredefined distance measurement is used to cluster data points based on theirdissimilarity. While there exist numerous distance-based measures for data withpure numerical attributes and several ordered and unordered categoricalmetrics, an efficient and accurate distance for mixed-type data that utilizesthe continuous and discrete properties simulatenously is an open problem. Manymetrics convert numerical attributes to categorical ones or vice versa. Theyhandle the data points as a single attribute type or calculate a distancebetween each attribute separately and add them up. We propose a metric calledKDSUM that uses mixed kernels to measure dissimilarity, with cross-validatedoptimal bandwidth selection. We demonstrate that KDSUM is a shrinkage methodfrom existing mixed-type metrics to a uniform dissimilarity metric, andimproves clustering accuracy when utilized in existing distance-basedclustering algorithms on simulated and real-world datasets containingcontinuous-only, categorical-only, and mixed-type data.</description><author>Jesse S. Ghashti, John R. J. Thompson</author><pubDate>Fri, 11 Oct 2024 16:16:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01890v3</guid></item><item><title>Parallel Watershed Partitioning: GPU-Based Hierarchical Image Segmentation</title><link>http://arxiv.org/abs/2410.08946v1</link><description>Many image processing applications rely on partitioning an image intodisjoint regions whose pixels are 'similar.' The watershed and waterfalltransforms are established mathematical morphology pixel clustering techniques.They are both relevant to modern applications where groups of pixels are to bedecided upon in one go, or where adjacency information is relevant. Weintroduce three new parallel partitioning algorithms for GPUs. By repeatedlyapplying watershed algorithms, we produce waterfall results which form ahierarchy of partition regions over an input image. Our watershed algorithmsattain competitive execution times in both 2D and 3D, processing an 800megavoxel image in less than 1.4 sec. We also show how to use this fullydeterministic image partitioning as a pre-processing step to machine learningbased semantic segmentation. This replaces the role of superpixel algorithms,and results in comparable accuracy and faster training times.</description><author>Varduhi Yeghiazaryan, Yeva Gabrielyan, Irina Voiculescu</author><pubDate>Fri, 11 Oct 2024 16:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08946v1</guid></item><item><title>NeLLCom-X: A Comprehensive Neural-Agent Framework to Simulate Language Learning and Group Communication</title><link>http://arxiv.org/abs/2407.13999v2</link><description>Recent advances in computational linguistics include simulating the emergenceof human-like languages with interacting neural network agents, starting fromsets of random symbols. The recently introduced NeLLCom framework (Lian et al.,2023) allows agents to first learn an artificial language and then use it tocommunicate, with the aim of studying the emergence of specific linguisticsproperties. We extend this framework (NeLLCom-X) by introducing more realisticrole-alternating agents and group communication in order to investigate theinterplay between language learnability, communication pressures, and groupsize effects. We validate NeLLCom-X by replicating key findings from priorresearch simulating the emergence of a word-order/case-marking trade-off. Next,we investigate how interaction affects linguistic convergence and emergence ofthe trade-off. The novel framework facilitates future simulations of diverselinguistic aspects, emphasizing the importance of interaction and groupdynamics in language evolution.</description><author>Yuchen Lian, Tessa Verhoef, Arianna Bisazza</author><pubDate>Fri, 11 Oct 2024 16:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13999v2</guid></item><item><title>Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory</title><link>http://arxiv.org/abs/2410.08942v1</link><description>Synthetic data has gained attention for training large language models, butpoor-quality data can harm performance (see, e.g., Shumailov et al. (2023);Seddik et al. (2024)). A potential solution is data pruning, which retains onlyhigh-quality data based on a score function (human or machine feedback).Previous work Feng et al. (2024) analyzed models trained on synthetic data assample size increases. We extend this by using random matrix theory to derivethe performance of a binary classifier trained on a mix of real and prunedsynthetic data in a high dimensional setting. Our findings identify conditionswhere synthetic data could improve performance, focusing on the quality of thegenerative model and verification strategy. We also show a smooth phasetransition in synthetic label noise, contrasting with prior sharp behavior ininfinite sample limits. Experiments with toy models and large language modelsvalidate our theoretical results.</description><author>Aymane El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Reda Alami, Ahmed Alzubaidi, Hakim Hacid</author><pubDate>Fri, 11 Oct 2024 16:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08942v1</guid></item><item><title>MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality Rendering</title><link>http://arxiv.org/abs/2410.08941v1</link><description>Recently, 3D Gaussian splatting has gained attention for its capability togenerate high-fidelity rendering results. At the same time, most applicationssuch as games, animation, and AR/VR use mesh-based representations to representand render 3D scenes. We propose a novel approach that integrates meshrepresentation with 3D Gaussian splats to perform high-quality rendering ofreconstructed real-world scenes. In particular, we introduce a distance-basedGaussian splatting technique to align the Gaussian splats with the mesh surfaceand remove redundant Gaussian splats that do not contribute to the rendering.We consider the distance between each Gaussian splat and the mesh surface todistinguish between tightly-bound and loosely-bound Gaussian splats. Thetightly-bound splats are flattened and aligned well with the mesh geometry. Theloosely-bound Gaussian splats are used to account for the artifacts inreconstructed 3D meshes in terms of rendering. We present a training strategyof binding Gaussian splats to the mesh geometry, and take into account bothtypes of splats. In this context, we introduce several regularizationtechniques aimed at precisely aligning tightly-bound Gaussian splats with themesh surface during the training process. We validate the effectiveness of ourmethod on large and unbounded scene from mip-NeRF 360 and Deep Blendingdatasets. Our method surpasses recent mesh-based neural rendering techniques byachieving a 2dB higher PSNR, and outperforms mesh-based Gaussian splattingmethods by 1.3 dB PSNR, particularly on the outdoor mip-NeRF 360 dataset,demonstrating better rendering quality. We provide analyses for each type ofGaussian splat and achieve a reduction in the number of Gaussian splats by 30%compared to the original 3D Gaussian splatting.</description><author>Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Heesung Kwon, Dinesh Manocha</author><pubDate>Fri, 11 Oct 2024 16:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08941v1</guid></item><item><title>A tutorial on automatic differentiation with complex numbers</title><link>http://arxiv.org/abs/2409.06752v2</link><description>Automatic differentiation is everywhere, but there exists only minimaldocumentation of how it works in complex arithmetic beyond stating "derivativesin $\mathbb{C}^d$" $\cong$ "derivatives in $\mathbb{R}^{2d}$" and, at best,shallow references to Wirtinger calculus. Unfortunately, the equivalence$\mathbb{C}^d \cong \mathbb{R}^{2d}$ becomes insufficient as soon as we need toderive custom gradient rules, e.g., to avoid differentiating "through"expensive linear algebra functions or differential equation simulators. Tocombat such a lack of documentation, this article surveys forward- andreverse-mode automatic differentiation with complex numbers, covering topicssuch as Wirtinger derivatives, a modified chain rule, and different gradientconventions while explicitly avoiding holomorphicity and the Cauchy--Riemannequations (which would be far too restrictive). To be precise, we will derive,explain, and implement a complex version of Jacobian-vector and vector-Jacobianproducts almost entirely with linear algebra without relying on complexanalysis or differential geometry. This tutorial is a call to action, for usersand developers alike, to take complex values seriously when implementing customgradient propagation rules -- the manuscript explains how.</description><author>Nicholas Krämer</author><pubDate>Fri, 11 Oct 2024 16:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06752v2</guid></item><item><title>Match me if you can: Semi-Supervised Semantic Correspondence Learning with Unpaired Images</title><link>http://arxiv.org/abs/2311.18540v2</link><description>Semantic correspondence methods have advanced to obtaining high-qualitycorrespondences employing complicated networks, aiming to maximize the modelcapacity. However, despite the performance improvements, they may remainconstrained by the scarcity of training keypoint pairs, a consequence of thelimited training images and the sparsity of keypoints. This paper builds on thehypothesis that there is an inherent data-hungry matter in learning semanticcorrespondences and uncovers the models can be more trained by employingdensified training pairs. We demonstrate a simple machine annotator reliablyenriches paired key points via machine supervision, requiring neither extralabeled key points nor trainable modules from unlabeled images. Consequently,our models surpass current state-of-the-art models on semantic correspondencelearning benchmarks like SPair-71k, PF-PASCAL, and PF-WILLOW and enjoy furtherrobustness on corruption benchmarks. Our code is available athttps://github.com/naver-ai/matchme.</description><author>Jiwon Kim, Byeongho Heo, Sangdoo Yun, Seungryong Kim, Dongyoon Han</author><pubDate>Fri, 11 Oct 2024 16:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18540v2</guid></item><item><title>Linear-cost unbiased posterior estimates for crossed effects and matrix factorization models via couplings</title><link>http://arxiv.org/abs/2410.08939v1</link><description>We design and analyze unbiased Markov chain Monte Carlo (MCMC) schemes basedon couplings of blocked Gibbs samplers (BGSs), whose total computational costsscale linearly with the number of parameters and data points. Our methodologyis designed for and applicable to high-dimensional BGS with conditionallyindependent blocks, which are often encountered in Bayesian modeling. Weprovide bounds on the expected number of iterations needed for coalescence forGaussian targets, which imply that practical two-step coupling strategiesachieve coalescence times that match the relaxation times of the original BGSscheme up to a logarithmic factor. To illustrate the practical relevance of ourmethodology, we apply it to high-dimensional crossed random effect andprobabilistic matrix factorization models, for which we develop a novel BGSscheme with improved convergence speed. Our methodology provides unbiasedposterior estimates at linear cost (usually requiring only a few BGS iterationsfor problems with thousands of parameters), matching state-of-the-artprocedures for both frequentist and Bayesian estimation of those models.</description><author>Paolo Maria Ceriani, Giacomo Zanella</author><pubDate>Fri, 11 Oct 2024 16:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08939v1</guid></item><item><title>Streaming Diffusion Policy: Fast Policy Synthesis with Variable Noise Diffusion Models</title><link>http://arxiv.org/abs/2406.04806v4</link><description>Diffusion models have seen rapid adoption in robotic imitation learning,enabling autonomous execution of complex dexterous tasks. However, actionsynthesis is often slow, requiring many steps of iterative denoising, limitingthe extent to which models can be used in tasks that require fast reactivepolicies. To sidestep this, recent works have explored how the distillation ofthe diffusion process can be used to accelerate policy synthesis. However,distillation is computationally expensive and can hurt both the accuracy anddiversity of synthesized actions. We propose SDP (Streaming Diffusion Policy),an alternative method to accelerate policy synthesis, leveraging the insightthat generating a partially denoised action trajectory is substantially fasterthan a full output action trajectory. At each observation, our approach outputsa partially denoised action trajectory with variable levels of noisecorruption, where the immediate action to execute is noise-free, withsubsequent actions having increasing levels of noise and uncertainty. Thepartially denoised action trajectory for a new observation can then be quicklygenerated by applying a few steps of denoising to the previously predictednoisy action trajectory (rolled over by one timestep). We illustrate theefficacy of this approach, dramatically speeding up policy synthesis whilepreserving performance across both simulated and real-world settings.</description><author>Sigmund H. Høeg, Yilun Du, Olav Egeland</author><pubDate>Fri, 11 Oct 2024 16:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04806v4</guid></item><item><title>For a semiotic AI: Bridging computer vision and visual semiotics for computational observation of large scale facial image archives</title><link>http://arxiv.org/abs/2407.03268v2</link><description>Social networks are creating a digital world in which the cognitive,emotional, and pragmatic value of the imagery of human faces and bodies isarguably changing. However, researchers in the digital humanities are oftenill-equipped to study these phenomena at scale. This work presents FRESCO (FaceRepresentation in E-Societies through Computational Observation), a frameworkdesigned to explore the socio-cultural implications of images on social mediaplatforms at scale. FRESCO deconstructs images into numerical and categoricalvariables using state-of-the-art computer vision techniques, aligning with theprinciples of visual semiotics. The framework analyzes images across threelevels: the plastic level, encompassing fundamental visual features like linesand colors; the figurative level, representing specific entities or concepts;and the enunciation level, which focuses particularly on constructing the pointof view of the spectator and observer. These levels are analyzed to discerndeeper narrative layers within the imagery. Experimental validation confirmsthe reliability and utility of FRESCO, and we assess its consistency andprecision across two public datasets. Subsequently, we introduce the FRESCOscore, a metric derived from the framework's output that serves as a reliablemeasure of similarity in image content.</description><author>Lia Morra, Antonio Santangelo, Pietro Basci, Luca Piano, Fabio Garcea, Fabrizio Lamberti, Massimo Leone</author><pubDate>Fri, 11 Oct 2024 16:03:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03268v2</guid></item><item><title>KinDEL: DNA-Encoded Library Dataset for Kinase Inhibitors</title><link>http://arxiv.org/abs/2410.08938v1</link><description>DNA-Encoded Libraries (DEL) are combinatorial small molecule libraries thatoffer an efficient way to characterize diverse chemical spaces. Selectionexperiments using DELs are pivotal to drug discovery efforts, enablinghigh-throughput screens for hit finding. However, limited availability ofpublic DEL datasets hinders the advancement of computational techniquesdesigned to process such data. To bridge this gap, we present KinDEL, one ofthe first large, publicly available DEL datasets on two kinases:Mitogen-Activated Protein Kinase 14 (MAPK14) and Discoidin Domain ReceptorTyrosine Kinase 1 (DDR1). Interest in this data modality is growing due to itsability to generate extensive supervised chemical data that densely samplesaround select molecular structures. Demonstrating one such application of thedata, we benchmark different machine learning techniques to develop predictivemodels for hit identification; in particular, we highlight recentstructure-based probabilistic approaches. Finally, we provide biophysical assaydata, both on- and off-DNA, to validate our models on a smaller subset ofmolecules. Data and code for our benchmarks can be found at:https://github.com/insitro/kindel.</description><author>Benson Chen, Tomasz Danel, Patrick J. McEnaney, Nikhil Jain, Kirill Novikov, Spurti Umesh Akki, Joshua L. Turnbull, Virja Atul Pandya, Boris P. Belotserkovskii, Jared Bryce Weaver, Ankita Biswas, Dat Nguyen, Gabriel H. S. Dreiman, Mohammad Sultan, Nathaniel Stanley, Daniel M Whalen, Divya Kanichar, Christoph Klein, Emily Fox, R. Edward Watts</author><pubDate>Fri, 11 Oct 2024 16:03:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08938v1</guid></item><item><title>The Effect of Personalization in FedProx: A Fine-grained Analysis on Statistical Accuracy and Communication Efficiency</title><link>http://arxiv.org/abs/2410.08934v1</link><description>FedProx is a simple yet effective federated learning method that enablesmodel personalization via regularization. Despite remarkable success inpractice, a rigorous analysis of how such a regularization provably improvesthe statistical accuracy of each client's local model hasn't been fullyestablished. Setting the regularization strength heuristically presents a risk,as an inappropriate choice may even degrade accuracy. This work fills in thegap by analyzing the effect of regularization on statistical accuracy, therebyproviding a theoretical guideline for setting the regularization strength forachieving personalization. We prove that by adaptively choosing theregularization strength under different statistical heterogeneity, FedProx canconsistently outperform pure local training and achieve a nearlyminimax-optimal statistical rate. In addition, to shed light on resourceallocation, we design an algorithm, provably showing that strongerpersonalization reduces communication complexity without increasing thecomputation cost overhead. Finally, our theory is validated on both syntheticand real-world datasets and its generalizability is verified in a non-convexsetting.</description><author>Xin Yu, Zelin He, Ying Sun, Lingzhou Xue, Runze Li</author><pubDate>Fri, 11 Oct 2024 16:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08934v1</guid></item><item><title>Enhancing Motion Variation in Text-to-Motion Models via Pose and Video Conditioned Editing</title><link>http://arxiv.org/abs/2410.08931v1</link><description>Text-to-motion models that generate sequences of human poses from textualdescriptions are garnering significant attention. However, due to datascarcity, the range of motions these models can produce is still limited. Forinstance, current text-to-motion models cannot generate a motion of kicking afootball with the instep of the foot, since the training data only includesmartial arts kicks. We propose a novel method that uses short video clips orimages as conditions to modify existing basic motions. In this approach, themodel's understanding of a kick serves as the prior, while the video or imageof a football kick acts as the posterior, enabling the generation of thedesired motion. By incorporating these additional modalities as conditions, ourmethod can create motions not present in the training set, overcoming thelimitations of text-motion datasets. A user study with 26 participantsdemonstrated that our approach produces unseen motions with realism comparableto commonly represented motions in text-motion datasets (e.g., HumanML3D), suchas walking, running, squatting, and kicking.</description><author>Clayton Leite, Yu Xiao</author><pubDate>Fri, 11 Oct 2024 15:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08931v1</guid></item><item><title>Towards Cross-Lingual LLM Evaluation for European Languages</title><link>http://arxiv.org/abs/2410.08928v1</link><description>The rise of Large Language Models (LLMs) has revolutionized natural languageprocessing across numerous languages and tasks. However, evaluating LLMperformance in a consistent and meaningful way across multiple Europeanlanguages remains challenging, especially due to the scarcity of multilingualbenchmarks. We introduce a cross-lingual evaluation approach tailored forEuropean languages. We employ translated versions of five widely-usedbenchmarks to assess the capabilities of 40 LLMs across 21 European languages.Our contributions include examining the effectiveness of translated benchmarks,assessing the impact of different translation services, and offering amultilingual evaluation framework for LLMs that includes newly createddatasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K.The benchmarks and results are made publicly available to encourage furtherresearch in multilingual LLM evaluation.</description><author>Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim Köhler, René Jäkel, Mehdi Ali</author><pubDate>Fri, 11 Oct 2024 15:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08928v1</guid></item><item><title>Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images</title><link>http://arxiv.org/abs/2410.08926v1</link><description>We explore the transformative potential of SAM 2, a vision foundation model,in advancing gaze estimation and eye tracking technologies. By significantlyreducing annotation time, lowering technical barriers through its ease ofdeployment, and enhancing segmentation accuracy, SAM 2 addresses criticalchallenges faced by researchers and practitioners. Utilizing its zero-shotsegmentation capabilities with minimal user input-a single click per video-wetested SAM 2 on over 14 million eye images from diverse datasets, includingvirtual reality setups and the world's largest unified dataset recorded usingwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matchesthe performance of domain-specific models trained solely on eye images,achieving competitive mean Intersection over Union (mIoU) scores of up to 93%without fine-tuning. Additionally, we provide our code and segmentation masksfor these widely used datasets to promote further research.</description><author>Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci</author><pubDate>Fri, 11 Oct 2024 15:50:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08926v1</guid></item><item><title>HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable Deep Learning</title><link>http://arxiv.org/abs/2410.08925v1</link><description>Prototype Learning methods provide an interpretable alternative to black-boxdeep learning models. Approaches such as ProtoPNet learn, which part of a testimage "look like" known prototypical parts from training images, combiningpredictive power with the inherent interpretability of case-based reasoning.However, existing approaches have two main drawbacks: A) They rely solely ondeterministic similarity scores without statistical confidence. B) Theprototypes are learned in a black-box manner without human input. This workintroduces HyperPg, a new prototype representation leveraging Gaussiandistributions on a hypersphere in latent space, with learnable mean andvariance. HyperPg prototypes adapt to the spread of clusters in the latentspace and output likelihood scores. The new architecture, HyperPgNet, leveragesHyperPg to learn prototypes aligned with human concepts from pixel-levelannotations. Consequently, each prototype represents a specific concept such ascolor, image texture, or part of the image subject. A concept extractionpipeline built on foundation models provides pixel-level annotations,significantly reducing human labeling effort. Experiments on CUB-200-2011 andStanford Cars datasets demonstrate that HyperPgNet outperforms other prototypelearning architectures while using fewer parameters and training steps.Additionally, the concept-aligned HyperPg prototypes are learned transparently,enhancing model interpretability.</description><author>Maximilian Xiling Li, Korbinian Franz Rudolf, Nils Blank, Rudolf Lioutikov</author><pubDate>Fri, 11 Oct 2024 15:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08925v1</guid></item><item><title>DiffPO: A causal diffusion model for learning distributions of potential outcomes</title><link>http://arxiv.org/abs/2410.08924v1</link><description>Predicting potential outcomes of interventions from observational data iscrucial for decision-making in medicine, but the task is challenging due to thefundamental problem of causal inference. Existing methods are largely limitedto point estimates of potential outcomes with no uncertain quantification;thus, the full information about the distributions of potential outcomes istypically ignored. In this paper, we propose a novel causal diffusion modelcalled DiffPO, which is carefully designed for reliable inferences in medicineby learning the distribution of potential outcomes. In our DiffPO, we leveragea tailored conditional denoising diffusion model to learn complexdistributions, where we address the selection bias through a novel orthogonaldiffusion loss. Another strength of our DiffPO method is that it is highlyflexible (e.g., it can also be used to estimate different causal quantitiessuch as CATE). Across a wide range of experiments, we show that our methodachieves state-of-the-art performance.</description><author>Yuchen Ma, Valentyn Melnychuk, Jonas Schweisthal, Stefan Feuerriegel</author><pubDate>Fri, 11 Oct 2024 15:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08924v1</guid></item><item><title>Path-minimizing Latent ODEs for improved extrapolation and inference</title><link>http://arxiv.org/abs/2410.08923v1</link><description>Latent ODE models provide flexible descriptions of dynamic systems, but theycan struggle with extrapolation and predicting complicated non-linear dynamics.The latent ODE approach implicitly relies on encoders to identify unknownsystem parameters and initial conditions, whereas the evaluation times areknown and directly provided to the ODE solver. This dichotomy can be exploitedby encouraging time-independent latent representations. By replacing the commonvariational penalty in latent space with an $\ell_2$ penalty on the path lengthof each system, the models learn data representations that can easily bedistinguished from those of systems with different configurations. This resultsin faster training, smaller models, more accurate interpolation and long-timeextrapolation compared to the baseline ODE models with GRU, RNN, and LSTMencoder/decoders on tests with damped harmonic oscillator, self-gravitatingfluid, and predator-prey systems. We also demonstrate superior results forsimulation-based inference of the Lotka-Volterra parameters and initialconditions by using the latents as data summaries for a conditional normalizingflow. Our change to the training loss is agnostic to the specific recognitionnetwork used by the decoder and can therefore easily be adopted by other latentODE models.</description><author>Matt L. Sampson, Peter Melchior</author><pubDate>Fri, 11 Oct 2024 15:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08923v1</guid></item><item><title>Exploring the Design Space of Cognitive Engagement Techniques with AI-Generated Code for Enhanced Learning</title><link>http://arxiv.org/abs/2410.08922v1</link><description>Novice programmers are increasingly relying on Large Language Models (LLMs)to generate code for learning programming concepts. However, this interactioncan lead to superficial engagement, giving learners an illusion of learning andhindering skill development. To address this issue, we conducted a systematicdesign exploration to develop seven cognitive engagement techniques aimed atpromoting deeper engagement with AI-generated code. In this paper, we describeour design process, the initial seven techniques and results from abetween-subjects study (N=82). We then iteratively refined the top techniquesand further evaluated them through a within-subjects study (N=42). We evaluatethe friction each technique introduces, their effectiveness in helping learnersapply concepts to isomorphic tasks without AI assistance, and their success inaligning learners' perceived and actual coding abilities. Ultimately, ourresults highlight the most effective technique: guiding learners through thestep-by-step problem-solving process, where they engage in an interactivedialog with the AI, prompting what needs to be done at each stage before thecorresponding code is revealed.</description><author>Majeed Kazemitabaar, Oliver Huang, Sangho Suh, Austin Z. Henley, Tovi Grossman</author><pubDate>Fri, 11 Oct 2024 15:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08922v1</guid></item><item><title>Accurately Classifying Out-Of-Distribution Data in Facial Recognition</title><link>http://arxiv.org/abs/2404.03876v5</link><description>Standard classification theory assumes that the distribution of images in thetest and training sets are identical. Unfortunately, real-life scenariostypically feature unseen data (``out-of-distribution data") which is differentfrom data in the training distribution (``in-distribution"). This issue is mostprevalent in social justice problems where data from under-represented groupsmay appear in the test data without representing an equal proportion of thetraining data. This may result in a model returning confidently wrong decisionsand predictions. We are interested in the following question: Can theperformance of a neural network improve on facial images of out-of-distributiondata when it is trained simultaneously on multiple datasets of in-distributiondata? We approach this problem by incorporating the Outlier Exposure model andinvestigate how the model's performance changes when other datasets of facialimages were implemented. We observe that the accuracy and other metrics of themodel can be increased by applying Outlier Exposure, incorporating a trainableweight parameter to increase the machine's emphasis on outlier images, and byre-weighting the importance of different class labels. We also experimentedwith whether sorting the images and determining outliers via image featureswould have more of an effect on the metrics than sorting by average pixelvalue, and found no conclusive results. Our goal was to make models not onlymore accurate but also more fair by scanning a more expanded range of images.Utilizing Python and the Pytorch package, we found models utilizing outlierexposure could result in more fair classification.</description><author>Gianluca Barone, Aashrit Cunchala, Rudy Nunez</author><pubDate>Fri, 11 Oct 2024 15:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03876v5</guid></item><item><title>Efficient Hyperparameter Importance Assessment for CNNs</title><link>http://arxiv.org/abs/2410.08920v1</link><description>Hyperparameter selection is an essential aspect of the machine learningpipeline, profoundly impacting models' robustness, stability, andgeneralization capabilities. Given the complex hyperparameter spaces associatedwith Neural Networks and the constraints of computational resources and time,optimizing all hyperparameters becomes impractical. In this context, leveraginghyperparameter importance assessment (HIA) can provide valuable guidance bynarrowing down the search space. This enables machine learning practitioners tofocus their optimization efforts on the hyperparameters with the mostsignificant impact on model performance while conserving time and resources.This paper aims to quantify the importance weights of some hyperparameters inConvolutional Neural Networks (CNNs) with an algorithm called N-RReliefF,laying the groundwork for applying HIA methodologies in the Deep Learningfield. We conduct an extensive study by training over ten thousand CNN modelsacross ten popular image classification datasets, thereby acquiring acomprehensive dataset containing hyperparameter configuration instances andtheir corresponding performance metrics. It is demonstrated that among theinvestigated hyperparameters, the top five important hyperparameters of the CNNmodel are the number of convolutional layers, learning rate, dropout rate,optimizer and epoch.</description><author>Ruinan Wang, Ian Nabney, Mohammad Golbabaee</author><pubDate>Fri, 11 Oct 2024 15:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08920v1</guid></item><item><title>AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments</title><link>http://arxiv.org/abs/2410.08917v1</link><description>We introduce AutoPersuade, a three-part framework for constructing persuasivemessages. First, we curate a large dataset of arguments with human evaluations.Next, we develop a novel topic model to identify argument features thatinfluence persuasiveness. Finally, we use this model to predict theeffectiveness of new arguments and assess the causal impact of differentcomponents to provide explanations. We validate AutoPersuade through anexperimental study on arguments for veganism, demonstrating its effectivenesswith human studies and out-of-sample predictions.</description><author>Till Raphael Saenger, Musashi Hinck, Justin Grimmer, Brandon M. Stewart</author><pubDate>Fri, 11 Oct 2024 15:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08917v1</guid></item><item><title>Revisiting Hierarchical Text Classification: Inference and Metrics</title><link>http://arxiv.org/abs/2410.01305v2</link><description>Hierarchical text classification (HTC) is the task of assigning labels to atext within a structured space organized as a hierarchy. Recent works treat HTCas a conventional multilabel classification problem, therefore evaluating it assuch. We instead propose to evaluate models based on specifically designedhierarchical metrics and we demonstrate the intricacy of metric choice andprediction inference method. We introduce a new challenging dataset and weevaluate fairly, recent sophisticated models, comparing them with a range ofsimple but strong baselines, including a new theoretically motivated loss.Finally, we show that those baselines are very often competitive with thelatest models. This highlights the importance of carefully considering theevaluation methodology when proposing new methods for HTC. Code implementationand dataset are available at \url{https://github.com/RomanPlaud/revisitingHTC}.</description><author>Roman Plaud, Matthieu Labeau, Antoine Saillenfest, Thomas Bonald</author><pubDate>Fri, 11 Oct 2024 15:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01305v2</guid></item><item><title>Multimodal Auto Validation For Self-Refinement in Web Agents</title><link>http://arxiv.org/abs/2410.00689v2</link><description>As our world digitizes, web agents that can automate complex and monotonoustasks are becoming essential in streamlining workflows. This paper introducesan approach to improving web agent performance through multi-modal validationand self-refinement. We present a comprehensive study of different modalities(text, vision) and the effect of hierarchy for the automatic validation of webagents, building upon the state-of-the-art Agent-E web automation framework. Wealso introduce a self-refinement mechanism for web automation, using thedeveloped auto-validator, that enables web agents to detect and self-correctworkflow failures. Our results show significant gains on Agent-E's (a SOTA webagent) prior state-of-art performance, boosting task-completion rates from76.2\% to 81.24\% on the subset of the WebVoyager benchmark. The approachpresented in this paper paves the way for more reliable digital assistants incomplex, real-world scenarios.</description><author>Ruhana Azam, Tamer Abuelsaad, Aditya Vempaty, Ashish Jagmohan</author><pubDate>Fri, 11 Oct 2024 15:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00689v2</guid></item><item><title>An End-to-End Deep Learning Method for Solving Nonlocal Allen-Cahn and Cahn-Hilliard Phase-Field Models</title><link>http://arxiv.org/abs/2410.08914v1</link><description>We propose an efficient end-to-end deep learning method for solving nonlocalAllen-Cahn (AC) and Cahn-Hilliard (CH) phase-field models. One motivation forthis effort emanates from the fact that discretized partial differentialequation-based AC or CH phase-field models result in diffuse interfaces betweenphases, with the only recourse for remediation is to severely refine thespatial grids in the vicinity of the true moving sharp interface whose width isdetermined by a grid-independent parameter that is substantially larger thanthe local grid size. In this work, we introduce non-mass conserving nonlocal ACor CH phase-field models with regular, logarithmic, or obstacle double-wellpotentials. Because of non-locality, some of these models feature totally sharpinterfaces separating phases. The discretization of such models can lead to atransition between phases whose width is only a single grid cell wide. Anothermotivation is to use deep learning approaches to ameliorate the otherwise highcost of solving discretized nonlocal phase-field models. To this end, lossfunctions of the customized neural networks are defined using the residual ofthe fully discrete approximations of the AC or CH models, which results fromapplying a Fourier collocation method and a temporal semi-implicitapproximation. To address the long-range interactions in the models, we tailorthe architecture of the neural network by incorporating a nonlocal kernel as aninput channel to the neural network model. We then provide the results ofextensive computational experiments to illustrate the accuracy,structure-preserving properties, predictive capabilities, and cost reductionsof the proposed method.</description><author>Yuwei Geng, Olena Burkovska, Lili Ju, Guannan Zhang, Max Gunzburger</author><pubDate>Fri, 11 Oct 2024 15:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08914v1</guid></item><item><title>From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis</title><link>http://arxiv.org/abs/2406.19934v2</link><description>We explore multi-step reasoning in vision-language models (VLMs). The problemis challenging, as reasoning data consisting of multiple steps of visual andlanguage processing are barely available. To overcome the challenge, we firstintroduce a least-to-most visual reasoning paradigm, which interleaves steps ofdecomposing a question into sub-questions and invoking external tools forresolving sub-questions. Based on the paradigm, we further propose a novel datasynthesis approach that can automatically create questions and multi-stepreasoning paths for an image in a bottom-up manner. Our approach divides thecomplex synthesis task into a few simple sub-tasks, and (almost entirely)relies on open-sourced models to accomplish the sub-tasks. Therefore, theentire synthesis process is reproducible and cost-efficient, and thesynthesized data is quality guaranteed. With the approach, we construct $50$kvisual reasoning examples. Then, we develop a visual reasoner throughsupervised fine-tuning, which is capable of generally enhancing the reasoningabilities of a wide range of existing VLMs in a plug-and-play fashion.Extensive experiments indicate that the visual reasoner can consistently andsignificantly improve four VLMs on four VQA benchmarks. Our code and datasetare available at https://github.com/steven-ccq/VisualReasoner.</description><author>Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan</author><pubDate>Fri, 11 Oct 2024 15:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19934v2</guid></item><item><title>LoTLIP: Improving Language-Image Pre-training for Long Text Understanding</title><link>http://arxiv.org/abs/2410.05249v3</link><description>Understanding long text is of great demands in practice but beyond the reachof most language-image pre-training (LIP) models. In this work, we empiricallyconfirm that the key reason causing such an issue is that the training imagesare usually paired with short captions, leaving certain tokens easilyovershadowed by salient tokens. Towards this problem, our initial attempt is torelabel the data with long captions, however, directly learning with which maylead to performance degradation in understanding short text (e.g., in the imageclassification task). Then, after incorporating corner tokens to aggregatediverse textual information, we manage to help the model catch up to itsoriginal level of short text understanding yet greatly enhance its capabilityof long text understanding. We further look into whether the model cancontinuously benefit from longer captions and notice a clear trade-off betweenthe performance and the efficiency. Finally, we validate the effectiveness ofour approach using a self-constructed large-scale dataset, which consists of100M long caption oriented text-image pairs. It is noteworthy that, on the taskof long-text image retrieval, we beat the competitor using long captions with11.1% improvement (i.e., from 72.62% to 83.72%). We will release the code, themodel, and the new dataset to facilitate the reproducibility and furtherresearch. The project page is available at https://wuw2019.github.io/lot-lip.</description><author>Wei Wu, Kecheng Zheng, Shuailei Ma, Fan Lu, Yuxin Guo, Yifei Zhang, Wei Chen, Qingpei Guo, Yujun Shen, Zheng-Jun Zha</author><pubDate>Fri, 11 Oct 2024 15:37:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05249v3</guid></item><item><title>Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own</title><link>http://arxiv.org/abs/2310.02635v4</link><description>Reinforcement learning (RL) is a promising approach for solving roboticmanipulation tasks. However, it is challenging to apply the RL algorithmsdirectly in the real world. For one thing, RL is data-intensive and typicallyrequires millions of interactions with environments, which are impractical inreal scenarios. For another, it is necessary to make heavy engineering effortsto design reward functions manually. To address these issues, we leveragefoundation models in this paper. We propose Reinforcement Learning withFoundation Priors (RLFP) to utilize guidance and feedback from policy, value,and success-reward foundation models. Within this framework, we introduce theFoundation-guided Actor-Critic (FAC) algorithm, which enables embodied agentsto explore more efficiently with automatic reward functions. The benefits ofour framework are threefold: (1) \textit{sample efficient}; (2) \textit{minimaland effective reward engineering}; (3) \textit{agnostic to foundation modelforms and robust to noisy priors}. Our method achieves remarkable performancesin various manipulation tasks on both real robots and in simulation. Across 5dexterous tasks with real robots, FAC achieves an average success rate of 86\%after one hour of real-time learning. Across 8 tasks in the simulatedMeta-world, FAC achieves 100\% success rates in 7/8 tasks under less than 100kframes (about 1-hour training), outperforming baseline methods withmanual-designed rewards in 1M frames. We believe the RLFP framework can enablefuture robots to explore and learn autonomously in the physical world for moretasks. Visualizations and code are available at\url{https://yewr.github.io/rlfp}.</description><author>Weirui Ye, Yunsheng Zhang, Haoyang Weng, Xianfan Gu, Shengjie Wang, Tong Zhang, Mengchen Wang, Pieter Abbeel, Yang Gao</author><pubDate>Fri, 11 Oct 2024 15:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02635v4</guid></item><item><title>Exploring Quantization for Efficient Pre-Training of Transformer Language Models</title><link>http://arxiv.org/abs/2407.11722v2</link><description>The increasing scale of Transformer models has led to an increase in theirpre-training computational requirements. While quantization has proven to beeffective after pre-training and during fine-tuning, applying quantization inTransformers during pre-training has remained largely unexplored at scale forlanguage modeling. This study aims to explore the impact of quantization forefficient pre-training of Transformers, with a focus on linear layercomponents. By systematically applying straightforward linear quantization toweights, activations, gradients, and optimizer states, we assess its effects onmodel efficiency, stability, and performance during training. By offering acomprehensive recipe of effective quantization strategies to be applied duringthe pre-training of Transformers, we promote high training efficiency fromscratch while retaining language modeling ability. Code is available athttps://github.com/chandar-lab/EfficientLLMs.</description><author>Kamran Chitsaz, Quentin Fournier, Gonçalo Mordido, Sarath Chandar</author><pubDate>Fri, 11 Oct 2024 15:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11722v2</guid></item><item><title>SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments</title><link>http://arxiv.org/abs/2407.18913v2</link><description>This work compares ways of extending Reinforcement Learning algorithms toPartially Observed Markov Decision Processes (POMDPs) with options. One view ofoptions is as temporally extended action, which can be realized as a memorythat allows the agent to retain historical information beyond the policy'scontext window. While option assignment could be handled using heuristics andhand-crafted objectives, learning temporally consistent options and associatedsub-policies without explicit supervision is a challenge. Two algorithms, PPOEMand SOAP, are proposed and studied in depth to address this problem. PPOEMapplies the forward-backward algorithm (for Hidden Markov Models) to optimizethe expected returns for an option-augmented policy. However, this learningapproach is unstable during on-policy rollouts. It is also unsuited forlearning causal policies without the knowledge of future trajectories, sinceoption assignments are optimized for offline sequences where the entire episodeis available. As an alternative approach, SOAP evaluates the policy gradientfor an optimal option assignment. It extends the concept of the generalizedadvantage estimation (GAE) to propagate option advantages through time, whichis an analytical equivalent to performing temporal back-propagation of optionpolicy gradients. This option policy is only conditional on the history of theagent, not future actions. Evaluated against competing baselines, SOAPexhibited the most robust performance, correctly discovering options for POMDPcorridor environments, as well as on standard benchmarks including Atari andMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. Theopen-sourced code is available at https://github.com/shuishida/SoapRL.</description><author>Shu Ishida, João F. Henriques</author><pubDate>Fri, 11 Oct 2024 15:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18913v2</guid></item><item><title>Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization</title><link>http://arxiv.org/abs/2403.14425v2</link><description>We present a method for end-to-end learning of Koopman surrogate models foroptimal performance in a specific control task. In contrast to previouscontributions that employ standard reinforcement learning (RL) algorithms, weuse a training algorithm that exploits the potential differentiability ofenvironments based on mechanistic simulation models to aid the policyoptimization. We evaluate the performance of our method by comparing it to thatof other controller type and training algorithm combinations on an existingeconomic nonlinear model predictive control (eNMPC) case study of a continuousstirred-tank reactor (CSTR) model. Compared to the benchmark methods, ourmethod produces similar economic performance but causes considerably fewer andless severe constraint violations. Thus, for this case study, our methodoutperforms the others and offers a promising path toward more performantcontrollers that employ dynamic surrogate models.</description><author>Daniel Mayfrank, Na Young Ahn, Alexander Mitsos, Manuel Dahmen</author><pubDate>Fri, 11 Oct 2024 15:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14425v2</guid></item><item><title>Test-driven Software Experimentation with LASSO: an LLM Benchmarking Example</title><link>http://arxiv.org/abs/2410.08911v1</link><description>Empirical software engineering faces a critical gap: the lack of standardizedtools for rapid development and execution of Test-Driven Software Experiments(TDSEs) - that is, experiments that involve the execution of software subjectsand the observation and analysis of their "de facto" run-time behavior. In thispaper we present a general-purpose analysis platform called LASSO that providesa minimal set of domain-specific languages and data structures to conductTDSEs. By empowering users with an executable scripting language to design andexecute TDSEs, LASSO enables efficient evaluation of run-time semantics andexecution characteristics in addition to statically determined properties. Wepresent an example TDSE that demonstrates the practical benefits of LASSO'sscripting capabilities for assessing the reliability of LLMs for codegeneration by means of a self-contained, reusable and extensible study script.The LASSO platform is freely available at:https://softwareobservatorium.github.io/, and a demo video is available onYouTube: https://youtu.be/tzY9oNTWXzw</description><author>Marcus Kessel</author><pubDate>Fri, 11 Oct 2024 15:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08911v1</guid></item><item><title>Gradient Boosting Decision Trees on Medical Diagnosis over Tabular Data</title><link>http://arxiv.org/abs/2410.03705v2</link><description>Medical diagnosis is a crucial task in the medical field, in terms ofproviding accurate classification and respective treatments. Havingnear-precise decisions based on correct diagnosis can affect a patient's lifeitself, and may extremely result in a catastrophe if not classified correctly.Several traditional machine learning (ML), such as support vector machines(SVMs) and logistic regression, and state-of-the-art tabular deep learning (DL)methods, including TabNet and TabTransformer, have been proposed and used overtabular medical datasets. Additionally, due to the superior performances, lowercomputational costs, and easier optimization over different tasks, ensemblemethods have been used in the field more recently. They offer a powerfulalternative in terms of providing successful medical decision-making processesin several diagnosis tasks. In this study, we investigated the benefits ofensemble methods, especially the Gradient Boosting Decision Tree (GBDT)algorithms in medical classification tasks over tabular data, focusing onXGBoost, CatBoost, and LightGBM. The experiments demonstrate that GBDT methodsoutperform traditional ML and deep neural network architectures and have thehighest average rank over several benchmark tabular medical diagnosis datasets.Furthermore, they require much less computational power compared to DL models,creating the optimal methodology in terms of high performance and lowercomplexity.</description><author>A. Yarkın Yıldız, Asli Kalayci</author><pubDate>Fri, 11 Oct 2024 15:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03705v2</guid></item></channel></rss>