<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 05 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</title><link>http://arxiv.org/abs/2309.00616v1</link><description>Current 3D open-vocabulary scene understanding methods mostly utilizewell-aligned 2D images as the bridge to learn 3D features with language.However, applying these approaches becomes challenging in scenarios where 2Dimages are absent. In this work, we introduce a completely new pipeline,namely, OpenIns3D, which requires no 2D image inputs, for 3D open-vocabularyscene understanding at the instance level. The OpenIns3D framework employs a"Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic maskproposals in 3D point clouds. The "Snap" module generates synthetic scene-levelimages at multiple scales and leverages 2D vision language models to extractinteresting objects. The "Lookup" module searches through the outcomes of"Snap" with the help of Mask2Pixel maps, which contain the precisecorrespondence between 3D masks and synthetic images, to assign category namesto the proposed masks. This 2D input-free, easy-to-train, and flexible approachachieved state-of-the-art results on a wide range of indoor and outdoordatasets with a large margin. Furthermore, OpenIns3D allows for effortlessswitching of 2D detectors without re-training. When integrated withstate-of-the-art 2D open-world models such as ODISE and GroundingDINO, superbresults are observed on open-vocabulary instance segmentation. When integratedwith LLM-powered 2D models like LISA, it demonstrates a remarkable capacity toprocess highly complex text queries, including those that require intricatereasoning and world knowledge. The code and model will be made publiclyavailable.</description><author>Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby</author><pubDate>Fri, 01 Sep 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00616v1</guid></item><item><title>Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</title><link>http://arxiv.org/abs/2309.00615v1</link><description>We introduce Point-Bind, a 3D multi-modality model aligning point clouds with2D image, language, audio, and video. Guided by ImageBind, we construct a jointembedding space between 3D and multi-modalities, enabling many promisingapplications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3Dopen-world understanding. On top of this, we further present Point-LLM, thefirst 3D large language model (LLM) following 3D multi-modal instructions. Byparameter-efficient fine-tuning techniques, Point-LLM injects the semantics ofPoint-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instructiondata, but exhibits superior 3D and multi-modal question-answering capacity. Wehope our work may cast a light on the community for extending 3D point cloudsto multi-modality applications. Code is available athttps://github.com/ZiyuGuo99/Point-Bind_Point-LLM.</description><author>Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng</author><pubDate>Fri, 01 Sep 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00615v1</guid></item><item><title>Baseline Defenses for Adversarial Attacks Against Aligned Language Models</title><link>http://arxiv.org/abs/2309.00614v1</link><description>As Large Language Models quickly become ubiquitous, their securityvulnerabilities are critical to understand. Recent work shows that textoptimizers can produce jailbreaking prompts that bypass moderation andalignment. Drawing from the rich body of work on adversarial machine learning,we approach these attacks with three questions: What threat models arepractically useful in this domain? How do baseline defense techniques performin this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarialattacks on LLMs, discussing the various settings in which each is feasible andeffective. Particularly, we look at three types of defenses: detection(perplexity based), input preprocessing (paraphrase and retokenization), andadversarial training. We discuss white-box and gray-box settings and discussthe robustness-performance trade-off for each of the defenses considered.Surprisingly, we find much more success with filtering and preprocessing thanwe would expect from other domains, such as vision, providing a firstindication that the relative strengths of these defenses may be weigheddifferently in these domains.</description><author>Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein</author><pubDate>Fri, 01 Sep 2023 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00614v1</guid></item><item><title>Iterative Multi-granular Image Editing using Diffusion Models</title><link>http://arxiv.org/abs/2309.00613v1</link><description>Recent advances in text-guided image synthesis has dramatically changed howcreative professionals generate artistic and aesthetically pleasing visualassets. To fully support such creative endeavors, the process should possessthe ability to: 1) iteratively edit the generations and 2) control the spatialreach of desired changes (global, local or anything in between). We formalizethis pragmatic problem setting as Iterative Multi-granular Editing. While therehas been substantial progress with diffusion-based models for image synthesisand editing, they are all one shot (i.e., no iterative editing capabilities)and do not naturally yield multi-granular control (i.e., covering the fullspectrum of local-to-global edits). To overcome these drawbacks, we proposeEMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latentiteration strategy, which re-purposes a pre-trained diffusion model tofacilitate iterative editing. This is complemented by a gradient controloperation for multi-granular control. We introduce a new benchmark dataset toevaluate our newly proposed setting. We conduct exhaustive quantitatively andqualitatively evaluation against recent state-of-the-art approaches adapted toour task, to being out the mettle of EMILIE. We hope our work would attractattention to this newly identified, pragmatic problem setting.</description><author>K J Joseph, Prateksha Udhayanan, Tripti Shukla, Aishwarya Agarwal, Srikrishna Karanam, Koustava Goswami, Balaji Vasan Srinivasan</author><pubDate>Fri, 01 Sep 2023 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00613v1</guid></item><item><title>Bayesian deep learning for cosmic volumes with modified gravity</title><link>http://arxiv.org/abs/2309.00612v1</link><description>The new generation of galaxy surveys will provide unprecedented data allowingus to test gravity at cosmological scales. A robust cosmological analysis ofthe large-scale structure demands exploiting the nonlinear information encodedin the cosmic web. Machine Learning techniques provide such tools, however, donot provide a priori assessment of uncertainties. This study aims at extractingcosmological parameters from modified gravity (MG) simulations through deepneural networks endowed with uncertainty estimations. We implement Bayesianneural networks (BNNs) with an enriched approximate posterior distributionconsidering two cases: one with a single Bayesian last layer (BLL), and anotherone with Bayesian layers at all levels (FullB). We train both BNNs withreal-space density fields and power-spectra from a suite of 2000 dark matteronly particle mesh $N$-body simulations including modified gravity modelsrelying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with128$^3$ particles. BNNs excel in accurately predicting parameters for$\Omega_m$ and $\sigma_8$ and their respective correlation with the MGparameter. We find out that BNNs yield well-calibrated uncertainty estimatesovercoming the over- and under-estimation issues in traditional neuralnetworks. We observe that the presence of MG parameter leads to a significantdegeneracy with $\sigma_8$ being one of the possible explanations of the poorMG predictions. Ignoring MG, we obtain a deviation of the relative errors in$\Omega_m$ and $\sigma_8$ by at least $30\%$. Moreover, we report consistentresults from the density field and power spectra analysis, and comparableresults between BLL and FullB experiments which permits us to save computingtime by a factor of two. This work contributes in setting the path to extractcosmological parameters from complete small cosmic volumes towards the highlynonlinear regime.</description><author>Jorge Enrique García-Farieta, Héctor J Hortúa, Francisco-Shu Kitaura</author><pubDate>Fri, 01 Sep 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00612v1</guid></item><item><title>CityDreamer: Compositional Generative Model of Unbounded 3D Cities</title><link>http://arxiv.org/abs/2309.00610v1</link><description>In recent years, extensive research has focused on 3D natural scenegeneration, but the domain of 3D city generation has not received as muchexploration. This is due to the greater challenges posed by 3D city generation,mainly because humans are more sensitive to structural distortions in urbanenvironments. Additionally, generating 3D cities is more complex than 3Dnatural scenes since buildings, as objects of the same class, exhibit a widerrange of appearances compared to the relatively consistent appearance ofobjects like trees in natural scenes. To address these challenges, we proposeCityDreamer, a compositional generative model designed specifically forunbounded 3D cities, which separates the generation of building instances fromother background objects, such as roads, green lands, and water areas, intodistinct modules. Furthermore, we construct two datasets, OSM and GoogleEarth,containing a vast amount of real-world city imagery to enhance the realism ofthe generated 3D cities both in their layouts and appearances. Throughextensive experiments, CityDreamer has proven its superiority overstate-of-the-art methods in generating a wide range of lifelike 3D cities.</description><author>Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu</author><pubDate>Fri, 01 Sep 2023 18:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00610v1</guid></item><item><title>Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair</title><link>http://arxiv.org/abs/2309.00608v1</link><description>During Automated Program Repair (APR), it can be challenging to synthesizecorrect patches for real-world systems in general-purpose programminglanguages. Recent Large Language Models (LLMs) have been shown to be helpful"copilots" in assisting developers with various coding tasks, and have alsobeen directly applied for patch synthesis. However, most LLMs treat programs assequences of tokens, meaning that they are ignorant of the underlying semanticsconstraints of the target programming language. This results in plenty ofstatically invalid generated patches, impeding the practicality of thetechnique. Therefore, we propose Repilot, a framework to further copilot the AI"copilots" (i.e., LLMs) by synthesizing more valid patches during the repairprocess. Our key insight is that many LLMs produce outputs autoregressively(i.e., token by token), resembling human writing programs, which can besignificantly boosted and guided through a Completion Engine. Repilotsynergistically synthesizes a candidate patch through the interaction betweenan LLM and a Completion Engine, which 1) prunes away infeasible tokenssuggested by the LLM and 2) proactively completes the token based on thesuggestions provided by the Completion Engine. Our evaluation on a subset ofthe widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and50 bugs, respectively, surpassing the best-performing baseline by 14 and 16bugs fixed. More importantly, Repilot is capable of producing more valid andcorrect patches than the base LLM when given the same generation budget.</description><author>Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang</author><pubDate>Fri, 01 Sep 2023 18:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00608v1</guid></item><item><title>LEVER: Learning to Verify Language-to-Code Generation with Execution</title><link>http://arxiv.org/abs/2302.08468v3</link><description>The advent of large language models trained on code (code LLMs) has led tosignificant progress in language-to-code generation. State-of-the-artapproaches in this area combine LLM decoding with sample pruning and rerankingusing test cases or heuristics based on the execution results. However, it ischallenging to obtain test cases for many real-world language-to-codeapplications, and heuristics cannot well capture the semantic features of theexecution results, such as data type and value range, which often indicates thecorrectness of the program. In this work, we propose LEVER, a simple approachto improve language-to-code generation by learning to verify the generatedprograms with their execution results. Specifically, we train verifiers todetermine whether a program sampled from the LLMs is correct or not based onthe natural language input, the program itself and its execution results. Thesampled programs are reranked by combining the verification score with the LLMgeneration probability, and marginalizing over programs with the same executionresults. On four datasets across the domains of table QA, math QA and basicPython programming, LEVER consistently improves over the base code LLMs(4.6% to10.9% with code-davinci-002) and achieves new state-of-the-art results on allof them.</description><author>Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I. Wang, Xi Victoria Lin</author><pubDate>Fri, 01 Sep 2023 18:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08468v3</guid></item><item><title>Neural-network quantum state study of the long-range antiferromagnetic Ising chain</title><link>http://arxiv.org/abs/2308.09709v2</link><description>We investigate quantum phase transitions in the transverse field Ising chainwith algebraically decaying long-range antiferromagnetic interactions by usingthe variational Monte Carlo method with the restricted Boltzmann machine beingemployed as a trial wave function ansatz. In the finite-size scaling analysiswith the order parameter and the second R\'enyi entropy, we find that thecentral charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$in contrast to the critical exponents staying very close to the short-range(SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting thepreviously proposed scenario of conformal invariance breakdown. To identify thethreshold of the Ising universality and the conformal symmetry, we perform twoadditional tests for the universal Binder ratio and the conformal field theory(CFT) description of the correlation function. It turns out that both indicatea noticeable deviation from the SR Ising class at $\alpha_\mathrm{LR} &lt; 2$.However, a closer look at the scaled correlation function for$\alpha_\mathrm{LR} \ge 2$ shows a gradual change from the asymptotic line ofthe CFT verified at $\alpha_\mathrm{LR} = 3$, providing a rough estimate of thethreshold being in the range of $2 \lesssim \alpha_\mathrm{LR} &lt; 3$.</description><author>Jicheol Kim, Dongkyu Kim, Dong-Hee Kim</author><pubDate>Fri, 01 Sep 2023 18:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09709v2</guid></item><item><title>Time Series Analysis of Urban Liveability</title><link>http://arxiv.org/abs/2309.00594v1</link><description>In this paper we explore deep learning models to monitor longitudinalliveability changes in Dutch cities at the neighbourhood level. Our liveabilityreference data is defined by a country-wise yearly survey based on a set ofindicators combined into a liveability score, the Leefbaarometer. We pair thisreference data with yearly-available high-resolution aerial images, whichcreates yearly timesteps at which liveability can be monitored. We deploy aconvolutional neural network trained on an aerial image from 2016 and theLeefbaarometer score to predict liveability at new timesteps 2012 and 2020. Theresults in a city used for training (Amsterdam) and one never seen duringtraining (Eindhoven) show some trends which are difficult to interpret,especially in light of the differences in image acquisitions at the differenttime steps. This demonstrates the complexity of liveability monitoring acrosstime periods and the necessity for more sophisticated methods compensating forchanges unrelated to liveability dynamics.</description><author>Alex Levering, Diego Marcos, Devis Tuia</author><pubDate>Fri, 01 Sep 2023 18:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00594v1</guid></item><item><title>Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms</title><link>http://arxiv.org/abs/2309.00591v1</link><description>This paper considers a stochastic multi-armed bandit (MAB) problem with dualobjectives: (i) quick identification and commitment to the optimal arm, and(ii) reward maximization throughout a sequence of $T$ consecutive rounds.Though each objective has been individually well-studied, i.e., best armidentification for (i) and regret minimization for (ii), the simultaneousrealization of both objectives remains an open problem, despite its practicalimportance. This paper introduces \emph{Regret Optimal Best Arm Identification}(ROBAI) which aims to achieve these dual objectives. To solve ROBAI with bothpre-determined stopping time and adaptive stopping time requirements, wepresent the $\mathsf{EOCP}$ algorithm and its variants respectively, which notonly achieve asymptotic optimal regret in both Gaussian and general bandits,but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds withpre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptivestopping time. We further characterize lower bounds on the commitment time(equivalent to sample complexity) of ROBAI, showing that $\mathsf{EOCP}$ andits variants are sample optimal with pre-determined stopping time, and almostsample optimal with adaptive stopping time. Numerical results confirm ourtheoretical analysis and reveal an interesting ``over-exploration'' phenomenoncarried by classic $\mathsf{UCB}$ algorithms, such that $\mathsf{EOCP}$ hassmaller regret even though it stops exploration much earlier than$\mathsf{UCB}$ ($\mathcal{O}(\log T)$ versus $\mathcal{O}(T)$), which suggestsover-exploration is unnecessary and potentially harmful to system performance.</description><author>Qining Zhang, Lei Ying</author><pubDate>Fri, 01 Sep 2023 18:12:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00591v1</guid></item><item><title>Contrastive Image Synthesis and Self-supervised Feature Adaptation for Cross-Modality Biomedical Image Segmentation</title><link>http://arxiv.org/abs/2207.13240v3</link><description>This work presents a novel framework CISFA (Contrastive Image synthesis andSelf-supervised Feature Adaptation)that builds on image domain translation andunsupervised feature adaptation for cross-modality biomedical imagesegmentation. Different from existing works, we use a one-sided generativemodel and add a weighted patch-wise contrastive loss between sampled patches ofthe input image and the corresponding synthetic image, which serves as shapeconstraints. Moreover, we notice that the generated images and input imagesshare similar structural information but are in different modalities. As such,we enforce contrastive losses on the generated images and the input images totrain the encoder of a segmentation model to minimize the discrepancy betweenpaired images in the learned embedding space. Compared with existing works thatrely on adversarial learning for feature adaptation, such a method enables theencoder to learn domain-independent features in a more explicit way. Weextensively evaluate our methods on segmentation tasks containing CT and MRIimages for abdominal cavities and whole hearts. Experimental results show thatthe proposed framework not only outputs synthetic images with less distortionof organ shapes, but also outperforms state-of-the-art domain adaptationmethods by a large margin.</description><author>Xinrong Hu, Corey Wang, Yiyu Shi</author><pubDate>Fri, 01 Sep 2023 18:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.13240v3</guid></item><item><title>Activation Addition: Steering Language Models Without Optimization</title><link>http://arxiv.org/abs/2308.10248v2</link><description>Reliably controlling the behavior of large language models is a pressing openproblem. Existing methods include supervised finetuning, reinforcement learningfrom human feedback, prompt engineering, and guided decoding. We insteadinvestigate activation engineering: modifying activations at inference time topredictably alter model behavior. In particular, we bias the forward pass withan added 'steering vector' implicitly specified through natural language. Unlike past work which learned these steering vectors, our ActivationAddition (ActAdd) method computes them by taking the activation differencesthat result from pairs of prompts. We demonstrate ActAdd on GPT-2 onOpenWebText and ConceptNet. Our inference-time approach yields control overhigh-level properties of output and preserves off-target model performance. Itinvolves far less compute and implementation effort than finetuning, allowsusers to provide natural language specifications, and its overhead scalesnaturally with model size.</description><author>Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, Monte MacDiarmid</author><pubDate>Fri, 01 Sep 2023 18:07:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10248v2</guid></item><item><title>Discrete Morphological Neural Networks</title><link>http://arxiv.org/abs/2309.00588v1</link><description>A classical approach to designing binary image operators is MathematicalMorphology (MM). We propose the Discrete Morphological Neural Networks (DMNN)for binary image analysis to represent W-operators and estimate them viamachine learning. A DMNN architecture, which is represented by a MorphologicalComputational Graph, is designed as in the classical heuristic design ofmorphological operators, in which the designer should combine a set of MMoperators and Boolean operations based on prior information and theoreticalknowledge. Then, once the architecture is fixed, instead of adjusting itsparameters (i.e., structural elements or maximal intervals) by hand, we proposea lattice gradient descent algorithm (LGDA) to train these parameters based ona sample of input and output images under the usual machine learning approach.We also propose a stochastic version of the LGDA that is more efficient, isscalable and can obtain small error in practical problems. The classrepresented by a DMNN can be quite general or specialized according to expectedproperties of the target operator, i.e., prior information, and the semanticexpressed by algebraic properties of classes of operators is a differentialrelative to other methods. The main contribution of this paper is the merger ofthe two main paradigms for designing morphological operators: classicalheuristic design and automatic design via machine learning. Thus, conciliatingclassical heuristic morphological operator design with machine learning. Weapply the DMNN to recognize the boundary of digits with noise, and we discussmany topics for future research.</description><author>Diego Marcondes, Junior Barrera</author><pubDate>Fri, 01 Sep 2023 18:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00588v1</guid></item><item><title>Artificial intelligence is ineffective and potentially harmful for fact checking</title><link>http://arxiv.org/abs/2308.10800v2</link><description>Fact checking can be an effective strategy against misinformation, but itsimplementation at scale is impeded by the overwhelming volume of informationonline. Recent artificial intelligence (AI) language models have shownimpressive ability in fact-checking tasks, but how humans interact withfact-checking information provided by these models is unclear. Here weinvestigate the impact of fact checks generated by a popular AI model on beliefin, and sharing intent of, political news in a preregistered randomized controlexperiment. Although the AI performs reasonably well in debunking falseheadlines, we find that it does not significantly affect participants' abilityto discern headline accuracy or share accurate news. However, the AIfact-checker is harmful in specific cases: it decreases beliefs in trueheadlines that it mislabels as false and increases beliefs for false headlinesthat it is unsure about. On the positive side, the AI increases sharing intentsfor correctly labeled true headlines. When participants are given the option toview AI fact checks and choose to do so, they are significantly more likely toshare both true and false news but only more likely to believe false news. Ourfindings highlight an important source of potential harm stemming from AIapplications and underscore the critical need for policies to prevent ormitigate such unintended consequences.</description><author>Matthew R. DeVerna, Harry Yaojun Yan, Kai-Cheng Yang, Filippo Menczer</author><pubDate>Fri, 01 Sep 2023 18:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10800v2</guid></item><item><title>PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer</title><link>http://arxiv.org/abs/2309.00585v1</link><description>Polymer simulation with both accuracy and efficiency is a challenging task.Machine learning (ML) forcefields have been developed to achieve both theaccuracy of ab initio methods and the efficiency of empirical force fields.However, existing ML force fields are usually limited to single-moleculesettings, and their simulations are not robust enough. In this paper, wepresent PolyGET, a new framework for Polymer Forcefields with GeneralizableEquivariant Transformers. PolyGET is designed to capture complex quantuminteractions between atoms and generalize across various polymer families,using a deep learning model called Equivariant Transformers. We propose a newtraining paradigm that focuses exclusively on optimizing forces, which isdifferent from existing methods that jointly optimize forces and energy. Thissimple force-centric objective function avoids competing objectives betweenenergy and forces, thereby allowing for learning a unified forcefield ML modelover different polymer families. We evaluated PolyGET on a large-scale datasetof 24 distinct polymer types and demonstrated state-of-the-art performance inforce accuracy and robust MD simulations. Furthermore, PolyGET can simulatelarge polymers with high fidelity to the reference ab initio DFT method whilebeing able to generalize to unseen polymers.</description><author>Rui Feng, Huan Tran, Aubrey Toland, Binghong Chen, Qi Zhu, Rampi Ramprasad, Chao Zhang</author><pubDate>Fri, 01 Sep 2023 18:01:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00585v1</guid></item><item><title>Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion</title><link>http://arxiv.org/abs/2309.00584v1</link><description>This paper introduces Laminar, a novel serverless framework based ondispel4py, a parallel stream-based dataflow library. Laminar efficientlymanages streaming workflows and components through a dedicated registry,offering a seamless serverless experience. Leveraging large lenguage models,Laminar enhances the framework with semantic code search, code summarization,and code completion. This contribution enhances serverless computing bysimplifying the execution of streaming computations, managing data streams moreefficiently, and offering a valuable tool for both researchers andpractitioners.</description><author>Zaynab Zahra, Zihao Li, Rosa Filgueira</author><pubDate>Fri, 01 Sep 2023 18:00:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00584v1</guid></item><item><title>Geometry-Informed Neural Operator for Large-Scale 3D PDEs</title><link>http://arxiv.org/abs/2309.00583v1</link><description>We propose the geometry-informed neural operator (GINO), a highly efficientapproach to learning the solution operator of large-scale partial differentialequations with varying geometries. GINO uses a signed distance function andpoint-cloud representations of the input shape and neural operators based ongraph and Fourier architectures to learn the solution operator. The graphneural operator handles irregular grids and transforms them into and fromregular latent grids on which Fourier neural operator can be efficientlyapplied. GINO is discretization-convergent, meaning the trained model can beapplied to arbitrary discretization of the continuous domain and it convergesto the continuum operator as the discretization is refined. To empiricallyvalidate the performance of our method on large-scale simulation, we generatethe industry-standard aerodynamics dataset of 3D vehicle geometries withReynolds numbers as high as five million. For this large-scale 3D fluidsimulation, numerical methods are expensive to compute surface pressure. Wesuccessfully trained GINO to predict the pressure on car surfaces using onlyfive hundred data points. The cost-accuracy experiments show a $26,000 \times$speed-up compared to optimized GPU-based computational fluid dynamics (CFD)simulators on computing the drag coefficient. When tested on new combinationsof geometries and boundary conditions (inlet velocities), GINO obtains aone-fourth reduction in error rate compared to deep neural network approaches.</description><author>Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, Anima Anandkumar</author><pubDate>Fri, 01 Sep 2023 17:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00583v1</guid></item><item><title>Consistency of Lloyd's Algorithm Under Perturbations</title><link>http://arxiv.org/abs/2309.00578v1</link><description>In the context of unsupervised learning, Lloyd's algorithm is one of the mostwidely used clustering algorithms. It has inspired a plethora of workinvestigating the correctness of the algorithm under various settings withground truth clusters. In particular, in 2016, Lu and Zhou have shown that themis-clustering rate of Lloyd's algorithm on $n$ independent samples from asub-Gaussian mixture is exponentially bounded after $O(\log(n))$ iterations,assuming proper initialization of the algorithm. However, in many applications,the true samples are unobserved and need to be learned from the data viapre-processing pipelines such as spectral methods on appropriate data matrices.We show that the mis-clustering rate of Lloyd's algorithm on perturbed samplesfrom a sub-Gaussian mixture is also exponentially bounded after $O(\log(n))$iterations under the assumptions of proper initialization and that theperturbation is small relative to the sub-Gaussian noise. In canonical settingswith ground truth clusters, we derive bounds for algorithms such as$k$-means$++$ to find good initializations and thus leading to the correctnessof clustering via the main result. We show the implications of the results forpipelines measuring the statistical significance of derived clusters from datasuch as SigClust. We use these general results to derive implications inproviding theoretical guarantees on the misclustering rate for Lloyd'salgorithm in a host of applications, including high-dimensional time series,multi-dimensional scaling, and community detection for sparse networks viaspectral clustering.</description><author>Dhruv Patel, Hui Shen, Shankar Bhamidi, Yufeng Liu, Vladas Pipiras</author><pubDate>Fri, 01 Sep 2023 17:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00578v1</guid></item><item><title>From prediction markets to interpretable collective intelligence</title><link>http://arxiv.org/abs/2204.13424v3</link><description>We outline how to create a mechanism that provides an optimal way to elicit,from an arbitrary group of experts, the probability of the truth of anarbitrary logical proposition together with collective information that has anexplicit form and interprets this probability. Namely, we provide strongarguments for the possibility of the development of a self-resolving predictionmarket with play money that incentivizes direct information exchange betweenexperts. Such a system could, in particular, motivate simultaneously manyexperts to collectively solve scientific or medical problems in a veryefficient manner. We also note that in our considerations, experts are notassumed to be Bayesian.</description><author>Alexey V. Osipov, Nikolay N. Osipov</author><pubDate>Fri, 01 Sep 2023 17:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.13424v3</guid></item><item><title>Mechanism of feature learning in convolutional neural networks</title><link>http://arxiv.org/abs/2309.00570v1</link><description>Understanding the mechanism of how convolutional neural networks learnfeatures from image data is a fundamental problem in machine learning andcomputer vision. In this work, we identify such a mechanism. We posit theConvolutional Neural Feature Ansatz, which states that covariances of filtersin any convolutional layer are proportional to the average gradient outerproduct (AGOP) taken with respect to patches of the input to that layer. Wepresent extensive empirical evidence for our ansatz, including identifying highcorrelation between covariances of filters and patch-based AGOPs forconvolutional layers in standard neural architectures, such as AlexNet, VGG,and ResNets pre-trained on ImageNet. We also provide supporting theoreticalevidence. We then demonstrate the generality of our result by using thepatch-based AGOP to enable deep feature learning in convolutional kernelmachines. We refer to the resulting algorithm as (Deep) ConvRFM and show thatour algorithm recovers similar features to deep convolutional networksincluding the notable emergence of edge detectors. Moreover, we find that DeepConvRFM overcomes previously identified limitations of convolutional kernels,such as their inability to adapt to local signals in images and, as a result,leads to sizable performance improvement over fixed convolutional kernels.</description><author>Daniel Beaglehole, Adityanarayanan Radhakrishnan, Parthe Pandit, Mikhail Belkin</author><pubDate>Fri, 01 Sep 2023 17:30:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00570v1</guid></item><item><title>Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer's Disease</title><link>http://arxiv.org/abs/2309.00569v1</link><description>In this work, an image translation model is implemented to produce syntheticamyloid-beta PET images from structural MRI that are quantitatively accurate.Image pairs of amyloid-beta PET and structural MRI were used to train themodel. We found that the synthetic PET images could be produced with a highdegree of similarity to truth in terms of shape, contrast and overall high SSIMand PSNR. This work demonstrates that performing structural to quantitativeimage translation is feasible to enable the access amyloid-beta informationfrom only MRI.</description><author>Fernando Vega, Abdoljalil Addeh, M. Ethan MacDonald</author><pubDate>Fri, 01 Sep 2023 17:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00569v1</guid></item><item><title>Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data</title><link>http://arxiv.org/abs/2309.00564v1</link><description>High-dimensional linear regression is important in many scientific fields.This article considers discrete measured data of underlying smooth latentprocesses, as is often obtained from chemical or biological systems.Interpretation in high dimensions is challenging because the nullspace and itsinterplay with regularization shapes regression coefficients. The data'snullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thusallowing very different coefficients to yield identical predictions. Wedeveloped an optimization formulation to compare regression coefficients andcoefficients obtained by physical engineering knowledge to understand whichpart of the coefficient differences are close to the nullspace. This nullspacemethod is tested on a synthetic example and lithium-ion battery data. The casestudies show that regularization and z-scoring are design choices that, ifchosen corresponding to prior physical knowledge, lead to interpretableregression results. Otherwise, the combination of the nullspace andregularization hinders interpretability and can make it impossible to obtainregression coefficients close to the true coefficients when there is a trueunderlying linear model. Furthermore, we demonstrate that regression methodsthat do not produce coefficients orthogonal to the nullspace, such as fusedlasso, can improve interpretability. In conclusion, the insights gained fromthe nullspace perspective help to make informed design choices for buildingregression models on high-dimensional data and reasoning about potentialunderlying linear models, which are important for system optimization andimproving scientific understanding.</description><author>Joachim Schaeffer, Eric Lenz, William C. Chueh, Martin Z. Bazant, Rolf Findeisen, Richard D. Braatz</author><pubDate>Fri, 01 Sep 2023 17:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00564v1</guid></item><item><title>SSD-MonoDETR: Supervised Scale-aware Deformable Transformer for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2305.07270v4</link><description>Transformer-based methods have demonstrated superior performance formonocular 3D object detection recently, which aims at predicting 3D attributesfrom a single 2D image. Most existing transformer-based methods leverage bothvisual and depth representations to explore valuable query points on objects,and the quality of the learned query points has a great impact on detectionaccuracy. Unfortunately, existing unsupervised attention mechanisms intransformers are prone to generate low-quality query features due to inaccuratereceptive fields, especially on hard objects. To tackle this problem, thispaper proposes a novel "Supervised Scale-aware Deformable Attention" (SSDA) formonocular 3D object detection. Specifically, SSDA presets several masks withdifferent scales and utilizes depth and visual features to adaptively learn ascale-aware filter for object query augmentation. Imposing the scale awareness,SSDA could well predict the accurate receptive field of an object query tosupport robust query feature generation. Aside from this, SSDA is assigned witha Weighted Scale Matching (WSM) loss to supervise scale prediction, whichpresents more confident results as compared to the unsupervised attentionmechanisms. Extensive experiments on the KITTI and Waymo Open datasetsdemonstrate that SSDA significantly improves the detection accuracy, especiallyon moderate and hard objects, yielding state-of-the-art performance as comparedto the existing approaches. Our code will be made publicly available athttps://github.com/mikasa3lili/SSD-MonoDETR.</description><author>Xuan He, Fan Yang, Kailun Yang, Jiacheng Lin, Haolong Fu, Meng Wang, Jin Yuan, Zhiyong Li</author><pubDate>Fri, 01 Sep 2023 17:17:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07270v4</guid></item><item><title>C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation</title><link>http://arxiv.org/abs/2306.15245v3</link><description>Existing reference-free turn-level evaluation metrics for chatbotsinadequately capture the interaction between the user and the system.Consequently, they often correlate poorly with human evaluations. To addressthis issue, we propose a novel model-agnostic approach that leveragesConditional Pointwise Mutual Information (C-PMI) to measure the turn-levelinteraction between the system and the user based on a given evaluationdimension. Experimental results on the widely used FED dialogue evaluationdataset demonstrate that our approach significantly improves the correlationwith human judgment compared with existing evaluation systems. By replacing thenegative log-likelihood-based scorer with our proposed C-PMI scorer, we achievea relative 62.6% higher Spearman correlation on average for the FED evaluationmetric. Our code is publicly available at https://github.com/renll/C-PMI.</description><author>Liliang Ren, Mankeerat Sidhu, Qi Zeng, Revanth Gangi Reddy, Heng Ji, ChengXiang Zhai</author><pubDate>Fri, 01 Sep 2023 17:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15245v3</guid></item><item><title>Interactive and Concentrated Differential Privacy for Bandits</title><link>http://arxiv.org/abs/2309.00557v1</link><description>Bandits play a crucial role in interactive learning schemes and modernrecommender systems. However, these systems often rely on sensitive user data,making privacy a critical concern. This paper investigates privacy in banditswith a trusted centralized decision-maker through the lens of interactiveDifferential Privacy (DP). While bandits under pure $\epsilon$-global DP havebeen well-studied, we contribute to the understanding of bandits under zeroConcentrated DP (zCDP). We provide minimax and problem-dependent lower boundson regret for finite-armed and linear bandits, which quantify the cost of$\rho$-global zCDP in these settings. These lower bounds reveal two hardnessregimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDPincurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-globalzCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linearbandits respectively. Both algorithms use a common recipe of Gaussian mechanismand adaptive episodes. We analyze the regret of these algorithms to show thatAdaC-UCB achieves the problem-dependent regret lower bound up to multiplicativeconstants, while AdaC-GOPE achieves the minimax regret lower bound up topoly-logarithmic factors. Finally, we provide experimental validation of ourtheoretical results under different settings.</description><author>Achraf Azize, Debabrota Basu</author><pubDate>Fri, 01 Sep 2023 17:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00557v1</guid></item><item><title>Impact of Image Context for Single Deep Learning Face Morphing Attack Detection</title><link>http://arxiv.org/abs/2309.00549v1</link><description>The increase in security concerns due to technological advancements has ledto the popularity of biometric approaches that utilize physiological orbehavioral characteristics for enhanced recognition. Face recognition systems(FRSs) have become prevalent, but they are still vulnerable to imagemanipulation techniques such as face morphing attacks. This study investigatesthe impact of the alignment settings of input images on deep learning facemorphing detection performance. We analyze the interconnections between theface contour and image context and suggest optimal alignment conditions forface morphing detection.</description><author>Joana Pimenta, Iurii Medvedev, Nuno Gonçalves</author><pubDate>Fri, 01 Sep 2023 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00549v1</guid></item><item><title>M^2UNet: MetaFormer Multi-scale Upsampling Network for Polyp Segmentation</title><link>http://arxiv.org/abs/2306.08600v2</link><description>Polyp segmentation has recently garnered significant attention, and multiplemethods have been formulated to achieve commendable outcomes. However, thesetechniques often confront difficulty when working with the complex polypforeground and their surrounding regions because of the nature of convolutionoperation. Besides, most existing methods forget to exploit the potentialinformation from multiple decoder stages. To address this challenge, we suggestcombining MetaFormer, introduced as a baseline for integrating CNN andTransformer, with UNet framework and incorporating our Multi-scale Upsamplingblock (MU). This simple module makes it possible to combine multi-levelinformation by exploring multiple receptive field paths of the shallow decoderstage and then adding with the higher stage to aggregate better featurerepresentation, which is essential in medical image segmentation. Taken alltogether, we propose MetaFormer Multi-scale Upsampling Network (M$^2$UNet) forthe polyp segmentation task. Extensive experiments on five benchmark datasetsdemonstrate that our method achieved competitive performance compared withseveral previous methods.</description><author>Quoc-Huy Trinh, Nhat-Tan Bui, Trong-Hieu Nguyen Mau, Minh-Van Nguyen, Hai-Minh Phan, Minh-Triet Tran, Hai-Dang Nguyen</author><pubDate>Fri, 01 Sep 2023 16:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08600v2</guid></item><item><title>Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare</title><link>http://arxiv.org/abs/2309.00543v1</link><description>Deep learning models have shown promising predictive accuracy for time-serieshealthcare applications. However, ensuring the robustness of these models isvital for building trustworthy AI systems. Existing research predominantlyfocuses on robustness to synthetic adversarial examples, crafted by addingimperceptible perturbations to clean input data. However, these syntheticadversarial examples do not accurately reflect the most challenging real-worldscenarios, especially in the context of healthcare data. Consequently,robustness to synthetic adversarial examples may not necessarily translate torobustness against naturally occurring adversarial examples, which is highlydesirable for trustworthy AI. We propose a method to curate datasets comprisedof natural adversarial examples to evaluate model robustness. The method relieson probabilistic labels obtained from automated weakly-supervised labeling thatcombines noisy and cheap-to-obtain labeling heuristics. Based on these labels,our method adversarially orders the input data and uses this ordering toconstruct a sequence of increasingly adversarial datasets. Our evaluation onsix medical case studies and three non-medical case studies demonstrates theefficacy and statistical validity of our approach to generating naturallyadversarial datasets</description><author>Sydney Pugh, Ivan Ruchkin, Insup Lee, James Weimer</author><pubDate>Fri, 01 Sep 2023 16:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00543v1</guid></item><item><title>A Smart Robotic System for Industrial Plant Supervision</title><link>http://arxiv.org/abs/2308.05612v2</link><description>In today's chemical plants, human field operators perform frequent integritychecks to guarantee high safety standards, and thus are possibly the first toencounter dangerous operating conditions. To alleviate their task, we present asystem consisting of an autonomously navigating robot integrated with varioussensors and intelligent data processing. It is able to detect methane leaks andestimate its flow rate, detect more general gas anomalies, recognize oil films,localize sound sources and detect failure cases, map the environment in 3D, andnavigate autonomously, employing recognition and avoidance of dynamicobstacles. We evaluate our system at a wastewater facility in full workingconditions. Our results demonstrate that the system is able to robustlynavigate the plant and provide useful information about critical operatingconditions.</description><author>D. Adriana Gómez-Rosal, Max Bergau, Georg K. J. Fischer, Andreas Wachaja, Johannes Gräter, Matthias Odenweller, Uwe Piechottka, Fabian Hoeflinger, Nikhil Gosala, Niklas Wetzel, Daniel Büscher, Abhinav Valada, Wolfram Burgard</author><pubDate>Fri, 01 Sep 2023 16:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05612v2</guid></item><item><title>Lingua Manga: A Generic Large Language Model Centric System for Data Curation</title><link>http://arxiv.org/abs/2306.11702v2</link><description>Data curation is a wide-ranging area which contains many critical buttime-consuming data processing tasks. However, the diversity of such tasksmakes it challenging to develop a general-purpose data curation system. Toaddress this issue, we present Lingua Manga, a user-friendly and versatilesystem that utilizes pre-trained large language models. Lingua Manga offersautomatic optimization for achieving high performance and label efficiencywhile facilitating flexible and rapid development. Through three exampleapplications with distinct objectives and users of varying levels of technicalproficiency, we demonstrate that Lingua Manga can effectively assist bothskilled programmers and low-code or even no-code users in addressing datacuration challenges.</description><author>Zui Chen, Lei Cao, Sam Madden</author><pubDate>Fri, 01 Sep 2023 16:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11702v2</guid></item><item><title>Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations</title><link>http://arxiv.org/abs/2308.16505v2</link><description>Recommender models excel at providing domain-specific item recommendations byleveraging extensive user behavior data. Despite their ability to act aslightweight domain experts, they struggle to perform versatile tasks such asproviding explanations and engaging in conversations. On the other hand, largelanguage models (LLMs) represent a significant step towards artificial generalintelligence, showcasing remarkable capabilities in instruction comprehension,commonsense reasoning, and human interaction. However, LLMs lack the knowledgeof domain-specific item catalogs and behavioral patterns, particularly in areasthat diverge from general world knowledge, such as online e-commerce.Finetuning LLMs for each domain is neither economic nor efficient. In this paper, we bridge the gap between recommender models and LLMs,combining their respective strengths to create a versatile and interactiverecommender system. We introduce an efficient framework called InteRecAgent,which employs LLMs as the brain and recommender models as tools. We firstoutline a minimal set of essential tools required to transform LLMs intoInteRecAgent. We then propose an efficient workflow within InteRecAgent fortask execution, incorporating key components such as a memory bus, dynamicdemonstration-augmented task planning, and reflection. InteRecAgent enablestraditional recommender systems, such as those ID-based matrix factorizationmodels, to become interactive systems with a natural language interface throughthe integration of LLMs. Experimental results on several public datasets showthat InteRecAgent achieves satisfying performance as a conversationalrecommender system, outperforming general-purpose LLMs.</description><author>Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie</author><pubDate>Fri, 01 Sep 2023 16:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16505v2</guid></item><item><title>Adaptive function approximation based on the Discrete Cosine Transform (DCT)</title><link>http://arxiv.org/abs/2309.00530v1</link><description>This paper studies the cosine as basis function for the approximation ofunivariate and continuous functions without memory. This work studies asupervised learning to obtain the approximation coefficients, instead of usingthe Discrete Cosine Transform (DCT). Due to the finite dynamics andorthogonality of the cosine basis functions, simple gradient algorithms, suchas the Normalized Least Mean Squares (NLMS), can benefit from it and present acontrolled and predictable convergence time and error misadjustment. Due to itssimplicity, the proposed technique ranks as the best in terms of learningquality versus complexity, and it is presented as an attractive technique to beused in more complex supervised learning systems. Simulations illustrate theperformance of the approach. This paper celebrates the 50th anniversary of thepublication of the DCT by Nasir Ahmed in 1973.</description><author>Ana I. Pérez-Neira, Marc Martinez-Gost, Miguel Ángel Lagunas</author><pubDate>Fri, 01 Sep 2023 16:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00530v1</guid></item><item><title>Learning to Prompt in the Classroom to Understand AI Limits: A pilot study</title><link>http://arxiv.org/abs/2307.01540v2</link><description>Artificial intelligence's (AI) progress holds great promise in tacklingpressing societal concerns such as health and climate. Large Language Models(LLM) and the derived chatbots, like ChatGPT, have highly improved the naturallanguage processing capabilities of AI systems allowing them to process anunprecedented amount of unstructured data. However, the ensuing excitement hasled to negative sentiments, even as AI methods demonstrate remarkablecontributions (e.g. in health and genetics). A key factor contributing to thissentiment is the misleading perception that LLMs can effortlessly providesolutions across domains, ignoring their limitations such as hallucinations andreasoning constraints. Acknowledging AI fallibility is crucial to address theimpact of dogmatic overconfidence in possibly erroneous suggestions generatedby LLMs. At the same time, it can reduce fear and other negative attitudestoward AI. This necessitates comprehensive AI literacy interventions thateducate the public about LLM constraints and effective usage techniques, i.eprompting strategies. With this aim, a pilot educational intervention wasperformed in a high school with 21 students. It involved presenting high-levelconcepts about intelligence, AI, and LLMs, followed by practical exercisesinvolving ChatGPT in creating natural educational conversations and applyingestablished prompting strategies. Encouraging preliminary results emerged,including high appreciation of the activity, improved interaction quality withthe LLM, reduced negative AI sentiments, and a better grasp of limitations,specifically unreliability, limited understanding of commands leading tounsatisfactory responses, and limited presentation flexibility. Our aim is toexplore AI acceptance factors and refine this approach for more controlledfuture studies.</description><author>Emily Theophilou, Cansu Koyuturk, Mona Yavari, Sathya Bursic, Gregor Donabauer, Alessia Telari, Alessia Testa, Raffaele Boiano, Davinia Hernandez-Leo, Martin Ruskov, Davide Taibi, Alessandro Gabbiadini, Dimitri Ognibene</author><pubDate>Fri, 01 Sep 2023 16:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01540v2</guid></item><item><title>Trust your Good Friends: Source-free Domain Adaptation by Reciprocal Neighborhood Clustering</title><link>http://arxiv.org/abs/2309.00528v1</link><description>Domain adaptation (DA) aims to alleviate the domain shift between sourcedomain and target domain. Most DA methods require access to the source data,but often that is not possible (e.g. due to data privacy or intellectualproperty). In this paper, we address the challenging source-free domainadaptation (SFDA) problem, where the source pretrained model is adapted to thetarget domain in the absence of source data. Our method is based on theobservation that target data, which might not align with the source domainclassifier, still forms clear clusters. We capture this intrinsic structure bydefining local affinity of the target data, and encourage label consistencyamong data with high local affinity. We observe that higher affinity should beassigned to reciprocal neighbors. To aggregate information with more context,we consider expanded neighborhoods with small affinity values. Furthermore, weconsider the density around each target sample, which can alleviate thenegative impact of potential outliers. In the experimental results we verifythat the inherent structure of the target features is an important source ofinformation for domain adaptation. We demonstrate that this local structure canbe efficiently captured by considering the local neighbors, the reciprocalneighbors, and the expanded neighborhood. Finally, we achieve state-of-the-artperformance on several 2D image and 3D point cloud recognition datasets.</description><author>Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, Shangling Jui, Jian Yang</author><pubDate>Fri, 01 Sep 2023 16:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00528v1</guid></item><item><title>SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</title><link>http://arxiv.org/abs/2309.00526v1</link><description>Recently, self-supervised monocular depth estimation has gained popularitywith numerous applications in autonomous driving and robotics. However,existing solutions primarily seek to estimate depth from immediate visualfeatures, and struggle to recover fine-grained scene details with limitedgeneralization. In this paper, we introduce SQLdepth, a novel approach that caneffectively learn fine-grained scene structures from motion. In SQLdepth, wepropose a novel Self Query Layer (SQL) to build a self-cost volume and inferdepth from it, rather than inferring depth from feature maps. The self-costvolume implicitly captures the intrinsic geometry of the scene within a singleframe. Each individual slice of the volume signifies the relative distancesbetween points and objects within a latent space. Ultimately, this volume iscompressed to the depth map via a novel decoding approach. Experimental resultson KITTI and Cityscapes show that our method attains remarkablestate-of-the-art performance (AbsRel = $0.082$ on KITTI, $0.052$ on KITTI withimproved ground-truth and $0.106$ on Cityscapes), achieves $9.9\%$, $5.5\%$ and$4.5\%$ error reduction from the previous best. In addition, our approachshowcases reduced training complexity, computational efficiency, improvedgeneralization, and the ability to recover fine-grained scene details.Moreover, the self-supervised pre-trained and metric fine-tuned SQLdepth cansurpass existing supervised methods by significant margins (AbsRel = $0.043$,$14\%$ error reduction). self-matching-oriented relative distance querying inSQL improves the robustness and zero-shot generalization capability ofSQLdepth. Code and the pre-trained weights will be publicly available. Code isavailable at\href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}.</description><author>Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, Hongkai Yu</author><pubDate>Fri, 01 Sep 2023 16:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00526v1</guid></item><item><title>Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement</title><link>http://arxiv.org/abs/2306.14704v3</link><description>Mentions of new concepts appear regularly in texts and require automatedapproaches to harvest and place them into Knowledge Bases (KB), e.g.,ontologies and taxonomies. Existing datasets suffer from three issues, (i)mostly assuming that a new concept is pre-discovered and cannot supportout-of-KB mention discovery; (ii) only using the concept label as the inputalong with the KB and thus lacking the contexts of a concept label; and (iii)mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,instead of complex concepts, i.e., with logical operators. To address theseissues, we propose a new benchmark, adapting MedMentions dataset (PubMedabstracts) with SNOMED CT versions in 2014 and 2017 under the Diseasessub-category and the broader categories of Clinical finding, Procedure, andPharmaceutical / biologic product. We provide usage on the evaluation with thedataset for out-of-KB mention discovery and concept placement, adapting recentLarge Language Model based methods.</description><author>Hang Dong, Jiaoyan Chen, Yuan He, Ian Horrocks</author><pubDate>Fri, 01 Sep 2023 16:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14704v3</guid></item><item><title>Mapping the landscape of histomorphological cancer phenotypes using self-supervised learning on unlabeled, unannotated pathology slides</title><link>http://arxiv.org/abs/2205.01931v3</link><description>Definitive cancer diagnosis and management depend upon the extraction ofinformation from microscopy images by pathologists. These images containcomplex information requiring time-consuming expert human interpretation thatis prone to human bias. Supervised deep learning approaches have provenpowerful for classification tasks, but they are inherently limited by the costand quality of annotations used for training these models. To address thislimitation of supervised methods, we developed Histomorphological PhenotypeLearning (HPL), a fully blue{self-}supervised methodology that requires noexpert labels or annotations and operates via the automatic discovery ofdiscriminatory image features in small image tiles. Tiles are grouped intomorphologically similar clusters which constitute a library ofhistomorphological phenotypes, revealing trajectories from benign to malignanttissue via inflammatory and reactive phenotypes. These clusters have distinctfeatures which can be identified using orthogonal methods, linking histologic,molecular and clinical phenotypes. Applied to lung cancer tissues, we show thatthey align closely with patient survival, with histopathologically recognisedtumor types and growth patterns, and with transcriptomic measures ofimmunophenotype. We then demonstrate that these properties are maintained in amulti-cancer study. These results show the clusters represent recurrent hostresponses and modes of tumor growth emerging under natural selection. Code,pre-trained models, learned embeddings, and documentation are available to thecommunity athttps://github.com/AdalbertoCq/Histomorphological-Phenotype-Learning</description><author>Adalberto Claudio Quiros, Nicolas Coudray, Anna Yeaton, Xinyu Yang, Bojing Liu, Hortense Le, Luis Chiriboga, Afreen Karimkhan, Navneet Narula, David A. Moore, Christopher Y. Park, Harvey Pass, Andre L. Moreira, John Le Quesne, Aristotelis Tsirigos, Ke Yuan</author><pubDate>Fri, 01 Sep 2023 16:26:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.01931v3</guid></item><item><title>Clutter Detection and Removal in 3D Scenes with View-Consistent Inpainting</title><link>http://arxiv.org/abs/2304.03763v2</link><description>Removing clutter from scenes is essential in many applications, ranging fromprivacy-concerned content filtering to data augmentation. In this work, wepresent an automatic system that removes clutter from 3D scenes and inpaintswith coherent geometry and texture. We propose techniques for its two keycomponents: 3D segmentation from shared properties and 3D inpainting, both ofwhich are important problems. The definition of 3D scene clutter(frequently-moving objects) is not well captured by commonly-studied objectcategories in computer vision. To tackle the lack of well-defined clutterannotations, we group noisy fine-grained labels, leverage virtual rendering,and impose an instance-level area-sensitive loss. Once clutter is removed, weinpaint geometry and texture in the resulting holes by merging inpainted RGB-Dimages. This requires novel voting and pruning strategies that guaranteemulti-view consistency across individually inpainted images for meshreconstruction. Experiments on ScanNet and Matterport dataset show that ourmethod outperforms baselines for clutter segmentation and 3D inpainting, bothvisually and quantitatively.</description><author>Fangyin Wei, Thomas Funkhouser, Szymon Rusinkiewicz</author><pubDate>Fri, 01 Sep 2023 16:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03763v2</guid></item><item><title>Online Distributed Learning over Random Networks</title><link>http://arxiv.org/abs/2309.00520v1</link><description>The recent deployment of multi-agent systems in a wide range of scenarios hasenabled the solution of learning problems in a distributed fashion. In thiscontext, agents are tasked with collecting local data and then cooperativelytrain a model, without directly sharing the data. While distributed learningoffers the advantage of preserving agents' privacy, it also poses severalchallenges in terms of designing and analyzing suitable algorithms. This workfocuses specifically on the following challenges motivated by practicalimplementation: (i) online learning, where the local data change over time;(ii) asynchronous agent computations; (iii) unreliable and limitedcommunications; and (iv) inexact local computations. To tackle thesechallenges, we introduce the Distributed Operator Theoretical (DOT) version ofthe Alternating Direction Method of Multipliers (ADMM), which we call theDOT-ADMM Algorithm. We prove that it converges with a linear rate for a largeclass of convex learning problems (e.g., linear and logistic regressionproblems) toward a bounded neighborhood of the optimal time-varying solution,and characterize how the neighborhood depends on~$\text{(i)--(iv)}$. Wecorroborate the theoretical analysis with numerical simulations comparing theDOT-ADMM Algorithm with other state-of-the-art algorithms, showing that onlythe proposed algorithm exhibits robustness to (i)--(iv).</description><author>Nicola Bastianello, Diego Deplano, Mauro Franceschelli, Karl H. Johansson</author><pubDate>Fri, 01 Sep 2023 16:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00520v1</guid></item><item><title>Causal Policy Gradient for Whole-Body Mobile Manipulation</title><link>http://arxiv.org/abs/2305.04866v3</link><description>Developing the next generation of household robot helpers requires combininglocomotion and interaction capabilities, which is generally referred to asmobile manipulation (MoMa). MoMa tasks are difficult due to the large actionspace of the robot and the common multi-objective nature of the task, e.g.,efficiently reaching a goal while avoiding obstacles. Current approaches oftensegregate tasks into navigation without manipulation and stationarymanipulation without locomotion by manually matching parts of the action spaceto MoMa sub-objectives (e.g. base actions for locomotion objectives and armactions for manipulation). This solution prevents simultaneous combinations oflocomotion and interaction degrees of freedom and requires human domainknowledge for both partitioning the action space and matching the action partsto the sub-objectives. In this paper, we introduce Causal MoMa, a new frameworkto train policies for typical MoMa tasks that makes use of the most favorablesubspace of the robot's action space to address each sub-objective. Causal MoMaautomatically discovers the causal dependencies between actions and terms ofthe reward function and exploits these dependencies in a causal policy learningprocedure that reduces gradient variance compared to previous state-of-the-artpolicy gradient algorithms, improving convergence and results. We evaluate theperformance of Causal MoMa on three types of simulated robots across differentMoMa tasks and demonstrate success in transferring the policies trained insimulation directly to a real robot, where our agent is able to follow movinggoals and react to dynamic obstacles while simultaneously and synergisticallycontrolling the whole-body: base, arm, and head. More information athttps://sites.google.com/view/causal-moma.</description><author>Jiaheng Hu, Peter Stone, Roberto Martín-Martín</author><pubDate>Fri, 01 Sep 2023 16:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04866v3</guid></item><item><title>A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm</title><link>http://arxiv.org/abs/2309.00514v1</link><description>In the procedure of surface defects detection for large-aperture asphericaloptical elements, it is of vital significance to adjust the optical axis of theelement to be coaxial with the mechanical spin axis accurately. Therefore, amachine vision method for eccentric error correction is proposed in this paper.Focusing on the severe defocus blur of reference crosshair image caused by theimaging characteristic of the aspherical optical element, which may lead to thefailure of correction, an Adaptive Enhancement Algorithm (AEA) is proposed tostrengthen the crosshair image. AEA is consisted of existed Guided Filter DarkChannel Dehazing Algorithm (GFA) and proposed lightweight Multi-scale DenselyConnected Network (MDC-Net). The enhancement effect of GFA is excellent buttime-consuming, and the enhancement effect of MDC-Net is slightly inferior butstrongly real-time. As AEA will be executed dozens of times during eachcorrection procedure, its real-time performance is very important. Therefore,by setting the empirical threshold of definition evaluation function SMD2, GFAand MDC-Net are respectively applied to highly and slightly blurred crosshairimages so as to ensure the enhancement effect while saving as much time aspossible. AEA has certain robustness in time-consuming performance, which takesan average time of 0.2721s and 0.0963s to execute GFA and MDC-Net separately onten 200pixels 200pixels Region of Interest (ROI) images with different degreesof blur. And the eccentricity error can be reduced to within 10um by ourmethod.</description><author>Fanyi Wang, Pin Cao, Yihui Zhang, Haotian Hu, Yongying Yang</author><pubDate>Fri, 01 Sep 2023 16:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00514v1</guid></item><item><title>Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks</title><link>http://arxiv.org/abs/2309.00508v1</link><description>Under mild assumptions, we investigate the structure of loss landscape oftwo-layer neural networks near global minima, determine the set of parameterswhich give perfect generalization, and fully characterize the gradient flowsaround it. With novel techniques, our work uncovers some simple aspects of thecomplicated loss landscape and reveals how model, target function, samples andinitialization affect the training dynamics differently. Based on theseresults, we also explain why (overparametrized) neural networks couldgeneralize well.</description><author>Leyang Zhang, Yaoyu Zhang, Tao Luo</author><pubDate>Fri, 01 Sep 2023 15:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00508v1</guid></item><item><title>Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems</title><link>http://arxiv.org/abs/2309.00498v1</link><description>This PhD thesis thoroughly examines the utilization of deep learningtechniques as a means to advance the algorithms employed in the monitoring andoptimization of electric power systems. The first major contribution of thisthesis involves the application of graph neural networks to enhance powersystem state estimation. The second key aspect of this thesis focuses onutilizing reinforcement learning for dynamic distribution networkreconfiguration. The effectiveness of the proposed methods is affirmed throughextensive experimentation and simulations.</description><author>Ognjen Kundacina</author><pubDate>Fri, 01 Sep 2023 15:42:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00498v1</guid></item><item><title>Multi-stage Deep Learning Artifact Reduction for Computed Tomography</title><link>http://arxiv.org/abs/2309.00494v1</link><description>In Computed Tomography (CT), an image of the interior structure of an objectis computed from a set of acquired projection images. The quality of thesereconstructed images is essential for accurate analysis, but this quality canbe degraded by a variety of imaging artifacts. To improve reconstructionquality, the acquired projection images are often processed by a pipelineconsisting of multiple artifact-removal steps applied in various image domains(e.g., outlier removal on projection images and denoising of reconstructionimages). These artifact-removal methods exploit the fact that certain artifactsare easier to remove in a certain domain compared with other domains. Recently, deep learning methods have shown promising results for artifactremoval for CT images. However, most existing deep learning methods for CT areapplied as a post-processing method after reconstruction. Therefore, artifactsthat are relatively difficult to remove in the reconstruction domain may not beeffectively removed by these methods. As an alternative, we propose amulti-stage deep learning method for artifact removal, in which neural networksare applied to several domains, similar to a classical CT processing pipeline.We show that the neural networks can be effectively trained in succession,resulting in easy-to-use and computationally efficient training. Experiments onboth simulated and real-world experimental datasets show that our method iseffective in reducing artifacts and superior to deep learning-basedpost-processing.</description><author>Jiayang Shi, Daniel M. Pelt, K. Joost Batenburg</author><pubDate>Fri, 01 Sep 2023 15:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00494v1</guid></item><item><title>How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?</title><link>http://arxiv.org/abs/2309.00489v1</link><description>The success of immersive applications such as virtual reality (VR) gaming andmetaverse services depends on low latency and reliable connectivity. To provideseamless user experiences, the open radio access network (O-RAN) architectureand 6G networks are expected to play a crucial role. RAN slicing, a criticalcomponent of the O-RAN paradigm, enables network resources to be allocatedbased on the needs of immersive services, creating multiple virtual networks ona single physical infrastructure. In the O-RAN literature, deep reinforcementlearning (DRL) algorithms are commonly used to optimize resource allocation.However, the practical adoption of DRL in live deployments has been sluggish.This is primarily due to the slow convergence and performance instabilitiessuffered by the DRL agents both upon initial deployment and when there aresignificant changes in network conditions. In this paper, we investigate theimpact of time series forecasting of traffic demands on the convergence of theDRL-based slicing agents. For that, we conduct an exhaustive experiment thatsupports multiple services including real VR gaming traffic. We then propose anovel forecasting-aided DRL approach and its respective O-RAN practicaldeployment workflow to enhance DRL convergence. Our approach shows up to 22.8%,86.3%, and 300% improvements in the average initial reward value, convergencerate, and number of converged scenarios respectively, enhancing thegeneralizability of the DRL agents compared with the implemented baselines. Theresults also indicate that our approach is robust against forecasting errorsand that forecasting models do not have to be ideal.</description><author>Ahmad M. Nagib, Hatem Abou-Zeid, Hossam S. Hassanein</author><pubDate>Fri, 01 Sep 2023 15:30:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00489v1</guid></item><item><title>Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction</title><link>http://arxiv.org/abs/2309.00483v1</link><description>Molecular property prediction with deep learning has gained much attentionover the past years. Owing to the scarcity of labeled molecules, there has beengrowing interest in self-supervised learning methods that learn generalizablemolecular representations from unlabeled data. Molecules are typically treatedas 2D topological graphs in modeling, but it has been discovered that their 3Dgeometry is of great importance in determining molecular functionalities. Inthis paper, we propose the Geometry-aware line graph transformer (Galformer)pre-training, a novel self-supervised learning framework that aims to enhancemolecular representation learning with 2D and 3D modalities. Specifically, wefirst design a dual-modality line graph transformer backbone to encode thetopological and geometric information of a molecule. The designed backboneincorporates effective structural encodings to capture graph structures fromboth modalities. Then we devise two complementary pre-training tasks at theinter and intra-modality levels. These tasks provide properly supervisedinformation and extract discriminative 2D and 3D knowledge from unlabeledmolecules. Finally, we evaluate Galformer against six state-of-the-artbaselines on twelve property prediction benchmarks via downstream fine-tuning.Experimental results show that Galformer consistently outperforms all baselineson both classification and regression tasks, demonstrating its effectiveness.</description><author>Peizhen Bai, Xianyuan Liu, Haiping Lu</author><pubDate>Fri, 01 Sep 2023 15:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00483v1</guid></item><item><title>Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network</title><link>http://arxiv.org/abs/2309.00480v1</link><description>The global navigation satellite systems (GNSS) play a vital role in transportsystems for accurate and consistent vehicle localization. However, GNSSobservations can be distorted due to multipath effects and non-line-of-sight(NLOS) receptions in challenging environments such as urban canyons. In suchcases, traditional methods to classify and exclude faulty GNSS observations mayfail, leading to unreliable state estimation and unsafe system operations. Thiswork proposes a Deep-Learning-based method to detect NLOS receptions andpredict GNSS pseudorange errors by analyzing GNSS observations as aspatio-temporal modeling problem. Compared to previous works, we construct atransformer-like attention mechanism to enhance the long short-term memory(LSTM) networks, improving model performance and generalization. For thetraining and evaluation of the proposed network, we used labeled datasets fromthe cities of Hong Kong and Aachen. We also introduce a dataset generationprocess to label the GNSS observations using lidar maps. In experimentalstudies, we compare the proposed network with a deep-learning-based model andclassical machine-learning models. Furthermore, we conduct ablation studies ofour network components and integrate the NLOS detection with dataout-of-distribution in a state estimator. As a result, our network presentsimproved precision and recall ratios compared to other models. Additionally, weshow that the proposed method avoids trajectory divergence in real-worldvehicle localization by classifying and excluding NLOS observations.</description><author>Haoming Zhang, Zhanxin Wang, Heike Vallery</author><pubDate>Fri, 01 Sep 2023 15:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00480v1</guid></item><item><title>Asymmetric double-winged multi-view clustering network for exploring Diverse and Consistent Information</title><link>http://arxiv.org/abs/2309.00474v1</link><description>In unsupervised scenarios, deep contrastive multi-view clustering (DCMVC) isbecoming a hot research spot, which aims to mine the potential relationshipsbetween different views. Most existing DCMVC algorithms focus on exploring theconsistency information for the deep semantic features, while ignoring thediverse information on shallow features. To fill this gap, we propose a novelmulti-view clustering network termed CodingNet to explore the diverse andconsistent information simultaneously in this paper. Specifically, instead ofutilizing the conventional auto-encoder, we design an asymmetric structurenetwork to extract shallow and deep features separately. Then, by aligning thesimilarity matrix on the shallow feature to the zero matrix, we ensure thediversity for the shallow features, thus offering a better description ofmulti-view data. Moreover, we propose a dual contrastive mechanism thatmaintains consistency for deep features at both view-feature and pseudo-labellevels. Our framework's efficacy is validated through extensive experiments onsix widely used benchmark datasets, outperforming most state-of-the-artmulti-view clustering algorithms.</description><author>Qun Zheng, Xihong Yang, Siwei Wang, Xinru An, Qi Liu</author><pubDate>Fri, 01 Sep 2023 15:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00474v1</guid></item><item><title>General and Practical Tuning Method for Off-the-Shelf Graph-Based Index: SISAP Indexing Challenge Report by Team UTokyo</title><link>http://arxiv.org/abs/2309.00472v1</link><description>Despite the efficacy of graph-based algorithms for Approximate NearestNeighbor (ANN) searches, the optimal tuning of such systems remains unclear.This study introduces a method to tune the performance of off-the-shelfgraph-based indexes, focusing on the dimension of vectors, database size, andentry points of graph traversal. We utilize a black-box optimization algorithmto perform integrated tuning to meet the required levels of recall and QueriesPer Second (QPS). We applied our approach to Task A of the SISAP 2023 IndexingChallenge and got second place in the 10M and 30M tracks. It improvesperformance substantially compared to brute force methods. This research offersa universally applicable tuning method for graph-based indexes, extendingbeyond the specific conditions of the competition to broader uses.</description><author>Yutaro Oguri, Yusuke Matsui</author><pubDate>Fri, 01 Sep 2023 15:11:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00472v1</guid></item><item><title>Graph Structural Residuals: A Learning Approach to Diagnosis</title><link>http://arxiv.org/abs/2308.06961v2</link><description>Traditional model-based diagnosis relies on constructing explicit systemmodels, a process that can be laborious and expertise-demanding. In this paper,we propose a novel framework that combines concepts of model-based diagnosiswith deep graph structure learning. This data-driven approach leverages data tolearn the system's underlying structure and provide dynamic observations,represented by two distinct graph adjacency matrices. Our work facilitates aseamless integration of graph structure learning with model-based diagnosis bymaking three main contributions: (i) redefining the constructs of systemrepresentation, observations, and faults (ii) introducing two distinct versionsof a self-supervised graph structure learning model architecture and (iii)demonstrating the potential of our data-driven diagnostic method throughexperiments on a system of coupled oscillators.</description><author>Jan Lukas Augustin, Oliver Niggemann</author><pubDate>Fri, 01 Sep 2023 15:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06961v2</guid></item><item><title>An Improved Encoder-Decoder Framework for Food EnergyEstimation</title><link>http://arxiv.org/abs/2309.00468v1</link><description>Dietary assessment is essential to maintaining a healthy lifestyle. Automaticimage-based dietary assessment is a growing field of research due to theincreasing prevalence of image capturing devices (e.g. mobile phones). In thiswork, we estimate food energy from a single monocular image, a difficult taskdue to the limited hard-to-extract amount of energy information present in animage. To do so, we employ an improved encoder-decoder framework for energyestimation; the encoder transforms the image into a representation embeddedwith food energy information in an easier-to-extract format, which the decoderthen extracts the energy information from. To implement our method, we compilea high-quality food image dataset verified by registered dietitians containingeating scene images, food-item segmentation masks, and ground truth calorievalues. Our method improves upon previous caloric estimation methods by over10\% and 30 kCal in terms of MAPE and MAE respectively.</description><author>Jack Ma, Jiangpeng He, Fengqing Zhu</author><pubDate>Fri, 01 Sep 2023 15:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00468v1</guid></item><item><title>A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection</title><link>http://arxiv.org/abs/2309.00464v1</link><description>The proliferation of Deep Neural Networks has resulted in machine learningsystems becoming increasingly more present in various real-world applications.Consequently, there is a growing demand for highly reliable models in thesedomains, making the problem of uncertainty calibration pivotal, whenconsidering the future of deep learning. This is especially true whenconsidering object detection systems, that are commonly present insafety-critical application such as autonomous driving and robotics. For thisreason, this work presents a novel theoretical and practical framework toevaluate object detection systems in the context of uncertainty calibration.The robustness of the proposed uncertainty calibration metrics is shown througha series of representative experiments. Code for the proposed uncertaintycalibration metrics at:https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection.</description><author>Pedro Conde, Rui L. Lopes, Cristiano Premebida</author><pubDate>Fri, 01 Sep 2023 15:02:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00464v1</guid></item><item><title>Making a Case for 3D Convolutions for Object Segmentation in Videos</title><link>http://arxiv.org/abs/2008.11516v2</link><description>The task of object segmentation in videos is usually accomplished byprocessing appearance and motion information separately using standard 2Dconvolutional networks, followed by a learned fusion of the two sources ofinformation. On the other hand, 3D convolutional networks have beensuccessfully applied for video classification tasks, but have not beenleveraged as effectively to problems involving dense per-pixel interpretationof videos compared to their 2D convolutional counterparts and lag behind theaforementioned networks in terms of performance. In this work, we show that 3DCNNs can be effectively applied to dense video prediction tasks such as salientobject segmentation. We propose a simple yet effective encoder-decoder networkarchitecture consisting entirely of 3D convolutions that can be trainedend-to-end using a standard cross-entropy loss. To this end, we leverage anefficient 3D encoder, and propose a 3D decoder architecture, that comprisesnovel 3D Global Convolution layers and 3D Refinement modules. Our approachoutperforms existing state-of-the-arts by a large margin on the DAVIS'16Unsupervised, FBMS and ViSal dataset benchmarks in addition to being faster,thus showing that our architecture can efficiently learn expressivespatio-temporal features and produce high quality video segmentation masks. Wehave made our code and trained models publicly available athttps://github.com/sabarim/3DC-Seg.</description><author>Sabarinath Mahadevan, Ali Athar, Aljoša Ošep, Sebastian Hennen, Laura Leal-Taixé, Bastian Leibe</author><pubDate>Fri, 01 Sep 2023 15:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.11516v2</guid></item><item><title>One Object at a Time: Accurate and Robust Structure From Motion for Robots</title><link>http://arxiv.org/abs/2208.00487v3</link><description>A gaze-fixating robot perceives distance to the fixated object and relativepositions of surrounding objects immediately, accurately, and robustly. We showhow fixation, which is the act of looking at one object while moving, exploitsregularities in the geometry of 3D space to obtain this information. Theseregularities introduce rotation-translation couplings that are not commonlyused in structure from motion. To validate, we use a Franka Emika Robot with anRGB camera. We a) find that error in distance estimate is less than 5 mm at adistance of 15 cm, and b) show how relative position can be used to findobstacles under challenging scenarios. We combine accurate distance estimatesand obstacle information into a reactive robot behavior that is able to pick upobjects of unknown size, while impeded by unforeseen obstacles. Project page:https://oxidification.com/p/one-object-at-a-time/ .</description><author>Aravind Battaje, Oliver Brock</author><pubDate>Fri, 01 Sep 2023 15:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.00487v3</guid></item><item><title>Euler Characteristic Tools For Topological Data Analysis</title><link>http://arxiv.org/abs/2303.14040v2</link><description>In this article, we study Euler characteristic techniques in topological dataanalysis. Pointwise computing the Euler characteristic of a family ofsimplicial complexes built from data gives rise to the so-called Eulercharacteristic profile. We show that this simple descriptor achievestate-of-the-art performance in supervised tasks at a very low computationalcost. Inspired by signal analysis, we compute hybrid transforms of Eulercharacteristic profiles. These integral transforms mix Euler characteristictechniques with Lebesgue integration to provide highly efficient compressors oftopological signals. As a consequence, they show remarkable performances inunsupervised settings. On the qualitative side, we provide numerous heuristicson the topological and geometric information captured by Euler profiles andtheir hybrid transforms. Finally, we prove stability results for thesedescriptors as well as asymptotic guarantees in random settings.</description><author>Olympio Hacquard, Vadim Lebovici</author><pubDate>Fri, 01 Sep 2023 15:00:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14040v2</guid></item><item><title>Self-Supervised Representation Learning with Cross-Context Learning between Global and Hypercolumn Features</title><link>http://arxiv.org/abs/2308.13392v2</link><description>Whilst contrastive learning yields powerful representations by matchingdifferent augmented views of the same instance, it lacks the ability to capturethe similarities between different instances. One popular way to address thislimitation is by learning global features (after the global pooling) to captureinter-instance relationships based on knowledge distillation, where the globalfeatures of the teacher are used to guide the learning of the global featuresof the student. Inspired by cross-modality learning, we extend this existingframework that only learns from global features by encouraging the globalfeatures and intermediate layer features to learn from each other. This leadsto our novel self-supervised framework: cross-context learning between globaland hypercolumn features (CGH), that enforces the consistency of instancerelations between low- and high-level semantics. Specifically, we stack theintermediate feature maps to construct a hypercolumn representation so that wecan measure instance relations using two contexts (hypercolumn and globalfeature) separately, and then use the relations of one context to guide thelearning of the other. This cross-context learning allows the model to learnfrom the differences between the two contexts. The experimental results onlinear classification and downstream tasks show that our method outperforms thestate-of-the-art methods.</description><author>Zheng Gao, Chen Feng, Ioannis Patras</author><pubDate>Fri, 01 Sep 2023 14:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13392v2</guid></item><item><title>New metrics for analyzing continual learners</title><link>http://arxiv.org/abs/2309.00462v1</link><description>Deep neural networks have shown remarkable performance when trained onindependent and identically distributed data from a fixed set of classes.However, in real-world scenarios, it can be desirable to train models on acontinuous stream of data where multiple classification tasks are presentedsequentially. This scenario, known as Continual Learning (CL) poses challengesto standard learning algorithms which struggle to maintain knowledge of oldtasks while learning new ones. This stability-plasticity dilemma remainscentral to CL and multiple metrics have been proposed to adequately measurestability and plasticity separately. However, none considers the increasingdifficulty of the classification task, which inherently results in performanceloss for any model. In that sense, we analyze some limitations of currentmetrics and identify the presence of setup-induced forgetting. Therefore, wepropose new metrics that account for the task's increasing difficulty. Throughexperiments on benchmark datasets, we demonstrate that our proposed metrics canprovide new insights into the stability-plasticity trade-off achieved by modelsin the continual learning environment.</description><author>Nicolas Michel, Giovanni Chierchia, Romain Negrel, Jean-François Bercher, Toshihiko Yamasaki</author><pubDate>Fri, 01 Sep 2023 14:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00462v1</guid></item><item><title>dacl10k: Benchmark for Semantic Bridge Damage Segmentation</title><link>http://arxiv.org/abs/2309.00460v1</link><description>Reliably identifying reinforced concrete defects (RCDs)plays a crucial rolein assessing the structural integrity, traffic safety, and long-term durabilityof concrete bridges, which represent the most common bridge type worldwide.Nevertheless, available datasets for the recognition of RCDs are small in termsof size and class variety, which questions their usability in real-worldscenarios and their role as a benchmark. Our contribution to this problem is"dacl10k", an exceptionally diverse RCD dataset for multi-label semanticsegmentation comprising 9,920 images deriving from real-world bridgeinspections. dacl10k distinguishes 12 damage classes as well as 6 bridgecomponents that play a key role in the building assessment and recommendingactions, such as restoration works, traffic load limitations or bridgeclosures. In addition, we examine baseline models for dacl10k which aresubsequently evaluated. The best model achieves a mean intersection-over-unionof 0.42 on the test set. dacl10k, along with our baselines, will be openlyaccessible to researchers and practitioners, representing the currently biggestdataset regarding number of images and class diversity for semanticsegmentation in the bridge inspection domain.</description><author>Johannes Flotzinger, Philipp J. Rösch, Thomas Braml</author><pubDate>Fri, 01 Sep 2023 14:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00460v1</guid></item><item><title>Constructing Holistic Measures for Social Biases in Masked Language Models</title><link>http://arxiv.org/abs/2305.07795v2</link><description>Masked Language Models (MLMs) have been successful in many natural languageprocessing tasks. However, real-world stereotype biases are likely to bereflected in MLMs due to their learning from large text corpora. Most of theevaluation metrics proposed in the past adopt different masking strategies,designed with the log-likelihood of MLMs. They lack holistic considerationssuch as variance for stereotype bias and anti-stereotype bias samples. In thispaper, the log-likelihoods of stereotype bias and anti-stereotype bias samplesoutput by MLMs are considered Gaussian distributions. Two evaluation metrics,Kullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score(JSDivS) are proposed to evaluate social biases in MLMs The experimentalresults on the public datasets StereoSet and CrowS-Pairs demonstrate thatKLDivS and JSDivS are more stable and interpretable compared to the metricsproposed in the past.</description><author>Yang Liu, Yuexian Hou</author><pubDate>Fri, 01 Sep 2023 14:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07795v2</guid></item><item><title>Unsupervised bias discovery in medical image segmentation</title><link>http://arxiv.org/abs/2309.00451v1</link><description>It has recently been shown that deep learning models for anatomicalsegmentation in medical images can exhibit biases against certainsub-populations defined in terms of protected attributes like sex or ethnicity.In this context, auditing fairness of deep segmentation models becomes crucial.However, such audit process generally requires access to ground-truthsegmentation masks for the target population, which may not always beavailable, especially when going from development to deployment. Here wepropose a new method to anticipate model biases in biomedical imagesegmentation in the absence of ground-truth annotations. Our unsupervised biasdiscovery method leverages the reverse classification accuracy framework toestimate segmentation quality. Through numerical experiments in synthetic andrealistic scenarios we show how our method is able to successfully anticipatefairness issues in the absence of ground-truth labels, constituting a novel andvaluable tool in this field.</description><author>Nicolás Gaggion, Rodrigo Echeveste, Lucas Mansilla, Diego H. Milone, Enzo Ferrante</author><pubDate>Fri, 01 Sep 2023 14:29:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00451v1</guid></item><item><title>Towards solving model bias in cosmic shear forward modeling</title><link>http://arxiv.org/abs/2210.16243v2</link><description>As the volume and quality of modern galaxy surveys increase, so does thedifficulty of measuring the cosmological signal imprinted in galaxy shapes.Weak gravitational lensing sourced by the most massive structures in theUniverse generates a slight shearing of galaxy morphologies called cosmicshear, key probe for cosmological models. Modern techniques of shear estimationbased on statistics of ellipticity measurements suffer from the fact that theellipticity is not a well-defined quantity for arbitrary galaxy light profiles,biasing the shear estimation. We show that a hybrid physical and deep learningHierarchical Bayesian Model, where a generative model captures the galaxymorphology, enables us to recover an unbiased estimate of the shear onrealistic galaxies, thus solving the model bias.</description><author>Benjamin Remy, Francois Lanusse, Jean-Luc Starck</author><pubDate>Fri, 01 Sep 2023 14:25:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16243v2</guid></item><item><title>STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos</title><link>http://arxiv.org/abs/2003.08429v4</link><description>Existing methods for instance segmentation in videos typically involvemulti-stage pipelines that follow the tracking-by-detection paradigm and modela video clip as a sequence of images. Multiple networks are used to detectobjects in individual frames, and then associate these detections over time.Hence, these methods are often non-end-to-end trainable and highly tailored tospecific tasks. In this paper, we propose a different approach that iswell-suited to a variety of tasks involving instance segmentation in videos. Inparticular, we model a video clip as a single 3D spatio-temporal volume, andpropose a novel approach that segments and tracks instances across space andtime in a single stage. Our problem formulation is centered around the idea ofspatio-temporal embeddings which are trained to cluster pixels belonging to aspecific object instance over an entire video clip. To this end, we introduce(i) novel mixing functions that enhance the feature representation ofspatio-temporal embeddings, and (ii) a single-stage, proposal-free network thatcan reason about temporal context. Our network is trained end-to-end to learnspatio-temporal embeddings as well as parameters required to cluster theseembeddings, thus simplifying inference. Our method achieves state-of-the-artresults across multiple datasets and tasks. Code and models are available athttps://github.com/sabarim/STEm-Seg.</description><author>Ali Athar, Sabarinath Mahadevan, Aljoša Ošep, Laura Leal-Taixé, Bastian Leibe</author><pubDate>Fri, 01 Sep 2023 14:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.08429v4</guid></item><item><title>Multi-granulariy Time-based Transformer for Knowledge Tracing</title><link>http://arxiv.org/abs/2304.05257v2</link><description>In this paper, we present a transformer architecture for predicting studentperformance on standardized tests. Specifically, we leverage studentshistorical data, including their past test scores, study habits, and otherrelevant information, to create a personalized model for each student. We thenuse these models to predict their future performance on a given test. Applyingthis model to the RIIID dataset, we demonstrate that using multiplegranularities for temporal features as the decoder input significantly improvemodel performance. Our results also show the effectiveness of our approach,with substantial improvements over the LightGBM method. Our work contributes tothe growing field of AI in education, providing a scalable and accurate toolfor predicting student outcomes.</description><author>Tong Zhou</author><pubDate>Fri, 01 Sep 2023 14:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05257v2</guid></item><item><title>Improving the matching of deformable objects by learning to detect keypoints</title><link>http://arxiv.org/abs/2309.00434v1</link><description>We propose a novel learned keypoint detection method to increase the numberof correct matches for the task of non-rigid image correspondence. Byleveraging true correspondences acquired by matching annotated image pairs witha specified descriptor extractor, we train an end-to-end convolutional neuralnetwork (CNN) to find keypoint locations that are more appropriate to theconsidered descriptor. For that, we apply geometric and photometric warpings toimages to generate a supervisory signal, allowing the optimization of thedetector. Experiments demonstrate that our method enhances the Mean MatchingAccuracy of numerous descriptors when used in conjunction with our detectionmethod, while outperforming the state-of-the-art keypoint detectors on realimages of non-rigid objects by 20 p.p. We also apply our method on the complexreal-world task of object retrieval where our detector performs on par with thefinest keypoint detectors currently available for this task. The source codeand trained models are publicly available athttps://github.com/verlab/LearningToDetect_PRL_2023</description><author>Felipe Cadar, Welerson, Vaishnavi Kanagasabapathi, Guilherme Potje, Renato Martins, Erickson R. Nascimento</author><pubDate>Fri, 01 Sep 2023 14:02:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00434v1</guid></item><item><title>CLIPAG: Towards Generator-Free Text-to-Image Generation</title><link>http://arxiv.org/abs/2306.16805v2</link><description>Perceptually Aligned Gradients (PAG) refer to an intriguing property observedin robust image classification models, wherein their input gradients align withhuman perception and pose semantic meanings. While this phenomenon has gainedsignificant research attention, it was solely studied in the context ofunimodal vision-only architectures. In this work, we extend the study of PAG toVision-Language architectures, which form the foundations for diverseimage-text tasks and applications. Through an adversarial robustificationfinetuning of CLIP, we demonstrate that robust Vision-Language models exhibitPAG in contrast to their vanilla counterparts. This work reveals the merits ofCLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, weshow that seamlessly integrating CLIPAG in a "plug-n-play" manner leads tosubstantial improvements in vision-language generative applications.Furthermore, leveraging its PAG property, CLIPAG enables text-to-imagegeneration without any generative model, which typically requires hugegenerators.</description><author>Roy Ganz, Michael Elad</author><pubDate>Fri, 01 Sep 2023 13:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16805v2</guid></item><item><title>Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction</title><link>http://arxiv.org/abs/2308.16259v2</link><description>Recently, the remarkable capabilities of large language models (LLMs) havebeen illustrated across a variety of research domains such as natural languageprocessing, computer vision, and molecular modeling. We extend this paradigm byutilizing LLMs for material property prediction by introducing our modelMaterials Informatics Transformer (MatInFormer). Specifically, we introduce anovel approach that involves learning the grammar of crystallography throughthe tokenization of pertinent space group information. We further illustratethe adaptability of MatInFormer by incorporating task-specific data pertainingto Metal-Organic Frameworks (MOFs). Through attention visualization, we uncoverthe key features that the model prioritizes during property prediction. Theeffectiveness of our proposed model is empirically validated across 14 distinctdatasets, hereby underscoring its potential for high throughput screeningthrough accurate material property prediction.</description><author>Hongshuo Huang, Rishikesh Magar, Changwen Xu, Amir Barati Farimani</author><pubDate>Fri, 01 Sep 2023 13:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16259v2</guid></item><item><title>A Locality-based Neural Solver for Optical Motion Capture</title><link>http://arxiv.org/abs/2309.00428v1</link><description>We present a novel locality-based learning method for cleaning and solvingoptical motion capture data. Given noisy marker data, we propose a newheterogeneous graph neural network which treats markers and joints as differenttypes of nodes, and uses graph convolution operations to extract the localfeatures of markers and joints and transform them to clean motions. To dealwith anomaly markers (e.g. occluded or with big tracking errors), the keyinsight is that a marker's motion shows strong correlations with the motions ofits immediate neighboring markers but less so with other markers, a.k.a.locality, which enables us to efficiently fill missing markers (e.g. due toocclusion). Additionally, we also identify marker outliers due to trackingerrors by investigating their acceleration profiles. Finally, we propose atraining regime based on representation learning and data augmentation, bytraining the model on data with masking. The masking schemes aim to mimic theoccluded and noisy markers often observed in the real data. Finally, we showthat our method achieves high accuracy on multiple metrics across variousdatasets. Extensive comparison shows our method outperforms state-of-the-artmethods in terms of prediction accuracy of occluded marker position error byapproximately 20%, which leads to a further error reduction on thereconstructed joint rotations and positions by 30%. The code and data for thispaper are available at https://github.com/non-void/LocalMoCap.</description><author>Xiaoyu Pan, Bowen Zheng, Xinwei Jiang, Guanglong Xu, Xianli Gu, Jingxiang Li, Qilong Kou, He Wang, Tianjia Shao, Kun Zhou, Xiaogang Jin</author><pubDate>Fri, 01 Sep 2023 13:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00428v1</guid></item><item><title>Sparse resultant based minimal solvers in computer vision and their connection with the action matrix</title><link>http://arxiv.org/abs/2301.06443v2</link><description>Many computer vision applications require robust and efficient estimation ofcamera geometry from a minimal number of input data measurements, i.e., solvingminimal problems in a RANSAC framework. Minimal problems are usually formulatedas complex systems of sparse polynomials. The systems usually areoverdetermined and consist of polynomials with algebraically constrainedcoefficients. Most state-of-the-art efficient polynomial solvers are based onthe action matrix method that has been automated and highly optimized in recentyears. On the other hand, the alternative theory of sparse resultants andNewton polytopes has been less successful for generating efficient solvers,primarily because the polytopes do not respect the constraints on thecoefficients. Therefore, in this paper, we propose a simple iterative scheme totest various subsets of the Newton polytopes and search for the most efficientsolver. Moreover, we propose to use an extra polynomial with a special form tofurther improve the solver efficiency via a Schur complement computation. Weshow that for some camera geometry problems our extra polynomial-based methodleads to smaller and more stable solvers than the state-of-the-art Grobnerbasis-based solvers. The proposed method can be fully automated andincorporated into existing tools for automatic generation of efficientpolynomial solvers. It provides a competitive alternative to popular Grobnerbasis-based methods for minimal problems in computer vision. We also study theconditions under which the minimal solvers generated by the state-of-the-artaction matrix-based methods and the proposed extra polynomial resultant-basedmethod, are equivalent. Specifically we consider a step-by-step comparisonbetween the approaches based on the action matrix and the sparse resultant,followed by a set of substitutions, which would lead to equivalent minimalsolvers.</description><author>Snehal Bhayani, Janne Heikkilä, Zuzana Kukelova</author><pubDate>Fri, 01 Sep 2023 13:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06443v2</guid></item><item><title>CPSP: Learning Speech Concepts From Phoneme Supervision</title><link>http://arxiv.org/abs/2309.00424v1</link><description>For fine-grained generation and recognition tasks such asminimally-supervised text-to-speech (TTS), voice conversion (VC), and automaticspeech recognition (ASR), the intermediate representation extracted from speechshould contain information that is between text coding and acoustic coding. Thelinguistic content is salient, while the paralinguistic information such asspeaker identity and acoustic details should be removed. However, existingmethods for extracting fine-grained intermediate representations from speechsuffer from issues of excessive redundancy and dimension explosion.Additionally, existing contrastive learning methods in the audio field focus onextracting global descriptive information for downstream audio classificationtasks, making them unsuitable for TTS, VC, and ASR tasks. To address theseissues, we propose a method named Contrastive Phoneme-Speech Pretraining(CPSP), which uses three encoders, one decoder, and contrastive learning tobring phoneme and speech into a joint multimodal space, learning how to connectphoneme and speech at the frame level. The CPSP model is trained on 210k speechand phoneme text pairs, achieving minimally-supervised TTS, VC, and ASR. Theproposed CPSP method offers a promising solution for fine-grained generationand recognition downstream tasks in speech processing. We provide a websitewith audio samples.</description><author>Chunyu Qiang, Hao Li, Yixin Tian, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</author><pubDate>Fri, 01 Sep 2023 13:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00424v1</guid></item><item><title>Declarative Reasoning on Explanations Using Constraint Logic Programming</title><link>http://arxiv.org/abs/2309.00422v1</link><description>Explaining opaque Machine Learning (ML) models is an increasingly relevantproblem. Current explanation in AI (XAI) methods suffer several shortcomings,among others an insufficient incorporation of background knowledge, and a lackof abstraction and interactivity with the user. We propose REASONX, anexplanation method based on Constraint Logic Programming (CLP). REASONX canprovide declarative, interactive explanations for decision trees, which can bethe ML models under analysis or global/local surrogate models of any black-boxmodel. Users can express background or common sense knowledge using linearconstraints and MILP optimization over features of factual and contrastiveinstances, and interact with the answer constraints at different levels ofabstraction through constraint projection. We present here the architecture ofREASONX, which consists of a Python layer, closer to the user, and a CLP layer.REASONX's core execution engine is a Prolog meta-program with declarativesemantics in terms of logic theories.</description><author>Laura State, Salvatore Ruggieri, Franco Turini</author><pubDate>Fri, 01 Sep 2023 13:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00422v1</guid></item><item><title>Area-norm COBRA on Conditional Survival Prediction</title><link>http://arxiv.org/abs/2309.00417v1</link><description>The paper explores a different variation of combined regression strategy tocalculate the conditional survival function. We use regression based weaklearners to create the proposed ensemble technique. The proposed combinedregression strategy uses proximity measure as area between two survival curves.The proposed model shows a construction which ensures that it performs betterthan the Random Survival Forest. The paper discusses a novel technique toselect the most important variable in the combined regression setup. We performa simulation study to show that our proposition for finding relevance of thevariables works quite well. We also use three real-life datasets to illustratethe model.</description><author>Rahul Goswami, Arabin Kr. Dey</author><pubDate>Fri, 01 Sep 2023 13:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00417v1</guid></item><item><title>Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond</title><link>http://arxiv.org/abs/2309.00416v1</link><description>Federated learning (FL) is a framework for training machine learning modelsin a distributed and collaborative manner. During training, a set ofparticipating clients process their data stored locally, sharing only the modelupdates obtained by minimizing a cost function over their local inputs. FL wasproposed as a stepping-stone towards privacy-preserving machine learning, butit has been shown vulnerable to issues such as leakage of private information,lack of personalization of the model, and the possibility of having a trainedmodel that is fairer to some groups than to others. In this paper, we addressthe triadic interaction among personalization, privacy guarantees, and fairnessattained by models trained within the FL framework. Differential privacy andits variants have been studied and applied as cutting-edge standards forproviding formal privacy guarantees. However, clients in FL often hold verydiverse datasets representing heterogeneous communities, making it important toprotect their sensitive information while still ensuring that the trained modelupholds the aspect of fairness for the users. To attain this objective, amethod is put forth that introduces group privacy assurances through theutilization of $d$-privacy (aka metric privacy). $d$-privacy represents alocalized form of differential privacy that relies on a metric-orientedobfuscation approach to maintain the original data's topological distribution.This method, besides enabling personalized model training in a federatedapproach and providing formal privacy guarantees, possesses significantlybetter group fairness measured under a variety of standard metrics than aglobal model trained within a classical FL template. Theoretical justificationsfor the applicability are provided, as well as experimental validation onreal-world datasets to illustrate the working of the proposed method.</description><author>Filippo Galli, Kangsoo Jung, Sayan Biswas, Catuscia Palamidessi, Tommaso Cucinotta</author><pubDate>Fri, 01 Sep 2023 13:20:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00416v1</guid></item><item><title>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding</title><link>http://arxiv.org/abs/2307.15484v2</link><description>Recently, there has been a growing interest in text-to-speech (TTS) methodsthat can be trained with minimal supervision by combining two types of discretespeech representations and using two sequence-to-sequence tasks to decoupleTTS. However, existing methods suffer from three problems: the highdimensionality and waveform distortion of discrete speech representations, theprosodic averaging problem caused by the duration prediction model innon-autoregressive frameworks, and the information redundancy and dimensionexplosion problems of existing semantic encoding methods. To address theseproblems, three progressive methods are proposed. First, we proposeDiff-LM-Speech, an autoregressive structure consisting of a language model anddiffusion models, which models the semantic embedding into the mel-spectrogrambased on a diffusion model to achieve higher audio quality. We also introduce aprompt encoder structure based on a variational autoencoder and a prosodybottleneck to improve prompt representation ability. Second, we proposeTetra-Diff-Speech, a non-autoregressive structure consisting of four diffusionmodel-based modules that design a duration diffusion model to achieve diverseprosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressivestructure consisting of three diffusion model-based modules that verify thenon-necessity of existing semantic encoding models and achieve the bestresults. Experimental results show that our proposed methods outperformbaseline methods. We provide a website with audio samples.</description><author>Chunyu Qiang, Hao Li, Hao Ni, He Qu, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</author><pubDate>Fri, 01 Sep 2023 13:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15484v2</guid></item><item><title>Selective Scene Text Removal</title><link>http://arxiv.org/abs/2309.00410v1</link><description>Scene text removal (STR) is the image transformation task to remove textregions in scene images. The conventional STR methods remove all scene text.This means that the existing methods cannot select text to be removed. In thispaper, we propose a novel task setting named selective scene text removal(SSTR) that removes only target words specified by the user. Although SSTR is amore complex task than STR, the proposed multi-module structure enablesefficient training for SSTR. Experimental results show that the proposed methodcan remove target words as expected.</description><author>Hayato Mitani, Akisato Kimura, Seiichi Uchida</author><pubDate>Fri, 01 Sep 2023 13:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00410v1</guid></item><item><title>Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning</title><link>http://arxiv.org/abs/2308.14705v2</link><description>Ensembling a neural network is a widely recognized approach to enhance modelperformance, estimate uncertainty, and improve robustness in deep supervisedlearning. However, deep ensembles often come with high computational costs andmemory demands. In addition, the efficiency of a deep ensemble is related todiversity among the ensemble members which is challenging for large,over-parameterized deep neural networks. Moreover, ensemble learning has notyet seen such widespread adoption, and it remains a challenging endeavor forself-supervised or unsupervised representation learning. Motivated by thesechallenges, we present a novel self-supervised training regime that leveragesan ensemble of independent sub-networks, complemented by a new loss functiondesigned to encourage diversity. Our method efficiently builds a sub-modelensemble with high diversity, leading to well-calibrated estimates of modeluncertainty, all achieved with minimal computational overhead compared totraditional deep self-supervised ensembles. To evaluate the effectiveness ofour approach, we conducted extensive experiments across various tasks,including in-distribution generalization, out-of-distribution detection,dataset corruption, and semi-supervised settings. The results demonstrate thatour method significantly improves prediction reliability. Our approach not onlyachieves excellent accuracy but also enhances calibration, surpassing baselineperformance across a wide range of self-supervised architectures in computervision, natural language processing, and genomics data.</description><author>Amirhossein Vahidi, Lisa Wimmer, Hüseyin Anil Gündüz, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei</author><pubDate>Fri, 01 Sep 2023 12:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14705v2</guid></item><item><title>Communication-Efficient Distributed Deep Learning: A Comprehensive Survey</title><link>http://arxiv.org/abs/2003.06307v2</link><description>Distributed deep learning (DL) has become prevalent in recent years to reducetraining time by leveraging multiple computing devices (e.g., GPUs/TPUs) due tolarger models and datasets. However, system scalability is limited bycommunication becoming the performance bottleneck. Addressing thiscommunication issue has become a prominent research topic. In this paper, weprovide a comprehensive survey of the communication-efficient distributedtraining algorithms, focusing on both system-level and algorithmic-leveloptimizations. We first propose a taxonomy of data-parallel distributedtraining algorithms that incorporates four primary dimensions: communicationsynchronization, system architectures, compression techniques, and parallelismof communication and computing tasks. We then investigate state-of-the-artstudies that address problems in these four dimensions. We also compare theconvergence rates of different algorithms to understand their convergencespeed. Additionally, we conduct extensive experiments to empirically comparethe convergence performance of various mainstream distributed trainingalgorithms. Based on our system-level communication cost analysis, theoreticaland experimental convergence speed comparison, we provide readers with anunderstanding of which algorithms are more efficient under specific distributedenvironments. Our research also extrapolates potential directions for furtheroptimizations.</description><author>Zhenheng Tang, Shaohuai Shi, Wei Wang, Bo Li, Xiaowen Chu</author><pubDate>Fri, 01 Sep 2023 12:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.06307v2</guid></item><item><title>Fine-grained Recognition with Learnable Semantic Data Augmentation</title><link>http://arxiv.org/abs/2309.00399v1</link><description>Fine-grained image recognition is a longstanding computer vision challengethat focuses on differentiating objects belonging to multiple subordinatecategories within the same meta-category. Since images belonging to the samemeta-category usually share similar visual appearances, mining discriminativevisual cues is the key to distinguishing fine-grained categories. Althoughcommonly used image-level data augmentation techniques have achieved greatsuccess in generic image classification problems, they are rarely applied infine-grained scenarios, because their random editing-region behavior is proneto destroy the discriminative visual cues residing in the subtle regions. Inthis paper, we propose diversifying the training data at the feature-level toalleviate the discriminative region loss problem. Specifically, we producediversified augmented samples by translating image features along semanticallymeaningful directions. The semantic directions are estimated with a covarianceprediction network, which predicts a sample-wise covariance matrix to adapt tothe large intra-class variation inherent in fine-grained images. Furthermore,the covariance prediction network is jointly optimized with the classificationnetwork in a meta-learning manner to alleviate the degenerate solution problem.Experiments on four competitive fine-grained recognition benchmarks(CUB-200-2011, Stanford Cars, FGVC Aircrafts, NABirds) demonstrate that ourmethod significantly improves the generalization performance on several popularclassification networks (e.g., ResNets, DenseNets, EfficientNets, RegNets andViT). Combined with a recently proposed method, our semantic data augmentationapproach achieves state-of-the-art performance on the CUB-200-2011 dataset. Thesource code will be released.</description><author>Yifan Pu, Yizeng Han, Yulin Wang, Junlan Feng, Chao Deng, Gao Huang</author><pubDate>Fri, 01 Sep 2023 12:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00399v1</guid></item><item><title>VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation</title><link>http://arxiv.org/abs/2309.00398v1</link><description>In this paper, we present VideoGen, a text-to-video generation approach,which can generate a high-definition video with high frame fidelity and strongtemporal consistency using reference-guided latent diffusion. We leverage anoff-the-shelf text-to-image generation model, e.g., Stable Diffusion, togenerate an image with high content quality from the text prompt, as areference image to guide video generation. Then, we introduce an efficientcascaded latent diffusion module conditioned on both the reference image andthe text prompt, for generating latent video representations, followed by aflow-based temporal upsampling step to improve the temporal resolution.Finally, we map latent video representations into a high-definition videothrough an enhanced video decoder. During training, we use the first frame of aground-truth video as the reference image for training the cascaded latentdiffusion module. The main characterises of our approach include: the referenceimage generated by the text-to-image model improves the visual fidelity; usingit as the condition makes the diffusion model focus more on learning the videodynamics; and the video decoder is trained over unlabeled video data, thusbenefiting from high-quality easily-available videos. VideoGen sets a newstate-of-the-art in text-to-video generation in terms of both qualitative andquantitative evaluation.</description><author>Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, Jingdong Wang</author><pubDate>Fri, 01 Sep 2023 12:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00398v1</guid></item><item><title>Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data</title><link>http://arxiv.org/abs/2306.07881v2</link><description>We present Viewset Diffusion, a diffusion-based generator that outputs 3Dobjects while only using multi-view 2D data for supervision. We note that thereexists a one-to-one mapping between viewsets, i.e., collections of several 2Dviews of an object, and 3D models. Hence, we train a diffusion model togenerate viewsets, but design the neural network generator to reconstructinternally corresponding 3D models, thus generating those too. We fit adiffusion model to a large number of viewsets for a given category of objects.The resulting generator can be conditioned on zero, one or more input views.Conditioned on a single view, it performs 3D reconstruction accounting for theambiguity of the task and allowing to sample multiple solutions compatible withthe input. The model performs reconstruction efficiently, in a feed-forwardmanner, and is trained using only rendering losses using as few as three viewsper viewset. Project page: szymanowiczs.github.io/viewset-diffusion.</description><author>Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Fri, 01 Sep 2023 12:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07881v2</guid></item><item><title>Satisfiability Checking of Multi-Variable TPTL with Unilateral Intervals Is PSPACE-Complete</title><link>http://arxiv.org/abs/2309.00386v1</link><description>We investigate the decidability of the ${0,\infty}$ fragment of TimedPropositional Temporal Logic (TPTL). We show that the satisfiability checkingof TPTL$^{0,\infty}$ is PSPACE-complete. Moreover, even its 1-variable fragment(1-TPTL$^{0,\infty}$) is strictly more expressive than Metric Interval TemporalLogic (MITL) for which satisfiability checking is EXPSPACE complete. Hence, wehave a strictly more expressive logic with computationally easiersatisfiability checking. To the best of our knowledge, TPTL$^{0,\infty}$ is thefirst multi-variable fragment of TPTL for which satisfiability checking isdecidable without imposing any bounds/restrictions on the timed words (e.g.bounded variability, bounded time, etc.). The membership in PSPACE is obtainedby a reduction to the emptiness checking problem for a new "non-punctual"subclass of Alternating Timed Automata with multiple clocks called UnilateralVery Weak Alternating Timed Automata (VWATA$^{0,\infty}$) which we prove to bein PSPACE. We show this by constructing a simulation equivalentnon-deterministic timed automata whose number of clocks is polynomial in thesize of the given VWATA$^{0,\infty}$.</description><author>Shankara Narayanan Krishna, Khushraj Nanik Madnani, Rupak Majumdar, Paritosh K. Pandya</author><pubDate>Fri, 01 Sep 2023 11:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00386v1</guid></item><item><title>Topology-aware Tensor Decomposition for Meta-graph Learning</title><link>http://arxiv.org/abs/2101.01078v2</link><description>Heterogeneous graphs generally refers to graphs with different types of nodesand edges. A common approach for extracting useful information fromheterogeneous graphs is to use meta-graphs, which can be seen as a special kindof directed acyclic graph (DAG) with same node and edge types as theheterogeneous graph. However, how to design proper meta-graphs is challenging.Recently, there have been many works on learning suitable meta-graphs from aheterogeneous graph. Existing methods generally introduce continuous weightsfor edges that are independent of each other, which ignores the topologicalstucture of meta-graphs and can be ineffective. To address this issue, wepropose a new viewpoint from tensor on learning meta-graphs. Such a viewpointnot only helps interpret the limitation of existing works by CANDECOMP/PARAFAC(CP) decomposition, but also inspires us to propose a topology-aware tensordecomposition, called TENSUS, that reflects the structure of DAGs. The proposedtopology-aware tensor decomposition is easy to use and simple to implement, andit can be taken as a plug-in part to upgrade many existing works, includingnode classification and recommendation on heterogeneous graphs. Experimentalresults on different tasks demonstrate that the proposed method cansignificantly improve the state-of-the-arts for all these tasks.</description><author>Hansi Yang, Peiyu Zhang, Quanming Yao</author><pubDate>Fri, 01 Sep 2023 11:49:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2101.01078v2</guid></item><item><title>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</title><link>http://arxiv.org/abs/2309.00385v1</link><description>Event cameras are sensors inspired by biological systems that specialize incapturing changes in brightness. These emerging cameras offer many advantagesover conventional frame-based cameras, including high dynamic range, high framerates, and extremely low power consumption. Due to these advantages, eventcameras have increasingly been adapted in various fields, such as frameinterpolation, semantic segmentation, odometry, and SLAM. However, theirapplication in 3D reconstruction for VR applications is underexplored. Previousmethods in this field mainly focused on 3D reconstruction through depth mapestimation. Methods that produce dense 3D reconstruction generally requiremultiple cameras, while methods that utilize a single event camera can onlyproduce a semi-dense result. Other single-camera methods that can produce dense3D reconstruction rely on creating a pipeline that either incorporates theaforementioned methods or other existing Structure from Motion (SfM) orMulti-view Stereo (MVS) methods. In this paper, we propose a novel approach forsolving dense 3D reconstruction using only a single event camera. To the bestof our knowledge, our work is the first attempt in this regard. Our preliminaryresults demonstrate that the proposed method can produce visuallydistinguishable dense 3D reconstructions directly without requiring pipelineslike those used by existing methods. Additionally, we have created a syntheticdataset with $39,739$ object scans using an event camera simulator. Thisdataset will help accelerate other relevant research in this field.</description><author>Haodong Chen, Vera Chung, Li Tan, Xiaoming Chen</author><pubDate>Fri, 01 Sep 2023 11:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00385v1</guid></item><item><title>BatchPrompt: Accomplish more with less</title><link>http://arxiv.org/abs/2309.00384v1</link><description>Many LLMs are trained to perform zero-shot or few-shot inference usinginstruction-based prompts. Crafting prompts for these LLMs typically requiresthe user to provide a detailed task description, examples of context andcompletion, and single example of context for inference. This regular promptbaseline is referred to as SinglePrompt in this paper. However, for NLP taskswhere each data point for inference is not necessarily lengthy, the token countfor instructions and few-shot examples in the prompt may be considerably largerthan that of the data point, resulting in lower token-resource utilizationcompared with encoder-based models like fine-tuned BERT. This cost-efficiencyissue, affecting inference speed and compute budget, counteracts the manybenefits LLMs have to offer. This paper aims to alleviate the preceding problemby batching multiple data points into a single prompt, a prompting strategy werefer to as BatchPrompt. This strategy increases the density of data points,which in turn leads to improved token utilization. Applying BatchPromptnaively, however, is very challenging due to significant performancedegradation, as observed in our experiments. We also noticed varying inferenceoutcomes for the same data point appearing in different positions within aprompt. To address the quality issue while remain high token-resourceutilization, we introduce Batch Permutation and Ensembling for BatchPrompt, asimple way that recovers labeling quality through majority votes from datapoints placed in varying positions in a batch at the price of more token usage.To counterbalance the additional token usage caused by the voting process, wefurther propose Self-reflection-guided EArly Stopping, which can terminate thevoting process early for data points the LLM confidently handles.</description><author>Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham</author><pubDate>Fri, 01 Sep 2023 11:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00384v1</guid></item><item><title>Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds</title><link>http://arxiv.org/abs/2309.00380v1</link><description>Devising deep latent variable models for multi-modal data has been along-standing theme in machine learning research. Multi-modal VariationalAutoencoders (VAEs) have been a popular generative model class that learnslatent representations which jointly explain multiple modalities. Variousobjective functions for such models have been suggested, often motivated aslower bounds on the multi-modal data log-likelihood or frominformation-theoretic considerations. In order to encode latent variables fromdifferent modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts(MoE) aggregation schemes have been routinely used and shown to yield differenttrade-offs, for instance, regarding their generative quality or consistencyacross multiple modalities. In this work, we consider a variational bound thatcan tightly lower bound the data log-likelihood. We develop more flexibleaggregation schemes that generalise PoE or MoE approaches by combining encodedfeatures from different modalities based on permutation-invariant neuralnetworks. Our numerical experiments illustrate trade-offs for multi-modalvariational bounds and various aggregation schemes. We show that tightervariational bounds and more flexible aggregation models can become beneficialwhen one wants to approximate the true joint distribution over observedmodalities and latent variables in identifiable models.</description><author>Marcel Hirt, Domenico Campolo, Victoria Leong, Juan-Pablo Ortega</author><pubDate>Fri, 01 Sep 2023 11:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00380v1</guid></item><item><title>Anomaly detection with semi-supervised classification based on risk estimators</title><link>http://arxiv.org/abs/2309.00379v1</link><description>A significant limitation of one-class classification anomaly detectionmethods is their reliance on the assumption that unlabeled training data onlycontains normal instances. To overcome this impractical assumption, we proposetwo novel classification-based anomaly detection methods. Firstly, we introducea semi-supervised shallow anomaly detection method based on an unbiased riskestimator. Secondly, we present a semi-supervised deep anomaly detection methodutilizing a nonnegative (biased) risk estimator. We establish estimation errorbounds and excess risk bounds for both risk minimizers. Additionally, wepropose techniques to select appropriate regularization parameters that ensurethe nonnegativity of the empirical risk in the shallow model under specificloss functions. Our extensive experiments provide strong evidence of theeffectiveness of the risk-based anomaly detection methods.</description><author>Le Thi Khanh Hien, Sukanya Patra, Souhaib Ben Taieb</author><pubDate>Fri, 01 Sep 2023 11:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00379v1</guid></item><item><title>Long-Term Memorability On Advertisements</title><link>http://arxiv.org/abs/2309.00378v1</link><description>Marketers spend billions of dollars on advertisements but to what end? At thepurchase time, if customers cannot recognize a brand for which they saw an ad,the money spent on the ad is essentially wasted. Despite its importance inmarketing, until now, there has been no study on the memorability of ads in theML literature. Most studies have been conducted on short-term recall (&lt;5 mins)on specific content types like object and action videos. On the other hand, theadvertising industry only cares about long-term memorability (a few hours orlonger), and advertisements are almost always highly multimodal, depicting astory through its different modalities (text, images, and videos). With thismotivation, we conduct the first large scale memorability study consisting of1203 participants and 2205 ads covering 276 brands. Running statistical testsover different participant subpopulations and ad-types, we find manyinteresting insights into what makes an ad memorable - both content and humanfactors. For example, we find that brands which use commercials with fastmoving scenes are more memorable than those with slower scenes (p=8e-10) andthat people who use ad-blockers remember lower number of ads than those whodon't (p=5e-3). Further, with the motivation of simulating the memorability ofmarketing materials for a particular audience, ultimately helping create one,we present a novel model, Sharingan, trained to leverage real-world knowledgeof LLMs and visual knowledge of visual encoders to predict the memorability ofa content. We test our model on all the prominent memorability datasets inliterature (both images and videos) and achieve state of the art across all ofthem. We conduct extensive ablation studies across memory types, modality,brand, and architectural choices to find insights into what drives memory.</description><author>Harini S I, Somesh Singh, Yaman K Singla, Aanisha Bhattacharyya, Veeky Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy</author><pubDate>Fri, 01 Sep 2023 11:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00378v1</guid></item><item><title>A Comprehensive Empirical Evaluation on Online Continual Learning</title><link>http://arxiv.org/abs/2308.10328v2</link><description>Online continual learning aims to get closer to a live learning experience bylearning directly on a stream of data with temporally shifting distribution andby storing a minimum amount of data from that stream. In this empiricalevaluation, we evaluate various methods from the literature that tackle onlinecontinual learning. More specifically, we focus on the class-incrementalsetting in the context of image classification, where the learner must learnnew classes incrementally from a stream of data. We compare these methods onthe Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their averageaccuracy, forgetting, stability, and quality of the representations, toevaluate various aspects of the algorithm at the end but also during the wholetraining period. We find that most methods suffer from stability andunderfitting issues. However, the learned representations are comparable toi.i.d. training under the same computational budget. No clear winner emergesfrom the results and basic experience replay, when properly tuned andimplemented, is a very strong baseline. We release our modular and extensiblecodebase at https://github.com/AlbinSou/ocl_survey based on the avalancheframework to reproduce our results and encourage future research.</description><author>Albin Soutif--Cormerais, Antonio Carta, Andrea Cossu, Julio Hurtado, Hamed Hemati, Vincenzo Lomonaco, Joost Van de Weijer</author><pubDate>Fri, 01 Sep 2023 11:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10328v2</guid></item><item><title>Scenario-based model predictive control of water reservoir systems</title><link>http://arxiv.org/abs/2309.00373v1</link><description>The optimal operation of water reservoir systems is a challenging taskinvolving multiple conflicting objectives. The main source of complexity is thepresence of the water inflow, which acts as an exogenous, highly uncertaindisturbance on the system. When model predictive control (MPC) is employed, theoptimal water release is usually computed based on the (predicted) trajectoryof the inflow. This choice may jeopardize the closed-loop performance when theactual inflow differs from its forecast. In this work, we consider - for thefirst time - a stochastic MPC approach for water reservoirs, in which thecontrol is optimized based on a set of plausible future inflows directlygenerated from past data. Such a scenario-based MPC strategy allows thecontroller to be more cautious, counteracting droughty periods (e.g., the lakelevel going below the dry limit) while at the same time guaranteeing that theagricultural water demand is satisfied. The method's effectiveness is validatedthrough extensive Monte Carlo tests using actual inflow data from Lake Como,Italy.</description><author>Raffaele Giuseppe Cestari, Andrea Castelletti, Simone Formentin</author><pubDate>Fri, 01 Sep 2023 11:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00373v1</guid></item><item><title>On the Localization of Ultrasound Image Slices within Point Distribution Models</title><link>http://arxiv.org/abs/2309.00372v1</link><description>Thyroid disorders are most commonly diagnosed using high-resolutionUltrasound (US). Longitudinal nodule tracking is a pivotal diagnostic protocolfor monitoring changes in pathological thyroid morphology. This task, however,imposes a substantial cognitive load on clinicians due to the inherentchallenge of maintaining a mental 3D reconstruction of the organ. We thuspresent a framework for automated US image slice localization within a 3D shaperepresentation to ease how such sonographic diagnoses are carried out. Ourproposed method learns a common latent embedding space between US image patchesand the 3D surface of an individual's thyroid shape, or a statisticalaggregation in the form of a statistical shape model (SSM), via contrastivemetric learning. Using cross-modality registration and Procrustes analysis, weleverage features from our model to register US slices to a 3D meshrepresentation of the thyroid shape. We demonstrate that our multi-modalregistration framework can localize images on the 3D surface topology of apatient-specific organ and the mean shape of an SSM. Experimental resultsindicate slice positions can be predicted within an average of 1.2 mm of theground-truth slice location on the patient-specific 3D anatomy and 4.6 mm onthe SSM, exemplifying its usefulness for slice localization during sonographicacquisitions. Code is publically available:\href{https://github.com/vuenc/slice-to-shape}{https://github.com/vuenc/slice-to-shape}</description><author>Lennart Bastian, Vincent Bürgin, Ha Young Kim, Alexander Baumann, Benjamin Busam, Mahdi Saleh, Nassir Navab</author><pubDate>Fri, 01 Sep 2023 11:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00372v1</guid></item><item><title>GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields</title><link>http://arxiv.org/abs/2308.16891v2</link><description>It is a long-standing problem in robotics to develop agents capable ofexecuting diverse manipulation tasks from visual observations in unstructuredreal-world environments. To achieve this goal, the robot needs to have acomprehensive understanding of the 3D structure and semantics of the scene. Inthis work, we present $\textbf{GNFactor}$, a visual behavior cloning agent formulti-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$euralfeature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neuralfield (GNF) as a reconstruction module and a Perceiver Transformer as adecision-making module, leveraging a shared deep 3D voxel representation. Toincorporate semantics in 3D, the reconstruction module utilizes avision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distillrich semantic information into the deep 3D voxel. We evaluate GNFactor on 3real robot tasks and perform detailed ablations on 10 RLBench tasks with alimited number of demonstrations. We observe a substantial improvement ofGNFactor over current state-of-the-art methods in seen and unseen tasks,demonstrating the strong generalization ability of GNFactor. Our projectwebsite is https://yanjieze.com/GNFactor/ .</description><author>Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, Xiaolong Wang</author><pubDate>Fri, 01 Sep 2023 11:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16891v2</guid></item><item><title>LaserMix for Semi-Supervised LiDAR Semantic Segmentation</title><link>http://arxiv.org/abs/2207.00026v4</link><description>Densely annotating LiDAR point clouds is costly, which restrains thescalability of fully-supervised learning methods. In this work, we study theunderexplored semi-supervised learning (SSL) in LiDAR segmentation. Our coreidea is to leverage the strong spatial cues of LiDAR point clouds to betterexploit unlabeled data. We propose LaserMix to mix laser beams from differentLiDAR scans, and then encourage the model to make consistent and confidentpredictions before and after mixing. Our framework has three appealingproperties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g.,range view and voxel), and hence our SSL framework can be universally applied.2) Statistically grounded: We provide a detailed analysis to theoreticallyexplain the applicability of the proposed framework. 3) Effective:Comprehensive experimental analysis on popular LiDAR segmentation datasets(nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness andsuperiority. Notably, we achieve competitive results over fully-supervisedcounterparts with 2x to 5x fewer labels and improve the supervised-onlybaseline significantly by 10.8% on average. We hope this concise yethigh-performing framework could facilitate future research in semi-supervisedLiDAR segmentation. Code is publicly available.</description><author>Lingdong Kong, Jiawei Ren, Liang Pan, Ziwei Liu</author><pubDate>Fri, 01 Sep 2023 10:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00026v4</guid></item><item><title>Generalised Bayesian Inference for Discrete Intractable Likelihood</title><link>http://arxiv.org/abs/2206.08420v2</link><description>Discrete state spaces represent a major computational challenge tostatistical inference, since the computation of normalisation constantsrequires summation over large or possibly infinite sets, which can beimpractical. This paper addresses this computational challenge through thedevelopment of a novel generalised Bayesian inference procedure suitable fordiscrete intractable likelihood. Inspired by recent methodological advances forcontinuous data, the main idea is to update beliefs about model parametersusing a discrete Fisher divergence, in lieu of the problematic intractablelikelihood. The result is a generalised posterior that can be sampled fromusing standard computational tools, such as Markov chain Monte Carlo,circumventing the intractable normalising constant. The statistical propertiesof the generalised posterior are analysed, with sufficient conditions forposterior consistency and asymptotic normality established. In addition, anovel and general approach to calibration of generalised posteriors isproposed. Applications are presented on lattice models for discrete spatialdata and on multivariate models for count data, where in each case themethodology facilitates generalised Bayesian inference at low computationalcost.</description><author>Takuo Matsubara, Jeremias Knoblauch, François-Xavier Briol, Chris. J. Oates</author><pubDate>Fri, 01 Sep 2023 10:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08420v2</guid></item><item><title>When Do Discourse Markers Affect Computational Sentence Understanding?</title><link>http://arxiv.org/abs/2309.00368v1</link><description>The capabilities and use cases of automatic natural language processing (NLP)have grown significantly over the last few years. While much work has beendevoted to understanding how humans deal with discourse connectives, thisphenomenon is understudied in computational systems. Therefore, it is importantto put NLP models under the microscope and examine whether they can adequatelycomprehend, process, and reason within the complexity of natural language. Inthis chapter, we introduce the main mechanisms behind automatic sentenceprocessing systems step by step and then focus on evaluating discourseconnective processing. We assess nine popular systems in their ability tounderstand English discourse connectives and analyze how context and languageunderstanding tasks affect their connective comprehension. The results showthat NLP systems do not process all discourse connectives equally well and thatthe computational processing complexity of different connective kinds is notalways consistently in line with the presumed complexity order found in humanprocessing. In addition, while humans are more inclined to be influenced duringthe reading procedure but not necessarily in the final comprehensionperformance, discourse connectives have a significant impact on the finalaccuracy of NLP systems. The richer knowledge of connectives a system learns,the more negative effect inappropriate connectives have on it. This suggeststhat the correct explicitation of discourse connectives is important forcomputational natural language processing.</description><author>Ruiqi Li, Liesbeth Allein, Damien Sileo, Marie-Francine Moens</author><pubDate>Fri, 01 Sep 2023 10:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00368v1</guid></item><item><title>Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark</title><link>http://arxiv.org/abs/2309.00367v1</link><description>The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduceda set of graph learning tasks strongly dependent on long-range interactionbetween vertices. Empirical evidence suggests that on these tasks GraphTransformers significantly outperform Message Passing GNNs (MPGNNs). In thispaper, we carefully reevaluate multiple MPGNN baselines as well as the GraphTransformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorousempirical analysis, we demonstrate that the reported performance gap isoverestimated due to suboptimal hyperparameter choices. It is noteworthy thatacross multiple datasets the performance gap completely vanishes after basichyperparameter optimization. In addition, we discuss the impact of lackingfeature normalization for LRGB's vision datasets and highlight a spuriousimplementation of LRGB's link prediction metric. The principal aim of our paperis to establish a higher standard of empirical rigor within the graph machinelearning community.</description><author>Jan Tönshoff, Martin Ritzert, Eran Rosenbluth, Martin Grohe</author><pubDate>Fri, 01 Sep 2023 10:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00367v1</guid></item><item><title>FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning</title><link>http://arxiv.org/abs/2309.00363v1</link><description>LLMs have demonstrated great capabilities in various NLP tasks. Differententities can further improve the performance of those LLMs on their specificdownstream tasks by fine-tuning LLMs. When several entities have similarinterested tasks, but their data cannot be shared because of privacy concernsregulations, federated learning (FL) is a mainstream solution to leverage thedata of different entities. However, fine-tuning LLMs in federated learningsettings still lacks adequate support from existing FL frameworks because ithas to deal with optimizing the consumption of significant communication andcomputational resources, data preparation for different tasks, and distinctinformation protection demands. This paper first discusses these challenges offederated fine-tuning LLMs, and introduces our package FS-LLM as a maincontribution, which consists of the following components: (1) we build anend-to-end benchmarking pipeline, automizing the processes of datasetpreprocessing, federated fine-tuning execution, and performance evaluation onfederated LLM fine-tuning; (2) we provide comprehensive federatedparameter-efficient fine-tuning algorithm implementations and versatileprogramming interfaces for future extension in FL scenarios with lowcommunication and computation costs, even without accessing the full model; (3)we adopt several accelerating and resource-efficient operators for fine-tuningLLMs with limited resources and the flexible pluggable sub-routines forinterdisciplinary study. We conduct extensive experiments to validate theeffectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-artparameter-efficient fine-tuning algorithms in FL settings, which also yieldsvaluable insights into federated fine-tuning LLMs for the research community.To facilitate further research and adoption, we release FS-LLM athttps://github.com/alibaba/FederatedScope/tree/llm.</description><author>Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou</author><pubDate>Fri, 01 Sep 2023 10:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00363v1</guid></item><item><title>A Penalty-Based Method for Communication-Efficient Decentralized Bilevel Programming</title><link>http://arxiv.org/abs/2211.04088v3</link><description>Bilevel programming has recently received attention in the literature, due toits wide range of applications, including reinforcement learning andhyper-parameter optimization. However, it is widely assumed that the underlyingbilevel optimization problem is solved either by a single machine or in thecase of multiple machines connected in a star-shaped network, i.e., federatedlearning setting. The latter approach suffers from a high communication cost onthe central node (e.g., parameter server) and exhibits privacy vulnerabilities.Hence, it is of interest to develop methods that solve bilevel optimizationproblems in a communication-efficient decentralized manner. To that end, thispaper introduces a penalty function based decentralized algorithm withtheoretical guarantees for this class of optimization problems. Specifically, adistributed alternating gradient-type algorithm for solving consensus bilevelprogramming over a decentralized network is developed. A key feature of theproposed algorithm is to estimate the hyper-gradient of the penalty functionvia decentralized computation of matrix-vector products and few vectorcommunications, which is then integrated within an alternating algorithm toobtain finite-time convergence analysis under different convexity assumptions.Our theoretical result highlights improvements in the iteration complexity ofdecentralized bilevel optimization, all while making efficient use of vectorcommunication. Empirical results on both synthetic and real datasetsdemonstrate that the proposed method performs well in real-world settings.</description><author>Parvin Nazari, Ahmad Mousavi, Davoud Ataee Tarzanagh, George Michailidis</author><pubDate>Fri, 01 Sep 2023 10:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.04088v3</guid></item><item><title>Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior</title><link>http://arxiv.org/abs/2309.00359v1</link><description>Shannon, in his seminal paper introducing information theory, divided thecommunication into three levels: technical, semantic, and effectivenss. Whilethe technical level is concerned with accurate reconstruction of transmittedsymbols, the semantic and effectiveness levels deal with the inferred meaningand its effect on the receiver. Thanks to telecommunications, the first levelproblem has produced great advances like the internet. Large Language Models(LLMs) make some progress towards the second goal, but the third level stillremains largely untouched. The third problem deals with predicting andoptimizing communication for desired receiver behavior. LLMs, while showingwide generalization capabilities across a wide range of tasks, are unable tosolve for this. One reason for the underperformance could be a lack of"behavior tokens" in LLMs' training corpora. Behavior tokens define receiverbehavior over a communication, such as shares, likes, clicks, purchases,retweets, etc. While preprocessing data for LLM training, behavior tokens areoften removed from the corpora as noise. Therefore, in this paper, we make someinitial progress towards reintroducing behavior tokens in LLM training. Thetrained models, other than showing similar performance to LLMs on contentunderstanding tasks, show generalization capabilities on behavior simulation,content simulation, behavior understanding, and behavior domain adaptation.Using a wide range of tasks on two corpora, we show results on all thesecapabilities. We call these models Large Content and Behavior Models (LCBMs).Further, to spur more research on LCBMs, we release our new Content BehaviorCorpus (CBC), a repository containing communicator, message, and correspondingreceiver behavior.</description><author>Ashmit Khandelwal, Aditya Agrawal, Aanisha Bhattacharyya, Yaman K Singla, Somesh Singh, Uttaran Bhattacharya, Ishita Dasgupta, Stefano Petrangeli, Rajiv Ratn Shah, Changyou Chen, Balaji Krishnamurthy</author><pubDate>Fri, 01 Sep 2023 10:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00359v1</guid></item><item><title>Discrete Versus Continuous Algorithms in Dynamics of Affective Decision Making</title><link>http://arxiv.org/abs/2309.00357v1</link><description>The dynamics of affective decision making is considered for an intelligentnetwork composed of agents with different types of memory: long-term andshort-term memory. The consideration is based on probabilistic affectivedecision theory, which takes into account the rational utility of alternativesas well as the emotional alternative attractiveness. The objective of thispaper is the comparison of two multistep operational algorithms of theintelligent network: one based on discrete dynamics and the other on continuousdynamics. By means of numerical analysis, it is shown that, depending on thenetwork parameters, the characteristic probabilities for continuous anddiscrete operations can exhibit either close or drastically different behavior.Thus, depending on which algorithm is employed, either discrete or continuous,theoretical predictions can be rather different, which does not allow for auniquely defined description of practical problems. This finding is importantfor understanding which of the algorithms is more appropriate for the correctanalysis of decision-making tasks. A discussion is given, revealing that thediscrete operation seems to be more realistic for describing intelligentnetworks as well as affective artificial intelligence.</description><author>V. I. Yukalov, E. P. Yukalova</author><pubDate>Fri, 01 Sep 2023 10:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00357v1</guid></item><item><title>Explainable Active Learning for Preference Elicitation</title><link>http://arxiv.org/abs/2309.00356v1</link><description>Gaining insights into the preferences of new users and subsequentlypersonalizing recommendations necessitate managing user interactionsintelligently, namely, posing pertinent questions to elicit valuableinformation effectively. In this study, our focus is on a specific scenario ofthe cold-start problem, where the recommendation system lacks adequate userpresence or access to other users' data is restricted, obstructing employinguser profiling methods utilizing existing data in the system. We employ ActiveLearning (AL) to solve the addressed problem with the objective of maximizinginformation acquisition with minimal user effort. AL operates for selectinginformative data from a large unlabeled set to inquire an oracle to label themand eventually updating a machine learning (ML) model. We operate AL in anintegrated process of unsupervised, semi-supervised, and supervised ML withinan explanatory preference elicitation process. It harvests user feedback (givenfor the system's explanations on the presented items) over informative samplesto update an underlying ML model estimating user preferences. The designed userinteraction facilitates personalizing the system by incorporating user feedbackinto the ML model and also enhances user trust by refining the system'sexplanations on recommendations. We implement the proposed preferenceelicitation methodology for food recommendation. We conducted human experimentsto assess its efficacy in the short term and also experimented with several ALstrategies over synthetic user profiles that we created for two food datasets,aiming for long-term performance analysis. The experimental results demonstratethe efficiency of the proposed preference elicitation with limited user-labeleddata while also enhancing user trust through accurate explanations.</description><author>Furkan Cantürk, Reyhan Aydoğan</author><pubDate>Fri, 01 Sep 2023 10:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00356v1</guid></item></channel></rss>