<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 22 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Active Stereo Without Pattern Projector</title><link>http://arxiv.org/abs/2309.12315v1</link><description>This paper proposes a novel framework integrating the principles of activestereo in standard passive camera systems without a physical pattern projector.We virtually project a pattern over the left and right images according to thesparse measurements obtained from a depth sensor. Any such devices can beseamlessly plugged into our framework, allowing for the deployment of a virtualactive stereo setup in any possible environment, overcoming the limitation ofpattern projectors, such as limited working range or environmental conditions.Experiments on indoor/outdoor datasets, featuring both long and close-range,support the seamless effectiveness of our approach, boosting the accuracy ofboth stereo algorithms and deep networks.</description><author>Luca Bartolomei, Matteo Poggi, Fabio Tosi, Andrea Conti, Stefano Mattoccia</author><pubDate>Thu, 21 Sep 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12315v1</guid></item><item><title>TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance</title><link>http://arxiv.org/abs/2309.12314v1</link><description>In this paper, we propose a novel cross-modal distillation method, calledTinyCLIP, for large-scale language-image pre-trained models. The methodintroduces two core techniques: affinity mimicking and weight inheritance.Affinity mimicking explores the interaction between modalities duringdistillation, enabling student models to mimic teachers' behavior of learningcross-modal feature alignment in a visual-linguistic affinity space. Weightinheritance transmits the pre-trained weights from the teacher models to theirstudent counterparts to improve distillation efficiency. Moreover, we extendthe method into a multi-stage progressive distillation to mitigate the loss ofinformative weights during extreme compression. Comprehensive experimentsdemonstrate the efficacy of TinyCLIP, showing that it can reduce the size ofthe pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shotperformance. While aiming for comparable performance, distillation with weightinheritance can speed up the training by 1.4 - 7.8 $\times$ compared totraining from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M,achieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet,surpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9%parameters. Finally, we demonstrate the good transferability of TinyCLIP invarious downstream tasks. Code and models will be open-sourced athttps://aka.ms/tinyclip.</description><author>Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi, Chen, Xinggang Wang, Hongyang Chao, Han Hu</author><pubDate>Thu, 21 Sep 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12314v1</guid></item><item><title>ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals</title><link>http://arxiv.org/abs/2309.12312v1</link><description>We present ForceSight, a system for text-guided mobile manipulation thatpredicts visual-force goals using a deep neural network. Given a single RGBDimage combined with a text prompt, ForceSight determines a target end-effectorpose in the camera frame (kinematic goal) and the associated forces (forcegoal). Together, these two components form a visual-force goal. Prior work hasdemonstrated that deep models outputting human-interpretable kinematic goalscan enable dexterous manipulation by real robots. Forces are critical tomanipulation, yet have typically been relegated to lower-level execution inthese systems. When deployed on a mobile manipulator equipped with aneye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,drawer opening, and object handovers with an 81% success rate in unseenenvironments with object instances that differed significantly from thetraining data. In a separate experiment, relying exclusively on visual servoingand ignoring force goals dropped the success rate from 90% to 45%,demonstrating that force goals can significantly enhance performance. Theappendix, videos, code, and trained models are available athttps://force-sight.github.io/.</description><author>Jeremy A. Collins, Cody Houff, You Liang Tan, Charles C. Kemp</author><pubDate>Thu, 21 Sep 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12312v1</guid></item><item><title>LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent</title><link>http://arxiv.org/abs/2309.12311v1</link><description>3D visual grounding is a critical skill for household robots, enabling themto navigate, manipulate objects, and answer questions based on theirenvironment. While existing approaches often rely on extensive labeled data orexhibit limitations in handling complex language queries, we proposeLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM todecompose complex natural language queries into semantic constituents andemploys a visual grounding tool, such as OpenScene or LERF, to identify objectsin a 3D scene. The LLM then evaluates the spatial and commonsense relationsamong the proposed objects to make a final grounding decision. Our method doesnot require any labeled training data and can generalize to novel 3D scenes andarbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark anddemonstrate state-of-the-art zero-shot grounding accuracy. Our findingsindicate that LLMs significantly improve the grounding capability, especiallyfor complex language queries, making LLM-Grounder an effective approach for 3Dvision-language tasks in robotics. Videos and interactive demos can be found onthe project website https://chat-with-nerf.github.io/ .</description><author>Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F. Fouhey, Joyce Chai</author><pubDate>Thu, 21 Sep 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12311v1</guid></item><item><title>Rehearsal: Simulating Conflict to Teach Conflict Resolution</title><link>http://arxiv.org/abs/2309.12309v1</link><description>Interpersonal conflict is an uncomfortable but unavoidable fact of life.Navigating conflict successfully is a skill -- one that can be learned throughdeliberate practice -- but few have access to effective training or feedback.To expand this access, we introduce Rehearsal, a system that allows users torehearse conflicts with a believable simulated interlocutor, explorecounterfactual "what if?" scenarios to identify alternative conversationalpaths, and learn through feedback on how and when to apply specific conflictstrategies. Users can utilize Rehearsal to practice handling a variety ofpredefined conflict scenarios, from office disputes to relationship issues, orthey can choose to create their own. To enable Rehearsal, we develop IRPprompting, a method of conditioning output of a large language model on theinfluential Interest-Rights-Power (IRP) theory from conflict resolution.Rehearsal uses IRP to generate utterances grounded in conflict resolutiontheory, guiding users towards counterfactual conflict resolution strategiesthat help de-escalate difficult conversations. In a between-subjectsevaluation, 40 participants engaged in an actual conflict with a confederateafter training. Compared to a control group with lecture material covering thesame IRP theory, participants with simulated training from Rehearsalsignificantly improved their performance in the unaided conflict: they reducedtheir use of escalating competitive strategies by an average of 67%, whiledoubling their use of cooperative strategies. Overall, Rehearsal highlights thepotential effectiveness of language models as tools for learning and practicinginterpersonal skills.</description><author>Omar Shaikh, Valentino Chai, Michele J. Gelfand, Diyi Yang, Michael S. Bernstein</author><pubDate>Thu, 21 Sep 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12309v1</guid></item><item><title>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</title><link>http://arxiv.org/abs/2309.12307v1</link><description>We present LongLoRA, an efficient fine-tuning approach that extends thecontext sizes of pre-trained large language models (LLMs), with limitedcomputation cost. Typically, training LLMs with long context sizes iscomputationally expensive, requiring extensive training hours and GPUresources. For example, training on the context length of 8192 needs 16xcomputational costs in self-attention layers as that of 2048. In this paper, wespeed up the context extension of LLMs in two aspects. On the one hand,although dense global attention is needed during inference, fine-tuning themodel can be effectively and efficiently done by sparse local attention. Theproposed shift short attention effectively enables context extension, leadingto non-trivial computation saving with similar performance to fine-tuning withvanilla attention. Particularly, it can be implemented with only two lines ofcode in training, while being optional in inference. On the other hand, werevisit the parameter-efficient fine-tuning regime for context expansion.Notably, we find that LoRA for context extension works well under the premiseof trainable embedding and normalization. LongLoRA demonstrates strongempirical results on various tasks on LLaMA2 models from 7B/13B to 70B.LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on asingle 8x A100 machine. LongLoRA extends models' context while retaining theiroriginal architectures, and is compatible with most existing techniques, likeFlashAttention-2. In addition, to make LongLoRA practical, we collect adataset, LongQA, for supervised fine-tuning. It contains more than 3k longcontext question-answer pairs.</description><author>Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia</author><pubDate>Thu, 21 Sep 2023 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12307v1</guid></item><item><title>TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive Learning</title><link>http://arxiv.org/abs/2309.12306v1</link><description>The goal of this work is Active Speaker Detection (ASD), a task to determinewhether a person is speaking or not in a series of video frames. Previous workshave dealt with the task by exploring network architectures while learningeffective representations has been less explored. In this work, we proposeTalkNCE, a novel talk-aware contrastive loss. The loss is only applied to partof the full segments where a person on the screen is actually speaking. Thisencourages the model to learn effective representations through the naturalcorrespondence of speech and facial movements. Our loss can be jointlyoptimized with the existing objectives for training ASD models without the needfor additional supervision or training data. The experiments demonstrate thatour loss can be easily integrated into the existing ASD frameworks, improvingtheir performance. Our method achieves state-of-the-art performances onAVA-ActiveSpeaker and ASW datasets.</description><author>Chaeyoung Jung, Suyeon Lee, Kihyun Nam, Kyeongha Rho, You Jin Kim, Youngjoon Jang, Joon Son Chung</author><pubDate>Thu, 21 Sep 2023 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12306v1</guid></item><item><title>SlowFast Network for Continuous Sign Language Recognition</title><link>http://arxiv.org/abs/2309.12304v1</link><description>The objective of this work is the effective extraction of spatial and dynamicfeatures for Continuous Sign Language Recognition (CSLR). To accomplish this,we utilise a two-pathway SlowFast network, where each pathway operates atdistinct temporal resolutions to separately capture spatial (hand shapes,facial expressions) and dynamic (movements) information. In addition, weintroduce two distinct feature fusion methods, carefully designed for thecharacteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), whichfacilitates the transfer of dynamic semantics into spatial semantics and viceversa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic andspatial representations through auxiliary subnetworks, while avoiding the needfor extra inference time. As a result, our model further strengthens spatialand dynamic representations in parallel. We demonstrate that the proposedframework outperforms the current state-of-the-art performance on popular CSLRdatasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.</description><author>Junseok Ahn, Youngjoon Jang, Joon Son Chung</author><pubDate>Thu, 21 Sep 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12304v1</guid></item><item><title>PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation</title><link>http://arxiv.org/abs/2309.12303v1</link><description>Panoramic videos contain richer spatial information and have attractedtremendous amounts of attention due to their exceptional experience in somefields such as autonomous driving and virtual reality. However, existingdatasets for video segmentation only focus on conventional planar images. Toaddress the challenge, in this paper, we present a panoramic video dataset,PanoVOS. The dataset provides 150 videos with high video resolutions anddiverse motions. To quantify the domain gap between 2D planar videos andpanoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)models on PanoVOS. Through error analysis, we found that all of them fail totackle pixel-level content discontinues of panoramic videos. Thus, we present aPanoramic Space Consistency Transformer (PSCFormer), which can effectivelyutilize the semantic boundary information of the previous frame for pixel-levelmatching with the current frame. Extensive experiments demonstrate thatcompared with the previous SOTA models, our PSCFormer network exhibits a greatadvantage in terms of segmentation results under the panoramic setting. Ourdataset poses new challenges in panoramic VOS and we hope that our PanoVOS canadvance the development of panoramic segmentation/tracking.</description><author>Shilin Yan, Xiaohao Xu, Lingyi Hong, Wenchao Chen, Wenqiang Zhang, Wei Zhang</author><pubDate>Thu, 21 Sep 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12303v1</guid></item><item><title>Text-Guided Vector Graphics Customization</title><link>http://arxiv.org/abs/2309.12302v1</link><description>Vector graphics are widely used in digital art and valued by designers fortheir scalability and layer-wise topological properties. However, the creationand editing of vector graphics necessitate creativity and design expertise,leading to a time-consuming process. In this paper, we propose a novel pipelinethat generates high-quality customized vector graphics based on textual promptswhile preserving the properties and layer-wise information of a given exemplarSVG. Our method harnesses the capabilities of large pre-trained text-to-imagemodels. By fine-tuning the cross-attention layers of the model, we generatecustomized raster images guided by textual prompts. To initialize the SVG, weintroduce a semantic-based path alignment method that preserves and transformscrucial paths from the exemplar SVG. Additionally, we optimize path parametersusing both image-level and vector-level losses, ensuring smooth shapedeformation while aligning with the customized raster image. We extensivelyevaluate our method using multiple metrics from vector-level, image-level, andtext-level perspectives. The evaluation results demonstrate the effectivenessof our pipeline in generating diverse customizations of vector graphics withexceptional quality. The project page ishttps://intchous.github.io/SVGCustomization.</description><author>Peiying Zhang, Nanxuan Zhao, Jing Liao</author><pubDate>Thu, 21 Sep 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12302v1</guid></item><item><title>Environment-biased Feature Ranking for Novelty Detection Robustness</title><link>http://arxiv.org/abs/2309.12301v1</link><description>We tackle the problem of robust novelty detection, where we aim to detectnovelties in terms of semantic content while being invariant to changes inother, irrelevant factors. Specifically, we operate in a setup with multipleenvironments, where we determine the set of features that are associated morewith the environments, rather than to the content relevant for the task. Thus,we propose a method that starts with a pretrained embedding and a multi-envsetup and manages to rank the features based on their environment-focus. First,we compute a per-feature score based on the feature distribution variancebetween envs. Next, we show that by dropping the highly scored ones, we manageto remove spurious correlations and improve the overall performance by up to6%, both in covariance and sub-population shift cases, both for a real and asynthetic benchmark, that we introduce for this task.</description><author>Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu</author><pubDate>Thu, 21 Sep 2023 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12301v1</guid></item><item><title>See to Touch: Learning Tactile Dexterity through Visual Incentives</title><link>http://arxiv.org/abs/2309.12300v1</link><description>Equipping multi-fingered robots with tactile sensing is crucial for achievingthe precise, contact-rich, and dexterous manipulation that humans excel at.However, relying solely on tactile sensing fails to provide adequate cues forreasoning about objects' spatial configurations, limiting the ability tocorrect errors and adapt to changing situations. In this paper, we presentTactile Adaptation from Visual Incentives (TAVI), a new framework that enhancestactile-based dexterity by optimizing dexterous policies using vision-basedrewards. First, we use a contrastive-based objective to learn visualrepresentations. Next, we construct a reward function using these visualrepresentations through optimal-transport based matching on one humandemonstration. Finally, we use online reinforcement learning on our robot tooptimize tactile-based policies that maximize the visual reward. On sixchallenging tasks, such as peg pick-and-place, unstacking bowls, and flippingslender objects, TAVI achieves a success rate of 73% using our four-fingeredAllegro robot hand. The increase in performance is 108% higher than policiesusing tactile and vision-based rewards and 135% higher than policies withouttactile observational input. Robot videos are best viewed on our projectwebsite: https://see-to-touch.github.io/.</description><author>Irmak Guzey, Yinlong Dai, Ben Evans, Soumith Chintala, Lerrel Pinto</author><pubDate>Thu, 21 Sep 2023 18:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12300v1</guid></item><item><title>Learning to Drive Anywhere</title><link>http://arxiv.org/abs/2309.12295v1</link><description>Human drivers can seamlessly adapt their driving decisions acrossgeographical locations with diverse conditions and rules of the road, e.g.,left vs. right-hand traffic. In contrast, existing models for autonomousdriving have been thus far only deployed within restricted operational domains,i.e., without accounting for varying driving behaviors across locations ormodel scalability. In this work, we propose AnyD, a single geographically-awareconditional imitation learning (CIL) model that can efficiently learn fromheterogeneous and globally distributed data with dynamic environmental,traffic, and social characteristics. Our key insight is to introduce ahigh-capacity geo-location-based channel attention mechanism that effectivelyadapts to local nuances while also flexibly modeling similarities among regionsin a data-driven manner. By optimizing a contrastive imitation objective, ourproposed approach can efficiently scale across inherently imbalanced datadistributions and location-dependent events. We demonstrate the benefits of ourAnyD agent across multiple datasets, cities, and scalable deployment paradigms,i.e., centralized, semi-supervised, and distributed agent training.Specifically, AnyD outperforms CIL baselines by over 14% in open-loopevaluation and 30% in closed-loop testing on CARLA.</description><author>Ruizhao Zhu, Peng Huang, Eshed Ohn-Bar, Venkatesh Saligrama</author><pubDate>Thu, 21 Sep 2023 18:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12295v1</guid></item><item><title>Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models</title><link>http://arxiv.org/abs/2309.12294v1</link><description>Large language models (LLMs) have demonstrated impressive capabilities innatural language generation. However, their output quality can be inconsistent,posing challenges for generating natural language from logical forms (LFs).This task requires the generated outputs to embody the exact semantics of LFs,without missing any LF semantics or creating any hallucinations. In this work,we tackle this issue by proposing a novel generate-and-rerank approach. Ourapproach involves initially generating a set of candidate outputs by promptingan LLM and subsequently reranking them using a task-specific reranker model. Inaddition, we curate a manually collected dataset to evaluate the alignmentbetween different ranking metrics and human judgements. The chosen rankingmetrics are utilized to enhance the training and evaluation of the rerankermodel. By conducting extensive experiments on three diverse datasets, wedemonstrate that the candidates selected by our reranker outperform thoseselected by baseline methods in terms of semantic consistency and fluency, asmeasured by three comprehensive metrics. Our findings provide strong evidencefor the effectiveness of our approach in improving the quality of generatedoutputs.</description><author>Levon Haroutunian, Zhuang Li, Lucian Galescu, Philip Cohen, Raj Tumuluri, Gholamreza Haffari</author><pubDate>Thu, 21 Sep 2023 18:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12294v1</guid></item><item><title>The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"</title><link>http://arxiv.org/abs/2309.12288v1</link><description>We expose a surprising failure of generalization in auto-regressive largelanguage models (LLMs). If a model is trained on a sentence of the form "A isB", it will not automatically generalize to the reverse direction "B is A".This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholzwas the ninth Chancellor of Germany", it will not automatically be able toanswer the question, "Who was the ninth Chancellor of Germany?". Moreover, thelikelihood of the correct answer ("Olaf Scholz") will not be higher than for arandom name. Thus, models exhibit a basic failure of logical deduction and donot generalize a prevalent pattern in their training set (i.e. if "A is B''occurs, "B is A" is more likely to occur). We provide evidence for the ReversalCurse by finetuning GPT-3 and Llama-1 on fictitious statements such as "UriahHawthorne is the composer of 'Abyssal Melodies'" and showing that they fail tocorrectly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse isrobust across model sizes and model families and is not alleviated by dataaugmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions aboutreal-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary LeePfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctlyanswers questions like the former 79% of the time, compared to 33% for thelatter. This shows a failure of logical deduction that we hypothesize is causedby the Reversal Curse. Code is available athttps://github.com/lukasberglund/reversal_curse.</description><author>Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans</author><pubDate>Thu, 21 Sep 2023 18:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12288v1</guid></item><item><title>MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</title><link>http://arxiv.org/abs/2309.12284v1</link><description>Large language models (LLMs) have pushed the limits of natural languageunderstanding and exhibited excellent problem-solving ability. Despite thegreat success, most existing open-source LLMs (\eg, LLaMA-2) are still far awayfrom satisfactory for solving mathematical problem due to the complex reasoningprocedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tunedlanguage model that specializes in mathematical reasoning. Specifically, westart by bootstrapping mathematical questions by rewriting the question frommultiple perspectives without extra knowledge, which results in a new datasetcalled {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA.Experimental results on two popular benchmarks (\ie, GSM8K and MATH) formathematical reasoning demonstrate that MetaMath outperforms a suite ofopen-source LLMs by a significant margin. Our MetaMath-7B model achieves$66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art modelsof the same size by $11.5\%$ and $8.7\%$. Particularly, {MetaMath-70B} achievesan accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. Werelease the {MetaMathQA} dataset, the {MetaMath} models with different modelsizes and the training code for public use.</description><author>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu</author><pubDate>Thu, 21 Sep 2023 18:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12284v1</guid></item><item><title>Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis</title><link>http://arxiv.org/abs/2309.12283v1</link><description>Generating multi-instrument music from symbolic music representations is animportant task in Music Information Retrieval (MIR). A central but stilllargely unsolved problem in this context is musically and acoustically informedcontrol in the generation process. As the main contribution of this work, wepropose enhancing control of multi-instrument synthesis by conditioning agenerative model on a specific performance and recording environment, thusallowing for better guidance of timbre and style. Building on state-of-the-artdiffusion-based music generative models, we introduce performance conditioning- a simple tool indicating the generative model to synthesize music with styleand timbre of specific instruments taken from specific performances. Ourprototype is evaluated using uncurated performances with diverseinstrumentation and achieves state-of-the-art FAD realism scores while allowingnovel timbre and style control. Our project page, including samples anddemonstrations, is available at benadar293.github.io/midipm</description><author>Ben Maman, Johannes Zeitler, Meinard Müller, Amit H. Bermano</author><pubDate>Thu, 21 Sep 2023 18:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12283v1</guid></item><item><title>The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains</title><link>http://arxiv.org/abs/2309.12279v1</link><description>Initialization of neural network weights plays a pivotal role in determiningtheir performance. Feature Imitating Networks (FINs) offer a novel strategy byinitializing weights to approximate specific closed-form statistical features,setting a promising foundation for deep learning architectures. While theapplicability of FINs has been chiefly tested in biomedical domains, this studyextends its exploration into other time series datasets. Three differentexperiments are conducted in this study to test the applicability of imitatingTsallis entropy for performance enhancement: Bitcoin price prediction, speechemotion recognition, and chronic neck pain detection. For the Bitcoin priceprediction, models embedded with FINs reduced the root mean square error byaround 1000 compared to the baseline. In the speech emotion recognition task,the FIN-augmented model increased classification accuracy by over 3 percent.Lastly, in the CNP detection experiment, an improvement of about 7 percent wasobserved compared to established classifiers. These findings validate the broadutility and potency of FINs in diverse applications.</description><author>Reza Khanmohammadi, Tuka Alhanai, Mohammad M. Ghassemi</author><pubDate>Thu, 21 Sep 2023 18:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12279v1</guid></item><item><title>Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition</title><link>http://arxiv.org/abs/2309.12278v1</link><description>Large language models (LLMs) have demonstrated dominating performance in manyNLP tasks, especially on generative tasks. However, they often fall short insome information extraction tasks, particularly those requiring domain-specificknowledge, such as Biomedical Named Entity Recognition (NER). In this paper,inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NERstep-by-step: break down the NER task into entity span extraction and entitytype determination. Additionally, for entity type determination, we injectentity knowledge to address the problem that LLM's lack of domain knowledgewhen predicting entity category. Experimental results show a significantimprovement in our two-step BioNER approach compared to previous few-shot LLMbaseline. Additionally, the incorporation of external knowledge significantlyenhances entity category determination performance.</description><author>Junyi Bian, Jiaxuan Zheng, Yuyi Zhang, Shanfeng Zhu</author><pubDate>Thu, 21 Sep 2023 18:39:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12278v1</guid></item><item><title>LLMR: Real-time Prompting of Interactive Worlds using Large Language Models</title><link>http://arxiv.org/abs/2309.12276v1</link><description>We present Large Language Model for Mixed Reality (LLMR), a framework for thereal-time creation and modification of interactive Mixed Reality experiencesusing LLMs. LLMR leverages novel strategies to tackle difficult cases whereideal training data is scarce, or where the design goal requires the synthesisof internal dynamics, intuitive analysis, or advanced interactivity. Ourframework relies on text interaction and the Unity game engine. Byincorporating techniques for scene understanding, task planning,self-debugging, and memory management, LLMR outperforms the standard GPT-4 by4x in average error rate. We demonstrate LLMR's cross-platform interoperabilitywith several example worlds, and evaluate it on a variety of creation andmodification tasks to show that it can produce and edit diverse objects, tools,and scenes. Finally, we conducted a usability study (N=11) with a diverse setthat revealed participants had positive experiences with the system and woulduse it again.</description><author>Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier</author><pubDate>Thu, 21 Sep 2023 18:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12276v1</guid></item><item><title>Survey of Aspect-based Sentiment Analysis Datasets</title><link>http://arxiv.org/abs/2204.05232v5</link><description>Aspect-based sentiment analysis (ABSA) is a natural language processingproblem that requires analyzing user-generated reviews to determine: a) Thetarget entity being reviewed, b) The high-level aspect to which it belongs, andc) The sentiment expressed toward the targets and the aspects. Numerous yetscattered corpora for ABSA make it difficult for researchers to identifycorpora best suited for a specific ABSA subtask quickly. This study aims topresent a database of corpora that can be used to train and assess autonomousABSA systems. Additionally, we provide an overview of the major corpora forABSA and its subtasks and highlight several features that researchers shouldconsider when selecting a corpus. Finally, we discuss the advantages anddisadvantages of current collection approaches and make recommendations forfuture corpora creation. This survey examines 65 publicly available ABSAdatasets covering over 25 domains, including 45 English and 20 other languagesdatasets.</description><author>Siva Uday Sampreeth Chebolu, Franck Dernoncourt, Nedim Lipka, Thamar Solorio</author><pubDate>Thu, 21 Sep 2023 18:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05232v5</guid></item><item><title>Controllable Dynamic Appearance for Neural 3D Portraits</title><link>http://arxiv.org/abs/2309.11009v2</link><description>Recent advances in Neural Radiance Fields (NeRFs) have made it possible toreconstruct and reanimate dynamic portrait scenes with control over head-pose,facial expressions and viewing direction. However, training such models assumesphotometric consistency over the deformed region e.g. the face must be evenlylit as it deforms with changing head-pose and facial expression. Suchphotometric consistency across frames of a video is hard to maintain, even instudio environments, thus making the created reanimatable neural portraitsprone to artifacts during reanimation. In this work, we propose CoDyNeRF, asystem that enables the creation of fully controllable 3D portraits inreal-world capture conditions. CoDyNeRF learns to approximate illuminationdependent effects via a dynamic appearance model in the canonical space that isconditioned on predicted surface normals and the facial expressions andhead-pose deformations. The surface normals prediction is guided using 3DMMnormals that act as a coarse prior for the normals of the human head, wheredirect prediction of normals is hard due to rigid and non-rigid deformationsinduced by head-pose and facial expression changes. Using only asmartphone-captured short video of a subject for training, we demonstrate theeffectiveness of our method on free view synthesis of a portrait scene withexplicit head pose and expression controls, and realistic lighting effects. Theproject page can be found here:http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html</description><author>ShahRukh Athar, Zhixin Shu, Zexiang Xu, Fujun Luan, Sai Bi, Kalyan Sunkavalli, Dimitris Samaras</author><pubDate>Thu, 21 Sep 2023 18:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11009v2</guid></item><item><title>Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports</title><link>http://arxiv.org/abs/2309.12273v1</link><description>Rapid and accurate identification of Venous thromboembolism (VTE), a severecardiovascular condition including deep vein thrombosis (DVT) and pulmonaryembolism (PE), is important for effective treatment. Leveraging NaturalLanguage Processing (NLP) on radiology reports, automated methods have shownpromising advancements in identifying VTE events from retrospective datacohorts or aiding clinical experts in identifying VTE events from radiologyreports. However, effectively training Deep Learning (DL) and the NLP models ischallenging due to limited labeled medical text data, the complexity andheterogeneity of radiology reports, and data imbalance. This study proposesnovel method combinations of DL methods, along with data augmentation, adaptivepre-trained NLP model selection, and a clinical expert NLP rule-basedclassifier, to improve the accuracy of VTE identification in unstructured(free-text) radiology reports. Our experimental results demonstrate the model'sefficacy, achieving an impressive 97\% accuracy and 97\% F1 score in predictingDVT, and an outstanding 98.3\% accuracy and 98.4\% F1 score in predicting PE.These findings emphasize the model's robustness and its potential tosignificantly contribute to VTE research.</description><author>Jamie Deng, Yusen Wu, Hilary Hayssen, Brain Englum, Aman Kankaria, Minerva Mayorga-Carlin, Shalini Sahoo, John Sorkin, Brajesh Lal, Yelena Yesha, Phuong Nguyen</author><pubDate>Thu, 21 Sep 2023 18:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12273v1</guid></item><item><title>A Constructive Approach to Function Realization by Neural Stochastic Differential Equations</title><link>http://arxiv.org/abs/2307.00215v2</link><description>The problem of function approximation by neural dynamical systems hastypically been approached in a top-down manner: Any continuous function can beapproximated to an arbitrary accuracy by a sufficiently complex model with agiven architecture. This can lead to high-complexity controls which areimpractical in applications. In this paper, we take the opposite, constructiveapproach: We impose various structural restrictions on system dynamics andconsequently characterize the class of functions that can be realized by such asystem. The systems are implemented as a cascade interconnection of a neuralstochastic differential equation (Neural SDE), a deterministic dynamicalsystem, and a readout map. Both probabilistic and geometric (Lie-theoretic)methods are used to characterize the classes of functions realized by suchsystems.</description><author>Tanya Veeravalli, Maxim Raginsky</author><pubDate>Thu, 21 Sep 2023 18:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00215v2</guid></item><item><title>Domain-knowledge Inspired Pseudo Supervision (DIPS) for Unsupervised Image-to-Image Translation Models to Support Cross-Domain Classification</title><link>http://arxiv.org/abs/2303.10310v3</link><description>The ability to classify images is dependent on having access to large labeleddatasets and testing on data from the same domain that the model can train on.Classification becomes more challenging when dealing with new data from adifferent domain, where gathering and especially labeling a larger imagedataset for retraining a classification model requires a labor-intensive humaneffort. Cross-domain classification frameworks were developed to handle thisdata domain shift problem by utilizing unsupervised image-to-image translationmodels to translate an input image from the unlabeled domain to the labeleddomain. The problem with these unsupervised models lies in their unsupervisednature. For lack of annotations, it is not possible to use the traditionalsupervised metrics to evaluate these translation models to pick the best-savedcheckpoint model. This paper introduces a new method called Domain-knowledgeInspired Pseudo Supervision (DIPS) which utilizes domain-informed GaussianMixture Models to generate pseudo annotations to enable the use of traditionalsupervised metrics. This method was designed specifically to supportcross-domain classification applications contrary to other typically usedmetrics such as the FID which were designed to evaluate the model in terms ofthe quality of the generated image from a human-eye perspective. DIPS provesits effectiveness by outperforming various GAN evaluation metrics, includingFID, when selecting the optimal saved checkpoint model. It is also evaluatedagainst truly supervised metrics. Furthermore, DIPS showcases its robustnessand interpretability by demonstrating a strong correlation with trulysupervised metrics, highlighting its superiority over existing state-of-the-artalternatives. The code and data to replicate the results can be found on theofficial Github repository: https://github.com/Hindawi91/DIPS</description><author>Firas Al-Hindawi, Md Mahfuzur Rahman Siddiquee, Teresa Wu, Han Hu, Ying Sun</author><pubDate>Thu, 21 Sep 2023 18:25:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10310v3</guid></item><item><title>The Cambridge Law Corpus: A Corpus for Legal AI Research</title><link>http://arxiv.org/abs/2309.12269v1</link><description>We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research.It consists of over 250 000 court cases from the UK. Most cases are from the21st century, but the corpus includes cases as old as the 16th century. Thispaper presents the first release of the corpus, containing the raw text andmeta-data. Together with the corpus, we provide annotations on case outcomesfor 638 cases, done by legal experts. Using our annotated data, we have trainedand evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models toprovide benchmarks. We include an extensive legal and ethical discussion toaddress the potentially sensitive nature of this material. As a consequence,the corpus will only be released for research purposes under certainrestrictions.</description><author>Andreas Östling, Holli Sargeant, Huiyuan Xie, Ludwig Bull, Alexander Terenin, Leif Jonsson, Måns Magnusson, Felix Steffek</author><pubDate>Thu, 21 Sep 2023 18:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12269v1</guid></item><item><title>The Impact of Incumbent/Opposition Status and Ideological Similitude on Emotions in Political Manifestos</title><link>http://arxiv.org/abs/2305.08383v2</link><description>The study involved the analysis of emotion-associated language in the UKConservative and Labour party general election manifestos between 2000 to 2019.While previous research have shown a general correlation between ideologicalpositioning and overlap of public policies, there are still conflicting resultsin matters of sentiments in such manifestos. Using new data, we present howvalence level can be swayed by party status within government with incumbentparties presenting a higher frequency in positive emotion-associated wordswhile negative emotion-associated words are more prevalent in oppositionparties. We also demonstrate that parties with ideological similitude usepositive language prominently further adding to the literature on therelationship between sentiments and party status.</description><author>Takumi Nishi</author><pubDate>Thu, 21 Sep 2023 18:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08383v2</guid></item><item><title>Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications</title><link>http://arxiv.org/abs/2309.12267v1</link><description>Federated Learning (FL) has revolutionized how we train deep neural networksby enabling decentralized collaboration while safeguarding sensitive data andimproving model performance. However, FL faces two crucial challenges: thediverse nature of data held by individual clients and the vulnerability of theFL system to security breaches. This paper introduces an innovative solutionnamed Estimated Mean Aggregation (EMA) that not only addresses these challengesbut also provides a fundamental reference point as a $\mathsf{baseline}$ foradvanced aggregation techniques in FL systems. EMA's significance lies in itsdual role: enhancing model security by effectively handling malicious outliersthrough trimmed means and uncovering data heterogeneity to ensure that trainedmodels are adaptable across various client datasets. Through a wealth ofexperiments, EMA consistently demonstrates high accuracy and area under thecurve (AUC) compared to alternative methods, establishing itself as a robustbaseline for evaluating the effectiveness and security of FL aggregationmethods. EMA's contributions thus offer a crucial step forward in advancing theefficiency, security, and versatility of decentralized deep learning in thecontext of FL.</description><author>Yusen Wu, Jamie Deng, Hao Chen, Phuong Nguyen, Yelena Yesha</author><pubDate>Thu, 21 Sep 2023 18:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12267v1</guid></item><item><title>LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles</title><link>http://arxiv.org/abs/2308.10855v2</link><description>With the continuous evolution and refinement of LLMs, they are endowed withimpressive logical reasoning or vertical thinking capabilities. But can theythink out of the box? Do they possess proficient lateral thinking abilities?Following the setup of Lateral Thinking Puzzles, we propose a novel evaluationbenchmark, LatEval, which assesses the model's lateral thinking within aninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: thequality of questions posed by the model and the model's capability to integrateinformation for problem-solving. We find that nearly all LLMs struggle withemploying lateral thinking during interactions. For example, even the mostadvanced model, GPT-4, exhibits the advantage to some extent, yet stillmaintain a noticeable gap when compared to human. This evaluation benchmarkprovides LLMs with a highly challenging and distinctive task that is crucial toan effective AI assistant.</description><author>Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Hai-Tao Zheng</author><pubDate>Thu, 21 Sep 2023 18:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10855v2</guid></item><item><title>On the Relationship between Skill Neurons and Robustness in Prompt Tuning</title><link>http://arxiv.org/abs/2309.12263v1</link><description>Prompt Tuning is a popular parameter-efficient finetuning method forpre-trained large language models (PLMs). Recently, based on experiments withRoBERTa, it has been suggested that Prompt Tuning activates specific neurons inthe transformer's feed-forward networks, that are highly predictive andselective for the given task. In this paper, we study the robustness of PromptTuning in relation to these "skill neurons", using RoBERTa and T5. We show thatprompts tuned for a specific task are transferable to tasks of the same typebut are not very robust to adversarial data, with higher robustness for T5 thanRoBERTa. At the same time, we replicate the existence of skill neurons inRoBERTa and further show that skill neurons also seem to exist in T5.Interestingly, the skill neurons of T5 determined on non-adversarial data arealso among the most predictive neurons on the adversarial data, which is notthe case for RoBERTa. We conclude that higher adversarial robustness may berelated to a model's ability to activate the relevant skill neurons onadversarial data.</description><author>Leon Ackermann, Xenia Ohmer</author><pubDate>Thu, 21 Sep 2023 18:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12263v1</guid></item><item><title>Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance</title><link>http://arxiv.org/abs/2309.12259v1</link><description>Stochastic Gradient Descent (SGD), a widely used optimization algorithm indeep learning, is often limited to converging to local optima due to thenon-convex nature of the problem. Leveraging these local optima to improvemodel performance remains a challenging task. Given the inherent complexity ofneural networks, the simple arithmetic averaging of the obtained local optimamodels in undesirable results. This paper proposes a {\em soft merging} methodthat facilitates rapid merging of multiple models, simplifies the merging ofspecific parts of neural networks, and enhances robustness against maliciousmodels with extreme values. This is achieved by learning gate parametersthrough a surrogate of the $l_0$ norm using hard concrete distribution withoutmodifying the model weights of the given local optima models. This mergingprocess not only enhances the model performance by converging to a better localoptimum, but also minimizes computational costs, offering an efficient andexplicit learning process integrated with stochastic gradient descent. Thoroughexperiments underscore the effectiveness and superior performance of the mergedneural networks.</description><author>Hao Chen, Yusen Wu, Phuong Nguyen, Chao Liu, Yelena Yesha</author><pubDate>Thu, 21 Sep 2023 18:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12259v1</guid></item><item><title>Multi-agent Deep Covering Skill Discovery</title><link>http://arxiv.org/abs/2210.03269v3</link><description>The use of skills (a.k.a., options) can greatly accelerate exploration inreinforcement learning, especially when only sparse reward signals areavailable. While option discovery methods have been proposed for individualagents, in multi-agent reinforcement learning settings, discoveringcollaborative options that can coordinate the behavior of multiple agents andencourage them to visit the under-explored regions of their joint state spacehas not been considered. In this case, we propose Multi-agent Deep CoveringOption Discovery, which constructs the multi-agent options through minimizingthe expected cover time of the multiple agents' joint state space. Also, wepropose a novel framework to adopt the multi-agent options in the MARL process.In practice, a multi-agent task can usually be divided into some sub-tasks,each of which can be completed by a sub-group of the agents. Therefore, ouralgorithm framework first leverages an attention mechanism to findcollaborative agent sub-groups that would benefit most from coordinatedactions. Then, a hierarchical algorithm, namely HA-MSAC, is developed to learnthe multi-agent options for each sub-group to complete their sub-tasks first,and then to integrate them through a high-level policy as the solution of thewhole task. This hierarchical option construction allows our framework tostrike a balance between scalability and effective collaboration among theagents. The evaluation based on multi-agent collaborative tasks shows that theproposed algorithm can effectively capture the agent interactions with theattention mechanism, successfully identify multi-agent options, andsignificantly outperforms prior works using single-agent options or no options,in terms of both faster exploration and higher task rewards.</description><author>Jiayu Chen, Marina Haliem, Tian Lan, Vaneet Aggarwal</author><pubDate>Thu, 21 Sep 2023 18:01:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03269v3</guid></item><item><title>SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning</title><link>http://arxiv.org/abs/2309.12253v1</link><description>We introduce an extension to the CLRS algorithmic learning benchmark,prioritizing scalability and the utilization of sparse representations. Manyalgorithms in CLRS require global memory or information exchange, mirrored inits execution model, which constructs fully connected (not sparse) graphs basedon the underlying problem. Despite CLRS's aim of assessing how effectivelylearned algorithms can generalize to larger instances, the existing executionmodel becomes a significant constraint due to its demanding memory requirementsand runtime (hard to scale). However, many important algorithms do not demand afully connected graph; these algorithms, primarily distributed in nature, alignclosely with the message-passing paradigm employed by Graph Neural Networks.Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmarkspecifically with scalability and sparseness in mind. Our approach includesadapted algorithms from the original CLRS benchmark and introduces new problemsfrom distributed and randomized algorithms. Moreover, we perform a thoroughempirical evaluation of our benchmark. Code is publicly available athttps://github.com/jkminder/SALSA-CLRS.</description><author>Julian Minder, Florian Grötschla, Joël Mathys, Roger Wattenhofer</author><pubDate>Thu, 21 Sep 2023 17:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12253v1</guid></item><item><title>Instruction Tuning for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2308.10792v2</link><description>This paper surveys research works in the quickly advancing field ofinstruction tuning (IT), a crucial technique to enhance the capabilities andcontrollability of large language models (LLMs). Instruction tuning refers tothe process of further training LLMs on a dataset consisting of\textsc{(instruction, output)} pairs in a supervised fashion, which bridges thegap between the next-word prediction objective of LLMs and the users' objectiveof having LLMs adhere to human instructions. In this work, we make a systematicreview of the literature, including the general methodology of IT, theconstruction of IT datasets, the training of IT models, and applications todifferent modalities, domains and applications, along with an analysis onaspects that influence the outcome of IT (e.g., generation of instructionoutputs, size of the instruction dataset, etc). We also review the potentialpitfalls of IT along with criticism against it, along with efforts pointing outcurrent deficiencies of existing strategies and suggest some avenues forfruitful research.</description><author>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang</author><pubDate>Thu, 21 Sep 2023 17:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10792v2</guid></item><item><title>Parallelizing non-linear sequential models over the sequence length</title><link>http://arxiv.org/abs/2309.12252v1</link><description>Sequential models, such as Recurrent Neural Networks and Neural OrdinaryDifferential Equations, have long suffered from slow training due to theirinherent sequential nature. For many years this bottleneck has persisted, asmany thought sequential models could not be parallelized. We challenge thislong-held belief with our parallel algorithm that accelerates GPU evaluation ofsequential models by up to 3 orders of magnitude faster without compromisingoutput accuracy. The algorithm does not need any special structure in thesequential models' architecture, making it applicable to a wide range ofarchitectures. Using our method, training sequential models can be more than 10times faster than the common sequential method without any meaningfuldifference in the training results. Leveraging this accelerated training, wediscovered the efficacy of the Gated Recurrent Unit in a long time seriesclassification problem with 17k time samples. By overcoming the trainingbottleneck, our work serves as the first step to unlock the potential ofnon-linear sequential models for long sequence problems.</description><author>Yi Heng Lim, Qi Zhu, Joshua Selfridge, Muhammad Firmansyah Kasim</author><pubDate>Thu, 21 Sep 2023 17:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12252v1</guid></item><item><title>SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References</title><link>http://arxiv.org/abs/2309.12250v1</link><description>Evaluation of QA systems is very challenging and expensive, with the mostreliable approach being human annotations of correctness of answers forquestions. Recent works (AVA, BEM) have shown that transformer LM encoder basedsimilarity metrics transfer well for QA evaluation, but they are limited by theusage of a single correct reference answer. We propose a new evaluation metric:SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple referenceanswers (combining multiple correct and incorrect references) for sentence-formQA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) andgenerative (GenQA) QA systems, across multiple academic and industrialdatasets, and show that it outperforms previous baselines and obtains thehighest correlation with human annotations.</description><author>Matteo Gabburo, Siddhant Garg, Rik Koncel Kedziorski, Alessandro Moschitti</author><pubDate>Thu, 21 Sep 2023 17:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12250v1</guid></item><item><title>Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection</title><link>http://arxiv.org/abs/2309.12247v1</link><description>Detecting fake news requires both a delicate sense of diverse clues and aprofound understanding of the real-world background, which remains challengingfor detectors based on small language models (SLMs) due to their knowledge andcapability limitations. Recent advances in large language models (LLMs) haveshown remarkable performance in various tasks, but whether and how LLMs couldhelp with fake news detection remains underexplored. In this paper, weinvestigate the potential of LLMs in fake news detection. First, we conduct anempirical study and find that a sophisticated LLM such as GPT 3.5 couldgenerally expose fake news and provide desirable multi-perspective rationalesbut still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysisattributes such a gap to the LLM's inability to select and integrate rationalesproperly to conclude. Based on these findings, we propose that current LLMs maynot substitute fine-tuned SLMs in fake news detection but can be a good advisorfor SLMs by providing multi-perspective instructive rationales. To instantiatethis proposal, we design an adaptive rationale guidance network for fake newsdetection (ARG), in which SLMs selectively acquire insights on news analysisfrom the LLMs' rationales. We further derive a rationale-free version of ARG bydistillation, namely ARG-D, which services cost-sensitive scenarios withoutinquiring LLMs. Experiments on two real-world datasets demonstrate that ARG andARG-D outperform three types of baseline methods, including SLM-based,LLM-based, and combinations of small and large language models.</description><author>Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, Peng Qi</author><pubDate>Thu, 21 Sep 2023 17:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12247v1</guid></item><item><title>Fine-tuned Generative Adversarial Network-based Model for Medical Image Super-Resolution</title><link>http://arxiv.org/abs/2211.00577v7</link><description>In the field of medical image analysis, there is a substantial need forhigh-resolution (HR) images to improve diagnostic accuracy. However, It is achallenging task to obtain HR medical images, as it requires advancedinstruments and significant time. Deep learning-based super-resolution methodscan help to improve the resolution and perceptual quality of low-resolution(LR) medical images. Recently, Generative Adversarial Network (GAN) basedmethods have shown remarkable performance among deep learning-basedsuper-resolution methods. Real-Enhanced Super-Resolution Generative AdversarialNetwork (Real-ESRGAN) is a practical model for recovering HR images fromreal-world LR images. In our proposed approach, we use transfer learningtechnique and fine-tune the pre-trained Real-ESRGAN model using medical imagedatasets. This technique helps in improving the performance of the model. Thefocus of this paper is on enhancing the resolution and perceptual quality ofchest X-ray and retinal images. We use the Tuberculosis chest X-ray (Shenzhen)dataset and the STARE dataset of retinal images for fine-tuning the model. Theproposed model achieves superior perceptual quality compared to the Real-ESRGANmodel, effectively preserving fine details and generating images with morerealistic textures.</description><author>Alireza Aghelan, Modjtaba Rouhani</author><pubDate>Thu, 21 Sep 2023 17:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00577v7</guid></item><item><title>Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images</title><link>http://arxiv.org/abs/2309.12245v1</link><description>Biomedical image datasets can be imbalanced due to the rarity of targeteddiseases. Generative Adversarial Networks play a key role in addressing thisimbalance by enabling the generation of synthetic images to augment datasets.It is important to generate synthetic images that incorporate a diverse rangeof features to accurately represent the distribution of features present in thetraining imagery. Furthermore, the absence of diverse features in syntheticimages can degrade the performance of machine learning classifiers. The modecollapse problem impacts Generative Adversarial Networks' capacity to generatediversified images. Mode collapse comes in two varieties: intra-class andinter-class. In this paper, both varieties of the mode collapse problem areinvestigated, and their subsequent impact on the diversity of synthetic X-rayimages is evaluated. This work contributes an empirical demonstration of thebenefits of integrating the adaptive input-image normalization with the DeepConvolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapseproblems. Synthetically generated images are utilized for data augmentation andtraining a Vision Transformer model. The classification performance of themodel is evaluated using accuracy, recall, and precision scores. Resultsdemonstrate that the DCGAN and the ACGAN with adaptive input-imagenormalization outperform the DCGAN and ACGAN with un-normalized X-ray images asevidenced by the superior diversity scores and classification scores.</description><author>Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O'Reilly</author><pubDate>Thu, 21 Sep 2023 17:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12245v1</guid></item><item><title>ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events</title><link>http://arxiv.org/abs/2309.12244v1</link><description>Children typically learn to identify and express emotions through sharingtheir stories and feelings with others, particularly their family. However, itis challenging for parents or siblings to have emotional communication withchildren since children are still developing their communication skills. Wepresent ChaCha, a chatbot that encourages and guides children to share personalevents and associated emotions. ChaCha combines a state machine and largelanguage models (LLMs) to keep the dialogue on track while carrying onfree-form conversations. Through an exploratory study with 20 children (aged8-12), we examine how ChaCha prompts children to share personal events andguides them to describe associated emotions. Participants perceived ChaCha as aclose friend and shared their stories on various topics, such as family tripsand personal achievements. Based on the quantitative and qualitative findings,we discuss opportunities for leveraging LLMs to design child-friendly chatbotsto support children in sharing their emotions.</description><author>Woosuk Seo, Chanmo Yang, Young-Ho Kim</author><pubDate>Thu, 21 Sep 2023 17:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12244v1</guid></item><item><title>Weakly-supervised Automated Audio Captioning via text only training</title><link>http://arxiv.org/abs/2309.12242v1</link><description>In recent years, datasets of paired audio and captions have enabledremarkable success in automatically generating descriptions for audio clips,namely Automated Audio Captioning (AAC). However, it is labor-intensive andtime-consuming to collect a sufficient number of paired audio and captions.Motivated by the recent advances in Contrastive Language-Audio Pretraining(CLAP), we propose a weakly-supervised approach to train an AAC model assumingonly text data and a pre-trained CLAP model, alleviating the need for pairedtarget data. Our approach leverages the similarity between audio and textembeddings in CLAP. During training, we learn to reconstruct the text from theCLAP text embedding, and during inference, we decode using the audioembeddings. To mitigate the modality gap between the audio and text embeddingswe employ strategies to bridge the gap during training and inference stages. Weevaluate our proposed method on Clotho and AudioCaps datasets demonstrating itsability to achieve a relative performance of up to ~$83\%$ compared to fullysupervised approaches trained with paired target data.</description><author>Theodoros Kouzelis, Vassilis Katsouros</author><pubDate>Thu, 21 Sep 2023 17:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12242v1</guid></item><item><title>Learning Deformable 3D Graph Similarity to Track Plant Cells in Unregistered Time Lapse Images</title><link>http://arxiv.org/abs/2309.11157v2</link><description>Tracking of plant cells in images obtained by microscope is a challengingproblem due to biological phenomena such as large number of cells, non-uniformgrowth of different layers of the tightly packed plant cells and cell division.Moreover, images in deeper layers of the tissue being noisy and unavoidablesystemic errors inherent in the imaging process further complicates theproblem. In this paper, we propose a novel learning-based method that exploitsthe tightly packed three-dimensional cell structure of plant cells to create athree-dimensional graph in order to perform accurate cell tracking. We furtherpropose novel algorithms for cell division detection and effectivethree-dimensional registration, which improve upon the state-of-the-artalgorithms. We demonstrate the efficacy of our algorithm in terms of trackingaccuracy and inference-time on a benchmark dataset.</description><author>Md Shazid Islam, Arindam Dutta, Calvin-Khang Ta, Kevin Rodriguez, Christian Michael, Mark Alber, G. Venugopala Reddy, Amit K. Roy-Chowdhury</author><pubDate>Thu, 21 Sep 2023 17:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11157v2</guid></item><item><title>Model-based Clustering using Non-parametric Hidden Markov Models</title><link>http://arxiv.org/abs/2309.12238v1</link><description>Thanks to their dependency structure, non-parametric Hidden Markov Models(HMMs) are able to handle model-based clustering without specifying groupdistributions. The aim of this work is to study the Bayes risk of clusteringwhen using HMMs and to propose associated clustering procedures. We first givea result linking the Bayes risk of classification and the Bayes risk ofclustering, which we use to identify the key quantity determining thedifficulty of the clustering task. We also give a proof of this result in thei.i.d. framework, which might be of independent interest. Then we study theexcess risk of the plugin classifier. All these results are shown to remainvalid in the online setting where observations are clustered sequentially.Simulations illustrate our findings.</description><author>Elisabeth Gassiat, Ibrahim Kaddouri, Zacharie Naulet</author><pubDate>Thu, 21 Sep 2023 17:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12238v1</guid></item><item><title>t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators</title><link>http://arxiv.org/abs/2309.12237v1</link><description>Presentation attack (spoofing) detection (PAD) typically operates alongsidebiometric verification to improve reliablity in the face of spoofing attacks.Even though the two sub-systems operate in tandem to solve the single task ofreliable biometric verification, they address different detection tasks and arehence typically evaluated separately. Evidence shows that this approach issuboptimal. We introduce a new metric for the joint evaluation of PAD solutionsoperating in situ with biometric verification. In contrast to the tandemdetection cost function proposed recently, the new tandem equal error rate(t-EER) is parameter free. The combination of two classifiers nonetheless leadsto a \emph{set} of operating points at which false alarm and miss rates areequal and also dependent upon the prevalence of attacks. We therefore introducethe \emph{concurrent} t-EER, a unique operating point which is invariable tothe prevalence of attacks. Using both modality (and even application) agnosticsimulated scores, as well as real scores for a voice biometrics application, wedemonstrate application of the t-EER to a wide range of biometric systemevaluations under attack. The proposed approach is a strong candidate metricfor the tandem evaluation of PAD systems and biometric comparators.</description><author>Tomi Kinnunen, Kong Aik Lee, Hemlata Tak, Nicholas Evans, Andreas Nautsch</author><pubDate>Thu, 21 Sep 2023 17:30:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12237v1</guid></item><item><title>Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing</title><link>http://arxiv.org/abs/2309.12236v1</link><description>Calibration measures and reliability diagrams are two fundamental tools formeasuring and interpreting the calibration of probabilistic predictors.Calibration measures quantify the degree of miscalibration, and reliabilitydiagrams visualize the structure of this miscalibration. However, the mostcommon constructions of reliability diagrams and calibration measures --binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). Weshow that a simple modification fixes both constructions: first smooth theobservations using an RBF kernel, then compute the Expected Calibration Error(ECE) of this smoothed function. We prove that with a careful choice ofbandwidth, this method yields a calibration measure that is well-behaved in thesense of (B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistentcalibration measure. We call this measure the SmoothECE. Moreover, thereliability diagram obtained from this smoothed function visually encodes theSmoothECE, just as binned reliability diagrams encode the BinnedECE. We also provide a Python package with simple, hyperparameter-free methods formeasuring and plotting calibration: `pip install relplot\`.</description><author>Jarosław Błasiok, Preetum Nakkiran</author><pubDate>Thu, 21 Sep 2023 17:30:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12236v1</guid></item><item><title>Prodigy: An Expeditiously Adaptive Parameter-Free Learner</title><link>http://arxiv.org/abs/2306.06101v2</link><description>We consider the problem of estimating the learning rate in adaptive methods,such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, toprovably estimate the distance to the solution $D$, which is needed to set thelearning rate optimally. Our techniques are modifications of the D-Adaptationmethod for learning-rate-free learning. Our methods improve upon theconvergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where$d_0$ is the initial estimate of $D$. We test our methods on 12 commonlogistic-regression benchmark datasets, VGG11 and ResNet-50 training onCIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training onCriteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPTtransformer training on BookWiki. Our experimental results show that ourapproaches consistently outperform D-Adaptation and reach test accuracy valuesclose to that of hand-tuned Adam.</description><author>Konstantin Mishchenko, Aaron Defazio</author><pubDate>Thu, 21 Sep 2023 17:29:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06101v2</guid></item><item><title>Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition</title><link>http://arxiv.org/abs/2309.12234v1</link><description>In this study, we present synchronous bilingual Connectionist TemporalClassification (CTC), an innovative framework that leverages dual CTC to bridgethe gaps of both modality and language in the speech translation (ST) task.Utilizing transcript and translation as concurrent objectives for CTC, ourmodel bridges the gap between audio and text as well as between source andtarget languages. Building upon the recent advances in CTC application, wedevelop an enhanced variant, BiL-CTC+, that establishes new state-of-the-artperformances on the MuST-C ST benchmarks under resource-constrained scenarios.Intriguingly, our method also yields significant improvements in speechrecognition performance, revealing the effect of cross-lingual learning ontranscription and demonstrating its broad applicability. The source code isavailable at https://github.com/xuchennlp/S2T.</description><author>Chen Xu, Xiaoqian Liu, Erfeng He, Yuhao Zhang, Qianqian Dong, Tong Xiao, Jingbo Zhu, Dapeng Man, Wu Yang</author><pubDate>Thu, 21 Sep 2023 17:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12234v1</guid></item><item><title>Smooth Nash Equilibria: Algorithms and Complexity</title><link>http://arxiv.org/abs/2309.12226v1</link><description>A fundamental shortcoming of the concept of Nash equilibrium is itscomputational intractability: approximating Nash equilibria in normal-formgames is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis,we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nashequilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nashequilibrium, players only need to achieve utility at least as high as theirbest deviation to a $\sigma$-smooth strategy, which is a distribution that doesnot put too much mass (as parametrized by $\sigma$) on any fixed action. Wedistinguish two variants of $\sigma$-smooth Nash equilibria: strong$\sigma$-smooth Nash equilibria, in which players are required to play$\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smoothNash equilibria, where there is no such requirement. We show that both weak and strong $\sigma$-smooth Nash equilibria havesuperior computational properties to Nash equilibria: when $\sigma$ as well asan approximation parameter $\epsilon$ and the number of players are allconstants, there is a constant-time randomized algorithm to find a weak$\epsilon$-approximate $\sigma$-smooth Nash equilibrium in normal-form games.In the same parameter regime, there is a polynomial-time deterministicalgorithm to find a strong $\epsilon$-approximate $\sigma$-smooth Nashequilibrium in a normal-form game. These results stand in contrast to theoptimal algorithm for computing $\epsilon$-approximate Nash equilibria, whichcannot run in faster than quasipolynomial-time. We complement our upper boundsby showing that when either $\sigma$ or $\epsilon$ is an inverse polynomial,finding a weak $\epsilon$-approximate $\sigma$-smooth Nash equilibria becomescomputationally intractable.</description><author>Constantinos Daskalakis, Noah Golowich, Nika Haghtalab, Abhishek Shetty</author><pubDate>Thu, 21 Sep 2023 17:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12226v1</guid></item><item><title>Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches</title><link>http://arxiv.org/abs/2309.12224v1</link><description>The increase in the availability of online videos has transformed the way weaccess information and knowledge. A growing number of individuals now preferinstructional videos as they offer a series of step-by-step procedures toaccomplish particular tasks. The instructional videos from the medical domainmay provide the best possible visual answers to first aid, medical emergency,and medical education questions. Toward this, this paper is focused onanswering health-related questions asked by the public by providing visualanswers from medical videos. The scarcity of large-scale datasets in themedical domain is a key challenge that hinders the development of applicationsthat can help the public with their health-related questions. To address thisissue, we first proposed a pipelined approach to create two large-scaledatasets: HealthVidQA-CRF and HealthVidQA-Prompt. Later, we proposed monomodaland multimodal approaches that can effectively provide visual answers frommedical videos to natural language questions. We conducted a comprehensiveanalysis of the results, focusing on the impact of the created datasets onmodel training and the significance of visual features in enhancing theperformance of the monomodal and multi-modal approaches. Our findings suggestthat these datasets have the potential to enhance the performance of medicalvisual answer localization tasks and provide a promising future direction tofurther enhance the performance by using pre-trained language-vision models.</description><author>Deepak Gupta, Kush Attal, Dina Demner-Fushman</author><pubDate>Thu, 21 Sep 2023 17:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12224v1</guid></item><item><title>Regionally Additive Models: Explainable-by-design models minimizing feature interactions</title><link>http://arxiv.org/abs/2309.12215v1</link><description>Generalized Additive Models (GAMs) are widely used explainable-by-designmodels in various applications. GAMs assume that the output can be representedas a sum of univariate functions, referred to as components. However, thisassumption fails in ML problems where the output depends on multiple featuressimultaneously. In these cases, GAMs fail to capture the interaction terms ofthe underlying function, leading to subpar accuracy. To (partially) addressthis issue, we propose Regionally Additive Models (RAMs), a novel class ofexplainable-by-design models. RAMs identify subregions within the feature spacewhere interactions are minimized. Within these regions, it is more accurate toexpress the output as a sum of univariate functions (components). Consequently,RAMs fit one component per subregion of each feature instead of one componentper feature. This approach yields a more expressive model compared to GAMswhile retaining interpretability. The RAM framework consists of three steps.Firstly, we train a black-box model. Secondly, using Regional Effect Plots, weidentify subregions where the black-box model exhibits near-local additivity.Lastly, we fit a GAM component for each identified subregion. We validate theeffectiveness of RAMs through experiments on both synthetic and real-worlddatasets. The results confirm that RAMs offer improved expressiveness comparedto GAMs while maintaining interpretability.</description><author>Vasilis Gkolemis, Anargiros Tzerefos, Theodore Dalamagas, Eirini Ntoutsi, Christos Diou</author><pubDate>Thu, 21 Sep 2023 17:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12215v1</guid></item><item><title>Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?</title><link>http://arxiv.org/abs/2309.12214v1</link><description>Photovoltaic (PV) energy is crucial for the decarbonization of energysystems. Due to the lack of centralized data, remote sensing of rooftop PVinstallations is the best option to monitor the evolution of the rooftop PVinstalled fleet at a regional scale. However, current techniques lackreliability and are notably sensitive to shifts in the acquisition conditions.To overcome this, we leverage the wavelet scale attribution method (WCAM),which decomposes a model's prediction in the space-scale domain. The WCAMenables us to assess on which scales the representation of a PV model rests andprovides insights to derive methods that improve the robustness to acquisitionconditions, thus increasing trust in deep learning systems to encourage theiruse for the safe integration of clean energy in electric systems.</description><author>Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint-Drenan, Philippe Blanc</author><pubDate>Thu, 21 Sep 2023 17:15:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12214v1</guid></item><item><title>SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices</title><link>http://arxiv.org/abs/2309.12212v1</link><description>Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic withextremely high energy efficiency. By employing the distinct polarity of currentto denote logic `0' and `1', AQFP devices serve as excellent carriers forbinary neural network (BNN) computations. Although recent research has madeinitial strides toward developing an AQFP-based BNN accelerator, severalcritical challenges remain, preventing the design from being a comprehensivesolution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNNacceleration framework that leverages software-hardware co-optimization toeventually make the AQFP devices a feasible solution for BNN acceleration.Specifically, we investigate the randomized behavior of the AQFP devices andanalyze the impact of crossbar size on current attenuation, subsequentlyformulating the current amplitude into the values suitable for use in BNNcomputation. To tackle the accumulation problem and improve overall hardwareperformance, we propose a stochastic computing-based accumulation module and aclocking scheme adjustment-based circuit optimization method. We validate ourSupeRBNN framework across various datasets and network architectures, comparingit with implementations based on different technologies, including CMOS, ReRAM,and superconducting RSFQ/ERSFQ. Experimental results demonstrate that ourdesign achieves an energy efficiency of approximately 7.8x10^4 times higherthan that of the ReRAM-based BNN framework while maintaining a similar level ofmodel accuracy. Furthermore, when compared with superconductor-basedcounterparts, our framework demonstrates at least two orders of magnitudehigher energy efficiency.</description><author>Zhengang Li, Geng Yuan, Tomoharu Yamauchi, Zabihi Masoud, Yanyue Xie, Peiyan Dong, Xulong Tang, Nobuyuki Yoshikawa, Devesh Tiwari, Yanzhi Wang, Olivia Chen</author><pubDate>Thu, 21 Sep 2023 17:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12212v1</guid></item><item><title>Physics-informed State-space Neural Networks for Transport Phenomena</title><link>http://arxiv.org/abs/2309.12211v1</link><description>This work introduces Physics-informed State-space neural network Models(PSMs), a novel solution to achieving real-time optimization, flexibility, andfault tolerance in autonomous systems, particularly in transport-dominatedsystems such as chemical, biomedical, and power plants. Traditional data-drivenmethods fall short due to a lack of physical constraints like massconservation; PSMs address this issue by training deep neural networks withsensor data and physics-informing using components' Partial DifferentialEquations (PDEs), resulting in a physics-constrained, end-to-end differentiableforward dynamics model. Through two in silico experiments - a heated channeland a cooling system loop - we demonstrate that PSMs offer a more accurateapproach than purely data-driven models. Beyond accuracy, there are several compelling use cases for PSMs. In thiswork, we showcase two: the creation of a nonlinear supervisory controllerthrough a sequentially updated state-space representation and the proposal of adiagnostic algorithm using residuals from each of the PDEs. The formerdemonstrates the ability of PSMs to handle both constant and time-dependentconstraints, while the latter illustrates their value in system diagnostics andfault detection. We further posit that PSMs could serve as a foundation forDigital Twins, constantly updated digital representations of physical systems.</description><author>Akshay J Dave, Richard B. Vilim</author><pubDate>Thu, 21 Sep 2023 17:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12211v1</guid></item><item><title>Boolformer: Symbolic Regression of Logic Functions with Transformers</title><link>http://arxiv.org/abs/2309.12207v1</link><description>In this work, we introduce Boolformer, the first Transformer architecturetrained to perform end-to-end symbolic regression of Boolean functions. First,we show that it can predict compact formulas for complex functions which werenot seen during training, when provided a clean truth table. Then, wedemonstrate its ability to find approximate expressions when providedincomplete and noisy observations. We evaluate the Boolformer on a broad set ofreal-world binary classification datasets, demonstrating its potential as aninterpretable alternative to classic machine learning methods. Finally, weapply it to the widespread task of modelling the dynamics of gene regulatorynetworks. Using a recent benchmark, we show that Boolformer is competitive withstate-of-the art genetic algorithms with a speedup of several orders ofmagnitude. Our code and models are available publicly.</description><author>Stéphane d'Ascoli, Samy Bengio, Josh Susskind, Emmanuel Abbé</author><pubDate>Thu, 21 Sep 2023 17:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12207v1</guid></item><item><title>Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting</title><link>http://arxiv.org/abs/2307.15299v3</link><description>Accurate load forecasting plays a vital role in numerous sectors, butaccurately capturing the complex dynamics of dynamic power systems remains achallenge for traditional statistical models. For these reasons, time-seriesmodels (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonlydeployed and often experience higher success. In this paper, we analyze theefficacy of the recently developed Transformer-based Neural Network model inLoad forecasting. Transformer models have the potential to improve Loadforecasting because of their ability to learn long-range dependencies derivedfrom their Attention Mechanism. We apply several metaheuristics namelyDifferential Evolution to find the optimal hyperparameters of theTransformer-based Neural Network to produce accurate forecasts. DifferentialEvolution provides scalable, robust, global solutions to non-differentiable,multi-objective, or constrained optimization problems. Our work compares theproposed Transformer based Neural Network model integrated with differentmetaheuristic algorithms by their performance in Load forecasting based onnumerical metrics such as Mean Squared Error (MSE) and Mean Absolute PercentageError (MAPE). Our findings demonstrate the potential of metaheuristic-enhancedTransformer-based Neural Network models in Load forecasting accuracy andprovide optimal hyperparameters for each model.</description><author>Anuvab Sen, Arul Rhik Mazumder, Udayon Sen</author><pubDate>Thu, 21 Sep 2023 17:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15299v3</guid></item><item><title>Brain Tumor Detection Using Deep Learning Approaches</title><link>http://arxiv.org/abs/2309.12193v1</link><description>Brain tumors are collections of abnormal cells that can develop into massesor clusters. Because they have the potential to infiltrate other tissues, theypose a risk to the patient. The main imaging technique used, MRI, may be ableto identify a brain tumor with accuracy. The fast development of Deep Learningmethods for use in computer vision applications has been facilitated by a vastamount of training data and improvements in model construction that offerbetter approximations in a supervised setting. The need for these approacheshas been the main driver of this expansion. Deep learning methods have shownpromise in improving the precision of brain tumor detection and classificationusing magnetic resonance imaging (MRI). The study on the use of deep learningtechniques, especially ResNet50, for brain tumor identification is presented inthis abstract. As a result, this study investigates the possibility ofautomating the detection procedure using deep learning techniques. In thisstudy, I utilized five transfer learning models which are VGG16, VGG19,DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the best or highestaccuracy 99.54%. The goal of the study is to guide researchers and medicalprofessionals toward powerful brain tumor detecting systems by employing deeplearning approaches by way of this evaluation and analysis.</description><author>Razia Sultana Misu</author><pubDate>Thu, 21 Sep 2023 16:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12193v1</guid></item><item><title>SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs</title><link>http://arxiv.org/abs/2309.12188v1</link><description>Object rearrangement is pivotal in robotic-environment interactions,representing a significant capability in embodied AI. In this paper, we presentSG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine schemewith a scene graph as the scene representation. Unlike previous methods thatrely on either known goal priors or zero-shot large models, SG-Bot exemplifieslightweight, real-time, and user-controllable characteristics, seamlesslyblending the consideration of commonsense knowledge with automatic generationcapabilities. SG-Bot employs a three-fold procedure--observation, imagination,and execution--to adeptly address the task. Initially, objects are discernedand extracted from a cluttered scene during the observation. These objects arefirst coarsely organized and depicted within a scene graph, guided by eithercommonsense or user-defined criteria. Then, this scene graph subsequentlyinforms a generative model, which forms a fine-grained goal scene consideringthe shape information from the initial scene and object semantics. Finally, forexecution, the initial and envisioned goal scenes are matched to formulaterobotic action policies. Experimental results demonstrate that SG-Botoutperforms competitors by a large margin.</description><author>Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam</author><pubDate>Thu, 21 Sep 2023 16:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12188v1</guid></item><item><title>ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding</title><link>http://arxiv.org/abs/2309.12183v1</link><description>In 3D human shape and pose estimation from a monocular video, models trainedwith limited labeled data cannot generalize well to videos with occlusion,which is common in the wild videos. The recent human neural renderingapproaches focusing on novel view synthesis initialized by the off-the-shelfhuman shape and pose methods have the potential to correct the initial humanshape. However, the existing methods have some drawbacks such as, erroneous inhandling occlusion, sensitive to inaccurate human segmentation, and ineffectiveloss computation due to the non-regularized opacity field. To address theseproblems, we introduce ORTexME, an occlusion-robust temporal method thatutilizes temporal information from the input video to better regularize theoccluded body parts. While our ORTexME is based on NeRF, to determine thereliable regions for the NeRF ray sampling, we utilize our novel averagetexture learning approach to learn the average appearance of a person, and toinfer a mask based on the average texture. In addition, to guide theopacity-field updates in NeRF to suppress blur and noise, we propose the use ofhuman body mesh. The quantitative evaluation demonstrates that our methodachieves significant improvement on the challenging multi-person 3DPW dataset,where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-basedmethods fail and enlarge the error up to 5.6 on the same dataset.</description><author>Yu Cheng, Bo Wang, Robby T. Tan</author><pubDate>Thu, 21 Sep 2023 16:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12183v1</guid></item><item><title>Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations</title><link>http://arxiv.org/abs/2309.12179v1</link><description>Gloss-free Sign Language Production (SLP) offers a direct translation ofspoken language sentences into sign language, bypassing the need for glossintermediaries. This paper presents the Sign language Vector QuantizationNetwork, a novel approach to SLP that leverages Vector Quantization to derivediscrete representations from sign pose sequences. Our method, rooted in bothmanual and non-manual elements of signing, supports advanced decoding methodsand integrates latent-level alignment for enhanced linguistic coherence.Through comprehensive evaluations, we demonstrate superior performance of ourmethod over prior SLP methods and highlight the reliability of Back-Translationand Fr\'echet Gesture Distance as evaluation metrics.</description><author>Eui Jun Hwang, Huije Lee, Jong C. Park</author><pubDate>Thu, 21 Sep 2023 16:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12179v1</guid></item><item><title>Explainable Artificial Intelligence for Drug Discovery and Development -- A Comprehensive Survey</title><link>http://arxiv.org/abs/2309.12177v1</link><description>The field of drug discovery has experienced a remarkable transformation withthe advent of artificial intelligence (AI) and machine learning (ML)technologies. However, as these AI and ML models are becoming more complex,there is a growing need for transparency and interpretability of the models.Explainable Artificial Intelligence (XAI) is a novel approach that addressesthis issue and provides a more interpretable understanding of the predictionsmade by machine learning models. In recent years, there has been an increasinginterest in the application of XAI techniques to drug discovery. This reviewarticle provides a comprehensive overview of the current state-of-the-art inXAI for drug discovery, including various XAI methods, their application indrug discovery, and the challenges and limitations of XAI techniques in drugdiscovery. The article also covers the application of XAI in drug discovery,including target identification, compound design, and toxicity prediction.Furthermore, the article suggests potential future research directions for theapplication of XAI in drug discovery. The aim of this review article is toprovide a comprehensive understanding of the current state of XAI in drugdiscovery and its potential to transform the field.</description><author>Roohallah Alizadehsani, Sadiq Hussain, Rene Ripardo Calixto, Victor Hugo C. de Albuquerque, Mohamad Roshanzamir, Mohamed Rahouti, Senthil Kumar Jagatheesaperumal</author><pubDate>Thu, 21 Sep 2023 16:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12177v1</guid></item><item><title>SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset</title><link>http://arxiv.org/abs/2309.12172v1</link><description>We introduce SANPO, a large-scale egocentric video dataset focused on denseprediction in outdoor environments. It contains stereo video sessions collectedacross diverse outdoor environments, as well as rendered synthetic videosessions. (Synthetic data was provided by Parallel Domain.) All sessions have(dense) depth and odometry labels. All synthetic sessions and a subset of realsessions have temporally consistent dense panoptic segmentation labels. To ourknowledge, this is the first human egocentric video dataset with both largescale dense panoptic segmentation and depth annotations. In addition to thedataset we also provide zero-shot baselines and SANPO benchmarks for futureresearch. We hope that the challenging nature of SANPO will help advance thestate-of-the-art in video segmentation, depth estimation, multi-task visualmodeling, and synthetic-to-real domain adaptation, while enabling humannavigation systems. SANPO is available here:https://google-research-datasets.github.io/sanpo_dataset/</description><author>Sagar M. Waghmare, Kimberly Wilber, Dave Hawkey, Xuan Yang, Matthew Wilson, Stephanie Debats, Cattalyya Nuengsigkapian, Astuti Sharma, Lars Pandikow, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko</author><pubDate>Thu, 21 Sep 2023 16:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12172v1</guid></item><item><title>L1-aware Multilingual Mispronunciation Detection Framework</title><link>http://arxiv.org/abs/2309.07719v2</link><description>The phonological discrepancies between a speaker's native (L1) and thenon-native language (L2) serves as a major factor for mispronunciation. Thispaper introduces a novel multilingual MDD architecture, L1-MultiMDD, enrichedwith L1-aware speech representation. An end-to-end speech encoder is trained onthe input signal and its corresponding reference phoneme sequence. First, anattention mechanism is deployed to align the input audio with the referencephoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from anauxiliary model, pretrained in a multi-task setup identifying L1 and L2language, and are infused with the primary network. Finally, the L1-MultiMDD isthen optimized for a unified multilingual phoneme recognition task usingconnectionist temporal classification (CTC) loss for the target languages:English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness ofthe proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, andAraVoiceL2v2; and unseen -- EpaDB and Speechocean762 datasets. The consistentgains in PER, and false rejection rate (FRR) across all target languagesconfirm our approach's robustness, efficacy, and generalizability.</description><author>Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</author><pubDate>Thu, 21 Sep 2023 16:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07719v2</guid></item><item><title>Optimal Conditional Inference in Adaptive Experiments</title><link>http://arxiv.org/abs/2309.12162v1</link><description>We study batched bandit experiments and consider the problem of inferenceconditional on the realized stopping time, assignment probabilities, and targetparameter, where all of these may be chosen adaptively using information up tothe last batch of the experiment. Absent further restrictions on theexperiment, we show that inference using only the results of the last batch isoptimal. When the adaptive aspects of the experiment are known to belocation-invariant, in the sense that they are unchanged when we shift allbatch-arm means by a constant, we show that there is additional information inthe data, captured by one additional linear function of the batch-arm means. Inthe more restrictive case where the stopping time, assignment probabilities,and target parameter are known to depend on the data only through a collectionof polyhedral events, we derive computationally tractable and optimalconditional inference procedures.</description><author>Jiafeng Chen, Isaiah Andrews</author><pubDate>Thu, 21 Sep 2023 16:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12162v1</guid></item><item><title>Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning</title><link>http://arxiv.org/abs/2309.11489v2</link><description>Designing reward functions is a longstanding challenge in reinforcementlearning (RL); it requires specialized knowledge or domain data, leading tohigh costs for development. To address this, we introduce Text2Reward, adata-free framework that automates the generation of dense reward functionsbased on large language models (LLMs). Given a goal described in naturallanguage, Text2Reward generates dense reward functions as an executable programgrounded in a compact representation of the environment. Unlike inverse RL andrecent work that uses LLMs to write sparse reward codes, Text2Reward producesinterpretable, free-form dense reward codes that cover a wide range of tasks,utilize existing packages, and allow iterative refinement with human feedback.We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17manipulation tasks, policies trained with generated reward codes achievesimilar or better task success rates and convergence speed than expert-writtenreward codes. For locomotion tasks, our method learns six novel locomotionbehaviors with a success rate exceeding 94%. Furthermore, we show that thepolicies trained in the simulator with our method can be deployed in the realworld. Finally, Text2Reward further improves the policies by refining theirreward functions with human feedback. Video results are available athttps://text-to-reward.github.io</description><author>Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</author><pubDate>Thu, 21 Sep 2023 16:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11489v2</guid></item><item><title>Code Soliloquies for Accurate Calculations in Large Language Models</title><link>http://arxiv.org/abs/2309.12161v1</link><description>High-quality conversational datasets are integral to the successfuldevelopment of Intelligent Tutoring Systems (ITS) that employ a Large LanguageModel (LLM) backend. These datasets, when used to fine-tune the LLM backend,significantly enhance the quality of interactions between students and ITS. Acommon strategy for developing these datasets involves generating syntheticstudent-teacher dialogues using advanced GPT-4 models. However, challengesarise when these dialogues demand complex calculations, common in subjects likephysics. Despite its advanced capabilities, GPT-4's performance falls short inreliably handling even simple multiplication tasks, marking a significantlimitation in its utility for these subjects. To address these challenges, thispaper introduces an innovative stateful prompt design. Our approach generates amock conversation between a student and a tutorbot, both roles simulated byGPT-4. Each student response triggers a soliloquy (an inner monologue) in theGPT-tutorbot, which assesses whether its response would necessitatecalculations. If so, it proceeds to script the required code in Python and thenuses the resulting output to construct its response to the student. Ourapproach notably enhances the quality of synthetic conversation datasets,especially for subjects that are calculation-intensive. Our findings show thatour Higgs model -- a LLaMA finetuned with datasets generated through our novelstateful prompt design -- proficiently utilizes Python for computations.Consequently, finetuning with our datasets enriched with code soliloquiesenhances not just the accuracy but also the computational reliability of Higgs'responses.</description><author>Shashank Sonkar, MyCo Le, Xinghe Chen, Naiming Liu, Debshila Basu Mallick, Richard G. Baraniuk</author><pubDate>Thu, 21 Sep 2023 16:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12161v1</guid></item><item><title>Information Forensics and Security: A quarter-century-long journey</title><link>http://arxiv.org/abs/2309.12159v1</link><description>Information Forensics and Security (IFS) is an active R&amp;D area whose goal isto ensure that people use devices, data, and intellectual properties forauthorized purposes and to facilitate the gathering of solid evidence to holdperpetrators accountable. For over a quarter century since the 1990s, the IFSresearch area has grown tremendously to address the societal needs of thedigital information era. The IEEE Signal Processing Society (SPS) has emergedas an important hub and leader in this area, and the article below celebratessome landmark technical contributions. In particular, we highlight the majortechnological advances on some selected focus areas in the field developed inthe last 25 years from the research community and present future trends.</description><author>Mauro Barni, Patrizio Campisi, Edward J. Delp, Gwenael Doërr, Jessica Fridrich, Nasir Memon, Fernando Pérez-González, Anderson Rocha, Luisa Verdoliva, Min Wu</author><pubDate>Thu, 21 Sep 2023 16:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12159v1</guid></item><item><title>Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval</title><link>http://arxiv.org/abs/2309.12158v1</link><description>A range of applications of multi-modal music information retrieval is centredaround the problem of connecting large collections of sheet music (images) tocorresponding audio recordings, that is, identifying pairs of audio and scoreexcerpts that refer to the same musical content. One of the typical and mostrecent approaches to this task employs cross-modal deep learning architecturesto learn joint embedding spaces that link the two distinct modalities - audioand sheet music images. While there has been steady improvement on this frontover the past years, a number of open problems still prevent large-scaleemployment of this methodology. In this article we attempt to provide aninsightful examination of the current developments on audio-sheet musicretrieval via deep learning methods. We first identify a set of main challengeson the road towards robust and large-scale cross-modal music retrieval in realscenarios. We then highlight the steps we have taken so far to address some ofthese challenges, documenting step-by-step improvement along severaldimensions. We conclude by analysing the remaining challenges and present ideasfor solving these, in order to pave the way to a unified and robust methodologyfor cross-modal music retrieval.</description><author>Luis Carvalho, Gerhard Widmer</author><pubDate>Thu, 21 Sep 2023 16:11:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12158v1</guid></item><item><title>Neural Modelling of Dynamic Systems with Time Delays Based on an Adjusted NEAT Algorithm</title><link>http://arxiv.org/abs/2309.12148v1</link><description>A problem related to the development of an algorithm designed to find anarchitecture of artificial neural network used for black-box modelling ofdynamic systems with time delays has been addressed in this paper. The proposedalgorithm is based on a well-known NeuroEvolution of Augmenting Topologies(NEAT) algorithm. The NEAT algorithm has been adjusted by allowing additionalconnections within an artificial neural network and developing originalspecialised evolutionary operators. This resulted in a compromise between thesize of neural network and its accuracy in capturing the response of themathematical model under which it has been learnt. The research involved anextended validation study based on data generated from a mathematical model ofan exemplary system as well as the fast processes occurring in a pressurisedwater nuclear reactor. The obtaining simulation results demonstrate the higheffectiveness of the devised neural (black-box) models of dynamic systems withtime delays.</description><author>Krzysztof Laddach, Rafał Łangowskii</author><pubDate>Thu, 21 Sep 2023 16:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12148v1</guid></item><item><title>Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features</title><link>http://arxiv.org/abs/2309.12140v1</link><description>The rapid development of 3D object detection systems for self-driving carshas significantly improved accuracy. However, these systems struggle togeneralize across diverse driving environments, which can lead tosafety-critical failures in detecting traffic participants. To address this, wepropose a method that utilizes unlabeled repeated traversals of multiplelocations to adapt object detectors to new driving environments. Byincorporating statistics computed from repeated LiDAR scans, we guide theadaptation process effectively. Our approach enhances LiDAR-based detectionmodels using spatial quantized historical features and introduces a lightweightregression head to leverage the statistics for feature regularization.Additionally, we leverage the statistics for a novel self-training process tostabilize the training. The framework is detector model-agnostic andexperiments on real-world datasets demonstrate significant improvements,achieving up to a 20-point performance gain, especially in detectingpedestrians and distant objects. Code is available athttps://github.com/zhangtravis/Hist-DA.</description><author>Travis Zhang, Katie Luo, Cheng Perng Phoo, Yurong You, Wei-Lun Chao, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger</author><pubDate>Thu, 21 Sep 2023 16:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12140v1</guid></item><item><title>On the relationship between Benchmarking, Standards and Certification in Robotics and AI</title><link>http://arxiv.org/abs/2309.12139v1</link><description>Benchmarking, standards and certification are closely related processes.Standards can provide normative requirements that robotics and AI systems mayor may not conform to. Certification generally relies upon conformance with oneor more standards as the key determinant of granting a certificate to operate.And benchmarks are sets of standardised tests against which robots and AIsystems can be measured. Benchmarks therefore can be thought of as informalstandards. In this paper we will develop these themes with examples frombenchmarking, standards and certification, and argue that these three linkedprocesses are not only useful but vital to the broader practice of ResponsibleInnovation.</description><author>Alan F. T. Winfield, Matthew Studley</author><pubDate>Thu, 21 Sep 2023 15:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12139v1</guid></item><item><title>ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations</title><link>http://arxiv.org/abs/2308.10457v2</link><description>Federated Learning (FL) is a distributed machine learning technique thatallows model training among multiple devices or organizations by sharingtraining parameters instead of raw data. However, adversaries can still inferindividual information through inference attacks (e.g. differential attacks) onthese training parameters. As a result, Differential Privacy (DP) has beenwidely used in FL to prevent such attacks. We consider differentially privatefederated learning in a resource-constrained scenario, where both privacybudget and communication round are constrained. By theoretically analyzing theconvergence, we can find the optimal number of differentially private localiterations for clients between any two sequential global updates. Based onthis, we design an algorithm of differentially private federated learning withadaptive local iterations (ALI-DPFL). We experiment our algorithm on theFashionMNIST and CIFAR10 datasets, and demonstrate significantly betterperformances than previous work in the resource-constraint scenario.</description><author>Xinpeng Ling, Jie Fu, Kuncan Wang, Haitao Liu, Zhili Chen</author><pubDate>Thu, 21 Sep 2023 15:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10457v2</guid></item><item><title>OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media</title><link>http://arxiv.org/abs/2309.12137v1</link><description>While resources for English language are fairly sufficient to understandcontent on social media, similar resources in Arabic are still immature. Themain reason that the resources in Arabic are insufficient is that Arabic hasmany dialects in addition to the standard version (MSA). Arabs do not use MSAin their daily communications; rather, they use dialectal versions.Unfortunately, social users transfer this phenomenon into their use of socialmedia platforms, which in turn has raised an urgent need for building suitableAI models for language-dependent applications. Existing machine translation(MT) systems designed for MSA fail to work well with Arabic dialects. In lightof this, it is necessary to adapt to the informal nature of communication onsocial networks by developing MT systems that can effectively handle thevarious dialects of Arabic. Unlike for MSA that shows advanced progress in MTsystems, little effort has been exerted to utilize Arabic dialects for MTsystems. While few attempts have been made to build translation datasets fordialectal Arabic, they are domain dependent and are not OSN cultural-languagefriendly. In this work, we attempt to alleviate these limitations by proposingan online social network-based multidialect Arabic dataset that is crafted bycontextually translating English tweets into four Arabic dialects: Gulf,Yemeni, Iraqi, and Levantine. To perform the translation, we followed ourproposed guideline framework for content translation, which could beuniversally applicable for translation between foreign languages and localdialects. We validated the authenticity of our proposed dataset by developingneural MT models for four Arabic dialects. Our results have shown a superiorperformance of our NMT models trained using our dataset. We believe that ourdataset can reliably serve as an Arabic multidialectal translation dataset forinformal MT tasks.</description><author>Fatimah Alzamzami, Abdulmotaleb El Saddik</author><pubDate>Thu, 21 Sep 2023 15:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12137v1</guid></item><item><title>Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts</title><link>http://arxiv.org/abs/2205.04952v3</link><description>How should a robot speak in a formal, quiet and dark, or a bright, lively andnoisy environment? By designing robots to speak in a more social andambient-appropriate manner we can improve perceived awareness and intelligencefor these agents. We describe a process and results toward selecting robotvoice styles for perceived social appropriateness and ambiance awareness.Understanding how humans adapt their voices in different acoustic settings canbe challenging due to difficulties in voice capture in the wild. Our approachincludes 3 steps: (a) Collecting and validating voice data interactions invirtual Zoom ambiances, (b) Exploration and clustering human vocal utterancesto identify primary voice styles, and (c) Testing robot voice styles inrecreated ambiances using projections, lighting and sound. We focus on foodservice scenarios as a proof-of-concept setting. We provide results using thePepper robot's voice with different styles, towards robots that speak in acontextually appropriate and adaptive manner. Our results with N=120participants provide evidence that the choice of voice style in differentambiances impacted a robot's perceived intelligence in several factorsincluding: social appropriateness, comfort, awareness, human-likeness andcompetency.</description><author>Paige Tuttosi, Emma Hughson, Akihiro Matsufuji, Angelica Lim</author><pubDate>Thu, 21 Sep 2023 15:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04952v3</guid></item><item><title>Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems</title><link>http://arxiv.org/abs/2309.12134v1</link><description>Linking sheet music images to audio recordings remains a key problem for thedevelopment of efficient cross-modal music retrieval systems. One of thefundamental approaches toward this task is to learn a cross-modal embeddingspace via deep neural networks that is able to connect short snippets of audioand sheet music. However, the scarcity of annotated data from real musicalcontent affects the capability of such methods to generalize to real retrievalscenarios. In this work, we investigate whether we can mitigate this limitationwith self-supervised contrastive learning, by exposing a network to a largeamount of real music data as a pre-training step, by contrasting randomlyaugmented views of snippets of both modalities, namely audio and sheet images.Through a number of experiments on synthetic and real piano data, we show thatpre-trained models are able to retrieve snippets with better precision in allscenarios and pre-training configurations. Encouraged by these results, weemploy the snippet embeddings in the higher-level task of cross-modal pieceidentification and conduct more experiments on several retrievalconfigurations. In this task, we observe that the retrieval quality improvesfrom 30% up to 100% when real music data is present. We then conclude byarguing for the potential of self-supervised contrastive learning foralleviating the annotated data scarcity in multi-modal music retrieval models.</description><author>Luis Carvalho, Tobias Washüttl, Gerhard Widmer</author><pubDate>Thu, 21 Sep 2023 15:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12134v1</guid></item><item><title>A knowledge representation approach for construction contract knowledge modeling</title><link>http://arxiv.org/abs/2309.12132v1</link><description>The emergence of large language models (LLMs) presents an unprecedentedopportunity to automate construction contract management, reducing human errorsand saving significant time and costs. However, LLMs may produce convincing yetinaccurate and misleading content due to a lack of domain expertise. To addressthis issue, expert-driven contract knowledge can be represented in a structuredmanner to constrain the automatic contract management process. This paperintroduces the Nested Contract Knowledge Graph (NCKG), a knowledgerepresentation approach that captures the complexity of contract knowledgeusing a nested structure. It includes a nested knowledge representationframework, a NCKG ontology built on the framework, and an implementationmethod. Furthermore, we present the LLM-assisted contract review pipelineenhanced with external knowledge in NCKG. Our pipeline achieves a promisingperformance in contract risk reviewing, shedding light on the combination ofLLM and KG towards more reliable and interpretable contract management.</description><author>Chunmo Zheng, Saika Wong, Xing Su, Yinqiu Tang</author><pubDate>Thu, 21 Sep 2023 15:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12132v1</guid></item><item><title>Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation</title><link>http://arxiv.org/abs/2308.07929v2</link><description>Recently, large multimodal models, such as CLIP and Stable Diffusion haveexperimented tremendous successes in both foundations and applications.However, as these models increase in parameter size and computationalrequirements, it becomes more challenging for users to personalize them forspecific tasks or preferences. In this work, we address the problem of adaptingthe previous models towards sets of particular human preferences, aligning theretrieved or generated images with the preferences of the user. We leverage theBradley-Terry preference model to develop a fast adaptation method thatefficiently fine-tunes the original model, with few examples and with minimalcomputing resources. Extensive evidence of the capabilities of this frameworkis provided through experiments in different domains related to multimodal textand image understanding, including preference prediction as a reward model, andgeneration tasks.</description><author>Victor Gallego</author><pubDate>Thu, 21 Sep 2023 15:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07929v2</guid></item><item><title>Compositional Foundation Models for Hierarchical Planning</title><link>http://arxiv.org/abs/2309.08587v2</link><description>To make effective decisions in novel environments with long-horizon goals, itis crucial to engage in hierarchical reasoning across spatial and temporalscales. This entails planning abstract subgoal sequences, visually reasoningabout the underlying plans, and executing actions in accordance with thedevised plan through visual-motor control. We propose Compositional FoundationModels for Hierarchical Planning (HiP), a foundation model which leveragesmultiple expert foundation model trained on language, vision and action dataindividually jointly together to solve long-horizon tasks. We use a largelanguage model to construct symbolic plans that are grounded in the environmentthrough a large video diffusion model. Generated video plans are then groundedto visual-motor control, through an inverse dynamics model that infers actionsfrom generated videos. To enable effective reasoning within this hierarchy, weenforce consistency between the models via iterative refinement. We illustratethe efficacy and adaptability of our approach in three different long-horizontable-top manipulation tasks.</description><author>Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, Pulkit Agrawal</author><pubDate>Thu, 21 Sep 2023 15:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08587v2</guid></item><item><title>Transforming Geospatial Ontologies by Homomorphisms</title><link>http://arxiv.org/abs/2305.13135v2</link><description>In this paper, we study the geospatial ontologies that we are interested intogether as a geospatial ontology system, consisting of a set of the geospatialontologies and a set of geospatial ontology operations, without any internaldetails of the geospatial ontologies and their operations being needed,algebraically. A homomorphism between two geospatial ontology systems is afunction between two sets of geospatial ontologies in the systems, whichpreserves the geospatial ontology operations. We view clustering a set of theontologies as partitioning the set or defining an equivalence relation on theset or forming a quotient set of the set or obtaining the surjective image ofthe set. Each geospatial ontology system homomorphism can be factored as asurjective clustering to a quotient space, followed by an embedding. Geospatialontology merging systems, natural partial orders on the systems, and geospatialontology merging closures in the systems are then transformed under geospatialontology system homomorphisms that are given by quotients and embeddings.</description><author>Xiuzhan Guo, Wei Huang, Min Luo, Priya Rangarajan</author><pubDate>Thu, 21 Sep 2023 15:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13135v2</guid></item><item><title>Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems</title><link>http://arxiv.org/abs/2309.12128v1</link><description>Neural networks have become a prominent approach to solve inverse problems inrecent years. While a plethora of such methods was developed to solve inverseproblems empirically, we are still lacking clear theoretical guarantees forthese methods. On the other hand, many works proved convergence to optimalsolutions of neural networks in a more general setting usingoverparametrization as a way to control the Neural Tangent Kernel. In this workwe investigate how to bridge these two worlds and we provide deterministicconvergence and recovery guarantees for the class of unsupervised feedforwardmultilayer neural networks trained to solve inverse problems. We also deriveoverparametrization bounds under which a two-layers Deep Inverse Prior networkwith smooth activation function will benefit from our guarantees.</description><author>Nathan Buskulic, Jalal Fadili, Yvain Quéau</author><pubDate>Thu, 21 Sep 2023 15:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12128v1</guid></item><item><title>Learning End-to-End Channel Coding with Diffusion Models</title><link>http://arxiv.org/abs/2309.10505v2</link><description>The training of neural encoders via deep learning necessitates adifferentiable channel model due to the backpropagation algorithm. Thisrequirement can be sidestepped by approximating either the channel distributionor its gradient through pilot signals in real-world scenarios. The initialapproach draws upon the latest advancements in image generation, utilizinggenerative adversarial networks (GANs) or their enhanced variants to generatechannel distributions. In this paper, we address this channel approximationchallenge with diffusion models, which have demonstrated high sample quality inimage generation. We offer an end-to-end channel coding framework underpinnedby diffusion models and propose an efficient training algorithm. Oursimulations with various channel models establish that our diffusion modelslearn the channel distribution accurately, thereby achieving near-optimalend-to-end symbol error rates (SERs). We also note a significant advantage ofdiffusion models: A robust generalization capability in high signal-to-noiseratio regions, in contrast to GAN variants that suffer from error floor.Furthermore, we examine the trade-off between sample quality and samplingspeed, when an accelerated sampling algorithm is deployed, and investigate theeffect of the noise scheduling on this trade-off. With an apt choice of noisescheduling, sampling time can be significantly reduced with a minor increase inSER.</description><author>Muah Kim, Rick Fritschek, Rafael F. Schaefer</author><pubDate>Thu, 21 Sep 2023 15:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10505v2</guid></item><item><title>Bayesian Flow Networks</title><link>http://arxiv.org/abs/2308.07037v2</link><description>This paper introduces Bayesian Flow Networks (BFNs), a new class ofgenerative model in which the parameters of a set of independent distributionsare modified with Bayesian inference in the light of noisy data samples, thenpassed as input to a neural network that outputs a second, interdependentdistribution. Starting from a simple prior and iteratively updating the twodistributions yields a generative procedure similar to the reverse process ofdiffusion models; however it is conceptually simpler in that no forward processis required. Discrete and continuous-time loss functions are derived forcontinuous, discretised and discrete data, along with sample generationprocedures. Notably, the network inputs for discrete data lie on theprobability simplex, and are therefore natively differentiable, paving the wayfor gradient-based sample guidance and few-step generation in discrete domainssuch as language modelling. The loss function directly optimises datacompression and places no restrictions on the network architecture. In ourexperiments BFNs achieve competitive log-likelihoods for image modelling ondynamically binarized MNIST and CIFAR-10, and outperform all known discretediffusion models on the text8 character-level language modelling task.</description><author>Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez</author><pubDate>Thu, 21 Sep 2023 15:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07037v2</guid></item><item><title>State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards</title><link>http://arxiv.org/abs/2102.11941v2</link><description>A common formulation of constrained reinforcement learning involves multiplerewards that must individually accumulate to given thresholds. In this class ofproblems, we show a simple example in which the desired optimal policy cannotbe induced by any weighted linear combination of rewards. Hence, there existconstrained reinforcement learning problems for which neither regularized norclassical primal-dual methods yield optimal policies. This work addresses thisshortcoming by augmenting the state with Lagrange multipliers andreinterpreting primal-dual methods as the portion of the dynamics that drivesthe multipliers evolution. This approach provides a systematic stateaugmentation procedure that is guaranteed to solve reinforcement learningproblems with constraints. Thus, as we illustrate by an example, while previousmethods can fail at finding optimal policies, running the dual dynamics whileexecuting the augmented policy yields an algorithm that provably samplesactions from the optimal policy.</description><author>Miguel Calvo-Fullana, Santiago Paternain, Luiz F. O. Chamon, Alejandro Ribeiro</author><pubDate>Thu, 21 Sep 2023 15:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.11941v2</guid></item><item><title>Vulnerability of 3D Face Recognition Systems to Morphing Attacks</title><link>http://arxiv.org/abs/2309.12118v1</link><description>In recent years face recognition systems have been brought to the mainstreamdue to development in hardware and software. Consistent efforts are being madeto make them better and more secure. This has also brought developments in 3Dface recognition systems at a rapid pace. These 3DFR systems are expected toovercome certain vulnerabilities of 2DFR systems. One such problem that thedomain of 2DFR systems face is face image morphing. A substantial amount ofresearch is being done for generation of high quality face morphs along withdetection of attacks from these morphs. Comparatively the understanding ofvulnerability of 3DFR systems against 3D face morphs is less. But at the sametime an expectation is set from 3DFR systems to be more robust against suchattacks. This paper attempts to research and gain more information on thismatter. The paper describes a couple of methods that can be used to generate 3Dface morphs. The face morphs that are generated using this method are thencompared to the contributing faces to obtain similarity scores. The highestMMPMR is obtained around 40% with RMMR of 41.76% when 3DFRS are attacked withlook-a-like morphs.</description><author>Sanjeet Vardam, Luuk Spreeuwers</author><pubDate>Thu, 21 Sep 2023 15:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12118v1</guid></item><item><title>How-to Guides for Specific Audiences: A Corpus and Initial Findings</title><link>http://arxiv.org/abs/2309.12117v1</link><description>Instructional texts for specific target groups should ideally take intoaccount the prior knowledge and needs of the readers in order to guide themefficiently to their desired goals. However, targeting specific groups alsocarries the risk of reflecting disparate social norms and subtle stereotypes.In this paper, we investigate the extent to which how-to guides from oneparticular platform, wikiHow, differ in practice depending on the intendedaudience. We conduct two case studies in which we examine qualitative featuresof texts written for specific audiences. In a generalization study, weinvestigate which differences can also be systematically demonstrated usingcomputational methods. The results of our studies show that guides fromwikiHow, like other text genres, are subject to subtle biases. We aim to raiseawareness of these inequalities as a first step to addressing them in futurework.</description><author>Nicola Fanton, Agnieszka Falenska, Michael Roth</author><pubDate>Thu, 21 Sep 2023 15:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12117v1</guid></item><item><title>AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net</title><link>http://arxiv.org/abs/2309.12114v1</link><description>Tumor segmentation in medical imaging is crucial and relies on precisedelineation. Fluorodeoxyglucose Positron-Emission Tomography (FDG-PET) iswidely used in clinical practice to detect metabolically active tumors.However, FDG-PET scans may misinterpret irregular glucose consumption inhealthy or benign tissues as cancer. Combining PET with Computed Tomography(CT) can enhance tumor segmentation by integrating metabolic and anatomicinformation. FDG-PET/CT scans are pivotal for cancer staging and reassessment,utilizing radiolabeled fluorodeoxyglucose to highlight metabolically activeregions. Accurately distinguishing tumor-specific uptake from physiologicaluptake in normal tissues is a challenging aspect of precise tumor segmentation.The AutoPET challenge addresses this by providing a dataset of 1014 FDG-PET/CTstudies, encouraging advancements in accurate tumor segmentation and analysiswithin the FDG-PET/CT domain. Code:https://github.com/matt3o/AutoPET2-Submission/</description><author>Matthias Hadlich, Zdravko Marinov, Rainer Stiefelhagen</author><pubDate>Thu, 21 Sep 2023 15:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12114v1</guid></item><item><title>THE Benchmark: Transferable Representation Learning for Monocular Height Estimation</title><link>http://arxiv.org/abs/2112.14985v2</link><description>Generating 3D city models rapidly is crucial for many applications. Monocularheight estimation is one of the most efficient and timely ways to obtainlarge-scale geometric information. However, existing works focus primarily ontraining and testing models using unbiased datasets, which does not align wellwith real-world applications. Therefore, we propose a new benchmark dataset tostudy the transferability of height estimation models in a cross-datasetsetting. To this end, we first design and construct a large-scale benchmarkdataset for cross-dataset transfer learning on the height estimation task. Thisbenchmark dataset includes a newly proposed large-scale synthetic dataset, anewly collected real-world dataset, and four existing datasets from differentcities. Next, a new experimental protocol, few-shot cross-dataset transfer, isdesigned. Furthermore, in this paper, we propose a scale-deformable convolutionmodule to enhance the window-based Transformer for handling the scale-variationproblem in the height estimation task. Experimental results have demonstratedthe effectiveness of the proposed methods in the traditional and cross-datasettransfer settings. The datasets and codes are publicly available athttps://mediatum.ub.tum.de/1662763 and https://thebenchmarkh.github.io/.</description><author>Zhitong Xiong, Wei Huang, Jingtao Hu, Xiao Xiang Zhu</author><pubDate>Thu, 21 Sep 2023 15:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.14985v2</guid></item><item><title>Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives</title><link>http://arxiv.org/abs/2309.12113v1</link><description>Although the uncertainties of the workers can be addressed by the standardCombinatorial Multi-Armed Bandit (CMAB) framework in existing proposals througha trade-off between exploration and exploitation, we may not have sufficientbudget to enable the trade-off among the individual workers, especially whenthe number of the workers is huge while the budget is limited. Moreover, thestandard CMAB usually assumes the workers always stay in the system, whereasthe workers may join in or depart from the system over time, such that what wehave learnt for an individual worker cannot be applied after the worker leaves.To address the above challenging issues, in this paper, we first propose anoff-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate inleveraging the exploration-exploitation trade-off in a elaborately partitionedcontext space instead of the individual workers, to effectively incentivize themassive unknown workers with very limited budget. We also extend the abovebasic idea to the on-line setting where unknown workers may join in or departfrom the systems dynamically, and propose an on-line version of the CACImechanism. Specifically, by the exploitation-exploration trade-off in thecontext space, we learn to estimate the sensing ability of any unknown worker(even it never appeared in the system before) according to its contextinformation. We perform rigorous theoretical analysis to reveal the upperbounds on the regrets of our CACI mechanisms and to prove their truthfulnessand individual rationality, respectively. Extensive experiments on bothsynthetic and real datasets are also conducted to verify the efficacy of ourmechanisms.</description><author>Feng Li, Yuqi Chai, Huan Yang, Pengfei Hu, Lingjie Duan</author><pubDate>Thu, 21 Sep 2023 15:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12113v1</guid></item><item><title>Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval</title><link>http://arxiv.org/abs/2309.12111v1</link><description>Many applications of cross-modal music retrieval are related to connectingsheet music images to audio recordings. A typical and recent approach to thisis to learn, via deep neural networks, a joint embedding space that correlatesshort fixed-size snippets of audio and sheet music by means of an appropriatesimilarity structure. However, two challenges that arise out of this strategyare the requirement of strongly aligned data to train the networks, and theinherent discrepancies of musical content between audio and sheet musicsnippets caused by local and global tempo differences. In this paper, weaddress these two shortcomings by designing a cross-modal recurrent networkthat learns joint embeddings that can summarize longer passages ofcorresponding audio and sheet music. The benefits of our method are that itonly requires weakly aligned audio-sheet music pairs, as well as that therecurrent network handles the non-linearities caused by tempo variationsbetween audio and sheet music. We conduct a number of experiments on syntheticand real piano data and scores, showing that our proposed recurrent methodleads to more accurate retrieval in all possible configurations.</description><author>Luis Carvalho, Gerhard Widmer</author><pubDate>Thu, 21 Sep 2023 15:30:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12111v1</guid></item><item><title>Exploiting CLIP-based Multi-modal Approach for Artwork Classification and Retrieval</title><link>http://arxiv.org/abs/2309.12110v1</link><description>Given the recent advances in multimodal image pretraining where visual modelstrained with semantically dense textual supervision tend to have bettergeneralization capabilities than those trained using categorical attributes orthrough unsupervised techniques, in this work we investigate how recent CLIPmodel can be applied in several tasks in artwork domain. We perform exhaustiveexperiments on the NoisyArt dataset which is a dataset of artwork imagescrawled from public resources on the web. On such dataset CLIP achievesimpressive results on (zero-shot) classification and promising results in bothartwork-to-artwork and description-to-artwork domain.</description><author>Alberto Baldrati, Marco Bertini, Tiberio Uricchio, Alberto Del Bimbo</author><pubDate>Thu, 21 Sep 2023 15:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12110v1</guid></item><item><title>PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models</title><link>http://arxiv.org/abs/2309.12109v1</link><description>In this era of large language models (LLMs), the traditional training ofmodels has become increasingly unimaginable for regular users and institutions.The exploration of efficient fine-tuning for high-resource languages on thesemodels is an undeniable trend that is gradually gaining popularity. However,there has been very little exploration for various low-resource languages, suchas Tibetan. Research in Tibetan NLP is inherently scarce and limited. Whilethere is currently no existing large language model for Tibetan due to itslow-resource nature, that day will undoubtedly arrive. Therefore, research onefficient fine-tuning for low-resource language models like Tibetan is highlynecessary. Our research can serve as a reference to fill this crucial gap.Efficient fine-tuning strategies for pre-trained language models (PLMs) inTibetan have seen minimal exploration. We conducted three types of efficientfine-tuning experiments on the publicly available TNCC-title dataset:"prompt-tuning," "Adapter lightweight fine-tuning," and "prompt-tuning +Adapter fine-tuning." The experimental results demonstrate significantimprovements using these methods, providing valuable insights for advancingTibetan language applications in the context of pre-trained models.</description><author>Zhou Mingjun, Daiqing Zhuoma, Qun Nuo, Nyima Tashi</author><pubDate>Thu, 21 Sep 2023 15:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12109v1</guid></item><item><title>A Computational Analysis of Vagueness in Revisions of Instructional Texts</title><link>http://arxiv.org/abs/2309.12107v1</link><description>WikiHow is an open-domain repository of instructional articles for a varietyof tasks, which can be revised by users. In this paper, we extract pairwiseversions of an instruction before and after a revision was made. Starting froma noisy dataset of revision histories, we specifically extract and analyzeedits that involve cases of vagueness in instructions. We further investigatethe ability of a neural model to distinguish between two versions of aninstruction in our data by adopting a pairwise ranking task from previous workand showing improvements over existing baselines.</description><author>Alok Debnath, Michael Roth</author><pubDate>Thu, 21 Sep 2023 15:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12107v1</guid></item><item><title>FourierLoss: Shape-Aware Loss Function with Fourier Descriptors</title><link>http://arxiv.org/abs/2309.12106v1</link><description>Encoder-decoder networks become a popular choice for various medical imagesegmentation tasks. When they are trained with a standard loss function, thesenetworks are not explicitly enforced to preserve the shape integrity of anobject in an image. However, this ability of the network is important to obtainmore accurate results, especially when there is a low-contrast differencebetween the object and its surroundings. In response to this issue, this workintroduces a new shape-aware loss function, which we name FourierLoss. Thisloss function relies on quantifying the shape dissimilarity between the groundtruth and the predicted segmentation maps through the Fourier descriptorscalculated on their objects, and penalizing this dissimilarity in networktraining. Different than the previous studies, FourierLoss offers an adaptiveloss function with trainable hyperparameters that control the importance of thelevel of the shape details that the network is enforced to learn in thetraining process. This control is achieved by the proposed adaptive loss updatemechanism, which end-to-end learns the hyperparameters simultaneously with thenetwork weights by backpropagation. As a result of using this mechanism, thenetwork can dynamically change its attention from learning the general outlineof an object to learning the details of its contour points, or vice versa, indifferent training epochs. Working on 2879 computed tomography images of 93subjects, our experiments revealed that the proposed adaptive shape-aware lossfunction led to statistically significantly better results for liversegmentation, compared to its counterparts.</description><author>Mehmet Bahadir Erden, Selahattin Cansiz, Onur Caki, Haya Khattak, Durmus Etiz, Melek Cosar Yakar, Kerem Duruer, Berke Barut, Cigdem Gunduz-Demir</author><pubDate>Thu, 21 Sep 2023 15:23:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12106v1</guid></item><item><title>Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law</title><link>http://arxiv.org/abs/2308.11531v2</link><description>Our project aims at helping and supporting stakeholders in refugee statusadjudications, such as lawyers, judges, governing bodies, and claimants, inorder to make better decisions through data-driven intelligence and increasethe understanding and transparency of the refugee application process for allinvolved parties. This PhD project has two primary objectives: (1) to retrievepast cases, and (2) to analyze legal decision-making processes on a dataset ofCanadian cases. In this paper, we present the current state of our work, whichincludes a completed experiment on part (1) and ongoing efforts related to part(2). We believe that NLP-based solutions are well-suited to address thesechallenges, and we investigate the feasibility of automating all stepsinvolved. In addition, we introduce a novel benchmark for future NLP researchin refugee law. Our methodology aims to be inclusive to all end-users andstakeholders, with expected benefits including reduced time-to-decision, fairerand more transparent outcomes, and improved decision quality.</description><author>Claire Barale</author><pubDate>Thu, 21 Sep 2023 15:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11531v2</guid></item><item><title>SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts</title><link>http://arxiv.org/abs/2309.12102v1</link><description>We describe SemEval-2022 Task 7, a shared task on rating the plausibility ofclarifications in instructional texts. The dataset for this task consists ofmanually clarified how-to guides for which we generated alternativeclarifications and collected human plausibility judgements. The task ofparticipating systems was to automatically determine the plausibility of aclarification in the respective context. In total, 21 participants took part inthis task, with the best system achieving an accuracy of 68.9%. This reportsummarizes the results and findings from 8 teams and their system descriptions.Finally, we show in an additional evaluation that predictions by the topparticipating team make it possible to identify contexts with multipleplausible clarifications with an accuracy of 75.2%.</description><author>Michael Roth, Talita Anthonio, Anna Sauer</author><pubDate>Thu, 21 Sep 2023 15:19:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12102v1</guid></item><item><title>Traffic Forecasting on New Roads Using Spatial Contrastive Pre-Training (SCPT)</title><link>http://arxiv.org/abs/2305.05237v4</link><description>New roads are being constructed all the time. However, the capabilities ofprevious deep forecasting models to generalize to new roads not seen in thetraining data (unseen roads) are rarely explored. In this paper, we introduce anovel setup called a spatio-temporal (ST) split to evaluate the models'capabilities to generalize to unseen roads. In this setup, the models aretrained on data from a sample of roads, but tested on roads not seen in thetraining data. Moreover, we also present a novel framework called SpatialContrastive Pre-Training (SCPT) where we introduce a spatial encoder module toextract latent features from unseen roads during inference time. This spatialencoder is pre-trained using contrastive learning. During inference, thespatial encoder only requires two days of traffic data on the new roads anddoes not require any re-training. We also show that the output from the spatialencoder can be used effectively to infer latent node embeddings on unseen roadsduring inference time. The SCPT framework also incorporates a new layer, namedthe spatially gated addition (SGA) layer, to effectively combine the latentfeatures from the output of the spatial encoder to existing backbones.Additionally, since there is limited data on the unseen roads, we argue that itis better to decouple traffic signals to trivial-to-capture periodic signalsand difficult-to-capture Markovian signals, and for the spatial encoder to onlylearn the Markovian signals. Finally, we empirically evaluated SCPT using theST split setup on four real-world datasets. The results showed that adding SCPTto a backbone consistently improves forecasting performance on unseen roads.More importantly, the improvements are greater when forecasting further intothe future. The codes are available on GitHub:https://github.com/cruiseresearchgroup/forecasting-on-new-roads .</description><author>Arian Prabowo, Hao Xue, Wei Shao, Piotr Koniusz, Flora D. Salim</author><pubDate>Thu, 21 Sep 2023 15:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05237v4</guid></item><item><title>Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks</title><link>http://arxiv.org/abs/2303.01682v2</link><description>Bayesian Optimization (BO) is an effective approach for global optimizationof black-box functions when function evaluations are expensive. Most priorworks use Gaussian processes to model the black-box function, however, the useof kernels in Gaussian processes leads to two problems: first, the kernel-basedmethods scale poorly with the number of data points and second, kernel methodsare usually not effective on complex structured high dimensional data due tocurse of dimensionality. Therefore, we propose a novel black-box optimizationalgorithm where the black-box function is modeled using a neural network. Ouralgorithm does not need a Bayesian neural network to estimate predictiveuncertainty and is therefore computationally favorable. We analyze thetheoretical behavior of our algorithm in terms of regret bound using advancesin NTK theory showing its efficient convergence. We perform experiments withboth synthetic and real-world optimization tasks and show that our algorithm ismore sample efficient compared to existing methods.</description><author>Dat Phan-Trong, Hung Tran-The, Sunil Gupta</author><pubDate>Thu, 21 Sep 2023 15:12:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01682v2</guid></item><item><title>Bayesian sparsification for deep neural networks with Bayesian model reduction</title><link>http://arxiv.org/abs/2309.12095v1</link><description>Deep learning's immense capabilities are often constrained by the complexityof its models, leading to an increasing demand for effective sparsificationtechniques. Bayesian sparsification for deep learning emerges as a crucialapproach, facilitating the design of models that are both computationallyefficient and competitive in terms of performance across various deep learningapplications. The state-of-the-art -- in Bayesian sparsification of deep neuralnetworks -- combines structural shrinkage priors on model weights with anapproximate inference scheme based on black-box stochastic variationalinference. However, model inversion of the full generative model isexceptionally computationally demanding, especially when compared to standarddeep learning of point estimates. In this context, we advocate for the use ofBayesian model reduction (BMR) as a more efficient alternative for pruning ofmodel weights. As a generalization of the Savage-Dickey ratio, BMR allows apost-hoc elimination of redundant model weights based on the posteriorestimates under a straightforward (non-hierarchical) generative model. Ourcomparative study highlights the computational efficiency and the pruning rateof the BMR method relative to the established stochastic variational inference(SVI) scheme, when applied to the full hierarchical generative model. Weillustrate the potential of BMR to prune model parameters across various deeplearning architectures, from classical networks like LeNet to modern frameworkssuch as Vision Transformers and MLP-Mixers.</description><author>Dimitrije Marković, Karl J. Friston, Stefan J. Kiebel</author><pubDate>Thu, 21 Sep 2023 15:10:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12095v1</guid></item><item><title>Multi-Task Cooperative Learning via Searching for Flat Minima</title><link>http://arxiv.org/abs/2309.12090v1</link><description>Multi-task learning (MTL) has shown great potential in medical imageanalysis, improving the generalizability of the learned features and theperformance in individual tasks. However, most of the work on MTL focuses oneither architecture design or gradient manipulation, while in both scenarios,features are learned in a competitive manner. In this work, we propose toformulate MTL as a multi/bi-level optimization problem, and therefore forcefeatures to learn from each task in a cooperative approach. Specifically, weupdate the sub-model for each task alternatively taking advantage of thelearned sub-models of the other tasks. To alleviate the negative transferproblem during the optimization, we search for flat minima for the currentobjective function with regard to features from other tasks. To demonstrate theeffectiveness of the proposed approach, we validate our method on threepublicly available datasets. The proposed method shows the advantage ofcooperative learning, and yields promising results when compared with thestate-of-the-art MTL approaches. The code will be available online.</description><author>Fuping Wu, Le Zhang, Yang Sun, Yuanhan Mo, Thomas Nichols, Bartlomiej W. Papiez</author><pubDate>Thu, 21 Sep 2023 15:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12090v1</guid></item><item><title>PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence</title><link>http://arxiv.org/abs/2307.09683v3</link><description>Biomedical research yields a wealth of information, much of which is onlyaccessible through the literature. Consequently, literature search is anessential tool for building on prior knowledge in clinical and biomedicalresearch. Although recent improvements in artificial intelligence have expandedfunctionality beyond keyword-based search, these advances may be unfamiliar toclinicians and researchers. In response, we present a survey of literaturesearch tools tailored to both general and specific information needs inbiomedicine, with the objective of helping readers efficiently fulfill theirinformation needs. We first examine the widely used PubMed search engine,discussing recent improvements and continued challenges. We then describeliterature search tools catering to five specific information needs: 1.Identifying high-quality clinical research for evidence-based medicine. 2.Retrieving gene-related information for precision medicine and genomics. 3.Searching by meaning, including natural language questions. 4. Locating relatedarticles with literature recommendation. 5. Mining literature to discoverassociations between concepts such as diseases and genetic variants.Additionally, we cover practical considerations and best practices for choosingand using these tools. Finally, we provide a perspective on the future ofliterature search engines, considering recent breakthroughs in large languagemodels such as ChatGPT. In summary, our survey provides a comprehensive view ofbiomedical literature search functionalities with 36 publicly available tools.</description><author>Qiao Jin, Robert Leaman, Zhiyong Lu</author><pubDate>Thu, 21 Sep 2023 14:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09683v3</guid></item><item><title>Clustering-based Domain-Incremental Learning</title><link>http://arxiv.org/abs/2309.12078v1</link><description>We consider the problem of learning multiple tasks in a continual learningsetting in which data from different tasks is presented to the learner in astreaming fashion. A key challenge in this setting is the so-called"catastrophic forgetting problem", in which the performance of the learner inan "old task" decreases when subsequently trained on a "new task". Existingcontinual learning methods, such as Averaged Gradient Episodic Memory (A-GEM)and Orthogonal Gradient Descent (OGD), address catastrophic forgetting byminimizing the loss for the current task without increasing the loss forprevious tasks. However, these methods assume the learner knows when the taskchanges, which is unrealistic in practice. In this paper, we alleviate the needto provide the algorithm with information about task changes by using an onlineclustering-based approach on a dynamically updated finite pool of samples orgradients. We thereby successfully counteract catastrophic forgetting in one ofthe hardest settings, namely: domain-incremental learning, a setting for whichthe problem was previously unsolved. We showcase the benefits of our approachby applying these ideas to projection-based methods, such as A-GEM and OGD,which lead to task-agnostic versions of them. Experiments on real datasetsdemonstrate the effectiveness of the proposed strategy and its promisingperformance compared to state-of-the-art methods.</description><author>Christiaan Lamers, Rene Vidal, Nabil Belbachir, Niki van Stein, Thomas Baeck, Paris Giampouras</author><pubDate>Thu, 21 Sep 2023 14:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12078v1</guid></item></channel></rss>