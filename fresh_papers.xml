<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 03 Jan 2024 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Street Gaussians for Modeling Dynamic Urban Scenes</title><link>http://arxiv.org/abs/2401.01339v1</link><description>This paper aims to tackle the problem of modeling dynamic urban street scenesfrom monocular videos. Recent methods extend NeRF by incorporating trackedvehicle poses to animate vehicles, enabling photo-realistic view synthesis ofdynamic urban street scenes. However, significant limitations are their slowtraining and rendering speed, coupled with the critical need for high precisionin tracked vehicle poses. We introduce Street Gaussians, a new explicit scenerepresentation that tackles all these limitations. Specifically, the dynamicurban street is represented as a set of point clouds equipped with semanticlogits and 3D Gaussians, each associated with either a foreground vehicle orthe background. To model the dynamics of foreground object vehicles, eachobject point cloud is optimized with optimizable tracked poses, along with adynamic spherical harmonics model for the dynamic appearance. The explicitrepresentation allows easy composition of object vehicles and background, whichin turn allows for scene editing operations and rendering at 133 FPS(1066$\times$1600 resolution) within half an hour of training. The proposedmethod is evaluated on multiple challenging benchmarks, including KITTI andWaymo Open datasets. Experiments show that the proposed method consistentlyoutperforms state-of-the-art methods across all datasets. Furthermore, theproposed representation delivers performance on par with that achieved usingprecise ground-truth poses, despite relying only on poses from an off-the-shelftracker. The code is available at https://zju3dv.github.io/street_gaussians/.</description><author>Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng</author><pubDate>Tue, 02 Jan 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01339v1</guid></item><item><title>Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</title><link>http://arxiv.org/abs/2401.01335v1</link><description>Harnessing the power of human-annotated data through Supervised Fine-Tuning(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, wedelve into the prospect of growing a strong LLM out of a weak one without theneed for acquiring additional human-annotated data. We propose a newfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from asupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,where the LLM refines its capability by playing against instances of itself.More specifically, the LLM generates its own training data from its previousiterations, refining its policy by discerning these self-generated responsesfrom those obtained from human-annotated data. Our method progressivelyelevates the LLM from a nascent model to a formidable one, unlocking the fullpotential of human-annotated demonstration data for SFT. Theoretically, weprove that the global optimum to the training objective function of our methodis achieved only when the LLM policy aligns with the target data distribution.Empirically, we evaluate our method on several benchmark datasets including theHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Ourresults show that SPIN can significantly improve the LLM's performance across avariety of benchmarks and even outperform models trained through directpreference optimization (DPO) supplemented with extra GPT-4 preference data.This sheds light on the promise of self-play, enabling the achievement ofhuman-level performance in LLMs without the need for expert opponents.</description><author>Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu</author><pubDate>Tue, 02 Jan 2024 18:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01335v1</guid></item><item><title>Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space</title><link>http://arxiv.org/abs/2312.17300v2</link><description>Domain generalization focuses on leveraging knowledge from multiple relateddomains with ample training data and labels to enhance inference on unseenin-distribution (IN) and out-of-distribution (OOD) domains. In our study, weintroduce a two-phase representation learning technique using multi-tasklearning. This approach aims to cultivate a latent space from features spanningmultiple domains, encompassing both native and cross-domains, to amplifygeneralization to IN and OOD territories. Additionally, we attempt todisentangle the latent space by minimizing the mutual information between theprior and latent space, effectively de-correlating spurious featurecorrelations. Collectively, the joint optimization will facilitatedomain-invariant feature learning. We assess the model's efficacy acrossmultiple cybersecurity datasets, using standard classification metrics on bothunseen IN and OOD sets, and juxtapose the results with contemporary domaingeneralization methods.</description><author>Padmaksha Roy, Tyler Cody, Himanshu Singhal, Kevin Choi, Ming Jin</author><pubDate>Tue, 02 Jan 2024 18:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17300v2</guid></item><item><title>TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview</title><link>http://arxiv.org/abs/2401.01330v1</link><description>Conversational Information Seeking stands as a pivotal research area withsignificant contributions from previous works. The TREC Interactive KnowledgeAssistance Track (iKAT) builds on the foundational work of the TRECConversational Assistance Track (CAsT). However, iKAT distinctively emphasizesthe creation and research of conversational search agents that adapt responsesbased on user's prior interactions and present context. The challenge lies inenabling Conversational Search Agents (CSA) to incorporate this personalizedcontext to efficiency and effectively guide users through the relevantinformation to them. iKAT also emphasizes decisional search tasks, where userssift through data and information to weigh up options in order to reach aconclusion or perform an action. These tasks, prevalent in everydayinformation-seeking decisions -- be it related to travel, health, or shopping-- often revolve around a subset of high-level information operators wherequeries or questions about the information space include: finding options,comparing options, identifying the pros and cons of options, etc. Given thedifferent personas and their information need (expressed through the sequenceof questions), diverse conversation trajectories will arise -- because theanswers to these similar queries will be very different. In this paper, wereport on the first year of TREC iKAT, describing the task, topics, datacollection, and evaluation framework. We further review the submissions andsummarize the findings.</description><author>Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, Leif Azzopardi</author><pubDate>Tue, 02 Jan 2024 18:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01330v1</guid></item><item><title>An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction</title><link>http://arxiv.org/abs/2401.01326v1</link><description>In this paper, we propose a novel method for joint entity and relationextraction from unstructured text by framing it as a conditional sequencegeneration problem. In contrast to conventional generative informationextraction models that are left-to-right token-level generators, our approachis \textit{span-based}. It generates a linearized graph where nodes representtext spans and edges represent relation triplets. Our method employs atransformer encoder-decoder architecture with pointing mechanism on a dynamicvocabulary of spans and relation types. Our model can capture the structuralcharacteristics and boundaries of entities and relations through spanrepresentations while simultaneously grounding the generated output in theoriginal text thanks to the pointing mechanism. Evaluation on benchmarkdatasets validates the effectiveness of our approach, demonstrating competitiveresults. Code is available at https://github.com/urchade/ATG.</description><author>Zaratiana Urchade, Nadi Tomeh, Pierre Holat, Thierry Charnois</author><pubDate>Tue, 02 Jan 2024 18:32:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01326v1</guid></item><item><title>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</title><link>http://arxiv.org/abs/2401.01325v1</link><description>This work elicits LLMs' inherent ability to handle long contexts withoutfine-tuning. The limited length of the training sequence during training maylimit the application of Large Language Models (LLMs) on long input sequencesfor inference. In this work, we argue that existing LLMs themselves haveinherent capabilities for handling long contexts. Based on this argument, wesuggest extending LLMs' context window by themselves to fully utilize theinherent ability.We propose Self-Extend to stimulate LLMs' long contexthandling potential. The basic idea is to construct bi-level attentioninformation: the group level and the neighbor level. The two levels arecomputed by the original model's self-attention, which means the proposed doesnot require any training. With only four lines of code modification, theproposed method can effortlessly extend existing LLMs' context window withoutany fine-tuning. We conduct comprehensive experiments and the results show thatthe proposed method can effectively extend existing LLMs' context window'slength.</description><author>Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu</author><pubDate>Tue, 02 Jan 2024 18:30:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01325v1</guid></item><item><title>Sample-Efficient Safety Assurances using Conformal Prediction</title><link>http://arxiv.org/abs/2109.14082v5</link><description>When deploying machine learning models in high-stakes robotics applications,the ability to detect unsafe situations is crucial. Early warning systems canprovide alerts when an unsafe situation is imminent (in the absence ofcorrective action). To reliably improve safety, these warning systems shouldhave a provable false negative rate; i.e. of the situations that are unsafe,fewer than $\epsilon$ will occur without an alert. In this work, we present aframework that combines a statistical inference technique known as conformalprediction with a simulator of robot/environment dynamics, in order to tunewarning systems to provably achieve an $\epsilon$ false negative rate using asfew as $1/\epsilon$ data points. We apply our framework to a driver warningsystem and a robotic grasping application, and empirically demonstrateguaranteed false negative rate while also observing low false detection(positive) rate.</description><author>Rachel Luo, Shengjia Zhao, Jonathan Kuck, Boris Ivanovic, Silvio Savarese, Edward Schmerling, Marco Pavone</author><pubDate>Tue, 02 Jan 2024 18:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.14082v5</guid></item><item><title>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models</title><link>http://arxiv.org/abs/2401.01313v1</link><description>As Large Language Models (LLMs) continue to advance in their ability to writehuman-like text, a key challenge remains around their tendency to hallucinategenerating content that appears factual but is ungrounded. This issue ofhallucination is arguably the biggest hindrance to safely deploying thesepowerful LLMs into real-world production systems that impact people's lives.The journey toward widespread adoption of LLMs in practical settings heavilyrelies on addressing and mitigating hallucinations. Unlike traditional AIsystems focused on limited tasks, LLMs have been exposed to vast amounts ofonline text data during training. While this allows them to display impressivelanguage fluency, it also means they are capable of extrapolating informationfrom the biases in training data, misinterpreting ambiguous prompts, ormodifying the information to align superficially with the input. This becomeshugely alarming when we rely on language generation capabilities for sensitiveapplications, such as summarizing medical records, financial analysis reports,etc. This paper presents a comprehensive survey of over 32 techniques developedto mitigate hallucination in LLMs. Notable among these are Retrieval AugmentedGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, weintroduce a detailed taxonomy categorizing these methods based on variousparameters, such as dataset utilization, common tasks, feedback mechanisms, andretriever types. This classification helps distinguish the diverse approachesspecifically designed to tackle hallucination issues in LLMs. Additionally, weanalyze the challenges and limitations inherent in these techniques, providinga solid foundation for future research in addressing hallucinations and relatedphenomena within the realm of LLMs.</description><author>S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das</author><pubDate>Tue, 02 Jan 2024 17:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01313v1</guid></item><item><title>OpenVoice: Versatile Instant Voice Cloning</title><link>http://arxiv.org/abs/2312.01479v5</link><description>We introduce OpenVoice, a versatile voice cloning approach that requires onlya short audio clip from the reference speaker to replicate their voice andgenerate speech in multiple languages. OpenVoice represents a significantadvancement in addressing the following open challenges in the field: 1)Flexible Voice Style Control. OpenVoice enables granular control over voicestyles, including emotion, accent, rhythm, pauses, and intonation, in additionto replicating the tone color of the reference speaker. The voice styles arenot directly copied from and constrained by the style of the reference speaker.Previous approaches lacked the ability to flexibly manipulate voice stylesafter cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieveszero-shot cross-lingual voice cloning for languages not included in themassive-speaker training set. Unlike previous approaches, which typicallyrequire extensive massive-speaker multi-lingual (MSML) dataset for alllanguages, OpenVoice can clone voices into a new language without anymassive-speaker training data for that language. OpenVoice is alsocomputationally efficient, costing tens of times less than commerciallyavailable APIs that offer even inferior performance. To foster further researchin the field, we have made the source code and trained model publiclyaccessible. We also provide qualitative results in our demo website. Prior toits public release, our internal version of OpenVoice was used tens of millionsof times by users worldwide between May and October 2023, serving as thebackend of MyShell.</description><author>Zengyi Qin, Wenliang Zhao, Xumin Yu, Xin Sun</author><pubDate>Tue, 02 Jan 2024 17:45:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01479v5</guid></item><item><title>Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces</title><link>http://arxiv.org/abs/2401.01306v1</link><description>In this work we present deep learning implementations of two populartheoretical constrained optimization algorithms in infinite dimensional Hilbertspaces, namely, the penalty and the augmented Lagrangian methods. We test thesealgorithms on some toy problems originating in either calculus of variations orphysics. We demonstrate that both methods are able to produce decentapproximations for the test problems and are comparable in terms of differenterrors. Leveraging the common occurrence of the Lagrange multiplier update rulebeing computationally less expensive than solving subproblems in the penaltymethod, we achieve significant speedups in cases when the output of theconstraint function is itself a function.</description><author>Pinak Mandal</author><pubDate>Tue, 02 Jan 2024 17:32:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01306v1</guid></item><item><title>Experimental Validation of Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles</title><link>http://arxiv.org/abs/2401.01304v1</link><description>In this paper, we validate the performance of the a sensor fusion-basedGlobal Navigation Satellite System (GNSS) spoofing attack detection frameworkfor Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSSreceiver, along with Inertial Measurement Unit (IMU) is used. The detectionframework incorporates two strategies: The first strategy involves comparingthe predicted location shift, which is the distance traveled between twoconsecutive timestamps, with the inertial sensor-based location shift. For thispurpose, data from low-cost in-vehicle inertial sensors such as theaccelerometer and gyroscope sensor are fused and fed into a long short-termmemory (LSTM) neural network. The second strategy employs a Random-Forestsupervised machine learning model to detect and classify turns, distinguishingbetween left and right turns using the output from the steering angle sensor.In experiments, two types of spoofing attack models: turn-by-turn and wrongturn are simulated. These spoofing attacks are modeled as SQL injectionattacks, where, upon successful implementation, the navigation system perceivesinjected spoofed location information as legitimate while being unable todetect legitimate GNSS signals. Importantly, the IMU data remains uncompromisedthroughout the spoofing attack. To test the effectiveness of the detectionframework, experiments are conducted in Tuscaloosa, AL, mimicking urban roadstructures. The results demonstrate the framework's ability to detect varioussophisticated GNSS spoofing attacks, even including slow position driftingattacks. Overall, the experimental results showcase the robustness and efficacyof the sensor fusion-based spoofing attack detection approach in safeguardingAVs against GNSS spoofing threats.</description><author>Sagar Dasgupta, Kazi Hassan Shakib, Mizanur Rahman</author><pubDate>Tue, 02 Jan 2024 17:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01304v1</guid></item><item><title>Integrating Edges into U-Net Models with Explainable Activation Maps for Brain Tumor Segmentation using MR Images</title><link>http://arxiv.org/abs/2401.01303v1</link><description>Manual delineation of tumor regions from magnetic resonance (MR) images istime-consuming, requires an expert, and is prone to human error. In recentyears, deep learning models have been the go-to approach for the segmentationof brain tumors. U-Net and its' variants for semantic segmentation of medicalimages have achieved good results in the literature. However, U-Net and its'variants tend to over-segment tumor regions and may not accurately segment thetumor edges. The edges of the tumor are as important as the tumor regions foraccurate diagnosis, surgical precision, and treatment planning. In the proposedwork, the authors aim to extract edges from the ground truth using aderivative-like filter followed by edge reconstruction to obtain an edge groundtruth in addition to the brain tumor ground truth. Utilizing both groundtruths, the author studies several U-Net and its' variant architectures withand without tumor edges ground truth as a target along with the tumor groundtruth for brain tumor segmentation. The author used the BraTS2020 benchmarkdataset to perform the study and the results are tabulated for the dice andHausdorff95 metrics. The mean and median metrics are calculated for the wholetumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to thebaseline U-Net and its variants, the models that learned edges along with thetumor regions performed well in core tumor regions in both training andvalidation datasets. The improved performance of edge-trained models trained onbaseline models like U-Net and V-Net achieved performance similar to baselinestate-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-targettrained models are capable of generating edge maps that can be useful fortreatment planning. Additionally, for further explainability of the results,the activation map generated by the hybrid MR-U-Net has been studied.</description><author>Subin Sahayam, Umarani Jayaraman</author><pubDate>Tue, 02 Jan 2024 17:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01303v1</guid></item><item><title>Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models</title><link>http://arxiv.org/abs/2401.01301v1</link><description>Large language models (LLMs) have the potential to transform the practice oflaw, but this potential is threatened by the presence of legal hallucinations-- responses from these models that are not consistent with legal facts. Weinvestigate the extent of these hallucinations using an original suite of legalqueries, comparing LLMs' responses to structured legal metadata and examiningtheir consistency. Our work makes four key contributions: (1) We develop atypology of legal hallucinations, providing a conceptual framework for futureresearch in this area. (2) We find that legal hallucinations are alarminglyprevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% withLlama 2, when these models are asked specific, verifiable questions aboutrandom federal court cases. (3) We illustrate that LLMs often fail to correct auser's incorrect legal assumptions in a contra-factual question setup. (4) Weprovide evidence that LLMs cannot always predict, or do not always know, whenthey are producing legal hallucinations. Taken together, these findings cautionagainst the rapid and unsupervised integration of popular LLMs into legaltasks. Even experienced lawyers must remain wary of legal hallucinations, andthe risks are highest for those who stand to benefit from LLMs the most -- prose litigants or those without access to traditional legal resources.</description><author>Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho</author><pubDate>Tue, 02 Jan 2024 17:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01301v1</guid></item><item><title>KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching</title><link>http://arxiv.org/abs/2312.17050v2</link><description>Dual-lens super-resolution (SR) is a practical scenario for reference (Ref)based SR by utilizing the telephoto image (Ref) to assist the super-resolutionof the low-resolution wide-angle image (LR input). Different from generalRefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV)area. However, current dual-lens SR methods rarely utilize these specificcharacteristics and directly perform dense matching between the LR input andRef. Due to the resolution gap between LR and Ref, the matching may miss thebest-matched candidate and destroy the consistent structures in the overlappedFoV area. Different from them, we propose to first align the Ref with thecenter region (namely the overlapped FoV area) of the LR input by combiningglobal warping and local warping to make the aligned Ref be sharp andconsistent. Then, we formulate the aligned Ref and LR center as value-keypairs, and the corner region of the LR is formulated as queries. In this way,we propose a kernel-free matching strategy by matching between the LR-corner(query) and LR-center (key) regions, and the corresponding aligned Ref (value)can be warped to the corner region of the target. Our kernel-free matchingstrategy avoids the resolution gap between LR and Ref, which makes our networkhave better generalization ability. In addition, we construct a DuSR-Realdataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.Experiments on three datasets demonstrate that our method outperforms thesecond-best method by a large margin. Our code and dataset are available athttps://github.com/ZifanCui/KeDuSR.</description><author>Huanjing Yue, Zifan Cui, Kun Li, Jingyu Yang</author><pubDate>Tue, 02 Jan 2024 17:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17050v2</guid></item><item><title>Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control</title><link>http://arxiv.org/abs/2312.05332v3</link><description>In this paper, we introduce a new class of parameterized controllers, drawinginspiration from Model Predictive Control (MPC). The controller resembles aQuadratic Programming (QP) solver of a linear MPC problem, with the parametersof the controller being trained via Deep Reinforcement Learning (DRL) ratherthan derived from system models. This approach addresses the limitations ofcommon controllers with Multi-Layer Perceptron (MLP) or other general neuralnetwork architecture used in DRL, in terms of verifiability and performanceguarantees, and the learned controllers possess verifiable properties likepersistent feasibility and asymptotic stability akin to MPC. On the other hand,numerical examples illustrate that the proposed controller empirically matchesMPC and MLP controllers in terms of control performance and has superiorrobustness against modeling uncertainty and noises. Furthermore, the proposedcontroller is significantly more computationally efficient compared to MPC andrequires fewer parameters to learn than MLP controllers. Real-world experimentson vehicle drift maneuvering task demonstrate the potential of thesecontrollers for robotics and other demanding control tasks.</description><author>Yiwen Lu, Zishuo Li, Yihan Zhou, Na Li, Yilin Mo</author><pubDate>Tue, 02 Jan 2024 17:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05332v3</guid></item><item><title>Efficient Sparse Least Absolute Deviation Regression with Differential Privacy</title><link>http://arxiv.org/abs/2401.01294v1</link><description>In recent years, privacy-preserving machine learning algorithms haveattracted increasing attention because of their important applications in manyscientific fields. However, in the literature, most privacy-preservingalgorithms demand learning objectives to be strongly convex and Lipschitzsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,quantile/least absolute loss). In this work, we aim to develop a fastprivacy-preserving learning solution for a sparse robust regression problem.Our learning loss consists of a robust least absolute loss and an $\ell_1$sparse penalty term. To fast solve the non-smooth loss under a given privacybudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)algorithm for least absolute deviation regression. Our algorithm achieves afast estimation by reformulating the sparse LAD problem as a penalized leastsquare estimation problem and adopts a three-stage noise injection to guaranteethe $(\epsilon,\delta)$-differential privacy. We show that our algorithm canachieve better privacy and statistical accuracy trade-off compared with thestate-of-the-art privacy-preserving regression algorithms. In the end, weconduct experiments to verify the efficiency of our proposed FRAPPE algorithm.</description><author>Weidong Liu, Xiaojun Mao, Xiaofei Zhang, Xin Zhang</author><pubDate>Tue, 02 Jan 2024 17:13:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01294v1</guid></item><item><title>AI Alignment: A Comprehensive Survey</title><link>http://arxiv.org/abs/2310.19852v3</link><description>AI alignment aims to make AI systems behave in line with human intentions andvalues. As AI systems grow more capable, so do risks from misalignment. Toprovide a comprehensive and up-to-date overview of the alignment field, in thissurvey, we delve into the core concepts, methodology, and practice ofalignment. First, we identify four principles as the key objectives of AIalignment: Robustness, Interpretability, Controllability, and Ethicality(RICE). Guided by these four principles, we outline the landscape of currentalignment research and decompose them into two key components: forwardalignment and backward alignment. The former aims to make AI systems alignedvia alignment training, while the latter aims to gain evidence about thesystems' alignment and govern them appropriately to avoid exacerbatingmisalignment risks. On forward alignment, we discuss techniques for learningfrom feedback and learning under distribution shift. On backward alignment, wediscuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com)which features tutorials, collections of papers, blog posts, and otherresources.</description><author>Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao</author><pubDate>Tue, 02 Jan 2024 17:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19852v3</guid></item><item><title>Physics-informed Generalizable Wireless Channel Modeling with Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges</title><link>http://arxiv.org/abs/2401.01288v1</link><description>Channel modeling is fundamental in advancing wireless systems and has thusattracted considerable research focus. Recent trends have seen a growingreliance on data-driven techniques to facilitate the modeling process and yieldaccurate channel predictions. In this work, we first provide a concise overviewof data-driven channel modeling methods, highlighting their limitations.Subsequently, we introduce the concept and advantages of physics-informedneural network (PINN)-based modeling and a summary of recent contributions inthis area. Our findings demonstrate that PINN-based approaches in channelmodeling exhibit promising attributes such as generalizability,interpretability, and robustness. We offer a comprehensive architecture forPINN methodology, designed to inform and inspire future model development. Acase-study of our recent work on precise indoor channel prediction withsemantic segmentation and deep learning is presented. The study concludes byaddressing the challenges faced and suggesting potential research directions inthis field.</description><author>Ethan Zhu, Haijian Sun, Mingyue Ji</author><pubDate>Tue, 02 Jan 2024 16:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01288v1</guid></item><item><title>A Comprehensive Study of Knowledge Editing for Large Language Models</title><link>http://arxiv.org/abs/2401.01286v1</link><description>Large Language Models (LLMs) have shown extraordinary capabilities inunderstanding and generating text that closely mirrors human communication.However, a primary limitation lies in the significant computational demandsduring training, arising from their extensive parameterization. This challengeis further intensified by the dynamic nature of the world, necessitatingfrequent updates to LLMs to correct outdated information or integrate newknowledge, thereby ensuring their continued relevance. Note that manyapplications demand continual model adjustments post-training to addressdeficiencies or undesirable behaviors. There is an increasing interest inefficient, lightweight methods for on-the-fly model modifications. To this end,recent years have seen a burgeoning in the techniques of knowledge editing forLLMs, which aim to efficiently modify LLMs' behaviors within specific domainswhile preserving overall performance across various inputs. In this paper, wefirst define the knowledge editing problem and then provide a comprehensivereview of cutting-edge approaches. Drawing inspiration from educational andcognitive research theories, we propose a unified categorization criterion thatclassifies knowledge editing methods into three groups: resorting to externalknowledge, merging knowledge into the model, and editing intrinsic knowledge.Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensiveempirical evaluation of representative knowledge editing approaches.Additionally, we provide an in-depth analysis of knowledge location, which canprovide a deeper understanding of the knowledge structures inherent withinLLMs. Finally, we discuss several potential applications of knowledge editing,outlining its broad and impactful implications.</description><author>Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen</author><pubDate>Tue, 02 Jan 2024 16:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01286v1</guid></item><item><title>Quality and Quantity of Machine Translation References for Automated Metrics</title><link>http://arxiv.org/abs/2401.01283v1</link><description>Automatic machine translation metrics often use human translations todetermine the quality system translations. Common wisdom in the field dictatesthat the human references should be of very high quality. However, there are nocost-benefit analyses that could be used to guide practitioners who plan tocollect references for machine translation evaluation. We find thathigher-quality references lead to better metric correlations with humans at thesegment-level. Having up to 7 references per segment and taking their averagehelps all metrics. Interestingly, the references from vendors of differentqualities can be mixed together and improve metric success. Higher qualityreferences, however, cost more to create and we frame this as an optimizationproblem: given a specific budget, what references should be collected tomaximize metric success. These findings can be used by evaluators of sharedtasks when references need to be created under a certain budget.</description><author>Vilém Zouhar, Ondřej Bojar</author><pubDate>Tue, 02 Jan 2024 16:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01283v1</guid></item><item><title>GEqO: ML-Accelerated Semantic Equivalence Detection</title><link>http://arxiv.org/abs/2401.01280v1</link><description>Large scale analytics engines have become a core dependency for moderndata-driven enterprises to derive business insights and drive actions. Theseengines support a large number of analytic jobs processing huge volumes of dataon a daily basis, and workloads are often inundated with overlappingcomputations across multiple jobs. Reusing common computation is crucial forefficient cluster resource utilization and reducing job execution time.Detecting common computation is the first and key step for reducing thiscomputational redundancy. However, detecting equivalence on large-scaleanalytics engines requires efficient and scalable solutions that are fullyautomated. In addition, to maximize computation reuse, equivalence needs to bedetected at the semantic level instead of just the syntactic level (i.e., theability to detect semantic equivalence of seemingly different-looking queries).Unfortunately, existing solutions fall short of satisfying these requirements. In this paper, we take a major step towards filling this gap by proposingGEqO, a portable and lightweight machine-learning-based framework forefficiently identifying semantically equivalent computations at scale. GEqOintroduces two machine-learning-based filters that quickly prune outnonequivalent subexpressions and employs a semi-supervised learning feedbackloop to iteratively improve its model with an intelligent sampling mechanism.Further, with its novel database-agnostic featurization method, GEqO cantransfer the learning from one workload and database to another. Our extensiveempirical evaluation shows that, on TPC-DS-like queries, GEqO yieldssignificant performance gains-up to 200x faster than automated verifiers-andfinds up to 2x more equivalences than optimizer and signature-based equivalencedetection approaches.</description><author>Brandon Haynes, Rana Alotaibi, Anna Pavlenko, Jyoti Leeka, Alekh Jindal, Yuanyuan Tian</author><pubDate>Tue, 02 Jan 2024 16:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01280v1</guid></item><item><title>CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation</title><link>http://arxiv.org/abs/2401.01275v1</link><description>Recently, the advent of large language models (LLMs) has revolutionizedgenerative agents. Among them, Role-Playing Conversational Agents (RPCAs)attract considerable attention due to their ability to emotionally engageusers. However, the absence of a comprehensive benchmark impedes progress inthis field. To bridge this gap, we introduce CharacterEval, a Chinese benchmarkfor comprehensive RPCA assessment, complemented by a tailored high-qualitydataset. The dataset comprises 1,785 multi-turn role-playing dialogues,encompassing 23,020 examples and featuring 77 characters derived from Chinesenovels and scripts. It was carefully constructed, beginning with initialdialogue extraction via GPT-4, followed by rigorous human-led quality control,and enhanced with in-depth character profiles sourced from Baidu Baike.CharacterEval employs a multifaceted evaluation approach, encompassing thirteentargeted metrics on four dimensions. Comprehensive experiments on CharacterEvaldemonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 inChinese role-playing conversation. Source code, data source and reward modelwill be publicly accessible at https://github.com/morecry/CharacterEval.</description><author>Quan Tu, Shilong Fan, Zihang Tian, Rui Yan</author><pubDate>Tue, 02 Jan 2024 16:20:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01275v1</guid></item><item><title>Learning-based agricultural management in partially observable environments subject to climate variability</title><link>http://arxiv.org/abs/2401.01273v1</link><description>Agricultural management, with a particular focus on fertilization strategies,holds a central role in shaping crop yield, economic profitability, andenvironmental sustainability. While conventional guidelines offer valuableinsights, their efficacy diminishes when confronted with extreme weatherconditions, such as heatwaves and droughts. In this study, we introduce aninnovative framework that integrates Deep Reinforcement Learning (DRL) withRecurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we trainan intelligent agent to master optimal nitrogen fertilization management.Through a series of simulation experiments conducted on corn crops in Iowa, wecompare Partially Observable Markov Decision Process (POMDP) models with MarkovDecision Process (MDP) models. Our research underscores the advantages ofutilizing sequential observations in developing more efficient nitrogen inputpolicies. Additionally, we explore the impact of climate variability,particularly during extreme weather events, on agricultural outcomes andmanagement. Our findings demonstrate the adaptability of fertilization policiesto varying climate conditions. Notably, a fixed policy exhibits resilience inthe face of minor climate fluctuations, leading to commendable corn yields,cost-effectiveness, and environmental conservation. However, our studyilluminates the need for agent retraining to acquire new optimal policies underextreme weather events. This research charts a promising course towardadaptable fertilization strategies that can seamlessly align with dynamicclimate scenarios, ultimately contributing to the optimization of cropmanagement practices.</description><author>Zhaoan Wang, Shaoping Xiao, Junchao Li, Jun Wang</author><pubDate>Tue, 02 Jan 2024 16:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01273v1</guid></item><item><title>MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic Communication</title><link>http://arxiv.org/abs/2401.01272v1</link><description>Vector quantization-based image semantic communication systems havesuccessfully boosted transmission efficiency, but face a challenge withconflicting requirements between codebook design and digital constellationmodulation. Traditional codebooks need a wide index range, while modulationfavors few discrete states. To address this, we propose a multilevel generativesemantic communication system with a two-stage training framework. In the firststage, we train a high-quality codebook, using a multi-head octonary codebook(MOC) to compress the index range. We also integrate a residual vectorquantization (RVQ) mechanism for effective multilevel communication. In thesecond stage, a noise reduction block (NRB) based on Swin Transformer isintroduced, coupled with the multilevel codebook from the first stage, servingas a high-quality semantic knowledge base (SKB) for generative featurerestoration. Experimental results highlight MOC-RVQ's superior performance overmethods like BPG or JPEG, even without channel error correction coding.</description><author>Yingbin Zhou, Yaping Sun, Guanying Chen, Xiaodong Xu, Hao Chen, Binhong Huang, Shuguang Cui, Ping Zhang</author><pubDate>Tue, 02 Jan 2024 16:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01272v1</guid></item><item><title>Optimal Rates of Kernel Ridge Regression under Source Condition in Large Dimensions</title><link>http://arxiv.org/abs/2401.01270v1</link><description>Motivated by the studies of neural networks (e.g.,the neural tangent kerneltheory), we perform a study on the large-dimensional behavior of kernel ridgeregression (KRR) where the sample size $n \asymp d^{\gamma}$ for some $\gamma &gt;0$. Given an RKHS $\mathcal{H}$ associated with an inner product kernel definedon the sphere $\mathbb{S}^{d}$, we suppose that the true function $f_{\rho}^{*}\in [\mathcal{H}]^{s}$, the interpolation space of $\mathcal{H}$ with sourcecondition $s&gt;0$. We first determined the exact order (both upper and lowerbound) of the generalization error of kernel ridge regression for the optimallychosen regularization parameter $\lambda$. We then further showed that when$0&lt;s\le1$, KRR is minimax optimal; and when $s&gt;1$, KRR is not minimax optimal(a.k.a. he saturation effect). Our results illustrate that the curves of ratevarying along $\gamma$ exhibit the periodic plateau behavior and the multipledescent behavior and show how the curves evolve with $s&gt;0$. Interestingly, ourwork provides a unified viewpoint of several recent works on kernel regressionin the large-dimensional setting, which correspond to $s=0$ and $s=1$respectively.</description><author>Haobo Zhang, Yicheng Li, Weihao Lu, Qian Lin</author><pubDate>Tue, 02 Jan 2024 16:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01270v1</guid></item><item><title>LLbezpeky: Leveraging Large Language Models for Vulnerability Detection</title><link>http://arxiv.org/abs/2401.01269v1</link><description>Despite the continued research and progress in building secure systems,Android applications continue to be ridden with vulnerabilities, necessitatingeffective detection methods. Current strategies involving static and dynamicanalysis tools come with limitations like overwhelming number of falsepositives and limited scope of analysis which make either difficult to adopt.Over the past years, machine learning based approaches have been extensivelyexplored for vulnerability detection, but its real-world applicability isconstrained by data requirements and feature engineering challenges. LargeLanguage Models (LLMs), with their vast parameters, have shown tremendouspotential in understanding semnatics in human as well as programming languages.We dive into the efficacy of LLMs for detecting vulnerabilities in the contextof Android security. We focus on building an AI-driven workflow to assistdevelopers in identifying and rectifying vulnerabilities. Our experiments showthat LLMs outperform our expectations in finding issues within applicationscorrectly flagging insecure apps in 91.67% of cases in the Ghera benchmark. Weuse inferences from our experiments towards building a robust and actionablevulnerability detection system and demonstrate its effectiveness. Ourexperiments also shed light on how different various simple configurations canaffect the True Positive (TP) and False Positive (FP) rates.</description><author>Noble Saji Mathews, Yelizaveta Brus, Yousra Aafer, Mei Nagappan, Shane McIntosh</author><pubDate>Tue, 02 Jan 2024 16:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01269v1</guid></item><item><title>$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy</title><link>http://arxiv.org/abs/2401.01268v1</link><description>In deep learning, classification tasks are formalized as optimizationproblems solved via the minimization of the cross-entropy. However, recentadvancements in the design of objective functions allow the $f$-divergencemeasure to generalize the formulation of the optimization problem forclassification. With this goal in mind, we adopt a Bayesian perspective andformulate the classification task as a maximum a posteriori probabilityproblem. We propose a class of objective functions based on the variationalrepresentation of the $f$-divergence, from which we extract a list of fiveposterior probability estimators leveraging well-known $f$-divergences. Inaddition, driven by the challenge of improving the state-of-the-art approach,we propose a bottom-up method that leads us to the formulation of a newobjective function (and posterior probability estimator) corresponding to anovel $f$-divergence referred to as shifted log (SL). First, we theoreticallyprove the convergence property of the posterior probability estimators. Then,we numerically test the set of proposed objective functions in threeapplication scenarios: toy examples, image data sets, and signaldetection/decoding problems. The analyzed tasks demonstrate the effectivenessof the proposed estimators and that the SL divergence achieves the highestclassification accuracy in almost all the scenarios.</description><author>Nicola Novello, Andrea M. Tonello</author><pubDate>Tue, 02 Jan 2024 16:14:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01268v1</guid></item><item><title>Tuned Compositional Feature Replays for Efficient Stream Learning</title><link>http://arxiv.org/abs/2104.02206v8</link><description>Our brains extract durable, generalizable knowledge from transientexperiences of the world. Artificial neural networks come nowhere close to thisability. When tasked with learning to classify objects by training onnon-repeating video frames in temporal order (online stream learning), modelsthat learn well from shuffled datasets catastrophically forget old knowledgeupon learning new stimuli. We propose a new continual learning algorithm,Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting byreplaying feature maps reconstructed by combining generic parts. CRUMBconcatenates trainable and re-usable "memory block" vectors to compositionallyreconstruct feature map tensors in convolutional neural networks. Storing theindices of memory blocks used to reconstruct new stimuli enables memories ofthe stimuli to be replayed during later tasks. This reconstruction mechanismalso primes the neural network to minimize catastrophic forgetting by biasingit towards attending to information about object shapes more than informationabout image textures, and stabilizes the network during stream learning byproviding a shared feature-level basis for all training examples. Theseproperties allow CRUMB to outperform an otherwise identical algorithm thatstores and replays raw images, while occupying only 3.6% as much memory. Westress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.To address the limited number of existing online stream learning datasets, weintroduce 2 new benchmarks by adapting existing datasets for stream learning.With only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigatescatastrophic forgetting more effectively than the state-of-the-art. Our code isavailable at https://github.com/MorganBDT/crumb.git.</description><author>Morgan B. Talbot, Rushikesh Zawar, Rohil Badkundri, Mengmi Zhang, Gabriel Kreiman</author><pubDate>Tue, 02 Jan 2024 16:12:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.02206v8</guid></item><item><title>Optimal Synthesis of Finite State Machines with Universal Gates using Evolutionary Algorithm</title><link>http://arxiv.org/abs/2401.01265v1</link><description>This work presents an optimization method for the synthesis of finite statemachines. The focus is on the reduction in the on-chip area and the cost of thecircuit. A list of finite state machines from MCNC91 benchmark circuits havebeen evolved using Cartesian Genetic Programming. On the average, almost 30% ofreduction in the total number of gates has been achieved. The effects of someparameters on the evolutionary process have also been discussed in the paper.</description><author>Noor Ullah, Khawaja M. Yahya, Irfan Ahmed</author><pubDate>Tue, 02 Jan 2024 16:11:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01265v1</guid></item><item><title>Fairness Certification for Natural Language Processing and Large Language Models</title><link>http://arxiv.org/abs/2401.01262v1</link><description>Natural Language Processing (NLP) plays an important role in our daily lives,particularly due to the enormous progress of Large Language Models (LLM).However, NLP has many fairness-critical use cases, e.g., as an expert system inrecruitment or as an LLM-based tutor in education. Since NLP is based on humanlanguage, potentially harmful biases can diffuse into NLP systems and produceunfair results, discriminate against minorities or generate legal issues.Hence, it is important to develop a fairness certification for NLP approaches.We follow a qualitative research approach towards a fairness certification forNLP. In particular, we have reviewed a large body of literature on algorithmicfairness, and we have conducted semi-structured expert interviews with a widerange of experts from that area. We have systematically devised six fairnesscriteria for NLP, which can be further refined into 18 sub-categories. Ourcriteria offer a foundation for operationalizing and testing processes tocertify fairness, both from the perspective of the auditor and the auditedorganization.</description><author>Vincent Freiberger, Erik Buchmann</author><pubDate>Tue, 02 Jan 2024 16:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01262v1</guid></item><item><title>Do Concept Bottleneck Models Obey Locality?</title><link>http://arxiv.org/abs/2401.01259v1</link><description>Concept-based learning improves a deep learning model's interpretability byexplaining its predictions via human-understandable concepts. Deep learningmodels trained under this paradigm heavily rely on the assumption that neuralnetworks can learn to predict the presence or absence of a given conceptindependently of other concepts. Recent work, however, strongly suggests thatthis assumption may fail to hold in Concept Bottleneck Models (CBMs), aquintessential family of concept-based interpretable architectures. In thispaper, we investigate whether CBMs correctly capture the degree of conditionalindependence across concepts when such concepts are localised both spatially,by having their values entirely defined by a fixed subset of features, andsemantically, by having their values correlated with only a fixed subset ofpredefined concepts. To understand locality, we analyse how changes to featuresoutside of a concept's spatial or semantic locality impact concept predictions.Our results suggest that even in well-defined scenarios where the presence of aconcept is localised to a fixed feature subspace, or whose semantics arecorrelated to a small subset of other concepts, CBMs fail to learn thislocality. These results cast doubt upon the quality of concept representationslearnt by CBMs and strongly suggest that concept-based explanations may befragile to changes outside their localities.</description><author>Naveen Raman, Mateo Espinosa Zarlenga, Juyeon Heo, Mateja Jamnik</author><pubDate>Tue, 02 Jan 2024 16:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01259v1</guid></item><item><title>Towards Model-Free LQR Control over Rate-Limited Channels</title><link>http://arxiv.org/abs/2401.01258v1</link><description>Given the success of model-free methods for control design in many problemsettings, it is natural to ask how things will change if realisticcommunication channels are utilized for the transmission of gradients orpolicies. While the resulting problem has analogies with the formulationsstudied under the rubric of networked control systems, the rich literature inthat area has typically assumed that the model of the system is known. As astep towards bridging the fields of model-free control design and networkedcontrol systems, we ask: \textit{Is it possible to solve basic control problems- such as the linear quadratic regulator (LQR) problem - in a model-free mannerover a rate-limited channel?} Toward answering this question, we study asetting where a worker agent transmits quantized policy gradients (of the LQRcost) to a server over a noiseless channel with a finite bit-rate. We propose anew algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), andprove that above a certain finite threshold bit-rate, \texttt{AQGD} guaranteesexponentially fast convergence to the globally optimal policy, with \textit{nodeterioration of the exponent relative to the unquantized setting}. Moregenerally, our approach reveals the benefits of adaptive quantization inpreserving fast linear convergence rates, and, as such, may be of independentinterest to the literature on compressed optimization.</description><author>Aritra Mitra, Lintao Ye, Vijay Gupta</author><pubDate>Tue, 02 Jan 2024 15:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01258v1</guid></item><item><title>VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM</title><link>http://arxiv.org/abs/2401.01256v1</link><description>The recent innovations and breakthroughs in diffusion models havesignificantly expanded the possibilities of generating high-quality videos forthe given prompts. Most existing works tackle the single-scene scenario withonly one video event occurring in a single background. Extending to generatemulti-scene videos nevertheless is not trivial and necessitates to nicelymanage the logic in between while preserving the consistent visual appearanceof key content across video scenes. In this paper, we propose a novelframework, namely VideoDrafter, for content-consistent multi-scene videogeneration. Technically, VideoDrafter leverages Large Language Models (LLM) toconvert the input prompt into comprehensive multi-scene script that benefitsfrom the logical knowledge learnt by LLM. The script for each scene includes aprompt describing the event, the foreground/background entities, as well ascamera movement. VideoDrafter identifies the common entities throughout thescript and asks LLM to detail each entity. The resultant entity description isthen fed into a text-to-image model to generate a reference image for eachentity. Finally, VideoDrafter outputs a multi-scene video by generating eachscene video via a diffusion process that takes the reference images, thedescriptive prompt of the event and camera movement into account. The diffusionmodel incorporates the reference images as the condition and alignment tostrengthen the content consistency of multi-scene videos. Extensive experimentsdemonstrate that VideoDrafter outperforms the SOTA video generation models interms of visual quality, content consistency, and user preference.</description><author>Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei</author><pubDate>Tue, 02 Jan 2024 15:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01256v1</guid></item><item><title>tf.data service: A Case for Disaggregating ML Input Data Processing</title><link>http://arxiv.org/abs/2210.14826v3</link><description>Machine learning (ML) computations commonly execute on expensive specializedhardware, such as GPUs and TPUs, which provide high FLOPs andperformance-per-watt. For cost efficiency, it is essential to keep theseaccelerators highly utilized. This requires preprocessing input data at therate at which the accelerators can ingest and perform ML computations on thedata. To avoid data stalls, the host CPU and RAM required for input dataprocessing per accelerator core used for ML computations varies across jobs.Hence, the traditional approach of processing input data on ML acceleratorhosts with a fixed hardware ratio leads to either under-utilizing theaccelerators or the host CPU and RAM. In this paper, we address these concernsby building a disaggregated ML data processing system. We present tf.data service, an open-source disaggregated input dataprocessing service built on top of tf.data in TensorFlow. We show thatdisaggregating data preprocessing has three key advantages for large-scale MLtraining jobs. First, the service can horizontally scale-out to right-sizeCPU/RAM host resources for data processing in each job, saving 32x trainingtime and 26x cost, on average. Second, the service can share ephemeralpreprocessed data results across jobs, to optimize CPU usage and reduceredundant computations. Finally, the service supports coordinated reads, atechnique that avoids stragglers due to different input sizes in distributedtraining, reducing training time by 2.2x, on average. Our design is inspired bylessons learned from deploying tf.data service in production, includingrelaxing data visitation guarantees without impacting model accuracy.</description><author>Andrew Audibert, Yang Chen, Dan Graur, Ana Klimovic, Jiri Simsa, Chandramohan A. Thekkath</author><pubDate>Tue, 02 Jan 2024 15:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.14826v3</guid></item><item><title>Recovering 3D Human Mesh from Monocular Images: A Survey</title><link>http://arxiv.org/abs/2203.01923v6</link><description>Estimating human pose and shape from monocular images is a long-standingproblem in computer vision. Since the release of statistical body models, 3Dhuman mesh recovery has been drawing broader attention. With the same goal ofobtaining well-aligned and physically plausible mesh results, two paradigmshave been developed to overcome challenges in the 2D-to-3D lifting process: i)an optimization-based paradigm, where different data terms and regularizationterms are exploited as optimization objectives; and ii) a regression-basedparadigm, where deep learning techniques are embraced to solve the problem inan end-to-end fashion. Meanwhile, continuous efforts are devoted to improvingthe quality of 3D mesh labels for a wide range of datasets. Though remarkableprogress has been achieved in the past decade, the task is still challengingdue to flexible body motions, diverse appearances, complex environments, andinsufficient in-the-wild annotations. To the best of our knowledge, this is thefirst survey that focuses on the task of monocular 3D human mesh recovery. Westart with the introduction of body models and then elaborate recoveryframeworks and training objectives by providing in-depth analyses of theirstrengths and weaknesses. We also summarize datasets, evaluation metrics, andbenchmark results. Open issues and future directions are discussed in the end,hoping to motivate researchers and facilitate their research in this area. Aregularly updated project page can be found athttps://github.com/tinatiansjz/hmr-survey.</description><author>Yating Tian, Hongwen Zhang, Yebin Liu, Limin Wang</author><pubDate>Tue, 02 Jan 2024 15:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.01923v6</guid></item><item><title>Multimodality and Attention Increase Alignment in Natural Language Prediction Between Humans and Computational Models</title><link>http://arxiv.org/abs/2308.06035v3</link><description>The potential of multimodal generative artificial intelligence (mAI) toreplicate human grounded language understanding, including the pragmatic,context-rich aspects of communication, remains to be clarified. Humans areknown to use salient multimodal features, such as visual cues, to facilitatethe processing of upcoming words. Correspondingly, multimodal computationalmodels can integrate visual and linguistic data using a visual attentionmechanism to assign next-word probabilities. To test whether these processesalign, we tasked both human participants (N = 200) as well as severalstate-of-the-art computational models with evaluating the predictability offorthcoming words after viewing short audio-only or audio-visual clips withspeech. During the task, the model's attention weights were recorded and humanattention was indexed via eye tracking. Results show that predictabilityestimates from humans aligned more closely with scores generated frommultimodal models vs. their unimodal counterparts. Furthermore, including anattention mechanism doubled alignment with human judgments when visual andlinguistic context facilitated predictions. In these cases, the model'sattention patches and human eye tracking significantly overlapped. Our resultsindicate that improved modeling of naturalistic language processing in mAI doesnot merely depend on training diet but can be driven by multimodality incombination with attention-based architectures. Humans and computational modelsalike can leverage the predictive constraints of multimodal information byattending to relevant features in the input.</description><author>Viktor Kewenig, Andrew Lampinen, Samuel A. Nastase, Christopher Edwards, Quitterie Lacome DEstalenx, Akilles Rechardt, Jeremy I Skipper, Gabriella Vigliocco</author><pubDate>Tue, 02 Jan 2024 15:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06035v3</guid></item><item><title>ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter</title><link>http://arxiv.org/abs/2305.07490v5</link><description>In recent years, advancements in large language models have been remarkable,with models such as ChatGPT demonstrating exceptional proficiency in diverselinguistic tasks. The pre-training of large models with billions of parameters,poses a formidable challenge, primarily due to the scarcity of datasets of acommensurate scale for effective training. Nevertheless, innovative strategieshave emerged, including methods to fine-tune these pre-trained models usingfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despitetheir potential in various domains, these models remain limited in theirunderstanding of artistic imagery. They have yet to fully grasp the intricatenuances of art images or to provide an objective articulation of the emotionsthey evoke, in a manner akin to human perception. This work introducesArtGPT-4, a pioneering large vision-language model tailored to address thedeficiencies of contemporary models in artistic comprehension. ArtGPT-4underwent training on image-text pairs utilizing a Tesla A100 device in a mere2 hours, with a dataset comprising approximately 0.52M entries. Impressively,the model can render images with an artistic-understanding and convey theemotions they inspire, mirroring human interpretation. Additionally, this workpresents a unique dataset designed to evaluate the efficacy of vision-languagemodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-artperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded theestablished benchmarks introduced in This study, lagging behind professionalartists' descriptions by a negligible 0.15 points on a 6-point scale. The codeand the pre-trained model are accessible inhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.</description><author>Zhengqing Yuan, Xinyi Wang, Kun Wang, Lichao Sun</author><pubDate>Tue, 02 Jan 2024 15:29:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07490v5</guid></item><item><title>Deep Learning-Based Computational Model for Disease Identification in Cocoa Pods (Theobroma cacao L.)</title><link>http://arxiv.org/abs/2401.01247v1</link><description>The early identification of diseases in cocoa pods is an important task toguarantee the production of high-quality cocoa. The use of artificialintelligence techniques such as machine learning, computer vision and deeplearning are promising solutions to help identify and classify diseases incocoa pods. In this paper we introduce the development and evaluation of a deeplearning computational model applied to the identification of diseases in cocoapods, focusing on "monilia" and "black pod" diseases. An exhaustive review ofstate-of-the-art of computational models was carried out, based on scientificarticles related to the identification of plant diseases using computer visionand deep learning techniques. As a result of the search, EfficientDet-Lite4, anefficient and lightweight model for object detection, was selected. A dataset,including images of both healthy and diseased cocoa pods, has been utilized totrain the model to detect and pinpoint disease manifestations with considerableaccuracy. Significant enhancements in the model training and evaluationdemonstrate the capability of recognizing and classifying diseases throughimage analysis. Furthermore, the functionalities of the model were integratedinto an Android native mobile with an user-friendly interface, allowing toyounger or inexperienced farmers a fast and accuracy identification of healthstatus of cocoa pods</description><author>Darlyn Buenaño Vera, Byron Oviedo, Washington Chiriboga Casanova, Cristian Zambrano-Vega</author><pubDate>Tue, 02 Jan 2024 15:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01247v1</guid></item><item><title>Temporal Adaptive RGBT Tracking with Modality Prompt</title><link>http://arxiv.org/abs/2401.01244v1</link><description>RGBT tracking has been widely used in various fields such as robotics,surveillance processing, and autonomous driving. Existing RGBT trackers fullyexplore the spatial information between the template and the search region andlocate the target based on the appearance matching results. However, these RGBTtrackers have very limited exploitation of temporal information, eitherignoring temporal information or exploiting it through online sampling andtraining. The former struggles to cope with the object state changes, while thelatter neglects the correlation between spatial and temporal information. Toalleviate these limitations, we propose a novel Temporal Adaptive RGBT Trackingframework, named as TATrack. TATrack has a spatio-temporal two-stream structureand captures temporal information by an online updated template, where thetwo-stream structure refers to the multi-modal feature extraction andcross-modal interaction for the initial template and the online update templaterespectively. TATrack contributes to comprehensively exploit spatio-temporalinformation and multi-modal information for target localization. In addition,we design a spatio-temporal interaction (STI) mechanism that bridges twobranches and enables cross-modal interaction to span longer time scales.Extensive experiments on three popular RGBT tracking benchmarks show that ourmethod achieves state-of-the-art performance, while running at real-time speed.</description><author>Hongyu Wang, Xiaotao Liu, Yifan Li, Meng Sun, Dian Yuan, Jing Liu</author><pubDate>Tue, 02 Jan 2024 15:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01244v1</guid></item><item><title>Contrastive Sequential Interaction Network Learning on Co-Evolving Riemannian Spaces</title><link>http://arxiv.org/abs/2401.01243v1</link><description>The sequential interaction network usually find itself in a variety ofapplications, e.g., recommender system. Herein, inferring future interaction isof fundamental importance, and previous efforts are mainly focused on thedynamics in the classic zero-curvature Euclidean space. Despite the promisingresults achieved by previous methods, a range of significant issues stilllargely remains open: On the bipartite nature, is it appropriate to place userand item nodes in one identical space regardless of their inherent difference?On the network dynamics, instead of a fixed curvature space, will therepresentation spaces evolve when new interactions arrive continuously? On thelearning paradigm, can we get rid of the label information costly to acquire?To address the aforementioned issues, we propose a novel Contrastive model forSequential Interaction Network learning on Co-Evolving RiEmannian spaces,CSINCERE. To the best of our knowledge, we are the first to introduce a coupleof co-evolving representation spaces, rather than a single or static space, andpropose a co-contrastive learning for the sequential interaction network. InCSINCERE, we formulate a Cross-Space Aggregation for message-passing acrossrepresentation spaces of different Riemannian geometries, and design a NeuralCurvature Estimator based on Ricci curvatures for modeling the space evolvementover time. Thereafter, we present a Reweighed Co-Contrast between the temporalviews of the sequential network, so that the couple of Riemannian spacesinteract with each other for the interaction prediction without labels.Empirical results on 5 public datasets show the superiority of CSINCERE overthe state-of-the-art methods.</description><author>Li Sun, Junda Ye, Jiawei Zhang, Yong Yang, Mingsheng Liu, Feiyang Wang, Philip S. Yu</author><pubDate>Tue, 02 Jan 2024 15:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01243v1</guid></item><item><title>Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning</title><link>http://arxiv.org/abs/2401.01242v1</link><description>Broadband infrastructure owners do not always know how their customers areconnected in the local networks, which are structured as rooted trees. A recentstudy is able to infer the topology of a local network using discrete timeseries data from the leaves of the tree (customers). In this study we propose acontrastive approach for learning a binary event encoder from continuous timeseries data. As a preliminary result, we show that our approach has somepotential in learning a valuable encoder.</description><author>Tobias Engelhardt Rasmussen, Siv Sørensen</author><pubDate>Tue, 02 Jan 2024 15:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01242v1</guid></item><item><title>Data-Efficient Multimodal Fusion on a Single GPU</title><link>http://arxiv.org/abs/2312.10144v2</link><description>The goal of multimodal alignment is to learn a single latent space that isshared between multimodal inputs. The most powerful models in this space havebeen trained using massive datasets of paired inputs and large-scalecomputational resources, making them prohibitively expensive to train in manypractical scenarios. We surmise that existing unimodal encoders pre-trained onlarge amounts of unimodal data should provide an effective bootstrap to createmultimodal models from unimodal ones at much lower costs. We therefore proposeFuseMix, a multimodal augmentation scheme that operates on the latent spaces ofarbitrary pre-trained unimodal encoders. Using FuseMix for multimodalalignment, we achieve competitive performance -- and in certain casesoutperform state-of-the art methods -- in both image-text and audio-textretrieval, with orders of magnitude less compute and data: for example, weoutperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.Additionally, we show how our method can be applied to convert pre-trainedtext-to-image generative models into audio-to-image ones. Code is available at:https://github.com/layer6ai-labs/fusemix.</description><author>Noël Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims Volkovs</author><pubDate>Tue, 02 Jan 2024 15:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10144v2</guid></item><item><title>Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis</title><link>http://arxiv.org/abs/2311.15218v4</link><description>The application of Machine learning to finance has become a familiarapproach, even more so in stock market forecasting. The stock market is highlyvolatile, and huge amounts of data are generated every minute globally. Theextraction of effective intelligence from this data is of critical importance.However, a collaboration of numerical stock data with qualitative text data canbe a challenging task. In this work, we accomplish this by providing anunprecedented, publicly available dataset with technical and fundamental dataand sentiment that we gathered from news archives, TV news captions, radiotranscripts, tweets, daily financial newspapers, etc. The text data entriesused for sentiment extraction total more than 1.4 Million. The dataset consistsof daily entries from January 2018 to December 2022 for eight companiesrepresenting diverse industrial sectors and the Dow Jones Industrial Average(DJIA) as a whole. Holistic Fundamental and Technical data is provided trainingready for Model learning and deployment. Most importantly, the data generatedcould be used for incremental online learning with real-time data pointsretrieved daily since no stagnant data was utilized. All the data was retiredfrom APIs or self-designed robust information retrieval technologies withextremely low latency and zero monetary cost. These adaptable technologiesfacilitate data extraction for any stock. Moreover, the utilization ofSpearman's rank correlation over real-time data, linking stock returns withsentiment analysis has produced noteworthy results for the DJIA and the eightother stocks, achieving accuracy levels surpassing 60%. The dataset is madeavailable at https://github.com/batking24/Huge-Stock-Dataset.</description><author>Sai Akash Bathini, Dagli Cihan</author><pubDate>Tue, 02 Jan 2024 15:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15218v4</guid></item><item><title>Graph Elimination Networks</title><link>http://arxiv.org/abs/2401.01233v1</link><description>Graph Neural Networks (GNNs) are widely applied across various domains, yetthey perform poorly in deep layers. Existing research typically attributes thisproblem to node over-smoothing, where node representations becomeindistinguishable after multiple rounds of propagation. In this paper, we delveinto the neighborhood propagation mechanism of GNNs and discover that the realroot cause of GNNs' performance degradation in deep layers lies in ineffectiveneighborhood feature propagation. This propagation leads to an exponentialgrowth of a node's current representation at every propagation step, making itextremely challenging to capture valuable dependencies between long-distancenodes. To address this issue, we introduce Graph Elimination Networks (GENs),which employ a specific algorithm to eliminate redundancies during neighborhoodpropagation. We demonstrate that GENs can enhance nodes' perception of distantneighborhoods and extend the depth of network propagation. Extensiveexperiments show that GENs outperform the state-of-the-art methods on variousgraph-level and node-level datasets.</description><author>Shuo Wang, Ge Cheng, Yun Zhang</author><pubDate>Tue, 02 Jan 2024 14:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01233v1</guid></item><item><title>Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning</title><link>http://arxiv.org/abs/2401.01232v1</link><description>Graphs are typical non-Euclidean data of complex structures. In recent years,Riemannian graph representation learning has emerged as an exciting alternativeto Euclidean ones. However, Riemannian methods are still in an early stage:most of them present a single curvature (radius) regardless of structuralcomplexity, suffer from numerical instability due to theexponential/logarithmic map, and lack the ability to capture motif regularity.In light of the issues above, we propose the problem of \emph{Motif-awareRiemannian Graph Representation Learning}, seeking a numerically stable encoderto capture motif regularity in a diverse-curvature manifold without labels. Tothis end, we present a novel Motif-aware Riemannian model withGenerative-Contrastive learning (MotifRGC), which conducts a minmax game inRiemannian manifold in a self-supervised manner. First, we propose a new typeof Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifoldby a product layer with the diversified factor, and replace theexponential/logarithmic map by a stable kernel layer. Second, we introduce amotif-aware Riemannian generative-contrastive learning to capture motifregularity in the constructed manifold and learn motif-aware noderepresentation without external labels. Empirical results show the superiorityof MofitRGC.</description><author>Li Sun, Zhenhao Huang, Zixi Wang, Feiyang Wang, Hao Peng, Philip Yu</author><pubDate>Tue, 02 Jan 2024 14:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01232v1</guid></item><item><title>Lossy Image Compression with Conditional Diffusion Models</title><link>http://arxiv.org/abs/2209.06950v8</link><description>This paper outlines an end-to-end optimized lossy image compression frameworkusing diffusion generative models. The approach relies on the transform codingparadigm, where an image is mapped into a latent space for entropy coding and,from there, mapped back to the data space for reconstruction. In contrast toVAE-based neural compression, where the (mean) decoder is a deterministicneural network, our decoder is a conditional diffusion model. Our approach thusintroduces an additional ``content'' latent variable on which the reversediffusion process is conditioned and uses this variable to store informationabout the image. The remaining ``texture'' variables characterizing thediffusion process are synthesized at decoding time. We show that the model'sperformance can be tuned toward perceptual metrics of interest. Our extensiveexperiments involving multiple datasets and image quality assessment metricsshow that our approach yields stronger reported FID scores than the GAN-basedmodel, while also yielding competitive performance with VAE-based models inseveral distortion metrics. Furthermore, training the diffusion with$\mathcal{X}$-parameterization enables high-quality reconstructions in only ahandful of decoding steps, greatly affecting the model's practicality. Our codeis available at: \url{https://github.com/buggyyang/CDC_compression}</description><author>Ruihan Yang, Stephan Mandt</author><pubDate>Tue, 02 Jan 2024 14:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.06950v8</guid></item><item><title>Structured Packing in LLM Training Improves Long Context Utilization</title><link>http://arxiv.org/abs/2312.17296v2</link><description>Recent advances in long-context Large Language Models (LCLMs) have generatedsignificant interest, especially in applications such as querying scientificresearch papers. However, their potential is often limited by inadequatecontext utilization. We identify the absence of long-range semanticdependencies in typical training data as a primary hindrance. To address this,we delve into the benefits of frequently incorporating related documents intotraining inputs. Using the inherent directory structure of code data as asource of training examples, we demonstrate improvements in perplexity, evenfor tasks unrelated to coding. Building on these findings, but with a broaderfocus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is aninnovative method for creating training examples by using a retrieval method tocollate the most mutually relevant documents into a single training context.Our results indicate that \method{} enhances model performance and can be usedto train large models to utilize long contexts better. We validate our resultsby training a large $3$B model, showing both perplexity improvements and betterlong-context performance on downstream tasks.</description><author>Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Henryk Michalewski, Łukasz Kuciński, Piotr Miłoś</author><pubDate>Tue, 02 Jan 2024 14:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17296v2</guid></item><item><title>SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling</title><link>http://arxiv.org/abs/2308.04365v5</link><description>Causal inference is a crucial goal of science, enabling researchers to arriveat meaningful conclusions regarding the predictions of hypotheticalinterventions using observational data. Path models, Structural Equation Models(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means tounambiguously specify assumptions regarding the causal structure underlying aphenomenon. Unlike DAGs, which make very few assumptions about the functionaland parametric form, SEM assumes linearity. This can result in functionalmisspecification which prevents researchers from undertaking reliable effectsize estimation. In contrast, we propose Super Learner Equation Modeling, apath modeling technique integrating machine learning Super Learner ensembles.We empirically demonstrate its ability to provide consistent and unbiasedestimates of causal effects, its competitive performance for linear models whencompared with SEM, and highlight its superiority over SEM when dealing withnon-linear relationships. We provide open-source code, and a tutorial notebookwith example usage, accentuating the easy-to-use nature of the method.</description><author>Matthew J. Vowels</author><pubDate>Tue, 02 Jan 2024 14:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04365v5</guid></item><item><title>IdentiFace : A VGG Based Multimodal Facial Biometric System</title><link>http://arxiv.org/abs/2401.01227v1</link><description>The development of facial biometric systems has contributed greatly to thedevelopment of the computer vision field. Nowadays, there's always a need todevelop a multimodal system that combines multiple biometric traits in anefficient, meaningful way. In this paper, we introduce "IdentiFace" which is amultimodal facial biometric system that combines the core of facial recognitionwith some of the most important soft biometric traits such as gender, faceshape, and emotion. We also focused on developing the system using only VGG-16inspired architecture with minor changes across different subsystems. Thisunification allows for simpler integration across modalities. It makes iteasier to interpret the learned features between the tasks which gives a goodindication about the decision-making process across the facial modalities andpotential connection. For the recognition problem, we acquired a 99.2% testaccuracy for five classes with high intra-class variations using data collectedfrom the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on thepublic dataset[2] in the gender recognition problem. We were also able toachieve a testing accuracy of 88.03% in the face-shape problem using thecelebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracyof 66.13% in the emotion task which is considered a very acceptable accuracycompared to related work on the FER2013 dataset[4].</description><author>Mahmoud Rabea, Hanya Ahmed, Sohaila Mahmoud, Nourhan Sayed</author><pubDate>Tue, 02 Jan 2024 14:36:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01227v1</guid></item><item><title>Era Splitting -- Invariant Learning for Decision Trees</title><link>http://arxiv.org/abs/2309.14496v3</link><description>Real-life machine learning problems exhibit distributional shifts in the datafrom one time to another or from on place to another. This behavior is beyondthe scope of the traditional empirical risk minimization paradigm, whichassumes i.i.d. distribution of data over time and across locations. Theemerging field of out-of-distribution (OOD) generalization addresses thisreality with new theory and algorithms which incorporate environmental, orera-wise information into the algorithms. So far, most research has beenfocused on linear models and/or neural networks. In this research we developtwo new splitting criteria for decision trees, which allow us to apply ideasfrom OOD generalization research to decision tree models, including randomforest and gradient-boosting decision trees. The new splitting criteria useera-wise information associated with each data point to allow tree-based modelsto find split points that are optimal across all disjoint eras in the data,instead of optimal over the entire data set pooled together, which is thedefault setting. In this paper we describe the problem setup in the context offinancial markets. We describe the new splitting criteria in detail and developunique experiments to showcase the benefits of these new criteria, whichimprove metrics in our experiments out-of-sample. The new criteria areincorporated into the a state-of-the-art gradient boosted decision tree modelin the Scikit-Learn code base, which is made freely available.</description><author>Timothy DeLise</author><pubDate>Tue, 02 Jan 2024 14:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14496v3</guid></item><item><title>Hybrid Pooling and Convolutional Network for Improving Accuracy and Training Convergence Speed in Object Detection</title><link>http://arxiv.org/abs/2401.01134v1</link><description>This paper introduces HPC-Net, a high-precision and rapidly convergent objectdetection network.</description><author>Shiwen Zhao, Wei Wang, Junhui Hou, Hai Wu</author><pubDate>Tue, 02 Jan 2024 10:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01134v1</guid></item><item><title>Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections</title><link>http://arxiv.org/abs/2309.16741v2</link><description>Financial firms commonly process and store billions of time-series data,generated continuously and at a high frequency. To support efficient datastorage and retrieval, specialized time-series databases and systems haveemerged. These databases support indexing and querying of time-series by aconstrained Structured Query Language(SQL)-like format to enable queries like"Stocks with monthly price returns greater than 5%", and expressed in rigidformats. However, such queries do not capture the intrinsic complexity of highdimensional time-series data, which can often be better described by images orlanguage (e.g., "A stock in low volatility regime"). Moreover, the requiredstorage, computational time, and retrieval complexity to search in thetime-series space are often non-trivial. In this paper, we propose anddemonstrate a framework to store multi-modal data for financial time-series ina lower-dimensional latent space using deep encoders, such that the latentspace projections capture not only the time series trends but also otherdesirable information or properties of the financial time-series data (such asprice volatility). Moreover, our approach allows user-friendly queryinterfaces, enabling natural language text or sketches of time-series, forwhich we have developed intuitive interfaces. We demonstrate the advantages ofour method in terms of computational efficiency and accuracy on real historicaldata as well as synthetic data, and highlight the utility of latent-spaceprojections in the storage and retrieval of financial time-series data withintuitive query modalities.</description><author>Tom Bamford, Andrea Coletta, Elizabeth Fons, Sriram Gopalakrishnan, Svitlana Vyetrenko, Tucker Balch, Manuela Veloso</author><pubDate>Tue, 02 Jan 2024 10:18:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16741v2</guid></item><item><title>Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of Agents</title><link>http://arxiv.org/abs/2309.17207v2</link><description>Memory Gym presents a suite of 2D partially observable environments, namelyMortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmarkmemory capabilities in decision-making agents. These environments, originallywith finite tasks, are expanded into innovative, endless formats, mirroring theescalating challenges of cumulative memory games such as ``I packed my bag''.This progression in task design shifts the focus from merely assessing sampleefficiency to also probing the levels of memory effectiveness in dynamic,prolonged scenarios. To address the gap in available memory-based DeepReinforcement Learning baselines, we introduce an implementation thatintegrates Transformer-XL (TrXL) with Proximal Policy Optimization. Thisapproach utilizes TrXL as a form of episodic memory, employing a sliding windowtechnique. Our comparative study between the Gated Recurrent Unit (GRU) andTrXL reveals varied performances across different settings. TrXL, on the finiteenvironments, demonstrates superior sample efficiency in Mystery Path andoutperforms in Mortar Mayhem. However, GRU is more efficient on SearingSpotlights. Most notably, in all endless tasks, GRU makes a remarkableresurgence, consistently outperforming TrXL by significant margins. Website andSource Code: \url{https://github.com/MarcoMeter/endless-memory-gym/}</description><author>Marco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss</author><pubDate>Tue, 02 Jan 2024 10:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17207v2</guid></item><item><title>Joint Generative Modeling of Scene Graphs and Images via Diffusion Models</title><link>http://arxiv.org/abs/2401.01130v1</link><description>In this paper, we present a novel generative task: joint scene graph - imagegeneration. While previous works have explored image generation conditioned onscene graphs or layouts, our task is distinctive and important as it involvesgenerating scene graphs themselves unconditionally from noise, enablingefficient and interpretable control for image generation. Our task ischallenging, requiring the generation of plausible scene graphs withheterogeneous attributes for nodes (objects) and edges (relations amongobjects), including continuous object bounding boxes and discrete object andrelation categories. We introduce a novel diffusion model, DiffuseSG, thatjointly models the adjacency matrix along with heterogeneous node and edgeattributes. We explore various types of encodings for the categorical data,relaxing it into a continuous space. With a graph transformer being thedenoiser, DiffuseSG successively denoises the scene graph representation in acontinuous space and discretizes the final representation to generate the cleanscene graph. Additionally, we introduce an IoU regularization to enhance theempirical performance. Our model significantly outperforms existing methods inscene graph generation on the Visual Genome and COCO-Stuff datasets, both onstandard and newly introduced metrics that better capture the problemcomplexity. Moreover, we demonstrate the additional benefits of our model intwo downstream applications: 1) excelling in a series of scene graph completiontasks, and 2) improving scene graph detection models by using extra trainingsamples generated from DiffuseSG.</description><author>Bicheng Xu, Qi Yan, Renjie Liao, Lele Wang, Leonid Sigal</author><pubDate>Tue, 02 Jan 2024 10:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01130v1</guid></item><item><title>Content Bias in Deep Learning Image Age Approximation: A new Approach Towards better Explainability</title><link>http://arxiv.org/abs/2310.02067v2</link><description>In the context of temporal image forensics, it is not evident that a neuralnetwork, trained on images from different time-slots (classes), exploits solelyimage age related features. Usually, images taken in close temporal proximity(e.g., belonging to the same age class) share some common content properties.Such content bias can be exploited by a neural network. In this work, a novelapproach is proposed that evaluates the influence of image content. Thisapproach is verified using synthetic images (where content bias can be ruledout) with an age signal embedded. Based on the proposed approach, it is shownthat a deep learning approach proposed in the context of age classification ismost likely highly dependent on the image content. As a possiblecountermeasure, two different models from the field of image steganalysis,along with three different preprocessing techniques to increase thesignal-to-noise ratio (age signal to image content), are evaluated using theproposed method.</description><author>Robert Jöchl, Andreas Uhl</author><pubDate>Tue, 02 Jan 2024 10:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02067v2</guid></item><item><title>Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</title><link>http://arxiv.org/abs/2310.19923v2</link><description>Text embedding models have emerged as powerful tools for transformingsentences into fixed-sized feature vectors that encapsulate semanticinformation. While these models are essential for tasks like informationretrieval, semantic clustering, and text re-ranking, most existing open-sourcemodels, especially those built on architectures like BERT, struggle torepresent lengthy documents and often resort to truncation. One common approachto mitigate this challenge involves splitting documents into smaller paragraphsfor embedding. However, this strategy results in a much larger set of vectors,consequently leading to increased memory consumption and computationallyintensive vector searches with elevated latency. To address these challenges, we introduce Jina Embeddings 2, an open-sourcetext embedding model capable of accommodating up to 8192 tokens. This model isdesigned to transcend the conventional 512-token limit and adeptly process longdocuments. Jina Embeddings 2 not only achieves state-of-the-art performance ona range of embedding-related tasks in the MTEB benchmark but also matches theperformance of OpenAI's proprietary ada-002 model. Additionally, ourexperiments indicate that an extended context can enhance performance in taskssuch as NarrativeQA.</description><author>Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao</author><pubDate>Tue, 02 Jan 2024 10:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19923v2</guid></item><item><title>A Stochastic Analysis of the Linguistic Provenance of English Place Names</title><link>http://arxiv.org/abs/2312.12850v3</link><description>In English place name analysis, meanings are often derived from theresemblance of roots in place names to topographical features, proper namesand/or habitation terms in one of the languages that have had an influence onEnglish place names. The problem here is that it is sometimes difficult todetermine the base language to use to interpret the roots. The purpose of thispaper is to stochastically determine the resemblance between 18799 Englishplace names and 84687 place names from Ireland, Scotland, Wales, Denmark,Norway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each Englishplace name is ranked according to the extent to which it resembles place namesfrom the other countries, and this provides a basis for determining the likelylanguage to use to interpret the place name. A number of observations can bemade using the ranking provided. In particular, it is found that `Harlington'is the most archetypically English place name in the English sample, and `Anna'is the least. Furthermore, it is found that the place names in the non-Englishdatasets are most similar to Norwegian place names and least similar to Welshplace names.</description><author>Michael Dalvean</author><pubDate>Tue, 02 Jan 2024 10:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12850v3</guid></item><item><title>SSP: A Simple and Safe automatic Prompt engineering method towards realistic image synthesis on LVM</title><link>http://arxiv.org/abs/2401.01128v1</link><description>Recently, text-to-image (T2I) synthesis has undergone significantadvancements, particularly with the emergence of Large Language Models (LLM)and their enhancement in Large Vision Models (LVM), greatly enhancing theinstruction-following capabilities of traditional T2I models. Nevertheless,previous methods focus on improving generation quality but introduce unsafefactors into prompts. We explore that appending specific camera descriptions toprompts can enhance safety performance. Consequently, we propose a simple andsafe prompt engineering method (SSP) to improve image generation quality byproviding optimal camera descriptions. Specifically, we create a dataset frommulti-datasets as original prompts. To select the optimal camera, we design anoptimal camera matching approach and implement a classifier for originalprompts capable of automatically matching. Appending camera descriptions tooriginal prompts generates optimized prompts for further LVM image generation.Experiments demonstrate that SSP improves semantic consistency by an average of16% compared to others and safety metrics by 48.9%.</description><author>Weijin Cheng, Jianzhi Liu, Jiawen Deng, Fuji Ren</author><pubDate>Tue, 02 Jan 2024 09:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01128v1</guid></item><item><title>Accelerated First-Order Optimization under Nonlinear Constraints</title><link>http://arxiv.org/abs/2302.00316v2</link><description>We exploit analogies between first-order algorithms for constrainedoptimization and non-smooth dynamical systems to design a new class ofaccelerated first-order algorithms for constrained optimization. UnlikeFrank-Wolfe or projected gradients, these algorithms avoid optimization overthe entire feasible set at each iteration. We prove convergence to stationarypoints even in a nonconvex setting and we derive accelerated rates for theconvex setting both in continuous time, as well as in discrete time. Animportant property of these algorithms is that constraints are expressed interms of velocities instead of positions, which naturally leads to sparse,local and convex approximations of the feasible set (even if the feasible setis nonconvex). Thus, the complexity tends to grow mildly in the number ofdecision variables and in the number of constraints, which makes the algorithmssuitable for machine learning applications. We apply our algorithms to acompressed sensing and a sparse regression problem, showing that we can treatnonconvex $\ell^p$ constraints ($p&lt;1$) efficiently, while recoveringstate-of-the-art performance for $p=1$.</description><author>Michael Muehlebach, Michael I. Jordan</author><pubDate>Tue, 02 Jan 2024 09:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00316v2</guid></item><item><title>Pseudo-Hamiltonian neural networks for learning partial differential equations</title><link>http://arxiv.org/abs/2304.14374v3</link><description>Pseudo-Hamiltonian neural networks (PHNN) were recently introduced forlearning dynamical systems that can be modelled by ordinary differentialequations. In this paper, we extend the method to partial differentialequations. The resulting model is comprised of up to three neural networks,modelling terms representing conservation, dissipation and external forces, anddiscrete convolution operators that can either be learned or be given as input.We demonstrate numerically the superior performance of PHNN compared to abaseline model that models the full dynamics by a single neural network.Moreover, since the PHNN model consists of three parts with different physicalinterpretations, these can be studied separately to gain insight into thesystem, and the learned model is applicable also if external forces are removedor changed.</description><author>Sølve Eidnes, Kjetil Olsen Lye</author><pubDate>Tue, 02 Jan 2024 09:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14374v3</guid></item><item><title>Explainable Adaptive Tree-based Model Selection for Time Series Forecasting</title><link>http://arxiv.org/abs/2401.01124v1</link><description>Tree-based models have been successfully applied to a wide variety of tasks,including time series forecasting. They are increasingly in demand and widelyaccepted because of their comparatively high level of interpretability.However, many of them suffer from the overfitting problem, which limits theirapplication in real-world decision-making. This problem becomes even moresevere in online-forecasting settings where time series observations areincrementally acquired, and the distributions from which they are drawn maykeep changing over time. In this context, we propose a novel method for theonline selection of tree-based models using the TreeSHAP explainability methodin the task of time series forecasting. We start with an arbitrary set ofdifferent tree-based models. Then, we outline a performance-based ranking witha coherent design to make TreeSHAP able to specialize the tree-basedforecasters across different regions in the input time series. In thisframework, adequate model selection is performed online, adaptively followingdrift detection in the time series. In addition, explainability is supported onthree levels, namely online input importance, model selection, and model outputexplanation. An extensive empirical study on various real-world datasetsdemonstrates that our method achieves excellent or on-par results in comparisonto the state-of-the-art approaches as well as several baselines.</description><author>Matthias Jakobs, Amal Saadallah</author><pubDate>Tue, 02 Jan 2024 09:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01124v1</guid></item><item><title>In-depth analysis of music structure as a text network</title><link>http://arxiv.org/abs/2303.13631v2</link><description>Music, enchanting and poetic, permeates every corner of human civilization.Although music is not unfamiliar to people, our understanding of its essenceremains limited, and there is still no universally accepted scientificdescription. This is primarily due to music being regarded as a product of bothreason and emotion, making it difficult to define. In this article, we focus onthe fundamental elements of music and construct an evolutionary network fromthe perspective of music as a natural language, aligning with the statisticalcharacteristics of texts. Through this approach, we aim to comprehend thestructural differences in music across different periods, enabling a morescientific exploration of music. Relying on the advantages of structuralism, wecan concentrate on the relationships and order between the physical elements ofmusic, rather than getting entangled in the blurred boundaries of science andphilosophy. The scientific framework we present not only conforms to pastconclusions in music, but also serves as a bridge that connects music tonatural language processing and knowledge graphs.</description><author>Ping-Rui Tsai, Yen-Ting Chou, Nathan-Christopher Wang, Hui-Ling Chen, Hong-Yue Huang, Zih-Jia Luo, Tzay-Ming Hong</author><pubDate>Tue, 02 Jan 2024 09:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13631v2</guid></item><item><title>Collaborative Watermarking for Adversarial Speech Synthesis</title><link>http://arxiv.org/abs/2309.15224v2</link><description>Advances in neural speech synthesis have brought us technology that is notonly close to human naturalness, but is also capable of instant voice cloningwith little data, and is highly accessible with pre-trained models available.Naturally, the potential flood of generated content raises the need forsynthetic speech detection and watermarking. Recently, considerable researcheffort in synthetic speech detection has been related to the Automatic SpeakerVerification and Spoofing Countermeasure Challenge (ASVspoof), which focuses onpassive countermeasures. This paper takes a complementary view to generatedspeech detection: a synthesis system should make an active effort to watermarkthe generated speech in a way that aids detection by another machine, butremains transparent to a human listener. We propose a collaborative trainingscheme for synthetic speech watermarking and show that a HiFi-GAN neuralvocoder collaborating with the ASVspoof 2021 baseline countermeasure modelsconsistently improves detection performance over conventional classifiertraining. Furthermore, we demonstrate how collaborative training can be pairedwith augmentation strategies for added robustness against noise andtime-stretching. Finally, listening tests demonstrate that collaborativetraining has little adverse effect on perceptual quality of vocoded speech.</description><author>Lauri Juvela, Xin Wang</author><pubDate>Tue, 02 Jan 2024 09:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15224v2</guid></item><item><title>Adaptive learning of density ratios in RKHS</title><link>http://arxiv.org/abs/2307.16164v3</link><description>Estimating the ratio of two probability densities from finitely manyobservations of the densities is a central problem in machine learning andstatistics with applications in two-sample testing, divergence estimation,generative modeling, covariate shift adaptation, conditional densityestimation, and novelty detection. In this work, we analyze a large class ofdensity ratio estimation methods that minimize a regularized Bregman divergencebetween the true density ratio and a model in a reproducing kernel Hilbertspace (RKHS). We derive new finite-sample error bounds, and we propose aLepskii type parameter choice principle that minimizes the bounds withoutknowledge of the regularity of the density ratio. In the special case ofquadratic loss, our method adaptively achieves a minimax optimal error rate. Anumerical illustration is provided.</description><author>Werner Zellinger, Stefan Kindermann, Sergei V. Pereverzyev</author><pubDate>Tue, 02 Jan 2024 09:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16164v3</guid></item><item><title>Utilizing Autoregressive Networks for Full Lifecycle Data Generation of Rolling Bearings for RUL Prediction</title><link>http://arxiv.org/abs/2401.01119v1</link><description>The prediction of rolling bearing lifespan is of significant importance inindustrial production. However, the scarcity of high-quality, full lifecycledata has been a major constraint in achieving precise predictions. To addressthis challenge, this paper introduces the CVGAN model, a novel frameworkcapable of generating one-dimensional vibration signals in both horizontal andvertical directions, conditioned on historical vibration data and remaininguseful life. In addition, we propose an autoregressive generation method thatcan iteratively utilize previously generated vibration information to guide thegeneration of current signals. The effectiveness of the CVGAN model isvalidated through experiments conducted on the PHM 2012 dataset. Our findingsdemonstrate that the CVGAN model, in terms of both MMD and FID metrics,outperforms many advanced methods in both autoregressive and non-autoregressivegeneration modes. Notably, training using the full lifecycle data generated bythe CVGAN model significantly improves the performance of the predictive model.This result highlights the effectiveness of the data generated by CVGans inenhancing the predictive power of these models.</description><author>Junliang Wang, Qinghua Zhang, Guanhua Zhu, Guoxi Sun</author><pubDate>Tue, 02 Jan 2024 09:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01119v1</guid></item><item><title>Linear Discriminant Analysis with High-dimensional Mixed Variables</title><link>http://arxiv.org/abs/2112.07145v3</link><description>Datasets containing both categorical and continuous variables are frequentlyencountered in many areas, and with the rapid development of modern measurementtechnologies, the dimensions of these variables can be very high. Despite therecent progress made in modelling high-dimensional data for continuousvariables, there is a scarcity of methods that can deal with a mixed set ofvariables. To fill this gap, this paper develops a novel approach forclassifying high-dimensional observations with mixed variables. Our frameworkbuilds on a location model, in which the distributions of the continuousvariables conditional on categorical ones are assumed Gaussian. We overcome thechallenge of having to split data into exponentially many cells, orcombinations of the categorical variables, by kernel smoothing, and provide newperspectives for its bandwidth choice to ensure an analogue of Bochner's Lemma,which is different to the usual bias-variance tradeoff. We show that the twosets of parameters in our model can be separately estimated and providepenalized likelihood for their estimation. Results on the estimation accuracyand the misclassification rates are established, and the competitiveperformance of the proposed classifier is illustrated by extensive simulationand real data studies.</description><author>Binyan Jiang, Chenlei Leng, Cheng Wang, Zhongqing Yang, Xinyang Yu</author><pubDate>Tue, 02 Jan 2024 09:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.07145v3</guid></item><item><title>Pseudo-Hamiltonian system identification</title><link>http://arxiv.org/abs/2305.06920v2</link><description>Identifying the underlying dynamics of physical systems can be challengingwhen only provided with observational data. In this work, we consider systemsthat can be modelled as first-order ordinary differential equations. Byassuming a certain pseudo-Hamiltonian formulation, we are able to learn theanalytic terms of internal dynamics even if the model is trained on data wherethe system is affected by unknown damping and external disturbances. In caseswhere it is difficult to find analytic terms for the disturbances, a hybridmodel that uses a neural network to learn these can still accurately identifythe dynamics of the system as if under ideal conditions. This makes the modelsapplicable in some situations where other system identification models fail.Furthermore, we propose to use a fourth-order symmetric integration scheme inthe loss function and avoid actual integration in the training, and demonstrateon varied examples how this leads to increased performance on noisy data.</description><author>Sigurd Holmsen, Sølve Eidnes, Signe Riemer-Sørensen</author><pubDate>Tue, 02 Jan 2024 09:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06920v2</guid></item><item><title>Q-Refine: A Perceptual Quality Refiner for AI-Generated Image</title><link>http://arxiv.org/abs/2401.01117v1</link><description>With the rapid evolution of the Text-to-Image (T2I) model in recent years,their unsatisfactory generation result has become a challenge. However,uniformly refining AI-Generated Images (AIGIs) of different qualities not onlylimited optimization capabilities for low-quality AIGIs but also broughtnegative optimization to high-quality AIGIs. To address this issue, aquality-award refiner named Q-Refine is proposed. Based on the preference ofthe Human Visual System (HVS), Q-Refine uses the Image Quality Assessment (IQA)metric to guide the refining process for the first time, and modify images ofdifferent qualities through three adaptive pipelines. Experimental shows thatfor mainstream T2I models, Q-Refine can perform effective optimization to AIGIsof different qualities. It can be a general refiner to optimize AIGIs from bothfidelity and aesthetic quality levels, thus expanding the application of theT2I generation models.</description><author>Chunyi Li, Haoning Wu, Zicheng Zhang, Hongkun Hao, Kaiwei Zhang, Lei Bai, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai</author><pubDate>Tue, 02 Jan 2024 09:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01117v1</guid></item><item><title>Unveiling Comparative Sentiments in Vietnamese Product Reviews: A Sequential Classification Framework</title><link>http://arxiv.org/abs/2401.01108v1</link><description>Comparative opinion mining is a specialized field of sentiment analysis thataims to identify and extract sentiments expressed comparatively. To addressthis task, we propose an approach that consists of solving three sequentialsub-tasks: (i) identifying comparative sentence, i.e., if a sentence has acomparative meaning, (ii) extracting comparative elements, i.e., what arecomparison subjects, objects, aspects, predicates, and (iii) classifyingcomparison types which contribute to a deeper comprehension of user sentimentsin Vietnamese product reviews. Our method is ranked fifth at the VietnameseLanguage and Speech Processing (VLSP) 2023 challenge on Comparative OpinionMining (ComOM) from Vietnamese Product Reviews.</description><author>Ha Le, Bao Tran, Phuong Le, Tan Nguyen, Dac Nguyen, Ngoan Pham, Dang Huynh</author><pubDate>Tue, 02 Jan 2024 08:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01108v1</guid></item><item><title>CityPulse: Fine-Grained Assessment of Urban Change with Street View Time Series</title><link>http://arxiv.org/abs/2401.01107v1</link><description>Urban transformations have profound societal impact on both individuals andcommunities at large. Accurately assessing these shifts is essential forunderstanding their underlying causes and ensuring sustainable urban planning.Traditional measurements often encounter constraints in spatial and temporalgranularity, failing to capture real-time physical changes. While street viewimagery, capturing the heartbeat of urban spaces from a pedestrian point ofview, can add as a high-definition, up-to-date, and on-the-ground visual proxyof urban change. We curate the largest street view time series dataset to date,and propose an end-to-end change detection model to effectively capturephysical alterations in the built environment at scale. We demonstrate theeffectiveness of our proposed method by benchmark comparisons with previousliterature and implementing it at the city-wide level. Our approach has thepotential to supplement existing dataset and serve as a fine-grained andaccurate assessment of urban change.</description><author>Tianyuan Huang, Zejia Wu, Jiajun Wu, Jackelyn Hwang, Ram Rajagopal</author><pubDate>Tue, 02 Jan 2024 08:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01107v1</guid></item><item><title>AI-FLARES: Artificial Intelligence for the Analysis of Solar Flares Data</title><link>http://arxiv.org/abs/2401.01104v1</link><description>AI-FLARES (Artificial Intelligence for the Analysis of Solar Flares Data) isa research project funded by the Agenzia Spaziale Italiana and by the IstitutoNazionale di Astrofisica within the framework of the ``Attivit\`a di Studio perla Comunit\`a Scientifica Nazionale Sole, Sistema Solare ed Esopianeti''program. The topic addressed by this project was the development and use ofcomputational methods for the analysis of remote sensing space data associatedto solar flare emission. This paper overviews the main results obtained by theproject, with specific focus on solar flare forecasting, reconstruction ofmorphologies of the flaring sources, and interpretation of accelerationmechanisms triggered by solar flares.</description><author>Michele Piana, Federico Benvenuto, Anna Maria Massone, Cristina Campi, Sabrina Guastavino, Francesco Marchetti, Paolo Massa, Emma Perracchione, Anna Volpara</author><pubDate>Tue, 02 Jan 2024 08:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01104v1</guid></item><item><title>VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders</title><link>http://arxiv.org/abs/2309.01141v3</link><description>Large-scale text-to-image diffusion models have shown impressive capabilitiesfor generative tasks by leveraging strong vision-language alignment frompre-training. However, most vision-language discriminative tasks requireextensive fine-tuning on carefully-labeled datasets to acquire such alignment,with great cost in time and computing resources. In this work, we exploredirectly applying a pre-trained generative diffusion model to the challengingdiscriminative task of visual grounding without any fine-tuning and additionaltraining dataset. Specifically, we propose VGDiffZero, a simple yet effectivezero-shot visual grounding framework based on text-to-image diffusion models.We also design a comprehensive region-scoring method considering both globaland local contexts of each isolated proposal. Extensive experiments on RefCOCO,RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance onzero-shot visual grounding. Our code is available athttps://github.com/xuyang-liu16/VGDiffZero.</description><author>Xuyang Liu, Siteng Huang, Yachen Kang, Honggang Chen, Donglin Wang</author><pubDate>Tue, 02 Jan 2024 08:48:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01141v3</guid></item><item><title>Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing</title><link>http://arxiv.org/abs/2401.01102v1</link><description>Face recognition systems have raised concerns due to their vulnerability todifferent presentation attacks, and system security has become an increasinglycritical concern. Although many face anti-spoofing (FAS) methods perform wellin intra-dataset scenarios, their generalization remains a challenge. Toaddress this issue, some methods adopt domain adversarial training (DAT) toextract domain-invariant features. However, the competition between the encoderand the domain discriminator can cause the network to be difficult to train andconverge. In this paper, we propose a domain adversarial attack (DAA) method tomitigate the training instability problem by adding perturbations to the inputimages, which makes them indistinguishable across domains and enables domainalignment. Moreover, since models trained on limited data and types of attackscannot generalize well to unknown attacks, we propose a dual perceptual andgenerative knowledge distillation framework for face anti-spoofing thatutilizes pre-trained face-related models containing rich face priors.Specifically, we adopt two different face-related models as teachers totransfer knowledge to the target student model. The pre-trained teacher modelsare not from the task of face anti-spoofing but from perceptual and generativetasks, respectively, which implicitly augment the data. By combining both DAAand dual-teacher knowledge distillation, we develop a dual teacher knowledgedistillation with domain alignment framework (DTDA) for face anti-spoofing. Theadvantage of our proposed method has been verified through extensive ablationstudies and comparison with state-of-the-art methods on public datasets acrossmultiple protocols.</description><author>Zhe Kong, Wentian Zhang, Tao Wang, Kaihao Zhang, Yuexiang Li, Xiaoying Tang, Wenhan Luo</author><pubDate>Tue, 02 Jan 2024 08:45:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01102v1</guid></item><item><title>Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding</title><link>http://arxiv.org/abs/2401.01100v1</link><description>As a pivotal approach in machine learning and data science, manifold learningaims to uncover the intrinsic low-dimensional structure within complexnonlinear manifolds in high-dimensional space. By exploiting the manifoldhypothesis, various techniques for nonlinear dimension reduction have beendeveloped to facilitate visualization, classification, clustering, and gainingkey insights. Although existing manifold learning methods have achievedremarkable successes, they still suffer from extensive distortions incurred inthe global structure, which hinders the understanding of underlying patterns.Scalability issues also limit their applicability for handling large-scaledata. Here, we propose a scalable manifold learning (scML) method that canmanipulate large-scale and high-dimensional data in an efficient manner. Itstarts by seeking a set of landmarks to construct the low-dimensional skeletonof the entire data and then incorporates the non-landmarks into the landmarkspace based on the constrained locally linear embedding (CLLE). We empiricallyvalidated the effectiveness of scML on synthetic datasets and real-worldbenchmarks of different types, and applied it to analyze the single-celltranscriptomics and detect anomalies in electrocardiogram (ECG) signals. scMLscales well with increasing data sizes and exhibits promising performance inpreserving the global structure. The experiments demonstrate notable robustnessin embedding quality as the sample rate decreases.</description><author>Dehua Peng, Zhipeng Gui, Wenzhang Wei, Huayi Wu</author><pubDate>Tue, 02 Jan 2024 08:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01100v1</guid></item><item><title>Efficient Parallel Audio Generation using Group Masked Language Modeling</title><link>http://arxiv.org/abs/2401.01099v1</link><description>We present a fast and high-quality codec language model for parallel audiogeneration. While SoundStorm, a state-of-the-art parallel audio generationmodel, accelerates inference speed compared to autoregressive models, it stillsuffers from slow inference due to iterative sampling. To resolve this problem,we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative ParallelDecoding~(G-IPD) for efficient parallel audio generation. Both the training andsampling schemes enable the model to synthesize high-quality audio with a smallnumber of iterations by effectively modeling the group-wise conditionaldependencies. In addition, our model employs a cross-attention-basedarchitecture to capture the speaker style of the prompt voice and improvescomputational efficiency. Experimental results demonstrate that our proposedmodel outperforms the baselines in prompt-based audio generation.</description><author>Myeonghun Jeong, Minchan Kim, Joun Yeop Lee, Nam Soo Kim</author><pubDate>Tue, 02 Jan 2024 08:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01099v1</guid></item><item><title>Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework</title><link>http://arxiv.org/abs/2401.00744v2</link><description>Deep learning for Hamiltonian regression of quantum systems in materialresearch necessitates satisfying the covariance laws, among which achievingSO(3)-equivariance without sacrificing the expressiveness of networks remainsan elusive challenge due to the restriction to non-linear mappings onguaranteeing theoretical equivariance. To alleviate thecovariance-expressiveness dilemma, we propose a hybrid framework with twocascaded regression stages. The first stage, with a theoretically-guaranteedcovariant neural network modeling symmetry properties of 3D atom systems,yields theoretically covariant features and baseline Hamiltonian predictions,assisting the second stage in learning covariance. Meanwhile, the second stage,powered by a non-linear 3D graph Transformer network we propose for structuralmodeling of 3D atomic systems, refines the first stage's output as afine-grained prediction of Hamiltonians with better expressiveness capability.The combination of a theoretically covariant yet inevitably less expressivemodel with a highly expressive non-linear network enables precise,generalizable predictions while maintaining robust covariance under coordinatetransformations. Our method achieves state-of-the-art performance inHamiltonian prediction for electronic structure calculations, confirmed throughexperiments on five crystalline material databases.</description><author>Shi Yin, Xudong Zhu, Tianyu Gao, Haochong Zhang, Feng Wu, Lixin He</author><pubDate>Tue, 02 Jan 2024 08:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00744v2</guid></item><item><title>Robust single-particle cryo-EM image denoising and restoration</title><link>http://arxiv.org/abs/2401.01097v1</link><description>Cryo-electron microscopy (cryo-EM) has achieved near-atomic level resolutionof biomolecules by reconstructing 2D micrographs. However, the resolution andaccuracy of the reconstructed particles are significantly reduced due to theextremely low signal-to-noise ratio (SNR) and complex noise structure ofcryo-EM images. In this paper, we introduce a diffusion model withpost-processing framework to effectively denoise and restore single particlecryo-EM images. Our method outperforms the state-of-the-art (SOTA) denoisingmethods by effectively removing structural noise that has not been addressedbefore. Additionally, more accurate and high-resolution three-dimensionalreconstruction structures can be obtained from denoised cryo-EM images.</description><author>Jing Zhang, Tengfei Zhao, ShiYu Hu, Xin Zhao</author><pubDate>Tue, 02 Jan 2024 08:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01097v1</guid></item><item><title>Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots</title><link>http://arxiv.org/abs/2307.09330v3</link><description>We investigate the ability of individuals to visually validate statisticalmodels in terms of their fit to the data. While visual model estimation hasbeen studied extensively, visual model validation remains under-investigated.It is unknown how well people are able to visually validate models, and howtheir performance compares to visual and computational estimation. As astarting point, we conducted a study across two populations (crowdsourced andvolunteers). Participants had to both visually estimate (i.e, draw) andvisually validate (i.e., accept or reject) the frequently studied model ofaverages. Across both populations, the level of accuracy of the models thatwere considered valid was lower than the accuracy of the estimated models. Wefind that participants' validation and estimation were unbiased. Moreover,their natural critical point between accepting and rejecting a given mean valueis close to the boundary of its 95% confidence interval, indicating that thevisually perceived confidence interval corresponds to a common statisticalstandard. Our work contributes to the understanding of visual model validationand opens new research opportunities.</description><author>Daniel Braun, Ashley Suh, Remco Chang, Michael Gleicher, Tatiana von Landesberger</author><pubDate>Tue, 02 Jan 2024 08:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09330v3</guid></item><item><title>Exploring Hyperspectral Anomaly Detection with Human Vision: A Small Target Aware Detector</title><link>http://arxiv.org/abs/2401.01093v1</link><description>Hyperspectral anomaly detection (HAD) aims to localize pixel points whosespectral features differ from the background. HAD is essential in scenarios ofunknown or camouflaged target features, such as water quality monitoring, cropgrowth monitoring and camouflaged target detection, where prior information oftargets is difficult to obtain. Existing HAD methods aim to objectively detectand distinguish background and anomalous spectra, which can be achieved almosteffortlessly by human perception. However, the underlying processes of humanvisual perception are thought to be quite complex. In this paper, we analyzehyperspectral image (HSI) features under human visual perception, and transferthe solution process of HAD to the more robust feature space for the firsttime. Specifically, we propose a small target aware detector (STAD), whichintroduces saliency maps to capture HSI features closer to human visualperception. STAD not only extracts more anomalous representations, but alsoreduces the impact of low-confidence regions through a proposed small targetfilter (STF). Furthermore, considering the possibility of HAD algorithms beingapplied to edge devices, we propose a full connected network to convolutionalnetwork knowledge distillation strategy. It can learn the spectral and spatialfeatures of the HSI while lightening the network. We train the network on theHAD100 training set and validate the proposed method on the HAD100 test set.Our method provides a new solution space for HAD that is closer to human visualperception with high confidence. Sufficient experiments on real HSI withmultiple method comparisons demonstrate the excellent performance and uniquepotential of the proposed method. The code is available athttps://github.com/majitao-xd/STAD-HAD.</description><author>Jitao Ma, Weiying Xie, Yunsong Li</author><pubDate>Tue, 02 Jan 2024 08:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01093v1</guid></item><item><title>Quokka: An Open-source Large Language Model ChatBot for Material Science</title><link>http://arxiv.org/abs/2401.01089v1</link><description>This paper presents the development of a specialized chatbot for materialsscience, leveraging the Llama-2 language model, and continuing pre-training onthe expansive research articles in the materials science domain from the S2ORCdataset. The methodology involves an initial pretraining phase on over onemillion domain-specific papers, followed by an instruction-tuning process torefine the chatbot's capabilities. The chatbot is designed to assistresearchers, educators, and students by providing instant, context-awareresponses to queries in the field of materials science. We make the fourtrained checkpoints (7B, 13B, with or without chat ability) freely available tothe research community at https://github.com/Xianjun-Yang/Quokka.</description><author>Xianjun Yang, Stephen D. Wilson, Linda Petzold</author><pubDate>Tue, 02 Jan 2024 08:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01089v1</guid></item><item><title>Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach</title><link>http://arxiv.org/abs/2206.02164v2</link><description>Curb space is one of the busiest areas in urban road networks. Especially inrecent years, the rapid increase of ride-hailing trips and commercialdeliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy thelimited curb space that was designed and built decades ago. These PUDOs couldjam curbside utilization and disturb the mainline traffic flow, evidentlyleading to significant negative societal externalities. However, there is alack of an analytical framework that rigorously quantifies and mitigates thecongestion effect of PUDOs in the system view, particularly with little datasupport and involvement of confounding effects. To bridge this research gap,this paper develops a rigorous causal inference approach to estimate thecongestion effect of PUDOs on general regional networks. A causal graph is setto represent the spatio-temporal relationship between PUDOs and traffic speed,and a double and separated machine learning (DSML) method is proposed toquantify how PUDOs affect traffic congestion. Additionally, a re-routingformulation is developed and solved to encourage passenger walking and trafficflow re-routing to achieve system optimization. Numerical experiments areconducted using real-world data in the Manhattan area. On average, 100additional units of PUDOs in a region could reduce the traffic speed by 3.70and 4.54 mph on weekdays and weekends, respectively. Re-routing trips withPUDOs on curb space could respectively reduce the system-wide total travel timeby 2.44% and 2.12% in Midtown and Central Park on weekdays. Sensitivityanalysis is also conducted to demonstrate the effectiveness and robustness ofthe proposed framework.</description><author>Xiaohui Liu, Sean Qian, Hock-Hai Teo, Wei Ma</author><pubDate>Tue, 02 Jan 2024 08:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02164v2</guid></item><item><title>EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked Audio Gesture Modeling</title><link>http://arxiv.org/abs/2401.00374v2</link><description>We propose EMAGE, a framework to generate full-body human gestures from audioand masked gestures, encompassing facial, local body, hands, and globalmovements. To achieve this, we first introduce BEATX (BEAT-SMPLX-FLAME), a newmesh-level holistic co-speech dataset. BEATX combines MoShed SMPLX body withFLAME head parameters and further refines the modeling of head, neck, andfinger movements, offering a community-standardized, high-quality 3D motioncaptured dataset. EMAGE leverages masked body gesture priors during training toboost inference performance. It involves a Masked Audio Gesture Transformer,facilitating joint training on audio-to-gesture generation and masked gesturereconstruction to effectively encode audio and body gesture hints. Encoded bodyhints from masked gestures are then separately employed to generate facial andbody movements. Moreover, EMAGE adaptively merges speech features from theaudio's rhythm and content and utilizes four compositional VQ-VAEs to enhancethe results' fidelity and diversity. Experiments demonstrate that EMAGEgenerates holistic gestures with state-of-the-art performance and is flexiblein accepting predefined spatial-temporal gesture inputs, generating complete,audio-synchronized results. Our code and dataset are available athttps://pantomatrix.github.io/EMAGE/</description><author>Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Naoya Iwamoto, Bo Zheng, Michael J. Black</author><pubDate>Tue, 02 Jan 2024 08:05:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00374v2</guid></item><item><title>Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control</title><link>http://arxiv.org/abs/2401.01085v1</link><description>Revolutionized by the transformer architecture, natural language processing(NLP) has received unprecedented attention. While advancements in NLP modelshave led to extensive research into their backdoor vulnerabilities, thepotential for these advancements to introduce new backdoor threats remainsunexplored. This paper proposes Imperio, which harnesses the languageunderstanding capabilities of NLP models to enrich backdoor attacks. Imperioprovides a new model control experience. It empowers the adversary to controlthe victim model with arbitrary output through language-guided instructions.This is achieved using a language model to fuel a conditional triggergenerator, with optimizations designed to extend its language understandingcapabilities to backdoor instruction interpretation and execution. Ourexperiments across three datasets, five attacks, and nine defenses confirmImperio's effectiveness. It can produce contextually adaptive triggers fromtext descriptions and control the victim model with desired outputs, even inscenarios not encountered during training. The attack maintains a high successrate across complex datasets without compromising the accuracy of clean inputsand also exhibits resilience against representative defenses. The source codeis available at \url{https://khchow.com/Imperio}.</description><author>Ka-Ho Chow, Wenqi Wei, Lei Yu</author><pubDate>Tue, 02 Jan 2024 07:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01085v1</guid></item><item><title>Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction</title><link>http://arxiv.org/abs/2401.01084v1</link><description>Natural policy gradient (NPG) and its variants are widely-used policy searchmethods in reinforcement learning. Inspired by prior work, a new NPG variantcoined NPG-HM is developed in this paper, which utilizes the Hessian-aidedmomentum technique for variance reduction, while the sub-problem is solved viathe stochastic gradient descent method. It is shown that NPG-HM can achieve theglobal last iterate $\epsilon$-optimality with a sample complexity of$\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policygradient type methods under the generic Fisher non-degenerate policyparameterizations. The convergence analysis is built upon a relaxed weakgradient dominance property tailored for NPG under the compatible functionapproximation framework, as well as a neat way to decompose the error whenhandling the sub-problem. Moreover, numerical experiments on Mujoco-basedenvironments demonstrate the superior performance of NPG-HM over otherstate-of-the-art policy gradient methods.</description><author>Jie Feng, Ke Wei, Jinchi Chen</author><pubDate>Tue, 02 Jan 2024 07:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01084v1</guid></item><item><title>Aircraft Landing Time Prediction with Deep Learning on Trajectory Images</title><link>http://arxiv.org/abs/2401.01083v1</link><description>Aircraft landing time (ALT) prediction is crucial for air traffic management,especially for arrival aircraft sequencing on the runway. In this study, atrajectory image-based deep learning method is proposed to predict ALTs for theaircraft entering the research airspace that covers the Terminal ManeuveringArea (TMA). Specifically, the trajectories of all airborne arrival aircraftwithin the temporal capture window are used to generate an image with thetarget aircraft trajectory labeled as red and all background aircrafttrajectory labeled as blue. The trajectory images contain various information,including the aircraft position, speed, heading, relative distances, andarrival traffic flows. It enables us to use state-of-the-art deep convolutionneural networks for ALT modeling. We also use real-time runway usage obtainedfrom the trajectory data and the external information such as aircraft typesand weather conditions as additional inputs. Moreover, a convolution neuralnetwork (CNN) based module is designed for automatic holding-relatedfeaturizing, which takes the trajectory images, the leading aircraft holdingstatus, and their time and speed gap at the research airspace boundary as itsinputs. Its output is further fed into the final end-to-end ALT prediction. Theproposed ALT prediction approach is applied to Singapore Changi Airport (ICAOCode: WSSS) using one-month Automatic Dependent Surveillance-Broadcast (ADS-B)data from November 1 to November 30, 2022. Experimental results show that byintegrating the holding featurization, we can reduce the mean absolute error(MAE) from 82.23 seconds to 43.96 seconds, and achieve an average accuracy of96.1\%, with 79.4\% of the predictions errors being less than 60 seconds.</description><author>Liping Huang, Sheng Zhang, Yicheng Zhang, Yi Zhang, Yifang Yin</author><pubDate>Tue, 02 Jan 2024 07:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01083v1</guid></item><item><title>From Statistical Relational to Neurosymbolic Artificial Intelligence: a Survey</title><link>http://arxiv.org/abs/2108.11451v4</link><description>This survey explores the integration of learning and reasoning in twodifferent fields of artificial intelligence: neurosymbolic and statisticalrelational artificial intelligence. Neurosymbolic artificial intelligence(NeSy) studies the integration of symbolic reasoning and neural networks, whilestatistical relational artificial intelligence (StarAI) focuses on integratinglogic with probabilistic graphical models. This survey identifies seven shareddimensions between these two subfields of AI. These dimensions can be used tocharacterize different NeSy and StarAI systems. They are concerned with (1) theapproach to logical inference, whether model or proof-based; (2) the syntax ofthe used logical theories; (3) the logical semantics of the systems and theirextensions to facilitate learning; (4) the scope of learning, encompassingeither parameter or structure learning; (5) the presence of symbolic andsubsymbolic representations; (6) the degree to which systems capture theoriginal logic, probabilistic, and neural paradigms; and (7) the classes oflearning tasks the systems are applied to. By positioning various NeSy andStarAI systems along these dimensions and pointing out similarities anddifferences between them, this survey contributes fundamental concepts forunderstanding the integration of learning and reasoning.</description><author>Giuseppe Marra, Sebastijan Dumančić, Robin Manhaeve, Luc De Raedt</author><pubDate>Tue, 02 Jan 2024 07:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.11451v4</guid></item><item><title>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis</title><link>http://arxiv.org/abs/2310.10477v4</link><description>The rapid development of large language models (LLMs) has not only providednumerous opportunities but also presented significant challenges. This becomesparticularly evident when LLMs inadvertently generate harmful or toxic content,either unintentionally or because of intentional inducement. Existing alignmentmethods usually direct LLMs toward the favorable outcomes by utilizinghuman-annotated, flawless instruction-response pairs. Conversely, this studyproposes a novel alignment technique based on mistake analysis, whichdeliberately exposes LLMs to erroneous content to learn the reasons formistakes and how to avoid them. In this case, mistakes are repurposed intovaluable data for alignment, effectively helping to avoid the production oferroneous responses. Without external models or human annotations, our methodleverages a model's intrinsic ability to discern undesirable mistakes andimproves the safety of its generated responses. Experimental results revealthat our method outperforms existing alignment approaches in enhancing modelsafety while maintaining the overall utility.</description><author>Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</author><pubDate>Tue, 02 Jan 2024 07:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10477v4</guid></item><item><title>Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation</title><link>http://arxiv.org/abs/2401.01078v1</link><description>Poetry generation has been a challenging task in the field of NaturalLanguage Processing, as it requires the model to understand the nuances oflanguage, sentiment, and style. In this paper, we propose using Large LanguageModels to generate Vietnamese poems from natural language prompts, therebyfacilitating an intuitive process with enhanced content control. Our mostefficacious model, the GPT-3 Babbage variant, achieves a custom evaluationscore of 0.8, specifically tailored to the "luc bat" genre of Vietnamesepoetry. Furthermore, we also explore the idea of paraphrasing poems into normaltext prompts and yield a relatively high score of 0.718 in the "luc bat" genre.This experiment presents the potential for cross-Language poem-to-poemtranslation with translated poems as the inputs while concurrently maintainingcomplete control over the generated content.</description><author>Triet Huynh Minh, Quan Le Bao</author><pubDate>Tue, 02 Jan 2024 07:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01078v1</guid></item><item><title>Constrained Online Two-stage Stochastic Optimization: Algorithm with (and without) Predictions</title><link>http://arxiv.org/abs/2401.01077v1</link><description>We consider an online two-stage stochastic optimization with long-termconstraints over a finite horizon of $T$ periods. At each period, we take thefirst-stage action, observe a model parameter realization and then take thesecond-stage action from a feasible set that depends both on the first-stagedecision and the model parameter. We aim to minimize the cumulative objectivevalue while guaranteeing that the long-term average second-stage decisionbelongs to a set. We develop online algorithms for the online two-stage problemfrom adversarial learning algorithms. Also, the regret bound of our algorithmcan be reduced to the regret bound of embedded adversarial learning algorithms.Based on this framework, we obtain new results under various settings. When themodel parameters are drawn from unknown non-stationary distributions and we aregiven machine-learned predictions of the distributions, we develop a newalgorithm from our framework with a regret $O(W_T+\sqrt{T})$, where $W_T$measures the total inaccuracy of the machine-learned predictions. We thendevelop another algorithm that works when no machine-learned predictions aregiven and show the performances.</description><author>Piao Hu, Jiashuo Jiang, Guodong Lyu, Hao Su</author><pubDate>Tue, 02 Jan 2024 07:46:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01077v1</guid></item><item><title>DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever</title><link>http://arxiv.org/abs/2401.01076v1</link><description>Recently, substantial advancements in pre-trained vision-language models havegreatly enhanced the capabilities of multi-modal dialog systems. These modelshave demonstrated significant improvements by fine-tuning on downstream tasks.However, the existing pre-trained models primarily focus on effectivelycapturing the alignment between vision and language modalities, often ignoringthe intricate nature of dialog context. In this paper, we propose aparameter-efficient prompt-tuning method named DialCLIP for multi-modal dialogretrieval. Specifically, our approach introduces a multi-modal context promptgenerator to learn context features which are subsequently distilled intoprompts within the pre-trained vision-language model CLIP. Besides, weintroduce domain prompt to mitigate the disc repancy from the downstream dialogdata. To facilitate various types of retrieval, we also design multiple expertsto learn mappings from CLIP outputs to multi-modal representation space, witheach expert being responsible to one specific retrieval type. Extensiveexperiments show that DialCLIP achieves state-of-the-art performance on twowidely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning amere 0.04% of the total parameters. These results highlight the efficacy andefficiency of our proposed approach, underscoring its potential to advance thefield of multi-modal dialog retrieval.</description><author>Zhichao Yin, Binyuan Hui, Min Yang, Fei Huang, Yongbin Li</author><pubDate>Tue, 02 Jan 2024 07:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01076v1</guid></item><item><title>Arbitrary Distributions Mapping via SyMOT-Flow: A Flow-based Approach Integrating Maximum Mean Discrepancy and Optimal Transport</title><link>http://arxiv.org/abs/2308.13815v2</link><description>Finding a transformation between two unknown probability distributions fromfinite samples is crucial for modeling complex data distributions andperforming tasks such as sample generation, domain adaptation and statisticalinference. One powerful framework for such transformations is normalizing flow,which transforms an unknown distribution into a standard normal distributionusing an invertible network. In this paper, we introduce a novel model calledSyMOT-Flow that trains an invertible transformation by minimizing the symmetricmaximum mean discrepancy between samples from two unknown distributions, and anoptimal transport cost is incorporated as regularization to obtain ashort-distance and interpretable transformation. The resulted transformationleads to more stable and accurate sample generation. Several theoreticalresults are established for the proposed model and its effectiveness isvalidated with low-dimensional illustrative examples as well ashigh-dimensional bi-modality medical image generation through the forward andreverse flows.</description><author>Zhe Xiong, Qiaoqiao Ding, Xiaoqun Zhang</author><pubDate>Tue, 02 Jan 2024 07:34:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13815v2</guid></item><item><title>Depth-discriminative Metric Learning for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2401.01075v1</link><description>Monocular 3D object detection poses a significant challenge due to the lackof depth information in RGB images. Many existing methods strive to enhance theobject depth estimation performance by allocating additional parameters forobject depth estimation, utilizing extra modules or data. In contrast, weintroduce a novel metric learning scheme that encourages the model to extractdepth-discriminative features regardless of the visual attributes withoutincreasing inference time and model size. Our method employs thedistance-preserving function to organize the feature space manifold in relationto ground-truth object depth. The proposed (K, B, eps)-quasi-isometric lossleverages predetermined pairwise distance restriction as guidance for adjustingthe distance among object descriptors without disrupting the non-linearity ofthe natural feature manifold. Moreover, we introduce an auxiliary head forobject-wise depth estimation, which enhances depth quality while maintainingthe inference time. The broad applicability of our method is demonstratedthrough experiments that show improvements in overall performance whenintegrated into various baselines. The results show that our methodconsistently improves the performance of various baselines by 23.51% and 5.78%on average across KITTI and Waymo, respectively.</description><author>Wonhyeok Choi, Mingyu Shin, Sunghoon Im</author><pubDate>Tue, 02 Jan 2024 07:34:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01075v1</guid></item><item><title>AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided Diagnosis</title><link>http://arxiv.org/abs/2401.01074v1</link><description>Medical data collected for making a diagnostic decision are typicallymulti-modal and provide complementary perspectives of a subject. Acomputer-aided diagnosis system welcomes multi-modal inputs; however, how toeffectively fuse such multi-modal data is a challenging task and attracts a lotof attention in the medical research field. In this paper, we propose atransformer-based framework, called Alifuse, for aligning and fusingmulti-modal medical data. Specifically, we convert images and unstructured andstructured texts into vision and language tokens, and use intramodal andintermodal attention mechanisms to learn holistic representations of allimaging and non-imaging data for classification. We apply Alifuse to classifyAlzheimer's disease and obtain state-of-the-art performance on five publicdatasets, by outperforming eight baselines. The source code will be availableonline later.</description><author>Qiuhui Chen, Xinyue Hu, Zirui Wang, Yi Hong</author><pubDate>Tue, 02 Jan 2024 07:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01074v1</guid></item><item><title>Large Search Model: Redefining Search Stack in the Era of LLMs</title><link>http://arxiv.org/abs/2310.14587v2</link><description>Modern search engines are built on a stack of different components, includingquery understanding, retrieval, multi-stage ranking, and question answering,among others. These components are often optimized and deployed independently.In this paper, we introduce a novel conceptual framework called large searchmodel, which redefines the conventional search stack by unifying search taskswith one large language model (LLM). All tasks are formulated as autoregressivetext generation problems, allowing for the customization of tasks through theuse of natural language prompts. This proposed framework capitalizes on thestrong language understanding and reasoning capabilities of LLMs, offering thepotential to enhance search result quality while simultaneously simplifying theexisting cumbersome search stack. To substantiate the feasibility of thisframework, we present a series of proof-of-concept experiments and discuss thepotential challenges associated with implementing this approach withinreal-world search systems.</description><author>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei</author><pubDate>Tue, 02 Jan 2024 07:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14587v2</guid></item><item><title>A Novel Dual-Stage Evolutionary Algorithm for Finding Robust Solutions</title><link>http://arxiv.org/abs/2401.01070v1</link><description>In robust optimization problems, the magnitude of perturbations is relativelysmall. Consequently, solutions within certain regions are less likely torepresent the robust optima when perturbations are introduced. Hence, a moreefficient search process would benefit from increased opportunities to explorepromising regions where global optima or good local optima are situated. Inthis paper, we introduce a novel robust evolutionary algorithm named thedual-stage robust evolutionary algorithm (DREA) aimed at discovering robustsolutions. DREA operates in two stages: the peak-detection stage and the robustsolution-searching stage. The primary objective of the peak-detection stage isto identify peaks in the fitness landscape of the original optimizationproblem. Conversely, the robust solution-searching stage focuses on swiftlyidentifying the robust optimal solution using information obtained from thepeaks discovered in the initial stage. These two stages collectively enable theproposed DREA to efficiently obtain the robust optimal solution for theoptimization problem. This approach achieves a balance between solutionoptimality and robustness by separating the search processes for optimal androbust optimal solutions. Experimental results demonstrate that DREAsignificantly outperforms five state-of-the-art algorithms across 18 testproblems characterized by diverse complexities. Moreover, when evaluated onhigher-dimensional robust optimization problems (100-$D$ and 200-$D$), DREAalso demonstrates superior performance compared to all five counterpartalgorithms.</description><author>Wei Du, Wenxuan Fang, Chen Liang, Yang Tang, Yaochu Jin</author><pubDate>Tue, 02 Jan 2024 07:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01070v1</guid></item><item><title>Multi-Moving Camera Pedestrian Tracking with a New Dataset and Global Link Model</title><link>http://arxiv.org/abs/2312.11035v2</link><description>Ensuring driving safety for autonomous vehicles has become increasinglycrucial, highlighting the need for systematic tracking of pedestrians on theroad. Most vehicles are equipped with visual sensors, however, the large-scalevisual dataset from different agents has not been well studied yet. Basically,most of the multi-target multi-camera (MTMC) tracking systems are composed oftwo modules: single camera tracking (SCT) and inter-camera tracking (ICT). Toreliably coordinate between them, MTMC tracking has been a very complicatedtask, while tracking across multi-moving cameras makes it even morechallenging. In this paper, we focus on multi-target multi-moving camera(MTMMC) tracking, which is attracting increasing attention from the researchcommunity. Observing there are few datasets for MTMMC tracking, we collect anew dataset, called Multi-Moving Camera Track (MMCT), which contains sequencesunder various driving scenarios. To address the common problems of identityswitch easily faced by most existing SCT trackers, especially for movingcameras due to ego-motion between the camera and targets, a lightweightappearance-free global link model, called Linker, is proposed to mitigate theidentity switch by associating two disjoint tracklets of the same target into acomplete trajectory within the same camera. Incorporated with Linker, existingSCT trackers generally obtain a significant improvement. Moreover, a strongbaseline approach of re-identification (Re-ID) is effectively incorporated toextract robust appearance features under varying surroundings for pedestrianassociation across moving cameras for ICT, resulting in a much improved MTMMCtracking system, which can constitute a step further towards coordinated miningof multiple moving cameras. The dataset is available athttps://github.com/dhu-mmct/DHU-MMCT}{https://github.com/dhu-mmct/DHU-MMCT .</description><author>Yanting Zhang, Shuanghong Wang, Qingxiang Wang, Cairong Yan, Rui Fan</author><pubDate>Tue, 02 Jan 2024 07:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11035v2</guid></item><item><title>Discovering Significant Topics from Legal Decisions with Selective Inference</title><link>http://arxiv.org/abs/2401.01068v1</link><description>We propose and evaluate an automated pipeline for discovering significanttopics from legal decision texts by passing features synthesized with topicmodels through penalised regressions and post-selection significance tests. Themethod identifies case topics significantly correlated with outcomes,topic-word distributions which can be manually-interpreted to gain insightsabout significant topics, and case-topic weights which can be used to identifyrepresentative cases for each topic. We demonstrate the method on a new datasetof domain name disputes and a canonical dataset of European Court of HumanRights violation cases. Topic models based on latent semantic analysis as wellas language model embeddings are evaluated. We show that topics derived by thepipeline are consistent with legal doctrines in both areas and can be useful inother related legal analysis tasks.</description><author>Jerrold Soh</author><pubDate>Tue, 02 Jan 2024 07:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01068v1</guid></item><item><title>The Contextual Lasso: Sparse Linear Models via Deep Neural Networks</title><link>http://arxiv.org/abs/2302.00878v4</link><description>Sparse linear models are one of several core tools for interpretable machinelearning, a field of emerging importance as predictive models permeatedecision-making in many domains. Unfortunately, sparse linear models are farless flexible as functions of their input features than black-box models likedeep neural networks. With this capability gap in mind, we study a not-uncommonsituation where the input features dichotomize into two groups: explanatoryfeatures, which are candidates for inclusion as variables in an interpretablemodel, and contextual features, which select from the candidate variables anddetermine their effects. This dichotomy leads us to the contextual lasso, a newstatistical estimator that fits a sparse linear model to the explanatoryfeatures such that the sparsity pattern and coefficients vary as a function ofthe contextual features. The fitting process learns this functionnonparametrically via a deep neural network. To attain sparse coefficients, wetrain the network with a novel lasso regularizer in the form of a projectionlayer that maps the network's output onto the space of $\ell_1$-constrainedlinear models. An extensive suite of experiments on real and synthetic datasuggests that the learned models, which remain highly transparent, can besparser than the regular lasso without sacrificing the predictive power of astandard deep neural network.</description><author>Ryan Thompson, Amir Dezfouli, Robert Kohn</author><pubDate>Tue, 02 Jan 2024 06:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00878v4</guid></item><item><title>DTBS: Dual-Teacher Bi-directional Self-training for Domain Adaptation in Nighttime Semantic Segmentation</title><link>http://arxiv.org/abs/2401.01066v1</link><description>Due to the poor illumination and the difficulty in annotating, nighttimeconditions pose a significant challenge for autonomous vehicle perceptionsystems. Unsupervised domain adaptation (UDA) has been widely applied tosemantic segmentation on such images to adapt models from normal conditions totarget nighttime-condition domains. Self-training (ST) is a paradigm in UDA,where a momentum teacher is utilized for pseudo-label prediction, but aconfirmation bias issue exists. Because the one-directional knowledge transferfrom a single teacher is insufficient to adapt to a large domain shift. Tomitigate this issue, we propose to alleviate domain gap by incrementallyconsidering style influence and illumination change. Therefore, we introduce aone-stage Dual-Teacher Bi-directional Self-training (DTBS) framework for smoothknowledge transfer and feedback. Based on two teacher models, we present anovel pipeline to respectively decouple style and illumination shift. Inaddition, we propose a new Re-weight exponential moving average (EMA) to mergethe knowledge of style and illumination factors, and provide feedback to thestudent model. In this way, our method can be embedded in other UDA methods toenhance their performance. For example, the Cityscapes to ACDC night taskyielded 53.8 mIoU (\%), which corresponds to an improvement of +5\% over theprevious state-of-the-art. The code is available at\url{https://github.com/hf618/DTBS}.</description><author>Fanding Huang, Zihao Yao, Wenhui Zhou</author><pubDate>Tue, 02 Jan 2024 06:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01066v1</guid></item><item><title>BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving</title><link>http://arxiv.org/abs/2401.01065v1</link><description>The demand for the retrieval of complex scene data in autonomous driving isincreasing, especially as passenger vehicles have been equipped with theability to navigate urban settings, with the imperative to address long-tailscenarios. Meanwhile, under the pre-existing two dimensional image retrievalmethod, some problems may arise with scene retrieval, such as lack of globalfeature representation and subpar text retrieval ability. To address theseissues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird's-EyeView(BEV) retrieval methodology that utilizes descriptive text as an input toretrieve corresponding scenes. This methodology applies the semantic featureextraction abilities of a large language model (LLM) to facilitate zero-shotretrieval of extensive text descriptions, and incorporates semi-structuredinformation from a knowledge graph to improve the semantic richness and varietyof the language embedding. Our experiments result in 87.66% accuracy onNuScenes dataset in text-to-BEV feature retrieval. The demonstrated cases inour paper support that our retrieval method is also indicated to be effectivein identifying certain long-tail corner scenes.</description><author>Dafeng Wei, Tian Gao, Zhengyu Jia, Changwei Cai, Chengkai Hou, Peng Jia, Fu Liu, Kun Zhan, Jingchen Fan, Yixing Zhao, Yang Wang</author><pubDate>Tue, 02 Jan 2024 06:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01065v1</guid></item></channel></rss>