<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 15 Aug 2023 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Platypus: Quick, Cheap, and Powerful Refinement of LLMs</title><link>http://arxiv.org/abs/2308.07317v1</link><description>We present $\textbf{Platypus}$, a family of fine-tuned and merged LargeLanguage Models (LLMs) that achieves the strongest performance and currentlystands at first place in HuggingFace's Open LLM Leaderboard as of the releasedate of this work. In this work we describe (1) our curated dataset$\textbf{Open-Platypus}$, that is a subset of other open datasets and which$\textit{we release to the public}$ (2) our process of fine-tuning and mergingLoRA modules in order to conserve the strong prior of pretrained LLMs, whilebringing specific domain knowledge to the surface (3) our efforts in checkingfor test data leaks and contamination in the training data, which can informfuture research. Specifically, the Platypus family achieves strong performancein quantitative LLM metrics across model sizes, topping the global Open LLMleaderboard while using just a fraction of the fine-tuning data and overallcompute that are required for other state-of-the-art fine-tuned LLMs. Inparticular, a 13B Platypus model can be trained on $\textit{a single}$ A100 GPUusing 25k questions in 5 hours. This is a testament of the quality of ourOpen-Platypus dataset, and opens opportunities for more improvements in thefield. Project page: https://platypus-llm.github.io</description><author>Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz</author><pubDate>Mon, 14 Aug 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07317v1</guid></item><item><title>Jurassic World Remake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-Image Translation</title><link>http://arxiv.org/abs/2308.07316v1</link><description>With a strong understanding of the target domain from natural language, weproduce promising results in translating across large domain gaps and bringingskeletons back to life. In this work, we use text-guided latent diffusionmodels for zero-shot image-to-image translation (I2I) across large domain gaps(longI2I), where large amounts of new visual features and new geometry need tobe generated to enter the target domain. Being able to perform translationsacross large domain gaps has a wide variety of real-world applications incriminology, astrology, environmental conservation, and paleontology. In thiswork, we introduce a new task Skull2Animal for translating between skulls andliving animals. On this task, we find that unguided Generative AdversarialNetworks (GANs) are not capable of translating across large domain gaps.Instead of these traditional I2I methods, we explore the use of guideddiffusion and image editing models and provide a new benchmark model,Revive-2I, capable of performing zero-shot I2I via text-prompting latentdiffusion models. We find that guidance is necessary for longI2I because, tobridge the large domain gap, prior knowledge about the target domain is needed.In addition, we find that prompting provides the best and most scalableinformation about the target domain as classifier-guided diffusion modelsrequire retraining for specific use cases and lack stronger constraints on thetarget domain because of the wide variety of images they are trained on.</description><author>Alexander Martin, Haitian Zheng, Jie An, Jiebo Luo</author><pubDate>Mon, 14 Aug 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07316v1</guid></item><item><title>Dual Associated Encoder for Face Restoration</title><link>http://arxiv.org/abs/2308.07314v1</link><description>Restoring facial details from low-quality (LQ) images has remained achallenging problem due to its ill-posedness induced by various degradations inthe wild. The existing codebook prior mitigates the ill-posedness by leveragingan autoencoder and learned codebook of high-quality (HQ) features, achievingremarkable quality. However, existing approaches in this paradigm frequentlydepend on a single encoder pre-trained on HQ data for restoring HQ images,disregarding the domain gap between LQ and HQ images. As a result, the encodingof LQ inputs may be insufficient, resulting in suboptimal performance. Totackle this problem, we propose a novel dual-branch framework named DAEFR. Ourmethod introduces an auxiliary LQ branch that extracts crucial information fromthe LQ inputs. Additionally, we incorporate association training to promoteeffective synergy between the two branches, enhancing code prediction andoutput quality. We evaluate the effectiveness of DAEFR on both synthetic andreal-world datasets, demonstrating its superior performance in restoring facialdetails.</description><author>Yu-Ju Tsai, Yu-Lun Liu, Lu Qi, Kelvin C. K. Chan, Ming-Hsuan Yang</author><pubDate>Mon, 14 Aug 2023 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07314v1</guid></item><item><title>Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation</title><link>http://arxiv.org/abs/2308.07313v1</link><description>In this paper, we study the problem of end-to-end multi-person poseestimation. State-of-the-art solutions adopt the DETR-like framework, andmainly develop the complex decoder, e.g., regarding pose estimation as keypointbox detection and combining with human detection in ED-Pose, hierarchicallypredicting with pose decoder and joint (keypoint) decoder in PETR. We present asimple yet effective transformer approach, named Group Pose. We simply regard$K$-keypoint pose estimation as predicting a set of $N\times K$ keypointpositions, each from a keypoint query, as well as representing each pose withan instance query for scoring $N$ pose predictions. Motivated by the intuitionthat the interaction, among across-instance queries of different types, is notdirectly helpful, we make a simple modification to decoder self-attention. Wereplace single self-attention over all the $N\times(K+1)$ queries with twosubsequent group self-attentions: (i) $N$ within-instance self-attention, witheach over $K$ keypoint queries and one instance query, and (ii) $(K+1)$same-type across-instance self-attention, each over $N$ queries of the sametype. The resulting decoder removes the interaction among across-instancetype-different queries, easing the optimization and thus improving theperformance. Experimental results on MS COCO and CrowdPose show that ourapproach without human box supervision is superior to previous methods withcomplex decoders, and even is slightly better than ED-Pose that uses human boxsupervision. $\href{https://github.com/Michel-liu/GroupPose-Paddle}{\rmPaddle}$ and $\href{https://github.com/Michel-liu/GroupPose}{\rm PyTorch}$ codeare available.</description><author>Huan Liu, Qiang Chen, Zichang Tan, Jiang-Jiang Liu, Jian Wang, Xiangbo Su, Xiaolong Li, Kun Yao, Junyu Han, Errui Ding, Yao Zhao, Jingdong Wang</author><pubDate>Mon, 14 Aug 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07313v1</guid></item><item><title>LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked</title><link>http://arxiv.org/abs/2308.07308v1</link><description>Large language models (LLMs) have skyrocketed in popularity in recent yearsdue to their ability to generate high-quality text in response to humanprompting. However, these models have been shown to have the potential togenerate harmful content in response to user prompting (e.g., giving usersinstructions on how to commit crimes). There has been a focus in the literatureon mitigating these risks, through methods like aligning models with humanvalues through reinforcement learning. However, it has been shown that evenaligned language models are susceptible to adversarial attacks that bypasstheir restrictions on generating harmful text. We propose a simple approach todefending against these attacks by having a large language model filter its ownresponses. Our current results show that even if a model is not fine-tuned tobe aligned with human values, it is possible to stop it from presenting harmfulcontent to users by validating the content using a language model.</description><author>Alec Helbling, Mansi Phute, Matthew Hull, Duen Horng Chau</author><pubDate>Mon, 14 Aug 2023 18:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07308v1</guid></item><item><title>Extend Wave Function Collapse to Large-Scale Content Generation</title><link>http://arxiv.org/abs/2308.07307v1</link><description>Wave Function Collapse (WFC) is a widely used tile-based algorithm inprocedural content generation, including textures, objects, and scenes.However, the current WFC algorithm and related research lack the ability togenerate commercialized large-scale or infinite content due to constraintconflict and time complexity costs. This paper proposes a Nested WFC (N-WFC)algorithm framework to reduce time complexity. To avoid conflict andbacktracking problems, we offer a complete and sub-complete tileset preparationstrategy, which requires only a small number of tiles to generate aperiodic anddeterministic infinite content. We also introduce the weight-brush system thatcombines N-WFC and sub-complete tileset, proving its suitability for gamedesign. Our contribution addresses WFC's challenge in massive contentgeneration and provides a theoretical basis for implementing concrete games.</description><author>Yuhe Nie, Shaoming Zheng, Zhan Zhuang, Xuan Song</author><pubDate>Mon, 14 Aug 2023 18:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07307v1</guid></item><item><title>Neural Authorship Attribution: Stylometric Analysis on Large Language Models</title><link>http://arxiv.org/abs/2308.07305v1</link><description>Large language models (LLMs) such as GPT-4, PaLM, and Llama havesignificantly propelled the generation of AI-crafted text. With rising concernsabout their potential misuse, there is a pressing need for AI-generated-textforensics. Neural authorship attribution is a forensic effort, seeking to traceAI-generated text back to its originating LLM. The LLM landscape can be dividedinto two primary categories: proprietary and open-source. In this work, wedelve into these emerging categories of LLMs, focusing on the nuances of neuralauthorship attribution. To enrich our understanding, we carry out an empiricalanalysis of LLM writing signatures, highlighting the contrasts betweenproprietary and open-source models, and scrutinizing variations within eachgroup. By integrating stylometric features across lexical, syntactic, andstructural aspects of language, we explore their potential to yieldinterpretable results and augment pre-trained language model-based classifiersutilized in neural authorship attribution. Our findings, based on a range ofstate-of-the-art LLMs, provide empirical insights into neural authorshipattribution, paving the way for future investigations aimed at mitigating thethreats posed by AI-generated misinformation.</description><author>Tharindu Kumarage, Huan Liu</author><pubDate>Mon, 14 Aug 2023 18:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07305v1</guid></item><item><title>A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis</title><link>http://arxiv.org/abs/2308.07301v1</link><description>The synthesis of human motion has traditionally been addressed throughtask-dependent models that focus on specific challenges, such as predictingfuture motions or filling in intermediate poses conditioned on known key-poses.In this paper, we present a novel task-independent model called UNIMASK-M,which can effectively address these challenges using a unified architecture.Our model obtains comparable or better performance than the state-of-the-art ineach field. Inspired by Vision Transformers (ViTs), our UNIMASK-M modeldecomposes a human pose into body parts to leverage the spatio-temporalrelationships existing in human motion. Moreover, we reformulate variouspose-conditioned motion synthesis tasks as a reconstruction problem withdifferent masking patterns given as input. By explicitly informing our modelabout the masked joints, our UNIMASK-M becomes more robust to occlusions.Experimental results show that our model successfully forecasts human motion onthe Human3.6M dataset. Moreover, it achieves state-of-the-art results in motioninbetweening on the LaFAN1 dataset, particularly in long transition periods.More information can be found on the project websitehttps://sites.google.com/view/estevevallsmascaro/publications/unimask-m.</description><author>Esteve Valls Mascaro, Hyemin Ahn, Dongheui Lee</author><pubDate>Mon, 14 Aug 2023 18:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07301v1</guid></item><item><title>G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer</title><link>http://arxiv.org/abs/2305.03153v2</link><description>Various template-based and template-free approaches have been proposed forsingle-step retrosynthesis prediction in recent years. While these approachesdemonstrate strong performance from a data-driven metrics standpoint, manymodel architectures do not incorporate underlying chemistry principles. Here,we propose a novel chemistry-aware retrosynthesis prediction framework thatcombines powerful data-driven models with prior domain knowledge. We present atree-to-sequence transformer architecture that utilizes hierarchical SMILESgrammar-based trees, incorporating crucial chemistry information that is oftenoverlooked by SMILES text-based representations, such as local structures andfunctional groups. The proposed framework, grammar-based molecular attentiontree transformer (G-MATT), achieves significant performance improvementscompared to baseline retrosynthesis models. G-MATT achieves a promising top-1accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, and bioactivesimilarity rate of 74.8% on the USPTO- 50K dataset. Additional analyses ofG-MATT attention maps demonstrate the ability to retain chemistry knowledgewithout relying on excessively complex model architectures.</description><author>Kevin Zhang, Vipul Mann, Venkat Venkatasubramanian</author><pubDate>Mon, 14 Aug 2023 18:38:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03153v2</guid></item><item><title>Accurate Eye Tracking from Dense 3D Surface Reconstructions using Single-Shot Deflectometry</title><link>http://arxiv.org/abs/2308.07298v1</link><description>Eye-tracking plays a crucial role in the development of virtual realitydevices, neuroscience research, and psychology. Despite its significance innumerous applications, achieving an accurate, robust, and fast eye-trackingsolution remains a considerable challenge for current state-of-the-art methods.While existing reflection-based techniques (e.g., "glint tracking") areconsidered the most accurate, their performance is limited by their reliance onsparse 3D surface data acquired solely from the cornea surface. In this paper,we rethink the way how specular reflections can be used for eye tracking: Wepropose a novel method for accurate and fast evaluation of the gaze directionthat exploits teachings from single-shot phase-measuring-deflectometry (PMD).In contrast to state-of-the-art reflection-based methods, our method acquiresdense 3D surface information of both cornea and sclera within only one singlecamera frame (single-shot). Improvements in acquired reflection surfacepoints("glints") of factors $&gt;3300 \times$ are easily achievable. We show thefeasibility of our approach with experimentally evaluated gaze errors of only$\leq 0.25^\circ$ demonstrating a significant improvement over the currentstate-of-the-art.</description><author>Jiazhang Wang, Tianfu Wang, Bingjie Xu, Oliver Cossairt And Florian Willomitzer</author><pubDate>Mon, 14 Aug 2023 18:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07298v1</guid></item><item><title>Why Not? Explaining Missing Entailments with $\rm E{\scriptsize VEE}$ (Technical Report)</title><link>http://arxiv.org/abs/2308.07294v1</link><description>Understanding logical entailments derived by a description logic reasoner isnot always straight-forward for ontology users. For this reason, variousmethods for explaining entailments using justifications and proofs have beendeveloped and implemented as plug-ins for the ontology editor Prot\'eg\'e.However, when the user expects a missing consequence to hold, it is equallyimportant to explain why it does not follow from the ontology. In this paper,we describe a new version of $\rm E{\scriptsize VEE}$, a Prot\'eg\'e pluginthat now also provides explanations for missing consequences, via existing andnew techniques based on abduction and counterexamples.</description><author>Christian Alrabbaa, Stefan Borgwardt, Tom Friese, Patrick Koopmann, Mikhail Kotlov</author><pubDate>Mon, 14 Aug 2023 18:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07294v1</guid></item><item><title>DiffSED: Sound Event Detection with Denoising Diffusion</title><link>http://arxiv.org/abs/2308.07293v1</link><description>Sound Event Detection (SED) aims to predict the temporal boundaries of allthe events of interest and their class labels, given an unconstrained audiosample. Taking either the splitand-classify (i.e., frame-level) strategy or themore principled event-level modeling approach, all existing methods considerthe SED problem from the discriminative learning perspective. In this work, wereformulate the SED problem by taking a generative learning perspective.Specifically, we aim to generate sound temporal boundaries from noisy proposalsin a denoising diffusion process, conditioned on a target audio sample. Duringtraining, our model learns to reverse the noising process by converting noisylatent queries to the groundtruth versions in the elegant Transformer decoderframework. Doing so enables the model generate accurate event boundaries fromeven noisy queries during inference. Extensive experiments on the Urban-SED andEPIC-Sounds datasets demonstrate that our model significantly outperformsexisting alternatives, with 40+% faster convergence in training.</description><author>Swapnil Bhosale, Sauradip Nag, Diptesh Kanojia, Jiankang Deng, Xiatian Zhu</author><pubDate>Mon, 14 Aug 2023 18:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07293v1</guid></item><item><title>On the Sublinear Regret of GP-UCB</title><link>http://arxiv.org/abs/2307.07539v2</link><description>In the kernelized bandit problem, a learner aims to sequentially compute theoptimum of a function lying in a reproducing kernel Hilbert space given onlynoisy evaluations at sequentially chosen points. In particular, the learneraims to minimize regret, which is a measure of the suboptimality of the choicesmade. Arguably the most popular algorithm is the Gaussian Process UpperConfidence Bound (GP-UCB) algorithm, which involves acting based on a simplelinear estimator of the unknown function. Despite its popularity, existinganalyses of GP-UCB give a suboptimal regret rate, which fails to be sublinearfor many commonly used kernels such as the Mat\'ern kernel. This has led to alongstanding open question: are existing regret analyses for GP-UCB tight, orcan bounds be improved by using more sophisticated analytical techniques? Inthis work, we resolve this open question and show that GP-UCB enjoys nearlyoptimal regret. In particular, our results yield sublinear regret rates for theMat\'ern kernel, improving over the state-of-the-art analyses and partiallyresolving a COLT open problem posed by Vakili et al. Our improvements rely on akey technical contribution -- regularizing kernel ridge estimators inproportion to the smoothness of the underlying kernel $k$. Applying this keyidea together with a largely overlooked concentration result in separableHilbert spaces (for which we provide an independent, simplified derivation), weare able to provide a tighter analysis of the GP-UCB algorithm.</description><author>Justin Whitehouse, Zhiwei Steven Wu, Aaditya Ramdas</author><pubDate>Mon, 14 Aug 2023 18:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07539v2</guid></item><item><title>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</title><link>http://arxiv.org/abs/2308.07286v1</link><description>Automatic evaluation of machine translation (MT) is a critical tool drivingthe rapid iterative development of MT systems. While considerable progress hasbeen made on estimating a single scalar quality score, current metrics lack theinformativeness of more detailed schemes that annotate individual errors, suchas Multidimensional Quality Metrics (MQM). In this paper, we help fill this gapby proposing AutoMQM, a prompting technique which leverages the reasoning andin-context learning capabilities of large language models (LLMs) and asks themto identify and categorize errors in translations. We start by evaluatingrecent LLMs, such as PaLM and PaLM-2, through simple score predictionprompting, and we study the impact of labeled data through in-context learningand finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find thatit improves performance compared to just prompting for scores (withparticularly large gains for larger models) while providing interpretabilitythrough error spans that align with human annotations.</description><author>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat</author><pubDate>Mon, 14 Aug 2023 18:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07286v1</guid></item><item><title>Cross-Attribute Matrix Factorization Model with Shared User Embedding</title><link>http://arxiv.org/abs/2308.07284v1</link><description>Over the past few years, deep learning has firmly established its prowessacross various domains, including computer vision, speech recognition, andnatural language processing. Motivated by its outstanding success, researchershave been directing their efforts towards applying deep learning techniques torecommender systems. Neural collaborative filtering (NCF) and Neural MatrixFactorization (NeuMF) refreshes the traditional inner product in matrixfactorization with a neural architecture capable of learning complex anddata-driven functions. While these models effectively capture user-iteminteractions, they overlook the specific attributes of both users and items.This can lead to robustness issues, especially for items and users that belongto the "long tail". Such challenges are commonly recognized in recommendersystems as a part of the cold-start problem. A direct and intuitive approach toaddress this issue is by leveraging the features and attributes of the itemsand users themselves. In this paper, we introduce a refined NeuMF model thatconsiders not only the interaction between users and items, but also acrossingassociated attributes. Moreover, our proposed architecture features a shareduser embedding, seamlessly integrating with user embeddings to imporve therobustness and effectively address the cold-start problem. Rigorous experimentson both the Movielens and Pinterest datasets demonstrate the superiority of ourCross-Attribute Matrix Factorization model, particularly in scenarioscharacterized by higher dataset sparsity.</description><author>Wen Liang, Zeng Fan, Youzhi Liang, Jianguo Jia</author><pubDate>Mon, 14 Aug 2023 18:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07284v1</guid></item><item><title>Autonomous Point Cloud Segmentation for Power Lines Inspection in Smart Grid</title><link>http://arxiv.org/abs/2308.07283v1</link><description>LiDAR is currently one of the most utilized sensors to effectively monitorthe status of power lines and facilitate the inspection of remote powerdistribution networks and related infrastructures. To ensure the safe operationof the smart grid, various remote data acquisition strategies, such as AirborneLaser Scanning (ALS), Mobile Laser Scanning (MLS), and Terrestrial LaserScanning (TSL) have been leveraged to allow continuous monitoring of regionalpower networks, which are typically surrounded by dense vegetation. In thisarticle, an unsupervised Machine Learning (ML) framework is proposed, todetect, extract and analyze the characteristics of power lines of both high andlow voltage, as well as the surrounding vegetation in a Power Line Corridor(PLC) solely from LiDAR data. Initially, the proposed approach eliminates theground points from higher elevation points based on statistical analysis thatapplies density criteria and histogram thresholding. After denoising andtransforming of the remaining candidate points by applying Principle ComponentAnalysis (PCA) and Kd-tree, power line segmentation is achieved by utilizing atwo-stage DBSCAN clustering to identify each power line individually. Finally,all high elevation points in the PLC are identified based on their distance tothe newly segmented power lines. Conducted experiments illustrate that theproposed framework is an agnostic method that can efficiently detect the powerlines and perform PLC-based hazard analysis.</description><author>Alexander Kyuroson, Anton Koval, George Nikolakopoulos</author><pubDate>Mon, 14 Aug 2023 18:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07283v1</guid></item><item><title>Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification</title><link>http://arxiv.org/abs/2308.07282v1</link><description>Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuningtechniques designed to make the training of language models more efficient.Previous results demonstrated that these methods can even improve performanceon some classification tasks. This paper complements the existing research byinvestigating how these techniques influence the classification performance andcomputation costs compared to full fine-tuning when applied to multilingualtext classification tasks (genre, framing, and persuasion techniques detection;with different input lengths, number of predicted classes and classificationdifficulty), some of which have limited training data. In addition, we conductin-depth analyses of their efficacy across different training scenarios(training on the original multilingual data; on the translations into English;and on a subset of English-only data) and different languages. Our findingsprovide valuable insights into the applicability of the parameter-efficientfine-tuning techniques, particularly to complex multilingual and multilabelclassification tasks.</description><author>Olesya Razuvayevskaya, Ben Wu, Joao A. Leite, Freddy Heppell, Ivan Srba, Carolina Scarton, Kalina Bontcheva, Xingyi Song</author><pubDate>Mon, 14 Aug 2023 18:12:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07282v1</guid></item><item><title>A Robust Approach Towards Distinguishing Natural and Computer Generated Images using Multi-Colorspace fused and Enriched Vision Transformer</title><link>http://arxiv.org/abs/2308.07279v1</link><description>The works in literature classifying natural and computer generated images aremostly designed as binary tasks either considering natural images versuscomputer graphics images only or natural images versus GAN generated imagesonly, but not natural images versus both classes of the generated images. Also,even though this forensic classification task of distinguishing natural andcomputer generated images gets the support of the new convolutional neuralnetworks and transformer based architectures that can give remarkableclassification accuracies, they are seen to fail over the images that haveundergone some post-processing operations usually performed to deceive theforensic algorithms, such as JPEG compression, gaussian noise, etc. This workproposes a robust approach towards distinguishing natural and computergenerated images including both, computer graphics and GAN generated imagesusing a fusion of two vision transformers where each of the transformernetworks operates in different color spaces, one in RGB and the other in YCbCrcolor space. The proposed approach achieves high performance gain when comparedto a set of baselines, and also achieves higher robustness and generalizabilitythan the baselines. The features of the proposed model when visualized are seento obtain higher separability for the classes than the input image features andthe baseline features. This work also studies the attention map visualizationsof the networks of the fused model and observes that the proposed methodologycan capture more image information relevant to the forensic task of classifyingnatural and generated images.</description><author>Manjary P Gangan, Anoop Kadan, Lajish V L</author><pubDate>Mon, 14 Aug 2023 18:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07279v1</guid></item><item><title>Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning</title><link>http://arxiv.org/abs/2308.07273v1</link><description>Unmanned aerial vehicle (UAV)-enabled edge federated learning (FL) hassparked a rise in research interest as a result of the massive andheterogeneous data collected by UAVs, as well as the privacy concerns relatedto UAV data transmissions to edge servers. However, due to the redundancy ofUAV collected data, e.g., imaging data, and non-rigorous FL participantselection, the convergence time of the FL learning process and bias of the FLmodel may increase. Consequently, we investigate in this paper the problem ofselecting UAV participants for edge FL, aiming to improve the FL model'saccuracy, under UAV constraints of energy consumption, communication quality,and local datasets' heterogeneity. We propose a novel UAV participant selectionscheme, called data-efficient energy-aware participant selection strategy(DEEPS), which consists of selecting the best FL participant in each sub-regionbased on the structural similarity index measure (SSIM) average score of itslocal dataset and its power consumption profile. Through experiments, wedemonstrate that the proposed selection scheme is superior to the benchmarkrandom selection method, in terms of model accuracy, training time, and UAVenergy consumption.</description><author>Youssra Cheriguene, Wael Jaafar, Chaker Abdelaziz Kerrache, Halim Yanikomeroglu, Fatima Zohra Bousbaa, Nasreddine Lagraa</author><pubDate>Mon, 14 Aug 2023 18:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07273v1</guid></item><item><title>Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning</title><link>http://arxiv.org/abs/2308.07272v1</link><description>Prompt-based pre-trained language models (PLMs) paradigm have succeededsubstantially in few-shot natural language processing (NLP) tasks. However,prior discrete prompt optimization methods require expert knowledge to designthe base prompt set and identify high-quality prompts, which is costly,inefficient, and subjective. Meanwhile, existing continuous prompt optimizationmethods improve the performance by learning the ideal prompts through thegradient information of PLMs, whose high computational cost, and lowreadability and generalizability are often concerning. To address the researchgap, we propose a Dialogue-comprised Policy-gradient-based Discrete PromptOptimization ($DP_2O$) method. We first design a multi-round dialogue alignmentstrategy for readability prompt set generation based on GPT-4. Furthermore, wepropose an efficient prompt screening metric to identify high-quality promptswith linear complexity. Finally, we construct a reinforcement learning (RL)framework based on policy gradients to match the prompts to inputs optimally.By training a policy network with only 0.67% of the PLM parameter size on thetasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA)method by 1.52% in accuracy on average on four open-source datasets. Moreover,subsequent experiments also demonstrate that $DP_2O$ has good universality,robustness, and generalization ability.</description><author>Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi Li, Yu Lan, Chao Shen</author><pubDate>Mon, 14 Aug 2023 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07272v1</guid></item><item><title>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</title><link>http://arxiv.org/abs/2308.07269v1</link><description>Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacyissues, which means they are unaware of unseen events or generate text withincorrect facts owing to the outdated/noisy data. To this end, many knowledgeediting approaches for LLMs have emerged -- aiming to subtly inject/editupdated knowledge or adjust undesired behavior while minimizing the impact onunrelated inputs. Nevertheless, due to significant differences among variousknowledge editing methods and the variations in task setups, there is nostandard implementation framework available for the community, which hinderspractitioners to apply knowledge editing to applications. To address theseissues, we propose EasyEdit, an easy-to-use knowledge editing framework forLLMs. It supports various cutting-edge knowledge editing approaches and can bereadily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc.Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,demonstrating that knowledge editing surpasses traditional fine-tuning in termsof reliability and generalization. We have released the source code on GitHubat https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials andcomprehensive documentation for beginners to get started. Besides, we presentan online system for real-time knowledge editing, and a demo video athttp://knowlm.zjukg.cn/easyedit.mp4.</description><author>Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, Huajun Chen</author><pubDate>Mon, 14 Aug 2023 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07269v1</guid></item><item><title>Diving with Penguins: Detecting Penguins and their Prey in Animal-borne Underwater Videos via Deep Learning</title><link>http://arxiv.org/abs/2308.07267v1</link><description>African penguins (Spheniscus demersus) are an endangered species. Little isknown regarding their underwater hunting strategies and associated predationsuccess rates, yet this is essential for guiding conservation. Modernbio-logging technology has the potential to provide valuable insights, butmanually analysing large amounts of data from animal-borne video recorders(AVRs) is time-consuming. In this paper, we publish an animal-borne underwatervideo dataset of penguins and introduce a ready-to-deploy deep learning systemcapable of robustly detecting penguins (mAP50@98.0%) and also instances of fish(mAP50@73.3%). We note that the detectors benefit explicitly from air-bubblelearning to improve accuracy. Extending this detector towards a dual-streambehaviour recognition network, we also provide the first results foridentifying predation behaviour in penguin underwater videos. Whilst resultsare promising, further work is required for useful applicability of predationbehaviour detection in field scenarios. In summary, we provide a highlyreliable underwater penguin detector, a fish detector, and a valuable firstattempt towards an automated visual detection of complex behaviours in a marinepredator. We publish the networks, the DivingWithPenguins video dataset,annotations, splits, and weights for full reproducibility and immediateusability by practitioners.</description><author>Kejia Zhang, Mingyu Yang, Stephen D. J. Lang, Alistair M. McInnes, Richard B. Sherley, Tilo Burghardt</author><pubDate>Mon, 14 Aug 2023 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07267v1</guid></item><item><title>NECE: Narrative Event Chain Extraction Toolkit</title><link>http://arxiv.org/abs/2208.08063v5</link><description>To understand a narrative, it is essential to comprehend the temporal eventflows, especially those associated with main characters; however, this can bechallenging with lengthy and unstructured narrative texts. To address this, weintroduce NECE, an open-access, document-level toolkit that automaticallyextracts and aligns narrative events in the temporal order of their occurrence.Through extensive evaluations, we show the high quality of the NECE toolkit anddemonstrates its downstream application in analyzing narrative bias regardinggender. We also openly discuss the shortcomings of the current approach, andpotential of leveraging generative models in future works. Lastly the NECEtoolkit includes both a Python library and a user-friendly web interface, whichoffer equal access to professionals and layman audience alike, to visualizeevent chain, obtain narrative flows, or study narrative bias.</description><author>Guangxuan Xu, Paulina Toro Isaza, Moshi Li, Akintoye Oloko, Bingsheng Yao, Cassia Sanctos, Aminat Adebiyi, Yufang Hou, Nanyun Peng, Dakuo Wang</author><pubDate>Mon, 14 Aug 2023 17:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08063v5</guid></item><item><title>Evaluating the Impact of Social Determinants on Health Prediction in the Intensive Care Unit</title><link>http://arxiv.org/abs/2305.12622v2</link><description>Social determinants of health (SDOH) -- the conditions in which people live,grow, and age -- play a crucial role in a person's health and well-being. Thereis a large, compelling body of evidence in population health studies showingthat a wide range of SDOH is strongly correlated with health outcomes. Yet, amajority of the risk prediction models based on electronic health records (EHR)do not incorporate a comprehensive set of SDOH features as they are often noisyor simply unavailable. Our work links a publicly available EHR database,MIMIC-IV, to well-documented SDOH features. We investigate the impact of suchfeatures on common EHR prediction tasks across different patient populations.We find that community-level SDOH features do not improve model performance fora general patient population, but can improve data-limited model fairness forspecific subpopulations. We also demonstrate that SDOH features are vital forconducting thorough audits of algorithmic biases beyond protective attributes.We hope the new integrated EHR-SDOH database will enable studies on therelationship between community health and individual outcomes and provide newbenchmarks to study algorithmic biases beyond race, gender, and age.</description><author>Ming Ying Yang, Gloria Hyunjung Kwak, Tom Pollard, Leo Anthony Celi, Marzyeh Ghassemi</author><pubDate>Mon, 14 Aug 2023 17:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12622v2</guid></item><item><title>Efficient Real-time Smoke Filtration with 3D LiDAR for Search and Rescue with Autonomous Heterogeneous Robotic Systems</title><link>http://arxiv.org/abs/2308.07264v1</link><description>Search and Rescue (SAR) missions in harsh and unstructured Sub-Terranean(Sub-T) environments in the presence of aerosol particles have recently becomethe main focus in the field of robotics. Aerosol particles such as smoke anddust directly affect the performance of any mobile robotic platform due totheir reliance on their onboard perception systems for autonomous navigationand localization in Global Navigation Satellite System (GNSS)-deniedenvironments. Although obstacle avoidance and object detection algorithms arerobust to the presence of noise to some degree, their performance directlyrelies on the quality of captured data by onboard sensors such as LightDetection And Ranging (LiDAR) and camera. Thus, this paper proposes a novelmodular agnostic filtration pipeline based on intensity and spatial informationsuch as local point density for removal of detected smoke particles from PointCloud (PCL) prior to its utilization for collision detection. Furthermore, theefficacy of the proposed framework in the presence of smoke during multiplefrontier exploration missions is investigated while the experimental resultsare presented to facilitate comparison with other methodologies and theircomputational impact. This provides valuable insight to the research communityfor better utilization of filtration schemes based on available computationresources while considering the safe autonomous navigation of mobile robots.</description><author>Alexander Kyuroson, Anton Koval, George Nikolakopoulos</author><pubDate>Mon, 14 Aug 2023 17:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07264v1</guid></item><item><title>Self-Alignment with Instruction Backtranslation</title><link>http://arxiv.org/abs/2308.06259v2</link><description>We present a scalable method to build a high quality instruction followinglanguage model by automatically labelling human-written text with correspondinginstructions. Our approach, named instruction backtranslation, starts with alanguage model finetuned on a small amount of seed data, and a given webcorpus. The seed model is used to construct training examples by generatinginstruction prompts for web documents (self-augmentation), and then selectinghigh quality examples from among these candidates (self-curation). This data isthen used to finetune a stronger model. Finetuning LLaMa on two iterations ofour approach yields a model that outperforms all other LLaMa-based models onthe Alpaca leaderboard not relying on distillation data, demonstrating highlyeffective self-alignment.</description><author>Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, Mike Lewis</author><pubDate>Mon, 14 Aug 2023 17:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06259v2</guid></item><item><title>Contrastive Learning for API Aspect Analysis</title><link>http://arxiv.org/abs/2307.16878v2</link><description>We present a novel approach - CLAA - for API aspect detection in API reviewsthat utilizes transformer models trained with a supervised contrastive lossobjective function. We evaluate CLAA using performance and impact analysis. Forperformance analysis, we utilized a benchmark dataset on developer discussionscollected from Stack Overflow and compare the results to those obtained usingstate-of-the-art transformer models. Our experiments show that contrastivelearning can significantly improve the performance of transformer models indetecting aspects such as Performance, Security, Usability, and Documentation.For impact analysis, we performed empirical and developer study. On a randomlyselected and manually labeled 200 online reviews, CLAA achieved 92% accuracywhile the SOTA baseline achieved 81.5%. According to our developer studyinvolving 10 participants, the use of 'Stack Overflow + CLAA' resulted inincreased accuracy and confidence during API selection. Replication package:https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023</description><author>G. M. Shahariar, Tahmid Hasan, Anindya Iqbal, Gias Uddin</author><pubDate>Mon, 14 Aug 2023 17:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16878v2</guid></item><item><title>Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation</title><link>http://arxiv.org/abs/2308.07251v1</link><description>Vision transformers are effective deep learning models for vision tasks,including medical image segmentation. However, they lack efficiency andtranslational invariance, unlike convolutional neural networks (CNNs). To modellong-range interactions in 3D brain lesion segmentation, we propose anall-convolutional transformer block variant of the U-Net architecture. Wedemonstrate that our model provides the greatest compromise in three factors:performance competitive with the state-of-the-art; parameter efficiency of aCNN; and the favourable inductive biases of a transformer. Our publicimplementation is available at https://github.com/liamchalcroft/MDUNet .</description><author>Liam Chalcroft, Ruben Lourenço Pereira, Mikael Brudfors, Andrew S. Kayser, Mark D'Esposito, Cathy J. Price, Ioannis Pappas, John Ashburner</author><pubDate>Mon, 14 Aug 2023 17:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07251v1</guid></item><item><title>LCE -- An Augmented Combination of Bagging and Boosting in Python</title><link>http://arxiv.org/abs/2308.07250v1</link><description>lcensemble is a high-performing, scalable and user-friendly Python packagefor the general tasks of classification and regression. The package implementsLocal Cascade Ensemble (LCE), a machine learning method that further enhancesthe prediction performance of the current state-of-the-art methods RandomForest and XGBoost. LCE combines their strengths and adopts a complementarydiversification approach to obtain a better generalizing predictor. The packageis compatible with scikit-learn, therefore it can interact with scikit-learnpipelines and model selection tools. It is distributed under the Apache 2.0license, and its source code is available athttps://github.com/LocalCascadeEnsemble/LCE.</description><author>Kevin Fauvel, Élisa Fromont, Véronique Masson, Philippe Faverdin, Alexandre Termier</author><pubDate>Mon, 14 Aug 2023 17:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07250v1</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v2</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task,named source-free domain adaptive HPE, which aims to address the challenges ofcross-domain learning of HPE without access to source data during theadaptation process. We further propose a novel framework that consists of threemodels: source model, intermediate model, and target model, which explores thetask from both source-protect and target-relevant perspectives. Thesource-protect module preserves source information more effectively whileresisting noise, and the target-relevant module reduces the sparsity of spatialrepresentations by building a novel spatial probability space, andpose-specific contrastive learning and information maximization are proposed onthe basis of this space. Comprehensive experiments on several domain adaptiveHPE benchmarks show that the proposed method outperforms existing approaches bya considerable margin.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Mon, 14 Aug 2023 17:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v2</guid></item><item><title>Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI</title><link>http://arxiv.org/abs/2308.07247v1</link><description>The Rash\=omon effect poses challenges for deriving reliable knowledge frommachine learning models. This study examined the influence of sample size onexplanations from models in a Rash\=omon set using SHAP. Experiments on 5public datasets showed that explanations gradually converged as the sample sizeincreased. Explanations from &lt;128 samples exhibited high variability, limitingreliable knowledge extraction. However, agreement between models improved withmore data, allowing for consensus. Bagging ensembles often had higheragreement. The results provide guidance on sufficient data to trustexplanations. Variability at low samples suggests that conclusions may beunreliable without validation. Further work is needed with more model types,data domains, and explanation methods. Testing convergence in neural networksand with model-specific explanation methods would be impactful. The approachesexplored here point towards principled techniques for eliciting knowledge fromambiguous models.</description><author>Clement Poiret, Antoine Grigis, Justin Thomas, Marion Noulhiane</author><pubDate>Mon, 14 Aug 2023 17:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07247v1</guid></item><item><title>AAFACE: Attribute-aware Attentional Network for Face Recognition</title><link>http://arxiv.org/abs/2308.07243v1</link><description>In this paper, we present a new multi-branch neural network thatsimultaneously performs soft biometric (SB) prediction as an auxiliary modalityand face recognition (FR) as the main task. Our proposed network named AAFaceutilizes SB attributes to enhance the discriminative ability of FRrepresentation. To achieve this goal, we propose an attribute-aware attentionalintegration (AAI) module to perform weighted integration of FR with SB featuremaps. Our proposed AAI module is not only fully context-aware but also capableof learning complex relationships between input features by means of thesequential multi-scale channel and spatial sub-modules. Experimental resultsverify the superiority of our proposed network compared with thestate-of-the-art (SoTA) SB prediction and FR methods.</description><author>Niloufar Alipour Talemi, Hossein Kashiani, Sahar Rahimi Malakshan, Mohammad Saeed Ebrahimi Saadabadi, Nima Najafzadeh, Mohammad Akyash, Nasser M. Nasrabadi</author><pubDate>Mon, 14 Aug 2023 17:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07243v1</guid></item><item><title>Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents</title><link>http://arxiv.org/abs/2308.07241v1</link><description>Accomplishing household tasks such as 'bringing a cup of water' requiresplanning step-by-step actions by maintaining knowledge about the spatialarrangement of objects and the consequences of previous actions. Perceptionmodels of the current embodied AI agents, however, often make mistakes due to alack of such knowledge but rely on imperfect learning of imitating agents or analgorithmic planner without knowledge about the changed environment by theprevious actions. To address the issue, we propose CPEM (Context-aware Plannerand Environment-aware Memory) to incorporate the contextual information ofprevious actions for planning and maintaining spatial arrangement of objectswith their states (e.g., if an object has been moved or not) in an environmentto the perception model for improving both visual navigation and objectinteraction. We observe that CPEM achieves state-of-the-art task successperformance in various metrics using a challenging interactive instructionfollowing benchmark both in seen and unseen environments by large margins (upto +10.70% in unseen env.). CPEM with the templated actions, named ECLAIR, alsowon the 1st generalist language grounding agents challenge at Embodied AIWorkshop in CVPR'23.</description><author>Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi</author><pubDate>Mon, 14 Aug 2023 17:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07241v1</guid></item><item><title>UniWorld: Autonomous Driving Pre-training via World Models</title><link>http://arxiv.org/abs/2308.07234v1</link><description>In this paper, we draw inspiration from Alberto Elfes' pioneering work in1989, where he introduced the concept of the occupancy grid as World Models forrobots. We imbue the robot with a spatial-temporal world model, termedUniWorld, to perceive its surroundings and predict the future behavior of otherparticipants. UniWorld involves initially predicting 4D geometric occupancy asthe World Models for foundational stage and subsequently fine-tuning ondownstream tasks. UniWorld can estimate missing information concerning theworld state and predict plausible future states of the world. Besides,UniWorld's pre-training process is label-free, enabling the utilization ofmassive amounts of image-LiDAR pairs to build a Foundational Model.The proposedunified pre-training framework demonstrates promising results in key tasks suchas motion prediction, multi-camera 3D object detection, and surroundingsemantic scene completion. When compared to monocular pre-training methods onthe nuScenes dataset, UniWorld shows a significant improvement of about 1.5% inIoU for motion prediction, 2.0% in mAP and 2.0% in NDS for multi-camera 3Dobject detection, as well as a 3% increase in mIoU for surrounding semanticscene completion. By adopting our unified pre-training method, a 25% reductionin 3D training annotation costs can be achieved, offering significant practicalvalue for the implementation of real-world autonomous driving. Codes arepublicly available at https://github.com/chaytonmin/UniWorld.</description><author>Chen Min, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai</author><pubDate>Mon, 14 Aug 2023 17:17:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07234v1</guid></item><item><title>Procedural Generation of Complex Roundabouts for Autonomous Vehicle Testing</title><link>http://arxiv.org/abs/2303.17900v2</link><description>High-definition roads are an essential component of realistic drivingscenario simulation for autonomous vehicle testing. Roundabouts are one of thekey road segments that have not been thoroughly investigated. Based on thegeometric constraints of the nearby road structure, this work presents a novelmethod for procedurally building roundabouts. The suggested method can resultin roundabout lanes that are not perfectly circular and resemble real-worldroundabouts by allowing approaching roadways to be connected to a roundabout atany angle. One can easily incorporate the roundabout in their HD roadgeneration process or use the standalone roundabouts in scenario-based testingof autonomous driving.</description><author>Zarif Ikram, Golam Md Muktadir, Jim Whitehead</author><pubDate>Mon, 14 Aug 2023 17:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17900v2</guid></item><item><title>A Unifying Generator Loss Function for Generative Adversarial Networks</title><link>http://arxiv.org/abs/2308.07233v1</link><description>A unifying $\alpha$-parametrized generator loss function is introduced for adual-objective generative adversarial network (GAN), which uses a canonical (orclassical) discriminator loss function such as the one in the original GAN(VanillaGAN) system. The generator loss function is based on a symmetric classprobability estimation type function, $\mathcal{L}_\alpha$, and the resultingGAN system is termed $\mathcal{L}_\alpha$-GAN. Under an optimal discriminator,it is shown that the generator's optimization problem consists of minimizing aJensen-$f_\alpha$-divergence, a natural generalization of the Jensen-Shannondivergence, where $f_\alpha$ is a convex function expressed in terms of theloss function $\mathcal{L}_\alpha$. It is also demonstrated that this$\mathcal{L}_\alpha$-GAN problem recovers as special cases a number of GANproblems in the literature, including VanillaGAN, Least Squares GAN (LSGAN),Least $k$th order GAN (L$k$GAN) and the recently introduced$(\alpha_D,\alpha_G)$-GAN with $\alpha_D=1$. Finally, experimental results areconducted on three datasets, MNIST, CIFAR-10, and Stacked MNIST to illustratethe performance of various examples of the $\mathcal{L}_\alpha$-GAN system.</description><author>Justin Veiner, Fady Alajaji, Bahman Gharesifard</author><pubDate>Mon, 14 Aug 2023 17:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07233v1</guid></item><item><title>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence</title><link>http://arxiv.org/abs/2307.07522v2</link><description>Recent advances in machine learning and AI, including Generative AI and LLMs,are disrupting technological innovation, product development, and society as awhole. AI's contribution to technology can come from multiple approaches thatrequire access to large training data sets and clear performance evaluationcriteria, ranging from pattern recognition and classification to generativemodels. Yet, AI has contributed less to fundamental science in part becauselarge data sets of high-quality data for scientific practice and modeldiscovery are more difficult to access. Generative AI, in general, and LargeLanguage Models in particular, may represent an opportunity to augment andaccelerate the scientific discovery of fundamental deep science withquantitative models. Here we explore and investigate aspects of an AI-driven,automated, closed-loop approach to scientific discovery, including self-drivenhypothesis generation and open-ended autonomous exploration of the hypothesisspace. Integrating AI-driven automation into the practice of science wouldmitigate current problems, including the replication of findings, systematicproduction of data, and ultimately democratisation of the scientific process.Realising these possibilities requires a vision for augmented AI coupled with adiversity of AI approaches able to deal with fundamental aspects of causalityanalysis and model discovery while enabling unbiased search across the space ofputative explanations. These advances hold the promise to unleash AI'spotential for searching and discovering the fundamental structure of our worldbeyond what human scientists have been able to achieve. Such a vision wouldpush the boundaries of new fundamental science rather than automatize currentworkflows and instead open doors for technological innovation to tackle some ofthe greatest challenges facing humanity today.</description><author>Hector Zenil, Jesper Tegnér, Felipe S. Abrahão, Alexander Lavin, Vipin Kumar, Jeremy G. Frey, Adrian Weller, Larisa Soldatova, Alan R. Bundy, Nicholas R. Jennings, Koichi Takahashi, Lawrence Hunter, Saso Dzeroski, Andrew Briggs, Frederick D. Gregory, Carla P. Gomes, Christopher K. I. Williams, Jon Rowe, James Evans, Hiroaki Kitano, Joshua B. Tenenbaum, Ross King</author><pubDate>Mon, 14 Aug 2023 17:12:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07522v2</guid></item><item><title>RestoreFormer++: Towards Real-World Blind Face Restoration from Undegraded Key-Value Pairs</title><link>http://arxiv.org/abs/2308.07228v1</link><description>Blind face restoration aims at recovering high-quality face images from thosewith unknown degradations. Current algorithms mainly introduce priors tocomplement high-quality details and achieve impressive progress. However, mostof these algorithms ignore abundant contextual information in the face and itsinterplay with the priors, leading to sub-optimal performance. Moreover, theypay less attention to the gap between the synthetic and real-world scenarios,limiting the robustness and generalization to real-world applications. In thiswork, we propose RestoreFormer++, which on the one hand introducesfully-spatial attention mechanisms to model the contextual information and theinterplay with the priors, and on the other hand, explores an extendingdegrading model to help generate more realistic degraded face images toalleviate the synthetic-to-real-world gap. Compared with current algorithms,RestoreFormer++ has several crucial benefits. First, instead of using amulti-head self-attention mechanism like the traditional visual transformer, weintroduce multi-head cross-attention over multi-scale features to fully explorespatial interactions between corrupted information and high-quality priors. Inthis way, it can facilitate RestoreFormer++ to restore face images with higherrealness and fidelity. Second, in contrast to the recognition-orienteddictionary, we learn a reconstruction-oriented dictionary as priors, whichcontains more diverse high-quality facial details and better accords with therestoration target. Third, we introduce an extending degrading model thatcontains more realistic degraded scenarios for training data synthesizing, andthus helps to enhance the robustness and generalization of our RestoreFormer++model. Extensive experiments show that RestoreFormer++ outperformsstate-of-the-art algorithms on both synthetic and real-world datasets.</description><author>Zhouxia Wang, Jiawei Zhang, Tianshui Chen, Wenping Wang, Ping Luo</author><pubDate>Mon, 14 Aug 2023 17:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07228v1</guid></item><item><title>Deconfounded Causal Collaborative Filtering</title><link>http://arxiv.org/abs/2110.07122v2</link><description>Recommender systems may be confounded by various types of confounding factors(also called confounders) that may lead to inaccurate recommendations andsacrificed recommendation performance. Current approaches to solving theproblem usually design each specific model for each specific confounder.However, real-world systems may include a huge number of confounders and thusdesigning each specific model for each specific confounder could beunrealistic. More importantly, except for those ``explicit confounders'' thatexperts can manually identify and process such as item's position in theranking list, there are also many ``latent confounders'' that are beyond theimagination of experts. For example, users' rating on a song may depend ontheir current mood or the current weather, and users' preference on ice creamsmay depend on the air temperature. Such latent confounders may be unobservablein the recorded training data. To solve the problem, we propose DeconfoundedCausal Collaborative Filtering (DCCF). We first frame user behaviors withunobserved confounders into a causal graph, and then we design a front-dooradjustment model carefully fused with machine learning to deconfound theinfluence of unobserved confounders. Experiments on real-world datasets showthat our method is able to deconfound unobserved confounders to achieve betterrecommendation performance.</description><author>Shuyuan Xu, Juntao Tan, Shelby Heinecke, Jia Li, Yongfeng Zhang</author><pubDate>Mon, 14 Aug 2023 17:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.07122v2</guid></item><item><title>Causal Collaborative Filtering</title><link>http://arxiv.org/abs/2102.01868v5</link><description>Many of the traditional recommendation algorithms are designed based on thefundamental idea of mining or learning correlative patterns from data toestimate the user-item correlative preference. However, pure correlativelearning may lead to Simpson's paradox in predictions, and thus results insacrificed recommendation performance. Simpson's paradox is a well-knownstatistical phenomenon, which causes confusions in statistical conclusions andignoring the paradox may result in inaccurate decisions. Fortunately, causaland counterfactual modeling can help us to think outside of the observationaldata for user modeling and personalization so as to tackle such issues. In thispaper, we propose Causal Collaborative Filtering (CCF) -- a general frameworkfor modeling causality in collaborative filtering and recommendation. Weprovide a unified causal view of CF and mathematically show that many of thetraditional CF algorithms are actually special cases of CCF under simplifiedcausal graphs. We then propose a conditional intervention approach for$do$-operations so that we can estimate the user-item causal preference basedon the observational data. Finally, we further propose a general counterfactualconstrained learning framework for estimating the user-item preferences.Experiments are conducted on two types of real-world datasets -- traditionaland randomized trial data -- and results show that our framework can improvethe recommendation performance and reduce the Simpson's paradox problem of manyCF algorithms.</description><author>Shuyuan Xu, Yingqiang Ge, Yunqi Li, Zuohui Fu, Xu Chen, Yongfeng Zhang</author><pubDate>Mon, 14 Aug 2023 16:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.01868v5</guid></item><item><title>DS-Depth: Dynamic and Static Depth Estimation via a Fusion Cost Volume</title><link>http://arxiv.org/abs/2308.07225v1</link><description>Self-supervised monocular depth estimation methods typically rely on thereprojection error to capture geometric relationships between successive framesin static environments. However, this assumption does not hold in dynamicobjects in scenarios, leading to errors during the view synthesis stage, suchas feature mismatch and occlusion, which can significantly reduce the accuracyof the generated depth maps. To address this problem, we propose a noveldynamic cost volume that exploits residual optical flow to describe movingobjects, improving incorrectly occluded regions in static cost volumes used inprevious work. Nevertheless, the dynamic cost volume inevitably generates extraocclusions and noise, thus we alleviate this by designing a fusion module thatmakes static and dynamic cost volumes compensate for each other. In otherwords, occlusion from the static volume is refined by the dynamic volume, andincorrect information from the dynamic volume is eliminated by the staticvolume. Furthermore, we propose a pyramid distillation loss to reducephotometric error inaccuracy at low resolutions and an adaptive photometricerror loss to alleviate the flow direction of the large gradient in theocclusion regions. We conducted extensive experiments on the KITTI andCityscapes datasets, and the results demonstrate that our model outperformspreviously published baselines for self-supervised monocular depth estimation.</description><author>Xingyu Miao, Yang Bai, Haoran Duan, Yawen Huang, Fan Wan, Xinxing Xu, Yang Long, Yefeng Zheng</author><pubDate>Mon, 14 Aug 2023 16:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07225v1</guid></item><item><title>Distance Matters For Improving Performance Estimation Under Covariate Shift</title><link>http://arxiv.org/abs/2308.07223v1</link><description>Performance estimation under covariate shift is a crucial component of safeAI model deployment, especially for sensitive use-cases. Recently, severalsolutions were proposed to tackle this problem, most leveraging modelpredictions or softmax confidence to derive accuracy estimates. However, underdataset shifts, confidence scores may become ill-calibrated if samples are toofar from the training distribution. In this work, we show that taking intoaccount distances of test samples to their expected training distribution cansignificantly improve performance estimation under covariate shift. Precisely,we introduce a "distance-check" to flag samples that lie too far from theexpected distribution, to avoid relying on their untrustworthy model outputs inthe accuracy estimation step. We demonstrate the effectiveness of this methodon 13 image classification tasks, across a wide-range of natural and syntheticdistribution shifts and hundreds of models, with a median relative MAEimprovement of 27% over the best baseline across all tasks, and SOTAperformance on 10 out of 13 tasks. Our code is publicly available athttps://github.com/melanibe/distance_matters_performance_estimation.</description><author>Mélanie Roschewitz, Ben Glocker</author><pubDate>Mon, 14 Aug 2023 16:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07223v1</guid></item><item><title>MM-GEF: Multi-modal representation meet collaborative filtering</title><link>http://arxiv.org/abs/2308.07222v1</link><description>In modern e-commerce, item content features in various modalities offeraccurate yet comprehensive information to recommender systems. The majority ofprevious work either focuses on learning effective item representation duringmodelling user-item interactions, or exploring item-item relationships byanalysing multi-modal features. Those methods, however, fail to incorporate thecollaborative item-user-item relationships into the multi-modal feature-baseditem structure. In this work, we propose a graph-based item structureenhancement method MM-GEF: Multi-Modal recommendation with Graph Early-Fusion,which effectively combines the latent item structure underlying multi-modalcontents with the collaborative signals. Instead of processing the contentfeature in different modalities separately, we show that the early-fusion ofmulti-modal features provides significant improvement. MM-GEF learns refineditem representations by injecting structural information obtained from bothmulti-modal and collaborative signals. Through extensive experiments on fourpublicly available datasets, we demonstrate systematical improvements of ourmethod over state-of-the-art multi-modal recommendation methods.</description><author>Hao Wu, Alejandro Ariza-Casabona, Bartłomiej Twardowski, Tri Kurniawan Wijaya</author><pubDate>Mon, 14 Aug 2023 16:47:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07222v1</guid></item><item><title>AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes</title><link>http://arxiv.org/abs/2308.07221v1</link><description>We propose a method named AudioFormer, which learns audio featurerepresentations through the acquisition of discrete acoustic codes andsubsequently fine-tunes them for audio classification tasks. Initially, weintroduce a novel perspective by considering the audio classification task as aform of natural language understanding (NLU). Leveraging an existing neuralaudio codec model, we generate discrete acoustic codes and utilize them totrain a masked language model (MLM), thereby obtaining audio featurerepresentations. Furthermore, we pioneer the integration of a\textbf{M}ulti-\textbf{P}ositive sample \textbf{C}ontrastive (MPC) learningapproach. This method enables the learning of joint representations amongmultiple discrete acoustic codes within the same audio input. In ourexperiments, we treat discrete acoustic codes as textual data and train amasked language model using a cloze-like methodology, ultimately derivinghigh-quality audio representations. Notably, the MPC learning techniqueeffectively captures collaborative representations among distinct positivesamples. Our research outcomes demonstrate that AudioFormer attainssignificantly improved performance compared to prevailing monomodal audioclassification models across multiple datasets, and even outperformsaudio-visual multimodal classification models on select datasets. Specifically,our approach achieves remarkable results on datasets including AudioSet (2M,20K), and FSD50K, with performance scores of 53.9, 45.1, and 65.6,respectively. We have openly shared both the code and models:\url{https://github.com/LZH-0225/AudioFormer.git}.</description><author>Zhaohui Li, Haitao Wang, Xinghua Jiang</author><pubDate>Mon, 14 Aug 2023 16:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07221v1</guid></item><item><title>Developability Approximation for Neural Implicits through Rank Minimization</title><link>http://arxiv.org/abs/2308.03900v2</link><description>Developability refers to the process of creating a surface without anytearing or shearing from a two-dimensional plane. It finds practicalapplications in the fabrication industry. An essential characteristic of adevelopable 3D surface is its zero Gaussian curvature, which means that eitherone or both of the principal curvatures are zero. This paper introduces amethod for reconstructing an approximate developable surface from a neuralimplicit surface. The central idea of our method involves incorporating aregularization term that operates on the second-order derivatives of the neuralimplicits, effectively promoting zero Gaussian curvature. Implicit surfacesoffer the advantage of smoother deformation with infinite resolution,overcoming the high polygonal constraints of state-of-the-art methods usingdiscrete representations. We draw inspiration from the properties of surfacecurvature and employ rank minimization techniques derived from compressedsensing. Experimental results on both developable and non-developable surfaces,including those affected by noise, validate the generalizability of our method.</description><author>Pratheba Selvaraju</author><pubDate>Mon, 14 Aug 2023 16:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03900v2</guid></item><item><title>Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data</title><link>http://arxiv.org/abs/2308.07214v1</link><description>Brain tumors, particularly glioblastoma, continue to challenge medicaldiagnostics and treatments globally. This paper explores the application ofdeep learning to multi-modality magnetic resonance imaging (MRI) data forenhanced brain tumor segmentation precision in the Sub-Saharan Africa patientpopulation. We introduce an ensemble method that comprises eleven uniquevariations based on three core architectures: UNet3D, ONet3D, SphereNet3D andmodified loss functions. The study emphasizes the need for both age- andpopulation-based segmentation models, to fully account for the complexities inthe brain. Our findings reveal that the ensemble approach, combining differentarchitectures, outperforms single models, leading to improved evaluationmetrics. Specifically, the results exhibit Dice scores of 0.82, 0.82, and 0.87for enhancing tumor, tumor core, and whole tumor labels respectively. Theseresults underline the potential of tailored deep learning techniques inprecisely segmenting brain tumors and lay groundwork for future work tofine-tune models and assess performance across different brain regions.</description><author>Chiranjeewee Prasad Koirala, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</author><pubDate>Mon, 14 Aug 2023 16:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07214v1</guid></item><item><title>Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data</title><link>http://arxiv.org/abs/2308.07212v1</link><description>Brain tumors remain a critical global health challenge, necessitatingadvancements in diagnostic techniques and treatment methodologies. In responseto the growing need for age-specific segmentation models, particularly forpediatric patients, this study explores the deployment of deep learningtechniques using magnetic resonance imaging (MRI) modalities. By introducing anovel ensemble approach using ONet and modified versions of UNet, coupled withinnovative loss functions, this study achieves a precise segmentation model forthe BraTS-PEDs 2023 Challenge. Data augmentation, including both single andcomposite transformations, ensures model robustness and accuracy acrossdifferent scanning protocols. The ensemble strategy, integrating the ONet andUNet models, shows greater effectiveness in capturing specific features andmodeling diverse aspects of the MRI images which result in lesion_wise dicescores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumorlabels respectively. Visual comparisons further confirm the superiority of theensemble method in accurate tumor region coverage. The results indicate thatthis advanced ensemble approach, building upon the unique strengths ofindividual models, offers promising prospects for enhanced diagnostic accuracyand effective treatment planning for brain tumors in pediatric brains.</description><author>Shashidhar Reddy Javaji, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</author><pubDate>Mon, 14 Aug 2023 16:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07212v1</guid></item><item><title>Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning</title><link>http://arxiv.org/abs/2308.07209v1</link><description>Structured pruning and quantization are promising approaches for reducing theinference time and memory footprint of neural networks. However, most existingmethods require the original training dataset to fine-tune the model. This notonly brings heavy resource consumption but also is not possible forapplications with sensitive or proprietary data due to privacy and securityconcerns. Therefore, a few data-free methods are proposed to address thisproblem, but they perform data-free pruning and quantization separately, whichdoes not explore the complementarity of pruning and quantization. In thispaper, we propose a novel framework named Unified Data-Free Compression(UDFC),which performs pruning and quantization simultaneously without any data andfine-tuning process. Specifically, UDFC starts with the assumption that thepartial information of a damaged(e.g., pruned or quantized) channel can bepreserved by a linear combination of other channels, and then derives thereconstruction form from the assumption to restore the information loss due tocompression. Finally, we formulate the reconstruction error between theoriginal network and its compressed network, and theoretically deduce theclosed-form solution. We evaluate the UDFC on the large-scale imageclassification task and obtain significant improvements over various networkarchitectures and compression methods. For example, we achieve a 20.54%accuracy improvement on ImageNet dataset compared to SOTA method with 30%pruning ratio and 6-bit quantization on ResNet-34.</description><author>Shipeng Bai, Jun Chen, Xintian Shen, Yixuan Qian, Yong Liu</author><pubDate>Mon, 14 Aug 2023 16:25:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07209v1</guid></item><item><title>FOLT: Fast Multiple Object Tracking from UAV-captured Videos Based on Optical Flow</title><link>http://arxiv.org/abs/2308.07207v1</link><description>Multiple object tracking (MOT) has been successfully investigated in computervision. However, MOT for the videos captured by unmanned aerial vehicles (UAV) isstill challenging due to small object size, blurred object appearance, and verylarge and/or irregular motion in both ground objects and UAV platforms. In this paper, we propose FOLT to mitigate these problems and reach fast andaccurate MOT in UAV view. Aiming at speed-accuracy trade-off, FOLT adopts a modern detector andlight-weight optical flow extractor to extract object detection features andmotion features at a minimum cost. Given the extracted flow, the flow-guided feature augmentation is designed toaugment the object detection feature based on its optical flow, which improvesthe detection of small objects. Then the flow-guided motion prediction is also proposed to predict theobject's position in the next frame, which improves the tracking performance ofobjects with very large displacements between adjacent frames. Finally, the tracker matches the detected objects and predicted objects usinga spatially matching scheme to generate tracks for every object. Experiments on Visdrone and UAVDT datasets show that our proposed model cansuccessfully track small objects with large and irregular motion and outperformexisting state-of-the-art methods in UAV-MOT tasks.</description><author>Mufeng Yao, Jiaqi Wang, Jinlong Peng, Mingmin Chi, Chao Liu</author><pubDate>Mon, 14 Aug 2023 16:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07207v1</guid></item><item><title>Algorithms for the Training of Neural Support Vector Machines</title><link>http://arxiv.org/abs/2308.07204v1</link><description>Neural support vector machines (NSVMs) allow for the incorporation of domainknowledge in the design of the model architecture. In this article we introducea set of training algorithms for NSVMs that leverage the Pegasos algorithm andprovide a proof of concept by solving a set of standard machine learning tasks.</description><author>Lars Simon, Manuel Radons</author><pubDate>Mon, 14 Aug 2023 16:16:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07204v1</guid></item><item><title>SLIC: Large Receptive Field Learning with Self-Conditioned Adaptability for Learned Image Compression</title><link>http://arxiv.org/abs/2304.09571v2</link><description>Recently, transformers are trending as replacements for CNNs in vision tasks,including compression. This trend compels us to question the inherentlimitations of CNNs compared to transformers and to explore if CNNs can beenhanced to achieve the same or even better performance than transformers. Wewant to design a pure CNN based model for compression as most devices areoptimized for CNNs well. In our analysis, we find that the key strengths oftransformers lie in their dynamic weights and large receptive fields. To enableCNNs with such properties, we propose a novel transform module with largereceptive filed learning and self-conditioned adaptability for learned imagecompression, named SLIC. Specifically, we enlarge the receptive field ofdepth-wise convolution with suitable complexity and generate the weightsaccording to given conditions. In addition, we also investigate theself-conditioned factor for channels. To prove the effectiveness of ourproposed transform module, we equip it with existing entropy models ChARM,SCCTX, and SWAtten and we obtain models SLIC-ChARM, SLIC-SCCTX, andSLIC-SWAtten. Extensive experiments demonstrate our SLIC-ChARM, SLIC-SCCTX, andSLIC-SWAtten have significant improvements over corresponding baselines andachieve SOTA performances with suitable complexity on 5 test datasets (Kodak,Tecnick, CLIC 20, CLIC 21, JPEGAI). Code will be available athttps://github.com/JiangWeibeta/SLIC.</description><author>Wei Jiang, Peirong Ning, Jiayu Yang, Yongqi Zhai, Ronggang Wang</author><pubDate>Mon, 14 Aug 2023 16:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09571v2</guid></item><item><title>Towards Robust Real-Time Scene Text Detection: From Semantic to Instance Representation Learning</title><link>http://arxiv.org/abs/2308.07202v1</link><description>Due to the flexible representation of arbitrary-shaped scene text and simplepipeline, bottom-up segmentation-based methods begin to be mainstream inreal-time scene text detection. Despite great progress, these methods showdeficiencies in robustness and still suffer from false positives and instanceadhesion. Different from existing methods which integrate multiple-granularityfeatures or multiple outputs, we resort to the perspective of representationlearning in which auxiliary tasks are utilized to enable the encoder to jointlylearn robust features with the main task of per-pixel classification duringoptimization. For semantic representation learning, we propose global-densesemantic contrast (GDSC), in which a vector is extracted for global semanticrepresentation, then used to perform element-wise contrast with the dense gridfeatures. To learn instance-aware representation, we propose to combinetop-down modeling (TDM) with the bottom-up framework to provide implicitinstance-level clues for the encoder. With the proposed GDSC and TDM, theencoder network learns stronger representation without introducing anyparameters and computations during inference. Equipped with a very lightdecoder, the detector can achieve more robust real-time scene text detection.Experimental results on four public datasets show that the proposed method canoutperform or be comparable to the state-of-the-art on both accuracy and speed.Specifically, the proposed method achieves 87.2% F-measure with 48.2 FPS onTotal-Text and 89.6% F-measure with 36.9 FPS on MSRA-TD500 on a single GeForceRTX 2080 Ti GPU.</description><author>Xugong Qin, Pengyuan Lyu, Chengquan Zhang, Yu Zhou, Kun Yao, Peng Zhang, Hailun Lin, Weiping Wang</author><pubDate>Mon, 14 Aug 2023 16:14:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07202v1</guid></item><item><title>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</title><link>http://arxiv.org/abs/2308.07201v1</link><description>Text evaluation has historically posed significant challenges, oftendemanding substantial labor and time cost. With the emergence of large languagemodels (LLMs), researchers have explored LLMs' potential as alternatives forhuman evaluation. While these single-agent-based approaches show promise,experimental results suggest that further advancements are needed to bridge thegap between their current effectiveness and human-level evaluation quality.Recognizing that best practices of human evaluation processes often involvemultiple human annotators collaborating in the evaluation, we resort to amulti-agent debate framework, moving beyond single-agent prompting strategies.The multi-agent-based approach enables a group of LLMs to synergize with anarray of intelligent counterparts, harnessing their distinct capabilities andexpertise to enhance efficiency and effectiveness in handling intricate tasks.In this paper, we construct a multi-agent referee team called ChatEval toautonomously discuss and evaluate the quality of generated responses fromdifferent models on open-ended questions and traditional natural languagegeneration (NLG) tasks. Our analysis shows that ChatEval transcends meretextual scoring, offering a human-mimicking evaluation process for reliableassessments. Our code is available at https://github.com/chanchimin/ChatEval.</description><author>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu</author><pubDate>Mon, 14 Aug 2023 16:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07201v1</guid></item><item><title>Neural Categorical Priors for Physics-Based Character Control</title><link>http://arxiv.org/abs/2308.07200v1</link><description>Recent advances in learning reusable motion priors have demonstrated theireffectiveness in generating naturalistic behaviors. In this paper, we propose anew learning framework in this paradigm for controlling physics-basedcharacters with significantly improved motion quality and diversity overexisting state-of-the-art methods. The proposed method uses reinforcementlearning (RL) to initially track and imitate life-like movements fromunstructured motion clips using the discrete information bottleneck, as adoptedin the Vector Quantized Variational AutoEncoder (VQ-VAE). This structurecompresses the most relevant information from the motion clips into a compactyet informative latent space, i.e., a discrete space over vector quantizedcodes. By sampling codes in the space from a trained categorical priordistribution, high-quality life-like behaviors can be generated, similar to theusage of VQ-VAE in computer vision. Although this prior distribution can betrained with the supervision of the encoder's output, it follows the originalmotion clip distribution in the dataset and could lead to imbalanced behaviorsin our setting. To address the issue, we further propose a technique namedprior shifting to adjust the prior distribution using curiosity-driven RL. Theoutcome distribution is demonstrated to offer sufficient behavioral diversityand significantly facilitates upper-level policy learning for downstream tasks.We conduct comprehensive experiments using humanoid characters on twochallenging downstream tasks, sword-shield striking and two-player boxing game.Our results demonstrate that the proposed framework is capable of controllingthe character to perform considerably high-quality movements in terms ofbehavioral strategies, diversity, and realism. Videos, codes, and data areavailable at https://tencent-roboticsx.github.io/NCP/.</description><author>Qingxu Zhu, He Zhang, Mengting Lan, Lei Han</author><pubDate>Mon, 14 Aug 2023 16:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07200v1</guid></item><item><title>C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction</title><link>http://arxiv.org/abs/2306.10003v2</link><description>There is an emerging effort to combine the two popular 3D frameworks usingMulti-View Stereo (MVS) and Neural Implicit Surfaces (NIS) with a specificfocus on the few-shot / sparse view setting. In this paper, we introduce anovel integration scheme that combines the multi-view stereo with neural signeddistance function representations, which potentially overcomes the limitationsof both methods. MVS uses per-view depth estimation and cross-view fusion togenerate accurate surfaces, while NIS relies on a common coordinate volume.Based on this strategy, we propose to construct per-view cost frustum for finergeometry estimation, and then fuse cross-view frustums and estimate theimplicit signed distance functions to tackle artifacts that are due to noiseand holes in the produced surface reconstruction. We further apply a cascadefrustum fusion strategy to effectively captures global-local information andstructural consistency. Finally, we apply cascade sampling and apseudo-geometric loss to foster stronger integration between the twoarchitectures. Extensive experiments demonstrate that our method reconstructsrobust surfaces and outperforms existing state-of-the-art methods.</description><author>Luoyuan Xu, Tao Guan, Yuesong Wang, Wenkai Liu, Zhaojie Zeng, Junle Wang, Wei Yang</author><pubDate>Mon, 14 Aug 2023 16:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10003v2</guid></item><item><title>Explaining Black-Box Models through Counterfactuals</title><link>http://arxiv.org/abs/2308.07198v1</link><description>We present CounterfactualExplanations.jl: a package for generatingCounterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-boxmodels in Julia. CE explain how inputs into a model need to change to yieldspecific model predictions. Explanations that involve realistic and actionablechanges can be used to provide AR: a set of proposed actions for individuals tochange an undesirable outcome for the better. In this article, we discuss theusefulness of CE for Explainable Artificial Intelligence and demonstrate thefunctionality of our package. The package is straightforward to use anddesigned with a focus on customization and extensibility. We envision it to oneday be the go-to place for explaining arbitrary predictive models in Juliathrough a diverse suite of counterfactual generators.</description><author>Patrick Altmeyer, Arie van Deursen, Cynthia C. S. Liem</author><pubDate>Mon, 14 Aug 2023 16:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07198v1</guid></item><item><title>Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum Estimators</title><link>http://arxiv.org/abs/2303.11908v2</link><description>Spectrum estimation is a fundamental methodology in the analysis oftime-series data, with applications including medicine, speech analysis, andcontrol design. The asymptotic theory of spectrum estimation iswell-understood, but the theory is limited when the number of samples is fixedand finite. This paper gives non-asymptotic error bounds for a broad class ofspectral estimators, both pointwise (at specific frequencies) and in the worstcase over all frequencies. The general method is used to derive error boundsfor the classical Blackman-Tukey, Bartlett, and Welch estimators. Inparticular, these are first non-asymptotic error bounds for Bartlett and Welchestimators.</description><author>Andrew Lamperski</author><pubDate>Mon, 14 Aug 2023 16:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11908v2</guid></item><item><title>Task Offloading for Smart Glasses in Healthcare: Enhancing Detection of Elevated Body Temperature</title><link>http://arxiv.org/abs/2308.07193v1</link><description>Wearable devices like smart glasses have gained popularity across variousapplications. However, their limited computational capabilities pose challengesfor tasks that require extensive processing, such as image and videoprocessing, leading to drained device batteries. To address this, offloadingsuch tasks to nearby powerful remote devices, such as mobile devices or remoteservers, has emerged as a promising solution. This paper focuses on analyzingtask-offloading scenarios for a healthcare monitoring application performed onsmart wearable glasses, aiming to identify the optimal conditions foroffloading. The study evaluates performance metrics including task completiontime, computing capabilities, and energy consumption under realisticconditions. A specific use case is explored within an indoor area like anairport, where security agents wearing smart glasses to detect elevated bodytemperature in individuals, potentially indicating COVID-19. The findingshighlight the potential benefits of task offloading for wearable devices inhealthcare settings, demonstrating its practicality and relevance.</description><author>Abdenacer Naouri, Nabil Abdelkader Nouri, Attia Qammar, Feifei Shi, Huansheng Ning, Sahraoui Dhelim</author><pubDate>Mon, 14 Aug 2023 15:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07193v1</guid></item><item><title>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling</title><link>http://arxiv.org/abs/2308.07192v1</link><description>A large catalogue size is one of the central challenges in trainingrecommendation models: a large number of items makes them memory andcomputationally inefficient to compute scores for all items during training,forcing these models to deploy negative sampling. However, negative samplingincreases the proportion of positive interactions in the training data, andtherefore models trained with negative sampling tend to overestimate theprobabilities of positive interactions a phenomenon we call overconfidence.While the absolute values of the predicted scores or probabilities are notimportant for the ranking of retrieved recommendations, overconfident modelsmay fail to estimate nuanced differences in the top-ranked items, resulting indegraded performance. In this paper, we show that overconfidence explains whythe popular SASRec model underperforms when compared to BERT4Rec. This iscontrary to the BERT4Rec authors explanation that the difference in performanceis due to the bi-directional attention mechanism. To mitigate overconfidence,we propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) andtheoretically prove that it can mitigate overconfidence. We further propose thegSASRec model, an improvement over SASRec that deploys an increased number ofnegatives and the gBCE loss. We show through detailed experiments on threedatasets that gSASRec does not exhibit the overconfidence problem. As a result,gSASRec can outperform BERT4Rec (e.g. +9.47% NDCG on the MovieLens-1M dataset),while requiring less training time (e.g. -73% training time on MovieLens-1M).Moreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets thatcontain more than 1 million items.</description><author>Aleksandr Petrov, Craig Macdonald</author><pubDate>Mon, 14 Aug 2023 15:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07192v1</guid></item><item><title>Point Cloud Registration for LiDAR and Photogrammetric Data: a Critical Synthesis and Performance Analysis on Classic and Deep Learning Algorithms</title><link>http://arxiv.org/abs/2302.07184v2</link><description>Recent advances in computer vision and deep learning have shown promisingperformance in estimating rigid/similarity transformation between unregisteredpoint clouds of complex objects and scenes. However, their performances aremostly evaluated using a limited number of datasets from a single sensor (e.g.Kinect or RealSense cameras), lacking a comprehensive overview of theirapplicability in photogrammetric 3D mapping scenarios. In this work, we providea comprehensive review of the state-of-the-art (SOTA) point cloud registrationmethods, where we analyze and evaluate these methods using a diverse set ofpoint cloud data from indoor to satellite sources. The quantitative analysisallows for exploring the strengths, applicability, challenges, and futuretrends of these methods. In contrast to existing analysis works that introducepoint cloud registration as a holistic process, our experimental analysis isbased on its inherent two-step process to better comprehend these approachesincluding feature/keypoint-based initial coarse registration and dense fineregistration through cloud-to-cloud (C2C) optimization. More than ten methods,including classic hand-crafted, deep-learning-based feature correspondence, androbust C2C methods were tested. We observed that the success rate of most ofthe algorithms are fewer than 40% over the datasets we tested and there arestill are large margin of improvement upon existing algorithms concerning 3Dsparse corresopondence search, and the ability to register point clouds withcomplex geometry and occlusions. With the evaluated statistics on threedatasets, we conclude the best-performing methods for each step and provide ourrecommendations, and outlook future efforts.</description><author>Ningli Xu, Rongjun Qin, Shuang Song</author><pubDate>Mon, 14 Aug 2023 15:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07184v2</guid></item><item><title>Fairness in Machine Learning meets with Equity in Healthcare</title><link>http://arxiv.org/abs/2305.07041v2</link><description>With the growing utilization of machine learning in healthcare, there isincreasing potential to enhance healthcare outcomes. However, this also bringsthe risk of perpetuating biases in data and model design that can harm certaindemographic groups based on factors such as age, gender, and race. This studyproposes an artificial intelligence framework, grounded in software engineeringprinciples, for identifying and mitigating biases in data and models whileensuring fairness in healthcare settings. A case study is presented todemonstrate how systematic biases in data can lead to amplified biases in modelpredictions, and machine learning methods are suggested to prevent such biases.Future research aims to test and validate the proposed ML framework inreal-world clinical settings to evaluate its impact on promoting health equity.</description><author>Shaina Raza, Parisa Osivand Pour, Syed Raza Bashir</author><pubDate>Mon, 14 Aug 2023 15:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07041v2</guid></item><item><title>SEMI-CenterNet: A Machine Learning Facilitated Approach for Semiconductor Defect Inspection</title><link>http://arxiv.org/abs/2308.07180v1</link><description>Continual shrinking of pattern dimensions in the semiconductor domain ismaking it increasingly difficult to inspect defects due to factors such as thepresence of stochastic noise and the dynamic behavior of defect patterns andtypes. Conventional rule-based methods and non-parametric supervised machinelearning algorithms like KNN mostly fail at the requirements of semiconductordefect inspection at these advanced nodes. Deep Learning (DL)-based methodshave gained popularity in the semiconductor defect inspection domain becausethey have been proven robust towards these challenging scenarios. In thisresearch work, we have presented an automated DL-based approach for efficientlocalization and classification of defects in SEM images. We have proposedSEMI-CenterNet (SEMI-CN), a customized CN architecture trained on SEM images ofsemiconductor wafer defects. The use of the proposed CN approach allowsimproved computational efficiency compared to previously studied DL models.SEMI-CN gets trained to output the center, class, size, and offset of a defectinstance. This is different from the approach of most object detection modelsthat use anchors for bounding box prediction. Previous methods predictredundant bounding boxes, most of which are discarded in postprocessing. CNmitigates this by only predicting boxes for likely defect center points. Wetrain SEMI-CN on two datasets and benchmark two ResNet backbones for theframework. Initially, ResNet models pretrained on the COCO dataset undergotraining using two datasets separately. Primarily, SEMI-CN shows significantimprovement in inference time against previous research works. Finally,transfer learning (using weights of custom SEM dataset) is applied from ADIdataset to AEI dataset and vice-versa, which reduces the required training timefor both backbones to reach the best mAP against conventional training method.</description><author>Vic De Ridder, Bappaditya Dey, Enrique Dehaerne, Sandip Halder, Stefan De Gendt, Bartel Van Waeyenberge</author><pubDate>Mon, 14 Aug 2023 15:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07180v1</guid></item><item><title>Incorporating Annotator Uncertainty into Representations of Discourse Relations</title><link>http://arxiv.org/abs/2308.07179v1</link><description>Annotation of discourse relations is a known difficult task, especially fornon-expert annotators. In this paper, we investigate novice annotators'uncertainty on the annotation of discourse relations on spoken conversationaldata. We find that dialogue context (single turn, pair of turns within speaker,and pair of turns across speakers) is a significant predictor of confidencescores. We compute distributed representations of discourse relations fromco-occurrence statistics that incorporate information about confidence scoresand dialogue context. We perform a hierarchical clustering analysis using theserepresentations and show that weighting discourse relation representations withinformation about confidence and dialogue context coherently models ourannotators' uncertainty about discourse relation labels.</description><author>S. Magalí López Cortez, Cassandra L. Jacobs</author><pubDate>Mon, 14 Aug 2023 15:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07179v1</guid></item><item><title>Knowledge Restore and Transfer for Multi-label Class-Incremental Learning</title><link>http://arxiv.org/abs/2302.13334v3</link><description>Current class-incremental learning research mainly focuses on single-labelclassification tasks while multi-label class-incremental learning (MLCIL) withmore practical application scenarios is rarely studied. Although there havebeen many anti-forgetting methods to solve the problem of catastrophicforgetting in class-incremental learning, these methods have difficulty insolving the MLCIL problem due to label absence and information dilution. Inthis paper, we propose a knowledge restore and transfer (KRT) framework forMLCIL, which includes a dynamic pseudo-label (DPL) module to restore the oldclass knowledge and an incremental cross-attention(ICA) module to savesession-specific knowledge and transfer old class knowledge to the new modelsufficiently. Besides, we propose a token loss to jointly optimize theincremental cross-attention module. Experimental results on MS-COCO and PASCALVOC datasets demonstrate the effectiveness of our method for improvingrecognition performance and mitigating forgetting on multi-labelclass-incremental learning tasks.</description><author>Songlin Dong, Haoyu Luo, Yuhang He, Xing Wei, Yihong Gong</author><pubDate>Mon, 14 Aug 2023 15:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13334v3</guid></item><item><title>Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements</title><link>http://arxiv.org/abs/2308.07175v1</link><description>Recent work has shown that $n$-qubit quantum states output by circuits withat most $t$ single-qubit non-Clifford gates can be learned to trace distance$\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples. All prioralgorithms achieving this runtime use entangled measurements across two copiesof the input state. In this work, we give a similarly efficient algorithm thatlearns the same class of states using only single-copy measurements.</description><author>Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang</author><pubDate>Mon, 14 Aug 2023 15:32:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07175v1</guid></item><item><title>PitchNet: A Fully Convolutional Neural Network for Pitch Estimation</title><link>http://arxiv.org/abs/2308.07170v1</link><description>In the domain of music and sound processing, pitch extraction plays a pivotalrole. This research introduces "PitchNet", a convolutional neural networktailored for pitch extraction from the human singing voice, including acapellaperformances. Integrating autocorrelation with deep learning techniques,PitchNet aims to optimize the accuracy of pitch detection. Evaluation acrossdatasets comprising synthetic sounds, opera recordings, and time-stretchedvowels demonstrates its efficacy. This work paves the way for enhanced pitchextraction in both music and voice settings.</description><author>Jeremy Cochoy</author><pubDate>Mon, 14 Aug 2023 15:26:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07170v1</guid></item><item><title>Inadequately Pre-trained Models are Better Feature Extractors</title><link>http://arxiv.org/abs/2203.04668v2</link><description>Pre-training has been a popular learning paradigm in deep learning era,especially in annotation-insufficient scenario. Better ImageNet pre-trainedmodels have been demonstrated, from the perspective of architecture, byprevious research to have better transferability to downstream tasks. However,in this paper, we found that during the same pre-training process, models atmiddle epochs, which is inadequately pre-trained, can outperform fully trainedmodels when used as feature extractors (FE), while the fine-tuning (FT)performance still grows with the source performance. This reveals that there isnot a solid positive correlation between top-1 accuracy on ImageNet and thetransferring result on target data. Based on the contradictory phenomenonbetween FE and FT that better feature extractor fails to be fine-tuned betteraccordingly, we conduct comprehensive analyses on features before softmax layerto provide insightful explanations. Our discoveries suggest that, duringpre-training, models tend to first learn spectral components corresponding tolarge singular values and the residual components contribute more whenfine-tuning.</description><author>Andong Deng, Xingjian Li, Di Hu, Tianyang Wang, Haoyi Xiong, Chengzhong Xu</author><pubDate>Mon, 14 Aug 2023 15:24:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.04668v2</guid></item><item><title>HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization</title><link>http://arxiv.org/abs/2308.07163v1</link><description>Sparse neural networks are a key factor in developing resource-efficientmachine learning applications. We propose the novel and powerful sparselearning method Adaptive Regularized Training (ART) to compress dense intosparse networks. Instead of the commonly used binary mask during training toreduce the number of model weights, we inherently shrink weights close to zeroin an iterative manner with increasing weight regularization. Our methodcompresses the pre-trained model knowledge into the weights of highestmagnitude. Therefore, we introduce a novel regularization loss namedHyperSparse that exploits the highest weights while conserving the ability ofweight exploration. Extensive experiments on CIFAR and TinyImageNet show thatour method leads to notable performance gains compared to other sparsificationmethods, especially in extremely high sparsity regimes up to 99.8 percent modelsparsity. Additional investigations provide new insights into the patterns thatare encoded in weights with high magnitudes.</description><author>Patrick Glandorf, Timo Kaiser, Bodo Rosenhahn</author><pubDate>Mon, 14 Aug 2023 15:18:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07163v1</guid></item><item><title>Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation</title><link>http://arxiv.org/abs/2303.01464v2</link><description>We present the OMG-CMDP! algorithm for regret minimization in adversarialContextual MDPs. The algorithm operates under the minimal assumptions ofrealizable function class and access to online least squares and log lossregression oracles. Our algorithm is efficient (assuming efficient onlineregression oracles), simple and robust to approximation errors. It enjoys an$\widetilde{O}(H^{2.5} \sqrt{ T|S||A| ( \mathcal{R}(\mathcal{O}) + H\log(\delta^{-1}) )})$ regret guarantee, with $T$ being the number of episodes,$S$ the state space, $A$ the action space, $H$ the horizon and$\mathcal{R}(\mathcal{O}) = \mathcal{R}(\mathcal{O}_{\mathrm{sq}}^\mathcal{F})+ \mathcal{R}(\mathcal{O}_{\mathrm{log}}^\mathcal{P})$ is the sum of theregression oracles' regret, used to approximate the context-dependent rewardsand dynamics, respectively. To the best of our knowledge, our algorithm is thefirst efficient rate optimal regret minimization algorithm for adversarialCMDPs that operates under the minimal standard assumption of online functionapproximation.</description><author>Orin Levy, Alon Cohen, Asaf Cassel, Yishay Mansour</author><pubDate>Mon, 14 Aug 2023 15:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01464v2</guid></item><item><title>Hybrid quantum-classical machine learning for generative chemistry and drug design</title><link>http://arxiv.org/abs/2108.11644v3</link><description>Deep generative chemistry models emerge as powerful tools to expedite drugdiscovery. However, the immense size and complexity of the structural space ofall possible drug-like molecules pose significant obstacles, which could beovercome with hybrid architectures combining quantum computers with deepclassical networks. As the first step toward this goal, we built a compactdiscrete variational autoencoder (DVAE) with a Restricted Boltzmann Machine(RBM) of reduced size in its latent layer. The size of the proposed model wassmall enough to fit on a state-of-the-art D-Wave quantum annealer and allowedtraining on a subset of the ChEMBL dataset of biologically active compounds.Finally, we generated 2331 novel chemical structures with medicinal chemistryand synthetic accessibility properties in the ranges typical for molecules fromChEMBL. The presented results demonstrate the feasibility of using alreadyexisting or soon-to-be-available quantum computing devices as testbeds forfuture drug discovery applications.</description><author>A. I. Gircha, A. S. Boev, K. Avchaciov, P. O. Fedichev, A. K. Fedorov</author><pubDate>Mon, 14 Aug 2023 15:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.11644v3</guid></item><item><title>SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation</title><link>http://arxiv.org/abs/2308.07156v1</link><description>The Segment Anything Model (SAM) serves as a fundamental model for semanticsegmentation and demonstrates remarkable generalization capabilities across awide range of downstream scenarios. In this empirical study, we examine SAM'srobustness and zero-shot generalizability in the field of robotic surgery. Wecomprehensively explore different scenarios, including prompted and unpromptedsituations, bounding box and points-based prompt approaches, as well as theability to generalize under corruptions and perturbations at five severitylevels. Additionally, we compare the performance of SAM with state-of-the-artsupervised models. We conduct all the experiments with two well-known roboticinstrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges.Our extensive evaluation results reveal that although SAM shows remarkablezero-shot generalization ability with bounding box prompts, it struggles tosegment the whole instrument with point-based prompts and unprompted settings.Furthermore, our qualitative figures demonstrate that the model either failedto predict certain parts of the instrument mask (e.g., jaws, wrist) orpredicted parts of the instrument as wrong classes in the scenario ofoverlapping instruments within the same bounding box or with the point-basedprompt. In fact, SAM struggles to identify instruments in complex surgicalscenarios characterized by the presence of blood, reflection, blur, and shade.Additionally, SAM is insufficiently robust to maintain high performance whensubjected to various forms of data corruption. We also attempt to fine-tune SAMusing Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows thecapability in class-wise mask prediction without prompt. Therefore, we canargue that, without further domain-specific fine-tuning, SAM is not ready fordownstream surgical tasks.</description><author>An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren</author><pubDate>Mon, 14 Aug 2023 15:09:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07156v1</guid></item><item><title>Discrete Message via Online Clustering Labels in Decentralized POMDP</title><link>http://arxiv.org/abs/2308.03358v2</link><description>Communication is crucial for solving cooperative Multi-Agent ReinforcementLearning tasks in Partially-Observable Markov Decision Processes. Existingworks often rely on black-box methods to encode local information/features intomessages shared with other agents. However, such black-box approaches areunable to provide any quantitative guarantees on the expected return and oftenlead to the generation of continuous messages with high communication overheadand poor interpretability. In this paper, we establish an upper bound on thereturn gap between an ideal policy with full observability and an optimalpartially-observable policy with discrete communication. This result enables usto recast multi-agent communication into a novel online clustering problem overthe local observations at each agent, with messages as cluster labels and theupper bound on the return gap as clustering loss. By minimizing the upperbound, we propose a surprisingly simple design of message generation functionsin multi-agent communication and integrate it with reinforcement learning usinga Regularized Information Maximization loss function. Evaluations show that theproposed discrete communication significantly outperforms state-of-the-artmulti-agent communication baselines and can achieve nearly-optimal returns withfew-bit messages that are naturally interpretable.</description><author>Jingdi Chen, Tian Lan</author><pubDate>Mon, 14 Aug 2023 15:06:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03358v2</guid></item><item><title>DELO: Deep Evidential LiDAR Odometry using Partial Optimal Transport</title><link>http://arxiv.org/abs/2308.07153v1</link><description>Accurate, robust, and real-time LiDAR-based odometry (LO) is imperative formany applications like robot navigation, globally consistent 3D scene mapreconstruction, or safe motion-planning. Though LiDAR sensor is known for itsprecise range measurement, the non-uniform and uncertain point sampling densityinduce structural inconsistencies. Hence, existing supervised and unsupervisedpoint set registration methods fail to establish one-to-one matchingcorrespondences between LiDAR frames. We introduce a novel deep learning-basedreal-time (approx. 35-40ms per frame) LO method that jointly learns accurateframe-to-frame correspondences and model's predictive uncertainty (PU) asevidence to safe-guard LO predictions. In this work, we propose (i) partialoptimal transportation of LiDAR feature descriptor for robust LO estimation,(ii) joint learning of predictive uncertainty while learning odometry overdriving sequences, and (iii) demonstrate how PU can serve as evidence fornecessary pose-graph optimization when LO network is either under or overconfident. We evaluate our method on KITTI dataset and show competitiveperformance, even superior generalization ability over recent state-of-the-artapproaches. Source codes are available.</description><author>Sk Aziz Ali, Djamila Aouada, Gerd Reis, Didier Stricker</author><pubDate>Mon, 14 Aug 2023 15:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07153v1</guid></item><item><title>Decoupling Dynamic Monocular Videos for Dynamic View Synthesis</title><link>http://arxiv.org/abs/2304.01716v2</link><description>The challenge of dynamic view synthesis from dynamic monocular videos, i.e.,synthesizing novel views for free viewpoints given a monocular video of adynamic scene captured by a moving camera, mainly lies in accurately modelingthe dynamic objects of a scene using limited 2D frames, each with a varyingtimestamp and viewpoint. Existing methods usually require pre-processed 2Doptical flow and depth maps by off-the-shelf methods to supervise the network,making them suffer from the inaccuracy of the pre-processed supervision and theambiguity when lifting the 2D information to 3D. In this paper, we tackle thischallenge in an unsupervised fashion. Specifically, we decouple the motion ofthe dynamic objects into object motion and camera motion, respectivelyregularized by proposed unsupervised surface consistency and patch-basedmulti-view constraints. The former enforces the 3D geometric surfaces of movingobjects to be consistent over time, while the latter regularizes theirappearances to be consistent across different viewpoints. Such a fine-grainedmotion formulation can alleviate the learning difficulty for the network, thusenabling it to produce not only novel views with higher quality but also moreaccurate scene flows and depth than existing methods requiring extrasupervision.</description><author>Meng You, Junhui Hou</author><pubDate>Mon, 14 Aug 2023 15:02:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01716v2</guid></item><item><title>Diffusion Based Augmentation for Captioning and Retrieval in Cultural Heritage</title><link>http://arxiv.org/abs/2308.07151v1</link><description>Cultural heritage applications and advanced machine learning models arecreating a fruitful synergy to provide effective and accessible ways ofinteracting with artworks. Smart audio-guides, personalized art-related contentand gamification approaches are just a few examples of how technology can beexploited to provide additional value to artists or exhibitions. Nonetheless,from a machine learning point of view, the amount of available artistic data isoften not enough to train effective models. Off-the-shelf computer visionmodules can still be exploited to some extent, yet a severe domain shift ispresent between art images and standard natural image datasets used to trainsuch models. As a result, this can lead to degraded performance. This paperintroduces a novel approach to address the challenges of limited annotated dataand domain shifts in the cultural heritage domain. By leveraging generativevision-language models, we augment art datasets by generating diversevariations of artworks conditioned on their captions. This augmentationstrategy enhances dataset diversity, bridging the gap between natural imagesand artworks, and improving the alignment of visual cues with knowledge fromgeneral-purpose datasets. The generated variations assist in training visionand language models with a deeper understanding of artistic characteristics andthat are able to generate better captions with appropriate jargon.</description><author>Dario Cioni, Lorenzo Berlincioni, Federico Becattini, Alberto del Bimbo</author><pubDate>Mon, 14 Aug 2023 14:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07151v1</guid></item><item><title>OctoPack: Instruction Tuning Code Large Language Models</title><link>http://arxiv.org/abs/2308.07124v1</link><description>Finetuning large language models (LLMs) on instructions leads to vastperformance improvements on natural language tasks. We apply instruction tuningusing code, leveraging the natural structure of Git commits, which pair codechanges with human instructions. We compile CommitPack: 4 terabytes of Gitcommits across 350 programming languages. We benchmark CommitPack against othernatural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16Bparameter StarCoder model, and achieve state-of-the-art performance amongmodels not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2%pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmarkto a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis)across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models,OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack amongall permissive models, demonstrating CommitPack's benefits in generalizing to awider set of languages and natural coding tasks. Code, models and data arefreely available at https://github.com/bigcode-project/octopack.</description><author>Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, Shayne Longpre</author><pubDate>Mon, 14 Aug 2023 14:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07124v1</guid></item><item><title>CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation</title><link>http://arxiv.org/abs/2308.07146v1</link><description>Vision-Language Pretraining (VLP) has shown impressive results on diversedownstream tasks by offline training on large-scale datasets. Regarding thegrowing nature of real-world data, such an offline training paradigm onever-expanding data is unsustainable, because models lack the continuallearning ability to accumulate knowledge constantly. However, most continuallearning studies are limited to uni-modal classification and existingmulti-modal datasets cannot simulate continual non-stationary data streamscenarios. To support the study of Vision-Language Continual Pretraining(VLCP), we first contribute a comprehensive and unified benchmark dataset P9Dwhich contains over one million product image-text pairs from 9 industries. Thedata from each industry as an independent task supports continual learning andconforms to the real-world long-tail nature to simulate pretraining on webdata. We comprehensively study the characteristics and challenges of VLCP, andpropose a new algorithm: Compatible momentum contrast with TopologyPreservation, dubbed CTP. The compatible momentum model absorbs the knowledgeof the current and previous-task models to flexibly update the modal feature.Moreover, Topology Preservation transfers the knowledge of embedding acrosstasks while preserving the flexibility of feature adjustment. The experimentalresults demonstrate our method not only achieves superior performance comparedwith other baselines but also does not bring an expensive training burden.Dataset and codes are available at https://github.com/KevinLight831/CTP.</description><author>Hongguang Zhu, Yunchao Wei, Xiaodan Liang, Chunjie Zhang, Yao Zhao</author><pubDate>Mon, 14 Aug 2023 14:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07146v1</guid></item><item><title>MADiff: Offline Multi-agent Learning with Diffusion Models</title><link>http://arxiv.org/abs/2305.17330v2</link><description>Diffusion model (DM), as a powerful generative model, recently achieved hugesuccess in various scenarios including offline reinforcement learning, wherethe policy learns to conduct planning by generating trajectory in the onlineevaluation. However, despite the effectiveness shown for single-agent learning,it remains unclear how DMs can operate in multi-agent problems, where agentscan hardly complete teamwork without good coordination by independentlymodeling each agent's trajectories. In this paper, we propose MADiff, a novelgenerative multi-agent learning framework to tackle this problem. MADiff isrealized with an attention-based diffusion model to model the complexcoordination among behaviors of multiple diffusion agents. To the best of ourknowledge, MADiff is the first diffusion-based multi-agent offline RLframework, which behaves as both a decentralized policy and a centralizedcontroller, which includes opponent modeling and can be used for multi-agenttrajectory prediction. MADiff takes advantage of the powerful generativeability of diffusion while well-suited in modeling complex multi-agentinteractions. Our experiments show the superior performance of MADiff comparedto baseline algorithms in a range of multi-agent learning tasks.</description><author>Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai Xu, Yong Yu, Stefano Ermon, Weinan Zhang</author><pubDate>Mon, 14 Aug 2023 14:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17330v2</guid></item><item><title>Optimizing the AI Development Process by Providing the Best Support Environment</title><link>http://arxiv.org/abs/2305.00136v2</link><description>The purpose of this study is to investigate the development process forArtificial inelegance (AI) and machine learning (ML) applications in order toprovide the best support environment. The main stages of ML are problemunderstanding, data management, model building, model deployment andmaintenance. This project focuses on investigating the data management stage ofML development and its obstacles as it is the most important stage of machinelearning development because the accuracy of the end model is relying on thekind of data fed into the model. The biggest obstacle found on this stage wasthe lack of sufficient data for model learning, especially in the fields wheredata is confidential. This project aimed to build and develop a framework forresearchers and developers that can help solve the lack of sufficient dataduring data management stage. The framework utilizes several data augmentationtechniques that can be used to generate new data from the original datasetwhich can improve the overall performance of the ML applications by increasingthe quantity and quality of available data to feed the model with the bestpossible data. The framework was built using python language to perform dataaugmentation using deep learning advancements.</description><author>Taha Khamis, Hamam Mokayed</author><pubDate>Mon, 14 Aug 2023 14:48:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00136v2</guid></item><item><title>Pairing interacting protein sequences using masked language modeling</title><link>http://arxiv.org/abs/2308.07136v1</link><description>Predicting which proteins interact together from amino-acid sequences is animportant task. We develop a method to pair interacting protein sequences whichleverages the power of protein language models trained on multiple sequencealignments, such as MSA Transformer and the EvoFormer module of AlphaFold. Weformulate the problem of pairing interacting partners among the paralogs of twoprotein families in a differentiable way. We introduce a method called DiffPALMthat solves it by exploiting the ability of MSA Transformer to fill in maskedamino acids in multiple sequence alignments using the surrounding context. MSATransformer encodes coevolution between functionally or structurally coupledamino acids. We show that it captures inter-chain coevolution, while it wastrained on single-chain data, which means that it can be usedout-of-distribution. Relying on MSA Transformer without fine-tuning, DiffPALMoutperforms existing coevolution-based pairing methods on difficult benchmarksof shallow multiple sequence alignments extracted from ubiquitous prokaryoticprotein datasets. It also outperforms an alternative method based on astate-of-the-art protein language model trained on single sequences. Pairedalignments of interacting protein sequences are a crucial ingredient ofsupervised deep learning methods to predict the three-dimensional structure ofprotein complexes. DiffPALM substantially improves the structure prediction ofsome eukaryotic protein complexes by AlphaFold-Multimer, without significantlydeteriorating any of those we tested. It also achieves competitive performancewith using orthology-based pairing.</description><author>Umberto Lupo, Damiano Sgarbossa, Anne-Florence Bitbol</author><pubDate>Mon, 14 Aug 2023 14:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07136v1</guid></item><item><title>TeViS:Translating Text Synopses to Video Storyboards</title><link>http://arxiv.org/abs/2301.00135v3</link><description>A video storyboard is a roadmap for video creation which consists ofshot-by-shot images to visualize key plots in a text synopsis. Creating videostoryboards, however, remains challenging which not only requires cross-modalassociation between high-level texts and images but also demands long-termreasoning to make transitions smooth across shots. In this paper, we propose anew task called Text synopsis to Video Storyboard (TeViS) which aims toretrieve an ordered sequence of images as the video storyboard to visualize thetext synopsis. We construct a MovieNet-TeViS dataset based on the publicMovieNet dataset. It contains 10K text synopses each paired with keyframesmanually selected from corresponding movies by considering both relevance andcinematic coherence. To benchmark the task, we present strong CLIP-basedbaselines and a novel VQ-Trans. VQ-Trans first encodes text synopsis and imagesinto a joint embedding space and uses vector quantization (VQ) to improve thevisual representation. Then, it auto-regressively generates a sequence ofvisual features for retrieval and ordering. Experimental results demonstratethat VQ-Trans significantly outperforms prior methods and the CLIP-basedbaselines. Nevertheless, there is still a large gap compared to humanperformance suggesting room for promising future work. The code and data areavailable at: \url{https://ruc-aimind.github.io/projects/TeViS/}</description><author>Xu Gu, Yuchong Sun, Feiyue Ni, Shizhe Chen, Xihua Wang, Ruihua Song, Boyuan Li, Xiang Cao</author><pubDate>Mon, 14 Aug 2023 14:41:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00135v3</guid></item><item><title>FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis</title><link>http://arxiv.org/abs/2306.17723v4</link><description>Neural Radiance Field (NeRF) has been a mainstream in novel view synthesiswith its remarkable quality of rendered images and simple architecture.Although NeRF has been developed in various directions improving continuouslyits performance, the necessity of a dense set of multi-view images still existsas a stumbling block to progress for practical application. In this work, wepropose FlipNeRF, a novel regularization method for few-shot novel viewsynthesis by utilizing our proposed flipped reflection rays. The flippedreflection rays are explicitly derived from the input ray directions andestimated normal vectors, and play a role of effective additional training rayswhile enabling to estimate more accurate surface normals and learn the 3Dgeometry effectively. Since the surface normal and the scene depth are bothderived from the estimated densities along a ray, the accurate surface normalleads to more exact depth estimation, which is a key factor for few-shot novelview synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Lossand Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate morereliable outputs with reducing floating artifacts effectively across thedifferent scene structures, and enhance the feature-level consistency betweenthe pair of the rays cast toward the photo-consistent pixels without anyadditional feature extractor, respectively. Our FlipNeRF achieves the SOTAperformance on the multiple benchmarks across all the scenarios.</description><author>Seunghyeon Seo, Yeonjin Chang, Nojun Kwak</author><pubDate>Mon, 14 Aug 2023 14:41:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17723v4</guid></item><item><title>Natural Language is All a Graph Needs</title><link>http://arxiv.org/abs/2308.07134v1</link><description>The emergence of large-scale pre-trained language models, such as ChatGPT,has revolutionized various research fields in artificial intelligence.Transformers-based large language models (LLMs) have gradually replaced CNNsand RNNs to unify fields of computer vision and natural language processing.Compared with the data that exists relatively independently such as images,videos or texts, graph is a type of data that contains rich structural andrelational information. Meanwhile, natural language, as one of the mostexpressive mediums, excels in describing complex structures. However, existingwork on incorporating graph learning problems into the generative languagemodeling framework remains very limited. As the importance of language modelscontinues to grow, it becomes essential to explore whether LLMs can alsoreplace GNNs as the foundational model for graphs. In this paper, we proposeInstructGLM (Instruction-finetuned Graph Language Model), systematically designhighly scalable prompts based on natural language instructions, and use naturallanguage to describe the geometric structure and node features of the graph forinstruction tuning an LLMs to perform learning and inference on graphs in agenerative manner. Our method exceeds all competitive GNN baselines onogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness ofour method and sheds light on generative language models replacing GNNs as thefoundation model for graph machine learning.</description><author>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang</author><pubDate>Mon, 14 Aug 2023 14:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07134v1</guid></item><item><title>SAT Requires Exhaustive Search</title><link>http://arxiv.org/abs/2302.09512v6</link><description>In this paper, by constructing extremely hard examples of CSP (with largedomains) and SAT (with long clauses), we prove that such examples cannot besolved without exhaustive search, which is stronger than P $\neq$ NP. Thisconstructive approach for proving impossibility results is very different (andmissing) from those currently used in computational complexity theory, but issimilar to that used by Kurt G\"{o}del in proving his famous logicalimpossibility results. Just as shown by G\"{o}del's results that proving formalunprovability is feasible in mathematics, the results of this paper show thatproving computational hardness is not hard in mathematics. Specifically,proving lower bounds for many problems, such as 3-SAT, can be challengingbecause these problems have various effective strategies available for avoidingexhaustive search. However, in cases of extremely hard examples, exhaustivesearch may be the only viable option, and proving its necessity becomes morestraightforward. Consequently, it makes the separation between SAT (with longclauses) and 3-SAT much easier than that between 3-SAT and 2-SAT. Finally, themain results of this paper demonstrate that the fundamental difference betweenthe syntax and the semantics revealed by G\"{o}del's results also exists in CSPand SAT.</description><author>Ke Xu, Guangyan Zhou</author><pubDate>Mon, 14 Aug 2023 14:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09512v6</guid></item><item><title>RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information</title><link>http://arxiv.org/abs/2302.13084v2</link><description>Remotely captured images possess an immense scale and object appearancevariability due to the complex scene. It becomes challenging to capture theunderlying attributes in the global and local context for their segmentation.Existing networks struggle to capture the inherent features due to thecluttered background. To address these issues, we propose a remote sensingimage segmentation network, RemoteNet, for semantic segmentation of remotesensing images. We capture the global and local features by leveraging thebenefits of the transformer and convolution mechanisms. RemoteNet is anencoder-decoder design that uses multi-scale features. We construct anattention map module to generate channel-wise attention scores for fusing thesefeatures. We construct a global-local transformer block (GLTB) in the decodernetwork to support learning robust representations during a decoding phase.Further, we designed a feature refinement module to refine the fused output ofthe shallow stage encoder feature and the deepest GLTB feature of the decoder.Experimental findings on the two public datasets show the effectiveness of theproposed RemoteNet.</description><author>Satyawant Kumar, Abhishek Kumar, Dong-Gyu Lee</author><pubDate>Mon, 14 Aug 2023 14:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13084v2</guid></item><item><title>A Time-aware tensor decomposition for tracking evolving patterns</title><link>http://arxiv.org/abs/2308.07126v1</link><description>Time-evolving data sets can often be arranged as a higher-order tensor withone of the modes being the time mode. While tensor factorizations have beensuccessfully used to capture the underlying patterns in such higher-order datasets, the temporal aspect is often ignored, allowing for the reordering of timepoints. In recent studies, temporal regularizers are incorporated in the timemode to tackle this issue. Nevertheless, existing approaches still do not allowunderlying patterns to change in time (e.g., spatial changes in the brain,contextual changes in topics). In this paper, we propose temporal PARAFAC2(tPARAFAC2): a PARAFAC2-based tensor factorization method with temporalregularization to extract gradually evolving patterns from temporal data.Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2can capture the underlying evolving patterns accurately performing better thanPARAFAC2 and coupled matrix factorization with temporal smoothnessregularization.</description><author>Christos Chatzis, Max Pfeffer, Pedro Lind, Evrim Acar</author><pubDate>Mon, 14 Aug 2023 14:13:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07126v1</guid></item><item><title>An Outlook into the Future of Egocentric Vision</title><link>http://arxiv.org/abs/2308.07123v1</link><description>What will the future be? We wonder! In this survey, we explore the gapbetween current research in egocentric vision and the ever-anticipated future,where wearable computing, with outward facing cameras and digital overlays, isexpected to be integrated in our every day lives. To understand this gap, thearticle starts by envisaging the future through character-based stories,showcasing through examples the limitations of current technology. We thenprovide a mapping between this future and previously defined research tasks.For each task, we survey its seminal works, current state-of-the-artmethodologies and available datasets, then reflect on shortcomings that limitits applicability to future research. Note that this survey focuses on softwaremodels for egocentric vision, independent of any specific hardware. The paperconcludes with recommendations for areas of immediate explorations so as tounlock our path to the future always-on, personalised and life-enhancingegocentric vision.</description><author>Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Giovanni Maria Farinella, Dima Damen, Tatiana Tommasi</author><pubDate>Mon, 14 Aug 2023 14:10:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07123v1</guid></item><item><title>Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers</title><link>http://arxiv.org/abs/2308.07121v1</link><description>We propose a shift towards end-to-end learning in bird sound monitoring bycombining self-supervised (SSL) and deep active learning (DAL). Leveragingtransformer models, we aim to bypass traditional spectrogram conversions,enabling direct raw audio processing. ActiveBird2Vec is set to generatehigh-quality bird sound representations through SSL, potentially acceleratingthe assessment of environmental changes and decision-making processes for windfarms. Additionally, we seek to utilize the wide variety of bird vocalizationsthrough DAL, reducing the reliance on extensively labeled datasets by humanexperts. We plan to curate a comprehensive set of tasks through HuggingfaceDatasets, enhancing future comparability and reproducibility of bioacousticresearch. A comparative analysis between various transformer models will beconducted to evaluate their proficiency in bird sound recognition tasks. We aimto accelerate the progression of avian bioacoustic research and contribute tomore effective conservation strategies.</description><author>Lukas Rauch, Raphael Schwinger, Moritz Wirth, Bernhard Sick, Sven Tomforde, Christoph Scholz</author><pubDate>Mon, 14 Aug 2023 14:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07121v1</guid></item><item><title>Inverse Extended Kalman Filter -- Part I: Fundamentals</title><link>http://arxiv.org/abs/2201.01539v3</link><description>Recent advances in counter-adversarial systems have garnered significantresearch attention to inverse filtering from a Bayesian perspective. Forexample, interest in estimating the adversary's Kalman filter tracked estimatewith the purpose of predicting the adversary's future steps has led to recentformulations of inverse Kalman filter (I-KF). In this context of inversefiltering, we address the key challenges of non-linear process dynamics andunknown input to the forward filter by proposing an inverse extended Kalmanfilter (I-EKF). The purpose of this paper and the companion paper (Part II) isto develop the theory of I-EKF in detail. In this paper, we assume perfectsystem model information and derive I-EKF with and without an unknown inputwhen both forward and inverse state-space models are non-linear. In theprocess, I-KF-with-unknown-input is also obtained. We then provide theoreticalstability guarantees using both bounded non-linearity and unknown matrixapproaches and prove the I-EKF's consistency. Numerical experiments validateour methods for various proposed inverse filters using the recursiveCram\'{e}r-Rao lower bound as a benchmark. In the companion paper (Part II), wefurther generalize these formulations to highly non-linear models and proposereproducing kernel Hilbert space-based EKF to handle incomplete system modelinformation.</description><author>Himali Singh, Arpan Chattopadhyay, Kumar Vijay Mishra</author><pubDate>Mon, 14 Aug 2023 14:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.01539v3</guid></item><item><title>Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice</title><link>http://arxiv.org/abs/2308.07120v1</link><description>Much of the recent discourse within the NLP research community has beencentered around Large Language Models (LLMs), their functionality and potential-- yet not only do we not have a working definition of LLMs, but much of thisdiscourse relies on claims and assumptions that are worth re-examining. Thisposition paper contributes a definition of LLMs, explicates some of theassumptions made regarding their functionality, and outlines the existingevidence for and against them. We conclude with suggestions for researchdirections and their framing in future work.</description><author>Alexandra Sasha Luccioni, Anna Rogers</author><pubDate>Mon, 14 Aug 2023 14:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07120v1</guid></item><item><title>On the Importance of Spatial Relations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2308.07119v1</link><description>Deep learning has achieved great success in video recognition, yet stillstruggles to recognize novel actions when faced with only a few examples. Totackle this challenge, few-shot action recognition methods have been proposedto transfer knowledge from a source dataset to a novel target dataset with onlyone or a few labeled videos. However, existing methods mainly focus on modelingthe temporal relations between the query and support videos while ignoring thespatial relations. In this paper, we find that the spatial misalignment betweenobjects also occurs in videos, notably more common than the temporalinconsistency. We are thus motivated to investigate the importance of spatialrelations and propose a more accurate few-shot action recognition method thatleverages both spatial and temporal information. Particularly, a novel SpatialAlignment Cross Transformer (SA-CT) which learns to re-adjust the spatialrelations and incorporates the temporal information is contributed. Experimentsreveal that, even without using any temporal information, the performance ofSA-CT is comparable to temporal based methods on 3/4 benchmarks. To furtherincorporate the temporal information, we propose a simple yet effectiveTemporal Mixer module. The Temporal Mixer enhances the video representation andimproves the performance of the full SA-CT model, achieving very competitiveresults. In this work, we also exploit large-scale pretrained models forfew-shot action recognition, providing useful insights for this researchdirection.</description><author>Yilun Zhang, Yuqian Fu, Xingjun Ma, Lizhe Qi, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 14 Aug 2023 13:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07119v1</guid></item><item><title>Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases</title><link>http://arxiv.org/abs/2308.07118v1</link><description>The proliferation of technologies, such as extended reality (XR), hasincreased the demand for high-quality three-dimensional (3D) graphicalrepresentations. Industrial 3D applications encompass computer-aided design(CAD), finite element analysis (FEA), scanning, and robotics. However, currentmethods employed for industrial 3D representations suffer from highimplementation costs and reliance on manual human input for accurate 3Dmodeling. To address these challenges, neural radiance fields (NeRFs) haveemerged as a promising approach for learning 3D scene representations based onprovided training 2D images. Despite a growing interest in NeRFs, theirpotential applications in various industrial subdomains are still unexplored.In this paper, we deliver a comprehensive examination of NeRF industrialapplications while also providing direction for future research endeavors. Wealso present a series of proof-of-concept experiments that demonstrate thepotential of NeRFs in the industrial domain. These experiments includeNeRF-based video compression techniques and using NeRFs for 3D motionestimation in the context of collision avoidance. In the video compressionexperiment, our results show compression savings up to 48\% and 74\% forresolutions of 1920x1080 and 300x168, respectively. The motion estimationexperiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF)and achieved an average disparity map PSNR of 23 dB and an SSIM of 0.97. Thecode for our experiments is publicly available athttps://github.com/Maftej/iisnerf .</description><author>Eugen Šlapak, Enric Pardo, Matúš Dopiriak, Taras Maksymyuk, Juraj Gazda</author><pubDate>Mon, 14 Aug 2023 13:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07118v1</guid></item><item><title>A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness</title><link>http://arxiv.org/abs/2308.01050v2</link><description>Autonomous Vehicles (AVs) have the potential to provide numerous societalbenefits, such as decreased road accidents and increased overall transportationefficiency. However, quantifying the risk associated with AVs is challengingdue to the lack of historical data and the rapidly evolving technology. Thispaper presents a data-driven framework for comparing the risk of different AVs'behaviors in various operational design domains (ODDs), based on counterfactualsimulations of "misbehaving" road users. We introduce the concept ofcounterfactual safety margin, which represents the minimum deviation fromnormal behavior that could lead to a collision. This concept helps to find themost critical scenarios but also to assess the frequency and severity of riskof AVs. We show that the proposed methodology is applicable even when the AV'sbehavioral policy is unknown -- through worst- and best-case analyses -- makingthe method useful also to external third-party risk assessors. Our experimentalresults demonstrate the correlation between the safety margin, the drivingpolicy quality, and the ODD shedding light on the relative risk associated withdifferent AV providers. This work contributes to AV safety assessment and aidsin addressing legislative and insurance concerns surrounding this emergingtechnology.</description><author>Alessandro Zanardi, Andrea Censi, Margherita Atzei, Luigi Di Lillo, Emilio Frazzoli</author><pubDate>Mon, 14 Aug 2023 13:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01050v2</guid></item><item><title>iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN</title><link>http://arxiv.org/abs/2308.07117v1</link><description>The inverse short-time Fourier transform network (iSTFTNet) has garneredattention owing to its fast, lightweight, and high-fidelity speech synthesis.It obtains these characteristics using a fast and lightweight 1D CNN as thebackbone and replacing some neural processes with iSTFT. Owing to thedifficulty of a 1D CNN to model high-dimensional spectrograms, the frequencydimension is reduced via temporal upsampling. However, this strategycompromises the potential to enhance the speed. Therefore, we proposeiSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and2D CNNs to model temporal and spectrogram structures, respectively. We designeda 2D CNN that performs frequency upsampling after conversion in a few-frequencyspace. This design facilitates the modeling of high-dimensional spectrogramswithout compromising the speed. The results demonstrated that iSTFTNet2 madeiSTFTNet faster and more lightweight with comparable speech quality. Audiosamples are available athttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.</description><author>Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Shogo Seki</author><pubDate>Mon, 14 Aug 2023 13:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07117v1</guid></item><item><title>SCSC: Spatial Cross-scale Convolution Module to Strengthen both CNNs and Transformers</title><link>http://arxiv.org/abs/2308.07110v1</link><description>This paper presents a module, Spatial Cross-scale Convolution (SCSC), whichis verified to be effective in improving both CNNs and Transformers. Nowadays,CNNs and Transformers have been successful in a variety of tasks. Especiallyfor Transformers, increasing works achieve state-of-the-art performance in thecomputer vision community. Therefore, researchers start to explore themechanism of those architectures. Large receptive fields, sparse connections,weight sharing, and dynamic weight have been considered keys to designingeffective base models. However, there are still some issues to be addressed:large dense kernels and self-attention are inefficient, and large receptivefields make it hard to capture local features. Inspired by the above analysesand to solve the mentioned problems, in this paper, we design a general moduletaking in these design keys to enhance both CNNs and Transformers. SCSCintroduces an efficient spatial cross-scale encoder and spatial embed module tocapture assorted features in one layer. On the face recognition task,FaceResNet with SCSC can improve 2.7% with 68% fewer FLOPs and 79% fewerparameters. On the ImageNet classification task, Swin Transformer with SCSC canachieve even better performance with 22% fewer FLOPs, and ResNet with CSCS canimprove 5.3% with similar complexity. Furthermore, a traditional network (e.g.,ResNet) embedded with SCSC can match Swin Transformer's performance.</description><author>Xijun Wang, Xiaojie Chu, Chunrui Han, Xiangyu Zhang</author><pubDate>Mon, 14 Aug 2023 13:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07110v1</guid></item><item><title>2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds</title><link>http://arxiv.org/abs/2308.05667v2</link><description>The commonly adopted detect-then-match approach to registration findsdifficulties in the cross-modality cases due to the incompatible keypointdetection and inconsistent feature description. We propose, 2D3D-MATR, adetection-free method for accurate and robust registration between images andpoint clouds. Our method adopts a coarse-to-fine pipeline where it firstcomputes coarse correspondences between downsampled patches of the input imageand the point cloud and then extends them to form dense correspondences betweenpixels and points within the patch region. The coarse-level patch matching isbased on transformer which jointly learns global contextual constraints withself-attention and cross-modality correlations with cross-attention. To resolvethe scale ambiguity in patch matching, we construct a multi-scale pyramid foreach image patch and learn to find for each point patch the best matching imagepatch at a proper resolution level. Extensive experiments on two publicbenchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-artP2-Net by around $20$ percentage points on inlier ratio and over $10$ points onregistration recall. Our code and models are available athttps://github.com/minhaolee/2D3DMATR.</description><author>Minhao Li, Zheng Qin, Zhirui Gao, Renjiao Yi, Chenyang Zhu, Yulan Guo, Kai Xu</author><pubDate>Mon, 14 Aug 2023 13:49:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05667v2</guid></item><item><title>Large Language Models for Information Retrieval: A Survey</title><link>http://arxiv.org/abs/2308.07107v1</link><description>As a primary means of information acquisition, information retrieval (IR)systems, such as search engines, have integrated themselves into our dailylives. These systems also serve as components of dialogue, question-answering,and recommender systems. The trajectory of IR has evolved dynamically from itsorigins in term-based methods to its integration with advanced neural models.While the neural models excel at capturing complex contextual signals andsemantic nuances, thereby reshaping the IR landscape, they still facechallenges such as data scarcity, interpretability, and the generation ofcontextually plausible yet potentially inaccurate responses. This evolutionrequires a combination of both traditional methods (such as term-based sparseretrieval methods with rapid response) and modern neural architectures (such aslanguage models with powerful language understanding capacity). Meanwhile, theemergence of large language models (LLMs), typified by ChatGPT and GPT-4, hasrevolutionized natural language processing due to their remarkable languageunderstanding, generation, generalization, and reasoning abilities.Consequently, recent research has sought to leverage LLMs to improve IRsystems. Given the rapid evolution of this research trajectory, it is necessaryto consolidate existing methodologies and provide nuanced insights through acomprehensive overview. In this survey, we delve into the confluence of LLMsand IR systems, including crucial aspects such as query rewriters, retrievers,rerankers, and readers. Additionally, we explore promising directions withinthis expanding field.</description><author>Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen</author><pubDate>Mon, 14 Aug 2023 13:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07107v1</guid></item><item><title>InterTracker: Discovering and Tracking General Objects Interacting with Hands in the Wild</title><link>http://arxiv.org/abs/2308.03061v2</link><description>Understanding human interaction with objects is an important research topicfor embodied Artificial Intelligence and identifying the objects that humansare interacting with is a primary problem for interaction understanding.Existing methods rely on frame-based detectors to locate interacting objects.However, this approach is subjected to heavy occlusions, background clutter,and distracting objects. To address the limitations, in this paper, we proposeto leverage spatio-temporal information of hand-object interaction to trackinteractive objects under these challenging cases. Without prior knowledge ofthe general objects to be tracked like object tracking problems, we firstutilize the spatial relation between hands and objects to adaptively discoverthe interacting objects from the scene. Second, the consistency and continuityof the appearance of objects between successive frames are exploited to trackthe objects. With this tracking formulation, our method also benefits fromtraining on large-scale general object-tracking datasets. We further curate avideo-level hand-object interaction dataset for testing and evaluation from100DOH. The quantitative results demonstrate that our proposed methodoutperforms the state-of-the-art methods. Specifically, in scenes withcontinuous interaction with different objects, we achieve an impressiveimprovement of about 10% as evaluated using the Average Precision (AP) metric.Our qualitative findings also illustrate that our method can produce morecontinuous trajectories for interacting objects.</description><author>Yanyan Shao, Qi Ye, Wenhan Luo, Kaihao Zhang, Jiming Chen</author><pubDate>Mon, 14 Aug 2023 13:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03061v2</guid></item><item><title>Checklist to Transparently Define Test Oracles for TP, FP, and FN Objects in Automated Driving</title><link>http://arxiv.org/abs/2308.07106v1</link><description>Popular test oracles for the perception subsystem of driving automationsystems identify true-positive (TP), false-positive (FP), and false-negative(FN) objects. Oracle transparency is needed for comparing test results and forsafety cases. To date, there exists a common notion of TPs, FPs, and FNs in thefield, but apparently no published way to comprehensively define their oracles.Therefore, this paper provides a checklist of functional aspects andimplementation details that affect the oracle behavior. Besides labelingpolicies of the test set, we cover fields of view, occlusion handling,safety-relevant areas, matching criteria, temporal and probabilistic issues,and further aspects. Even though our checklist can hardly be formalized, it canhelp practitioners maximize the transparency of their oracles, which, in turn,makes statements on object perception more reliable and comparable.</description><author>Michael Hoss</author><pubDate>Mon, 14 Aug 2023 13:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07106v1</guid></item><item><title>Can We Transfer Noise Patterns? A Multi-environment Spectrum Analysis Model Using Generated Cases</title><link>http://arxiv.org/abs/2308.01138v2</link><description>Spectrum analysis systems in online water quality testing are designed todetect types and concentrations of pollutants and enable regulatory agencies torespond promptly to pollution incidents. However, spectral data-based testingdevices suffer from complex noise patterns when deployed in non-laboratoryenvironments. To make the analysis model applicable to more environments, wepropose a noise patterns transferring model, which takes the spectrum ofstandard water samples in different environments as cases and learns thedifferences in their noise patterns, thus enabling noise patterns to transferto unknown samples. Unfortunately, the inevitable sample-level baseline noisemakes the model unable to obtain the paired data that only differ indataset-level environmental noise. To address the problem, we generate asample-to-sample case-base to exclude the interference of sample-level noise ondataset-level noise learning, enhancing the system's learning performance.Experiments on spectral data with different background noises demonstrate thegood noise-transferring ability of the proposed method against baseline systemsranging from wavelet denoising, deep neural networks, and generative models.From this research, we posit that our method can enhance the performance of DLmodels by generating high-quality cases. The source code is made publiclyavailable online at https://github.com/Magnomic/CNST.</description><author>Haiwen Du, Zheng Ju, Yu An, Honghui Du, Dongjie Zhu, Zhaoshuo Tian, Aonghus Lawlor, Ruihai Dong</author><pubDate>Mon, 14 Aug 2023 13:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01138v2</guid></item></channel></rss>