<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 29 Aug 2024 13:00:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Q-MRS: A Deep Learning Framework for Quantitative Magnetic Resonance Spectra Analysis</title><link>http://arxiv.org/abs/2408.15999v1</link><description>Magnetic resonance spectroscopy (MRS) is an established technique forstudying tissue metabolism, particularly in central nervous system disorders.While powerful and versatile, MRS is often limited by challenges associatedwith data quality, processing, and quantification. Existing MRS quantificationmethods face difficulties in balancing model complexity and reproducibilityduring spectral modeling, often falling into the trap of eitheroversimplification or over-parameterization. To address these limitations, thisstudy introduces a deep learning (DL) framework that employs transfer learning,in which the model is pre-trained on simulated datasets before it undergoesfine-tuning on in vivo data. The proposed framework showed promisingperformance when applied to the Philips dataset from the BIG GABA repositoryand represents an exciting advancement in MRS data analysis.</description><author>Christopher J. Wu, Lawrence S. Kegeles, Jia Guo</author><pubDate>Wed, 28 Aug 2024 18:05:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15999v1</guid></item><item><title>Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</title><link>http://arxiv.org/abs/2408.15998v1</link><description>The ability to accurately interpret complex visual information is a crucialtopic of multimodal large language models (MLLMs). Recent work indicates thatenhanced visual perception significantly reduces hallucinations and improvesperformance on resolution-sensitive tasks, such as optical characterrecognition and document analysis. A number of recent MLLMs achieve this goalusing a mixture of vision encoders. Despite their success, there is a lack ofsystematic comparisons and detailed ablation studies addressing criticalaspects, such as expert selection and the integration of multiple visionexperts. This study provides an extensive exploration of the design space forMLLMs using a mixture of vision encoders and resolutions. Our findings revealseveral underlying principles common to various existing strategies, leading toa streamlined yet effective design approach. We discover that simplyconcatenating visual tokens from a set of complementary vision encoders is aseffective as more complex mixing architectures or strategies. We additionallyintroduce Pre-Alignment to bridge the gap between vision-focused encoders andlanguage tokens, enhancing model coherence. The resulting family of MLLMs,Eagle, surpasses other leading open-source models on major MLLM benchmarks.Models and code: https://github.com/NVlabs/Eagle</description><author>Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu</author><pubDate>Wed, 28 Aug 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15998v1</guid></item><item><title>Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need</title><link>http://arxiv.org/abs/2408.15997v1</link><description>Time series forecasting requires balancing short-term and long-termdependencies for accurate predictions. Existing methods mainly focus onlong-term dependency modeling, neglecting the complexities of short-termdynamics, which may hinder performance. Transformers are superior in modelinglong-term dependencies but are criticized for their quadratic computationalcost. Mamba provides a near-linear alternative but is reported less effectivein time series longterm forecasting due to potential information loss. Currentarchitectures fall short in offering both high efficiency and strongperformance for long-term dependency modeling. To address these challenges, weintroduce Mixture of Universals (MoU), a versatile model to capture bothshort-term and long-term dependencies for enhancing performance in time seriesforecasting. MoU is composed of two novel designs: Mixture of FeatureExtractors (MoF), an adaptive method designed to improve time series patchrepresentations for short-term dependency, and Mixture of Architectures (MoA),which hierarchically integrates Mamba, FeedForward, Convolution, andSelf-Attention architectures in a specialized order to model long-termdependency from a hybrid perspective. The proposed approach achievesstate-of-the-art performance while maintaining relatively low computationalcosts. Extensive experiments on seven real-world datasets demonstrate thesuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.</description><author>Sijia Peng, Yun Xiong, Yangyong Zhu, Zhiqiang Shen</author><pubDate>Wed, 28 Aug 2024 17:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15997v1</guid></item><item><title>Spatio-Temporal Context Prompting for Zero-Shot Action Detection</title><link>http://arxiv.org/abs/2408.15996v1</link><description>Spatio-temporal action detection encompasses the tasks of localizing andclassifying individual actions within a video. Recent works aim to enhance thisprocess by incorporating interaction modeling, which captures the relationshipbetween people and their surrounding context. However, these approaches haveprimarily focused on fully-supervised learning, and the current limitation liesin the lack of generalization capability to recognize unseen action categories.In this paper, we aim to adapt the pretrained image-language models to detectunseen actions. To this end, we propose a method which can effectively leveragethe rich knowledge of visual-language models to perform Person-ContextInteraction. Meanwhile, our Context Prompting module will utilize contextualinformation to prompt labels, thereby enhancing the generation of morerepresentative text features. Moreover, to address the challenge of recognizingdistinct actions by multiple people at the same timestamp, we design theInterest Token Spotting mechanism which employs pretrained visual knowledge tofind each person's interest context tokens, and then these tokens will be usedfor prompting to generate text features tailored to each individual. Toevaluate the ability to detect unseen actions, we propose a comprehensivebenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that ourmethod achieves superior results compared to previous approaches and can befurther extended to multi-action videos, bringing it closer to real-worldapplications. The code and data can be found inhttps://webber2933.github.io/ST-CLIP-project-page.</description><author>Wei-Jhe Huang, Min-Hung Chen, Shang-Hong Lai</author><pubDate>Wed, 28 Aug 2024 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15996v1</guid></item><item><title>TEDRA: Text-based Editing of Dynamic and Photoreal Actors</title><link>http://arxiv.org/abs/2408.15995v1</link><description>Over the past years, significant progress has been made in creatingphotorealistic and drivable 3D avatars solely from videos of real humans.However, a core remaining challenge is the fine-grained and user-friendlyediting of clothing styles by means of textual descriptions. To this end, wepresent TEDRA, the first method allowing text-based edits of an avatar, whichmaintains the avatar's high fidelity, space-time coherency, as well asdynamics, and enables skeletal pose and view control. We begin by training amodel to create a controllable and high-fidelity digital replica of the realactor. Next, we personalize a pretrained generative diffusion model byfine-tuning it on various frames of the real character captured from differentcamera angles, ensuring the digital representation faithfully captures thedynamics and movements of the real person. This two-stage process lays thefoundation for our approach to dynamic human avatar editing. Utilizing thispersonalized diffusion model, we modify the dynamic avatar based on a providedtext prompt using our Personalized Normal Aligned Score Distillation Sampling(PNA-SDS) within a model-based guidance framework. Additionally, we propose atime step annealing strategy to ensure high-quality edits. Our resultsdemonstrate a clear improvement over prior work in functionality and visualquality.</description><author>Basavaraj Sunagad, Heming Zhu, Mohit Mendiratta, Adam Kortylewski, Christian Theobalt, Marc Habermann</author><pubDate>Wed, 28 Aug 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15995v1</guid></item><item><title>Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration</title><link>http://arxiv.org/abs/2408.15994v1</link><description>The limitations of task-specific and general image restoration methods forspecific degradation have prompted the development of all-in-one imagerestoration techniques. However, the diversity of patterns among multipledegradation, along with the significant uncertainties in mapping betweendegraded images of different severities and their corresponding undistortedversions, pose significant challenges to the all-in-one restoration tasks. Toaddress these challenges, we propose Perceive-IR, an all-in-one image restorerdesigned to achieve fine-grained quality control that enables restored imagesto more closely resemble their undistorted counterparts, regardless of the typeor severity of degradation. Specifically, Perceive-IR contains two stages: (1)prompt learning stage and (2) restoration stage. In the prompt learning stage,we leverage prompt learning to acquire a fine-grained quality perceiver capableof distinguishing three-tier quality levels by constraining the prompt-imagesimilarity in the CLIP perception space. Subsequently, this quality perceiverand difficulty-adaptive perceptual loss are integrated as a quality-awarelearning strategy to realize fine-grained quality control in restoration stage.For the restoration stage, a semantic guidance module (SGM) and compact featureextraction (CFE) are proposed to further promote the restoration process byutilizing the robust semantic information from the pre-trained large scalevision models and distinguishing degradation-specific features. Extensiveexperiments demonstrate that our Perceive-IR outperforms state-of-the-artmethods in all-in-one image restoration tasks and exhibit superiorgeneralization ability when dealing with unseen tasks.</description><author>Xu Zhang, Jiaqi Ma, Guoli Wang, Qian Zhang, Huan Zhang, Lefei Zhang</author><pubDate>Wed, 28 Aug 2024 17:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15994v1</guid></item><item><title>ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution</title><link>http://arxiv.org/abs/2408.15993v1</link><description>Detecting and attributing temperature increases due to climate change iscrucial for understanding global warming and guiding adaptation strategies. Thecomplexity of distinguishing human-induced climate signals from naturalvariability has challenged traditional detection and attribution (D&amp;A)approaches, which seek to identify specific "fingerprints" in climate responsevariables. Deep learning offers potential for discerning these complex patternsin expansive spatial datasets. However, lack of standard protocols has hinderedconsistent comparisons across studies. We introduce ClimDetect, a standardizeddataset of over 816k daily climate snapshots, designed to enhance modelaccuracy in identifying climate change signals. ClimDetect integrates variousinput and target variables used in past research, ensuring comparability andconsistency. We also explore the application of vision transformers (ViT) toclimate data, a novel and modernizing approach in this context. Our open-accessdata and code serve as a benchmark for advancing climate science throughimproved model evaluations. ClimDetect is publicly accessible via Huggingfacedataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.</description><author>Sungduk Yu, Brian L. White, Anahita Bhiwandiwalla, Musashi Hinck, Matthew Lyle Olson, Tung Nguyen, Vasudev Lal</author><pubDate>Wed, 28 Aug 2024 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15993v1</guid></item><item><title>CoGen: Learning from Feedback with Coupled Comprehension and Generation</title><link>http://arxiv.org/abs/2408.15992v1</link><description>Systems with both language comprehension and generation capabilities canbenefit from the tight connection between the two. This work studies couplingcomprehension and generation with focus on continually learning frominteraction with users. We propose techniques to tightly integrate the twocapabilities for both learning and inference. We situate our studies intwo-player reference games, and deploy various models for thousands ofinteractions with human users, while learning from interaction feedbacksignals. We show dramatic improvements in performance over time, withcomprehension-generation coupling leading to performance improvements up to 26%in absolute terms and up to 17% higher accuracies compared to a non-coupledsystem. Our analysis also shows coupling has substantial qualitative impact onthe system's language, making it significantly more human-like.</description><author>Mustafa Omer Gul, Yoav Artzi</author><pubDate>Wed, 28 Aug 2024 17:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15992v1</guid></item><item><title>Distribution Backtracking Builds A Faster Convergence Trajectory for One-step Diffusion Distillation</title><link>http://arxiv.org/abs/2408.15991v1</link><description>Accelerating the sampling speed of diffusion models remains a significantchallenge. Recent score distillation methods distill a heavy teacher model intoan one-step student generator, which is optimized by calculating the differencebetween the two score functions on the samples generated by the student model.However, there is a score mismatch issue in the early stage of the distillationprocess, because existing methods mainly focus on using the endpoint ofpre-trained diffusion models as teacher models, overlooking the importance ofthe convergence trajectory between the student generator and the teacher model.To address this issue, we extend the score distillation process by introducingthe entire convergence trajectory of teacher models and propose DistributionBacktracking Distillation (DisBack) for distilling student generators. DisBaskis composed of two stages: Degradation Recording and Distribution Backtracking.Degradation Recording is designed to obtain the convergence trajectory ofteacher models, which records the degradation path from the trained teachermodel to the untrained initial student generator. The degradation pathimplicitly represents the intermediate distributions of teacher models. ThenDistribution Backtracking trains a student generator to backtrack theintermediate distributions for approximating the convergence trajectory ofteacher models. Extensive experiments show that DisBack achieves faster andbetter convergence than the existing distillation method and accomplishescomparable generation performance. Notably, DisBack is easy to implement andcan be generalized to existing distillation methods to boost performance. Ourcode is publicly available on https://github.com/SYZhang0805/DisBack.</description><author>Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan Yang, Guang Yang, Zhiyuan Yang, Lingyun Sun</author><pubDate>Wed, 28 Aug 2024 17:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15991v1</guid></item><item><title>In-Context Imitation Learning via Next-Token Prediction</title><link>http://arxiv.org/abs/2408.15980v1</link><description>We explore how to enhance next-token prediction models to perform in-contextimitation learning on a real robot, where the robot executes new tasks byinterpreting contextual information provided during the input phase, withoutupdating its underlying policy parameters. We propose In-Context RobotTransformer (ICRT), a causal transformer that performs autoregressiveprediction on sensorimotor trajectories without relying on any linguistic dataor reward function. This formulation enables flexible and training-freeexecution of new tasks at test time, achieved by prompting the model withsensorimotor trajectories of the new task composing of image observations,actions and states tuples, collected through human teleoperation. Experimentswith a Franka Emika robot demonstrate that the ICRT can adapt to new tasksspecified by prompts, even in environment configurations that differ from boththe prompt and the training data. In a multitask environment setup, ICRTsignificantly outperforms current state-of-the-art next-token prediction modelsin robotics on generalizing to unseen tasks. Code, checkpoints and data areavailable on https://icrt.dev/</description><author>Letian Fu, Huang Huang, Gaurav Datta, Lawrence Yunliang Chen, William Chung-Ho Panitch, Fangchen Liu, Hui Li, Ken Goldberg</author><pubDate>Wed, 28 Aug 2024 17:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15980v1</guid></item><item><title>WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration</title><link>http://arxiv.org/abs/2408.15978v1</link><description>LLM-based autonomous agents often fail to execute complex web tasks thatrequire dynamic interaction due to the inherent uncertainty and complexity ofthese environments. Existing LLM-based web agents typically rely on rigid,expert-designed policies specific to certain states and actions, which lack theflexibility and generalizability needed to adapt to unseen tasks. In contrast,humans excel by exploring unknowns, continuously adapting strategies, andresolving ambiguities through exploration. To emulate human-like adaptability,web agents need strategic exploration and complex decision-making. Monte CarloTree Search (MCTS) is well-suited for this, but classical MCTS struggles withvast action spaces, unpredictable state transitions, and incomplete informationin web tasks. In light of this, we develop WebPilot, a multi-agent system witha dual optimization strategy that improves MCTS to better handle complex webenvironments. Specifically, the Global Optimization phase involves generating ahigh-level plan by breaking down tasks into manageable subtasks andcontinuously refining this plan, thereby focusing the search process andmitigating the challenges posed by vast action spaces in classical MCTS.Subsequently, the Local Optimization phase executes each subtask using atailored MCTS designed for complex environments, effectively addressinguncertainties and managing incomplete information. Experimental results onWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, onWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%relative increase in success rate over the concurrent tree search-based method.WebPilot marks a significant advancement in general autonomous agentcapabilities, paving the way for more advanced and reliable decision-making inpractical environments.</description><author>Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, Volker Tresp</author><pubDate>Wed, 28 Aug 2024 17:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15978v1</guid></item><item><title>Embedded FPGA Developments in 130nm and 28nm CMOS for Machine Learning in Particle Detector Readout</title><link>http://arxiv.org/abs/2404.17701v5</link><description>Embedded field programmable gate array (eFPGA) technology allows theimplementation of reconfigurable logic within the design of anapplication-specific integrated circuit (ASIC). This approach offers the lowpower and efficiency of an ASIC along with the ease of FPGA configuration,particularly beneficial for the use case of machine learning in the datapipeline of next-generation collider experiments. An open-source frameworkcalled "FABulous" was used to design eFPGAs using 130 nm and 28 nm CMOStechnology nodes, which were subsequently fabricated and verified throughtesting. The capability of an eFPGA to act as a front-end readout chip wasassessed using simulation of high energy particles passing through a siliconpixel sensor. A machine learning-based classifier, designed for reduction ofsensor data at the source, was synthesized and configured onto the eFPGA. Asuccessful proof-of-concept was demonstrated through reproduction of theexpected algorithm result on the eFPGA with perfect accuracy. Furtherdevelopment of the eFPGA technology and its application to collider detectorreadout is discussed.</description><author>Julia Gonski, Aseem Gupta, Haoyi Jia, Hyunjoon Kim, Lorenzo Rota, Larry Ruckman, Angelo Dragone, Ryan Herbst</author><pubDate>Wed, 28 Aug 2024 17:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17701v5</guid></item><item><title>BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems</title><link>http://arxiv.org/abs/2408.15971v1</link><description>Large Language Models (LLMs) are becoming increasingly powerful and capableof handling complex tasks, e.g., building single agents and multi-agentsystems. Compared to single agents, multi-agent systems have higherrequirements for the collaboration capabilities of language models. Manybenchmarks are proposed to evaluate their collaborative abilities. However,these benchmarks lack fine-grained evaluations of LLM collaborativecapabilities. Additionally, multi-agent collaborative and competitive scenariosare ignored in existing works. To address these two problems, we propose abenchmark, called BattleAgentBench, which defines seven sub-stages of threevarying difficulty levels and conducts a fine-grained evaluation of languagemodels in terms of single-agent scenario navigation capabilities, paired-agenttask execution abilities, and multi-agent collaboration and competitioncapabilities. We conducted extensive evaluations on leading four closed-sourceand seven open-source models. Experimental results indicate that API-basedmodels perform excellently on simple tasks but open-source small modelsstruggle with simple tasks. Regarding difficult tasks that requirecollaborative and competitive abilities, although API-based models havedemonstrated some collaborative capabilities, there is still enormous room forimprovement.</description><author>Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang</author><pubDate>Wed, 28 Aug 2024 17:43:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15971v1</guid></item><item><title>Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex Optimization Problems</title><link>http://arxiv.org/abs/2408.15969v1</link><description>We examine stability properties of primal-dual gradient flow dynamics forcomposite convex optimization problems with multiple, possibly nonsmooth, termsin the objective function under the generalized consensus constraint. Theproposed dynamics are based on the proximal augmented Lagrangian and theyprovide a viable alternative to ADMM which faces significant challenges fromboth analysis and implementation viewpoints in large-scale multi-blockscenarios. In contrast to customized algorithms with individualized convergenceguarantees, we provide a systematic approach for solving a broad class ofchallenging composite optimization problems. We leverage various structuralproperties to establish global (exponential) convergence guarantees for theproposed dynamics. Our assumptions are much weaker than those required to prove(exponential) stability of various primal-dual dynamics as well as (linear)convergence of discrete-time methods, e.g., standard two-block and multi-blockADMM and EXTRA algorithms. Finally, we show necessity of some of our structuralassumptions for exponential stability and provide computational experiments todemonstrate the convenience of the proposed dynamics for parallel anddistributed computing applications.</description><author>Ibrahim K. Ozaslan, Panagiotis Patrinos, Mihailo R. Jovanović</author><pubDate>Wed, 28 Aug 2024 17:43:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15969v1</guid></item><item><title>More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding</title><link>http://arxiv.org/abs/2408.15966v1</link><description>Enabling Large Language Models (LLMs) to comprehend the 3D physical worldremains a significant challenge. Due to the lack of large-scale 3D-text pairdatasets, the success of LLMs has yet to be replicated in 3D understanding. Inthis paper, we rethink this issue and propose a new task: 3D Data-EfficientPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3Dobject understanding with minimal 3D point cloud and text data pairs. Toaddress this task, we introduce GreenPLM, which leverages more text data tocompensate for the lack of 3D data. First, inspired by using CLIP to alignimages and text, we utilize a pre-trained point cloud-text encoder to map the3D point cloud space to the text space. This mapping leaves us to seamlesslyconnect the text space with LLMs. Once the point-text-LLM connection isestablished, we further enhance text-LLM alignment by expanding theintermediate text space, thereby reducing the reliance on 3D point cloud data.Specifically, we generate 6M free-text descriptions of 3D objects, and design athree-stage training strategy to help LLMs better explore the intrinsicconnections between different modalities. To achieve efficient modalityalignment, we design a zero-parameter cross-attention module for token pooling.Extensive experimental results show that GreenPLM requires only 12% of the 3Dtraining data used by existing state-of-the-art models to achieve superior 3Dunderstanding. Remarkably, GreenPLM also achieves competitive performance usingtext-only data. The code and weights are available at:https://github.com/TangYuan96/GreenPLM.</description><author>Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen</author><pubDate>Wed, 28 Aug 2024 17:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15966v1</guid></item><item><title>Flextron: Many-in-One Flexible Large Language Model</title><link>http://arxiv.org/abs/2406.10260v2</link><description>Training modern LLMs is extremely resource intensive, and customizing themfor various deployment scenarios characterized by limited compute and memoryresources through repeated training is impractical. In this paper, we introduceFlextron, a network architecture and post-training model optimization frameworksupporting flexible model deployment. The Flextron architecture utilizes anested elastic structure to rapidly adapt to specific user-defined latency andaccuracy targets during inference with no additional fine-tuning required. Itis also input-adaptive, and can automatically route tokens through itssub-networks for improved performance and efficiency. We present asample-efficient training method and associated routing algorithms forsystematically transforming an existing trained LLM into a Flextron model. Weevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstratesuperior performance over multiple end-to-end trained variants and otherstate-of-the-art elastic networks, all with a single pretraining run thatconsumes a mere 7.63% tokens compared to original pretraining.</description><author>Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, Pavlo Molchanov</author><pubDate>Wed, 28 Aug 2024 17:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10260v2</guid></item><item><title>Efficient Slice Anomaly Detection Network for 3D Brain MRI Volume</title><link>http://arxiv.org/abs/2408.15958v1</link><description>Current anomaly detection methods excel with benchmark industrial data butstruggle with natural images and medical data due to varying definitions of'normal' and 'abnormal.' This makes accurate identification of deviations inthese fields particularly challenging. Especially for 3D brain MRI data, allthe state-of-the-art models are reconstruction-based with 3D convolutionalneural networks which are memory-intensive, time-consuming and producing noisyoutputs that require further post-processing. We propose a framework calledSimple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trainedon ImageNet and fine-tuned on a separate MRI dataset as a 2D slice featureextractor to reduce computational cost. We aggregate the extracted features toperform anomaly detection tasks on 3D brain MRI volumes. Our model integrates aconditional normalizing flow to calculate log likelihood of features andemploys the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy. Theresults indicate improved performance, showcasing our model's remarkableadaptability and effectiveness when addressing the challenges exists in brainMRI data. In addition, for the large-scale 3D brain volumes, our modelSimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms ofaccuracy, memory usage and time consumption. Code is available at:https://anonymous.4open.science/r/SimpleSliceNet-8EA3.</description><author>Zeduo Zhang, Yalda Mohsenzadeh</author><pubDate>Wed, 28 Aug 2024 17:20:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15958v1</guid></item><item><title>HER2 and FISH Status Prediction in Breast Biopsy H&amp;E-Stained Images Using Deep Learning</title><link>http://arxiv.org/abs/2408.13818v2</link><description>The current standard for detecting human epidermal growth factor receptor 2(HER2) status in breast cancer patients relies on HER2 amplification,identified through fluorescence in situ hybridization (FISH) orimmunohistochemistry (IHC). However, hematoxylin and eosin (H\&amp;E) tumor stainsare more widely available, and accurately predicting HER2 status using H\&amp;Ecould reduce costs and expedite treatment selection. Deep Learning algorithmsfor H&amp;E have shown effectiveness in predicting various cancer features andclinical outcomes, including moderate success in HER2 status prediction. Inthis work, we employed a customized weak supervision classification techniquecombined with MoCo-v2 contrastive learning to predict HER2 status. We trainedour pipeline on 182 publicly available H&amp;E Whole Slide Images (WSIs) from TheCancer Genome Atlas (TCGA), for which annotations by the pathology team at YaleSchool of Medicine are publicly available. Our pipeline achieved an Area Underthe Curve (AUC) of 0.85 across four different test folds. Additionally, wetested our model on 44 H&amp;E slides from the TCGA-BRCA dataset, which had an HER2score of 2+ and included corresponding HER2 status and FISH test results. Thesecases are considered equivocal for IHC, requiring an expensive FISH test ontheir IHC slides for disambiguation. Our pipeline demonstrated an AUC of 0.81on these challenging H&amp;E slides. Reducing the need for FISH test can havesignificant implications in cancer treatment equity for underservedpopulations.</description><author>Ardhendu Sekhar, Vrinda Goel, Garima Jain, Abhijeet Patil, Ravi Kant Gupta, Amit Sethi</author><pubDate>Wed, 28 Aug 2024 17:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13818v2</guid></item><item><title>Halfway Escape Optimization: A Quantum-Inspired Solution for General Optimization Problems</title><link>http://arxiv.org/abs/2405.02850v5</link><description>This paper first proposes the Halfway Escape Optimization (HEO) algorithm, aquantum-inspired metaheuristic designed to address general optimizationproblems characterized by rugged landscapes and high-dimensionality with anefficient convergence rate. The study presents a comprehensive comparativeevaluation of HEO's performance against established optimization algorithms,including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), ArtificialFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behavedParticle Swarm Optimization (QPSO). The primary analysis encompasses 14benchmark functions with dimension 30, demonstrating HEO's effectiveness andadaptability in navigating general optimization problems and providing valuableinsights into its performance. The test of HEO in Pressure Vessel Design andTubular Column Design infers its feasibility and potential in real-timeapplications. Further validation in Osmancik-97 and Cammeo Rice Classificationproves the effectiveness of HEO and achieves a higher accuracy record.</description><author>Jiawen Li, Anwar PP Abdul Majeed, Pascal Lefevre</author><pubDate>Wed, 28 Aug 2024 17:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02850v5</guid></item><item><title>Generating Binary Species Range Maps</title><link>http://arxiv.org/abs/2408.15956v1</link><description>Accurately predicting the geographic ranges of species is crucial forassisting conservation efforts. Traditionally, range maps were manually createdby experts. However, species distribution models (SDMs) and, more recently,deep learning-based variants offer a potential automated alternative. Deeplearning-based SDMs generate a continuous probability representing thepredicted presence of a species at a given location, which must be binarized bysetting per-species thresholds to obtain binary range maps. However, selectingappropriate per-species thresholds to binarize these predictions is non-trivialas different species can require distinct thresholds. In this work, we evaluatedifferent approaches for automatically identifying the best thresholds forbinarizing range maps using presence-only data. This includes approaches thatrequire the generation of additional pseudo-absence data, along with ones thatonly require presence data. We also propose an extension of an existingpresence-only technique that is more robust to outliers. We perform a detailedevaluation of different thresholding techniques on the tasks of binary rangeestimation and large-scale fine-grained visual classification, and wedemonstrate improved performance over existing pseudo-absence free approachesusing our method.</description><author>Filip Dorm, Christian Lange, Scott Loarie, Oisin Mac Aodha</author><pubDate>Wed, 28 Aug 2024 17:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15956v1</guid></item><item><title>Fall Detection for Smart Living using YOLOv5</title><link>http://arxiv.org/abs/2408.15955v1</link><description>This work introduces a fall detection system using the YOLOv5mu model, whichachieved a mean average precision (mAP) of 0.995, demonstrating exceptionalaccuracy in identifying fall events within smart home environments. Enhanced byadvanced data augmentation techniques, the model demonstrates significantrobustness and adaptability across various conditions. The integration ofYOLOv5mu offers precise, real-time fall detection, which is crucial forimproving safety and emergency response for residents. Future research willfocus on refining the system by incorporating contextual data and exploringmulti-sensor approaches to enhance its performance and practical applicabilityin diverse environments.</description><author>Gracile Astlin Pereira</author><pubDate>Wed, 28 Aug 2024 17:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15955v1</guid></item><item><title>InstanSeg: an embedding-based instance segmentation algorithm optimized for accurate, efficient and portable cell segmentation</title><link>http://arxiv.org/abs/2408.15954v1</link><description>Cell and nucleus segmentation are fundamental tasks for quantitative bioimageanalysis. Despite progress in recent years, biologists and other domain expertsstill require novel algorithms to handle increasingly large and complexreal-world datasets. These algorithms must not only achieve state-of-the-artaccuracy, but also be optimized for efficiency, portability anduser-friendliness. Here, we introduce InstanSeg: a novel embedding-basedinstance segmentation pipeline designed to identify cells and nuclei inmicroscopy images. Using six public cell segmentation datasets, we demonstratethat InstanSeg can significantly improve accuracy when compared to the mostwidely used alternative methods, while reducing the processing time by at least60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScriptand supports GPU acceleration on a range of hardware. We provide an open-sourceimplementation of InstanSeg in Python, in addition to a user-friendly,interactive QuPath extension for inference written in Java. Our code andpre-trained models are available at https://github.com/instanseg/instanseg .</description><author>Thibaut Goldsborough, Ben Philps, Alan O'Callaghan, Fiona Inglis, Leo Leplat, Andrew Filby, Hakan Bilen, Peter Bankhead</author><pubDate>Wed, 28 Aug 2024 17:14:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15954v1</guid></item><item><title>Modeling and Analyzing the Influence of Non-Item Pages on Sequential Next-Item Prediction</title><link>http://arxiv.org/abs/2408.15953v1</link><description>Analyzing the sequence of historical interactions between users and items,sequential recommendation models learn user intent and make predictions aboutthe next item of interest. Next to these item interactions, most systems alsohave interactions with pages not related to specific items, for examplenavigation pages, account pages, and pages for a specific category, which mayprovide additional insights into the user's interests. However, while there areseveral approaches to integrate additional information about items and users,the topic of integrating non-item pages has been less explored. We use thehypotheses testing framework HypTrails to show that there is indeed arelationship between these non-item pages and the items of interest and fillthis gap by proposing various approaches of representing non-item pages (e.g,based on their content) to use them as an additional information source for thetask of sequential next-item prediction. We create a synthetic dataset with non-item pages highly related to thesubsequent item to show that the models are generally capable of learning fromthese interactions, and subsequently evaluate the improvements gained byincluding non-item pages in two real-world datasets. We adapt eight popular sequential recommender models, covering CNN-, RNN- andtransformer-based architectures, to integrate non-item pages and investigatethe capabilities of these models to leverage their information for next itemprediction. We also analyze their behavior on noisy data and compare differentitem representation strategies. Our results show that non-item pages are a valuable source of information,but representing such a page well is the key to successfully leverage them. Theinclusion of non-item pages can increase the performance for next-itemprediction in all examined model architectures with a varying degree.</description><author>Elisabeth Fischer, Daniel Schlör, Albin Zehe, Andreas Hotho</author><pubDate>Wed, 28 Aug 2024 17:12:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15953v1</guid></item><item><title>Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games</title><link>http://arxiv.org/abs/2408.15950v1</link><description>Recent advancements in large language models (LLMs) have expanded theircapabilities beyond traditional text-based tasks to multimodal domains,integrating visual, auditory, and textual data. While multimodal LLMs have beenextensively explored for high-level planning in domains like robotics andgames, their potential as low-level controllers remains largely untapped. Thispaper explores the application of multimodal LLMs as low-level controllers inthe domain of Atari video games, introducing Atari game performance as a newbenchmark for evaluating the ability of multimodal LLMs to perform low-levelcontrol tasks. Unlike traditional reinforcement learning (RL) and imitationlearning (IL) methods that require extensive computational resources as well asreward function specification, these LLMs utilize pre-existing multimodalknowledge to directly engage with game environments. Our study assessesmultiple multimodal LLMs performance against traditional RL agents, humanplayers, and random agents, focusing on their ability to understand andinteract with complex visual scenes and formulate strategic responses.Additionally, we examine the impact of In-Context Learning (ICL) byincorporating human-demonstrated game-play trajectories to enhance the modelscontextual understanding. Through this investigation, we aim to determine theextent to which multimodal LLMs can leverage their extensive training toeffectively function as low-level controllers, thereby redefining potentialapplications in dynamic and visually complex environments. Additional resultsand videos are available at our project webpage:https://sites.google.com/view/atari-gpt/.</description><author>Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks</author><pubDate>Wed, 28 Aug 2024 17:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15950v1</guid></item><item><title>Proceedings of The second international workshop on eXplainable AI for the Arts (XAIxArts)</title><link>http://arxiv.org/abs/2406.14485v6</link><description>This second international workshop on explainable AI for the Arts (XAIxArts)brought together a community of researchers in HCI, Interaction Design, AI,explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.Workshop held at the 16th ACM Conference on Creativity and Cognition (C&amp;C2024), Chicago, USA.</description><author>Nick Bryan-Kinns, Corey Ford, Shuoyang Zheng, Helen Kennedy, Alan Chamberlain, Makayla Lewis, Drew Hemment, Zijin Li, Qiong Wu, Lanxi Xiao, Gus Xia, Jeba Rezwana, Michael Clemens, Gabriel Vigliensoni</author><pubDate>Wed, 28 Aug 2024 17:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14485v6</guid></item><item><title>Auxiliary Input in Training: Incorporating Catheter Features into Deep Learning Models for ECG-Free Dynamic Coronary Roadmapping</title><link>http://arxiv.org/abs/2408.15947v1</link><description>Dynamic coronary roadmapping is a technology that overlays the vessel maps(the "roadmap") extracted from an offline image sequence of X-ray angiographyonto a live stream of X-ray fluoroscopy in real-time. It aims to offernavigational guidance for interventional surgeries without the need forrepeated contrast agent injections, thereby reducing the risks associated withradiation exposure and kidney failure. The precision of the roadmaps iscontingent upon the accurate alignment of angiographic and fluoroscopic imagesbased on their cardiac phases, as well as precise catheter tip tracking. Theformer ensures the selection of a roadmap that closely matches the vessel shapein the current frame, while the latter uses catheter tips as reference pointsto adjust for translational motion between the roadmap and the present vesseltree. Training deep learning models for both tasks is challenging andunderexplored. However, incorporating catheter features into the models couldoffer substantial benefits, given humans heavily rely on catheters to completethe tasks. To this end, we introduce a simple but effective method, auxiliaryinput in training (AIT), and demonstrate that it enhances model performanceacross both tasks, outperforming baseline methods in knowledge incorporationand transfer learning.</description><author>Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun</author><pubDate>Wed, 28 Aug 2024 17:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15947v1</guid></item><item><title>Sigma Flows for Image and Data Labeling and Learning Structured Prediction</title><link>http://arxiv.org/abs/2408.15946v1</link><description>This paper introduces the sigma flow model for the prediction of structuredlabelings of data observed on Riemannian manifolds, including Euclidean imagedomains as special case. The approach combines the Laplace-Beltrami frameworkfor image denoising and enhancement, introduced by Sochen, Kimmel and Malladiabout 25 years ago, and the assignment flow approach introduced and studied bythe authors. The sigma flow arises as Riemannian gradient flow of generalized harmonicenergies and thus is governed by a nonlinear geometric PDE which determines aharmonic map from a closed Riemannian domain manifold to a statisticalmanifold, equipped with the Fisher-Rao metric from information geometry. Aspecific ingredient of the sigma flow is the mutual dependency of theRiemannian metric of the domain manifold on the evolving state. This makes theapproach amenable to machine learning in a specific way, by realizing thisdependency through a mapping with compact time-variant parametrization that canbe learned from data. Proof of concept experiments demonstrate the expressivityof the sigma flow model and prediction performance. Structural similarities to transformer network architectures and networksgenerated by the geometric integration of sigma flows are pointed out, whichhighlights the connection to deep learning and, conversely, may stimulate theuse of geometric design principles for structured prediction in other areas ofscientific machine learning.</description><author>Jonas Cassel, Bastian Boll, Stefania Petra, Peter Albers, Christoph Schnörr</author><pubDate>Wed, 28 Aug 2024 17:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15946v1</guid></item><item><title>SCP: Soft Conditional Prompt Learning for Aerial Video Action Recognition</title><link>http://arxiv.org/abs/2305.12437v4</link><description>We present a new learning approach, Soft Conditional Prompt Learning (SCP),which leverages the strengths of prompt learning for aerial video actionrecognition. Our approach is designed to predict the action of each agent byhelping the models focus on the descriptions or instructions associated withactions in the input videos for aerial/robot visual perception. Our formulationsupports various prompts, including learnable prompts, auxiliary visualinformation, and large vision models to improve the recognition performance. Wepresent a soft conditional prompt method that learns to dynamically generateprompts from a pool of prompt experts under different video inputs. By sharingthe same objective with the task, our proposed SCP can optimize prompts thatguide the model's predictions while explicitly learning input-invariant (promptexperts pool) and input-specific (data-dependent) prompt knowledge. Inpractice, we observe a 3.17-10.2% accuracy improvement on the aerial videodatasets (Okutama, NECDrone), which consist of scenes with single-agent andmulti-agent actions. We further evaluate our approach on ground camera videosto verify the effectiveness and generalization and achieve a 1.0-3.6%improvement on dataset SSV2. We integrate our method into the ROS2 as well.</description><author>Xijun Wang, Ruiqi Xian, Tianrui Guan, Fuxiao Liu, Dinesh Manocha</author><pubDate>Wed, 28 Aug 2024 16:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12437v4</guid></item><item><title>Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model</title><link>http://arxiv.org/abs/2402.09786v4</link><description>Generative adversarial networks (GANs) generate photorealistic faces that areoften indistinguishable by humans from real faces. While biases in machinelearning models are often assumed to be due to biases in training data, we findpathological internal color and luminance biases in the discriminator of apre-trained StyleGAN3-r model that are not explicable by the training data. Wealso find that the discriminator systematically stratifies scores by bothimage- and face-level qualities and that this disproportionately affects imagesacross gender, race, and other categories. We examine axes common in researchon stereotyping in social psychology.</description><author>Alvin Grissom II, Ryan F. Lei, Matt Gusdorff, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter</author><pubDate>Wed, 28 Aug 2024 16:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09786v4</guid></item><item><title>Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning</title><link>http://arxiv.org/abs/2408.15924v1</link><description>Few-shot image classification is a challenging task in the field of machinelearning, involving the identification of new categories using a limited numberof labeled samples. In recent years, methods based on local descriptors havemade significant progress in this area. However, the key to improvingclassification accuracy lies in effectively filtering background noise andaccurately selecting critical local descriptors highly relevant to imagecategory information. To address this challenge, we propose an innovative weighted adaptivethreshold filtering (WATF) strategy for local descriptors. This strategy candynamically adjust based on the current task and image context, therebyselecting local descriptors most relevant to the image category. This enablesthe model to better focus on category-related information while effectivelymitigating interference from irrelevant background regions. To evaluate the effectiveness of our method, we adopted the N-way K-shotexperimental framework. Experimental results show that our method not onlyimproves the clustering effect of selected local descriptors but alsosignificantly enhances the discriminative ability between image categories.Notably, our method maintains a simple and lightweight design philosophywithout introducing additional learnable parameters. This feature ensuresconsistency in filtering capability during both training and testing phases,further enhancing the reliability and practicality of the method.</description><author>Bingchen Yan</author><pubDate>Wed, 28 Aug 2024 16:36:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15924v1</guid></item><item><title>Generalized Naive Bayes</title><link>http://arxiv.org/abs/2408.15923v1</link><description>In this paper we introduce the so-called Generalized Naive Bayes structure asan extension of the Naive Bayes structure. We give a new greedy algorithm thatfinds a good fitting Generalized Naive Bayes (GNB) probability distribution. Weprove that this fits the data at least as well as the probability distributiondetermined by the classical Naive Bayes (NB). Then, under a not veryrestrictive condition, we give a second algorithm for which we can prove thatit finds the optimal GNB probability distribution, i.e. best fitting structurein the sense of KL divergence. Both algorithms are constructed to maximize theinformation content and aim to minimize redundancy. Based on these algorithms,new methods for feature selection are introduced. We discuss the similaritiesand differences to other related algorithms in terms of structure, methodology,and complexity. Experimental results show, that the algorithms introducedoutperform the related algorithms in many cases.</description><author>Edith Alice Kovács, Anna Ország, Dániel Pfeifer, András Benczúr</author><pubDate>Wed, 28 Aug 2024 16:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15923v1</guid></item><item><title>DiffAge3D: Diffusion-based 3D-aware Face Aging</title><link>http://arxiv.org/abs/2408.15922v1</link><description>Face aging is the process of converting an individual's appearance to ayounger or older version of themselves. Existing face aging techniques havebeen limited to 2D settings, which often weaken their applications as there isa growing demand for 3D face modeling. Moreover, existing aging methodsstruggle to perform faithful aging, maintain identity, and retain the finedetails of the input images. Given these limitations and the need for a3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging frameworkthat not only performs faithful aging and identity preservation but alsooperates in a 3D setting. Our aging framework allows to model the aging andcamera pose separately by only taking a single image with a target age. Ourframework includes a robust 3D-aware aging dataset generation pipeline byutilizing a pre-trained 3D GAN and the rich text embedding capabilities withinCLIP model. Notably, we do not employ any inversion bottleneck in datasetgeneration. Instead, we randomly generate training samples from the latentspace of 3D GAN, allowing us to manipulate the rich latent space of GAN togenerate ages even with large gaps. With the generated dataset, we train aviewpoint-aware diffusion-based aging model to control the camera pose andfacial age. Through quantitative and qualitative evaluations, we demonstratethat DiffAge3D outperforms existing methods, particularly inmultiview-consistent aging and fine details preservation.</description><author>Junaid Wahid, Fangneng Zhan, Pramod Rao, Christian Theobalt</author><pubDate>Wed, 28 Aug 2024 16:36:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15922v1</guid></item><item><title>Multi-modal Adversarial Training for Zero-Shot Voice Cloning</title><link>http://arxiv.org/abs/2408.15916v1</link><description>A text-to-speech (TTS) model trained to reconstruct speech given text tendstowards predictions that are close to the average characteristics of a dataset,failing to model the variations that make human speech sound natural. Thisproblem is magnified for zero-shot voice cloning, a task that requires trainingdata with high variance in speaking styles. We build off of recent works whichhave used Generative Advsarial Networks (GAN) by proposing a Transformerencoder-decoder architecture to conditionally discriminates between real andgenerated speech features. The discriminator is used in a training pipelinethat improves both the acoustic and prosodic features of a TTS model. Weintroduce our novel adversarial training technique by applying it to aFastSpeech2 acoustic model and training on Libriheavy, a large multi-speakerdataset, for the task of zero-shot voice cloning. Our model achievesimprovements over the baseline in terms of speech quality and speakersimilarity. Audio examples from our system are available online.</description><author>John Janiczek, Dading Chong, Dongyang Dai, Arlo Faria, Chao Wang, Tao Wang, Yuzong Liu</author><pubDate>Wed, 28 Aug 2024 16:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15916v1</guid></item><item><title>Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models</title><link>http://arxiv.org/abs/2408.15915v1</link><description>The cultivation of expertise for large language models (LLMs) to solve tasksof specific areas often requires special-purpose tuning with calibratedbehaviors on the expected stable outputs. To avoid huge cost brought by manualpreparation of instruction datasets and training resources up to hundreds ofhours, the exploitation of open knowledge including a wealth of low rankadaptation (LoRA) models and instruction datasets serves as a good startingpoint. However, existing methods on model and data selection focus on theperformance of general-purpose capabilities while neglecting the knowledge gapexposed in domain-specific deployment. In the present study, we propose tobridge such gap by introducing few human-annotated samples (i.e., K-shot) foradvancing task expertise of LLMs with open knowledge. Specifically, we developan efficient and scalable pipeline to cost-efficiently produce task expertswhere K-shot data intervene in selecting the most promising expert candidatesand the task-relevant instructions. A mixture-of-expert (MoE) system is builtto make the best use of individual-yet-complementary knowledge between multipleexperts. We unveil the two keys to the success of a MoE system, 1) the abidanceby K-shot, and 2) the insistence on diversity. For the former, we ensure thatmodels that truly possess problem-solving abilities on K-shot are selectedrather than those blind guessers. Besides, during data selection, instructionsthat share task-relevant contexts with K-shot are prioritized. For the latter,we highlight the diversity of constituting experts and that of the fine-tuninginstructions throughout the model and data selection process. Extensiveexperimental results confirm the superiority of our approach over existingmethods on utilization of open knowledge across various tasks. Codes and modelswill be released later.</description><author>Yuncheng Yang, Yulei Qin, Tong Wu, Zihan Xu, Gang Li, Pengcheng Guo, Hang Shao, Yucheng Shi, Ke Li, Xing Sun, Jie Yang, Yun Gu</author><pubDate>Wed, 28 Aug 2024 16:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15915v1</guid></item><item><title>CoRe: Context-Regularized Text Embedding Learning for Text-to-Image Personalization</title><link>http://arxiv.org/abs/2408.15914v1</link><description>Recent advances in text-to-image personalization have enabled high-qualityand controllable image synthesis for user-provided concepts. However, existingmethods still struggle to balance identity preservation with text alignment.Our approach is based on the fact that generating prompt-aligned imagesrequires a precise semantic understanding of the prompt, which involvesaccurately processing the interactions between the new concept and itssurrounding context tokens within the CLIP text encoder. To address this, weaim to embed the new concept properly into the input embedding space of thetext encoder, allowing for seamless integration with existing tokens. Weintroduce Context Regularization (CoRe), which enhances the learning of the newconcept's text embedding by regularizing its context tokens in the prompt. Thisis based on the insight that appropriate output vectors of the text encoder forthe context tokens can only be achieved if the new concept's text embedding iscorrectly learned. CoRe can be applied to arbitrary prompts without requiringthe generation of corresponding images, thus improving the generalization ofthe learned text embedding. Additionally, CoRe can serve as a test-timeoptimization technique to further enhance the generations for specific prompts.Comprehensive experiments demonstrate that our method outperforms severalbaseline methods in both identity preservation and text alignment. Code will bemade publicly available.</description><author>Feize Wu, Yun Pang, Junyi Zhang, Lianyu Pang, Jian Yin, Baoquan Zhao, Qing Li, Xudong Mao</author><pubDate>Wed, 28 Aug 2024 16:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15914v1</guid></item><item><title>Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles in Public Policy Documents</title><link>http://arxiv.org/abs/2311.11844v3</link><description>Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4promise automation with better results and less programming, opening up newopportunities for text analysis in political science. In this study, weevaluate LLMs on three original coding tasks involving typical complexitiesencountered in political science settings: a non-English language, legal andpolitical jargon, and complex labels based on abstract constructs. Along thepaper, we propose a practical workflow to optimize the choice of the model andthe prompt. We find that the best prompting strategy consists of providing theLLMs with a detailed codebook, as the one provided to human coders. In thissetting, an LLM can be as good as or possibly better than a human annotatorwhile being much faster, considerably cheaper, and much easier to scale tolarge amounts of text. We also provide a comparison of GPT and popularopen-source LLMs, discussing the trade-offs in the model's choice. Our softwareallows LLMs to be easily used as annotators and is publicly available:https://github.com/lorelupo/pappa.</description><author>Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena Wängnerud</author><pubDate>Wed, 28 Aug 2024 16:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11844v3</guid></item><item><title>Infusion: internal diffusion for inpainting of dynamic textures and complex motion</title><link>http://arxiv.org/abs/2311.01090v3</link><description>Video inpainting is the task of filling a region in a video in a visuallyconvincing manner. It is very challenging due to the high dimensionality of thedata and the temporal consistency required for obtaining convincing results.Recently, diffusion models have shown impressive results in modeling complexdata distributions, including images and videos. Such models remain nonethelessvery expensive to train and to perform inference with, which strongly reducetheir applicability to videos, and yields unreasonable computational loads. Weshow that in the case of video inpainting, thanks to the highly auto-similarnature of videos, the training data of a diffusion model can be restricted tothe input video and still produce very satisfying results. This leads us toadopt an internal learning approach, which also allows us to greatly reduce theneural network size by about three orders of magnitude less than currentdiffusion models used for image inpainting. We also introduce a new method forefficient training and inference of diffusion models in the context of internallearning, by splitting the diffusion process into different learning intervalscorresponding to different noise levels of the diffusion process. To the bestof our knowledge, this is the first video inpainting method based purely ondiffusion. Other methods require additional components such as optical flowestimation, which limits their performance in the case of dynamic textures andcomplex motions. We show qualitative and quantitative results, demonstratingthat our method reaches state of the art performance in the case of dynamictextures and complex dynamic backgrounds.</description><author>Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson</author><pubDate>Wed, 28 Aug 2024 16:23:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01090v3</guid></item><item><title>MetaGFN: Exploring Distant Modes with Adapted Metadynamics for Continuous GFlowNets</title><link>http://arxiv.org/abs/2408.15905v1</link><description>Generative Flow Networks (GFlowNets) are a class of generative models thatsample objects in proportion to a specified reward function through a learnedpolicy. They can be trained either on-policy or off-policy, needing a balancebetween exploration and exploitation for fast convergence to a targetdistribution. While exploration strategies for discrete GFlowNets have beenstudied, exploration in the continuous case remains to be investigated, despitethe potential for novel exploration algorithms due to the local connectednessof continuous domains. Here, we introduce Adapted Metadynamics, a variant ofmetadynamics that can be applied to arbitrary black-box reward functions oncontinuous domains. We use Adapted Metadynamics as an exploration strategy forcontinuous GFlowNets. We show three continuous domains where the resultingalgorithm, MetaGFN, accelerates convergence to the target distribution anddiscovers more distant reward modes than previous off-policy explorationstrategies used for GFlowNets.</description><author>Dominic Phillips, Flaviu Cipcigan</author><pubDate>Wed, 28 Aug 2024 16:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15905v1</guid></item><item><title>LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments</title><link>http://arxiv.org/abs/2408.15903v1</link><description>The rapid obsolescence of information in Large Language Models (LLMs) hasdriven the development of various techniques to incorporate new facts. However,existing methods for knowledge editing still face difficulties with multi-hopquestions that require accurate fact identification and sequential logicalreasoning, particularly among numerous fact updates. To tackle thesechallenges, this paper introduces Graph Memory-based Editing for Large LanguageModels (GMeLLo), a straitforward and effective method that merges the explicitknowledge representation of Knowledge Graphs (KGs) with the linguisticflexibility of LLMs. Beyond merely leveraging LLMs for question answering,GMeLLo employs these models to convert free-form language into structuredqueries and fact triples, facilitating seamless interaction with KGs for rapidupdates and precise multi-hop reasoning. Our results show that GMeLLosignificantly surpasses current state-of-the-art knowledge editing methods inthe multi-hop question answering benchmark, MQuAKE, especially in scenarioswith extensive knowledge edits.</description><author>Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai</author><pubDate>Wed, 28 Aug 2024 16:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15903v1</guid></item><item><title>Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts</title><link>http://arxiv.org/abs/2408.15901v1</link><description>Efficiency, specialization, and adaptability to new data distributions arequalities that are hard to combine in current Large Language Models. TheMixture of Experts (MoE) architecture has been the focus of significantresearch because its inherent conditional computation enables such desirableproperties. In this work, we focus on "upcycling" dense expert models into anMoE, aiming to improve specialization while also adding the ability to adapt tonew tasks easily. We introduce Nexus, an enhanced MoE architecture withadaptive routing where the model learns to project expert embeddings fromdomain representations. This approach allows Nexus to flexibly add new expertsafter the initial upcycling through separately trained dense models, withoutrequiring large-scale MoE training for unseen data domains. Our experimentsshow that Nexus achieves a relative gain of up to 2.1% over the baseline forinitial upcycling, and a 18.8% relative gain for extending the MoE with a newexpert by using limited finetuning data. This flexibility of Nexus is crucialto enable an open-source ecosystem where every user continuously assemblestheir own MoE-mix according to their needs.</description><author>Nikolas Gritsch, Qizhen Zhang, Acyr Locatelli, Sara Hooker, Ahmet Üstün</author><pubDate>Wed, 28 Aug 2024 16:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15901v1</guid></item><item><title>Gen-Swarms: Adapting Deep Generative Models to Swarms of Drones</title><link>http://arxiv.org/abs/2408.15899v1</link><description>Gen-Swarms is an innovative method that leverages and combines thecapabilities of deep generative models with reactive navigation algorithms toautomate the creation of drone shows. Advancements in deep generative models,particularly diffusion models, have demonstrated remarkable effectiveness ingenerating high-quality 2D images. Building on this success, various works haveextended diffusion models to 3D point cloud generation. In contrast,alternative generative models such as flow matching have been proposed,offering a simple and intuitive transition from noise to meaningful outputs.However, the application of flow matching models to 3D point cloud generationremains largely unexplored. Gen-Swarms adapts these models to automaticallygenerate drone shows. Existing 3D point cloud generative models create pointtrajectories which are impractical for drone swarms. In contrast, our methodnot only generates accurate 3D shapes but also guides the swarm motion,producing smooth trajectories and accounting for potential collisions through areactive navigation algorithm incorporated into the sampling process. Forexample, when given a text category like Airplane, Gen-Swarms can rapidly andcontinuously generate numerous variations of 3D airplane shapes. Ourexperiments demonstrate that this approach is particularly well-suited fordrone shows, providing feasible trajectories, creating representative finalshapes, and significantly enhancing the overall performance of drone showgeneration.</description><author>Carlos Plou, Pablo Pueyo, Ruben Martinez-Cantin, Mac Schwager, Ana C. Murillo, Eduardo Montijano</author><pubDate>Wed, 28 Aug 2024 16:12:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15899v1</guid></item><item><title>Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation</title><link>http://arxiv.org/abs/2408.15898v1</link><description>The design of aerodynamic shapes, such as airfoils, has traditionallyrequired significant computational resources and relied on predefined designparameters, which limit the potential for novel shape synthesis. In this work,we introduce a data-driven methodology for airfoil generation using a diffusionmodel. Trained on a dataset of preexisting airfoils, our model can generate anarbitrary number of new airfoils from random vectors, which can be conditionedon specific aerodynamic performance metrics such as lift and drag, or geometriccriteria. Our results demonstrate that the diffusion model effectively producesairfoil shapes with realistic aerodynamic properties, offering substantialimprovements in efficiency, flexibility, and the potential for discoveringinnovative airfoil designs. This approach significantly expands the designspace, facilitating the synthesis of high-performance aerodynamic shapes thattranscend the limitations of traditional methods.</description><author>Reid Graves, Amir Barati Farimani</author><pubDate>Wed, 28 Aug 2024 16:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15898v1</guid></item><item><title>A New Method for Cross-Lingual-based Semantic Role Labeling</title><link>http://arxiv.org/abs/2408.15896v1</link><description>Semantic role labeling is a crucial task in natural language processing,enabling better comprehension of natural language. However, the lack ofannotated data in multiple languages has posed a challenge for researchers. Toaddress this, a deep learning algorithm based on model transfer has beenproposed. The algorithm utilizes a dataset consisting of the English portion ofCoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiencyof training, only ten percent of the educational data from each language isused. The results of the proposed model demonstrate significant improvementscompared to Niksirt et al.'s model. In monolingual mode, the proposed modelachieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,the improvement was even more substantial, reaching 6.23 percent. Worth notingis that the compared model only trained two of the four stages of semantic rolelabeling and employed golden data for the remaining two stages. This suggeststhat the actual superiority of the proposed model surpasses the reportednumbers by a significant margin. The development of cross-lingual methods forsemantic role labeling holds promise, particularly in addressing the scarcityof annotated data for various languages. These advancements pave the way forfurther research in understanding and processing natural language acrossdifferent linguistic contexts.</description><author>Mohammad Ebrahimi, Behrouz Minaei Bidgoli, Nasim Khozouei</author><pubDate>Wed, 28 Aug 2024 16:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15896v1</guid></item><item><title>Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models</title><link>http://arxiv.org/abs/2408.15895v1</link><description>Human coders are biased. We test similar biases in Large Language Models(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik andMeyer (2018), we find evidence that LLMs use political information, andspecifically party cues, to judge political statements. Not only do LLMs userelevant information to contextualize whether a statement is positive,negative, or neutral based on the party cue, they also reflect the biases ofthe human-generated data upon which they have been trained. We also find thatunlike humans, who are only biased when faced with statements from extremeparties, LLMs exhibit significant bias even when prompted with statements fromcenter-left and center-right parties. The implications of our findings arediscussed in the conclusion.</description><author>Sebastian Vallejo Vera, Hunter Driggers</author><pubDate>Wed, 28 Aug 2024 16:05:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15895v1</guid></item><item><title>The Role of Fibration Symmetries in Geometric Deep Learning</title><link>http://arxiv.org/abs/2408.15894v1</link><description>Geometric Deep Learning (GDL) unifies a broad class of machine learningtechniques from the perspectives of symmetries, offering a framework forintroducing problem-specific inductive biases like Graph Neural Networks(GNNs). However, the current formulation of GDL is limited to global symmetriesthat are not often found in real-world problems. We propose to relax GDL toallow for local symmetries, specifically fibration symmetries in graphs, toleverage regularities of realistic instances. We show that GNNs apply theinductive bias of fibration symmetries and derive a tighter upper bound fortheir expressive power. Additionally, by identifying symmetries in networks, wecollapse network nodes, thereby increasing their computational efficiencyduring both inference and training of deep neural networks. The mathematicalextension introduced here applies beyond graphs to manifolds, bundles, andgrids for the development of models with inductive biases induced by localsymmetries that can lead to better generalization.</description><author>Osvaldo Velarde, Lucas Parra, Paolo Boldi, Hernan Makse</author><pubDate>Wed, 28 Aug 2024 16:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15894v1</guid></item><item><title>Disentangled Diffusion Autoencoder for Harmonization of Multi-site Neuroimaging Data</title><link>http://arxiv.org/abs/2408.15890v1</link><description>Combining neuroimaging datasets from multiple sites and scanners can helpincrease statistical power and thus provide greater insight into subtleneuroanatomical effects. However, site-specific effects pose a challenge bypotentially obscuring the biological signal and introducing unwanted variance.Existing harmonization techniques, which use statistical models to remove sucheffects, have been shown to incompletely remove site effects while also failingto preserve biological variability. More recently, generative models using GANsor autoencoder-based approaches, have been proposed for site adjustment.However, such methods are known for instability during training or blurry imagegeneration. In recent years, diffusion models have become increasingly popularfor their ability to generate high-quality synthetic images. In this work, weintroduce the disentangled diffusion autoencoder (DDAE), a novel diffusionmodel designed for controlling specific aspects of an image. We apply the DDAEto the task of harmonizing MR images by generating high-quality site-adjustedimages that preserve biological variability. We use data from 7 different sitesand demonstrate the DDAE's superiority in generating high-resolution,harmonized 2D MR images over previous approaches. As far as we are aware, thiswork marks the first diffusion-based model for site adjustment of neuroimagingdata.</description><author>Ayodeji Ijishakin, Ana Lawry Aguila, Elizabeth Levitis, Ahmed Abdulaal, Andre Altmann, James Cole</author><pubDate>Wed, 28 Aug 2024 16:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15890v1</guid></item><item><title>SpineMamba: Enhancing 3D Spinal Segmentation in Clinical Imaging through Residual Visual Mamba Layers and Shape Priors</title><link>http://arxiv.org/abs/2408.15887v1</link><description>Accurate segmentation of 3D clinical medical images is critical in thediagnosis and treatment of spinal diseases. However, the inherent complexity ofspinal anatomy and uncertainty inherent in current imaging technologies, posessignificant challenges for semantic segmentation of spinal images. Althoughconvolutional neural networks (CNNs) and Transformer-based models have madesome progress in spinal segmentation, their limitations in handling long-rangedependencies hinder further improvements in segmentation accuracy.To addressthese challenges, we introduce a residual visual Mamba layer to effectivelycapture and model the deep semantic features and long-range spatialdependencies of 3D spinal data. To further enhance the structural semanticunderstanding of the vertebrae, we also propose a novel spinal shape priormodule that captures specific anatomical information of the spine from medicalimages, significantly enhancing the model's ability to extract structuralsemantic information of the vertebrae. Comparative and ablation experiments ontwo datasets demonstrate that SpineMamba outperforms existing state-of-the-artmodels. On the CT dataset, the average Dice similarity coefficient forsegmentation reaches as high as 94.40, while on the MR dataset, it reaches86.95. Notably, compared to the renowned nnU-Net, SpineMamba achieves superiorsegmentation performance, exceeding it by up to 2 percentage points. Thisunderscores its accuracy, robustness, and excellent generalizationcapabilities.</description><author>Zhiqing Zhang, Tianyong Liu, Guojia Fan, Bin Li, Qianjin Feng, Shoujun Zhou</author><pubDate>Wed, 28 Aug 2024 15:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15887v1</guid></item><item><title>Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2408.15886v1</link><description>In recent years, the evolution of machine learning techniques hassignificantly impacted the field of intrusion detection, particularly withinthe context of the Internet of Things (IoT). As IoT networks expand, the needfor robust security measures to counteract potential threats has becomeincreasingly critical. This paper introduces a hybrid Intrusion DetectionSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)with the XGBoost algorithm. Our proposed IDS leverages the unique capabilitiesof KANs, which utilize learnable activation functions to model complexrelationships within data, alongside the powerful ensemble learning techniquesof XGBoost, known for its high performance in classification tasks. This hybridapproach not only enhances the detection accuracy but also improves theinterpretability of the model, making it suitable for dynamic and intricate IoTenvironments. Experimental evaluations demonstrate that our hybrid IDS achievesan impressive detection accuracy exceeding 99% in distinguishing between benignand malicious activities. Additionally, we were able to achieve F1 scores,precision, and recall that exceeded 98%. Furthermore, we conduct a comparativeanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessingperformance metrics such as Precision, Recall, and F1-score. The resultsunderscore the efficacy of integrating KANs with XGBoost, highlighting thepotential of this innovative approach to significantly strengthen the securityframework of IoT networks.</description><author>Amar Amouri, Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Ismail Butun, Imad Mahgoub</author><pubDate>Wed, 28 Aug 2024 15:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15886v1</guid></item><item><title>Secret Collusion among Generative AI Agents</title><link>http://arxiv.org/abs/2402.07510v2</link><description>Recent capability increases in large language models (LLMs) open upapplications in which groups of communicating generative AI agents solve jointtasks. This poses privacy and security challenges concerning the unauthorisedsharing of information, or other unwanted forms of agent coordination. Modernsteganographic techniques could render such dynamics hard to detect. In thispaper, we comprehensively formalise the problem of secret collusion in systemsof generative AI agents by drawing on relevant concepts from both AI andsecurity literature. We study incentives for the use of steganography, andpropose a variety of mitigation measures. Our investigations result in a modelevaluation framework that systematically tests capabilities required forvarious forms of secret collusion. We provide extensive empirical resultsacross a range of contemporary LLMs. While the steganographic capabilities ofcurrent models remain limited, GPT-4 displays a capability jump suggesting theneed for continuous monitoring of steganographic frontier model capabilities.We conclude by laying out a comprehensive research program to mitigate futurerisks of collusion between generative AI models.</description><author>Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H. S. Torr, Lewis Hammond, Christian Schroeder de Witt</author><pubDate>Wed, 28 Aug 2024 15:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07510v2</guid></item><item><title>LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation</title><link>http://arxiv.org/abs/2408.15881v1</link><description>We introduce LLaVA-MoD, a novel framework designed to enable the efficienttraining of small-scale Multimodal Language Models (s-MLLM) by distillingknowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamentalchallenges in MLLM distillation. First, we optimize the network structure ofs-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into thelanguage model, striking a balance between computational efficiency and modelexpressiveness. Second, we propose a progressive knowledge transfer strategy toensure comprehensive knowledge migration. This strategy begins with mimicdistillation, where we minimize the Kullback-Leibler (KL) divergence betweenoutput distributions to enable the student model to emulate the teachernetwork's understanding. Following this, we introduce preference distillationvia Direct Preference Optimization (DPO), where the key lies in treating l-MLLMas the reference model. During this phase, the s-MLLM's ability to discriminatebetween superior and inferior examples is significantly enhanced beyond l-MLLM,leading to a better student that surpasses its teacher, particularly inhallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoDoutperforms existing models across various multimodal benchmarks whilemaintaining a minimal number of activated parameters and low computationalcosts. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpassesQwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% ofthe training data and 23% trainable parameters. These results underscoreLLaVA-MoD's ability to effectively distill comprehensive knowledge from itsteacher model, paving the way for the development of more efficient MLLMs. Thecode will be available on: https://github.com/shufangxun/LLaVA-MoD.</description><author>Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, Haoyuan Li, Bolin Li, Zhelun Yu, Si Liu, Hongsheng Li, Hao Jiang</author><pubDate>Wed, 28 Aug 2024 15:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15881v1</guid></item><item><title>Persuasion Games using Large Language Models</title><link>http://arxiv.org/abs/2408.15879v1</link><description>Large Language Models (LLMs) have emerged as formidable instruments capableof comprehending and producing human-like text. This paper explores thepotential of LLMs, to shape human perspectives and subsequently influence theirdecisions on particular tasks. This capability finds applications in diversedomains such as Investment, Credit cards and Insurance, wherein they assistusers in selecting appropriate insurance policies, investment plans, Creditcards, Retail, as well as in Behavioral Change Support Systems (BCSS). We present a sophisticated multi-agent framework wherein a consortium ofagents operate in collaborative manner. The primary agent engages directly withusers through persuasive dialogue, while the auxiliary agents perform taskssuch as information retrieval, response analysis, development of persuasionstrategies, and validation of facts. Empirical evidence from our experimentsdemonstrates that this collaborative methodology significantly enhances thepersuasive efficacy of the LLM. We analyze user resistance to persuasiveefforts continuously and counteract it by employing a combination of rule-basedand LLM-based resistance-persuasion mapping techniques. We employ simulated personas and generate conversations in insurance,banking, and retail domains to evaluate the proficiency of large languagemodels (LLMs) in recognizing, adjusting to, and influencing various personalitytypes. Concurrently, we examine the resistance mechanisms employed by LLMsimulated personas. Persuasion is quantified via measurable surveys before andafter interaction, LLM-generated scores on conversation, and user decisions(purchase or non-purchase).</description><author>Ganesh Prasath Ramani, Shirish Karande, Santhosh V, Yash Bhatia</author><pubDate>Wed, 28 Aug 2024 15:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15879v1</guid></item><item><title>GINN-KAN: Interpretability pipelining with applications in Physics Informed Neural Networks</title><link>http://arxiv.org/abs/2408.14780v2</link><description>Neural networks are powerful function approximators, yet their ``black-box"nature often renders them opaque and difficult to interpret. While manypost-hoc explanation methods exist, they typically fail to capture theunderlying reasoning processes of the networks. A truly interpretable neuralnetwork would be trained similarly to conventional models using techniques suchas backpropagation, but additionally provide insights into the learnedinput-output relationships. In this work, we introduce the concept ofinterpretability pipelineing, to incorporate multiple interpretabilitytechniques to outperform each individual technique. To this end, we firstevaluate several architectures that promise such interpretability, with aparticular focus on two recent models selected for their potential toincorporate interpretability into standard neural network architectures whilestill leveraging backpropagation: the Growing Interpretable Neural Network(GINN) and Kolmogorov Arnold Networks (KAN). We analyze the limitations andstrengths of each and introduce a novel interpretable neural network GINN-KANthat synthesizes the advantages of both models. When tested on the Feynmansymbolic regression benchmark datasets, GINN-KAN outperforms both GINN and KAN.To highlight the capabilities and the generalizability of this approach, weposition GINN-KAN as an alternative to conventional black-box networks inPhysics-Informed Neural Networks (PINNs). We expect this to have far-reachingimplications in the application of deep learning pipelines in the naturalsciences. Our experiments with this interpretable PINN on 15 different partialdifferential equations demonstrate that GINN-KAN augmented PINNs outperformPINNs with black-box networks in solving differential equations and surpass thecapabilities of both GINN and KAN.</description><author>Nisal Ranasinghe, Yu Xia, Sachith Seneviratne, Saman Halgamuge</author><pubDate>Wed, 28 Aug 2024 15:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14780v2</guid></item><item><title>Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation</title><link>http://arxiv.org/abs/2408.15876v1</link><description>In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2)pipeline to explore the training-free paradigm for audio andlanguage-referenced video object segmentation, namely AVS and RVOS tasks. Theintuitive solution leverages GroundingDINO to identify the target object from asingle frame and SAM 2 to segment the identified object throughout the video,which is less robust to spatiotemporal variations due to a lack of videocontext exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novelGPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to performtwo-step temporal-spatial reasoning for sequentially selecting pivot frames andpivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.Within GPT-PS, two task-specific Chain-of-Thought prompts are designed tounleash GPT's temporal-spatial reasoning capacity by guiding GPT to makeselections based on a comprehensive understanding of video and referenceinformation. Furthermore, we propose a Language-Binded Reference Unification(LBRU) module to convert audio signals into language-formatted references,thereby unifying the formats of AVS and RVOS tasks in the same pipeline.Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2pipeline achieves performances comparable to or even better thanfully-supervised fine-tuning methods. The code is available at:https://github.com/appletea233/AL-Ref-SAM2.</description><author>Shaofei Huang, Rui Ling, Hongyu Li, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, Si Liu</author><pubDate>Wed, 28 Aug 2024 15:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15876v1</guid></item><item><title>A Deep Learning Based Resource Allocator for Communication Systems with Dynamic User Utility Demands</title><link>http://arxiv.org/abs/2311.04600v2</link><description>Deep learning (DL) based resource allocation (RA) has recently gainedsignificant attention due to its performance efficiency. However, most relatedstudies assume an ideal case where the number of users and their utilitydemands, e.g., data rate constraints, are fixed, and the designed DL-based RAscheme exploits a policy trained only for these fixed parameters. Consequently,computationally complex policy retraining is required whenever these parameterschange. In this paper, we introduce a DL-based resource allocator (ALCOR) thatallows users to adjust their utility demands freely, such as based on theirapplication layer requirements. ALCOR employs deep neural networks (DNNs) asthe policy in a time-sharing problem. The underlying optimization algorithmiteratively optimizes the on-off status of users to satisfy their utilitydemands in expectation. The policy performs unconstrained RA (URA)--RA withoutconsidering user utility demands--among active users to maximize the sumutility (SU) at each time instant. Depending on the chosen URA scheme, ALCORcan perform RA in either a centralized or distributed scenario. Derivedconvergence analyses provide guarantees for ALCOR's convergence, and numericalexperiments corroborate its effectiveness.</description><author>Pourya Behmandpoor, Mark Eisen, Panagiotis Patrinos, Marc Moonen</author><pubDate>Wed, 28 Aug 2024 15:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04600v2</guid></item><item><title>Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)</title><link>http://arxiv.org/abs/2408.15874v1</link><description>Outlier detection algorithms typically assign an outlier score to eachobservation in a dataset, indicating the degree to which an observation is anoutlier. However, these scores are often not comparable across algorithms andcan be difficult for humans to interpret. Statistical scaling addresses thisproblem by transforming outlier scores into outlier probabilities without usingground-truth labels, thereby improving interpretability and comparabilityacross algorithms. However, the quality of this transformation can be differentfor outliers and inliers. Missing outliers in scenarios where they are ofparticular interest - such as healthcare, finance, or engineering - can becostly or dangerous. Thus, ensuring good probabilities for outliers isessential. This paper argues that statistical scaling, as commonly used in theliterature, does not produce equally good probabilities for outliers as forinliers. Therefore, we propose robust statistical scaling, which uses robustestimators to improve the probabilities for outliers. We evaluate severalvariants of our method against other outlier score transformations forreal-world datasets and outlier detection algorithms, where it can improve theprobabilities for outliers.</description><author>Philipp Röchner, Henrique O. Marques, Ricardo J. G. B. Campello, Arthur Zimek, Franz Rothlauf</author><pubDate>Wed, 28 Aug 2024 15:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15874v1</guid></item><item><title>HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus</title><link>http://arxiv.org/abs/2309.02731v3</link><description>ChatGPT has garnered significant interest due to its impressive performance;however, there is growing concern about its potential risks, particularly inthe detection of AI-generated content (AIGC), which is often challenging foruntrained individuals to identify. Current datasets used for detectingChatGPT-generated text primarily focus on question-answering tasks, oftenoverlooking tasks with semantic-invariant properties, such as summarization,translation, and paraphrasing. In this paper, we demonstrate that detectingmodel-generated text in semantic-invariant tasks is more challenging. Toaddress this gap, we introduce a more extensive and comprehensive dataset thatincorporates a wider range of tasks than previous work, including those withsemantic-invariant properties.</description><author>Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu</author><pubDate>Wed, 28 Aug 2024 15:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02731v3</guid></item><item><title>Geometric Neural Network based on Phase Space for BCI-EEG decoding</title><link>http://arxiv.org/abs/2403.05645v3</link><description>Objective: The integration of Deep Learning (DL) algorithms on brain signalanalysis is still in its nascent stages compared to their success in fieldslike Computer Vision. This is particularly true for BCI, where the brainactivity is decoded to control external devices without requiring musclecontrol. Electroencephalography (EEG) is a widely adopted choice for designingBCI systems due to its non-invasive and cost-effective nature and excellenttemporal resolution. Still, it comes at the expense of limited training data,poor signal-to-noise, and a large variability across and within-subjectrecordings. Finally, setting up a BCI system with many electrodes takes a longtime, hindering the widespread adoption of reliable DL architectures in BCIsoutside research laboratories. To improve adoption, we need to improve usercomfort using, for instance, reliable algorithms that operate with fewelectrodes. Approach: Our research aims to develop a DL algorithm that deliverseffective results with a limited number of electrodes. Taking advantage of theAugmented Covariance Method and the framework of SPDNet, we propose thePhase-SPDNet architecture and analyze its performance and the interpretabilityof the results. The evaluation is conducted on 5-fold cross-validation, usingonly three electrodes positioned above the Motor Cortex. The methodology wastested on nearly 100 subjects from several open-source datasets using theMother Of All BCI Benchmark (MOABB) framework. Main results: The results of ourPhase-SPDNet demonstrate that the augmented approach combined with the SPDNetsignificantly outperforms all the current state-of-the-art DL architecture inMI decoding. Significance: This new architecture is explainable and with a lownumber of trainable parameters.</description><author>Igor Carrara, Bruno Aristimunha, Marie-Constance Corsi, Raphael Y. de Camargo, Sylvain Chevallier, Théodore Papadopoulo</author><pubDate>Wed, 28 Aug 2024 15:39:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05645v3</guid></item><item><title>GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model</title><link>http://arxiv.org/abs/2408.15868v1</link><description>Autonomous driving training requires a diverse range of datasets encompassingvarious traffic conditions, weather scenarios, and road types. Traditional dataaugmentation methods often struggle to generate datasets that represent rareoccurrences. To address this challenge, we propose GenDDS, a novel approach forgenerating driving scenarios generation by leveraging the capabilities ofStable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodologyinvolves the use of descriptive prompts to guide the synthesis process, aimedat producing realistic and diverse driving scenarios. With the power of thelatest computer vision techniques, such as ControlNet and Hotshot-XL, we havebuilt a complete pipeline for video generation together with SDXL. We employthe KITTI dataset, which includes real-world driving videos, to train themodel. Through a series of experiments, we demonstrate that our model cangenerate high-quality driving videos that closely replicate the complexity andvariability of real-world driving scenarios. This research contributes to thedevelopment of sophisticated training data for autonomous driving systems andopens new avenues for creating virtual environments for simulation andvalidation purposes.</description><author>Yongjie Fu, Yunlong Li, Xuan Di</author><pubDate>Wed, 28 Aug 2024 15:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15868v1</guid></item><item><title>On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers</title><link>http://arxiv.org/abs/2407.10734v2</link><description>On-device training of DNNs allows models to adapt and fine-tune to newlycollected data or changing domains while deployed on microcontroller units(MCUs). However, DNN training is a resource-intensive task, making theimplementation and execution of DNN training algorithms on MCUs challenging dueto low processor speeds, constrained throughput, limited floating-pointsupport, and memory constraints. In this work, we explore on-device training ofDNNs for Cortex-M MCUs. We present a method that enables efficient training ofDNNs completely in place on the MCU using fully quantized training (FQT) anddynamic partial gradient updates. We demonstrate the feasibility of ourapproach on multiple vision and time-series datasets and provide insights intothe tradeoff between training accuracy, memory overhead, energy, and latency onreal hardware.</description><author>Mark Deutel, Frank Hannig, Christopher Mutschler, Jürgen Teich</author><pubDate>Wed, 28 Aug 2024 15:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10734v2</guid></item><item><title>Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection</title><link>http://arxiv.org/abs/2408.15866v1</link><description>The current technology landscape lacks a foundational AI model for solvingprocess engineering calculations. In this work, we introduce a novel autonomousagent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) toenhance open, customizable small code language models (SLMs) for thesecalculations. By combining instruction tuned code SLMs with Retrieval-AugmentedCode Generation (RACG) using external tools, the agent generates, debugs, andoptimizes code from natural language specifications. Our approach addresses thelimitations of the current lack of a foundational AI model for specializedprocess engineering tasks and offers benefits of explainability, knowledgeediting, and cost-effectiveness. Additionally, we curate custom datasets ofchemical and process engineering problems and solutions to overcome datascarcity. Experimental results show that our framework matches the performanceof large-scale proprietary models on benchmark datasets, proving itseffectiveness and usability.</description><author>Sagar Srinivas Sakhinana, Geethan Sannidhi, Venkataramana Runkana</author><pubDate>Wed, 28 Aug 2024 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15866v1</guid></item><item><title>microYOLO: Towards Single-Shot Object Detection on Microcontrollers</title><link>http://arxiv.org/abs/2408.15865v1</link><description>This work-in-progress paper presents results on the feasibility ofsingle-shot object detection on microcontrollers using YOLO. Single-shot objectdetectors like YOLO are widely used, however due to their complexity mainly onlarger GPU-based platforms. We present microYOLO, which can be used on Cortex-Mbased microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS whenclassifying 128x128 RGB images while using less than 800 KB Flash and less than350 KB RAM. Furthermore, we share experimental results for three differentobject detection tasks, analyzing the accuracy of microYOLO on them.</description><author>Mark Deutel, Christopher Mutschler, Jürgen Teich</author><pubDate>Wed, 28 Aug 2024 15:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15865v1</guid></item><item><title>Provable Probabilistic Imaging using Score-Based Generative Priors</title><link>http://arxiv.org/abs/2310.10835v3</link><description>Estimating high-quality images while also quantifying their uncertainty aretwo desired features in an image reconstruction algorithm for solving ill-posedinverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) asa principled framework for characterizing the space of possible solutions to ageneral inverse problem. PMC is able to incorporate expressive score-basedgenerative priors for high-quality image reconstruction while also performinguncertainty quantification via posterior sampling. In particular, we developtwo PMC algorithms that can be viewed as the sampling analogues of thetraditional plug-and-play priors (PnP) and regularization by denoising (RED)algorithms. To improve the sampling efficiency, we introduce weighted annealinginto these PMC algorithms, further developing two additional annealed PMCalgorithms (APMC). We establish a theoretical analysis for characterizing theconvergence behavior of PMC algorithms. Our analysis provides non-asymptoticstationarity guarantees in terms of the Fisher information, fully compatiblewith the joint presence of weighted annealing, potentially non-log-concavelikelihoods, and imperfect score networks. We demonstrate the performance ofthe PMC algorithms on multiple representative inverse problems with both linearand nonlinear forward models. Experimental results show that PMC significantlyimproves reconstruction quality and enables high-fidelity uncertaintyquantification.</description><author>Yu Sun, Zihui Wu, Yifan Chen, Berthy T. Feng, Katherine L. Bouman</author><pubDate>Wed, 28 Aug 2024 15:29:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10835v3</guid></item><item><title>From Complexity to Clarity: How AI Enhances Perceptions of Scientists and the Public's Understanding of Science</title><link>http://arxiv.org/abs/2405.00706v3</link><description>This paper evaluated the effectiveness of using generative AI to simplifyscience communication and enhance the public's understanding of science. Bycomparing lay summaries of journal articles from PNAS, yoked to those generatedby AI, this work first assessed linguistic simplicity differences across suchsummaries and public perceptions in follow-up experiments. Specifically, Study1a analyzed simplicity features of PNAS abstracts (scientific summaries) andsignificance statements (lay summaries), observing that lay summaries wereindeed linguistically simpler, but effect size differences were small. Study 1bused a large language model, GPT-4, to create significance statements based onpaper abstracts and this more than doubled the average effect size withoutfine-tuning. Study 2 experimentally demonstrated that simply-written GPTsummaries facilitated more favorable perceptions of scientists (they wereperceived as more credible and trustworthy, but less intelligent) than morecomplexly-written human PNAS summaries. Crucially, Study 3 experimentallydemonstrated that participants comprehended scientific writing better afterreading simple GPT summaries compared to complex PNAS summaries. In their ownwords, participants also summarized scientific papers in a more detailed andconcrete manner after reading GPT summaries compared to PNAS summaries of thesame article. AI has the potential to engage scientific communities and thepublic via a simple language heuristic, advocating for its integration intoscientific dissemination for a more informed society.</description><author>David M. Markowitz</author><pubDate>Wed, 28 Aug 2024 15:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00706v3</guid></item><item><title>AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors</title><link>http://arxiv.org/abs/2406.18394v4</link><description>The complexity of financial data, characterized by its variability and lowsignal-to-noise ratio, necessitates advanced methods in quantitative investmentthat prioritize both performance and interpretability.Transitioning from earlymanual extraction to genetic programming, the most advanced approach in thealpha factor mining domain currently employs reinforcement learning to mine aset of combination factors with fixed weights. However, the performance ofresultant alpha factors exhibits inconsistency, and the inflexibility of fixedfactor weights proves insufficient in adapting to the dynamic nature offinancial markets. To address this issue, this paper proposes a two-stageformulaic alpha generating framework AlphaForge, for alpha factor mining andfactor combination. This framework employs a generative-predictive neuralnetwork to generate factors, leveraging the robust spatial explorationcapabilities inherent in deep learning while concurrently preserving diversity.The combination model within the framework incorporates the temporalperformance of factors for selection and dynamically adjusts the weightsassigned to each component alpha factor. Experiments conducted on real-worlddatasets demonstrate that our proposed model outperforms contemporarybenchmarks in formulaic alpha factor mining. Furthermore, our model exhibits anotable enhancement in portfolio returns within the realm of quantitativeinvestment and real money investment.</description><author>Hao Shi, Weili Song, Xinting Zhang, Jiahe Shi, Cuicui Luo, Xiang Ao, Hamid Arian, Luis Seco</author><pubDate>Wed, 28 Aug 2024 15:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18394v4</guid></item><item><title>Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation</title><link>http://arxiv.org/abs/2408.15861v1</link><description>Backdoor attacks present a serious security threat to deep neuron networks(DNNs). Although numerous effective defense techniques have been proposed inrecent years, they inevitably rely on the availability of either clean orpoisoned data. In contrast, data-free defense techniques have evolved slowlyand still lag significantly in performance. To address this issue, differentfrom the traditional approach of pruning followed by fine-tuning, we propose anovel data-free defense method named Optimal Transport-based Backdoor Repairing(OTBR) in this work. This method, based on our findings on neuron weightchanges (NWCs) of random unlearning, uses optimal transport (OT)-based modelfusion to combine the advantages of both pruned and backdoored models.Specifically, we first demonstrate our findings that the NWCs of randomunlearning are positively correlated with those of poison unlearning. Based onthis observation, we propose a random-unlearning NWC pruning technique toeliminate the backdoor effect and obtain a backdoor-free pruned model. Then,motivated by the OT-based model fusion, we propose the pruned-to-backdooredOT-based fusion technique, which fuses pruned and backdoored models to combinethe advantages of both, resulting in a model that demonstrates high cleanaccuracy and a low attack success rate. To our knowledge, this is the firstwork to apply OT and model fusion techniques to backdoor defense. Extensiveexperiments show that our method successfully defends against all sevenbackdoor attacks across three benchmark datasets, outperforming bothstate-of-the-art (SOTA) data-free and data-dependent methods. The codeimplementation and Appendix are provided in the Supplementary Material.</description><author>Weilin Lin, Li Liu, Jianze Li, Hui Xiong</author><pubDate>Wed, 28 Aug 2024 15:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15861v1</guid></item><item><title>What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector</title><link>http://arxiv.org/abs/2408.15857v1</link><description>This study presents a detailed analysis of the YOLOv8 object detection model,focusing on its architecture, training techniques, and performance improvementsover previous iterations like YOLOv5. Key innovations, including the CSPNetbackbone for enhanced feature extraction, the FPN+PAN neck for superiormulti-scale object detection, and the transition to an anchor-free approach,are thoroughly examined. The paper reviews YOLOv8's performance acrossbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracyand real-time capabilities across diverse hardware platforms. Additionally, thestudy explores YOLOv8's developer-friendly enhancements, such as its unifiedPython package and CLI, which streamline model training and deployment.Overall, this research positions YOLOv8 as a state-of-the-art solution in theevolving object detection field.</description><author>Muhammad Yaseen</author><pubDate>Wed, 28 Aug 2024 15:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15857v1</guid></item><item><title>Correlation recurrent units: A novel neural architecture for improving the predictive performance of time-series data</title><link>http://arxiv.org/abs/2211.16653v3</link><description>The time-series forecasting (TSF) problem is a traditional problem in thefield of artificial intelligence. Models such as Recurrent Neural Network(RNN), Long Short Term Memory (LSTM), and GRU (Gate Recurrent Units) havecontributed to improving the predictive accuracy of TSF. Furthermore, modelstructures have been proposed to combine time-series decomposition methods,such as seasonal-trend decomposition using Loess (STL) to ensure improvedpredictive accuracy. However, because this approach is learned in anindependent model for each component, it cannot learn the relationships betweentime-series components. In this study, we propose a new neural architecturecalled a correlation recurrent unit (CRU) that can perform time seriesdecomposition within a neural cell and learn correlations (autocorrelation andcorrelation) between each decomposition component. The proposed neuralarchitecture was evaluated through comparative experiments with previousstudies using five univariate time-series datasets and four multivariatetime-series data. The results showed that long- and short-term predictiveperformance was improved by more than 10%. The experimental results show thatthe proposed CRU is an excellent method for TSF problems compared to otherneural architectures.</description><author>Sunghyun Sim, Dohee Kim, Hyerim Bae</author><pubDate>Wed, 28 Aug 2024 15:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16653v3</guid></item><item><title>chemtrain: Learning Deep Potential Models via Automatic Differentiation and Statistical Physics</title><link>http://arxiv.org/abs/2408.15852v1</link><description>Neural Networks (NNs) are promising models for refining the accuracy ofmolecular dynamics, potentially opening up new fields of application. Typicallytrained bottom-up, atomistic NN potential models can reach first-principleaccuracy, while coarse-grained implicit solvent NN potentials surpass classicalcontinuum solvent models. However, overcoming the limitations of costlygeneration of accurate reference data and data inefficiency of common bottom-uptraining demands efficient incorporation of data from many sources. This paperintroduces the framework chemtrain to learn sophisticated NN potential modelsthrough customizable training routines and advanced training algorithms. Theseroutines can combine multiple top-down and bottom-up algorithms, e.g., toincorporate both experimental and simulation data or pre-train potentials withless costly algorithms. chemtrain provides an object-oriented high-levelinterface to simplify the creation of custom routines. On the lower level,chemtrain relies on JAX to compute gradients and scale the computations to useavailable resources. We demonstrate the simplicity and importance of combiningmultiple algorithms in the examples of parametrizing an all-atomistic model oftitanium and a coarse-grained implicit solvent model of alanine dipeptide.</description><author>Paul Fuchs, Stephan Thaler, Sebastien Röcken, Julija Zavadlav</author><pubDate>Wed, 28 Aug 2024 15:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15852v1</guid></item><item><title>Imperceptible Protection against Style Imitation from Diffusion Models</title><link>http://arxiv.org/abs/2403.19254v2</link><description>Recent progress in diffusion models has profoundly enhanced the fidelity ofimage generation, but it has raised concerns about copyright infringements.While prior methods have introduced adversarial perturbations to prevent styleimitation, most are accompanied by the degradation of artworks' visual quality.Recognizing the importance of maintaining this, we introduce a visuallyimproved protection method while preserving its protection capability. To thisend, we devise a perceptual map to highlight areas sensitive to human eyes,guided by instance-aware refinement, which refines the protection intensityaccordingly. We also introduce a difficulty-aware protection by predicting howdifficult the artwork is to protect and dynamically adjusting the intensitybased on this. Lastly, we integrate a perceptual constraints bank to furtherimprove the imperceptibility. Results show that our method substantiallyelevates the quality of the protected image without compromising on protectionefficacy.</description><author>Namhyuk Ahn, Wonhyuk Ahn, KiYoon Yoo, Daesik Kim, Seung-Hun Nam</author><pubDate>Wed, 28 Aug 2024 15:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19254v2</guid></item><item><title>RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</title><link>http://arxiv.org/abs/2404.07839v2</link><description>We introduce RecurrentGemma, a family of open language models which usesGoogle's novel Griffin architecture. Griffin combines linear recurrences withlocal attention to achieve excellent performance on language. It has afixed-sized state, which reduces memory use and enables efficient inference onlong sequences. We provide two sizes of models, containing 2B and 9Bparameters, and provide pre-trained and instruction tuned variants for both.Our models achieve comparable performance to similarly-sized Gemma baselinesdespite being trained on fewer tokens.</description><author>Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz Gustavo Martins, Elisa Bandy, David Huntsperger, Glen</author><pubDate>Wed, 28 Aug 2024 15:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07839v2</guid></item><item><title>Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction</title><link>http://arxiv.org/abs/2408.15844v1</link><description>Video key frame extraction is important in various fields, such as videosummary, retrieval, and compression. Therefore, we suggest a video key frameextraction algorithm based on shot segmentation using Von Neumann entropy. Thesegmentation of shots is achieved through the computation of Von Neumannentropy of the similarity matrix among frames within the video sequence. Theinitial frame of each shot is selected as key frames, which combines thetemporal sequence information of frames. The experimental results show theextracted key frames can fully and accurately represent the original videocontent while minimizing the number of repeated frames.</description><author>Xueqing Zhang. Di Fu, Naihao Liu</author><pubDate>Wed, 28 Aug 2024 15:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15844v1</guid></item><item><title>A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules</title><link>http://arxiv.org/abs/2404.01245v2</link><description>Since ChatGPT was introduced in November 2022, embedding (nearly)unnoticeable statistical signals into text generated by large language models(LLMs), also known as watermarking, has been used as a principled approach toprovable detection of LLM-generated text from its human-written counterpart. Inthis paper, we introduce a general and flexible framework for reasoning aboutthe statistical efficiency of watermarks and designing powerful detectionrules. Inspired by the hypothesis testing formulation of watermark detection,our framework starts by selecting a pivotal statistic of the text and a secretkey -- provided by the LLM to the verifier -- to enable controlling the falsepositive rate (the error of mistakenly detecting human-written text asLLM-generated). Next, this framework allows one to evaluate the power ofwatermark detection rules by obtaining a closed-form expression of theasymptotic false negative rate (the error of incorrectly classifyingLLM-generated text as human-written). Our framework further reduces the problemof determining the optimal detection rule to solving a minimax optimizationprogram. We apply this framework to two representative watermarks -- one ofwhich has been internally implemented at OpenAI -- and obtain several findingsthat can be instrumental in guiding the practice of implementing watermarks. Inparticular, we derive optimal detection rules for these watermarks under ourframework. These theoretically derived detection rules are demonstrated to becompetitive and sometimes enjoy a higher power than existing detectionapproaches through numerical experiments.</description><author>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</author><pubDate>Wed, 28 Aug 2024 15:01:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01245v2</guid></item><item><title>Guaranteed Coverage Prediction Intervals with Gaussian Process Regression</title><link>http://arxiv.org/abs/2310.15641v2</link><description>Gaussian Process Regression (GPR) is a popular regression method, whichunlike most Machine Learning techniques, provides estimates of uncertainty forits predictions. These uncertainty estimates however, are based on theassumption that the model is well-specified, an assumption that is violated inmost practical applications, since the required knowledge is rarely available.As a result, the produced uncertainty estimates can become very misleading; forexample the prediction intervals (PIs) produced for the 95% confidence levelmay cover much less than 95% of the true labels. To address this issue, thispaper introduces an extension of GPR based on a Machine Learning frameworkcalled, Conformal Prediction (CP). This extension guarantees the production ofPIs with the required coverage even when the model is completely misspecified.The proposed approach combines the advantages of GPR with the valid coverageguarantee of CP, while the performed experimental results demonstrate itssuperiority over existing methods.</description><author>Harris Papadopoulos</author><pubDate>Wed, 28 Aug 2024 15:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15641v2</guid></item><item><title>Downstream bias mitigation is all you need</title><link>http://arxiv.org/abs/2408.00612v2</link><description>The advent of transformer-based architectures and large language models(LLMs) have significantly advanced the performance of natural languageprocessing (NLP) models. Since these LLMs are trained on huge corpuses of datafrom the web and other sources, there has been a major concern about harmfulprejudices that may potentially be transferred from the data. In manyapplications, these pre-trained LLMs are fine-tuned on task specific datasets,which can further contribute to biases. This paper studies the extent of biasesabsorbed by LLMs during pre-training as well as task-specific behaviour afterfine-tuning. We found that controlled interventions on pre-trained LLMs, priorto fine-tuning, have minimal effect on lowering biases in classifiers. However,the biases present in domain-specific datasets play a much bigger role, andhence mitigating them at this stage has a bigger impact. While pre-trainingdoes matter, but after the model has been pre-trained, even slight changes toco-occurrence rates in the fine-tuning dataset has a significant effect on thebias of the model.</description><author>Arkadeep Baksi, Rahul Singh, Tarun Joshi</author><pubDate>Wed, 28 Aug 2024 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00612v2</guid></item><item><title>Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models</title><link>http://arxiv.org/abs/2402.16696v3</link><description>Tool-augmented large language models (LLMs) are attracting widespreadattention when accessing up-to-date knowledge and alleviating hallucinationissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstratedsurprising tool-usage capabilities through prompting and in-context learningtechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) inmanipulating tools, current efforts focus on either template-driven ortoken-triggered tool-usage. However, the former hampers LLMs' flexibility toaddress diverse user's queries due to constrained tool interactions, while thelatter limits the generalizability when engaging with new tools, sincetool-usage learning is based on task- and tool-specific datasets. To alleviatethese concerns, in this paper, we propose a decision-aware and generalizabletool-usage framework (DEER). Specifically, we first construct the tool-usagesamples with multiple decision branches via an automatic generation pipeline,thereby inspiring the decision-making awareness of LLMs under diversescenarios. Meanwhile, we propose a novel tool sampling strategy to enhance thegeneralizability of LLMs over unseen tools. Extensive experiments demonstratethat our proposed DEER is effective and significantly outperforms baselinesacross various datasets.</description><author>Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao</author><pubDate>Wed, 28 Aug 2024 14:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16696v3</guid></item><item><title>Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature</title><link>http://arxiv.org/abs/2408.15836v1</link><description>The exponential growth of scientific literature necessitates advanced toolsfor effective knowledge exploration. We present Knowledge Navigator, a systemdesigned to enhance exploratory search abilities by organizing and structuringthe retrieved documents from broad topical queries into a navigable, two-levelhierarchy of named and descriptive scientific topics and subtopics. Thisstructured organization provides an overall view of the research themes in adomain, while also enabling iterative search and deeper knowledge discoverywithin specific subtopics by allowing users to refine their focus and retrieveadditional relevant documents. Knowledge Navigator combines LLM capabilitieswith cluster-based methods to enable an effective browsing method. Wedemonstrate our approach's effectiveness through automatic and manualevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,prompts, and benchmarks are made publicly available.</description><author>Uri Katz, Mosh Levy, Yoav Goldberg</author><pubDate>Wed, 28 Aug 2024 14:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15836v1</guid></item><item><title>Network transferability of adversarial patches in real-time object detection</title><link>http://arxiv.org/abs/2408.15833v1</link><description>Adversarial patches in computer vision can be used, to fool deep neuralnetworks and manipulate their decision-making process. One of the mostprominent examples of adversarial patches are evasion attacks for objectdetectors. By covering parts of objects of interest, these patches suppress thedetections and thus make the target object 'invisible' to the object detector.Since these patches are usually optimized on a specific network with a specifictrain dataset, the transferability across multiple networks and datasets is notgiven. This paper addresses these issues and investigates the transferabilityacross numerous object detector architectures. Our extensive evaluation acrossvarious models on two distinct datasets indicates that patches optimized withlarger models provide better network transferability than patches that areoptimized with smaller models.</description><author>Jens Bayer, Stefan Becker, David Münch, Michael Arens</author><pubDate>Wed, 28 Aug 2024 14:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15833v1</guid></item><item><title>FRANC: A Lightweight Framework for High-Quality Code Generation</title><link>http://arxiv.org/abs/2307.08220v2</link><description>In recent years, the use of automated source code generation utilizingtransformer-based generative models has expanded, and these models can generatefunctional code according to the requirements of the developers. However,recent research revealed that these automatically generated source codes cancontain vulnerabilities and other quality issues. Despite researchers' andpractitioners' attempts to enhance code generation models, retraining andfine-tuning large language models is time-consuming and resource-intensive.Thus, we describe FRANC, a lightweight framework for recommending more secureand high-quality source code derived from transformer-based code generationmodels. FRANC includes a static filter to make the generated code compilablewith heuristics and a quality-aware ranker to sort the code snippets based on aquality score. Moreover, the framework uses prompt engineering to fixpersistent quality issues. We evaluated the framework with five Python and Javacode generation models and six prompt datasets, including a newly created onein this work (SOEval). The static filter improves 9% to 46% Java suggestionsand 10% to 43% Python suggestions regarding compilability. The averageimprovement over the NDCG@10 score for the ranking system is 0.0763, and therepairing techniques repair the highest 80% of prompts. FRANC takes, onaverage, 1.98 seconds for Java; for Python, it takes 0.08 seconds.</description><author>Mohammed Latif Siddiq, Beatrice Casey, Joanna C. S. Santos</author><pubDate>Wed, 28 Aug 2024 14:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08220v2</guid></item><item><title>eRST: A Signaled Graph Theory of Discourse Relations and Organization</title><link>http://arxiv.org/abs/2403.13560v2</link><description>In this article we present Enhanced Rhetorical Structure Theory (eRST), a newtheoretical framework for computational discourse analysis, based on anexpansion of Rhetorical Structure Theory (RST). The framework encompassesdiscourse relation graphs with tree-breaking, non-projective and concurrentrelations, as well as implicit and explicit signals which give explainablerationales to our analyses. We survey shortcomings of RST and other existingframeworks, such as Segmented Discourse Representation Theory (SDRT), the PennDiscourse Treebank (PDTB) and Discourse Dependencies, and address these usingconstructs in the proposed theory. We provide annotation, search andvisualization tools for data, and present and evaluate a freely availablecorpus of English annotated according to our framework, encompassing 12 spokenand written genres with over 200K tokens. Finally, we discuss automaticparsing, evaluation metrics and applications for data in our framework.</description><author>Amir Zeldes, Tatsuya Aoyama, Yang Janet Liu, Siyao Peng, Debopam Das, Luke Gessler</author><pubDate>Wed, 28 Aug 2024 14:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13560v2</guid></item><item><title>SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization</title><link>http://arxiv.org/abs/2408.15829v1</link><description>Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes anattractive summarization approach by integrating various types of informationto create extremely concise yet informative summaries for individualmodalities. Existing methods overlook the issue that multimodal data oftencontains more topic irrelevant information, which can mislead the model intoproducing inaccurate summaries especially for extremely short ones. In thispaper, we propose SITransformer, a \textbf{S}hared \textbf{I}nformation-guided\textbf{T}ransformer for extreme multimodal summarization. It has a sharedinformation guided pipeline which involves a cross-modal shared informationextractor and a cross-modal interaction module. The extractor formulatessemantically shared salient information from different modalities by devising anovel filtering process consisting of a differentiable top-k selector and ashared-information guided gating unit. As a result, the common, salient, andrelevant contents across modalities are identified. Next, a transformer withcross-modal attentions is developed for intra- and inter-modality learning withthe shared information guidance to produce the extreme summary. Comprehensiveexperiments demonstrate that SITransformer significantly enhances thesummarization quality for both video and text summaries for XMSMO. Our codewill be publicly available at https://github.com/SichengLeoLiu/MMAsia24-XMSMO.</description><author>Sicheng Liu, Lintao Wang, Xiaogan Zhu, Xuequan Lu, Zhiyong Wang, Kun Hu</author><pubDate>Wed, 28 Aug 2024 14:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15829v1</guid></item><item><title>Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law</title><link>http://arxiv.org/abs/2408.05231v2</link><description>A new unsupervised predictive maintenance analysis method based on therenormalization group approach used to discover critical behavior in complexsystems has been proposed. The algorithm analyzes univariate time series anddetects critical points based on a newly proposed theorem that identifiescritical points using a Log Periodic Power Law function fits. Application of anew algorithm for predictive maintenance analysis of industrial data collectedfrom reciprocating compressor systems is presented. Based on the knowledge ofthe dynamics of the analyzed compressor system, the proposed algorithm predictsvalve and piston rod seal failures well in advance.</description><author>Bogdan Łobodziński</author><pubDate>Wed, 28 Aug 2024 14:40:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05231v2</guid></item><item><title>Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification</title><link>http://arxiv.org/abs/2408.15827v1</link><description>As the field of artificial intelligence progresses, assistive technologiesare becoming more widely used across all industries. The healthcare industry isno different, with numerous studies being done to develop assistive tools forhealthcare professionals. Automatic diagnostic systems are one such beneficialtool that can assist with a variety of tasks, including collecting patientinformation, analyzing test results, and diagnosing patients. However, the ideaof developing systems that can provide a differential diagnosis has beenlargely overlooked in most of these research studies. In this study, we proposea transformer-based approach for providing differential diagnoses based on apatient's age, sex, medical history, and symptoms. We use the DDXPlus dataset,which provides differential diagnosis information for patients based on 49disease types. Firstly, we propose a method to process the tabular patient datafrom the dataset and engineer them into patient reports to make them suitablefor our research. In addition, we introduce two data modification modules todiversify the training data and consequently improve the robustness of themodels. We approach the task as a multi-label classification problem andconduct extensive experiments using four transformer models. All the modelsdisplayed promising results by achieving over 97% F1 score on the held-out testset. Moreover, we design additional behavioral tests to get a broaderunderstanding of the models. In particular, for one of our test cases, weprepared a custom test set of 100 samples with the assistance of a doctor. Theresults on the custom set showed that our proposed data modification modulesimproved the model's generalization capabilities. We hope our findings willprovide future researchers with valuable insights and inspire them to developreliable systems for automatic differential diagnosis.</description><author>Abu Adnan Sadi, Mohammad Ashrafuzzaman Khan, Lubaba Binte Saber</author><pubDate>Wed, 28 Aug 2024 14:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15827v1</guid></item><item><title>The Fault in our Stars: Quality Assessment of Code Generation Benchmarks</title><link>http://arxiv.org/abs/2404.10155v2</link><description>Large Language Models (LLMs) are gaining popularity among software engineers.A crucial aspect of developing effective code generation LLMs is to evaluatethese models using a robust benchmark. Evaluation benchmarks with qualityissues can provide a false sense of performance. In this work, we conduct thefirst-of-its-kind study of the quality of prompts within benchmarks used tocompare the performance of different code generation models. To conduct thisstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identifyquality issues in them. We also investigated whether fixing the identifiedquality issues in the benchmarks' prompts affects a model's performance. Wealso studied memorization issues of the evaluation dataset, which can put intoquestion a benchmark's trustworthiness. We found that code generationevaluation benchmarks mainly focused on Python and coding exercises and hadvery limited contextual dependencies to challenge the model. These datasets andthe developers' prompts suffer from quality issues like spelling andgrammatical errors, unclear sentences to express developers' intent, and notusing proper documentation style. Fixing all these issues in the benchmarks canlead to a better performance for Python code generation, but not a significantimprovement was observed for Java code generation. We also found evidence thatGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.</description><author>Mohammed Latif Siddiq, Simantika Dristi, Joy Saha, Joanna C. S. Santos</author><pubDate>Wed, 28 Aug 2024 14:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10155v2</guid></item><item><title>Benchmarking foundation models as feature extractors for weakly-supervised computational pathology</title><link>http://arxiv.org/abs/2408.15823v1</link><description>Advancements in artificial intelligence have driven the development ofnumerous pathology foundation models capable of extracting clinically relevantinformation. However, there is currently limited literature independentlyevaluating these foundation models on truly external cohorts andclinically-relevant tasks to uncover adjustments for future improvements. Inthis study, we benchmarked ten histopathology foundation models on 13 patientcohorts with 6,791 patients and 9,493 slides from lung, colorectal, gastric,and breast cancers. The models were evaluated on weakly-supervised tasksrelated to biomarkers, morphological properties, and prognostic outcomes. Weshow that a vision-language foundation model, CONCH, yielded the highestperformance in 42% of tasks when compared to vision-only foundation models. Theexperiments reveal that foundation models trained on distinct cohorts learncomplementary features to predict the same label, and can be fused tooutperform the current state of the art. Creating an ensemble of complementaryfoundation models outperformed CONCH in 66% of tasks. Moreover, our findingssuggest that data diversity outweighs data volume for foundation models. Ourwork highlights actionable adjustments to improve pathology foundation models.</description><author>Peter Neidlinger, Omar S. M. El Nahhas, Hannah Sophie Muti, Tim Lenz, Michael Hoffmeister, Hermann Brenner, Marko van Treeck, Rupert Langer, Bastian Dislich, Hans Michael Behrens, Christoph Röcken, Sebastian Foersch, Daniel Truhn, Antonio Marra, Oliver Lester Saldanha, Jakob Nikolas Kather</author><pubDate>Wed, 28 Aug 2024 14:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15823v1</guid></item><item><title>Automated Mixture Analysis via Structural Evaluation</title><link>http://arxiv.org/abs/2408.15819v1</link><description>The determination of chemical mixture components is vital to a multitude ofscientific fields. Oftentimes spectroscopic methods are employed to decipherthe composition of these mixtures. However, the sheer density of spectralfeatures present in spectroscopic databases can make unambiguous assignment toindividual species challenging. Yet, components of a mixture are commonlychemically related due to environmental processes or shared precursormolecules. Therefore, analysis of the chemical relevance of a molecule isimportant when determining which species are present in a mixture. In thispaper, we combine machine-learning molecular embedding methods with agraph-based ranking system to determine the likelihood of a molecule beingpresent in a mixture based on the other known species and/or chemical priors.By incorporating this metric in a rotational spectroscopy mixture analysisalgorithm, we demonstrate that the mixture components can be identified withextremely high accuracy (&gt;97%) in an efficient manner.</description><author>Zachary T. P. Fried, Brett A. McGuire</author><pubDate>Wed, 28 Aug 2024 14:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15819v1</guid></item><item><title>u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model</title><link>http://arxiv.org/abs/2311.05348v4</link><description>Recent advancements in multi-modal large language models (MLLMs) have led tosubstantial improvements in visual understanding, primarily driven bysophisticated modality alignment strategies. However, predominant approachesprioritize global or regional comprehension, with less focus on fine-grained,pixel-level tasks. To address this gap, we introduce u-LLaVA, an innovativeunifying multi-task framework that integrates pixel, regional, and globalfeatures to refine the perceptual faculties of MLLMs. We commence by leveragingan efficient modality alignment approach, harnessing both image and videodatasets to bolster the model's foundational understanding across diversevisual contexts. Subsequently, a joint instruction tuning method withtask-specific projectors and decoders for end-to-end downstream training ispresented. Furthermore, this work contributes a novel mask-based multi-taskdataset comprising 277K samples, crafted to challenge and assess thefine-grained perception capabilities of MLLMs. The overall framework is simple,effective, and achieves state-of-the-art performance across multiplebenchmarks. We also make our model, data, and code publicly accessible athttps://github.com/OPPOMKLab/u-LLaVA.</description><author>Jinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Fanyi Wang, Yanchun Xie, Yi-Jie Huang, Yaqian Li</author><pubDate>Wed, 28 Aug 2024 14:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05348v4</guid></item><item><title>Mining Field Data for Tree Species Recognition at Scale</title><link>http://arxiv.org/abs/2408.15816v1</link><description>Individual tree species labels are particularly hard to acquire due to theexpert knowledge needed and the limitations of photointerpretation. Here, wepresent a methodology to automatically mine species labels from public forestinventory data, using available pretrained tree detection models. We identifytree instances in aerial imagery and match them with field data with close tozero human involvement. We conduct a series of experiments on the resultingdataset, and show a beneficial effect when adding noisy or even unlabeled datapoints, highlighting a strong potential for large-scale individual speciesmapping.</description><author>Dimitri Gominski, Daniel Ortiz-Gonzalo, Martin Brandt, Maurice Mugabowindekwe, Rasmus Fensholt</author><pubDate>Wed, 28 Aug 2024 14:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15816v1</guid></item><item><title>DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled Queries</title><link>http://arxiv.org/abs/2408.15813v1</link><description>LiDAR panoptic segmentation, which jointly performs instance and semanticsegmentation for things and stuff classes, plays a fundamental role in LiDARperception tasks. While most existing methods explicitly separate these twosegmentation tasks and utilize different branches (i.e., semantic and instancebranches), some recent methods have embraced the query-based paradigm to unifyLiDAR panoptic segmentation. However, the distinct spatial distribution andinherent characteristics of objects(things) and their surroundings(stuff) in 3Dscenes lead to challenges, including the mutual competition of things/stuff andthe ambiguity of classification/segmentation. In this paper, we proposedecoupling things/stuff queries according to their intrinsic properties forindividual decoding and disentangling classification/segmentation to mitigateambiguity. To this end, we propose a novel framework dubbed DQFormer toimplement semantic and instance segmentation in a unified workflow.Specifically, we design a decoupled query generator to propose informativequeries with semantics by localizing things/stuff positions and fusingmulti-level BEV embeddings. Moreover, a query-oriented mask decoder isintroduced to decode corresponding segmentation masks by performing maskedcross-attention between queries and mask embeddings. Finally, the decoded masksare combined with the semantics of the queries to produce panoptic results.Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate thesuperiority of our DQFormer framework.</description><author>Yu Yang, Jianbiao Mei, Liang Liu, Siliang Du, Yilin Xiao, Jongwon Ra, Yong Liu, Xiao Xu, Huifeng Wu</author><pubDate>Wed, 28 Aug 2024 14:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15813v1</guid></item><item><title>Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods</title><link>http://arxiv.org/abs/2408.14511v2</link><description>Chain-of-Thought (CoT) prompting and its variants have gained popularity aseffective methods for solving multi-step reasoning problems using pretrainedlarge language models (LLMs). In this work, we analyze CoT prompting from astatistical estimation perspective, providing a comprehensive characterizationof its sample complexity. To this end, we introduce a multi-step latentvariable model that encapsulates the reasoning process, where the latentvariable encodes the task information. Under this framework, we demonstratethat when the pretraining dataset is sufficiently large, the estimator formedby CoT prompting is equivalent to a Bayesian estimator. This estimatoreffectively solves the multi-step reasoning problem by aggregating a posteriordistribution inferred from the demonstration examples in the prompt. Moreover,we prove that the statistical error of the CoT estimator can be decomposed intotwo main components: (i) a prompting error, which arises from inferring thetrue task using CoT prompts, and (ii) the statistical error of the pretrainedLLM. We establish that, under appropriate assumptions, the prompting errordecays exponentially to zero as the number of demonstrations increases.Additionally, we explicitly characterize the approximation and generalizationerrors of the pretrained LLM. Notably, we construct a transformer model thatapproximates the target distribution of the multi-step reasoning problem withan error that decreases exponentially in the number of transformer blocks. Ouranalysis extends to other variants of CoT, including Self-Consistent CoT,Tree-of-Thought, and Selection-Inference, offering a broad perspective on theefficacy of these methods. We also provide numerical experiments to validatethe theoretical findings.</description><author>Xinyang Hu, Fengzhuo Zhang, Siyu Chen, Zhuoran Yang</author><pubDate>Wed, 28 Aug 2024 14:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14511v2</guid></item><item><title>A Framework to Model ML Engineering Processes</title><link>http://arxiv.org/abs/2404.18531v2</link><description>The development of Machine Learning (ML) based systems is complex andrequires multidisciplinary teams with diverse skill sets. This may lead tocommunication issues or misapplication of best practices. Process models canalleviate these challenges by standardizing task orchestration, providing acommon language to facilitate communication, and nurturing a collaborativeenvironment. Unfortunately, current process modeling languages are not suitablefor describing the development of such systems. In this paper, we introduce aframework for modeling ML-based software development processes, built around adomain-specific language and derived from an analysis of scientific and grayliterature. A supporting toolkit is also available.</description><author>Sergio Morales, Robert Clarisó, Jordi Cabot</author><pubDate>Wed, 28 Aug 2024 14:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18531v2</guid></item><item><title>Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2408.15810v1</link><description>Robust 3D human pose estimation is crucial to ensure safe and effectivehuman-robot collaboration. Accurate human perception,however, is particularlychallenging in these scenarios due to strong occlusions and limited cameraviewpoints. Current 3D human pose estimation approaches are rather vulnerablein such conditions. In this work we present a novel approach for robust 3Dhuman pose estimation in the context of human-robot collaboration. Instead ofrelying on noisy 2D features triangulation, we perform multi-view fusion on 3Dskeletons provided by absolute monocular methods. Accurate 3D pose estimationis then obtained via reprojection error optimization, introducing limbs lengthsymmetry constraints. We evaluate our approach on the public dataset Human3.6Mand on a novel version Human3.6M-Occluded, derived adding synthetic occlusionson the camera views with the purpose of testing pose estimation algorithmsunder severe occlusions. We further validate our method on real human-robotcollaboration workcells, in which we strongly surpass current 3D human poseestimation methods. Our approach outperforms state-of-the-art multi-view humanpose estimation techniques and demonstrates superior capabilities in handlingchallenging scenarios with strong occlusions, representing a reliable andeffective solution for real human-robot collaboration setups.</description><author>Laura Bragagnolo, Matteo Terreran, Davide Allegro, Stefano Ghidoni</author><pubDate>Wed, 28 Aug 2024 14:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15810v1</guid></item><item><title>Object Detection for Vehicle Dashcams using Transformers</title><link>http://arxiv.org/abs/2408.15809v1</link><description>The use of intelligent automation is growing significantly in the automotiveindustry, as it assists drivers and fleet management companies, thus increasingtheir productivity. Dash cams are now been used for this purpose which enablesthe instant identification and understanding of multiple objects andoccurrences in the surroundings. In this paper, we propose a novel approach forobject detection in dashcams using transformers. Our system is based on thestate-of-the-art DEtection TRansformer (DETR), which has demonstrated strongperformance in a variety of conditions, including different weather andillumination scenarios. The use of transformers allows for the consideration ofcontextual information in decisionmaking, improving the accuracy of objectdetection. To validate our approach, we have trained our DETR model on adataset that represents real-world conditions. Our results show that the use ofintelligent automation through transformers can significantly enhance thecapabilities of dashcam systems. The model achieves an mAP of 0.95 ondetection.</description><author>Osama Mustafa, Khizer Ali, Anam Bibi, Imran Siddiqi, Momina Moetesum</author><pubDate>Wed, 28 Aug 2024 14:08:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15809v1</guid></item><item><title>Stick to your Role! Stability of Personal Values Expressed in Large Language Models</title><link>http://arxiv.org/abs/2402.14846v4</link><description>The standard way to study Large Language Models (LLMs) with benchmarks orpsychology questionnaires is to provide many different queries from similarminimal contexts (e.g. multiple choice questions). However, due to LLMs' highlycontext-dependent nature, conclusions from such minimal-context evaluations maybe little informative about the model's behavior in deployment (where it willbe exposed to many new contexts). We argue that context-dependence(specifically, value stability) should be studied as a specific property ofLLMs and used as another dimension of LLM comparison (alongside others such ascognitive abilities, knowledge, or model size). We present a case-study on thestability of value expression over different contexts (simulated conversationson different topics) as measured using a standard psychology questionnaire(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, westudy Rank-order stability on the population (interpersonal) level, andIpsative stability on the individual (intrapersonal) level. We consider twosettings (with and without instructing LLMs to simulate particular personas),two simulated populations, and three downstream tasks. We observe consistenttrends in the stability of models and model families - Mixtral, Mistral,GPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistencyof these trends implies that some models exhibit higher value stability thanothers, and that stability can be estimated with the set of introducedmethodological tools. When instructed to simulate particular personas, LLMsexhibit low Rank-order stability, which further diminishes with conversationlength. This highlights the need for future research on LLMs that coherentlysimulate different personas. This paper provides a foundational step in thatdirection, and, to our knowledge, it is the first study of value stability inLLMs.</description><author>Grgur Kovač, Rémy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer</author><pubDate>Wed, 28 Aug 2024 14:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14846v4</guid></item><item><title>ModalityMirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation</title><link>http://arxiv.org/abs/2408.15803v1</link><description>Multimodal Federated Learning frequently encounters challenges of clientmodality heterogeneity, leading to undesired performances for secondarymodality in multimodal learning. It is particularly prevalent in audiovisuallearning, with audio is often assumed to be the weaker modality in recognitiontasks. To address this challenge, we introduce ModalityMirror to improve audiomodel performance by leveraging knowledge distillation from an audiovisualfederated learning model. ModalityMirror involves two phases: a modality-wiseFL stage to aggregate uni-modal encoders; and a federated knowledgedistillation stage on multi-modality clients to train an unimodal studentmodel. Our results demonstrate that ModalityMirror significantly improves theaudio classification compared to the state-of-the-art FL methods such asHarmony, particularly in audiovisual FL facing video missing. Our approachunlocks the potential for exploiting the diverse modality spectrum inherent inmulti-modal FL.</description><author>Tiantian Feng, Tuo Zhang, Salman Avestimehr, Shrikanth S. Narayanan</author><pubDate>Wed, 28 Aug 2024 13:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15803v1</guid></item><item><title>Visual Prompt Engineering for Medical Vision Language Models in Radiology</title><link>http://arxiv.org/abs/2408.15802v1</link><description>Medical image classification in radiology faces significant challenges,particularly in generalizing to unseen pathologies. In contrast, CLIP offers apromising solution by leveraging multimodal learning to improve zero-shotclassification performance. However, in the medical domain, lesions can besmall and might not be well represented in the embedding space. Therefore, inthis paper, we explore the potential of visual prompt engineering to enhancethe capabilities of Vision Language Models (VLMs) in radiology. LeveragingBiomedCLIP, trained on extensive biomedical image-text pairs, we investigatethe impact of embedding visual markers directly within radiological images toguide the model's attention to critical regions. Our evaluation on the JSRTdataset, focusing on lung nodule malignancy classification, demonstrates thatincorporating visual prompts $\unicode{x2013}$ such as arrows, circles, andcontours $\unicode{x2013}$ significantly improves classification metricsincluding AUROC, AUPRC, F1 score, and accuracy. Moreover, the study providesattention maps, showcasing enhanced model interpretability and focus onclinically relevant areas. These findings underscore the efficacy of visualprompt engineering as a straightforward yet powerful approach to advance VLMperformance in medical image analysis.</description><author>Stefan Denner, Markus Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Paul F. Jäger, Klaus Maier-Hein</author><pubDate>Wed, 28 Aug 2024 13:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15802v1</guid></item><item><title>Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization</title><link>http://arxiv.org/abs/2408.15801v1</link><description>In an era where digital text is proliferating at an unprecedented rate,efficient summarization tools are becoming indispensable. While Large LanguageModels (LLMs) have been successfully applied in various NLP tasks, their rolein extractive text summarization remains underexplored. This paper introducesEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractiveSummarization), a framework that leverages LLMs, specifically LLAMA2-7B andChatGLM2-6B, for extractive summarization of lengthy text documents. Instead ofabstractive methods, which often suffer from issues like factual inaccuraciesand hallucinations, EYEGLAXS focuses on extractive summarization to ensurefactual and grammatical integrity. Utilizing state-of-the-art techniques suchas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXSaddresses the computational and resource challenges typically associated withLLMs. The system sets new performance benchmarks on well-known datasets likePubMed and ArXiv. Furthermore, we extend our research through additionalanalyses that explore the adaptability of LLMs in handling different sequencelengths and their efficiency in training on smaller datasets. Thesecontributions not only set a new standard in the field but also open uppromising avenues for future research in extractive text summarization.</description><author>Léo Hemamou, Mehdi Debiane</author><pubDate>Wed, 28 Aug 2024 13:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15801v1</guid></item><item><title>Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing</title><link>http://arxiv.org/abs/2408.15800v1</link><description>Achieving personalized intelligence at the edge with real-time learningcapabilities holds enormous promise in enhancing our daily experiences andhelping decision making, planning, and sensing. However, efficient and reliableedge learning remains difficult with current technology due to the lack ofpersonalized data, insufficient hardware capabilities, and inherent challengesposed by online learning. Over time and across multiple developmental stages, the brain has evolved toefficiently incorporate new knowledge by gradually building on previousknowledge. In this work, we emulate the multiple stages of learning withdigital neuromorphic technology that simulates the neural and synapticprocesses of the brain using two stages of learning. First, a meta-trainingstage trains the hyperparameters of synaptic plasticity for one-shot learningusing a differentiable simulation of the neuromorphic hardware. Thismeta-training process refines a hardware local three-factor synaptic plasticityrule and its associated hyperparameters to align with the trained task domain.In a subsequent deployment stage, these optimized hyperparameters enable fast,data-efficient, and accurate learning of new classes. We demonstrate ourapproach using event-driven vision sensor data and the Intel Loihi neuromorphicprocessor with its plasticity dynamics, achieving real-time one-shot learningof new classes that is vastly improved over transfer learning. Our methodologycan be deployed with arbitrary plasticity models and can be applied tosituations demanding quick learning and adaptation at the edge, such asnavigating unfamiliar environments or learning unexpected categories of datathrough user engagement.</description><author>Kenneth Stewart, Michael Neumeier, Sumit Bam Shrestha, Garrick Orchard, Emre Neftci</author><pubDate>Wed, 28 Aug 2024 13:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15800v1</guid></item><item><title>A Metric-based Principal Curve Approach for Learning One-dimensional Manifold</title><link>http://arxiv.org/abs/2405.12390v2</link><description>Principal curve is a well-known statistical method oriented in manifoldlearning using concepts from differential geometry. In this paper, we propose anovel metric-based principal curve (MPC) method that learns one-dimensionalmanifold of spatial data. Synthetic datasets Real applications using MNISTdataset show that our method can learn the one-dimensional manifold well interms of the shape.</description><author>Elvis Han Cui</author><pubDate>Wed, 28 Aug 2024 13:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12390v2</guid></item><item><title>Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models</title><link>http://arxiv.org/abs/2408.15796v1</link><description>This paper evaluates Few-Shot Prompting with Large Language Models for NamedEntity Recognition (NER). Traditional NER systems rely on extensive labeleddatasets, which are costly and time-consuming to obtain. Few-Shot Prompting orin-context learning enables models to recognize entities with minimal examples.We assess state-of-the-art models like GPT-4 in NER tasks, comparing theirfew-shot performance to fully supervised benchmarks. Results show that whilethere is a performance gap, large models excel in adapting to new entity typesand domains with very limited data. We also explore the effects of promptengineering, guided output format and context length on performance. This studyunderscores Few-Shot Learning's potential to reduce the need for large labeleddatasets, enhancing NER scalability and accessibility.</description><author>Hédi Zhegidi, Ludovic Moncla</author><pubDate>Wed, 28 Aug 2024 13:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15796v1</guid></item><item><title>Automated Real-World Sustainability Data Generation from Images of Buildings</title><link>http://arxiv.org/abs/2405.18064v2</link><description>When data on building features is unavailable, the task of determining how toimprove that building in terms of carbon emissions becomes infeasible. We showthat from only a set of images, a Large Language Model with appropriate promptengineering and domain knowledge can successfully estimate a range of buildingfeatures relevant for sustainability calculations. We compare our novelimage-to-data method with a ground truth comprising real building data for 47apartments and achieve accuracy better than a human performing the same task.We also demonstrate that the method can generate tailored recommendations tothe owner on how best to improve their properties and discuss methods to scalethe approach.</description><author>Peter J Bentley, Soo Ling Lim, Rajat Mathur, Sid Narang</author><pubDate>Wed, 28 Aug 2024 13:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18064v2</guid></item></channel></rss>