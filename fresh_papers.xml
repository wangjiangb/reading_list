<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 25 Oct 2024 01:02:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices</title><link>http://arxiv.org/abs/2410.17998v2</link><description>Analyzing the structure of sampled features from an input data distributionis challenging when constrained by limited measurements in both the number ofinputs and features. Traditional approaches often rely on the eigenvaluespectrum of the sample covariance matrix derived from finite measurementmatrices; however, these spectra are sensitive to the size of the measurementmatrix, leading to biased insights. In this paper, we introduce a novelalgorithm that provides unbiased estimates of the spectral moments of thekernel integral operator in the limit of infinite inputs and features fromfinitely sampled measurement matrices. Our method, based on dynamicprogramming, is efficient and capable of estimating the moments of the operatorspectrum. We demonstrate the accuracy of our estimator on radial basis function(RBF) kernels, highlighting its consistency with the theoretical spectra.Furthermore, we showcase the practical utility and robustness of our method inunderstanding the geometry of learned representations in neural networks.</description><author>Chanwoo Chun, SueYeon Chung, Daniel D. Lee</author><pubDate>Thu, 24 Oct 2024 17:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17998v2</guid></item><item><title>Interação entre robôs humanoides: desenvolvendo a colaboração e comunicação autônoma</title><link>http://arxiv.org/abs/2410.17450v2</link><description>This study investigates the interaction between humanoid robots NAO andPepper, emphasizing their potential applications in educational settings. NAO,widely used in education, and Pepper, designed for social interactions, of ernew opportunities for autonomous communication and collaboration. Through aseries of programmed interactions, the robots demonstrated their ability tocommunicate and coordinate actions autonomously, highlighting their potentialas tools for enhancing learning environments. The research also explores theintegration of emerging technologies, such as artificial intelligence, intothese systems, allowing robots to learn from each other and adapt theirbehavior. The findings suggest that NAO and Pepper can significantly contributeto both technical learning and the development of social and emotional skillsin students, of ering innovative pedagogical approaches through the use ofhumanoid robotics.</description><author>Moraes Pablo, Rodríguez Mónica, Peters Christopher, Sodre Hiago, Mazondo Ahilen, Sandin Vincent, Barcelona Sebastian, Moraes William, Fernández Santiago, Assunção Nathalie, de Vargas Bruna, Dörnbach Tobias, Kelbouscas André, Grando Ricardo</author><pubDate>Thu, 24 Oct 2024 17:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17450v2</guid></item><item><title>Hierarchical Multi-agent Reinforcement Learning for Cyber Network Defense</title><link>http://arxiv.org/abs/2410.17351v2</link><description>Recent advances in multi-agent reinforcement learning (MARL) have createdopportunities to solve complex real-world tasks. Cybersecurity is a notableapplication area, where defending networks against sophisticated adversariesremains a challenging task typically performed by teams of security operators.In this work, we explore novel MARL strategies for building autonomous cybernetwork defenses that address challenges such as large policy spaces, partialobservability, and stealthy, deceptive adversarial strategies. To facilitateefficient and generalized learning, we propose a hierarchical Proximal PolicyOptimization (PPO) architecture that decomposes the cyber defense task intospecific sub-tasks like network investigation and host recovery. Our approachinvolves training sub-policies for each sub-task using PPO enhanced with domainexpertise. These sub-policies are then leveraged by a master defense policythat coordinates their selection to solve complex network defense tasks.Furthermore, the sub-policies can be fine-tuned and transferred with minimalcost to defend against shifts in adversarial behavior or changes in networksettings. We conduct extensive experiments using CybORG Cage 4, thestate-of-the-art MARL environment for cyber defense. Comparisons with multiplebaselines across different adversaries show that our hierarchical learningapproach achieves top performance in terms of convergence speed, episodicreturn, and several interpretable metrics relevant to cybersecurity, includingthe fraction of clean machines on the network, precision, and false positiveson recoveries.</description><author>Aditya Vikram Singh, Ethan Rathbun, Emma Graham, Lisa Oakley, Simona Boboila, Alina Oprea, Peter Chin</author><pubDate>Thu, 24 Oct 2024 15:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17351v2</guid></item><item><title>Learning Mathematical Rules with Large Language Models</title><link>http://arxiv.org/abs/2410.16973v2</link><description>In this paper, we study the ability of large language models to learnspecific mathematical rules such as distributivity or simplifying equations. Wepresent an empirical analysis of their ability to generalize these rules, aswell as to reuse them in the context of word problems. For this purpose, weprovide a rigorous methodology to build synthetic data incorporating suchrules, and perform fine-tuning of large language models on such data. Ourexperiments show that our model can learn and generalize these rules to someextent, as well as suitably reuse them in the context of word problems.</description><author>Nelson Vadori, Antoine Gorceix, Bastien Le Chenadec, Ahmad Rammal, Manuela Veloso</author><pubDate>Thu, 24 Oct 2024 15:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16973v2</guid></item><item><title>On high-dimensional modifications of the nearest neighbor classifier</title><link>http://arxiv.org/abs/2407.05145v3</link><description>Nearest neighbor classifier is arguably the most simple and popularnonparametric classifier available in the literature. However, due to theconcentration of pairwise distances and the violation of the neighborhoodstructure, this classifier often suffers in high-dimension, low-sample size(HDLSS) situations, especially when the scale difference between the competingclasses dominates their location difference. Several attempts have been made inthe literature to take care of this problem. In this article, we discuss someof these existing methods and propose some new ones. We carry out sometheoretical investigations in this regard and analyze several simulated andbenchmark datasets to compare the empirical performances of proposed methodswith some of the existing ones.</description><author>Annesha Ghosh, Deep Ghoshal, Bilol Banerjee, Anil K. Ghosh</author><pubDate>Thu, 24 Oct 2024 15:47:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05145v3</guid></item><item><title>HyperspectralViTs: General Hyperspectral Models for On-board Remote Sensing</title><link>http://arxiv.org/abs/2410.17248v2</link><description>On-board processing of hyperspectral data with machine learning models wouldenable unprecedented amount of autonomy for a wide range of tasks, for examplemethane detection or mineral identification. This can enable early warningsystem and could allow new capabilities such as automated scheduling acrossconstellations of satellites. Classical methods suffer from high false positiverates and previous deep learning models exhibit prohibitive computationalrequirements. We propose fast and accurate machine learning architectures whichsupport end-to-end training with data of high spectral dimension withoutrelying on hand-crafted products or spectral band compression preprocessing. Weevaluate our models on two tasks related to hyperspectral data processing. Withour proposed general architectures, we improve the F1 score of the previousmethane detection state-of-the-art models by 27% on a newly created syntheticdataset and by 13% on the previously released large benchmark dataset. We alsodemonstrate that training models on the synthetic dataset improves performanceof models finetuned on the dataset of real events by 6.9% in F1 score incontrast with training from scratch. On a newly created dataset for mineralidentification, our models provide 3.5% improvement in the F1 score in contrastto the default versions of the models. With our proposed models we improve theinference speed by 85% in contrast to previous classical and deep learningapproaches by removing the dependency on classically computed features. Withour architecture, one capture from the EMIT sensor can be processed within 30seconds on realistic proxy of the ION-SCV 004 satellite.</description><author>Vít Růžička, Andrew Markham</author><pubDate>Thu, 24 Oct 2024 15:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17248v2</guid></item><item><title>PnLCalib: Sports Field Registration via Points and Lines Optimization</title><link>http://arxiv.org/abs/2404.08401v4</link><description>Camera calibration in broadcast sports videos presents numerous challengesfor accurate sports field registration due to multiple camera angles, varyingcamera parameters, and frequent occlusions of the field. Traditionalsearch-based methods depend on initial camera pose estimates, which canstruggle in non-standard positions and dynamic environments. In response, wepropose an optimization-based calibration pipeline that leverages a 3D soccerfield model and a predefined set of keypoints to overcome these limitations.Our method also introduces a novel refinement module that improves initialcalibration by using detected field lines in a non-linear optimization process.This approach outperforms existing techniques in both multi-view andsingle-view 3D camera calibration tasks, while maintaining competitiveperformance in homography estimation. Extensive experimentation on real-worldsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, andTS-WorldCup, highlights the robustness and accuracy of our method acrossdiverse broadcast scenarios. Our approach offers significant improvements incamera calibration precision and reliability.</description><author>Marc Gutiérrez-Pérez, Antonio Agudo</author><pubDate>Thu, 24 Oct 2024 14:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08401v4</guid></item><item><title>Evaluating AI-Generated Essays with GRE Analytical Writing Assessment</title><link>http://arxiv.org/abs/2410.17439v2</link><description>The recent revolutionary advance in generative AI enables the generation ofrealistic and coherent texts by large language models (LLMs). Despite manyexisting evaluation metrics on the quality of the generated texts, there isstill a lack of rigorous assessment of how well LLMs perform in complex anddemanding writing assessments. This study examines essays generated by tenleading LLMs for the analytical writing assessment of the Graduate Record Exam(GRE). We assessed these essays using both human raters and the e-raterautomated scoring engine as used in the GRE scoring pipeline. Notably, thetop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,respectively, falling between "generally thoughtful, well-developed analysis ofthe issue and conveys meaning clearly" and "presents a competent analysis ofthe issue and conveys meaning with acceptable clarity" according to the GREscoring guideline. We also evaluated the detection accuracy of these essays,with detectors trained on essays generated by the same and different LLMs.</description><author>Yang Zhong, Jiangang Hao, Michael Fauss, Chen Li, Yuan Wang</author><pubDate>Thu, 24 Oct 2024 13:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17439v2</guid></item><item><title>System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam</title><link>http://arxiv.org/abs/2410.07114v4</link><description>The processes underlying human cognition are often divided into System 1,which involves fast, intuitive thinking, and System 2, which involves slow,deliberate reasoning. Previously, large language models were criticized forlacking the deeper, more analytical capabilities of System 2. In September2024, OpenAI introduced the o1 model series, designed to handle System 2-likereasoning. While OpenAI's benchmarks are promising, independent validation isstill needed. In this study, we tested the o1-preview model twice on the Dutch'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76points. For context, only 24 out of 16,414 students in the Netherlands achieveda perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,well above the Dutch students' average of 40.63 points. Neither model hadaccess to the exam figures. Since there was a risk of model contami-nation(i.e., the knowledge cutoff for o1-preview and GPT-4o was after the exam waspublished online), we repeated the procedure with a new Mathematics B exam thatwas published after the cutoff date. The results again indicated thato1-preview performed strongly (97.8th percentile), which suggests thatcontamination was not a factor. We also show that there is some variability inthe output of o1-preview, which means that sometimes there is 'luck' (theanswer is correct) or 'bad luck' (the output has diverged into something thatis incorrect). We demonstrate that the self-consistency approach, whererepeated prompts are given and the most common answer is selected, is a usefulstrategy for identifying the correct answer. It is concluded that whileOpenAI's new model series holds great potential, certain risks must beconsidered.</description><author>Joost de Winter, Dimitra Dodou, Yke Bauke Eisma</author><pubDate>Thu, 24 Oct 2024 12:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07114v4</guid></item><item><title>Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination</title><link>http://arxiv.org/abs/2410.17820v2</link><description>Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models(LLMs) that employs a generator to suggest reasoning steps and a discriminatorto decide which steps to implement. ToT demonstrates strong performance onreasoning tasks, often surpassing simple methods such as Input-Output (IO)prompting and Chain-of-Thought (CoT) reasoning. However, ToT does notconsistently outperform such simpler methods across all models, leaving largeknowledge gaps on the conditions under which ToT is most beneficial. In thispaper, we analyze the roles of the generator and discriminator separately tobetter understand the conditions when ToT is beneficial. We find that thegenerator plays a more critical role than the discriminator in driving thesuccess of ToT. Scaling the generator leads to notable improvements in ToTperformance, even when using a smaller model as the discriminator, whereasscaling the discriminator with a fixed generator yields only marginal gains.Our results show that models across different scales exhibit comparablediscrimination capabilities, yet differ significantly in their generativeperformance for ToT.</description><author>Qiqi Chen, Xinpeng Wang, Philipp Mondorf, Michael A. Hedderich, Barbara Plank</author><pubDate>Thu, 24 Oct 2024 12:01:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17820v2</guid></item><item><title>ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in Robotic Surgical Skill Assessment</title><link>http://arxiv.org/abs/2407.05180v3</link><description>In surgical skill assessment, the Objective Structured Assessments ofTechnical Skills (OSATS) and Global Rating Scale (GRS) are well-establishedtools for evaluating surgeons during training. These metrics, along withperformance feedback, help surgeons improve and reach practice standards.Recent research on the open-source JIGSAWS dataset, which includes both GRS andOSATS labels, has focused on regressing GRS scores from kinematic data, video,or their combination. However, we argue that regressing GRS alone is limiting,as it aggregates OSATS scores and overlooks clinically meaningful variationsduring a surgical trial. To address this, we developed a recurrent transformermodel that tracks a surgeon's performance throughout a session by mappinghidden states to six OSATS, derived from kinematic data, using a clinicallymotivated objective function. These OSATS scores are averaged to predict GRS,allowing us to compare our model's performance against state-of-the-art (SOTA)methods. We report Spearman's Correlation Coefficients (SCC) demonstrating thatour model outperforms SOTA using kinematic data (SCC 0.83-0.88), and matchesperformance with video-based models. Our model also surpasses SOTA in mosttasks for average OSATS predictions (SCC 0.46-0.70) and specific OSATS (SCC0.56-0.95). The generation of pseudo-labels at the segment level translatesquantitative predictions into qualitative feedback, vital for automatedsurgical skill assessment pipelines. A senior surgeon validated our model'soutputs, agreeing with 77% of the weakly-supervised predictions (p=0.006).</description><author>Julien Quarez, Marc Modat, Sebastien Ourselin, Jonathan Shapey, Alejandro Granados</author><pubDate>Thu, 24 Oct 2024 11:18:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05180v3</guid></item><item><title>Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs</title><link>http://arxiv.org/abs/2410.15859v3</link><description>Large language models (LLMs), although having revolutionized many fields,still suffer from the challenging extrapolation problem, where the inferenceability of LLMs sharply declines beyond their max training lengths. In thiswork, we conduct a theoretical analysis to better understand why No PositionEncoding (NoPE) fails outside its effective range, as well as examining thepower of Position Encoding (PE) in this context. Our findings reveal that withmeticulous weave position, PE can indeed be extended beyond effective range.Our theorems establish that LLMs equipped with weave PE can achieve improvedextrapolation performance without additional cost. Furthermore, we introduce anovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-basedtriangular attention matrix and applies Stair PE to manage the final chunk.This method not only retains competitive performance but also offerssubstantial benefits such as significantly reduced memory demand and fasterinference speed. Extensive experiments validate the effectiveness ofMesa-Extrapolation, demonstrating its potential as a scalable solution toenhancing LLMs applicative reach. Our code is available at\url{https://github.com/soacker/Mesa-Extrapolation}.</description><author>Xin Ma, Yang Liu, Jingjing Liu, Xiaoxu Ma</author><pubDate>Thu, 24 Oct 2024 10:29:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15859v3</guid></item><item><title>A Historical Trajectory Assisted Optimization Method for Zeroth-Order Federated Learning</title><link>http://arxiv.org/abs/2409.15955v5</link><description>Federated learning heavily relies on distributed gradient descent techniques.In the situation where gradient information is not available, the gradientsneed to be estimated from zeroth-order information, which typically involvescomputing finite-differences along isotropic random directions. This methodsuffers from high estimation errors, as the geometric features of the objectivelandscape may be overlooked during the isotropic sampling. In this work, wepropose a non-isotropic sampling method to improve the gradient estimationprocedure. Gradients in our method are estimated in a subspace spanned byhistorical trajectories of solutions, aiming to encourage the exploration ofpromising regions and hence improve the convergence. The proposed method uses acovariance matrix for sampling which is a convex combination of two parts. Thefirst part is a thin projection matrix containing the basis of the subspacewhich is designed to improve the exploitation ability. The second part is thehistorical trajectories. We implement this method in zeroth-order federatedsettings, and show that the convergence rate aligns with existing ones whileintroducing no significant overheads in communication or local computation. Theeffectiveness of our proposal is verified on several numerical experiments incomparison to several commonly-used zeroth-order federated optimizationalgorithms.</description><author>Chenlin Wu, Xiaoyu He, Zike Li, Jing Gong, Zibin Zheng</author><pubDate>Thu, 24 Oct 2024 09:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15955v5</guid></item><item><title>ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning</title><link>http://arxiv.org/abs/2305.01486v5</link><description>In this paper, we introduce a framework ARBEx, a novel attentive featureextraction framework driven by Vision Transformer with reliability balancing tocope against poor class distributions, bias, and uncertainty in the facialexpression learning (FEL) task. We reinforce several data pre-processing andrefinement methods along with a window-based cross-attention ViT to squeeze thebest of the data. We also employ learnable anchor points in the embedding spacewith label distributions and multi-head self-attention mechanism to optimizeperformance against weak predictions with reliability balancing, which is astrategy that leverages anchor points, attention scores, and confidence valuesto enhance the resilience of label predictions. To ensure correct labelclassification and improve the models' discriminative power, we introduceanchor loss, which encourages large margins between anchor points.Additionally, the multi-head self-attention mechanism, which is also trainable,plays an integral role in identifying accurate labels. This approach providescritical elements for improving the reliability of predictions and has asubstantial positive effect on final prediction capabilities. Our adaptivemodel can be integrated with any deep neural network to forestall challenges invarious recognition tasks. Our strategy outperforms current state-of-the-artmethodologies, according to extensive experiments conducted in a variety ofcontexts.</description><author>Azmine Toushik Wasi, Karlo Šerbetar, Raima Islam, Taki Hasan Rafi, Dong-Kyu Chae</author><pubDate>Thu, 24 Oct 2024 09:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01486v5</guid></item><item><title>A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning</title><link>http://arxiv.org/abs/2406.05804v5</link><description>Tool use, planning, and feedback learning are currently three prominentparadigms for developing Large Language Model (LLM)-based agents across varioustasks. Although numerous frameworks have been devised for each paradigm, theirintricate workflows and inconsistent taxonomy create challenges inunderstanding and reviewing the frameworks across different paradigms. Thissurvey introduces a unified taxonomy to systematically review and discuss theseframeworks. Specifically, 1) the taxonomy defines environments/tasks, commonLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),and universally applicable workflows found in prior work, and 2) it enables acomparison of key perspectives on the implementations of LMPRs and workflowdesigns across different agent paradigms and frameworks. 3) Finally, weidentify three limitations in existing workflow designs and systematicallydiscuss the future work. Resources have been made publicly available at in ourGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.</description><author>Xinzhe Li</author><pubDate>Thu, 24 Oct 2024 07:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05804v5</guid></item><item><title>Bonsai: Gradient-free Graph Distillation for Node Classification</title><link>http://arxiv.org/abs/2410.17579v2</link><description>Graph distillation has emerged as a promising avenue to enable scalabletraining of GNNs by compressing the training dataset while preserving essentialgraph characteristics. Our study uncovers significant shortcomings in currentgraph distillation techniques. First, the majority of the algorithmsparadoxically require training on the full dataset to perform distillation.Second, due to their gradient-emulating approach, these methods require freshdistillation for any change in hyperparameters or GNN architecture, limitingtheir flexibility and reusability. Finally, they fail to achieve substantialsize reduction due to synthesizing fully-connected, edge-weighted graphs. Toaddress these challenges, we present Bonsai, a novel graph distillation methodempowered by the observation that \textit{computation trees} form thefundamental processing units of message-passing GNNs. Bonsai distills datasetsby encoding a careful selection of \textit{exemplar} trees that maximize therepresentation of all computation trees in the training set. This uniqueapproach imparts Bonsai as the first linear-time, model-agnostic graphdistillation algorithm for node classification that outperforms existingbaselines across $6$ real-world datasets on accuracy, while being $22$ timesfaster on average. Bonsai is grounded in rigorous mathematical guarantees onthe adopted approximation strategies making it robust to GNN architectures,datasets, and parameters.</description><author>Mridul Gupta, Samyak Jain, Vansh Ramani, Hariprasad Kodamana, Sayan Ranu</author><pubDate>Thu, 24 Oct 2024 05:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17579v2</guid></item><item><title>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks</title><link>http://arxiv.org/abs/2307.07840v4</link><description>Graph regression is a fundamental task that has gained significant attentionin various graph learning tasks. However, the inference process is often noteasily interpretable. Current explanation techniques are limited tounderstanding Graph Neural Network (GNN) behaviors in classification tasks,leaving an explanation gap for graph regression models. In this work, wepropose a novel explanation method to interpret the graph regression models(XAIG-R). Our method addresses the distribution shifting problem andcontinuously ordered decision boundary issues that hinder existing methods awayfrom being applied in regression tasks. We introduce a novel objective based onthe graph information bottleneck theory (GIB) and a new mix-up framework, whichcan support various GNNs and explainers in a model-agnostic manner.Additionally, we present a self-supervised learning strategy to tackle thecontinuously ordered labels in regression tasks. We evaluate our proposedmethod on three benchmark datasets and a real-life dataset introduced by us,and extensive experiments demonstrate its effectiveness in interpreting GNNmodels in regression tasks.</description><author>Jiaxing Zhang, Zhuomin Chen, Hao Mei, Longchao Da, Dongsheng Luo, Hua Wei</author><pubDate>Thu, 24 Oct 2024 05:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07840v4</guid></item><item><title>ERX: A Fast Real-Time Anomaly Detection Algorithm for Hyperspectral Line Scanning</title><link>http://arxiv.org/abs/2408.14947v3</link><description>Detecting unexpected objects (anomalies) in real time has great potential formonitoring, managing, and protecting the environment. Hyperspectral line-scancameras are a low-cost solution that enhance confidence in anomaly detectionover RGB and multispectral imagery. However, existing line-scan algorithms aretoo slow when using small computers (e.g. those onboard a drone or smallsatellite), do not adapt to changing scenery, or lack robustness againstgeometric distortions. This paper introduces the Exponentially moving RXalgorithm (ERX) to address these issues, and compares it with existing RX-basedanomaly detection methods for hyperspectral line scanning. Three large and morecomplex datasets are also introduced to better assess the practical challengeswhen using line-scan cameras (two hyperspectral and one multispectral). ERX isevaluated using a Jetson Xavier NX compute module, achieving the bestcombination of speed and detection performance. This research paves the way forfuture studies in grouping and locating anomalous objects, adaptive andautomatic threshold selection, and real-time field tests. The datasets and thePython code are available at: https://github.com/WiseGamgee/HyperAD.</description><author>Samuel Garske, Bradley Evans, Christopher Artlett, KC Wong</author><pubDate>Thu, 24 Oct 2024 04:57:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14947v3</guid></item><item><title>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</title><link>http://arxiv.org/abs/2407.12883v3</link><description>Existing retrieval benchmarks primarily consist of information-seekingqueries (e.g., aggregated questions from search engines) where keyword orsemantic-based retrieval is usually sufficient. However, many complexreal-world queries require in-depth reasoning to identify relevant documentsthat go beyond surface form matching. For example, finding documentation for acoding question requires understanding the logic and syntax of the functionsinvolved. To better benchmark retrieval on such challenging queries, weintroduce BRIGHT, the first text retrieval benchmark that requires intensivereasoning to retrieve relevant documents. Our dataset consists of 1,384real-world queries spanning diverse domains, such as economics, psychology,mathematics, and coding. These queries are drawn from naturally occurring andcarefully curated human data. Extensive evaluation reveals that evenstate-of-the-art retrieval models perform poorly on BRIGHT. The leading modelon the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of59.0 nDCG@10, produces a score of nDCG@10 of 18.3 on BRIGHT. We show thatincorporating explicit reasoning about the query improves retrieval performanceby up to 12.2 points. Moreover, incorporating retrieved documents from thetop-performing retriever boosts question-answering performance by over 6.6points. We believe that BRIGHT paves the way for future research on retrievalsystems in more realistic and challenging settings.</description><author>Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu</author><pubDate>Thu, 24 Oct 2024 04:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12883v3</guid></item><item><title>Advancing Interpretability in Text Classification through Prototype Learning</title><link>http://arxiv.org/abs/2410.17546v2</link><description>Deep neural networks have achieved remarkable performance in varioustext-based tasks but often lack interpretability, making them less suitable forapplications where transparency is critical. To address this, we proposeProtoLens, a novel prototype-based model that provides fine-grained,sub-sentence level interpretability for text classification. ProtoLens uses aPrototype-aware Span Extraction module to identify relevant text spansassociated with learned prototypes and a Prototype Alignment mechanism toensure prototypes are semantically meaningful throughout training. By aligningthe prototype embeddings with human-understandable examples, ProtoLens providesinterpretable predictions while maintaining competitive accuracy. Extensiveexperiments demonstrate that ProtoLens outperforms both prototype-based andnon-interpretable baselines on multiple text classification benchmarks. Codeand data are available at\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.</description><author>Bowen Wei, Ziwei Zhu</author><pubDate>Thu, 24 Oct 2024 04:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17546v2</guid></item><item><title>FDF: Flexible Decoupled Framework for Time Series Forecasting with Conditional Denoising and Polynomial Modeling</title><link>http://arxiv.org/abs/2410.13253v3</link><description>Time series forecasting is vital in numerous web applications, influencingcritical decision-making across industries. While diffusion models haverecently gained increasing popularity for this task, we argue they suffer froma significant drawback: indiscriminate noise addition to the original timeseries followed by denoising, which can obscure underlying dynamic evolvingtrend and complicate forecasting. To address this limitation, we propose anovel flexible decoupled framework (FDF) that learns high-quality time seriesrepresentations for enhanced forecasting performance. A key characteristic ofour approach leverages the inherent inductive bias of time series data of itsdecomposed trend and seasonal components, each modeled separately to enabledecoupled analysis and modeling. Specifically, we propose an innovativeConditional Denoising Seasonal Module (CDSM) within the diffusion model, whichleverages statistical information from the historical window to conditionallymodel the complex seasonal component. Notably, we incorporate a PolynomialTrend Module (PTM) to effectively capture the smooth trend component, therebyenhancing the model's ability to represent temporal dependencies. Extensiveexperiments validate the effectiveness of our framework, demonstrating superiorperformance over existing methods and highlighting its flexibility in timeseries forecasting. The source code is available athttps://github.com/zjt-gpu/FDF.</description><author>Jintao Zhang, Mingyue Cheng, Xiaoyu Tao, Zhiding Liu, Daoyu Wang</author><pubDate>Thu, 24 Oct 2024 03:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13253v3</guid></item><item><title>RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine Semantic Re-alignment</title><link>http://arxiv.org/abs/2305.19599v5</link><description>Recent advances in text-to-image diffusion models have achieved remarkablesuccess in generating high-quality, realistic images from textual descriptions.However, these approaches have faced challenges in precisely aligning thegenerated visual content with the textual concepts described in the prompts. Inthis paper, we propose a two-stage coarse-to-fine semantic re-alignment method,named RealignDiff, aimed at improving the alignment between text and images intext-to-image diffusion models. In the coarse semantic re-alignment phase, anovel caption reward, leveraging the BLIP-2 model, is proposed to evaluate thesemantic discrepancy between the generated image caption and the given textprompt. Subsequently, the fine semantic re-alignment stage employs a localdense caption generation module and a re-weighting attention modulation moduleto refine the previously generated images from a local semantic view.Experimental results on the MS-COCO and ViLG-300 datasets demonstrate that theproposed two-stage coarse-to-fine semantic re-alignment method outperformsother baseline re-alignment techniques by a substantial margin in both visualquality and semantic similarity with the input prompt.</description><author>Zutao Jiang, Guian Fang, Jianhua Han, Guansong Lu, Hang Xu, Shengcai Liao, Xiaojun Chang, Xiaodan Liang</author><pubDate>Thu, 24 Oct 2024 03:14:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19599v5</guid></item><item><title>ESpeW: Robust Copyright Protection for LLM-based EaaS via Embedding-Specific Watermark</title><link>http://arxiv.org/abs/2410.17552v2</link><description>Embeddings as a Service (EaaS) is emerging as a crucial role in AIapplications. Unfortunately, EaaS is vulnerable to model extraction attacks,highlighting the urgent need for copyright protection. Although somepreliminary works propose applying embedding watermarks to protect EaaS, recentresearch reveals that these watermarks can be easily removed. Hence, it iscrucial to inject robust watermarks resistant to watermark removal attacks.Existing watermarking methods typically inject a target embedding intoembeddings through linear interpolation when the text contains triggers.However, this mechanism results in each watermarked embedding having the samecomponent, which makes the watermark easy to identify and eliminate. Motivatedby this, in this paper, we propose a novel embedding-specific watermarking(ESpeW) mechanism to offer robust copyright protection for EaaS. Our approachinvolves injecting unique, yet readily identifiable watermarks into eachembedding. Watermarks inserted by ESpeW are designed to maintain a significantdistance from one another and to avoid sharing common components, thus makingit significantly more challenging to remove the watermarks. Extensiveexperiments on four popular datasets demonstrate that ESpeW can even watermarksuccessfully against a highly aggressive removal strategy without sacrificingthe quality of embeddings. Code is available athttps://github.com/liudan193/ESpeW.</description><author>Zongqi Wang, Baoyuan Wu, Jingyuan Deng, Yujiu Yang</author><pubDate>Thu, 24 Oct 2024 02:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17552v2</guid></item><item><title>The Neuromorphic Analog Electronic Nose</title><link>http://arxiv.org/abs/2410.16677v2</link><description>Rapid detection of gas concentration is important in different domains likegas leakage monitoring, pollution control, and so on, for the prevention ofhealth hazards. Out of different types of gas sensors, Metal oxide (MOx)sensors are extensively used in such applications because of their portability,low cost, and high sensitivity for specific gases. However, how to effectivelysample the MOx data for the real-time detection of gas and its concentrationlevel remains an open question. Here we introduce a simple analog front-end forone MOx sensor that encodes the gas concentration in the time differencebetween pulses of two separate pathways. This front-end design is inspired bythe spiking output of a mammalian olfactory bulb. We show that for a gas pulseinjected in a constant airflow, the time difference between pulses decreaseswith increasing gas concentration, similar to the spike time difference betweenthe two principal output neurons in the olfactory bulb. The circuit design isfurther extended to a MOx sensor array and this sensor array front-end wastested in the same environment for gas identification and concentrationestimation. Encoding of gas stimulus features in analog spikes at the sensorlevel itself may result in data and power-efficient real-time gas sensingsystems in the future that can ultimately be used in uncontrolled and turbulentenvironments for longer periods without data explosion.</description><author>Shavika Rastogi, Nik Dennler, Michael Schmuker, André van Schaik</author><pubDate>Thu, 24 Oct 2024 01:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16677v2</guid></item><item><title>Optimal Partial Graph Matching</title><link>http://arxiv.org/abs/2410.16718v2</link><description>Partial graph matching addresses the limitations of traditional graphmatching by allowing some nodes to remain unmatched, making it applicable tomore complex scenarios. However, this flexibility introduces additionalcomplexity, as both the subset of nodes to match and the optimal mapping mustbe determined. While recent studies have explored deep learning techniques forpartial graph matching, a significant limitation remains: the absence of anoptimization objective that fully captures the problem's intrinsic nature whileenabling efficient solutions. In this paper, we propose a novel optimizationframework for partial graph matching, inspired by optimal partial transport.Our approach formulates an objective that enables partial assignments whileincorporating matching biases, using weighted total variation as the divergencefunction to guarantee optimal partial assignments. We employ the Hungarianalgorithm to achieve efficient, exact solutions with cubic time complexity. Ourcontributions are threefold: (i) we introduce a robust optimization objectivethat balances matched and unmatched nodes; (ii) we establish a connectionbetween partial graph matching and the linear sum assignment problem, enablingefficient solutions; (iii) we propose a deep graph matching architecture with anovel partial matching loss, providing an end-to-end solution. The empiricalevaluations on standard graph matching benchmarks demonstrate the efficacy ofthe proposed approach.</description><author>Gathika Ratnayaka, James Nichols, Qing Wang</author><pubDate>Wed, 23 Oct 2024 23:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16718v2</guid></item><item><title>DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</title><link>http://arxiv.org/abs/2410.18084v1</link><description>LiDAR scene generation has been developing rapidly recently. However,existing methods primarily focus on generating static and single-frame scenes,overlooking the inherently dynamic nature of real-world driving environments.In this work, we introduce DynamicCity, a novel 4D LiDAR generation frameworkcapable of generating large-scale, high-quality LiDAR scenes that capture thetemporal evolution of dynamic environments. DynamicCity mainly consists of twokey models. 1) A VAE model for learning HexPlane as the compact 4Drepresentation. Instead of using naive averaging operations, DynamicCityemploys a novel Projection Module to effectively compress 4D LiDAR featuresinto six 2D feature maps for HexPlane construction, which significantlyenhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, weutilize an Expansion &amp; Squeeze Strategy to reconstruct 3D feature volumes inparallel, which improves both network training efficiency and reconstructionaccuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06xtraining speedup, and 70.84% memory reduction). 2) A DiT-based diffusion modelfor HexPlane generation. To make HexPlane feasible for DiT generation, a PaddedRollout Operation is proposed to reorganize all six feature planes of theHexPlane as a squared 2D feature map. In particular, various conditions couldbe introduced in the diffusion or sampling process, supporting versatile 4Dgeneration applications, such as trajectory- and command-driven generation,inpainting, and layout-conditioned generation. Extensive experiments on theCarlaSC and Waymo datasets demonstrate that DynamicCity significantlyoutperforms existing state-of-the-art 4D LiDAR generation methods acrossmultiple metrics. The code will be released to facilitate future research.</description><author>Hengwei Bian, Lingdong Kong, Haozhe Xie, Liang Pan, Yu Qiao, Ziwei Liu</author><pubDate>Wed, 23 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18084v1</guid></item><item><title>FIPER: Generalizable Factorized Fields for Joint Image Compression and Super-Resolution</title><link>http://arxiv.org/abs/2410.18083v1</link><description>In this work, we propose a unified representation for Super-Resolution (SR)and Image Compression, termed **Factorized Fields**, motivated by the sharedprinciples between these two tasks. Both SISR and Image Compression requirerecovering and preserving fine image details--whether by enhancing resolutionor reconstructing compressed data. Unlike previous methods that mainly focus onnetwork architecture, our proposed approach utilizes a basis-coefficientdecomposition to explicitly capture multi-scale visual features and structuralcomponents in images, addressing the core challenges of both tasks. We firstderive our SR model, which includes a Coefficient Backbone and Basis SwinTransformer for generalizable Factorized Fields. Then, to further unify thesetwo tasks, we leverage the strong information-recovery capabilities of thetrained SR modules as priors in the compression pipeline, improving bothcompression efficiency and detail reconstruction. Additionally, we introduce amerged-basis compression branch that consolidates shared structures, furtheroptimizing the compression process. Extensive experiments show that our unifiedrepresentation delivers state-of-the-art performance, achieving an averagerelative improvement of 204.4% in PSNR over the baseline in Super-Resolution(SR) and 9.35% BD-rate reduction in Image Compression compared to the previousSOTA.</description><author>Yang-Che Sun, Cheng Yu Yeo, Ernie Chu, Jun-Cheng Chen, Yu-Lun Liu</author><pubDate>Wed, 23 Oct 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18083v1</guid></item><item><title>Prioritized Generative Replay</title><link>http://arxiv.org/abs/2410.18082v1</link><description>Sample-efficient online reinforcement learning often uses replay buffers tostore experience for reuse when updating the value function. However, uniformreplay is inefficient, since certain classes of transitions can be morerelevant to learning. While prioritization of more useful samples is helpful,this strategy can also lead to overfitting, as useful samples are likely to bemore rare. In this work, we instead propose a prioritized, parametric versionof an agent's memory, using generative models to capture online experience.This paradigm enables (1) densification of past experience, with newgenerations that benefit from the generative model's generalization capacityand (2) guidance via a family of "relevance functions" that push thesegenerations towards more useful parts of an agent's acquired history. We showthis recipe can be instantiated using conditional diffusion models and simplerelevance functions such as curiosity- or value-based metrics. Our approachconsistently improves performance and sample efficiency in both state- andpixel-based domains. We expose the mechanisms underlying these gains, showinghow guidance promotes diversity in our generated transitions and reducesoverfitting. We also showcase how our approach can train policies with evenhigher update-to-data ratios than before, opening up avenues to better scaleonline RL agents.</description><author>Renhao Wang, Kevin Frans, Pieter Abbeel, Sergey Levine, Alexei A. Efros</author><pubDate>Wed, 23 Oct 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18082v1</guid></item><item><title>FreeVS: Generative View Synthesis on Free Driving Trajectory</title><link>http://arxiv.org/abs/2410.18079v1</link><description>Existing reconstruction-based novel view synthesis methods for driving scenesfocus on synthesizing camera views along the recorded trajectory of the egovehicle. Their image rendering performance will severely degrade on viewpointsfalling out of the recorded trajectory, where camera rays are untrained. Wepropose FreeVS, a novel fully generative approach that can synthesize cameraviews on free new trajectories in real driving scenes. To control thegeneration results to be 3D consistent with the real scenes and accurate inviewpoint pose, we propose the pseudo-image representation of view priors tocontrol the generation process. Viewpoint transformation simulation is appliedon pseudo-images to simulate camera movement in each direction. Once trained,FreeVS can be applied to any validation sequences without reconstructionprocess and synthesis views on novel trajectories. Moreover, we propose two newchallenging benchmarks tailored to driving scenes, which are novel camerasynthesis and novel trajectory synthesis, emphasizing the freedom ofviewpoints. Given that no ground truth images are available on noveltrajectories, we also propose to evaluate the consistency of images synthesizedon novel trajectories with 3D perception models. Experiments on the Waymo OpenDataset show that FreeVS has a strong image synthesis performance on both therecorded trajectories and novel trajectories. Project Page:https://freevs24.github.io/</description><author>Qitai Wang, Lue Fan, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang</author><pubDate>Wed, 23 Oct 2024 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18079v1</guid></item><item><title>ALTA: Compiler-Based Analysis of Transformers</title><link>http://arxiv.org/abs/2410.18077v1</link><description>We propose a new programming language called ALTA and a compiler that can mapALTA programs to Transformer weights. ALTA is inspired by RASP, a languageproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compilerfrom RASP programs to Transformer weights. ALTA complements and extends thisprior work, offering the ability to express loops and to compile programs toUniversal Transformers, among other advantages. ALTA allows us toconstructively show how Transformers can represent length-invariant algorithmsfor computing parity and addition, as well as a solution to the SCAN benchmarkof compositional generalization tasks, without requiring intermediatescratchpad decoding steps. We also propose tools to analyze cases where theexpressibility of an algorithm is established, but end-to-end training on agiven training set fails to induce behavior consistent with the desiredalgorithm. To this end, we explore training from ALTA execution traces as amore fine-grained supervision signal. This enables additional experiments andtheoretical analyses relating the learnability of various algorithms to dataavailability and modeling decisions, such as positional encodings. We make theALTA framework -- language specification, symbolic interpreter, and weightcompiler -- available to the community to enable further applications andinsights.</description><author>Peter Shaw, James Cohan, Jacob Eisenstein, Kenton Lee, Jonathan Berant, Kristina Toutanova</author><pubDate>Wed, 23 Oct 2024 17:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18077v1</guid></item><item><title>Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</title><link>http://arxiv.org/abs/2410.18076v1</link><description>Unsupervised pretraining has been transformative in many supervised domains.However, applying such ideas to reinforcement learning (RL) presents a uniquechallenge in that fine-tuning does not involve mimicking task-specific data,but rather exploring and locating the solution through iterativeself-improvement. In this work, we study how unlabeled prior trajectory datacan be leveraged to learn efficient exploration strategies. While prior datacan be used to pretrain a set of low-level skills, or as additional off-policydata for online RL, it has been unclear how to combine these ideas effectivelyfor online exploration. Our method SUPE (Skills from Unlabeled Prior data forExploration) demonstrates that a careful combination of these ideas compoundstheir benefits. Our method first extracts low-level skills using a variationalautoencoder (VAE), and then pseudo-relabels unlabeled trajectories using anoptimistic reward model, transforming prior data into high-level, task-relevantexamples. Finally, SUPE uses these transformed examples as additionaloff-policy data for online RL to learn a high-level policy that composespretrained low-level skills to explore efficiently. We empirically show thatSUPE reliably outperforms prior strategies, successfully solving a suite oflong-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.</description><author>Max Wilcoxson, Qiyang Li, Kevin Frans, Sergey Levine</author><pubDate>Wed, 23 Oct 2024 17:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18076v1</guid></item><item><title>ProFL: Performative Robust Optimal Federated Learning</title><link>http://arxiv.org/abs/2410.18075v1</link><description>Performative prediction (PP) is a framework that captures distribution shiftsthat occur during the training of machine learning models due to theirdeployment. As the trained model is used, its generated data could cause themodel to evolve, leading to deviations from the original data distribution. Theimpact of such model-induced distribution shifts in the federated learning (FL)setup remains unexplored despite being increasingly likely to transpire inreal-life use cases. Although Jin et al. (2024) recently extended PP to FL in astraightforward manner, the resulting model only converges to a performativestable point, which may be far from optimal. The methods in Izzo et al. (2021);Miller et al. (2021) can find a performative optimal point in centralizedsettings, but they require the performative risk to be convex and the trainingdata to be noiseless, assumptions often violated in realistic FL systems. Thispaper overcomes all of these shortcomings and proposes Performative robustoptimal Federated Learning (ProFL), an algorithm that finds performativeoptimal points in FL from noisy and contaminated data. We present theconvergence analysis under the Polyak-Lojasiewicz condition, which applies tonon-convex objectives. Extensive experiments on multiple datasets validate ourproposed algorithms' efficiency.</description><author>Xue Zheng, Tian Xie, Xuwei Tan, Aylin Yener, Xueru Zhang, Ali Payani, Myungjin Lee</author><pubDate>Wed, 23 Oct 2024 17:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18075v1</guid></item><item><title>UnCLe: Unsupervised Continual Learning of Depth Completion</title><link>http://arxiv.org/abs/2410.18074v1</link><description>We propose UnCLe, a standardized benchmark for Unsupervised ContinualLearning of a multimodal depth estimation task: Depth completion aims to infera dense depth map from a pair of synchronized RGB image and sparse depth map.We benchmark depth completion models under the practical scenario ofunsupervised learning over continuous streams of data. Existing methods aretypically trained on a static, or stationary, dataset. However, when adaptingto novel non-stationary distributions, they "catastrophically forget"previously learned information. UnCLe simulates these non-stationarydistributions by adapting depth completion models to sequences of datasetscontaining diverse scenes captured from distinct domains using different visualand range sensors. We adopt representative methods from continual learningparadigms and translate them to enable unsupervised continual learning of depthcompletion. We benchmark these models for indoor and outdoor and investigatethe degree of catastrophic forgetting through standard quantitative metrics.Furthermore, we introduce model inversion quality as an additional measure offorgetting. We find that unsupervised continual learning of depth completion isan open problem, and we invite researchers to leverage UnCLe as a developmentplatform.</description><author>Suchisrit Gangopadhyay, Xien Chen, Michael Chu, Patrick Rim, Hyoungseob Park, Alex Wong</author><pubDate>Wed, 23 Oct 2024 17:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18074v1</guid></item><item><title>WorldSimBench: Towards Video Generation Models as World Simulators</title><link>http://arxiv.org/abs/2410.18072v1</link><description>Recent advancements in predictive models have demonstrated exceptionalcapabilities in predicting the future state of objects and scenes. However, thelack of categorization based on inherent characteristics continues to hinderthe progress of predictive model development. Additionally, existing benchmarksare unable to effectively evaluate higher-capability, highly embodiedpredictive models from an embodied perspective. In this work, we classify thefunctionalities of predictive models into a hierarchy and take the first stepin evaluating World Simulators by proposing a dual evaluation framework calledWorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation andImplicit Manipulative Evaluation, encompassing human preference assessmentsfrom the visual perspective and action-level evaluations in embodied tasks,covering three representative embodied scenarios: Open-Ended EmbodiedEnvironment, Autonomous, Driving, and Robot Manipulation. In the ExplicitPerceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessmentdataset based on fine-grained human feedback, which we use to train a HumanPreference Evaluator that aligns with human perception and explicitly assessesthe visual fidelity of World Simulators. In the Implicit ManipulativeEvaluation, we assess the video-action consistency of World Simulators byevaluating whether the generated situation-aware video can be accuratelytranslated into the correct control signals in dynamic environments. Ourcomprehensive evaluation offers key insights that can drive further innovationin video generation models, positioning World Simulators as a pivotaladvancement toward embodied artificial intelligence.</description><author>Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, Lei Bai, Wanli Ouyang, Ruimao Zhang</author><pubDate>Wed, 23 Oct 2024 17:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18072v1</guid></item><item><title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</title><link>http://arxiv.org/abs/2410.18071v1</link><description>Recently, multimodal large language models (MLLMs) have received muchattention for their impressive capabilities. The evaluation of MLLMs isbecoming critical to analyzing attributes of MLLMs and providing valuableinsights. However, current benchmarks overlook the problem of promptsensitivity - minor prompt variations may lead to significant performancefluctuations. Thus, inappropriate prompts may obscure the models' capabilities,underestimating the models' performance. Moreover, different models havedifferent preferences for different prompts, and thus, using the same promptfor all models will cause evaluation bias. This paper analyzes this deficiencyin existing benchmarks and further introduces a new evaluation framework namedTP-Eval, which introduces a prompt customization method to reduce evaluationbiases and tap models' potential. TP-Eval will rewrite the original prompts todifferent customized prompts for different models. In particular, we proposesome well-designed modules for prompt customization tailored to the scenario ofMLLM evaluation. Extensive experiments demonstrate the effectiveness of ourapproach to uncovering models' capabilities, and TP-Eval should benefit thecommunity in developing more comprehensive and convincing MLLM evaluationbenchmarks.</description><author>Yuxuan Xie, Tianhua Li, Wenqi Shao, Kaipeng Zhang</author><pubDate>Wed, 23 Oct 2024 17:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18071v1</guid></item><item><title>Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers</title><link>http://arxiv.org/abs/2408.12568v2</link><description>To solve ever more complex problems, Deep Neural Networks are scaled tobillions of parameters, leading to huge computational costs. An effectiveapproach to reduce computational requirements and increase efficiency is toprune unnecessary components of these often over-parameterized networks.Previous work has shown that attribution methods from the field of eXplainableAI serve as effective means to extract and prune the least relevant networkcomponents in a few-shot fashion. We extend the current state by proposing toexplicitly optimize hyperparameters of attribution methods for the task ofpruning, and further include transformer-based networks in our analysis. Ourapproach yields higher model compression rates of large transformer- andconvolutional architectures (VGG, ResNet, ViT) compared to previous works,while still attaining high performance on ImageNet classification tasks. Here,our experiments indicate that transformers have a higher degree ofover-parameterization compared to convolutional neural networks. Code isavailable at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.</description><author>Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</author><pubDate>Wed, 23 Oct 2024 17:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12568v2</guid></item><item><title>Training Free Guided Flow Matching with Optimal Control</title><link>http://arxiv.org/abs/2410.18070v1</link><description>Controlled generation with pre-trained Diffusion and Flow Matching models hasvast applications. One strategy for guiding ODE-based generative models isthrough optimizing a target loss $R(x_1)$ while staying close to the priordistribution. Along this line, some recent work showed the effectiveness ofguiding flow model by differentiating through its ODE sampling process. Despitethe superior performance, the theoretical understanding of this line of methodsis still preliminary, leaving space for algorithm improvement. Moreover,existing methods predominately focus on Euclidean data manifold, and there is acompelling need for guided flow methods on complex geometries such as SO(3),which prevails in high-stake scientific applications like protein design. Wepresent OC-Flow, a general and theoretically grounded training-free frameworkfor guided flow matching using optimal control. Building upon advances inoptimal control theory, we develop effective and practical algorithms forsolving optimal control in guided ODE-based generation and provide a systematictheoretical analysis of the convergence guarantee in both Euclidean and SO(3).We show that existing backprop-through-ODE methods can be interpreted asspecial cases of Euclidean OC-Flow. OC-Flow achieved superior performance inextensive experiments on text-guided image manipulation, conditional moleculegeneration, and all-atom peptide design.</description><author>Luran Wang, Chaoran Cheng, Yizhen Liao, Yanru Qu, Ge Liu</author><pubDate>Wed, 23 Oct 2024 17:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18070v1</guid></item><item><title>Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking</title><link>http://arxiv.org/abs/2403.03185v2</link><description>Because it is difficult to precisely specify complex objectives,reinforcement learning policies are often optimized using flawed proxy rewardsthat seem to capture the true objective. However, optimizing proxy rewardsfrequently leads to reward hacking: the optimized reward function ceases to bea good proxy, and the resulting policy performs poorly with respect to theunspecified true reward. Principled solutions to reward hacking have beenimpeded by the lack of a good definition for the problem. To address this, weintroduce a definition of reward hacking based on the correlation between proxyand true rewards for states and actions seen by a "base policy" that breaksdown under optimization. We show that this definition captures reward hackingbehavior across several realistic settings, including in reinforcement learningfrom human feedback (RLHF). We then show theoretically that regularization tothe base policy can effectively prevent reward hacking. While current RLHFapproaches apply a KL penalty between the action distributions of policies, ourtheory suggests that it is more effective to regularize using the $\chi^2$divergence between the policies' occupancy measures. We intuitively show whythis type of regularization is superior and demonstrate that it bettermitigates reward hacking in practice across four realistic domains, includingRLHF for LLMs. Our code is available at https://github.com/cassidylaidlaw/orpo.</description><author>Cassidy Laidlaw, Shivam Singhal, Anca Dragan</author><pubDate>Wed, 23 Oct 2024 17:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03185v2</guid></item><item><title>Physical Reasoning and Object Planning for Household Embodied Agents</title><link>http://arxiv.org/abs/2311.13577v2</link><description>In this study, we explore the sophisticated domain of task planning forrobust household embodied agents, with a particular emphasis on the intricatetask of selecting substitute objects. We introduce the CommonSense ObjectAffordance Task (COAT), a novel framework designed to analyze reasoningcapabilities in commonsense scenarios. This approach is centered onunderstanding how these agents can effectively identify and utilize alternativeobjects when executing household tasks, thereby offering insights into thecomplexities of practical decision-making in real-world environments. Drawinginspiration from factors affecting human decision-making, we explore how largelanguage models tackle this challenge through four meticulously craftedcommonsense question-and-answer datasets featuring refined rules and humanannotations. Our evaluation of state-of-the-art language models on thesedatasets sheds light on three pivotal considerations: 1) aligning an object'sinherent utility with the task at hand, 2) navigating contextual dependencies(societal norms, safety, appropriateness, and efficiency), and 3) accountingfor the current physical state of the object. To maintain accessibility, weintroduce five abstract variables reflecting an object's physical condition,modulated by human insights, to simulate diverse household scenarios. Ourcontributions include insightful human preference mappings for all threefactors and four extensive QA datasets (2K, 15k, 60k, 70K questions) probingthe intricacies of utility dependencies, contextual dependencies and objectphysical states. The datasets, along with our findings, are accessible at:https://github.com/Ayush8120/COAT. This research not only advances ourunderstanding of physical commonsense reasoning in language models but alsopaves the way for future improvements in household agent intelligence.</description><author>Ayush Agrawal, Raghav Prabhakar, Anirudh Goyal, Dianbo Liu</author><pubDate>Wed, 23 Oct 2024 17:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13577v2</guid></item><item><title>Beyond position: how rotary embeddings shape representations and memory in autoregressive transfomers</title><link>http://arxiv.org/abs/2410.18067v1</link><description>Rotary Positional Embeddings (RoPE) enhance positional encoding inTransformer models, yet their full impact on model dynamics remainsunderexplored. This paper studies how RoPE introduces position-dependentrotations, causing phase shifts in token embeddings that influencehigher-frequency components within the model's internal representations.Through spectral analysis, we demonstrate that RoPE's rotation matrices induceoscillatory behaviors in embeddings, affecting information retention acrosslayers and shaping temporal modeling capabilities. We show that activationfunctions in feed-forward networks interact with RoPE-modulated embeddings togenerate harmonics, leading to constructive or destructive interference basedon phase alignment. Our findings reveal that phase alignment amplifiesactivations and sharpens attention, while misalignment weakens activations anddisrupts focus on positional patterns. This study underscores the importance offrequency components as intrinsic elements of model behavior, offering newinsights beyond traditional analyses.</description><author>Valeria Ruscio, Fabrizio Silvestri</author><pubDate>Wed, 23 Oct 2024 17:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18067v1</guid></item><item><title>MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation</title><link>http://arxiv.org/abs/2409.15240v2</link><description>Long-term memory is important for chatbots and dialogue systems (DS) tocreate consistent and human-like conversations, evidenced by numerous developedmemory-augmented DS (MADS). To evaluate the effectiveness of such MADS,existing commonly used evaluation metrics, like retrieval accuracy andperplexity (PPL), mainly focus on query-oriented factualness and languagequality assessment. However, these metrics often lack practical value.Moreover, the evaluation dimensions are insufficient for human-like assessmentin DS. Regarding memory-recalling paradigms, current evaluation schemes onlyconsider passive memory retrieval while ignoring diverse memory recall withrich triggering factors, e.g., emotions and surroundings, which can beessential in emotional support scenarios. To bridge the gap, we construct anovel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering variousmemory-recalling paradigms based on cognitive science and psychology theories.The benchmark assesses two tasks separately: memory retrieval and memoryrecognition with the incorporation of both passive and proactive memory recalldata. We introduce new scoring criteria to the evaluation, including memoryinjection, emotion support (ES) proficiency, and intimacy, to comprehensivelyassess generated responses. Results from cutting-edge embedding models andlarge language models on this benchmark indicate the potential for furtheradvancement. Extensive testing further reveals correlations between memoryinjection, ES proficiency, and intimacy.</description><author>Junqing He, Liang Zhu, Rui Wang, Xi Wang, Reza Haffari, Jiaxing Zhang</author><pubDate>Wed, 23 Oct 2024 17:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15240v2</guid></item><item><title>Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs</title><link>http://arxiv.org/abs/2404.19442v2</link><description>Nigeria is a multilingual country with 500+ languages. Naija is aNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixedlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it hasmainly been a spoken language until recently, there are now various platformspublishing exclusively in Naija such as Naija Wikipedia. However, it is hard todistinguish by non-native from a larger pidgin languages spoken across WestAfrica known as West African Pidgin English (WAPE) -- which is more simpliedand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC newsplatform publishes exclusively in WAPE to cater for several countries in WestAfrica. In our paper, we show through statistical analyses and MachineTranslation experiments that these two creole varieties do not represent eachother (i.e., there are linguistic differences in word order and vocabulary) andGenerative AI operates only based on WAPE. In other words, Naija isunder-represented in Generative AI, and it is hard to teach LLMs with fewexamples.</description><author>David Ifeoluwa Adelani, A. Seza Doğruöz, Iyanuoluwa Shode, Anuoluwapo Aremu</author><pubDate>Wed, 23 Oct 2024 17:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19442v2</guid></item><item><title>The Double-Edged Sword of Behavioral Responses in Strategic Classification: Theory and User Studies</title><link>http://arxiv.org/abs/2410.18066v1</link><description>When humans are subject to an algorithmic decision system, they canstrategically adjust their behavior accordingly (``game'' the system). While agrowing line of literature on strategic classification has used game-theoreticmodeling to understand and mitigate such gaming, these existing works considerstandard models of fully rational agents. In this paper, we propose a strategicclassification model that considers behavioral biases in human responses toalgorithms. We show how misperceptions of a classifier (specifically, of itsfeature weights) can lead to different types of discrepancies between biasedand rational agents' responses, and identify when behavioral agents over- orunder-invest in different features. We also show that strategic agents withbehavioral biases can benefit or (perhaps, unexpectedly) harm the firm comparedto fully rational strategic agents. We complement our analytical results withuser studies, which support our hypothesis of behavioral biases in humanresponses to the algorithm. Together, our findings highlight the need toaccount for human (cognitive) biases when designing AI systems, and providingexplanations of them, to strategic human in the loop.</description><author>Raman Ebrahimi, Kristen Vaccaro, Parinaz Naghizadeh</author><pubDate>Wed, 23 Oct 2024 17:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18066v1</guid></item><item><title>Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning</title><link>http://arxiv.org/abs/2407.15762v2</link><description>Reward-based finetuning is crucial for aligning language policies withintended behaviors (e.g., creativity and safety). A key challenge is to developsteerable language models that trade-off multiple (conflicting) objectives in aflexible and efficient manner. This paper presents Conditional Language Policy(CLP), a general framework for finetuning language models on multipleobjectives. Building on techniques from multi-task training andparameter-efficient finetuning, CLP learn steerable models that effectivelytrade-off conflicting objectives at inference time. Notably, this does notrequire training or maintaining multiple models to achieve different trade-offsbetween the objectives. Through extensive experiments and ablations on twosummarization datasets, we show that CLP learns steerable language models thatoutperform and Pareto-dominate the existing approaches for multi-objectivefinetuning.</description><author>Kaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Avinava Dubey, Alexandre Ramé, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, Léonard Hussenot, Olivier Bachem, Edouard Leurent</author><pubDate>Wed, 23 Oct 2024 17:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15762v2</guid></item><item><title>SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation</title><link>http://arxiv.org/abs/2410.18065v1</link><description>Robot learning has proven to be a general and effective technique forprogramming manipulators. Imitation learning is able to teach robots solelyfrom human demonstrations but is bottlenecked by the capabilities of thedemonstrations. Reinforcement learning uses exploration to discover betterbehaviors; however, the space of possible improvements can be too large tostart from scratch. And for both techniques, the learning difficulty increasesproportional to the length of the manipulation task. Accounting for this, wepropose SPIRE, a system that first uses Task and Motion Planning (TAMP) todecompose tasks into smaller learning subproblems and second combines imitationand reinforcement learning to maximize their strengths. We develop novelstrategies to train learning agents when deployed in the context of a planningsystem. We evaluate SPIRE on a suite of long-horizon and contact-rich robotmanipulation problems. We find that SPIRE outperforms prior approaches thatintegrate imitation learning, reinforcement learning, and planning by 35% to50% in average task performance, is 6 times more data efficient in the numberof human demonstrations needed to train proficient agents, and learns tocomplete tasks nearly twice as efficiently. Viewhttps://sites.google.com/view/spire-corl-2024 for more details.</description><author>Zihan Zhou, Animesh Garg, Dieter Fox, Caelan Garrett, Ajay Mandlekar</author><pubDate>Wed, 23 Oct 2024 17:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18065v1</guid></item><item><title>Utilitarian Algorithm Configuration for Infinite Parameter Spaces</title><link>http://arxiv.org/abs/2405.18246v2</link><description>Utilitarian algorithm configuration is a general-purpose technique forautomatically searching the parameter space of a given algorithm to optimizeits performance, as measured by a given utility function, on a given set ofinputs. Recently introduced utilitarian configuration procedures offeroptimality guarantees about the returned parameterization while provablyadapting to the hardness of the underlying problem. However, the applicabilityof these approaches is severely limited by the fact that they only search afinite, relatively small set of parameters. They cannot effectively search theconfiguration space of algorithms with continuous or uncountable parameters. Inthis paper we introduce a new procedure, which we dub COUP (Continuous,Optimistic Utilitarian Procrastination). COUP is designed to search infiniteparameter spaces efficiently to find good configurations quickly. Furthermore,COUP maintains the theoretical benefits of previous utilitarian configurationprocedures when applied to finite parameter spaces but is significantly faster,both provably and experimentally.</description><author>Devon Graham, Kevin Leyton-Brown</author><pubDate>Wed, 23 Oct 2024 17:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18246v2</guid></item><item><title>Explaining Bayesian Networks in Natural Language using Factor Arguments. Evaluation in the medical domain</title><link>http://arxiv.org/abs/2410.18060v1</link><description>In this paper, we propose a model for building natural language explanationsfor Bayesian Network Reasoning in terms of factor arguments, which areargumentation graphs of flowing evidence, relating the observed evidence to atarget variable we want to learn about. We introduce the notion of factorargument independence to address the outstanding question of defining whenarguments should be presented jointly or separately and present an algorithmthat, starting from the evidence nodes and a target node, produces a list ofall independent factor arguments ordered by their strength. Finally, weimplemented a scheme to build natural language explanations of BayesianReasoning using this approach. Our proposal has been validated in the medicaldomain through a human-driven evaluation study where we compare the BayesianNetwork Reasoning explanations obtained using factor arguments with analternative explanation method. Evaluation results indicate that our proposedexplanation approach is deemed by users as significantly more useful forunderstanding Bayesian Network Reasoning than another existing explanationmethod it is compared to.</description><author>Jaime Sevilla, Nikolay Babakov, Ehud Reiter, Alberto Bugarin</author><pubDate>Wed, 23 Oct 2024 17:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18060v1</guid></item><item><title>CLEAR: Character Unlearning in Textual and Visual Modalities</title><link>http://arxiv.org/abs/2410.18057v1</link><description>Machine Unlearning (MU) is critical for enhancing privacy and security indeep learning models, particularly in large multimodal language models (MLLMs),by removing specific private or hazardous information. While MU has madesignificant progress in textual and visual modalities, multimodal unlearning(MMU) remains significantly underexplored, partially due to the absence of asuitable open-source benchmark. To address this, we introduce CLEAR, a newbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitiousindividuals and 3,700 images linked with corresponding question-answer pairs,enabling a thorough evaluation across modalities. We assess 10 MU methods,adapting them for MMU, and highlight new challenges specific to multimodalforgetting. We also demonstrate that simple $\ell_1$ regularization on LoRAweights significantly mitigates catastrophic forgetting, preserving modelperformance on retained data. The dataset is available athttps://huggingface.co/datasets/therem/CLEAR</description><author>Alexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin, Boris Mikheev, Denis Bobkov, Aibek Alanov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina</author><pubDate>Wed, 23 Oct 2024 17:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18057v1</guid></item><item><title>In-Pixel Foreground and Contrast Enhancement Circuits with Customizable Mapping</title><link>http://arxiv.org/abs/2410.18052v1</link><description>This paper presents an innovative in-pixel contrast enhancement circuit thatperforms image processing directly within the pixel circuit. The circuit can betuned for different modes of operation. In foreground enhancement mode, itsuppresses low-intensity background pixels to nearly zero, isolating theforeground for better object visibility. In contrast enhancement mode, itimproves overall image contrast. The contrast enhancement function iscustomizable both during the design phase and in real-time, allowing thecircuit to adapt to specific applications and varying lighting conditions. Amodel of the designed pixel circuit is developed and applied to a full pixelarray, demonstrating significant improvements in image quality. Simulationsperformed in HSPICE show a nearly 6x increase in Michelson Contrast Ratio (CR)in the foreground enhancement mode. The simulation results indicate itspotential for real-time, adaptive contrast enhancement across various imagingenvironments.</description><author>Md Rahatul Islam Udoy, Md Mazharul Islam, Elijah Johnson, Ahmedullah Aziz</author><pubDate>Wed, 23 Oct 2024 17:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18052v1</guid></item><item><title>Safeguard is a Double-edged Sword: Denial-of-service Attack on Large Language Models</title><link>http://arxiv.org/abs/2410.02916v2</link><description>Safety is a paramount concern of large language models (LLMs) in their opendeployment. To this end, safeguard methods aim to enforce the ethical andresponsible use of LLMs through safety alignment or guardrail mechanisms.However, we found that the malicious attackers could exploit false positives ofsafeguards, i.e., fooling the safeguard model to block safe content mistakenly,leading to a new denial-of-service (DoS) attack on LLMs. Specifically, bysoftware or phishing attacks on user client software, attackers insert a short,seemingly innocuous adversarial prompt into to user prompt templates inconfiguration files; thus, this prompt appears in final user requests withoutvisibility in the user interface and is not trivial to identify. By designingan optimization process that utilizes gradient and attention information, ourattack can automatically generate seemingly safe adversarial prompts,approximately only 30 characters long, that universally block over 97\% of userrequests on Llama Guard 3. The attack presents a new dimension of evaluatingLLM safeguards focusing on false positives, fundamentally different from theclassic jailbreak.</description><author>Qingzhao Zhang, Ziyang Xiong, Z. Morley Mao</author><pubDate>Wed, 23 Oct 2024 17:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02916v2</guid></item><item><title>Real time anomalies detection on video</title><link>http://arxiv.org/abs/2410.18051v1</link><description>Nowadays, many places use security cameras. Unfortunately, when an incidentoccurs, these technologies are used to show past events. So it can beconsidered as a deterrence tool than a detection tool. In this article, we willpropose a deep learning approach trying to solve this problematic. Thisapproach uses convolutional models (CNN) to extract relevant characteristicslinked to the video images, theses characteristics will form times series to beanalyzed by LSTM / GRU models.</description><author>Fabien Poirier</author><pubDate>Wed, 23 Oct 2024 17:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18051v1</guid></item><item><title>LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering</title><link>http://arxiv.org/abs/2410.18050v1</link><description>Long-Context Question Answering (LCQA), a challenging task, aims to reasonover long-context documents to yield accurate answers to questions. Existinglong-context Large Language Models (LLMs) for LCQA often struggle with the"lost in the middle" issue. Retrieval-Augmented Generation (RAG) mitigates thisissue by providing external factual evidence. However, its chunking strategydisrupts the global long-context information, and its low-quality retrieval inlong contexts hinders LLMs from identifying effective factual details due tosubstantial noise. To this end, we propose LongRAG, a general,dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhanceRAG's understanding of complex long-context knowledge (i.e., global informationand factual details). We design LongRAG as a plug-and-play paradigm,facilitating adaptation to various domains and LLMs. Extensive experiments onthree multi-hop datasets demonstrate that LongRAG significantly outperformslong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG(up by 17.25%). Furthermore, we conduct quantitative ablation studies andmulti-dimensional analyses, highlighting the effectiveness of the system'scomponents and fine-tuning strategies. Data and code are available athttps://github.com/QingFei1/LongRAG.</description><author>Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, Jie Tang</author><pubDate>Wed, 23 Oct 2024 17:24:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18050v1</guid></item><item><title>Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for Russian Scientific Keyphrases</title><link>http://arxiv.org/abs/2410.18040v1</link><description>Keyphrase selection is a challenging task in natural language processing thathas a wide range of applications. Adapting existing supervised and unsupervisedsolutions for the Russian language faces several limitations due to the richmorphology of Russian and the limited number of training datasets available.Recent studies conducted on English texts show that large language models(LLMs) successfully address the task of generating keyphrases. LLMs allowachieving impressive results without task-specific fine-tuning, using textprompts instead. In this work, we access the performance of prompt-basedmethods for generating keyphrases for Russian scientific abstracts. First, wecompare the performance of zero-shot and few-shot prompt-based methods,fine-tuned models, and unsupervised methods. Then we assess strategies forselecting keyphrase examples in a few-shot setting. We present the outcomes ofhuman evaluation of the generated keyphrases and analyze the strengths andweaknesses of the models through expert assessment. Our results suggest thatprompt-based methods can outperform common baselines even using simple textprompts.</description><author>Anna Glazkova, Dmitry Morozov, Timur Garipov</author><pubDate>Wed, 23 Oct 2024 17:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18040v1</guid></item><item><title>POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference</title><link>http://arxiv.org/abs/2410.18038v1</link><description>Each request in LLM inference goes through two phases: compute-bound prefilland memory-bandwidth-bound decode. To improve GPU utilization, recent systemsuse hybrid batching that combines the prefill and decode phases of differentrequests into the same batch. Hybrid batching works well for linear operationsas it amortizes the cost of loading model weights from HBM. However, attentioncomputation in hybrid batches remains inefficient because existing attentionkernels are optimized for either prefill or decode. In this paper, we present POD-Attention -- the first GPU kernel thatefficiently computes attention for hybrid batches. POD-Attention aims tomaximize the utilization of both compute and memory bandwidth by carefullyallocating the GPU's resources such that prefill and decode operations happenconcurrently on the same multiprocessor. We integrate POD-Attention in astate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds upattention computation by up to 75% (mean 28%) and increases LLM servingthroughput by up to 22% in offline inference. In online inference,POD-Attention enables lower time-to-first-token (TTFT), time-between-tokens(TBT), and request execution latency versus Sarathi-Serve.</description><author>Aditya K Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee, Ashish Panwar</author><pubDate>Wed, 23 Oct 2024 17:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18038v1</guid></item><item><title>MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning</title><link>http://arxiv.org/abs/2410.18035v1</link><description>Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants arehighly effective parameter-efficient fine-tuning (PEFT) methods. However, theyintroduce significant latency in multi-tenant settings due to the LoRA modulesand MOE routers added to multiple linear modules in the Transformer layer. Toaddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a noveland efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methodsby considering each LoRA module as an expert and employing a prompt-awarerouting mechanism. This mechanism calculates expert routing results once beforegenerating the first new token and reuses these results for subsequent tokens,reducing latency. Extensive experiments and analysis on commonsense reasoningtasks, math reasoning tasks, and widely used LLM evaluation benchmarksdemonstrate that MiLoRA consistently outperforms strong PEFT baselines withcomparable tunable parameter budgets. Additionally, MiLoRA significantlyreduces latency in multi-tenant settings compared to previous LoRA-basedmethods.</description><author>Jingfan Zhang, Yi Zhao, Dan Chen, Xing Tian, Huanran Zheng, Wei Zhu</author><pubDate>Wed, 23 Oct 2024 17:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18035v1</guid></item><item><title>GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2410.18032v1</link><description>Graphs are widely used for modeling relational data in real-world scenarios,such as social networks and urban computing. Existing LLM-based graph analysisapproaches either integrate graph neural networks (GNNs) for specific machinelearning tasks, limiting their transferability, or rely solely on LLMs'internal reasoning ability, resulting in suboptimal performance. To addressthese limitations, we take advantage of recent advances in LLM-based agents,which have shown capabilities of utilizing external knowledge or tools forproblem solving. By simulating human problem-solving strategies such as analogyand collaboration, we propose a multi-agent system based on LLMs namedGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents fromthree modules, and the agents with different specialities can collaborate witheach other to address complex problems. Specifically, (1) input-outputnormalization module: the question agent extracts and refines four keyarguments from the original question, facilitating the problem understanding,and the answer agent organizes the results to meet the output requirement; (2)external knowledge retrieval module: we first build a knowledge base consistingof relevant documentation and experience information, and then the search agentretrieves the most relevant entries for each question. (3) problem-solvingmodule: given the retrieved information from search agent, the coding agentuses established algorithms via programming to generate solutions, and in casethe coding agent does not work, the reasoning agent will directly compute theresults without programming. Extensive experiments on six graph analysisbenchmarks demonstrate that GraphTeam achieves state-of-the-art performancewith an average 25.85% improvement over the best baseline in terms of accuracy.The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.</description><author>Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang</author><pubDate>Wed, 23 Oct 2024 17:02:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18032v1</guid></item><item><title>Exploring Large Language Models for Feature Selection: A Data-centric Perspective</title><link>http://arxiv.org/abs/2408.12025v2</link><description>The rapid advancement of Large Language Models (LLMs) has significantlyinfluenced various domains, leveraging their exceptional few-shot and zero-shotlearning capabilities. In this work, we aim to explore and understand theLLMs-based feature selection methods from a data-centric perspective. We beginby categorizing existing feature selection methods with LLMs into two groups:data-driven feature selection which requires numerical values of samples to dostatistical inference and text-based feature selection which utilizes priorknowledge of LLMs to do semantical associations using descriptive context. Weconduct experiments in both classification and regression tasks with LLMs invarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize theeffectiveness and robustness of text-based feature selection methods andshowcase their potentials using a real-world medical application. We alsodiscuss the challenges and future opportunities in employing LLMs for featureselection, offering insights for further research and development in thisemerging field.</description><author>Dawei Li, Zhen Tan, Huan Liu</author><pubDate>Wed, 23 Oct 2024 17:01:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12025v2</guid></item><item><title>Cross-lingual Transfer of Reward Models in Multilingual Alignment</title><link>http://arxiv.org/abs/2410.18027v1</link><description>Reinforcement learning with human feedback (RLHF) is shown to largely benefitfrom precise reward models (RMs). However, recent studies in reward modelingschemes are skewed towards English, limiting the applicability of RLHF inmultilingual alignments. In this work, we investigate the cross-lingualtransfer of RMs trained in diverse languages, primarily from English. Ourexperimental results demonstrate the strong cross-lingual transfer of EnglishRMs, exceeding target language RMs by 3~4% average increase in MultilingualRewardBench. Furthermore, we analyze the cross-lingual transfer of RMs throughthe representation shifts. Finally, we perform multilingual alignment toexemplify how cross-lingual transfer in RM propagates to enhanced multilingualinstruction-following capability, along with extensive analyses onoff-the-shelf RMs. We release the code, model, and data.</description><author>Jiwoo Hong, Noah Lee, Rodrigo Martínez-Castaño, César Rodríguez, James Thorne</author><pubDate>Wed, 23 Oct 2024 17:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18027v1</guid></item><item><title>Scalable Ranked Preference Optimization for Text-to-Image Generation</title><link>http://arxiv.org/abs/2410.18013v1</link><description>Direct Preference Optimization (DPO) has emerged as a powerful approach toalign text-to-image (T2I) models with human feedback. Unfortunately, successfulapplication of DPO to T2I models requires a huge amount of resources to collectand label large-scale datasets, e.g., millions of generated paired imagesannotated with human preferences. In addition, these human preference datasetscan get outdated quickly as the rapid improvements of T2I models lead to higherquality images. In this work, we investigate a scalable approach for collectinglarge-scale and fully synthetic datasets for DPO training. Specifically, thepreferences for paired images are generated using a pre-trained rewardfunction, eliminating the need for involving humans in the annotation process,greatly improving the dataset collection efficiency. Moreover, we demonstratethat such datasets allow averaging predictions across multiple models andcollecting ranked preferences as opposed to pairwise preferences. Furthermore,we introduce RankDPO to enhance DPO-based methods using the ranking feedback.Applying RankDPO on SDXL and SD3-Medium models with our synthetically generatedpreference dataset ``Syn-Pic'' improves both prompt-following (on benchmarkslike T2I-Compbench, GenEval, and DPG-Bench) and visual quality (through userstudies). This pipeline presents a practical and scalable solution to developbetter preference datasets to enhance the performance of text-to-image models.</description><author>Shyamgopal Karthik, Huseyin Coskun, Zeynep Akata, Sergey Tulyakov, Jian Ren, Anil Kag</author><pubDate>Wed, 23 Oct 2024 16:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18013v1</guid></item><item><title>VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</title><link>http://arxiv.org/abs/2409.04429v2</link><description>VILA-U is a Unified foundation model that integrates Video, Image, Languageunderstanding and generation. Traditional visual language models (VLMs) useseparate modules for understanding and generating visual content, which canlead to misalignment and increased complexity. In contrast, VILA-U employs asingle autoregressive next-token prediction framework for both tasks,eliminating the need for additional components like diffusion models. Thisapproach not only simplifies the model but also achieves near state-of-the-artperformance in visual language understanding and generation. The success ofVILA-U is attributed to two main factors: the unified vision tower that alignsdiscrete visual tokens with textual inputs during pretraining, which enhancesvisual perception, and autoregressive image generation can achieve similarquality as diffusion models with high-quality dataset. This allows VILA-U toperform comparably to more complex models using a fully token-basedautoregressive framework.</description><author>Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu</author><pubDate>Wed, 23 Oct 2024 16:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04429v2</guid></item><item><title>STAR: SocioTechnical Approach to Red Teaming Language Models</title><link>http://arxiv.org/abs/2406.11757v4</link><description>This research introduces STAR, a sociotechnical framework that improves oncurrent best practices for red teaming safety of large language models. STARmakes two key contributions: it enhances steerability by generatingparameterised instructions for human red teamers, leading to improved coverageof the risk surface. Parameterised instructions also provide more detailedinsights into model failures at no increased cost. Second, STAR improves signalquality by matching demographics to assess harms for specific groups, resultingin more sensitive annotations. STAR further employs a novel step of arbitrationto leverage diverse viewpoints and improve label reliability, treatingdisagreement not as noise but as a valuable contribution to signal quality.</description><author>Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac</author><pubDate>Wed, 23 Oct 2024 16:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11757v4</guid></item><item><title>JointMotion: Joint Self-Supervision for Joint Motion Prediction</title><link>http://arxiv.org/abs/2403.05489v2</link><description>We present JointMotion, a self-supervised pre-training method for jointmotion prediction in self-driving vehicles. Our method jointly optimizes ascene-level objective connecting motion and environments, and an instance-levelobjective to refine learned representations. Scene-level representations arelearned via non-contrastive similarity learning of past motion sequences andenvironment context. At the instance level, we use masked autoencoding torefine multimodal polyline representations. We complement this with an adaptivepre-training decoder that enables JointMotion to generalize across differentenvironment representations, fusion mechanisms, and dataset characteristics.Notably, our method reduces the joint final displacement error of Wayformer,HPTR, and Scene Transformer models by 3\%, 8\%, and 12\%, respectively; andenables transfer learning between the Waymo Open Motion and the Argoverse 2Motion Forecasting datasets. Code: https://github.com/kit-mrt/future-motion</description><author>Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez</author><pubDate>Wed, 23 Oct 2024 16:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05489v2</guid></item><item><title>Counter-Current Learning: A Biologically Plausible Dual Network Approach for Deep Learning</title><link>http://arxiv.org/abs/2409.19841v2</link><description>Despite its widespread use in neural networks, error backpropagation hasfaced criticism for its lack of biological plausibility, suffering from issuessuch as the backward locking problem and the weight transport problem. Theselimitations have motivated researchers to explore more biologically plausiblelearning algorithms that could potentially shed light on how biological neuralsystems adapt and learn. Inspired by the counter-current exchange mechanismsobserved in biological systems, we propose counter-current learning (CCL), abiologically plausible framework for credit assignment in neural networks. Thisframework employs a feedforward network to process input data and a feedbacknetwork to process targets, with each network enhancing the other throughanti-parallel signal propagation. By leveraging the more informative signalsfrom the bottom layer of the feedback network to guide the updates of the toplayer of the feedforward network and vice versa, CCL enables the simultaneoustransformation of source inputs to target outputs and the dynamic mutualinfluence of these transformations. Experimental results on MNIST,FashionMNIST, CIFAR10, and CIFAR100 datasets using multi-layer perceptrons andconvolutional neural networks demonstrate that CCL achieves comparableperformance to other biologically plausible algorithms while offering a morebiologically realistic learning mechanism. Furthermore, we showcase theapplicability of our approach to an autoencoder task, underscoring itspotential for unsupervised representation learning. Our work presents adirection for biologically inspired and plausible learning algorithms, offeringan alternative mechanism of learning and adaptation in neural networks.</description><author>Chia-Hsiang Kao, Bharath Hariharan</author><pubDate>Wed, 23 Oct 2024 16:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19841v2</guid></item><item><title>Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning</title><link>http://arxiv.org/abs/2409.17270v2</link><description>Large Language Models (LLMs) have revolutionized natural language processing,yet they struggle with inconsistent reasoning, particularly in novel domainsand complex logical sequences. This research introduces Proof of Thought, aframework that enhances the reliability and transparency of LLM outputs. Ourapproach bridges LLM-generated ideas with formal logic verification, employinga custom interpreter to convert LLM outputs into First Order Logic constructsfor theorem prover scrutiny. Central to our method is an intermediaryJSON-based Domain-Specific Language, which by design balances precise logicalstructures with intuitive human concepts. This hybrid representation enablesboth rigorous validation and accessible human comprehension of LLM reasoningprocesses. Key contributions include a robust type system with sort managementfor enhanced logical integrity, explicit representation of rules for cleardistinction between factual and inferential knowledge, and a flexiblearchitecture that allows for easy extension to various domain-specificapplications. We demonstrate Proof of Thought's effectiveness throughbenchmarking on StrategyQA and a novel multimodal reasoning task, showingimproved performance in open-ended scenarios. By providing verifiable andinterpretable results, our technique addresses critical needs for AI systemaccountability and sets a foundation for human-in-the-loop oversight inhigh-stakes domains.</description><author>Debargha Ganguly, Srinivasan Iyengar, Vipin Chaudhary, Shivkumar Kalyanaraman</author><pubDate>Wed, 23 Oct 2024 16:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17270v2</guid></item><item><title>Inferring stability properties of chaotic systems on autoencoders' latent spaces</title><link>http://arxiv.org/abs/2410.18003v1</link><description>The data-driven learning of solutions of partial differential equations canbe based on a divide-and-conquer strategy. First, the high dimensional data iscompressed to a latent space with an autoencoder; and, second, the temporaldynamics are inferred on the latent space with a form of recurrent neuralnetwork. In chaotic systems and turbulence, convolutional autoencoders and echostate networks (CAE-ESN) successfully forecast the dynamics, but little isknown about whether the stability properties can also be inferred. We show thatthe CAE-ESN model infers the invariant stability properties and the geometry ofthe tangent space in the low-dimensional manifold (i.e. the latent space)through Lyapunov exponents and covariant Lyapunov vectors. This work opens upnew opportunities for inferring the stability of high-dimensional chaoticsystems in latent spaces.</description><author>Elise Özalp, Luca Magri</author><pubDate>Wed, 23 Oct 2024 16:25:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18003v1</guid></item><item><title>Telling Stories for Common Sense Zero-Shot Action Recognition</title><link>http://arxiv.org/abs/2309.17327v2</link><description>Video understanding has long suffered from reliance on large labeleddatasets, motivating research into zero-shot learning. Recent progress inlanguage modeling presents opportunities to advance zero-shot video analysis,but constructing an effective semantic space relating action classes remainschallenging. We address this by introducing a novel dataset, Stories, whichcontains rich textual descriptions for diverse action classes extracted fromWikiHow articles. For each class, we extract multi-sentence narrativesdetailing the necessary steps, scenes, objects, and verbs that characterize theaction. This contextual data enables modeling of nuanced relationships betweenactions, paving the way for zero-shot transfer. We also propose an approachthat harnesses Stories to improve feature generation for training zero-shotclassification. Without any target dataset fine-tuning, our method achieves newstate-of-the-art on multiple benchmarks, improving top-1 accuracy by up to6.1%. We believe Stories provides a valuable resource that can catalyzeprogress in zero-shot action recognition. The textual narratives forgeconnections between seen and unseen classes, overcoming the bottleneck oflabeled data that has long impeded advancements in this exciting domain. Thedata can be found here: https://github.com/kini5gowda/Stories .</description><author>Shreyank N Gowda, Laura Sevilla-Lara</author><pubDate>Wed, 23 Oct 2024 16:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17327v2</guid></item><item><title>Benchmarking Foundation Models on Exceptional Cases: Dataset Creation and Validation</title><link>http://arxiv.org/abs/2410.18001v1</link><description>Foundation models (FMs) have achieved significant success across varioustasks, leading to research on benchmarks for reasoning abilities. However,there is a lack of studies on FMs performance in exceptional scenarios, whichwe define as out-of-distribution (OOD) reasoning tasks. This paper is the firstto address these cases, developing a novel dataset for evaluation of FMs acrossmultiple modalities, including graphic novels, calligraphy, news articles, andlyrics. It includes tasks for instance classification, character recognition,token prediction, and text generation. The paper also proposes promptengineering techniques like Chain-of-Thought (CoT) and CoT+Few-Shot to enhanceperformance. Validation of FMs using various methods revealed improvements. Thecode repository is accessible at:https://github.com/MLAI-Yonsei/ExceptionalBenchmark</description><author>Suho Kang, Jungyang Park, Joonseo Ha, SoMin Kim, JinHyeong Kim, Subeen Park, Kyungwoo Song</author><pubDate>Wed, 23 Oct 2024 16:24:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18001v1</guid></item><item><title>Generalizable Prompt Tuning for Vision-Language Models</title><link>http://arxiv.org/abs/2410.03189v2</link><description>Prompt tuning for vision-language models such as CLIP involves optimizing thetext prompts used to generate image-text pairs for specific downstream tasks.While hand-crafted or template-based prompts are generally applicable to awider range of unseen classes, they tend to perform poorly in downstream tasks(i.e., seen classes). Learnable soft prompts, on the other hand, often performwell in downstream tasks but lack generalizability. Additionally, priorresearch has predominantly concentrated on the textual modality, with very fewstudies attempting to explore the prompt's generalization potential from thevisual modality. Keeping these limitations in mind, we investigate how toprompt tuning to obtain both a competitive downstream performance andgeneralization. The study shows that by treating soft and hand-crafted promptsas dual views of the textual modality, and maximizing their mutual information,we can better ensemble task-specific and general semantic information.Moreover, to generate more expressive prompts, the study introduces aclass-wise augmentation from the visual modality, resulting in significantrobustness to a wider range of unseen classes. Extensive evaluations on severalbenchmarks report that the proposed approach achieves competitive results interms of both task-specific performance and general abilities.</description><author>Qian Zhang</author><pubDate>Wed, 23 Oct 2024 16:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03189v2</guid></item><item><title>AlleNoise: large-scale text classification benchmark dataset with real-world label noise</title><link>http://arxiv.org/abs/2407.10992v2</link><description>Label noise remains a challenge for training robust classification models.Most methods for mitigating label noise have been benchmarked using primarilydatasets with synthetic noise. While the need for datasets with realistic noisedistribution has partially been addressed by web-scraped benchmarks such asWebVision and Clothing1M, those benchmarks are restricted to the computervision domain. With the growing importance of Transformer-based models, it iscrucial to establish text classification benchmarks for learning with noisylabels. In this paper, we present AlleNoise, a new curated text classificationbenchmark dataset with real-world instance-dependent label noise, containingover 500,000 examples across approximately 5,600 classes, complemented with ameaningful, hierarchical taxonomy of categories. The noise distribution comesfrom actual users of a major e-commerce marketplace, so it realisticallyreflects the semantics of human mistakes. In addition to the noisy labels, weprovide human-verified clean labels, which help to get a deeper insight intothe noise distribution, unlike web-scraped datasets typically used in thefield. We demonstrate that a representative selection of established methodsfor learning with noisy labels is inadequate to handle such real-world noise.In addition, we show evidence that these algorithms do not alleviate excessivememorization. As such, with AlleNoise, we set the bar high for the developmentof label noise methods that can handle real-world label noise in textclassification tasks. The code and dataset are available for download athttps://github.com/allegro/AlleNoise.</description><author>Alicja Rączkowska, Aleksandra Osowska-Kurczab, Jacek Szczerbiński, Kalina Jasinska-Kobus, Klaudia Nazarko</author><pubDate>Wed, 23 Oct 2024 16:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10992v2</guid></item><item><title>Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices</title><link>http://arxiv.org/abs/2410.17998v1</link><description>Analyzing the structure of sampled features from an input data distributionis challenging when constrained by limited measurements in both the number ofinputs and features. Traditional approaches often rely on the eigenvaluespectrum of the sample covariance matrix derived from finite measurementmatrices; however, these spectra are sensitive to the size of the measurementmatrix, leading to biased insights. In this paper, we introduce a novelalgorithm that provides unbiased estimates of the spectral moments of thekernel integral operator in the limit of infinite inputs and features fromfinitely sampled measurement matrices. Our method, based upon dynamicprogramming, is efficient and capable of estimating the moments of the operatorspectrum. We demonstrate the accuracy of our estimator on radial basis function(RBF) kernels, highlighting its consistency with the theoretical spectra.Furthermore, we showcase the practical utility and robustness of our method inunderstanding the geometry of learned representations in neural networks.</description><author>Chanwoo Chun, SueYeon Chung, Daniel D. Lee</author><pubDate>Wed, 23 Oct 2024 16:12:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17998v1</guid></item><item><title>Annotator-Centric Active Learning for Subjective NLP Tasks</title><link>http://arxiv.org/abs/2404.15720v4</link><description>Active Learning (AL) addresses the high costs of collecting human annotationsby strategically annotating the most informative samples. However, forsubjective NLP tasks, incorporating a wide range of perspectives in theannotation process is crucial to capture the variability in human judgments. Weintroduce Annotator-Centric Active Learning (ACAL), which incorporates anannotator selection strategy following data sampling. Our objective istwo-fold: 1) to efficiently approximate the full diversity of human judgments,and 2) to assess model performance using annotator-centric metrics, which valueminority and majority perspectives equally. We experiment with multipleannotator selection strategies across seven subjective NLP tasks, employingboth traditional and novel, human-centered evaluation metrics. Our findingsindicate that ACAL improves data efficiency and excels in annotator-centricperformance evaluations. However, its success depends on the availability of asufficiently large and diverse pool of annotators to sample from.</description><author>Michiel van der Meer, Neele Falk, Pradeep K. Murukannaiah, Enrico Liscio</author><pubDate>Wed, 23 Oct 2024 16:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15720v4</guid></item><item><title>Characterization of the multiplicity of solutions for camera pose given two vertically-aligned landmarks and accelerometer</title><link>http://arxiv.org/abs/2410.17997v1</link><description>We consider the problem of recovering the position and orientation of acamera equipped with an accelerometer from sensor images of two labeledlandmarks whose positions in a coordinate system aligned in a known way withgravity are known. This a variant on the much studied P$n$P problem ofrecovering camera position and orientation from $n$ points without anygravitational data. It is proved that in three types of singular cases thereare infinitely many solutions, in another type of case there is one, and in afinal type of case there are two. A precise characterization of each type ofcase. In particular, there is always a unique solution in the practicallyinteresting case where the two landmarks are at the same altitude and thecamera is at a different altitude. This case is studied by numerical simulationand an implementation on a consumer cellphone. It is also proved that if thetwo landmarks are unlabeled, then apart from the same singular cases, there arestill always one or two solutions.</description><author>Alexander R. Pruss</author><pubDate>Wed, 23 Oct 2024 16:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17997v1</guid></item><item><title>Robust Variable Selection for High-dimensional Regression with Missing Data and Measurement Errors</title><link>http://arxiv.org/abs/2410.16722v2</link><description>In our paper,we focus on robust variable selection for missing data andmeasurement error.Missing data and measurement errors can lead to confusingdata distribution.We propose an exponential loss function with tuning parameterto apply to Missing and measurement errors data.By adjusting the parameter,theloss functioncan be better and more robust under various different datadistributions.We use inverse probability weighting and additivityerrormodels toaddress missing data and measurement errors.Also,we find that the Atanpunishment method works better.We used Monte Carlo simulations to assess thevalidity of robust variable selection and validated our findings with thebreast cancer dataset</description><author>Zhenhao Zhang</author><pubDate>Wed, 23 Oct 2024 16:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16722v2</guid></item><item><title>AI driven health recommender</title><link>http://arxiv.org/abs/2410.17991v1</link><description>As AI emerged as highest valued technology, We used that to create a webapplication that makes a patient work easier .It detects the disease name basedon the symptoms given by the patient and recommends medication for respectivedisease, precautions to take, diet to follow and workouts to do, so the diseasecan be minimized. The web application is made with clean and Realtime data byusing Machine learning as root. We used flask to create a user-friendlyplatform.</description><author>K. Vignesh, B. Pranavi, Ch. Sreenidhi</author><pubDate>Wed, 23 Oct 2024 16:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17991v1</guid></item><item><title>Exploring the Adversarial Robustness of CLIP for AI-generated Image Detection</title><link>http://arxiv.org/abs/2407.19553v2</link><description>In recent years, many forensic detectors have been proposed to detectAI-generated images and prevent their use for malicious purposes. Convolutionalneural networks (CNNs) have long been the dominant architecture in this fieldand have been the subject of intense study. However, recently proposedTransformer-based detectors have been shown to match or even outperformCNN-based detectors, especially in terms of generalization. In this paper, westudy the adversarial robustness of AI-generated image detectors, focusing onContrastive Language-Image Pretraining (CLIP)-based methods that rely on VisualTransformer (ViT) backbones and comparing their performance with CNN-basedmethods. We study the robustness to different adversarial attacks under avariety of conditions and analyze both numerical results and frequency-domainpatterns. CLIP-based detectors are found to be vulnerable to white-box attacksjust like CNN-based detectors. However, attacks do not easily transfer betweenCNN-based and CLIP-based methods. This is also confirmed by the differentdistribution of the adversarial noise patterns in the frequency domain.Overall, this analysis provides new insights into the properties of forensicdetectors that can help to develop more effective strategies.</description><author>Vincenzo De Rosa, Fabrizio Guillaro, Giovanni Poggi, Davide Cozzolino, Luisa Verdoliva</author><pubDate>Wed, 23 Oct 2024 16:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19553v2</guid></item><item><title>CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting</title><link>http://arxiv.org/abs/2406.02131v4</link><description>Dataset condensation is a newborn technique that generates a small datasetthat can be used in training deep neural networks to lower training costs. Theobjective of dataset condensation is to ensure that the model trained with thesynthetic dataset can perform comparably to the model trained with fulldatasets. However, existing methods predominantly concentrate on classificationtasks, posing challenges in their adaptation to time series forecasting(TS-forecasting). This challenge arises from disparities in the evaluation ofsynthetic data. In classification, the synthetic data is consideredwell-distilled if the model trained with the full dataset and the model trainedwith the synthetic dataset yield identical labels for the same input,regardless of variations in output logits distribution. Conversely, inTS-forecasting, the effectiveness of synthetic data distillation is determinedby the distance between predictions of the two models. The synthetic data isdeemed well-distilled only when all data points within the predictions aresimilar. Consequently, TS-forecasting has a more rigorous evaluationmethodology compared to classification. To mitigate this gap, we theoreticallyanalyze the optimization objective of dataset condensation for TS-forecastingand propose a new one-line plugin of dataset condensation designated as DatasetCondensation for Time Series Forecasting (CondTSF) based on our analysis.Plugging CondTSF into previous dataset condensation methods facilitates areduction in the distance between the predictions of the model trained with thefull dataset and the model trained with the synthetic dataset, therebyenhancing performance. We conduct extensive experiments on eight commonly usedtime series datasets. CondTSF consistently improves the performance of allprevious dataset condensation methods across all datasets, particularly at lowcondensing ratios.</description><author>Jianrong Ding, Zhanyu Liu, Guanjie Zheng, Haiming Jin, Linghe Kong</author><pubDate>Wed, 23 Oct 2024 16:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02131v4</guid></item><item><title>Learning a quantum computer's capability</title><link>http://arxiv.org/abs/2304.10650v2</link><description>Accurately predicting a quantum computer's capability -- which circuits itcan run and how well it can run them -- is a foundational goal of quantumcharacterization and benchmarking. As modern quantum computers becomeincreasingly hard to simulate, we must develop accurate and scalable predictivecapability models to help researchers and stakeholders decide which quantumcomputers to build and use. In this work, we propose a hardware-agnostic methodto efficiently construct scalable predictive models of a quantum computer'scapability for almost any class of circuits, and demonstrate our method usingconvolutional neural networks (CNNs). Our CNN-based approach works byefficiently representing a circuit as a three-dimensional tensor and then usinga CNN to predict its success rate. Our CNN capability models obtainapproximately a $1\%$ average absolute prediction error when modelingprocessors experiencing both Markovian and non-Markovian stochastic Paulierrors. We also apply our CNNs to model the capabilities of cloud-accessquantum computing systems, obtaining moderate prediction accuracy (averageabsolute error around $2-5\%$), and we highlight the challenges to buildingbetter neural network capability models.</description><author>Daniel Hothem, Kevin Young, Tommie Catanach, Timothy Proctor</author><pubDate>Wed, 23 Oct 2024 16:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10650v2</guid></item><item><title>A Pipeline for Segmenting and Structuring RGB-D Data for Robotics Applications</title><link>http://arxiv.org/abs/2410.17988v1</link><description>We introduce a novel pipeline for segmenting and structuring color and depth(RGB-D) data. Existing processing pipelines for RGB-D data have focused onextracting geometric information alone. This approach precludes the developmentof more advanced robotic navigation and manipulation algorithms, which benefitfrom a semantic understanding of their environment. Our pipeline can segmentRGB-D data into accurate semantic masks. These masks are then used to fuse rawcaptured point clouds into semantically separated point clouds. We store thisinformation using the Universal Scene Description (USD) file format, a formatsuitable for easy querying by downstream robotics algorithms, human-friendlyvisualization, and robotics simulation.</description><author>Zhiwu Zheng, Lauren Mentzer, Berk Iskender, Michael Price, Colm Prendergast, Audren Cloitre</author><pubDate>Wed, 23 Oct 2024 16:01:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17988v1</guid></item><item><title>Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data</title><link>http://arxiv.org/abs/2410.17986v1</link><description>Federated Learning (FL) is an evolving paradigm that enables multiple partiesto collaboratively train models without sharing raw data. Among its variants,Vertical Federated Learning (VFL) is particularly relevant in real-world,cross-organizational collaborations, where distinct features of a sharedinstance group are contributed by different parties. In these scenarios,parties are often linked using fuzzy identifiers, leading to a common practicetermed as multi-party fuzzy VFL. Existing models generally address eithermulti-party VFL or fuzzy VFL between two parties. Extending these models topractical multi-party fuzzy VFL typically results in significant performancedegradation and increased costs for maintaining privacy. To overcome theselimitations, we introduce the Federated Transformer (FeT), a novel frameworkthat supports multi-party VFL with fuzzy identifiers. FeT innovatively encodesthese identifiers into data representations and employs a transformerarchitecture distributed across different parties, incorporating three newtechniques to enhance performance. Furthermore, we have developed a multi-partyprivacy framework for VFL that integrates differential privacy with securemulti-party computation, effectively protecting local representations whileminimizing associated utility costs. Our experiments demonstrate that the FeTsurpasses the baseline models by up to 46\% in terms of accuracy when scaled to50 parties. Additionally, in two-party fuzzy VFL settings, FeT also showsimproved performance and privacy over cutting-edge VFL models.</description><author>Zhaomin Wu, Junyi Hou, Yiqun Diao, Bingsheng He</author><pubDate>Wed, 23 Oct 2024 16:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17986v1</guid></item><item><title>Robust Two-View Geometry Estimation with Implicit Differentiation</title><link>http://arxiv.org/abs/2410.17983v1</link><description>We present a novel two-view geometry estimation framework which is based on adifferentiable robust loss function fitting. We propose to treat the robustfundamental matrix estimation as an implicit layer, which allows us to avoidbackpropagation through time and significantly improves the numericalstability. To take full advantage of the information from the feature matchingstage we incorporate learnable weights that depend on the matching confidences.In this way our solution brings together feature extraction, matching andtwo-view geometry estimation in a unified end-to-end trainable pipeline. Weevaluate our approach on the camera pose estimation task in both outdoor andindoor scenarios. The experiments on several datasets show that the proposedmethod outperforms both classic and learning-based state-of-the-art methods bya large margin. The project webpage is available at:https://github.com/VladPyatov/ihls</description><author>Vladislav Pyatov, Iaroslav Koshelev, Stamatis Lefkimmiatis</author><pubDate>Wed, 23 Oct 2024 15:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17983v1</guid></item><item><title>Stick-breaking Attention</title><link>http://arxiv.org/abs/2410.17980v1</link><description>The self-attention mechanism traditionally relies on the softmax operator,necessitating positional embeddings like RoPE, or position biases to accountfor token order. But current methods using still face length generalisationchallenges. We propose an alternative attention mechanism based on thestick-breaking process: For each token before the current, we determine a breakpoint $\beta_{i,j}$, which represents the proportion of the remaining stick toallocate to the current token. We repeat the process until the stick is fullyallocated, resulting in a sequence of attention weights. This process naturallyincorporates recency bias, which has linguistic motivations for grammar parsing(Shen et. al., 2017). We study the implications of replacing the conventionalsoftmax-based attention mechanism with stick-breaking attention. We thendiscuss implementation of numerically stable stick-breaking attention and adaptFlash Attention to accommodate this mechanism. When used as a drop-inreplacement for current softmax+RoPE attention systems, we find thatstick-breaking attention performs competitively with current methods on lengthgeneralisation and downstream tasks. Stick-breaking also performs well atlength generalisation, allowing a model trained with $2^{11}$ context window toperform well at $2^{14}$ with perplexity improvements.</description><author>Shawn Tan, Yikang Shen, Songlin Yang, Aaron Courville, Rameswar Panda</author><pubDate>Wed, 23 Oct 2024 15:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17980v1</guid></item><item><title>Federated Class-Incremental Learning with Hierarchical Generative Prototypes</title><link>http://arxiv.org/abs/2406.02447v3</link><description>Federated Learning (FL) aims at unburdening the training of deep models bydistributing computation across multiple devices (clients) while safeguardingdata privacy. On top of that, Federated Continual Learning (FCL) also accountsfor data distribution evolving over time, mirroring the dynamic nature ofreal-world environments. While previous studies have identified CatastrophicForgetting and Client Drift as primary causes of performance degradation inFCL, we shed light on the importance of Incremental Bias and Federated Bias,which cause models to prioritize classes that are recently introduced orlocally predominant, respectively. Our proposal constrains both biases in thelast layer by efficiently finetuning a pre-trained backbone using learnableprompts, resulting in clients that produce less biased representations and morebiased classifiers. Therefore, instead of solely relying on parameteraggregation, we leverage generative prototypes to effectively balance thepredictions of the global model. Our method significantly improves the currentState Of The Art, providing an average increase of +7.8% in accuracy. Code toreproduce the results is provided in the suppl. material.</description><author>Riccardo Salami, Pietro Buzzega, Matteo Mosconi, Mattia Verasani, Simone Calderara</author><pubDate>Wed, 23 Oct 2024 15:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02447v3</guid></item><item><title>Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration</title><link>http://arxiv.org/abs/2410.14979v2</link><description>Despite their proficiency in math tasks, the mechanisms underlying LLMs'mathematical reasoning abilities remain a subject of debate. Recent studiessuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoningby encouraging LLMs to employ human-like logical reasoning (System 2), enablingthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMsgenuinely possess System 2-like logical reasoning, we introduced targetedmodifications to CRT problems. Our findings reveal that, despite the use of CoTprompts, mainstream LLMs, including the latest o1-preview model, continue toexhibit a significant error rate. Further analysis indicates that theypredominantly rely on System 1-like intuitive reasoning and pattern matchingderived from training data, rather than demonstrating mastery of mathematicalthinking. This discovery challenges the prevailing notion that LLMs possessgenuine logical reasoning abilities and that CoT can enhance them.Consequently, this work may temper overly optimistic projections regardingLLMs' advancement toward artificial general intelligence.</description><author>Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Baosheng Wang, Jinshu Su</author><pubDate>Wed, 23 Oct 2024 15:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14979v2</guid></item><item><title>metasnf: Meta Clustering with Similarity Network Fusion in R</title><link>http://arxiv.org/abs/2410.17976v1</link><description>metasnf is an R package that enables users to apply meta clustering, a methodfor efficiently searching a broad space of cluster solutions by clustering thesolutions themselves, to clustering workflows based on similarity networkfusion (SNF). SNF is a multi-modal data integration algorithm commonly used forbiomedical subtype discovery. The package also contains functions to assistwith cluster visualization, characterization, and validation. This package canhelp researchers identify SNF-derived cluster solutions that are guided bycontext-specific utility over context-agnostic measures of quality.</description><author>Prashanth S Velayudhan, Xiaoqiao Xu, Prajkta Kallurkar, Ana Patricia Balbon, Maria T Secara, Adam Taback, Denise Sabac, Nicholas Chan, Shihao Ma, Bo Wang, Daniel Felsky, Stephanie H Ameis, Brian Cox, Colin Hawco, Lauren Erdman, Anne L Wheeler</author><pubDate>Wed, 23 Oct 2024 15:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17976v1</guid></item><item><title>Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages</title><link>http://arxiv.org/abs/2410.17973v1</link><description>This exploratory study investigates the potential of multilingual AutomaticPost-Editing (APE) systems to enhance the quality of machine translations forlow-resource Indo-Aryan languages. Focusing on two closely related languagepairs, English-Marathi and English-Hindi, we exploit the linguisticsimilarities to develop a robust multilingual APE model. To facilitatecross-linguistic transfer, we generate synthetic Hindi-Marathi andMarathi-Hindi APE triplets. Additionally, we incorporate a Quality Estimation(QE)-APE multi-task learning framework. While the experimental resultsunderline the complementary nature of APE and QE, we also observe that QE-APEmultitask learning facilitates effective domain adaptation. Our experimentsdemonstrate that the multilingual APE models outperform their correspondingEnglish-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TERpoints, respectively, with further notable improvements over the multilingualAPE model observed through multi-task learning ($+1.29$ and $+1.44$ TERpoints), data augmentation ($+0.53$ and $+0.45$ TER points) and domainadaptation ($+0.35$ and $+0.45$ TER points). We release the synthetic data,code, and models accrued during this study publicly athttps://github.com/cfiltnlp/Multilingual-APE.</description><author>Sourabh Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya</author><pubDate>Wed, 23 Oct 2024 15:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17973v1</guid></item><item><title>Dependency Graph Parsing as Sequence Labeling</title><link>http://arxiv.org/abs/2410.17972v1</link><description>Various linearizations have been proposed to cast syntactic dependencyparsing as sequence labeling. However, these approaches do not support morecomplex graph-based representations, such as semantic dependencies or enhanceduniversal dependencies, as they cannot handle reentrancy or cycles. Byextending them, we define a range of unbounded and bounded linearizations thatcan be used to cast graph parsing as a tagging task, enlarging the toolbox ofproblems that can be solved under this paradigm. Experimental results onsemantic dependency and enhanced UD parsing show that with a good choice ofencoding, sequence-labeling dependency graph parsers combine high efficiencywith accuracies close to the state of the art, in spite of their simplicity.</description><author>Ana Ezquerro, David Vilares, Carlos Gómez-Rodríguez</author><pubDate>Wed, 23 Oct 2024 15:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17972v1</guid></item><item><title>Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning</title><link>http://arxiv.org/abs/2410.17971v1</link><description>Spectrum access is an essential problem in device-to-device (D2D)communications. However, with the recent growth in the number of mobiledevices, the wireless spectrum is becoming scarce, resulting in low spectralefficiency for D2D communications. To address this problem, this paper aims tointegrate the ambient backscatter communication technology into D2D devices toallow them to backscatter ambient RF signals to transmit their data when theshared spectrum is occupied by mobile users. To obtain the optimal spectrumaccess policy, i.e., stay idle or access the shared spectrum and perform activetransmissions or backscattering ambient RF signals for transmissions, tomaximize the average throughput for D2D users, deep reinforcement learning(DRL) can be adopted. However, DRL-based solutions may require long trainingtime due to the curse of dimensionality issue as well as complex deep neuralnetwork architectures. For that, we develop a novel quantum reinforcementlearning (RL) algorithm that can achieve a faster convergence rate with fewertraining parameters compared to DRL thanks to the quantum superposition andquantum entanglement principles. Specifically, instead of using conventionaldeep neural networks, the proposed quantum RL algorithm uses a parametrizedquantum circuit to approximate an optimal policy. Extensive simulations thendemonstrate that the proposed solution not only can significantly improve theaverage throughput of D2D devices when the shared spectrum is busy but also canachieve much better performance in terms of convergence rate and learningcomplexity compared to existing DRL-based methods.</description><author>Nguyen Van Huynh, Bolun Zhang, Dinh-Hieu Tran, Dinh Thai Hoang, Diep N. Nguyen, Gan Zheng, Dusit Niyato, Quoc-Viet Pham</author><pubDate>Wed, 23 Oct 2024 15:36:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17971v1</guid></item><item><title>Optical Generative Models</title><link>http://arxiv.org/abs/2410.17970v1</link><description>Generative models cover various application areas, including image, video andmusic synthesis, natural language processing, and molecular design, among manyothers. As digital generative models become larger, scalable inference in afast and energy-efficient manner becomes a challenge. Here, we present opticalgenerative models inspired by diffusion models, where a shallow and fastdigital encoder first maps random noise into phase patterns that serve asoptical generative seeds for a desired data distribution; a jointly-trainedfree-space-based reconfigurable decoder all-optically processes thesegenerative seeds to create novel images (never seen before) following thetarget data distribution. Except for the illumination power and the random seedgeneration through a shallow encoder, these optical generative models do notconsume computing power during the synthesis of novel images. We report theoptical generation of monochrome and multi-color novel images of handwrittendigits, fashion products, butterflies, and human faces, following the datadistributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets,respectively, achieving an overall performance comparable to digital neuralnetwork-based generative models. To experimentally demonstrate opticalgenerative models, we used visible light to generate, in a snapshot, novelimages of handwritten digits and fashion products. These optical generativemodels might pave the way for energy-efficient, scalable and rapid inferencetasks, further exploiting the potentials of optics and photonics for artificialintelligence-generated content.</description><author>Shiqi Chen, Yuhang Li, Hanlong Chen, Aydogan Ozcan</author><pubDate>Wed, 23 Oct 2024 15:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17970v1</guid></item><item><title>On the potential of Optimal Transport in Geospatial Data Science</title><link>http://arxiv.org/abs/2410.11709v2</link><description>Prediction problems in geographic information science and transportation areoften motivated by the possibility to enhance operational efficiency andthereby reduce emissions. Examples range from predicting car sharing demand forrelocation planning to forecasting traffic congestion for navigation purposes.However, conventional accuracy metrics ignore the spatial distribution of theerrors, despite its relevance for operations. Here, we put forward a spatiallyaware evaluation metric and loss function based on Optimal Transport (OT). Ourframework leverages partial OT and can minimize relocation costs in any spatialprediction problem. We showcase the advantages of OT-based evaluation overconventional metrics and further demonstrate the application of an OT lossfunction for improving forecasts of bike sharing demand and charging stationoccupancy. Thus, our framework not only aligns with operational considerations,but also signifies a step forward in refining predictions within geospatialapplications. All code is available at https://github.com/mie-lab/geospatialOT.</description><author>Nina Wiedemann, Théo Uscidda, Martin Raubal</author><pubDate>Wed, 23 Oct 2024 15:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11709v2</guid></item><item><title>POMDP-Driven Cognitive Massive MIMO Radar: Joint Target Detection-Tracking In Unknown Disturbances</title><link>http://arxiv.org/abs/2410.17967v1</link><description>The joint detection and tracking of a moving target embedded in an unknowndisturbance represents a key feature that motivates the development of thecognitive radar paradigm. Building upon recent advancements in robust targetdetection with multiple-input multiple-output (MIMO) radars, this work exploresthe application of a Partially Observable Markov Decision Process (POMDP)framework to enhance the tracking and detection tasks in a statisticallyunknown environment. In the POMDP setup, the radar system is considered as anintelligent agent that continuously senses the surrounding environment,optimizing its actions to maximize the probability of detection $(P_D)$ andimprove the target position and velocity estimation, all this while keeping aconstant probability of false alarm $(P_{FA})$. The proposed approach employsan online algorithm that does not require any apriori knowledge of the noisestatistics, and it relies on a much more general observation model than thetraditional range-azimuth-elevation model employed by conventional trackingalgorithms. Simulation results clearly show substantial performance improvementof the POMDP-based algorithm compared to the State-Action-Reward-State-Action(SARSA)-based one that has been recently investigated in the context of massiveMIMO (MMIMO) radar systems.</description><author>Imad Bouhou, Stefano Fortunati, Leila Gharsalli, Alexandre Renaux</author><pubDate>Wed, 23 Oct 2024 15:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17967v1</guid></item><item><title>A Wavelet Diffusion GAN for Image Super-Resolution</title><link>http://arxiv.org/abs/2410.17966v1</link><description>In recent years, diffusion models have emerged as a superior alternative togenerative adversarial networks (GANs) for high-fidelity image generation, withwide applications in text-to-image generation, image-to-image translation, andsuper-resolution. However, their real-time feasibility is hindered by slowtraining and inference speeds. This study addresses this challenge by proposinga wavelet-based conditional Diffusion GAN scheme for Single-ImageSuper-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm toreduce the timesteps required by the reverse diffusion process and the DiscreteWavelet Transform (DWT) to achieve dimensionality reduction, decreasingtraining and inference times significantly. The results of an experimentalvalidation on the CelebA-HQ dataset confirm the effectiveness of our proposedscheme. Our approach outperforms other state-of-the-art methodologiessuccessfully ensuring high-fidelity output while overcoming inherent drawbacksassociated with diffusion models in time-sensitive applications.</description><author>Lorenzo Aloisi, Luigi Sigillo, Aurelio Uncini, Danilo Comminiello</author><pubDate>Wed, 23 Oct 2024 15:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17966v1</guid></item><item><title>Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation</title><link>http://arxiv.org/abs/2408.15205v2</link><description>Promptable segmentation typically requires instance-specific manual promptsto guide the segmentation of each desired object. To minimize such a need,task-generic promptable segmentation has been introduced, which employs asingle task-generic prompt to segment various images of different objects inthe same task. Current methods use Multimodal Large Language Models (MLLMs) toreason detailed instance-specific prompts from a task-generic prompt forimproving segmentation accuracy. The effectiveness of this segmentation heavilydepends on the precision of these derived prompts. However, MLLMs often sufferhallucinations during reasoning, resulting in inaccurate prompting. Whileexisting methods focus on eliminating hallucinations to improve a model, weargue that MLLM hallucinations can reveal valuable contextual insights whenleveraged correctly, as they represent pre-trained large-scale knowledge beyondindividual images. In this paper, we utilize hallucinations to minetask-related information from images and verify its accuracy for enhancingprecision of the generated prompts. Specifically, we introduce an iterativePrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and amask generator.The prompt generator uses a multi-scale chain of thoughtprompting, initially exploring hallucinations for extracting extendedcontextual knowledge on a test image.These hallucinations are then reduced toformulate precise instance-specific prompts, directing the mask generator toproduce masks that are consistent with task semantics by mask semanticalignment. The generated masks iteratively induce the prompt generator to focusmore on task-relevant image areas and reduce irrelevant hallucinations,resulting jointly in better prompts and masks. Experiments on 5 benchmarksdemonstrate the effectiveness of ProMaC. Code given inhttps://lwpyh.github.io/ProMaC/.</description><author>Jian Hu, Jiayi Lin, Junchi Yan, Shaogang Gong</author><pubDate>Wed, 23 Oct 2024 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15205v2</guid></item><item><title>Quantum Architecture Search with Unsupervised Representation Learning</title><link>http://arxiv.org/abs/2401.11576v3</link><description>Unsupervised representation learning presents new opportunities for advancingQuantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ)devices. QAS is designed to optimize quantum circuits for Variational QuantumAlgorithms (VQAs). Most QAS algorithms tightly couple the search space andsearch algorithm, typically requiring the evaluation of numerous quantumcircuits, resulting in high computational costs and limiting scalability tolarger quantum circuits. Predictor-based QAS algorithms mitigate this issue byestimating circuit performance based on structure or embedding. However, thesemethods often demand time-intensive labeling to optimize gate parameters acrossmany circuits, which is crucial for training accurate predictors. Inspired bythe classical neural architecture search algorithm Arch2vec, we investigate thepotential of unsupervised representation learning for QAS without relying onpredictors. Our framework decouples unsupervised architecture representationlearning from the search process, enabling the learned representations to beapplied across various downstream tasks. Additionally, it integrates animproved quantum circuit graph encoding scheme, addressing the limitations ofexisting representations and enhancing search efficiency. This predictor-freeapproach removes the need for large labeled datasets. During the search, weemploy REINFORCE and Bayesian Optimization to explore the latent representationspace and compare their performance against baseline methods. Our resultsdemonstrate that the framework efficiently identifies high-performing quantumcircuits with fewer search iterations.</description><author>Yize Sun, Zixin Wu, Yunpu Ma, Volker Tresp</author><pubDate>Wed, 23 Oct 2024 15:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11576v3</guid></item><item><title>A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024</title><link>http://arxiv.org/abs/2410.17963v1</link><description>The eRisk laboratory aims to address issues related to early risk detectionon the Web. In this year's edition, three tasks were proposed, where Task 2 wasabout early detection of signs of anorexia. Early risk detection is a problemwhere precision and speed are two crucial objectives. Our research group solvedTask 2 by defining a CPI+DMC approach, addressing both objectivesindependently, and a time-aware approach, where precision and speed areconsidered a combined single-objective. We implemented the last approach byexplicitly integrating time during the learning process, considering theERDE{\theta} metric as the training objective. It also allowed us toincorporate temporal metrics to validate and select the optimal models. Weachieved outstanding results for the ERDE50 metric and ranking-based metrics,demonstrating consistency in solving ERD problems.</description><author>Horacio Thompson, Marcelo Errecalde</author><pubDate>Wed, 23 Oct 2024 15:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17963v1</guid></item><item><title>Closed-form merging of parameter-efficient modules for Federated Continual Learning</title><link>http://arxiv.org/abs/2410.17961v1</link><description>Model merging has emerged as a crucial technique in Deep Learning, enablingthe integration of multiple models into a unified system while preservingperformance and scalability. In this respect, the compositional properties oflow-rank adaptation techniques (e.g., LoRA) have proven beneficial, as simpleaveraging LoRA modules yields a single model that mostly integrates thecapabilities of all individual modules. Building on LoRA, we take a stepfurther by imposing that the merged model matches the responses of all learnedmodules. Solving this objective in closed form yields an indeterminate systemwith A and B as unknown variables, indicating the existence of infinitely manyclosed-form solutions. To address this challenge, we introduce LoRM, analternating optimization strategy that trains one LoRA matrix at a time. Thisallows solving for each unknown variable individually, thus finding a uniquesolution. We apply our proposed methodology to Federated Class-IncrementalLearning (FCIL), ensuring alignment of model responses both between clients andacross tasks. Our method demonstrates state-of-the-art performance across arange of FCIL scenarios.</description><author>Riccardo Salami, Pietro Buzzega, Matteo Mosconi, Jacopo Bonato, Luigi Sabetta, Simone Calderara</author><pubDate>Wed, 23 Oct 2024 15:30:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17961v1</guid></item><item><title>MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction</title><link>http://arxiv.org/abs/2406.08649v2</link><description>Drug-target interaction (DTI) prediction is crucial for identifying newtherapeutics and detecting mechanisms of action. While structure-based methodsaccurately model physical interactions between a drug and its protein target,cell-based assays such as Cell Painting can better capture complex DTIinteractions. This paper introduces MOTIVE, a Morphological cOmpound TargetInteraction Graph dataset comprising Cell Painting features for 11,000 genesand 3,600 compounds, along with their relationships extracted from sevenpublicly available databases. We provide random, cold-source (new drugs), andcold-target (new genes) data splits to enable rigorous evaluation underrealistic use cases. Our benchmark results show that graph neural networks thatuse Cell Painting features consistently outperform those that learn from graphstructure alone, feature-based models, and topological heuristics. MOTIVEaccelerates both graph ML research and drug discovery by promoting thedevelopment of more reliable DTI prediction models. MOTIVE resources areavailable at https://github.com/carpenter-singh-lab/motive.</description><author>John Arevalo, Ellen Su, Anne E Carpenter, Shantanu Singh</author><pubDate>Wed, 23 Oct 2024 15:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08649v2</guid></item><item><title>Bounded KRnet and its applications to density estimation and approximation</title><link>http://arxiv.org/abs/2305.09063v3</link><description>In this paper, we develop an invertible mapping, called B-KRnet, on a boundeddomain and apply it to density estimation/approximation for data or thesolutions of PDEs such as the Fokker-Planck equation and the Keller-Segelequation. Similar to KRnet, the structure of B-KRnet adapts thepseudo-triangular structure into a normalizing flow model. The main differencebetween B-KRnet and KRnet is that B-KRnet is defined on a hypercube while KRnetis defined on the whole space, in other words, a new mechanism is introduced inB-KRnet to maintain the exact invertibility. Using B-KRnet as a transport map,we obtain an explicit probability density function (PDF) model that correspondsto the pushforward of a prior (uniform) distribution on the hypercube. It canbe directly applied to density estimation when only data are available. Bycoupling KRnet and B-KRnet, we define a deep generative model on ahigh-dimensional domain where some dimensions are bounded and other dimensionsare unbounded. A typical case is the solution of the stationary kineticFokker-Planck equation, which is a PDF of position and momentum. Based onB-KRnet, we develop an adaptive learning approach to approximate partialdifferential equations whose solutions are PDFs or can be treated as PDFs. Avariety of numerical experiments is presented to demonstrate the effectivenessof B-KRnet.</description><author>Li Zeng, Xiaoliang Wan, Tao Zhou</author><pubDate>Wed, 23 Oct 2024 15:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09063v3</guid></item><item><title>Zeitenwenden: Detecting changes in the German political discourse</title><link>http://arxiv.org/abs/2410.17960v1</link><description>From a monarchy to a democracy, to a dictatorship and back to a democracy --the German political landscape has been constantly changing ever since thefirst German national state was formed in 1871. After World War II, the FederalRepublic of Germany was formed in 1949. Since then every plenary session of theGerman Bundestag was logged and even has been digitized over the course of thelast few years. We analyze these texts using a time series variant of the topicmodel LDA to investigate which events had a lasting effect on the politicaldiscourse and how the political topics changed over time. This allows us todetect changes in word frequency (and thus key discussion points) in politicaldiscourse.</description><author>Kai-Robin Lange, Jonas Rieger, Niklas Benner, Carsten Jentsch</author><pubDate>Wed, 23 Oct 2024 15:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17960v1</guid></item><item><title>StockGPT: A GenAI Model for Stock Prediction and Trading</title><link>http://arxiv.org/abs/2404.05101v3</link><description>This paper introduces StockGPT, an autoregressive ``number'' model trainedand tested on 70 million daily U.S.\ stock returns over nearly 100 years.Treating each return series as a sequence of tokens, StockGPT automaticallylearns the hidden patterns predictive of future returns via its attentionmechanism. On a held-out test sample from 2001 to 2023, daily and monthlyrebalanced long-short portfolios formed from StockGPT predictions yield strongperformance. The StockGPT-based portfolios span momentum and long-/short-termreversals, eliminating the need for manually crafted price-based strategies,and yield highly significant alphas against leading stock market factors,suggesting a novel AI pricing effect. This highlights the immense promise ofgenerative AI in surpassing human in making complex financial investmentdecisions.</description><author>Dat Mai</author><pubDate>Wed, 23 Oct 2024 15:28:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05101v3</guid></item><item><title>Linear Adversarial Concept Erasure</title><link>http://arxiv.org/abs/2201.12091v5</link><description>Modern neural models trained on textual data rely on pre-trainedrepresentations that emerge without direct supervision. As theserepresentations are increasingly being used in real-world applications, theinability to \emph{control} their content becomes an increasingly importantproblem. We formulate the problem of identifying and erasing a linear subspacethat corresponds to a given concept, in order to prevent linear predictors fromrecovering the concept. We model this problem as a constrained, linear maximingame, and show that existing solutions are generally not optimal for this task.We derive a closed-form solution for certain objectives, and propose a convexrelaxation, \method, that works well for others. When evaluated in the contextof binary gender removal, the method recovers a low-dimensional subspace whoseremoval mitigates bias by intrinsic and extrinsic evaluation. We show that themethod is highly expressive, effectively mitigating bias in deep nonlinearclassifiers while maintaining tractability and interpretability.</description><author>Shauli Ravfogel, Michael Twiton, Yoav Goldberg, Ryan Cotterell</author><pubDate>Wed, 23 Oct 2024 15:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.12091v5</guid></item></channel></rss>