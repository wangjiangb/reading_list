<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 08 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries</title><link>http://arxiv.org/abs/2404.00086v2</link><description>Modern video segmentation methods adopt object queries to perform inter-frameassociation and demonstrate satisfactory performance in tracking continuouslyappearing objects despite large-scale motion and transient occlusion. However,they all underperform on newly emerging and disappearing objects that arecommon in the real world because they attempt to model object emergence anddisappearance through feature transitions between background and foregroundqueries that have significant feature gaps. We introduce Dynamic Anchor Queries(DAQ) to shorten the transition gap between the anchor and target queries bydynamically generating anchor queries based on the features of potentialcandidates. Furthermore, we introduce a query-level object Emergence andDisappearance Simulation (EDS) strategy, which unleashes DAQ's potentialwithout any additional cost. Finally, we combine our proposed DAQ and EDS withDVIS to obtain DVIS-DAQ. Extensive experiments demonstrate that DVIS-DAQachieves a new state-of-the-art (SOTA) performance on five mainstream videosegmentation benchmarks. Code and models are available at\url{https://github.com/SkyworkAI/DAQ-VS}.</description><author>Yikang Zhou, Tao Zhang, Shunping Ji, Shuicheng Yan, Xiangtai Li</author><pubDate>Fri, 05 Apr 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00086v2</guid></item><item><title>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</title><link>http://arxiv.org/abs/2404.04256v1</link><description>Multi-modal semantic segmentation significantly enhances AI agents'perception and scene understanding, especially under adverse conditions likelow-light or overexposed environments. Leveraging additional modalities(X-modality) like thermal and depth alongside traditional RGB providescomplementary information, enabling more robust and reliable segmentation. Inthis work, we introduce Sigma, a Siamese Mamba network for multi-modal semanticsegmentation, utilizing the Selective Structured State Space Model, Mamba.Unlike conventional methods that rely on CNNs, with their limited localreceptive fields, or Vision Transformers (ViTs), which offer global receptivefields at the cost of quadratic complexity, our model achieves global receptivefields coverage with linear complexity. By employing a Siamese encoder andinnovating a Mamba fusion mechanism, we effectively select essentialinformation from different modalities. A decoder is then developed to enhancethe channel-wise modeling ability of the model. Our method, Sigma, isrigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,demonstrating its superiority and marking the first successful application ofState Space Models (SSMs) in multi-modal perception tasks. Code is available athttps://github.com/zifuwan/Sigma.</description><author>Zifu Wan, Yuhao Wang, Silong Yong, Pingping Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie</author><pubDate>Fri, 05 Apr 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04256v1</guid></item><item><title>Watermark-based Detection and Attribution of AI-Generated Content</title><link>http://arxiv.org/abs/2404.04254v1</link><description>Several companies--such as Google, Microsoft, and OpenAI--have deployedtechniques to watermark AI-generated content to enable proactive detection.However, existing literature mainly focuses on user-agnostic detection.Attribution aims to further trace back the user of a generative-AI service whogenerated a given content detected as AI-generated. Despite its growingimportance, attribution is largely unexplored. In this work, we aim to bridgethis gap by providing the first systematic study on watermark-based, user-awaredetection and attribution of AI-generated content. Specifically, wetheoretically study the detection and attribution performance via rigorousprobabilistic analysis. Moreover, we develop an efficient algorithm to selectwatermarks for the users to enhance attribution performance. Both ourtheoretical and empirical results show that watermark-based detection andattribution inherit the accuracy and (non-)robustness properties of thewatermarking method.</description><author>Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong</author><pubDate>Fri, 05 Apr 2024 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04254v1</guid></item><item><title>Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution</title><link>http://arxiv.org/abs/2404.04253v1</link><description>Recent reinforcement learning approaches have shown surprisingly strongcapabilities of bang-bang policies for solving continuous control benchmarks.The underlying coarse action space discretizations often yield favourableexploration characteristics while final performance does not visibly suffer inthe absence of action penalization in line with optimal control theory. Inrobotics applications, smooth control signals are commonly preferred to reducesystem wear and energy efficiency, but action costs can be detrimental toexploration during early training. In this work, we aim to bridge thisperformance gap by growing discrete action spaces from coarse to fine controlresolution, taking advantage of recent results in decoupled Q-learning to scaleour approach to high-dimensional action spaces up to dim(A) = 38. Our workindicates that an adaptive control resolution in combination with valuedecomposition yields simple critic-only algorithms that yield surprisinglystrong performance on continuous control tasks.</description><author>Tim Seyde, Peter Werner, Wilko Schwarting, Markus Wulfmeier, Daniela Rus</author><pubDate>Fri, 05 Apr 2024 18:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04253v1</guid></item><item><title>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)</title><link>http://arxiv.org/abs/2404.04251v1</link><description>With advances in the quality of text-to-image (T2I) models has come interestin benchmarking their prompt faithfulness-the semantic coherence of generatedimages to the prompts they were conditioned on. A variety of T2I faithfulnessmetrics have been proposed, leveraging advances in cross-modal embeddings andvision-language models (VLMs). However, these metrics are not rigorouslycompared and benchmarked, instead presented against few weak baselines bycorrelation to human Likert scores over a set of easy-to-discriminate images. We introduce T2IScoreScore (TS2), a curated set of semantic error graphscontaining a prompt and a set increasingly erroneous images. These allow us torigorously judge whether a given prompt faithfulness metric can correctly orderimages with respect to their objective error count and significantlydiscriminate between different error nodes, using meta-metric scores derivedfrom established statistical tests. Surprisingly, we find that thestate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) wetested fail to significantly outperform simple feature-based metrics likeCLIPScore, particularly on a hard subset of naturally-occurring T2I modelerrors. TS2 will enable the development of better T2I prompt faithfulnessmetrics through more rigorous comparison of their conformity to expectedorderings and separations under objective criteria.</description><author>Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, William Yang Wang</author><pubDate>Fri, 05 Apr 2024 18:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04251v1</guid></item><item><title>CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation</title><link>http://arxiv.org/abs/2312.08240v2</link><description>Reliable object grasping is a crucial capability for autonomous robots.However, many existing grasping approaches focus on general clutter removalwithout explicitly modeling objects and thus only relying on the visible localgeometry. We introduce CenterGrasp, a novel framework that combines objectawareness and holistic grasping. CenterGrasp learns a general object prior byencoding shapes and valid grasps in a continuous latent space. It consists ofan RGB-D image encoder that leverages recent advances to detect objects andinfer their pose and latent code, and a decoder to predict shape and grasps foreach object in the scene. We perform extensive experiments on simulated as wellas real-world cluttered scenes and demonstrate strong scene reconstruction and6-DoF grasp-pose estimation performance. Compared to the state of the art,CenterGrasp achieves an improvement of 38.5 mm in shape reconstruction and 33percentage points on average in grasp success. We make the code and trainedmodels publicly available at http://centergrasp.cs.uni-freiburg.de.</description><author>Eugenio Chisari, Nick Heppert, Tim Welschehold, Wolfram Burgard, Abhinav Valada</author><pubDate>Fri, 05 Apr 2024 18:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08240v2</guid></item><item><title>Modeling 3D Surface Manifolds with a Locally Conditioned Atlas</title><link>http://arxiv.org/abs/2102.05984v2</link><description>Recently proposed 3D object reconstruction methods represent a mesh with anatlas - a set of planar patches approximating the surface. However, theirapplication in a real-world scenario is limited since the surfaces ofreconstructed objects contain discontinuities, which degrades the quality ofthe final mesh. This is mainly caused by independent processing of individualpatches, and in this work, we postulate to mitigate this limitation bypreserving local consistency around patch vertices. To that end, we introduce aLocally Conditioned Atlas (LoCondA), a framework for representing a 3D objecthierarchically in a generative model. Firstly, the model maps a point cloud ofan object into a sphere. Secondly, by leveraging a spherical prior, we enforcethe mapping to be locally consistent on the sphere and on the target object.This way, we can sample a mesh quad on that sphere and project it back onto theobject's manifold. With LoCondA, we can produce topologically diverse objectswhile maintaining quads to be stitched together. We show that the proposedapproach provides structurally coherent reconstructions while producing meshesof quality comparable to the competitors.</description><author>Przemysław Spurek, Sebastian Winczowski, Maciej Zięba, Tomasz Trzciński, Kacper Kania, Marcin Mazur</author><pubDate>Fri, 05 Apr 2024 18:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.05984v2</guid></item><item><title>Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism</title><link>http://arxiv.org/abs/2404.04245v1</link><description>This technical report delves into an in-depth exploration of adversarialattacks specifically targeted at Deep Neural Networks (DNNs) utilized for imageclassification. The study also investigates defense mechanisms aimed atbolstering the robustness of machine learning models. The research focuses oncomprehending the ramifications of two prominent attack methodologies: the FastGradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacksare examined concerning three pre-trained image classifiers: Resnext50_32x4d,DenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, thestudy proposes the robustness of defensive distillation as a defense mechanismto counter FGSM and CW attacks. This defense mechanism is evaluated using theCIFAR-10 dataset, where CNN models, specifically resnet101 and Resnext50_32x4d,serve as the teacher and student models, respectively. The proposed defensivedistillation model exhibits effectiveness in thwarting attacks such as FGSM.However, it is noted to remain susceptible to more sophisticated techniqueslike the CW attack. The document presents a meticulous validation of theproposed scheme. It provides detailed and comprehensive results, elucidatingthe efficacy and limitations of the defense mechanisms employed. Throughrigorous experimentation and analysis, the study offers insights into thedynamics of adversarial attacks on DNNs, as well as the effectiveness ofdefensive strategies in mitigating their impact.</description><author>Trilokesh Ranjan Sarkar, Nilanjan Das, Pralay Sankar Maitra, Bijoy Some, Ritwik Saha, Orijita Adhikary, Bishal Bose, Jaydip Sen</author><pubDate>Fri, 05 Apr 2024 18:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04245v1</guid></item><item><title>DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration</title><link>http://arxiv.org/abs/2404.04244v1</link><description>Existing unsupervised deformable image registration methods usually rely onmetrics applied to the gradients of predicted displacement or velocity fieldsas a regularization term to ensure transformation smoothness, which potentiallylimits registration accuracy. In this study, we propose a novel approach toenhance unsupervised deformable image registration by introducing a newdifferential operator into the registration framework. This operator, acting onthe velocity field and mapping it to a dual space, ensures the smoothness ofthe velocity field during optimization, facilitating accurate deformableregistration. In addition, to tackle the challenge of capturing largedeformations inside image pairs, we introduce a Cross-Coordinate Attentionmodule (CCA) and embed it into a proposed Fully Convolutional Networks(FCNs)-based multi-resolution registration architecture. Evaluation experimentsare conducted on two magnetic resonance imaging (MRI) datasets. Compared tovarious state-of-the-art registration approaches, including a traditionalalgorithm and three representative unsupervised learning-based methods, ourmethod achieves superior accuracies, maintaining desirable diffeomorphicproperties, and exhibiting promising registration speed.</description><author>Jiong Wu</author><pubDate>Fri, 05 Apr 2024 18:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04244v1</guid></item><item><title>Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models</title><link>http://arxiv.org/abs/2404.04243v1</link><description>Text-to-image diffusion models have shown remarkable success in generating apersonalized subject based on a few reference images. However, current methodsstruggle with handling multiple subjects simultaneously, often resulting inmixed identities with combined attributes from different subjects. In thiswork, we present MuDI, a novel framework that enables multi-subjectpersonalization by effectively decoupling identities from multiple subjects.Our main idea is to utilize segmented subjects generated by the SegmentAnything Model for both training and inference, as a form of data augmentationfor training and initialization for the generation process. Our experimentsdemonstrate that MuDI can produce high-quality personalized images withoutidentity mixing, even for highly similar subjects as shown in Figure 1. Inhuman evaluation, MuDI shows twice as many successes for personalizing multiplesubjects without identity mixing over existing baselines and is preferred over70% compared to the strongest baseline. More results are available athttps://mudi-t2i.github.io/.</description><author>Sangwon Jang, Jaehyeong Jo, Kimin Lee, Sung Ju Hwang</author><pubDate>Fri, 05 Apr 2024 18:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04243v1</guid></item><item><title>Physical Property Understanding from Language-Embedded Feature Fields</title><link>http://arxiv.org/abs/2404.04242v1</link><description>Can computers perceive the physical properties of objects solely throughvision? Research in cognitive science and vision science has shown that humansexcel at identifying materials and estimating their physical properties basedpurely on visual appearance. In this paper, we present a novel approach fordense prediction of the physical properties of objects using a collection ofimages. Inspired by how humans reason about physics through vision, we leveragelarge language models to propose candidate materials for each object. We thenconstruct a language-embedded point cloud and estimate the physical propertiesof each 3D point using a zero-shot kernel regression approach. Our method isaccurate, annotation-free, and applicable to any object in the open world.Experiments demonstrate the effectiveness of the proposed approach in variousphysical property reasoning tasks, such as estimating the mass of commonobjects, as well as other properties like friction and hardness.</description><author>Albert J. Zhai, Yuan Shen, Emily Y. Chen, Gloria X. Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, Shenlong Wang</author><pubDate>Fri, 05 Apr 2024 18:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04242v1</guid></item><item><title>Dynamic Conditional Optimal Transport through Simulation-Free Flows</title><link>http://arxiv.org/abs/2404.04240v1</link><description>We study the geometry of conditional optimal transport (COT) and prove adynamical formulation which generalizes the Benamou-Brenier Theorem. With thesetools, we propose a simulation-free flow-based method for conditionalgenerative modeling. Our method couples an arbitrary source distribution to aspecified target distribution through a triangular COT plan. We build on theframework of flow matching to train a conditional generative model byapproximating the geodesic path of measures induced by this COT plan. Ourtheory and methods are applicable in the infinite-dimensional setting, makingthem well suited for inverse problems. Empirically, we demonstrate our proposedmethod on two image-to-image translation tasks and an infinite-dimensionalBayesian inverse problem.</description><author>Gavin Kerrigan, Giosue Migliorini, Padhraic Smyth</author><pubDate>Fri, 05 Apr 2024 18:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04240v1</guid></item><item><title>Finding AI-Generated Faces in the Wild</title><link>http://arxiv.org/abs/2311.08577v3</link><description>AI-based image generation has continued to rapidly improve, producingincreasingly more realistic images with fewer obvious visual flaws.AI-generated images are being used to create fake online profiles which in turnare being used for spam, fraud, and disinformation campaigns. As the generalproblem of detecting any type of manipulated or synthesized content isreceiving increasing attention, here we focus on a more narrow task ofdistinguishing a real face from an AI-generated face. This is particularlyapplicable when tackling inauthentic online accounts with a fake user profilephoto. We show that by focusing on only faces, a more resilient andgeneral-purpose artifact can be detected that allows for the detection ofAI-generated faces from a variety of GAN- and diffusion-based synthesisengines, and across image resolutions (as low as 128 x 128 pixels) andqualities.</description><author>Gonzalo J. Aniano Porcile, Jack Gindi, Shivansh Mundra, James R. Verbus, Hany Farid</author><pubDate>Fri, 05 Apr 2024 18:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08577v3</guid></item><item><title>Cleared for Takeoff? Compositional &amp; Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents</title><link>http://arxiv.org/abs/2404.04237v1</link><description>The rapid progress of large language models (LLMs) has seen them excel andfrequently surpass human performance on standard benchmarks. This has enabledmany downstream applications, such as LLM agents, to rely on theirsophisticated reasoning to navigate complex task requirements. However, LLMsare known to unexpectedly falter in simple tasks and under seeminglystraightforward circumstances - underscoring the need for better and morediverse evaluation setups to measure their true capabilities. To this end, wechoose to study compositional and conditional reasoning, two cornerstones ofhuman cognition, and introduce GroundCocoa - a lexically diverse benchmarkconnecting these reasoning skills to the real-world problem of flight booking.Our task involves aligning detailed user preferences with available flightoptions presented in a multiple-choice format. Results indicate a significantdisparity in performance among current state-of-the-art LLMs with even the bestperforming model, GPT-4 Turbo, not exceeding 67% accuracy despite advancedprompting techniques.</description><author>Harsh Kohli, Huan Sun</author><pubDate>Fri, 05 Apr 2024 18:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04237v1</guid></item><item><title>player2vec: A Language Modeling Approach to Understand Player Behavior in Games</title><link>http://arxiv.org/abs/2404.04234v1</link><description>Methods for learning latent user representations from historical behaviorlogs have gained traction for recommendation tasks in e-commerce, contentstreaming, and other settings. However, this area still remains relativelyunderexplored in video and mobile gaming contexts. In this work, we present anovel method for overcoming this limitation by extending a long-rangeTransformer model from the natural language processing domain to playerbehavior data. We discuss specifics of behavior tracking in games and proposepreprocessing and tokenization approaches by viewing in-game events in ananalogous way to words in sentences, thus enabling learning playerrepresentations in a self-supervised manner in the absence of ground-truthannotations. We experimentally demonstrate the efficacy of the proposedapproach in fitting the distribution of behavior events by evaluating intrinsiclanguage modeling metrics. Furthermore, we qualitatively analyze the emergingstructure of the learned embedding space and show its value for generatinginsights into behavior patterns to inform downstream applications.</description><author>Tianze Wang, Maryam Honari-Jahromi, Styliani Katsarou, Olga Mikheeva, Theodoros Panagiotakopoulos, Sahar Asadi, Oleg Smirnov</author><pubDate>Fri, 05 Apr 2024 18:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04234v1</guid></item><item><title>WorDepth: Variational Language Prior for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2404.03635v2</link><description>Three-dimensional (3D) reconstruction from a single image is an ill-posedproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from textdescription(s) is similarly ill-posed, i.e. spatial arrangements of objectsdescribed. We investigate the question of whether two inherently ambiguousmodalities can be used in conjunction to produce metric-scaled reconstructions.To test this, we focus on monocular depth estimation, the problem of predictinga dense depth map from a single image, but with an additional text captiondescribing the scene. To this end, we begin by encoding the text caption as amean and standard deviation; using a variational framework, we learn thedistribution of the plausible metric reconstructions of 3D scenes correspondingto the text captions as a prior. To "select" a specific reconstruction or depthmap, we encode the given image through a conditional sampler that samples fromthe latent space of the variational text encoder, which is then decoded to theoutput depth map. Our approach is trained alternatingly between the text andimage branches: in one optimization step, we predict the mean and standarddeviation from the text description and sample from a standard Gaussian, and inthe other, we sample using a (image) conditional sampler. Once trained, wedirectly predict depth from the encoded text using the conditional sampler. Wedemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, wherewe show that language can consistently improve performance in both.</description><author>Ziyao Zeng, Daniel Wang, Fengyu Yang, Hyoungseob Park, Yangchao Wu, Stefano Soatto, Byung-Woo Hong, Dong Lao, Alex Wong</author><pubDate>Fri, 05 Apr 2024 18:27:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03635v2</guid></item><item><title>Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation</title><link>http://arxiv.org/abs/2404.04232v1</link><description>Compositional generalization, representing the model's ability to generatetext with new attribute combinations obtained by recombining single attributesfrom the training data, is a crucial property for multi-aspect controllabletext generation (MCTG) methods. Nonetheless, a comprehensive compositionalgeneralization evaluation benchmark of MCTG is still lacking. We proposeCompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and acrafted three-dimensional evaluation protocol, to holistically evaluate thecompositional generalization of MCTG approaches. We observe that existing MCTGworks generally confront a noticeable performance drop in compositionaltesting. To mitigate this issue, we introduce Meta-MCTG, a training frameworkincorporating meta-learning, where we enable models to learn how to generalizeby simulating compositional generalization scenarios in the training phase. Wedemonstrate the effectiveness of Meta-MCTG through achieving obviousimprovement (by at most 3.64%) for compositional testing performance in 94.4%cases.</description><author>Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, Zhendong Mao</author><pubDate>Fri, 05 Apr 2024 18:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04232v1</guid></item><item><title>Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2404.04231v1</link><description>This paper addresses text-supervised semantic segmentation, aiming to learn amodel capable of segmenting arbitrary visual concepts within images by usingonly image-text pairs without dense annotations. Existing methods havedemonstrated that contrastive learning on image-text pairs effectively alignsvisual segments with the meanings of texts. We notice that there is adiscrepancy between text alignment and semantic segmentation: A text oftenconsists of multiple semantic concepts, whereas semantic segmentation strivesto create semantically homogeneous segments. To address this issue, we proposea novel framework, Image-Text Co-Decomposition (CoDe), where the paired imageand text are jointly decomposed into a set of image regions and a set of wordsegments, respectively, and contrastive learning is developed to enforceregion-word alignment. To work with a vision-language model, we present aprompt learning mechanism that derives an extra representation to highlight animage segment or a word segment of interest, with which more effective featurescan be extracted from that segment. Comprehensive experimental resultsdemonstrate that our method performs favorably against existing text-supervisedsemantic segmentation methods on six benchmark datasets.</description><author>Ji-Jia Wu, Andy Chia-Hao Chang, Chieh-Yu Chuang, Chun-Pei Chen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Yung-Yu Chuang, Yen-Yu Lin</author><pubDate>Fri, 05 Apr 2024 18:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04231v1</guid></item><item><title>Prompt-based Pseudo-labeling Strategy for Sample-Efficient Semi-Supervised Extractive Summarization</title><link>http://arxiv.org/abs/2311.09559v2</link><description>Semi-supervised learning (SSL) is a widely used technique in scenarios wherelabeled data is scarce and unlabeled data is abundant. While SSL is popular forimage and text classification, it is relatively underexplored for the task ofextractive text summarization. Standard SSL methods follow a teacher-studentparadigm to first train a classification model and then use the classifier'sconfidence values to select pseudo-labels for the subsequent training cycle;however, such classifiers are not suitable to measure the accuracy ofpseudo-labels as they lack specific tuning for evaluation, which leads toconfidence values that fail to capture the semantics and correctness of thegenerated summary. To address this problem, we propose a prompt-basedpseudo-labeling strategy with LLMs that picks unlabeled examples with moreaccurate pseudo-labels than using just the classifier's probability outputs.Our approach also includes a relabeling mechanism that improves the quality ofpseudo-labels. We evaluate our method on three text summarization datasets:TweetSumm, WikiHow, and ArXiv/PubMed. We empirically show that aprompting-based LLM that scores and generates pseudo-labels outperformsexisting SSL methods on ROUGE-1, ROUGE-2, and ROUGE-L scores on all thedatasets. Furthermore, our method achieves competitive G-Eval scores(evaluation with GPT-4) as a fully supervised method that uses 100% of thelabeled data with only 16.67% of the labeled data.</description><author>Gaurav Sahu, Olga Vechtomova, Issam H. Laradji</author><pubDate>Fri, 05 Apr 2024 18:19:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09559v2</guid></item><item><title>Twins in rotational spectroscopy: Does a rotational spectrum uniquely identify a molecule?</title><link>http://arxiv.org/abs/2404.04225v1</link><description>Rotational spectroscopy is the most accurate method for determiningstructures of molecules in the gas phase. It is often assumed that a rotationalspectrum is a unique "fingerprint" of a molecule. The availability of largemolecular databases and the development of artificial intelligence methods forspectroscopy makes the testing of this assumption timely. In this paper, wepose the determination of molecular structures from rotational spectra as aninverse problem. Within this framework, we adopt a funnel-based approach tosearch for molecular twins, which are two or more molecules, which have similarrotational spectra but distinctly different molecular structures. Wedemonstrate that there are twins within standard levels of computationalaccuracy by generating rotational constants for many molecules from severallarge molecular databases, indicating the inverse problem is ill-posed.However, some twins can be distinguished by increasing the accuracy of thetheoretical methods or by performing additional experiments.</description><author>Marcus Schwarting, Nathan A. Seifert, Michael J. Davis, Ben Blaiszik, Ian Foster, Kirill Prozument</author><pubDate>Fri, 05 Apr 2024 18:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04225v1</guid></item><item><title>Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions</title><link>http://arxiv.org/abs/2404.04224v1</link><description>Predicting and enhancing inherent properties based on molecular structures isparamount to design tasks in medicine, materials science, and environmentalmanagement. Most of the current machine learning and deep learning approacheshave become standard for predictions, but they face challenges when appliedacross different datasets due to reliance on correlations between molecularrepresentation and target properties. These approaches typically depend onlarge datasets to capture the diversity within the chemical space, facilitatinga more accurate approximation, interpolation, or extrapolation of the chemicalbehavior of molecules. In our research, we introduce an active learningapproach that discerns underlying cause-effect relationships through strategicsampling with the use of a graph loss function. This method identifies thesmallest subset of the dataset capable of encoding the most informationrepresentative of a much larger chemical space. The identified causal relationsare then leveraged to conduct systematic interventions, optimizing the designtask within a chemical space that the models have not encountered previously.While our implementation focused on the QM9 quantum-chemical dataset for aspecific design task-finding molecules with a large dipole moment-our activecausal learning approach, driven by intelligent sampling and interventions,holds potential for broader applications in molecular, materials design anddiscovery.</description><author>Zachary R. Fox, Ayana Ghosh</author><pubDate>Fri, 05 Apr 2024 18:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04224v1</guid></item><item><title>How Lexical is Bilingual Lexicon Induction?</title><link>http://arxiv.org/abs/2404.04221v1</link><description>In contemporary machine learning approaches to bilingual lexicon induction(BLI), a model learns a mapping between the embedding spaces of a languagepair. Recently, retrieve-and-rank approach to BLI has achieved state of the artresults on the task. However, the problem remains challenging in low-resourcesettings, due to the paucity of data. The task is complicated by factors suchas lexical variation across languages. We argue that the incorporation ofadditional lexical information into the recent retrieve-and-rank approachshould improve lexicon induction. We demonstrate the efficacy of our proposedapproach on XLING, improving over the previous state of the art by an averageof 2\% across all language pairs.</description><author>Harsh Kohli, Helian Feng, Nicholas Dronen, Calvin McCarter, Sina Moeini, Ali Kebarighotbi</author><pubDate>Fri, 05 Apr 2024 18:10:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04221v1</guid></item><item><title>Multi-modal perception for soft robotic interactions using generative models</title><link>http://arxiv.org/abs/2404.04220v1</link><description>Perception is essential for the active interaction of physical agents withthe external environment. The integration of multiple sensory modalities, suchas touch and vision, enhances this perceptual process, creating a morecomprehensive and robust understanding of the world. Such fusion isparticularly useful for highly deformable bodies such as soft robots.Developing a compact, yet comprehensive state representation from multi-sensoryinputs can pave the way for the development of complex control strategies. Thispaper introduces a perception model that harmonizes data from diversemodalities to build a holistic state representation and assimilate essentialinformation. The model relies on the causality between sensory input androbotic actions, employing a generative model to efficiently compress fusedinformation and predict the next observation. We present, for the first time, astudy on how touch can be predicted from vision and proprioception on softrobots, the importance of the cross-modal generation and why this is essentialfor soft robotic interactions in unstructured environments.</description><author>Enrico Donato, Egidio Falotico, Thomas George Thuruthel</author><pubDate>Fri, 05 Apr 2024 18:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04220v1</guid></item><item><title>Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation</title><link>http://arxiv.org/abs/2404.04219v1</link><description>Dexterous manipulation, often facilitated by multi-fingered robotic hands,holds solid impact for real-world applications. Soft robotic hands, due totheir compliant nature, offer flexibility and adaptability during objectgrasping and manipulation. Yet, benefits come with challenges, particularly inthe control development for finger coordination. Reinforcement Learning (RL)can be employed to train object-specific in-hand manipulation policies, butlimiting adaptability and generalizability. We introduce a Continual PolicyDistillation (CPD) framework to acquire a versatile controller for in-handmanipulation, to rotate different objects in shape and size within afour-fingered soft gripper. The framework leverages Policy Distillation (PD) totransfer knowledge from expert policies to a continually evolving studentpolicy network. Exemplar-based rehearsal methods are then integrated tomitigate catastrophic forgetting and enhance generalization. The performance ofthe CPD framework over various replay strategies demonstrates its effectivenessin consolidating knowledge from multiple experts and achieving versatile andadaptive behaviours for in-hand manipulation tasks.</description><author>Lanpei Li, Enrico Donato, Vincenzo Lomonaco, Egidio Falotico</author><pubDate>Fri, 05 Apr 2024 18:05:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04219v1</guid></item><item><title>SnAG: Scalable and Accurate Video Grounding</title><link>http://arxiv.org/abs/2404.02257v2</link><description>Temporal grounding of text descriptions in videos is a central problem invision-language learning and video understanding. Existing methods oftenprioritize accuracy over scalability -- they have been optimized for groundingonly a few text queries within short videos, and fail to scale up to longvideos with hundreds of queries. In this paper, we study the effect ofcross-modal fusion on the scalability of video grounding models. Our analysisestablishes late fusion as a more cost-effective fusion scheme for long-formvideos with many text queries. Moreover, it leads us to a novel, video-centricsampling scheme for efficient training. Based on these findings, we presentSnAG, a simple baseline for scalable and accurate video grounding. Withoutbells and whistles, SnAG is 43% more accurate and 1.5x faster than CONE, astate of the art for long-form video grounding on the challenging MAD dataset,while achieving highly competitive results on short videos.</description><author>Fangzhou Mu, Sicheng Mo, Yin Li</author><pubDate>Fri, 05 Apr 2024 18:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02257v2</guid></item><item><title>State Space Models for Event Cameras</title><link>http://arxiv.org/abs/2402.15584v2</link><description>Today, state-of-the-art deep neural networks that process event-camera datafirst convert a temporal window of events into dense, grid-like inputrepresentations. As such, they exhibit poor generalizability when deployed athigher inference frequencies (i.e., smaller temporal windows) than the onesthey were trained on. We address this challenge by introducing state-spacemodels (SSMs) with learnable timescale parameters to event-based vision. Thisdesign adapts to varying frequencies without the need to retrain the network atdifferent frequencies. Additionally, we investigate two strategies tocounteract aliasing effects when deploying the model at higher frequencies. Wecomprehensively evaluate our approach against existing methods based on RNN andTransformer architectures across various benchmarks, including Gen1 and 1 Mpxevent camera datasets. Our results demonstrate that SSM-based models train 33%faster and also exhibit minimal performance degradation when tested at higherfrequencies than the training input. Traditional RNN and Transformer modelsexhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.31mAP, highlighting the effectiveness of SSMs in event-based vision tasks.</description><author>Nikola Zubić, Mathias Gehrig, Davide Scaramuzza</author><pubDate>Fri, 05 Apr 2024 18:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15584v2</guid></item><item><title>Opti-CAM: Optimizing saliency maps for interpretability</title><link>http://arxiv.org/abs/2301.07002v3</link><description>Methods based on class activation maps (CAM) provide a simple mechanism tointerpret predictions of convolutional neural networks by using linearcombinations of feature maps as saliency maps. By contrast, masking-basedmethods optimize a saliency map directly in the image space or learn it bytraining another network on additional data. In this work we introduce Opti-CAM, combining ideas from CAM-based andmasking-based approaches. Our saliency map is a linear combination of featuremaps, where weights are optimized per image such that the logit of the maskedimage for a given class is maximized. We also fix a fundamental flaw in two ofthe most common evaluation metrics of attribution methods. On several datasets,Opti-CAM largely outperforms other CAM-based approaches according to the mostrelevant classification metrics. We provide empirical evidence supporting thatlocalization and classifier interpretability are not necessarily aligned.</description><author>Hanwei Zhang, Felipe Torres, Ronan Sicre, Yannis Avrithis, Stephane Ayache</author><pubDate>Fri, 05 Apr 2024 17:50:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07002v3</guid></item><item><title>Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation</title><link>http://arxiv.org/abs/2404.04212v1</link><description>Parameter-efficient fine-tuning (PEFT) methods are increasingly vital inadapting large-scale pre-trained language models for diverse tasks, offering abalance between adaptability and computational efficiency. They are importantin Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhancetranslation accuracy with minimal resources. However, their practicaleffectiveness varies significantly across different languages. We conductedcomprehensive empirical experiments with varying LRL domains and sizes toevaluate the performance of 8 PEFT methods with in total of 15 architecturesusing the SacreBLEU score. We showed that 6 PEFT architectures outperform thebaseline for both in-domain and out-domain tests and the Houlsby+Inversionadapter has the best performance overall, proving the effectiveness of PEFTmethods.</description><author>Tong Su, Xin Peng, Sarubi Thillainathan, David Guzmán, Surangika Ranathunga, En-Shiun Annie Lee</author><pubDate>Fri, 05 Apr 2024 17:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04212v1</guid></item><item><title>Robust Gaussian Splatting</title><link>http://arxiv.org/abs/2404.04211v1</link><description>In this paper, we address common error sources for 3D Gaussian Splatting(3DGS) including blur, imperfect camera poses, and color inconsistencies, withthe goal of improving its robustness for practical applications likereconstructions from handheld phone captures. Our main contribution involvesmodeling motion blur as a Gaussian distribution over camera poses, allowing usto address both camera pose refinement and motion blur correction in a unifiedway. Additionally, we propose mechanisms for defocus blur compensation and foraddressing color in-consistencies caused by ambient light, shadows, or due tocamera-related factors like varying white balancing settings. Our proposedsolutions integrate in a seamless way with the 3DGS formulation whilemaintaining its benefits in terms of training efficiency and rendering speed.We experimentally validate our contributions on relevant benchmark datasetsincluding Scannet++ and Deblur-NeRF, obtaining state-of-the-art results andthus consistent improvements over relevant baselines.</description><author>François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder</author><pubDate>Fri, 05 Apr 2024 17:42:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04211v1</guid></item><item><title>Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology</title><link>http://arxiv.org/abs/2404.04205v1</link><description>The proliferation of the Internet of Things (IoT) has led to an explosion ofdata generated by interconnected devices, presenting both opportunities andchallenges for intelligent decision-making in complex environments. TraditionalReinforcement Learning (RL) approaches often struggle to fully harness thisdata due to their limited ability to process and interpret the intricatepatterns and dependencies inherent in IoT applications. This paper introduces anovel framework that integrates transformer architectures with Proximal PolicyOptimization (PPO) to address these challenges. By leveraging theself-attention mechanism of transformers, our approach enhances RL agents'capacity for understanding and acting within dynamic IoT environments, leadingto improved decision-making processes. We demonstrate the effectiveness of ourmethod across various IoT scenarios, from smart home automation to industrialcontrol systems, showing marked improvements in decision-making efficiency andadaptability. Our contributions include a detailed exploration of thetransformer's role in processing heterogeneous IoT data, a comprehensiveevaluation of the framework's performance in diverse environments, and abenchmark against traditional RL methods. The results indicate significantadvancements in enabling RL agents to navigate the complexities of IoTecosystems, highlighting the potential of our approach to revolutionizeintelligent automation and decision-making in the IoT landscape.</description><author>Gaith Rjoub, Saidul Islam, Jamal Bentahar, Mohammed Amin Almaiah, Rana Alrawashdeh</author><pubDate>Fri, 05 Apr 2024 17:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04205v1</guid></item><item><title>Social Skill Training with Large Language Models</title><link>http://arxiv.org/abs/2404.04204v1</link><description>People rely on social skills like conflict resolution to communicateeffectively and to thrive in both work and personal life. However, practiceenvironments for social skills are typically out of reach for most people. Howcan we make social skill training more available, accessible, and inviting?Drawing upon interdisciplinary research from communication and psychology, thisperspective paper identifies social skill barriers to enter specialized fields.Then we present a solution that leverages large language models for socialskill training via a generic framework. Our AI Partner, AI Mentor frameworkmerges experiential learning with realistic practice and tailored feedback.This work ultimately calls for cross-disciplinary innovation to address thebroader implications for workforce development and social equality.</description><author>Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S. Bernstein, John Mitchell</author><pubDate>Fri, 05 Apr 2024 17:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04204v1</guid></item><item><title>Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs</title><link>http://arxiv.org/abs/2403.12553v2</link><description>Existing neural operator architectures face challenges when solvingmultiphysics problems with coupled partial differential equations (PDEs), dueto complex geometries, interactions between physical variables, and the lack oflarge amounts of high-resolution training data. To address these issues, wepropose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functionsalong the codomain or channel space, enabling self-supervised learning orpretraining of multiple PDE systems. Specifically, we extend positionalencoding, self-attention, and normalization layers to the function space.CoDA-NO can learn representations of different PDE systems with a single model.We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEsover multiple systems by considering few-shot learning settings. On complexdownstream tasks with limited data, such as fluid flow simulations andfluid-structure interactions, we found CoDA-NO to outperform existing methodson the few-shot learning task by over $36\%$. The code is available athttps://github.com/ashiq24/CoDA-NO.</description><author>Md Ashiqur Rahman, Robert Joseph George, Mogab Elleithy, Daniel Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A. Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar</author><pubDate>Fri, 05 Apr 2024 17:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12553v2</guid></item><item><title>Deep-learning Segmentation of Small Volumes in CT images for Radiotherapy Treatment Planning</title><link>http://arxiv.org/abs/2404.04202v1</link><description>Our understanding of organs at risk is progressing to include physical smalltissues such as coronary arteries and the radiosensitivities of many smallorgans and tissues are high. Therefore, the accurate segmentation of smallvolumes in external radiotherapy is crucial to protect them fromover-irradiation. Moreover, with the development of the particle therapy andon-board imaging, the treatment becomes more accurate and precise. The purposeof this work is to optimize organ segmentation algorithms for small organs. Weused 50 three-dimensional (3-D) computed tomography (CT) head and neck imagesfrom StructSeg2019 challenge to develop a general-purpose V-Net model tosegment 20 organs in the head and neck region. We applied specific strategiesto improve the segmentation accuracy of the small volumes in this anatomicalregion, i.e., the lens of the eye. Then, we used 17 additional head images fromOSF healthcare to validate the robustness of the V Net model optimized forsmall-volume segmentation. With the study of the StructSeg2019 images, we foundthat the optimization of the image normalization range and classificationthreshold yielded a segmentation improvement of the lens of the eye ofapproximately 50%, compared to the use of the V-Net not optimized for smallvolumes. We used the optimized model to segment 17 images acquired usingheterogeneous protocols. We obtained comparable Dice coefficient values for theclinical and StructSeg2019 images (0.61 plus/minus 0.07 and 0.58 plus/minus0.10 for the left and right lens of the eye, respectively)</description><author>Jianxin Zhou, Kadishe Fejza, Massimiliano Salvatori, Daniele Della Latta, Gregory M. Hermann, Angela Di Fulvio</author><pubDate>Fri, 05 Apr 2024 17:25:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04202v1</guid></item><item><title>LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud</title><link>http://arxiv.org/abs/2309.17157v5</link><description>In the current user-server interaction paradigm of prompted generation withlarge language models (LLM) on cloud, the server fully controls the generationprocess, which leaves zero options for users who want to keep the generatedtext to themselves. We propose LatticeGen, a cooperative framework in which theserver still handles most of the computation while the user controls thesampling operation. The key idea is that the true generated sequence is mixedwith noise tokens by the user and hidden in a noised lattice. Consideringpotential attacks from a hypothetically malicious server and how the user candefend against it, we propose the repeated beam-search attack and the mixingnoise scheme. In our experiments we apply LatticeGen to protect both prompt andgeneration. It is shown that while the noised lattice degrades generationquality, LatticeGen successfully protects the true generation to a remarkabledegree under strong attacks (more than 50% of the semantic remains hidden asmeasured by BERTScore).</description><author>Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov</author><pubDate>Fri, 05 Apr 2024 17:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17157v5</guid></item><item><title>Exploring Probabilistic Models for Semi-supervised Learning</title><link>http://arxiv.org/abs/2404.04199v1</link><description>This thesis studies advanced probabilistic models, including both theirtheoretical foundations and practical applications, for differentsemi-supervised learning (SSL) tasks. The proposed probabilistic methods areable to improve the safety of AI systems in real applications by providingreliable uncertainty estimates quickly, and at the same time, achievecompetitive performance compared to their deterministic counterparts. Theexperimental results indicate that the methods proposed in the thesis havegreat value in safety-critical areas, such as the autonomous driving or medicalimaging analysis domain, and pave the way for the future discovery of highlyeffective and efficient probabilistic approaches in the SSL sector.</description><author>Jianfeng Wang</author><pubDate>Fri, 05 Apr 2024 17:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04199v1</guid></item><item><title>EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation</title><link>http://arxiv.org/abs/2403.01482v4</link><description>Semantic segmentation has innately relied on extensive pixel-level annotateddata, leading to the emergence of unsupervised methodologies. Among them,leveraging self-supervised Vision Transformers for unsupervised semanticsegmentation (USS) has been making steady progress with expressive deepfeatures. Yet, for semantically segmenting images with complex objects, apredominant challenge remains: the lack of explicit object-level semanticencoding in patch-level features. This technical limitation often leads toinadequate segmentation of complex objects with diverse structures. To addressthis gap, we present a novel approach, EAGLE, which emphasizes object-centricrepresentation learning for unsupervised semantic segmentation. Specifically,we introduce EiCue, a spectral technique providing semantic and structural cuesthrough an eigenbasis derived from the semantic similarity matrix of deep imagefeatures and color affinity from an image. Further, by incorporating ourobject-centric contrastive loss with EiCue, we guide our model to learnobject-level representations with intra- and inter-image object-featureconsistency, thereby enhancing semantic accuracy. Extensive experiments onCOCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-artUSS results of EAGLE with accurate and consistent semantic segmentation acrosscomplex scenes.</description><author>Chanyoung Kim, Woojung Han, Dayun Ju, Seong Jae Hwang</author><pubDate>Fri, 05 Apr 2024 17:11:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01482v4</guid></item><item><title>On Inherent Adversarial Robustness of Active Vision Systems</title><link>http://arxiv.org/abs/2404.00185v2</link><description>Current Deep Neural Networks are vulnerable to adversarial examples, whichalter their predictions by adding carefully crafted noise. Since human eyes arerobust to such inputs, it is possible that the vulnerability stems from thestandard way of processing inputs in one shot by processing every pixel withthe same importance. In contrast, neuroscience suggests that the human visionsystem can differentiate salient features by (1) switching between multiplefixation points (saccades) and (2) processing the surrounding with anon-uniform external resolution (foveation). In this work, we advocate that theintegration of such active vision mechanisms into current deep learning systemscan offer robustness benefits. Specifically, we empirically demonstrate theinherent robustness of two active vision methods - GFNet and FALcon - under ablack box threat model. By learning and inferencing based on downsampledglimpses obtained from multiple distinct fixation points within an input, weshow that these active methods achieve (2-3) times greater robustness comparedto a standard passive convolutional network under state-of-the-art adversarialattacks. More importantly, we provide illustrative and interpretablevisualization analysis that demonstrates how performing inference from distinctfixation points makes active vision methods less vulnerable to maliciousinputs.</description><author>Amitangshu Mukherjee, Timur Ibrayev, Kaushik Roy</author><pubDate>Fri, 05 Apr 2024 17:10:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00185v2</guid></item><item><title>Contrastive UCB: Provably Efficient Contrastive Self-Supervised Learning in Online Reinforcement Learning</title><link>http://arxiv.org/abs/2207.14800v2</link><description>In view of its power in extracting feature representation, contrastiveself-supervised learning has been successfully integrated into the practice of(deep) reinforcement learning (RL), leading to efficient policy learning invarious applications. Despite its tremendous empirical successes, theunderstanding of contrastive learning for RL remains elusive. To narrow such agap, we study how RL can be empowered by contrastive learning in a class ofMarkov decision processes (MDPs) and Markov games (MGs) with low-ranktransitions. For both models, we propose to extract the correct featurerepresentations of the low-rank model by minimizing a contrastive loss.Moreover, under the online setting, we propose novel upper confidence bound(UCB)-type algorithms that incorporate such a contrastive loss with online RLalgorithms for MDPs or MGs. We further theoretically prove that our algorithmrecovers the true representations and simultaneously achieves sample efficiencyin learning the optimal policy and Nash equilibrium in MDPs and MGs. We alsoprovide empirical studies to demonstrate the efficacy of the UCB-basedcontrastive learning method for RL. To the best of our knowledge, we providethe first provably efficient online RL algorithm that incorporates contrastivelearning for representation learning. Our codes are available athttps://github.com/Baichenjia/Contrastive-UCB.</description><author>Shuang Qiu, Lingxiao Wang, Chenjia Bai, Zhuoran Yang, Zhaoran Wang</author><pubDate>Fri, 05 Apr 2024 17:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.14800v2</guid></item><item><title>SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians</title><link>http://arxiv.org/abs/2403.10427v2</link><description>Implicit neural representation methods have shown impressive advancements inlearning 3D scenes from unstructured in-the-wild photo collections but arestill limited by the large computational cost of volumetric rendering. Morerecently, 3D Gaussian Splatting emerged as a much faster alternative withsuperior rendering quality and training efficiency, especially for small-scaleand object-centric scenarios. Nevertheless, this technique suffers from poorperformance on unstructured in-the-wild data. To tackle this, we extend over 3DGaussian Splatting to handle unstructured image collections. We achieve this bymodeling appearance to seize photometric variations in the rendered images.Additionally, we introduce a new mechanism to train transient Gaussians tohandle the presence of scene occluders in an unsupervised manner. Experimentson diverse photo collection scenes and multi-pass acquisition of outdoorlandmarks show the effectiveness of our method over prior works achievingstate-of-the-art results with improved efficiency.</description><author>Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou</author><pubDate>Fri, 05 Apr 2024 17:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10427v2</guid></item><item><title>Efficient Detection of Exchangeable Factors in Factor Graphs</title><link>http://arxiv.org/abs/2403.10167v2</link><description>To allow for tractable probabilistic inference with respect to domain sizes,lifted probabilistic inference exploits symmetries in probabilistic graphicalmodels. However, checking whether two factors encode equivalent semantics andhence are exchangeable is computationally expensive. In this paper, weefficiently solve the problem of detecting exchangeable factors in a factorgraph. In particular, we introduce the detection of exchangeable factors (DEFT)algorithm, which allows us to drastically reduce the computational effort forchecking whether two factors are exchangeable in practice. While previousapproaches iterate all $O(n!)$ permutations of a factor's argument list in theworst case (where $n$ is the number of arguments of the factor), we prove thatDEFT efficiently identifies restrictions to drastically reduce the number ofpermutations and validate the efficiency of DEFT in our empirical evaluation.</description><author>Malte Luttermann, Johann Machemer, Marcel Gehrke</author><pubDate>Fri, 05 Apr 2024 17:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10167v2</guid></item><item><title>Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection</title><link>http://arxiv.org/abs/2404.04188v1</link><description>The growing cybersecurity threats make it essential to use high-quality datato train Machine Learning (ML) models for network traffic analysis, withoutnoisy or missing data. By selecting the most relevant features for cyber-attackdetection, it is possible to improve both the robustness and computationalefficiency of the models used in a cybersecurity system. This work presents afeature selection and consensus process that combines multiple methods andapplies them to several network datasets. Two different feature sets wereselected and were used to train multiple ML models with regular and adversarialtraining. Finally, an adversarial evasion robustness benchmark was performed toanalyze the reliability of the different feature sets and their impact on thesusceptibility of the models to adversarial examples. By using an improveddataset with more data diversity, selecting the best time-related features anda more specific feature set, and performing adversarial training, the ML modelswere able to achieve a better adversarially robust generalization. Therobustness of the models was significantly improved without theirgeneralization to regular traffic flows being affected, without increases offalse alarms, and without requiring too many computational resources, whichenables a reliable detection of suspicious activity and perturbed traffic flowsin enterprise computer networks.</description><author>João Vitorino, Miguel Silva, Eva Maia, Isabel Praça</author><pubDate>Fri, 05 Apr 2024 17:01:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04188v1</guid></item><item><title>SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers</title><link>http://arxiv.org/abs/2404.04179v1</link><description>Traditional deep learning-based object detection networks often resize imagesduring the data preprocessing stage to achieve a uniform size and scale in thefeature map. Resizing is done to facilitate model propagation and fullyconnected classification. However, resizing inevitably leads to objectdeformation and loss of valuable information in the images. This drawbackbecomes particularly pronounced for tiny objects like distribution towers withlinear shapes and few pixels. To address this issue, we propose abandoning theresizing operation. Instead, we introduce Positional-Encoding Multi-headCriss-Cross Attention. This allows the model to capture contextual informationand learn from multiple representation subspaces, effectively enriching thesemantics of distribution towers. Additionally, we enhance Spatial PyramidPooling by reshaping three pooled feature maps into a new unified one whilealso reducing the computational burden. This approach allows images ofdifferent sizes and scales to generate feature maps with uniform dimensions andcan be employed in feature map propagation. Our SCAResNet incorporates theseaforementioned improvements into the backbone network ResNet. We evaluated ourSCAResNet using the Electric Transmission and Distribution InfrastructureImagery dataset from Duke University. Without any additional tricks, weemployed various object detection models with Gaussian Receptive Field basedLabel Assignment as the baseline. When incorporating the SCAResNet into thebaseline model, we achieved a 2.1% improvement in mAPs. This demonstrates theadvantages of our SCAResNet in detecting transmission and distribution towersand its value in tiny object detection. The source code is available athttps://github.com/LisavilaLee/SCAResNet_mmdet.</description><author>Weile Li, Muqing Shi, Zhonghua Hong</author><pubDate>Fri, 05 Apr 2024 16:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04179v1</guid></item><item><title>Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems</title><link>http://arxiv.org/abs/2403.13869v3</link><description>Intelligent systems are increasingly integral to our daily lives, yet raresafety-critical events present significant latent threats to their practicaldeployment. Addressing this challenge hinges on accurately predicting theprobability of safety-critical events occurring within a given time step fromthe current state, a metric we define as 'criticality'. The complexity ofpredicting criticality arises from the extreme data imbalance caused by rareevents in high dimensional variables associated with the rare events, achallenge we refer to as the curse of rarity. Existing methods tend to beeither overly conservative or prone to overlooking safety-critical events, thusstruggling to achieve both high precision and recall rates, which severelylimits their applicability. This study endeavors to develop a criticalityprediction model that excels in both precision and recall rates for evaluatingthe criticality of safety-critical autonomous systems. We propose a multi-stagelearning framework designed to progressively densify the dataset, mitigatingthe curse of rarity across stages. To validate our approach, we evaluate it intwo cases: lunar lander and bipedal walker scenarios. The results demonstratethat our method surpasses traditional approaches, providing a more accurate anddependable assessment of criticality in intelligent systems.</description><author>Ruoxuan Bai, Jingxuan Yang, Weiduo Gong, Yi Zhang, Qiujing Lu, Shuo Feng</author><pubDate>Fri, 05 Apr 2024 16:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13869v3</guid></item><item><title>Embedded Heterogeneous Attention Transformer for Cross-lingual Image Captioning</title><link>http://arxiv.org/abs/2307.09915v2</link><description>Cross-lingual image captioning is a challenging task that requires addressingboth cross-lingual and cross-modal obstacles in multimedia analysis. Thecrucial issue in this task is to model the global and the local matchingbetween the image and different languages. Existing cross-modal embeddingmethods based on the transformer architecture oversee the local matchingbetween the image region and monolingual words, especially when dealing withdiverse languages. To overcome these limitations, we propose an EmbeddedHeterogeneous Attention Transformer (EHAT) to establish cross-domainrelationships and local correspondences between images and different languagesby using a heterogeneous network. EHAT comprises Masked HeterogeneousCross-attention (MHCA), Heterogeneous Attention Reasoning Network (HARN), andHeterogeneous Co-attention (HCA). The HARN serves as the core network and itcaptures cross-domain relationships by leveraging visual bounding boxrepresentation features to connect word features from two languages and tolearn heterogeneous maps. MHCA and HCA facilitate cross-domain integration inthe encoder through specialized heterogeneous attention mechanisms, enabling asingle model to generate captions in two languages. We evaluate our approach onthe MSCOCO dataset to generate captions in English and Chinese, two languagesthat exhibit significant differences in their language families. Theexperimental results demonstrate the superior performance of our methodcompared to existing advanced monolingual methods. Our proposed EHAT frameworkeffectively addresses the challenges of cross-lingual image captioning, pavingthe way for improved multilingual image analysis and understanding.</description><author>Zijie Song, Zhenzhen Hu, Yuanen Zhou, Ye Zhao, Richang Hong, Meng Wang</author><pubDate>Fri, 05 Apr 2024 16:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09915v2</guid></item><item><title>Multimodal Learning for Materials</title><link>http://arxiv.org/abs/2312.00111v2</link><description>Artificial intelligence is transforming computational materials science,improving the prediction of material properties, and accelerating the discoveryof novel materials. Recently, publicly available material data repositorieshave grown rapidly. This growth encompasses not only more materials, but also agreater variety and quantity of their associated properties. Existing machinelearning efforts in materials science focus primarily on single-modality tasks,i.e., relationships between materials and a single physical property, thus nottaking advantage of the rich and multimodal set of material properties. Here,we introduce Multimodal Learning for Materials (MultiMat), which enablesself-supervised multi-modality training of foundation models for materials. Wedemonstrate our framework's potential using data from the Materials Projectdatabase on multiple axes: (i) MultiMat achieves state-of-the-art performancefor challenging material property prediction tasks; (ii) MultiMat enables noveland accurate material discovery via latent space similarity, enabling screeningfor stable materials with desired properties; and (iii) MultiMat encodesinterpretable emergent features that may provide novel scientific insights.</description><author>Viggo Moro, Charlotte Loh, Rumen Dangovski, Ali Ghorashi, Andrew Ma, Zhuo Chen, Peter Y. Lu, Thomas Christensen, Marin Soljačić</author><pubDate>Fri, 05 Apr 2024 16:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00111v2</guid></item><item><title>Self-Correcting Self-Consuming Loops for Generative Model Training</title><link>http://arxiv.org/abs/2402.07087v2</link><description>As synthetic data becomes higher quality and proliferates on the internet,machine learning models are increasingly trained on a mix of human- andmachine-generated data. Despite the successful stories of using synthetic datafor representation learning, using synthetic data for generative model trainingcreates "self-consuming loops" which may lead to training instability or evencollapse, unless certain conditions are met. Our paper aims to stabilizeself-consuming generative model training. Our theoretical results demonstratethat by introducing an idealized correction function, which maps a data pointto be more likely under the true data distribution, self-consuming loops can bemade exponentially more stable. We then propose self-correction functions,which rely on expert knowledge (e.g. the laws of physics programmed in asimulator), and aim to approximate the idealized corrector automatically and atscale. We empirically validate the effectiveness of self-correctingself-consuming loops on the challenging human motion synthesis task, andobserve that it successfully avoids model collapse, even when the ratio ofsynthetic data to real data is as high as 100%.</description><author>Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun</author><pubDate>Fri, 05 Apr 2024 16:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07087v2</guid></item><item><title>Individualized Dynamic Model for Multi-resolutional Data with Application to Mobile Health</title><link>http://arxiv.org/abs/2311.12392v3</link><description>Mobile health has emerged as a major success for tracking individual healthstatus, due to the popularity and power of smartphones and wearable devices.This has also brought great challenges in handling heterogeneous,multi-resolution data which arise ubiquitously in mobile health due toirregular multivariate measurements collected from individuals. In this paper,we propose an individualized dynamic latent factor model for irregularmulti-resolution time series data to interpolate unsampled measurements of timeseries with low resolution. One major advantage of the proposed method is thecapability to integrate multiple irregular time series and multiple subjects bymapping the multi-resolution data to the latent space. In addition, theproposed individualized dynamic latent factor model is applicable to capturingheterogeneous longitudinal information through individualized dynamic latentfactors. Our theory provides a bound on the integrated interpolation error andthe convergence rate for B-spline approximation methods. Both the simulationstudies and the application to smartwatch data demonstrate the superiorperformance of the proposed method compared to existing methods.</description><author>Jiuchen Zhang, Fei Xue, Qi Xu, Jung-Ah Lee, Annie Qu</author><pubDate>Fri, 05 Apr 2024 16:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12392v3</guid></item><item><title>H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations</title><link>http://arxiv.org/abs/2404.04173v1</link><description>Disentangling attributes of various sensory signals is central to human-likeperception and reasoning and a critical task for higher-order cognitive andneuro-symbolic AI systems. An elegant approach to represent this intricatefactorization is via high-dimensional holographic vectors drawing onbrain-inspired vector symbolic architectures. However, holographicfactorization involves iterative computation with high-dimensionalmatrix-vector multiplications and suffers from non-convergence problems. In this paper, we present H3DFact, a heterogeneous 3D integrated in-memorycompute engine capable of efficiently factorizing high-dimensional holographicrepresentations. H3DFact exploits the computation-in-superposition capabilityof holographic vectors and the intrinsic stochasticity associated withmemristive-based 3D compute-in-memory. Evaluated on large-scale factorizationand perceptual problems, H3DFact demonstrates superior capability infactorization accuracy and operational capacity by up to five orders ofmagnitude, with 5.5x compute density, 1.2x energy efficiency improvements, and5.9x less silicon footprint compared to iso-capacity 2D designs.</description><author>Zishen Wan, Che-Kai Liu, Mohamed Ibrahim, Hanchen Yang, Samuel Spetalnick, Tushar Krishna, Arijit Raychowdhury</author><pubDate>Fri, 05 Apr 2024 16:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04173v1</guid></item><item><title>Diffusion Model-based Probabilistic Downscaling for 180-year East Asian Climate Reconstruction</title><link>http://arxiv.org/abs/2402.06646v2</link><description>As our planet is entering into the "global boiling" era, understandingregional climate change becomes imperative. Effective downscaling methods thatprovide localized insights are crucial for this target. Traditional approaches,including computationally-demanding regional dynamical models or statisticaldownscaling frameworks, are often susceptible to the influence of downscalinguncertainty. Here, we address these limitations by introducing a diffusionprobabilistic downscaling model (DPDM) into the meteorological field. Thismodel can efficiently transform data from 1{\deg} to 0.1{\deg} resolution.Compared with deterministic downscaling schemes, it not only has more accuratelocal details, but also can generate a large number of ensemble members basedon probability distribution sampling to evaluate the uncertainty ofdownscaling. Additionally, we apply the model to generate a 180-year dataset ofmonthly surface variables in East Asia, offering a more detailed perspectivefor understanding local scale climate change over the past centuries.</description><author>Fenghua Ling, Zeyu Lu, Jing-Jia Luo, Lei Bai, Swadhin K. Behera, Dachao Jin, Baoxiang Pan, Huidong Jiang, Toshio Yamagata</author><pubDate>Fri, 05 Apr 2024 16:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06646v2</guid></item><item><title>Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text?</title><link>http://arxiv.org/abs/2404.04169v1</link><description>Sentence transformers are language models designed to perform semanticsearch. This study investigates the capacity of sentence transformers,fine-tuned on general question-answering datasets for asymmetric semanticsearch, to associate descriptions of human-generated routes across GreatBritain with queries often used to describe hiking experiences. We find thatsentence transformers have some zero-shot capabilities to understandquasi-geospatial concepts, such as route types and difficulty, suggesting theirpotential utility for routing recommendation systems.</description><author>Ilya Ilyankou, Aldo Lipani, Stefano Cavazzi, Xiaowei Gao, James Haworth</author><pubDate>Fri, 05 Apr 2024 16:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04169v1</guid></item><item><title>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</title><link>http://arxiv.org/abs/2311.08046v3</link><description>Large language models have demonstrated impressive universal capabilitiesacross a wide range of open-ended tasks and have extended their utility toencompass multimodal conversations. However, existing methods encounterchallenges in effectively handling both image and video understanding,particularly with limited visual tokens. In this work, we introduce Chat-UniVi,a Unified Vision-language model capable of comprehending and engaging inconversations involving images and videos through a unified visualrepresentation. Specifically, we employ a set of dynamic visual tokens touniformly represent images and videos. This representation framework empowersthe model to efficiently utilize a limited number of visual tokens tosimultaneously capture the spatial details necessary for images and thecomprehensive temporal relationship required for videos. Moreover, we leveragea multi-scale representation, enabling the model to perceive both high-levelsemantic concepts and low-level visual details. Notably, Chat-UniVi is trainedon a mixed dataset containing both images and videos, allowing directapplication to tasks involving both mediums without requiring anymodifications. Extensive experimental results demonstrate that Chat-UniViconsistently outperforms even existing methods exclusively designed for eitherimages or videos. Code is available athttps://github.com/PKU-YuanGroup/Chat-UniVi.</description><author>Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan</author><pubDate>Fri, 05 Apr 2024 16:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08046v3</guid></item><item><title>Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model</title><link>http://arxiv.org/abs/2404.04167v1</link><description>In this study, we introduce CT-LLM, a 2B large language model (LLM) thatillustrates a pivotal shift towards prioritizing the Chinese language indeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from theconventional methodology by primarily incorporating Chinese textual data,utilizing an extensive corpus of 1,200 billion tokens, including 800 billionChinese tokens, 300 billion English tokens, and 100 billion code tokens. Thisstrategic composition facilitates the model's exceptional proficiency inunderstanding and processing Chinese, a capability further enhanced throughalignment techniques. Demonstrating remarkable performance on the CHC-Bench,CT-LLM excels in Chinese language tasks, and showcases its adeptness in Englishthrough SFT. This research challenges the prevailing paradigm of training LLMspredominantly on English corpora and then adapting them to other languages,broadening the horizons for LLM training methodologies. By open-sourcing thefull process of training a Chinese LLM, including a detailed data processingprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to fosterfurther exploration and innovation in both academia and industry, paving theway for more inclusive and versatile language models.</description><author>Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang</author><pubDate>Fri, 05 Apr 2024 16:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04167v1</guid></item><item><title>AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators</title><link>http://arxiv.org/abs/2303.16854v2</link><description>Many natural language processing (NLP) tasks rely on labeled data to trainmachine learning models with high performance. However, data annotation istime-consuming and expensive, especially when the task involves a large amountof data or requires specialized domains. Recently, GPT-3.5 series models havedemonstrated remarkable few-shot and zero-shot ability across various NLPtasks. In this paper, we first claim that large language models (LLMs), such asGPT-3.5, can serve as an excellent crowdsourced annotator when provided withsufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM,an annotation system powered by LLMs, which adopts a two-step approach,explain-then-annotate. Concretely, we first prompt LLMs to provide explanationsfor why the specific ground truth answer/label was assigned for a givenexample. Then, we construct the few-shot chain-of-thought prompt with theself-generated explanation and employ it to annotate the unlabeled data withLLMs. Our experiment results on three tasks, including user input and keywordrelevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses orperforms on par with crowdsourced annotators. Furthermore, we build the firstconversation-based information retrieval dataset employing AnnoLLM. Thisdataset is designed to facilitate the development of retrieval models capableof retrieving pertinent documents for conversational text. Human evaluation hasvalidated the dataset's high quality.</description><author>Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen</author><pubDate>Fri, 05 Apr 2024 16:19:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16854v2</guid></item><item><title>Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval</title><link>http://arxiv.org/abs/2404.04163v1</link><description>This study investigates the existence of positional biases inTransformer-based models for text representation learning, particularly in thecontext of web document retrieval. We build on previous research thatdemonstrated loss of information in the middle of input sequences for causallanguage models, extending it to the domain of representation learning. Weexamine positional biases at various stages of training for an encoder-decodermodel, including language model pre-training, contrastive pre-training, andcontrastive fine-tuning. Experiments with the MS-MARCO document collectionreveal that after contrastive pre-training the model already generatesembeddings that better capture early contents of the input, with fine-tuningfurther aggravating this effect.</description><author>João Coelho, Bruno Martins, João Magalhães, Jamie Callan, Chenyan Xiong</author><pubDate>Fri, 05 Apr 2024 16:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04163v1</guid></item><item><title>Noisy Label Processing for Classification: A Survey</title><link>http://arxiv.org/abs/2404.04159v1</link><description>In recent years, deep neural networks (DNNs) have gained remarkableachievement in computer vision tasks, and the success of DNNs often dependsgreatly on the richness of data. However, the acquisition process of data andhigh-quality ground truth requires a lot of manpower and money. In the long,tedious process of data annotation, annotators are prone to make mistakes,resulting in incorrect labels of images, i.e., noisy labels. The emergence ofnoisy labels is inevitable. Moreover, since research shows that DNNs can easilyfit noisy labels, the existence of noisy labels will cause significant damageto the model training process. Therefore, it is crucial to combat noisy labelsfor computer vision tasks, especially for classification tasks. In this survey,we first comprehensively review the evolution of different deep learningapproaches for noisy label combating in the image classification task. Inaddition, we also review different noise patterns that have been proposed todesign robust algorithms. Furthermore, we explore the inner pattern ofreal-world label noise and propose an algorithm to generate a synthetic labelnoise pattern guided by real-world data. We test the algorithm on thewell-known real-world dataset CIFAR-10N to form a new real-world data-guidedsynthetic benchmark and evaluate some typical noise-robust methods on thebenchmark.</description><author>Mengting Li, Chuang Zhu</author><pubDate>Fri, 05 Apr 2024 16:11:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04159v1</guid></item><item><title>MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector</title><link>http://arxiv.org/abs/2404.04155v1</link><description>The segmentation and interpretation of the Martian surface play a pivotalrole in Mars exploration, providing essential data for the trajectory planningand obstacle avoidance of rovers. However, the complex topography, similarsurface features, and the lack of extensive annotated data pose significantchallenges to the high-precision semantic segmentation of the Martian surface.To address these challenges, we propose a novel encoder-decoder based Marssegmentation network, termed MarsSeg. Specifically, we employ anencoder-decoder structure with a minimized number of down-sampling layers topreserve local details. To facilitate a high-level semantic understandingacross the shadow multi-level feature maps, we introduce a feature enhancementconnection layer situated between the encoder and decoder. This layerincorporates Mini Atrous Spatial Pyramid Pooling (Mini-ASPP), PolarizedSelf-Attention (PSA), and Strip Pyramid Pooling Module (SPPM). The Mini-ASPPand PSA are specifically designed for shadow feature enhancement, therebyenabling the expression of local details and small objects. Conversely, theSPPM is employed for deep feature enhancement, facilitating the extraction ofhigh-level semantic category-related information. Experimental results derivedfrom the Mars-Seg and AI4Mars datasets substantiate that the proposed MarsSegoutperforms other state-of-the-art methods in segmentation performance,validating the efficacy of each proposed component.</description><author>Junbo Li, Keyan Chen, Gengju Tian, Lu Li, Zhenwei Shi</author><pubDate>Fri, 05 Apr 2024 16:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04155v1</guid></item><item><title>Finding Outliers in Gaussian Model-Based Clustering</title><link>http://arxiv.org/abs/1907.01136v5</link><description>Clustering, or unsupervised classification, is a task often plagued byoutliers. Yet there is a paucity of work on handling outliers in clustering.Outlier identification algorithms tend to fall into three broad categories:outlier inclusion, outlier trimming, and \textit{post hoc} outlieridentification methods, with the former two often requiring pre-specificationof the number of outliers. The fact that sample Mahalanobis distance isbeta-distributed is used to derive an approximate distribution for thelog-likelihoods of subset finite Gaussian mixture models. An algorithm is thenproposed that removes the least plausible points according to the subsetlog-likelihoods, which are deemed outliers, until the subset log-likelihoodsadhere to the reference distribution. This results in a trimming method, calledOCLUST, that inherently estimates the number of outliers.</description><author>Katharine M. Clark, Paul D. McNicholas</author><pubDate>Fri, 05 Apr 2024 16:01:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1907.01136v5</guid></item><item><title>PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce Lidar</title><link>http://arxiv.org/abs/2312.14239v2</link><description>3D reconstruction from a single-view is challenging because of the ambiguityfrom monocular cues and lack of information about occluded regions. Neuralradiance fields (NeRF), while popular for view synthesis and 3D reconstruction,are typically reliant on multi-view images. Existing methods for single-view 3Dreconstruction with NeRF rely on either data priors to hallucinate views ofoccluded regions, which may not be physically accurate, or shadows observed byRGB cameras, which are difficult to detect in ambient light and low albedobackgrounds. We propose using time-of-flight data captured by a single-photonavalanche diode to overcome these limitations. Our method models two-bounceoptical paths with NeRF, using lidar transient data for supervision. Byleveraging the advantages of both NeRF and two-bounce light measured by lidar,we demonstrate that we can reconstruct visible and occluded geometry withoutdata priors or reliance on controlled ambient lighting or scene albedo. Inaddition, we demonstrate improved generalization under practical constraints onsensor spatial- and temporal-resolution. We believe our method is a promisingdirection as single-photon lidars become ubiquitous on consumer devices, suchas phones, tablets, and headsets.</description><author>Tzofi Klinghoffer, Xiaoyu Xiang, Siddharth Somasundaram, Yuchen Fan, Christian Richardt, Ramesh Raskar, Rakesh Ranjan</author><pubDate>Fri, 05 Apr 2024 16:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14239v2</guid></item><item><title>Plug-and-Play image restoration with Stochastic deNOising REgularization</title><link>http://arxiv.org/abs/2402.01779v2</link><description>Plug-and-Play (PnP) algorithms are a class of iterative algorithms thataddress image inverse problems by combining a physical model and a deep neuralnetwork for regularization. Even if they produce impressive image restorationresults, these algorithms rely on a non-standard use of a denoiser on imagesthat are less and less noisy along the iterations, which contrasts with recentalgorithms based on Diffusion Models (DM), where the denoiser is applied onlyon re-noised images. We propose a new PnP framework, called StochasticdeNOising REgularization (SNORE), which applies the denoiser only on imageswith noise of the adequate level. It is based on an explicit stochasticregularization, which leads to a stochastic gradient descent algorithm to solveill-posed inverse problems. A convergence analysis of this algorithm and itsannealing extension is provided. Experimentally, we prove that SNORE iscompetitive with respect to state-of-the-art methods on deblurring andinpainting tasks, both quantitatively and qualitatively.</description><author>Marien Renaud, Jean Prost, Arthur Leclaire, Nicolas Papadakis</author><pubDate>Fri, 05 Apr 2024 15:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01779v2</guid></item><item><title>EGTR: Extracting Graph from Transformer for Scene Graph Generation</title><link>http://arxiv.org/abs/2404.02072v3</link><description>Scene Graph Generation (SGG) is a challenging task of detecting objects andpredicting relationships between objects. After DETR was developed, one-stageSGG models based on a one-stage object detector have been actively studied.However, complex modeling is used to predict the relationship between objects,and the inherent relationship between object queries learned in the multi-headself-attention of the object detector has been neglected. We propose alightweight one-stage SGG model that extracts the relation graph from thevarious relationships learned in the multi-head self-attention layers of theDETR decoder. By fully utilizing the self-attention by-products, the relationgraph can be extracted effectively with a shallow relation extraction head.Considering the dependency of the relation extraction task on the objectdetection task, we propose a novel relation smoothing technique that adjuststhe relation label adaptively according to the quality of the detected objects.By the relation smoothing, the model is trained according to the continuouscurriculum that focuses on object detection task at the beginning of trainingand performs multi-task learning as the object detection performance graduallyimproves. Furthermore, we propose a connectivity prediction task that predictswhether a relation exists between object pairs as an auxiliary task of therelation extraction. We demonstrate the effectiveness and efficiency of ourmethod for the Visual Genome and Open Image V6 datasets. Our code is publiclyavailable at https://github.com/naver-ai/egtr.</description><author>Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park</author><pubDate>Fri, 05 Apr 2024 15:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02072v3</guid></item><item><title>Open-vocabulary object 6D pose estimation</title><link>http://arxiv.org/abs/2312.00690v3</link><description>We introduce the new setting of open-vocabulary object 6D pose estimation, inwhich a textual prompt is used to specify the object of interest. In contrastto existing approaches, in our setting (i) the object of interest is specifiedsolely through the textual prompt, (ii) no object model (e.g., CAD or videosequence) is required at inference, and (iii) the object is imaged from twoRGBD viewpoints of different scenes. To operate in this setting, we introduce anovel approach that leverages a Vision-Language Model to segment the object ofinterest from the scenes and to estimate its relative 6D pose. The key of ourapproach is a carefully devised strategy to fuse object-level informationprovided by the prompt with local image features, resulting in a feature spacethat can generalize to novel concepts. We validate our approach on a newbenchmark based on two popular datasets, REAL275 and Toyota-Light, whichcollectively encompass 34 object instances appearing in four thousand imagepairs. The results demonstrate that our approach outperforms both awell-established hand-crafted method and a recent deep learning-based baselinein estimating the relative 6D pose of objects in different scenes. Code anddataset are available at https://jcorsetti.github.io/oryon.</description><author>Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, Fabio Poiesi</author><pubDate>Fri, 05 Apr 2024 15:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00690v3</guid></item><item><title>Improving Detection in Aerial Images by Capturing Inter-Object Relationships</title><link>http://arxiv.org/abs/2404.04140v1</link><description>In many image domains, the spatial distribution of objects in a sceneexhibits meaningful patterns governed by their semantic relationships. In mostmodern detection pipelines, however, the detection proposals are processedindependently, overlooking the underlying relationships between objects. Inthis work, we introduce a transformer-based approach to capture theseinter-object relationships to refine classification and regression outcomes fordetected objects. Building on two-stage detectors, we tokenize the region ofinterest (RoI) proposals to be processed by a transformer encoder. Specificspatial and geometric relations are incorporated into the attention weights andadaptively modulated and regularized. Experimental results demonstrate that theproposed method achieves consistent performance improvement on three benchmarksincluding DOTA-v1.0, DOTA-v1.5, and HRSC 2016, especially ranking first on bothDOTA-v1.5 and HRSC 2016. Specifically, our new method has an increase of 1.59mAP on DOTA-v1.0, 4.88 mAP on DOTA-v1.5, and 2.1 mAP on HRSC 2016,respectively, compared to the baselines.</description><author>Botao Ren, Botian Xu, Yifan Pu, Jingyi Wang, Zhidong Deng</author><pubDate>Fri, 05 Apr 2024 15:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04140v1</guid></item><item><title>Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning</title><link>http://arxiv.org/abs/2404.04139v1</link><description>Federated Learning (FL) is a collaborative learning paradigm enablingparticipants to collectively train a shared machine learning model whilepreserving the privacy of their sensitive data. Nevertheless, the inherentdecentralized and data-opaque characteristics of FL render its susceptibilityto data poisoning attacks. These attacks introduce malformed or maliciousinputs during local model training, subsequently influencing the global modeland resulting in erroneous predictions. Current FL defense strategies againstdata poisoning attacks either involve a trade-off between accuracy androbustness or necessitate the presence of a uniformly distributed root datasetat the server. To overcome these limitations, we present FedZZ, which harnessesa zone-based deviating update (ZBDU) mechanism to effectively counter datapoisoning attacks in FL. Further, we introduce a precision-guided methodologythat actively characterizes these client clusters (zones), which in turn aidsin recognizing and discarding malicious updates at the server. Our evaluationof FedZZ across two widely recognized datasets: CIFAR10 and EMNIST, demonstrateits efficacy in mitigating data poisoning attacks, surpassing the performanceof prevailing state-of-the-art methodologies in both single and multi-clientattack scenarios and varying attack volumes. Notably, FedZZ also functions as arobust client selection strategy, even in highly non-IID and attack-freescenarios. Moreover, in the face of escalating poisoning rates, the modelaccuracy attained by FedZZ displays superior resilience compared to existingtechniques. For instance, when confronted with a 50% presence of maliciousclients, FedZZ sustains an accuracy of 67.43%, while the accuracy of thesecond-best solution, FL-Defender, diminishes to 43.36%.</description><author>K Naveen Kumar, C Krishna Mohan, Aravind Machiry</author><pubDate>Fri, 05 Apr 2024 15:37:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04139v1</guid></item><item><title>The Missing U for Efficient Diffusion Models</title><link>http://arxiv.org/abs/2310.20092v4</link><description>Diffusion Probabilistic Models stand as a critical tool in generativemodelling, enabling the generation of complex data distributions. This familyof generative models yields record-breaking performance in tasks such as imagesynthesis, video generation, and molecule design. Despite their capabilities,their efficiency, especially in the reverse process, remains a challenge due toslow convergence rates and high computational costs. In this paper, weintroduce an approach that leverages continuous dynamical systems to design anovel denoising network for diffusion models that is more parameter-efficient,exhibits faster convergence, and demonstrates increased noise robustness.Experimenting with Denoising Diffusion Probabilistic Models (DDPMs), ourframework operates with approximately a quarter of the parameters, and $\sim$30\% of the Floating Point Operations (FLOPs) compared to standard U-Nets inDDPMs. Furthermore, our model is notably faster in inference than the baselinewhen measured in fair and equal conditions. We also provide a mathematicalintuition as to why our proposed reverse process is faster as well as amathematical discussion of the empirical tradeoffs in the denoising downstreamtask. Finally, we argue that our method is compatible with existing performanceenhancement techniques, enabling further improvements in efficiency, quality,and speed.</description><author>Sergio Calvo-Ordonez, Chun-Wun Cheng, Jiahao Huang, Lipei Zhang, Guang Yang, Carola-Bibiane Schonlieb, Angelica I Aviles-Rivero</author><pubDate>Fri, 05 Apr 2024 15:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20092v4</guid></item><item><title>Prompt-prompted Mixture of Experts for Efficient LLM Generation</title><link>http://arxiv.org/abs/2404.01365v2</link><description>With the development of transformer-based large language models (LLMs), theyhave been applied to many fields due to their remarkable utility, but thiscomes at a considerable computational cost at deployment. Fortunately, somemethods such as pruning or constructing a mixture of experts (MoE) aim atexploiting sparsity in transformer feedforward (FF) blocks to gain boosts inspeed and reduction in memory requirements. However, these techniques can bevery costly and inflexible in practice, as they often require training or arerestricted to specific types of architectures. To address this, we introduceGRIFFIN, a novel training-free MoE that selects unique FF experts at thesequence level for efficient generation across a plethora of LLMs withdifferent non-ReLU activation functions. This is possible due to a criticalobservation that many trained LLMs naturally produce highly structured FFactivation patterns within a sequence, which we call flocking. Despite ourmethod's simplicity, we show with 50% of the FF parameters, GRIFFIN maintainsthe original model's performance with little to no degradation on a variety ofclassification and generation tasks, all while improving latency (e.g.1.25$\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code is available athttps://github.com/hdong920/GRIFFIN.</description><author>Harry Dong, Beidi Chen, Yuejie Chi</author><pubDate>Fri, 05 Apr 2024 15:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01365v2</guid></item><item><title>Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components</title><link>http://arxiv.org/abs/2404.04126v1</link><description>Machine learning plays an important role in the operation of current windenergy production systems. One central application is predictive maintenance toincrease efficiency and lower electricity costs by reducing downtimes.Integrating physics-based knowledge in neural networks to enforce theirphysical plausibilty is a promising method to improve current approaches, butincomplete system information often impedes their application in real worldscenarios. We describe a simple and efficient way for physics-constrained deeplearning-based predictive maintenance for wind turbine gearbox bearings withpartial system knowledge. The approach is based on temperature nowcastingconstrained by physics, where unknown system coefficients are treated aslearnable neural network parameters. Results show improved generalizationperformance to unseen environments compared to a baseline neural network, whichis especially important in low data scenarios often encountered in real-worldapplications.</description><author>Johannes Exenberger, Matteo Di Salvo, Thomas Hirsch, Franz Wotawa, Gerald Schweiger</author><pubDate>Fri, 05 Apr 2024 15:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04126v1</guid></item><item><title>GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System</title><link>http://arxiv.org/abs/2404.04118v1</link><description>We hypothesize that the absence of a standardized benchmark has allowedseveral fundamental pitfalls in GNN System design and evaluation that thecommunity has overlooked. In this work, we propose GNNBench, a plug-and-playbenchmarking platform focused on system innovation. GNNBench presents a newprotocol to exchange their captive tensor data, supports custom classes inSystem APIs, and allows automatic integration of the same system module to manydeep learning frameworks, such as PyTorch and TensorFlow. To demonstrate theimportance of such a benchmark framework, we integrated several GNN systems.Our results show that integration with GNNBench helped us identify severalmeasurement issues that deserve attention from the community.</description><author>Yidong Gong, Pradeep Kumar</author><pubDate>Fri, 05 Apr 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04118v1</guid></item><item><title>Recent Advances, Applications, and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2023 Symposium</title><link>http://arxiv.org/abs/2403.01628v2</link><description>The third ML4H symposium was held in person on December 10, 2023, in NewOrleans, Louisiana, USA. The symposium included research roundtable sessions tofoster discussions between participants and senior researchers on timely andrelevant topics for the \ac{ML4H} community. Encouraged by the successfulvirtual roundtables in the previous year, we organized eleven in-personroundtables and four virtual roundtables at ML4H 2022. The organization of theresearch roundtables at the conference involved 17 Senior Chairs and 19 JuniorChairs across 11 tables. Each roundtable session included invited senior chairs(with substantial experience in the field), junior chairs (responsible forfacilitating the discussion), and attendees from diverse backgrounds withinterest in the session's topic. Herein we detail the organization process andcompile takeaways from these roundtable discussions, including recent advances,applications, and open challenges for each topic. We conclude with a summaryand lessons learned across all roundtables. This document serves as acomprehensive review paper, summarizing the recent advancements in machinelearning for healthcare as contributed by foremost researchers in the field.</description><author>Hyewon Jeong, Sarah Jabbour, Yuzhe Yang, Rahul Thapta, Hussein Mozannar, William Jongwon Han, Nikita Mehandru, Michael Wornow, Vladislav Lialin, Xin Liu, Alejandro Lozano, Jiacheng Zhu, Rafal Dariusz Kocielnik, Keith Harrigian, Haoran Zhang, Edward Lee, Milos Vukadinovic, Aparna Balagopalan, Vincent Jeanselme, Katherine Matton, Ilker Demirel, Jason Fries, Parisa Rashidi, Brett Beaulieu-Jones, Xuhai Orson Xu, Matthew McDermott, Tristan Naumann, Monica Agrawal, Marinka Zitnik, Berk Ustun, Edward Choi, Kristen Yeom, Gamze Gursoy, Marzyeh Ghassemi, Emma Pierson, George Chen, Sanjat Kanjilal, Michael Oberst, Linying Zhang, Harvineet Singh, Tom Hartvigsen, Helen Zhou, Chinasa T. Okolo</author><pubDate>Fri, 05 Apr 2024 15:15:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01628v2</guid></item><item><title>BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models</title><link>http://arxiv.org/abs/2404.04113v1</link><description>Knowledge probing assesses to which degree a language model (LM) hassuccessfully learned relational knowledge during pre-training. Probing is aninexpensive way to compare LMs of different sizes and training configurations.However, previous approaches rely on the objective function used inpre-training LMs and are thus applicable only to masked or causal LMs. As aresult, comparing different types of LMs becomes impossible. To address this,we propose an approach that uses an LM's inherent ability to estimate thelog-likelihood of any given textual statement. We carefully design anevaluation dataset of 7,731 instances (40,916 in a larger variant) from whichwe produce alternative statements for each relational fact, one of which iscorrect. We then evaluate whether an LM correctly assigns the highestlog-likelihood to the correct statement. Our experimental evaluation of 22common LMs shows that our proposed framework, BEAR, can effectively probe forknowledge across different LM types. We release the BEAR datasets and anopen-source framework that implements the probing approach to the researchcommunity to facilitate the evaluation and development of LMs.</description><author>Jacek Wiland, Max Ploner, Alan Akbik</author><pubDate>Fri, 05 Apr 2024 15:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04113v1</guid></item><item><title>The Unreasonable Effectiveness Of Early Discarding After One Epoch In Neural Network Hyperparameter Optimization</title><link>http://arxiv.org/abs/2404.04111v1</link><description>To reach high performance with deep learning, hyperparameter optimization(HPO) is essential. This process is usually time-consuming due to costlyevaluations of neural networks. Early discarding techniques limit the resourcesgranted to unpromising candidates by observing the empirical learning curvesand canceling neural network training as soon as the lack of competitiveness ofa candidate becomes evident. Despite two decades of research, little isunderstood about the trade-off between the aggressiveness of discarding and theloss of predictive performance. Our paper studies this trade-off for severalcommonly used discarding techniques such as successive halving and learningcurve extrapolation. Our surprising finding is that these commonly usedtechniques offer minimal to no added value compared to the simple strategy ofdiscarding after a constant number of epochs of training. The chosen number ofepochs depends mostly on the available compute budget. We call this approachi-Epoch (i being the constant number of epochs with which neural networks aretrained) and suggest to assess the quality of early discarding techniques bycomparing how their Pareto-Front (in consumed training epochs and predictiveperformance) complement the Pareto-Front of i-Epoch.</description><author>Romain Egele, Felix Mohr, Tom Viering, Prasanna Balaprakash</author><pubDate>Fri, 05 Apr 2024 15:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04111v1</guid></item><item><title>DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</title><link>http://arxiv.org/abs/2304.03560v2</link><description>Self-supervised multi-frame depth estimation achieves high accuracy bycomputing matching costs of pixel correspondences between adjacent frames,injecting geometric information into the network. These pixel-correspondencecandidates are computed based on the relative pose estimates between theframes. Accurate pose predictions are essential for precise matching costcomputation as they influence the epipolar geometry. Furthermore, improveddepth estimates can, in turn, be used to align pose estimates. Inspired by traditional structure-from-motion (SfM) principles, we proposethe DualRefine model, which tightly couples depth and pose estimation through afeedback loop. Our novel update pipeline uses a deep equilibrium modelframework to iteratively refine depth estimates and a hidden state of featuremaps by computing local matching costs based on epipolar geometry. Importantly,we used the refined depth estimates and feature maps to compute pose updates ateach step. This update in the pose estimates slowly alters the epipolargeometry during the refinement process. Experimental results on the KITTIdataset demonstrate competitive depth prediction and odometry predictionperformance surpassing published self-supervised baselines.</description><author>Antyanta Bangunharcana, Ahmed Magd, Kyung-Soo Kim</author><pubDate>Fri, 05 Apr 2024 15:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03560v2</guid></item><item><title>Large language models as oracles for instantiating ontologies with domain-specific knowledge</title><link>http://arxiv.org/abs/2404.04108v1</link><description>Background. Endowing intelligent systems with semantic data commonly requiresdesigning and instantiating ontologies with domain-specific knowledge.Especially in the early phases, those activities are typically performedmanually by human experts possibly leveraging on their own experience. Theresulting process is therefore time-consuming, error-prone, and often biased bythe personal background of the ontology designer. Objective. To mitigate thatissue, we propose a novel domain-independent approach to automaticallyinstantiate ontologies with domain-specific knowledge, by leveraging on largelanguage models (LLMs) as oracles. Method. Starting from (i) an initial schemacomposed by inter-related classes andproperties and (ii) a set of querytemplates, our method queries the LLM multi- ple times, and generates instancesfor both classes and properties from its replies. Thus, the ontology isautomatically filled with domain-specific knowledge, compliant to the initialschema. As a result, the ontology is quickly and automatically enriched withmanifold instances, which experts may consider to keep, adjust, discard, orcomplement according to their own needs and expertise. Contribution. Weformalise our method in general way and instantiate it over various LLMs, aswell as on a concrete case study. We report experiments rooted in thenutritional domain where an ontology of food meals and their ingredients issemi-automatically instantiated from scratch, starting from a categorisation ofmeals and their relationships. There, we analyse the quality of the generatedontologies and compare ontologies attained by exploiting different LLMs.Finally, we provide a SWOT analysis of the proposed method.</description><author>Giovanni Ciatto, Andrea Agiollo, Matteo Magnini, Andrea Omicini</author><pubDate>Fri, 05 Apr 2024 15:04:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04108v1</guid></item><item><title>Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report</title><link>http://arxiv.org/abs/2404.04106v1</link><description>Deep Reinforcement Learning (DRL) offers a powerful approach to trainingneural network control policies for stochastic queuing networks (SQN). However,traditional DRL methods rely on offline simulations or static datasets,limiting their real-world application in SQN control. This work proposes OnlineDeep Reinforcement Learning-based Controls (ODRLC) as an alternative, where anintelligent agent interacts directly with a real environment and learns anoptimal control policy from these online interactions. SQNs present a challengefor ODRLC due to the unbounded nature of the queues within the networkresulting in an unbounded state-space. An unbounded state-space is particularlychallenging for neural network policies as neural networks are notoriously poorat extrapolating to unseen states. To address this challenge, we propose anintervention-assisted framework that leverages strategic interventions fromknown stable policies to ensure the queue sizes remain bounded. This frameworkcombines the learning power of neural networks with the guaranteed stability ofclassical control policies for SQNs. We introduce a method to design theseintervention-assisted policies to ensure strong stability of the network.Furthermore, we extend foundational DRL theorems for intervention-assistedpolicies and develop two practical algorithms specifically for ODRLC of SQNs.Finally, we demonstrate through experiments that our proposed algorithmsoutperform both classical control approaches and prior ODRLC algorithms.</description><author>Jerrod Wigmore, Brooke Shrader, Eytan Modiano</author><pubDate>Fri, 05 Apr 2024 15:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04106v1</guid></item><item><title>3D Facial Expressions through Analysis-by-Neural-Synthesis</title><link>http://arxiv.org/abs/2404.04104v1</link><description>While existing methods for 3D face reconstruction from in-the-wild imagesexcel at recovering the overall face shape, they commonly miss subtle, extreme,asymmetric, or rarely observed expressions. We improve upon these methods withSMIRK (Spatial Modeling for Image-based Reconstruction of Kinesics), whichfaithfully reconstructs expressive 3D faces from images. We identify two keylimitations in existing methods: shortcomings in their self-supervised trainingformulation, and a lack of expression diversity in the training images. Fortraining, most methods employ differentiable rendering to compare a predictedface mesh with the input image, along with a plethora of additional lossfunctions. This differentiable rendering loss not only has to providesupervision to optimize for 3D face geometry, camera, albedo, and lighting,which is an ill-posed optimization problem, but the domain gap betweenrendering and input image further hinders the learning process. Instead, SMIRKreplaces the differentiable rendering with a neural rendering module that,given the rendered predicted mesh geometry, and sparsely sampled pixels of theinput image, generates a face image. As the neural rendering gets colorinformation from sampled image pixels, supervising with neural rendering-basedreconstruction loss can focus solely on the geometry. Further, it enables us togenerate images of the input identity with varying expressions while training.These are then utilized as input to the reconstruction model and used assupervision with ground truth geometry. This effectively augments the trainingdata and enhances the generalization for diverse expressions. Our qualitative,quantitative and particularly our perceptual evaluations demonstrate that SMIRKachieves the new state-of-the art performance on accurate expressionreconstruction. Project webpage: https://georgeretsi.github.io/smirk/.</description><author>George Retsinas, Panagiotis P. Filntisis, Radek Danecek, Victoria F. Abrevaya, Anastasios Roussos, Timo Bolkart, Petros Maragos</author><pubDate>Fri, 05 Apr 2024 15:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04104v1</guid></item><item><title>Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo</title><link>http://arxiv.org/abs/2404.04103v1</link><description>Neural Table-to-Text models tend to hallucinate, producing texts that containfactual errors. We investigate whether such errors in the output can be tracedback to problems with the input. We manually annotated 1,837 texts generated bymultiple models in the politics domain of the ToTTo dataset. We identify theinput problems that are responsible for many output errors and show that fixingthese inputs reduces factual errors by between 52% and 76% (depending on themodel). In addition, we observe that models struggle in processing tabularinputs that are structured in a non-standard way, particularly when the inputlacks distinct row and column values or when the column headers are notcorrectly mapped to corresponding values.</description><author>Barkavi Sundararajan, Somayajulu Sripada, Ehud Reiter</author><pubDate>Fri, 05 Apr 2024 14:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04103v1</guid></item><item><title>Robust Preference Optimization with Provable Noise Tolerance for LLMs</title><link>http://arxiv.org/abs/2404.04102v1</link><description>The preference alignment aims to enable large language models (LLMs) togenerate responses that conform to human values, which is essential fordeveloping general AI systems. Ranking-based methods -- a promising class ofalignment approaches -- learn human preferences from datasets containingresponse pairs by optimizing the log-likelihood margins between preferred anddis-preferred responses. However, due to the inherent differences inannotators' preferences, ranking labels of comparisons for response pairs areunavoidably noisy. This seriously hurts the reliability of existingranking-based methods. To address this problem, we propose a provablynoise-tolerant preference alignment method, namely RObust PreferenceOptimization (ROPO). To the best of our knowledge, ROPO is the first preferencealignment method with noise-tolerance guarantees. The key idea of ROPO is todynamically assign conservative gradient weights to response pairs with highlabel uncertainty, based on the log-likelihood margins between the responses.By effectively suppressing the gradients of noisy samples, our weightingstrategy ensures that the expected risk has the same gradient directionindependent of the presence and proportion of noise. Experiments on threeopen-ended text generation tasks with four base models ranging in size from2.8B to 13B demonstrate that ROPO significantly outperforms existingranking-based methods.</description><author>Xize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, Jieping Ye</author><pubDate>Fri, 05 Apr 2024 14:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04102v1</guid></item><item><title>Neural Sign Actors: A diffusion model for 3D sign language production from text</title><link>http://arxiv.org/abs/2312.02702v2</link><description>Sign Languages (SL) serve as the primary mode of communication for the Deafand Hard of Hearing communities. Deep learning methods for SL recognition andtranslation have achieved promising results. However, Sign Language Production(SLP) poses a challenge as the generated motions must be realistic and haveprecise semantic meaning. Most SLP methods rely on 2D data, which hinders theirrealism. In this work, a diffusion-based SLP model is trained on a curatedlarge-scale dataset of 4D signing avatars and their corresponding texttranscripts. The proposed method can generate dynamic sequences of 3D avatarsfrom an unconstrained domain of discourse using a diffusion process formed on anovel and anatomically informed graph neural network defined on the SMPL-X bodyskeleton. Through quantitative and qualitative experiments, we show that theproposed method considerably outperforms previous methods of SLP. This workmakes an important step towards realistic neural sign avatars, bridging thecommunication gap between Deaf and hearing communities.</description><author>Vasileios Baltatzis, Rolandos Alexandros Potamias, Evangelos Ververas, Guanxiong Sun, Jiankang Deng, Stefanos Zafeiriou</author><pubDate>Fri, 05 Apr 2024 14:49:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02702v2</guid></item><item><title>Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It</title><link>http://arxiv.org/abs/2312.06420v2</link><description>The task of online mapping is to predict a local map using current sensorobservations, e.g. from lidar and camera, without relying on a pre-built map.State-of-the-art methods are based on supervised learning and are trainedpredominantly using two datasets: nuScenes and Argoverse 2. However, thesedatasets revisit the same geographic locations across training, validation, andtest sets. Specifically, over $80$% of nuScenes and $40$% of Argoverse 2validation and test samples are less than $5$ m from a training sample. At testtime, the methods are thus evaluated more on how well they localize within amemorized implicit map built from the training data than on extrapolating tounseen locations. Naturally, this data leakage causes inflated performancenumbers and we propose geographically disjoint data splits to reveal the trueperformance in unseen environments. Experimental results show that methodsperform considerably worse, some dropping more than $45$ mAP, when trained andevaluated on proper data splits. Additionally, a reassessment of prior designchoices reveals diverging conclusions from those based on the original split.Notably, the impact of lifting methods and the support from auxiliary tasks(e.g., depth supervision) on performance appears less substantial or follows adifferent trajectory than previously perceived. Splits can be found athttps://github.com/LiljaAdam/geographical-splits</description><author>Adam Lilja, Junsheng Fu, Erik Stenborg, Lars Hammarstrand</author><pubDate>Fri, 05 Apr 2024 14:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06420v2</guid></item><item><title>Dynamic Prompt Optimizing for Text-to-Image Generation</title><link>http://arxiv.org/abs/2404.04095v1</link><description>Text-to-image generative models, specifically those based on diffusion modelslike Imagen and Stable Diffusion, have made substantial advancements. Recently,there has been a surge of interest in the delicate refinement of text prompts.Users assign weights or alter the injection time steps of certain words in thetext prompts to improve the quality of generated images. However, the successof fine-control prompts depends on the accuracy of the text prompts and thecareful selection of weights and time steps, which requires significant manualintervention. To address this, we introduce the \textbf{P}rompt\textbf{A}uto-\textbf{E}diting (PAE) method. Besides refining the originalprompts for image generation, we further employ an online reinforcementlearning strategy to explore the weights and injection time steps of each word,leading to the dynamic fine-control prompts. The reward function duringtraining encourages the model to consider aesthetic score, semanticconsistency, and user preferences. Experimental results demonstrate that ourproposed method effectively improves the original prompts, generating visuallymore appealing images while maintaining semantic alignment. Code is availableat https://github.com/Mowenyii/PAE.</description><author>Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, Qing Yang</author><pubDate>Fri, 05 Apr 2024 14:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04095v1</guid></item><item><title>Estimation of Concept Explanations Should be Uncertainty Aware</title><link>http://arxiv.org/abs/2312.08063v2</link><description>Model explanations can be valuable for interpreting and debugging predictivemodels. We study a specific kind called Concept Explanations, where the goal isto interpret a model using human-understandable concepts. Although popular fortheir easy interpretation, concept explanations are known to be noisy. We beginour work by identifying various sources of uncertainty in the estimationpipeline that lead to such noise. We then propose an uncertainty-aware Bayesianestimation method to address these issues, which readily improved the qualityof explanations. We demonstrate with theoretical analysis and empiricalevaluation that explanations computed by our method are robust to train-timechoices while also being label-efficient. Further, our method proved capable ofrecovering relevant concepts amongst a bank of thousands, in an evaluation withreal-datasets and off-the-shelf models, demonstrating its scalability. Webelieve the improved quality of uncertainty-aware concept explanations makethem a strong candidate for more reliable model interpretation. We release ourcode at https://github.com/vps-anonconfs/uace.</description><author>Vihari Piratla, Juyeon Heo, Katherine M. Collins, Sukriti Singh, Adrian Weller</author><pubDate>Fri, 05 Apr 2024 14:42:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08063v2</guid></item><item><title>Contextual Encoder-Decoder Network for Visual Saliency Prediction</title><link>http://arxiv.org/abs/1902.06634v4</link><description>Predicting salient regions in natural images requires the detection ofobjects that are present in a scene. To develop robust representations for thischallenging task, high-level visual features at multiple spatial scales must beextracted and augmented with contextual information. However, existing modelsaimed at explaining human fixation maps do not incorporate such a mechanismexplicitly. Here we propose an approach based on a convolutional neural networkpre-trained on a large-scale image classification task. The architecture formsan encoder-decoder structure and includes a module with multiple convolutionallayers at different dilation rates to capture multi-scale features in parallel.Moreover, we combine the resulting representations with global sceneinformation for accurately predicting visual saliency. Our model achievescompetitive and consistent results across multiple evaluation metrics on twopublic saliency benchmarks and we demonstrate the effectiveness of thesuggested approach on five datasets and selected examples. Compared to state ofthe art approaches, the network is based on a lightweight image classificationbackbone and hence presents a suitable choice for applications with limitedcomputational resources, such as (virtual) robotic systems, to estimate humanfixations across complex natural scenes.</description><author>Alexander Kroner, Mario Senden, Kurt Driessens, Rainer Goebel</author><pubDate>Fri, 05 Apr 2024 14:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1902.06634v4</guid></item><item><title>Learning by Self-Explaining</title><link>http://arxiv.org/abs/2309.08395v2</link><description>Current AI research mainly treats explanations as a means for modelinspection. Yet, this neglects findings from human psychology that describe thebenefit of self-explanations in an agent's learning process. Motivated by this,we introduce a novel approach in the context of image classification, termedLearning by Self-Explaining (LSX). LSX utilizes aspects of self-refining AI andhuman-guided explanatory machine learning. The underlying idea is that alearner model, in addition to optimizing for the original predictive task, isfurther optimized based on explanatory feedback from an internal critic model.Intuitively, a learner's explanations are considered "useful" if the internalcritic can perform the same task given these explanations. We provide anoverview of important components of LSX and, based on this, perform extensiveexperimental evaluations via three different example instantiations. Ourresults indicate improvements via Learning by Self-Explaining on severallevels: in terms of model generalization, reducing the influence of confoundingfactors, and providing more task-relevant and faithful model explanations.Overall, our work provides evidence for the potential of self-explaining withinthe learning phase of an AI model.</description><author>Wolfgang Stammer, Felix Friedrich, David Steinmann, Manuel Brack, Hikaru Shindo, Kristian Kersting</author><pubDate>Fri, 05 Apr 2024 13:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08395v2</guid></item><item><title>Label Propagation for Zero-shot Classification with Vision-Language Models</title><link>http://arxiv.org/abs/2404.04072v1</link><description>Vision-Language Models (VLMs) have demonstrated impressive performance onzero-shot classification, i.e. classification when provided merely with a listof class names. In this paper, we tackle the case of zero-shot classificationin the presence of unlabeled data. We leverage the graph structure of theunlabeled data and introduce ZLaP, a method based on label propagation (LP)that utilizes geodesic distances for classification. We tailor LP to graphscontaining both text and image features and further propose an efficient methodfor performing inductive inference based on a dual solution and asparsification step. We perform extensive experiments to evaluate theeffectiveness of our method on 14 common datasets and show that ZLaPoutperforms the latest related works. Code:https://github.com/vladan-stojnic/ZLaP</description><author>Vladan Stojnić, Yannis Kalantidis, Giorgos Tolias</author><pubDate>Fri, 05 Apr 2024 13:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04072v1</guid></item><item><title>Single Domain Generalization for Crowd Counting</title><link>http://arxiv.org/abs/2403.09124v2</link><description>Due to its promising results, density map regression has been widely employedfor image-based crowd counting. The approach, however, often suffers fromsevere performance degradation when tested on data from unseen scenarios, theso-called "domain shift" problem. To address the problem, we investigate inthis work single domain generalization (SDG) for crowd counting. The existingSDG approaches are mainly for image classification and segmentation, and canhardly be extended to our case due to its regression nature and label ambiguity(i.e., ambiguous pixel-level ground truths). We propose MPCount, a noveleffective SDG approach even for narrow source distribution. MPCount storesdiverse density values for density map regression and reconstructsdomain-invariant features by means of only one memory bank, a content errormask and attention consistency loss. By partitioning the image into grids, itemploys patch-wise classification as an auxiliary task to mitigate labelambiguity. Through extensive experiments on different datasets, MPCount isshown to significantly improve counting accuracy compared to the state of theart under diverse scenarios unobserved in the training data characterized bynarrow source distribution. Code is available athttps://github.com/Shimmer93/MPCount.</description><author>Zhuoxuan Peng, S. -H. Gary Chan</author><pubDate>Fri, 05 Apr 2024 13:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09124v2</guid></item><item><title>Hierarchical Neural Additive Models for Interpretable Demand Forecasts</title><link>http://arxiv.org/abs/2404.04070v1</link><description>Demand forecasts are the crucial basis for numerous business decisions,ranging from inventory management to strategic facility planning. While machinelearning (ML) approaches offer accuracy gains, their interpretability andacceptance are notoriously lacking. Addressing this dilemma, we introduceHierarchical Neural Additive Models for time series (HNAM). HNAM expands uponNeural Additive Models (NAM) by introducing a time-series specific additivemodel with a level and interacting covariate components. Covariate interactions are only allowed according to a user-specifiedinteraction hierarchy. For example, weekday effects may be estimatedindependently of other covariates, whereas a holiday effect may depend on theweekday and an additional promotion may depend on both former covariates thatare lower in the interaction hierarchy. Thereby, HNAM yields an intuitive forecasting interface in which analysts canobserve the contribution for each known covariate. We evaluate the proposedapproach and benchmark its performance against other state-of-the-art machinelearning and statistical models extensively on real-world retail data. Theresults reveal that HNAM offers competitive prediction performance whilstproviding plausible explanations.</description><author>Leif Feddersen, Catherine Cleophas</author><pubDate>Fri, 05 Apr 2024 13:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04070v1</guid></item><item><title>Assessing the quality of information extraction</title><link>http://arxiv.org/abs/2404.04068v1</link><description>Advances in large language models have notably enhanced the efficiency ofinformation extraction from unstructured and semi-structured data sources. Asthese technologies become integral to various applications, establishing anobjective measure for the quality of information extraction becomes imperative.However, the scarcity of labeled data presents significant challenges to thisendeavor. In this paper, we introduce an automatic framework to assess thequality of the information extraction and its completeness. The frameworkfocuses on information extraction in the form of entity and its properties. Wediscuss how to handle the input/output size limitations of the large languagemodels and analyze their performance when iteratively extracting theinformation. Finally, we introduce metrics to evaluate the quality of theextraction and provide an extensive discussion on how to interpret the metrics.</description><author>Filip Seitl, Tomáš Kovářík, Soheyla Mirshahi, Jan Kryštůfek, Rastislav Dujava, Matúš Ondreička, Herbert Ullrich, Petr Gronat</author><pubDate>Fri, 05 Apr 2024 13:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04068v1</guid></item><item><title>CLUE: A Clinical Language Understanding Evaluation for LLMs</title><link>http://arxiv.org/abs/2404.04067v1</link><description>Large Language Models (LLMs) have shown the potential to significantlycontribute to patient care, diagnostics, and administrative processes. Emergingbiomedical LLMs address healthcare-specific challenges, including privacydemands and computational constraints. However, evaluation of these models hasprimarily been limited to non-clinical tasks, which do not reflect thecomplexity of practical clinical applications. Additionally, there has been nothorough comparison between biomedical and general-domain LLMs for clinicaltasks. To fill this gap, we present the Clinical Language UnderstandingEvaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinicaltasks. CLUE includes two novel datasets derived from MIMIC IV discharge lettersand four existing tasks designed to test the practical applicability of LLMs inhealthcare settings. Our evaluation covers several biomedical and generaldomain LLMs, providing insights into their clinical performance andapplicability. CLUE represents a step towards a standardized approach toevaluating and developing LLMs in healthcare to align future model developmentwith the real-world needs of clinical application. We publish our evaluationand data generation scripts: https://github.com/dadaamin/CLUE</description><author>Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek</author><pubDate>Fri, 05 Apr 2024 13:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04067v1</guid></item><item><title>One model to use them all: Training a segmentation model with complementary datasets</title><link>http://arxiv.org/abs/2402.19340v2</link><description>Understanding a surgical scene is crucial for computer-assisted surgerysystems to provide any intelligent assistance functionality. One way ofachieving this scene understanding is via scene segmentation, where every pixelof a frame is classified and therefore identifies the visible structures andtissues. Progress on fully segmenting surgical scenes has been made usingmachine learning. However, such models require large amounts of annotatedtraining data, containing examples of all relevant object classes. Such fullyannotated datasets are hard to create, as every pixel in a frame needs to beannotated by medical experts and, therefore, are rarely available. In thiswork, we propose a method to combine multiple partially annotated datasets,which provide complementary annotations, into one model, enabling better scenesegmentation and the use of multiple readily available datasets. Our methodaims to combine available data with complementary labels by leveraging mutualexclusive properties to maximize information. Specifically, we propose to usepositive annotations of other classes as negative samples and to excludebackground pixels of binary annotations, as we cannot tell if they contain aclass not annotated but predicted by the model. We evaluate our method bytraining a DeepLabV3 on the publicly available Dresden Surgical AnatomyDataset, which provides multiple subsets of binary segmented anatomicalstructures. Our approach successfully combines 6 classes into one model,increasing the overall Dice Score by 4.4% compared to an ensemble of modelstrained on the classes individually. By including information on multipleclasses, we were able to reduce confusion between stomach and colon by 24%. Ourresults demonstrate the feasibility of training a model on multiple datasets.This paves the way for future work further alleviating the need for one large,fully segmented datasets.</description><author>Alexander C. Jenke, Sebastian Bodenstedt, Fiona R. Kolbinger, Marius Distler, Jürgen Weitz, Stefanie Speidel</author><pubDate>Fri, 05 Apr 2024 13:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19340v2</guid></item><item><title>VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots</title><link>http://arxiv.org/abs/2404.04066v1</link><description>Physically assistive robots present an opportunity to significantly increasethe well-being and independence of individuals with motor impairments or otherforms of disability who are unable to complete activities of daily living.Speech interfaces, especially ones that utilize Large Language Models (LLMs),can enable individuals to effectively and naturally communicate high-levelcommands and nuanced preferences to robots. Frameworks for integrating LLMs asinterfaces to robots for high level task planning and code generation have beenproposed, but fail to incorporate human-centric considerations which areessential while developing assistive interfaces. In this work, we present aframework for incorporating LLMs as speech interfaces for physically assistiverobots, constructed iteratively with 3 stages of testing involving a feedingrobot, culminating in an evaluation with 11 older adults at an independentliving facility. We use both quantitative and qualitative data from the finalstudy to validate our framework and additionally provide design guidelines forusing LLMs as speech interfaces for assistive robots. Videos and supportingfiles are located on our project website:https://sites.google.com/andrew.cmu.edu/voicepilot/</description><author>Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson</author><pubDate>Fri, 05 Apr 2024 13:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04066v1</guid></item><item><title>Part-Attention Based Model Make Occluded Person Re-Identification Stronger</title><link>http://arxiv.org/abs/2404.03443v2</link><description>The goal of occluded person re-identification (ReID) is to retrieve specificpedestrians in occluded situations. However, occluded person ReID still suffersfrom background clutter and low-quality local feature representations, whichlimits model performance. In our research, we introduce a new framework calledPAB-ReID, which is a novel ReID model incorporating part-attention mechanismsto tackle the aforementioned issues effectively. Firstly, we introduce thehuman parsing label to guide the generation of more accurate human partattention maps. In addition, we propose a fine-grained feature focuser forgenerating fine-grained human local feature representations while suppressingbackground interference. Moreover, We also design a part triplet loss tosupervise the learning of human local features, which optimizesintra/inter-class distance. We conducted extensive experiments on specializedocclusion and regular ReID datasets, showcasing that our approach outperformsthe existing state-of-the-art methods.</description><author>Zhihao Chen, Yiyuan Ge</author><pubDate>Fri, 05 Apr 2024 13:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03443v2</guid></item><item><title>Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2404.04064v1</link><description>We study in this paper the improvement of one-class support vector machines(OC-SVM) through sparse representation techniques for unsupervised anomalydetection. As Dictionary Learning (DL) became recently a common analysistechnique that reveals hidden sparse patterns of data, our approach uses thisinsight to endow unsupervised detection with more control on pattern findingand dimensions. We introduce a new anomaly detection model that unifies theOC-SVM and DL residual functions into a single composite objective,subsequently solved through K-SVD-type iterative algorithms. A closed-form ofthe alternating K-SVD iteration is explicitly derived for the new compositemodel and practical implementable schemes are discussed. The standard DL modelis adapted for the Dictionary Pair Learning (DPL) context, where the usualsparsity constraints are naturally eliminated. Finally, we extend bothobjectives to the more general setting that allows the use of kernel functions.The empirical convergence properties of the resulting algorithms are providedand an in-depth analysis of their parametrization is performed while alsodemonstrating their numerical performance in comparison with existing methods.</description><author>Paul Irofti, Iulian-Andrei Hîji, Andrei Pătraşcu, Nicolae Cleju</author><pubDate>Fri, 05 Apr 2024 13:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04064v1</guid></item><item><title>Derivative-free tree optimization for complex systems</title><link>http://arxiv.org/abs/2404.04062v1</link><description>A tremendous range of design tasks in materials, physics, and biology can beformulated as finding the optimum of an objective function depending on manyparameters without knowing its closed-form expression or the derivative.Traditional derivative-free optimization techniques often rely on strongassumptions about objective functions, thereby failing at optimizing non-convexsystems beyond 100 dimensions. Here, we present a tree search method forderivative-free optimization that enables accelerated optimal design ofhigh-dimensional complex systems. Specifically, we introduce stochastic treeexpansion, dynamic upper confidence bound, and short-range backpropagationmechanism to evade local optimum, iteratively approximating the global optimumusing machine learning models. This development effectively confronts thedimensionally challenging problems, achieving convergence to global optimaacross various benchmark functions up to 2,000 dimensions, surpassing theexisting methods by 10- to 20-fold. Our method demonstrates wide applicabilityto a wide range of real-world complex systems spanning materials, physics, andbiology, considerably outperforming state-of-the-art algorithms. This enablesefficient autonomous knowledge discovery and facilitates self-driving virtuallaboratories. Although we focus on problems within the realm of naturalscience, the advancements in optimization techniques achieved herein areapplicable to a broader spectrum of challenges across all quantitativedisciplines.</description><author>Ye Wei, Bo Peng, Ruiwen Xie, Yangtao Chen, Yu Qin, Peng Wen, Stefan Bauer, Po-Yen Tung</author><pubDate>Fri, 05 Apr 2024 13:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04062v1</guid></item><item><title>OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation</title><link>http://arxiv.org/abs/2403.06546v2</link><description>Unsupervised Semantic Segmentation (USS) involves segmenting images withoutrelying on predefined labels, aiming to alleviate the burden of extensive humanlabeling. Existing methods utilize features generated by self-supervised modelsand specific priors for clustering. However, their clustering objectives arenot involved in the optimization of the features during training. Additionally,due to the lack of clear class definitions in USS, the resulting segments maynot align well with the clustering objective. In this paper, we introduce anovel approach called Optimally Matched Hierarchy (OMH) to simultaneouslyaddress the above issues. The core of our method lies in imposing structuredsparsity on the feature space, which allows the features to encode informationwith different levels of granularity. The structure of this sparsity stems fromour hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchyamong parallel clusters through Optimal Transport. Our OMH yields betterunsupervised segmentation performance compared to existing USS methods. Ourextensive experiments demonstrate the benefits of OMH when utilizing ourdifferentiable paradigm. We will make our code publicly available.</description><author>Baran Ozaydin, Tong Zhang, Deblina Bhattacharjee, Sabine Süsstrunk, Mathieu Salzmann</author><pubDate>Fri, 05 Apr 2024 13:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06546v2</guid></item><item><title>Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation</title><link>http://arxiv.org/abs/2404.04057v1</link><description>We introduce Score identity Distillation (SiD), an innovative data-freemethod that distills the generative capabilities of pretrained diffusion modelsinto a single-step generator. SiD not only facilitates an exponentially fastreduction in Fr\'echet inception distance (FID) during distillation but alsoapproaches or even exceeds the FID performance of the original teacherdiffusion models. By reformulating forward diffusion processes as semi-implicitdistributions, we leverage three score-related identities to create aninnovative loss mechanism. This mechanism achieves rapid FID reduction bytraining the generator using its own synthesized images, eliminating the needfor real data or reverse-diffusion-based generation, all accomplished withinsignificantly shortened generation time. Upon evaluation across four benchmarkdatasets, the SiD algorithm demonstrates high iteration efficiency duringdistillation and surpasses competing distillation approaches, whether they areone-step or few-step, data-free, or dependent on training data, in terms ofgeneration quality. This achievement not only redefines the benchmarks forefficiency and effectiveness in diffusion distillation but also in the broaderfield of diffusion-based generation. Our PyTorch implementation will bepublicly accessible on GitHub.</description><author>Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, Hai Huang</author><pubDate>Fri, 05 Apr 2024 13:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04057v1</guid></item><item><title>Evaluating Frontier Models for Dangerous Capabilities</title><link>http://arxiv.org/abs/2403.13793v2</link><description>To understand the risks posed by a new AI system, we must understand what itcan and cannot do. Building on prior work, we introduce a programme of new"dangerous capability" evaluations and pilot them on Gemini 1.0 models. Ourevaluations cover four areas: (1) persuasion and deception; (2) cyber-security;(3) self-proliferation; and (4) self-reasoning. We do not find evidence ofstrong dangerous capabilities in the models we evaluated, but we flag earlywarning signs. Our goal is to help advance a rigorous science of dangerouscapability evaluation, in preparation for future models.</description><author>Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad, Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan, Rohin Shah, Allan Dafoe, Toby Shevlane</author><pubDate>Fri, 05 Apr 2024 13:26:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13793v2</guid></item><item><title>SADA: Semantic adversarial unsupervised domain adaptation for Temporal Action Localization</title><link>http://arxiv.org/abs/2312.13377v2</link><description>Temporal Action Localization (TAL) is a complex task that poses relevantchallenges, particularly when attempting to generalize on new -- unseen --domains in real-world applications. These scenarios, despite realistic, areoften neglected in the literature, exposing these solutions to importantperformance degradation. In this work, we tackle this issue by introducing, forthe first time, an approach for Unsupervised Domain Adaptation (UDA) in sparseTAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation(SADA). Our contributions are threefold: (1) we pioneer the development of adomain adaptation model that operates on realistic sparse action detectionbenchmarks; (2) we tackle the limitations of global-distribution alignmenttechniques by introducing a novel adversarial loss that is sensitive to localclass distributions, ensuring finer-grained adaptation; and (3) we present anovel set of benchmarks based on EpicKitchens100 and CharadesEgo, that evaluatemultiple domain shifts in a comprehensive manner. Our experiments indicate thatSADA improves the adaptation across domains when compared to fully supervisedstate-of-the-art and alternative UDA methods, attaining a performance boost ofup to 6.14% mAP.</description><author>David Pujol-Perich, Albert Clapés, Sergio Escalera</author><pubDate>Fri, 05 Apr 2024 13:14:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13377v2</guid></item><item><title>SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric hybrid solution</title><link>http://arxiv.org/abs/2403.10344v2</link><description>Neural implicit surface representation methods have recently shown impressive3D reconstruction results. However, existing solutions struggle to reconstructurban outdoor scenes due to their large, unbounded, and highly detailed nature.Hence, to achieve accurate reconstructions, additional supervision data such asLiDAR, strong geometric priors, and long training times are required. To tacklesuch issues, we present SCILLA, a new hybrid implicit surface learning methodto reconstruct large driving scenes from 2D images. SCILLA's hybridarchitecture models two separate implicit fields: one for the volumetricdensity and another for the signed distance to the surface. To accuratelyrepresent urban outdoor scenarios, we introduce a novel volume-renderingstrategy that relies on self-supervised probabilistic density estimation tosample points near the surface and transition progressively from volumetric tosurface representation. Our solution permits a proper and fast initializationof the signed distance field without relying on any geometric prior on thescene, compared to concurrent methods. By conducting extensive experiments onfour outdoor driving datasets, we show that SCILLA can learn an accurate anddetailed 3D surface scene representation in various urban scenarios while beingtwo times faster to train compared to previous state-of-the-art solutions.</description><author>Hala Djeghim, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Désiré Sidibé</author><pubDate>Fri, 05 Apr 2024 13:14:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10344v2</guid></item><item><title>QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection</title><link>http://arxiv.org/abs/2312.06587v2</link><description>Quick and automated earthquake-damaged building detection from post-eventsatellite imagery is crucial, yet it is challenging due to the scarcity oftraining data required to develop robust algorithms. This letter presents thefirst dataset dedicated to detecting earthquake-damaged buildings frompost-event very high resolution (VHR) Synthetic Aperture Radar (SAR) andoptical imagery. Utilizing open satellite imagery and annotations acquiredafter the 2023 Turkey-Syria earthquakes, we deliver a dataset of coregisteredbuilding footprints and satellite image patches of both SAR and optical data,encompassing more than four thousand buildings. The task of damaged buildingdetection is formulated as a binary image classification problem, that can alsobe treated as an anomaly detection problem due to extreme class imbalance. Weprovide baseline methods and results to serve as references for comparison.Researchers can utilize this dataset to expedite algorithm development,facilitating the rapid detection of damaged buildings in response to futureevents. The dataset and codes together with detailed explanations andvisualization are made publicly available at\url{https://github.com/ya0-sun/PostEQ-SARopt-BuildingDamage}.</description><author>Yao Sun, Yi Wang, Michael Eineder</author><pubDate>Fri, 05 Apr 2024 13:10:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06587v2</guid></item><item><title>No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation</title><link>http://arxiv.org/abs/2404.04050v1</link><description>To reduce the reliance on large-scale datasets, recent works in 3Dsegmentation resort to few-shot learning. Current 3D few-shot segmentationmethods first pre-train models on 'seen' classes, and then evaluate theirgeneralization performance on 'unseen' classes. However, the prior pre-trainingstage not only introduces excessive time overhead but also incurs a significantdomain gap on 'unseen' classes. To tackle these issues, we propose aNon-parametric Network for few-shot 3D Segmentation, Seg-NN, and its Parametricvariant, Seg-PN. Without training, Seg-NN extracts dense representations byhand-crafted filters and achieves comparable performance to existing parametricmodels. Due to the elimination of pre-training, Seg-NN can alleviate the domaingap issue and save a substantial amount of time. Based on Seg-NN, Seg-PN onlyrequires training a lightweight QUEry-Support Transferring (QUEST) module,which enhances the interaction between the support set and query set.Experiments suggest that Seg-PN outperforms previous state-of-the-art method by+4.19% and +7.71% mIoU on S3DIS and ScanNet datasets respectively, whilereducing training time by -90%, indicating its effectiveness and efficiency.</description><author>Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, Peng Gao</author><pubDate>Fri, 05 Apr 2024 13:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04050v1</guid></item><item><title>Cycle Life Prediction for Lithium-ion Batteries: Machine Learning and More</title><link>http://arxiv.org/abs/2404.04049v1</link><description>Batteries are dynamic systems with complicated nonlinear aging, highlydependent on cell design, chemistry, manufacturing, and operational conditions.Prediction of battery cycle life and estimation of aging states is important toaccelerate battery R&amp;D, testing, and to further the understanding of howbatteries degrade. Beyond testing, battery management systems rely on real-timemodels and onboard diagnostics and prognostics for safe operation. Estimatingthe state of health and remaining useful life of a battery is important tooptimize performance and use resources optimally. This tutorial begins with an overview of first-principles, machine learning,and hybrid battery models. Then, a typical pipeline for the development ofinterpretable machine learning models is explained and showcased for cycle lifeprediction from laboratory testing data. We highlight the challenges of machinelearning models, motivating the incorporation of physics in hybrid modelingapproaches, which are needed to decipher the aging trajectory of batteries butrequire more data and further work on the physics of battery degradation. Thetutorial closes with a discussion on generalization and further researchdirections.</description><author>Joachim Schaeffer, Giacomo Galuppini, Jinwook Rhyu, Patrick A. Asinger, Robin Droop, Rolf Findeisen, Richard D. Braatz</author><pubDate>Fri, 05 Apr 2024 13:05:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04049v1</guid></item></channel></rss>