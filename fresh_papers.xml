<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 21 Nov 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.12028v1</link><description>Transformers have been successfully applied in the field of video-based 3Dhuman pose estimation. However, the high computational costs of these videopose transformers (VPTs) make them impractical on resource-constrained devices.In this paper, we present a plug-and-play pruning-and-recovering framework,called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human poseestimation from videos. Our HoT begins with pruning pose tokens of redundantframes and ends with recovering full-length tokens, resulting in a few posetokens in the intermediate transformer blocks and thus improving the modelefficiency. To effectively achieve this, we propose a token pruning cluster(TPC) that dynamically selects a few representative tokens with high semanticdiversity while eliminating the redundancy of video frames. In addition, wedevelop a token recovering attention (TRA) to restore the detailedspatio-temporal information based on the selected tokens, thereby expanding thenetwork output to the original full-length temporal resolution for fastinference. Extensive experiments on two benchmark datasets (i.e., Human3.6M andMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency andestimation accuracy compared to the original VPT models. For instance, applyingto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPswithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,respectively. Our source code will be open-sourced.</description><author>Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe</author><pubDate>Mon, 20 Nov 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12028v1</guid></item><item><title>PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction</title><link>http://arxiv.org/abs/2311.12024v1</link><description>We propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstructinga 3D object from a few unposed images even with little visual overlap, whilesimultaneously estimating the relative camera poses in ~1.3 seconds on a singleA100 GPU. PF-LRM is a highly scalable method utilizing the self-attentionblocks to exchange information between 3D object tokens and 2D image tokens; wepredict a coarse point cloud for each view, and then use a differentiablePerspective-n-Point (PnP) solver to obtain camera poses. When trained on a hugeamount of multi-view posed data of ~1M objects, PF-LRM shows strongcross-dataset generalization ability, and outperforms baseline methods by alarge margin in terms of pose prediction accuracy and 3D reconstruction qualityon various unseen evaluation datasets. We also demonstrate our model'sapplicability in downstream text/image-to-3D task with fast feed-forwardinference. Our project website is at: https://totoro97.github.io/pf-lrm .</description><author>Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, Kai Zhang</author><pubDate>Mon, 20 Nov 2023 18:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12024v1</guid></item><item><title>LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning</title><link>http://arxiv.org/abs/2311.12023v1</link><description>We propose a simple approach for memory-efficient adaptation of pretrainedlanguage models. Our approach uses an iterative algorithm to decompose eachpretrained matrix into a high-precision low-rank component and amemory-efficient quantized component. During finetuning, the quantizedcomponent remains fixed and only the low-rank component is updated. We presentan integer linear programming formulation of the quantization component whichenables dynamic configuration of quantization parameters (e.g., bit-width,block size) for each matrix given an overall target memory budget. We furtherexplore a data-aware version of the algorithm which uses an approximation ofthe Fisher information matrix to weight the reconstruction objective duringmatrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B)demonstrate that our low-rank plus quantized matrix decomposition approach(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enablesmore aggressive quantization. For example, on the OpenAssistant benchmarkLQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with amodel finetuned with 4-bit QLoRA. When finetuned on a language modelingcalibration dataset, LQ-LoRA can also be used for model compression; in thissetting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average whenincluding the low-rank components and requires 27GB of GPU memory) iscompetitive with the original model in full precision.</description><author>Han Guo, Philip Greengard, Eric P. Xing, Yoon Kim</author><pubDate>Mon, 20 Nov 2023 18:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12023v1</guid></item><item><title>GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark</title><link>http://arxiv.org/abs/2311.12022v1</link><description>We present GPQA, a challenging dataset of 448 multiple-choice questionswritten by domain experts in biology, physics, and chemistry. We ensure thatthe questions are high-quality and extremely difficult: experts who have or arepursuing PhDs in the corresponding domains reach 65% accuracy (74% whendiscounting clear mistakes the experts identified in retrospect), while highlyskilled non-expert validators only reach 34% accuracy, despite spending onaverage over 30 minutes with unrestricted access to the web (i.e., thequestions are "Google-proof"). The questions are also difficult forstate-of-the-art AI systems, with our strongest GPT-4 based baseline achieving39% accuracy. If we are to use future AI systems to help us answer very hardquestions, for example, when developing new scientific knowledge, we need todevelop scalable oversight methods that enable humans to supervise theiroutputs, which may be difficult even if the supervisors are themselves skilledand knowledgeable. The difficulty of GPQA both for skilled non-experts andfrontier AI systems should enable realistic scalable oversight experiments,which we hope can help devise ways for human experts to reliably get truthfulinformation from AI systems that surpass human capabilities.</description><author>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman</author><pubDate>Mon, 20 Nov 2023 18:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12022v1</guid></item><item><title>GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration</title><link>http://arxiv.org/abs/2311.12015v1</link><description>We introduce a pipeline that enhances a general-purpose Vision LanguageModel, GPT-4V(ision), by integrating observations of human actions tofacilitate robotic manipulation. This system analyzes videos of humansperforming tasks and creates executable robot programs that incorporateaffordance insights. The computation starts by analyzing the videos with GPT-4Vto convert environmental and action details into text, followed by aGPT-4-empowered task planner. In the following analyses, vision systemsreanalyze the video with the task plan. Object names are grounded using anopen-vocabulary object detector, while focus on the hand-object relation helpsto detect the moment of grasping and releasing. This spatiotemporal groundingallows the vision systems to further gather affordance data (e.g., grasp type,way points, and body postures). Experiments across various scenariosdemonstrate this method's efficacy in achieving real robots' operations fromhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 areavailable at this project page:https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/</description><author>Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi</author><pubDate>Mon, 20 Nov 2023 18:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12015v1</guid></item><item><title>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models</title><link>http://arxiv.org/abs/2310.15127v2</link><description>Pre-trained and frozen large language models (LLMs) can effectively mapsimple scene rearrangement instructions to programs over a robot's visuomotorfunctions through appropriate few-shot example prompting. To parse open-domainnatural language and adapt to a user's idiosyncratic procedures, not knownduring prompt engineering time, fixed prompts fall short. In this paper, weintroduce HELPER, an embodied agent equipped with an external memory oflanguage-program pairs that parses free-form human-robot dialogue into actionprograms through retrieval-augmented LLM prompting: relevant memories areretrieved based on the current dialogue, instruction, correction, or VLMdescription, and used as in-context prompt examples for LLM querying. Thememory is expanded during deployment to include pairs of user's language andaction plans, to assist future inferences and personalize them to the user'slanguage and routines. HELPER sets a new state-of-the-art in the TEAChbenchmark in both Execution from Dialog History (EDH) and Trajectory fromDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art forTfD. Our models, code, and video results can be found in our project's website:https://helper-agent-llm.github.io.</description><author>Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki</author><pubDate>Mon, 20 Nov 2023 18:51:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15127v2</guid></item><item><title>Steering Responsible AI: A Case for Algorithmic Pluralism</title><link>http://arxiv.org/abs/2311.12010v1</link><description>In this paper, I examine questions surrounding AI neutrality through theprism of existing literature and scholarship about mediation and mediapluralism. Such traditions, I argue, provide a valuable theoretical frameworkfor how we should approach the (likely) impending era of AI mediation. Inparticular, I suggest examining further the notion of algorithmic pluralism.Contrasting this notion to the dominant idea of algorithmic transparency, Iseek to describe what algorithmic pluralism may be, and present both itsopportunities and challenges. Implemented thoughtfully and responsibly, Iargue, Algorithmic or AI pluralism has the potential to sustain the diversity,multiplicity, and inclusiveness that are so vital to democracy.</description><author>Stefaan G. Verhulst</author><pubDate>Mon, 20 Nov 2023 18:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12010v1</guid></item><item><title>Risk-averse Batch Active Inverse Reward Design</title><link>http://arxiv.org/abs/2311.12004v1</link><description>Designing a perfect reward function that depicts all the aspects of theintended behavior is almost impossible, especially generalizing it outside ofthe training environments. Active Inverse Reward Design (AIRD) proposed the useof a series of queries, comparing possible reward functions in a singletraining environment. This allows the human to give information to the agentabout suboptimal behaviors, in order to compute a probability distribution overthe intended reward function. However, it ignores the possibility of unknownfeatures appearing in real-world environments, and the safety measures neededuntil the agent completely learns the reward function. I improved this methodand created Risk-averse Batch Active Inverse Reward Design (RBAIRD), whichconstructs batches, sets of environments the agent encounters when being usedin the real world, processes them sequentially, and, for a predetermined numberof iterations, asks queries that the human needs to answer for each environmentof the batch. After this process is completed in one batch, the probabilitieshave been improved and are transferred to the next batch. This makes it capableof adapting to real-world scenarios and learning how to treat unknown featuresit encounters for the first time. I also integrated a risk-averse planner,similar to that of Inverse Reward Design (IRD), which samples a set of rewardfunctions from the probability distribution and computes a trajectory thattakes the most certain rewards possible. This ensures safety while the agent isstill learning the reward function, and enables the use of this approach insituations where cautiousness is vital. RBAIRD outperformed the previousapproaches in terms of efficiency, accuracy, and action certainty, demonstratedquick adaptability to new, unknown features, and can be more widely used forthe alignment of crucial, powerful AI models.</description><author>Panagiotis Liampas</author><pubDate>Mon, 20 Nov 2023 18:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12004v1</guid></item><item><title>"Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion</title><link>http://arxiv.org/abs/2308.10974v3</link><description>Firm competition and collusion involve complex dynamics, particularly whenconsidering communication among firms. Such issues can be modeled as problemsof complex systems, traditionally approached through experiments involvinghuman subjects or agent-based modeling methods. We propose an innovativeframework called Smart Agent-Based Modeling (SABM), wherein smart agents,supported by GPT-4 technologies, represent firms, and interact with oneanother. We conducted a controlled experiment to study firm price competitionand collusion behaviors under various conditions. SABM is more cost-effectiveand flexible compared to conducting experiments with human subjects. Smartagents possess an extensive knowledge base for decision-making and exhibithuman-like strategic abilities, surpassing traditional ABM agents. Furthermore,smart agents can simulate human conversation and be personalized, making themideal for studying complex situations involving communication. Our resultsdemonstrate that, in the absence of communication, smart agents consistentlyreach tacit collusion, leading to prices converging at levels higher than theBertrand equilibrium price but lower than monopoly or cartel prices. Whencommunication is allowed, smart agents achieve a higher-level collusion withprices close to cartel prices. Collusion forms more quickly with communication,while price convergence is smoother without it. These results indicate thatcommunication enhances trust between firms, encouraging frequent small pricedeviations to explore opportunities for a higher-level win-win situation andreducing the likelihood of triggering a price war. We also assigned differentpersonas to firms to analyze behavioral differences and tested variant modelsunder diverse market structures. The findings showcase the effectiveness androbustness of SABM and provide intriguing insights into competition andcollusion.</description><author>Xu Han, Zengqing Wu, Chuan Xiao</author><pubDate>Mon, 20 Nov 2023 18:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10974v3</guid></item><item><title>BrainWash: A Poisoning Attack to Forget in Continual Learning</title><link>http://arxiv.org/abs/2311.11995v1</link><description>Continual learning has gained substantial attention within the deep learningcommunity, offering promising solutions to the challenging problem ofsequential learning. Yet, a largely unexplored facet of this paradigm is itssusceptibility to adversarial attacks, especially with the aim of inducingforgetting. In this paper, we introduce "BrainWash," a novel data poisoningmethod tailored to impose forgetting on a continual learner. By adding theBrainWash noise to a variety of baselines, we demonstrate how a trainedcontinual learner can be induced to forget its previously learned taskscatastrophically, even when using these continual learning baselines. Animportant feature of our approach is that the attacker requires no access toprevious tasks' data and is armed merely with the model's current parametersand the data belonging to the most recent task. Our extensive experimentshighlight the efficacy of BrainWash, showcasing degradation in performanceacross various regularization-based continual learning methods.</description><author>Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri</author><pubDate>Mon, 20 Nov 2023 18:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11995v1</guid></item><item><title>Exploring Lip Segmentation Techniques in Computer Vision: A Comparative Analysis</title><link>http://arxiv.org/abs/2311.11992v1</link><description>Lip segmentation is crucial in computer vision, especially for lip reading.Despite extensive face segmentation research, lip segmentation has receivedlimited attention. The aim of this study is to compare state-of-the-art lipsegmentation models using a standardized setting and a publicly availabledataset. Five techniques, namely EHANet, Mask2Former, BiSeNet V2, PIDNet, andSTDC1, are qualitatively selected based on their reported performance,inference time, code availability, recency, and popularity. The CelebAMask-HQdataset, comprising manually annotated face images, is used to fairly assessthe lip segmentation performance of the selected models. Inference experimentsare conducted on a Raspberry Pi4 to emulate limited computational resources.The results show that Mask2Former and EHANet have the best performances interms of mIoU score. BiSeNet V2 demonstrate competitive performance, whilePIDNet excels in recall but has lower precision. Most models present inferencetime ranging from 1000 to around 3000 milliseconds on a Raspberry Pi4, withPIDNet having the lowest mean inference time. This study provides acomprehensive evaluation of lip segmentation models, highlighting theirperformance and inference times. The findings contribute to the development oflightweight techniques and establish benchmarks for future advances in lipsegmentation, especially in IoT and edge computing scenarios.</description><author>Pietro B. S. Masur, Francisco Braulio Oliveira, Lucas Moreira Medino, Emanuel Huber, Milene Haraguchi Padilha, Cassio de Alcantara, Renata Sellaro</author><pubDate>Mon, 20 Nov 2023 18:23:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11992v1</guid></item><item><title>Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN</title><link>http://arxiv.org/abs/2311.11990v1</link><description>Using the atomic cluster expansion (ACE) framework, we develop a machinelearning interatomic potential for fast and accurately modelling the phonontransport properties of wurtzite aluminum nitride. The predictive power of theACE potential against density functional theory (DFT) is demonstrated across abroad range of properties of w-AlN, including ground-state lattice parameters,specific heat capacity, coefficients of thermal expansion, bulk modulus, andharmonic phonon dispersions. Validation of lattice thermal conductivity isfurther carried out by comparing the ACE-predicted values to the DFTcalculations and experiments, exhibiting the overall capability of our ACEpotential in sufficiently describing anharmonic phonon interactions. As apractical application, we perform a lattice dynamics analysis using thepotential to unravel the effects of biaxial strains on thermal conductivity andphonon properties of w-AlN, which is identified as a significant tuning factorfor near-junction thermal design of w-AlN-based electronics.</description><author>Guang Yang, Yuan-Bin Liu, Lei Yang, Bing-Yang Cao</author><pubDate>Mon, 20 Nov 2023 18:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11990v1</guid></item><item><title>Categorizing the Visual Environment and Analyzing the Visual Attention of Dogs</title><link>http://arxiv.org/abs/2311.11988v1</link><description>Dogs have a unique evolutionary relationship with humans and serve manyimportant roles e.g. search and rescue, blind assistance, emotional support.However, few datasets exist to categorize visual features and objects availableto dogs, as well as how dogs direct their visual attention within theirenvironment. We collect and study a dataset with over 11,698 gazes tocategorize the objects available to be gazed at by 11 dogs in everyday outdoorenvironments i.e. a walk around a college campus and urban area. We explore theavailability of these object categories and the visual attention of dogs overthese categories using a head mounted eye tracking apparatus. A small portion(approx. 600 images or &lt; 20% of total dataset) of the collected data is used tofine tune a MaskRCNN for the novel image domain to segment objects present inthe scene, enabling further statistical analysis on the visual gaze tendenciesof dogs. The MaskRCNN, with eye tracking apparatus, serves as an end to endmodel for automatically classifying the visual fixations of dogs. The finetuned MaskRCNN performs far better than chance. There are few individualdifferences between the 11 dogs and we observe greater visual fixations onbuses, plants, pavement, and construction equipment. This work takes a steptowards understanding visual behavior of dogs and their interaction with thephysical world.</description><author>Shreyas Sundara Raman, Madeline H. Pelgrim, Daphna Buchsbaum, Thomas Serre</author><pubDate>Mon, 20 Nov 2023 18:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11988v1</guid></item><item><title>H-COAL: Human Correction of AI-Generated Labels for Biomedical Named Entity Recognition</title><link>http://arxiv.org/abs/2311.11981v1</link><description>With the rapid advancement of machine learning models for NLP tasks,collecting high-fidelity labels from AI models is a realistic possibility.Firms now make AI available to customers via predictions as a service (PaaS).This includes PaaS products for healthcare. It is unclear whether these labelscan be used for training a local model without expensive annotation checking byin-house experts. In this work, we propose a new framework for Human Correctionof AI-Generated Labels (H-COAL). By ranking AI-generated outputs, one canselectively correct labels and approach gold standard performance (100% humanlabeling) with significantly less human effort. We show that correcting 5% oflabels can close the AI-human performance gap by up to 64% relativeimprovement, and correcting 20% of labels can close the performance gap by upto 86% relative improvement.</description><author>Xiaojing Duan, John P. Lalor</author><pubDate>Mon, 20 Nov 2023 18:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11981v1</guid></item><item><title>Leveraging Previous Facial Action Units Knowledge for Emotion Recognition on Faces</title><link>http://arxiv.org/abs/2311.11980v1</link><description>People naturally understand emotions, thus permitting a machine to do thesame could open new paths for human-computer interaction. Facial expressionscan be very useful for emotion recognition techniques, as these are the biggesttransmitters of non-verbal cues capable of being correlated with emotions.Several techniques are based on Convolutional Neural Networks (CNNs) to extractinformation in a machine learning process. However, simple CNNs are not alwayssufficient to locate points of interest on the face that can be correlated withemotions. In this work, we intend to expand the capacity of emotion recognitiontechniques by proposing the usage of Facial Action Units (AUs) recognitiontechniques to recognize emotions. This recognition will be based on the FacialAction Coding System (FACS) and computed by a machine learning system. Inparticular, our method expands over EmotiRAM, an approach for multi-cue emotionrecognition, in which we improve over their facial encoding module.</description><author>Pietro B. S. Masur, Willams Costa, Lucas S. Figueredo, Veronica Teichrieb</author><pubDate>Mon, 20 Nov 2023 18:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11980v1</guid></item><item><title>On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software</title><link>http://arxiv.org/abs/2311.11979v1</link><description>Due to the ever-increasing complexity of income tax laws in the UnitedStates, the number of US taxpayers filing their taxes using tax preparationsoftware (henceforth, tax software) continues to increase. According to theU.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filedtheir individual income taxes using tax software. Given the legal consequencesof incorrectly filing taxes for the taxpayer, ensuring the correctness of taxsoftware is of paramount importance. Metamorphic testing has emerged as aleading solution to test and debug legal-critical tax software due to theabsence of correctness requirements and trustworthy datasets. The key ideabehind metamorphic testing is to express the properties of a system in terms ofthe relationship between one input and its slightly metamorphosed twinnedinput. Extracting metamorphic properties from IRS tax publications is a tediousand time-consuming process. As a response, this paper formulates the task ofgenerating metamorphic specifications as a translation task between propertiesextracted from tax documents - expressed in natural language - to a contrastivefirst-order logic form. We perform a systematic analysis on the potential andlimitations of in-context learning with Large Language Models(LLMs) for thistask, and outline a research agenda towards automating the generation ofmetamorphic specifications for tax preparation software.</description><author>Dananjay Srinivas, Rohan Das, Saeid Tizpaz-Niari, Ashutosh Trivedi, Maria Leonor Pacheco</author><pubDate>Mon, 20 Nov 2023 18:12:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11979v1</guid></item><item><title>Context-aware Neural Machine Translation for English-Japanese Business Scene Dialogues</title><link>http://arxiv.org/abs/2311.11976v1</link><description>Despite the remarkable advancements in machine translation, the currentsentence-level paradigm faces challenges when dealing with highly-contextuallanguages like Japanese. In this paper, we explore how context-awareness canimprove the performance of the current Neural Machine Translation (NMT) modelsfor English-Japanese business dialogues translation, and what kind of contextprovides meaningful information to improve translation. As business dialogueinvolves complex discourse phenomena but offers scarce training resources, weadapted a pretrained mBART model, finetuning on multi-sentence dialogue data,which allows us to experiment with different contexts. We investigate theimpact of larger context sizes and propose novel context tokens encodingextra-sentential information, such as speaker turn and scene type. We make useof Conditional Cross-Mutual Information (CXMI) to explore how much of thecontext the model uses and generalise CXMI to study the impact of theextra-sentential context. Overall, we find that models leverage both precedingsentences and extra-sentential context (with CXMI increasing with context size)and we provide a more focused analysis on honorifics translation. Regardingtranslation quality, increased source-side context paired with scene andspeaker information improves the model performance compared to previous workand our context-agnostic baselines, measured in BLEU and COMET metrics.</description><author>Sumire Honda, Patrick Fernandes, Chrysoula Zerva</author><pubDate>Mon, 20 Nov 2023 18:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11976v1</guid></item><item><title>Evaluating Supervision Levels Trade-Offs for Infrared-Based People Counting</title><link>http://arxiv.org/abs/2311.11974v1</link><description>Object detection models are commonly used for people counting (andlocalization) in many applications but require a dataset with costly boundingbox annotations for training. Given the importance of privacy in peoplecounting, these models rely more and more on infrared images, making the taskeven harder. In this paper, we explore how weaker levels of supervision canaffect the performance of deep person counting architectures for imageclassification and point-level localization. Our experiments indicate thatcounting people using a CNN Image-Level model achieves competitive results withYOLO detectors and point-level models, yet provides a higher frame rate and asimilar amount of model parameters.</description><author>David Latortue, Moetez Kdayem, Fidel A Guerrero Peña, Eric Granger, Marco Pedersoli</author><pubDate>Mon, 20 Nov 2023 18:02:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11974v1</guid></item><item><title>Adaptive Training Distributions with Scalable Online Bilevel Optimization</title><link>http://arxiv.org/abs/2311.11973v1</link><description>Large neural networks pretrained on web-scale corpora are central to modernmachine learning. In this paradigm, the distribution of the large,heterogeneous pretraining data rarely matches that of the application domain.This work considers modifying the pretraining distribution in the case whereone has a small sample of data reflecting the targeted test conditions. Wepropose an algorithm motivated by a recent formulation of this setting as anonline, bilevel optimization problem. With scalability in mind, our algorithmprioritizes computing gradients at training points which are likely to mostimprove the loss on the targeted distribution. Empirically, we show that insome cases this approach is beneficial over existing strategies from the domainadaptation literature but may not succeed in other cases. We propose a simpletest to evaluate when our approach can be expected to work well and pointtowards further research to address current limitations.</description><author>David Grangier, Pierre Ablin, Awni Hannun</author><pubDate>Mon, 20 Nov 2023 18:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11973v1</guid></item><item><title>LiDAR-HMR: 3D Human Mesh Recovery from LiDAR</title><link>http://arxiv.org/abs/2311.11971v1</link><description>In recent years, point cloud perception tasks have been garnering increasingattention. This paper presents the first attempt to estimate 3D human body meshfrom sparse LiDAR point clouds. We found that the major challenge in estimatinghuman pose and mesh from point clouds lies in the sparsity, noise, andincompletion of LiDAR point clouds. Facing these challenges, we propose aneffective sparse-to-dense reconstruction scheme to reconstruct 3D human mesh.This involves estimating a sparse representation of a human (3D human pose) andgradually reconstructing the body mesh. To better leverage the 3D structuralinformation of point clouds, we employ a cascaded graph transformer(graphormer) to introduce point cloud features during sparse-to-densereconstruction. Experimental results on three publicly available databasesdemonstrate the effectiveness of the proposed approach. Code:https://github.com/soullessrobot/LiDAR-HMR/</description><author>Bohao Fan, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Mon, 20 Nov 2023 17:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11971v1</guid></item><item><title>SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20 Million masks</title><link>http://arxiv.org/abs/2311.11969v1</link><description>Segment Anything Model (SAM) has achieved impressive results for naturalimage segmentation with input prompts such as points and bounding boxes. Itssuccess largely owes to massive labeled training data. However, directlyapplying SAM to medical image segmentation cannot perform well because SAMlacks medical knowledge -- it does not use medical images for training. Toincorporate medical knowledge into SAM, we introduce SA-Med2D-20M, alarge-scale segmentation dataset of 2D medical images built upon numerouspublic and private datasets. It consists of 4.6 million 2D medical images and19.7 million corresponding masks, covering almost the whole body and showingsignificant diversity. This paper describes all the datasets collected inSA-Med2D-20M and details how to process these datasets. Furthermore,comprehensive statistics of SA-Med2D-20M are presented to facilitate the betteruse of our dataset, which can help the researchers build medical visionfoundation models or apply their models to downstream medical applications. Wehope that the large scale and diversity of SA-Med2D-20M can be leveraged todevelop medical artificial intelligence for enhancing diagnosis, medical imageanalysis, knowledge sharing, and education. The data with the redistributionlicense is publicly available at https://github.com/OpenGVLab/SAM-Med2D.</description><author>Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Min Zhu, Shaoting Zhang, Junjun He, Yu Qiao</author><pubDate>Mon, 20 Nov 2023 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11969v1</guid></item><item><title>Automatic Analysis of Substantiation in Scientific Peer Reviews</title><link>http://arxiv.org/abs/2311.11967v1</link><description>With the increasing amount of problematic peer reviews in top AI conferences,the community is urgently in need of automatic quality control measures. Inthis paper, we restrict our attention to substantiation -- one popular qualityaspect indicating whether the claims in a review are sufficiently supported byevidence -- and provide a solution automatizing this evaluation process. Toachieve this goal, we first formulate the problem as claim-evidence pairextraction in scientific peer reviews, and collect SubstanReview, the firstannotated dataset for this task. SubstanReview consists of 550 reviews from NLPconferences annotated by domain experts. On the basis of this dataset, we trainan argument mining system to automatically analyze the level of substantiationin peer reviews. We also perform data analysis on the SubstanReview dataset toobtain meaningful insights on peer reviewing quality in NLP conferences overrecent years.</description><author>Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, Chloé Clavel</author><pubDate>Mon, 20 Nov 2023 17:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11967v1</guid></item><item><title>ERUDITE: Human-in-the-Loop IoT for an Adaptive Personalized Learning System</title><link>http://arxiv.org/abs/2303.04292v2</link><description>Thanks to the rapid growth in wearable technologies and recent advancement inmachine learning and signal processing, monitoring complex human contextsbecomes feasible, paving the way to develop human-in-the-loop IoT systems thatnaturally evolve to adapt to the human and environment state autonomously.Nevertheless, a central challenge in designing many of these IoT systems arisesfrom the requirement to infer the human mental state, such as intention,stress, cognition load, or learning ability. While different human contexts canbe inferred from the fusion of different sensor modalities that can correlateto a particular mental state, the human brain provides a richer sensor modalitythat gives us more insights into the required human context. This paperproposes ERUDITE, a human-in-the-loop IoT system for the learning environmentthat exploits recent wearable neurotechnology to decode brain signals. Throughinsights from concept learning theory, ERUDITE can infer the human state oflearning and understand when human learning increases or declines. Byquantifying human learning as an input sensory signal, ERUDITE can provideadequate personalized feedback to humans in a learning environment to enhancetheir learning experience. ERUDITE is evaluated across $15$ participants andshowed that by using the brain signals as a sensor modality to infer the humanlearning state and providing personalized adaptation to the learningenvironment, the participants' learning performance increased on average by$26\%$. Furthermore, we showed that ERUDITE can be deployed on an edge-basedprototype to evaluate its practicality and scalability.</description><author>Mojtaba Taherisadr, Mohammad Abdullah Al Faruque, Salma Elmalaki</author><pubDate>Mon, 20 Nov 2023 17:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04292v2</guid></item><item><title>Provably Efficient CVaR RL in Low-rank MDPs</title><link>http://arxiv.org/abs/2311.11965v1</link><description>We study risk-sensitive Reinforcement Learning (RL), where we aim to maximizethe Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$. Priortheoretical work studying risk-sensitive RL focuses on the tabular MarkovDecision Processes (MDPs) setting. To extend CVaR RL to settings where statespace is large, function approximation must be deployed. We study CVaR RL inlow-rank MDPs with nonlinear function approximation. Low-rank MDPs assume theunderlying transition kernel admits a low-rank decomposition, but unlike priorlinear models, low-rank MDPs do not assume the feature or state-actionrepresentation is known. We propose a novel Upper Confidence Bound (UCB)bonus-driven algorithm to carefully balance the interplay between exploration,exploitation, and representation learning in CVaR RL. We prove that ouralgorithm achieves a sample complexity of $\tilde{O}\left(\frac{H^7 A^2d^4}{\tau^2 \epsilon^2}\right)$ to yield an $\epsilon$-optimal CVaR, where $H$is the length of each episode, $A$ is the capacity of action space, and $d$ isthe dimension of representations. Computational-wise, we design a noveldiscretized Least-Squares Value Iteration (LSVI) algorithm for the CVaRobjective as the planning oracle and show that we can find the near-optimalpolicy in a polynomial running time with a Maximum Likelihood Estimationoracle. To our knowledge, this is the first provably efficient CVaR RLalgorithm in low-rank MDPs.</description><author>Yulai Zhao, Wenhao Zhan, Xiaoyan Hu, Ho-fung Leung, Farzan Farnia, Wen Sun, Jason D. Lee</author><pubDate>Mon, 20 Nov 2023 17:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11965v1</guid></item><item><title>What Can AutoML Do For Continual Learning?</title><link>http://arxiv.org/abs/2311.11963v1</link><description>This position paper outlines the potential of AutoML for incremental(continual) learning to encourage more research in this direction. Incrementallearning involves incorporating new data from a stream of tasks anddistributions to learn enhanced deep representations and adapt better to newtasks. However, a significant limitation of incremental learners is that mostcurrent techniques freeze the backbone architecture, hyperparameters, and theorder &amp; structure of the learning tasks throughout the learning and adaptationprocess. We strongly believe that AutoML offers promising solutions to addressthese limitations, enabling incremental learning to adapt to more diversereal-world tasks. Therefore, instead of directly proposing a new method, thispaper takes a step back by posing the question: "What can AutoML do forincremental learning?" We outline three key areas of research that cancontribute to making incremental learners more dynamic, highlighting concreteopportunities to apply AutoML methods in novel ways as well as entirely newchallenges for AutoML research.</description><author>Mert Kilickaya, Joaquin Vanschoren</author><pubDate>Mon, 20 Nov 2023 17:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11963v1</guid></item><item><title>Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2207.02249v2</link><description>Successful deployment of multi-agent reinforcement learning often requiresagents to adapt their behaviour. In this work, we discuss the problem ofteamwork adaptation in which a team of agents needs to adapt their policies tosolve novel tasks with limited fine-tuning. Motivated by the intuition thatagents need to be able to identify and distinguish tasks in order to adapttheir behaviour to the current task, we propose to learn multi-agent taskembeddings (MATE). These task embeddings are trained using an encoder-decoderarchitecture optimised for reconstruction of the transition and rewardfunctions which uniquely identify tasks. We show that a team of agents is ableto adapt to novel tasks when provided with task embeddings. We propose threeMATE training paradigms: independent MATE, centralised MATE, and mixed MATEwhich vary in the information used for the task encoding. We show that theembeddings learned by MATE identify tasks and provide useful information whichagents leverage during adaptation to novel tasks.</description><author>Lukas Schäfer, Filippos Christianos, Amos Storkey, Stefano V. Albrecht</author><pubDate>Mon, 20 Nov 2023 17:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02249v2</guid></item><item><title>NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly Generation</title><link>http://arxiv.org/abs/2311.11961v1</link><description>Anomaly detection (AD) is essential in identifying rare and often criticalevents in complex systems, finding applications in fields such as networkintrusion detection, financial fraud detection, and fault detection ininfrastructure and industrial systems. While AD is typically treated as anunsupervised learning task due to the high cost of label annotation, it is morepractical to assume access to a small set of labeled anomaly samples fromdomain experts, as is the case for semi-supervised anomaly detection.Semi-supervised and supervised approaches can leverage such labeled data,resulting in improved performance. In this paper, rather than proposing a newsemi-supervised or supervised approach for AD, we introduce a novel algorithmfor generating additional pseudo-anomalies on the basis of the limited labeledanomalies and a large volume of unlabeled data. This serves as an augmentationto facilitate the detection of new anomalies. Our proposed algorithm, namedNearest Neighbor Gaussian Mixup (NNG-Mix), efficiently integrates informationfrom both labeled and unlabeled data to generate pseudo-anomalies. We comparethe performance of this novel algorithm with commonly applied augmentationtechniques, such as Mixup and Cutout. We evaluate NNG-Mix by training variousexisting semi-supervised and supervised anomaly detection algorithms on theoriginal training data along with the generated pseudo-anomalies. Throughextensive experiments on 57 benchmark datasets in ADBench, reflecting differentdata types, we demonstrate that NNG-Mix outperforms other data augmentationmethods. It yields significant performance improvements compared to thebaselines trained exclusively on the original training data. Notably, NNG-Mixyields up to 16.4%, 8.8%, and 8.0% improvements on Classical, CV, and NLPdatasets in ADBench. Our source code will be available athttps://github.com/donghao51/NNG-Mix.</description><author>Hao Dong, Gaëtan Frusque, Yue Zhao, Eleni Chatzi, Olga Fink</author><pubDate>Mon, 20 Nov 2023 17:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11961v1</guid></item><item><title>Correlated Attention in Transformers for Multivariate Time Series</title><link>http://arxiv.org/abs/2311.11959v1</link><description>Multivariate time series (MTS) analysis prevails in real-world applicationssuch as finance, climate science and healthcare. The various self-attentionmechanisms, the backbone of the state-of-the-art Transformer-based models,efficiently discover the temporal dependencies, yet cannot well capture theintricate cross-correlation between different features of MTS data, whichinherently stems from complex dynamical systems in practice. To this end, wepropose a novel correlated attention mechanism, which not only efficientlycaptures feature-wise dependencies, but can also be seamlessly integratedwithin the encoder blocks of existing well-known Transformers to gainefficiency improvement. In particular, correlated attention operates acrossfeature channels to compute cross-covariance matrices between queries and keyswith different lag values, and selectively aggregate representations at thesub-series level. This architecture facilitates automated discovery andrepresentation learning of not only instantaneous but also laggedcross-correlations, while inherently capturing time series auto-correlation.When combined with prevalent Transformer baselines, correlated attentionmechanism constitutes a better alternative for encoder-only architectures,which are suitable for a wide range of tasks including imputation, anomalydetection and classification. Extensive experiments on the aforementioned tasksconsistently underscore the advantages of correlated attention mechanism inenhancing base Transformer models, and demonstrate our state-of-the-art resultsin imputation, anomaly detection and classification.</description><author>Quang Minh Nguyen, Lam M. Nguyen, Subhro Das</author><pubDate>Mon, 20 Nov 2023 17:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11959v1</guid></item><item><title>Meta-Path Learning for Multi-relational Graph Neural Networks</title><link>http://arxiv.org/abs/2309.17113v2</link><description>Existing multi-relational graph neural networks use one of two strategies foridentifying informative relations: either they reduce this problem to low-levelweight learning, or they rely on handcrafted chains of relational dependencies,called meta-paths. However, the former approach faces challenges in thepresence of many relations (e.g., knowledge graphs), while the latter requiressubstantial domain expertise to identify relevant meta-paths. In this work wepropose a novel approach to learn meta-paths and meta-path GNNs that are highlyaccurate based on a small number of informative meta-paths. Key element of ourapproach is a scoring function for measuring the potential informativeness of arelation in the incremental construction of the meta-path. Our experimentalevaluation shows that the approach manages to correctly identify relevantmeta-paths even with a large number of relations, and substantially outperformsexisting multi-relational GNNs on synthetic and real-world experiments.</description><author>Francesco Ferrini, Antonio Longa, Andrea Passerini, Manfred Jaeger</author><pubDate>Mon, 20 Nov 2023 17:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17113v2</guid></item><item><title>FinanceBench: A New Benchmark for Financial Question Answering</title><link>http://arxiv.org/abs/2311.11944v1</link><description>FinanceBench is a first-of-its-kind test suite for evaluating the performanceof LLMs on open book financial question answering (QA). It comprises 10,231questions about publicly traded companies, with corresponding answers andevidence strings. The questions in FinanceBench are ecologically valid andcover a diverse set of scenarios. They are intended to be clear-cut andstraightforward to answer to serve as a minimum performance standard. We test16 state of the art model configurations (including GPT-4-Turbo, Llama2 andClaude2, with vector stores and long context prompts) on a sample of 150 casesfrom FinanceBench, and manually review their answers (n=2,400). The cases areavailable open-source. We show that existing LLMs have clear limitations forfinancial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectlyanswered or refused to answer 81% of questions. While augmentation techniquessuch as using longer context window to feed in relevant evidence improveperformance, they are unrealistic for enterprise settings due to increasedlatency and cannot support larger financial documents. We find that all modelsexamined exhibit weaknesses, such as hallucinations, that limit theirsuitability for use by enterprises.</description><author>Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, Bertie Vidgen</author><pubDate>Mon, 20 Nov 2023 17:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11944v1</guid></item><item><title>A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains</title><link>http://arxiv.org/abs/2310.13849v2</link><description>The human visual system uses two parallel pathways for spatial processing andobject recognition. In contrast, computer vision systems tend to use a singlefeedforward pathway, rendering them less robust, adaptive, or efficient thanhuman vision. To bridge this gap, we developed a dual-stream vision modelinspired by the human eyes and brain. At the input level, the model samples twocomplementary visual patterns to mimic how the human eyes use magnocellular andparvocellular retinal ganglion cells to separate retinal inputs to the brain.At the backend, the model processes the separate input patterns through twobranches of convolutional neural networks (CNN) to mimic how the human brainuses the dorsal and ventral cortical pathways for parallel visual processing.The first branch (WhereCNN) samples a global view to learn spatial attentionand control eye movements. The second branch (WhatCNN) samples a local view torepresent the object around the fixation. Over time, the two branches interactrecurrently to build a scene representation from moving fixations. We comparedthis model with the human brains processing the same movie and evaluated theirfunctional alignment by linear transformation. The WhereCNN and WhatCNNbranches were found to differentially match the dorsal and ventral pathways ofthe visual cortex, respectively, primarily due to their different learningobjectives. These model-based results lead us to speculate that the distinctresponses and representations of the ventral and dorsal streams are moreinfluenced by their distinct goals in visual attention and object recognitionthan by their specific bias or selectivity in retinal inputs. This dual-streammodel takes a further step in brain-inspired computer vision, enabling parallelneural networks to actively explore and understand the visual surroundings.</description><author>Minkyu Choi, Kuan Han, Xiaokai Wang, Yizhen Zhang, Zhongming Liu</author><pubDate>Mon, 20 Nov 2023 17:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13849v2</guid></item><item><title>Estimation of entropy-regularized optimal transport maps between non-compactly supported measures</title><link>http://arxiv.org/abs/2311.11934v1</link><description>This paper addresses the problem of estimating entropy-regularized optimaltransport (EOT) maps with squared-Euclidean cost between source and targetmeasures that are subGaussian. In the case that the target measure is compactlysupported or strongly log-concave, we show that for a recently proposedin-sample estimator, the expected squared $L^2$-error decays at least as fastas $O(n^{-1/3})$ where $n$ is the sample size. For the general subGaussian casewe show that the expected $L^1$-error decays at least as fast as $O(n^{-1/6})$,and in both cases we have polynomial dependence on the regularizationparameter. While these results are suboptimal compared to known results in thecase of compactness of both the source and target measures (squared $L^2$-errorconverging at a rate $O(n^{-1})$) and for when the source is subGaussian whilethe target is compactly supported (squared $L^2$-error converging at a rate$O(n^{-1/2})$), their importance lie in eliminating the compact supportrequirements. The proof technique makes use of a bias-variance decompositionwhere the variance is controlled using standard concentration of measureresults and the bias is handled by T1-transport inequalities along with samplecomplexity results in estimation of EOT cost under subGaussian assumptions. Ourexperimental results point to a looseness in controlling the variance terms andwe conclude by posing several open problems.</description><author>Matthew Werenski, James M. Murphy, Shuchin Aeron</author><pubDate>Mon, 20 Nov 2023 17:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11934v1</guid></item><item><title>Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance</title><link>http://arxiv.org/abs/2311.11932v1</link><description>Background and objectives: By extracting this information, Machine or DeepLearning (ML/DL)-based autonomous data analysis tools can assist clinicians andcancer researchers in discovering patterns and relationships from complex datasets. Many DL-based analyses on ovarian cancer (OC) data have recently beenpublished. These analyses are highly diverse in various aspects of cancer(e.g., subdomain(s) and cancer type they address) and data analysis features.However, a comprehensive understanding of these analyses in terms of thesefeatures and AI assurance (AIA) is currently lacking. This systematic reviewaims to fill this gap by examining the existing literature and identifyingimportant aspects of OC data analysis using DL, explicitly focusing on the keyfeatures and AI assurance perspectives. Methods: The PRISMA framework was usedto conduct comprehensive searches in three journal databases. Only studiespublished between 2015 and 2023 in peer-reviewed journals were included in theanalysis. Results: In the review, a total of 96 DL-driven analyses wereexamined. The findings reveal several important insights regarding DL-drivenovarian cancer data analysis: - Most studies 71% (68 out of 96) focused ondetection and diagnosis, while no study addressed the prediction and preventionof OC. - The analyses were predominantly based on samples from a non-diversepopulation (75% (72/96 studies)), limited to a geographic location or country.- Only a small proportion of studies (only 33% (32/96)) performed integratedanalyses, most of which used homogeneous data (clinical or omics). - Notably, amere 8.3% (8/96) of the studies validated their models using external anddiverse data sets, highlighting the need for enhanced model validation, and -The inclusion of AIA in cancer data analysis is in a very early stage; only2.1% (2/96) explicitly addressed AIA through explainability.</description><author>Muta Tah Hira, Mohammad A. Razzaque, Mosharraf Sarker</author><pubDate>Mon, 20 Nov 2023 17:17:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11932v1</guid></item><item><title>Infinite Width Graph Neural Networks for Node Regression/ Classification</title><link>http://arxiv.org/abs/2310.08176v4</link><description>This work analyzes Graph Neural Networks, a generalization of Fully-ConnectedDeep Neural Nets on Graph structured data, when their width, that is the numberof nodes in each fullyconnected layer is increasing to infinity. Infinite WidthNeural Networks are connecting Deep Learning to Gaussian Processes and Kernels,both Machine Learning Frameworks with long traditions and extensive theoreticalfoundations. Gaussian Processes and Kernels have much less hyperparameters thenNeural Networks and can be used for uncertainty estimation, making them moreuser friendly for applications. This works extends the increasing amount ofresearch connecting Gaussian Processes and Kernels to Neural Networks. TheKernel and Gaussian Process closed forms are derived for a variety ofarchitectures, namely the standard Graph Neural Network, the Graph NeuralNetwork with Skip-Concatenate Connections and the Graph Attention NeuralNetwork. All architectures are evaluated on a variety of datasets on the taskof transductive Node Regression and Classification. Additionally, a SpectralSparsification method known as Effective Resistance is used to improve runtimeand memory requirements. Extending the setting to inductive graph learningtasks (Graph Regression/ Classification) is straightforward and is brieflydiscussed in 3.5.</description><author>Yunus Cobanoglu</author><pubDate>Mon, 20 Nov 2023 17:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08176v4</guid></item><item><title>A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures</title><link>http://arxiv.org/abs/2311.10217v2</link><description>The present paper introduces a novel object of study - a language fractalstructure. We hypothesize that a set of embeddings of all $n$-grams of anatural language constitutes a representative sample of this fractal set. (Weuse the term Hailonakea to refer to the sum total of all language fractalstructures, over all $n$). The paper estimates intrinsic (genuine) dimensionsof language fractal structures for the Russian and English languages. To thisend, we employ methods based on (1) topological data analysis and (2) a minimumspanning tree of a data graph for a cloud of points considered (Steeletheorem). For both languages, for all $n$, the intrinsic dimensions appear tobe non-integer values (typical for fractal sets), close to 9 for both of theRussian and English language.</description><author>Vasilii A. Gromov, Nikita S. Borodin, Asel S. Yerbolova</author><pubDate>Mon, 20 Nov 2023 17:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10217v2</guid></item><item><title>Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets</title><link>http://arxiv.org/abs/2305.17010v3</link><description>Combinatorial optimization (CO) problems are often NP-hard and thus out ofreach for exact algorithms, making them a tempting domain to apply machinelearning methods. The highly structured constraints in these problems canhinder either optimization or sampling directly in the solution space. On theother hand, GFlowNets have recently emerged as a powerful machinery toefficiently sample from composite unnormalized densities sequentially and havethe potential to amortize such solution-searching processes in CO, as well asgenerate diverse solution candidates. In this paper, we design Markov decisionprocesses (MDPs) for different combinatorial problems and propose to trainconditional GFlowNets to sample from the solution space. Efficient trainingtechniques are also developed to benefit long-range credit assignment. Throughextensive experiments on a variety of different CO tasks with synthetic andrealistic data, we demonstrate that GFlowNet policies can efficiently findhigh-quality solutions. Our implementation is open-sourced athttps://github.com/zdhNarsil/GFlowNet-CombOpt.</description><author>Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron Courville, Yoshua Bengio, Ling Pan</author><pubDate>Mon, 20 Nov 2023 16:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17010v3</guid></item><item><title>Language Varieties of Italy: Technology Challenges and Opportunities</title><link>http://arxiv.org/abs/2209.09757v2</link><description>Italy is characterized by a one-of-a-kind linguistic diversity landscape inEurope, which implicitly encodes local knowledge, cultural traditions, artisticexpressions and history of its speakers. However, most local languages anddialects in Italy are at risk of disappearing within few generations. The NLPcommunity has recently begun to engage with endangered languages, includingthose of Italy. Yet, most efforts assume that these varieties areunder-resourced language monoliths with an established written form andhomogeneous functions and needs, and thus highly interchangeable with eachother and with high-resource, standardized languages. In this paper, weintroduce the linguistic context of Italy and challenge the defaultmachine-centric assumptions of NLP for Italy's language varieties. We advocatefor a shift in the paradigm from machine-centric to speaker-centric NLP, andprovide recommendations and opportunities for work that prioritizes languagesand their speakers over technological advances. To facilitate the process, wefinally propose building a local community towards responsible, participatoryefforts aimed at supporting vitality of languages and dialects of Italy.</description><author>Alan Ramponi</author><pubDate>Mon, 20 Nov 2023 16:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.09757v2</guid></item><item><title>An Image is Worth Multiple Words: Multi-attribute Inversion for Constrained Text-to-Image Synthesis</title><link>http://arxiv.org/abs/2311.11919v1</link><description>We consider the problem of constraining diffusion model outputs with auser-supplied reference image. Our key objective is to extract multipleattributes (e.g., color, object, layout, style) from this single referenceimage, and then generate new samples with them. One line of existing workproposes to invert the reference images into a single textual conditioningvector, enabling generation of new samples with this learned token. Thesemethods, however, do not learn multiple tokens that are necessary to conditionmodel outputs on the multiple attributes noted above. Another line oftechniques expand the inversion space to learn multiple embeddings but they dothis only along the layer dimension (e.g., one per layer of the DDPM model) orthe timestep dimension (one for a set of timesteps in the denoising process),leading to suboptimal attribute disentanglement. To address the aforementionedgaps, the first contribution of this paper is an extensive analysis todetermine which attributes are captured in which dimension of the denoisingprocess. As noted above, we consider both the time-step dimension (in reversedenoising) as well as the DDPM model layer dimension. We observe that often asubset of these attributes are captured in the same set of model layers and/oracross same denoising timesteps. For instance, color and style are capturedacross same U-Net layers, whereas layout and color are captured across sametimestep stages. Consequently, an inversion process that is designed only forthe time-step dimension or the layer dimension is insufficient to disentangleall attributes. This leads to our second contribution where we design a newmulti-attribute inversion algorithm, MATTE, with associateddisentanglement-enhancing regularization losses, that operates across bothdimensions and explicitly leads to four disentangled tokens (color, style,layout, and object).</description><author>Aishwarya Agarwal, Srikrishna Karanam, Tripti Shukla, Balaji Vasan Srinivasan</author><pubDate>Mon, 20 Nov 2023 16:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11919v1</guid></item><item><title>Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks</title><link>http://arxiv.org/abs/2311.11913v1</link><description>The ability to construct a realistic simulator of financial exchanges,including reproducing the dynamics of the limit order book, can give insightinto many counterfactual scenarios, such as a flash crash, a margin call, orchanges in macroeconomic outlook. In recent years, agent-based models have beendeveloped that reproduce many features of an exchange, as summarised by a setof stylised facts and statistics. However, the ability to calibrate simulatorsto a specific period of trading remains an open challenge. In this work, wedevelop a novel approach to the calibration of market simulators by leveragingrecent advances in deep learning, specifically using neural density estimatorsand embedding networks. We demonstrate that our approach is able to correctlyidentify high probability parameter sets, both when applied to synthetic andhistorical data, and without reliance on manually selected or weightedensembles of stylised facts.</description><author>Namid R. Stillman, Rory Baggott, Justin Lyon, Jianfei Zhang, Dingqiu Zhu, Tao Chen, Perukrishnen Vytelingum</author><pubDate>Mon, 20 Nov 2023 16:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11913v1</guid></item><item><title>Certification of Distributional Individual Fairness</title><link>http://arxiv.org/abs/2311.11911v1</link><description>Providing formal guarantees of algorithmic fairness is of paramountimportance to socially responsible deployment of machine learning algorithms.In this work, we study formal guarantees, i.e., certificates, for individualfairness (IF) of neural networks. We start by introducing a novel convexapproximation of IF constraints that exponentially decreases the computationalcost of providing formal guarantees of local individual fairness. We highlightthat prior methods are constrained by their focus on global IF certificationand can therefore only scale to models with a few dozen hidden neurons, thuslimiting their practical impact. We propose to certify distributionalindividual fairness which ensures that for a given empirical distribution andall distributions within a $\gamma$-Wasserstein ball, the neural network hasguaranteed individually fair predictions. Leveraging developments inquasi-convex optimization, we provide novel and efficient certified bounds ondistributional individual fairness and show that our method allows us tocertify and regularize neural networks that are several orders of magnitudelarger than those considered by prior works. Moreover, we study real-worlddistribution shifts and find our bounds to be a scalable, practical, and soundsource of IF guarantees.</description><author>Matthew Wicker, Vihari Piratia, Adrian Weller</author><pubDate>Mon, 20 Nov 2023 16:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11911v1</guid></item><item><title>Generalization of Fitness Exercise Recognition from Doppler Measurements by Domain-adaption and Few-Shot Learning</title><link>http://arxiv.org/abs/2311.11910v1</link><description>In previous works, a mobile application was developed using an unmodifiedcommercial off-the-shelf smartphone to recognize whole-body exercises. Theworking principle was based on the ultrasound Doppler sensing with the devicebuilt-in hardware. Applying such a lab-environment trained model on realisticapplication variations causes a significant drop in performance, and thusdecimate its applicability. The reason of the reduced performance can bemanifold. It could be induced by the user, environment, and device variationsin realistic scenarios. Such scenarios are often more complex and diverse,which can be challenging to anticipate in the initial training data. To studyand overcome this issue, this paper presents a database with controlled anduncontrolled subsets of fitness exercises. We propose two concepts to utilizesmall adaption data to successfully improve model generalization in anuncontrolled environment, increasing the recognition accuracy by two to sixfolds compared to the baseline for different users.</description><author>Biying Fu, Naser Damer, Florian Kirchbuchner, Arjan Kuijper</author><pubDate>Mon, 20 Nov 2023 16:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11910v1</guid></item><item><title>Continual Learning: Applications and the Road Forward</title><link>http://arxiv.org/abs/2311.11908v1</link><description>Continual learning is a sub-field of machine learning, which aims to allowmachine learning models to continuously learn on new data, by accumulatingknowledge without forgetting what was learned in the past. In this work, wetake a step back, and ask: "Why should one care about continual learning in thefirst place?". We set the stage by surveying recent continual learning paperspublished at three major machine learning conferences, and show thatmemory-constrained settings dominate the field. Then, we discuss five openproblems in machine learning, and even though they seem unrelated to continuallearning at first sight, we show that continual learning will inevitably bepart of their solution. These problems are model-editing, personalization,on-device learning, faster (re-)training and reinforcement learning. Finally,by comparing the desiderata from these unsolved problems and the currentassumptions in continual learning, we highlight and discuss four futuredirections for continual learning research. We hope that this work offers aninteresting perspective on the future of continual learning, while displayingits potential value and the paths we have to pursue in order to make itsuccessful. This work is the result of the many discussions the authors had atthe Dagstuhl seminar on Deep Continual Learning, in March 2023.</description><author>Eli Verwimp, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven</author><pubDate>Mon, 20 Nov 2023 16:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11908v1</guid></item><item><title>Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning</title><link>http://arxiv.org/abs/2311.11905v1</link><description>Surface-to-Air Missiles (SAMs) are crucial in modern air defense systems. Acritical aspect of their effectiveness is the Engagement Zone (EZ), the spatialregion within which a SAM can effectively engage and neutralize a target.Notably, the EZ is intrinsically related to the missile's maximum range; itdefines the furthest distance at which a missile can intercept a target. Theaccurate computation of this EZ is essential but challenging due to the dynamicand complex factors involved, which often lead to high computational costs andextended processing times when using conventional simulation methods. In lightof these challenges, our study investigates the potential of machine learningtechniques, proposing an approach that integrates machine learning with acustom-designed simulation tool to train supervised algorithms. We leverage acomprehensive dataset of pre-computed SAM EZ simulations, enabling our model toaccurately predict the SAM EZ for new input parameters. It accelerates SAM EZsimulations, enhances air defense strategic planning, and provides real-timeinsights, improving SAM system performance. The study also includes acomparative analysis of machine learning algorithms, illuminating theircapabilities and performance metrics and suggesting areas for future research,highlighting the transformative potential of machine learning in SAM EZsimulations.</description><author>Joao P. A. Dantas, Diego Geraldo, Felipe L. L. Medeiros, Marcos R. O. A. Maximo, Takashi Yoneyama</author><pubDate>Mon, 20 Nov 2023 16:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11905v1</guid></item><item><title>LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions</title><link>http://arxiv.org/abs/2311.11904v1</link><description>Vision-language models (VLMs) offer a promising paradigm for imageclassification by comparing the similarity between images and class embeddings.A critical challenge lies in crafting precise textual representations for classnames. While previous studies have leveraged recent advancements in largelanguage models (LLMs) to enhance these descriptors, their outputs often sufferfrom ambiguity and inaccuracy. We identify two primary causes: 1) The prevalentreliance on textual interactions with LLMs, leading to a mismatch between thegenerated text and the visual content in VLMs' latent space - a phenomenon weterm the "explain without seeing" dilemma. 2) The oversight of the inter-classrelationships, resulting in descriptors that fail to differentiate similarclasses effectively. To address these issues, we propose a novel imageclassification framework combining VLMs with LLMs, named Iterative Optimizationwith Visual Feedback. In particular, our method develops an LLM-based agent,employing an evolutionary optimization strategy to refine class descriptors.Crucially, we incorporate visual feedback from VLM classification metrics,thereby guiding the optimization process with concrete visual data. Our methodleads to improving accuracy on a wide range of image classification benchmarks,with 3.47\% average gains over state-of-the-art methods. We also highlight theresulting descriptions serve as explainable and robust features that canconsistently improve the performance across various backbone models.</description><author>Songhao Han, Le Zhuo, Yue Liao, Si Liu</author><pubDate>Mon, 20 Nov 2023 16:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11904v1</guid></item><item><title>SWAT: Spatial Structure Within and Among Tokens</title><link>http://arxiv.org/abs/2111.13677v3</link><description>Modeling visual data as tokens (i.e., image patches) using attentionmechanisms, feed-forward networks or convolutions has been highly effective inrecent years. Such methods usually have a common pipeline: a tokenizationmethod, followed by a set of layers/blocks for information mixing, both withinand among tokens. When image patches are converted into tokens, they are oftenflattened, discarding the spatial structure within each patch. As a result, anyprocessing that follows (eg: multi-head self-attention) may fail to recoverand/or benefit from such information. In this paper, we argue that models canhave significant gains when spatial structure is preserved during tokenization,and is explicitly used during the mixing stage. We propose two keycontributions: (1) Structure-aware Tokenization and, (2) Structure-awareMixing, both of which can be combined with existing models with minimal effort.We introduce a family of models (SWAT), showing improvements over the likes ofDeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks includingImageNet classification and ADE20K segmentation. Our code is available athttps://github.com/kkahatapitiya/SWAT.</description><author>Kumara Kahatapitiya, Michael S. Ryoo</author><pubDate>Mon, 20 Nov 2023 16:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.13677v3</guid></item><item><title>Identifying the Defective: Detecting Damaged Grains for Cereal Appearance Inspection</title><link>http://arxiv.org/abs/2311.11901v1</link><description>Cereal grain plays a crucial role in the human diet as a major source ofessential nutrients. Grain Appearance Inspection (GAI) serves as an essentialprocess to determine grain quality and facilitate grain circulation andprocessing. However, GAI is routinely performed manually by inspectors withcumbersome procedures, which poses a significant bottleneck in smartagriculture. In this paper, we endeavor to develop an automated GAI system:AI4GrainInsp.By analyzing the distinctive characteristics of grain kernels, we formulate GAIas a ubiquitous problem: Anomaly Detection (AD), in which healthy and ediblekernels are considered normal samples while damaged grains or unknown objectsare regarded as anomalies. We further propose an AD model, called AD-GAI, whichis trained using only normal samples yet can identify anomalies duringinference. Moreover, we customize a prototype device for data acquisition andcreate a large-scale dataset including 220K high-quality images of wheat andmaize kernels. Through extensive experiments, AD-GAI achieves considerableperformance in comparison with advanced AD methods, and AI4GrainInsp has highlyconsistent performance compared to human experts and excels at inspectionefficiency over 20x speedup. The dataset, code and models will be released athttps://github.com/hellodfan/AI4GrainInsp.</description><author>Lei Fan, Yiwen Ding, Dongdong Fan, Yong Wu, Maurice Pagnucco, Yang Song</author><pubDate>Mon, 20 Nov 2023 16:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11901v1</guid></item><item><title>Measuring and Mitigating Biases in Motor Insurance Pricing</title><link>http://arxiv.org/abs/2311.11900v1</link><description>The non-life insurance sector operates within a highly competitive andtightly regulated framework, confronting a pivotal juncture in the formulationof pricing strategies. Insurers are compelled to harness a range of statisticalmethodologies and available data to construct optimal pricing structures thatalign with the overarching corporate strategy while accommodating the dynamicsof market competition. Given the fundamental societal role played by insurance,premium rates are subject to rigorous scrutiny by regulatory authorities. Theserates must conform to principles of transparency, explainability, and ethicalconsiderations. Consequently, the act of pricing transcends mere statisticalcalculations and carries the weight of strategic and societal factors. Thesemultifaceted concerns may drive insurers to establish equitable premiums,taking into account various variables. For instance, regulations mandate theprovision of equitable premiums, considering factors such as policyholdergender or mutualist group dynamics in accordance with respective corporatestrategies. Age-based premium fairness is also mandated. In certain insurancedomains, variables such as the presence of serious illnesses or disabilitiesare emerging as new dimensions for evaluating fairness. Regardless of themotivating factor prompting an insurer to adopt fairer pricing strategies for aspecific variable, the insurer must possess the capability to define, measure,and ultimately mitigate any ethical biases inherent in its pricing practiceswhile upholding standards of consistency and performance. This study seeks toprovide a comprehensive set of tools for these endeavors and assess theireffectiveness through practical application in the context of automobileinsurance.</description><author>Mulah Moriah, Franck Vermet, Arthur Charpentier</author><pubDate>Mon, 20 Nov 2023 16:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11900v1</guid></item><item><title>AMES: A Differentiable Embedding Space Selection Framework for Latent Graph Inference</title><link>http://arxiv.org/abs/2311.11891v1</link><description>In real-world scenarios, although data entities may possess inherentrelationships, the specific graph illustrating their connections might not bedirectly accessible. Latent graph inference addresses this issue by enablingGraph Neural Networks (GNNs) to operate on point cloud data, dynamicallylearning the necessary graph structure. These graphs are often derived from alatent embedding space, which can be modeled using Euclidean, hyperbolic,spherical, or product spaces. However, currently, there is no principleddifferentiable method for determining the optimal embedding space. In thiswork, we introduce the Attentional Multi-Embedding Selection (AMES) framework,a differentiable method for selecting the best embedding space for latent graphinference through backpropagation, considering a downstream task. Our frameworkconsistently achieves comparable or superior results compared to previousmethods for latent graph inference across five benchmark datasets. Importantly,our approach eliminates the need for conducting multiple experiments toidentify the optimal embedding space. Furthermore, we explore interpretabilitytechniques that track the gradient contributions of different latent graphs,shedding light on how our attention-based, fully differentiable approach learnsto choose the appropriate latent space. In line with previous works, ourexperiments emphasize the advantages of hyperbolic spaces in enhancingperformance. More importantly, our interpretability framework provides ageneral approach for quantitatively comparing embedding spaces across differenttasks based on their contributions, a dimension that has been overlooked inprevious literature on latent graph inference.</description><author>Yuan Lu, Haitz Sáez de Ocáriz Borde, Pietro Liò</author><pubDate>Mon, 20 Nov 2023 16:24:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11891v1</guid></item><item><title>SniffyArt: The Dataset of Smelling Persons</title><link>http://arxiv.org/abs/2311.11888v1</link><description>Smell gestures play a crucial role in the investigation of past smells in thevisual arts yet their automated recognition poses significant challenges. Thispaper introduces the SniffyArt dataset, consisting of 1941 individualsrepresented in 441 historical artworks. Each person is annotated with a tightlyfitting bounding box, 17 pose keypoints, and a gesture label. By integratingthese annotations, the dataset enables the development of hybrid classificationapproaches for smell gesture recognition. The datasets high-quality human poseestimation keypoints are achieved through the merging of five separate sets ofkeypoint annotations per person. The paper also presents a baseline analysis,evaluating the performance of representative algorithms for detection, keypointestimation, and classification tasks, showcasing the potential of combiningkeypoint estimation with smell gesture classification. The SniffyArt datasetlays a solid foundation for future research and the exploration of multi-taskapproaches leveraging pose keypoints and person boxes to advance human gestureand olfactory dimension analysis in historical artworks.</description><author>Mathias Zinnen, Azhar Hussian, Hang Tran, Prathmesh Madhu, Andreas Maier, Vincent Christlein</author><pubDate>Mon, 20 Nov 2023 16:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11888v1</guid></item><item><title>Look into the Mirror: Evolving Self-Dual Bent Boolean Functions</title><link>http://arxiv.org/abs/2311.11884v1</link><description>Bent Boolean functions are important objects in cryptography and codingtheory, and there are several general approaches for constructing suchfunctions. Metaheuristics proved to be a strong choice as they can provide manybent functions, even when the size of the Boolean function is large (e.g., morethan 20 inputs). While bent Boolean functions represent only a small part ofall Boolean functions, there are several subclasses of bent functions providingspecific properties and challenges. One of the most interesting subclassescomprises (anti-)self-dual bent Boolean functions. This paper provides adetailed experimentation with evolutionary algorithms with the goal of evolving(anti-)self-dual bent Boolean functions. We experiment with two encodings andtwo fitness functions to directly evolve self-dual bent Boolean functions. Ourexperiments consider Boolean functions with sizes of up to 16 inputs, and wesuccessfully construct self-dual bent functions for each dimension. Moreover,when comparing with the evolution of bent Boolean functions, we notice that thedifficulty for evolutionary algorithms is rather similar. Finally, we alsotried evolving secondary constructions for self-dual bent functions, but thisdirection provided no successful results.</description><author>Claude Carlet, Marko Ðurasevic, Domagoj Jakobovic, Luca Mariot, Stjepan Picek</author><pubDate>Mon, 20 Nov 2023 16:20:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11884v1</guid></item><item><title>Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review</title><link>http://arxiv.org/abs/2311.11883v1</link><description>The field of Tiny Machine Learning (TinyML) has gained significant attentiondue to its potential to enable intelligent applications on resource-constraineddevices. This review provides an in-depth analysis of the advancements inefficient neural networks and the deployment of deep learning models onultra-low power microcontrollers (MCUs) for TinyML applications. It begins byintroducing neural networks and discussing their architectures and resourcerequirements. It then explores MEMS-based applications on ultra-low power MCUs,highlighting their potential for enabling TinyML on resource-constraineddevices. The core of the review centres on efficient neural networks forTinyML. It covers techniques such as model compression, quantization, andlow-rank factorization, which optimize neural network architectures for minimalresource utilization on MCUs. The paper then delves into the deployment of deeplearning models on ultra-low power MCUs, addressing challenges such as limitedcomputational capabilities and memory resources. Techniques like model pruning,hardware acceleration, and algorithm-architecture co-design are discussed asstrategies to enable efficient deployment. Lastly, the review provides anoverview of current limitations in the field, including the trade-off betweenmodel complexity and resource constraints. Overall, this review paper presentsa comprehensive analysis of efficient neural networks and deployment strategiesfor TinyML on ultra-low-power MCUs. It identifies future research directionsfor unlocking the full potential of TinyML applications on resource-constraineddevices.</description><author>Minh Tri Lê, Pierre Wolinski, Julyan Arbel</author><pubDate>Mon, 20 Nov 2023 16:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11883v1</guid></item><item><title>Multi-Task Faces (MTF) Data Set: A Legally and Ethically Compliant Collection of Face Images for Various Classification Tasks</title><link>http://arxiv.org/abs/2311.11882v1</link><description>Human facial data hold tremendous potential to address a variety ofclassification problems, including face recognition, age estimation, genderidentification, emotion analysis, and race classification. However, recentprivacy regulations, such as the EU General Data Protection Regulation andothers, have restricted the ways in which human images may be collected andused for research. As a result, several previously published data setscontaining human faces have been removed from the internet due to inadequatedata collection methods that failed to meet privacy regulations. Data setsconsisting of synthetic data have been proposed as an alternative, but theyfall short of accurately representing the real data distribution. On the otherhand, most available data sets are labeled for just a single task, which limitstheir applicability. To address these issues, we present the Multi-Task Faces(MTF) image data set, a meticulously curated collection of face images designedfor various classification tasks, including face recognition, as well as race,gender, and age classification. The MTF data set has been ethically gathered byleveraging publicly available images of celebrities and strictly adhering tocopyright regulations. In this paper, we present this data set and providedetailed descriptions of the followed data collection and processingprocedures. Furthermore, we evaluate the performance of five deep learning (DL)models on the MTF data set across the aforementioned classification tasks.Additionally, we compare the performance of DL models over the processed MTFdata and over raw data crawled from the internet. The reported resultsconstitute a baseline for further research employing these data. The MTF dataset can be accessed through the following link (please cite the present paperif you use the data set): https://github.com/RamiHaf/MTF_data_set</description><author>Rami Haffar, David Sánchez, Josep Domingo-Ferrer</author><pubDate>Mon, 20 Nov 2023 16:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11882v1</guid></item><item><title>Sustainable Concrete via Bayesian Optimization</title><link>http://arxiv.org/abs/2310.18288v3</link><description>Eight percent of global carbon dioxide emissions can be attributed to theproduction of cement, the main component of concrete, which is also thedominant source of CO2 emissions in the construction of data centers. Thediscovery of lower-carbon concrete formulae is therefore of high significancefor sustainability. However, experimenting with new concrete formulae is timeconsuming and labor intensive, as one usually has to wait to record theconcrete's 28-day compressive strength, a quantity whose measurement can by itsdefinition not be accelerated. This provides an opportunity for experimentaldesign methodology like Bayesian Optimization (BO) to accelerate the search forstrong and sustainable concrete formulae. Herein, we 1) propose modeling stepsthat make concrete strength amenable to be predicted accurately by a Gaussianprocess model with relatively few measurements, 2) formulate the search forsustainable concrete as a multi-objective optimization problem, and 3) leveragethe proposed model to carry out multi-objective BO with real-world strengthmeasurements of the algorithmically proposed mixes. Our experimental resultsshow improved trade-offs between the mixtures' global warming potential (GWP)and their associated compressive strengths, compared to mixes based on currentindustry practices. Our methods are open-sourced atgithub.com/facebookresearch/SustainableConcrete.</description><author>Sebastian Ament, Andrew Witte, Nishant Garg, Julius Kusuma</author><pubDate>Mon, 20 Nov 2023 16:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18288v3</guid></item><item><title>A New Angle: On Evolving Rotation Symmetric Boolean Functions</title><link>http://arxiv.org/abs/2311.11881v1</link><description>Rotation symmetric Boolean functions represent an interesting class ofBoolean functions as they are relatively rare compared to general Booleanfunctions. At the same time, the functions in this class can have excellentproperties, making them interesting for various practical applications. Theusage of metaheuristics to construct rotation symmetric Boolean functions is adirection that has been explored for almost twenty years. Despite that, thereare very few results considering evolutionary computation methods. This paperuses several evolutionary algorithms to evolve rotation symmetric Booleanfunctions with different properties. Despite using generic metaheuristics, weobtain results that are competitive with prior work relying on customizedheuristics. Surprisingly, we find that bitstring and floating point encodingswork better than the tree encoding. Moreover, evolving highly nonlinear generalBoolean functions is easier than rotation symmetric ones.</description><author>Claude Carlet, Marko Ðurasevic, Bruno Gašperov, Domagoj Jakobovic, Luca Mariot, Stjepan Picek</author><pubDate>Mon, 20 Nov 2023 16:16:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11881v1</guid></item><item><title>Forward Gradients for Data-Driven CFD Wall Modeling</title><link>http://arxiv.org/abs/2311.11876v1</link><description>Computational Fluid Dynamics (CFD) is used in the design and optimization ofgas turbines and many other industrial/ scientific applications. However, thepractical use is often limited by the high computational cost, and the accurateresolution of near-wall flow is a significant contributor to this cost. Machinelearning (ML) and other data-driven methods can complement existing wallmodels. Nevertheless, training these models is bottlenecked by the largecomputational effort and memory footprint demanded by back-propagation. Recentwork has presented alternatives for computing gradients of neural networkswhere a separate forward and backward sweep is not needed and storage ofintermediate results between sweeps is not required because an unbiasedestimator for the gradient is computed in a single forward sweep. In thispaper, we discuss the application of this approach for training a subgrid wallmodel that could potentially be used as a surrogate in wall-bounded flow CFDsimulations to reduce the computational overhead while preserving predictiveaccuracy.</description><author>Jan Hückelheim, Tadbhagya Kumar, Krishnan Raghavan, Pinaki Pal</author><pubDate>Mon, 20 Nov 2023 16:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11876v1</guid></item><item><title>Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework</title><link>http://arxiv.org/abs/2310.04741v3</link><description>Continual learning (CL) algorithms strive to acquire new knowledge whilepreserving prior information. However, this stability-plasticity trade-offremains a central challenge. This paper introduces a framework that dissectsthis trade-off, offering valuable insights into CL algorithms. TheReadout-Decomposition of Activation Change (RDAC) framework first addresses thestability-plasticity dilemma and its relation to catastrophic forgetting. Itrelates learning-induced activation changes in the range of prior readouts tothe degree of stability and changes in the null space to the degree ofplasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, theframework clarifies the stability-plasticity trade-offs of the popularregularization algorithms Synaptic intelligence (SI), Elastic-weightconsolidation (EWC), and learning without Forgetting (LwF), and replay-basedalgorithms Gradient episodic memory (GEM), and data replay. GEM and data replaypreserved stability and plasticity, while SI, EWC, and LwF traded offplasticity for stability. The inability of the regularization algorithms tomaintain plasticity was linked to them restricting the change of activations inthe null space of the prior readout. Additionally, for one-hidden-layer linearneural networks, we derived a gradient decomposition algorithm to restrictactivation change only in the range of the prior readouts, to maintain highstability while not further sacrificing plasticity. Results demonstrate thatthe algorithm maintained stability without significant plasticity loss. TheRDAC framework informs the behavior of existing CL algorithms and paves the wayfor novel CL approaches. Finally, it sheds light on the connection betweenlearning-induced activation/representation changes and the stability-plasticitydilemma, also offering insights into representational drift in biologicalsystems.</description><author>Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann</author><pubDate>Mon, 20 Nov 2023 16:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04741v3</guid></item><item><title>Training robust and generalizable quantum models</title><link>http://arxiv.org/abs/2311.11871v1</link><description>Adversarial robustness and generalization are both crucial properties ofreliable machine learning models. In this paper, we study these properties inthe context of quantum machine learning based on Lipschitz bounds. We derivetailored, parameter-dependent Lipschitz bounds for quantum models withtrainable encoding, showing that the norm of the data encoding has a crucialimpact on the robustness against perturbations in the input data. Further, wederive a bound on the generalization error which explicitly depends on theparameters of the data encoding. Our theoretical findings give rise to apractical strategy for training robust and generalizable quantum models byregularizing the Lipschitz bound in the cost. Further, we show that, for fixedand non-trainable encodings as frequently employed in quantum machine learning,the Lipschitz bound cannot be influenced by tuning the parameters. Thus,trainable encodings are crucial for systematically adapting robustness andgeneralization during training. With numerical results, we demonstrate that,indeed, Lipschitz bound regularization leads to substantially more robust andgeneralizable quantum models.</description><author>Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm</author><pubDate>Mon, 20 Nov 2023 16:06:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11871v1</guid></item><item><title>Towards Exploratory Reformulation of Constraint Models</title><link>http://arxiv.org/abs/2311.11868v1</link><description>It is well established that formulating an effective constraint model of aproblem of interest is crucial to the efficiency with which it can subsequentlybe solved. Following from the observation that it is difficult, if notimpossible, to know a priori which of a set of candidate models will performbest in practice, we envisage a system that explores the space of modelsthrough a process of reformulation from an initial model, guided by performanceon a set of training instances from the problem class under consideration. Weplan to situate this system in a refinement-based approach, where a user writesa constraint specification describing a problem above the level of abstractionat which many modelling decisions are made. In this position paper we set outour plan for an exploratory reformulation system, and discuss progress made sofar.</description><author>Ian Miguel, András Z. Salamon, Christopher Stone</author><pubDate>Mon, 20 Nov 2023 16:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11868v1</guid></item><item><title>Analyzing Emissions and Energy Efficiency in Mixed Traffic Control at Unsignalized Intersections</title><link>http://arxiv.org/abs/2311.11866v1</link><description>Greenhouse gas emissions have dramatically risen since the early 1900s withU.S. transportation generating 28% of the U.S' emissions. As such, there isinterest in reducing transportation-related emissions. Specifically,sustainability research has sprouted around signalized intersections asintersections allow different streams of traffic to cross and changedirections. Recent research has developed mixed traffic control eco-drivingstrategies at signalized intersections to decrease emissions. However, theinherent structure of a signalized intersection generates increased emissionsby creating frequent acceleration/deceleration events, excessive idling fromtraffic congestion, and stop-and-go waves. Thus, we believe unsignalizedintersections hold potential for further sustainability improvements. In thiswork, we provide an emissions analysis on unsignalized intersections withcomplex, real-world topologies and traffic demands where mixed traffic controlstrategies are employed by robot vehicles (RVs) to reduce waiting times andcongestion. We find with at least 10% RV penetration rate, RVs generate lessfuel consumption and NOx emissions than signalized intersections by up to 27%and 28%, respectively. With at least 30% RVs, CO and HC emissions are reducedby up to 42% and 43%, respectively. Additionally, RVs can reduce emissionsacross the whole network despite only employing their strategies at theintersections.</description><author>Michael Villarreal, Dawei Wang, Jia Pan, Weizi Li</author><pubDate>Mon, 20 Nov 2023 16:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11866v1</guid></item><item><title>VLM-Eval: A General Evaluation on Video Large Language Models</title><link>http://arxiv.org/abs/2311.11865v1</link><description>Despite the rapid development of video Large Language Models (LLMs), acomprehensive evaluation is still absent. In this paper, we introduce a unifiedevaluation that encompasses multiple video tasks, including captioning,question and answering, retrieval, and action recognition. In addition toconventional metrics, we showcase how GPT-based evaluation can match human-likeperformance in assessing response quality across multiple aspects. We propose asimple baseline: Video-LLaVA, which uses a single linear projection andoutperforms existing video LLMs. Finally, we evaluate video LLMs beyondacademic datasets, which show encouraging recognition and reasoningcapabilities in driving scenarios with only hundreds of video-instruction pairsfor fine-tuning. We hope our work can serve as a unified evaluation for videoLLMs, and help expand more practical scenarios. The evaluation code will beavailable soon.</description><author>Shuailin Li, Yuang Zhang, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, Tiancai Wang</author><pubDate>Mon, 20 Nov 2023 16:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11865v1</guid></item><item><title>Preserving Patient Privacy in MRI Scans: A Comprehensive Approach with 3D Masked Autoencoders</title><link>http://arxiv.org/abs/2310.15778v2</link><description>MRI scans provide valuable medical information, however they also containsensitive and personally identifiable information (PII) that needs to beprotected. Whereas MRI metadata is easily sanitized, MRI image data is aprivacy risk because it contains information to render highly-realistic 3Dvisualizations of a patient's head, enabling malicious actors to possiblyidentify the subject by cross-referencing a database. Data anonymization andde-identification is concerned with ensuring the privacy and confidentiality ofindividuals' personal information. Traditional MRI de-identification methodsremove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. Thiscomes at the expense of introducing a domain shift that can throw offdownstream analyses. Recently, a GAN-based approach was proposed to de-identifya patient's scan by remodeling it (\eg changing the face) rather than byremoving parts. In this work, we propose CP-MAE, a model that de-identifies theface using masked autoencoders and that outperforms all previous approaches interms of downstream task performance as well as de-identification. With ourmethod we are able to synthesize scans of resolution up to $256^3$ (previously$128^3$) which constitutes an eight-fold increase in the number of voxels.Using our construction we were able to design a system that exhibits a highlyrobust training stage, making it easy to fit the network on novel data.</description><author>Lennart Alexander Van der Goten, Kevin Smith</author><pubDate>Mon, 20 Nov 2023 15:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15778v2</guid></item><item><title>GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding</title><link>http://arxiv.org/abs/2311.11863v1</link><description>Applying NeRF to downstream perception tasks for scene understanding andrepresentation is becoming increasingly popular. Most existing methods treatsemantic prediction as an additional rendering task, \textit{i.e.}, the "labelrendering" task, to build semantic NeRFs. However, by renderingsemantic/instance labels per pixel without considering the contextualinformation of the rendered image, these methods usually suffer from unclearboundary segmentation and abnormal segmentation of pixels within an object. Tosolve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novelpipeline that makes the widely used segmentation model and NeRF work compatiblyunder a unified framework, for facilitating context-aware 3D scene perception.To accomplish this goal, we introduce transformers to aggregate radiance aswell as semantic embedding fields jointly for novel views and facilitate thejoint volumetric rendering of both fields. In addition, we propose twoself-distillation mechanisms, i.e., the Semantic Distill Loss and theDepth-Guided Semantic Distill Loss, to enhance the discrimination and qualityof the semantic field and the maintenance of geometric consistency. Inevaluation, we conduct experimental comparisons under two perception tasks(\textit{i.e.} semantic and instance segmentation) using both synthetic andreal-world datasets. Notably, our method outperforms SOTA approaches by 6.94\%,11.76\%, and 8.47\% on generalized semantic segmentation, finetuning semanticsegmentation, and instance segmentation, respectively.</description><author>Hao Li, Dingwen Zhang, Yalun Dai, Nian Liu, Lechao Cheng, Jingfeng Li, Jingdong Wang, Junwei Han</author><pubDate>Mon, 20 Nov 2023 15:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11863v1</guid></item><item><title>Establishing Central Sensitization Inventory Cut-off Values in patients with Chronic Low Back Pain by Unsupervised Machine Learning</title><link>http://arxiv.org/abs/2311.11862v1</link><description>Human Assumed Central Sensitization is involved in the development andmaintenance of chronic low back pain (CLBP). The Central SensitizationInventory (CSI) was developed to evaluate the presence of HACS, with a cut-offvalue of 40/100 based on patients with chronic pain. However, various factorsincluding pain conditions (e.g., CLBP), and gender may influence this cut-offvalue. For chronic pain condition such as CLBP, unsupervised clusteringapproaches can take these factors into consideration and automatically learnthe HACS-related patterns. Therefore, this study aimed to determine the cut-offvalues for a Dutch-speaking population with CLBP, considering the total groupand stratified by gender based on unsupervised machine learning. In this study,questionnaire data covering pain, physical, and psychological aspects werecollected from patients with CLBP and aged-matched pain-free adults (referredto as healthy controls, HC). Four clustering approaches were applied toidentify HACS-related clusters based on the questionnaire data and gender. Theclustering performance was assessed using internal and external indicators.Subsequently, receiver operating characteristic analysis was conducted on thebest clustering results to determine the optimal cut-off values. The studyincluded 151 subjects, consisting of 63 HCs and 88 patients with CLBP.Hierarchical clustering yielded the best results, identifying three clusters:healthy group, CLBP with low HACS level, and CLBP with high HACS level groups.Based on the low HACS levels group (including HC and CLBP with low HACS level)and high HACS level group, the cut-off value for the overall groups were 35, 34for females, and 35 for. The findings suggest that the optimal cut-off valuesfor CLBP is 35. The gender-related cut-off values should be interpreted withcaution due to the unbalanced gender distribution in the sample.</description><author>Xiaoping Zheng, Claudine JC Lamoth, Hans Timmerman, Ebert Otten, Michiel F Reneman</author><pubDate>Mon, 20 Nov 2023 15:57:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11862v1</guid></item><item><title>Unsupervised Opinion Summarization Using Approximate Geodesics</title><link>http://arxiv.org/abs/2209.07496v3</link><description>Opinion summarization is the task of creating summaries capturing popularopinions from user reviews. In this paper, we introduce Geodesic Summarizer(GeoSumm), a novel system to perform unsupervised extractive opinionsummarization. GeoSumm involves an encoder-decoder based representationlearning model, that generates representations of text as a distribution overlatent semantic units. GeoSumm generates these representations by performingdictionary learning over pre-trained text representations at multiple decoderlayers. We then use these representations to quantify the relevance of reviewsentences using a novel approximate geodesic distance based scoring mechanism.We use the relevance scores to identify popular opinions in order to composegeneral and aspect-specific summaries. Our proposed model, GeoSumm, achievesstate-of-the-art performance on three opinion summarization datasets. Weperform additional experiments to analyze the functioning of our model andshowcase the generalization ability of {\X} across different domains.</description><author>Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi</author><pubDate>Mon, 20 Nov 2023 15:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.07496v3</guid></item><item><title>Generating Valid and Natural Adversarial Examples with Large Language Models</title><link>http://arxiv.org/abs/2311.11861v1</link><description>Deep learning-based natural language processing (NLP) models, particularlypre-trained language models (PLMs), have been revealed to be vulnerable toadversarial attacks. However, the adversarial examples generated by manymainstream word-level adversarial attack models are neither valid nor natural,leading to the loss of semantic maintenance, grammaticality, and humanimperceptibility. Based on the exceptional capacity of language understandingand generation of large language models (LLMs), we propose LLM-Attack, whichaims at generating both valid and natural adversarial examples with LLMs. Themethod consists of two stages: word importance ranking (which searches for themost vulnerable words) and word synonym replacement (which substitutes themwith their synonyms obtained from LLMs). Experimental results on the MovieReview (MR), IMDB, and Yelp Review Polarity datasets against the baselineadversarial attack models illustrate the effectiveness of LLM-Attack, and itoutperforms the baselines in human and GPT-4 evaluation by a significantmargin. The model can generate adversarial examples that are typically validand natural, with the preservation of semantic meaning, grammaticality, andhuman imperceptibility.</description><author>Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen</author><pubDate>Mon, 20 Nov 2023 15:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11861v1</guid></item><item><title>LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</title><link>http://arxiv.org/abs/2311.11860v1</link><description>Multimodal Large Language Models (MLLMs) have endowed LLMs with the abilityto perceive and understand multi-modal signals. However, most of the existingMLLMs mainly adopt vision encoders pretrained on coarsely aligned image-textpairs, leading to insufficient extraction and reasoning of visual knowledge. Toaddress this issue, we devise a dual-Level vIsual knOwledge eNhanced MultimodalLarge Language Model (LION), which empowers the MLLM by injecting visualknowledge in two levels. 1) Progressive incorporation of fine-grainedspatial-aware visual knowledge. We design a vision aggregator cooperated withregion-level vision-language (VL) tasks to incorporate fine-grainedspatial-aware visual knowledge into the MLLM. To alleviate the conflict betweenimage-level and region-level VL tasks during incorporation, we devise adedicated stage-wise instruction-tuning strategy with mixture-of-adapters. Thisprogressive incorporation scheme contributes to the mutual promotion betweenthese two kinds of VL tasks. 2) Soft prompting of high-level semantic visualevidence. We facilitate the MLLM with high-level semantic visual evidence byleveraging diverse image tags. To mitigate the potential influence caused byimperfect predicted tags, we propose a soft prompting method by embedding alearnable token into the tailored text instruction. Comprehensive experimentson several multi-modal benchmarks demonstrate the superiority of our model(e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps overInstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).</description><author>Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, Liqiang Nie</author><pubDate>Mon, 20 Nov 2023 15:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11860v1</guid></item><item><title>FATURA: A Multi-Layout Invoice Image Dataset for Document Analysis and Understanding</title><link>http://arxiv.org/abs/2311.11856v1</link><description>Document analysis and understanding models often require extensive annotateddata to be trained. However, various document-related tasks extend beyond meretext transcription, requiring both textual content and precise bounding-boxannotations to identify different document elements. Collecting such databecomes particularly challenging, especially in the context of invoices, whereprivacy concerns add an additional layer of complexity. In this paper, weintroduce FATURA, a pivotal resource for researchers in the field of documentanalysis and understanding. FATURA is a highly diverse dataset featuringmulti-layout, annotated invoice document images. Comprising $10,000$ invoiceswith $50$ distinct layouts, it represents the largest openly accessible imagedataset of invoice documents known to date. We also provide comprehensivebenchmarks for various document analysis and understanding tasks and conductexperiments under diverse training and evaluation scenarios. The dataset isfreely accessible at https://zenodo.org/record/8261508, empowering researchersto advance the field of document analysis and understanding.</description><author>Mahmoud Limam, Marwa Dhiaf, Yousri Kessentini</author><pubDate>Mon, 20 Nov 2023 15:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11856v1</guid></item><item><title>JaxMARL: Multi-Agent RL Environments in JAX</title><link>http://arxiv.org/abs/2311.10090v3</link><description>Benchmarks play an important role in the development of machine learningalgorithms. For example, research in reinforcement learning (RL) has beenheavily influenced by available environments and benchmarks. However, RLenvironments are traditionally run on the CPU, limiting their scalability withtypical academic compute. Recent advancements in JAX have enabled the wider useof hardware acceleration to overcome these computational hurdles, enablingmassively parallel RL training pipelines and environments. This is particularlyuseful for multi-agent reinforcement learning (MARL) research. First of all,multiple agents must be considered at each environment step, addingcomputational burden, and secondly, the sample complexity is increased due tonon-stationarity, decentralised partial observability, or other MARLchallenges. In this paper, we present JaxMARL, the first open-source code basethat combines ease-of-use with GPU enabled efficiency, and supports a largenumber of commonly used MARL environments as well as popular baselinealgorithms. When considering wall clock time, our experiments show that per-runour JAX-based training pipeline is up to 12500x faster than existingapproaches. This enables efficient and thorough evaluations, with the potentialto alleviate the evaluation crisis of the field. We also introduce andbenchmark SMAX, a vectorised, simplified version of the popular StarCraftMulti-Agent Challenge, which removes the need to run the StarCraft II gameengine. This not only enables GPU acceleration, but also provides a moreflexible MARL environment, unlocking the potential for self-play,meta-learning, and other future applications in MARL. We provide code athttps://github.com/flairox/jaxmarl.</description><author>Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, Saptarashmi Bandyopadhyay, Mikayel Samvelyan, Minqi Jiang, Robert Tjarko Lange, Shimon Whiteson, Bruno Lacerda, Nick Hawes, Tim Rocktaschel, Chris Lu, Jakob Nicolaus Foerster</author><pubDate>Mon, 20 Nov 2023 15:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10090v3</guid></item><item><title>Evil Geniuses: Delving into the Safety of LLM-based Agents</title><link>http://arxiv.org/abs/2311.11855v1</link><description>The rapid advancements in large language models (LLMs) have led to aresurgence in LLM-based agents, which demonstrate impressive human-likebehaviors and cooperative capabilities in various interactions and strategyformulations. However, evaluating the safety of LLM-based agents remains acomplex challenge. This paper elaborately conducts a series of manual jailbreakprompts along with a virtual chat-powered evil plan development team, dubbedEvil Geniuses, to thoroughly probe the safety aspects of these agents. Ourinvestigation reveals three notable phenomena: 1) LLM-based agents exhibitreduced robustness against malicious attacks. 2) the attacked agents couldprovide more nuanced responses. 3) the detection of the produced improperresponses is more challenging. These insights prompt us to question theeffectiveness of LLM-based attacks on agents, highlighting vulnerabilities atvarious levels and within different role specializations within thesystem/agent of LLM-based agents. Extensive evaluation and discussion revealthat LLM-based agents face significant challenges in safety and yield insightsfor future research. Our code is available athttps://github.com/T1aNS1R/Evil-Geniuses.</description><author>Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, Hang Su</author><pubDate>Mon, 20 Nov 2023 15:50:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11855v1</guid></item><item><title>Asynchronous Bioplausible Neuron for Spiking Neural Networks for Event-Based Vision</title><link>http://arxiv.org/abs/2311.11853v1</link><description>Spiking Neural Networks (SNNs) offer a biologically inspired approach tocomputer vision that can lead to more efficient processing of visual data withreduced energy consumption. However, maintaining homeostasis within thesenetworks is challenging, as it requires continuous adjustment of neuralresponses to preserve equilibrium and optimal processing efficiency amidstdiverse and often unpredictable input signals. In response to these challenges,we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firingmechanism to auto-adjust the variations in the input signal. Comprehensiveevaluation across various datasets demonstrates ABN's enhanced performance inimage classification and segmentation, maintenance of neural equilibrium, andenergy efficiency.</description><author>Sanket Kachole, Hussain Sajwani, Fariborz Baghaei Naeini, Dimitrios Makris, Yahya Zweiri</author><pubDate>Mon, 20 Nov 2023 15:45:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11853v1</guid></item><item><title>Deep learning complete intersection Calabi-Yau manifolds</title><link>http://arxiv.org/abs/2311.11847v1</link><description>We review advancements in deep learning techniques for complete intersectionCalabi-Yau (CICY) 3- and 4-folds, with the aim of understanding better how tohandle algebraic topological data with machine learning. We first discussmethodological aspects and data analysis, before describing neural networksarchitectures. Then, we describe the state-of-the art accuracy in predictingHodge numbers. We include new results on extrapolating predictions from low tohigh Hodge numbers, and conversely.</description><author>Harold Erbin, Riccardo Finotello</author><pubDate>Mon, 20 Nov 2023 15:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11847v1</guid></item><item><title>Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses</title><link>http://arxiv.org/abs/2311.11846v1</link><description>Segmenting an address into meaningful components, also known as addressparsing, is an essential step in many applications from record linkage togeocoding and package delivery. Consequently, a lot of work has been dedicatedto develop accurate address parsing techniques, with machine learning andneural network methods leading the state-of-the-art scoreboard. However, mostof the work on address parsing has been confined to academic endeavours withlittle availability of free and easy-to-use open-source solutions. This paper presents Deepparse, a Python open-source, extendable, fine-tunableaddress parsing solution under LGPL-3.0 licence to parse multinationaladdresses using state-of-the-art deep learning algorithms and evaluated on over60 countries. It can parse addresses written in any language and use anyaddress standard. The pre-trained model achieves average $99~\%$ parsingaccuracies on the countries used for training with no pre-processing norpost-processing needed. Moreover, the library supports fine-tuning with newdata to generate a custom address parser.</description><author>David Beauchemin, Marouane Yassine</author><pubDate>Mon, 20 Nov 2023 15:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11846v1</guid></item><item><title>Entangled View-Epipolar Information Aggregation for Generalizable Neural Radiance Fields</title><link>http://arxiv.org/abs/2311.11845v1</link><description>Generalizable NeRF can directly synthesize novel views across new scenes,eliminating the need for scene-specific retraining in vanilla NeRF. A criticalenabling factor in these approaches is the extraction of a generalizable 3Drepresentation by aggregating source-view features. In this paper, we proposean Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF.Different from existing methods that consider cross-view and along-epipolarinformation independently, EVE-NeRF conducts the view-epipolar featureaggregation in an entangled manner by injecting the scene-invariant appearancecontinuity and geometry consistency priors to the aggregation process. Ourapproach effectively mitigates the potential lack of inherent geometric andappearance constraint resulting from one-dimensional interactions, thus furtherboosting the 3D representation generalizablity. EVE-NeRF attainsstate-of-the-art performance across various evaluation scenarios. Extensiveexperiments demonstate that, compared to prevailing single-dimensionalaggregation, the entangled network excels in the accuracy of 3D scene geometryand appearance reconstruction.Our project page ishttps://github.com/tatakai1/EVENeRF.</description><author>Zhiyuan Min, Yawei Luo, Wei Yang, Yuesong Wang, Yi Yang</author><pubDate>Mon, 20 Nov 2023 15:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11845v1</guid></item><item><title>How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents</title><link>http://arxiv.org/abs/2311.11844v1</link><description>Recent advances in large language models (LLMs) like GPT-3 and GPT-4 haveopened up new opportunities for text analysis in political science. Theypromise automation with better results and less programming. In this study, weevaluate LLMs on three original coding tasks of non-English political sciencetexts, and we provide a detailed description of a general workflow for usingLLMs for text coding in political science research. Our use case offers apractical guide for researchers looking to incorporate LLMs into their researchon text analysis. We find that, when provided with detailed label definitionsand coding examples, an LLM can be as good as or even better than a humanannotator while being much faster (up to hundreds of times), considerablycheaper (costing up to 60% less than human coding), and much easier to scale tolarge amounts of text. Overall, LLMs present a viable option for most textcoding projects.</description><author>Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena Wängnerud</author><pubDate>Mon, 20 Nov 2023 15:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11844v1</guid></item><item><title>Reward Teaching for Federated Multi-armed Bandits</title><link>http://arxiv.org/abs/2305.02441v2</link><description>Most of the existing federated multi-armed bandits (FMAB) designs are basedon the presumption that clients will implement the specified design tocollaborate with the server. In reality, however, it may not be possible tomodify the clients' existing protocols. To address this challenge, this workfocuses on clients who always maximize their individual cumulative rewards, andintroduces a novel idea of ``reward teaching'', where the server guides theclients towards global optimality through implicit local reward adjustments.Under this framework, the server faces two tightly coupled tasks of banditlearning and target teaching, whose combination is non-trivial and challenging.A phased approach, called Teaching-After-Learning (TAL), is first designed toencourage and discourage clients' explorations separately. General performanceanalyses of TAL are established when the clients' strategies satisfy certainmild requirements. With novel technical approaches developed to analyze thewarm-start behaviors of bandit algorithms, particularized guarantees of TALwith clients running UCB or epsilon-greedy strategies are then obtained. Theseresults demonstrate that TAL achieves logarithmic regrets while only incurringlogarithmic adjustment costs, which is order-optimal w.r.t. a natural lowerbound. As a further extension, the Teaching-While-Learning (TWL) algorithm isdeveloped with the idea of successive arm elimination to break the non-adaptivephase separation in TAL. Rigorous analyses demonstrate that when facing clientswith UCB1, TWL outperforms TAL in terms of the dependencies on sub-optimalitygaps thanks to its adaptive design. Experimental results demonstrate theeffectiveness and generality of the proposed algorithms.</description><author>Chengshuai Shi, Wei Xiong, Cong Shen, Jing Yang</author><pubDate>Mon, 20 Nov 2023 15:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02441v2</guid></item><item><title>High Probability Guarantees for Random Reshuffling</title><link>http://arxiv.org/abs/2311.11841v1</link><description>We consider the stochastic gradient method with random reshuffling($\mathsf{RR}$) for tackling smooth nonconvex optimization problems.$\mathsf{RR}$ finds broad applications in practice, notably in training neuralnetworks. In this work, we first investigate the concentration property of$\mathsf{RR}$'s sampling procedure and establish a new high probability samplecomplexity guarantee for driving the gradient (without expectation) below$\varepsilon$, which effectively characterizes the efficiency of a single$\mathsf{RR}$ execution. Our derived complexity matches the best existingin-expectation one up to a logarithmic term while imposing no additionalassumptions nor changing $\mathsf{RR}$'s updating rule. Furthermore, byleveraging our derived high probability descent property and bound on thestochastic error, we propose a simple and computable stopping criterion for$\mathsf{RR}$ (denoted as $\mathsf{RR}$-$\mathsf{sc}$). This criterion isguaranteed to be triggered after a finite number of iterations, and then$\mathsf{RR}$-$\mathsf{sc}$ returns an iterate with its gradient below$\varepsilon$ with high probability. Moreover, building on the proposedstopping criterion, we design a perturbed random reshuffling method($\mathsf{p}$-$\mathsf{RR}$) that involves an additional randomizedperturbation procedure near stationary points. We derive that$\mathsf{p}$-$\mathsf{RR}$ provably escapes strict saddle points andefficiently returns a second-order stationary point with high probability,without making any sub-Gaussian tail-type assumptions on the stochasticgradient errors. Finally, we conduct numerical experiments on neural networktraining to support our theoretical findings.</description><author>Hengxu Yu, Xiao Li</author><pubDate>Mon, 20 Nov 2023 15:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11841v1</guid></item><item><title>SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics</title><link>http://arxiv.org/abs/2310.20049v3</link><description>Simulating fluid dynamics is crucial for the design and development process,ranging from simple valves to complex turbomachinery. Accurately solving theunderlying physical equations is computationally expensive. Therefore,learning-based solvers that model interactions on meshes have gained interestdue to their promising speed-ups. However, it is unknown to what extent thesemodels truly understand the underlying physical principles and can generalizerather than interpolate. Generalization is a key requirement for ageneral-purpose fluid simulator, which should adapt to different topologies,resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed totest the $\textit{generalization}$ of learned graph-based fluid simulators.SURF comprises individual datasets and provides specific performance andgeneralization metrics for evaluating and comparing different models. Weempirically demonstrate the applicability of SURF by thoroughly investigatingthe two state-of-the-art graph-based models, yielding new insights into theirgeneralization.</description><author>Stefan Künzli, Florian Grötschla, Joël Mathys, Roger Wattenhofer</author><pubDate>Mon, 20 Nov 2023 15:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20049v3</guid></item><item><title>Kandinsky Conformal Prediction: Efficient Calibration of Image Segmentation Algorithms</title><link>http://arxiv.org/abs/2311.11837v1</link><description>Image segmentation algorithms can be understood as a collection of pixelclassifiers, for which the outcomes of nearby pixels are correlated. Classifiermodels can be calibrated using Inductive Conformal Prediction, but thisrequires holding back a sufficiently large calibration dataset for computingthe distribution of non-conformity scores of the model's predictions. If oneonly requires only marginal calibration on the image level, this calibrationset consists of all individual pixels in the images available for calibration.However, if the goal is to attain proper calibration for each individual pixelclassifier, the calibration set consists of individual images. In a scenariowhere data are scarce (such as the medical domain), it may not always bepossible to set aside sufficiently many images for this pixel-levelcalibration. The method we propose, dubbed ``Kandinsky calibration'', makes useof the spatial structure present in the distribution of natural images tosimultaneously calibrate the classifiers of ``similar'' pixels. This can beseen as an intermediate approach between marginal (imagewise) and conditional(pixelwise) calibration, where non-conformity scores are aggregated oversimilar image regions, thereby making more efficient use of the imagesavailable for calibration. We run experiments on segmentation algorithmstrained and calibrated on subsets of the public MS-COCO and Medical Decathlondatasets, demonstrating that Kandinsky calibration method can significantlyimprove the coverage. When compared to both pixelwise and imagewise calibrationon little data, the Kandinsky method achieves much lower coverage errors,indicating the data efficiency of the Kandinsky calibration.</description><author>Joren Brunekreef, Eric Marcus, Ray Sheombarsing, Jan-Jakob Sonke, Jonas Teuwen</author><pubDate>Mon, 20 Nov 2023 15:11:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11837v1</guid></item><item><title>Towards a Transportable Causal Network Model Based on Observational Healthcare Data</title><link>http://arxiv.org/abs/2311.08427v2</link><description>Over the last decades, many prognostic models based on artificialintelligence techniques have been used to provide detailed predictions inhealthcare. Unfortunately, the real-world observational data used to train andvalidate these models are almost always affected by biases that can stronglyimpact the outcomes validity: two examples are values missing not-at-random andselection bias. Addressing them is a key element in achieving transportabilityand in studying the causal relationships that are critical in clinical decisionmaking, going beyond simpler statistical approaches based on probabilisticassociation. In this context, we propose a novel approach that combines selectiondiagrams, missingness graphs, causal discovery and prior knowledge into asingle graphical model to estimate the cardiovascular risk of adolescent andyoung females who survived breast cancer. We learn this model from datacomprising two different cohorts of patients. The resulting causal networkmodel is validated by expert clinicians in terms of risk assessment, accuracyand explainability, and provides a prognostic model that outperforms competingmachine learning methods.</description><author>Alice Bernasconi, Alessio Zanga, Peter J. F. Lucas, Marco Scutari, Fabio Stella</author><pubDate>Mon, 20 Nov 2023 15:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08427v2</guid></item><item><title>System 2 Attention (is something you might need too)</title><link>http://arxiv.org/abs/2311.11829v1</link><description>Soft attention in Transformer-based Large Language Models (LLMs) issusceptible to incorporating irrelevant information from the context into itslatent representations, which adversely affects next token generations. To helprectify these issues, we introduce System 2 Attention (S2A), which leveragesthe ability of LLMs to reason in natural language and follow instructions inorder to decide what to attend to. S2A regenerates the input context to onlyinclude the relevant portions, before attending to the regenerated context toelicit the final response. In experiments, S2A outperforms standardattention-based LLMs on three tasks containing opinion or irrelevantinformation, QA, math word problems and longform generation, where S2Aincreases factuality and objectivity, and decreases sycophancy.</description><author>Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Mon, 20 Nov 2023 15:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11829v1</guid></item><item><title>Few-shot Multispectral Segmentation with Representations Generated by Reinforcement Learning</title><link>http://arxiv.org/abs/2311.11827v1</link><description>The task of multispectral image segmentation (segmentation of images withnumerous channels/bands, each capturing a specific range of wavelengths ofelectromagnetic radiation) has been previously explored in contexts with largeamounts of labeled data. However, these models tend not to generalize well todatasets of smaller size. In this paper, we propose a novel approach forimproving few-shot segmentation performance on multispectral images usingreinforcement learning to generate representations. These representations aregenerated in the form of mathematical expressions between channels and aretailored to the specific class being segmented. Our methodology involvestraining an agent to identify the most informative expressions, updating thedataset using these expressions, and then using the updated dataset to performsegmentation. Due to the limited length of the expressions, the model receivesuseful representations without any added risk of overfitting. We evaluate theeffectiveness of our approach on several multispectral datasets and demonstrateits effectiveness in boosting the performance of segmentation algorithms.</description><author>Dilith Jayakody, Thanuja Ambegoda</author><pubDate>Mon, 20 Nov 2023 15:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11827v1</guid></item><item><title>Structural Node Embeddings with Homomorphism Counts</title><link>http://arxiv.org/abs/2308.15283v2</link><description>Graph homomorphism counts, first explored by Lov\'asz in 1967, have recentlygarnered interest as a powerful tool in graph-based machine learning. Grohe(PODS 2020) proposed the theoretical foundations for using homomorphism countsin machine learning on graph level as well as node level tasks. By their verynature, these capture local structural information, which enables the creationof robust structural embeddings. While a first approach for graph level taskshas been made by Nguyen and Maehara (ICML 2020), we experimentally show theeffectiveness of homomorphism count based node embeddings. Enriched with nodelabels, node weights, and edge weights, these offer an interpretablerepresentation of graph data, allowing for enhanced explainability of machinelearning models. We propose a theoretical framework for isomorphism-invariant homomorphismcount based embeddings which lend themselves to a wide variety of downstreamtasks. Our approach capitalises on the efficient computability of graphhomomorphism counts for bounded treewidth graph classes, rendering it apractical solution for real-world applications. We demonstrate theirexpressivity through experiments on benchmark datasets. Although our results donot match the accuracy of state-of-the-art neural architectures, they arecomparable to other advanced graph learning models. Remarkably, our approachdemarcates itself by ensuring explainability for each individual feature. Byintegrating interpretable machine learning algorithms like SVMs or RandomForests, we establish a seamless, end-to-end explainable pipeline. Our studycontributes to the advancement of graph-based techniques that offer bothperformance and interpretability.</description><author>Hinrikus Wolf, Luca Oeljeklaus, Pascal Kühner, Martin Grohe</author><pubDate>Mon, 20 Nov 2023 15:03:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15283v2</guid></item><item><title>Holistic Inverse Rendering of Complex Facade via Aerial 3D Scanning</title><link>http://arxiv.org/abs/2311.11825v1</link><description>In this work, we use multi-view aerial images to reconstruct the geometry,lighting, and material of facades using neural signed distance fields (SDFs).Without the requirement of complex equipment, our method only takes simple RGBimages captured by a drone as inputs to enable physically based andphotorealistic novel-view rendering, relighting, and editing. However, areal-world facade usually has complex appearances ranging from diffuse rockswith subtle details to large-area glass windows with specular reflections,making it hard to attend to everything. As a result, previous methods canpreserve the geometry details but fail to reconstruct smooth glass windows orverse vise. In order to address this challenge, we introduce three spatial- andsemantic-adaptive optimization strategies, including a semantic regularizationapproach based on zero-shot segmentation techniques to improve materialconsistency, a frequency-aware geometry regularization to balance surfacesmoothness and details in different surfaces, and a visibility probe-basedscheme to enable efficient modeling of the local lighting in large-scaleoutdoor environments. In addition, we capture a real-world facade aerial 3Dscanning image set and corresponding point clouds for training andbenchmarking. The experiment demonstrates the superior quality of our method onfacade holistic inverse rendering, novel view synthesis, and scene editingcompared to state-of-the-art baselines.</description><author>Zixuan Xie, Rengan Xie, Rong Li, Kai Huang, Pengju Qiao, Jingsen Zhu, Xu Yin, Qi Ye, Wei Hua, Yuchi Huo, Hujun Bao</author><pubDate>Mon, 20 Nov 2023 15:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11825v1</guid></item><item><title>Graph Variational Embedding Collaborative Filtering</title><link>http://arxiv.org/abs/2311.11824v1</link><description>The customization of recommended content to users holds significantimportance in enhancing user experiences across a wide spectrum of applicationssuch as e-commerce, music, and shopping. Graph-based methods have achievedconsiderable performance by capturing user-item interactions. However, thesemethods tend to utilize randomly constructed embeddings in the dataset used fortraining the recommender, which lacks any user preferences. Here, we proposethe concept of variational embeddings as a means of pre-training therecommender system to improve the feature propagation through the layers ofgraph convolutional networks (GCNs). The graph variational embeddingcollaborative filtering (GVECF) is introduced as a novel framework toincorporate representations learned through a variational graph auto-encoderwhich are embedded into a GCN-based collaborative filtering. This approacheffectively transforms latent high-order user-item interactions into moretrainable vectors, ultimately resulting in better performance in terms ofrecall and normalized discounted cumulative gain(NDCG) metrics. The experimentsconducted on benchmark datasets demonstrate that our proposed method achievesup to 13.78% improvement in the recall over the test data.</description><author>Narges Sadat Fazeli Dehkordi, Hadi Zare, Parham Moradi, Mahdi Jalili</author><pubDate>Mon, 20 Nov 2023 15:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11824v1</guid></item><item><title>SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification</title><link>http://arxiv.org/abs/2311.07750v2</link><description>Chest X-rays are widely used to diagnose thoracic diseases, but the lack ofdetailed information about these abnormalities makes it challenging to developaccurate automated diagnosis systems, which is crucial for early detection andeffective treatment. To address this challenge, we employed deep learningtechniques to identify patterns in chest X-rays that correspond to differentdiseases. We conducted experiments on the "ChestX-ray14" dataset using variouspre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classicalmodels. The best individual model was the CoAtNet, which achieved an area underthe receiver operating characteristic curve (AUROC) of 84.2%. By combining thepredictions of all trained models using a weighted average ensemble where theweight of each model was determined using differential evolution, we furtherimproved the AUROC to 85.4%, outperforming other state-of-the-art methods inthis field. Our findings demonstrate the potential of deep learning techniques,particularly ensemble deep learning, for improving the accuracy of automaticdiagnosis of thoracic diseases from chest X-rays.</description><author>S. M. Nabil Ashraf, Md. Adyelullahil Mamun, Hasnat Md. Abdullah, Md. Golam Rabiul Alam</author><pubDate>Mon, 20 Nov 2023 15:01:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07750v2</guid></item><item><title>Zero redundancy distributed learning with differential privacy</title><link>http://arxiv.org/abs/2311.11822v1</link><description>Deep learning using large models have achieved great success in a wide rangeof domains. However, training these models on billions of parameters is verychallenging in terms of the training speed, memory cost, and communicationefficiency, especially under the privacy-preserving regime with differentialprivacy (DP). On the one hand, DP optimization has comparable efficiency to thestandard non-private optimization on a single GPU, but on multiple GPUs,existing DP distributed learning (such as pipeline parallel) has suffered fromsignificantly worse efficiency. On the other hand, the Zero RedundancyOptimizer (ZeRO) is a state-of-the-art solution to the standard distributedlearning, exhibiting excellent training efficiency on large models, but to workcompatibly with DP is technically complicated. In this work, we develop a newsystematic solution, DP-ZeRO, (I) to scale up the trainable DP model size, e.g.to GPT-100B, (II) to obtain the same computation and communication efficiencyas the standard ZeRO, and (III) to enable mixed-precision DP training. OurDP-ZeRO, like the standard ZeRO, has the potential to train models witharbitrary size and is evaluated on the world's largest DP models in terms ofthe number of trainable parameters.</description><author>Zhiqi Bu, Justin Chiu, Ruixuan Liu, Sheng Zha, George Karypis</author><pubDate>Mon, 20 Nov 2023 14:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11822v1</guid></item><item><title>Cross-View Graph Consistency Learning for Invariant Graph Representations</title><link>http://arxiv.org/abs/2311.11821v1</link><description>Graph representation learning is fundamental for analyzing graph-structureddata. Exploring invariant graph representations remains a challenge for mostexisting graph representation learning methods. In this paper, we propose across-view graph consistency learning (CGCL) method that learns invariant graphrepresentations for link prediction. First, two complementary augmented viewsare derived from an incomplete graph structure through a bidirectional graphstructure augmentation scheme. This augmentation scheme mitigates the potentialinformation loss that is commonly associated with various data augmentationtechniques involving raw graph data, such as edge perturbation, node removal,and attribute masking. Second, we propose a CGCL model that can learn invariantgraph representations. A cross-view training scheme is proposed to train theproposed CGCL model. This scheme attempts to maximize the consistencyinformation between one augmented view and the graph structure reconstructedfrom the other augmented view. Furthermore, we offer a comprehensivetheoretical CGCL analysis. This paper empirically and experimentallydemonstrates the effectiveness of the proposed CGCL method, achievingcompetitive results on graph datasets in comparisons with severalstate-of-the-art algorithms.</description><author>Jie Chen, Zhiming Li, Hua Mao, Wai Lok Woo, Xi Peng</author><pubDate>Mon, 20 Nov 2023 14:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11821v1</guid></item><item><title>Generalized super-resolution 4D Flow MRI -- using ensemble learning to extend across the cardiovascular system</title><link>http://arxiv.org/abs/2311.11819v1</link><description>4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasivemeasurement technique capable of quantifying blood flow across thecardiovascular system. While practical use is limited by spatial resolution andimage noise, incorporation of trained super-resolution (SR) networks haspotential to enhance image quality post-scan. However, these efforts havepredominantly been restricted to narrowly defined cardiovascular domains, withlimited exploration of how SR performance extends across the cardiovascularsystem; a task aggravated by contrasting hemodynamic conditions apparent acrossthe cardiovasculature. The aim of our study was to explore the generalizabilityof SR 4D Flow MRI using a combination of heterogeneous training sets anddedicated ensemble learning. With synthetic training data generated acrossthree disparate domains (cardiac, aortic, cerebrovascular), varyingconvolutional base and ensemble learners were evaluated as a function of domainand architecture, quantifying performance on both in-silico and acquiredin-vivo data from the same three domains. Results show that both bagging andstacking ensembling enhance SR performance across domains, accuratelypredicting high-resolution velocities from low-resolution input data in-silico.Likewise, optimized networks successfully recover native resolution velocitiesfrom downsampled in-vivo data, as well as show qualitative potential ingenerating denoised SR-images from clinical level input data. In conclusion,our work presents a viable approach for generalized SR 4D Flow MRI, withensemble learning extending utility across various clinical areas of interest.</description><author>Leon Ericsson, Adam Hjalmarsson, Muhammad Usman Akbar, Edward Ferdian, Mia Bonini, Brandon Hardy, Jonas Schollenberger, Maria Aristova, Patrick Winter, Nicholas Burris, Alexander Fyrdahl, Andreas Sigfridsson, Susanne Schnell, C. Alberto Figueroa, David Nordsletten, Alistair A. Young, David Marlevi</author><pubDate>Mon, 20 Nov 2023 14:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11819v1</guid></item><item><title>Lag-Llama: Towards Foundation Models for Time Series Forecasting</title><link>http://arxiv.org/abs/2310.08278v2</link><description>Aiming to build foundation models for time-series forecasting and study theirscaling behavior, we present here our work-in-progress on Lag-Llama, ageneral-purpose univariate probabilistic time-series forecasting model trainedon a large collection of time-series data. The model shows good zero-shotprediction capabilities on unseen "out-of-distribution" time-series datasets,outperforming supervised baselines. We use smoothly broken power-laws to fitand predict model scaling behavior. The open source code is made available athttps://github.com/kashif/pytorch-transformer-ts.</description><author>Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Biloš, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, Irina Rish</author><pubDate>Mon, 20 Nov 2023 14:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08278v2</guid></item><item><title>CrackCLF: Automatic Pavement Crack Detection based on Closed-Loop Feedback</title><link>http://arxiv.org/abs/2311.11815v1</link><description>Automatic pavement crack detection is an important task to ensure thefunctional performances of pavements during their service life. Inspired bydeep learning (DL), the encoder-decoder framework is a powerful tool for crackdetection. However, these models are usually open-loop (OL) systems that tendto treat thin cracks as the background. Meanwhile, these models can notautomatically correct errors in the prediction, nor can it adapt to the changesof the environment to automatically extract and detect thin cracks. To tacklethis problem, we embed closed-loop feedback (CLF) into the neural network sothat the model could learn to correct errors on its own, based on generativeadversarial networks (GAN). The resulting model is called CrackCLF and includesthe front and back ends, i.e. segmentation and adversarial network. The frontend with U-shape framework is employed to generate crack maps, and the back endwith a multi-scale loss function is used to correct higher-orderinconsistencies between labels and crack maps (generated by the front end) toaddress open-loop system issues. Empirical results show that the proposedCrackCLF outperforms others methods on three public datasets. Moreover, theproposed CLF can be defined as a plug and play module, which can be embeddedinto different neural network models to improve their performances.</description><author>Chong Li, Zhun Fan, Ying Chen, Huibiao Lin, Laura Moretti, Giuseppe Loprencipe, Weihua Sheng, Kelvin C. P. Wang</author><pubDate>Mon, 20 Nov 2023 14:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11815v1</guid></item><item><title>MEAL: Stable and Active Learning for Few-Shot Prompting</title><link>http://arxiv.org/abs/2211.08358v3</link><description>Few-shot classification has made great strides due to foundation models that,through priming and prompting, are highly effective few-shot learners. However,this approach has high variance both across different sets of few shots (dataselection) and across different finetuning runs (run variability). This isproblematic not only because it impedes the fair comparison of differentapproaches, but especially because it makes few-shot learning too unreliablefor many real-world applications. To alleviate these issues, we make twocontributions for more stable and effective few-shot learning: First, wepropose novel ensembling methods and show that they substantially reduce runvariability. Second, we introduce a new active learning (AL) criterion for dataselection and present the first AL-based approach specifically tailored towardsprompt-based learning. In our experiments, we show that our combined method,MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),improves overall performance of prompt-based finetuning by 2.3 points on fivediverse tasks. We publicly share our code and data splits inhttps://github.com/akoksal/MEAL.</description><author>Abdullatif Köksal, Timo Schick, Hinrich Schütze</author><pubDate>Mon, 20 Nov 2023 14:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08358v3</guid></item><item><title>Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule</title><link>http://arxiv.org/abs/2311.11813v1</link><description>Progress in neural grammatical error correction (GEC) is hindered by the lackof annotated training data. Sufficient amounts of high-quality manuallyannotated data are not available, so recent research has relied on generatingsynthetic data, pretraining on it, and then fine-tuning on real datasets;performance gains have been achieved either by ensembling or by using hugepretrained models such as XXL-T5 as the backbone. In this work, we explore anorthogonal direction: how to use available data more efficiently. First, wepropose auxiliary tasks that exploit the alignment between the original andcorrected sentences, such as predicting a sequence of corrections. We formulateeach task as a sequence-to-sequence problem and perform multi-task training.Second, we discover that the order of datasets used for training and evenindividual instances within a dataset may have important effects on the finalperformance, so we set out to find the best training schedule. Together, thesetwo ideas lead to significant improvements, producing results that improvestate of the art with much smaller models; in particular, we outperform thebest models based on T5-XXL (11B parameters) with a BART-based model (400Mparameters).</description><author>Andrey Bout, Alexander Podolskiy, Sergey Nikolenko, Irina Piontkovskaya</author><pubDate>Mon, 20 Nov 2023 14:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11813v1</guid></item><item><title>Improving Real Estate Appraisal with POI Integration and Areal Embedding</title><link>http://arxiv.org/abs/2311.11812v1</link><description>Despite advancements in real estate appraisal methods, this study primarilyfocuses on two pivotal challenges. Firstly, we explore the often-underestimatedimpact of Points of Interest (POI) on property values, emphasizing thenecessity for a comprehensive, data-driven approach to feature selection.Secondly, we integrate road-network-based Areal Embedding to enhance spatialunderstanding for real estate appraisal. We first propose a revised method forPOI feature extraction, and discuss the impact of each POI for house priceappraisal. Then we present the Areal embedding-enabled Masked MultiheadAttention-based Spatial Interpolation for House Price Prediction (AMMASI)model, an improvement upon the existing ASI model, which leverages maskedmulti-head attention on geographic neighbor houses and similar-featured houses.Our model outperforms current baselines and also offers promising avenues forfuture optimization in real estate appraisal methodologies.</description><author>Sumin Han, Youngjun Park, Sonia Sabir, Jisun An, Dongman Lee</author><pubDate>Mon, 20 Nov 2023 14:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11812v1</guid></item><item><title>Large Language Models and Explainable Law: a Hybrid Methodology</title><link>http://arxiv.org/abs/2311.11811v1</link><description>The paper advocates for LLMs to enhance the accessibility, usage andexplainability of rule-based legal systems, contributing to a democratic andstakeholder-oriented view of legal technology. A methodology is developed toexplore the potential use of LLMs for translating the explanations produced byrule-based systems, from high-level programming languages to natural language,allowing all users a fast, clear, and accessible interaction with suchtechnologies. The study continues by building upon these explanations toempower laypeople with the ability to execute complex juridical tasks on theirown, using a Chain of Prompts for the autonomous legal comparison of differentrule-based inferences, applied to the same factual case.</description><author>Marco Billi, Alessandro Parenti, Giuseppe Pisano, Marco Sanchi</author><pubDate>Mon, 20 Nov 2023 14:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11811v1</guid></item><item><title>DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding</title><link>http://arxiv.org/abs/2311.11810v1</link><description>This work presents DocPedia, a novel large multimodal model (LMM) forversatile OCR-free document understanding, capable of parsing images up to2,560$\times$2,560 resolution. Unlike existing work either struggle withhigh-resolution documents or give up the large language model thus vision orlanguage ability constrained, our DocPedia directly processes visual input inthe frequency domain rather than the pixel space. The unique characteristicenables DocPedia to capture a greater amount of visual and textual informationusing a limited number of visual tokens. To consistently enhance bothperception and comprehension abilities of our model, we develop a dual-stagetraining strategy and enrich instructions/annotations of all training taskscovering multiple document types. Extensive quantitative and qualitativeexperiments conducted on various publicly available benchmarks confirm themutual benefits of jointly learning perception and comprehension tasks. Theresults provide further evidence of the effectiveness and superior performanceof our DocPedia over other methods.</description><author>Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, Can Huang</author><pubDate>Mon, 20 Nov 2023 14:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11810v1</guid></item><item><title>LogLead -- Fast and Integrated Log Loader, Enhancer, and Anomaly Detector</title><link>http://arxiv.org/abs/2311.11809v1</link><description>This paper introduces LogLead, a tool designed for efficient log analysis.LogLead combines three essential steps in log processing: loading, enhancing,and anomaly detection. The tool leverages Polars, a high-speed DataFramelibrary. We currently have 7 Loaders out of which 4 is for public data sets(HDFS, Hadoop, BGL, and Thunderbird). We have multiple enhancers with threeparsers (Drain, Spell, LenMa), Bert embedding creation and other logrepresentation techniques like bag-of-words. LogLead integrates to 5 supervisedand 4 unsupervised machine learning algorithms for anomaly detection fromSKLearn. By integrating diverse datasets, log representation methods andanomaly detectors, LogLead facilitates comprehensive benchmarking in loganalysis research. We demonstrate that log loading from raw file to dataframeis over 10x faster with LogLead is compared to past solutions. We demonstrateroughly 2x improvement in Drain parsing speed by off-loading log messagenormalization to LogLead. We demonstrate a brief benchmarking on HDFSsuggesting that log representations beyond bag-of-words provide limitedbenefits. Screencast demonstrating the tool: https://youtu.be/8stdbtTfJVo</description><author>Mika Mäntylä, Yuqing Wang, Jesse Nyyssölä</author><pubDate>Mon, 20 Nov 2023 14:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11809v1</guid></item><item><title>Robot Hand-Eye Calibration using Structure-from-Motion</title><link>http://arxiv.org/abs/2311.11808v1</link><description>In this paper we propose a new flexible method for hand-eye calibration. Thevast majority of existing hand-eye calibration techniques requires acalibration rig which is used in conjunction with camera pose estimationmethods. Instead, we combine structure-from-motion with known robot motions andwe show that the solution can be obtained in linear form. The latter solves forboth the hand-eye parameters and for the unknown scale factor inherent withstructure-from-motion methods. The algebraic analysis that is made possiblewith such a linear formulation allows to investigate not only the well knowncase of general screw motions but also such singular motions as puretranslations, pure rotations, and planar motions. In essence, the robot-mountedcamera looks to an unknown rigid layout, tracks points over an image sequenceand estimates the camera-to-robot relationship. Such a self calibration processis relevant for unmanned vehicles, robots working in remote places, and soforth. We conduct a large number of experiments which validate the quality ofthe method by comparing it with existing ones.</description><author>Nicolas Andreff, Bernard Espiau, Radu Horaud</author><pubDate>Mon, 20 Nov 2023 14:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11808v1</guid></item><item><title>Age-Friendly Route Planner: Calculating Comfortable Routes for Senior Citizens</title><link>http://arxiv.org/abs/2311.11802v1</link><description>The application of routing algorithms to real-world situations is a widelystudied research topic. Despite this, routing algorithms and applications areusually developed for a general purpose, meaning that certain groups, such asageing people, are often marginalized due to the broad approach of the designedalgorithms. This situation may pose a problem in cities which are suffering aslow but progressive ageing of their populations. With this motivation in mind,this paper focuses on describing our implemented Age-Friendly Route Planner,whose goal is to improve the experience in the city for senior citizens. Inorder to measure the age-friendliness of a route, several variables have beendeemed, such as the number of amenities along the route, the amount ofcomfortable elements found, or the avoidance of sloppy sections. In this paper,we describe one of the main features of the Age-Friendly Route Planner: thepreference-based routes, and we also demonstrate how it can contribute to thecreation of adapted friendly routes.</description><author>Andoni Aranguren, Eneko Osaba, Silvia Urra-Uriarte, Patricia Molina-Costa</author><pubDate>Mon, 20 Nov 2023 14:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11802v1</guid></item><item><title>Language-Agnostic Bias Detection in Language Models with Bias Probing</title><link>http://arxiv.org/abs/2305.13302v2</link><description>Pretrained language models (PLMs) are key components in NLP, but they containstrong social biases. Quantifying these biases is challenging because currentmethods focusing on fill-the-mask objectives are sensitive to slight changes ininput. To address this, we propose a bias probing technique called LABDet, forevaluating social bias in PLMs with a robust and language-agnostic method. Fornationality as a case study, we show that LABDet `surfaces' nationality bias bytraining a classifier on top of a frozen PLM on non-nationality sentimentdetection. We find consistent patterns of nationality bias across monolingualPLMs in six languages that align with historical and political context. We alsoshow for English BERT that bias surfaced by LABDet correlates well with bias inthe pretraining data; thus, our work is one of the few studies that directlylinks pretraining data to PLM behavior. Finally, we verify LABDet's reliabilityand applicability to different templates and languages through an extensive setof robustness checks. We publicly share our code and dataset inhttps://github.com/akoksal/LABDet.</description><author>Abdullatif Köksal, Omer Faruk Yalcin, Ahmet Akbiyik, M. Tahir Kilavuz, Anna Korhonen, Hinrich Schütze</author><pubDate>Mon, 20 Nov 2023 14:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13302v2</guid></item><item><title>Operator Learning for Continuous Spatial-Temporal Model with A Hybrid Optimization Scheme</title><link>http://arxiv.org/abs/2311.11798v1</link><description>Partial differential equations are often used in the spatial-temporalmodeling of complex dynamical systems in many engineering applications. In thiswork, we build on the recent progress of operator learning and present adata-driven modeling framework that is continuous in both space and time. A keyfeature of the proposed model is the resolution-invariance with respect to bothspatial and temporal discretizations. To improve the long-term performance ofthe calibrated model, we further propose a hybrid optimization scheme thatleverages both gradient-based and derivative-free optimization methods andefficiently trains on both short-term time series and long-term statistics. Weinvestigate the performance of the spatial-temporal continuous learningframework with three numerical examples, including the viscous Burgers'equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation.The results confirm the resolution-invariance of the proposed modelingframework and also demonstrate stable long-term simulations with onlyshort-term time series data. In addition, we show that the proposed model canbetter predict long-term statistics via the hybrid optimization scheme with acombined use of short-term and long-term data.</description><author>Chuanqi Chen, Jin-Long Wu</author><pubDate>Mon, 20 Nov 2023 14:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11798v1</guid></item></channel></rss>