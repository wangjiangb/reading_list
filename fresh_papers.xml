<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 27 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The Third Monocular Depth Estimation Challenge</title><link>http://arxiv.org/abs/2404.16831v1</link><description>This paper discusses the results of the third edition of the Monocular DepthEstimation Challenge (MDEC). The challenge focuses on zero-shot generalizationto the challenging SYNS-Patches dataset, featuring complex scenes in naturaland indoor settings. As with the previous edition, methods can use any form ofsupervision, i.e. supervised or self-supervised. The challenge received a totalof 19 submissions outperforming the baseline on the test set: 10 among themsubmitted a report describing their approach, highlighting a diffused use offoundational models such as Depth Anything at the core of their method. Thechallenge winners drastically improved 3D F-Score performance, from 17.51% to23.72%.</description><author>Jaime Spencer, Fabio Tosi, Matteo Poggi, Ripudaman Singh Arora, Chris Russell, Simon Hadfield, Richard Bowden, GuangYuan Zhou, ZhengXin Li, Qiang Rao, YiPing Bao, Xiao Liu, Dohyeong Kim, Jinseong Kim, Myunghyun Kim, Mykola Lavreniuk, Rui Li, Qing Mao, Jiang Wu, Yu Zhu, Jinqiu Sun, Yanning Zhang, Suraj Patni, Aradhye Agarwal, Chetan Arora, Pihai Sun, Kui Jiang, Gang Wu, Jian Liu, Xianming Liu, Junjun Jiang, Xidan Zhang, Jianing Wei, Fangjun Wang, Zhiming Tan, Jiabao Wang, Albert Luginov, Muhammad Shahzad, Seyed Hosseini, Aleksander Trajcevski, James H. Elder</author><pubDate>Thu, 25 Apr 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16831v1</guid></item><item><title>Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials</title><link>http://arxiv.org/abs/2404.16829v1</link><description>Physically realistic materials are pivotal in augmenting the realism of 3Dassets across various applications and lighting conditions. However, existing3D assets and generative models often lack authentic material properties.Manual assignment of materials using graphic software is a tedious andtime-consuming task. In this paper, we exploit advancements in Multimodal LargeLanguage Models (MLLMs), particularly GPT-4V, to present a novel approach,Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize anddescribe materials, allowing the construction of a detailed material library.2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4Vprecisely identifies and aligns materials with the corresponding components of3D objects. 3) The correctly matched materials are then meticulously applied asreference for the new SVBRDF material generation according to the originaldiffuse map, significantly enhancing their visual authenticity. Make-it-Realoffers a streamlined integration into the 3D content creation workflow,showcasing its utility as an essential tool for developers of 3D assets.</description><author>Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, Dahua Lin</author><pubDate>Thu, 25 Apr 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16829v1</guid></item><item><title>Made to Order: Discovering monotonic temporal changes via self-supervised video ordering</title><link>http://arxiv.org/abs/2404.16828v1</link><description>Our objective is to discover and localize monotonic temporal changes in asequence of images. To achieve this, we exploit a simple proxy task of orderinga shuffled image sequence, with `time' serving as a supervisory signal sinceonly changes that are monotonic with time can give rise to the correctordering. We also introduce a flexible transformer-based model forgeneral-purpose ordering of image sequences of arbitrary length with built-inattribution maps. After training, the model successfully discovers andlocalizes monotonic changes while ignoring cyclic and stochastic ones. Wedemonstrate applications of the model in multiple video settings coveringdifferent scene and object types, discovering both object-level andenvironmental changes in unseen sequences. We also demonstrate that theattention-based attribution maps function as effective prompts for segmentingthe changing regions, and that the learned representations can be used fordownstream applications. Finally, we show that the model achieves the state ofthe art on standard benchmarks for ordering a set of images.</description><author>Charig Yang, Weidi Xie, Andrew Zisserman</author><pubDate>Thu, 25 Apr 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16828v1</guid></item><item><title>ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images</title><link>http://arxiv.org/abs/2404.16825v1</link><description>With the advent of virtual reality technology, omnidirectional image (ODI)rescaling techniques are increasingly embraced for reducing transmitted andstored file sizes while preserving high image quality. Despite this progress,current ODI rescaling methods predominantly focus on enhancing the quality ofimages in equirectangular projection (ERP) format, which overlooks the factthat the content viewed on head mounted displays (HMDs) is actually a renderedviewport instead of an ERP image. In this work, we emphasize that focusingsolely on ERP quality results in inferior viewport visual experiences forusers. Thus, we propose ResVR, which is the first comprehensive framework forthe joint Rescaling and Viewport Rendering of ODIs. ResVR allows obtaining LRERP images for transmission while rendering high-quality viewports for users towatch on HMDs. In our ResVR, a novel discrete pixel sampling strategy isdeveloped to tackle the complex mapping between the viewport and ERP, enablingend-to-end training of ResVR pipeline. Furthermore, a spherical pixel shaperepresentation technique is innovatively derived from spherical differentiationto significantly improve the visual quality of rendered viewports. Extensiveexperiments demonstrate that our ResVR outperforms existing methods in viewportrendering tasks across different fields of view, resolutions, and viewdirections while keeping a low transmission overhead.</description><author>Weiqi Li, Shijie Zhao, Bin Chen, Xinhua Cheng, Junlin Li, Li Zhang, Jian Zhang</author><pubDate>Thu, 25 Apr 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16825v1</guid></item><item><title>V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection</title><link>http://arxiv.org/abs/2404.16824v1</link><description>AI-generated video has revolutionized short video production, filmmaking, andpersonalized media, making video local editing an essential tool. However, thisprogress also blurs the line between reality and fiction, posing challenges inmultimedia forensics. To solve this urgent issue, V2A-Mark is proposed toaddress the limitations of current video tampering forensics, such as poorgeneralizability, singular function, and single modality focus. Combining thefragility of video-into-video steganography with deep robust watermarking, ourmethod can embed invisible visual-audio localization watermarks and copyrightwatermarks into the original video frames and audio, enabling precisemanipulation localization and copyright protection. We also design a temporalalignment and fusion module and degradation prompt learning to enhance thelocalization accuracy and decoding robustness. Meanwhile, we introduce asample-level audio localization method and a cross-modal copyright extractionmechanism to couple the information of audio and video frames. Theeffectiveness of V2A-Mark has been verified on a visual-audio tamperingdataset, emphasizing its superiority in localization precision and copyrightaccuracy, crucial for the sustainable development of video editing in the AIGCvideo era.</description><author>Xuanyu Zhang, Youmin Xu, Runyi Li, Jiwen Yu, Weiqi Li, Zhipei Xu, Jian Zhang</author><pubDate>Thu, 25 Apr 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16824v1</guid></item><item><title>Learning Visuotactile Skills with Two Multifingered Hands</title><link>http://arxiv.org/abs/2404.16823v1</link><description>Aiming to replicate human-like dexterity, perceptual experiences, and motionpatterns, we explore learning from human demonstrations using a bimanual systemwith multifingered hands and visuotactile data. Two significant challengesexist: the lack of an affordable and accessible teleoperation system suitablefor a dual-arm setup with multifingered hands, and the scarcity ofmultifingered hand hardware equipped with touch sensing. To tackle the firstchallenge, we develop HATO, a low-cost hands-arms teleoperation system thatleverages off-the-shelf electronics, complemented with a software suite thatenables efficient data collection; the comprehensive software suite alsosupports multimodal data processing, scalable policy learning, and smoothpolicy deployment. To tackle the latter challenge, we introduce a novelhardware adaptation by repurposing two prosthetic hands equipped with touchsensors for research. Using visuotactile data collected from our system, welearn skills to complete long-horizon, high-precision tasks which are difficultto achieve without multifingered dexterity and touch feedback. Furthermore, weempirically investigate the effects of dataset size, sensing modality, andvisual input preprocessing on policy learning. Our results mark a promisingstep forward in bimanual multifingered manipulation from visuotactile data.Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .</description><author>Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent Yi, Sergey Levine, Jitendra Malik</author><pubDate>Thu, 25 Apr 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16823v1</guid></item><item><title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</title><link>http://arxiv.org/abs/2404.16821v1</link><description>In this report, we introduce InternVL 1.5, an open-source multimodal largelanguage model (MLLM) to bridge the capability gap between open-source andproprietary commercial models in multimodal understanding. We introduce threesimple improvements: (1) Strong Vision Encoder: we explored a continuouslearning strategy for the large-scale vision foundation model -- InternViT-6B,boosting its visual understanding capabilities, and making it can betransferred and reused in different LLMs. (2) Dynamic High-Resolution: wedivide images into tiles ranging from 1 to 40 of 448$\times$448 pixelsaccording to the aspect ratio and resolution of the input images, whichsupports up to 4K resolution input. (3) High-Quality Bilingual Dataset: wecarefully collected a high-quality bilingual dataset that covers common scenes,document images, and annotated them with English and Chinese question-answerpairs, significantly enhancing performance in OCR- and Chinese-related tasks.We evaluate InternVL 1.5 through a series of benchmarks and comparativestudies. Compared to both open-source and proprietary models, InternVL 1.5shows competitive performance, achieving state-of-the-art results in 8 of 18benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.</description><author>Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao</author><pubDate>Thu, 25 Apr 2024 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16821v1</guid></item><item><title>Bagging Provides Assumption-free Stability</title><link>http://arxiv.org/abs/2301.12600v3</link><description>Bagging is an important technique for stabilizing machine learning models. Inthis paper, we derive a finite-sample guarantee on the stability of bagging forany model. Our result places no assumptions on the distribution of the data, onthe properties of the base algorithm, or on the dimensionality of thecovariates. Our guarantee applies to many variants of bagging and is optimal upto a constant. Empirical results validate our findings, showing that baggingsuccessfully stabilizes even highly unstable base algorithms.</description><author>Jake A. Soloff, Rina Foygel Barber, Rebecca Willett</author><pubDate>Thu, 25 Apr 2024 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12600v3</guid></item><item><title>Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and Human Ratings</title><link>http://arxiv.org/abs/2404.16820v1</link><description>While text-to-image (T2I) generative models have become ubiquitous, they donot necessarily generate images that align with a given prompt. While previouswork has evaluated T2I alignment by proposing metrics, benchmarks, andtemplates for collecting human judgements, the quality of these components isnot systematically measured. Human-rated prompt sets are generally small andthe reliability of the ratings -- and thereby the prompt set used to comparemodels -- is not evaluated. We address this gap by performing an extensivestudy evaluating auto-eval metrics and human templates. We provide three maincontributions: (1) We introduce a comprehensive skills-based benchmark that candiscriminate models across different human templates. This skills-basedbenchmark categorises prompts into sub-skills, allowing a practitioner topinpoint not only which skills are challenging, but at what level of complexitya skill becomes challenging. (2) We gather human ratings across four templatesand four T2I models for a total of &gt;100K annotations. This allows us tounderstand where differences arise due to inherent ambiguity in the prompt andwhere they arise due to differences in metric and model quality. (3) Finally,we introduce a new QA-based auto-eval metric that is better correlated withhuman ratings than existing metrics for our new dataset, across different humantemplates, and on TIFA160.</description><author>Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajić, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset, Aida Nematzadeh</author><pubDate>Thu, 25 Apr 2024 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16820v1</guid></item><item><title>Boosting Unsupervised Semantic Segmentation with Principal Mask Proposals</title><link>http://arxiv.org/abs/2404.16818v1</link><description>Unsupervised semantic segmentation aims to automatically partition imagesinto semantically meaningful regions by identifying global categories within animage corpus without any form of annotation. Building upon recent advances inself-supervised representation learning, we focus on how to leverage theselarge pre-trained models for the downstream task of unsupervised segmentation.We present PriMaPs - Principal Mask Proposals - decomposing images intosemantically meaningful masks based on their feature representation. Thisallows us to realize unsupervised semantic segmentation by fitting classprototypes to PriMaPs with a stochastic expectation-maximization algorithm,PriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to competitiveresults across various pre-trained backbone models, including DINO and DINOv2,and across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3.Importantly, PriMaPs-EM is able to boost results when applied orthogonally tocurrent state-of-the-art unsupervised semantic segmentation pipelines.</description><author>Oliver Hahn, Nikita Araslanov, Simone Schaub-Meyer, Stefan Roth</author><pubDate>Thu, 25 Apr 2024 18:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16818v1</guid></item><item><title>IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages</title><link>http://arxiv.org/abs/2404.16816v1</link><description>As large language models (LLMs) see increasing adoption across the globe, itis imperative for LLMs to be representative of the linguistic diversity of theworld. India is a linguistically diverse country of 1.4 Billion people. Tofacilitate research on multilingual LLM evaluation, we release IndicGenBench -the largest benchmark for evaluating LLMs on user-facing generation tasksacross a diverse set 29 of Indic languages covering 13 scripts and 4 languagefamilies. IndicGenBench is composed of diverse generation tasks likecross-lingual summarization, machine translation, and cross-lingual questionanswering. IndicGenBench extends existing benchmarks to many Indic languagesthrough human curation providing multi-way parallel evaluation data for manyunder-represented Indic languages for the first time. We evaluate a wide rangeof proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5,Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largestPaLM-2 models performs the best on most tasks, however, there is a significantperformance gap in all languages compared to English showing that furtherresearch is needed for the development of more inclusive multilingual languagemodels. IndicGenBench is released atwww.github.com/google-research-datasets/indic-gen-bench</description><author>Harman Singh, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, Partha Talukdar</author><pubDate>Thu, 25 Apr 2024 18:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16816v1</guid></item><item><title>Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer Learning for Skin Disease Classification in Long-Tail Distribution</title><link>http://arxiv.org/abs/2404.16814v1</link><description>Addressing the challenges of rare diseases is difficult, especially with thelimited number of reference images and a small patient population. This is moreevident in rare skin diseases, where we encounter long-tailed datadistributions that make it difficult to develop unbiased and broadly effectivemodels. The diverse ways in which image datasets are gathered and theirdistinct purposes also add to these challenges. Our study conducts a detailedexamination of the benefits and drawbacks of episodic and conventional trainingmethodologies, adopting a few-shot learning approach alongside transferlearning. We evaluated our models using the ISIC2018, Derm7pt, and SD-198datasets. With minimal labeled examples, our models showed substantialinformation gains and better performance compared to previously trained models.Our research emphasizes the improved ability to represent features inDenseNet121 and MobileNetV2 models, achieved by using pre-trained models onImageNet to increase similarities within classes. Moreover, our experiments,ranging from 2-way to 5-way classifications with up to 10 examples, showed agrowing success rate for traditional transfer learning methods as the number ofexamples increased. The addition of data augmentation techniques significantlyimproved our transfer learning based model performance, leading to higherperformances than existing methods, especially in the SD-198 and ISIC2018datasets. All source code related to this work will be made publicly availablesoon at the provided URL.</description><author>Zeynep Özdemir, Hacer Yalim Keles, Ömer Özgür Tanrıöver</author><pubDate>Thu, 25 Apr 2024 18:56:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16814v1</guid></item><item><title>Make Your LLM Fully Utilize the Context</title><link>http://arxiv.org/abs/2404.16811v1</link><description>While many contemporary large language models (LLMs) can process lengthyinput, they still struggle to fully utilize information within the longcontext, known as the lost-in-the-middle challenge. We hypothesize that itstems from insufficient explicit supervision during the long-context training,which fails to emphasize that any position in a long context can hold crucialinformation. Based on this intuition, our study presents information-intensive(IN2) training, a purely data-driven solution to overcome lost-in-the-middle.Specifically, IN2 training leverages a synthesized long-context question-answerdataset, where the answer requires (1) fine-grained information awareness on ashort segment (~128 tokens) within a synthesized long context (4K-32K tokens),and (2) the integration and reasoning of information from two or more shortsegments. Through applying this information-intensive training on Mistral-7B,we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability ofFILM-7B for utilizing long contexts, we design three probing tasks thatencompass various context styles (document, code, and structured-data context)and information retrieval patterns (forward, backward, and bi-directionalretrieval). The probing results demonstrate that FILM-7B can robustly retrieveinformation from different positions in its 32K context window. Beyond theseprobing tasks, FILM-7B significantly improves the performance on real-worldlong-context tasks (e.g., 23.5-&gt;26.9 F1 score on NarrativeQA), whilemaintaining a comparable performance on short-context tasks (e.g., 59.3-&gt;59.2accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.</description><author>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou</author><pubDate>Thu, 25 Apr 2024 18:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16811v1</guid></item><item><title>Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning</title><link>http://arxiv.org/abs/2404.16807v1</link><description>Generative Commonsense Reasoning (GCR) requires a model to reason about asituation using commonsense knowledge, while generating coherent sentences.Although the quality of the generated sentences is crucial, the diversity ofthe generation is equally important because it reflects the model's ability touse a range of commonsense knowledge facts. Large Language Models (LLMs) haveshown proficiency in enhancing the generation quality across various tasksthrough in-context learning (ICL) using given examples without the need for anyfine-tuning. However, the diversity aspect in LLM outputs has not beensystematically studied before. To address this, we propose a simple method thatdiversifies the LLM generations, while preserving their quality. Experimentalresults on three benchmark GCR datasets show that our method achieves an idealbalance between the quality and diversity. Moreover, the sentences generated byour proposed method can be used as training data to improve diversity inexisting commonsense generators.</description><author>Tianhui Zhang, Bei Peng, Danushka Bollegala</author><pubDate>Thu, 25 Apr 2024 18:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16807v1</guid></item><item><title>AAPL: Adding Attributes to Prompt Learning for Vision-Language Models</title><link>http://arxiv.org/abs/2404.16804v1</link><description>Recent advances in large pre-trained vision-language models have demonstratedremarkable performance on zero-shot downstream tasks. Building upon this,recent studies, such as CoOp and CoCoOp, have proposed the use of promptlearning, where context within a prompt is replaced with learnable vectors,leading to significant improvements over manually crafted prompts. However, theperformance improvement for unseen classes is still marginal, and to tacklethis problem, data augmentation has been frequently used in traditionalzero-shot learning techniques. Through our experiments, we have identifiedimportant issues in CoOp and CoCoOp: the context learned through traditionalimage augmentation is biased toward seen classes, negatively impactinggeneralization to unseen classes. To address this problem, we proposeadversarial token embedding to disentangle low-level visual augmentationfeatures from high-level class information when inducing bias in learnableprompts. Through our novel mechanism called "Adding Attributes to PromptLearning", AAPL, we guide the learnable context to effectively extract textfeatures by focusing on high-level features for unseen classes. We haveconducted experiments across 11 datasets, and overall, AAPL shows favorableperformances compared to the existing methods in few-shot learning, zero-shotlearning, cross-dataset, and domain generalization tasks.</description><author>Gahyeon Kim, Sohee Kim, Seokju Lee</author><pubDate>Thu, 25 Apr 2024 18:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16804v1</guid></item><item><title>GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing</title><link>http://arxiv.org/abs/2403.08733v3</link><description>We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructedby the 3D Gaussian Splatting (3DGS). Our method first renders a collection of images by using the 3DGS and editsthem by using a pre-trained 2D diffusion model (ControlNet) based on the inputprompt, which is then used to optimise the 3D model. Our key contribution is multi-view consistent editing, which enables editingall images together instead of iteratively editing one image while updating the3D model as in previous works. It leads to faster editing as well as higher visual quality. This is achieved by the two terms: (a) depth-conditioned editing that enforces geometric consistency acrossmulti-view images by leveraging naturally consistent depth maps. (b) attention-based latent code alignment that unifies the appearance ofedited images by conditioning their editing to several reference views throughself and cross-view attention between images' latent representations. Experiments demonstrate that our method achieves faster editing and bettervisual results than previous state-of-the-art methods.</description><author>Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu</author><pubDate>Thu, 25 Apr 2024 18:50:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08733v3</guid></item><item><title>In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization</title><link>http://arxiv.org/abs/2404.16795v1</link><description>With the increasing computational costs associated with deep learning,automated hyperparameter optimization methods, strongly relying on black-boxBayesian optimization (BO), face limitations. Freeze-thaw BO offers a promisinggrey-box alternative, strategically allocating scarce resources incrementallyto different configurations. However, the frequent surrogate model updatesinherent to this approach pose challenges for existing methods, requiringretraining or fine-tuning their neural network surrogates online, introducingoverhead, instability, and hyper-hyperparameters. In this work, we proposeFT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-datafitted network (PFN) that leverages the transformers' in-context learningability to efficiently and reliably do Bayesian learning curve extrapolation ina single forward pass. Our empirical analysis across three benchmark suitesshows that the predictions made by FT-PFN are more accurate and 10-100 timesfaster than those of the deep Gaussian process and deep ensemble surrogatesused in previous work. Furthermore, we show that, when combined with our novelacquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BOmethod (ifBO), yields new state-of-the-art performance in the same threefamilies of deep learning HPO benchmarks considered in prior work.</description><author>Herilalaina Rakotoarison, Steven Adriaensen, Neeratyoy Mallik, Samir Garibov, Edward Bergman, Frank Hutter</author><pubDate>Thu, 25 Apr 2024 18:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16795v1</guid></item><item><title>Weak-to-Strong Extrapolation Expedites Alignment</title><link>http://arxiv.org/abs/2404.16792v1</link><description>Although the capabilities of large language models (LLMs) ideally scale upwith increasing data and compute, they are inevitably constrained by limitedresources in reality. Suppose we have a moderately trained LLM (e.g., trainedto align with human preference) in hand, can we further exploit its potentialand cheaply acquire a stronger model? In this paper, we propose a simple methodcalled ExPO to boost LLMs' alignment with human preference. ExPO assumes that amedium-aligned model can be interpolated between a less-aligned (weaker) model,e.g., the initial SFT model, and a better-aligned (stronger) one, therebydirectly obtaining this stronger model by extrapolating from the weights of theformer two relatively weaker models. On the AlpacaEval 2.0 benchmark, we showthat ExPO pushes models trained with less preference data (e.g., 10% or 20%) toreach and even surpass the fully-trained one, without any additional training.Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models andexhibits decent scalability across model sizes from 7B to 70B. Our workdemonstrates the efficacy of model extrapolation in exploiting LLMs'capabilities, suggesting a promising direction that deserves futureexploration.</description><author>Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, Nanyun Peng</author><pubDate>Thu, 25 Apr 2024 18:39:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16792v1</guid></item><item><title>SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension</title><link>http://arxiv.org/abs/2404.16790v1</link><description>Comprehending text-rich visual content is paramount for the practicalapplication of Multimodal Large Language Models (MLLMs), since text-richscenarios are ubiquitous in the real world, which are characterized by thepresence of extensive texts embedded within images. Recently, the advent ofMLLMs with impressive versatility has raised the bar for what we can expectfrom MLLMs. However, their proficiency in text-rich scenarios has yet to becomprehensively and objectively assessed, since current MLLM benchmarksprimarily focus on evaluating general visual comprehension. In this work, weintroduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating\textbf{text-rich visual comprehension} of MLLMs. Our benchmark comprises 2.3Kmultiple-choice questions with precise human annotations, spanning three broadcategories: Charts, Maps, and Webs, each of which covers a wide spectrum oftext-rich scenarios in the real world. These categories, due to their inherentcomplexity and diversity, effectively simulate real-world text-richenvironments. We further conduct a thorough evaluation involving 34 prominentMLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize thecurrent limitations of MLLMs in text-rich visual comprehension. We hope thatour work can serve as a valuable addition to existing MLLM benchmarks,providing insightful observations and inspiring further research in the area oftext-rich visual comprehension with MLLMs. The dataset and evaluation code canbe accessed at https://github.com/AILab-CVC/SEED-Bench.</description><author>Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, Ying Shan</author><pubDate>Thu, 25 Apr 2024 18:39:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16790v1</guid></item><item><title>Continual Learning of Large Language Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2404.16789v1</link><description>The recent success of large language models (LLMs) trained on static,pre-collected, general datasets has sparked numerous research directions andapplications. One such direction addresses the non-trivial challenge ofintegrating pre-trained LLMs into dynamic data distributions, task structures,and user preferences. Pre-trained LLMs, when tailored for specific needs, oftenexperience significant performance degradation in previous knowledge domains --a phenomenon known as "catastrophic forgetting". While extensively studied inthe continual learning (CL) community, it presents new manifestations in therealm of LLMs. In this survey, we provide a comprehensive overview of thecurrent research progress on LLMs within the context of CL. This survey isstructured into four main sections: we first describe an overview ofcontinually learning LLMs, consisting of two directions of continuity: verticalcontinuity (or vertical continual learning), i.e., continual adaptation fromgeneral to specific capabilities, and horizontal continuity (or horizontalcontinual learning), i.e., continual adaptation across time and domains(Section 3). We then summarize three stages of learning LLMs in the context ofmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview ofevaluation protocols for continual learning with LLMs, along with the currentavailable data sources (Section 5). Finally, we discuss intriguing questionspertaining to continual learning for LLMs (Section 6). The full list of papersexamined in this survey is available athttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.</description><author>Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Hao Wang</author><pubDate>Thu, 25 Apr 2024 18:38:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16789v1</guid></item><item><title>Registration by Regression (RbR): a framework for interpretable and flexible atlas registration</title><link>http://arxiv.org/abs/2404.16781v1</link><description>In human neuroimaging studies, atlas registration enables mapping MRI scansto a common coordinate frame, which is necessary to aggregate data frommultiple subjects. Machine learning registration methods have achievedexcellent speed and accuracy but lack interpretability. More recently,keypoint-based methods have been proposed to tackle this issue, but theiraccuracy is still subpar, particularly when fitting nonlinear transforms. Herewe propose Registration by Regression (RbR), a novel atlas registrationframework that is highly robust and flexible, conceptually simple, and can betrained with cheaply obtained data. RbR predicts the (x,y,z) atlas coordinatesfor every voxel of the input scan (i.e., every voxel is a keypoint), and thenuses closed-form expressions to quickly fit transforms using a wide array ofpossible deformation models, including affine and nonlinear (e.g., Bspline,Demons, invertible diffeomorphic models, etc.). Robustness is provided by thelarge number of voxels informing the registration and can be further increasedby robust estimators like RANSAC. Experiments on independent public datasetsshow that RbR yields more accurate registration than competing keypointapproaches, while providing full control of the deformation model.</description><author>Karthik Gopinath, Xiaoling Hu, Malte Hoffmann, Oula Puonti, Juan Eugenio Iglesias</author><pubDate>Thu, 25 Apr 2024 18:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16781v1</guid></item><item><title>Confidence-Triggered Detection: Accelerating Real-time Tracking-by-detection Systems</title><link>http://arxiv.org/abs/1902.00615v4</link><description>Real-time object tracking necessitates a delicate balance between speed andaccuracy, a challenge exacerbated by the computational demands of deep learningmethods. In this paper, we propose Confidence-Triggered Detection (CTD), aninnovative approach that strategically bypasses object detection for framesclosely resembling intermediate states, leveraging tracker confidence scores.CTD not only enhances tracking speed but also preserves accuracy, surpassingexisting tracking algorithms. Through extensive evaluation across varioustracker confidence thresholds, we identify an optimal trade-off betweentracking speed and accuracy, providing crucial insights for parameterfine-tuning and enhancing CTD's practicality in real-world scenarios. Ourexperiments across diverse detection models underscore the robustness andversatility of the CTD framework, demonstrating its potential to enablereal-time tracking in resource-constrained environments.</description><author>Zhicheng Ding, Zhixin Lai, Siyang Li, Panfeng Li, Qikai Yang, Edward Wong</author><pubDate>Thu, 25 Apr 2024 18:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1902.00615v4</guid></item><item><title>DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks</title><link>http://arxiv.org/abs/2404.16779v1</link><description>The success of many RL techniques heavily relies on human-engineered denserewards, which typically demand substantial domain expertise and extensivetrial and error. In our work, we propose DrS (Dense reward learning fromStages), a novel approach for learning reusable dense rewards for multi-stagetasks in a data-driven manner. By leveraging the stage structures of the task,DrS learns a high-quality dense reward from sparse rewards and demonstrationsif given. The learned rewards can be \textit{reused} in unseen tasks, thusreducing the human effort for reward engineering. Extensive experiments onthree physical robot manipulation task families with 1000+ task variantsdemonstrate that our learned rewards can be reused in unseen tasks, resultingin improved performance and sample efficiency of RL algorithms. The learnedrewards even achieve comparable performance to human-engineered rewards on sometasks. See our project page (https://sites.google.com/view/iclr24drs) for moredetails.</description><author>Tongzhou Mu, Minghua Liu, Hao Su</author><pubDate>Thu, 25 Apr 2024 18:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16779v1</guid></item><item><title>Modeling Selective Feature Attention for Representation-based Siamese Text Matching</title><link>http://arxiv.org/abs/2404.16776v1</link><description>Representation-based Siamese networks have risen to popularity in lightweighttext matching due to their low deployment and inference costs. While word-levelattention mechanisms have been implemented within Siamese networks to improveperformance, we propose Feature Attention (FA), a novel downstream blockdesigned to enrich the modeling of dependencies among embedding features.Employing "squeeze-and-excitation" techniques, the FA block dynamically adjuststhe emphasis on individual features, enabling the network to concentrate moreon features that significantly contribute to the final classification. Buildingupon FA, we introduce a dynamic "selection" mechanism called Selective FeatureAttention (SFA), which leverages a stacked BiGRU Inception structure. The SFAblock facilitates multi-scale semantic extraction by traversing differentstacked BiGRU layers, encouraging the network to selectively concentrate onsemantic information and embedding features across varying levels ofabstraction. Both the FA and SFA blocks offer a seamless integration capabilitywith various Siamese networks, showcasing a plug-and-play characteristic.Experimental evaluations conducted across diverse text matching baselines andbenchmarks underscore the indispensability of modeling feature attention andthe superiority of the "selection" mechanism.</description><author>Jianxiang Zang, Hui Liu</author><pubDate>Thu, 25 Apr 2024 18:26:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16776v1</guid></item><item><title>ConKeD++ -- Improving descriptor learning for retinal image registration: A comprehensive study of contrastive losses</title><link>http://arxiv.org/abs/2404.16773v1</link><description>Self-supervised contrastive learning has emerged as one of the mostsuccessful deep learning paradigms. In this regard, it has seen extensive usein image registration and, more recently, in the particular field of medicalimage registration. In this work, we propose to test and extend and improve astate-of-the-art framework for color fundus image registration, ConKeD. Usingthe ConKeD framework we test multiple loss functions, adapting them to theframework and the application domain. Furthermore, we evaluate our models usingthe standarized benchmark dataset FIRE as well as several datasets that havenever been used before for color fundus registration, for which we arereleasing the pairing data as well as a standardized evaluation approach. Ourwork demonstrates state-of-the-art performance across all datasets and metricsdemonstrating several advantages over current SOTA color fundus registrationmethods</description><author>David Rivas-Villar, Álvaro S. Hervella, José Rouco, Jorge Novo</author><pubDate>Thu, 25 Apr 2024 18:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16773v1</guid></item><item><title>ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving</title><link>http://arxiv.org/abs/2404.16771v1</link><description>Diffusion-based technologies have made significant strides, particularly inpersonalized and customized facialgeneration. However, existing methods facechallenges in achieving high-fidelity and detailed identity (ID)consistency,primarily due to insufficient fine-grained control over facial areas and thelack of a comprehensive strategy for ID preservation by fully consideringintricate facial details and the overall face. To address these limitations, weintroduce ConsistentID, an innovative method crafted fordiverseidentity-preserving portrait generation under fine-grained multimodalfacial prompts, utilizing only a single reference image. ConsistentID comprisestwo key components: a multimodal facial prompt generator that combines facialfeatures, corresponding facial descriptions and the overall facial context toenhance precision in facial details, and an ID-preservation network optimizedthrough the facial attention localization strategy, aimed at preserving IDconsistency in facial regions. Together, these components significantly enhancethe accuracy of ID preservation by introducing fine-grained multimodal IDinformation from facial regions. To facilitate training of ConsistentID, wepresent a fine-grained portrait dataset, FGID, with over 500,000 facial images,offering greater diversity and comprehensiveness than existing public facialdatasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental resultssubstantiate that our ConsistentID achieves exceptional precision and diversityin personalized facial generation, surpassing existing methods in the MyStyledataset. Furthermore, while ConsistentID introduces more multimodal IDinformation, it maintains a fast inference speed during generation.</description><author>Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun Zhou, Yuhao Cheng, Shutao Liao, Long Chen, Yiqiang Yan, Shengcai Liao, Xiaodan Liang</author><pubDate>Thu, 25 Apr 2024 18:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16771v1</guid></item><item><title>Redefining Safety for Autonomous Vehicles</title><link>http://arxiv.org/abs/2404.16768v1</link><description>Existing definitions and associated conceptual frameworks for computer-basedsystem safety should be revisited in light of real-world experiences fromdeploying autonomous vehicles. Current terminology used by industry safetystandards emphasizes mitigation of risk from specifically identified hazards,and carries assumptions based on human-supervised vehicle operation. Operationwithout a human driver dramatically increases the scope of safety concerns,especially due to operation in an open world environment, a requirement toself-enforce operational limits, participation in an ad hoc sociotechnicalsystem of systems, and a requirement to conform to both legal and ethicalconstraints. Existing standards and terminology only partially address thesenew challenges. We propose updated definitions for core system safety conceptsthat encompass these additional considerations as a starting point for evolvingsafe-ty approaches to address these additional safety challenges. These resultsmight additionally inform framing safety terminology for other autonomoussystem applications.</description><author>Philip Koopman, William Widen</author><pubDate>Thu, 25 Apr 2024 18:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16768v1</guid></item><item><title>Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction</title><link>http://arxiv.org/abs/2206.09821v3</link><description>Significant wave height forecasting is a key problem in ocean data analytics.This problem is relevant in several maritime operations, such as managing thepassage of vessels or estimating the energy production from waves. In thiswork, we focus on the prediction of extreme values of significant wave heightthat can cause coastal disasters. This task is framed as an exceedanceprobability forecasting problem. Accordingly, we aim to estimate theprobability that the significant wave height will exceed a predefined criticalthreshold. This problem is usually solved using a probabilistic binaryclassification model. Instead, we propose a novel approach based on aforecasting model. A probabilistic binary forecast streamlines information fordecision-making, and point forecasts can provide additional insights into thedata dynamics. The proposed method works by converting point forecasts intoexceedance probability estimates using the cumulative distribution function. Wecarried out experiments using data from a buoy placed on the coast of Halifax,Canada. The results suggest that the proposed methodology is better thanstate-of-the-art approaches for exceedance probability forecasting.</description><author>Vitor Cerqueira, Luis Torgo</author><pubDate>Thu, 25 Apr 2024 18:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.09821v3</guid></item><item><title>REBEL: Reinforcement Learning via Regressing Relative Rewards</title><link>http://arxiv.org/abs/2404.16767v1</link><description>While originally developed for continuous control problems, Proximal PolicyOptimization (PPO) has emerged as the work-horse of a variety of reinforcementlearning (RL) applications including the fine-tuning of generative models.Unfortunately, PPO requires multiple heuristics to enable stable convergence(e.g. value networks, clipping) and is notorious for its sensitivity to theprecise implementation of these components. In response, we take a step backand ask what a minimalist RL algorithm for the era of generative models wouldlook like. We propose REBEL, an algorithm that cleanly reduces the problem ofpolicy optimization to regressing the relative rewards via a direct policyparameterization between two completions to a prompt, enabling strikinglylightweight implementation. In theory, we prove that fundamental RL algorithmslike Natural Policy Gradient can be seen as variants of REBEL, which allows usto match the strongest known theoretical guarantees in terms of convergence andsample complexity in the RL literature. REBEL can also cleanly incorporateoffline data and handle the intransitive preferences we frequently see inpractice. Empirically, we find that REBEL provides a unified approach tolanguage modeling and image generation with stronger or similar performance asPPO and DPO, all while being simpler to implement and more computationallytractable than PPO.</description><author>Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun</author><pubDate>Thu, 25 Apr 2024 18:20:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16767v1</guid></item><item><title>Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model</title><link>http://arxiv.org/abs/2404.16766v1</link><description>While supervised fine-tuning (SFT) has been a straightforward approach fortailoring the output of foundation large language model (LLM) to specificpreferences, concerns have been raised about the depth of this alignment, withsome critiques suggesting it is merely "superficial". We critically examinethis hypothesis within the scope of cross-lingual generation tasks, proposingthat the effectiveness of SFT may be constrained by its reliance on priortokens to guide cross-lingual generation. Based on this crucial insight, and inresponse to the challenges posed by the costly and limited availability ofnon-English data for SFT, we introduce a novel training-free alignment methodnamed PreTTY, which employs minimal task-related prior tokens to bridge thefoundation LLM and the SFT LLM, achieving comparable performance withouttraining. Experiments on machine translation and part-of-speech tagging acrosseight languages demonstrate the efficacy of PreTTY in cross-lingual settings.Remarkably, by initiating the decoding process with only one or two priortokens, foundation LLMs can achieve performance comparable to their SFTcounterparts. This method presents a cost-effective alternative to SFT andadvances the democratization of multilingual LLMs.</description><author>Runzhe Zhan, Xinyi Yang, Derek F. Wong, Lidia S. Chao, Yue Zhang</author><pubDate>Thu, 25 Apr 2024 18:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16766v1</guid></item><item><title>Dataset of Quotation Attribution in German News Articles</title><link>http://arxiv.org/abs/2404.16764v1</link><description>Extracting who says what to whom is a crucial part in analyzing humancommunication in today's abundance of data such as online news articles. Yet,the lack of annotated data for this task in German news articles severelylimits the quality and usability of possible systems. To remedy this, wepresent a new, freely available, creative-commons-licensed dataset forquotation attribution in German news articles based on WIKINEWS. The datasetprovides curated, high-quality annotations across 1000 documents (250,000tokens) in a fine-grained annotation schema enabling various downstream usesfor the dataset. The annotations not only specify who said what but also how,in which context, to whom and define the type of quotation. We specify ourannotation schema, describe the creation of the dataset and provide aquantitative analysis. Further, we describe suitable evaluation metrics, applytwo existing systems for quotation attribution, discuss their results toevaluate the utility of our dataset and outline use cases of our dataset indownstream tasks.</description><author>Fynn Petersen-Frey, Chris Biemann</author><pubDate>Thu, 25 Apr 2024 18:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16764v1</guid></item><item><title>Interpretable Clustering with the Distinguishability Criterion</title><link>http://arxiv.org/abs/2404.15967v2</link><description>Cluster analysis is a popular unsupervised learning tool used in manydisciplines to identify heterogeneous sub-populations within a sample. However,validating cluster analysis results and determining the number of clusters in adata set remains an outstanding problem. In this work, we present a globalcriterion called the Distinguishability criterion to quantify the separabilityof identified clusters and validate inferred cluster configurations. Ourcomputational implementation of the Distinguishability criterion corresponds tothe Bayes risk of a randomized classifier under the 0-1 loss. We propose acombined loss function-based computational framework that integrates theDistinguishability criterion with many commonly used clustering procedures,such as hierarchical clustering, k-means, and finite mixture models. We presentthese new algorithms as well as the results from comprehensive data analysisbased on simulation studies and real data applications.</description><author>Ali Turfah, Xiaoquan Wen</author><pubDate>Thu, 25 Apr 2024 18:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15967v2</guid></item><item><title>RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis</title><link>http://arxiv.org/abs/2404.16754v1</link><description>Developing generalist foundation model has recently attracted tremendousattention among researchers in the field of AI for Medicine (AI4Medicine). Apivotal insight in developing these models is their reliance on datasetscaling, which emphasizes the requirements on developing open-source medicalimage datasets that incorporate diverse supervision signals across variousimaging modalities. In this paper, we introduce RadGenome-Chest CT, acomprehensive, large-scale, region-guided 3D chest CT interpretation datasetbased on CT-RATE. Specifically, we leverage the latest powerful universalsegmentation and large language models, to extend the original datasets (over25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) fromthe following aspects: (i) organ-level segmentation masks covering 197categories, which provide intermediate reasoning visual clues forinterpretation; (ii) 665 K multi-granularity grounded reports, where eachsentence of the report is linked to the corresponding anatomical region of CTvolume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs,where questions and answers are all linked with reference segmentation masks,enabling models to associate visual evidence with textual explanations. Allgrounded reports and VQA pairs in the validation set have gone through manualverification to ensure dataset quality. We believe that RadGenome-Chest CT cansignificantly advance the development of multimodal medical foundation models,by training to generate texts based on given segmentation regions, which isunattainable with previous relevant datasets. We will release all segmentationmasks, grounded reports, and VQA pairs to facilitate further research anddevelopment in this field.</description><author>Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Thu, 25 Apr 2024 18:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16754v1</guid></item><item><title>TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation</title><link>http://arxiv.org/abs/2404.16752v1</link><description>We address the problem of regressing 3D human pose and shape from a singleimage, with a focus on 3D accuracy. The current best methods leverage largedatasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robustperformance. With such methods, we observe a paradoxical decline in 3D poseaccuracy with increasing 2D accuracy. This is caused by biases in the p-GT andthe use of an approximate camera projection model. We quantify the errorinduced by current camera models and show that fitting 2D keypoints and p-GTaccurately causes incorrect 3D poses. Our analysis defines the invaliddistances within which minimizing 2D and p-GT losses is detrimental. We usethis to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) thatpenalizes gross 2D and p-GT losses but not smaller ones. With such a loss,there are many 3D poses that could equally explain the 2D evidence. To reducethis ambiguity we need a prior over valid human poses but such priors canintroduce unwanted bias. To address this, we exploit a tokenized representationof human pose and reformulate the problem as token prediction. This restrictsthe estimated poses to the space of valid poses, effectively providing auniform prior. Extensive experiments on the EMDB and 3DPW datasets show thatour reformulated keypoint loss and tokenization allows us to train onin-the-wild data while improving 3D accuracy over the state-of-the-art. Ourmodels and code are available for research at https://tokenhmr.is.tue.mpg.de.</description><author>Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, Michael J. Black</author><pubDate>Thu, 25 Apr 2024 18:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16752v1</guid></item><item><title>TELA: Text to Layer-wise 3D Clothed Human Generation</title><link>http://arxiv.org/abs/2404.16748v1</link><description>This paper addresses the task of 3D clothed human generation from texturaldescriptions. Previous works usually encode the human body and clothes as aholistic model and generate the whole model in a single-stage optimization,which makes them struggle for clothing editing and meanwhile lose fine-grainedcontrol over the whole generation process. To solve this, we propose alayer-wise clothed human representation combined with a progressiveoptimization strategy, which produces clothing-disentangled 3D human modelswhile providing control capacity for the generation process. The basic idea isprogressively generating a minimal-clothed human body and layer-wise clothes.During clothing generation, a novel stratified compositional rendering methodis proposed to fuse multi-layer human models, and a new loss function isutilized to help decouple the clothing model from the human body. The proposedmethod achieves high-quality disentanglement, which thereby provides aneffective way for 3D garment generation. Extensive experiments demonstrate thatour approach achieves state-of-the-art 3D clothed human generation while alsosupporting cloth editing applications such as virtual try-on. Project page:http://jtdong.com/tela_layer/</description><author>Junting Dong, Qi Fang, Zehuan Huang, Xudong Xu, Jingbo Wang, Sida Peng, Bo Dai</author><pubDate>Thu, 25 Apr 2024 18:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16748v1</guid></item><item><title>Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks</title><link>http://arxiv.org/abs/2402.04248v2</link><description>State-space models (SSMs), such as Mamba (Gu &amp; Dao, 2023), have been proposedas alternatives to Transformer networks in language modeling, by incorporatinggating, convolutions, and input-dependent token selection to mitigate thequadratic cost of multi-head attention. Although SSMs exhibit competitiveperformance, their in-context learning (ICL) capabilities, a remarkableemergent property of modern language models that enables task execution withoutparameter optimization, remain underexplored compared to Transformers. In thisstudy, we evaluate the ICL performance of SSMs, focusing on Mamba, againstTransformer models across various tasks. Our results show that SSMs performcomparably to Transformers in standard regression ICL tasks, whileoutperforming them in tasks like sparse parity learning. However, SSMs fallshort in tasks involving non-standard retrieval functionality. To address theselimitations, we introduce a hybrid model, MambaFormer, that combines Mamba withattention blocks, surpassing individual models in tasks where they struggleindependently. Our findings suggest that hybrid architectures offer promisingavenues for enhancing ICL in language models.</description><author>Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos</author><pubDate>Thu, 25 Apr 2024 18:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04248v2</guid></item><item><title>Estimating the Number of Components in Finite Mixture Models via Variational Approximation</title><link>http://arxiv.org/abs/2404.16746v1</link><description>This work introduces a new method for selecting the number of components infinite mixture models (FMMs) using variational Bayes, inspired by thelarge-sample properties of the Evidence Lower Bound (ELBO) derived frommean-field (MF) variational approximation. Specifically, we establish matchingupper and lower bounds for the ELBO without assuming conjugate priors,suggesting the consistency of model selection for FMMs based on maximizing theELBO. As a by-product of our proof, we demonstrate that the MF approximationinherits the stable behavior (benefited from model singularity) of theposterior distribution, which tends to eliminate the extra components undermodel misspecification where the number of mixture components isover-specified. This stable behavior also leads to the $n^{-1/2}$ convergencerate for parameter estimation, up to a logarithmic factor, under this modeloverspecification. Empirical experiments are conducted to validate ourtheoretical findings and compare with other state-of-the-art methods forselecting the number of components in FMMs.</description><author>Chenyang Wang, Yun Yang</author><pubDate>Thu, 25 Apr 2024 18:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16746v1</guid></item><item><title>Automatic Speech Recognition System-Independent Word Error Rate Estimatio</title><link>http://arxiv.org/abs/2404.16743v1</link><description>Word error rate (WER) is a metric used to evaluate the quality oftranscriptions produced by Automatic Speech Recognition (ASR) systems. In manyapplications, it is of interest to estimate WER given a pair of a speechutterance and a transcript. Previous work on WER estimation focused on buildingmodels that are trained with a specific ASR system in mind (referred to as ASRsystem-dependent). These are also domain-dependent and inflexible in real-worldapplications. In this paper, a hypothesis generation method for ASRSystem-Independent WER estimation (SIWE) is proposed. In contrast to priorwork, the WER estimators are trained using data that simulates ASR systemoutput. Hypotheses are generated using phonetically similar or linguisticallymore likely alternative words. In WER estimation experiments, the proposedmethod reaches a similar performance to ASR system-dependent WER estimators onin-domain data and achieves state-of-the-art performance on out-of-domain data.On the out-of-domain data, the SIWE model outperformed the baseline estimatorsin root mean square error and Pearson correlation coefficient by relative17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performancewas further improved when the WER of the training set was close to the WER ofthe evaluation dataset.</description><author>Chanho Park, Mingjie Chen, Thomas Hain</author><pubDate>Thu, 25 Apr 2024 17:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16743v1</guid></item><item><title>Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues</title><link>http://arxiv.org/abs/2402.00281v3</link><description>Although state-of-the-art classifiers for facial expression recognition (FER)can achieve a high level of accuracy, they lack interpretability, an importantfeature for end-users. Experts typically associate spatial action units (\aus)from a codebook to facial regions for the visual interpretation of expressions.In this paper, the same expert steps are followed. A new learning strategy isproposed to explicitly incorporate \au cues into classifier training, allowingto train deep interpretable models. During training, this \au codebook is used,along with the input image expression label, and facial landmarks, to constructa \au heatmap that indicates the most discriminative image regions of interestw.r.t the facial expression. This valuable spatial cue is leveraged to train adeep interpretable classifier for FER. This is achieved by constraining thespatial layer features of a classifier to be correlated with \au heatmaps.Using a composite loss, the classifier is trained to correctly classify animage while yielding interpretable visual layer-wise attention correlated with\au maps, simulating the expert decision process. Our strategy only relies onimage class expression for supervision, without additional manual annotations.Our new strategy is generic, and can be applied to any deep CNN- ortransformer-based classifier without requiring any architectural change orsignificant additional training time. Our extensive evaluation on two publicbenchmarks \rafdb, and \affectnet datasets shows that our proposed strategy canimprove layer-wise interpretability without degrading classificationperformance. In addition, we explore a common type of interpretable classifiersthat rely on class activation mapping (CAM) methods, and show that our approachcan also improve CAM interpretability.</description><author>Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras Koerich, Simon Bacon, Eric Granger</author><pubDate>Thu, 25 Apr 2024 17:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00281v3</guid></item><item><title>CBRW: A Novel Approach for Cancelable Biometric Template Generation based on</title><link>http://arxiv.org/abs/2404.16739v1</link><description>Cancelable Biometric is a challenging research field in which security of anoriginal biometric image is ensured by transforming the original biometric intoanother irreversible domain. Several approaches have been suggested inliterature for generating cancelable biometric templates. In this paper, twonovel and simple cancelable biometric template generation methods based onRandom Walk (CBRW) have been proposed. By employing random walk and other stepsgiven in the proposed two algorithms viz. CBRW-BitXOR and CBRW-BitCMP, theoriginal biometric is transformed into a cancellable template. The performanceof the proposed methods is compared with other state-of-the-art methods.Experiments have been performed on eight publicly available gray and colordatasets i.e. CP (ear) (gray and color), UTIRIS (iris) (gray and color), ORL(face) (gray), IIT Delhi (iris) (gray and color), and AR (face) (color).Performance of the generated templates is measured in terms of CorrelationCoefficient (Cr), Root Mean Square Error (RMSE), Peak Signal to Noise Ratio(PSNR), Structural Similarity (SSIM), Mean Absolute Error (MAE), Number ofPixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI). Byexperimental results, it has been proved that proposed methods are superiorthan other state-of-the-art methods in qualitative as well as quantitativeanalysis. Furthermore, CBRW performs better on both gray as well as colorimages.</description><author>Nitin Kumar, Manisha</author><pubDate>Thu, 25 Apr 2024 17:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16739v1</guid></item><item><title>Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning</title><link>http://arxiv.org/abs/2304.06768v2</link><description>Machine learning algorithms, both in their classical and quantum versions,heavily rely on optimization algorithms based on gradients, such as gradientdescent and alike. The overall performance is dependent on the appearance oflocal minima and barren plateaus, which slow-down calculations and lead tonon-optimal solutions. In practice, this results in dramatic computational andenergy costs for AI applications. In this paper we introduce a generic strategyto accelerate and improve the overall performance of such methods, allowing toalleviate the effect of barren plateaus and local minima. Our method is basedon coordinate transformations, somehow similar to variational rotations, addingextra directions in parameter space that depend on the cost function itself,and which allow to explore the configuration landscape more efficiently. Thevalidity of our method is benchmarked by boosting a number of quantum machinelearning algorithms, getting a very significant improvement in theirperformance.</description><author>Pablo Bermejo, Borja Aizpurua, Roman Orus</author><pubDate>Thu, 25 Apr 2024 17:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06768v2</guid></item><item><title>Using Deep Learning to Identify Initial Error Sensitivity of ENSO Forecasts</title><link>http://arxiv.org/abs/2404.15419v2</link><description>We introduce a hybrid method that integrates deep learning with model-analogforecasting, a straightforward yet effective approach that generates forecastsfrom similar initial climate states in a repository of model simulations. Thishybrid framework employs a convolutional neural network to estimatestate-dependent weights to identify analog states. The advantage of our methodlies in its physical interpretability, offering insights intoinitial-error-sensitive regions through estimated weights and the ability totrace the physically-based temporal evolution of the system through analogforecasting. We evaluate our approach using the Community Earth System ModelVersion 2 Large Ensemble to forecast the El Ni\~no-Southern Oscillation (ENSO)on a seasonal-to-annual time scale. Results show a 10% improvement inforecasting sea surface temperature anomalies over the equatorial Pacific at9-12 months leads compared to the traditional model-analog technique.Furthermore, our hybrid model demonstrates improvements in boreal winter andspring initialization when evaluated against a reanalysis dataset. Our deeplearning-based approach reveals state-dependent sensitivity linked to variousseasonally varying physical processes, including the Pacific Meridional Modes,equatorial recharge oscillator, and stochastic wind forcing. Notably,disparities emerge in the sensitivity associated with El Ni\~no and La Ni\~naevents. We find that sea surface temperature over the tropical Pacific plays amore crucial role in El Ni\~no forecasting, while zonal wind stress over thesame region exhibits greater significance in La Ni\~na prediction. Thisapproach has broad implications for forecasting diverse climate phenomena,including regional temperature and precipitation, which are challenging for thetraditional model-analog forecasting method.</description><author>Kinya Toride, Matthew Newman, Andrew Hoell, Antonietta Capotondi, Jakob Schlör, Dillon Amaya</author><pubDate>Thu, 25 Apr 2024 17:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15419v2</guid></item><item><title>History repeats itself: A Baseline for Temporal Knowledge Graph Forecasting</title><link>http://arxiv.org/abs/2404.16726v1</link><description>Temporal Knowledge Graph (TKG) Forecasting aims at predicting links inKnowledge Graphs for future timesteps based on a history of Knowledge Graphs.To this day, standardized evaluation protocols and rigorous comparison acrossTKG models are available, but the importance of simple baselines is oftenneglected in the evaluation, which prevents researchers from discerning actualand fictitious progress. We propose to close this gap by designing an intuitivebaseline for TKG Forecasting based on predicting recurring facts. Compared tomost TKG models, it requires little hyperparameter tuning and no iterativetraining. Further, it can help to identify failure modes in existingapproaches. The empirical findings are quite unexpected: compared to 11 methodson five datasets, our baseline ranks first or third in three of them, paintinga radically different picture of the predictive quality of the state of theart.</description><author>Julia Gastinger, Christian Meilicke, Federico Errica, Timo Sztyler, Anett Schuelke, Heiner Stuckenschmidt</author><pubDate>Thu, 25 Apr 2024 17:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16726v1</guid></item><item><title>Tverberg's theorem and multi-class support vector machines</title><link>http://arxiv.org/abs/2404.16724v1</link><description>We show how, using linear-algebraic tools developed to prove Tverberg'stheorem in combinatorial geometry, we can design new models of multi-classsupport vector machines (SVMs). These supervised learning protocols requirefewer conditions to classify sets of points, and can be computed using existingbinary SVM algorithms in higher-dimensional spaces, including soft-margin SVMalgorithms. We describe how the theoretical guarantees of standard supportvector machines transfer to these new classes of multi-class support vectormachines. We give a new simple proof of a geometric characterization of supportvectors for largest margin SVMs by Veelaert.</description><author>Pablo Soberón</author><pubDate>Thu, 25 Apr 2024 17:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16724v1</guid></item><item><title>TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations</title><link>http://arxiv.org/abs/2402.17660v2</link><description>Achieving a balance between computational speed, prediction accuracy, anduniversal applicability in molecular simulations has been a persistentchallenge. This paper presents substantial advancements in the TorchMD-Netsoftware, a pivotal step forward in the shift from conventional force fields toneural network-based potentials. The evolution of TorchMD-Net into a morecomprehensive and versatile framework is highlighted, incorporatingcutting-edge architectures such as TensorNet. This transformation is achievedthrough a modular design approach, encouraging customized applications withinthe scientific community. The most notable enhancement is a significantimprovement in computational efficiency, achieving a very remarkableacceleration in the computation of energy and forces for TensorNet models, withperformance gains ranging from 2-fold to 10-fold over previous iterations.Other enhancements include highly optimized neighbor search algorithms thatsupport periodic boundary conditions and the smooth integration with existingmolecular dynamics frameworks. Additionally, the updated version introduces thecapability to integrate physical priors, further enriching its applicationspectrum and utility in research. The software is available athttps://github.com/torchmd/torchmd-net.</description><author>Raul P. Pelaez, Guillem Simeon, Raimondas Galvelis, Antonio Mirarchi, Peter Eastman, Stefan Doerr, Philipp Thölke, Thomas E. Markland, Gianni De Fabritiis</author><pubDate>Thu, 25 Apr 2024 17:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17660v2</guid></item><item><title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title><link>http://arxiv.org/abs/2403.05530v2</link><description>In this report, we present the latest model of the Gemini family, Gemini 1.5Pro, a highly compute-efficient multimodal mixture-of-experts model capable ofrecalling and reasoning over fine-grained information from millions of tokensof context, including multiple long documents and hours of video and audio.Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasksacross modalities, improves the state-of-the-art in long-document QA,long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra'sstate-of-the-art performance across a broad set of benchmarks. Studying thelimits of Gemini 1.5 Pro's long-context ability, we find continued improvementin next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10Mtokens, a generational leap over existing models such as Claude 2.1 (200k) andGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of largelanguage models at the frontier; when given a grammar manual for Kalamang, alanguage with fewer than 200 speakers worldwide, the model learns to translateEnglish to Kalamang at a similar level to a person who learned from the samecontent.</description><author>Gemini Team, Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry, Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spence</author><pubDate>Thu, 25 Apr 2024 17:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05530v2</guid></item><item><title>Distilling Privileged Information for Dubins Traveling Salesman Problems with Neighborhoods</title><link>http://arxiv.org/abs/2404.16721v1</link><description>This paper presents a novel learning approach for Dubins Traveling SalesmanProblems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of anon-holonomic vehicle passing through neighborhoods of given task points. Themethod involves two learning phases: initially, a model-free reinforcementlearning approach leverages privileged information to distill knowledge fromexpert trajectories generated by the LinKernighan heuristic (LKH) algorithm.Subsequently, a supervised learning phase trains an adaptation network to solveproblems independently of privileged information. Before the first learningphase, a parameter initialization technique using the demonstration data wasalso devised to enhance training efficiency. The proposed learning methodproduces a solution about 50 times faster than LKH and substantiallyoutperforms other imitation learning and RL with demonstration schemes, most ofwhich fail to sense all the task points.</description><author>Min Kyu Shin, Su-Jeong Park, Seung-Keol Ryu, Heeyeon Kim, Han-Lim Choi</author><pubDate>Thu, 25 Apr 2024 17:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16721v1</guid></item><item><title>Features Fusion for Dual-View Mammography Mass Detection</title><link>http://arxiv.org/abs/2404.16718v1</link><description>Detection of malignant lesions on mammography images is extremely importantfor early breast cancer diagnosis. In clinical practice, images are acquiredfrom two different angles, and radiologists can fully utilize information fromboth views, simultaneously locating the same lesion. However, for automaticdetection approaches such information fusion remains a challenge. In thispaper, we propose a new model called MAMM-Net, which allows the processing ofboth mammography views simultaneously by sharing information not only on anobject level, as seen in existing works, but also on a feature level.MAMM-Net's key component is the Fusion Layer, based on deformable attention anddesigned to increase detection precision while keeping high recall. Ourexperiments show superior performance on the public DDSM dataset compared tothe previous state-of-the-art model, while introducing new helpful featuressuch as lesion annotation on pixel-level and classification of lesionsmalignancy.</description><author>Arina Varlamova, Valery Belotsky, Grigory Novikov, Anton Konushin, Evgeny Sidorov</author><pubDate>Thu, 25 Apr 2024 17:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16718v1</guid></item><item><title>Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class</title><link>http://arxiv.org/abs/2404.16717v1</link><description>Vision-language models enable open-world classification of objects withoutthe need for any retraining. While this zero-shot paradigm marks a significantadvance, even today's best models exhibit skewed performance when objects aredissimilar from their typical depiction. Real world objects such as pearsappear in a variety of forms -- from diced to whole, on a table or in a bowl --yet standard VLM classifiers map all instances of a class to a \it{singlevector based on the class label}. We argue that to represent this richdiversity within a class, zero-shot classification should move beyond a singlevector. We propose a method to encode and account for diversity within a classusing inferred attributes, still in the zero-shot setting without retraining.We find our method consistently outperforms standard zero-shot classificationover a large suite of datasets encompassing hierarchies, diverse object states,and real-world geographic diversity, as well finer-grained datasets whereintra-class diversity may be less prevalent. Importantly, our method isinherently interpretable, offering faithful explanations for each inference tofacilitate model debugging and enhance transparency. We also find our methodscales efficiently to a large number of attributes to account for diversity --leading to more accurate predictions for atypical instances. Finally, wecharacterize a principled trade-off between overall and worst class accuracy,which can be tuned via a hyperparameter of our method. We hope this work spursfurther research into the promise of zero-shot classification beyond a singleclass vector for capturing diversity in the world, and building transparent AIsystems without compromising performance.</description><author>Mazda Moayeri, Michael Rabbat, Mark Ibrahim, Diane Bouchacourt</author><pubDate>Thu, 25 Apr 2024 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16717v1</guid></item><item><title>Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding</title><link>http://arxiv.org/abs/2404.16710v1</link><description>We present LayerSkip, an end-to-end solution to speed-up inference of largelanguage models (LLMs). First, during training we apply layer dropout, with lowdropout rates for earlier layers and higher dropout rates for later layers, andan early exit loss where all transformer layers share the same exit. Second,during inference, we show that this training recipe increases the accuracy ofearly exit at earlier layers, without adding any auxiliary layers or modules tothe model. Third, we present a novel self-speculative decoding solution wherewe exit at early layers and verify and correct with remaining layers of themodel. Our proposed self-speculative decoding approach has less memoryfootprint than other speculative decoding approaches and benefits from sharedcompute and activations of the draft and verification stages. We runexperiments on different Llama model sizes on different types of training:pretraining from scratch, continual pretraining, finetuning on specific datadomain, and finetuning on specific task. We implement our inference solutionand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82xon coding, and 2.0x on TOPv2 semantic parsing task.</description><author>Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu</author><pubDate>Thu, 25 Apr 2024 17:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16710v1</guid></item><item><title>VerAs: Verify then Assess STEM Lab Reports</title><link>http://arxiv.org/abs/2402.05224v2</link><description>With an increasing focus in STEM education on critical thinking skills,science writing plays an ever more important role in curricula that stressinquiry skills. A recently published dataset of two sets of college level labreports from an inquiry-based physics curriculum relies on analytic assessmentrubrics that utilize multiple dimensions, specifying subject matter knowledgeand general components of good explanations. Each analytic dimension isassessed on a 6-point scale, to provide detailed feedback to students that canhelp them improve their science writing skills. Manual assessment can be slow,and difficult to calibrate for consistency across all students in largeclasses. While much work exists on automated assessment of open-ended questionsin STEM subjects, there has been far less work on long-form writing such as labreports. We present an end-to-end neural architecture that has separateverifier and assessment modules, inspired by approaches to Open Domain QuestionAnswering (OpenQA). VerAs first verifies whether a report contains any contentrelevant to a given rubric dimension, and if so, assesses the relevantsentences. On the lab reports, VerAs outperforms multiple baselines based onOpenQA systems or Automated Essay Scoring (AES). VerAs also performs well on ananalytic rubric for middle school physics essays.</description><author>Berk Atil, Mahsa Sheikhi Karizaki, Rebecca J. Passonneau</author><pubDate>Thu, 25 Apr 2024 17:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05224v2</guid></item><item><title>Multi-view Cardiac Image Segmentation via Trans-Dimensional Priors</title><link>http://arxiv.org/abs/2404.16708v1</link><description>We propose a novel multi-stage trans-dimensional architecture for multi-viewcardiac image segmentation. Our method exploits the relationship betweenlong-axis (2D) and short-axis (3D) magnetic resonance (MR) images to perform asequential 3D-to-2D-to-3D segmentation, segmenting the long-axis and short-axisimages. In the first stage, 3D segmentation is performed using the short-axisimage, and the prediction is transformed to the long-axis view and used as asegmentation prior in the next stage. In the second step, the heart region islocalized and cropped around the segmentation prior using a Heart Localizationand Cropping (HLC) module, focusing the subsequent model on the heart region ofthe image, where a 2D segmentation is performed. Similarly, we transform thelong-axis prediction to the short-axis view, localize and crop the heart regionand again perform a 3D segmentation to refine the initial short-axissegmentation. We evaluate our proposed method on the Multi-Disease, Multi-View&amp; Multi-Center Right Ventricular Segmentation in Cardiac MRI (M&amp;Ms-2) dataset,where our method outperforms state-of-the-art methods in segmenting cardiacregions of interest in both short-axis and long-axis images. The pre-trainedmodels, source code, and implementation details will be publicly available.</description><author>Abbas Khan, Muhammad Asad, Martin Benning, Caroline Roney, Gregory Slabaugh</author><pubDate>Thu, 25 Apr 2024 17:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16708v1</guid></item><item><title>Efficient and Near-Optimal Noise Generation for Streaming Differential Privacy</title><link>http://arxiv.org/abs/2404.16706v1</link><description>In the task of differentially private (DP) continual counting, we receive astream of increments and our goal is to output an approximate running total ofthese increments, without revealing too much about any specific increment.Despite its simplicity, differentially private continual counting has attractedsignificant attention both in theory and in practice. Existing algorithms fordifferentially private continual counting are either inefficient in terms oftheir space usage or add an excessive amount of noise, inducing suboptimalutility. The most practical DP continual counting algorithms add carefully correlatedGaussian noise to the values. The task of choosing the covariance for thisnoise can be expressed in terms of factoring the lower-triangular matrix ofones (which computes prefix sums). We present two approaches from this class(for different parameter regimes) that achieve near-optimal utility for DPcontinual counting and only require logarithmic or polylogarithmic space (andtime). Our first approach is based on a space-efficient streaming matrixmultiplication algorithm for a class of Toeplitz matrices. We show that toinstantiate this algorithm for DP continual counting, it is sufficient to finda low-degree rational function that approximates the square root on a circle inthe complex plane. We then apply and extend tools from approximation theory toachieve this. We also derive efficient closed-forms for the objective functionfor arbitrarily many steps, and show direct numerical optimization yields ahighly practical solution to the problem. Our second approach combines ourfirst approach with a recursive construction similar to the binary treemechanism.</description><author>Krishnamurthy, Dvijotham, H. Brendan McMahan, Krishna Pillutla, Thomas Steinke, Abhradeep Thakurta</author><pubDate>Thu, 25 Apr 2024 17:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16706v1</guid></item><item><title>Pix2HDR -- A pixel-wise acquisition and deep learning-based synthesis approach for high-speed HDR videos</title><link>http://arxiv.org/abs/2310.16139v2</link><description>Accurately capturing dynamic scenes with wide-ranging motion and lightintensity is crucial for many vision applications. However, acquiringhigh-speed high dynamic range (HDR) video is challenging because the camera'sframe rate restricts its dynamic range. Existing methods sacrifice speed toacquire multi-exposure frames. Yet, misaligned motion in these frames can stillpose complications for HDR fusion algorithms, resulting in artifacts. Insteadof frame-based exposures, we sample the videos using individual pixels atvarying exposures and phase offsets. Implemented on a monochrome pixel-wiseprogrammable image sensor, our sampling pattern simultaneously captures fastmotion at a high dynamic range. We then transform pixel-wise outputs into anHDR video using end-to-end learned weights from deep neural networks, achievinghigh spatiotemporal resolution with minimized motion blurring. We demonstratealiasing-free HDR video acquisition at 1000 FPS, resolving fast motion underlow-light conditions and against bright backgrounds - both challengingconditions for conventional cameras. By combining the versatility of pixel-wisesampling patterns with the strength of deep neural networks at decoding complexscenes, our method greatly enhances the vision system's adaptability andperformance in dynamic conditions.</description><author>Caixin Wang, Jie Zhang, Matthew A. Wilson, Ralph Etienne-Cummings</author><pubDate>Thu, 25 Apr 2024 17:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16139v2</guid></item><item><title>STaR: Distilling Speech Temporal Relation for Lightweight Speech Self-Supervised Learning Models</title><link>http://arxiv.org/abs/2312.09040v2</link><description>Albeit great performance of Transformer-based speech selfsupervised learning(SSL) models, their large parameter size and computational cost make themunfavorable to utilize. In this study, we propose to compress the speech SSLmodels by distilling speech temporal relation (STaR). Unlike previous worksthat directly match the representation for each speech frame, STaR distillationtransfers temporal relation between speech frames, which is more suitable forlightweight student with limited capacity. We explore three STaR distillationobjectives and select the best combination as the final STaR loss. Our modeldistilled from HuBERT BASE achieves an overall score of 79.8 on SUPERBbenchmark, the best performance among models with up to 27 million parameters.We show that our method is applicable across different speech SSL models andmaintains robust performance with further reduced parameters.</description><author>Kangwook Jang, Sungnyun Kim, Hoirin Kim</author><pubDate>Thu, 25 Apr 2024 17:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09040v2</guid></item><item><title>Toward Neuromic Computing: Neurons as Autoencoders</title><link>http://arxiv.org/abs/2403.02331v4</link><description>This short paper presents the idea that neural backpropagation is usingdendritic processing to enable individual neurons to perform autoencoding.Using a very simple connection weight search heuristic and artificial neuralnetwork model, the effects of interleaving autoencoding for each neuron in ahidden layer of a feedforward network are explored. This is contrasted to thestandard layered approach to autoencoding. It is shown that such individualisedprocessing is not detrimental and can improve network learning.</description><author>Larry Bull</author><pubDate>Thu, 25 Apr 2024 17:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02331v4</guid></item><item><title>Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents</title><link>http://arxiv.org/abs/2404.16698v1</link><description>In the rapidly evolving field of artificial intelligence, ensuring safedecision-making of Large Language Models (LLMs) is a significant challenge.This paper introduces Governance of the Commons Simulation (GovSim), asimulation platform designed to study strategic interactions and cooperativedecision-making in LLMs. Through this simulation environment, we explore thedynamics of resource sharing among AI agents, highlighting the importance ofethical considerations, strategic planning, and negotiation skills. GovSim isversatile and supports any text-based agent, including LLMs agents. Using theGenerative Agent framework, we create a standard agent that facilitates theintegration of different LLMs. Our findings reveal that within GovSim, only twoout of 15 tested LLMs managed to achieve a sustainable outcome, indicating asignificant gap in the ability of models to manage shared resources.Furthermore, we find that by removing the ability of agents to communicate,they overuse the shared resource, highlighting the importance of communicationfor cooperation. Interestingly, most LLMs lack the ability to makeuniversalized hypotheses, which highlights a significant weakness in theirreasoning skills. We open source the full suite of our research results,including the simulation environment, agent prompts, and a comprehensive webinterface.</description><author>Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, Mrinmaya Sachan, Rada Mihalcea</author><pubDate>Thu, 25 Apr 2024 16:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16698v1</guid></item><item><title>Report on Candidate Computational Indicators for Conscious Valenced Experience</title><link>http://arxiv.org/abs/2404.16696v1</link><description>This report enlists 13 functional conditions cashed out in computationalterms that have been argued to be constituent of conscious valenced experience.These are extracted from existing empirical and theoretical literature on,among others, animal sentience, medical disorders, anaesthetics, philosophy,evolution, neuroscience, and artificial intelligence.</description><author>Andres Campero</author><pubDate>Thu, 25 Apr 2024 16:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16696v1</guid></item><item><title>Adaptive Local Binary Pattern: A Novel Feature Descriptor for Enhanced Analysis of Kidney Abnormalities in CT Scan Images using ensemble based Machine Learning Approach</title><link>http://arxiv.org/abs/2404.14560v2</link><description>The shortage of nephrologists and the growing public health concern overrenal failure have spurred the demand for AI systems capable of autonomouslydetecting kidney abnormalities. Renal failure, marked by a gradual decline inkidney function, can result from factors like cysts, stones, and tumors.Chronic kidney disease may go unnoticed initially, leading to untreated casesuntil they reach an advanced stage. The dataset, comprising 12,427 images frommultiple hospitals in Dhaka, was categorized into four groups: cyst, tumor,stone, and normal. Our methodology aims to enhance CT scan image quality usingCropping, Resizing, and CALHE techniques, followed by feature extraction withour proposed Adaptive Local Binary Pattern (A-LBP) feature extraction methodcompared with the state-of-the-art local binary pattern (LBP) method. Ourproposed features fed into classifiers such as Random Forest, Decision Tree,Naive Bayes, K-Nearest Neighbor, and SVM. We explored an ensemble model withsoft voting to get a more robust model for our task. We got the highest of morethan 99% in accuracy using our feature descriptor and ensembling fiveclassifiers (Random Forest, Decision Tree, Naive Bayes, K-Nearest Neighbor,Support Vector Machine) with the soft voting method.</description><author>Tahmim Hossain, Faisal Sayed, Solehin Islam</author><pubDate>Thu, 25 Apr 2024 16:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14560v2</guid></item><item><title>Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4</title><link>http://arxiv.org/abs/2404.16692v1</link><description>We explored the addition bias, a cognitive tendency to prefer adding elementsover removing them to alter an initial state or structure, by conducting fourpreregistered experiments examining the problem-solving behavior of both humansand OpenAl's GPT-4 large language model. The experiments involved 588participants from the U.S. and 680 iterations of the GPT-4 model. Theproblem-solving task was either to create symmetry within a grid (Experiments 1and 3) or to edit a summary (Experiments 2 and 4). As hypothesized, we foundthat overall, the addition bias was present. Solution efficiency (Experiments 1and 2) and valence of the instruction (Experiments 3 and 4) played importantroles. Human participants were less likely to use additive strategies whensubtraction was relatively more efficient than when addition and subtractionwere equally efficient. GPT-4 exhibited the opposite behavior, with a strongaddition bias when subtraction was more efficient. In terms of instructionvalence, GPT-4 was more likely to add words when asked to "improve" compared to"edit", whereas humans did not show this effect. When we looked at the additionbias under different conditions, we found more biased responses for GPT-4compared to humans. Our findings highlight the importance of consideringcomparable and sometimes superior subtractive alternatives, as well asreevaluating one's own and particularly the language models' problem-solvingbehavior.</description><author>Lydia Uhler, Verena Jordan, Jürgen Buder, Markus Huff, Frank Papenmeier</author><pubDate>Thu, 25 Apr 2024 16:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16692v1</guid></item><item><title>Learning to Beat ByteRL: Exploitability of Collectible Card Game Agents</title><link>http://arxiv.org/abs/2404.16689v1</link><description>While Poker, as a family of games, has been studied extensively in the lastdecades, collectible card games have seen relatively little attention. Onlyrecently have we seen an agent that can compete with professional human playersin Hearthstone, one of the most popular collectible card games. Althoughartificial agents must be able to work with imperfect information in both ofthese genres, collectible card games pose another set of distinct challenges.Unlike in many poker variants, agents must deal with state space so vast thateven enumerating all states consistent with the agent's beliefs is intractable,rendering the current search methods unusable and requiring the agents to optfor other techniques. In this paper, we investigate the strength of suchtechniques for this class of games. Namely, we present preliminary analysisresults of ByteRL, the state-of-the-art agent in Legends of Code and Magic andHearthstone. Although ByteRL beat a top-10 Hearthstone player from China, weshow that its play in Legends of Code and Magic is highly exploitable.</description><author>Radovan Haluska, Martin Schmid</author><pubDate>Thu, 25 Apr 2024 16:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16689v1</guid></item><item><title>I3: Intent-Introspective Retrieval Conditioned on Instructions</title><link>http://arxiv.org/abs/2308.10025v2</link><description>Recent studies indicate that dense retrieval models struggle to perform wellon a wide variety of retrieval tasks that lack dedicated training data, asdifferent retrieval tasks often entail distinct search intents. To address thischallenge, in this work we leverage instructions to flexibly describe retrievalintents and introduce I3, a unified retrieval system that performsIntent-Introspective retrieval across various tasks, conditioned onInstructions without any task-specific training. I3 innovatively incorporates apluggable introspector in a parameter-isolated manner to comprehend specificretrieval intents by jointly reasoning over the input query and instruction,and seamlessly integrates the introspected intent into the original retrievalmodel for intent-aware retrieval. Furthermore, we propose progressively-prunedintent learning. It utilizes extensive LLM-generated data to train I3phase-by-phase, embodying two key designs: progressive structure pruning anddrawback extrapolation-based data refinement. Extensive experiments show thatin the BEIR benchmark, I3 significantly outperforms baseline methods designedwith task-specific retrievers, achieving state-of-the-art zero-shot performancewithout any task-specific tuning.</description><author>Kaihang Pan, Juncheng Li, Wenjie Wang, Hao Fei, Hongye Song, Wei Ji, Jun Lin, Xiaozhong Liu, Tat-Seng Chua, Siliang Tang</author><pubDate>Thu, 25 Apr 2024 16:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10025v2</guid></item><item><title>Prompt Cache: Modular Attention Reuse for Low-Latency Inference</title><link>http://arxiv.org/abs/2311.04934v2</link><description>We present Prompt Cache, an approach for accelerating inference for largelanguage models (LLM) by reusing attention states across different LLM prompts.Many input prompts have overlapping text segments, such as system messages,prompt templates, and documents provided for context. Our key insight is thatby precomputing and storing the attention states of these frequently occurringtext segments on the inference server, we can efficiently reuse them when thesesegments appear in user prompts. Prompt Cache employs a schema to explicitlydefine such reusable text segments, called prompt modules. The schema ensurespositional accuracy during attention state reuse and provides users with aninterface to access cached states in their prompt. Using a prototypeimplementation, we evaluate Prompt Cache across several LLMs. We show thatPrompt Cache significantly reduce latency in time-to-first-token, especiallyfor longer prompts such as document-based question answering andrecommendations. The improvements range from 8x for GPU-based inference to 60xfor CPU-based inference, all while maintaining output accuracy and without theneed for model parameter modifications.</description><author>In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, Lin Zhong</author><pubDate>Thu, 25 Apr 2024 16:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04934v2</guid></item><item><title>NTIRE 2024 Quality Assessment of AI-Generated Content Challenge</title><link>http://arxiv.org/abs/2404.16687v1</link><description>This paper reports on the NTIRE 2024 Quality Assessment of AI-GeneratedContent Challenge, which will be held in conjunction with the New Trends inImage Restoration and Enhancement Workshop (NTIRE) at CVPR 2024. This challengeis to address a major challenge in the field of image and video processing,namely, Image Quality Assessment (IQA) and Video Quality Assessment (VQA) forAI-Generated Content (AIGC). The challenge is divided into the image track andthe video track. The image track uses the AIGIQA-20K, which contains 20,000AI-Generated Images (AIGIs) generated by 15 popular generative models. Theimage track has a total of 318 registered participants. A total of 1,646submissions are received in the development phase, and 221 submissions arereceived in the test phase. Finally, 16 participating teams submitted theirmodels and fact sheets. The video track uses the T2VQA-DB, which contains10,000 AI-Generated Videos (AIGVs) generated by 9 popular Text-to-Video (T2V)models. A total of 196 participants have registered in the video track. A totalof 991 submissions are received in the development phase, and 185 submissionsare received in the test phase. Finally, 12 participating teams submitted theirmodels and fact sheets. Some methods have achieved better results than baselinemethods, and the winning methods in both tracks have demonstrated superiorprediction performance on AIGC.</description><author>Xiaohong Liu, Xiongkuo Min, Guangtao Zhai, Chunyi Li, Tengchuan Kou, Wei Sun, Haoning Wu, Yixuan Gao, Yuqin Cao, Zicheng Zhang, Xiele Wu, Radu Timofte</author><pubDate>Thu, 25 Apr 2024 16:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16687v1</guid></item><item><title>Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found Using Counterfactuals As Guides?</title><link>http://arxiv.org/abs/2403.00980v2</link><description>Recently, counterfactuals using "if-only" explanations have become verypopular in eXplainable AI (XAI), as they describe which changes tofeature-inputs of a black-box AI system result in changes to a (usuallynegative) decision-outcome. Even more recently, semi-factuals using "even-if"explanations have gained more attention. They elucidate the feature-inputchanges that do not change the decision-outcome of the AI system, with apotential to suggest more beneficial recourses. Some semi-factual methods usecounterfactuals to the query-instance to guide semi-factual production(so-called counterfactual-guided methods), whereas others do not (so-calledcounterfactual-free methods). In this work, we perform comprehensive tests of 8semi-factual methods on 7 datasets using 5 key metrics, to determine whethercounterfactual guidance is necessary to find the best semi-factuals. Theresults of these tests suggests not, but rather that computing other aspects ofthe decision space lead to better semi-factual XAI.</description><author>Saugat Aryal, Mark T. Keane</author><pubDate>Thu, 25 Apr 2024 16:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00980v2</guid></item><item><title>Multi-scale HSV Color Feature Embedding for High-fidelity NIR-to-RGB Spectrum Translation</title><link>http://arxiv.org/abs/2404.16685v1</link><description>The NIR-to-RGB spectral domain translation is a formidable task due to theinherent spectral mapping ambiguities within NIR inputs and RGB outputs. Thus,existing methods fail to reconcile the tension between maintaining texturedetail fidelity and achieving diverse color variations. In this paper, wepropose a Multi-scale HSV Color Feature Embedding Network (MCFNet) thatdecomposes the mapping process into three sub-tasks, including NIR texturemaintenance, coarse geometry reconstruction, and RGB color prediction. Thus, wepropose three key modules for each corresponding sub-task: the TexturePreserving Block (TPB), the HSV Color Feature Embedding Module (HSV-CFEM), andthe Geometry Reconstruction Module (GRM). These modules contribute to ourMCFNet methodically tackling spectral translation through a series ofescalating resolutions, progressively enriching images with color and texturefidelity in a scale-coherent fashion. The proposed MCFNet demonstratessubstantial performance gains over the NIR image colorization task. Code isreleased at: https://github.com/AlexYangxx/MCFNet.</description><author>Huiyu Zhai, Mo Chen, Xingxing Yang, Gusheng Kang</author><pubDate>Thu, 25 Apr 2024 16:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16685v1</guid></item><item><title>Multimodal Semantic-Aware Automatic Colorization with Diffusion Prior</title><link>http://arxiv.org/abs/2404.16678v1</link><description>Colorizing grayscale images offers an engaging visual experience. Existingautomatic colorization methods often fail to generate satisfactory results dueto incorrect semantic colors and unsaturated colors. In this work, we proposean automatic colorization pipeline to overcome these challenges. We leveragethe extraordinary generative ability of the diffusion prior to synthesize colorwith plausible semantics. To overcome the artifacts introduced by the diffusionprior, we apply the luminance conditional guidance. Moreover, we adoptmultimodal high-level semantic priors to help the model understand the imagecontent and deliver saturated colors. Besides, a luminance-aware decoder isdesigned to restore details and enhance overall visual quality. The proposedpipeline synthesizes saturated colors while maintaining plausible semantics.Experiments indicate that our proposed method considers both diversity andfidelity, surpassing previous methods in terms of perceptual realism and gainmost human preference.</description><author>Han Wang, Xinning Chai, Yiwen Wang, Yuhong Zhang, Rong Xie, Li Song</author><pubDate>Thu, 25 Apr 2024 16:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16678v1</guid></item><item><title>Semantic Positive Pairs for Enhancing Visual Representation Learning of Instance Discrimination methods</title><link>http://arxiv.org/abs/2306.16122v2</link><description>Self-supervised learning algorithms (SSL) based on instance discriminationhave shown promising results, performing competitively or even outperformingsupervised learning counterparts in some downstream tasks. Such approachesemploy data augmentation to create two views of the same instance (i.e.,positive pairs) and encourage the model to learn good representations byattracting these views closer in the embedding space without collapsing to thetrivial solution. However, data augmentation is limited in representingpositive pairs, and the repulsion process between the instances duringcontrastive learning may discard important features for instances that havesimilar categories. To address this issue, we propose an approach to identifythose images with similar semantic content and treat them as positiveinstances, thereby reducing the chance of discarding important features duringrepresentation learning and increasing the richness of the latentrepresentation. Our approach is generic and could work with any self-supervisedinstance discrimination frameworks such as MoCo and SimSiam. To evaluate ourmethod, we run experiments on three benchmark datasets: ImageNet, STL-10 andCIFAR-10 with different instance discrimination SSL approaches. Theexperimental results show that our approach consistently outperforms thebaseline methods across all three datasets; for instance, we improve upon thevanilla MoCo-v2 by 4.1% on ImageNet under a linear evaluation protocol over 800epochs. We also report results on semi-supervised learning, transfer learningon downstream tasks, and object detection.</description><author>Mohammad Alkhalefi, Georgios Leontidis, Mingjun Zhong</author><pubDate>Thu, 25 Apr 2024 16:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16122v2</guid></item><item><title>Data-driven building energy efficiency prediction using physics-informed neural networks</title><link>http://arxiv.org/abs/2311.08035v2</link><description>The analytical prediction of building energy performance in residentialbuildings based on the heat losses of its individual envelope components is achallenging task. It is worth noting that this field is still in its infancy,with relatively limited research conducted in this specific area to date,especially when it comes for data-driven approaches. In this paper we introducea novel physics-informed neural network model for addressing this problem.Through the employment of unexposed datasets that encompass general buildinginformation, audited characteristics, and heating energy consumption, we feedthe deep learning model with general building information, while the model'soutput consists of the structural components and several thermal propertiesthat are in fact the basic elements of an energy performance certificate (EPC).On top of this neural network, a function, based on physics equations,calculates the energy consumption of the building based on heat losses andenhances the loss function of the deep learning model. This methodology istested on a real case study for 256 buildings located in Riga, Latvia. Ourinvestigation comes up with promising results in terms of prediction accuracy,paving the way for automated, and data-driven energy efficiency performanceprediction based on basic properties of the building, contrary to exhaustiveenergy efficiency audits led by humans, which are the current status quo.</description><author>Vasilis Michalakopoulos, Sotiris Pelekis, Giorgos Kormpakis, Vagelis Karakolis, Spiros Mouzakitis, Dimitris Askounis</author><pubDate>Thu, 25 Apr 2024 16:26:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08035v2</guid></item><item><title>NeuraChip: Accelerating GNN Computations with a Hash-based Decoupled Spatial Accelerator</title><link>http://arxiv.org/abs/2404.15510v2</link><description>Graph Neural Networks (GNNs) are emerging as a formidable tool for processingnon-euclidean data across various domains, ranging from social network analysisto bioinformatics. Despite their effectiveness, their adoption has not beenpervasive because of scalability challenges associated with large-scale graphdatasets, particularly when leveraging message passing. To tackle these challenges, we introduce NeuraChip, a novel GNN spatialaccelerator based on Gustavson's algorithm. NeuraChip decouples themultiplication and addition computations in sparse matrix multiplication. Thisseparation allows for independent exploitation of their unique datadependencies, facilitating efficient resource allocation. We introduce arolling eviction strategy to mitigate data idling in on-chip memory as well asaddress the prevalent issue of memory bloat in sparse graph computations.Furthermore, the compute resource load balancing is achieved through a dynamicreseeding hash-based mapping, ensuring uniform utilization of computingresources agnostic of sparsity patterns. Finally, we present NeuraSim, anopen-source, cycle-accurate, multi-threaded, modular simulator forcomprehensive performance analysis. Overall, NeuraChip presents a significant improvement, yielding an averagespeedup of 22.1x over Intel's MKL, 17.1x over NVIDIA's cuSPARSE, 16.7x overAMD's hipSPARSE, and 1.5x over prior state-of-the-art SpGEMM accelerator and1.3x over GNN accelerator. The source code for our open-sourced simulator andperformance visualizer is publicly accessible on GitHub https://neurachip.us</description><author>Kaustubh Shivdikar, Nicolas Bohm Agostini, Malith Jayaweera, Gilbert Jonatan, Jose L. Abellan, Ajay Joshi, John Kim, David Kaeli</author><pubDate>Thu, 25 Apr 2024 16:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15510v2</guid></item><item><title>Multilayer Correlation Clustering</title><link>http://arxiv.org/abs/2404.16676v1</link><description>In this paper, we establish Multilayer Correlation Clustering, a novelgeneralization of Correlation Clustering (Bansal et al., FOCS '02) to themultilayer setting. In this model, we are given a series of inputs ofCorrelation Clustering (called layers) over the common set $V$. The goal isthen to find a clustering of $V$ that minimizes the $\ell_p$-norm ($p\geq 1$)of the disagreements vector, which is defined as the vector (with dimensionequal to the number of layers), each element of which represents thedisagreements of the clustering on the corresponding layer. For thisgeneralization, we first design an $O(L\log n)$-approximation algorithm, where$L$ is the number of layers, based on the well-known region growing technique.We then study an important special case of our problem, namely the problem withthe probability constraint. For this case, we first give an$(\alpha+2)$-approximation algorithm, where $\alpha$ is any possibleapproximation ratio for the single-layer counterpart. For instance, we can take$\alpha=2.5$ in general (Ailon et al., JACM '08) and $\alpha=1.73+\epsilon$ forthe unweighted case (Cohen-Addad et al., FOCS '23). Furthermore, we design a$4$-approximation algorithm, which improves the above approximation ratio of$\alpha+2=4.5$ for the general probability-constraint case. Computationalexperiments using real-world datasets demonstrate the effectiveness of ourproposed algorithms.</description><author>Atsushi Miyauchi, Florian Adriaens, Francesco Bonchi, Nikolaj Tatti</author><pubDate>Thu, 25 Apr 2024 16:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16676v1</guid></item><item><title>Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding</title><link>http://arxiv.org/abs/2306.08832v4</link><description>Vision-Language Models (VLMs), such as CLIP, exhibit strong image-textcomprehension abilities, facilitating advances in several downstream tasks suchas zero-shot image classification, image-text retrieval, and text-to-imagegeneration. However, the compositional reasoning abilities of existing VLMsremains subpar. The root of this limitation lies in the inadequate alignmentbetween the images and captions in the pretraining datasets. Additionally, thecurrent contrastive learning objective fails to focus on fine-grained groundingcomponents like relations, actions, and attributes, resulting in "bag-of-words"representations. We introduce a simple and effective method to improvecompositional reasoning in VLMs. Our method better leverages available datasetsby refining and expanding the standard image-text contrastive learningframework. Our approach does not require specific annotations and does notincur extra parameters. When integrated with CLIP, our technique yields notableimprovement over state-of-the-art baselines across five vision-languagecompositional benchmarks. We open-source our code athttps://github.com/lezhang7/Enhance-FineGrained.</description><author>Le Zhang, Rabiul Awal, Aishwarya Agrawal</author><pubDate>Thu, 25 Apr 2024 16:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08832v4</guid></item><item><title>EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning</title><link>http://arxiv.org/abs/2404.16670v1</link><description>Visual Instruction Tuning represents a novel learning paradigm involving thefine-tuning of pre-trained language models using task-specific instructions.This paradigm shows promising zero-shot results in various natural languageprocessing tasks but is still unexplored in vision emotion understanding. Inthis work, we focus on enhancing the model's proficiency in understanding andadhering to instructions related to emotional contexts. Initially, we identifykey visual clues critical to visual emotion recognition. Subsequently, weintroduce a novel GPT-assisted pipeline for generating emotion visualinstruction data, effectively addressing the scarcity of annotated instructiondata in this domain. Expanding on the groundwork established by InstructBLIP,our proposed EmoVIT architecture incorporates emotion-specific instructiondata, leveraging the powerful capabilities of Large Language Models to enhanceperformance. Through extensive experiments, our model showcases its proficiencyin emotion classification, adeptness in affective reasoning, and competence incomprehending humor. The comparative analysis provides a robust benchmark forEmotion Visual Instruction Tuning in the era of LLMs, providing valuableinsights and opening avenues for future exploration in this domain. Our code isavailable at \url{https://github.com/aimmemotion/EmoVIT}.</description><author>Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, Wen-Huang Cheng</author><pubDate>Thu, 25 Apr 2024 16:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16670v1</guid></item><item><title>PhyRecon: Physically Plausible Neural Scene Reconstruction</title><link>http://arxiv.org/abs/2404.16666v1</link><description>While neural implicit representations have gained popularity in multi-view 3Dreconstruction, previous work struggles to yield physically plausible results,thereby limiting their applications in physics-demanding domains like embodiedAI and robotics. The lack of plausibility originates from both the absence ofphysics modeling in the existing pipeline and their inability to recoverintricate geometrical structures. In this paper, we introduce PhyRecon, whichstands as the first approach to harness both differentiable rendering anddifferentiable physics simulation to learn implicit surface representations.Our framework proposes a novel differentiable particle-based physical simulatorseamlessly integrated with the neural implicit representation. At its core isan efficient transformation between SDF-based implicit representation andexplicit surface points by our proposed algorithm, Surface Points MarchingCubes (SP-MC), enabling differentiable learning with both rendering andphysical losses. Moreover, we model both rendering and physical uncertainty toidentify and compensate for the inconsistent and inaccurate monocular geometricpriors. The physical uncertainty additionally enables a physics-guided pixelsampling to enhance the learning of slender structures. By amalgamating thesetechniques, our model facilitates efficient joint modeling with appearance,geometry, and physics. Extensive experiments demonstrate that PhyReconsignificantly outperforms all state-of-the-art methods in terms ofreconstruction quality. Our reconstruction results also yield superior physicalstability, verified by Isaac Gym, with at least a 40% improvement across alldatasets, opening broader avenues for future physics-based applications.</description><author>Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Yixin Zhu, Song-Chun Zhu, Siyuan Huang</author><pubDate>Thu, 25 Apr 2024 16:06:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16666v1</guid></item><item><title>Lu.i -- A low-cost electronic neuron for education and outreach</title><link>http://arxiv.org/abs/2404.16664v1</link><description>With an increasing presence of science throughout all parts of society, thereis a rising expectation for researchers to effectively communicate their workand, equally, for teachers to discuss contemporary findings in theirclassrooms. While the community can resort to an established set of teachingaids for the fundamental concepts of most natural sciences, there is a need forsimilarly illustrative experiments and demonstrators in neuroscience. Wetherefore introduce Lu.i: a parametrizable electronic implementation of theleaky-integrate-and-fire neuron model in an engaging form factor. Thesepalm-sized neurons can be used to visualize and experience the dynamics ofindividual cells and small spiking neural networks. When stimulated with realor simulated sensory input, Lu.i demonstrates brain-inspired informationprocessing in the hands of a student. As such, it is actively used atworkshops, in classrooms, and for science communication. As a versatile toolfor teaching and outreach, Lu.i nurtures the comprehension of neuroscienceresearch and neuromorphic engineering among future generations of scientistsand in the general public.</description><author>Yannik Stradmann, Julian Göltz, Mihai A. Petrovici, Johannes Schemmel, Sebastian Billaudelle</author><pubDate>Thu, 25 Apr 2024 16:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16664v1</guid></item><item><title>How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments</title><link>http://arxiv.org/abs/2403.11807v2</link><description>Decision-making, a complicated task requiring various types of abilities,presents an excellent framework for assessing Large Language Models (LLMs). Ourresearch investigates LLMs' decision-making capabilities through the lens of awell-established field, Game Theory. We focus specifically on games thatsupport the participation of more than two agents simultaneously. Subsequently,we introduce our framework, GAMA-Bench, including eight classical multi-agentgames. We design a scoring scheme to assess a model's performance in thesegames quantitatively. Through GAMA-Bench, we investigate LLMs' robustness,generalizability, and enhancement strategies. Results reveal that while GPT-3.5shows satisfying robustness, its generalizability is relatively limited.However, its performance can be improved through approaches such asChain-of-Thought. Additionally, we conduct evaluations across various LLMs andfind that GPT-4 outperforms other models on GAMA-Bench, achieving a score of60.5. Moreover, Gemini-1.0-Pro and GPT-3.5 (0613, 1106, 0125) demonstratesimilar intelligence on GAMA-Bench. The code and experimental results are madepublicly available via https://github.com/CUHK-ARISE/GAMABench.</description><author>Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu</author><pubDate>Thu, 25 Apr 2024 16:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11807v2</guid></item><item><title>Formal Specification, Assessment, and Enforcement of Fairness for Generative AIs</title><link>http://arxiv.org/abs/2404.16663v1</link><description>The risk of reinforcing or exacerbating societal biases and inequalities isgrowing as generative AI increasingly produces content that resembles humanoutput, from text to images and beyond. Here we formally characterize thenotion of fairness for generative AI as a basis for monitoring and enforcingfairness. We define two levels of fairness utilizing the concept of infinitewords. The first is the fairness demonstrated on the generated sequences, whichis only evaluated on the outputs while agnostic to the prompts/models used. Thesecond is the inherent fairness of the generative AI model, which requires thatfairness be manifested when input prompts are neutral, that is, they do notexplicitly instruct the generative AI to produce a particular type of output.We also study relative intersectional fairness to counteract the combinatorialexplosion of fairness when considering multiple categories together with lazyfairness enforcement. Our implemented specification monitoring and enforcementtool shows interesting results when tested against several generative AImodels.</description><author>Chih-Hong Cheng, Changshun Wu, Harald Ruess, Xingyu Zhao, Saddek Bensalem</author><pubDate>Thu, 25 Apr 2024 16:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16663v1</guid></item><item><title>Benchmarking Mobile Device Control Agents across Diverse Configurations</title><link>http://arxiv.org/abs/2404.16660v1</link><description>Developing autonomous agents for mobile devices can significantly enhanceuser interactions by offering increased efficiency and accessibility. However,despite the growing interest in mobile device control agents, the absence of acommonly adopted benchmark makes it challenging to quantify scientific progressin this area. In this work, we introduce B-MoCA: a novel benchmark designedspecifically for evaluating mobile device control agents. To create a realisticbenchmark, we develop B-MoCA based on the Android operating system and define60 common daily tasks. Importantly, we incorporate a randomization feature thatchanges various aspects of mobile devices, including user interface layouts andlanguage settings, to assess generalization performance. We benchmark diverseagents, including agents employing large language models (LLMs) or multi-modalLLMs as well as agents trained from scratch using human expert demonstrations.While these agents demonstrate proficiency in executing straightforward tasks,their poor performance on complex tasks highlights significant opportunitiesfor future research to enhance their effectiveness. Our source code is publiclyavailable at https://b-moca.github.io.</description><author>Juyong Lee, Taywon Min, Minyong An, Changyeon Kim, Kimin Lee</author><pubDate>Thu, 25 Apr 2024 15:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16660v1</guid></item><item><title>ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through Probabilistic Threshold Filtering and Error Handling</title><link>http://arxiv.org/abs/2404.16659v1</link><description>Recently, deep learning-based language models have significantly enhancedtext-to-SQL tasks, with promising applications in retrieving patient recordswithin the medical domain. One notable challenge in such applications isdiscerning unanswerable queries. Through fine-tuning model, we demonstrate thefeasibility of converting medical record inquiries into SQL queries.Additionally, we introduce an entropy-based method to identify and filter outunanswerable results. We further enhance result quality by filteringlow-confidence SQL through log probability-based distribution, whilegrammatical and schema errors are mitigated by executing queries on the actualdatabase. We experimentally verified that our method can filter unanswerablequestions, which can be widely utilized even when the parameters of the modelare not accessible, and that it can be effectively utilized in practice.</description><author>Sangryul Kim, Donghee Han, Sehyun Kim</author><pubDate>Thu, 25 Apr 2024 15:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16659v1</guid></item><item><title>A Self-Organizing Clustering System for Unsupervised Distribution Shift Detection</title><link>http://arxiv.org/abs/2404.16656v1</link><description>Modeling non-stationary data is a challenging problem in the field ofcontinual learning, and data distribution shifts may result in negativeconsequences on the performance of a machine learning model. Classic learningtools are often vulnerable to perturbations of the input covariates, and aresensitive to outliers and noise, and some tools are based on rigid algebraicassumptions. Distribution shifts are frequently occurring due to changes in rawmaterials for production, seasonality, a different user base, or evenadversarial attacks. Therefore, there is a need for more effective distributionshift detection techniques. In this work, we propose a continual learning framework for monitoring anddetecting distribution changes. We explore the problem in a latent spacegenerated by a bio-inspired self-organizing clustering and statistical aspectsof the latent space. In particular, we investigate the projections made by twotopology-preserving maps: the Self-Organizing Map and the Scale Invariant Map.Our method can be applied in both a supervised and an unsupervised context. Weconstruct the assessment of changes in the data distribution as a comparison ofGaussian signals, making the proposed method fast and robust. We compare it toother unsupervised techniques, specifically Principal Component Analysis (PCA)and Kernel-PCA. Our comparison involves conducting experiments using sequencesof images (based on MNIST and injected shifts with adversarial samples),chemical sensor measurements, and the environmental variable related to ozonelevels. The empirical study reveals the potential of the proposed approach.</description><author>Sebastián Basterrech, Line Clemmensen, Gerardo Rubino</author><pubDate>Thu, 25 Apr 2024 15:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16656v1</guid></item><item><title>Análise de ambiguidade linguística em modelos de linguagem de grande escala (LLMs)</title><link>http://arxiv.org/abs/2404.16653v1</link><description>Linguistic ambiguity continues to represent a significant challenge fornatural language processing (NLP) systems, notwithstanding the advancements inarchitectures such as Transformers and BERT. Inspired by the recent success ofinstructional models like ChatGPT and Gemini (In 2023, the artificialintelligence was called Bard.), this study aims to analyze and discusslinguistic ambiguity within these models, focusing on three types prevalent inBrazilian Portuguese: semantic, syntactic, and lexical ambiguity. We create acorpus comprising 120 sentences, both ambiguous and unambiguous, forclassification, explanation, and disambiguation. The models capability togenerate ambiguous sentences was also explored by soliciting sets of sentencesfor each type of ambiguity. The results underwent qualitative analysis, drawingon recognized linguistic references, and quantitative assessment based on theaccuracy of the responses obtained. It was evidenced that even the mostsophisticated models, such as ChatGPT and Gemini, exhibit errors anddeficiencies in their responses, with explanations often providinginconsistent. Furthermore, the accuracy peaked at 49.58 percent, indicating theneed for descriptive studies for supervised learning.</description><author>Lavínia de Carvalho Moraes, Irene Cristina Silvério, Rafael Alexandre Sousa Marques, Bianca de Castro Anaia, Dandara Freitas de Paula, Maria Carolina Schincariol de Faria, Iury Cleveston, Alana de Santana Correia, Raquel Meister Ko Freitag</author><pubDate>Thu, 25 Apr 2024 15:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16653v1</guid></item><item><title>Structure in Deep Reinforcement Learning: A Survey and Open Problems</title><link>http://arxiv.org/abs/2306.16021v3</link><description>Reinforcement Learning (RL), bolstered by the expressive capabilities of DeepNeural Networks (DNNs) for function approximation, has demonstratedconsiderable success in numerous applications. However, its practicality inaddressing various real-world scenarios, characterized by diverse andunpredictable dynamics, noisy signals, and large state and action spaces,remains limited. This limitation stems from poor data efficiency, limitedgeneralization capabilities, a lack of safety guarantees, and the absence ofinterpretability, among other factors. To overcome these challenges and improveperformance across these crucial metrics, one promising avenue is toincorporate additional structural information about the problem into the RLlearning process. Various sub-fields of RL have proposed methods forincorporating such inductive biases. We amalgamate these diverse methodologiesunder a unified framework, shedding light on the role of structure in thelearning problem, and classify these methods into distinct patterns ofincorporating structure. By leveraging this comprehensive framework, we providevaluable insights into the challenges of structured RL and lay the groundworkfor a design pattern perspective on RL research. This novel perspective pavesthe way for future advancements and aids in developing more effective andefficient RL algorithms that can potentially handle real-world scenariosbetter.</description><author>Aditya Mohan, Amy Zhang, Marius Lindauer</author><pubDate>Thu, 25 Apr 2024 15:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16021v3</guid></item><item><title>Application of RESNET50 Convolution Neural Network for the Extraction of Optical Parameters in Scattering Media</title><link>http://arxiv.org/abs/2404.16647v1</link><description>Estimation of the optical properties of scattering media such as tissue isimportant in diagnostics as well as in the development of techniques to imagedeeper. As light penetrates the sample scattering events occur that alter thepropagation direction of the photons in a random manner leading degradation ofimage quality. The distribution of the scattered light does, however, give ameasure of the optical properties such as the reduced scattering coefficientand the absorption coefficient. Unfortunately, inverting scattering patterns torecover the optical properties is not simple, especially in the regime wherethe light is partially randomized. Machine learning has been proposed byseveral authors as a means of recovering these properties from either the backscattered or the transmitted light. In the present paper, we train a generalpurpose convolutional neural network RESNET 50 with simulated data based onMonte Carlo simulations. We show that compared with previous work our approachgives comparable or better reconstruction accuracy with training on a muchsmaller dataset. Moreover, by training on multiple parameters such as theintensity distribution at multiple planes or the exit angle and spatialdistribution one achieves improved performance compared to training on a singleinput such as the intensity distribution captured at the sample surface. Whileour approach gives good parameter reconstruction, we identify factors thatlimit the accuracy of the recovered properties, particularly the absorptioncoefficient. In the light of these limitations, we suggest how the presentapproach may be enhanced for even better performance.</description><author>Bowen Deng, Yihan Zhang, Andrew Parkes, Alex Bentley, Amanda Wright, Michael Pound, Michael Somekh</author><pubDate>Thu, 25 Apr 2024 15:36:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16647v1</guid></item><item><title>Tele-FLM Technical Report</title><link>http://arxiv.org/abs/2404.16645v1</link><description>Large language models (LLMs) have showcased profound capabilities in languageunderstanding and generation, facilitating a wide array of applications.However, there is a notable paucity of detailed, open-sourced methodologies onefficiently scaling LLMs beyond 50 billion parameters with minimumtrial-and-error cost and computational resources. In this report, we introduceTele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model thatfeatures a stable, efficient pre-training paradigm and enhanced factualjudgment capabilities. Tele-FLM demonstrates superior multilingual languagemodeling abilities, measured by BPB on textual corpus. Besides, in both Englishand Chinese foundation model evaluation, it is comparable to strongopen-sourced models that involve larger pre-training FLOPs, such as Llama2-70Band DeepSeek-67B. In addition to the model weights, we share the core designs,engineering practices, and training details, which we expect to benefit boththe academic and industrial communities.</description><author>Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Chao Wang, Xinzhang Liu, Zihan Wang, Yu Zhao, Xin Wang, Yuyao Huang, Shuangyong Song, Yongxiang Li, Zheng Zhang, Bo Zhao, Aixin Sun, Yequan Wang, Zhongjiang He, Zhongyuan Wang, Xuelong Li, Tiejun Huang</author><pubDate>Thu, 25 Apr 2024 15:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16645v1</guid></item><item><title>ChemLLM: A Chemical Large Language Model</title><link>http://arxiv.org/abs/2402.06852v2</link><description>Large language models (LLMs) have made impressive progress in chemistryapplications. However, the community lacks an LLM specifically designed forchemistry. The main challenges are two-fold: firstly, most chemical data andscientific knowledge are stored in structured databases, which limits themodel's ability to sustain coherent dialogue when used directly. Secondly,there is an absence of objective and fair benchmark that encompass mostchemistry tasks. Here, we introduce ChemLLM, a comprehensive framework thatfeatures the first LLM dedicated to chemistry. It also includes ChemData, adataset specifically designed for instruction tuning, and ChemBench, a robustbenchmark covering nine essential chemistry tasks. ChemLLM is adept atperforming various tasks across chemical disciplines with fluid dialogueinteraction. Notably, ChemLLM achieves results comparable to GPT-4 on the corechemical tasks and demonstrates competitive performance with LLMs of similarsize in general scenarios. ChemLLM paves a new path for exploration in chemicalstudies, and our method of incorporating structured chemical knowledge intodialogue systems sets a new standard for developing LLMs in various scientificfields. Codes, Datasets, and Model weights are publicly accessible athttps://hf.co/AI4Chem</description><author>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli Ouyang, Dongzhan Zhou, Shufei Zhang, Mao Su, Han-Sen Zhong, Yuqiang Li</author><pubDate>Thu, 25 Apr 2024 15:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06852v2</guid></item><item><title>Privacy-Preserving Statistical Data Generation: Application to Sepsis Detection</title><link>http://arxiv.org/abs/2404.16638v1</link><description>The biomedical field is among the sectors most impacted by the increasingregulation of Artificial Intelligence (AI) and data protection legislation,given the sensitivity of patient information. However, the rise of syntheticdata generation methods offers a promising opportunity for data-driventechnologies. In this study, we propose a statistical approach for syntheticdata generation applicable in classification problems. We assess the utilityand privacy implications of synthetic data generated by Kernel DensityEstimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-worldcontext, specifically focusing on its application in sepsis detection. Thedetection of sepsis is a critical challenge in clinical practice due to itsrapid progression and potentially life-threatening consequences. Moreover, weemphasize the benefits of KDE-KNN compared to current synthetic data generationmethodologies. Additionally, our study examines the effects of incorporatingsynthetic data into model training procedures. This investigation providesvaluable insights into the effectiveness of synthetic data generationtechniques in mitigating regulatory constraints within the biomedical field.</description><author>Eric Macias-Fassio, Aythami Morales, Cristina Pruenza, Julian Fierrez</author><pubDate>Thu, 25 Apr 2024 15:26:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16638v1</guid></item><item><title>Zero-Shot Distillation for Image Encoders: How to Make Effective Use of Synthetic Data</title><link>http://arxiv.org/abs/2404.16637v1</link><description>Multi-modal foundation models such as CLIP have showcased impressivezero-shot capabilities. However, their applicability in resource-constrainedenvironments is limited due to their large number of parameters and highinference time. While existing approaches have scaled down the entire CLIParchitecture, we focus on training smaller variants of the image encoder, whichsuffices for efficient zero-shot classification. The use of synthetic data hasshown promise in distilling representations from larger teachers, resulting instrong few-shot and linear probe performance. However, we find that thisapproach surprisingly fails in true zero-shot settings when using contrastivelosses. We identify the exploitation of spurious features as being responsiblefor poor generalization between synthetic and real data. However, by using theimage feature-based L2 distillation loss, we mitigate these problems and trainstudents that achieve zero-shot performance which on four domain-specificdatasets is on-par with a ViT-B/32 teacher model trained on DataCompXL, whilefeaturing up to 92% fewer parameters.</description><author>Niclas Popp, Jan Hendrik Metzen, Matthias Hein</author><pubDate>Thu, 25 Apr 2024 15:24:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16637v1</guid></item><item><title>TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning</title><link>http://arxiv.org/abs/2404.16635v1</link><description>Charts are important for presenting and explaining complex datarelationships. Recently, multimodal large language models (MLLMs) have shownremarkable capabilities in various chart understanding tasks. However, thesheer size of these models in terms of parameters and computationalrequirements limits their use in resource-constrained environments. In thispaper, we present TinyChart, an efficient MLLM for chart understanding withonly 3B parameters. TinyChart overcomes two key challenges in efficient chartunderstanding: (1) reduce the burden of learning numerical computations througha Program-of-Thoughts (PoT) learning strategy, which trains the model togenerate Python programs for numerical calculations, and (2) reduce lengthyvision feature sequences produced by the vision transformer for high-resolutionimages through a Vision Token Merging module, which gradually merges mostsimilar vision tokens. Extensive experiments demonstrate that our 3B TinyChartachieves SOTA performance on a variety of chart understanding benchmarksincluding ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. Itoutperforms several chart understanding MLLM with up to 13B parameters such asChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V onChartQA. It also demonstrates its superior efficiency with higher throughputduring inference due to a smaller model scale and more efficient visionencoding. Our code and model are available athttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.</description><author>Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, Fei Huang</author><pubDate>Thu, 25 Apr 2024 15:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16635v1</guid></item><item><title>Self-Balanced R-CNN for Instance Segmentation</title><link>http://arxiv.org/abs/2404.16633v1</link><description>Current state-of-the-art two-stage models on instance segmentation tasksuffer from several types of imbalances. In this paper, we address theIntersection over the Union (IoU) distribution imbalance of positive inputRegions of Interest (RoIs) during the training of the second stage. OurSelf-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade(HTC) model, brings brand new loop mechanisms of bounding box and maskrefinements. With an improved Generic RoI Extraction (GRoIE), we also addressthe feature-level imbalance at the Feature Pyramid Network (FPN) level,originated by a non-uniform integration between low- and high-level featuresfrom the backbone layers. In addition, the redesign of the architecture headstoward a fully convolutional approach with FCC further reduces the number ofparameters and obtains more clues to the connection between the task to solveand the layers used. Moreover, our SBR-CNN model shows the same or even betterimprovements if adopted in conjunction with other state-of-the-art models. Infact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017dataset, our model reaches 45.3% and 41.5% AP for object detection and instancesegmentation, with 12 epochs and without extra tricks. The code is available athttps://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn</description><author>Leonardo Rossi, Akbar Karimi, Andrea Prati</author><pubDate>Thu, 25 Apr 2024 15:22:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16633v1</guid></item><item><title>Legal Aspects for Software Developers Interested in Generative AI Applications</title><link>http://arxiv.org/abs/2404.16630v1</link><description>Recent successes in Generative Artificial Intelligence (GenAI) have led tonew technologies capable of generating high-quality code, natural language, andimages. The next step is to integrate GenAI technology into products, a tasktypically conducted by software developers. Such product development alwayscomes with a certain risk of liability. Within this article, we want to shedlight on the current state of two such risks: data protection and copyright.Both aspects are crucial for GenAI. This technology deals with data for bothmodel training and generated output. We summarize key aspects regarding ourcurrent knowledge that every software developer involved in product developmentusing GenAI should be aware of to avoid critical mistakes that may expose themto liability claims.</description><author>Steffen Herbold, Brian Valerius, Anamaria Mojica-Hanke, Isabella Lex, Joel Mittel</author><pubDate>Thu, 25 Apr 2024 15:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16630v1</guid></item><item><title>Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer</title><link>http://arxiv.org/abs/2404.16627v1</link><description>Unsupervised cross-lingual transfer involves transferring knowledge betweenlanguages without explicit supervision. Although numerous studies have beenconducted to improve performance in such tasks by focusing on cross-lingualknowledge, particularly lexical and syntactic knowledge, current approaches arelimited as they only incorporate syntactic or lexical information. Since eachtype of information offers unique advantages and no previous attempts havecombined both, we attempt to explore the potential of this approach. In thispaper, we present a novel framework called "Lexicon-Syntax EnhancedMultilingual BERT" that combines both lexical and syntactic knowledge.Specifically, we use Multilingual BERT (mBERT) as the base model and employ twotechniques to enhance its learning capabilities. The code-switching techniqueis used to implicitly teach the model lexical alignment information, while asyntactic-based graph attention network is designed to help the model encodesyntactic structure. To integrate both types of knowledge, we inputcode-switched sequences into both the syntactic module and the mBERT base modelsimultaneously. Our extensive experimental results demonstrate this frameworkcan consistently outperform all baselines of zero-shot cross-lingual transfer,with the gains of 1.0~3.7 points on text classification, named entityrecognition (ner), and semantic parsing tasks. Keywords:cross-lingual transfer,lexicon, syntax, code-switching, graph attention network</description><author>Jianyu Zheng, Fengfei Fan, Jianquan Li</author><pubDate>Thu, 25 Apr 2024 15:10:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16627v1</guid></item><item><title>DAVE -- A Detect-and-Verify Paradigm for Low-Shot Counting</title><link>http://arxiv.org/abs/2404.16622v1</link><description>Low-shot counters estimate the number of objects corresponding to a selectedcategory, based on only few or no exemplars annotated in the image. The currentstate-of-the-art estimates the total counts as the sum over the object locationdensity map, but does not provide individual object locations and sizes, whichare crucial for many applications. This is addressed by detection-basedcounters, which, however fall behind in the total count accuracy. Furthermore,both approaches tend to overestimate the counts in the presence of other objectclasses due to many false positives. We propose DAVE, a low-shot counter basedon a detect-and-verify paradigm, that avoids the aforementioned issues by firstgenerating a high-recall detection set and then verifying the detections toidentify and remove the outliers. This jointly increases the recall andprecision, leading to accurate counts. DAVE outperforms the top density-basedcounters by ~20% in the total count MAE, it outperforms the most recentdetection-based counter by ~20% in detection quality and sets a newstate-of-the-art in zero-shot as well as text-prompt-based counting.</description><author>Jer Pelhan, Alan Lukežič, Vitjan Zavrtanik, Matej Kristan</author><pubDate>Thu, 25 Apr 2024 15:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16622v1</guid></item><item><title>Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare</title><link>http://arxiv.org/abs/2404.16621v1</link><description>The integration of Large Language Models (LLMs) into healthcare promises totransform medical diagnostics, research, and patient care. Yet, the progressionof medical LLMs faces obstacles such as complex training requirements, rigorousevaluation demands, and the dominance of proprietary models that restrictacademic exploration. Transparent, comprehensive access to LLM resources isessential for advancing the field, fostering reproducibility, and encouraginginnovation in healthcare AI. We present Hippocrates, an open-source LLMframework specifically developed for the medical domain. In stark contrast toprevious efforts, it offers unrestricted access to its training datasets,codebase, checkpoints, and evaluation protocols. This open approach is designedto stimulate collaborative research, allowing the community to build upon,refine, and rigorously evaluate medical LLMs within a transparent ecosystem.Also, we introduce Hippo, a family of 7B models tailored for the medicaldomain, fine-tuned from Mistral and LLaMA2 through continual pre-training,instruction tuning, and reinforcement learning from human and AI feedback. Ourmodels outperform existing open medical LLMs models by a large-margin, evensurpassing models with 70B parameters. Through Hippocrates, we aspire to unlockthe full potential of LLMs not just to advance medical knowledge and patientcare but also to democratize the benefits of AI research in healthcare, makingthem available across the globe.</description><author>Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, Erkut Erdem</author><pubDate>Thu, 25 Apr 2024 15:06:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16621v1</guid></item><item><title>Benchmarking LLMs via Uncertainty Quantification</title><link>http://arxiv.org/abs/2401.12794v2</link><description>The proliferation of open-source Large Language Models (LLMs) from variousinstitutions has highlighted the urgent need for comprehensive evaluationmethods. However, current evaluation platforms, such as the widely recognizedHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,which is vital for thoroughly assessing LLMs. To bridge this gap, we introducea new benchmarking approach for LLMs that integrates uncertaintyquantification. Our examination involves eight LLMs (LLM series) spanning fiverepresentative natural language processing tasks. Our findings reveal that: I)LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMsmay display greater uncertainty compared to their smaller counterparts; andIII) Instruction-finetuning tends to increase the uncertainty of LLMs. Theseresults underscore the significance of incorporating uncertainty in theevaluation of LLMs.</description><author>Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu</author><pubDate>Thu, 25 Apr 2024 15:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12794v2</guid></item><item><title>Denoising: from classical methods to deep CNNs</title><link>http://arxiv.org/abs/2404.16617v1</link><description>This paper aims to explore the evolution of image denoising in apedagological way. We briefly review classical methods such as Fourier analysisand wavelet bases, highlighting the challenges they faced until the emergenceof neural networks, notably the U-Net, in the 2010s. The remarkable performanceof these networks has been demonstrated in studies such as Kadkhodaie et al.(2024). They exhibit adaptability to various image types, including those withfixed regularity, facial images, and bedroom scenes, achieving optimal resultsand biased towards geometry-adaptive harmonic basis. The introduction of scorediffusion has played a crucial role in image generation. In this context,denoising becomes essential as it facilitates the estimation of probabilitydensity scores. We discuss the prerequisites for genuine learning ofprobability densities, offering insights that extend from mathematical researchto the implications of universal structures.</description><author>Jean-Eric Campagne</author><pubDate>Thu, 25 Apr 2024 14:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16617v1</guid></item><item><title>Robust Capped lp-Norm Support Vector Ordinal Regression</title><link>http://arxiv.org/abs/2404.16616v1</link><description>Ordinal regression is a specialized supervised problem where the labels showan inherent order. The order distinguishes it from normal multi-class problem.Support Vector Ordinal Regression, as an outstanding ordinal regression model,is widely used in many ordinal regression tasks. However, like most supervisedlearning algorithms, the design of SVOR is based on the assumption that thetraining data are real and reliable, which is difficult to satisfy inreal-world data. In many practical applications, outliers are frequentlypresent in the training set, potentially leading to misguide the learningprocess, such that the performance is non-optimal. In this paper, we propose anovel capped $\ell_{p}$-norm loss function that is theoretically robust to bothlight and heavy outliers. The capped $\ell_{p}$-norm loss can help the modeldetect and eliminate outliers during training process. Adhering to thisconcept, we introduce a new model, Capped $\ell_{p}$-Norm Support VectorOrdinal Regression(CSVOR), that is robust to outliers. CSVOR uses a weightmatrix to detect and eliminate outliers during the training process to improvethe robustness to outliers. Moreover, a Re-Weighted algorithm algorithm whichis illustrated convergence by our theoretical results is proposed toeffectively minimize the corresponding problem. Extensive experimental resultsdemonstrate that our model outperforms state-of-the-art(SOTA) methods,particularly in the presence of outliers.</description><author>Haorui Xiang, Zhichang Wu, Guoxu Li, Rong Wang, Feiping Nie, Xuelong Li</author><pubDate>Thu, 25 Apr 2024 14:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16616v1</guid></item><item><title>Dropout Regularization Versus $\ell_2$-Penalization in the Linear Model</title><link>http://arxiv.org/abs/2306.10529v2</link><description>We investigate the statistical behavior of gradient descent iterates withdropout in the linear regression model. In particular, non-asymptotic boundsfor the convergence of expectations and covariance matrices of the iterates arederived. The results shed more light on the widely cited connection betweendropout and l2-regularization in the linear model. We indicate a more subtlerelationship, owing to interactions between the gradient descent dynamics andthe additional randomness induced by dropout. Further, we study a simplifiedvariant of dropout which does not have a regularizing effect and converges tothe least squares estimator</description><author>Gabriel Clara, Sophie Langer, Johannes Schmidt-Hieber</author><pubDate>Thu, 25 Apr 2024 14:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10529v2</guid></item><item><title>MuseumMaker: Continual Style Customization without Catastrophic Forgetting</title><link>http://arxiv.org/abs/2404.16612v1</link><description>Pre-trained large text-to-image (T2I) models with an appropriate text prompthas attracted growing interests in customized images generation field. However,catastrophic forgetting issue make it hard to continually synthesize newuser-provided styles while retaining the satisfying results amongst learnedstyles. In this paper, we propose MuseumMaker, a method that enables thesynthesis of images by following a set of customized styles in a never-endmanner, and gradually accumulate these creative artistic works as a Museum.When facing with a new customization style, we develop a style distillationloss module to transfer the style of the whole dataset into generation ofimages. It can minimize the learning biases caused by content of images, andaddress the catastrophic overfitting issue induced by few-shot images. To dealwith catastrophic forgetting amongst past learned styles, we devise a dualregularization for shared-LoRA module to optimize the direction of modelupdate, which could regularize the diffusion model from both weight and featureaspects, respectively. Meanwhile, a unique token embedding corresponding tothis new style is learned by a task-wise token learning module, which couldpreserve historical knowledge from past styles with the limitation of LoRAparameter quantity. As any new user-provided style come, our MuseumMaker cancapture the nuances of the new styles while maintaining the details of learnedstyles. Experimental results on diverse style datasets validate theeffectiveness of our proposed MuseumMaker method, showcasing its robustness andversatility across various scenarios.</description><author>Chenxi Liu, Gan Sun, Wenqi Liang, Jiahua Dong, Can Qin, Yang Cong</author><pubDate>Thu, 25 Apr 2024 14:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16612v1</guid></item><item><title>Two-Stage Aggregation with Dynamic Local Attention for Irregular Time Series</title><link>http://arxiv.org/abs/2311.07744v2</link><description>Irregular multivariate time series data is characterized by varying timeintervals between consecutive observations of measured variables/signals (i.e.,features) and varying sampling rates (i.e., recordings/measurement) acrossthese features. Modeling time series while taking into account theseirregularities is still a challenging task for machine learning methods. Here,we introduce TADA, a Two-stageAggregation process with Dynamic local Attentionto harmonize time-wise and feature-wise irregularities in multivariate timeseries. In the first stage, the irregular time series undergoes temporalembedding (TE) using all available features at each time step. This processpreserves the contribution of each available feature and generates afixed-dimensional representation per time step. The second stage introduces adynamic local attention (DLA) mechanism with adaptive window sizes. DLAaggregates time recordings using feature-specific windows to harmonizeirregular time intervals capturing feature-specific sampling rates. Thenhierarchical MLP mixer layers process the output of DLA through multiscalepatching to leverage information at various scales for the downstream tasks.TADA outperforms state-of-the-art methods on three real-world datasets,including the latest MIMIC IV dataset, and highlights its effectiveness inhandling irregular multivariate time series and its potential for variousreal-world applications.</description><author>Xingyu Chen, Xiaochen Zheng, Amina Mollaysa, Manuel Schürch, Ahmed Allam, Michael Krauthammer</author><pubDate>Thu, 25 Apr 2024 14:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07744v2</guid></item><item><title>Conformalized Ordinal Classification with Marginal and Conditional Coverage</title><link>http://arxiv.org/abs/2404.16610v1</link><description>Conformal prediction is a general distribution-free approach for constructingprediction sets combined with any machine learning algorithm that achieve validmarginal or conditional coverage in finite samples. Ordinal classification iscommon in real applications where the target variable has natural orderingamong the class labels. In this paper, we discuss constructingdistribution-free prediction sets for such ordinal classification problems byleveraging the ideas of conformal prediction and multiple testing with FWERcontrol. Newer conformal prediction methods are developed for constructingcontiguous and non-contiguous prediction sets based on marginal and conditional(class-specific) conformal $p$-values, respectively. Theoretically, we provethat the proposed methods respectively achieve satisfactory levels of marginaland class-specific conditional coverages. Through simulation study and realdata analysis, these proposed methods show promising performance compared tothe existing conformal method.</description><author>Subhrasish Chakraborty, Chhavi Tyagi, Haiyan Qiao, Wenge Guo</author><pubDate>Thu, 25 Apr 2024 14:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16610v1</guid></item></channel></rss>