<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 04 Dec 2023 06:00:19 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Dense Optical Tracking: Connecting the Dots</title><link>http://arxiv.org/abs/2312.00786v1</link><description>Recent approaches to point tracking are able to recover the trajectory of anyscene point through a large portion of a video despite the presence ofocclusions. They are, however, too slow in practice to track every pointobserved in a single frame in a reasonable amount of time. This paperintroduces DOT, a novel, simple and efficient method for solving this problem.It first extracts a small set of tracks from key regions at motion boundariesusing an off-the-shelf point tracking algorithm. Given source and targetframes, DOT then computes rough initial estimates of a dense flow field andvisibility mask through nearest-neighbor interpolation, before refining themusing a learnable optical flow estimator that explicitly handles occlusions andcan be trained on synthetic data with ground-truth correspondences. We showthat DOT is significantly more accurate than current optical flow techniques,outperforms sophisticated "universal" trackers like OmniMotion, and is on parwith, or better than, the best point tracking algorithms like CoTracker whilebeing at least two orders of magnitude faster. Quantitative and qualitativeexperiments with synthetic and real videos validate the promise of the proposedapproach. Code, data, and videos showcasing the capabilities of our approachare available in the project webpage: https://16lemoing.github.io/dot .</description><author>Guillaume Le Moing, Jean Ponce, Cordelia Schmid</author><pubDate>Fri, 01 Dec 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00786v1</guid></item><item><title>Sequential Modeling Enables Scalable Learning for Large Vision Models</title><link>http://arxiv.org/abs/2312.00785v1</link><description>We introduce a novel sequential modeling approach which enables learning aLarge Vision Model (LVM) without making use of any linguistic data. To do this,we define a common format, "visual sentences", in which we can represent rawimages and videos as well as annotated data sources such as semanticsegmentations and depth reconstructions without needing any meta-knowledgebeyond the pixels. Once this wide variety of visual data (comprising 420billion tokens) is represented as sequences, the model can be trained tominimize a cross-entropy loss for next token prediction. By training acrossvarious scales of model architecture and data diversity, we provide empiricalevidence that our models scale effectively. Many different vision tasks can besolved by designing suitable visual prompts at test time.</description><author>Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, Alexei A Efros</author><pubDate>Fri, 01 Dec 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00785v1</guid></item><item><title>Making Large Multimodal Models Understand Arbitrary Visual Prompts</title><link>http://arxiv.org/abs/2312.00784v1</link><description>While existing large vision-language multimodal models focus on whole imageunderstanding, there is a prominent gap in achieving region-specificcomprehension. Current approaches that use textual coordinates or spatialencodings often fail to provide a user-friendly interface for visual prompting.To address this challenge, we introduce a novel multimodal model capable ofdecoding arbitrary visual prompts. This allows users to intuitively mark imagesand interact with the model using natural cues like a "red bounding box" or"pointed arrow". Our simple design directly overlays visual markers onto theRGB image, eliminating the need for complex region encodings, yet achievesstate-of-the-art performance on region-understanding tasks like Visual7W,PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we presentViP-Bench, a comprehensive benchmark to assess the capability of models inunderstanding visual prompts across multiple dimensions, enabling futureresearch in this domain. Code, data, and model are publicly available.</description><author>Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee</author><pubDate>Fri, 01 Dec 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00784v1</guid></item><item><title>Topological properties of basins of attraction and expressiveness of width bounded neural networks</title><link>http://arxiv.org/abs/2011.04923v6</link><description>In Radhakrishnan et al. [2020], the authors empirically show thatautoencoders trained with usual SGD methods shape out basins of attractionaround their training data. We consider network functions of width notexceeding the input dimension and prove that in this situation basins ofattraction are bounded and their complement cannot have bounded components. Ourconditions in these results are met in several experiments of the latter workand we thus address a question posed therein. We also show that under some morerestrictive conditions the basins of attraction are path-connected. Thetightness of the conditions in our results is demonstrated by means of severalexamples. Finally, the arguments used to prove the above results allow us toderive a root cause why scalar-valued neural network functions that fulfill ourbounded width condition are not dense in spaces of continuous functions.</description><author>Hans-Peter Beise, Steve Dias Da Cruz</author><pubDate>Fri, 01 Dec 2023 18:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.04923v6</guid></item><item><title>Diagnosing Human-object Interaction Detectors</title><link>http://arxiv.org/abs/2308.08529v2</link><description>We have witnessed significant progress in human-object interaction (HOI)detection. The reliance on mAP (mean Average Precision) scores as a summarymetric, however, does not provide sufficient insight into the nuances of modelperformance (e.g., why one model is better than another), which can hinderfurther innovation in this field. To address this issue, in this paper, weintroduce a diagnosis toolbox to provide detailed quantitative break-downanalysis of HOI detection models, inspired by the success of object detectiondiagnosis toolboxes. We first conduct holistic investigations in the pipelineof HOI detection. By defining a set of errors and the oracles to fix each ofthem, we can have a quantitative analysis of the significance of differenterrors according to the mAP improvement obtained from fixing each error. Wethen delve into two sub-tasks of HOI detection: human-object pair detection andinteraction classification, respectively. For the first detection task, wecompute the coverage of ground-truth human-object pairs as well as thenoisiness level in the detection results. For the second classification task,we measure a model's performance of differentiating positive and negativedetection results and also classifying the actual interactions when thehuman-object pairs are correctly detected. We analyze eight state-of-the-artHOI detection models and provide valuable diagnosis insights to foster futureresearch. For instance, our diagnosis shows that state-of-the-art model RLIPv2outperforms others mainly because it significantly improves the multi-labelinteraction classification accuracy. Our toolbox is applicable for differentmethods across different datasets and available athttps://github.com/neu-vi/Diag-HOI.</description><author>Fangrui Zhu, Yiming Xie, Weidi Xie, Huaizu Jiang</author><pubDate>Fri, 01 Dec 2023 18:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08529v2</guid></item><item><title>MorpheuS: Neural Dynamic 360Â° Surface Reconstruction from Monocular RGB-D Video</title><link>http://arxiv.org/abs/2312.00778v1</link><description>Neural rendering has demonstrated remarkable success in dynamic scenereconstruction. Thanks to the expressiveness of neural representations, priorworks can accurately capture the motion and achieve high-fidelityreconstruction of the target object. Despite this, real-world video scenariosoften feature large unobserved regions where neural representations struggle toachieve realistic completion. To tackle this challenge, we introduce MorpheuS,a framework for dynamic 360{\deg} surface reconstruction from a casuallycaptured RGB-D video. Our approach models the target scene as a canonical fieldthat encodes its geometry and appearance, in conjunction with a deformationfield that warps points from the current frame to the canonical space. Weleverage a view-dependent diffusion prior and distill knowledge from it toachieve realistic completion of unobserved regions. Experimental results onvarious real-world and synthetic datasets show that our method can achievehigh-fidelity 360{\deg} surface reconstruction of a deformable object from amonocular RGB-D video.</description><author>Hengyi Wang, Jingwen Wang, Lourdes Agapito</author><pubDate>Fri, 01 Dec 2023 18:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00778v1</guid></item><item><title>VideoBooth: Diffusion-based Video Generation with Image Prompts</title><link>http://arxiv.org/abs/2312.00777v1</link><description>Text-driven video generation witnesses rapid progress. However, merely usingtext prompts is not enough to depict the desired subject appearance thataccurately aligns with users' intents, especially for customized contentcreation. In this paper, we study the task of video generation with imageprompts, which provide more accurate and direct content control beyond the textprompts. Specifically, we propose a feed-forward framework VideoBooth, with twodedicated designs: 1) We propose to embed image prompts in a coarse-to-finemanner. Coarse visual embeddings from image encoder provide high-levelencodings of image prompts, while fine visual embeddings from the proposedattention injection module provide multi-scale and detailed encoding of imageprompts. These two complementary embeddings can faithfully capture the desiredappearance. 2) In the attention injection module at fine level, multi-scaleimage prompts are fed into different cross-frame attention layers as additionalkeys and values. This extra spatial information refines the details in thefirst frame and then it is propagated to the remaining frames, which maintainstemporal consistency. Extensive experiments demonstrate that VideoBoothachieves state-of-the-art performance in generating customized high-qualityvideos with subjects specified in image prompts. Notably, VideoBooth is ageneralizable framework where a single model works for a wide range of imageprompts with feed-forward pass.</description><author>Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, Ziwei Liu</author><pubDate>Fri, 01 Dec 2023 18:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00777v1</guid></item><item><title>Towards Generalizable Zero-Shot Manipulation via Translating Human Interaction Plans</title><link>http://arxiv.org/abs/2312.00775v1</link><description>We pursue the goal of developing robots that can interact zero-shot withgeneric unseen objects via a diverse repertoire of manipulation skills and showhow passive human videos can serve as a rich source of data for learning suchgeneralist robots. Unlike typical robot learning approaches which directlylearn how a robot should act from interaction data, we adopt a factorizedapproach that can leverage large-scale human videos to learn how a human wouldaccomplish a desired task (a human plan), followed by translating this plan tothe robots embodiment. Specifically, we learn a human plan predictor that,given a current image of a scene and a goal image, predicts the future hand andobject configurations. We combine this with a translation module that learns aplan-conditioned robot manipulation policy, and allows following humans plansfor generic manipulation tasks in a zero-shot manner with no deployment-timetraining. Importantly, while the plan predictor can leverage large-scale humanvideos for learning, the translation module only requires a small amount ofin-domain data, and can generalize to tasks not seen during training. We showthat our learned system can perform over 16 manipulation skills that generalizeto 40 objects, encompassing 100 real-world tasks for table-top manipulation anddiverse in-the-wild manipulation. https://homangab.github.io/hopman/</description><author>Homanga Bharadhwaj, Abhinav Gupta, Vikash Kumar, Shubham Tulsiani</author><pubDate>Fri, 01 Dec 2023 18:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00775v1</guid></item><item><title>Context Retrieval via Normalized Contextual Latent Interaction for Conversational Agent</title><link>http://arxiv.org/abs/2312.00774v1</link><description>Conversational agents leveraging AI, particularly deep learning, are emergingin both academic research and real-world applications. However, theseapplications still face challenges, including disrespecting knowledge andfacts, not personalizing to user preferences, and enormous demand forcomputational resources during training and inference. Recent research effortshave been focused on addressing these challenges from various aspects,including supplementing various types of auxiliary information to theconversational agents. However, existing methods are still not able toeffectively and efficiently exploit relevant information from these auxiliarysupplements to further unleash the power of the conversational agents and thelanguage models they use. In this paper, we present a novel method, PK-NCLI,that is able to accurately and efficiently identify relevant auxiliaryinformation to improve the quality of conversational responses by learning therelevance among persona, chat history, and knowledge background throughlow-level normalized contextual latent interaction. Our experimental resultsindicate that PK-NCLI outperforms the state-of-the-art method, PK-FoCus, by47.80%/30.61%/24.14% in terms of perplexity, knowledge grounding, and trainingefficiency, respectively, and maintained the same level of persona groundingperformance. We also provide a detailed analysis of how different factors,including language model choices and trade-offs on training weights, wouldaffect the performance of PK-NCLI.</description><author>Junfeng Liu, Zhuocheng Mei, Kewen Peng, Ranga Raju Vatsavai</author><pubDate>Fri, 01 Dec 2023 18:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00774v1</guid></item><item><title>Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring</title><link>http://arxiv.org/abs/2309.16770v2</link><description>Recent advances in machine learning and deep learning have led to thewidespread use of Conversational AI in many practical applications. However, itis still very challenging to leverage auxiliary information that can provideconversational context or personalized tuning to improve the quality ofconversations. For example, there has only been limited research on using anindividuals persona information to improve conversation quality, and evenstate-of-the-art conversational AI techniques are unable to effectivelyleverage signals from heterogeneous sources of auxiliary data, such asmulti-modal interaction data, demographics, SDOH data, etc. In this paper, wepresent a novel Persona-Coded Poly-Encoder method that leverages personainformation in a multi-stream encoding scheme to improve the quality ofresponse generation for conversations. To show the efficacy of the proposedmethod, we evaluate our method on two different persona-based conversationaldatasets, and compared against two state-of-the-art methods. Our experimentalresults and analysis demonstrate that our method can improve conversationquality over the baseline method Poly-Encoder by 3.32% and 2.94% in terms ofBLEU score and HR@1, respectively. More significantly, our method offers a pathto better utilization of multi-modal data in conversational tasks. Lastly, ourstudy outlines several challenges and future research directions for advancingpersonalized conversational AI technology.</description><author>Junfeng Liu, Christopher Symons, Ranga Raju Vatsavai</author><pubDate>Fri, 01 Dec 2023 18:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16770v2</guid></item><item><title>Automated Material Properties Extraction For Enhanced Beauty Product Discovery and Makeup Virtual Try-on</title><link>http://arxiv.org/abs/2312.00766v1</link><description>The multitude of makeup products available can make it challenging to findthe ideal match for desired attributes. An intelligent approach for productdiscovery is required to enhance the makeup shopping experience to make it moreconvenient and satisfying. However, enabling accurate and efficient productdiscovery requires extracting detailed attributes like color and finish type.Our work introduces an automated pipeline that utilizes multiple customizedmachine learning models to extract essential material attributes from makeupproduct images. Our pipeline is versatile and capable of handling variousmakeup products. To showcase the efficacy of our pipeline, we conduct extensiveexperiments on eyeshadow products (both single and multi-shade ones), achallenging makeup product known for its diverse range of shapes, colors, andfinish types. Furthermore, we demonstrate the applicability of our approach bysuccessfully extending it to other makeup categories like lipstick andfoundation, showcasing its adaptability and effectiveness across differentbeauty products. Additionally, we conduct ablation experiments to demonstratethe superiority of our machine learning pipeline over human labeling methods interms of reliability. Our proposed method showcases its effectiveness incross-category product discovery, specifically in recommending makeup productsthat perfectly match a specified outfit. Lastly, we also demonstrate theapplication of these material attributes in enabling virtual-try-on experienceswhich makes makeup shopping experience significantly more engaging.</description><author>Fatemeh Taheri Dezaki, Himanshu Arora, Rahul Suresh, Amin Banitalebi-Dehkordi</author><pubDate>Fri, 01 Dec 2023 18:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00766v1</guid></item><item><title>Explaining Knock-on Effects of Bias Mitigation</title><link>http://arxiv.org/abs/2312.00765v1</link><description>In machine learning systems, bias mitigation approaches aim to make outcomesfairer across privileged and unprivileged groups. Bias mitigation methods workin different ways and have known "waterfall" effects, e.g., mitigating bias atone place may manifest bias elsewhere. In this paper, we aim to characteriseimpacted cohorts when mitigation interventions are applied. To do so, we treatintervention effects as a classification task and learn an explainablemeta-classifier to identify cohorts that have altered outcomes. We examine arange of bias mitigation strategies that work at various stages of the modellife cycle. We empirically demonstrate that our meta-classifier is able touncover impacted cohorts. Further, we show that all tested mitigationstrategies negatively impact a non-trivial fraction of cases, i.e., people whoreceive unfavourable outcomes solely on account of mitigation efforts. This isdespite improvement in fairness metrics. We use these results as a basis toargue for more careful audits of static mitigation interventions that go beyondaggregate metrics.</description><author>Svetoslav Nizhnichenkov, Rahul Nair, Elizabeth Daly, Brian Mac Namee</author><pubDate>Fri, 01 Dec 2023 18:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00765v1</guid></item><item><title>TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs</title><link>http://arxiv.org/abs/2308.13490v2</link><description>Precise hardware performance models play a crucial role in codeoptimizations. They can assist compilers in making heuristic decisions or aidautotuners in identifying the optimal configuration for a given program. Forexample, the autotuner for XLA, a machine learning compiler, discovered 10-20%speedup on state-of-the-art models serving substantial production traffic atGoogle. Although there exist a few datasets for program performance prediction,they target small sub-programs such as basic blocks or kernels. This paperintroduces TpuGraphs, a performance prediction dataset on full tensor programs,represented as computational graphs, running on Tensor Processing Units (TPUs).Each graph in the dataset represents the main computation of a machine learningworkload, e.g., a training epoch or an inference step. Each data samplecontains a computational graph, a compilation configuration, and the executiontime of the graph when compiled with the configuration. The graphs in thedataset are collected from open-source machine learning programs, featuringpopular model architectures, e.g., ResNet, EfficientNet, Mask R-CNN, andTransformer. TpuGraphs provides 25x more graphs than the largest graph propertyprediction dataset (with comparable graph sizes), and 770x larger graphs onaverage compared to existing performance prediction datasets on machinelearning programs. This graph-level prediction task on large graphs introducesnew challenges in learning, ranging from scalability, training efficiency, tomodel quality.</description><author>Phitchaya Mangpo Phothilimthana, Sami Abu-El-Haija, Kaidi Cao, Bahare Fatemi, Charith Mendis, Bryan Perozzi</author><pubDate>Fri, 01 Dec 2023 18:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13490v2</guid></item><item><title>Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses</title><link>http://arxiv.org/abs/2312.00763v1</link><description>Large language model (LLM) powered chatbots are primarily text-based today,and impose a large interactional cognitive load, especially for exploratory orsensemaking tasks such as planning a trip or learning about a new city. Becausethe interaction is textual, users have little scaffolding in the way ofstructure, informational "scent", or ability to specify high-level preferencesor goals. We introduce ExploreLLM that allows users to structure thoughts, helpexplore different options, navigate through the choices and recommendations,and to more easily steer models to generate more personalized responses. Weconduct a user study and show that users find it helpful to use ExploreLLM forexploratory or planning tasks, because it provides a useful schema-likestructure to the task, and guides users in planning. The study also suggeststhat users can more easily personalize responses with high-level preferenceswith ExploreLLM. Together, ExploreLLM points to a future where users interactwith LLMs beyond the form of chatbots, and instead designed to support complexuser tasks with a tighter integration between natural language and graphicaluser interfaces.</description><author>Xiao Ma, Swaroop Mishra, Ariel Liu, Sophie Su, Jilin Chen, Chinmay Kulkarni, Heng-Tze Cheng, Quoc Le, Ed Chi</author><pubDate>Fri, 01 Dec 2023 18:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00763v1</guid></item><item><title>Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting</title><link>http://arxiv.org/abs/2312.00761v1</link><description>Machine unlearning has emerged as a prominent and challenging area ofinterest, driven in large part by the rising regulatory demands for industriesto delete user data upon request and the heightened awareness of privacy.Existing approaches either retrain models from scratch or use severalfinetuning steps for every deletion request, often constrained by computationalresource limitations and restricted access to the original training data. Inthis work, we introduce a novel class unlearning algorithm designed tostrategically eliminate an entire class or a group of classes from the learnedmodel. To that end, our algorithm first estimates the Retain Space and theForget Space, representing the feature or activation spaces for samples fromclasses to be retained and unlearned, respectively. To obtain these spaces, wepropose a novel singular value decomposition-based technique that requireslayer wise collection of network activations from a few forward passes throughthe network. We then compute the shared information between these spaces andremove it from the forget space to isolate class-discriminatory feature spacefor unlearning. Finally, we project the model weights in the orthogonaldirection of the class-discriminatory space to obtain the unlearned model. Wedemonstrate our algorithm's efficacy on ImageNet using a Vision Transformerwith only $\sim$1.5% drop in retain accuracy compared to the original modelwhile maintaining under 1% accuracy on the unlearned class samples. Further,our algorithm consistently performs well when subject to Membership InferenceAttacks showing 7.8% improvement on average across a variety of imageclassification datasets and network architectures, as compared to otherbaselines while being $\sim$6x more computationally efficient.</description><author>Sangamesh Kodge, Gobinda Saha, Kaushik Roy</author><pubDate>Fri, 01 Dec 2023 18:29:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00761v1</guid></item><item><title>Revisiting DETR Pre-training for Object Detection</title><link>http://arxiv.org/abs/2308.01300v2</link><description>Motivated by the remarkable achievements of DETR-based approaches on COCOobject detection and segmentation benchmarks, recent endeavors have beendirected towards elevating their performance through self-supervisedpre-training of Transformers while preserving a frozen backbone. Noteworthyadvancements in accuracy have been documented in certain studies. Ourinvestigation delved deeply into a representative approach, DETReg, and itsperformance assessment in the context of emerging models like$\mathcal{H}$-Deformable-DETR. Regrettably, DETReg proves inadequate inenhancing the performance of robust DETR-based models under full dataconditions. To dissect the underlying causes, we conduct extensive experimentson COCO and PASCAL VOC probing elements such as the selection of pre-trainingdatasets and strategies for pre-training target generation. By contrast, weemploy an optimized approach named Simple Self-training which leads to markedenhancements through the combination of an improved box predictor and theObjects$365$ benchmark. The culmination of these endeavors results in aremarkable AP score of $59.3\%$ on the COCO val set, outperforming$\mathcal{H}$-Deformable-DETR + Swin-L without pre-training by $1.4\%$.Moreover, a series of synthetic pre-training datasets, generated by mergingcontemporary image-to-text(LLaVA) and text-to-image (SDXL) models,significantly amplifies object detection capabilities.</description><author>Yan Ma, Weicong Liang, Bohan Chen, Yiduo Hao, Bojian Hou, Xiangyu Yue, Chao Zhang, Yuhui Yuan</author><pubDate>Fri, 01 Dec 2023 18:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01300v2</guid></item><item><title>Contrastive losses as generalized models of global epistasis</title><link>http://arxiv.org/abs/2305.03136v3</link><description>Fitness functions map large combinatorial spaces of biological sequences toproperties of interest. Inferring these multimodal functions from experimentaldata is a central task in modern protein engineering. Global epistasis modelsare an effective and physically-grounded class of models for estimating fitnessfunctions from observed data. These models assume that a sparse latent functionis transformed by a monotonic nonlinearity to emit measurable fitness. Here wedemonstrate that minimizing contrastive loss functions, such as theBradley-Terry loss, is a simple and flexible technique for extracting thesparse latent function implied by global epistasis. We argue by way of afitness-epistasis uncertainty principle that the nonlinearities in globalepistasis models can produce observed fitness functions that do not admitsparse representations, and thus may be inefficient to learn from observationswhen using a Mean Squared Error (MSE) loss (a common practice). We show thatcontrastive losses are able to accurately estimate a ranking function fromlimited data even in regimes where MSE is ineffective. We validate thepractical utility of this insight by showing contrastive loss functions resultin consistently improved performance on benchmark tasks.</description><author>David H. Brookes, Jakub Otwinowski, Sam Sinai</author><pubDate>Fri, 01 Dec 2023 18:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03136v3</guid></item><item><title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title><link>http://arxiv.org/abs/2312.00752v1</link><description>Foundation models, now powering most of the exciting applications in deeplearning, are almost universally based on the Transformer architecture and itscore attention module. Many subquadratic-time architectures such as linearattention, gated convolution and recurrent models, and structured state spacemodels (SSMs) have been developed to address Transformers' computationalinefficiency on long sequences, but they have not performed as well asattention on important modalities such as language. We identify that a keyweakness of such models is their inability to perform content-based reasoning,and make several improvements. First, simply letting the SSM parameters befunctions of the input addresses their weakness with discrete modalities,allowing the model to selectively propagate or forget information along thesequence length dimension depending on the current token. Second, even thoughthis change prevents the use of efficient convolutions, we design ahardware-aware parallel algorithm in recurrent mode. We integrate theseselective SSMs into a simplified end-to-end neural network architecture withoutattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$higher throughput than Transformers) and linear scaling in sequence length, andits performance improves on real data up to million-length sequences. As ageneral sequence model backbone, Mamba achieves state-of-the-art performanceacross several modalities such as language, audio, and genomics. On languagemodeling, our Mamba-3B model outperforms Transformers of the same size andmatches Transformers twice its size, both in pretraining and downstreamevaluation.</description><author>Albert Gu, Tri Dao</author><pubDate>Fri, 01 Dec 2023 18:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00752v1</guid></item><item><title>Hard View Selection for Self-Supervised Learning</title><link>http://arxiv.org/abs/2310.03940v3</link><description>Many Self-Supervised Learning (SSL) methods train their models to beinvariant to different "views" of an image input for which a good dataaugmentation pipeline is crucial. While considerable efforts were directedtowards improving pre-text tasks, architectures, or robustness (e.g., Siamesenetworks or teacher-softmax centering), the majority of these methods remainstrongly reliant on the random sampling of operations within the imageaugmentation pipeline, such as the random resized crop or color distortionoperation. In this paper, we argue that the role of the view generation and itseffect on performance has so far received insufficient attention. To addressthis, we propose an easy, learning-free, yet powerful Hard View Selection (HVS)strategy designed to extend the random view generation to expose the pretrainedmodel to harder samples during SSL training. It encompasses the followingiterative steps: 1) randomly sample multiple views and create pairs of twoviews, 2) run forward passes for each view pair on the currently trained model,3) adversarially select the pair yielding the worst loss, and 4) run thebackward pass with the selected pair. In our empirical analysis we show thatunder the hood, HVS increases task difficulty by controlling the Intersectionover Union of views during pretraining. With only 300-epoch pretraining, HVS isable to closely rival the 800-epoch DINO baseline which remains very favorableeven when factoring in the slowdown induced by the additional forwards of HVS.Additionally, HVS consistently achieves accuracy improvements on ImageNetbetween 0.4% and 1.9% on linear evaluation and similar improvements on transfertasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.</description><author>Fabio Ferreira, Ivo Rapant, Frank Hutter</author><pubDate>Fri, 01 Dec 2023 17:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03940v3</guid></item><item><title>Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals</title><link>http://arxiv.org/abs/2312.00751v1</link><description>Transformers have achieved remarkable success in a wide range of naturallanguage processing and computer vision applications. However, therepresentation capacity of a deep transformer model is degraded due to theover-smoothing issue in which the token representations become identical whenthe model's depth grows. In this work, we show that self-attention layers intransformers minimize a functional which promotes smoothness, thereby causingtoken uniformity. We then propose a novel regularizer that penalizes the normof the difference between the smooth output tokens from self-attention and theinput tokens to preserve the fidelity of the tokens. Minimizing the resultingregularized energy functional, we derive the Neural Transformer with aRegularized Nonlocal Functional (NeuTRENO), a novel class of transformer modelsthat can mitigate the over-smoothing issue. We empirically demonstrate theadvantages of NeuTRENO over the baseline transformers and state-of-the-artmethods in reducing the over-smoothing of token representations on variouspractical tasks, including object classification, image segmentation, andlanguage modeling.</description><author>Tam Nguyen, Tan M. Nguyen, Richard G. Baraniuk</author><pubDate>Fri, 01 Dec 2023 17:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00751v1</guid></item><item><title>Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms</title><link>http://arxiv.org/abs/2306.01213v2</link><description>Learning disentangled causal representations is a challenging problem thathas gained significant attention recently due to its implications forextracting meaningful information for downstream tasks. In this work, we definea new notion of causal disentanglement from the perspective of independentcausal mechanisms. We propose ICM-VAE, a framework for learning causallydisentangled representations supervised by causally related observed labels. Wemodel causal mechanisms using learnable flow-based diffeomorphic functions tomap noise variables to latent causal variables. Further, to promote thedisentanglement of causal factors, we propose a causal disentanglement priorthat utilizes the known causal structure to encourage learning a causallyfactorized distribution in the latent space. Under relatively mild conditions,we provide theoretical results showing the identifiability of causal factorsand mechanisms up to permutation and elementwise reparameterization. Weempirically demonstrate that our framework induces highly disentangled causalfactors, improves interventional robustness, and is compatible withcounterfactual generation.</description><author>Aneesh Komanduri, Yongkai Wu, Feng Chen, Xintao Wu</author><pubDate>Fri, 01 Dec 2023 17:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01213v2</guid></item><item><title>Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer</title><link>http://arxiv.org/abs/2308.10196v2</link><description>By hiding the front-facing camera below the display panel, Under-DisplayCamera (UDC) provides users with a full-screen experience. However, due to thecharacteristics of the display, images taken by UDC suffer from significantquality degradation. Methods have been proposed to tackle UDC image restorationand advances have been achieved. There are still no specialized methods anddatasets for restoring UDC face images, which may be the most common problem inthe UDC scene. To this end, considering color filtering, brightnessattenuation, and diffraction in the imaging process of UDC, we propose atwo-stage network UDC Degradation Model Network named UDC-DMNet to synthesizeUDC images by modeling the processes of UDC imaging. Then we use UDC-DMNet andhigh-quality face images from FFHQ and CelebA-Test to create UDC face trainingdatasets FFHQ-P/T and testing datasets CelebA-Test-P/T for UDC facerestoration. We propose a novel dictionary-guided transformer network namedDGFormer. Introducing the facial component dictionary and the characteristicsof the UDC image in the restoration makes DGFormer capable of addressing blindface restoration in UDC scenarios. Experiments show that our DGFormer andUDC-DMNet achieve state-of-the-art performance.</description><author>Jingfan Tan, Xiaoxu Chen, Tao Wang, Kaihao Zhang, Wenhan Luo, Xiaocun Cao</author><pubDate>Fri, 01 Dec 2023 17:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10196v2</guid></item><item><title>HyperAttention: Long-context Attention in Near-Linear Time</title><link>http://arxiv.org/abs/2310.05869v3</link><description>We present an approximate attention mechanism named HyperAttention to addressthe computational challenges posed by the growing complexity of long contextsused in Large Language Models (LLMs). Recent work suggests that in theworst-case scenario, quadratic time is necessary unless the entries of theattention matrix are bounded or the matrix has low stable rank. We introducetwo parameters which measure: (1) the max column norm in the normalizedattention matrix, and (2) the ratio of row norms in the unnormalized attentionmatrix after detecting and removing large entries. We use these fine-grainedparameters to capture the hardness of the problem. Despite previous lowerbounds, we are able to achieve a linear time sampling algorithm even when thematrix has unbounded entries or a large stable rank, provided the aboveparameters are small. HyperAttention features a modular design that easilyaccommodates integration of other fast low-level implementations, particularlyFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) toidentify large entries, HyperAttention outperforms existing methods, givingsignificant speed improvements compared to state-of-the-art solutions likeFlashAttention. We validate the empirical performance of HyperAttention on avariety of different long-context length datasets. For example, HyperAttentionmakes the inference time of ChatGLM2 50\% faster on 32k context length whileperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,with causal masking, HyperAttention offers 5-fold speedup on a single attentionlayer.</description><author>Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh</author><pubDate>Fri, 01 Dec 2023 17:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05869v3</guid></item><item><title>Upper and lower bounds for the Lipschitz constant of random neural networks</title><link>http://arxiv.org/abs/2311.01356v2</link><description>Empirical studies have widely demonstrated that neural networks are highlysensitive to small, adversarial perturbations of the input. The worst-caserobustness against these so-called adversarial examples can be quantified bythe Lipschitz constant of the neural network. In this paper, we study upper andlower bounds for the Lipschitz constant of random ReLU neural networks.Specifically, we assume that the weights and biases follow a generalization ofthe He initialization, where general symmetric distributions for the biases arepermitted. For shallow neural networks, we characterize the Lipschitz constantup to an absolute numerical constant. For deep networks with fixed depth andsufficiently large width, our established bounds differ by a factor that islogarithmic in the width.</description><author>Paul Geuchen, Thomas Heindl, Dominik StÃ¶ger, Felix Voigtlaender</author><pubDate>Fri, 01 Dec 2023 17:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01356v2</guid></item><item><title>Physics-based Indirect Illumination for Inverse Rendering</title><link>http://arxiv.org/abs/2212.04705v2</link><description>We present a physics-based inverse rendering method that learns theillumination, geometry, and materials of a scene from posed multi-view RGBimages. To model the illumination of a scene, existing inverse rendering workseither completely ignore the indirect illumination or model it by coarseapproximations, leading to sub-optimal illumination, geometry, and materialprediction of the scene. In this work, we propose a physics-based illuminationmodel that first locates surface points through an efficient refined spheretracing algorithm, then explicitly traces the incoming indirect lights at eachsurface point based on reflection. Then, we estimate each identified indirectlight through an efficient neural network. Moreover, we utilize the Leibniz'sintegral rule to resolve non-differentiability in the proposed illuminationmodel caused by boundary lights inspired by differentiable irradiance incomputer graphics. As a result, the proposed differentiable illumination modelcan be learned end-to-end together with geometry and materials estimation. As aside product, our physics-based inverse rendering model also facilitatesflexible and realistic material editing as well as relighting. Extensiveexperiments on synthetic and real-world datasets demonstrate that the proposedmethod performs favorably against existing inverse rendering methods on novelview synthesis and inverse rendering.</description><author>Youming Deng, Xueting Li, Sifei Liu, Ming-Hsuan Yang</author><pubDate>Fri, 01 Dec 2023 17:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04705v2</guid></item><item><title>Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games</title><link>http://arxiv.org/abs/2312.00746v1</link><description>In this study, we explore the application of Large Language Models (LLMs) in"Jubensha" (Chinese murder mystery role-playing games), a novel area inAI-driven gaming. We introduce the first Chinese dataset specifically forJubensha, including character scripts and game rules, to foster AI agentdevelopment in this complex narrative environment. Our work also presents aunique multi-agent interaction framework using LLMs, allowing AI agents toautonomously engage in the game, enhancing the dynamics of Jubensha gameplay.To evaluate these AI agents, we developed specialized methods targeting theirmastery of case information and reasoning skills. Furthermore, we incorporatedthe latest advancements in in-context learning to improve the agents'performance in critical aspects like information gathering, murderer detection,and logical reasoning. The experimental results validate the effectiveness ofour proposed methods. This work aims to offer a fresh perspective onunderstanding LLM capabilities and establish a new benchmark for evaluatinglarge language model-based agents to researchers in the field.</description><author>Dekun Wu, Haochen Shi, Zhiyuan Sun, Bang Liu</author><pubDate>Fri, 01 Dec 2023 17:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00746v1</guid></item><item><title>Scalable Meta-Learning with Gaussian Processes</title><link>http://arxiv.org/abs/2312.00742v1</link><description>Meta-learning is a powerful approach that exploits historical data to quicklysolve new tasks from the same distribution. In the low-data regime, methodsbased on the closed-form posterior of Gaussian processes (GP) together withBayesian optimization have achieved high performance. However, these methodsare either computationally expensive or introduce assumptions that hinder aprincipled propagation of uncertainty between task models. This may disrupt thebalance between exploration and exploitation during optimization. In thispaper, we develop ScaML-GP, a modular GP model for meta-learning that isscalable in the number of tasks. Our core contribution is a carefully designedmulti-task kernel that enables hierarchical training and task scalability.Conditioning ScaML-GP on the meta-data exposes its modular nature yielding atest-task prior that combines the posteriors of meta-task GPs. In synthetic andreal-world meta-learning experiments, we demonstrate that ScaML-GP can learnefficiently both with few and many meta-tasks.</description><author>Petru Tighineanu, Lukas Grossberger, Paul Baireuther, Kathrin Skubch, Stefan Falkner, Julia Vinogradska, Felix Berkenkamp</author><pubDate>Fri, 01 Dec 2023 17:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00742v1</guid></item><item><title>Adversarial Score Distillation: When score distillation meets GAN</title><link>http://arxiv.org/abs/2312.00739v1</link><description>Existing score distillation methods are sensitive to classifier-free guidance(CFG) scale: manifested as over-smoothness or instability at small CFG scales,while over-saturation at large ones. To explain and analyze these issues, werevisit the derivation of Score Distillation Sampling (SDS) and decipherexisting score distillation with the Wasserstein Generative Adversarial Network(WGAN) paradigm. With the WGAN paradigm, we find that existing scoredistillation either employs a fixed sub-optimal discriminator or conductsincomplete discriminator optimization, resulting in the scale-sensitive issue.We propose the Adversarial Score Distillation (ASD), which maintains anoptimizable discriminator and updates it using the complete optimizationobjective. Experiments show that the proposed ASD performs favorably in 2Ddistillation and text-to-3D tasks against existing methods. Furthermore, toexplore the generalization ability of our WGAN paradigm, we extend ASD to theimage editing task, which achieves competitive results. The project page andcode are at https://github.com/2y7c3/ASD.</description><author>Min Wei, Jingkai Zhou, Junyao Sun, Xuesong Zhang</author><pubDate>Fri, 01 Dec 2023 17:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00739v1</guid></item><item><title>SeaLLMs -- Large Language Models for Southeast Asia</title><link>http://arxiv.org/abs/2312.00738v1</link><description>Despite the remarkable achievements of large language models (LLMs) invarious tasks, there remains a linguistic bias that favors high-resourcelanguages, such as English, often at the expense of low-resource and regionallanguages. To address this imbalance, we introduce SeaLLMs, an innovativeseries of language models that specifically focuses on Southeast Asian (SEA)languages. SeaLLMs are built upon the Llama-2 model and further advancedthrough continued pre-training with an extended vocabulary, specializedinstruction and alignment tuning to better capture the intricacies of regionallanguages. This allows them to respect and reflect local cultural norms,customs, stylistic preferences, and legal considerations. Our comprehensiveevaluation demonstrates that SeaLLM-13b models exhibit superior performanceacross a wide spectrum of linguistic tasks and assistant-styleinstruction-following capabilities relative to comparable open-source models.Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai,Khmer, Lao, and Burmese, by large margins while remaining lightweight andcost-effective to operate.</description><author>Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, Lidong Bing</author><pubDate>Fri, 01 Dec 2023 17:17:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00738v1</guid></item><item><title>Adaptive Deep Neural Network Inference Optimization with EENet</title><link>http://arxiv.org/abs/2301.07099v2</link><description>Well-trained deep neural networks (DNNs) treat all test samples equallyduring prediction. Adaptive DNN inference with early exiting leverages theobservation that some test examples can be easier to predict than others. Thispaper presents EENet, a novel early-exiting scheduling framework for multi-exitDNN models. Instead of having every sample go through all DNN layers duringprediction, EENet learns an early exit scheduler, which can intelligentlyterminate the inference earlier for certain predictions, which the model hashigh confidence of early exit. As opposed to previous early-exiting solutionswith heuristics-based methods, our EENet framework optimizes an early-exitingpolicy to maximize model accuracy while satisfying the given per-sample averageinference budget. Extensive experiments are conducted on four computer visiondatasets (CIFAR-10, CIFAR-100, ImageNet, Cityscapes) and two NLP datasets(SST-2, AgNews). The results demonstrate that the adaptive inference by EENetcan outperform the representative existing early exit techniques. We alsoperform a detailed visualization analysis of the comparison results tointerpret the benefits of EENet.</description><author>Fatih Ilhan, Ka-Ho Chow, Sihao Hu, Tiansheng Huang, Selim Tekin, Wenqi Wei, Yanzhao Wu, Myungjin Lee, Ramana Kompella, Hugo Latapie, Gaowen Liu, Ling Liu</author><pubDate>Fri, 01 Dec 2023 17:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07099v2</guid></item><item><title>Gaussian Grouping: Segment and Edit Anything in 3D Scenes</title><link>http://arxiv.org/abs/2312.00732v1</link><description>The recent Gaussian Splatting achieves high-quality and real-time novel-viewsynthesis of the 3D scenes. However, it is solely concentrated on theappearance and geometry modeling, while lacking in fine-grained object-levelscene understanding. To address this issue, we propose Gaussian Grouping, whichextends Gaussian Splatting to jointly reconstruct and segment anything inopen-world 3D scenes. We augment each Gaussian with a compact IdentityEncoding, allowing the Gaussians to be grouped according to their objectinstance or stuff membership in the 3D scene. Instead of resorting to expensive3D labels, we supervise the Identity Encodings during the differentiablerendering by leveraging the 2D mask predictions by SAM, along with introduced3D spatial consistency regularization. Comparing to the implicit NeRFrepresentation, we show that the discrete and grouped 3D Gaussians canreconstruct, segment and edit anything in 3D with high visual quality, finegranularity and efficiency. Based on Gaussian Grouping, we further propose alocal Gaussian Editing scheme, which shows efficacy in versatile scene editingapplications, including 3D object removal, inpainting, colorization and scenerecomposition. Our code and models will be athttps://github.com/lkeab/gaussian-grouping.</description><author>Mingqiao Ye, Martin Danelljan, Fisher Yu, Lei Ke</author><pubDate>Fri, 01 Dec 2023 17:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00732v1</guid></item><item><title>Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space</title><link>http://arxiv.org/abs/2312.00727v1</link><description>This paper delves into the problem of safe reinforcement learning (RL) in apartially observable environment with the aim of achieving safe-reachabilityobjectives. In traditional partially observable Markov decision processes(POMDP), ensuring safety typically involves estimating the belief in latentstates. However, accurately estimating an optimal Bayesian filter in POMDP toinfer latent states from observations in a continuous state space poses asignificant challenge, largely due to the intractable likelihood. To tacklethis issue, we propose a stochastic model-based approach that guarantees RLsafety almost surely in the face of unknown system dynamics and partialobservation environments. We leveraged the Predictive State Representation(PSR) and Reproducing Kernel Hilbert Space (RKHS) to represent futuremulti-step observations analytically, and the results in this context areprovable. Furthermore, we derived essential operators from the kernel Bayes'rule, enabling the recursive estimation of future observations using variousoperators. Under the assumption of \textit{undercompleness}, a polynomialsample complexity is established for the RL algorithm for the infinite size ofobservation and action spaces, ensuring an $\epsilon-$suboptimal safe policyguarantee.</description><author>Xiaoyuan Cheng, Boli Chen, Liz Varga, Yukun Hu</author><pubDate>Fri, 01 Dec 2023 17:01:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00727v1</guid></item><item><title>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning</title><link>http://arxiv.org/abs/2310.07918v3</link><description>Interpretable policy learning seeks to estimate intelligible decisionpolicies from observed actions; however, existing models fall short by forcinga tradeoff between accuracy and interpretability. This tradeoff limitsdata-driven interpretations of human decision-making process. e.g. to auditmedical decisions for biases and suboptimal practices, we require models ofdecision processes which provide concise descriptions of complex behaviors.Fundamentally, existing approaches are burdened by this tradeoff because theyrepresent the underlying decision process as a universal policy, when in facthuman decisions are dynamic and can change drastically with contextualinformation. Thus, we propose Contextualized Policy Recovery (CPR), whichre-frames the problem of modeling complex decision processes as a multi-tasklearning problem in which complex decision policies are comprised ofcontext-specific policies. CPR models each context-specific policy as a linearobservation-to-action mapping, and generates new decision models$\textit{on-demand}$ as contexts are updated with new observations. CPR iscompatible with fully offline and partially observable decision environments,and can be tailored to incorporate any recurrent black-box model orinterpretable decision model. We assess CPR through studies on simulated andreal data, achieving state-of-the-art performance on the canonical tasks ofpredicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs.previous SOTA) and predicting MRI prescription for Alzheimer's patients($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictiveperformance, CPR closes the accuracy gap between interpretable and black-boxmethods for policy learning, allowing high-resolution exploration and analysisof context-specific decision models.</description><author>Jannik Deuschel, Caleb N. Ellington, Benjamin J. Lengerich, Yingtao Luo, Pascal Friederich, Eric P. Xing</author><pubDate>Fri, 01 Dec 2023 17:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07918v3</guid></item><item><title>Algorithmic Information Forecastability</title><link>http://arxiv.org/abs/2304.10752v2</link><description>The outcome of all time series cannot be forecast, e.g. the flipping of afair coin. Others, like the repeated {01} sequence {010101...} can be forecastexactly. Algorithmic information theory can provide a measure offorecastability that lies between these extremes. The degree of forecastabilityis a function of only the data. For prediction (or classification) of labeleddata, we propose three categories for forecastability: oracle forecastabilityfor predictions that are always exact, precise forecastability for errors up toa bound, and probabilistic forecastability for any other predictions. Examplesare given in each case.</description><author>Glauco Amigo, Daniel AndrÃ©s DÃ­az-PachÃ³n, Robert J. Marks, Charles Baylis</author><pubDate>Fri, 01 Dec 2023 16:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10752v2</guid></item><item><title>Removing Biases from Molecular Representations via Information Maximization</title><link>http://arxiv.org/abs/2312.00718v1</link><description>High-throughput drug screening -- using cell imaging or gene expressionmeasurements as readouts of drug effect -- is a critical tool in biotechnologyto assess and understand the relationship between the chemical structure andbiological activity of a drug. Since large-scale screens have to be dividedinto multiple experiments, a key difficulty is dealing with batch effects,which can introduce systematic errors and non-biological associations in thedata. We propose InfoCORE, an Information maximization approach for COnfounderREmoval, to effectively deal with batch effects and obtain refined molecularrepresentations. InfoCORE establishes a variational lower bound on theconditional mutual information of the latent representations given a batchidentifier. It adaptively reweighs samples to equalize their implied batchdistribution. Extensive experiments on drug screening data reveal InfoCORE'ssuperior performance in a multitude of tasks including molecular propertyprediction and molecule-phenotype retrieval. Additionally, we show results forhow InfoCORE offers a versatile framework and resolves general distributionshifts and issues of data fairness by minimizing correlation with spuriousfeatures or removing sensitive attributes. The code is available athttps://github.com/uhlerlab/InfoCORE.</description><author>Chenyu Wang, Sharut Gupta, Caroline Uhler, Tommi Jaakkola</author><pubDate>Fri, 01 Dec 2023 16:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00718v1</guid></item><item><title>Learning Robust Precipitation Forecaster by Temporal Frame Interpolation</title><link>http://arxiv.org/abs/2311.18341v2</link><description>Recent advances in deep learning have significantly elevated weatherprediction models. However, these models often falter in real-world scenariosdue to their sensitivity to spatial-temporal shifts. This issue is particularlyacute in weather forecasting, where models are prone to overfit to local andtemporal variations, especially when tasked with fine-grained predictions. Inthis paper, we address these challenges by developing a robust precipitationforecasting model that demonstrates resilience against such spatial-temporaldiscrepancies. We introduce Temporal Frame Interpolation (TFI), a noveltechnique that enhances the training dataset by generating synthetic samplesthrough interpolating adjacent frames from satellite imagery and ground radardata, thus improving the model's robustness against frame noise. Moreover, weincorporate a unique Multi-Level Dice (ML-Dice) loss function, leveraging theordinal nature of rainfall intensities to improve the model's performance. Ourapproach has led to significant improvements in forecasting precision,culminating in our model securing \textit{1st place} in the transfer learningleaderboard of the \textit{Weather4cast'23} competition. This achievement notonly underscores the effectiveness of our methodologies but also establishes anew standard for deep learning applications in weather forecasting. Our codeand weights have been public on \url{https://github.com/Secilia-Cxy/UNetTFI}.</description><author>Lu Han, Xu-Yang Chen, Han-Jia Ye, De-Chuan Zhan</author><pubDate>Fri, 01 Dec 2023 16:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18341v2</guid></item><item><title>SpaCE: The Spatial Confounding Environment</title><link>http://arxiv.org/abs/2312.00710v1</link><description>Spatial confounding poses a significant challenge in scientific studiesinvolving spatial data, where unobserved spatial variables can influence bothtreatment and outcome, possibly leading to spurious associations. To addressthis problem, we introduce SpaCE: The Spatial Confounding Environment, thefirst toolkit to provide realistic benchmark datasets and tools forsystematically evaluating causal inference methods designed to alleviatespatial confounding. Each dataset includes training data, true counterfactuals,a spatial graph with coordinates, and smoothness and confounding scorescharacterizing the effect of a missing spatial confounder. It also includesrealistic semi-synthetic outcomes and counterfactuals, generated usingstate-of-the-art machine learning ensembles, following best practices forcausal inference benchmarks. The datasets cover real treatment and covariatesfrom diverse domains, including climate, health and social sciences. SpaCEfacilitates an automated end-to-end pipeline, simplifying data loading,experimental setup, and evaluating machine learning and causal inferencemodels. The SpaCE project provides several dozens of datasets of diverse sizesand spatial complexity. It is publicly available as a Python package,encouraging community feedback and contributions.</description><author>Mauricio Tec, Ana Trisovic, Michelle Audirac, Sophie Woodward, Naeem Khoshnevis, Francesca Dominici</author><pubDate>Fri, 01 Dec 2023 16:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00710v1</guid></item><item><title>PointBeV: A Sparse Approach to BeV Predictions</title><link>http://arxiv.org/abs/2312.00703v1</link><description>Bird's-eye View (BeV) representations have emerged as the de-facto sharedspace in driving applications, offering a unified space for sensor data fusionand supporting various downstream tasks. However, conventional models use gridswith fixed resolution and range and face computational inefficiencies due tothe uniform allocation of resources across all cells. To address this, wepropose PointBeV, a novel sparse BeV segmentation model operating on sparse BeVcells instead of dense grids. This approach offers precise control over memoryusage, enabling the use of long temporal contexts and accommodatingmemory-constrained platforms. PointBeV employs an efficient two-pass strategyfor training, enabling focused computation on regions of interest. At inferencetime, it can be used with various memory/performance trade-offs and flexiblyadjusts to new specific use cases. PointBeV achieves state-of-the-art resultson the nuScenes dataset for vehicle, pedestrian, and lane segmentation,showcasing superior performance in static and temporal settings despite beingtrained solely with sparse signals. We will release our code along with two newefficient modules used in the architecture: Sparse Feature Pulling, designedfor the effective extraction of features from images to BeV, and SubmanifoldAttention, which enables efficient temporal modeling. Our code is available athttps://github.com/valeoai/PointBeV.</description><author>Loick Chambon, Eloi Zablocki, Mickael Chen, Florent Bartoccioni, Patrick Perez, Matthieu Cord</author><pubDate>Fri, 01 Dec 2023 16:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00703v1</guid></item><item><title>Bayesian CART models for insurance claims frequency</title><link>http://arxiv.org/abs/2303.01923v3</link><description>Accuracy and interpretability of a (non-life) insurance pricing model areessential qualities to ensure fair and transparent premiums for policy-holders,that reflect their risk. In recent years, the classification and regressiontrees (CARTs) and their ensembles have gained popularity in the actuarialliterature, since they offer good prediction performance and are relativelyeasily interpretable. In this paper, we introduce Bayesian CART models forinsurance pricing, with a particular focus on claims frequency modelling.Additionally to the common Poisson and negative binomial (NB) distributionsused for claims frequency, we implement Bayesian CART for the zero-inflatedPoisson (ZIP) distribution to address the difficulty arising from theimbalanced insurance claims data. To this end, we introduce a general MCMCalgorithm using data augmentation methods for posterior tree exploration. Wealso introduce the deviance information criterion (DIC) for the tree modelselection. The proposed models are able to identify trees which can betterclassify the policy-holders into risk groups. Some simulations and realinsurance data will be discussed to illustrate the applicability of thesemodels.</description><author>Yaojun Zhang, Lanpeng Ji, Georgios Aivaliotis, Charles Taylor</author><pubDate>Fri, 01 Dec 2023 16:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01923v3</guid></item><item><title>GIFT: Generative Interpretable Fine-Tuning Transformers</title><link>http://arxiv.org/abs/2312.00700v1</link><description>We present GIFT (Generative Interpretable Fine-tuning Transformers) forfine-tuning pretrained (often large) Transformer models at downstream tasks ina parameter-efficient way with built-in interpretability. Our GIFT is a deepparameter-residual learning method, which addresses two problems in fine-tuninga pretrained Transformer model: Where to apply the parameter-efficientfine-tuning (PEFT) to be extremely lightweight yet sufficiently expressive, andHow to learn the PEFT to better exploit the knowledge of the pretrained modelin a direct way? For the former, we select the final projection (linear) layerin the multi-head self-attention of a Transformer model, and verify itseffectiveness. For the latter, in contrast to the prior art that directlyintroduce new model parameters (often in low-rank approximation form) to belearned in fine-tuning with downstream data, we propose a method for learningto generate the fine-tuning parameters. Our GIFT is a hyper-Transformer whichtake as input the pretrained parameters of the projection layer to generate itsfine-tuning parameters using a proposed Parameter-to-Cluster Attention (PaCa).The PaCa results in a simple clustering-based forward explainer that plays therole of semantic segmentation in testing. In experiments, our proposed GIFT istested on the VTAB benchmark and the fine-grained visual classification (FGVC)benchmark. It obtains significantly better performance than the prior art. Ourcode is available at https://github.com/savadikarc/gift</description><author>Chinmay Savadikar, Xi Song, Tianfu Wu</author><pubDate>Fri, 01 Dec 2023 16:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00700v1</guid></item><item><title>Rethinking Detection Based Table Structure Recognition for Visually Rich Documents</title><link>http://arxiv.org/abs/2312.00699v1</link><description>Table Structure Recognition (TSR) aims at transforming unstructured tableimages into structured formats, such as HTML sequences. One type of popularsolution is using detection models to detect components of a table, such ascolumns and rows, then applying a rule-based post-processing method to convertdetection results into HTML sequences. However, existing detection-basedstudies often have the following limitations. First, these studies usually paymore attention to improving the detection performance, which does notnecessarily lead to better performance regarding cell-level metrics, such asTEDS. Second, some solutions over-simplify the problem and can miss somecritical information. Lastly, even though some studies defined the problem todetect more components to provide as much information as other types ofsolutions, these studies ignore the fact this problem definition is amulti-label detection because row, projected row header and column header canshare identical bounding boxes. Besides, there is often a performance gapbetween two-stage and transformer-based detection models regarding thestructure-only TEDS, even though they have similar performance regarding theCOCO metrics. Therefore, we revisit the limitations of existing detection-basedsolutions, compare two-stage and transformer-based detection models, andidentify the key design aspects for the success of a two-stage detection modelfor the TSR task, including the multi-class problem definition, the aspectratio for anchor box generation, and the feature generation of the backbonenetwork. We applied simple methods to improve these aspects of the CascadeR-CNN model, achieved state-of-the-art performance, and improved the baselineCascade R-CNN model by 19.32%, 11.56% and 14.77% regarding the structure-onlyTEDS on SciTSR, FinTabNet, and PubTables1M datasets.</description><author>Bin Xiao, Murat Simsek, Burak Kantarci, Ala Abu Alkheir</author><pubDate>Fri, 01 Dec 2023 16:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00699v1</guid></item><item><title>Object Detector Differences when using Synthetic and Real Training Data</title><link>http://arxiv.org/abs/2312.00694v1</link><description>To train well-performing generalizing neural networks, sufficiently large anddiverse datasets are needed. Collecting data while adhering to privacylegislation becomes increasingly difficult and annotating these large datasetsis both a resource-heavy and time-consuming task. An approach to overcome thesedifficulties is to use synthetic data since it is inherently scalable and canbe automatically annotated. However, how training on synthetic data affects thelayers of a neural network is still unclear. In this paper, we train the YOLOv3object detector on real and synthetic images from city environments. We performa similarity analysis using Centered Kernel Alignment (CKA) to explore theeffects of training on synthetic data on a layer-wise basis. The analysiscaptures the architecture of the detector while showing both different andsimilar patterns between different models. With this similarity analysis wewant to give insights on how training synthetic data affects each layer and togive a better understanding of the inner workings of complex neural networks.The results show that the largest similarity between a detector trained on realdata and a detector trained on synthetic data was in the early layers, and thelargest difference was in the head part. The results also show that no majordifference in performance or similarity could be seen between frozen andunfrozen backbone.</description><author>Martin Georg Ljungqvist, Otto Nordander, Markus Skans, Arvid Mildner, Tony Liu, Pierre Nugues</author><pubDate>Fri, 01 Dec 2023 16:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00694v1</guid></item><item><title>VisionaryVR: An Optical Simulation Tool for Evaluating and Optimizing Vision Correction Solutions in Virtual Reality</title><link>http://arxiv.org/abs/2312.00692v1</link><description>Developing and evaluating vision science methods require robust and efficienttools for assessing their performance in various real-world scenarios. Thisstudy presents a novel virtual reality (VR) simulation tool that simulatesreal-world optical methods while giving high experimental control to theexperiment. The tool incorporates an experiment controller, to smoothly andeasily handle multiple conditions, a generic eye-tracking controller, thatworks with most common VR eye-trackers, a configurable defocus simulator, and ageneric VR questionnaire loader to assess participants' behavior in virtualreality. This VR-based simulation tool bridges the gap between theoretical andapplied research on new optical methods, corrections, and therapies. It enablesvision scientists to increase their research tools with a robust, realistic,and fast research environment.</description><author>Benedikt W. Hosp, Martin Dechant, Yannick Sauer, Rajat Agarwala, Siegfried Wahl</author><pubDate>Fri, 01 Dec 2023 16:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00692v1</guid></item><item><title>Open-vocabulary object 6D pose estimation</title><link>http://arxiv.org/abs/2312.00690v1</link><description>We introduce the new setting of open-vocabulary object 6D pose estimation, inwhich a textual prompt is used to specify the object of interest. In contrastto existing approaches, in our setting (i) the object of interest is specifiedsolely through the textual prompt, (ii) no object model (e.g. CAD or videosequence) is required at inference, (iii) the object is imaged from twodifferent viewpoints of two different scenes, and (iv) the object was notobserved during the training phase. To operate in this setting, we introduce anovel approach that leverages a Vision-Language Model to segment the object ofinterest from two distinct scenes and to estimate its relative 6D pose. The keyof our approach is a carefully devised strategy to fuse object-levelinformation provided by the prompt with local image features, resulting in afeature space that can generalize to novel concepts. We validate our approachon a new benchmark based on two popular datasets, REAL275 and Toyota-Light,which collectively encompass 39 object instances appearing in four thousandimage pairs. The results demonstrate that our approach outperforms both awell-established hand-crafted method and a recent deep learning-based baselinein estimating the relative 6D pose of objects in different scenes. Projectwebsite: https://jcorsetti.github.io/oryon-website/.</description><author>Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, Fabio Poiesi</author><pubDate>Fri, 01 Dec 2023 16:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00690v1</guid></item><item><title>Infrared Image Super-Resolution via GAN</title><link>http://arxiv.org/abs/2312.00689v1</link><description>The ability of generative models to accurately fit data distributions hasresulted in their widespread adoption and success in fields such as computervision and natural language processing. In this chapter, we provide a briefoverview of the application of generative models in the domain of infrared (IR)image super-resolution, including a discussion of the various challenges andadversarial training methods employed. We propose potential areas for furtherinvestigation and advancement in the application of generative models for IRimage super-resolution.</description><author>Yongsong Huang, Shinichiro Omachi</author><pubDate>Fri, 01 Dec 2023 16:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00689v1</guid></item><item><title>Towards Transparency in Coreference Resolution: A Quantum-Inspired Approach</title><link>http://arxiv.org/abs/2312.00688v1</link><description>Guided by grammatical structure, words compose to form sentences, and guidedby discourse structure, sentences compose to form dialogues and documents. Thecompositional aspect of sentence and discourse units is often overlooked bymachine learning algorithms. A recent initiative called Quantum NaturalLanguage Processing (QNLP) learns word meanings as points in a Hilbert spaceand acts on them via a translation of grammatical structure into ParametrisedQuantum Circuits (PQCs). Previous work extended the QNLP translation todiscourse structure using points in a closure of Hilbert spaces. In this paper,we evaluate this translation on a Winograd-style pronoun resolution task. Wetrain a Variational Quantum Classifier (VQC) for binary classification andimplement an end-to-end pronoun resolution system. The simulations executed onIBMQ software converged with an F1 score of 87.20%. The model outperformed twoout of three classical coreference resolution systems and nearedstate-of-the-art SpanBERT. A mixed quantum-classical model yet improved theseresults with an F1 score increase of around 6%.</description><author>Hadi Wazni, Mehrnoosh Sadrzadeh</author><pubDate>Fri, 01 Dec 2023 16:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00688v1</guid></item><item><title>Contextualized word senses: from attention to compositionality</title><link>http://arxiv.org/abs/2312.00680v1</link><description>The neural architectures of language models are becoming increasinglycomplex, especially that of Transformers, based on the attention mechanism.Although their application to numerous natural language processing tasks hasproven to be very fruitful, they continue to be models with little or nointerpretability and explainability. One of the tasks for which they are bestsuited is the encoding of the contextual sense of words using contextualizedembeddings. In this paper we propose a transparent, interpretable, andlinguistically motivated strategy for encoding the contextual sense of words bymodeling semantic compositionality. Particular attention is given to dependencyrelations and semantic notions such as selection preferences and paradigmaticclasses. A partial implementation of the proposed model is carried out andcompared with Transformer-based architectures for a given semantic task, namelythe similarity calculation of word senses in context. The results obtained showthat it is possible to be competitive with linguistically motivated modelsinstead of using the black boxes underlying complex neural architectures.</description><author>Pablo Gamallo</author><pubDate>Fri, 01 Dec 2023 16:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00680v1</guid></item><item><title>The Efficiency Spectrum of Large Language Models: An Algorithmic Survey</title><link>http://arxiv.org/abs/2312.00678v1</link><description>The rapid growth of Large Language Models (LLMs) has been a driving force intransforming various domains, reshaping the artificial general intelligencelandscape. However, the increasing computational and memory demands of thesemodels present substantial challenges, hindering both academic research andpractical applications. To address these issues, a wide array of methods,including both algorithmic and hardware solutions, have been developed toenhance the efficiency of LLMs. This survey delivers a comprehensive review ofalgorithmic advancements aimed at improving LLM efficiency. Unlike othersurveys that typically focus on specific areas such as training or modelcompression, this paper examines the multi-faceted dimensions of efficiencyessential for the end-to-end algorithmic development of LLMs. Specifically, itcovers various topics related to efficiency, including scaling laws, datautilization, architectural innovations, training and tuning strategies, andinference techniques. This paper aims to serve as a valuable resource forresearchers and practitioners, laying the groundwork for future innovations inthis critical research area. Our repository of relevant references ismaintained at url{https://github.com/tding1/Efficient-LLM-Survey}.</description><author>Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, Luming Liang</author><pubDate>Fri, 01 Dec 2023 16:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00678v1</guid></item><item><title>Unsupervised Adaptive Implicit Neural Representation Learning for Scan-Specific MRI Reconstruction</title><link>http://arxiv.org/abs/2312.00677v1</link><description>In recent studies on MRI reconstruction, advances have shown significantpromise for further accelerating the MRI acquisition. Most state-of-the-artmethods require a large amount of fully-sampled data to optimise reconstructionmodels, which is impractical and expensive under certain clinical settings. Onthe other hand, for unsupervised scan-specific reconstruction methods,overfitting is likely to happen due to insufficient supervision, whilerestrictions on acceleration rates and under-sampling patterns further limittheir applicability. To this end, we propose an unsupervised, adaptivecoarse-to-fine framework that enhances reconstruction quality without beingconstrained by the sparsity levels or patterns in under-sampling. The frameworkemploys an implicit neural representation for scan-specific MRI reconstruction,learning a mapping from multi-dimensional coordinates to their correspondingsignal intensities. Moreover, we integrate a novel learning strategy thatprogressively refines the use of acquired k-space signals for self-supervision.This approach effectively adjusts the proportion of supervising signals fromunevenly distributed information across different frequency bands, thusmitigating the issue of overfitting while improving the overall reconstruction.Comprehensive evaluation on a public dataset, including both 2D and 3D data,has shown that our method outperforms current state-of-the-art scan-specificMRI reconstruction techniques, for up to 8-fold under-sampling.</description><author>Junwei Yang, Pietro LiÃ²</author><pubDate>Fri, 01 Dec 2023 16:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00677v1</guid></item><item><title>Uncertainty Estimation and Out-of-Distribution Detection for Deep Learning-Based Image Reconstruction using the Local Lipschitz</title><link>http://arxiv.org/abs/2305.07618v3</link><description>Accurate image reconstruction is at the heart of diagnostics in medicalimaging. Supervised deep learning-based approaches have been investigated forsolving inverse problems including image reconstruction. However, these trainedmodels encounter unseen data distributions that are widely shifted fromtraining data during deployment. Therefore, it is essential to assess whether agiven input falls within the training data distribution for diagnosticpurposes. Uncertainty estimation approaches exist but focus on providing anuncertainty map to radiologists, rather than assessing the trainingdistribution fit. In this work, we propose a method based on the localLipschitz-based metric to distinguish out-of-distribution images fromin-distribution with an area under the curve of 99.94%. Empirically, wedemonstrate a very strong relationship between the local Lipschitz value andmean absolute error (MAE), supported by a high Spearman's rank correlationcoefficient of 0.8475, which determines the uncertainty estimation thresholdfor optimal model performance. Through the identification of false positives,the local Lipschitz and MAE relationship was used to guide data augmentationand reduce model uncertainty. Our study was validated using the AUTOMAParchitecture for sensor-to-image Magnetic Resonance Imaging (MRI)reconstruction. We compare our proposed approach with baseline methods:Monte-Carlo dropout and deep ensembles, and further analysis included MRIdenoising and Computed Tomography (CT) sparse-to-full view reconstruction usingUNET architectures. We show that our approach is applicable to variousarchitectures and learned functions, especially in the realm of medical imagereconstruction, where preserving the diagnostic accuracy of reconstructedimages remains paramount.</description><author>Danyal F. Bhutto, Bo Zhu, Jeremiah Z. Liu, Neha Koonjoo, Hongwei B. Li, Bruce R. Rosen, Matthew S. Rosen</author><pubDate>Fri, 01 Dec 2023 16:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07618v3</guid></item><item><title>On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training</title><link>http://arxiv.org/abs/2305.12224v2</link><description>Pre-training datasets are critical for building state-of-the-art machinelearning models, motivating rigorous study on their impact on downstream tasks.In this work, we study the impact of the trade-off between the intra-classdiversity (the number of samples per class) and the inter-class diversity (thenumber of classes) of a supervised pre-training dataset. Empirically, we foundthat with the size of the pre-training dataset fixed, the best downstreamperformance comes with a balance on the intra-/inter-class diversity. Tounderstand the underlying mechanism, we show theoretically that the downstreamperformance depends monotonically on both types of diversity. Notably, ourtheory reveals that the optimal class-to-sample ratio (#classes / #samples perclass) is invariant to the size of the pre-training dataset, which motivates anapplication of predicting the optimal number of pre-training classes. Wedemonstrate the effectiveness of this application by an improvement of around 2points on the downstream tasks when using ImageNet as the pre-training dataset.</description><author>Jieyu Zhang, Bohan Wang, Zhengyu Hu, Pang Wei Koh, Alexander Ratner</author><pubDate>Fri, 01 Dec 2023 15:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12224v2</guid></item><item><title>LightCLIP: Learning Multi-Level Interaction for Lightweight Vision-Language Models</title><link>http://arxiv.org/abs/2312.00674v1</link><description>Vision-language pre-training like CLIP has shown promising performance onvarious downstream tasks such as zero-shot image classification and image-textretrieval. Most of the existing CLIP-alike works usually adopt relatively largeimage encoders like ResNet50 and ViT, while the lightweight counterparts arerarely discussed. In this paper, we propose a multi-level interaction paradigmfor training lightweight CLIP models. Firstly, to mitigate the problem thatsome image-text pairs are not strictly one-to-one correspondence, we improvethe conventional global instance-level alignment objective by softening thelabel of negative samples progressively. Secondly, a relaxed bipartite matchingbased token-level alignment objective is introduced for finer-grained alignmentbetween image patches and textual words. Moreover, based on the observationthat the accuracy of CLIP model does not increase correspondingly as theparameters of text encoder increase, an extra objective of masked languagemodeling (MLM) is leveraged for maximizing the potential of the shortened textencoder. In practice, an auxiliary fusion module injecting unmasked imageembedding into masked text embedding at different network stages is proposedfor enhancing the MLM. Extensive experiments show that without introducingadditional computational cost during inference, the proposed method achieves ahigher performance on multiple downstream tasks.</description><author>Ying Nie, Wei He, Kai Han, Yehui Tang, Tianyu Guo, Fanyi Du, Yunhe Wang</author><pubDate>Fri, 01 Dec 2023 15:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00674v1</guid></item><item><title>Generalized 3D Self-supervised Learning Framework via Prompted Foreground-Aware Feature Contrast</title><link>http://arxiv.org/abs/2303.06388v4</link><description>Contrastive learning has recently demonstrated great potential forunsupervised pre-training in 3D scene understanding tasks. However, mostexisting work randomly selects point features as anchors while buildingcontrast, leading to a clear bias toward background points that often dominatein 3D scenes. Also, object awareness and foreground-to-backgrounddiscrimination are neglected, making contrastive learning less effective. Totackle these issues, we propose a general foreground-aware feature contrastFAC++ framework to learn more effective point cloud representations inpre-training. FAC++ consists of two novel contrast designs to construct moreeffective and informative contrast pairs. The first is building positive pairswithin the same foreground segment where points tend to have the samesemantics. The second is that we prevent over-discrimination between 3Dsegments/objects and encourage grouped foreground-to-background distinctions atthe segment level with adaptive feature learning in a Siamese correspondencenetwork, which adaptively learns feature correlations within and across pointcloud views effectively. Moreover, we have designed the foreground-promptedregional sampling to enhance more balanced foreground-aware learning, which istermed FAC++. Visualization with point activation maps shows that our contrastpairs capture clear correspondences among foreground regions duringpre-training. Quantitative experiments also show that FAC++ achieves superiorknowledge transfer and data efficiency in various downstream 3D semanticsegmentation, instance segmentation as well as object detection tasks. Allcodes, data, and models are available at:https://github.com/KangchengLiu/FAC_Foreground_Aware_Contrast</description><author>Kangcheng Liu, Xinhu Zheng, Chaoqun Wang, Kai Tang, Ming Liu, Baoquan Chen</author><pubDate>Fri, 01 Dec 2023 15:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06388v4</guid></item><item><title>Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey</title><link>http://arxiv.org/abs/2307.00309v2</link><description>Deep learning has successfully solved a wide range of tasks in 2D vision as adominant AI technique. Recently, deep learning on 3D point clouds is becomingincreasingly popular for addressing various tasks in this field. Despiteremarkable achievements, deep learning algorithms are vulnerable to adversarialattacks. These attacks are imperceptible to the human eye but can easily fooldeep neural networks in the testing and deployment stage. To encourage futureresearch, this survey summarizes the current progress on adversarial attack anddefense techniques on point cloud classification.This paper first introducesthe principles and characteristics of adversarial attacks and summarizes andanalyzes adversarial example generation methods in recent years. Additionally,it provides an overview of defense strategies, organized into data-focused andmodel-focused methods. Finally, it presents several current challenges andpotential future research directions in this domain.</description><author>Hanieh Naderi, Ivan V. BajiÄ</author><pubDate>Fri, 01 Dec 2023 15:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00309v2</guid></item><item><title>CellMixer: Annotation-free Semantic Cell Segmentation of Heterogeneous Cell Populations</title><link>http://arxiv.org/abs/2312.00671v1</link><description>In recent years, several unsupervised cell segmentation methods have beenpresented, trying to omit the requirement of laborious pixel-level annotationsfor the training of a cell segmentation model. Most if not all of these methodshandle the instance segmentation task by focusing on the detection of differentcell instances ignoring their type. While such models prove adequate forcertain tasks, like cell counting, other applications require theidentification of each cell's type. In this paper, we present CellMixer, aninnovative annotation-free approach for the semantic segmentation ofheterogeneous cell populations. Our augmentation-based method enables thetraining of a segmentation model from image-level labels of homogeneous cellpopulations. Our results show that CellMixer can achieve competitivesegmentation performance across multiple cell types and imaging modalities,demonstrating the method's scalability and potential for broader applicationsin medical imaging, cellular biology, and diagnostics.</description><author>Mehdi Naouar, Gabriel Kalweit, Anusha Klett, Yannick Vogt, Paula Silvestrini, Diana Laura Infante Ramirez, Roland Mertelsmann, Joschka Boedecker, Maria Kalweit</author><pubDate>Fri, 01 Dec 2023 15:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00671v1</guid></item><item><title>Generalized Label-Efficient 3D Scene Parsing via Hierarchical Feature Aligned Pre-Training and Region-Aware Fine-tuning</title><link>http://arxiv.org/abs/2312.00663v1</link><description>Deep neural network models have achieved remarkable progress in 3D sceneunderstanding while trained in the closed-set setting and with full labels.However, the major bottleneck for current 3D recognition approaches is thatthey do not have the capacity to recognize any unseen novel classes beyond thetraining categories in diverse kinds of real-world applications. In themeantime, current state-of-the-art 3D scene understanding approaches primarilyrequire high-quality labels to train neural networks, which merely perform wellin a fully supervised manner. This work presents a generalized and simpleframework for dealing with 3D scene understanding when the labeled scenes arequite limited. To extract knowledge for novel categories from the pre-trainedvision-language models, we propose a hierarchical feature-aligned pre-trainingand knowledge distillation strategy to extract and distill meaningfulinformation from large-scale vision-language models, which helps benefit theopen-vocabulary scene understanding tasks. To leverage the boundaryinformation, we propose a novel energy-based loss with boundary awarenessbenefiting from the region-level boundary predictions. To encourage latentinstance discrimination and to guarantee efficiency, we propose theunsupervised region-level semantic contrastive learning scheme for pointclouds, using confident predictions of the neural network to discriminate theintermediate feature embeddings at multiple stages. Extensive experiments withboth indoor and outdoor scenes demonstrated the effectiveness of our approachin both data-efficient learning and open-world few-shot learning. All codes,models, and data are made publicly available at:https://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.</description><author>Kangcheng Liu, Yong-Jin Liu, Kai Tang, Ming Liu, Baoquan Chen</author><pubDate>Fri, 01 Dec 2023 15:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00663v1</guid></item><item><title>Assessment of Deep Learning Segmentation for Real-Time Free-Breathing Cardiac Magnetic Resonance Imaging</title><link>http://arxiv.org/abs/2311.14049v3</link><description>In recent years, a variety of deep learning networks for cardiac MRI (CMR)segmentation have been developed and analyzed. However, nearly all of them arefocused on cine CMR under breathold. In this work, accuracy of deep learningmethods is assessed for volumetric analysis (via segmentation) of the leftventricle in real-time free-breathing CMR at rest and under exercise stress.Data from healthy volunteers (n=15) for cine and real-time free-breathing CMRwere analyzed retrospectively. Segmentations of a commercial software (comDL)and a freely available neural network (nnU-Net), were compared to a referencecreated via the manual correction of comDL segmentation. Segmentation of leftventricular endocardium (LV), left ventricular myocardium (MYO), and rightventricle (RV) is evaluated for both end-systolic and end-diastolic phases andanalyzed with Dice's coefficient (DC). The volumetric analysis includes LVend-diastolic volume (EDV), LV end-systolic volume (ESV), and LV ejectionfraction (EF). For cine CMR, nnU-Net and comDL achieve a DC above 0.95 for LVand 0.9 for MYO, and RV. For real-time CMR, the accuracy of nnU-Net exceedsthat of comDL overall. For real-time CMR at rest, nnU-Net achieves a DC of 0.94for LV, 0.89 for MYO, and 0.90 for RV; mean absolute differences betweennnU-Net and reference are 2.9mL for EDV, 3.5mL for ESV and 2.6% for EF. Forreal-time CMR under exercise stress, nnU-Net achieves a DC of 0.92 for LV, 0.85for MYO, and 0.83 for RV; mean absolute differences between nnU-Net andreference are 11.4mL for EDV, 2.9mL for ESV and 3.6% for EF. Deep learningmethods designed or trained for cine CMR segmentation can perform well onreal-time CMR. For real-time free-breathing CMR at rest, the performance ofdeep learning methods is comparable to inter-observer variability in cine CMRand is usable or fully automatic segmentation.</description><author>Martin Schilling, Christina Unterberg-Buchwald, Joachim Lotz, Martin Uecker</author><pubDate>Fri, 01 Dec 2023 15:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14049v3</guid></item><item><title>Nonparametric Variational Regularisation of Pretrained Transformers</title><link>http://arxiv.org/abs/2312.00662v1</link><description>The current paradigm of large-scale pre-training and fine-tuning Transformerlarge language models has lead to significant improvements across the board innatural language processing. However, such large models are susceptible tooverfitting to their training data, and as a result the models perform poorlywhen the domain changes. Also, due to the model's scale, the cost offine-tuning the model to the new domain is large. Nonparametric VariationalInformation Bottleneck (NVIB) has been proposed as a regulariser for trainingcross-attention in Transformers, potentially addressing the overfittingproblem. We extend the NVIB framework to replace all types of attentionfunctions in Transformers, and show that existing pretrained Transformers canbe reinterpreted as Nonparametric Variational (NV) models using a proposedidentity initialisation. We then show that changing the initialisationintroduces a novel, information-theoretic post-training regularisation in theattention mechanism, which improves out-of-domain generalisation without anytraining. This success supports the hypothesis that pretrained Transformers areimplicitly NV Bayesian models.</description><author>Fabio Fehr, James Henderson</author><pubDate>Fri, 01 Dec 2023 15:40:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00662v1</guid></item><item><title>Dual-Domain Multi-Contrast MRI Reconstruction with Synthesis-based Fusion Network</title><link>http://arxiv.org/abs/2312.00661v1</link><description>Purpose: To develop an efficient dual-domain reconstruction framework formulti-contrast MRI, with the focus on minimising cross-contrast misalignment inboth the image and the frequency domains to enhance optimisation. Theory andMethods: Our proposed framework, based on deep learning, facilitates theoptimisation for under-sampled target contrast using fully-sampled referencecontrast that is quicker to acquire. The method consists of three key steps: 1)Learning to synthesise data resembling the target contrast from the referencecontrast; 2) Registering the multi-contrast data to reduce inter-scan motion;and 3) Utilising the registered data for reconstructing the target contrast.These steps involve learning in both domains with regularisation applied toensure their consistency. We also compare the reconstruction performance withexisting deep learning-based methods using a dataset of brain MRI scans.Results: Extensive experiments demonstrate the superiority of our proposedframework, for up to an 8-fold acceleration rate, compared to state-of-the-artalgorithms. Comprehensive analysis and ablation studies further present theeffectiveness of the proposed components. Conclusion:Our dual-domain frameworkoffers a promising approach to multi-contrast MRI reconstruction. It can alsobe integrated with existing methods to further enhance the reconstruction.</description><author>Junwei Yang, Pietro LiÃ²</author><pubDate>Fri, 01 Dec 2023 15:40:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00661v1</guid></item><item><title>Resource-constrained knowledge diffusion processes inspired by human peer learning</title><link>http://arxiv.org/abs/2312.00660v1</link><description>We consider a setting where a population of artificial learners is given, andthe objective is to optimize aggregate measures of performance, underconstraints on training resources. The problem is motivated by the study ofpeer learning in human educational systems. In this context, we study naturalknowledge diffusion processes in networks of interacting artificial learners.By `natural', we mean processes that reflect human peer learning where thestudents' internal state and learning process is mostly opaque, and the maindegree of freedom lies in the formation of peer learning groups by acoordinator who can potentially evaluate the learners before assigning them topeer groups. Among else, we empirically show that such processes indeed makeeffective use of the training resources, and enable the design of modularneural models that have the capacity to generalize without being prone tooverfitting noisy labels.</description><author>Ehsan Beikihassan, Amy K. Hoover, Ioannis Koutis, Ali Parviz, Niloofar Aghaieabiane</author><pubDate>Fri, 01 Dec 2023 15:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00660v1</guid></item><item><title>MLLMs-Augmented Visual-Language Representation Learning</title><link>http://arxiv.org/abs/2311.18765v2</link><description>Visual-language pre-training (VLP) has achieved remarkable success inmulti-modal tasks, largely attributed to the availability of large-scaleimage-text datasets. In this work, we demonstrate that multi-modal largelanguage models (MLLMs) can enhance visual-language representation learning byimproving data quality. Our approach is simple, utilizing MLLMs to extendmultiple captions for each image. To prevent the bias introduced by MLLMs'hallucinations and intrinsic caption styles, we propose "text shearing" tomaintain the same length for extended captions as that of the originalcaptions. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0%and 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shotsettings, respectively. Notably, we obtain zero-shot results that arecomparable to fine-tuning on target datasets, which encourages more explorationof the versatile use of MLLMs.</description><author>Yanqing Liu, Kai Wang, Wenqi Shao, Ping Luo, Yu Qiao, Mike Zheng Shou, Kaipeng Zhang, Yang You</author><pubDate>Fri, 01 Dec 2023 15:38:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18765v2</guid></item><item><title>Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models</title><link>http://arxiv.org/abs/2311.03687v2</link><description>Large Language Models (LLMs) have seen great advance in both academia andindustry, and their popularity results in numerous open-source frameworks andtechniques in accelerating LLM pre-training, fine-tuning, and inference.Training and deploying LLMs are expensive as it requires considerable computingresources and memory, hence many efficient approaches have been developed forimproving system pipelines as well as operators. However, the runtimeperformance can vary significantly across hardware and software stacks, whichmakes it difficult to choose the best configuration. In this work, we aim tobenchmark the performance from both macro and micro perspectives. First, webenchmark the end-to-end performance of pre-training, fine-tuning, and servingLLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and70B) on three 8-GPU platforms with and without individual optimizationtechniques, including ZeRO, quantization, recomputation, FlashAttention. Then,we dive deeper to provide a detailed runtime analysis of the sub-modules,including computing and communication operators in LLMs. For end users, ourbenchmark and findings help better understand different optimizationtechniques, training and inference frameworks, together with hardware platformsin choosing configurations for deploying LLMs. For researchers, our in-depthmodule-wise analyses discover potential opportunities for future work tofurther optimize the runtime performance of LLMs.</description><author>Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo Fan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, Xiaowen Chu</author><pubDate>Fri, 01 Dec 2023 15:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03687v2</guid></item><item><title>Simple Transferability Estimation for Regression Tasks</title><link>http://arxiv.org/abs/2312.00656v1</link><description>We consider transferability estimation, the problem of estimating how welldeep learning models transfer from a source to a target task. We focus onregression tasks, which received little previous attention, and propose twosimple and computationally efficient approaches that estimate transferabilitybased on the negative regularized mean squared error of a linear regressionmodel. We prove novel theoretical results connecting our approaches to theactual transferability of the optimal target models obtained from the transferlearning process. Despite their simplicity, our approaches significantlyoutperform existing state-of-the-art regression transferability estimators inboth accuracy and efficiency. On two large-scale keypoint regressionbenchmarks, our approaches yield 12% to 36% better results on average whilebeing at least 27% faster than previous state-of-the-art methods.</description><author>Cuong N. Nguyen, Phong Tran, Lam Si Tung Ho, Vu Dinh, Anh T. Tran, Tal Hassner, Cuong V. Nguyen</author><pubDate>Fri, 01 Dec 2023 15:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00656v1</guid></item><item><title>Machine Learning for Health symposium 2023 -- Findings track</title><link>http://arxiv.org/abs/2312.00655v1</link><description>A collection of the accepted Findings papers that were presented at the 3rdMachine Learning for Health symposium (ML4H 2023), which was held on December10, 2023, in New Orleans, Louisiana, USA. ML4H 2023 invited high-qualitysubmissions on relevant problems in a variety of health-related disciplinesincluding healthcare, biomedicine, and public health. Two submission trackswere offered: the archival Proceedings track, and the non-archival Findingstrack. Proceedings were targeted at mature work with strong technicalsophistication and a high impact to health. The Findings track looked for newideas that could spark insightful discussion, serve as valuable resources forthe community, or could enable new collaborations. Submissions to theProceedings track, if not accepted, were automatically considered for theFindings track. All the manuscripts submitted to ML4H Symposium underwent adouble-blind peer-review process.</description><author>Stefan Hegselmann, Antonio Parziale, Divya Shanmugam, Shengpu Tang, Mercy Nyamewaa Asiedu, Serina Chang, Thomas Hartvigsen, Harvineet Singh</author><pubDate>Fri, 01 Dec 2023 15:30:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00655v1</guid></item><item><title>TrackDiffusion: Multi-object Tracking Data Generation via Diffusion Models</title><link>http://arxiv.org/abs/2312.00651v1</link><description>Diffusion models have gained prominence in generating data for perceptiontasks such as image classification and object detection. However, the potentialin generating high-quality tracking sequences, a crucial aspect in the field ofvideo perception, has not been fully investigated. To address this gap, wepropose TrackDiffusion, a novel architecture designed to generate continuousvideo sequences from the tracklets. TrackDiffusion represents a significantdeparture from the traditional layout-to-image (L2I) generation and copy-pastesynthesis focusing on static image elements like bounding boxes by empoweringimage diffusion models to encompass dynamic and continuous trackingtrajectories, thereby capturing complex motion nuances and ensuring instanceconsistency among video frames. For the first time, we demonstrate that thegenerated video sequences can be utilized for training multi-object tracking(MOT) systems, leading to significant improvement in tracker performance.Experimental results show that our model significantly enhances instanceconsistency in generated video sequences, leading to improved perceptualmetrics. Our approach achieves an improvement of 8.7 in TrackAP and 11.8 inTrackAP$_{50}$ on the YTVIS dataset, underscoring its potential to redefine thestandards of video data generation for MOT tasks and beyond.</description><author>Pengxiang Li, Zhili Liu, Kai Chen, Lanqing Hong, Yunzhi Zhuge, Dit-Yan Yeung, Huchuan Lu, Xu Jia</author><pubDate>Fri, 01 Dec 2023 15:24:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00651v1</guid></item><item><title>SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers</title><link>http://arxiv.org/abs/2312.00648v1</link><description>Unsupervised object-centric learning aims to decompose scenes intointerpretable object entities, termed slots. Slot-based auto-encoders stand outas a prominent method for this task. Within them, crucial aspects includeguiding the encoder to generate object-specific slots and ensuring the decoderutilizes them during reconstruction. This work introduces two novel techniques,(i) an attention-based self-training approach, which distills superiorslot-based attention masks from the decoder to the encoder, enhancing objectsegmentation, and (ii) an innovative patch-order permutation strategy forautoregressive transformers that strengthens the role of slot vectors inreconstruction. The effectiveness of these strategies is showcasedexperimentally. The combined approach significantly surpasses prior slot-basedautoencoder methods in unsupervised object segmentation, especially withcomplex real-world images. We provide the implementation code athttps://github.com/gkakogeorgiou/spot .</description><author>Ioannis Kakogeorgiou, Spyros Gidaris, Konstantinos Karantzalos, Nikos Komodakis</author><pubDate>Fri, 01 Dec 2023 15:20:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00648v1</guid></item><item><title>Does a Neural Network Really Encode Symbolic Concepts?</title><link>http://arxiv.org/abs/2302.13080v2</link><description>Recently, a series of studies have tried to extract interactions betweeninput variables modeled by a DNN and define such interactions as conceptsencoded by the DNN. However, strictly speaking, there still lacks a solidguarantee whether such interactions indeed represent meaningful concepts.Therefore, in this paper, we examine the trustworthiness of interactionconcepts from four perspectives. Extensive empirical studies have verified thata well-trained DNN usually encodes sparse, transferable, and discriminativeconcepts, which is partially aligned with human intuition.</description><author>Mingjie Li, Quanshi Zhang</author><pubDate>Fri, 01 Dec 2023 15:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13080v2</guid></item><item><title>Practical Blind Image Denoising via Swin-Conv-UNet and Data Synthesis</title><link>http://arxiv.org/abs/2203.13278v4</link><description>While recent years have witnessed a dramatic upsurge of exploiting deepneural networks toward solving image denoising, existing methods mostly rely onsimple noise assumptions, such as additive white Gaussian noise (AWGN), JPEGcompression noise and camera sensor noise, and a general-purpose blinddenoising method for real images remains unsolved. In this paper, we attempt tosolve this problem from the perspective of network architecture design andtraining data synthesis. Specifically, for the network architecture design, wepropose a swin-conv block to incorporate the local modeling ability of residualconvolutional layer and non-local modeling ability of swin transformer block,and then plug it as the main building block into the widely-used image-to-imagetranslation UNet architecture. For the training data synthesis, we design apractical noise degradation model which takes into consideration differentkinds of noise (including Gaussian, Poisson, speckle, JPEG compression, andprocessed camera sensor noises) and resizing, and also involves a randomshuffle strategy and a double degradation strategy. Extensive experiments onAGWN removal and real image denoising demonstrate that the new networkarchitecture design achieves state-of-the-art performance and the newdegradation model can help to significantly improve the practicability. Webelieve our work can provide useful insights into current denoising research.</description><author>Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Deng-Ping Fan, Radu Timofte, Luc Van Gool</author><pubDate>Fri, 01 Dec 2023 15:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.13278v4</guid></item><item><title>An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information</title><link>http://arxiv.org/abs/2305.03977v2</link><description>Non-autoregressive models have been widely studied in the CompleteInformation Scenario (CIS), in which the input has complete information ofcorresponding output. However, their explorations in the Incomplete InformationScenario (IIS) are extremely limited. Our analyses reveal that the IIS'sincomplete input information will augment the inherent limitations of existingnon-autoregressive models trained under Maximum Likelihood Estimation. In thispaper, we propose for the IIS an Adversarial Non-autoregressive Transformer(ANT) which has two features: 1) Position-Aware Self-Modulation to provide morereasonable hidden representations, and 2) Dependency Feed Forward Network tostrengthen its capacity in dependency modeling. We compare ANT with othermainstream models in the IIS and demonstrate that ANT can achieve comparableperformance with much fewer decoding iterations. Furthermore, we show its greatpotential in various applications like latent interpolation and semi-supervisedlearning.</description><author>Da Ren, Yi Cai, Qing Li</author><pubDate>Fri, 01 Dec 2023 15:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03977v2</guid></item><item><title>Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation</title><link>http://arxiv.org/abs/2312.00645v1</link><description>There is a growing need to gain insight into language model capabilities thatrelate to sensitive topics, such as bioterrorism or cyberwarfare. However,traditional open source benchmarks are not fit for the task, due to theassociated practice of publishing the correct answers in human-readable form.At the same time, enforcing mandatory closed-quarters evaluations might stifledevelopment and erode trust. In this context, we propose hashmarking, aprotocol for evaluating language models in the open without having to disclosethe correct answers. In its simplest form, a hashmark is a benchmark whosereference solutions have been cryptographically hashed prior to publication.Following an overview of the proposed evaluation protocol, we go on to assessits resilience against traditional attack vectors (e.g. rainbow table attacks),as well as against failure modes unique to increasingly capable generativemodels.</description><author>Paul Bricman</author><pubDate>Fri, 01 Dec 2023 15:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00645v1</guid></item><item><title>Trustworthy Large Models in Vision: A Survey</title><link>http://arxiv.org/abs/2311.09680v4</link><description>The rapid progress of Large Models (LMs) has recently revolutionized variousfields of deep learning with remarkable grades, ranging from Natural LanguageProcessing (NLP) to Computer Vision (CV). However, LMs are increasinglychallenged and criticized by academia and industry due to their powerfulperformance but untrustworthy behavior, which urgently needs to be alleviatedby reliable methods. Despite the abundance of literature on trustworthy LMs inNLP, a systematic survey specifically delving into the trustworthiness of LMsin CV remains absent. In order to mitigate this gap, we summarize four relevantconcerns that obstruct the trustworthy usage in vision of LMs in this survey,including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)interpretability. By highlighting corresponding challenge, countermeasures, anddiscussion in each topic, we hope this survey will facilitate readers'understanding of this field, promote alignment of LMs with human expectationsand enable trustworthy LMs to serve as welfare rather than disaster for humansociety.</description><author>Ziyan Guo, Li Xu, Jun Liu</author><pubDate>Fri, 01 Dec 2023 15:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09680v4</guid></item><item><title>Classification Utility, Fairness, and Compactness via Tunable Information Bottleneck and RÃ©nyi Measures</title><link>http://arxiv.org/abs/2206.10043v3</link><description>Designing machine learning algorithms that are accurate yet fair, notdiscriminating based on any sensitive attribute, is of paramount importance forsociety to accept AI for critical applications. In this article, we propose anovel fair representation learning method termed the R\'enyi Fair InformationBottleneck Method (RFIB) which incorporates constraints for utility, fairness,and compactness (compression) of representation, and apply it to image andtabular data classification. A key attribute of our approach is that weconsider - in contrast to most prior work - both demographic parity andequalized odds as fairness constraints, allowing for a more nuancedsatisfaction of both criteria. Leveraging a variational approach, we show thatour objectives yield a loss function involving classical Information Bottleneck(IB) measures and establish an upper bound in terms of two R\'enyi measures oforder $\alpha$ on the mutual information IB term measuring compactness betweenthe input and its encoded embedding. We study the influence of the $\alpha$parameter as well as two other tunable IB parameters on achievingutility/fairness trade-off goals, and show that the $\alpha$ parameter gives anadditional degree of freedom that can be used to control the compactness of therepresentation. Experimenting on three different image datasets (EyePACS,CelebA, and FairFace) and two tabular datasets (Adult and COMPAS), using bothbinary and categorical sensitive attributes, we show that on various utility,fairness, and compound utility/fairness metrics RFIB outperforms currentstate-of-the-art approaches.</description><author>Adam Gronowski, William Paul, Fady Alajaji, Bahman Gharesifard, Philippe Burlina</author><pubDate>Fri, 01 Dec 2023 15:06:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10043v3</guid></item><item><title>One to beat them all: "RYU'' -- a unifying framework for the construction of safe balls</title><link>http://arxiv.org/abs/2312.00640v1</link><description>In this paper, we put forth a novel framework (named ``RYU'') for theconstruction of ``safe'' balls, i.e. regions that provably contain the dualsolution of a target optimization problem. We concentrate on the standard setupwhere the cost function is the sum of two terms: a closed, proper, convexLipschitz-smooth function and a closed, proper, convex function. The RYUframework is shown to generalize or improve upon all the results proposed inthe last decade for the considered family of optimization problems.</description><author>Thu-Le Tran, ClÃ©ment Elvira, Hong-Phuong Dang, CÃ©dric Herzet</author><pubDate>Fri, 01 Dec 2023 15:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00640v1</guid></item><item><title>EvE: Exploiting Generative Priors for Radiance Field Enrichment</title><link>http://arxiv.org/abs/2312.00639v1</link><description>Modeling large-scale scenes from unconstrained image collections in-the-wildhas proven to be a major challenge in computer vision. Existing methodstackling in-the-wild neural rendering operate in a closed-world setting, whereknowledge is limited to a scene's captured images within a training set. Wepropose EvE, which is, to the best of our knowledge, the first methodleveraging generative priors to improve in-the-wild scene modeling. We employpre-trained generative networks to enrich K-Planes representations withextrinsic knowledge. To this end, we define an alternating training procedureto conduct optimization guidance of K-Planes trained on the training set. Wecarry out extensive experiments and verify the merit of our method on syntheticdata as well as real tourism photo collections. EvE enhances rendered sceneswith richer details and outperforms the state of the art on the task of novelview synthesis in-the-wild. Our project page can be found athttps://eve-nvs.github.io .</description><author>Karim Kassab, Antoine Schnepf, Jean-Yves Franceschi, Laurent Caraffa, Jeremie Mary, ValÃ©rie Gouet-Brunet</author><pubDate>Fri, 01 Dec 2023 14:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00639v1</guid></item><item><title>A Recent Survey of Vision Transformers for Medical Image Segmentation</title><link>http://arxiv.org/abs/2312.00634v1</link><description>Medical image segmentation plays a crucial role in various healthcareapplications, enabling accurate diagnosis, treatment planning, and diseasemonitoring. In recent years, Vision Transformers (ViTs) have emerged as apromising technique for addressing the challenges in medical imagesegmentation. In medical images, structures are usually highly interconnectedand globally distributed. ViTs utilize their multi-scale attention mechanism tomodel the long-range relationships in the images. However, they do lackimage-related inductive bias and translational invariance, potentiallyimpacting their performance. Recently, researchers have come up with variousViT-based approaches that incorporate CNNs in their architectures, known asHybrid Vision Transformers (HVTs) to capture local correlation in addition tothe global information in the images. This survey paper provides a detailedreview of the recent advancements in ViTs and HVTs for medical imagesegmentation. Along with the categorization of ViT and HVT-based medical imagesegmentation approaches we also present a detailed overview of their real-timeapplications in several medical image modalities. This survey may serve as avaluable resource for researchers, healthcare practitioners, and students inunderstanding the state-of-the-art approaches for ViT-based medical imagesegmentation.</description><author>Asifullah Khan, Zunaira Rauf, Abdul Rehman Khan, Saima Rathore, Saddam Hussain Khan, Sahar Shah, Umair Farooq, Hifsa Asif, Aqsa Asif, Umme Zahoora, Rafi Ullah Khalil, Suleman Qamar, Umme Hani Asif, Faiza Babar Khan, Abdul Majid, Jeonghwan Gwak</author><pubDate>Fri, 01 Dec 2023 14:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00634v1</guid></item><item><title>Towards Efficient 3D Object Detection in Bird's-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach</title><link>http://arxiv.org/abs/2312.00633v1</link><description>3D object detection in Bird's-Eye-View (BEV) space has recently emerged as aprevalent approach in the field of autonomous driving. Despite the demonstratedimprovements in accuracy and velocity estimation compared to perspective viewmethods, the deployment of BEV-based techniques in real-world autonomousvehicles remains challenging. This is primarily due to their reliance onvision-transformer (ViT) based architectures, which introduce quadraticcomplexity with respect to the input resolution. To address this issue, wepropose an efficient BEV-based 3D detection framework called BEVENet, whichleverages a convolutional-only architectural design to circumvent thelimitations of ViT models while maintaining the effectiveness of BEV-basedmethods. Our experiments show that BEVENet is 3$\times$ faster thancontemporary state-of-the-art (SOTA) approaches on the NuScenes challenge,achieving a mean average precision (mAP) of 0.456 and a nuScenes detectionscore (NDS) of 0.555 on the NuScenes validation dataset, with an inferencespeed of 47.6 frames per second. To the best of our knowledge, this studystands as the first to achieve such significant efficiency improvements forBEV-based methods, highlighting their enhanced feasibility for real-worldautonomous driving applications.</description><author>Yuxin Li, Qiang Han, Mengying Yu, Yuxin Jiang, Chaikiat Yeo, Yiheng Li, Zihang Huang, Nini Liu, Hsuanhan Chen, Xiaojun Wu</author><pubDate>Fri, 01 Dec 2023 14:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00633v1</guid></item><item><title>Joint Detection Algorithm for Multiple Cognitive Users in Spectrum Sensing</title><link>http://arxiv.org/abs/2311.18599v2</link><description>Spectrum sensing technology is a crucial aspect of modern communicationtechnology, serving as one of the essential techniques for efficientlyutilizing scarce information resources in tight frequency bands. This paperfirst introduces three common logical circuit decision criteria in harddecisions and analyzes their decision rigor. Building upon hard decisions, thepaper further introduces a method for multi-user spectrum sensing based on softdecisions. Then the paper simulates the false alarm probability and detectionprobability curves corresponding to the three criteria. The simulated resultsof multi-user collaborative sensing indicate that the simulation processsignificantly reduces false alarm probability and enhances detectionprobability. This approach effectively detects spectrum resources unoccupiedduring idle periods, leveraging the concept of time-division multiplexing andrationalizing the redistribution of information resources. The entirecomputation process relies on the calculation principles of power spectraldensity in communication theory, involving threshold decision detection fornoise power and the sum of noise and signal power. It provides a secondarydecision detection, reflecting the perceptual decision performance of logicaldetection methods with relative accuracy.</description><author>Fanfei Meng, Yuxin Wang, Lele Zhang, Yingxin Zhao</author><pubDate>Fri, 01 Dec 2023 14:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18599v2</guid></item><item><title>Rethinking the Domain Gap in Near-infrared Face Recognition</title><link>http://arxiv.org/abs/2312.00627v1</link><description>Heterogeneous face recognition (HFR) involves the intricate task of matchingface images across the visual domains of visible (VIS) and near-infrared (NIR).While much of the existing literature on HFR identifies the domain gap as aprimary challenge and directs efforts towards bridging it at either the inputor feature level, our work deviates from this trend. We observe that largeneural networks, unlike their smaller counterparts, when pre-trained on largescale homogeneous VIS data, demonstrate exceptional zero-shot performance inHFR, suggesting that the domain gap might be less pronounced than previouslybelieved. By approaching the HFR problem as one of low-data fine-tuning, weintroduce a straightforward framework: comprehensive pre-training, succeeded bya regularized fine-tuning strategy, that matches or surpasses the currentstate-of-the-art on four publicly available benchmarks. Corresponding codes canbe found at https://github.com/michaeltrs/RethinkNIRVIS.</description><author>Michail Tarasiou, Jiankang Deng, Stefanos Zafeiriou</author><pubDate>Fri, 01 Dec 2023 14:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00627v1</guid></item><item><title>Forecasting Trends in Food Security: a Reservoir Computing Approach</title><link>http://arxiv.org/abs/2312.00626v1</link><description>Early warning systems are an essential tool for effective humanitarianaction. Advance warnings on impending disasters facilitate timely and targetedresponse which help save lives, livelihoods, and scarce financial resources. Inthis work we present a new quantitative methodology to forecast levels of foodconsumption for 60 consecutive days, at the sub-national level, in fourcountries: Mali, Nigeria, Syria, and Yemen. The methodology is built onpublicly available data from the World Food Programme's integrated globalhunger monitoring system which collects, processes, and displays daily updateson key food security metrics, conflict, weather events, and other drivers offood insecurity across 90 countries (https://hungermap.wfp.org/). In thisstudy, we assessed the performance of various models including ARIMA, XGBoost,LSTMs, CNNs, and Reservoir Computing (RC), by comparing their Root Mean SquaredError (RMSE) metrics. This comprehensive analysis spanned classicalstatistical, machine learning, and deep learning approaches. Our findingshighlight Reservoir Computing as a particularly well-suited model in the fieldof food security given both its notable resistance to over-fitting on limiteddata samples and its efficient training capabilities. The methodology weintroduce establishes the groundwork for a global, data-driven early warningsystem designed to anticipate and detect food insecurity.</description><author>Joschka Herteux, Christoph RÃ¤th, Amine Baha, Giulia Martini, Duccio Piovani</author><pubDate>Fri, 01 Dec 2023 14:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00626v1</guid></item><item><title>Practical Path-based Bayesian Optimization</title><link>http://arxiv.org/abs/2312.00622v1</link><description>There has been a surge in interest in data-driven experimental design withapplications to chemical engineering and drug manufacturing. Bayesianoptimization (BO) has proven to be adaptable to such cases, since we can modelthe reactions of interest as expensive black-box functions. Sometimes, the costof this black-box functions can be separated into two parts: (a) the cost ofthe experiment itself, and (b) the cost of changing the input parameters. Inthis short paper, we extend the SnAKe algorithm to deal with both types ofcosts simultaneously. We further propose extensions to the case of a maximumallowable input change, as well as to the multi-objective setting.</description><author>Jose Pablo Folch, James Odgers, Shiqiang Zhang, Robert M Lee, Behrang Shafei, David Walz, Calvin Tsay, Mark van der Wilk, Ruth Misener</author><pubDate>Fri, 01 Dec 2023 14:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00622v1</guid></item><item><title>Weighted Riesz Particles</title><link>http://arxiv.org/abs/2312.00621v1</link><description>Markov chain Monte Carlo (MCMC) methods are simulated by local exploration ofcomplex statistical distributions, and while bypassing the cumbersomerequirement of a specific analytical expression for the target, this stochasticexploration of an uncertain parameter space comes at the expense of a largenumber of samples, and this computational complexity increases with parameterdimensionality. Although at the exploration level, some methods are proposed toaccelerate the convergence of the algorithm, such as tempering, HamiltonianMonte Carlo, Rao-redwellization, and scalable methods for better performance,it cannot avoid the stochastic nature of this exploration. We consider thetarget distribution as a mapping where the infinite-dimensional Eulerian spaceof the parameters consists of a number of deterministic submanifolds andpropose a generalized energy metric, termed weighted Riesz energy, where anumber of points is generated through pairwise interactions, to discretizerectifiable submanifolds. We study the properties of the point, called Rieszparticle, and embed it into sequential MCMC, and we find that there will behigher acceptance rates with fewer evaluations, we validate it throughexperimental comparative analysis from a linear Gaussian state-space model withsynthetic data and a non-linear stochastic volatility model with real-worlddata.</description><author>Xiongming Dai, Gerald Baumgartner</author><pubDate>Fri, 01 Dec 2023 14:36:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00621v1</guid></item><item><title>Spatio-Angular Convolutions for Super-resolution in Diffusion MRI</title><link>http://arxiv.org/abs/2306.00854v3</link><description>Diffusion MRI (dMRI) is a widely used imaging modality, but requires longscanning times to acquire high resolution datasets. By leveraging the uniquegeometry present within this domain, we present a novel approach to dMRIangular super-resolution that extends upon the parametric continuousconvolution (PCConv) framework. We introduce several additions to the operationincluding a Fourier feature mapping, global coordinates, and domain specificcontext. Using this framework, we build a fully parametric continuousconvolution network (PCCNN) and compare against existing models. We demonstratethe PCCNN performs competitively while using significantly less parameters.Moreover, we show that this formulation generalises well to clinically relevantdownstream analyses such as fixel-based analysis, and neurite orientationdispersion and density imaging.</description><author>Matthew Lyon, Paul Armitage, Mauricio A Ãlvarez</author><pubDate>Fri, 01 Dec 2023 14:30:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00854v3</guid></item><item><title>AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor</title><link>http://arxiv.org/abs/2306.14505v2</link><description>Magnetic resonance imaging (MRI) is commonly used for brain tumorsegmentation, which is critical for patient evaluation and treatment planning.To reduce the labor and expertise required for labeling, weakly-supervisedsemantic segmentation (WSSS) methods with class activation mapping (CAM) havebeen proposed. However, existing CAM methods suffer from low resolution due tostrided convolution and pooling layers, resulting in inaccurate predictions. Inthis study, we propose a novel CAM method, Attentive Multiple-Exit CAM(AME-CAM), that extracts activation maps from multiple resolutions tohierarchically aggregate and improve prediction accuracy. We evaluate ourmethod on the BraTS 2021 dataset and show that it outperforms state-of-the-artmethods.</description><author>Yu-Jen Chen, Xinrong Hu, Yiyu Shi, Tsung-Yi Ho</author><pubDate>Fri, 01 Dec 2023 14:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14505v2</guid></item><item><title>Investigating a domain adaptation approach for integrating different measurement instruments in a longitudinal clinical registry</title><link>http://arxiv.org/abs/2312.00616v1</link><description>In a longitudinal clinical registry, different measurement instruments mighthave been used for assessing individuals at different time points. To combinethem, we investigate deep learning techniques for obtaining a joint latentrepresentation, to which the items of different measurement instruments aremapped. This corresponds to domain adaptation, an established concept incomputer science for image data. Using the proposed approach as an example, weevaluate the potential of domain adaptation in a longitudinal cohort settingwith a rather small number of time points, motivated by an application withdifferent motor function measurement instruments in a registry of spinalmuscular atrophy (SMA) patients. There, we model trajectories in the latentrepresentation by ordinary differential equations (ODEs), where person-specificODE parameters are inferred from baseline characteristics. The goodness of fitand complexity of the ODE solutions then allows to judge the measurementinstrument mappings. We subsequently explore how alignment can be improved byincorporating corresponding penalty terms into model fitting. To systematicallyinvestigate the effect of differences between measurement instruments, weconsider several scenarios based on modified SMA data, including scenarioswhere a mapping should be feasible in principle and scenarios where no perfectmapping is available. While misalignment increases in more complex scenarios,some structure is still recovered, even if the availability of measurementinstruments depends on patient state. A reasonable mapping is feasible also inthe more complex real SMA dataset. These results indicate that domainadaptation might be more generally useful in statistical modeling forlongitudinal registry data.</description><author>Maren Hackenberg, Michelle Pfaffenlehner, Max Behrens, Astrid Pechmann, Janbernd Kirschner, Harald Binder</author><pubDate>Fri, 01 Dec 2023 14:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00616v1</guid></item><item><title>RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment</title><link>http://arxiv.org/abs/2304.06767v4</link><description>Generative foundation models are susceptible to implicit biases that canarise from extensive unsupervised training data. Such biases can producesuboptimal samples, skewed outcomes, and unfairness, with potentially seriousconsequences. Consequently, aligning these models with human ethics andpreferences is an essential step toward ensuring their responsible andeffective deployment in real-world applications. Prior research has primarilyemployed Reinforcement Learning from Human Feedback (RLHF) to address thisproblem, where generative models are fine-tuned with RL algorithms guided by ahuman-feedback-informed reward model. However, the inefficiencies andinstabilities associated with RL algorithms frequently present substantialobstacles to the successful alignment, necessitating the development of a morerobust and streamlined approach. To this end, we introduce a new framework,Reward rAnked FineTuning (RAFT), designed to align generative modelseffectively. Utilizing a reward model and a sufficient number of samples, ourapproach selects the high-quality samples, discarding those that exhibitundesired behavior, and subsequently enhancing the model by fine-tuning onthese filtered samples. Our studies show that RAFT can effectively improve themodel performance in both reward learning and other automated metrics in bothlarge language models and diffusion models.</description><author>Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang</author><pubDate>Fri, 01 Dec 2023 14:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06767v4</guid></item><item><title>Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings</title><link>http://arxiv.org/abs/2306.17670v3</link><description>Spiking Neural Networks (SNNs) are a promising research direction forbuilding power-efficient information processing systems, especially fortemporal tasks such as speech recognition. In SNNs, delays refer to the timeneeded for one spike to travel from one neuron to another. These delays matterbecause they influence the spike arrival times, and it is well-known thatspiking neurons respond more strongly to coincident input spikes. Moreformally, it has been shown theoretically that plastic delays greatly increasethe expressivity in SNNs. Yet, efficient algorithms to learn these delays havebeen lacking. Here, we propose a new discrete-time algorithm that addressesthis issue in deep feedforward SNNs using backpropagation, in an offlinemanner. To simulate delays between consecutive layers, we use 1D convolutionsacross time. The kernels contain only a few non-zero weights - one per synapse- whose positions correspond to the delays. These positions are learnedtogether with the weights using the recently proposed Dilated Convolution withLearnable Spacings (DCLS). We evaluated our method on three datasets: theSpiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and itsnon-spiking version Google Speech Commands v0.02 (GSC) benchmarks, whichrequire detecting temporal patterns. We used feedforward SNNs with two or threehidden fully connected layers, and vanilla leaky integrate-and-fire neurons. Weshowed that fixed random delays help and that learning them helps even more.Furthermore, our method outperformed the state-of-the-art in the three datasetswithout using recurrent connections and with substantially fewer parameters.Our work demonstrates the potential of delay learning in developing accurateand precise models for temporal data processing. Our code is based on PyTorch /SpikingJelly and available at: https://github.com/Thvnvtos/SNN-delays</description><author>Ilyass Hammouamri, Ismail Khalfaoui-Hassani, TimothÃ©e Masquelier</author><pubDate>Fri, 01 Dec 2023 14:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17670v3</guid></item><item><title>Decentralized policy learning with partial observation and mechanical constraints for multiperson modeling</title><link>http://arxiv.org/abs/2007.03155v2</link><description>Extracting the rules of real-world multi-agent behaviors is a currentchallenge in various scientific and engineering fields. Biological agentsindependently have limited observation and mechanical constraints; however,most of the conventional data-driven models ignore such assumptions, resultingin lack of biological plausibility and model interpretability for behavioralanalyses. Here we propose sequential generative models with partial observationand mechanical constraints in a decentralized manner, which can model agents'cognition and body dynamics, and predict biologically plausible behaviors. Weformulate this as a decentralized multi-agent imitation-learning problem,leveraging binary partial observation and decentralized policy models based onhierarchical variational recurrent neural networks with physical andbiomechanical penalties. Using real-world basketball and soccer datasets, weshow the effectiveness of our method in terms of the constraint violations,long-term trajectory prediction, and partial observation. Our approach can beused as a multi-agent simulator to generate realistic trajectories usingreal-world data.</description><author>Keisuke Fujii, Naoya Takeishi, Yoshinobu Kawahara, Kazuya Takeda</author><pubDate>Fri, 01 Dec 2023 14:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.03155v2</guid></item><item><title>Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search</title><link>http://arxiv.org/abs/2304.04336v3</link><description>Achieving tight bounding boxes of a shape while guaranteeing completeboundness is an essential task for efficient geometric operations andunsupervised semantic part detection. But previous methods fail to achieve bothfull coverage and tightness. Neural-network-based methods are not suitable forthese goals due to the non-differentiability of the objective, while classiciterative search methods suffer from their sensitivity to the initialization.We propose a novel framework for finding a set of tight bounding boxes of a 3Dshape via over-segmentation and iterative merging and refinement. Our resultshows that utilizing effective search methods with appropriate objectives isthe key to producing bounding boxes with both properties. We employ an existingpre-segmentation to split the shape and obtain over-segmentation. Then, weapply hierarchical merging with our novel tightness-aware merging and stoppingcriteria. To overcome the sensitivity to the initialization, we also defineactions to refine the bounding box parameters in an Markov Decision Process(MDP) setup with a soft reward function promoting a wider exploration. Lastly,we further improve the refinement step with Monte Carlo Tree Search (MCTS)based multi-action space exploration. By thoughtful evaluation on diverse 3Dshapes, we demonstrate full coverage, tightness, and an adequate number ofbounding boxes of our method without requiring any training data orsupervision. It thus can be applied to various downstream tasks in computervision and graphics.</description><author>Chanhyeok Park, Minhyuk Sung</author><pubDate>Fri, 01 Dec 2023 14:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04336v3</guid></item><item><title>Improving Plasticity in Online Continual Learning via Collaborative Learning</title><link>http://arxiv.org/abs/2312.00600v1</link><description>Online Continual Learning (CL) solves the problem of learning theever-emerging new classification tasks from a continuous data stream. Unlikeits offline counterpart, in online CL, the training data can only be seen once.Most existing online CL research regards catastrophic forgetting (i.e., modelstability) as almost the only challenge. In this paper, we argue that themodel's capability to acquire new knowledge (i.e., model plasticity) is anotherchallenge in online CL. While replay-based strategies have been shown to beeffective in alleviating catastrophic forgetting, there is a notable gap inresearch attention toward improving model plasticity. To this end, we proposeCollaborative Continual Learning (CCL), a collaborative learning based strategyto improve the model's capability in acquiring new concepts. Additionally, weintroduce Distillation Chain (DC), a novel collaborative learning scheme toboost the training of the models. We adapted CCL-DC to existing representativeonline CL works. Extensive experiments demonstrate that even if the learnersare well-trained with state-of-the-art online CL methods, our strategy canstill improve model plasticity dramatically, and thereby improve the overallperformance by a large margin.</description><author>Maorong Wang, Nicolas Michel, Ling Xiao, Toshihiko Yamasaki</author><pubDate>Fri, 01 Dec 2023 14:06:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00600v1</guid></item><item><title>Learning from One Continuous Video Stream</title><link>http://arxiv.org/abs/2312.00598v1</link><description>We introduce a framework for online learning from a single continuous videostream -- the way people and animals learn, without mini-batches, dataaugmentation or shuffling. This poses great challenges given the highcorrelation between consecutive video frames and there is very little priorwork on it. Our framework allows us to do a first deep dive into the topic andincludes a collection of streams and tasks composed from two existing videodatasets, plus methodology for performance evaluation that considers bothadaptation and generalization. We employ pixel-to-pixel modelling as apractical and flexible way to switch between pre-training and single-streamevaluation as well as between arbitrary tasks, without ever requiring changesto models and always using the same pixel loss. Equipped with this framework weobtained large single-stream learning gains from pre-training with a novelfamily of future prediction tasks, found that momentum hurts, and that the paceof weight updates matters. The combination of these insights leads to matchingthe performance of IID learning with batch size 1, when using the samearchitecture and without costly replay buffers.</description><author>JoÃ£o Carreira, Michael King, Viorica PÄtrÄucean, Dilara Gokay, CÄtÄlin Ionescu, Yi Yang, Daniel Zoran, Joseph Heyward, Carl Doersch, Yusuf Aytar, Dima Damen, Andrew Zisserman</author><pubDate>Fri, 01 Dec 2023 14:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00598v1</guid></item><item><title>Use Perturbations when Learning from Explanations</title><link>http://arxiv.org/abs/2303.06419v3</link><description>Machine learning from explanations (MLX) is an approach to learning that useshuman-provided explanations of relevant or irrelevant features for each inputto ensure that model predictions are right for the right reasons. Existing MLXapproaches rely on local model interpretation methods and require strong modelsmoothing to align model and human explanations, leading to sub-optimalperformance. We recast MLX as a robustness problem, where human explanationsspecify a lower dimensional manifold from which perturbations can be drawn, andshow both theoretically and empirically how this approach alleviates the needfor strong model smoothing. We consider various approaches to achievingrobustness, leading to improved performance over prior MLX methods. Finally, weshow how to combine robustness with an earlier MLX method, yieldingstate-of-the-art results on both synthetic and real-world benchmarks.</description><author>Juyeon Heo, Vihari Piratla, Matthew Wicker, Adrian Weller</author><pubDate>Fri, 01 Dec 2023 14:03:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06419v3</guid></item><item><title>UAVs and Birds: Enhancing Short-Range Navigation through Budgerigar Flight Studies</title><link>http://arxiv.org/abs/2312.00597v1</link><description>This study delves into the flight behaviors of Budgerigars (Melopsittacusundulatus) to gain insights into their flight trajectories and movements. Using3D reconstruction from stereo video camera recordings, we closely examine thevelocity and acceleration patterns during three flight motion takeoff, flyingand landing. The findings not only contribute to our understanding of birdbehaviors but also hold significant implications for the advancement ofalgorithms in Unmanned Aerial Vehicles (UAVs). The research aims to bridge thegap between biological principles observed in birds and the application ofthese insights in developing more efficient and autonomous UAVs. In the contextof the increasing use of drones, this study focuses on the biologicallyinspired principles drawn from bird behaviors, particularly during takeoff,flying and landing flight, to enhance UAV capabilities. The dataset created forthis research sheds light on Budgerigars' takeoff, flying, and landingtechniques, emphasizing their ability to control speed across differentsituations and surfaces. The study underscores the potential of incorporatingthese principles into UAV algorithms, addressing challenges related toshort-range navigation, takeoff, flying, and landing.</description><author>Md. Mahmudur Rahman, Sajid Islam, Showren Chowdhury, Sadia Jahan Zeba, Debajyoti Karmaker</author><pubDate>Fri, 01 Dec 2023 14:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00597v1</guid></item><item><title>BCN: Batch Channel Normalization for Image Classification</title><link>http://arxiv.org/abs/2312.00596v1</link><description>Normalization techniques have been widely used in the field of deep learningdue to their capability of enabling higher learning rates and are less carefulin initialization. However, the effectiveness of popular normalizationtechnologies is typically limited to specific areas. Unlike the standard BatchNormalization (BN) and Layer Normalization (LN), where BN computes the mean andvariance along the (N,H,W) dimensions and LN computes the mean and variancealong the (C,H,W) dimensions (N, C, H and W are the batch, channel, spatialheight and width dimension, respectively), this paper presents a novelnormalization technique called Batch Channel Normalization (BCN). To exploitboth the channel and batch dependence and adaptively and combine the advantagesof BN and LN based on specific datasets or tasks, BCN separately normalizesinputs along the (N, H, W) and (C, H, W) axes, then combines the normalizedoutputs based on adaptive parameters. As a basic block, BCN can be easilyintegrated into existing models for various applications in the field ofcomputer vision. Empirical results show that the proposed technique can beseamlessly applied to various versions of CNN or Vision Transformerarchitecture. The code is publicly available athttps://github.com/AfifaKhaled/BatchChannel-Normalization</description><author>Afifa Khaled, Chao Li, Jia Ning, Kun He</author><pubDate>Fri, 01 Dec 2023 14:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00596v1</guid></item><item><title>Event Recognition in Laparoscopic Gynecology Videos with Hybrid Transformers</title><link>http://arxiv.org/abs/2312.00593v1</link><description>Analyzing laparoscopic surgery videos presents a complex and multifacetedchallenge, with applications including surgical training, intra-operativesurgical complication prediction, and post-operative surgical assessment.Identifying crucial events within these videos is a significant prerequisite ina majority of these applications. In this paper, we introduce a comprehensivedataset tailored for relevant event recognition in laparoscopic gynecologyvideos. Our dataset includes annotations for critical events associated withmajor intra-operative challenges and post-operative complications. To validatethe precision of our annotations, we assess event recognition performance usingseveral CNN-RNN architectures. Furthermore, we introduce and evaluate a hybridtransformer architecture coupled with a customized training-inference frameworkto recognize four specific events in laparoscopic surgery videos. Leveragingthe Transformer networks, our proposed architecture harnesses inter-framedependencies to counteract the adverse effects of relevant content occlusion,motion blur, and surgical scene variation, thus significantly enhancing eventrecognition accuracy. Moreover, we present a frame sampling strategy designedto manage variations in surgical scenes and the surgeons' skill level,resulting in event recognition with high temporal resolution. We empiricallydemonstrate the superiority of our proposed methodology in event recognitioncompared to conventional CNN-RNN architectures through a series of extensiveexperiments.</description><author>Sahar Nasirihaghighi, Negin Ghamsarian, Heinrich Husslein, Klaus Schoeffmann</author><pubDate>Fri, 01 Dec 2023 13:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00593v1</guid></item><item><title>Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection (extended version)</title><link>http://arxiv.org/abs/2312.00592v1</link><description>Reinforcement learning (RL) for robot control typically requires a detailedrepresentation of the environment state, including information abouttask-relevant objects not directly measurable. Keypoint detectors, such asspatial autoencoders (SAEs), are a common approach to extracting alow-dimensional representation from high-dimensional image data. SAEs aim atspatial features such as object positions, which are often usefulrepresentations in robotic RL. However, whether an SAE is actually able totrack objects in the scene and thus yields a spatial state representation wellsuited for RL tasks has rarely been examined due to a lack of establishedmetrics. In this paper, we propose to assess the performance of an SAE instanceby measuring how well keypoints track ground truth objects in images. Wepresent a computationally lightweight metric and use it to evaluate commonbaseline SAE architectures on image data from a simulated robot task. We findthat common SAEs differ substantially in their spatial extraction capability.Furthermore, we validate that SAEs that perform well in our metric achievesuperior performance when used in downstream RL. Thus, our metric is aneffective and lightweight indicator of RL performance before executingexpensive RL training. Building on these insights, we identify three keymodifications of SAE architectures to improve tracking performance. We make ourcode available at anonymous.4open.science/r/sae-rl.</description><author>Emma Cramer, Jonas Reiher, Sebastian Trimpe</author><pubDate>Fri, 01 Dec 2023 13:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00592v1</guid></item><item><title>Less is More: Learning Reference Knowledge Using No-Reference Image Quality Assessment</title><link>http://arxiv.org/abs/2312.00591v1</link><description>Image Quality Assessment (IQA) with reference images have achieved greatsuccess by imitating the human vision system, in which the image quality iseffectively assessed by comparing the query image with its pristine referenceimage. However, for the images in the wild, it is quite difficult to accessaccurate reference images. We argue that it is possible to learn referenceknowledge under the No-Reference Image Quality Assessment (NR-IQA) setting,which is effective and efficient empirically. Concretely, by innovativelyintroducing a novel feature distillation method in IQA, we propose a newframework to learn comparative knowledge from non-aligned reference images. Andthen, to achieve fast convergence and avoid overfitting, we further propose aninductive bias regularization. Such a framework not only solves the congenitaldefects of NR-IQA but also improves the feature extraction framework, enablingit to express more abundant quality information. Surprisingly, our methodutilizes less input while obtaining a more significant improvement compared tothe teacher models. Extensive experiments on eight standard NR-IQA datasetsdemonstrate the superior performance to the state-of-the-art NR-IQA methods,i.e., achieving the PLCC values of 0.917 (vs. 0.884 in LIVEC) and 0.686 (vs.0.661 in LIVEFB).</description><author>Xudong Li, Jingyuan Zheng, Xiawu Zheng, Runze Hu, Enwei Zhang, Yuting Gao, Yunhang Shen, Ke Li, Yutao Liu, Pingyang Dai, Yan Zhang, Rongrong Ji</author><pubDate>Fri, 01 Dec 2023 13:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00591v1</guid></item><item><title>Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control</title><link>http://arxiv.org/abs/2205.11956v4</link><description>Most machine learning methods require tuning of hyper-parameters. For kernelridge regression with the Gaussian kernel, the hyper-parameter is thebandwidth. The bandwidth specifies the length scale of the kernel and has to becarefully selected to obtain a model with good generalization. The defaultmethods for bandwidth selection, cross-validation and marginal likelihoodmaximization, often yield good results, albeit at high computational costs.Inspired by Jacobian regularization, we formulate an approximate expression forhow the derivatives of the functions inferred by kernel ridge regression withthe Gaussian kernel depend on the kernel bandwidth. We use this expression topropose a closed-form, computationally feather-light, bandwidth selectionheuristic, based on controlling the Jacobian. In addition, the Jacobianexpression illuminates how the bandwidth selection is a trade-off between thesmoothness of the inferred function and the conditioning of the training datakernel matrix. We show on real and synthetic data that compared tocross-validation and marginal likelihood maximization, our method is on pair interms of model performance, but up to six orders of magnitude faster.</description><author>Oskar Allerbo, Rebecka JÃ¶rnsten</author><pubDate>Fri, 01 Dec 2023 13:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.11956v4</guid></item><item><title>A Definition of Continual Reinforcement Learning</title><link>http://arxiv.org/abs/2307.11046v2</link><description>In a standard view of the reinforcement learning problem, an agent's goal isto efficiently identify a policy that maximizes long-term reward. However, thisperspective is based on a restricted view of learning as finding a solution,rather than treating learning as endless adaptation. In contrast, continualreinforcement learning refers to the setting in which the best agents neverstop learning. Despite the importance of continual reinforcement learning, thecommunity lacks a simple definition of the problem that highlights itscommitments and makes its primary concepts precise and clear. To this end, thispaper is dedicated to carefully defining the continual reinforcement learningproblem. We formalize the notion of agents that "never stop learning" through anew mathematical language for analyzing and cataloging agents. Using this newlanguage, we define a continual learning agent as one that can be understood ascarrying out an implicit search process indefinitely, and continualreinforcement learning as the setting in which the best agents are allcontinual learning agents. We provide two motivating examples, illustratingthat traditional views of multi-task reinforcement learning and continualsupervised learning are special cases of our definition. Collectively, thesedefinitions and perspectives formalize many intuitive concepts at the heart oflearning, and open new research pathways surrounding continual learning agents.</description><author>David Abel, AndrÃ© Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh</author><pubDate>Fri, 01 Dec 2023 13:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11046v2</guid></item><item><title>Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning</title><link>http://arxiv.org/abs/2305.17886v2</link><description>Analysis of invasive sports such as soccer is challenging because the gamesituation changes continuously in time and space, and multiple agentsindividually recognize the game situation and make decisions. Previous studiesusing deep reinforcement learning have often considered teams as a single agentand valued the teams and players who hold the ball in each discrete event. Thenit was challenging to value the actions of multiple players, including playersfar from the ball, in a spatiotemporally continuous state space. In this paper,we propose a method of valuing possible actions for on- and off-ball soccerplayers in a single holistic framework based on multi-agent deep reinforcementlearning. We consider a discrete action space in a continuous state space thatmimics that of Google research football and leverages supervised learning foractions in reinforcement learning. In the experiment, we analyzed therelationships with conventional indicators, season goals, and game ratings byexperts, and showed the effectiveness of the proposed method. Our approach canassess how multiple players move continuously throughout the game, which isdifficult to be discretized or labeled but vital for teamwork, scouting, andfan engagement.</description><author>Hiroshi Nakahara, Kazushi Tsutsui, Kazuya Takeda, Keisuke Fujii</author><pubDate>Fri, 01 Dec 2023 13:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17886v2</guid></item><item><title>Explainable Fraud Detection with Deep Symbolic Classification</title><link>http://arxiv.org/abs/2312.00586v1</link><description>There is a growing demand for explainable, transparent, and data-drivenmodels within the domain of fraud detection. Decisions made by fraud detectionmodels need to be explainable in the event of a customer dispute. Additionally,the decision-making process in the model must be transparent to win the trustof regulators and business stakeholders. At the same time, fraud detectionsolutions can benefit from data due to the noisy, dynamic nature of fraud andthe availability of large historical data sets. Finally, fraud detection isnotorious for its class imbalance: there are typically several orders ofmagnitude more legitimate transactions than fraudulent ones. In this paper, wepresent Deep Symbolic Classification (DSC), an extension of the Deep SymbolicRegression framework to classification problems. DSC casts classification as asearch problem in the space of all analytic functions composed of a vocabularyof variables, constants, and operations and optimizes for an arbitraryevaluation metric directly. The search is guided by a deep neural networktrained with reinforcement learning. Because the functions are mathematicalexpressions that are in closed-form and concise, the model is inherentlyexplainable both at the level of a single classification decision and themodel's decision process. Furthermore, the class imbalance problem issuccessfully addressed by optimizing for metrics that are robust to classimbalance such as the F1 score. This eliminates the need for oversampling andundersampling techniques that plague traditional approaches. Finally, the modelallows to explicitly balance between the prediction accuracy and theexplainability. An evaluation on the PaySim data set demonstrates competitivepredictive performance with state-of-the-art models, while surpassing them interms of explainability. This establishes DSC as a promising model for frauddetection systems.</description><author>Samantha Visbeek, Erman Acar, Floris den Hengst</author><pubDate>Fri, 01 Dec 2023 13:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00586v1</guid></item></channel></rss>