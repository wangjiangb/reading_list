<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 22 May 2023 10:36:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2305.11870v1</link><description>We propose a 3D generation pipeline that uses diffusion models to generaterealistic human digital avatars. Due to the wide variety of human identities,poses, and stochastic details, the generation of 3D human meshes has been achallenging problem. To address this, we decompose the problem into 2D normalmap generation and normal map-based 3D reconstruction. Specifically, we firstsimultaneously generate realistic normal maps for the front and backside of aclothed human, dubbed dual normal maps, using a pose-conditional diffusionmodel. For 3D reconstruction, we ``carve'' the prior SMPL-X mesh to a detailed3D mesh according to the normal maps through mesh optimization. To furtherenhance the high-frequency details, we present a diffusion resampling scheme onboth body and facial regions, thus encouraging the generation of realisticdigital avatars. We also seamlessly incorporate a recent text-to-imagediffusion model to support text-based human identity control. Our method,namely, Chupa, is capable of generating realistic 3D clothed humans with betterperceptual quality and identity variety.</description><author>Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, Hanbyul Joo</author><pubDate>Fri, 19 May 2023 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11870v1</guid></item><item><title>Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift Estimates via Deep Learning</title><link>http://arxiv.org/abs/2305.11869v1</link><description>Upcoming photometric surveys will discover tens of thousands of Type Iasupernovae (SNe Ia), vastly outpacing the capacity of our spectroscopicresources. In order to maximize the science return of these observations in theabsence of spectroscopic information, we must accurately extract keyparameters, such as SN redshifts, with photometric information alone. Wepresent Photo-zSNthesis, a convolutional neural network-based method forpredicting full redshift probability distributions from multi-band supernovalightcurves, tested on both simulated Sloan Digital Sky Survey (SDSS) and VeraC. Rubin Legacy Survey of Space and Time (LSST) data as well as observed SDSSSNe. We show major improvements over predictions from existing methods on bothsimulations and real observations as well as minimal redshift-dependent bias,which is a challenge due to selection effects, e.g. Malmquist bias. The PDFsproduced by this method are well-constrained and will maximize the cosmologicalconstraining power of photometric SNe Ia samples.</description><author>Helen Qu, Masao Sako</author><pubDate>Fri, 19 May 2023 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11869v1</guid></item><item><title>Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers</title><link>http://arxiv.org/abs/2301.11899v2</link><description>The sustained growth of carbon emissions and global waste elicits significantsustainability concerns for our environment's future. The growing Internet ofThings (IoT) has the potential to exacerbate this issue. However, an emergingarea known as Tiny Machine Learning (TinyML) has the opportunity to helpaddress these environmental challenges through sustainable computing practices.TinyML, the deployment of machine learning (ML) algorithms onto low-cost,low-power microcontroller systems, enables on-device sensor analytics thatunlocks numerous always-on ML applications. This article discusses both thepotential of these TinyML applications to address critical sustainabilitychallenges, as well as the environmental footprint of this emerging technology.Through a complete life cycle analysis (LCA), we find that TinyML systemspresent opportunities to offset their carbon emissions by enabling applicationsthat reduce the emissions of other sectors. Nevertheless, when globally scaled,the carbon footprint of TinyML systems is not negligible, necessitating thatdesigners factor in environmental impact when formulating new devices. Finally,we outline research directions to enable further sustainable contributions ofTinyML.</description><author>Shvetank Prakash, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete Warden, Brian Plancher, Vijay Janapa Reddi</author><pubDate>Fri, 19 May 2023 18:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11899v2</guid></item><item><title>North SÃ¡mi Dialect Identification with Self-supervised Speech Models</title><link>http://arxiv.org/abs/2305.11864v1</link><description>The North S\'{a}mi (NS) language encapsulates four primary dialectal variantsthat are related but that also have differences in their phonology, morphology,and vocabulary. The unique geopolitical location of NS speakers means that inmany cases they are bilingual in S\'{a}mi as well as in the dominant statelanguage: Norwegian, Swedish, or Finnish. This enables us to study the NSvariants both with respect to the spoken state language and their acousticcharacteristics. In this paper, we investigate an extensive set of acousticfeatures, including MFCCs and prosodic features, as well as state-of-the-artself-supervised representations, namely, XLS-R, WavLM, and HuBERT, for theautomatic detection of the four NS variants. In addition, we examine how themajority state language is reflected in the dialects. Our results show that NSdialects are influenced by the state language and that the four dialects areseparable, reaching high classification accuracy, especially with the XLS-Rmodel.</description><author>Sofoklis Kakouros, Katri Hiovain-Asikainen</author><pubDate>Fri, 19 May 2023 18:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11864v1</guid></item><item><title>Scaling laws for language encoding models in fMRI</title><link>http://arxiv.org/abs/2305.11863v1</link><description>Representations from transformer-based unidirectional language models areknown to be effective at predicting brain responses to natural language.However, most studies comparing language models to brains have used GPT-2 orsimilarly sized language models. Here we tested whether larger open-sourcemodels such as those from the OPT and LLaMA families are better at predictingbrain responses recorded using fMRI. Mirroring scaling results from othercontexts, we found that brain prediction performance scales log-linearly withmodel size from 125M to 30B parameter models, with ~15% increased encodingperformance as measured by correlation with a held-out test set across 3subjects. Similar log-linear behavior was observed when scaling the size of thefMRI training set. We also characterized scaling for acoustic encoding modelsthat use HuBERT, WavLM, and Whisper, and we found comparable improvements withmodel size. A noise ceiling analysis of these large, high-performance encodingmodels showed that performance is nearing the theoretical maximum for brainareas such as the precuneus and higher auditory cortex. These results suggestthat increasing scale in both models and data will yield incredibly effectivemodels of language processing in the brain, enabling better scientificunderstanding as well as applications such as decoding.</description><author>Richard Antonello, Aditya Vaidya, Alexander G. Huth</author><pubDate>Fri, 19 May 2023 18:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11863v1</guid></item><item><title>Benchmarking White Blood Cell Classification Under Domain Shift</title><link>http://arxiv.org/abs/2303.01777v2</link><description>Recognizing the types of white blood cells (WBCs) in microscopic images ofhuman blood smears is a fundamental task in the fields of pathology andhematology. Although previous studies have made significant contributions tothe development of methods and datasets, few papers have investigatedbenchmarks or baselines that others can easily refer to. For instance, weobserved notable variations in the reported accuracies of the sameConvolutional Neural Network (CNN) model across different studies, yet nopublic implementation exists to reproduce these results. In this paper, weestablish a benchmark for WBC recognition. Our results indicate that CNN-basedmodels achieve high accuracy when trained and tested under similar imagingconditions. However, their performance drops significantly when tested underdifferent conditions. Moreover, the ResNet classifier, which has been widelyemployed in previous work, exhibits an unreasonably poor generalization abilityunder domain shifts due to batch normalization. We investigate this issue andsuggest some alternative normalization techniques that can mitigate it. We makefully-reproducible code publiclyavailable\footnote{\url{https://github.com/apple2373/wbc-benchmark}}.</description><author>Satoshi Tsutsui, Zhengyang Su, Bihan Wen</author><pubDate>Fri, 19 May 2023 18:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01777v2</guid></item><item><title>Reducing Sequence Length by Predicting Edit Operations with Large Language Models</title><link>http://arxiv.org/abs/2305.11862v1</link><description>Large Language Models (LLMs) have demonstrated remarkable performance invarious tasks and gained significant attention. LLMs are also used for localsequence transduction tasks, including grammatical error correction (GEC) andformality style transfer, where most tokens in a source text are keptunchanged. However, it is inefficient to generate all target tokens because aprediction error of a target token may cause a catastrophe in predictingsubsequent tokens and because the computational cost grows quadratically withthe target sequence length. This paper proposes to predict a set of editoperations for the source text for local sequence transduction tasks.Representing an edit operation with a span of the source text and changedtokens, we can reduce the length of the target sequence and thus thecomputational cost for inference. We apply instruction tuning for LLMs on thesupervision data of edit operations. Experiments show that the proposed methodachieves comparable performance to the baseline in four tasks, paraphrasing,formality style transfer, GEC, and text simplification, despite reducing thelength of the target text by as small as 21\%. Furthermore, we report that theinstruction tuning with the proposed method achieved the state-of-the-artperformance in the four tasks.</description><author>Masahiro Kaneko, Naoaki Okazaki</author><pubDate>Fri, 19 May 2023 18:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11862v1</guid></item><item><title>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</title><link>http://arxiv.org/abs/2305.11860v1</link><description>A popular approach for improving the correctness of output from largelanguage models (LLMs) is Self-Consistency - poll the LLM multiple times andoutput the most frequent solution. Existing Self-Consistency techniques alwaysdraw a constant number of samples per question, where a better approach will beto non-uniformly distribute the available budget based on the amount ofagreement in the samples drawn so far. In response, we introduceAdaptive-Consistency, a cost-efficient, model-agnostic technique thatdynamically adjusts the number of samples per question using a lightweightstopping criterion. Our experiments over 13 datasets and two LLMs demonstratethat Adaptive-Consistency reduces sample budget by up to 6.0 times with anaverage accuracy drop of less than 0.1%.</description><author>Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam</author><pubDate>Fri, 19 May 2023 18:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11860v1</guid></item><item><title>Complex Claim Verification with Evidence Retrieved in the Wild</title><link>http://arxiv.org/abs/2305.11859v1</link><description>Evidence retrieval is a core part of automatic fact-checking. Prior workmakes simplifying assumptions in retrieval that depart from real-world usecases: either no access to evidence, access to evidence curated by a humanfact-checker, or access to evidence available long after the claim has beenmade. In this work, we present the first fully automated pipeline to checkreal-world claims by retrieving raw evidence from the web. We restrict ourretriever to only search documents available prior to the claim's making,modeling the realistic scenario where an emerging claim needs to be checked.Our pipeline includes five components: claim decomposition, raw documentretrieval, fine-grained evidence retrieval, claim-focused summarization, andveracity judgment. We conduct experiments on complex political claims in theClaimDecomp dataset and show that the aggregated evidence produced by ourpipeline improves veracity judgments. Human evaluation finds the evidencesummary produced by our system is reliable (it does not hallucinateinformation) and relevant to answering key questions about a claim, suggestingthat it can assist fact-checkers even when it cannot surface a completeevidence set.</description><author>Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett, Eunsol Choi</author><pubDate>Fri, 19 May 2023 18:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11859v1</guid></item><item><title>Q-malizing flow and infinitesimal density ratio estimation</title><link>http://arxiv.org/abs/2305.11857v1</link><description>Continuous normalizing flows are widely used in generative tasks, where aflow network transports from a data distribution $P$ to a normal distribution.A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$and $Q$ are accessible via finite samples, would be of various applicationinterests, particularly in the recently developed telescoping density ratioestimation (DRE) which calls for the construction of intermediate densities tobridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow''by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$(and vice versa) from empirical samples and is regularized by minimizing thetransport cost. The trained flow model allows us to perform infinitesimal DREalong the time-parametrized $\log$-density by training an additionalcontinuous-time flow network using classification loss, which estimates thetime-partial derivative of the $\log$-density. Integrating the time-scorenetwork along time provides a telescopic DRE between $P$ and $Q$ that is morestable than a one-step DRE. The effectiveness of the proposed model isempirically demonstrated on mutual information estimation from high-dimensionaldata and energy-based generative models of image data.</description><author>Chen Xu, Xiuyuan Cheng, Yao Xie</author><pubDate>Fri, 19 May 2023 18:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11857v1</guid></item><item><title>Video Killed the HD-Map: Predicting Driving Behavior Directly From Drone Images</title><link>http://arxiv.org/abs/2305.11856v1</link><description>The development of algorithms that learn behavioral driving models usinghuman demonstrations has led to increasingly realistic simulations. In general,such models learn to jointly predict trajectories for all controlled agents byexploiting road context information such as drivable lanes obtained frommanually annotated high-definition (HD) maps. Recent studies show that thesemodels can greatly benefit from increasing the amount of human data availablefor training. However, the manual annotation of HD maps which is necessary forevery new location puts a bottleneck on efficiently scaling up human trafficdatasets. We propose a drone birdview image-based map (DBM) representation thatrequires minimal annotation and provides rich road context information. Weevaluate multi-agent trajectory prediction using the DBM by incorporating itinto a differentiable driving simulator as an image-texture-baseddifferentiable rendering module. Our results demonstrate competitivemulti-agent trajectory prediction performance when using our DBM representationas compared to models trained with rasterized HD maps.</description><author>Yunpeng Liu, Vasileios Lioutas, Jonathan Wilder Lavington, Matthew Niedoba, Justice Sefas, Setareh Dabiri, Dylan Green, Xiaoxuan Liang, Berend Zwartsenberg, Adam Åcibior, Frank Wood</author><pubDate>Fri, 19 May 2023 18:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11856v1</guid></item><item><title>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</title><link>http://arxiv.org/abs/2305.11854v1</link><description>The progress of autonomous web navigation has been hindered by the dependenceon billions of exploratory interactions via online reinforcement learning, anddomain-specific model designs that make it difficult to leverage generalizationfrom rich out-of-domain data. In this work, we study data-driven offlinetraining for web agents with vision-language foundation models. We propose aninstruction-following multimodal agent, WebGUM, that observes both webpagescreenshots and HTML pages and outputs web navigation actions, such as clickand type. WebGUM is trained by jointly finetuning an instruction-finetunedlanguage model and a vision transformer on a large corpus of demonstrations. Weempirically demonstrate this recipe improves the agent's ability of groundedvisual perception, HTML comprehension and multi-step reasoning, outperformingprior works by a significant margin. On the MiniWoB benchmark, we improve overthe previous best offline methods by more than 31.9%, being close to reachingonline-finetuned SoTA. On the WebShop benchmark, our 3-billion-parameter modelachieves superior performance to the existing SoTA, PaLM-540B. We also collect347K high-quality demonstrations using our trained models, 38 times larger thanprior work, and make them available to promote future research in thisdirection.</description><author>Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, Izzeddin Gur</author><pubDate>Fri, 19 May 2023 18:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11854v1</guid></item><item><title>How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings</title><link>http://arxiv.org/abs/2305.11853v1</link><description>Large language models (LLMs) with in-context learning have demonstratedremarkable capability in the text-to-SQL task. Previous research has promptedLLMs with various demonstration-retrieval strategies and intermediate reasoningsteps to enhance the performance of LLMs. However, those works often employvaried strategies when constructing the prompt text for text-to-SQL inputs,such as databases and demonstration examples. This leads to a lack ofcomparability in both the prompt constructions and their primary contributions.Furthermore, selecting an effective prompt construction has emerged as apersistent problem for future research. To address this limitation, wecomprehensively investigate the impact of prompt constructions across varioussettings and provide insights for future work.</description><author>Shuaichen Chang, Eric Fosler-Lussier</author><pubDate>Fri, 19 May 2023 18:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11853v1</guid></item><item><title>Understanding HTML with Large Language Models</title><link>http://arxiv.org/abs/2210.03945v2</link><description>Large language models (LLMs) have shown exceptional performance on a varietyof natural language tasks. Yet, their capabilities for HTML understanding --i.e., parsing the raw HTML of a webpage, with applications to automation ofweb-based tasks, crawling, and browser-assisted retrieval -- have not beenfully explored. We contribute HTML understanding models (fine-tuned LLMs) andan in-depth analysis of their capabilities under three tasks: (i) SemanticClassification of HTML elements, (ii) Description Generation for HTML inputs,and (iii) Autonomous Web Navigation of HTML pages. While previous work hasdeveloped dedicated architectures and training procedures for HTMLunderstanding, we show that LLMs pretrained on standard natural languagecorpora transfer remarkably well to HTML understanding tasks. For instance,fine-tuned LLMs are 12% more accurate at semantic classification compared tomodels trained exclusively on the task dataset. Moreover, when fine-tuned ondata from the MiniWoB benchmark, LLMs successfully complete 50% more tasksusing 192x less data compared to the previous best supervised model. Out of theLLMs we evaluate, we show evidence that T5-based models are ideal due to theirbidirectional encoder-decoder architecture. To promote further research on LLMsfor HTML understanding, we create and open-source a large-scale HTML datasetdistilled and auto-labeled from CommonCrawl.</description><author>Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust</author><pubDate>Fri, 19 May 2023 18:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03945v2</guid></item><item><title>Any-to-Any Generation via Composable Diffusion</title><link>http://arxiv.org/abs/2305.11846v1</link><description>We present Composable Diffusion (CoDi), a novel generative model capable ofgenerating any combination of output modalities, such as language, image,video, or audio, from any combination of input modalities. Unlike existinggenerative AI systems, CoDi can generate multiple modalities in parallel andits input is not limited to a subset of modalities like text or image. Despitethe absence of training datasets for many combinations of modalities, wepropose to align modalities in both the input and output space. This allowsCoDi to freely condition on any input combination and generate any group ofmodalities, even if they are not present in the training data. CoDi employs anovel composable generation strategy which involves building a sharedmultimodal space by bridging alignment in the diffusion process, enabling thesynchronized generation of intertwined modalities, such as temporally alignedvideo and audio. Highly customizable and flexible, CoDi achieves strongjoint-modality generation quality, and outperforms or is on par with theunimodal state-of-the-art for single-modality synthesis. The project page withdemonstrations and code is at https://codi-gen.github.io</description><author>Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal</author><pubDate>Fri, 19 May 2023 18:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11846v1</guid></item><item><title>RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing</title><link>http://arxiv.org/abs/2305.11845v1</link><description>Reaction diagram parsing is the task of extracting reaction schemes from adiagram in the chemistry literature. The reaction diagrams can be arbitrarilycomplex, thus robustly parsing them into structured data is an open challenge.In this paper, we present RxnScribe, a machine learning model for parsingreaction diagrams of varying styles. We formulate this structured predictiontask with a sequence generation approach, which condenses the traditionalpipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378diagrams and evaluate it with cross validation, achieving an 80.0% soft matchF1 score, with significant improvements over previous models. Our code and dataare publicly available at https://github.com/thomas0809/RxnScribe.</description><author>Yujie Qian, Jiang Guo, Zhengkai Tu, Connor W. Coley, Regina Barzilay</author><pubDate>Fri, 19 May 2023 18:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11845v1</guid></item><item><title>AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia</title><link>http://arxiv.org/abs/2305.11844v1</link><description>This paper presents a community-centered study of cultural limitations oftext-to-image (T2I) models in the South Asian context. We theorize thesefailures using scholarship on dominant media regimes of representations andlocate them within participants' reporting of their existing socialmarginalizations. We thus show how generative AI can reproduce an outsidersgaze for viewing South Asian cultures, shaped by global and regional powerinequities. By centering communities as experts and soliciting theirperspectives on T2I limitations, our study adds rich nuance into existingevaluative frameworks and deepens our understanding of the culturally-specificways AI technologies can fail in non-Western and Global South settings. Wedistill lessons for responsible development of T2I models, recommendingconcrete pathways forward that can allow for recognition of structuralinequalities.</description><author>Rida Qadri, Renee Shelby, Cynthia L. Bennett, Emily Denton</author><pubDate>Fri, 19 May 2023 18:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11844v1</guid></item><item><title>How Does Generative Retrieval Scale to Millions of Passages?</title><link>http://arxiv.org/abs/2305.11841v1</link><description>Popularized by the Differentiable Search Index, the emerging paradigm ofgenerative retrieval re-frames the classic information retrieval problem into asequence-to-sequence modeling task, forgoing external indices and encoding anentire document corpus within a single Transformer. Although many differentapproaches have been proposed to improve the effectiveness of generativeretrieval, they have only been evaluated on document corpora on the order of100k in size. We conduct the first empirical study of generative retrievaltechniques across various corpus scales, ultimately scaling up to the entire MSMARCO passage ranking task with a corpus of 8.8M passages and evaluating modelsizes up to 11B parameters. We uncover several findings about scalinggenerative retrieval to millions of passages; notably, the central importanceof using synthetic queries as document representations during indexing, theineffectiveness of existing proposed architecture modifications when accountingfor compute cost, and the limits of naively scaling model parameters withrespect to retrieval performance. While we find that generative retrieval iscompetitive with state-of-the-art dual encoders on small corpora, scaling tomillions of passages remains an important and unsolved challenge. We believethese findings will be valuable for the community to clarify the current stateof generative retrieval, highlight the unique challenges, and inspire newresearch directions.</description><author>Ronak Pradeep, Kai Hui, Jai Gupta, Adam D. Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, Vinh Q. Tran</author><pubDate>Fri, 19 May 2023 18:33:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11841v1</guid></item><item><title>SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models</title><link>http://arxiv.org/abs/2305.11840v1</link><description>Stereotype benchmark datasets are crucial to detect and mitigate socialstereotypes about groups of people in NLP models. However, existing datasetsare limited in size and coverage, and are largely restricted to stereotypesprevalent in the Western society. This is especially problematic as languagetechnologies gain hold across the globe. To address this gap, we presentSeeGULL, a broad-coverage stereotype dataset, built by utilizing generativecapabilities of large language models such as PaLM, and GPT-3, and leveraging aglobally diverse rater pool to validate the prevalence of those stereotypes insociety. SeeGULL is in English, and contains stereotypes about identity groupsspanning 178 countries across 8 different geo-political regions across 6continents, as well as state-level identities within the US and India. We alsoinclude fine-grained offensiveness scores for different stereotypes anddemonstrate their global disparities. Furthermore, we include comparativeannotations about the same groups by annotators living in the region vs. thosethat are based in North America, and demonstrate that within-region stereotypesabout groups differ from those prevalent in North America. CONTENT WARNING:This paper contains stereotype examples that may be offensive.</description><author>Akshita Jha, Aida Davani, Chandan K. Reddy, Shachi Dave, Vinodkumar Prabhakaran, Sunipa Dev</author><pubDate>Fri, 19 May 2023 18:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11840v1</guid></item><item><title>Anticorrelated Noise Injection for Improved Generalization</title><link>http://arxiv.org/abs/2202.02831v3</link><description>Injecting artificial noise into gradient descent (GD) is commonly employed toimprove the performance of machine learning models. Usually, uncorrelated noiseis used in such perturbed gradient descent (PGD) methods. It is, however, notknown if this is optimal or whether other types of noise could provide bettergeneralization performance. In this paper, we zoom in on the problem ofcorrelating the perturbations of consecutive PGD steps. We consider a varietyof objective functions for which we find that GD with anticorrelatedperturbations ("Anti-PGD") generalizes significantly better than GD andstandard (uncorrelated) PGD. To support these experimental findings, we alsoderive a theoretical analysis that demonstrates that Anti-PGD moves to widerminima, while GD and PGD remain stuck in suboptimal regions or even diverge.This new connection between anticorrelated noise and generalization opens thefield to novel ways to exploit noise for training machine learning models.</description><author>Antonio Orvieto, Hans Kersting, Frank Proske, Francis Bach, Aurelien Lucchi</author><pubDate>Fri, 19 May 2023 18:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.02831v3</guid></item><item><title>Comparing Software Developers with ChatGPT: An Empirical Investigation</title><link>http://arxiv.org/abs/2305.11837v1</link><description>The advent of automation in particular Software Engineering (SE) tasks hastransitioned from theory to reality. Numerous scholarly articles havedocumented the successful application of Artificial Intelligence to addressissues in areas such as project management, modeling, testing, and development.A recent innovation is the introduction of ChatGPT, an ML-infused chatbot,touted as a resource proficient in generating programming codes and formulatingsoftware testing strategies for developers and testers respectively. Althoughthere is speculation that AI-based computation can increase productivity andeven substitute software engineers in software development, there is currentlya lack of empirical evidence to verify this. Moreover, despite the primaryfocus on enhancing the accuracy of AI systems, non-functional requirementsincluding energy efficiency, vulnerability, fairness (i.e., human bias), andsafety frequently receive insufficient attention. This paper posits that acomprehensive comparison of software engineers and AI-based solutions,considering various evaluation criteria, is pivotal in fostering human-machinecollaboration, enhancing the reliability of AI-based methods, and understandingtask suitability for humans or AI. Furthermore, it facilitates the effectiveimplementation of cooperative work structures and human-in-the-loop processes.This paper conducts an empirical investigation, contrasting the performance ofsoftware engineers and AI systems, like ChatGPT, across different evaluationmetrics. The empirical study includes a case of assessing ChatGPT-generatedcode versus code produced by developers and uploaded in Leetcode.</description><author>Nathalia Nascimento, Paulo Alencar, Donald Cowan</author><pubDate>Fri, 19 May 2023 18:25:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11837v1</guid></item><item><title>Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions</title><link>http://arxiv.org/abs/2305.11833v1</link><description>We study the complexity of the problem of training neural networks definedvia various activation functions. The training problem is known to beexistsR-complete with respect to linear activation functions and the ReLUactivation function. We consider the complexity of the problem with respect tothe sigmoid activation function and other effectively continuous functions. Weshow that these training problems are polynomial-time many-one bireducible tothe existential theory of the reals extended with the corresponding activationfunctions. In particular, we establish that the sigmoid activation functionleads to the existential theory of the reals with the exponential function. Itis thus open, and equivalent with the decidability of the existential theory ofthe reals with the exponential function, whether training neural networks usingthe sigmoid activation function is algorithmically solvable. In contrast, weobtain that the training problem is undecidable if sinusoidal activationfunctions are considered. Finally, we obtain general upper bounds for thecomplexity of the training problem in the form of low levels of thearithmetical hierarchy.</description><author>Teemu Hankala, Miika Hannula, Juha Kontinen, Jonni Virtema</author><pubDate>Fri, 19 May 2023 18:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11833v1</guid></item><item><title>Application of Knowledge Distillation to Multi-task Speech Representation Learning</title><link>http://arxiv.org/abs/2210.16611v2</link><description>Model architectures such as wav2vec 2.0 and HuBERT have been proposed tolearn speech representations from audio waveforms in a self-supervised manner.When they are combined with downstream tasks such as keyword spotting andspeaker verification, they provide state-of-the-art performance. However, thesemodels use a large number of parameters, the smallest version of which has 95million parameters. This constitutes a challenge for edge AI devicedeployments. In this paper, we investigate the application of knowledgedistillation to speech representation learning (SRL) models followed by jointfine-tuning with multiple downstream voice-activated tasks. In our experimentson two such tasks, our approach results in nearly 75% reduction in model sizewhile suffering only 0.1% accuracy and 0.9% equal error rate degradationcompared to the full-size model. In addition, we show that fine-tuning the SRLmodels results in a significant performance boost compared to using frozen SRLmodels.</description><author>Mine Kerpicci, Van Nguyen, Shuhua Zhang, Erik Visser</author><pubDate>Fri, 19 May 2023 18:16:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16611v2</guid></item><item><title>Improving Multimodal Joint Variational Autoencoders through Normalizing Flows and Correlation Analysis</title><link>http://arxiv.org/abs/2305.11832v1</link><description>We propose a new multimodal variational autoencoder that enables to generatefrom the joint distribution and conditionally to any number of complexmodalities. The unimodal posteriors are conditioned on the Deep CanonicalCorrelation Analysis embeddings which preserve the shared information acrossmodalities leading to more coherent cross-modal generations. Furthermore, weuse Normalizing Flows to enrich the unimodal posteriors and achieve morediverse data generation. Finally, we propose to use a Product of Experts forinferring one modality from several others which makes the model scalable toany number of modalities. We demonstrate that our method improves likelihoodestimates, diversity of the generations and in particular coherence metrics inthe conditional generations on several datasets.</description><author>Agathe Senellart, ClÃ©ment Chadebec, StÃ©phanie AllassonniÃ¨re</author><pubDate>Fri, 19 May 2023 18:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11832v1</guid></item><item><title>Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment</title><link>http://arxiv.org/abs/2305.11831v1</link><description>This work presents a comprehensive analysis to regularize the SoftActor-Critic (SAC) algorithm with automatic temperature adjustment. The thepolicy evaluation, the policy improvement and the temperature adjustment arereformulated, addressing certain modification and enhancing the clarity of theoriginal theory in a more explicit manner.</description><author>Ben You</author><pubDate>Fri, 19 May 2023 18:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11831v1</guid></item><item><title>Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews</title><link>http://arxiv.org/abs/2305.11828v1</link><description>Medical systematic reviews are crucial for informing clinical decision makingand healthcare policy. But producing such reviews is onerous andtime-consuming. Thus, high-quality evidence synopses are not available for manyquestions and may be outdated even when they are available. Large languagemodels (LLMs) are now capable of generating long-form texts, suggesting thetantalizing possibility of automatically generating literature reviews ondemand. However, LLMs sometimes generate inaccurate (and potentiallymisleading) texts by hallucinating or omitting important information. In thehealthcare context, this may render LLMs unusable at best and dangerous atworst. Most discussion surrounding the benefits and risks of LLMs have beendivorced from specific applications. In this work, we seek to qualitativelycharacterize the potential utility and risks of LLMs for assisting inproduction of medical evidence reviews. We conducted 16 semi-structuredinterviews with international experts in systematic reviews, groundingdiscussion in the context of generating evidence reviews. Domain expertsindicated that LLMs could aid writing reviews, as a tool for drafting orcreating plain language summaries, generating templates or suggestions,distilling information, crosschecking, and synthesizing or interpreting textinputs. But they also identified issues with model outputs and expressedconcerns about potential downstream harms of confidently composed butinaccurate LLM outputs which might mislead. Other anticipated potentialdownstream harms included lessened accountability and proliferation ofautomatically generated reviews that might be of low quality. Informed by thisqualitative analysis, we identify criteria for rigorous evaluation ofbiomedical LLMs aligned with domain expert views.</description><author>Hye Sun Yun, Iain J. Marshall, Thomas Trikalinos, Byron C. Wallace</author><pubDate>Fri, 19 May 2023 18:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11828v1</guid></item><item><title>A Comprehensive Review of YOLO: From YOLOv1 and Beyond</title><link>http://arxiv.org/abs/2304.00501v2</link><description>YOLO has become a central real-time object detection system for robotics,driverless cars, and video monitoring applications. We present a comprehensiveanalysis of YOLO's evolution, examining the innovations and contributions ineach iteration from the original YOLO to YOLOv8 and YOLO-NAS. We start bydescribing the standard metrics and postprocessing; then, we discuss the majorchanges in network architecture and training tricks for each model. Finally, wesummarize the essential lessons from YOLO's development and provide aperspective on its future, highlighting potential research directions toenhance real-time object detection systems.</description><author>Juan Terven, Diana Cordova-Esparza</author><pubDate>Fri, 19 May 2023 18:07:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00501v2</guid></item><item><title>STOAT: Structured Data to Analytical Text With Controls</title><link>http://arxiv.org/abs/2305.11826v1</link><description>Recent language models have made tremendous progress in the structured datato text generation task. However, these models still give sub-optimalperformance where logical inference is required to generate the descriptions.In this work, we specifically focus on analytical text generation fromstructured data such as tables. Building on the taxonomy proposed in (Gupta etal., 2020) we focus on controllable table to text generation for the followingreasoning categories: numerical reasoning, commonsense reasoning, temporalreasoning, table knowledge, and entity knowledge. We propose STOAT model, whichis table and reasoning aware, with vector-quantization to infuse the givenreasoning categories in the output. We observe that our model provides 10.19%,1.13% improvement on the PARENT metric in iToTTo and Infotabs for theanalytical sentence task. We also found that our model generates 15.3% morefaithful and analytical descriptions as compared to the baseline models inhuman evaluation. We curate and release two reasoning category annotatedtable-to-interesting text generation datasets based on the ToTTo (Parikh etal., 2020) and InfoTabs datasets (Gupta et al.,2020).</description><author>Deepanway Ghosal, Preksha Nema, Aravindan Raghuveer</author><pubDate>Fri, 19 May 2023 18:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11826v1</guid></item><item><title>Phantom -- A RL-driven multi-agent framework to model complex systems</title><link>http://arxiv.org/abs/2210.06012v3</link><description>Agent based modelling (ABM) is a computational approach to modelling complexsystems by specifying the behaviour of autonomous decision-making components oragents in the system and allowing the system dynamics to emerge from theirinteractions. Recent advances in the field of Multi-agent reinforcementlearning (MARL) have made it feasible to study the equilibrium of complexenvironments where multiple agents learn simultaneously. However, most ABMframeworks are not RL-native, in that they do not offer concepts and interfacesthat are compatible with the use of MARL to learn agent behaviours. In thispaper, we introduce a new open-source framework, Phantom, to bridge the gapbetween ABM and MARL. Phantom is an RL-driven framework for agent-basedmodelling of complex multi-agent systems including, but not limited to economicsystems and markets. The framework aims to provide the tools to simplify theABM specification in a MARL-compatible way - including features to encodedynamic partial observability, agent utility functions, heterogeneity in agentpreferences or types, and constraints on the order in which agents can act(e.g. Stackelberg games, or more complex turn-taking environments). In thispaper, we present these features, their design rationale and present two newenvironments leveraging the framework.</description><author>Leo Ardon, Jared Vann, Deepeka Garg, Tom Spooner, Sumitra Ganesh</author><pubDate>Fri, 19 May 2023 18:02:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06012v3</guid></item><item><title>Differentiable Model Selection for Ensemble Learning</title><link>http://arxiv.org/abs/2211.00251v2</link><description>Model selection is a strategy aimed at creating accurate and robust models. Akey challenge in designing these algorithms is identifying the optimal modelfor classifying any particular input sample. This paper addresses thischallenge and proposes a novel framework for differentiable model selectionintegrating machine learning and combinatorial optimization. The framework istailored for ensemble learning, a strategy that combines the outputs ofindividually pre-trained models, and learns to select appropriate ensemblemembers for a particular input sample by transforming the ensemble learningtask into a differentiable selection program trained end-to-end within theensemble learning model. Tested on various tasks, the proposed frameworkdemonstrates its versatility and effectiveness, outperforming conventional andadvanced consensus rules across a variety of settings and learning tasks.</description><author>James Kotary, Vincenzo Di Vito, Ferdinando Fioretto</author><pubDate>Fri, 19 May 2023 17:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00251v2</guid></item><item><title>MaGIC: Multi-modality Guided Image Completion</title><link>http://arxiv.org/abs/2305.11818v1</link><description>The vanilla image completion approaches are sensitive to the large missingregions due to limited available reference information for plausiblegeneration. To mitigate this, existing methods incorporate the extra cue as aguidance for image completion. Despite improvements, these approaches are oftenrestricted to employing a single modality (e.g., segmentation or sketch maps),which lacks scalability in leveraging multi-modality for more plausiblecompletion. In this paper, we propose a novel, simple yet effective method forMulti-modal Guided Image Completion, dubbed MaGIC, which not only supports awide range of single modality as the guidance (e.g., text, canny edge, sketch,segmentation, reference image, depth, and pose), but also adapts to arbitrarilycustomized combination of these modalities (i.e., arbitrary multi-modality) forimage completion. For building MaGIC, we first introduce a modality-specificconditional U-Net (MCU-Net) that injects single-modal signal into a U-Netdenoiser for single-modal guided image completion. Then, we devise a consistentmodality blending (CMB) method to leverage modality signals encoded in multiplelearned MCU-Nets through gradient guidance in latent space. Our CMB istraining-free, and hence avoids the cumbersome joint re-training of differentmodalities, which is the secret of MaGIC to achieve exceptional flexibility inaccommodating new modalities for completion. Experiments show the superiorityof MaGIC over state-of-arts and its generalization to various completion tasksincluding in/out-painting and local editing. Our project with code and modelsis available at yeates.github.io/MaGIC-Page/.</description><author>Yongsheng Yu, Hao Wang, Tiejian Luo, Heng Fan, Libo Zhang</author><pubDate>Fri, 19 May 2023 17:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11818v1</guid></item><item><title>Summarizing Strategy Card Game AI Competition</title><link>http://arxiv.org/abs/2305.11814v1</link><description>This paper concludes five years of AI competitions based on Legends of Codeand Magic (LOCM), a small Collectible Card Game (CCG), designed with the goalof supporting research and algorithm development. The game was used in a numberof events, including Community Contests on the CodinGame platform, and StrategyCard Game AI Competition at the IEEE Congress on Evolutionary Computation andIEEE Conference on Games. LOCM has been used in a number of publicationsrelated to areas such as game tree search algorithms, neural networks,evaluation functions, and CCG deckbuilding. We present the rules of the game,the history of organized competitions, and a listing of the participant andtheir approaches, as well as some general advice on organizing AI competitionsfor the research community. Although the COG 2022 edition was announced to bethe last one, the game remains available and can be played using an onlineleaderboard arena.</description><author>Jakub Kowalski, RadosÅaw Miernik</author><pubDate>Fri, 19 May 2023 17:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11814v1</guid></item><item><title>Monte-Carlo Search for an Equilibrium in Dec-POMDPs</title><link>http://arxiv.org/abs/2305.11811v1</link><description>Decentralized partially observable Markov decision processes (Dec-POMDPs)formalize the problem of designing individual controllers for a group ofcollaborative agents under stochastic dynamics and partial observability.Seeking a global optimum is difficult (NEXP complete), but seeking a Nashequilibrium -- each agent policy being a best response to the other agents --is more accessible, and allowed addressing infinite-horizon problems withsolutions in the form of finite state controllers. In this paper, we show thatthis approach can be adapted to cases where only a generative model (asimulator) of the Dec-POMDP is available. This requires relying on asimulation-based POMDP solver to construct an agent's FSC node by node. Arelated process is used to heuristically derive initial FSCs. Experiment withbenchmarks shows that MC-JESP is competitive with exisiting Dec-POMDP solvers,even better than many offline methods using explicit models.</description><author>Yang You, Vincent Thomas, Francis Colas, Olivier Buffet</author><pubDate>Fri, 19 May 2023 17:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11811v1</guid></item><item><title>Pseudo-Label Training and Model Inertia in Neural Machine Translation</title><link>http://arxiv.org/abs/2305.11808v1</link><description>Like many other machine learning applications, neural machine translation(NMT) benefits from over-parameterized deep neural models. However, thesemodels have been observed to be brittle: NMT model predictions are sensitive tosmall input changes and can show significant variation across re-training orincremental model updates. This work studies a frequently used method in NMT,pseudo-label training (PLT), which is common to the related techniques offorward-translation (or self-training) and sequence-level knowledgedistillation. While the effect of PLT on quality is well-documented, wehighlight a lesser-known effect: PLT can enhance a model's stability to modelupdates and input perturbations, a set of properties we call model inertia. Westudy inertia effects under different training settings and we identifydistribution simplification as a mechanism behind the observed results.</description><author>Benjamin Hsu, Anna Currey, Xing Niu, Maria NÄdejde, Georgiana Dinu</author><pubDate>Fri, 19 May 2023 17:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11808v1</guid></item><item><title>Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis</title><link>http://arxiv.org/abs/2302.02813v2</link><description>The war in Ukraine seems to have positively changed the attitude toward thecritical societal topic of migration in Europe -- at least towards refugeesfrom Ukraine. We investigate whether this impression is substantiated by howthe topic is reflected in online news and social media, thus linking therepresentation of the issue on the Web to its perception in society. For thispurpose, we combine and adapt leading-edge automatic text processing for anovel multilingual stance detection approach. Starting from 5.5M Twitter postspublished by 565 European news outlets in one year, beginning September 2021,plus replies, we perform a multilingual analysis of migration-related mediacoverage and associated social media interaction for Europe and selectedEuropean countries. The results of our analysis show that there is actually a reframing of thediscussion illustrated by the terminology change, e.g., from "migrant" to"refugee", often even accentuated with phrases such as "real refugees".However, concerning a stance shift in public perception, the picture is morediverse than expected. All analyzed cases show a noticeable temporal stanceshift around the start of the war in Ukraine. Still, there are apparentnational differences in the size and stability of this shift.</description><author>Sergej Wildemann, Claudia NiederÃ©e, Erick Elejalde</author><pubDate>Fri, 19 May 2023 17:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02813v2</guid></item><item><title>On the Fairness Impacts of Private Ensembles Models</title><link>http://arxiv.org/abs/2305.11807v1</link><description>The Private Aggregation of Teacher Ensembles (PATE) is a machine learningframework that enables the creation of private models through the combinationof multiple "teacher" models and a "student" model. The student model learns topredict an output based on the voting of the teachers, and the resulting modelsatisfies differential privacy. PATE has been shown to be effective in creatingprivate models in semi-supervised settings or when protecting data labels is apriority. This paper explores whether the use of PATE can result in unfairness,and demonstrates that it can lead to accuracy disparities among groups ofindividuals. The paper also analyzes the algorithmic and data properties thatcontribute to these disproportionate impacts, why these aspects are affectingdifferent groups disproportionately, and offers recommendations for mitigatingthese effects</description><author>Cuong Tran, Ferdinando Fioretto</author><pubDate>Fri, 19 May 2023 17:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11807v1</guid></item><item><title>The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics</title><link>http://arxiv.org/abs/2305.11806v1</link><description>Neural metrics for machine translation evaluation, such as COMET, exhibitsignificant improvements in their correlation with human judgments, as comparedto traditional metrics based on lexical overlap, such as BLEU. Yet, neuralmetrics are, to a great extent, "black boxes" returning a single sentence-levelscore without transparency about the decision-making process. In this work, wedevelop and compare several neural explainability methods and demonstrate theireffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Ourstudy reveals that these metrics leverage token-level information that can bedirectly attributed to translation errors, as assessed through comparison oftoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)annotations and with synthetically-generated critical translation errors. Toease future research, we release our code at:https://github.com/Unbabel/COMET/tree/explainable-metrics.</description><author>Ricardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa Coheur, Alon Lavie, AndrÃ© F. T. Martins</author><pubDate>Fri, 19 May 2023 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11806v1</guid></item><item><title>PANNA 2.0: Efficient neural network interatomic potentials and new architectures</title><link>http://arxiv.org/abs/2305.11805v1</link><description>We present the latest release of PANNA 2.0 (Properties from Artificial NeuralNetwork Architectures), a code for the generation of neural network interatomicpotentials based on local atomic descriptors and multilayer perceptrons. Builton a new back end, this new release of PANNA features improved tools forcustomizing and monitoring network training, better GPU support including afast descriptor calculator, new plugins for external codes and a newarchitecture for the inclusion of long-range electrostatic interactions througha variational charge equilibration scheme. We present an overview of the mainfeatures of the new code, and several benchmarks comparing the accuracy ofPANNA models to the state of the art, on commonly used benchmarks as well asricher datasets.</description><author>Franco Pellegrini, Ruggero Lot, Yusuf Shaidu, Emine KÃ¼Ã§Ã¼kbenli</author><pubDate>Fri, 19 May 2023 17:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11805v1</guid></item><item><title>Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation</title><link>http://arxiv.org/abs/2212.09631v2</link><description>Neural machine translation (NMT) has become the de-facto standard inreal-world machine translation applications. However, NMT models canunpredictably produce severely pathological translations, known ashallucinations, that seriously undermine user trust. It becomes thus crucial toimplement effective preventive strategies to guarantee their properfunctioning. In this paper, we address the problem of hallucination detectionin NMT by following a simple intuition: as hallucinations are detached from thesource content, they exhibit encoder-decoder attention patterns that arestatistically different from those of good quality translations. We frame thisproblem with an optimal transport formulation and propose a fully unsupervised,plug-in detector that can be used with any attention-based NMT model.Experimental results show that our detector not only outperforms all previousmodel-based detectors, but is also competitive with detectors that employ largemodels trained on millions of samples.</description><author>Nuno M. Guerreiro, Pierre Colombo, Pablo Piantanida, AndrÃ© F. T. Martins</author><pubDate>Fri, 19 May 2023 17:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09631v2</guid></item><item><title>The probability flow ODE is provably fast</title><link>http://arxiv.org/abs/2305.11798v1</link><description>We provide the first polynomial-time convergence guarantees for theprobability flow ODE implementation (together with a corrector step) ofscore-based generative modeling. Our analysis is carried out in the wake ofrecent results obtaining such guarantees for the SDE-based implementation(i.e., denoising diffusion probabilistic modeling or DDPM), but requires thedevelopment of novel techniques for studying deterministic dynamics withoutcontractivity. Through the use of a specially chosen corrector step based onthe underdamped Langevin diffusion, we obtain better dimension dependence thanprior works on DDPM ($O(\sqrt{d})$ vs. $O(d)$, assuming smoothness of the datadistribution), highlighting potential advantages of the ODE framework.</description><author>Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, Adil Salim</author><pubDate>Fri, 19 May 2023 17:33:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11798v1</guid></item><item><title>A Comprehensive Survey on Segment Anything Model for Vision and Beyond</title><link>http://arxiv.org/abs/2305.08196v2</link><description>Artificial intelligence (AI) is evolving towards artificial generalintelligence, which refers to the ability of an AI system to perform a widerange of tasks and exhibit a level of intelligence similar to that of a humanbeing. This is in contrast to narrow or specialized AI, which is designed toperform specific tasks with a high degree of efficiency. Therefore, it isurgent to design a general class of models, which we term foundation models,trained on broad data that can be adapted to various downstream tasks. Therecently proposed segment anything model (SAM) has made significant progress inbreaking the boundaries of segmentation, greatly promoting the development offoundation models for computer vision. To fully comprehend SAM, we conduct asurvey study. As the first to comprehensively review the progress of segmentinganything task for vision and beyond based on the foundation model of SAM, thiswork focuses on its applications to various tasks and data types by discussingits historical development, recent progress, and profound impact on broadapplications. We first introduce the background and terminology for foundationmodels including SAM, as well as state-of-the-art methods contemporaneous withSAM that are significant for segmenting anything task. Then, we analyze andsummarize the advantages and limitations of SAM across various image processingapplications, including software scenes, real-world scenes, and complex scenes.Importantly, many insights are drawn to guide future research to develop moreversatile foundation models and improve the architecture of SAM. We alsosummarize massive other amazing applications of SAM in vision and beyond.Finally, we maintain a continuously updated paper list and an open-sourceproject summary for foundation model SAM at\href{https://github.com/liliu-avril/Awesome-Segment-Anything}{\color{magenta}{here}}.</description><author>Chunhui Zhang, Li Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, Yuehong Hu</author><pubDate>Fri, 19 May 2023 17:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08196v2</guid></item><item><title>A One-Class Classifier for the Detection of GAN Manipulated Multi-Spectral Satellite Images</title><link>http://arxiv.org/abs/2305.11795v1</link><description>The highly realistic image quality achieved by current image generativemodels has many academic and industrial applications. To limit the use of suchmodels to benign applications, though, it is necessary that tools toconclusively detect whether an image has been generated synthetically or notare developed. For this reason, several detectors have been developed providingexcellent performance in computer vision applications, however, they can not beapplied as they are to multispectral satellite images, and hence new modelsmust be trained. In general, two-class classifiers can achieve very gooddetection accuracies, however they are not able to generalise to image domainsand generative models architectures different than those used during training.For this reason, in this paper, we propose a one-class classifier based onVector Quantized Variational Autoencoder 2 (VQ-VAE 2) features to overcome thelimitations of two-class classifiers. First, we emphasize the generalizationproblem that binary classifiers suffer from by training and testing anEfficientNet-B4 architecture on multiple multispectral datasets. Then we showthat, since the VQ-VAE 2 based classifier is trained only on pristine images,it is able to detect images belonging to different domains and generated byarchitectures that have not been used during training. Last, we compare the twoclassifiers head-to-head on the same generated datasets, highlighting thesuperiori generalization capabilities of the VQ-VAE 2-based detector.</description><author>Lydia Abady, Giovanna Maria Dimitri, Mauro Barni</author><pubDate>Fri, 19 May 2023 17:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11795v1</guid></item><item><title>Chain-of-thought prompting for responding to in-depth dialogue questions with LLM</title><link>http://arxiv.org/abs/2305.11792v1</link><description>The way and content in which users ask questions can provide insight intotheir current status, including their personality, emotions, and psychology.Instead of directly prompting the large language models (LLMs), we explore howchain-of-thought prompting helps in this scenario to perform reasoning andplanning according to user status, aiming to provide a more personalized andengaging experience for the user query. To this end, we first construct abenchmark of 6 dialogue or question-answering datasets in both English andChinese, covering 3 different aspects of user status (\textit{including}\textit{personality}, \textit{emotion}, and \textit{psychology}). Then weprompt the LLMs to generate the response regarding the user status asintermediate reasoning processing. We propose a novel demonstration selectionstrategy using the semantic similarity of intermediate reasoning instead oftest queries. To evaluate the effectiveness and robustness of our approach, weconduct extensive experiments with 7 LLMs under zero-shot and one-shotsettings. The experimental results show that our approach consistentlyoutperforms standard prompting in terms of both \textit{helpfulness} and\textit{acceptness} across all datasets, regardless of the LLMs used. The codeand dataset can be found at\url{https://github.com/ruleGreen/Dialogue\_CoT.git}.</description><author>Hongru Wang, Rui Wang, Fei Mi, Zezhong Wang, Ruifeng Xu, Kam-Fai Wong</author><pubDate>Fri, 19 May 2023 17:27:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11792v1</guid></item><item><title>Enhancing Few-shot NER with Prompt Ordering based Data Augmentation</title><link>http://arxiv.org/abs/2305.11791v1</link><description>Recently, data augmentation (DA) methods have been proven to be effective forpre-trained language models (PLMs) in low-resource settings, including few-shotnamed entity recognition (NER). However, conventional NER DA methods are mostlyaimed at sequence labeling models, i.e., token-level classification, and feware compatible with unified autoregressive generation frameworks, which canhandle a wider range of NER tasks, such as nested NER. Furthermore, thesegeneration frameworks have a strong assumption that the entities will appear inthe target sequence with the same left-to-right order as the source sequence.In this paper, we claim that there is no need to keep this strict order, andmore diversified but reasonable target entity sequences can be provided duringthe training stage as a novel DA method. Nevertheless, a naive mixture ofaugmented data can confuse the model since one source sequence will then bepaired with different target sequences. Therefore, we propose a simple buteffective Prompt Ordering based Data Augmentation (PODA) method to improve thetraining of unified autoregressive generation frameworks under few-shot NERscenarios. Experimental results on three public NER datasets and furtheranalyses demonstrate the effectiveness of our approach.</description><author>Huiming Wang, Liying Cheng, Wenxuan Zhang, De Wen Soh, Lidong Bing</author><pubDate>Fri, 19 May 2023 17:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11791v1</guid></item><item><title>Prompting with Pseudo-Code Instructions</title><link>http://arxiv.org/abs/2305.11790v1</link><description>Prompting with natural language instructions has recently emerged as apopular method of harnessing the capabilities of large language models. Giventhe inherent ambiguity present in natural language, it is intuitive to considerthe possible advantages of prompting with less ambiguous prompt styles, such asthe use of pseudo-code. In this paper we explore if prompting via pseudo-code instructions helpsimprove the performance of pre-trained language models. We manually create adataset of pseudo-code prompts for 132 different tasks spanning classification,QA and generative language tasks, sourced from the Super-NaturalInstructionsdataset. Using these prompts along with their counterparts in natural language,we study their performance on two LLM families - BLOOM and CodeGen. Ourexperiments show that using pseudo-code instructions leads to better results,with an average increase (absolute) of 7-16 points in F1 scores forclassification tasks and an improvement (relative) of 12-38% in aggregate ROUGEscores across all tasks. We include detailed ablation studies which indicatethat code comments, docstrings, and the structural clues encoded in pseudo-codeall contribute towards the improvement in performance. To the best of our knowledge our work is the first to demonstrate howpseudo-code prompts can be helpful in improving the performance of pre-trainedLMs.</description><author>Mayank Mishra, Prince Kumar, Riyaz Bhat, Rudra Murthy V, Danish Contractor, Srikanth Tamilselvam</author><pubDate>Fri, 19 May 2023 17:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11790v1</guid></item><item><title>Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach</title><link>http://arxiv.org/abs/2305.11789v1</link><description>Humans work together to solve common problems by having discussions,explaining, and agreeing or disagreeing with each other. Similarly, if a systemcan have discussions with humans when solving tasks, it can improve thesystem's performance and reliability. In previous research on explainability,it has only been possible for the system to make predictions and for humans toask questions about them rather than having a mutual exchange of opinions. Thisresearch aims to create a dataset and computational framework for systems thatdiscuss and refine their predictions through dialogue. Through experiments, weshow that the proposed system can have beneficial discussions with humansimproving the accuracy by up to 25 points in the natural language inferencetask.</description><author>Masahiro Kaneko, Graham Neubig, Naoaki Okazaki</author><pubDate>Fri, 19 May 2023 17:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11789v1</guid></item><item><title>Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability</title><link>http://arxiv.org/abs/2305.11788v1</link><description>Recent research has observed that in machine learning optimization, gradientdescent (GD) often operates at the edge of stability (EoS) [Cohen, et al.,2021], where the stepsizes are set to be large, resulting in non-monotoniclosses induced by the GD iterates. This paper studies the convergence andimplicit bias of constant-stepsize GD for logistic regression on linearlyseparable data in the EoS regime. Despite the presence of local oscillations,we prove that the logistic loss can be minimized by GD with any constantstepsize over a long time scale. Furthermore, we prove that with any constantstepsize, the GD iterates tend to infinity when projected to a max-margindirection (the hard-margin SVM direction) and converge to a fixed vector thatminimizes a strongly convex potential when projected to the orthogonalcomplement of the max-margin direction. In contrast, we also show that in theEoS regime, GD iterates may diverge catastrophically under the exponentialloss, highlighting the superiority of the logistic loss. These theoreticalfindings are in line with numerical simulations and complement existingtheories on the convergence and implicit bias of GD, which are only applicablewhen the stepsizes are sufficiently small.</description><author>Jingfeng Wu, Vladimir Braverman, Jason D. Lee</author><pubDate>Fri, 19 May 2023 17:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11788v1</guid></item><item><title>DMDD: A Large-Scale Dataset for Dataset Mentions Detection</title><link>http://arxiv.org/abs/2305.11779v1</link><description>The recognition of dataset names is a critical task for automatic informationextraction in scientific literature, enabling researchers to understand andidentify research opportunities. However, existing corpora for dataset mentiondetection are limited in size and naming diversity. In this paper, we introducethe Dataset Mentions Detection Dataset (DMDD), the largest publicly availablecorpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219scientific articles with over 449,000 dataset mentions weakly annotated in theformat of in-text spans, and an evaluation set, which comprises of 450scientific articles manually annotated for evaluation purposes. We use DMDD toestablish baseline performance for dataset mention detection and linking. Byanalyzing the performance of various models on DMDD, we are able to identifyopen problems in dataset mention detection. We invite the community to use ourdataset as a challenge to develop novel dataset mention detection models.</description><author>Huitong Pan, Qi Zhang, Eduard Dragut, Cornelia Caragea, Longin Jan Latecki</author><pubDate>Fri, 19 May 2023 17:18:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11779v1</guid></item><item><title>Cross-Lingual Supervision improves Large Language Models Pre-training</title><link>http://arxiv.org/abs/2305.11778v1</link><description>The recent rapid progress in pre-training Large Language Models has relied onusing self-supervised language modeling objectives like next token predictionor span corruption. On the other hand, Machine Translation Systems are mostlytrained using cross-lingual supervision that requires aligned data betweensource and target languages. We demonstrate that pre-training Large LanguageModels on a mixture of a self-supervised Language Modeling objective and thesupervised Machine Translation objective, therefore including cross-lingualparallel data during pre-training, yields models with better in-contextlearning abilities. As pre-training is a very resource-intensive process and agrid search on the best mixing ratio between the two objectives isprohibitively expensive, we propose a simple yet effective strategy to learn itduring pre-training.</description><author>Andrea Schioppa, Xavier Garcia, Orhan Firat</author><pubDate>Fri, 19 May 2023 17:14:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11778v1</guid></item><item><title>Dynamic Sparse Training with Structured Sparsity</title><link>http://arxiv.org/abs/2305.02299v2</link><description>Dynamic Sparse Training (DST) methods achieve state-of-the-art results insparse neural network training, matching the generalization of dense modelswhile enabling sparse training and inference. Although the resulting models arehighly sparse and theoretically cheaper to train, achieving speedups withunstructured sparsity on real-world hardware is challenging. In this work, wepropose a sparse-to-sparse DST method to learn a variant of structured N:Msparsity by imposing a constant fan-in constraint. We demonstrate with both atheoretical analysis and empirical results: state-of-the-art spare-to-sparsestructured DST performance on a variety of network architectures, a condensedrepresentation with a reduced parameter and memory footprint, and reducedinference time compared to dense models with a naive PyTorch CPU implementationof the condensed representation. Our source code is available athttps://github.com/calgaryml/condensed-sparsity</description><author>Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, Yani Ioannou</author><pubDate>Fri, 19 May 2023 17:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02299v2</guid></item><item><title>Multi-Objective Optimization Using the R2 Utility</title><link>http://arxiv.org/abs/2305.11774v1</link><description>The goal of multi-objective optimization is to identify a collection ofpoints which describe the best possible trade-offs between the multipleobjectives. In order to solve this vector-valued optimization problem,practitioners often appeal to the use of scalarization functions in order totransform the multi-objective problem into a collection of single-objectiveproblems. This set of scalarized problems can then be solved using traditionalsingle-objective optimization techniques. In this work, we formalise thisconvention into a general mathematical framework. We show how this strategyeffectively recasts the original multi-objective optimization problem into asingle-objective optimization problem defined over sets. An appropriate classof objective functions for this new problem is the R2 utility function, whichis defined as a weighted integral over the scalarized optimization problems. Weshow that this utility function is a monotone and submodular set function,which can be optimised effectively using greedy optimization algorithms. Weanalyse the performance of these greedy algorithms both theoretically andempirically. Our analysis largely focusses on Bayesian optimization, which is apopular probabilistic framework for black-box optimization.</description><author>Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei</author><pubDate>Fri, 19 May 2023 17:01:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11774v1</guid></item><item><title>Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes</title><link>http://arxiv.org/abs/2305.11772v1</link><description>Humans and animals have a rich and flexible understanding of the physicalworld, which enables them to infer the underlying dynamical trajectories ofobjects and events, plausible future states, and use that to plan andanticipate the consequences of actions. However, the neural mechanismsunderlying these computations are unclear. We combine a goal-driven modelingapproach with dense neurophysiological data and high-throughput humanbehavioral readouts to directly impinge on this question. Specifically, weconstruct and evaluate several classes of sensory-cognitive networks to predictthe future state of rich, ethologically-relevant environments, ranging fromself-supervised end-to-end models with pixel-wise or object-centric objectives,to models that future predict in the latent space of purely static image-basedor dynamic video-based pretrained foundation models. We find strongdifferentiation across these model classes in their ability to predict neuraland behavioral data both within and across diverse environments. In particular,we find that neural responses are currently best predicted by models trained topredict the future state of their environment in the latent space of pretrainedfoundation models optimized for dynamic scenes in a self-supervised manner.Notably, models that future predict in the latent space of video foundationmodels that are optimized to support a diverse range of sensorimotor tasks,reasonably match both human behavioral error patterns and neural dynamicsacross all environmental scenarios that we were able to test. Overall, thesefindings suggest that the neural mechanisms and behaviors of primate mentalsimulation are thus far most consistent with being optimized to future predicton dynamic, reusable visual representations that are useful for embodied AImore generally.</description><author>Aran Nayebi, Rishi Rajalingham, Mehrdad Jazayeri, Guangyu Robert Yang</author><pubDate>Fri, 19 May 2023 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11772v1</guid></item><item><title>Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner</title><link>http://arxiv.org/abs/2305.11769v1</link><description>Large pre-trained multimodal models have demonstrated significant success ina range of downstream tasks, including image captioning, image-text retrieval,visual question answering (VQA), etc. However, many of these methods rely onimage-text pairs collected from the web as pre-training data and unfortunatelyoverlook the need for fine-grained feature alignment between vision andlanguage modalities, which requires detailed understanding of images andlanguage expressions. While integrating VQA and dense captioning (DC) intopre-training can address this issue, acquiring image-question-answer as well asimage-location-caption triplets is challenging and time-consuming.Additionally, publicly available datasets for VQA and dense captioning aretypically limited in scale due to manual data collection and labeling efforts.In this paper, we propose a novel method called Joint QA and DC GEneration(JADE), which utilizes a pre-trained multimodal model and easily-crawledimage-text pairs to automatically generate and filter large-scale VQA and densecaptioning datasets. We apply this method to the Conceptual Caption (CC3M)dataset to generate a new dataset called CC3M-QA-DC. Experiments show that whenused for pre-training in a multi-task manner, CC3M-QA-DC can improve theperformance with various backbones on various downstream tasks. Furthermore,our generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,CC15M) and achieve competitive results compared with models using much moredata. Code and dataset will be released.</description><author>Zikang Liu, Sihan Chen, Longteng Guo, Handong Li, Xingjian He, Jing Liu</author><pubDate>Fri, 19 May 2023 16:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11769v1</guid></item><item><title>Generating Visual Spatial Description via Holistic 3D Scene Understanding</title><link>http://arxiv.org/abs/2305.11768v1</link><description>Visual spatial description (VSD) aims to generate texts that describe thespatial relations of the given objects within images. Existing VSD work merelymodels the 2D geometrical vision features, thus inevitably falling prey to theproblem of skewed spatial understanding of target objects. In this work, weinvestigate the incorporation of 3D scene features for VSD. With an external 3Dscene extractor, we obtain the 3D objects and scene features for input images,based on which we construct a target object-centered 3D spatial scene graph(Go3D-S2G), such that we model the spatial semantics of target objects withinthe holistic 3D scenes. Besides, we propose a scene subgraph selectingmechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where thediverse local structure features are navigated to yield spatially-diversifiedtext generation. Experimental results on two VSD datasets demonstrate that ourframework outperforms the baselines significantly, especially improving on thecases with complex visual spatial relations. Meanwhile, our method can producemore spatially-diversified generation. Code is available athttps://github.com/zhaoyucs/VSD.</description><author>Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua</author><pubDate>Fri, 19 May 2023 16:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11768v1</guid></item><item><title>Transfer operators on graphs: Spectral clustering and beyond</title><link>http://arxiv.org/abs/2305.11766v1</link><description>Graphs and networks play an important role in modeling and analyzing complexinterconnected systems such as transportation networks, integrated circuits,power grids, citation graphs, and biological and artificial neural networks.Graph clustering algorithms can be used to detect groups of strongly connectedvertices and to derive coarse-grained models. We define transfer operators suchas the Koopman operator and the Perron-Frobenius operator on graphs, studytheir spectral properties, introduce Galerkin projections of these operators,and illustrate how reduced representations can be estimated from data. Inparticular, we show that spectral clustering of undirected graphs can beinterpreted in terms of eigenfunctions of the Koopman operator and proposenovel clustering algorithms for directed graphs based on generalized transferoperators. We demonstrate the efficacy of the resulting algorithms on severalbenchmark problems and provide different interpretations of clusters.</description><author>Stefan Klus, Maia Trower</author><pubDate>Fri, 19 May 2023 16:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11766v1</guid></item><item><title>Tester-Learners for Halfspaces: Universal Algorithms</title><link>http://arxiv.org/abs/2305.11765v1</link><description>We give the first tester-learner for halfspaces that succeeds universallyover a wide class of structured distributions. Our universal tester-learnerruns in fully polynomial time and has the following guarantee: the learnerachieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution thatthe tester accepts, and moreover, the tester accepts whenever the marginal isany distribution that satisfies a Poincar\'e inequality. In contrast to priorwork on testable learning, our tester is not tailored to any single targetdistribution but rather succeeds for an entire target class of distributions.The class of Poincar\'e distributions includes all strongly log-concavedistributions, and, assuming the Kannan--L\'{o}vasz--Simonovits (KLS)conjecture, includes all log-concave distributions. In the special case wherethe label noise is known to be Massart, our tester-learner achieves error$\mathrm{opt} + \epsilon$ while accepting all log-concave distributionsunconditionally (without assuming KLS). Our tests rely on checkinghypercontractivity of the unknown distribution using a sum-of-squares (SOS)program, and crucially make use of the fact that Poincar\'e distributions arecertifiably hypercontractive in the SOS framework.</description><author>Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan</author><pubDate>Fri, 19 May 2023 16:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11765v1</guid></item><item><title>ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation</title><link>http://arxiv.org/abs/2305.11761v1</link><description>Our proposed method, ReSeTOX (REdo SEarch if TOXic), addresses the issue ofNeural Machine Translation (NMT) generating translation outputs that containtoxic words not present in the input. The objective is to mitigate theintroduction of toxic language without the need for re-training. In the case ofidentified added toxicity during the inference process, ReSeTOX dynamicallyadjusts the key-value self-attention weights and re-evaluates the beam searchhypotheses. Experimental results demonstrate that ReSeTOX achieves a remarkable57% reduction in added toxicity while maintaining an average translationquality of 99.5% across 164 languages.</description><author>Javier GarcÃ­a Gilabert, Carlos Escolano, Marta R. Costa-JussÃ </author><pubDate>Fri, 19 May 2023 16:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11761v1</guid></item><item><title>Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning</title><link>http://arxiv.org/abs/2305.11759v1</link><description>Large Language Models (LLMs) are known to memorize significant portions oftheir training data. Parts of this memorized content have been shown to beextractable by simply querying the model, which poses a privacy risk. Wepresent a novel approach which uses prompt-tuning to control the extractionrates of memorized content in LLMs. We present two prompt training strategiesto increase and decrease extraction rates, which correspond to an attack and adefense, respectively. We demonstrate the effectiveness of our techniques byusing models from the GPT-Neo family on a public benchmark. For the 1.3Bparameter GPT-Neo model, our attack yields a 9.3 percentage point increase inextraction rate compared to our baseline. Our defense can be tuned to achievedifferent privacy-utility trade-offs by a user-specified hyperparameter. Weachieve an extraction rate reduction of up to 97.7% relative to our baseline,with a perplexity increase of 16.9%.</description><author>Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta</author><pubDate>Fri, 19 May 2023 16:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11759v1</guid></item><item><title>Visualization for Recommendation Explainability: A Survey and New Perspectives</title><link>http://arxiv.org/abs/2305.11755v1</link><description>Providing system-generated explanations for recommendations represents animportant step towards transparent and trustworthy recommender systems.Explainable recommender systems provide a human-understandable rationale fortheir outputs. Over the last two decades, explainable recommendation hasattracted much attention in the recommender systems research community. Thispaper aims to provide a comprehensive review of research efforts on visualexplanation in recommender systems. More concretely, we systematically reviewthe literature on explanations in recommender systems based on four dimensions,namely explanation goal, explanation scope, explanation style, and explanationformat. Recognizing the importance of visualization, we approach therecommender system literature from the angle of explanatory visualizations,that is using visualizations as a display style of explanation. As a result, wederive a set of guidelines that might be constructive for designing explanatoryvisualizations in recommender systems and identify perspectives for future workin this field. The aim of this review is to help recommendation researchers andpractitioners better understand the potential of visually explainablerecommendation research and to support them in the systematic design of visualexplanations in current and future recommender systems.</description><author>Mohamed Amine Chatti, Mouadh Guesmi, Arham Muslim</author><pubDate>Fri, 19 May 2023 16:42:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11755v1</guid></item><item><title>Marginalized Beam Search Algorithms for Hierarchical HMMs</title><link>http://arxiv.org/abs/2305.11752v1</link><description>Inferring a state sequence from a sequence of measurements is a fundamentalproblem in bioinformatics and natural language processing. The Viterbi and theBeam Search (BS) algorithms are popular inference methods, but they havelimitations when applied to Hierarchical Hidden Markov Models (HHMMs), wherethe interest lies in the outer state sequence. The Viterbi algorithm can notinfer outer states without inner states, while the BS algorithm requiresmarginalization over prohibitively large state spaces. We propose two newalgorithms to overcome these limitations: the greedy marginalized BS algorithmand the local focus BS algorithm. We show that they approximate the most likelyouter state sequence with higher performance than the Viterbi algorithm, and weevaluate the performance of these algorithms on an explicit duration HMM withsimulation and nanopore base calling data.</description><author>Xuechun Xu, Joakim JaldÃ©n</author><pubDate>Fri, 19 May 2023 16:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11752v1</guid></item><item><title>DELTA: Diverse Client Sampling for Fasting Federated Learning</title><link>http://arxiv.org/abs/2205.13925v3</link><description>Partial client participation has been widely adopted in Federated Learning(FL) to reduce the communication burden efficiently. However, an inadequateclient sampling scheme can lead to the selection of unrepresentative subsets,resulting in significant variance in model updates and slowed convergence.Existing sampling methods are either biased or can be further optimized forfaster convergence.In this paper, we present DELTA, an unbiased sampling schemedesigned to alleviate these issues. DELTA characterizes the effects of clientdiversity and local variance, and samples representative clients with valuableinformation for global model updates. In addition, DELTA is a proven optimalunbiased sampling scheme that minimizes variance caused by partial clientparticipation and outperforms other unbiased sampling schemes in terms ofconvergence. Furthermore, to address full-client gradient dependence,we providea practical version of DELTA depending on the available clients' information,and also analyze its convergence. Our results are validated through experimentson both synthetic and real-world datasets.</description><author>Lin Wang, YongXin Guo, Tao Lin, Xiaoying Tang</author><pubDate>Fri, 19 May 2023 16:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.13925v3</guid></item><item><title>HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models</title><link>http://arxiv.org/abs/2305.11747v1</link><description>Large language models (LLMs), such as ChatGPT, are prone to generatehallucinations, \ie content that conflicts with the source or cannot beverified by the factual knowledge. To understand what types of content and towhich extent LLMs are apt to hallucinate, we introduce the HallucinationEvaluation for Large Language Models (HELMA) benchmark, a large collection ofgenerated and human-annotated hallucinated samples for evaluating theperformance of LLMs in recognizing and alleviating hallucination. To generatethese samples, we propose a ChatGPT-based two-step framework, \iesampling-then-filtering. Specifically, we first adopt two different samplingmethods to generate hallucinated samples based on instructions, and then use anexample-enhanced filtering method to select the best one. Furthermore, we alsohire some human labelers to annotate the hallucinations in ChatGPT responses.The empirical results suggest that ChatGPT has some probabilities to generatehallucinations and existing LLMs face great challenges in recognizing thehallucinations in text. In addition, the performance can be improved byproviding external knowledge or adding reasoning steps. Our benchmark can beaccessed at https://github.com/RUCAIBox/HELMA.</description><author>Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen</author><pubDate>Fri, 19 May 2023 16:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11747v1</guid></item><item><title>HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation</title><link>http://arxiv.org/abs/2305.11746v1</link><description>Hallucinations in machine translation are translations that containinformation completely unrelated to the input. Omissions are translations thatdo not include some of the input information. While both cases tend to becatastrophic errors undermining user trust, annotated data with these types ofpathologies is extremely scarce and is limited to a few high-resourcelanguages. In this work, we release an annotated dataset for the hallucinationand omission phenomena covering 18 translation directions with varying resourcelevels and scripts. Our annotation covers different levels of partial and fullhallucinations as well as omissions both at the sentence and at the word level.Additionally, we revisit previous methods for hallucination and omissiondetection, show that conclusions made based on a single language pair largelydo not hold for a large-scale evaluation, and establish new solid baselines.</description><author>David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, LoÃ¯c Barrault, Marta R. Costa-jussÃ </author><pubDate>Fri, 19 May 2023 16:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11746v1</guid></item><item><title>Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval</title><link>http://arxiv.org/abs/2305.11744v1</link><description>Neural information retrieval often adopts a retrieve-and-rerank framework: abi-encoder network first retrieves K (e.g., 100) candidates that are thenre-ranked using a more powerful cross-encoder model to rank the bettercandidates higher. The re-ranker generally produces better candidate scoresthan the retriever, but is limited to seeing only the top K retrievedcandidates, thus providing no improvements in retrieval performance as measuredby Recall@K. In this work, we leverage the re-ranker to also improve retrievalby providing inference-time relevance feedback to the retriever. Concretely, weupdate the retriever's query representation for a test instance using alightweight inference-time distillation of the re-ranker's prediction for thatinstance. The distillation loss is designed to bring the retriever's candidatescores closer to those of the re-ranker. A second retrieval step is thenperformed with the updated query vector. We empirically show that our approach,which can serve arbitrary retrieve-and-rerank pipelines, significantly improvesretrieval recall in multiple domains, languages, and modalities.</description><author>Revanth Gangi Reddy, Pradeep Dasigi, Md Arafat Sultan, Arman Cohan, Avirup Sil, Heng Ji, Hannaneh Hajishirzi</author><pubDate>Fri, 19 May 2023 16:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11744v1</guid></item><item><title>TaLU: A Hybrid Activation Function Combining Tanh and Rectified Linear Unit to Enhance Neural Networks</title><link>http://arxiv.org/abs/2305.04402v2</link><description>The application of the deep learning model in classification plays animportant role in the accurate detection of the target objects. However, theaccuracy is affected by the activation function in the hidden and output layer.In this paper, an activation function called TaLU, which is a combination ofTanh and Rectified Linear Units (ReLU), is used to improve the prediction. ReLUactivation function is used by many deep learning researchers for itscomputational efficiency, ease of implementation, intuitive nature, etc.However, it suffers from a dying gradient problem. For instance, when the inputis negative, its output is always zero because its gradient is zero. A numberof researchers used different approaches to solve this issue. Some of the mostnotable are LeakyReLU, Softplus, Softsign, ELU, ThresholdedReLU, etc. Thisresearch developed TaLU, a modified activation function combining Tanh andReLU, which mitigates the dying gradient problem of ReLU. The deep learningmodel with the proposed activation function was tested on MNIST and CIFAR-10,and it outperforms ReLU and some other studied activation functions in terms ofaccuracy(upto 6% in most cases, when used with Batch Normalization and areasonable learning rate).</description><author>Md. Mehedi Hasan, Md. Ali Hossain, Azmain Yakin Srizon, Abu Sayeed</author><pubDate>Fri, 19 May 2023 16:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04402v2</guid></item><item><title>MedLens: Improve mortality prediction via medical signs selecting and regression interpolation</title><link>http://arxiv.org/abs/2305.11742v1</link><description>Monitoring the health status of patients and predicting mortality in advanceis vital for providing patients with timely care and treatment. Massive medicalsigns in electronic health records (EHR) are fitted into advanced machinelearning models to make predictions. However, the data-quality problem oforiginal clinical signs is less discussed in the literature. Based on anin-depth measurement of the missing rate and correlation score across variousmedical signs and a large amount of patient hospital admission records, wediscovered the comprehensive missing rate is extremely high, and a large numberof useless signs could hurt the performance of prediction models. Then weconcluded that only improving data-quality could improve the baseline accuracyof different prediction algorithms. We designed MEDLENS, with an automaticvital medical signs selection approach via statistics and a flexibleinterpolation approach for high missing rate time series. After augmenting thedata-quality of original medical signs, MEDLENS applies ensemble classifiers toboost the accuracy and reduce the computation overhead at the same time. Itachieves a very high accuracy performance of 0.96% AUC-ROC and 0.81% AUC-PR,which exceeds the previous benchmark.</description><author>Xuesong Ye, Jun Wu, Chengjie Mou, Weinan Dai</author><pubDate>Fri, 19 May 2023 16:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11742v1</guid></item><item><title>Survey of Automatic Plankton Image Recognition: Challenges, Existing Solutions and Future Perspectives</title><link>http://arxiv.org/abs/2305.11739v1</link><description>Planktonic organisms are key components of aquatic ecosystems and respondquickly to changes in the environment, therefore their monitoring is vital tounderstand the changes in the environment. Yet, monitoring plankton atappropriate scales still remains a challenge, limiting our understanding offunctioning of aquatic systems and their response to changes. Modern planktonimaging instruments can be utilized to sample at high frequencies, enablingnovel possibilities to study plankton populations. However, manual analysis ofthe data is costly, time consuming and expert based, making such approachunsuitable for large-scale application and urging for automatic solutions. Thekey problem related to the utilization of plankton datasets through imageanalysis is plankton recognition. Despite the large amount of research done,automatic methods have not been widely adopted for operational use. In thispaper, a comprehensive survey on existing solutions for automatic planktonrecognition is presented. First, we identify the most notable challenges thatthat make the development of plankton recognition systems difficult. Then, weprovide a detailed description of solutions for these challenges proposed inplankton recognition literature. Finally, we propose a workflow to identify thespecific challenges in new datasets and the recommended approaches to addressthem. For many of the challenges, applicable solutions exist. However,important challenges remain unsolved: 1) the domain shift between the datasetshindering the development of a general plankton recognition system that wouldwork across different imaging instruments, 2) the difficulty to identify andprocess the images of previously unseen classes, and 3) the uncertainty inexpert annotations that affects the training of the machine learning models forrecognition. These challenges should be addressed in the future research.</description><author>Tuomas Eerola, Daniel Batrakhanov, Nastaran Vatankhah Barazandeh, Kaisa Kraft, Lumi Haraguchi, Lasse Lensu, Sanna Suikkanen, Jukka SeppÃ¤lÃ¤, Timo Tamminen, Heikki KÃ¤lviÃ¤inen</author><pubDate>Fri, 19 May 2023 16:20:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11739v1</guid></item><item><title>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</title><link>http://arxiv.org/abs/2305.11738v1</link><description>Recent developments in large language models (LLMs) have been impressive.However, these models sometimes show inconsistencies and problematic behavior,such as hallucinating facts, generating flawed code, or creating offensive andtoxic content. Unlike these models, humans typically utilize external tools tocross-check and refine their initial content, like using a search engine forfact-checking, or a code interpreter for debugging. Inspired by thisobservation, we introduce a framework called CRITIC that allows LLMs, which areessentially "black boxes" to validate and progressively amend their own outputsin a manner similar to human interaction with tools. More specifically,starting with an initial output, CRITIC interacts with appropriate tools toevaluate certain aspects of the text, and then revises the output based on thefeedback obtained during this validation process. Comprehensive evaluationsinvolving free-form question answering, mathematical program synthesis, andtoxicity reduction demonstrate that CRITIC consistently enhances theperformance of LLMs. Meanwhile, our research highlights the crucial importanceof external feedback in promoting the ongoing self-improvement of LLMs.</description><author>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</author><pubDate>Fri, 19 May 2023 16:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11738v1</guid></item><item><title>TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems</title><link>http://arxiv.org/abs/2304.01951v4</link><description>Processing-in-memory (PIM) promises to alleviate the data movement bottleneckin modern computing systems. However, current real-world PIM systems have theinherent disadvantage that their hardware is more constrained than inconventional processors (CPU, GPU), due to the difficulty and cost of buildingprocessing elements near or inside the memory. As a result, general-purpose PIMarchitectures support fairly limited instruction sets and struggle to executecomplex operations such as transcendental functions and other hard-to-calculateoperations (e.g., square root). These operations are particularly important forsome modern workloads, e.g., activation functions in machine learningapplications. In order to provide support for transcendental (and other hard-to-calculate)functions in general-purpose PIM systems, we present \emph{TransPimLib}, alibrary that provides CORDIC-based and LUT-based methods for trigonometricfunctions, hyperbolic functions, exponentiation, logarithm, square root, etc.We develop an implementation of TransPimLib for the UPMEM PIM architecture andperform a thorough evaluation of TransPimLib's methods in terms of performanceand accuracy, using microbenchmarks and three full workloads (Blackscholes,Sigmoid, Softmax). We open-source all our code and datasetsat~\url{https://github.com/CMU-SAFARI/transpimlib}.</description><author>Maurus Item, Juan GÃ³mez-Luna, Yuxin Guo, Geraldo F. Oliveira, Mohammad Sadrosadati, Onur Mutlu</author><pubDate>Fri, 19 May 2023 16:16:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01951v4</guid></item><item><title>Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment</title><link>http://arxiv.org/abs/2305.11733v1</link><description>Long-tailed data is still a big challenge for deep neural networks, eventhough they have achieved great success on balanced data. We observe thatvanilla training on long-tailed data with cross-entropy loss makes theinstance-rich head classes severely squeeze the spatial distribution of thetail classes, which leads to difficulty in classifying tail class samples.Furthermore, the original cross-entropy loss can only propagate gradientshort-lively because the gradient in softmax form rapidly approaches zero asthe logit difference increases. This phenomenon is called softmax saturation.It is unfavorable for training on balanced data, but can be utilized to adjustthe validity of the samples in long-tailed data, thereby solving the distortedembedding space of long-tailed problems. To this end, this paper proposes theGaussian clouded logit adjustment by Gaussian perturbation of different classlogits with varied amplitude. We define the amplitude of perturbation as cloudsize and set relatively large cloud sizes to tail classes. The large cloud sizecan reduce the softmax saturation and thereby making tail class samples moreactive as well as enlarging the embedding space. To alleviate the bias in aclassifier, we therefore propose the class-based effective number samplingstrategy with classifier re-training. Extensive experiments on benchmarkdatasets validate the superior performance of the proposed method. Source codeis available at https://github.com/Keke921/GCLLoss.</description><author>Mengke Li, Yiu-ming Cheung, Yang Lu</author><pubDate>Fri, 19 May 2023 16:11:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11733v1</guid></item><item><title>An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System</title><link>http://arxiv.org/abs/2207.07886v4</link><description>Training machine learning (ML) algorithms is a computationally intensiveprocess, which is frequently memory-bound due to repeatedly accessing largetraining datasets. As a result, processor-centric systems (e.g., CPU, GPU)suffer from costly data movement between memory units and processing units,which consumes large amounts of energy and execution cycles. Memory-centriccomputing systems, i.e., with processing-in-memory (PIM) capabilities, canalleviate this data movement bottleneck. Our goal is to understand the potential of modern general-purpose PIMarchitectures to accelerate ML training. To do so, we (1) implement severalrepresentative classic ML algorithms (namely, linear regression, logisticregression, decision tree, K-Means clustering) on a real-world general-purposePIM architecture, (2) rigorously evaluate and characterize them in terms ofaccuracy, performance and scaling, and (3) compare to their counterpartimplementations on CPU and GPU. Our evaluation on a real memory-centriccomputing system with more than 2500 PIM cores shows that general-purpose PIMarchitectures can greatly accelerate memory-bound ML workloads, when thenecessary operations and datatypes are natively supported by PIM hardware. Forexample, our PIM implementation of decision tree is $27\times$ faster than astate-of-the-art CPU version on an 8-core Intel Xeon, and $1.34\times$ fasterthan a state-of-the-art GPU version on an NVIDIA A100. Our K-Means clusteringon PIM is $2.8\times$ and $3.2\times$ than state-of-the-art CPU and GPUversions, respectively. To our knowledge, our work is the first one to evaluate ML training on areal-world PIM architecture. We conclude with key observations, takeaways, andrecommendations that can inspire users of ML workloads, programmers of PIMarchitectures, and hardware designers &amp; architects of future memory-centriccomputing systems.</description><author>Juan GÃ³mez-Luna, Yuxin Guo, Sylvan Brocard, Julien Legriel, Remy Cimadomo, Geraldo F. Oliveira, Gagandeep Singh, Onur Mutlu</author><pubDate>Fri, 19 May 2023 16:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07886v4</guid></item><item><title>Persian Typographical Error Type Detection using Many-to-Many Deep Neural Networks on Algorithmically-Generated Misspellings</title><link>http://arxiv.org/abs/2305.11731v1</link><description>Digital technologies have led to an influx of text created daily in a varietyof languages, styles, and formats. A great deal of the popularity ofspell-checking systems can be attributed to this phenomenon since they arecrucial to polishing the digitally conceived text. In this study, we tackleTypographical Error Type Detection in Persian, which has been relativelyunderstudied. In this paper, we present a public dataset named FarsTypo,containing 3.4 million chronologically ordered and part-of-speech tagged wordsof diverse topics and linguistic styles. An algorithm for applyingPersian-specific errors is developed and applied to a scalable size of thesewords, forming a parallel dataset of correct and incorrect words. UsingFarsTypo, we establish a firm baseline and compare different methodologiesusing various architectures. In addition, we present a novel Many-to-Many DeepSequential Neural Network to perform token classification using both word andcharacter embeddings in combination with bidirectional LSTM layers to detecttypographical errors across 51 classes. We compare our approach withhighly-advanced industrial systems that, unlike this study, have been developedutilizing a variety of resources. The results of our final method werecompetitive in that we achieved an accuracy of 97.62%, a precision of 98.83%, arecall of 98.61%, and outperformed the rest in terms of speed.</description><author>Mohammad Dehghani, Heshaam Faili</author><pubDate>Fri, 19 May 2023 16:05:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11731v1</guid></item><item><title>ViDaS Video Depth-aware Saliency Network</title><link>http://arxiv.org/abs/2305.11729v1</link><description>We introduce ViDaS, a two-stream, fully convolutional Video, Depth-AwareSaliency network to address the problem of attention modeling ``in-the-wild",via saliency prediction in videos. Contrary to existing visual saliencyapproaches using only RGB frames as input, our network employs also depth as anadditional modality. The network consists of two visual streams, one for theRGB frames, and one for the depth frames. Both streams follow anencoder-decoder approach and are fused to obtain a final saliency map. Thenetwork is trained end-to-end and is evaluated in a variety of differentdatabases with eye-tracking data, containing a wide range of video content.Although the publicly available datasets do not contain depth, we estimate itusing three different state-of-the-art methods, to enable comparisons and adeeper insight. Our method outperforms in most cases state-of-the-art modelsand our RGB-only variant, which indicates that depth can be beneficial toaccurately estimating saliency in videos displayed on a 2D screen. Depth hasbeen widely used to assist salient object detection problems, where it has beenproven to be very beneficial. Our problem though differs significantly fromsalient object detection, since it is not restricted to specific salientobjects, but predicts human attention in a more general aspect. These twoproblems not only have different objectives, but also different ground truthdata and evaluation metrics. To our best knowledge, this is the firstcompetitive deep learning video saliency estimation approach that combines bothRGB and Depth features to address the general problem of saliency estimation``in-the-wild". The code will be publicly released.</description><author>Ioanna Diamanti, Antigoni Tsiami, Petros Koutras, Petros Maragos</author><pubDate>Fri, 19 May 2023 16:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11729v1</guid></item><item><title>Sequential image recovery using joint hierarchical Bayesian learning</title><link>http://arxiv.org/abs/2206.12745v2</link><description>Recovering temporal image sequences (videos) based on indirect, noisy, orincomplete data is an essential yet challenging task. We specifically considerthe case where each data set is missing vital information, which prevents theaccurate recovery of the individual images. Although some recent (variational)methods have demonstrated high-resolution image recovery based on jointlyrecovering sequential images, there remain robustness issues due to parametertuning and restrictions on the type of the sequential images. Here, we presenta method based on hierarchical Bayesian learning for the joint recovery ofsequential images that incorporates prior intra- and inter-image information.Our method restores the missing information in each image by "borrowing" itfrom the other images. As a result, \emph{all} of the individualreconstructions yield improved accuracy. Our method can be used for variousdata acquisitions and allows for uncertainty quantification. Some preliminaryresults indicate its potential use for sequential deblurring and magneticresonance imaging.</description><author>Yao Xiao, Jan Glaubitz</author><pubDate>Fri, 19 May 2023 16:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12745v2</guid></item><item><title>Towards More Transparent and Accurate Cancer Diagnosis with an Unsupervised CAE Approach</title><link>http://arxiv.org/abs/2305.11728v1</link><description>Digital pathology has revolutionized cancer diagnosis by leveragingContent-Based Medical Image Retrieval (CBMIR) for analyzing histopathologicalWhole Slide Images (WSIs). CBMIR enables searching for similar content,enhancing diagnostic reliability and accuracy. In 2020, breast and prostatecancer constituted 11.7% and 14.1% of cases, respectively, as reported by theGlobal Cancer Observatory (GCO). The proposed Unsupervised CBMIR (UCBMIR)replicates the traditional cancer diagnosis workflow, offering a dependablemethod to support pathologists in WSI-based diagnostic conclusions. Thisapproach alleviates pathologists' workload, potentially enhancing diagnosticefficiency. To address the challenge of the lack of labeled histopathologicalimages in CBMIR, a customized unsupervised Convolutional Auto Encoder (CAE) wasdeveloped, extracting 200 features per image for the search engine component.UCBMIR was evaluated using widely-used numerical techniques in CBMIR, alongsidevisual evaluation and comparison with a classifier. The validation involvedthree distinct datasets, with an external evaluation demonstrating itseffectiveness. UCBMIR outperformed previous studies, achieving a top 5 recallof 99% and 80% on BreaKHis and SICAPv2, respectively, using the firstevaluation technique. Precision rates of 91% and 70% were achieved for BreaKHisand SICAPv2, respectively, using the second evaluation technique. Furthermore,UCBMIR demonstrated the capability to identify various patterns in patches,achieving an 81% accuracy in the top 5 when tested on an external image fromArvaniti.</description><author>Zahra Tabatabaei, Adrian Colomer, Javier Oliver Moll, Valery Naranjo</author><pubDate>Fri, 19 May 2023 16:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11728v1</guid></item><item><title>Non-stationary Projection-free Online Learning with Dynamic and Adaptive Regret Guarantees</title><link>http://arxiv.org/abs/2305.11726v1</link><description>Projection-free online learning has drawn increasing interest due to itsefficiency in solving high-dimensional problems with complicated constraints.However, most existing projection-free online methods focus on minimizing thestatic regret, which unfortunately fails to capture the challenge of changingenvironments. In this paper, we investigate non-stationary projection-freeonline learning, and choose dynamic regret and adaptive regret to measure theperformance. Specifically, we first provide a novel dynamic regret analysis foran existing projection-free method named $\text{BOGD}_\text{IP}$, and establishan $\mathcal{O}(T^{3/4}(1+P_T))$ dynamic regret bound, where $P_T$ denotes thepath-length of the comparator sequence. Then, we improve the upper bound to$\mathcal{O}(T^{3/4}(1+P_T)^{1/4})$ by running multiple $\text{BOGD}_\text{IP}$algorithms with different step sizes in parallel, and tracking the best one onthe fly. Our results are the first general-case dynamic regret bounds forprojection-free online learning, and can recover the existing$\mathcal{O}(T^{3/4})$ static regret by setting $P_T = 0$. Furthermore, wepropose a projection-free method to attain an $\tilde{\mathcal{O}}(\tau^{3/4})$adaptive regret bound for any interval with length $\tau$, which nearly matchesthe static regret over that interval. The essential idea is to maintain a setof $\text{BOGD}_\text{IP}$ algorithms dynamically, and combine them by a metaalgorithm. Moreover, we demonstrate that it is also equipped with an$\mathcal{O}(T^{3/4}(1+P_T)^{1/4})$ dynamic regret bound. Finally, empiricalstudies verify our theoretical findings.</description><author>Yibo Wang, Wenhao Yang, Wei Jiang, Shiyin Lu, Bing Wang, Haihong Tang, Yuanyu Wan, Lijun Zhang</author><pubDate>Fri, 19 May 2023 16:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11726v1</guid></item><item><title>S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering</title><link>http://arxiv.org/abs/2305.11725v1</link><description>Answering multi-hop questions over hybrid factual knowledge from the giventext and table (TextTableQA) is a challenging task. Existing models mainlyadopt a retriever-reader framework, which have several deficiencies, such asnoisy labeling in training retriever, insufficient utilization of heterogeneousinformation over text and table, and deficient ability for different reasoningoperations. In this paper, we propose a three-stage TextTableQA frameworkS3HQA, which comprises of retriever, selector, and reasoner. We use a retrieverwith refinement training to solve the noisy labeling problem. Then, a hybridselector considers the linked relationships between heterogeneous data toselect the most relevant factual knowledge. For the final stage, instead ofadapting a reading comprehension module like in previous methods, we employ ageneration-based reasoner to obtain answers. This includes two approaches: arow-wise generator and an LLM prompting generator~(first time used in thistask). The experimental results demonstrate that our method achievescompetitive results in the few-shot setting. When trained on the full dataset,our approach outperforms all baseline methods, ranking first on the HybridQAleaderboard.</description><author>Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, Kang Liu</author><pubDate>Fri, 19 May 2023 16:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11725v1</guid></item><item><title>Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling</title><link>http://arxiv.org/abs/2305.11719v1</link><description>Existing research on multimodal relation extraction (MRE) faces twoco-existing challenges, internal-information over-utilization andexternal-information under-exploitation. To combat that, we propose a novelframework that simultaneously implements the idea of internal-informationscreening and external-information exploiting. First, we represent thefine-grained semantic structures of the input image and text with the visualand textual scene graphs, which are further fused into a unified cross-modalgraph (CMG). Based on CMG, we perform structure refinement with the guidance ofthe graph information bottleneck principle, actively denoising theless-informative features. Next, we perform topic modeling over the input imageand text, incorporating latent multimodal topic features to enrich thecontexts. On the benchmark MRE dataset, our system outperforms the current bestmodel significantly. With further in-depth analyses, we reveal the greatpotential of our method for the MRE task. Our codes are open athttps://github.com/ChocoWu/MRE-ISE.</description><author>Shengqiong Wu, Hao Fei, Yixin Cao, Lidong Bing, Tat-Seng Chua</author><pubDate>Fri, 19 May 2023 15:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11719v1</guid></item><item><title>Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization</title><link>http://arxiv.org/abs/2305.11718v1</link><description>Existing vector quantization (VQ) based autoregressive models follow atwo-stage generation paradigm that first learns a codebook to encode images asdiscrete codes, and then completes generation based on the learned codebook.However, they encode fixed-size image regions into fixed-length codes andignore their naturally different information densities, which results ininsufficiency in important regions and redundancy in unimportant ones, andfinally degrades the generation quality and speed. Moreover, the fixed-lengthcoding leads to an unnatural raster-scan autoregressive generation. To addressthe problem, we propose a novel two-stage framework: (1) Dynamic-QuantizationVAE (DQ-VAE) which encodes image regions into variable-length codes based ontheir information densities for an accurate and compact code representation.(2) DQ-Transformer which thereby generates images autoregressively fromcoarse-grained (smooth regions with fewer codes) to fine-grained (detailsregions with more codes) by modeling the position and content of codes in eachgranularity alternately, through a novel stacked-transformer architecture andshared-content, non-shared position input layers designs. Comprehensiveexperiments on various generation tasks validate our superiorities in botheffectiveness and efficiency. Code will be released athttps://github.com/CrossmodalGroup/DynamicVectorQuantization.</description><author>Mengqi Huang, Zhendong Mao, Zhuowei Chen, Yongdong Zhang</author><pubDate>Fri, 19 May 2023 15:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11718v1</guid></item><item><title>Efficient and Deterministic Search Strategy Based on Residual Projections for Point Cloud Registration</title><link>http://arxiv.org/abs/2305.11716v1</link><description>Estimating the rigid transformation between two LiDAR scans through putative3D correspondences is a typical point cloud registration paradigm. Current 3Dfeature matching approaches commonly lead to numerous outlier correspondences,making outlier-robust registration techniques indispensable. Many recentstudies have adopted the branch and bound (BnB) optimization framework to solvethe correspondence-based point cloud registration problem globally anddeterministically. Nonetheless, BnB-based methods are time-consuming to searchthe entire 6-dimensional parameter space, since their computational complexityis exponential to the dimension of the solution domain. In order to enhancealgorithm efficiency, existing works attempt to decouple the 6 degrees offreedom (DOF) original problem into two 3-DOF sub-problems, thereby reducingthe dimension of the parameter space. In contrast, our proposed approachintroduces a novel pose decoupling strategy based on residual projections,effectively decomposing the raw problem into three 2-DOF rotation searchsub-problems. Subsequently, we employ a novel BnB-based search method to solvethese sub-problems, achieving efficient and deterministic registration.Furthermore, our method can be adapted to address the challenging problem ofsimultaneous pose and correspondence registration (SPCR). Through extensiveexperiments conducted on synthetic and real-world datasets, we demonstrate thatour proposed method outperforms state-of-the-art methods in terms ofefficiency, while simultaneously ensuring robustness.</description><author>Xinyi Li, Yinlong Liu, Hu Cao, Xueli Liu, Feihu Zhang, Alois Knoll</author><pubDate>Fri, 19 May 2023 15:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11716v1</guid></item><item><title>A quality assurance framework for real-time monitoring of deep learning segmentation models in radiotherapy</title><link>http://arxiv.org/abs/2305.11715v1</link><description>To safely deploy deep learning models in the clinic, a quality assuranceframework is needed for routine or continuous monitoring of input-domain shiftand the models' performance without ground truth contours. In this work,cardiac substructure segmentation was used as an example task to establish a QAframework. A benchmark dataset consisting of Computed Tomography (CT) imagesalong with manual cardiac delineations of 241 patients were collected,including one 'common' image domain and five 'uncommon' domains. Segmentationmodels were tested on the benchmark dataset for an initial evaluation of modelcapacity and limitations. An image domain shift detector was developed byutilizing a trained Denoising autoencoder (DAE) and two hand-engineeredfeatures. Another Variational Autoencoder (VAE) was also trained to estimatethe shape quality of the auto-segmentation results. Using the extractedfeatures from the image/segmentation pair as inputs, a regression model wastrained to predict the per-patient segmentation accuracy, measured by Dicecoefficient similarity (DSC). The framework was tested across 19 segmentationmodels to evaluate the generalizability of the entire framework. As results, the predicted DSC of regression models achieved a mean absoluteerror (MAE) ranging from 0.036 to 0.046 with an averaged MAE of 0.041. Whentested on the benchmark dataset, the performances of all segmentation modelswere not significantly affected by scanning parameters: FOV, slice thicknessand reconstructions kernels. For input images with Poisson noise, CNN-basedsegmentation models demonstrated a decreased DSC ranging from 0.07 to 0.41,while the transformer-based model was not significantly affected.</description><author>Xiyao Jin, Yao Hao, Jessica Hilliard, Zhehao Zhang, Maria A. Thomas, Hua Li, Abhinav K. Jha, Geoffrey D. Hugo</author><pubDate>Fri, 19 May 2023 15:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11715v1</guid></item><item><title>What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability</title><link>http://arxiv.org/abs/2305.11707v1</link><description>In Natural Language Generation (NLG) tasks, for any input, multiplecommunicative goals are plausible, and any goal can be put into words, orproduced, in multiple ways. We characterise the extent to which humanproduction varies lexically, syntactically, and semantically across four NLGtasks, connecting human production variability to aleatoric or datauncertainty. We then inspect the space of output strings shaped by a generationsystem's predicted probability distribution and decoding algorithm to probe itsuncertainty. For each test input, we measure the generator's calibration tohuman production variability. Following this instance-level approach, weanalyse NLG models and decoding strategies, demonstrating that probing agenerator with multiple samples and, when possible, multiple references,provides the level of detail necessary to gain understanding of a model'srepresentation of uncertainty.</description><author>Mario Giulianelli, Joris Baan, Wilker Aziz, Raquel FernÃ¡ndez, Barbara Plank</author><pubDate>Fri, 19 May 2023 15:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11707v1</guid></item><item><title>SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities</title><link>http://arxiv.org/abs/2305.11000v2</link><description>Multi-modal large language models are regarded as a crucial step towardsArtificial General Intelligence (AGI) and have garnered significant interestwith the emergence of ChatGPT. However, current speech-language modelstypically adopt the cascade paradigm, preventing inter-modal knowledgetransfer. In this paper, we propose SpeechGPT, a large language model withintrinsic cross-modal conversational abilities, capable of perceiving andgenerating multi-model content. With discrete speech representations, we firstconstruct SpeechInstruct, a large-scale cross-modal speech instruction dataset.Additionally, we employ a three-stage training strategy that includesmodality-adaptation pre-training, cross-modal instruction fine-tuning, andchain-of-modality instruction fine-tuning. The experimental results demonstratethat SpeechGPT has an impressive capacity to follow multi-modal humaninstructions and highlight the potential of handling multiple modalities withone model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.</description><author>Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu</author><pubDate>Fri, 19 May 2023 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11000v2</guid></item><item><title>Execution-Based Evaluation for Open-Domain Code Generation</title><link>http://arxiv.org/abs/2212.10481v2</link><description>To extend the scope of coding queries to more realistic settings, we proposeODEX, the first Open-Domain EXecution-based natural language (NL) to Pythoncode generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverselibraries, along with 1,707 human-written test cases for execution. Our NL-Codepairs are harvested from StackOverflow forums to encourage natural andpractical coding queries. Moreover, ODEX supports four natural languages asintents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguingbehavioral differences among top-performing code language models (LM). WhileCODEX achieves better overall results, CODEGEN improves effectively via scaling-- CODEGEN 6.1B performs comparably with CODEX 12B. Both models showsubstantial gaps between open and closed domains, but CODEGEN gaps tend todecrease with model size while CODEX gaps increase. We release ODEX tofacilitate research into open-domain problems for the code generationcommunity.</description><author>Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig</author><pubDate>Fri, 19 May 2023 15:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10481v2</guid></item><item><title>Know What I don't Know: Handling Ambiguous and Unanswerable Questions for Text-to-SQL</title><link>http://arxiv.org/abs/2212.08902v2</link><description>The task of text-to-SQL aims to convert a natural language question into itscorresponding SQL query within the context of relational tables. Existingtext-to-SQL parsers generate a "plausible" SQL query for an arbitrary userquestion, thereby failing to correctly handle problematic user questions. Toformalize this problem, we conduct a preliminary study on the observedambiguous and unanswerable cases in text-to-SQL and summarize them into 6feature categories. Correspondingly, we identify the causes behind eachcategory and propose requirements for handling ambiguous and unanswerablequestions. Following this study, we propose a simple yet effectivecounterfactual example generation approach that automatically producesambiguous and unanswerable text-to-SQL examples. Furthermore, we propose aweakly supervised DTE (Detecting-Then-Explaining) model for error detection,localization, and explanation. Experimental results show that our modelachieves the best result on both real-world examples and generated examplescompared with various baselines. We release our data and code at:\href{https://github.com/wbbeyourself/DTE}{https://github.com/wbbeyourself/DTE}.</description><author>Bing Wang, Yan Gao, Zhoujun Li, Jian-Guang Lou</author><pubDate>Fri, 19 May 2023 15:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08902v2</guid></item><item><title>S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning</title><link>http://arxiv.org/abs/2305.11701v1</link><description>The recent emergence of Self-Supervised Learning (SSL) as a fundamentalparadigm for learning image representations has, and continues to, demonstratehigh empirical success in a variety of tasks. However, most SSL approaches failto learn embeddings that capture hierarchical semantic concepts that areseparable and interpretable. In this work, we aim to learn highly separablesemantic hierarchical representations by stacking Joint Embedding Architectures(JEA) where higher-level JEAs are input with representations of lower-levelJEA. This results in a representation space that exhibits distinctsub-categories of semantic concepts (e.g., model and colour of vehicles) inhigher-level JEAs. We empirically show that representations from stacked JEAperform on a similar level as traditional JEA with comparative parameter countsand visualise the representation spaces to validate the semantic hierarchies.</description><author>AlÅ¾bÄta ManovÃ¡, Aiden Durrant, Georgios Leontidis</author><pubDate>Fri, 19 May 2023 15:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11701v1</guid></item><item><title>RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design</title><link>http://arxiv.org/abs/2305.11699v1</link><description>Identifying molecules that exhibit some pre-specified properties is adifficult problem to solve. In the last few years, deep generative models havebeen used for molecule generation. Deep Graph Variational Autoencoders areamong the most powerful machine learning tools with which it is possible toaddress this problem. However, existing methods struggle in capturing the truedata distribution and tend to be computationally expensive. In this work, wepropose RGCVAE, an efficient and effective Graph Variational Autoencoder basedon: (i) an encoding network exploiting a new powerful Relational GraphIsomorphism Network; (ii) a novel probabilistic decoding component. Compared toseveral state-of-the-art VAE methods on two widely adopted datasets, RGCVAEshows state-of-the-art molecule generation performance while beingsignificantly faster to train.</description><author>Davide Rigoni, NicolÃ² Navarin, Alessandro Sperduti</author><pubDate>Fri, 19 May 2023 15:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11699v1</guid></item><item><title>Incorporating Unlabelled Data into Bayesian Neural Networks</title><link>http://arxiv.org/abs/2304.01762v2</link><description>Conventional Bayesian Neural Networks (BNNs) cannot leverage unlabelled datato improve their predictions. To overcome this limitation, we introduceSelf-Supervised Bayesian Neural Networks, which use unlabelled data to learnimproved prior predictive distributions by maximising an evidence lower boundduring an unsupervised pre-training step. With a novel methodology developed tobetter understand prior predictive distributions, we then show thatself-supervised prior predictives capture image semantics better thanconventional BNN priors. In our empirical evaluations, we see thatself-supervised BNNs offer the label efficiency of self-supervised methods andthe uncertainty estimates of Bayesian methods, particularly outperformingconventional BNNs in low-to-medium data regimes.</description><author>Mrinank Sharma, Tom Rainforth, Yee Whye Teh, Vincent Fortuin</author><pubDate>Fri, 19 May 2023 15:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01762v2</guid></item><item><title>QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations</title><link>http://arxiv.org/abs/2305.11694v1</link><description>Formulating selective information needs results in queries that implicitlyspecify set operations, such as intersection, union, and difference. Forinstance, one might search for "shorebirds that are not sandpipers" or"science-fiction films shot in England". To study the ability of retrievalsystems to meet such information needs, we construct QUEST, a dataset of 3357natural language queries with implicit set operations, that map to a set ofentities corresponding to Wikipedia documents. The dataset challenges models tomatch multiple constraints mentioned in queries with corresponding evidence indocuments and correctly perform various set operations. The dataset isconstructed semi-automatically using Wikipedia category names. Queries areautomatically composed from individual categories, then paraphrased and furthervalidated for naturalness and fluency by crowdworkers. Crowdworkers also assessthe relevance of entities based on their documents and highlight attribution ofquery constraints to spans of document text. We analyze several modernretrieval systems, finding that they often struggle on such queries. Queriesinvolving negation and conjunction are particularly challenging and systems arefurther challenged with combinations of these operations.</description><author>Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</author><pubDate>Fri, 19 May 2023 15:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11694v1</guid></item><item><title>ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages</title><link>http://arxiv.org/abs/2212.06742v2</link><description>Software engineers working with the same programming language (PL) may speakdifferent natural languages (NLs) and vice versa, erecting huge barriers tocommunication and working efficiency. Recent studies have demonstrated theeffectiveness of generative pre-training in computer programs, yet they arealways English-centric. In this work, we step towards bridging the gap betweenmultilingual NLs and multilingual PLs for large language models (LLMs). Werelease ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.We employ two methods for universal cross-lingual pre-training: span-corruptionlanguage modeling that learns patterns from monolingual NL or PL; andpivot-based translation language modeling that relies on parallel data of manyNLs and PLs. Extensive results show that ERNIE-Code outperforms previousmultilingual LLMs for PL or NL across a wide range of end tasks of codeintelligence, including multilingual code-to-text, text-to-code, code-to-code,and text-to-text generation. We further show its advantage of zero-shotprompting on multilingual code summarization and text-to-text translation. Werelease our code and pre-trained checkpoints.</description><author>Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu</author><pubDate>Fri, 19 May 2023 15:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06742v2</guid></item><item><title>Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery</title><link>http://arxiv.org/abs/2305.11692v1</link><description>Despite the availability of computer-aided simulators and recorded videos ofsurgical procedures, junior residents still heavily rely on experts to answertheir queries. However, expert surgeons are often overloaded with clinical andacademic workloads and limit their time in answering. For this purpose, wedevelop a surgical question-answering system to facilitate robot-assistedsurgical scene and activity understanding from recorded videos. Most of theexisting VQA methods require an object detector and regions based featureextractor to extract visual features and fuse them with the embedded text ofthe question for answer generation. However, (1) surgical object detectionmodel is scarce due to smaller datasets and lack of bounding box annotation;(2) current fusion strategy of heterogeneous modalities like text and image isnaive; (3) the localized answering is missing, which is crucial in complexsurgical scenarios. In this paper, we propose Visual QuestionLocalized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specificsurgical area during the answer prediction. To deal with the fusion of theheterogeneous modalities, we design gated vision-language embedding (GVLE) tobuild input patches for the Language Vision Transformer (LViT) to predict theanswer. To get localization, we add the detection head in parallel with theprediction head of the LViT. We also integrate GIoU loss to boost localizationperformance by preserving the accuracy of the question-answering model. Weannotate two datasets of VQLA by utilizing publicly available surgical videosfrom MICCAI challenges EndoVis-17 and 18. Our validation results suggest thatSurgical-VQLA can better understand the surgical scene and localize thespecific area related to the question-answering. GVLE presents an efficientlanguage-vision embedding technique by showing superior performance over theexisting benchmarks.</description><author>Long Bai, Mobarakol Islam, Lalithkumar Seenivasan, Hongliang Ren</author><pubDate>Fri, 19 May 2023 15:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11692v1</guid></item><item><title>Sim-to-Real Segmentation in Robot-assisted Transoral Tracheal Intubation</title><link>http://arxiv.org/abs/2305.11686v1</link><description>Robotic-assisted tracheal intubation requires the robot to distinguishanatomical features like an experienced physician using deep-learningtechniques. However, real datasets of oropharyngeal organs are limited due topatient privacy issues, making it challenging to train deep-learning models foraccurate image segmentation. We hereby consider generating a new data modalitythrough a virtual environment to assist the training process. Specifically,this work introduces a virtual dataset generated by the Simulation OpenFramework Architecture (SOFA) framework to overcome the limited availability ofactual endoscopic images. We also propose a domain adaptive Sim-to-Real methodfor oropharyngeal organ image segmentation, which employs an image blendingstrategy called IoU-Ranking Blend (IRB) and style-transfer techniques toaddress discrepancies between datasets. Experimental results demonstrate thesuperior performance of the proposed approach with domain adaptive models,improving segmentation accuracy and training stability. In the practicalapplication, the trained segmentation model holds great promise forrobot-assisted intubation surgery and intelligent surgical navigation.</description><author>Guankun Wang, Tian-Ao Ren, Jiewen Lai, Long Bai, Hongliang Ren</author><pubDate>Fri, 19 May 2023 15:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11686v1</guid></item><item><title>Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation</title><link>http://arxiv.org/abs/2305.11685v1</link><description>Transformer-based speech self-supervised learning (SSL) models, such asHuBERT, show surprising performance in various speech processing tasks.However, huge number of parameters in speech SSL models necessitate thecompression to a more compact model for wider usage in academia or smallcompanies. In this study, we suggest to reuse attention maps across theTransformer layers, so as to remove key and query parameters while retainingthe number of layers. Furthermore, we propose a novel masking distillationstrategy to improve the student model's speech representation quality. Weextend the distillation loss to utilize both masked and unmasked speech framesto fully leverage the teacher model's high-quality representation. Ouruniversal compression strategy yields the student model that achieves phonemeerror rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERBbenchmark.</description><author>Kangwook Jang, Sungnyun Kim, Se-Young Yun, Hoirin Kim</author><pubDate>Fri, 19 May 2023 15:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11685v1</guid></item><item><title>Self-Reinforcement Attention Mechanism For Tabular Learning</title><link>http://arxiv.org/abs/2305.11684v1</link><description>Apart from the high accuracy of machine learning models, what interests manyresearchers in real-life problems (e.g., fraud detection, credit scoring) is tofind hidden patterns in data; particularly when dealing with their challengingimbalanced characteristics. Interpretability is also a key requirement thatneeds to accompany the used machine learning model. In this concern, often,intrinsically interpretable models are preferred to complex ones, which are inmost cases black-box models. Also, linear models are used in some high-riskfields to handle tabular data, even if performance must be sacrificed. In thispaper, we introduce Self-Reinforcement Attention (SRA), a novel attentionmechanism that provides a relevance of features as a weight vector which isused to learn an intelligible representation. This weight is then used toreinforce or reduce some components of the raw input through element-wisevector multiplication. Our results on synthetic and real-world imbalanced datashow that our proposed SRA block is effective in end-to-end combination withbaseline models.</description><author>Kodjo Mawuena Amekoe, Mohamed Djallel Dilmi, Hanene Azzag, Mustapha Lebbah, Zaineb Chelly Dagdia, Gregoire Jaffre</author><pubDate>Fri, 19 May 2023 15:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11684v1</guid></item><item><title>Sensing of inspiration events from speech: comparison of deep learning and linguistic methods</title><link>http://arxiv.org/abs/2305.11683v1</link><description>Respiratory chest belt sensor can be used to measure the respiratory rate andother respiratory health parameters. Virtual Respiratory Belt, VRB, algorithmsestimate the belt sensor waveform from speech audio. In this paper we comparethe detection of inspiration events (IE) from respiratory belt sensor datausing a novel neural VRB algorithm and the detections based on time-alignedlinguistic content. The results show the superiority of the VRB method overword pause detection or grammatical content segmentation. The comparison of themethods show that both read and spontaneous speech content has a significantamount of ungrammatical breathing, that is, breathing events that are notaligned with grammatically appropriate places in language. This study gives newinsights into the development of VRB methods and adds to the generalunderstanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA,for the reconstruction of the continuous breathing waveform is demonstrated.</description><author>Aki HÃ¤rmÃ¤, Ulf GrossekathÃ¶fer, Okke Ouweltjes, Venkata Srikanth Nallanthighal</author><pubDate>Fri, 19 May 2023 15:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11683v1</guid></item><item><title>Towards the Practical Utility of Federated Learning in the Medical Domain</title><link>http://arxiv.org/abs/2207.03075v5</link><description>Federated learning (FL) is an active area of research. One of the mostsuitable areas for adopting FL is the medical domain, where patient privacymust be respected. Previous research, however, does not provide a practicalguide to applying FL in the medical domain. We propose empirical benchmarks andexperimental settings for three representative medical datasets with differentmodalities: longitudinal electronic health records, skin cancer images, andelectrocardiogram signals. The likely users of FL such as medical institutionsand IT companies can take these benchmarks as guides for adopting FL andminimize their trial and error. For each dataset, each client data is from adifferent source to preserve real-world heterogeneity. We evaluate six FLalgorithms designed for addressing data heterogeneity among clients, and ahybrid algorithm combining the strengths of two representative FL algorithms.Based on experiment results from three modalities, we discover that simple FLalgorithms tend to outperform more sophisticated ones, while the hybridalgorithm consistently shows good, if not the best performance. We also findthat a frequent global model update leads to better performance under a fixedtraining iteration budget. As the number of participating clients increases,higher cost is incurred due to increased IT administrators and GPUs, but theperformance consistently increases. We expect future users will refer to theseempirical benchmarks to design the FL experiments in the medical domainconsidering their clinical tasks and obtain stronger performance with lowercosts.</description><author>Seongjun Yang, Hyeonji Hwang, Daeyoung Kim, Radhika Dua, Jong-Yeup Kim, Eunho Yang, Edward Choi</author><pubDate>Fri, 19 May 2023 15:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.03075v5</guid></item><item><title>Probably Approximately Correct Federated Learning</title><link>http://arxiv.org/abs/2304.04641v4</link><description>Federated learning (FL) is a new distributed learning paradigm, with privacy,utility, and efficiency as its primary pillars. Existing research indicatesthat it is unlikely to simultaneously attain infinitesimal privacy leakage,utility loss, and efficiency. Therefore, how to find an optimal trade-offsolution is the key consideration when designing the FL algorithm. One commonway is to cast the trade-off problem as a multi-objective optimization problem,i.e., the goal is to minimize the utility loss and efficiency reduction whileconstraining the privacy leakage not exceeding a predefined value. However,existing multi-objective optimization frameworks are very time-consuming, anddo not guarantee the existence of the Pareto frontier, this motivates us toseek a solution to transform the multi-objective problem into asingle-objective problem because it is more efficient and easier to be solved.To this end, we propose FedPAC, a unified framework that leverages PAC learningto quantify multiple objectives in terms of sample complexity, suchquantification allows us to constrain the solution space of multiple objectivesto a shared dimension, so that it can be solved with the help of asingle-objective optimization algorithm. Specifically, we provide the resultsand detailed analyses of how to quantify the utility loss, privacy leakage,privacy-utility-efficiency trade-off, as well as the cost of the attacker fromthe PAC learning perspective.</description><author>Xiaojin Zhang, Anbu Huang, Lixin Fan, Kai Chen, Qiang Yang</author><pubDate>Fri, 19 May 2023 15:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04641v4</guid></item><item><title>Probabilistic Lexicase Selection</title><link>http://arxiv.org/abs/2305.11681v1</link><description>Lexicase selection is a widely used parent selection algorithm in geneticprogramming, known for its success in various task domains such as programsynthesis, symbolic regression, and machine learning. Due to its non-parametricand recursive nature, calculating the probability of each individual beingselected by lexicase selection has been proven to be an NP-hard problem, whichdiscourages deeper theoretical understanding and practical improvements to thealgorithm. In this work, we introduce probabilistic lexicase selection(plexicase selection), a novel parent selection algorithm that efficientlyapproximates the probability distribution of lexicase selection. Our method notonly demonstrates superior problem-solving capabilities as a semantic-awareselection method, but also benefits from having a probabilistic representationof the selection process for enhanced efficiency and flexibility. Experimentsare conducted in two prevalent domains in genetic programming: programsynthesis and symbolic regression, using standard benchmarks including PSB andSRBench. The empirical results show that plexicase selection achievesstate-of-the-art problem-solving performance that is competitive to thelexicase selection, and significantly outperforms lexicase selection incomputation efficiency.</description><author>Li Ding, Edward Pantridge, Lee Spector</author><pubDate>Fri, 19 May 2023 14:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11681v1</guid></item><item><title>Open-Set Likelihood Maximization for Few-Shot Learning</title><link>http://arxiv.org/abs/2301.08390v2</link><description>We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifyinginstances among a set of classes for which we only have a few labeled samples,while simultaneously detecting instances that do not belong to any known class.We explore the popular transductive setting, which leverages the unlabelledquery instances at inference. Motivated by the observation that existingtransductive methods perform poorly in open-set scenarios, we propose ageneralization of the maximum likelihood principle, in which latent scoresdown-weighing the influence of potential outliers are introduced alongside theusual parametric model. Our formulation embeds supervision constraints from thesupport set and additional penalties discouraging overconfident predictions onthe query set. We proceed with a block-coordinate descent, with the latentscores and parametric model co-optimized alternately, thereby benefiting fromeach other. We call our resulting formulation \textit{Open-Set LikelihoodOptimization} (OSLO). OSLO is interpretable and fully modular; it can beapplied on top of any pre-trained model seamlessly. Through extensiveexperiments, we show that our method surpasses existing inductive andtransductive methods on both aspects of open-set recognition, namely inlierclassification and outlier detection.</description><author>Malik Boudiaf, Etienne Bennequin, Myriam Tami, Antoine Toubhans, Pablo Piantanida, CÃ©line Hudelot, Ismail Ben Ayed</author><pubDate>Fri, 19 May 2023 14:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08390v2</guid></item><item><title>Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion</title><link>http://arxiv.org/abs/2305.04288v2</link><description>Federated learning (FL) enables participating parties to collaborativelybuild a global model with boosted utility without disclosing private datainformation. Appropriate protection mechanisms have to be adopted to fulfillthe requirements in preserving \textit{privacy} and maintaining high model\textit{utility}. The nature of the widely-adopted protection mechanismsincluding \textit{Randomization Mechanism} and \textit{Compression Mechanism}is to protect privacy via distorting model parameter. We measure the utilityvia the gap between the original model parameter and the distorted modelparameter. We want to identify under what general conditions privacy-preservingfederated learning can achieve near-optimal utility via data generation andparameter distortion. To provide an avenue for achieving near-optimal utility,we present an upper bound for utility loss, which is measured using two mainterms called variance-reduction and model parameter discrepancy separately. Ouranalysis inspires the design of appropriate protection parameters for theprotection mechanisms to achieve near-optimal utility and meet the privacyrequirements simultaneously. The main techniques for the protection mechanisminclude parameter distortion and data generation, which are generic and can beapplied extensively. Furthermore, we provide an upper bound for the trade-offbetween privacy and utility, which together with the lower bound illustrated inNFL form the conditions for achieving optimal trade-off.</description><author>Xiaojin Zhang, Kai Chen, Qiang Yang</author><pubDate>Fri, 19 May 2023 14:49:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04288v2</guid></item></channel></rss>