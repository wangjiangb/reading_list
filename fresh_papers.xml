<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 16 Oct 2024 01:00:26 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Capture Artifacts via Progressive Disentangling and Purifying Blended Identities for Deepfake Detection</title><link>http://arxiv.org/abs/2410.10244v2</link><description>The Deepfake technology has raised serious concerns regarding privacybreaches and trust issues. To tackle these challenges, Deepfake detectiontechnology has emerged. Current methods over-rely on the global feature space,which contains redundant information independent of the artifacts. As a result,existing Deepfake detection techniques suffer performance degradation whenencountering unknown datasets. To reduce information redundancy, the currentmethods use disentanglement techniques to roughly separate the fake faces intoartifacts and content information. However, these methods lack a soliddisentanglement foundation and cannot guarantee the reliability of theirdisentangling process. To address these issues, a Deepfake detection methodbased on progressive disentangling and purifying blended identities isinnovatively proposed in this paper. Based on the artifact generationmechanism, the coarse- and fine-grained strategies are combined to ensure thereliability of the disentanglement method. Our method aims to more accuratelycapture and separate artifact features in fake faces. Specifically, we firstperform the coarse-grained disentangling on fake faces to obtain a pair ofblended identities that require no additional annotation to distinguish betweensource face and target face. Then, the artifact features from each identity areseparated to achieve fine-grained disentanglement. To obtain pure identityinformation and artifacts, an Identity-Artifact Correlation Compression module(IACC) is designed based on the information bottleneck theory, effectivelyreducing the potential correlation between identity information and artifacts.Additionally, an Identity-Artifact Separation Contrast Loss is designed toenhance the independence of artifact features post-disentangling. Finally, theclassifier only focuses on pure artifact features to achieve a generalizedDeepfake detector.</description><author>Weijie Zhou, Xiaoqing Luo, Zhancheng Zhang, Jiachen He, Xiaojun Wu</author><pubDate>Tue, 15 Oct 2024 14:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10244v2</guid></item><item><title>Optimal Time Complexity Algorithms for Computing General Random Walk Graph Kernels on Sparse Graphs</title><link>http://arxiv.org/abs/2410.10368v2</link><description>We present the first linear time complexity randomized algorithms forunbiased approximation of the celebrated family of general random walk kernels(RWKs) for sparse graphs. This includes both labelled and unlabelled instances.The previous fastest methods for general RWKs were of cubic time complexity andnot applicable to labelled graphs. Our method samples dependent random walks tocompute novel graph embeddings in $\mathbb{R}^d$ whose dot product is equal tothe true RWK in expectation. It does so without instantiating the directproduct graph in memory, meaning we can scale to massive datasets that cannotbe stored on a single machine. We derive exponential concentration bounds toprove that our estimator is sharp, and show that the ability to approximategeneral RWKs (rather than just special cases) unlocks efficient implicit graphkernel learning. Our method is up to $\mathbf{27\times}$ faster than itscounterparts for efficient computation on large graphs and scales to graphs$\mathbf{128 \times}$ bigger than largest examples amenable to brute-forcecomputation.</description><author>Krzysztof Choromanski, Isaac Reid, Arijit Sehanobish, Avinava Dubey</author><pubDate>Tue, 15 Oct 2024 14:12:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10368v2</guid></item><item><title>Learning Quadruped Locomotion Using Differentiable Simulation</title><link>http://arxiv.org/abs/2403.14864v4</link><description>This work explores the potential of using differentiable simulation forlearning quadruped locomotion. Differentiable simulation promises fastconvergence and stable training by computing low-variance first-order gradientsusing robot dynamics. However, its usage for legged robots is still limited tosimulation. The main challenge lies in the complex optimization landscape ofrobotic tasks due to discontinuous dynamics. This work proposes a newdifferentiable simulation framework to overcome these challenges. Our approachcombines a high-fidelity, non-differentiable simulator for forward dynamicswith a simplified surrogate model for gradient backpropagation. This approachmaintains simulation accuracy by aligning the robot states from the surrogatemodel with those of the precise, non-differentiable simulator. Our frameworkenables learning quadruped walking in simulation in minutes withoutparallelization. When augmented with GPU parallelization, our approach allowsthe quadruped robot to master diverse locomotion skills on challenging terrainsin minutes. We demonstrate that differentiable simulation outperforms areinforcement learning algorithm (PPO) by achieving significantly better sampleefficiency while maintaining its effectiveness in handling large-scaleenvironments. Our method represents one of the first successful applications ofdifferentiable simulation to real-world quadruped locomotion, offering acompelling alternative to traditional RL methods.</description><author>Yunlong Song, Sangbae Kim, Davide Scaramuzza</author><pubDate>Tue, 15 Oct 2024 13:30:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14864v4</guid></item><item><title>Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key</title><link>http://arxiv.org/abs/2410.10210v2</link><description>As large language models rapidly evolve to support longer context, there is anotable disparity in their capability to generate output at greater lengths.Recent study suggests that the primary cause for this imbalance may arise fromthe lack of data with long-output during alignment training. In light of thisobservation, attempts are made to re-align foundation models with data thatfills the gap, which result in models capable of generating lengthy output wheninstructed. In this paper, we explore the impact of data-quality in tuning amodel for long output, and the possibility of doing so from the starting pointsof human-aligned (instruct or chat) models. With careful data curation, we showthat it possible to achieve similar performance improvement in our tunedmodels, with only a small fraction of training data instances and compute. Inaddition, we assess the generalizability of such approaches by applying ourtuning-recipes to several models. our findings suggest that, while capacitiesfor generating long output vary across different models out-of-the-box, ourapproach to tune them with high-quality data using lite compute, consistentlyyields notable improvement across all models we experimented on. We have madepublic our curated dataset for tuning long-writing capability, theimplementations of model tuning and evaluation, as well as the fine-tunedmodels, all of which can be openly-accessed.</description><author>Yingda Chen, Xingjun Wang, Jintao Huang, Yunlin Mao, Daoze Zhang, Yuze Zhao</author><pubDate>Tue, 15 Oct 2024 13:21:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10210v2</guid></item><item><title>Information propagation dynamics in Deep Graph Networks</title><link>http://arxiv.org/abs/2410.10464v2</link><description>Graphs are a highly expressive abstraction for modeling entities and theirrelations, such as molecular structures, social networks, and traffic networks.Deep Graph Networks (DGNs) have emerged as a family of deep learning modelsthat can effectively process and learn such structured information. However,learning effective information propagation patterns within DGNs remains acritical challenge that heavily influences the model capabilities, both in thestatic domain and in the temporal domain (where features and/or topologyevolve). Given this challenge, this thesis investigates the dynamics ofinformation propagation within DGNs for static and dynamic graphs, focusing ontheir design as dynamical systems. Throughout this work, we provide theoreticaland empirical evidence to demonstrate the effectiveness of our proposedarchitectures in propagating and preserving long-term dependencies betweennodes, and in learning complex spatio-temporal patterns from irregular andsparsely sampled dynamic graphs. In summary, this thesis provides acomprehensive exploration of the intersection between graphs, deep learning,and dynamical systems, offering insights and advancements for the field ofgraph representation learning and paving the way for more effective andversatile graph-based learning models.</description><author>Alessio Gravina</author><pubDate>Tue, 15 Oct 2024 10:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10464v2</guid></item><item><title>Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations</title><link>http://arxiv.org/abs/2410.10174v2</link><description>Variational Autoencoders (VAEs) are a powerful framework for learning compactlatent representations, while NeuralODEs excel in learning transient systemdynamics. This work combines the strengths of both to create fast surrogatemodels with adjustable complexity. By leveraging the VAE's dimensionalityreduction using a non-hierarchical prior, our method adaptively assignsstochastic noise, naturally complementing known NeuralODE training enhancementsand enabling probabilistic time series modeling. We show that standard LatentODEs struggle with dimensionality reduction in systems with time-varyinginputs. Our approach mitigates this by continuously propagating variationalparameters through time, establishing fixed information channels in latentspace. This results in a flexible and robust method that can learn differentsystem complexities, e.g. deep neural networks or linear matrices. Hereby, itenables efficient approximation of the Koopman operator without the need forpredefining its dimensionality. As our method balances dimensionality reductionand reconstruction accuracy, we call it Balanced Neural ODE (B-NODE). Wedemonstrate the effectiveness of this method on academic test cases and applyit to a real-world example of a thermal power plant.</description><author>Julius Aka, Johannes Brunnemann, Jörg Eiden, Arne Speerforck, Lars Mikelsons</author><pubDate>Tue, 15 Oct 2024 10:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10174v2</guid></item><item><title>Logit Separability-Driven Samples and Multiple Class-Related Words Selection for Advancing In-Context Learning</title><link>http://arxiv.org/abs/2406.10908v5</link><description>Effective organization of in-context learning (ICL) demonstrations is key toimproving the quality of large language model (LLM) responses. To create bettersample-label pairs that instruct LLM understanding, we introduce logitseparability, a criterion to assess the clarity of both samples andclass-related words at the logit level. This facilitates the optimization ofsample and label selection, enhancing the precision of information provided inICL demonstrations. Additionally, we find that incorporating multipleclass-related words for each sample, rather than relying on a single classname, improves performance by offering a broader range of label information.Building on these insights, we propose LICL, a logit separability-based methodthat jointly organizes samples and integrates multiple class-related words intoeach sample-label pair. Evaluations across seven classification datasets showthat this approach significantly improves ICL performance by providing clearerinstructions and richer label information.</description><author>Zhu Zixiao, Feng Zijian, Zhou Hanzhang, Qian Junlang, Mao Kezhi</author><pubDate>Tue, 15 Oct 2024 10:09:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10908v5</guid></item><item><title>4-LEGS: 4D Language Embedded Gaussian Splatting</title><link>http://arxiv.org/abs/2410.10719v2</link><description>The emergence of neural representations has revolutionized our means fordigitally viewing a wide range of 3D scenes, enabling the synthesis ofphotorealistic images rendered from novel views. Recently, several techniqueshave been proposed for connecting these low-level representations with thehigh-level semantics understanding embodied within the scene. These methodselevate the rich semantic understanding from 2D imagery to 3D representations,distilling high-dimensional spatial features onto 3D space. In our work, we areinterested in connecting language with a dynamic modeling of the world. We showhow to lift spatio-temporal features to a 4D representation based on 3DGaussian Splatting. This enables an interactive interface where the user canspatiotemporally localize events in the video from text prompts. We demonstrateour system on public 3D video datasets of people and animals performing variousactions.</description><author>Gal Fiebelman, Tamir Cohen, Ayellet Morgenstern, Peter Hedman, Hadar Averbuch-Elor</author><pubDate>Tue, 15 Oct 2024 09:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10719v2</guid></item><item><title>GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs</title><link>http://arxiv.org/abs/2410.10329v2</link><description>Recently, research on Text-Attributed Graphs (TAGs) has gained significantattention due to the prevalence of free-text node features in real-worldapplications and the advancements in Large Language Models (LLMs) that bolsterTAG methodologies. However, current TAG approaches face two primary challenges:(i) Heavy reliance on label information and (ii) Limited cross-domainzero/few-shot transferability. These issues constrain the scaling of both dataand model size, owing to high labor costs and scaling laws, complicating thedevelopment of graph foundation models with strong transferability. In thiswork, we propose the GraphCLIP framework to address these challenges bylearning graph foundation models with strong cross-domain zero/few-shottransferability through a self-supervised contrastive graph-summary pretrainingmethod. Specifically, we generate and curate large-scale graph-summary pairdata with the assistance of LLMs, and introduce a novel graph-summarypretraining method, combined with invariant learning, to enhance graphfoundation models with strong cross-domain zero-shot transferability. Forfew-shot learning, we propose a novel graph prompt tuning technique alignedwith our pretraining objective to mitigate catastrophic forgetting and minimizelearning costs. Extensive experiments show the superiority of GraphCLIP in bothzero-shot and few-shot settings, while evaluations across various downstreamtasks confirm the versatility of GraphCLIP. Our code is available at:https://github.com/ZhuYun97/GraphCLIP</description><author>Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang</author><pubDate>Tue, 15 Oct 2024 08:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10329v2</guid></item><item><title>EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task Planning of Foundation Model-based Embodied AI Agents</title><link>http://arxiv.org/abs/2408.04449v3</link><description>Embodied artificial intelligence (EAI) integrates advanced AI models intophysical entities for real-world interaction. The emergence of foundationmodels as the "brain" of EAI agents for high-level task planning has shownpromising results. However, the deployment of these agents in physicalenvironments presents significant safety challenges. For instance, ahousekeeping robot lacking sufficient risk awareness might place a metalcontainer in a microwave, potentially causing a fire. To address these criticalsafety concerns, comprehensive pre-deployment risk assessments are imperative.This study introduces EAIRiskBench, a novel framework for automated physicalrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agentcooperative system that leverages various foundation models to generate safetyguidelines, create risk-prone scenarios, make task planning, and evaluatesafety systematically. Utilizing this framework, we construct EAIRiskDataset,comprising diverse test cases across various domains, encompassing both textualand visual scenarios. Our comprehensive evaluation of state-of-the-artfoundation models reveals alarming results: all models exhibit high task riskrates (TRR), with an average of 95.75% across all evaluated models. To addressthese challenges, we further propose two prompting-based risk mitigationstrategies. While these strategies demonstrate some efficacy in reducing TRR,the improvements are limited, still indicating substantial safety concerns.This study provides the first large-scale assessment of physical risk awarenessin EAI agents. Our findings underscore the critical need for enhanced safetymeasures in EAI systems and provide valuable insights for future researchdirections in developing safer embodied artificial intelligence system.</description><author>Zihao Zhu, Bingzhe Wu, Zhengyou Zhang, Lei Han, Baoyuan Wu</author><pubDate>Tue, 15 Oct 2024 07:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04449v3</guid></item><item><title>On Representation of 3D Rotation in the Context of Deep Learning</title><link>http://arxiv.org/abs/2410.10350v2</link><description>This paper investigates various methods of representing 3D rotations andtheir impact on the learning process of deep neural networks. We evaluated theperformance of ResNet18 networks for 3D rotation estimation using severalrotation representations and loss functions on both synthetic and real data.The real datasets contained 3D scans of industrial bins, while the syntheticdatasets included views of a simple asymmetric object rendered under differentrotations. On synthetic data, we also assessed the effects of differentrotation distributions within the training and test sets, as well as the impactof the object's texture. In line with previous research, we found that networksusing the continuous 5D and 6D representations performed better than thediscontinuous ones.</description><author>Viktória Pravdová, Lukáš Gajdošech, Hassan Ali, Viktor Kocur</author><pubDate>Tue, 15 Oct 2024 07:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10350v2</guid></item><item><title>Advancing Supervised Local Learning Beyond Classification with Long-term Feature Bank</title><link>http://arxiv.org/abs/2406.00446v3</link><description>Local learning offers an alternative to traditional end-to-endback-propagation in deep neural networks, significantly reducing GPU memoryusage. While local learning has shown promise in image classification tasks,its application to other visual tasks remains limited. This limitation arisesprimarily from two factors: 1) architectures tailored for classification areoften not transferable to other tasks, leading to a lack of reusability oftask-specific knowledge; 2) the absence of cross-scale feature communicationresults in degraded performance in tasks such as object detection andsuper-resolution. To address these challenges, we propose the Memory-augmentedAuxiliary Network (MAN), which introduces a simplified design principle andincorporates a feature bank to enhance cross-task adaptability andcommunication. This work represents the first successful application of locallearning methods beyond classification, demonstrating that MAN not onlyconserves GPU memory but also achieves performance on par with end-to-endapproaches across multiple datasets for various visual tasks.</description><author>Feiyu Zhu, Yuming Zhang, Changpeng Cai, Chenghao He, Xiuyuan Guo, Jiao Li, Peizhe Wang, Junhao Su, Jialin Gao</author><pubDate>Tue, 15 Oct 2024 07:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00446v3</guid></item><item><title>Improved Depth Estimation of Bayesian Neural Networks</title><link>http://arxiv.org/abs/2410.10395v2</link><description>This paper proposes improvements over earlier work by Nazareth and Blei(2022) for estimating the depth of Bayesian neural networks. Here, we propose adiscrete truncated normal distribution over the network depth to independentlylearn its mean and variance. Posterior distributions are inferred by minimizingthe variational free energy, which balances the model complexity and accuracy.Our method improves test accuracy on the spiral data set and reduces thevariance in posterior depth estimates.</description><author>Bart van Erp, Bert de Vries</author><pubDate>Tue, 15 Oct 2024 07:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10395v2</guid></item><item><title>LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content</title><link>http://arxiv.org/abs/2410.10783v2</link><description>The large-scale training of multi-modal models on data scraped from the webhas shown outstanding utility in infusing these models with the required worldknowledge to perform effectively on multiple downstream tasks. However, onedownside of scraping data from the web can be the potential sacrifice of thebenchmarks on which the abilities of these models are often evaluated. Tosafeguard against test data contamination and to truly test the abilities ofthese foundation models we propose LiveXiv: A scalable evolving live benchmarkbased on scientific ArXiv papers. LiveXiv accesses domain-specific manuscriptsat any given timestamp and proposes to automatically generate visualquestion-answer pairs (VQA). This is done without any human-in-the-loop, usingthe multi-modal content in the manuscripts, like graphs, charts, and tables.Moreover, we introduce an efficient evaluation approach that estimates theperformance of all models on the evolving benchmark using evaluations of only asubset of models. This significantly reduces the overall evaluation cost. Webenchmark multiple open and proprietary Large Multi-modal Models (LMMs) on thefirst version of our benchmark, showing its challenging nature and exposing themodels true abilities, avoiding contamination. Lastly, in our commitment tohigh quality, we have collected and evaluated a manually verified subset. Bycomparing its overall results to our automatic annotations, we have found thatthe performance variance is indeed minimal (&lt;2.5%). Our dataset is availableonline on HuggingFace, and our code will be available here.</description><author>Nimrod Shabtay, Felipe Maia Polo, Sivan Doveh, Wei Lin, M. Jehanzeb Mirza, Leshem Chosen, Mikhail Yurochkin, Yuekai Sun, Assaf Arbelle, Leonid Karlinsky, Raja Giryes</author><pubDate>Tue, 15 Oct 2024 06:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10783v2</guid></item><item><title>MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling</title><link>http://arxiv.org/abs/2410.10798v2</link><description>Recent advancements in multi-modal large language models have propelled thedevelopment of joint probabilistic models capable of both image understandingand generation. However, we have identified that recent methods inevitablysuffer from loss of image information during understanding task, due to eitherimage discretization or diffusion denoising steps. To address this issue, wepropose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modelingframework. Unlike discretization line of method, MMAR takes incontinuous-valued image tokens to avoid information loss. Differing fromdiffusion-based approaches, we disentangle the diffusion process fromauto-regressive backbone model by employing a light-weight diffusion head ontop each auto-regressed image patch embedding. In this way, when the modeltransits from image generation to understanding through text generation, thebackbone model's hidden representation of the image is not limited to the lastdenoising step. To successfully train our method, we also propose atheoretically proven technique that addresses the numerical stability issue anda training strategy that balances the generation and understanding task goals.Through extensive evaluations on 18 image understanding benchmarks, MMARdemonstrates much more superior performance than other joint multi-modalmodels, matching the method that employs pretrained CLIP vision encoder,meanwhile being able to generate high quality images at the same time. We alsoshowed that our method is scalable with larger data and model size.</description><author>Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, Zheng-Jun Zha</author><pubDate>Tue, 15 Oct 2024 06:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10798v2</guid></item><item><title>Parameterize Structure with Differentiable Template for 3D Shape Generation</title><link>http://arxiv.org/abs/2410.10399v2</link><description>Structural representation is crucial for reconstructing and generatingeditable 3D shapes with part semantics. Recent 3D shape generation works employcomplicated networks and structure definitions relying on hierarchicalannotations and pay less attention to the details inside parts. In this paper,we propose the method that parameterizes the shared structure in the samecategory using a differentiable template and corresponding fixed-lengthparameters. Specific parameters are fed into the template to calculate cuboidsthat indicate a concrete shape. We utilize the boundaries of three-viewdrawings of each cuboid to further describe the inside details. Shapes arerepresented with the parameters and three-view details inside cuboids, fromwhich the SDF can be calculated to recover the object. Benefiting from ourfixed-length parameters and three-view details, our networks for reconstructionand generation are simple and effective to learn the latent space. Our methodcan reconstruct or generate diverse shapes with complicated details, andinterpolate them smoothly. Extensive evaluations demonstrate the superiority ofour method on reconstruction from point cloud, generation, and interpolation.</description><author>Changfeng Ma, Pengxiao Guo, Shuangyu Yang, Yinuo Chen, Jie Guo, Chongjun Wang, Yanwen Guo, Wenping Wang</author><pubDate>Tue, 15 Oct 2024 06:42:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10399v2</guid></item><item><title>SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers</title><link>http://arxiv.org/abs/2410.10629v2</link><description>We introduce Sana, a text-to-image framework that can efficiently generateimages up to 4096$\times$4096 resolution. Sana can synthesize high-resolution,high-quality images with strong text-image alignment at a remarkably fastspeed, deployable on laptop GPU. Core designs include: (1) Deep compressionautoencoder: unlike traditional AEs, which compress images only 8$\times$, wetrained an AE that can compress images 32$\times$, effectively reducing thenumber of latent tokens. (2) Linear DiT: we replace all vanilla attention inDiT with linear attention, which is more efficient at high resolutions withoutsacrificing quality. (3) Decoder-only text encoder: we replaced T5 with moderndecoder-only small LLM as the text encoder and designed complex humaninstruction with in-context learning to enhance the image-text alignment. (4)Efficient training and sampling: we propose Flow-DPM-Solver to reduce samplingsteps, with efficient caption labeling and selection to accelerate convergence.As a result, Sana-0.6B is very competitive with modern giant diffusion model(e.g. Flux-12B), being 20 times smaller and 100+ times faster in measuredthroughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, takingless than 1 second to generate a 1024$\times$1024 resolution image. Sanaenables content creation at low cost. Code and model will be publicly released.</description><author>Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, Song Han</author><pubDate>Tue, 15 Oct 2024 06:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10629v2</guid></item><item><title>Ensemble of ConvNeXt V2 and MaxViT for Long-Tailed CXR Classification with View-Based Aggregation</title><link>http://arxiv.org/abs/2410.10710v2</link><description>In this work, we present our solution for the MICCAI 2024 CXR-LT challenge,achieving 4th place in Subtask 2 and 5th in Subtask 1. We leveraged an ensembleof ConvNeXt V2 and MaxViT models, pretrained on an external chest X-raydataset, to address the long-tailed distribution of chest findings. Theproposed method combines state-of-the-art image classification techniques,asymmetric loss for handling class imbalance, and view-based predictionaggregation to enhance classification performance. Through experiments, wedemonstrate the advantages of our approach in improving both detection accuracyand the handling of the long-tailed distribution in CXR findings. The code isavailable at https://github.com/yamagishi0824/cxrlt24-multiview-pp.</description><author>Yosuke Yamagishi, Shouhei Hanaoka</author><pubDate>Tue, 15 Oct 2024 06:11:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10710v2</guid></item><item><title>Edge Unlearning is Not "on Edge"! An Adaptive Exact Unlearning System on Resource-Constrained Devices</title><link>http://arxiv.org/abs/2410.10128v2</link><description>The right to be forgotten mandates that machine learning models enable theerasure of a data owner's data and information from a trained model. Removingdata from the dataset alone is inadequate, as machine learning models canmemorize information from the training data, increasing the potential privacyrisk to users. To address this, multiple machine unlearning techniques havebeen developed and deployed. Among them, approximate unlearning is a popularsolution, but recent studies report that its unlearning effectiveness is notfully guaranteed. Another approach, exact unlearning, tackles this issue bydiscarding the data and retraining the model from scratch, but at the cost ofconsiderable computational and memory resources. However, not all devices havethe capability to perform such retraining. In numerous machine learningapplications, such as edge devices, Internet-of-Things (IoT), mobile devices,and satellites, resources are constrained, posing challenges for deployingexisting exact unlearning methods. In this study, we propose a Constraint-awareAdaptive Exact Unlearning System at the network Edge (CAUSE), an approach toenabling exact unlearning on resource-constrained devices. Aiming to minimizethe retrain overhead by storing sub-models on the resource-constrained device,CAUSE innovatively applies a Fibonacci-based replacement strategy and updatesthe number of shards adaptively in the user-based data partition process. Tofurther improve the effectiveness of memory usage, CAUSE leverages theadvantage of model pruning to save memory via compression with minimal accuracysacrifice. The experimental results demonstrate that CAUSE significantlyoutperforms other representative systems in realizing exact unlearning on theresource-constrained device by 9.23%-80.86%, 66.21%-83.46%, and 5.26%-194.13%in terms of unlearning speed, energy consumption, and accuracy.</description><author>Xiaoyu Xia, Ziqi Wang, Ruoxi Sun, Bowen Liu, Ibrahim Khalil, Minhui Xue</author><pubDate>Tue, 15 Oct 2024 04:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10128v2</guid></item><item><title>Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing</title><link>http://arxiv.org/abs/2405.04496v3</link><description>Existing diffusion-based methods have achieved impressive results in humanmotion editing. However, these methods often exhibit significant ghosting andbody distortion in unseen in-the-wild cases. In this paper, we introduceEdit-Your-Motion, a video motion editing method that tackles these challengesthrough one-shot fine-tuning on unseen cases. Specifically, firstly, weutilized DDIM inversion to initialize the noise, preserving the appearance ofthe source video and designed a lightweight motion attention adapter module toenhance motion fidelity. DDIM inversion aims to obtain the implicitrepresentations by estimating the prediction noise from the source video, whichserves as a starting point for the sampling process, ensuring the appearanceconsistency between the source and edited videos. The Motion Attention Module(MA) enhances the model's motion editing ability by resolving the conflictbetween the skeleton features and the appearance features. Secondly, toeffectively decouple motion and appearance of source video, we design aspatio-temporal two-stage learning strategy (STL). In the first stage, we focuson learning temporal features of human motion and propose recurrent causalattention (RCA) to ensure consistency between video frames. In the secondstage, we shift focus on learning the appearance features of the source video.With Edit-Your-Motion, users can edit the motion of humans in the source video,creating more engaging and diverse content. Extensive qualitative andquantitative experiments, along with user preference studies, show thatEdit-Your-Motion outperforms other methods.</description><author>Yi Zuo, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Shuyuan Yang, Yuwei Guo</author><pubDate>Tue, 15 Oct 2024 03:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04496v3</guid></item><item><title>Mobility-Aware Federated Learning: Multi-Armed Bandit Based Selection in Vehicular Network</title><link>http://arxiv.org/abs/2410.10451v2</link><description>In this paper, we study a vehicle selection problem for federated learning(FL) over vehicular networks. Specifically, we design a mobility-awarevehicular federated learning (MAVFL) scheme in which vehicles drive through aroad segment to perform FL. Some vehicles may drive out of the segment whichleads to unsuccessful training. In the proposed scheme, the real-timesuccessful training participation ratio is utilized to implement vehicleselection. We conduct the convergence analysis to indicate the influence ofvehicle mobility on training loss. Furthermore, we propose a multi-armedbandit-based vehicle selection algorithm to minimize the utility functionconsidering training loss and delay. The simulation results show that comparedwith baselines, the proposed algorithm can achieve better training performancewith approximately 28\% faster convergence.</description><author>Haoyu Tu, Lin Chen, Zuguang Li, Xiaopei Chen, Wen Wu</author><pubDate>Tue, 15 Oct 2024 03:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10451v2</guid></item><item><title>Pubic Symphysis-Fetal Head Segmentation Network Using BiFormer Attention Mechanism and Multipath Dilated Convolution</title><link>http://arxiv.org/abs/2410.10352v2</link><description>Pubic symphysis-fetal head segmentation in transperineal ultrasound imagesplays a critical role for the assessment of fetal head descent and progression.Existing transformer segmentation methods based on sparse attention mechanismuse handcrafted static patterns, which leads to great differences in terms ofsegmentation performance on specific datasets. To address this issue, weintroduce a dynamic, query-aware sparse attention mechanism for ultrasoundimage segmentation. Specifically, we propose a novel method, named BRAU-Net tosolve the pubic symphysis-fetal head segmentation task in this paper. Themethod adopts a U-Net-like encoder-decoder architecture with bi-level routingattention and skip connections, which effectively learns local-global semanticinformation. In addition, we propose an inverted bottleneck patch expanding(IBPE) module to reduce information loss while performing up-samplingoperations. The proposed BRAU-Net is evaluated on FH-PS-AoP and HC18 datasets.The results demonstrate that our method could achieve excellent segmentationresults. The code is available on GitHub.</description><author>Pengzhou Cai, Lu Jiang, Yanxin Li, Xiaojuan Liu, Libin Lan</author><pubDate>Tue, 15 Oct 2024 02:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10352v2</guid></item><item><title>Ada-K Routing: Boosting the Efficiency of MoE-based LLMs</title><link>http://arxiv.org/abs/2410.10456v2</link><description>In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE)architectures offer a promising approach to managing computational costs whilescaling up model parameters. Conventional MoE-based LLMs typically employstatic Top-K routing, which activates a fixed and equal number of experts foreach token regardless of their significance within the context. In this paper,we propose a novel Ada-K routing strategy that dynamically adjusts the numberof activated experts for each token, thereby improving the balance betweencomputational efficiency and model performance. Specifically, our strategyincorporates learnable and lightweight allocator modules that decide customizedexpert resource allocation tailored to the contextual needs for each token.These allocators are designed to be fully pluggable, making it broadlyapplicable across all mainstream MoE-based LLMs. We leverage the ProximalPolicy Optimization (PPO) algorithm to facilitate an end-to-end learningprocess for this non-differentiable decision-making framework. Extensiveevaluations on four popular baseline models demonstrate that our Ada-K routingmethod significantly outperforms conventional Top-K routing. Compared to Top-K,our method achieves over 25% reduction in FLOPs and more than 20% inferencespeedup while still improving performance across various benchmarks. Moreover,the training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-basedLLM with more than 140B parameters, the training time is limited to 8 hours.Detailed analysis shows that harder tasks, middle layers, and content wordstend to activate more experts, providing valuable insights for future adaptiveMoE system designs. Both the training code and model checkpoints will bepublicly available.</description><author>Tongtian Yue, Longteng Guo, Jie Cheng, Xuange Gao, Jing Liu</author><pubDate>Tue, 15 Oct 2024 02:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10456v2</guid></item><item><title>EasyRAG: Efficient Retrieval-Augmented Generation Framework for Automated Network Operations</title><link>http://arxiv.org/abs/2410.10315v2</link><description>This paper presents EasyRAG, a simple, lightweight, and efficientretrieval-augmented generation framework for automated network operations. Ourframework has three advantages. The first is accurate question answering. Wedesigned a straightforward RAG scheme based on (1) a specific data processingworkflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Rerankerfor reranking (4) LLM answer generation and optimization. This approachachieved first place in the GLM4 track in the preliminary round and secondplace in the GLM4 track in the semifinals. The second is simple deployment. Ourmethod primarily consists of BM25 retrieval and BGE-reranker reranking,requiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy,and highly scalable; we provide a flexible code library with various search andgeneration strategies, facilitating custom process implementation. The last oneis efficient inference. We designed an efficient inference acceleration schemefor the entire coarse ranking, reranking, and generation process thatsignificantly reduces the inference latency of RAG while maintaining a goodlevel of accuracy; each acceleration scheme can be plug-and-play into anycomponent of the RAG process, consistently enhancing the efficiency of the RAGsystem. Our code and data are released at\url{https://github.com/BUAADreamer/EasyRAG}.</description><author>Zhangchi Feng, Dongdong Kuang, Zhongyuan Wang, Zhijie Nie, Yaowei Zheng, Richong Zhang</author><pubDate>Tue, 15 Oct 2024 02:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10315v2</guid></item><item><title>Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models</title><link>http://arxiv.org/abs/2410.10821v1</link><description>3D meshes are widely used in computer vision and graphics for theirefficiency in animation and minimal memory use, playing a crucial role inmovies, games, AR, and VR. However, creating temporally consistent andrealistic textures for mesh sequences remains labor-intensive for professionalartists. On the other hand, while video diffusion models excel at text-drivenvideo generation, they often lack 3D geometry awareness and struggle withachieving multi-view consistent texturing for 3D meshes. In this work, wepresent Tex4D, a zero-shot approach that integrates inherent 3D geometryknowledge from mesh sequences with the expressiveness of video diffusion modelsto produce multi-view and temporally consistent 4D textures. Given anuntextured mesh sequence and a text prompt as inputs, our method enhancesmulti-view consistency by synchronizing the diffusion process across differentviews through latent aggregation in the UV space. To ensure temporalconsistency, we leverage prior knowledge from a conditional video generationmodel for texture synthesis. However, straightforwardly combining the videodiffusion model and the UV texture aggregation leads to blurry results. Weanalyze the underlying causes and propose a simple yet effective modificationto the DDIM sampling process to address this issue. Additionally, we introducea reference latent texture to strengthen the correlation between frames duringthe denoising process. To the best of our knowledge, Tex4D is the first methodspecifically designed for 4D scene texturing. Extensive experiments demonstrateits superiority in producing multi-view and multi-frame consistent videos basedon untextured mesh sequences.</description><author>Jingzhi Bao, Xueting Li, Ming-Hsuan Yang</author><pubDate>Mon, 14 Oct 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10821v1</guid></item><item><title>TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models</title><link>http://arxiv.org/abs/2410.10818v1</link><description>Understanding fine-grained temporal dynamics is crucial for multimodal videocomprehension and generation. Due to the lack of fine-grained temporalannotations, existing video benchmarks mostly resemble static image benchmarksand are incompetent at evaluating models for temporal understanding. In thispaper, we introduce TemporalBench, a new benchmark dedicated to evaluatingfine-grained temporal understanding in videos. TemporalBench consists of ~10Kvideo question-answer pairs, derived from ~2K high-quality human annotationsdetailing the temporal dynamics in video clips. As a result, our benchmarkprovides a unique testbed for evaluating various temporal understanding andreasoning abilities such as action frequency, motion magnitude, event order,etc. Moreover, it enables evaluations on various tasks like both video questionanswering and captioning, both short and long video understanding, as well asdifferent models such as multimodal video embedding models and text generationmodels. Results show that state-of-the-art models like GPT-4o achieve only38.5% question answering accuracy on TemporalBench, demonstrating a significantgap (~30%) between humans and AI in temporal understanding. Furthermore, wenotice a critical pitfall for multi-choice QA where LLMs can detect the subtlechanges in negative captions and find a centralized description as a cue forits prediction, where we propose Multiple Binary Accuracy (MBA) to correct suchbias. We hope that TemporalBench can foster research on improving models'temporal reasoning capabilities. Both dataset and evaluation code will be madeavailable.</description><author>Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, Jianwei Yang</author><pubDate>Mon, 14 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10818v1</guid></item><item><title>When Does Perceptual Alignment Benefit Vision Representations?</title><link>http://arxiv.org/abs/2410.10817v1</link><description>Humans judge perceptual similarity according to diverse visual attributes,including scene layout, subject location, and camera pose. Existing visionmodels understand a wide range of semantic abstractions but improperly weighthese attributes and thus make inferences misaligned with human perception.While vision representations have previously benefited from alignment incontexts like image generation, the utility of perceptually alignedrepresentations in more general-purpose settings remains unclear. Here, weinvestigate how aligning vision model representations to human perceptualjudgments impacts their usability across diverse computer vision tasks. Wefinetune state-of-the-art models on human similarity judgments for imagetriplets and evaluate them across standard vision benchmarks. We find thataligning models to perceptual judgments yields representations that improveupon the original backbones across many downstream tasks, including counting,segmentation, depth estimation, instance retrieval, and retrieval-augmentedgeneration. In addition, we find that performance is widely preserved on othertasks, including specialized out-of-distribution domains such as in medicalimaging and 3D environment frames. Our results suggest that injecting aninductive bias about human perceptual knowledge into vision models cancontribute to better representations.</description><author>Shobhita Sundaram, Stephanie Fu, Lukas Muttenthaler, Netanel Y. Tamir, Lucy Chai, Simon Kornblith, Trevor Darrell, Phillip Isola</author><pubDate>Mon, 14 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10817v1</guid></item><item><title>DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</title><link>http://arxiv.org/abs/2410.10819v1</link><description>Deploying long-context large language models (LLMs) is essential but posessignificant computational and memory challenges. Caching all Key and Value (KV)states across all attention heads consumes substantial memory. Existing KVcache pruning methods either damage the long-context capabilities of LLMs oroffer only limited efficiency improvements. In this paper, we identify thatonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical forprocessing long contexts and require full attention across all tokens. Incontrast, all other heads, which primarily focus on recent tokens and attentionsinks--referred to as Streaming Heads--do not require full attention. Based onthis insight, we introduce DuoAttention, a framework that only applies a fullKV cache to retrieval heads while using a light-weight, constant-length KVcache for streaming heads, which reduces both LLM's decoding and pre-fillingmemory and latency without compromising its long-context abilities.DuoAttention uses a lightweight, optimization-based algorithm with syntheticdata to identify retrieval heads accurately. Our method significantly reduceslong-context inference memory by up to 2.55x for MHA and 1.67x for GQA modelswhile speeding up decoding by up to 2.18x and 1.50x and acceleratingpre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, withminimal accuracy loss compared to full attention. Notably, combined withquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million contextlength on a single A100 GPU. Code is provided inhttps://github.com/mit-han-lab/duo-attention.</description><author>Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han</author><pubDate>Mon, 14 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10819v1</guid></item><item><title>LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</title><link>http://arxiv.org/abs/2410.10816v1</link><description>The efficacy of video generation models heavily depends on the quality oftheir training datasets. Most previous video generation models are trained onshort video clips, while recently there has been increasing interest intraining long video generation models directly on longer videos. However, thelack of such high-quality long videos impedes the advancement of long videogeneration. To promote research in long video generation, we desire a newdataset with four key features essential for training long video generationmodels: (1) long videos covering at least 10 seconds, (2) long-take videoswithout cuts, (3) large motion and diverse contents, and (4) temporally densecaptions. To achieve this, we introduce a new pipeline for selectinghigh-quality long-take videos and generating temporally dense captions.Specifically, we define a set of metrics to quantitatively assess video qualityincluding scene cuts, dynamic degrees, and semantic-level quality, enabling usto filter high-quality long-take videos from a large amount of source videos.Subsequently, we develop a hierarchical video captioning pipeline to annotatelong videos with temporally-dense captions. With this pipeline, we curate thefirst long-take video dataset, LVD-2M, comprising 2 million long-take videos,each covering more than 10 seconds and annotated with temporally densecaptions. We further validate the effectiveness of LVD-2M by fine-tuning videogeneration models to generate long videos with dynamic motions. We believe ourwork will significantly contribute to future research in long video generation.</description><author>Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, Xihui Liu</author><pubDate>Mon, 14 Oct 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10816v1</guid></item><item><title>Depth Any Video with Scalable Synthetic Data</title><link>http://arxiv.org/abs/2410.10815v1</link><description>Video depth estimation has long been hindered by the scarcity of consistentand scalable ground truth data, leading to inconsistent and unreliable results.In this paper, we introduce Depth Any Video, a model that tackles the challengethrough two key innovations. First, we develop a scalable synthetic datapipeline, capturing real-time video depth data from diverse syntheticenvironments, yielding 40,000 video clips of 5-second duration, each withprecise depth annotations. Second, we leverage the powerful priors ofgenerative video diffusion models to handle real-world videos effectively,integrating advanced techniques such as rotary position encoding and flowmatching to further enhance flexibility and efficiency. Unlike previous models,which are limited to fixed-length video sequences, our approach introduces anovel mixed-duration training strategy that handles videos of varying lengthsand performs robustly across different frame rates-even on single frames. Atinference, we propose a depth interpolation method that enables our model toinfer high-resolution video depth across sequences of up to 150 frames. Ourmodel outperforms all previous generative depth models in terms of spatialaccuracy and temporal consistency.</description><author>Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, Tong He</author><pubDate>Mon, 14 Oct 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10815v1</guid></item><item><title>Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free</title><link>http://arxiv.org/abs/2410.10814v1</link><description>While large language models (LLMs) excel on generation tasks, theirdecoder-only architecture often limits their potential as embedding models ifno further representation finetuning is applied. Does this contradict theirclaim of generalists? To answer the question, we take a closer look atMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoELLMs can serve as an off-the-shelf embedding model with promising performanceon a diverse class of embedding-focused tasks, without requiring anyfinetuning. Moreover, our extensive analysis shows that the MoE routing weights(RW) is complementary to the hidden state (HS) of LLMs, a widely-usedembedding. Compared to HS, we find that RW is more robust to the choice ofprompts and focuses on high-level semantics. Motivated by the analysis, wepropose MoEE combining RW and HS, which achieves better performance than usingeither separately. Our exploration of their combination and prompting strategyshed several novel insights, e.g., a weighted sum of RW and HS similaritiesoutperforms the similarity on their concatenation. Our experiments areconducted on 6 embedding tasks with 20 datasets from the Massive Text EmbeddingBenchmark (MTEB). The results demonstrate the significant improvement broughtby MoEE to LLM-based embedding without further finetuning.</description><author>Ziyue Li, Tianyi Zhou</author><pubDate>Mon, 14 Oct 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10814v1</guid></item><item><title>LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory</title><link>http://arxiv.org/abs/2410.10813v1</link><description>Recent large language model (LLM)-driven chat assistant systems haveintegrated memory components to track user-assistant chat histories, enablingmore accurate and personalized responses. However, their long-term memorycapabilities in sustained interactions remain underexplored. This paperintroduces LongMemEval, a comprehensive benchmark designed to evaluate fivecore long-term memory abilities of chat assistants: information extraction,multi-session reasoning, temporal reasoning, knowledge updates, and abstention.With 500 meticulously curated questions embedded within freely scalableuser-assistant chat histories, LongMemEval presents a significant challenge toexisting long-term memory systems, with commercial chat assistants andlong-context LLMs showing 30% accuracy drop on memorizing information acrosssustained interactions. We then present a unified framework that breaks downthe long-term memory design into four design choices across the indexing,retrieval, and reading stages. Built upon key experimental insights, we proposeseveral memory designs including session decomposition for optimizing valuegranularity, fact-augmented key expansion for enhancing the index structure,and time-aware query expansion for refining the search scope. Experimentresults show that these optimizations greatly improve both memory recall anddownstream question answering on LongMemEval. Overall, our study providesvaluable resources and guidance for advancing the long-term memory capabilitiesof LLM-based chat assistants, paving the way toward more personalized andreliable conversational AI.</description><author>Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu</author><pubDate>Mon, 14 Oct 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10813v1</guid></item><item><title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title><link>http://arxiv.org/abs/2410.10812v1</link><description>We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR)visual generation model capable of directly generating 1024x1024 images,rivaling diffusion models in image generation quality. Existing AR models facelimitations due to the poor image reconstruction quality of their discretetokenizers and the prohibitive training costs associated with generating 1024pximages. To address these challenges, we present the hybrid tokenizer, whichdecomposes the continuous latents from the autoencoder into two components:discrete tokens representing the big picture and continuous tokens representingthe residual components that cannot be represented by the discrete tokens. Thediscrete component is modeled by a scalable-resolution discrete AR model, whilethe continuous component is learned with a lightweight residual diffusionmodule with only 37M parameters. Compared with the discrete-only VAR tokenizer,our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K,leading to a 31% generation FID improvement from 7.85 to 5.38. HART alsooutperforms state-of-the-art diffusion models in both FID and CLIP score, with4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourcedat https://github.com/mit-han-lab/hart.</description><author>Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, Song Han</author><pubDate>Mon, 14 Oct 2024 17:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10812v1</guid></item><item><title>Deep Linear Probe Generators for Weight Space Learning</title><link>http://arxiv.org/abs/2410.10811v1</link><description>Weight space learning aims to extract information about a neural network,such as its training dataset or generalization error. Recent approaches learndirectly from model weights, but this presents many challenges as weights arehigh-dimensional and include permutation symmetries between neurons. Analternative approach, Probing, represents a model by passing a set of learnedinputs (probes) through the model, and training a predictor on top of thecorresponding outputs. Although probing is typically not used as a stand aloneapproach, our preliminary experiment found that a vanilla probing baselineworked surprisingly well. However, we discover that current probe learningstrategies are ineffective. We therefore propose Deep Linear Probe Generators(ProbeGen), a simple and effective modification to probing approaches. ProbeGenadds a shared generator module with a deep linear architecture, providing aninductive bias towards structured probes thus reducing overfitting. Whilesimple, ProbeGen performs significantly better than the state-of-the-art and isvery efficient, requiring between 30 to 1000 times fewer FLOPs than other topapproaches.</description><author>Jonathan Kahana, Eliahu Horwitz, Imri Shuval, Yedid Hoshen</author><pubDate>Mon, 14 Oct 2024 17:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10811v1</guid></item><item><title>Local and Global Decoding in Text Generation</title><link>http://arxiv.org/abs/2410.10810v1</link><description>Text generation, a key component in applications such as dialogue systems,relies on decoding algorithms that sample strings from a language modeldistribution. Traditional methods, such as top-$k$ and top-$\pi$, apply localnormalisation to the model's output distribution, which can distort it. In thispaper, we investigate the effect of this distortion by introducingglobally-normalised versions of these decoding methods. Additionally, wepropose an independent Metropolis-Hastings algorithm to approximate samplingfrom globally-normalised distributions without explicitly computing them. Ourempirical analysis compares the performance of local and global normalisationacross two decoding algorithms (top-$k$ and top-$\pi$) with varioushyperparameters, using Pythia language models. Results show that, in mostconfigurations, global decoding performs worse than the local decoding versionof the same algorithms -- despite preserving the distribution's integrity. Ourresults suggest that distortion is an important feature of local decodingalgorithms.</description><author>Daniel Gareev, Thomas Hofmann, Ezhilmathi Krishnasamy, Tiago Pimentel</author><pubDate>Mon, 14 Oct 2024 17:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10810v1</guid></item><item><title>Hard-Constrained Neural Networks with Universal Approximation Guarantees</title><link>http://arxiv.org/abs/2410.10807v1</link><description>Incorporating prior knowledge or specifications of input-output relationshipsinto machine learning models has gained significant attention, as it enhancesgeneralization from limited data and leads to conforming outputs. However, mostexisting approaches use soft constraints by penalizing violations throughregularization, which offers no guarantee of constraint satisfaction -- anessential requirement in safety-critical applications. On the other hand,imposing hard constraints on neural networks may hinder their representationalpower, adversely affecting performance. To address this, we propose HardNet, apractical framework for constructing neural networks that inherently satisfyhard constraints without sacrificing model capacity. Specifically, we encodeaffine and convex hard constraints, dependent on both inputs and outputs, byappending a differentiable projection layer to the network's output. Thisarchitecture allows unconstrained optimization of the network parameters usingstandard algorithms while ensuring constraint satisfaction by construction.Furthermore, we show that HardNet retains the universal approximationcapabilities of neural networks. We demonstrate the versatility andeffectiveness of HardNet across various applications: fitting functions underconstraints, learning optimization solvers, optimizing control policies insafety-critical systems, and learning safe decision logic for aircraft systems.</description><author>Youngjae Min, Anoopkumar Sonar, Navid Azizan</author><pubDate>Mon, 14 Oct 2024 17:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10807v1</guid></item><item><title>TL-PCA: Transfer Learning of Principal Component Analysis</title><link>http://arxiv.org/abs/2410.10805v1</link><description>Principal component analysis (PCA) can be significantly limited when there istoo few examples of the target data of interest. We propose a transfer learningapproach to PCA (TL-PCA) where knowledge from a related source task is used inaddition to the scarce data of a target task. Our TL-PCA has two versions, onethat uses a pretrained PCA solution of the source task, and another that usesthe source data. Our proposed approach extends the PCA optimization objectivewith a penalty on the proximity of the target subspace and the source subspaceas given by the pretrained source model or the source data. This optimizationis solved by eigendecomposition for which the number of data-dependenteigenvectors (i.e., principal directions of TL-PCA) is not limited to thenumber of target data examples, which is a root cause that limits the standardPCA performance. Accordingly, our results for image datasets show that therepresentation of test data is improved by TL-PCA for dimensionality reductionwhere the learned subspace dimension is lower or higher than the number oftarget data examples.</description><author>Sharon Hendy, Yehuda Dar</author><pubDate>Mon, 14 Oct 2024 17:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10805v1</guid></item><item><title>TrajDiffuse: A Conditional Diffusion Model for Environment-Aware Trajectory Prediction</title><link>http://arxiv.org/abs/2410.10804v1</link><description>Accurate prediction of human or vehicle trajectories with good diversity thatcaptures their stochastic nature is an essential task for many applications.However, many trajectory prediction models produce unreasonable trajectorysamples that focus on improving diversity or accuracy while neglecting otherkey requirements, such as collision avoidance with the surrounding environment.In this work, we propose TrajDiffuse, a planning-based trajectory predictionmethod using a novel guided conditional diffusion model. We form the trajectoryprediction problem as a denoising impaint task and design a map-based guidanceterm for the diffusion process. TrajDiffuse is able to generate trajectorypredictions that match or exceed the accuracy and diversity of the SOTA, whileadhering almost perfectly to environmental constraints. We demonstrate theutility of our model through experiments on the nuScenes and PFSD datasets andprovide an extensive benchmark analysis against the SOTA methods.</description><author>Qingze, Liu, Danrui Li, Samuel S. Sohn, Sejong Yoon, Mubbasir Kapadia, Vladimir Pavlovic</author><pubDate>Mon, 14 Oct 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10804v1</guid></item><item><title>Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies</title><link>http://arxiv.org/abs/2410.10803v1</link><description>Humanoid robots capable of autonomous operation in diverse environments havelong been a goal for roboticists. However, autonomous manipulation by humanoidrobots has largely been restricted to one specific scene, primarily due to thedifficulty of acquiring generalizable skills. Recent advances in 3D visuomotorpolicies, such as the 3D Diffusion Policy (DP3), have shown promise inextending these capabilities to wilder environments. However, 3D visuomotorpolicies often rely on camera calibration and point-cloud segmentation, whichpresent challenges for deployment on mobile robots like humanoids. In thiswork, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3Dvisuomotor policy that eliminates these constraints by leveraging egocentric 3Dvisual representations. We demonstrate that iDP3 enables a full-sized humanoidrobot to autonomously perform skills in diverse real-world scenarios, usingonly data collected in the lab. Videos are available at:https://humanoid-manipulation.github.io</description><author>Yanjie Ze, Zixuan Chen, Wenhao Wang, Tianyi Chen, Xialin He, Ying Yuan, Xue Bin Peng, Jiajun Wu</author><pubDate>Mon, 14 Oct 2024 17:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10803v1</guid></item><item><title>Boosting Camera Motion Control for Video Diffusion Transformers</title><link>http://arxiv.org/abs/2410.10802v1</link><description>Recent advancements in diffusion models have significantly enhanced thequality of video generation. However, fine-grained control over camera poseremains a challenge. While U-Net-based models have shown promising results forcamera control, transformer-based diffusion models (DiT)-the preferredarchitecture for large-scale video generation - suffer from severe degradationin camera motion accuracy. In this paper, we investigate the underlying causesof this issue and propose solutions tailored to DiT architectures. Our studyreveals that camera control performance depends heavily on the choice ofconditioning methods rather than camera pose representations that is commonlybelieved. To address the persistent motion degradation in DiT, we introduceCamera Motion Guidance (CMG), based on classifier-free guidance, which boostscamera control by over 400%. Additionally, we present a sparse camera controlpipeline, significantly simplifying the process of specifying camera poses forlong videos. Our method universally applies to both U-Net and DiT models,offering improved camera control for video generation tasks.</description><author>Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, Chun-Hao Paul Huang</author><pubDate>Mon, 14 Oct 2024 17:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10802v1</guid></item><item><title>Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment</title><link>http://arxiv.org/abs/2404.12318v2</link><description>Aligning language models (LMs) based on human-annotated preference data is acrucial step in obtaining practical and performant LM-based systems. However,multilingual human preference data are difficult to obtain at scale, making itchallenging to extend this framework to diverse languages. In this work, weevaluate a simple approach for zero-shot cross-lingual alignment, where areward model is trained on preference data in one source language and directlyapplied to other target languages. On summarization and open-ended dialoggeneration, we show that this method is consistently successful undercomprehensive evaluation settings, including human evaluation: cross-linguallyaligned models are preferred by humans over unaligned models on up to &gt;70% ofevaluation instances. We moreover find that a different-language reward modelsometimes yields better aligned models than a same-language reward model. Wealso identify best practices when there is no language-specific data for evensupervised finetuning, another component in alignment.</description><author>Zhaofeng Wu, Ananth Balashankar, Yoon Kim, Jacob Eisenstein, Ahmad Beirami</author><pubDate>Mon, 14 Oct 2024 17:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12318v2</guid></item><item><title>Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning</title><link>http://arxiv.org/abs/2410.10801v1</link><description>Large Language Models (LLMs) have been adopted and deployed worldwide for abroad variety of applications. However, ensuring their safe use remains asignificant challenge. Preference training and safety measures often overfit toharms prevalent in Western-centric datasets, and safety protocols frequentlyfail to extend to multilingual settings. In this work, we explore model mergingin a diverse multi-task setting, combining safety and general-purpose taskswithin a multilingual context. Each language introduces unique and variedlearning challenges across tasks. We find that objective-based merging is moreeffective than mixing data, with improvements of up to 8% and 10% in generalperformance and safety respectively. We also find that language-based mergingis highly effective -- by merging monolingually fine-tuned models, we achieve a4% increase in general performance and 7% reduction in harm across alllanguages on top of the data mixtures method using the same available data.Overall, our comprehensive study of merging approaches provides a usefulframework for building strong and safe multilingual models.</description><author>Aakanksha, Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, Sara Hooker</author><pubDate>Mon, 14 Oct 2024 17:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10801v1</guid></item><item><title>Learning Quadruped Locomotion Using Differentiable Simulation</title><link>http://arxiv.org/abs/2403.14864v3</link><description>This work explores the potential of using differentiable simulation forlearning quadruped locomotion. Differentiable simulation promises fastconvergence and stable training by computing low-variance first-order gradientsusing robot dynamics. However, its usage for legged robots is still limited tosimulation. The main challenge lies in the complex optimization landscape ofrobotic tasks due to discontinuous dynamics. This work proposes a newdifferentiable simulation framework to overcome these challenges. Our approachcombines a high-fidelity, non-differentiable simulator for forward dynamicswith a simplified surrogate model for gradient backpropagation. This approachmaintains simulation accuracy by aligning the robot states from the surrogatemodel with those of the precise, non-differentiable simulator. Our frameworkenables learning quadruped walking in simulation in minutes withoutparallelization. When augmented with GPU parallelization, our approach allowsthe quadruped robot to master diverse locomotion skills on challenging terrainsin minutes. We demonstrate that differentiable simulation outperforms areinforcement learning algorithm (PPO) by achieving significantly better sampleefficiency while maintaining its effectiveness in handling large-scaleenvironments. Our method represents one of the first successful applications ofdifferentiable simulation to real-world quadruped locomotion, offering acompelling alternative to traditional RL methods.</description><author>Yunlong Song, Sangbae Kim, Davide Scaramuzza</author><pubDate>Mon, 14 Oct 2024 17:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14864v3</guid></item><item><title>Towards Foundation Models for 3D Vision: How Close Are We?</title><link>http://arxiv.org/abs/2410.10799v1</link><description>Building a foundation model for 3D vision is a complex challenge that remainsunsolved. Towards that goal, it is important to understand the 3D reasoningcapabilities of current models as well as identify the gaps between thesemodels and humans. Therefore, we construct a new 3D visual understandingbenchmark that covers fundamental 3D vision tasks in the Visual QuestionAnswering (VQA) format. We evaluate state-of-the-art Vision-Language Models(VLMs), specialized models, and human subjects on it. Our results show thatVLMs generally perform poorly, while the specialized models are accurate butnot robust, failing under geometric perturbations. In contrast, human visioncontinues to be the most reliable 3D visual system. We further demonstrate thatneural networks align more closely with human 3D vision mechanisms compared toclassical computer vision methods, and Transformer-based networks such as ViTalign more closely with human 3D vision mechanisms than CNNs. We hope our studywill benefit the future development of foundation models for 3D vision.</description><author>Yiming Zuo, Karhan Kayan, Maggie Wang, Kevin Jeon, Jia Deng, Thomas L. Griffiths</author><pubDate>Mon, 14 Oct 2024 17:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10799v1</guid></item><item><title>MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling</title><link>http://arxiv.org/abs/2410.10798v1</link><description>Recent advancements in multi-modal large language models have propelled thedevelopment of joint probabilistic models capable of both image understandingand generation. However, we have identifed that recent methods inevitablysuffer from loss of image information during understanding task, due to eitherimage discretization or diffusion denoising steps. To address this issue, wepropose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modelingframework. Unlike discretization line of method, MMAR takes incontinuous-valued image tokens to avoid information loss. Differing fromdiffusion-based approaches, we disentangle the diffusion process fromauto-regressive backbone model by employing a light-weight diffusion head ontop each auto-regressed image patch embedding. In this way, when the modeltransits from image generation to understanding through text generation, thebackbone model's hidden representation of the image is not limited to the lastdenoising step. To successfully train our method, we also propose atheoretically proven technique that addresses the numerical stability issue anda training strategy that balances the generation and understanding task goals.Through extensive evaluations on 18 image understanding benchmarks, MMARdemonstrates much more superior performance than other joint multi-modalmodels, matching the method that employs pretrained CLIP vision encoder,meanwhile being able to generate high quality images at the same time. We alsoshowed that our method is scalable with larger data and model size.</description><author>Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, Zheng-Jun Zha</author><pubDate>Mon, 14 Oct 2024 17:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10798v1</guid></item><item><title>Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance</title><link>http://arxiv.org/abs/2410.10796v1</link><description>Large language models are instruction-finetuned to enhance their ability tofollow user instructions and process the input context. However, evenstate-of-the-art models often struggle to follow the instruction, especiallywhen the input context is not aligned with the model's parametric knowledge.This manifests as various failures, such as hallucinations where the responsesare outdated, biased or contain unverified facts. In this work, we try tounderstand the underlying reason for this poor context reliance, especiallyafter instruction tuning. We observe an intriguing phenomenon: duringinstruction tuning, the context reliance initially increases as expected, butthen gradually decreases as instruction finetuning progresses. We call thisphenomenon context-parametric inversion and observe it across multiple generalpurpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well asmodel families such as Llama, Mistral and Pythia. In a simple theoreticalsetup, we isolate why context-parametric inversion occurs along the gradientdescent trajectory of instruction finetuning. We tie this phenomena to examplesin the instruction finetuning data mixture where the input context providesinformation that is already present in the model's parametric knowledge. Ouranalysis suggests natural mitigation strategies that provide some limitedgains, while also validating our theoretical insights. We hope that our workserves as a starting point in addressing this failure mode in a staple part ofLLM training.</description><author>Sachin Goyal, Christina Baek, J. Zico Kolter, Aditi Raghunathan</author><pubDate>Mon, 14 Oct 2024 17:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10796v1</guid></item><item><title>Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations</title><link>http://arxiv.org/abs/2410.10792v1</link><description>Generative models transform random noise into images; their inversion aims totransform images back to structured noise for recovery and editing. This paperaddresses two key tasks: (i) inversion and (ii) editing of a real image usingstochastic equivalents of rectified flow models (such as Flux). AlthoughDiffusion Models (DMs) have recently dominated the field of generative modelingfor images, their inversion presents faithfulness and editability challengesdue to nonlinearities in drift and diffusion. Existing state-of-the-art DMinversion approaches rely on training of additional parameters or test-timeoptimization of latent variables; both are expensive in practice. RectifiedFlows (RFs) offer a promising alternative to diffusion models, yet theirinversion has been underexplored. We propose RF inversion using dynamic optimalcontrol derived via a linear quadratic regulator. We prove that the resultingvector field is equivalent to a rectified stochastic differential equation.Additionally, we extend our framework to design a stochastic sampler for Flux.Our inversion method allows for state-of-the-art performance in zero-shotinversion and editing, outperforming prior works in stroke-to-image synthesisand semantic image editing, with large-scale human evaluations confirming userpreference.</description><author>Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, Wen-Sheng Chu</author><pubDate>Mon, 14 Oct 2024 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10792v1</guid></item><item><title>Condition-Aware Multimodal Fusion for Robust Semantic Perception of Driving Scenes</title><link>http://arxiv.org/abs/2410.10791v1</link><description>Leveraging multiple sensors is crucial for robust semantic perception inautonomous driving, as each sensor type has complementary strengths andweaknesses. However, existing sensor fusion methods often treat sensorsuniformly across all conditions, leading to suboptimal performance. Bycontrast, we propose a novel, condition-aware multimodal fusion approach forrobust semantic perception of driving scenes. Our method, CAFuser uses an RGBcamera input to classify environmental conditions and generate a ConditionToken that guides the fusion of multiple sensor modalities. We further newlyintroduce modality-specific feature adapters to align diverse sensor inputsinto a shared latent space, enabling efficient integration with a single andshared pre-trained backbone. By dynamically adapting sensor fusion based on theactual condition, our model significantly improves robustness and accuracy,especially in adverse-condition scenarios. We set the new state of the art withCAFuser on the MUSES dataset with 59.7 PQ for multimodal panoptic segmentationand 78.2 mIoU for semantic segmentation, ranking first on the publicbenchmarks.</description><author>Tim Broedermann, Christos Sakaridis, Yuqian Fu, Luc Van Gool</author><pubDate>Mon, 14 Oct 2024 17:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10791v1</guid></item><item><title>Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes</title><link>http://arxiv.org/abs/2410.10790v1</link><description>Recent advancements in human motion synthesis have focused on specific typesof motions, such as human-scene interaction, locomotion or human-humaninteraction, however, there is a lack of a unified system capable of generatinga diverse combination of motion types. In response, we introduceSitcom-Crafter, a comprehensive and extendable system for human motiongeneration in 3D space, which can be guided by extensive plot contexts toenhance workflow efficiency for anime and game designers. The system iscomprised of eight modules, three of which are dedicated to motion generation,while the remaining five are augmentation modules that ensure consistent fusionof motion sequences and system functionality. Central to the generation modulesis our novel 3D scene-aware human-human interaction module, which addressescollision issues by synthesizing implicit 3D Signed Distance Function (SDF)points around motion spaces, thereby minimizing human-scene collisions withoutadditional data collection costs. Complementing this, our locomotion andhuman-scene interaction modules leverage existing methods to enrich thesystem's motion generation capabilities. Augmentation modules encompass plotcomprehension for command generation, motion synchronization for seamlessintegration of different motion types, hand pose retrieval to enhance motionrealism, motion collision revision to prevent human collisions, and 3Dretargeting to ensure visual fidelity. Experimental evaluations validate thesystem's ability to generate high-quality, diverse, and physically realisticmotions, underscoring its potential for advancing creative workflows.</description><author>Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Christian Kampffmeyer, Xiaodan Liang</author><pubDate>Mon, 14 Oct 2024 17:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10790v1</guid></item><item><title>On Information-Theoretic Measures of Predictive Uncertainty</title><link>http://arxiv.org/abs/2410.10786v1</link><description>Reliable estimation of predictive uncertainty is crucial for machine learningapplications, particularly in high-stakes scenarios where hedging against risksis essential. Despite its significance, a consensus on the correct measurementof predictive uncertainty remains elusive. In this work, we return to firstprinciples to develop a fundamental framework of information-theoreticpredictive uncertainty measures. Our proposed framework categorizes predictiveuncertainty measures according to two factors: (I) The predicting model (II)The approximation of the true predictive distribution. Examining all possiblecombinations of these two factors, we derive a set of predictive uncertaintymeasures that includes both known and newly introduced ones. We empiricallyevaluate these measures in typical uncertainty estimation settings, such asmisclassification detection, selective prediction, and out-of-distributiondetection. The results show that no single measure is universal, but theeffectiveness depends on the specific setting. Thus, our work provides clarityabout the suitability of predictive uncertainty measures by clarifying theirimplicit assumptions and relationships.</description><author>Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, Sepp Hochreiter</author><pubDate>Mon, 14 Oct 2024 17:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10786v1</guid></item><item><title>LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content</title><link>http://arxiv.org/abs/2410.10783v1</link><description>The large-scale training of multi-modal models on data scraped from the webhas shown outstanding utility in infusing these models with the required worldknowledge to perform effectively on multiple downstream tasks. However, onedownside of scraping data from the web can be the potential sacrifice of thebenchmarks on which the abilities of these models are often evaluated. Tosafeguard against test data contamination and to truly test the abilities ofthese foundation models we propose LiveXiv: A scalable evolving live benchmarkbased on scientific ArXiv papers. LiveXiv accesses domain-specific manuscriptsat any given timestamp and proposes to automatically generate visualquestion-answer pairs (VQA). This is done without any human-in-the-loop, usingthe multi-modal content in the manuscripts, like graphs, charts, and tables.Moreover, we introduce an efficient evaluation approach that estimates theperformance of all models on the evolving benchmark using evaluations of only asubset of models. This significantly reduces the overall evaluation cost. Webenchmark multiple open and proprietary Large Multi-modal Models (LMMs) on thefirst version of our benchmark, showing its challenging nature and exposing themodels true abilities, avoiding contamination. Lastly, in our commitment tohigh quality, we have collected and evaluated a manually verified subset. Bycomparing its overall results to our automatic annotations, we have found thatthe performance variance is indeed minimal (&lt;2.5%). Our dataset is availableonline on HuggingFace, and our code will be available here.</description><author>Nimrod Shabtay, Felipe Maia Polo, Sivan Doveh, Wei Lin, M. Jehanzeb Mirza, Leshem Chosen, Mikhail Yurochkin, Yuekai Sun, Assaf Arbelle, Leonid Karlinsky, Raja Giryes</author><pubDate>Mon, 14 Oct 2024 17:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10783v1</guid></item><item><title>3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for Human-Object Interaction (HOI) and Autonomous Driving Applications</title><link>http://arxiv.org/abs/2410.10782v1</link><description>Human-object interaction (HOI) and human-scene interaction (HSI) are crucialfor human-centric scene understanding applications in Embodied ArtificialIntelligence (EAI), robotics, and augmented reality (AR). A common limitationfaced in these research areas is the data scarcity problem: insufficientlabeled human-scene object pairs on the input images, and limited interactioncomplexity and granularity between them. Recent HOI and HSI methods haveaddressed this issue by generating dynamic interactions with rigid objects. Butmore complex dynamic interactions such as a human rider pedaling an articulatedbicycle have been unexplored. To address this limitation, and to enableresearch on complex dynamic human-articulated object interactions, in thispaper we propose a method to generate simulated 3D dynamic cyclist assets andinteractions. We designed a methodology for creating a new part-basedmulti-view articulated synthetic 3D bicycle dataset that we call 3DArticBikesthat can be used to train NeRF and 3DGS-based 3D reconstruction methods. Wethen propose a 3DGS-based parametric bicycle composition model to assemble8-DoF pose-controllable 3D bicycles. Finally, using dynamic information fromcyclist videos, we build a complete synthetic dynamic 3D cyclist (riderpedaling a bicycle) by re-posing a selectable synthetic 3D person whileautomatically placing the rider onto one of our new articulated 3D bicyclesusing a proposed 3D Keypoint optimization-based Inverse Kinematics poserefinement. We present both, qualitative and quantitative results where wecompare our generated cyclists against those from a recent stablediffusion-based method.</description><author>Eduardo R. Corral-Soto, Yang Liu, Tongtong Cao, Yuan Ren, Liu Bingbing</author><pubDate>Mon, 14 Oct 2024 17:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10782v1</guid></item><item><title>When Attention Sink Emerges in Language Models: An Empirical View</title><link>http://arxiv.org/abs/2410.10781v1</link><description>Language Models (LMs) assign significant attention to the first token, evenif it is not semantically important, which is known as attention sink. Thisphenomenon has been widely adopted in applications such as streaming/longcontext generation, KV cache optimization, inference acceleration, modelquantization, and others. Despite its widespread use, a deep understanding ofattention sink in LMs is still lacking. In this work, we first demonstrate thatattention sinks exist universally in LMs with various inputs, even in smallmodels. Furthermore, attention sink is observed to emerge during the LMpre-training, motivating us to investigate how optimization, data distribution,loss function, and model architecture in LM pre-training influence itsemergence. We highlight that attention sink emerges after effectiveoptimization on sufficient training data. The sink position is highlycorrelated with the loss function and data distribution. Most importantly, wefind that attention sink acts more like key biases, storing extra attentionscores, which could be non-informative and not contribute to the valuecomputation. We also observe that this phenomenon (at least partially) stemsfrom tokens' inner dependence on attention scores as a result of softmaxnormalization. After relaxing such dependence by replacing softmax attentionwith other attention operations, such as sigmoid attention withoutnormalization, attention sinks do not emerge in LMs up to 1B parameters. Thecode is available at https://github.com/sail-sg/Attention-Sink.</description><author>Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin</author><pubDate>Mon, 14 Oct 2024 17:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10781v1</guid></item><item><title>ControlMM: Controllable Masked Motion Generation</title><link>http://arxiv.org/abs/2410.10780v1</link><description>Recent advances in motion diffusion models have enabled spatiallycontrollable text-to-motion generation. However, despite achieving acceptablecontrol precision, these models suffer from generation speed and fidelitylimitations. To address these challenges, we propose ControlMM, a novelapproach incorporating spatial control signals into the generative maskedmotion model. ControlMM achieves real-time, high-fidelity, and high-precisioncontrollable motion generation simultaneously. Our approach introduces two keyinnovations. First, we propose masked consistency modeling, which ensureshigh-fidelity motion generation via random masking and reconstruction, whileminimizing the inconsistency between the input control signals and theextracted control signals from the generated motion. To further enhance controlprecision, we introduce inference-time logit editing, which manipulates thepredicted conditional motion distribution so that the generated motion, sampledfrom the adjusted distribution, closely adheres to the input control signals.During inference, ControlMM enables parallel and iterative decoding of multiplemotion tokens, allowing for high-speed motion generation. Extensive experimentsshow that, compared to the state of the art, ControlMM delivers superiorresults in motion quality, with better FID scores (0.061 vs 0.271), and highercontrol precision (average error 0.0091 vs 0.0108). ControlMM generates motions20 times faster than diffusion-based methods. Additionally, ControlMM unlocksdiverse applications such as any joint any frame control, body part timelinecontrol, and obstacle avoidance. Video visualization can be found athttps://exitudio.github.io/ControlMM-page</description><author>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, Sergey Tulyakov</author><pubDate>Mon, 14 Oct 2024 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10780v1</guid></item><item><title>Focused ReAct: Improving ReAct through Reiterate and Early Stop</title><link>http://arxiv.org/abs/2410.10779v1</link><description>Large language models (LLMs) have significantly improved their reasoning anddecision-making capabilities, as seen in methods like ReAct. However, despiteits effectiveness in tackling complex tasks, ReAct faces two main challenges:losing focus on the original question and becoming stuck in action loops. Toaddress these issues, we introduce Focused ReAct, an enhanced version of theReAct paradigm that incorporates reiteration and early stop mechanisms. Theseimprovements help the model stay focused on the original query and avoidrepetitive behaviors. Experimental results show accuracy gains of 18% to 530%and a runtime reduction of up to 34% compared to the original ReAct method.</description><author>Shuoqiu Li, Han Xu, Haipeng Chen</author><pubDate>Mon, 14 Oct 2024 17:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10779v1</guid></item><item><title>UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2410.10777v1</link><description>Semi-supervised semantic segmentation (SSS) aims at learning rich visualknowledge from cheap unlabeled images to enhance semantic segmentationcapability. Among recent works, UniMatch improves its precedents tremendouslyby amplifying the practice of weak-to-strong consistency regularization.Subsequent works typically follow similar pipelines and propose variousdelicate designs. Despite the achieved progress, strangely, even in thisflourishing era of numerous powerful vision models, almost all SSS works arestill sticking to 1) using outdated ResNet encoders with small-scaleImageNet-1K pre-training, and 2) evaluation on simple Pascal and Cityscapesdatasets. In this work, we argue that, it is necessary to switch the baselineof SSS from ResNet-based encoders to more capable ViT-based encoders (e.g.,DINOv2) that are pre-trained on massive data. A simple update on the encoder(even using 2x fewer parameters) can bring more significant improvement thancareful method designs. Built on this competitive baseline, we present ourupgraded and simplified UniMatch V2, inheriting the core spirit ofweak-to-strong consistency from V1, but requiring less training cost andproviding consistently better results. Additionally, witnessing the graduallysaturated performance on Pascal and Cityscapes, we appeal that we should focuson more challenging benchmarks with complex taxonomy, such as ADE20K and COCOdatasets. Code, models, and logs of all reported values, are available athttps://github.com/LiheYoung/UniMatch-V2.</description><author>Lihe Yang, Zhen Zhao, Hengshuang Zhao</author><pubDate>Mon, 14 Oct 2024 17:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10777v1</guid></item><item><title>Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention</title><link>http://arxiv.org/abs/2410.10774v1</link><description>In recent years there have been remarkable breakthroughs in image-to-videogeneration. However, the 3D consistency and camera controllability of generatedframes have remained unsolved. Recent studies have attempted to incorporatecamera control into the generation process, but their results are often limitedto simple trajectories or lack the ability to generate consistent videos frommultiple distinct camera paths for the same scene. To address theselimitations, we introduce Cavia, a novel framework for camera-controllable,multi-view video generation, capable of converting an input image into multiplespatiotemporally consistent videos. Our framework extends the spatial andtemporal attention modules into view-integrated attention modules, improvingboth viewpoint and temporal consistency. This flexible design allows for jointtraining with diverse curated data sources, including scene-level staticvideos, object-level synthetic multi-view dynamic videos, and real-worldmonocular dynamic videos. To our best knowledge, Cavia is the first of its kindthat allows the user to precisely specify camera motion while obtaining objectmotion. Extensive experiments demonstrate that Cavia surpasses state-of-the-artmethods in terms of geometric consistency and perceptual quality. Project Page:https://ir1d.github.io/Cavia/</description><author>Dejia Xu, Yifan Jiang, Chen Huang, Liangchen Song, Thorsten Gernoth, Liangliang Cao, Zhangyang Wang, Hao Tang</author><pubDate>Mon, 14 Oct 2024 17:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10774v1</guid></item><item><title>Designing a Dashboard for Transparency and Control of Conversational AI</title><link>http://arxiv.org/abs/2406.07882v3</link><description>Conversational LLMs function as black box systems, leaving users guessingabout why they see the output they do. This lack of transparency is potentiallyproblematic, especially given concerns around bias and truthfulness. To addressthis issue, we present an end-to-end prototype-connecting interpretabilitytechniques with user experience design-that seeks to make chatbots moretransparent. We begin by showing evidence that a prominent open-source LLM hasa "user model": examining the internal state of the system, we can extract datarelated to a user's age, gender, educational level, and socioeconomic status.Next, we describe the design of a dashboard that accompanies the chatbotinterface, displaying this user model in real time. The dashboard can also beused to control the user model and the system's behavior. Finally, we discuss astudy in which users conversed with the instrumented system. Our resultssuggest that users appreciate seeing internal states, which helped them exposebiased behavior and increased their sense of control. Participants also madevaluable suggestions that point to future directions for both design andmachine learning research. The project page and video demo of our TalkTunersystem are available at https://bit.ly/talktuner-project-page</description><author>Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Viégas</author><pubDate>Mon, 14 Oct 2024 17:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07882v3</guid></item><item><title>Enhancing JEPAs with Spatial Conditioning: Robust and Efficient Representation Learning</title><link>http://arxiv.org/abs/2410.10773v1</link><description>Image-based Joint-Embedding Predictive Architecture (IJEPA) offers anattractive alternative to Masked Autoencoder (MAE) for representation learningusing the Masked Image Modeling framework. IJEPA drives representations tocapture useful semantic information by predicting in latent rather than inputspace. However, IJEPA relies on carefully designed context and target windowsto avoid representational collapse. The encoder modules in IJEPA cannotadaptively modulate the type of predicted and/or target features based on thefeasibility of the masked prediction task as they are not given sufficientinformation of both context and targets. Based on the intuition that in naturalimages, information has a strong spatial bias with spatially local regionsbeing highly predictive of one another compared to distant ones. We conditionthe target encoder and context encoder modules in IJEPA with positions ofcontext and target windows respectively. Our "conditional" encoders showperformance gains on several image classification benchmark datasets, improvedrobustness to context window size and sample-efficiency during pretraining.</description><author>Etai Littwin, Vimal Thilak, Anand Gopalakrishnan</author><pubDate>Mon, 14 Oct 2024 17:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10773v1</guid></item><item><title>Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model</title><link>http://arxiv.org/abs/2405.14457v2</link><description>Machine learning models can be trained with formal privacy guarantees viadifferentially private optimizers such as DP-SGD. In this work, we focus on athreat model where the adversary has access only to the final model, with novisibility into intermediate updates. In the literature, this hidden statethreat model exhibits a significant gap between the lower bound from empiricalprivacy auditing and the theoretical upper bound provided by privacyaccounting. To challenge this gap, we propose to audit this threat model withadversaries that \emph{craft a gradient sequence} designed to maximize theprivacy loss of the final model without relying on intermediate updates. Ourexperiments show that this approach consistently outperforms previous attemptsat auditing the hidden state model. Furthermore, our results advance theunderstanding of achievable privacy guarantees within this threat model.Specifically, when the crafted gradient is inserted at every optimization step,we show that concealing the intermediate model updates in DP-SGD does notamplify privacy. The situation is more complex when the crafted gradient is notinserted at every step: our auditing lower bound matches the privacy upperbound only for an adversarially-chosen loss landscape and a sufficiently largebatch size. This suggests that existing privacy upper bounds can be improved incertain regimes.</description><author>Tudor Cebere, Aurélien Bellet, Nicolas Papernot</author><pubDate>Mon, 14 Oct 2024 17:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14457v2</guid></item><item><title>Enhancing Indonesian Automatic Speech Recognition: Evaluating Multilingual Models with Diverse Speech Variabilities</title><link>http://arxiv.org/abs/2410.08828v2</link><description>An ideal speech recognition model has the capability to transcribe speechaccurately under various characteristics of speech signals, such as speakingstyle (read and spontaneous), speech context (formal and informal), andbackground noise conditions (clean and moderate). Building such a modelrequires a significant amount of training data with diverse speechcharacteristics. Currently, Indonesian data is dominated by read, formal, andclean speech, leading to a scarcity of Indonesian data with other speechvariabilities. To develop Indonesian automatic speech recognition (ASR), wepresent our research on state-of-the-art speech recognition models, namelyMassively Multilingual Speech (MMS) and Whisper, as well as compiling a datasetcomprising Indonesian speech with variabilities to facilitate our study. Wefurther investigate the models' predictive ability to transcribe Indonesianspeech data across different variability groups. The best results were achievedby the Whisper fine-tuned model across datasets with various characteristics,as indicated by the decrease in word error rate (WER) and character error rate(CER). Moreover, we found that speaking style variability affected modelperformance the most.</description><author>Aulia Adila, Dessi Lestari, Ayu Purwarianti, Dipta Tanaya, Kurniawati Azizah, Sakriani Sakti</author><pubDate>Mon, 14 Oct 2024 17:44:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08828v2</guid></item><item><title>Adaptive Diffusion Terrain Generator for Autonomous Uneven Terrain Navigation</title><link>http://arxiv.org/abs/2410.10766v1</link><description>Model-free reinforcement learning has emerged as a powerful method fordeveloping robust robot control policies capable of navigating through complexand unstructured terrains. The effectiveness of these methods hinges on twoessential elements: (1) the use of massively parallel physics simulations toexpedite policy training, and (2) an environment generator tasked with craftingsufficiently challenging yet attainable terrains to facilitate continuouspolicy improvement. Existing methods of environment generation often rely onheuristics constrained by a set of parameters, limiting the diversity andrealism. In this work, we introduce the Adaptive Diffusion Terrain Generator(ADTG), a novel method that leverages Denoising Diffusion Probabilistic Modelsto dynamically expand existing training environments by adding more diverse andcomplex terrains adaptive to the current policy. ADTG guides the diffusionmodel's generation process through initial noise optimization, blendingnoise-corrupted terrains from existing training environments weighted by thepolicy's performance in each corresponding environment. By manipulating thenoise corruption level, ADTG seamlessly transitions between generating similarterrains for policy fine-tuning and novel ones to expand training diversity.Our experiments show that the policy trained by ADTG outperforms bothprocedural generated and natural environments, along with popular navigationmethods.</description><author>Youwei Yu, Junhong Xu, Lantao Liu</author><pubDate>Mon, 14 Oct 2024 17:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10766v1</guid></item><item><title>Towards Generalist Robot Learning from Internet Video: A Survey</title><link>http://arxiv.org/abs/2404.19664v3</link><description>Scaling deep learning to huge internet-scraped datasets has yieldedremarkably general capabilities in natural language processing and visualunderstanding and generation. In contrast, data is scarce and expensive tocollect in robotics. This has seen robot learning struggle to match thegenerality of capabilities observed in other domains. Learning from Videos(LfV) methods seek to address this data bottleneck by augmenting traditionalrobot data with large internet-scraped video datasets. Such video data mayprovide the model with foundational information regarding physical behavioursand the physics of the world. This holds great promise for improving thegenerality of our robots. In this survey, we present an overview of the emerging field of LfV. Weoutline fundamental concepts, including the benefits and challenges of LfV. Weprovide a comprehensive review of current methods for: extracting knowledgefrom large-scale internet video; tackling key LfV challenges; and boostingdownstream reinforcement and robot learning via the use of video data. LfVdatasets and benchmarks are also reviewed. The survey closes with a criticaldiscussion of challenges and opportunities. Here, we advocate for scalablefoundation model approaches that can leverage the full range of availableinternet video to aid the learning of robot policies and dynamics models. Wehope this survey can inform and catalyse further LfV research, facilitatingprogress towards the development of general-purpose robots.</description><author>Robert McCarthy, Daniel C. H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, Zhibin Li</author><pubDate>Mon, 14 Oct 2024 17:41:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19664v3</guid></item><item><title>AFlow: Automating Agentic Workflow Generation</title><link>http://arxiv.org/abs/2410.10762v1</link><description>Large language models (LLMs) have demonstrated remarkable potential insolving complex tasks across diverse domains, typically by employing agenticworkflows that follow detailed instructions and operational sequences. However,constructing these workflows requires significant human effort, limitingscalability and generalizability. Recent research has sought to automate thegeneration and optimization of these workflows, but existing methods still relyon initial manual setup and fall short of achieving fully automated andeffective workflow generation. To address this challenge, we reformulateworkflow optimization as a search problem over code-represented workflows,where LLM-invoking nodes are connected by edges. We introduce AFlow, anautomated framework that efficiently explores this space using Monte Carlo TreeSearch, iteratively refining workflows through code modification,tree-structured experience, and execution feedback. Empirical evaluationsacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%average improvement over state-of-the-art baselines. Furthermore, AFlow enablessmaller models to outperform GPT-4o on specific tasks at 4.55% of its inferencecost in dollars. The code will be available athttps://github.com/geekan/MetaGPT.</description><author>Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu</author><pubDate>Mon, 14 Oct 2024 17:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10762v1</guid></item><item><title>Denial-of-Service Poisoning Attacks against Large Language Models</title><link>http://arxiv.org/abs/2410.10760v1</link><description>Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS)attacks, where adversarial inputs like spelling errors or non-semantic promptstrigger endless outputs without generating an [EOS] token. These attacks canpotentially cause high latency and make LLM services inaccessible to otherusers or tasks. However, when there are speech-to-text interfaces (e.g., voicecommands to a robot), executing such DoS attacks becomes challenging, as it isdifficult to introduce spelling errors or non-semantic prompts through speech.A simple DoS attack in these scenarios would be to instruct the model to "Keeprepeating Hello", but we observe that relying solely on natural instructionslimits output length, which is bounded by the maximum length of the LLM'ssupervised finetuning (SFT) data. To overcome this limitation, we proposepoisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting asingle poisoned sample designed for DoS purposes can break the output lengthlimit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4omini (via OpenAI's finetuning API) using less than $1, causing repeated outputsup to the maximum inference length (16K tokens, compared to 0.5K beforepoisoning). Additionally, we perform comprehensive ablation studies onopen-source LLMs and extend our method to LLM agents, where attackers cancontrol both the finetuning dataset and algorithm. Our findings underscore theurgent need for defenses against P-DoS attacks to secure LLMs. Our code isavailable at https://github.com/sail-sg/P-DoS.</description><author>Kuofeng Gao, Tianyu Pang, Chao Du, Yong Yang, Shu-Tao Xia, Min Lin</author><pubDate>Mon, 14 Oct 2024 17:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10760v1</guid></item><item><title>SplitLLM: Collaborative Inference of LLMs for Model Placement and Throughput Optimization</title><link>http://arxiv.org/abs/2410.10759v1</link><description>Large language models (LLMs) have been a disruptive innovation in recentyears, and they play a crucial role in our daily lives due to their ability tounderstand and generate human-like text. Their capabilities include naturallanguage understanding, information retrieval and search, translation,chatbots, virtual assistance, and many more. However, it is well known thatLLMs are massive in terms of the number of parameters. Additionally, theself-attention mechanism in the underlying architecture of LLMs, Transformers,has quadratic complexity in terms of both computation and memory with respectto the input sequence length. For these reasons, LLM inference isresource-intensive, and thus, the throughput of LLM inference is limited,especially for the longer sequences. In this report, we design a collaborativeinference architecture between a server and its clients to alleviate thethroughput limit. In this design, we consider the available resources on bothsides, i.e., the computation and communication costs. We develop a dynamicprogramming-based algorithm to optimally allocate computation between theserver and the client device to increase the server throughput, while notviolating the service level agreement (SLA). We show in the experiments that weare able to efficiently distribute the workload allowing for roughly 1/3reduction in the server workload, while achieving 19 percent improvement over agreedy method. As a result, we are able to demonstrate that, in an environmentwith different types of LLM inference requests, the throughput of the server isimproved.</description><author>Akrit Mudvari, Yuang Jiang, Leandros Tassiulas</author><pubDate>Mon, 14 Oct 2024 17:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10759v1</guid></item><item><title>Arrhythmia Classification Using Graph Neural Networks Based on Correlation Matrix</title><link>http://arxiv.org/abs/2410.10758v1</link><description>With the advancements in graph neural network, there has been increasinginterest in applying this network to ECG signal analysis. In this study, wegenerated an adjacency matrix using correlation matrix of extracted featuresand applied a graph neural network to classify arrhythmias. The proposed modelwas compared with existing approaches from the literature. The resultsdemonstrated that precision and recall for all arrhythmia classes exceeded 50%,suggesting that this method can be considered an approach for arrhythmiaclassification.</description><author>Seungwoo Han</author><pubDate>Mon, 14 Oct 2024 17:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10758v1</guid></item><item><title>SimpleStrat: Diversifying Language Model Generation with Stratification</title><link>http://arxiv.org/abs/2410.09038v2</link><description>Generating diverse responses from large language models (LLMs) is crucial forapplications such as planning/search and synthetic data generation, wherediversity provides distinct answers across generations. Prior approaches relyon increasing temperature to increase diversity. However, contrary to popularbelief, we show not only does this approach produce lower quality individualgenerations as temperature increases, but it depends on model's next-tokenprobabilities being similar to the true distribution of answers. We proposeSimpleStrat, an alternative approach that uses the language model itself topartition the space into strata. At inference, a random stratum is selected anda sample drawn from within the strata. To measure diversity, we introduceCoverageQA, a dataset of underspecified questions with multiple equallyplausible answers, and assess diversity by measuring KL Divergence between theoutput distribution and uniform distribution over valid ground truth answers.As computing probability per response/solution for proprietary models isinfeasible, we measure recall on ground truth solutions. Our evaluation showusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36average reduction in KL Divergence compared to Llama 3.</description><author>Justin Wong, Yury Orlovskiy, Michael Luo, Sanjit A. Seshia, Joseph E. Gonzalez</author><pubDate>Mon, 14 Oct 2024 17:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09038v2</guid></item><item><title>Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification</title><link>http://arxiv.org/abs/2410.10756v1</link><description>The generative large language models (LLMs) are increasingly used for dataaugmentation tasks, where text samples are paraphrased (or generated anew) andthen used for classifier fine-tuning. Existing works on augmentation leveragethe few-shot scenarios, where samples are given to LLMs as part of prompts,leading to better augmentations. Yet, the samples are mostly selected randomlyand a comprehensive overview of the effects of other (more ``informed'') sampleselection strategies is lacking. In this work, we compare sample selectionstrategies existing in few-shot learning literature and investigate theireffects in LLM-based textual augmentation. We evaluate this on in-distributionand out-of-distribution classifier performance. Results indicate, that whilesome ``informed'' selection strategies increase the performance of models,especially for out-of-distribution data, it happens only seldom and withmarginal performance increases. Unless further advances are made, a default ofrandom sample selection remains a good option for augmentation practitioners.</description><author>Jan Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Maria Bielikova, Peter Brusilovsky</author><pubDate>Mon, 14 Oct 2024 17:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10756v1</guid></item><item><title>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</title><link>http://arxiv.org/abs/2410.09024v2</link><description>The robustness of LLMs to jailbreak attacks, where users design prompts tocircumvent safety measures and misuse model capabilities, has been studiedprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- whichuse external tools and can execute multi-stage tasks -- may pose a greater riskif misused, but their robustness remains underexplored. To facilitate researchon LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmarkincludes a diverse set of 110 explicitly malicious agent tasks (440 withaugmentations), covering 11 harm categories including fraud, cybercrime, andharassment. In addition to measuring whether models refuse harmful agenticrequests, scoring well on AgentHarm requires jailbroken agents to maintaintheir capabilities following an attack to complete a multi-step task. Weevaluate a range of leading LLMs, and find (1) leading LLMs are surprisinglycompliant with malicious agent requests without jailbreaking, (2) simpleuniversal jailbreak templates can be adapted to effectively jailbreak agents,and (3) these jailbreaks enable coherent and malicious multi-step agentbehavior and retain model capabilities. To enable simple and reliableevaluation of attacks and defenses for LLM-based agents, we publicly releaseAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.</description><author>Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies</author><pubDate>Mon, 14 Oct 2024 17:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09024v2</guid></item><item><title>DragEntity: Trajectory Guided Video Generation using Entity and Positional Relationships</title><link>http://arxiv.org/abs/2410.10751v1</link><description>In recent years, diffusion models have achieved tremendous success in thefield of video generation, with controllable video generation receivingsignificant attention. However, existing control methods still face twolimitations: Firstly, control conditions (such as depth maps, 3D Mesh) aredifficult for ordinary users to obtain directly. Secondly, it's challenging todrive multiple objects through complex motions with multiple trajectoriessimultaneously. In this paper, we introduce DragEntity, a video generationmodel that utilizes entity representation for controlling the motion ofmultiple objects. Compared to previous methods, DragEntity offers two mainadvantages: 1) Our method is more user-friendly for interaction because itallows users to drag entities within the image rather than individual pixels.2) We use entity representation to represent any object in the image, andmultiple objects can maintain relative spatial relationships. Therefore, weallow multiple trajectories to control multiple objects in the image withdifferent levels of complexity simultaneously. Our experiments validate theeffectiveness of DragEntity, demonstrating its excellent performance infine-grained control in video generation.</description><author>Zhang Wan, Sheng Tang, Jiawei Wei, Ruize Zhang, Juan Cao</author><pubDate>Mon, 14 Oct 2024 17:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10751v1</guid></item><item><title>FlexGen: Flexible Multi-View Generation from Text and Image Inputs</title><link>http://arxiv.org/abs/2410.10745v1</link><description>In this work, we introduce FlexGen, a flexible framework designed to generatecontrollable and consistent multi-view images, conditioned on a single-viewimage, or a text prompt, or both. FlexGen tackles the challenges ofcontrollable multi-view synthesis through additional conditioning on 3D-awaretext annotations. We utilize the strong reasoning capabilities of GPT-4V togenerate 3D-aware text annotations. By analyzing four orthogonal views of anobject arranged as tiled multi-view images, GPT-4V can produce text annotationsthat include 3D-aware information with spatial relationship. By integrating thecontrol signal with proposed adaptive dual-control module, our model cangenerate multi-view images that correspond to the specified text. FlexGensupports multiple controllable capabilities, allowing users to modify textprompts to generate reasonable and corresponding unseen parts. Additionally,users can influence attributes such as appearance and material properties,including metallic and roughness. Extensive experiments demonstrate that ourapproach offers enhanced multiple controllability, marking a significantadvancement over existing multi-view diffusion models. This work hassubstantial implications for fields requiring rapid and flexible 3D contentcreation, including game development, animation, and virtual reality. Projectpage: https://xxu068.github.io/flexgen.github.io/.</description><author>Xinli Xu, Wenhang Ge, Jiantao Lin, Jiawei Feng, Lie Xu, HanFeng Zhao, Shunsi Zhang, Ying-Cong Chen</author><pubDate>Mon, 14 Oct 2024 17:23:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10745v1</guid></item><item><title>Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings</title><link>http://arxiv.org/abs/2410.10744v1</link><description>Despite significant advancements in out-of-distribution (OOD) detection,existing methods still struggle to maintain robustness against adversarialattacks, compromising their reliability in critical real-world applications.Previous studies have attempted to address this challenge by exposing detectorsto auxiliary OOD datasets alongside adversarial training. However, theincreased data complexity inherent in adversarial training, and the myriad ofways that OOD samples can arise during testing, often prevent these approachesfrom establishing robust decision boundaries. To address these limitations, wepropose AROS, a novel approach leveraging neural ordinary differentialequations (NODEs) with Lyapunov stability theorem in order to obtain robustembeddings for OOD detection. By incorporating a tailored loss function, weapply Lyapunov stability theory to ensure that both in-distribution (ID) andOOD data converge to stable equilibrium points within the dynamical system.This approach encourages any perturbed input to return to its stableequilibrium, thereby enhancing the model's robustness against adversarialperturbations. To not use additional data, we generate fake OOD embeddings bysampling from low-likelihood regions of the ID data feature space,approximating the boundaries where OOD data are likely to reside. To thenfurther enhance robustness, we propose the use of an orthogonal binary layerfollowing the stable feature space, which maximizes the separation between theequilibrium points of ID and OOD samples. We validate our method throughextensive experiments across several benchmarks, demonstrating superiorperformance, particularly under adversarial attacks. Notably, our approachimproves robust detection performance from 37.8% to 80.1% on CIFAR-10 vs.CIFAR-100 and from 29.0% to 67.0% on CIFAR-100 vs. CIFAR-10.</description><author>Hossein Mirzaei, Mackenzie W. Mathis</author><pubDate>Mon, 14 Oct 2024 17:22:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10744v1</guid></item><item><title>NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models</title><link>http://arxiv.org/abs/2410.10743v1</link><description>Graphs are a fundamental data structure for representing relationships inreal-world scenarios. With the success of Large Language Models (LLMs) acrossvarious natural language processing (NLP) tasks, there has been growinginterest in integrating LLMs for graph learning. However, applying LLMs tograph-related tasks poses significant challenges, as these models are notinherently designed to capture the complex structural information present ingraphs. Existing approaches address this challenge through two strategies: thechain of tasks approach, which uses Graph Neural Networks (GNNs) to encode thegraph structure so that LLMs are relieved from understanding spatial positions;and Graph-to-Text Conversion, which translates graph structures into semantictext representations that LLMs can process. Despite their progress, thesemethods often struggle to fully preserve the topological information of graphsor require extensive computational resources, limiting their practicalapplicability. In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),a novel framework that efficiently encodes graph structures by selecting keynodes as anchors and representing each node based on its relative distance tothese anchors. This position-anchored encoding effectively captures the graphtopology, enabling enhanced reasoning capabilities in LLMs over graph data.Additionally, we implement a task-specific tuning procedure to further improvestructural understanding within LLMs. Through extensive empirical evaluations,NT-LLM demonstrates significant performance improvements across a variety ofgraph-related tasks.</description><author>Yanbiao Ji, Chang Liu, Xin Chen, Yue Ding, Dan Luo, Mei Li, Wenqing Lin, Hongtao Lu</author><pubDate>Mon, 14 Oct 2024 17:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10743v1</guid></item><item><title>SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing</title><link>http://arxiv.org/abs/2410.10741v1</link><description>Effective processing, interpretation, and management of sensor data haveemerged as a critical component of cyber-physical systems. Traditionally,processing sensor data requires profound theoretical knowledge and proficiencyin signal-processing tools. However, recent works show that Large LanguageModels (LLMs) have promising capabilities in processing sensory data,suggesting their potential as copilots for developing sensing systems. To explore this potential, we construct a comprehensive benchmark,SensorBench, to establish a quantifiable objective. The benchmark incorporatesdiverse real-world sensor datasets for various tasks. The results show thatwhile LLMs exhibit considerable proficiency in simpler tasks, they faceinherent challenges in processing compositional tasks with parameter selectionscompared to engineering experts. Additionally, we investigate four promptingstrategies for sensor processing and show that self-verification can outperformall other baselines in 48% of tasks. Our study provides a comprehensivebenchmark and prompting analysis for future developments, paving the way towardan LLM-based sensor processing copilot.</description><author>Pengrui Quan, Xiaomin Ouyang, Jeya Vikranth Jeyakumar, Ziqi Wang, Yang Xing, Mani Srivastava</author><pubDate>Mon, 14 Oct 2024 17:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10741v1</guid></item><item><title>Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs</title><link>http://arxiv.org/abs/2410.10739v1</link><description>Large Language Models (LLMs) for public use require continuous pre-trainingto remain up-to-date with the latest data. The models also need to befine-tuned with specific instructions to maintain their ability to followinstructions accurately. Typically, LLMs are released in two versions: the BaseLLM, pre-trained on diverse data, and the instruction-refined LLM, additionallytrained with specific instructions for better instruction following. Thequestion arises as to which model should undergo continuous pre-training tomaintain its instruction-following abilities while also staying current withthe latest data. In this study, we delve into the intricate relationshipbetween continuous pre-training and instruction fine-tuning of the LLMs andinvestigate the impact of continuous pre-training on the instruction followingabilities of both the base and its instruction finetuned model. Further, theinstruction fine-tuning process is computationally intense and requires asubstantial number of hand-annotated examples for the model to learneffectively. This study aims to find the most compute-efficient strategy togain up-to-date knowledge and instruction-following capabilities withoutrequiring any instruction data and fine-tuning. We empirically prove ourfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instructionmodels, providing a comprehensive exploration of our hypotheses across varyingsizes of pre-training data corpus and different LLMs settings.</description><author>Ishan Jindal, Chandana Badrinath, Pranjal Bharti, Lakkidi Vinay, Sachin Dev Sharma</author><pubDate>Mon, 14 Oct 2024 17:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10739v1</guid></item><item><title>DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model</title><link>http://arxiv.org/abs/2410.10738v1</link><description>Driving world models have gained increasing attention due to their ability tomodel complex physical dynamics. However, their superb modeling capability isyet to be fully unleashed due to the limited video diversity in current drivingdatasets. We introduce DrivingDojo, the first dataset tailor-made for traininginteractive world models with complex driving dynamics. Our dataset featuresvideo clips with a complete set of driving maneuvers, diverse multi-agentinterplay, and rich open-world driving knowledge, laying a stepping stone forfuture world model development. We further define an action instructionfollowing (AIF) benchmark for world models and demonstrate the superiority ofthe proposed dataset for generating action-controlled future predictions.</description><author>Yuqi Wang, Ke Cheng, Jiawei He, Qitai Wang, Hengchen Dai, Yuntao Chen, Fei Xia, Zhaoxiang Zhang</author><pubDate>Mon, 14 Oct 2024 17:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10738v1</guid></item><item><title>Online Statistical Inference for Time-varying Sample-averaged Q-learning</title><link>http://arxiv.org/abs/2410.10737v1</link><description>Reinforcement learning (RL) has emerged as a key approach for training agentsin complex and uncertain environments. Incorporating statistical inference inRL algorithms is essential for understanding and managing uncertainty in modelperformance. This paper introduces a time-varying batch-averaged Q-learningalgorithm, termed sampleaveraged Q-learning, which improves upon traditionalsingle-sample Q-learning by aggregating samples of rewards and next states tobetter account for data variability and uncertainty. We leverage the functionalcentral limit theorem (FCLT) to establish a novel framework that providesinsights into the asymptotic normality of the sample-averaged algorithm undermild conditions. Additionally, we develop a random scaling method for intervalestimation, enabling the construction of confidence intervals without requiringextra hyperparameters. Numerical experiments conducted on classic OpenAI Gymenvironments show that the time-varying sample-averaged Q-learning methodconsistently outperforms both single-sample and constant-batch Q-learningmethods, achieving superior accuracy while maintaining comparable learningspeeds.</description><author>Saunak Kumar Panda, Ruiqi Liu, Yisha Xiang</author><pubDate>Mon, 14 Oct 2024 17:17:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10737v1</guid></item><item><title>Towards Calibrated Losses for Adversarial Robust Reject Option Classification</title><link>http://arxiv.org/abs/2410.10736v1</link><description>Robustness towards adversarial attacks is a vital property for classifiers inseveral applications such as autonomous driving, medical diagnosis, etc. Also,in such scenarios, where the cost of misclassification is very high, knowingwhen to abstain from prediction becomes crucial. A natural question is whichsurrogates can be used to ensure learning in scenarios where the input pointsare adversarially perturbed and the classifier can abstain from prediction?This paper aims to characterize and design surrogates calibrated in"Adversarial Robust Reject Option" setting. First, we propose an adversarialrobust reject option loss $\ell_{d}^{\gamma}$ and analyze it for the hypothesisset of linear classifiers ($\mathcal{H}_{\textrm{lin}}$). Next, we provide acomplete characterization result for any surrogate to be$(\ell_{d}^{\gamma},\mathcal{H}_{\textrm{lin}})$- calibrated. To demonstratethe difficulty in designing surrogates to $\ell_{d}^{\gamma}$, we show negativecalibration results for convex surrogates and quasi-concave conditional riskcases (these gave positive calibration in adversarial setting without rejectoption). We also empirically argue that Shifted Double Ramp Loss (DRL) andShifted Double Sigmoid Loss (DSL) satisfy the calibration conditions. Finally,we demonstrate the robustness of shifted DRL and shifted DSL againstadversarial perturbations on a synthetically generated dataset.</description><author>Vrund Shah, Tejas Chaudhari, Naresh Manwani</author><pubDate>Mon, 14 Oct 2024 17:17:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10736v1</guid></item><item><title>Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning</title><link>http://arxiv.org/abs/2410.10735v1</link><description>Accurate mathematical reasoning with Large Language Models (LLMs) is crucialin revolutionizing domains that heavily rely on such reasoning. However, LLMsoften encounter difficulties in certain aspects of mathematical reasoning,leading to flawed reasoning and erroneous results. To mitigate these issues, weintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specificallydesigned to embed self-correction as an inherent ability in LLMs, enabling themto validate and rectify their own results. The CoSC mechanism operates througha sequence of self-correction stages. In each stage, the LLMs generate aprogram to address a given problem, execute this program using program-basedtools to obtain an output, subsequently verify this output. Based on theverification, the LLMs either proceed to the next correction stage or finalizethe answer. This iterative self-correction process allows the LLMs to refinetheir reasoning steps and improve the accuracy of their mathematical reasoning.To enable the CoSC mechanism at a low cost, we employ a two-phase finetuningapproach. In the first phase, the LLMs are trained with a relatively smallvolume of seeding data generated from GPT-4, establishing an initial CoSCcapability. In the second phase, the CoSC capability is further enhanced bytraining with a larger volume of self-generated data using the trained model inthe first phase, without relying on the paid GPT-4. Our comprehensiveexperiments demonstrate that CoSC significantly improves performance ontraditional mathematical datasets among existing open-source LLMs. Notably, ourCoSC-Code-34B model achieved a 53.5% score on MATH, the most challengingmathematical reasoning dataset in the public domain, surpassing the performanceof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMslike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra.</description><author>Kuofeng Gao, Huanqia Cai, Qingyao Shuai, Dihong Gong, Zhifeng Li</author><pubDate>Mon, 14 Oct 2024 17:16:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10735v1</guid></item><item><title>Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</title><link>http://arxiv.org/abs/2410.10733v1</link><description>We present Deep Compression Autoencoder (DC-AE), a new family of autoencodermodels for accelerating high-resolution diffusion models. Existing autoencodermodels have demonstrated impressive results at a moderate spatial compressionratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy forhigh spatial compression ratios (e.g., 64x). We address this challenge byintroducing two key techniques: (1) Residual Autoencoding, where we design ourmodels to learn residuals based on the space-to-channel transformed features toalleviate the optimization difficulty of high spatial-compression autoencoders;(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phasestraining strategy for mitigating the generalization penalty of highspatial-compression autoencoders. With these designs, we improve theautoencoder's spatial compression ratio up to 128 while maintaining thereconstruction quality. Applying our DC-AE to latent diffusion models, weachieve significant speedup without accuracy drop. For example, on ImageNet512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedupon H100 GPU for UViT-H while achieving a better FID, compared with the widelyused SD-VAE-f8 autoencoder. Our code is available athttps://github.com/mit-han-lab/efficientvit.</description><author>Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han</author><pubDate>Mon, 14 Oct 2024 17:15:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10733v1</guid></item><item><title>Multimodal MRI Accurately Identifies Amyloid Status in Unbalanced Cohorts in Alzheimer's Disease Continuum</title><link>http://arxiv.org/abs/2406.13305v2</link><description>Amyloid-$\beta$ (A$\beta$) plaques in conjunction with hyperphosphorylatedtau proteins in the form of neurofibrillary tangles are the twoneuropathological hallmarks of Alzheimer's disease. It is well-known that theidentification of individuals with A$\beta$ positivity could enable earlydiagnosis. In this work, we aim at capturing the A$\beta$ positivity status inan unbalanced cohort enclosing subjects at different disease stages, exploitingthe underlying structural and connectivity disease-induced modulations asrevealed by structural, functional, and diffusion MRI. Of note, due to theunbalanced cohort, the outcomes may be guided by those factors rather thanamyloid accumulation. The partial views provided by each modality areintegrated in the model allowing to take full advantage of theircomplementarity in encoding the effects of the A$\beta$ accumulation, leadingto an accuracy of $0.762\pm0.04$. The specificity of the information brought byeach modality is assessed by \textit{post-hoc} explainability analysis (guidedbackpropagation), highlighting the underlying structural and functionalchanges. Noteworthy, well-established biomarker key regions related to A$\beta$deposition could be identified by all modalities, including the hippocampus,thalamus, precuneus, and cingulate gyrus, witnessing in favor of thereliability of the method as well as its potential in shading light onmodality-specific possibly unknown A$\beta$ deposition signatures.</description><author>Giorgio Dolci, Charles A. Ellis, Federica Cruciani, Lorenza Brusini, Anees Abrol, Ilaria Boscolo Galazzo, Gloria Menegaz, Vince D. Calhoun</author><pubDate>Mon, 14 Oct 2024 17:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13305v2</guid></item><item><title>Towards LLM-guided Efficient and Interpretable Multi-linear Tensor Network Rank Selection</title><link>http://arxiv.org/abs/2410.10728v1</link><description>We propose a novel framework that leverages large language models (LLMs) toguide the rank selection in tensor network models for higher-order dataanalysis. By utilising the intrinsic reasoning capabilities and domainknowledge of LLMs, our approach offers enhanced interpretability of the rankchoices and can effectively optimise the objective function. This frameworkenables users without specialised domain expertise to utilise tensor networkdecompositions and understand the underlying rationale within the rankselection process. Experimental results validate our method on financialhigher-order datasets, demonstrating interpretable reasoning, stronggeneralisation to unseen test data, and its potential for self-enhancement oversuccessive iterations. This work is placed at the intersection of largelanguage models and higher-order data analysis.</description><author>Giorgos Iacovides, Wuyang Zhou, Danilo Mandic</author><pubDate>Mon, 14 Oct 2024 17:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10728v1</guid></item><item><title>A Counterexample in Image Registration</title><link>http://arxiv.org/abs/2410.10725v1</link><description>Image registration is a widespread problem which applies models about imagetransformation or image similarity to align discrete images of the same scene.Nevertheless, the theoretical limits on its accuracy are not understood even inthe case of one-dimensional data. Just as Nyquist's sampling theorem statesconditions for the perfect reconstruction of signals from samples, there arebounds to the quality of reproductions of quantized functions from sets ofideal, noiseless samples in the absence of additional assumptions. In this workwe estimate spatially-limited piecewise constant signals from two or more setsof noiseless sampling patterns. We mainly focus on the energy of the errorfunction and find that the uncertainties of the positions of the discontinuitypoints of the function depend on the discontinuity point selected as thereference point of the signal. As a consequence, the accuracy of the estimateof the signal depends on the reference point of that signal.</description><author>Serap A. Savari</author><pubDate>Mon, 14 Oct 2024 17:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10725v1</guid></item><item><title>Large Language Models Are Active Critics in NLG Evaluation</title><link>http://arxiv.org/abs/2410.10724v1</link><description>The conventional paradigm of using large language models (LLMs) forevaluating natural language generation (NLG) systems typically relies on twokey inputs: (1) a clear definition of the NLG task to be evaluated and (2) alist of pre-defined evaluation criteria. This process treats LLMs as ''passivecritics,'' strictly following human-defined criteria for evaluation. However,as new NLG tasks emerge, the criteria for assessing text quality can varygreatly. Consequently, these rigid evaluation methods struggle to adapt todiverse NLG tasks without extensive prompt engineering customized for eachspecific task. To address this limitation, we introduce Active-Critic, a novelLLM-based NLG evaluation protocol that enables LLMs to function as ''activecritics.'' Specifically, our protocol comprises two key stages. In the firststage, the LLM is instructed to infer the target NLG task and establishrelevant evaluation criteria from the data. Building on this self-inferredinformation, the second stage dynamically optimizes the prompt to guide the LLMtoward more human-aligned scoring decisions, while also generating detailedexplanations to justify its evaluations. Experiments across four NLG evaluationtasks show that our approach achieves stronger alignment with human judgmentsthan state-of-the-art evaluation methods. Our comprehensive analysis furtherhighlights the effectiveness and explainability of Active-Critic with only asmall amount of labeled data. We will share our code and data on GitHub.</description><author>Shuying Xu, Junjie Hu, Ming Jiang</author><pubDate>Mon, 14 Oct 2024 17:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10724v1</guid></item><item><title>Reducing the Barriers to Entry for Foundation Model Training</title><link>http://arxiv.org/abs/2404.08811v2</link><description>The world has recently witnessed an unprecedented acceleration in demands forMachine Learning and Artificial Intelligence applications. This spike in demandhas imposed tremendous strain on the underlying technology stack in supplychain, GPU-accelerated hardware, software, datacenter power density, and energyconsumption. If left on the current technological trajectory, future demandsshow insurmountable spending trends, further limiting market players, stiflinginnovation, and widening the technology gap. To address these challenges, wepropose a fundamental change in the AI training infrastructure throughout thetechnology ecosystem. The changes require advancements in supercomputing andnovel AI training approaches, from high-end software to low-level hardware,microprocessor, and chip design, while advancing the energy efficiency requiredby a sustainable infrastructure. This paper presents the analytical frameworkthat quantitatively highlights the challenges and points to the opportunitiesto reduce the barriers to entry for training large language models.</description><author>Paolo Faraboschi, Ellis Giles, Justin Hotard, Konstanty Owczarek, Andrew Wheeler</author><pubDate>Mon, 14 Oct 2024 17:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08811v2</guid></item><item><title>4-LEGS: 4D Language Embedded Gaussian Splatting</title><link>http://arxiv.org/abs/2410.10719v1</link><description>The emergence of neural representations has revolutionized our means fordigitally viewing a wide range of 3D scenes, enabling the synthesis ofphotorealistic images rendered from novel views. Recently, several techniqueshave been proposed for connecting these low-level representations with thehigh-level semantics understanding embodied within the scene. These methodselevate the rich semantic understanding from 2D imagery to 3D representations,distilling high-dimensional spatial features onto 3D space. In our work, we areinterested in connecting language with a dynamic modeling of the world. We showhow to lift spatio-temporal features to a 4D representation based on 3DGaussian Splatting. %, \gal{while introducing a feature-proximity attentionmechanism that allows for neighboring features in 3D space to interact}. Thisenables an interactive interface where the user can spatiotemporally localizeevents in the video from text prompts. We demonstrate our system on public 3Dvideo datasets of people and animals performing various actions.</description><author>Gal Fiebelman, Tamir Cohen, Ayellet Morgenstern, Peter Hedman, Hadar Averbuch-Elor</author><pubDate>Mon, 14 Oct 2024 17:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10719v1</guid></item><item><title>SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators</title><link>http://arxiv.org/abs/2410.10714v1</link><description>Large Language Models (LLMs) have transformed natural language processing,but face significant challenges in widespread deployment due to their highruntime cost. In this paper, we introduce SeedLM, a novel post-trainingcompression method that uses seeds of pseudo-random generators to encode andcompress model weights. Specifically, for each block of weights, we find a seedthat is fed into a Linear Feedback Shift Register (LFSR) during inference toefficiently generate a random matrix. This matrix is then linearly combinedwith compressed coefficients to reconstruct the weight block. SeedLM reducesmemory access and leverages idle compute cycles during inference, effectivelyspeeding up memory-bound tasks by trading compute for fewer memory accesses.Unlike state-of-the-art compression methods that rely on calibration data, ourapproach is data-free and generalizes well across diverse tasks. Ourexperiments with Llama 3 70B, which is particularly challenging to compress,show that SeedLM achieves significantly better zero-shot accuracy retention at4- and 3-bit than state-of-the-art techniques, while maintaining performancecomparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over anFP16 Llama 2/3 baseline.</description><author>Rasoul Shafipour, David Harrison, Maxwell Horton, Jeffrey Marker, Houman Bedayat, Sachin Mehta, Mohammad Rastegari, Mahyar Najibi, Saman Naderiparizi</author><pubDate>Mon, 14 Oct 2024 16:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10714v1</guid></item><item><title>Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents</title><link>http://arxiv.org/abs/2407.08516v5</link><description>This article explores the convergence of connectionist and symbolicartificial intelligence (AI), from historical debates to contemporaryadvancements. Traditionally considered distinct paradigms, connectionist AIfocuses on neural networks, while symbolic AI emphasizes symbolicrepresentation and logic. Recent advancements in large language models (LLMs),exemplified by ChatGPT and GPT-4, highlight the potential of connectionistarchitectures in handling human language as a form of symbols. The study arguesthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.By utilizing LLMs for text-based knowledge modeling and representation, LAAsintegrate neuro-symbolic AI principles, showcasing enhanced reasoning anddecision-making capabilities. Comparing LAAs with Knowledge Graphs within theneuro-symbolic AI theme highlights the unique strengths of LAAs in mimickinghuman-like reasoning processes, scaling effectively with large datasets, andleveraging in-context samples without explicit re-training. The researchunderscores promising avenues in neuro-vector-symbolic integration,instructional encoding, and implicit reasoning, aimed at further enhancing LAAcapabilities. By exploring the progression of neuro-symbolic AI and proposingfuture research trajectories, this work advances the understanding anddevelopment of AI technologies.</description><author>Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, Anwer Al-Dulaimi, Laura E. Barnes</author><pubDate>Mon, 14 Oct 2024 16:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08516v5</guid></item><item><title>Benefiting from Quantum? A Comparative Study of Q-Seg, Quantum-Inspired Techniques, and U-Net for Crack Segmentation</title><link>http://arxiv.org/abs/2410.10713v1</link><description>Exploring the potential of quantum hardware for enhancing classical andreal-world applications is an ongoing challenge. This study evaluates theperformance of quantum and quantum-inspired methods compared to classicalmodels for crack segmentation. Using annotated gray-scale image patches ofconcrete samples, we benchmark a classical mean Gaussian mixture technique, aquantum-inspired fermion-based method, Q-Seg a quantum annealing-based method,and a U-Net deep learning architecture. Our results indicate thatquantum-inspired and quantum methods offer a promising alternative for imagesegmentation, particularly for complex crack patterns, and could be applied innear-future applications.</description><author>Akshaya Srinivasan, Alexander Geng, Antonio Macaluso, Maximilian Kiefer-Emmanouilidis, Ali Moghiseh</author><pubDate>Mon, 14 Oct 2024 16:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10713v1</guid></item><item><title>Ensemble of ConvNeXt V2 and MaxViT for Long-Tailed CXR Classification with View-Based Aggregation</title><link>http://arxiv.org/abs/2410.10710v1</link><description>In this work, we present our solution for the MICCAI 2024 CXR-LT challenge,achieving 4th place in Subtask 2 and 5th in Subtask 1. We leveraged an ensembleof ConvNeXt V2 and MaxViT models, pretrained on an external chest X-raydataset, to address the long-tailed distribution of chest findings. Theproposed method combines state-of-the-art image classification techniques,asymmetric loss for handling class imbalance, and view-based predictionaggregation to enhance classification performance. Through experiments, wedemonstrate the advantages of our approach in improving both detection accuracyand the handling of the long-tailed distribution in CXR findings. The code isavailable at \url{https://github.com/yamagishi0824/cxrlt24-multiview-pp}.</description><author>Yosuke Yamagishi, SHouhei Hanaoka</author><pubDate>Mon, 14 Oct 2024 16:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10710v1</guid></item><item><title>Revisiting Few-Shot Object Detection with Vision-Language Models</title><link>http://arxiv.org/abs/2312.14494v4</link><description>The era of vision-language models (VLMs) trained on web-scale datasetschallenges conventional formulations of "open-world" perception. In this work,we revisit the task of few-shot object detection (FSOD) in the context ofrecent foundational VLMs. First, we point out that zero-shot predictions fromVLMs such as GroundingDINO significantly outperform state-of-the-art few-shotdetectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance,such foundation models may still be sub-optimal. For example, trucks on the webmay be defined differently from trucks for a target application such asautonomous vehicle perception. We argue that the task of few-shot recognitioncan be reformulated as aligning foundation models to target concepts using afew examples. Interestingly, such examples can be multi-modal, using both textand visual cues, mimicking instructions that are often given to humanannotators when defining a target concept of interest. Concretely, we proposeFoundational FSOD, a new benchmark protocol that evaluates detectorspre-trained on any external data and fine-tuned on multi-modal (text andvisual) K-shot examples per target class. We repurpose nuImages forFoundational FSOD, benchmark several popular open-source VLMs, and provide anempirical analysis of state-of-the-art methods. Lastly, we discuss our recentCVPR 2024 Foundational FSOD competition and share insights from the community.Notably, the winning team significantly outperforms our baseline by 23.3 mAP!Our code and dataset splits are available athttps://github.com/anishmadan23/foundational_fsod</description><author>Anish Madan, Neehar Peri, Shu Kong, Deva Ramanan</author><pubDate>Mon, 14 Oct 2024 16:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14494v4</guid></item><item><title>Early Diagnoses of Acute Lymphoblastic Leukemia Using YOLOv8 and YOLOv11 Deep Learning Models</title><link>http://arxiv.org/abs/2410.10701v1</link><description>Thousands of individuals succumb annually to leukemia alone. This studyexplores the application of image processing and deep learning techniques fordetecting Acute Lymphoblastic Leukemia (ALL), a severe form of blood cancerresponsible for numerous annual fatalities. As artificial intelligencetechnologies advance, the research investigates the reliability of thesemethods in real-world scenarios. The study focuses on recent developments inALL detection, particularly using the latest YOLO series models, to distinguishbetween malignant and benign white blood cells and to identify different stagesof ALL, including early stages. Additionally, the models are capable ofdetecting hematogones, which are often misclassified as ALL. By utilizingadvanced deep learning models like YOLOv8 and YOLOv11, the study achieves highaccuracy rates reaching 98.8%, demonstrating the effectiveness of thesealgorithms across multiple datasets and various real-world situations.</description><author>Alaa Awad, Mohamed Hegazy, Salah A. Aly</author><pubDate>Mon, 14 Oct 2024 16:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10701v1</guid></item><item><title>Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues</title><link>http://arxiv.org/abs/2410.10700v1</link><description>This study exposes the safety vulnerabilities of Large Language Models (LLMs)in multi-turn interactions, where malicious users can obscure harmful intentsacross several queries. We introduce ActorAttack, a novel multi-turn attackmethod inspired by actor-network theory, which models a network of semanticallylinked actors as attack clues to generate diverse and effective attack pathstoward harmful targets. ActorAttack addresses two main challenges in multi-turnattacks: (1) concealing harmful intents by creating an innocuous conversationtopic about the actor, and (2) uncovering diverse attack paths towards the sameharmful target by leveraging LLMs' knowledge to specify the correlated actorsas various attack clues. In this way, ActorAttack outperforms existingsingle-turn and multi-turn attack methods across advanced aligned LLMs, evenfor GPT-o1. We will publish a dataset called SafeMTData, which includesmulti-turn adversarial prompts and safety alignment data, generated byActorAttack. We demonstrate that models safety-tuned using our safety datasetare more robust to multi-turn attacks. Code is available athttps://github.com/renqibing/ActorAttack.</description><author>Qibing Ren, Hao Li, Dongrui Liu, Zhanxu Xie, Xiaoya Lu, Yu Qiao, Lei Sha, Junchi Yan, Lizhuang Ma, Jing Shao</author><pubDate>Mon, 14 Oct 2024 16:41:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10700v1</guid></item><item><title>Fast Convergence of $Φ$-Divergence Along the Unadjusted Langevin Algorithm and Proximal Sampler</title><link>http://arxiv.org/abs/2410.10699v1</link><description>We study the mixing time of two popular discrete time Markov chains incontinuous space, the unadjusted Langevin algorithm and the proximal sampler,which are discretizations of the Langevin dynamics. We extend mixing timeanalyses for these Markov chains to hold in $\Phi$-divergence. We show that any$\Phi$-divergence arising from a twice-differentiable strictly convex function$\Phi$ converges to $0$ exponentially fast along these Markov chains, under theassumption that their stationary distributions satisfies the corresponding$\Phi$-Sobolev inequality. Our rates of convergence are tight and include asspecial cases popular mixing time regimes, namely the mixing in chi-squareddivergence under a Poincar\'e inequality, and the mixing in relative entropyunder a log-Sobolev inequality. Our results follow by bounding the contractioncoefficients arising in the appropriate strong data processing inequalities.</description><author>Siddharth Mitra, Andre Wibisono</author><pubDate>Mon, 14 Oct 2024 16:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10699v1</guid></item><item><title>Dimension-free uniform concentration bound for logistic regression</title><link>http://arxiv.org/abs/2405.18055v5</link><description>We provide a novel dimension-free uniform concentration bound for theempirical risk function of constrained logistic regression. Our bound yields amilder sufficient condition for a uniform law of large numbers than conditionsderived by the Rademacher complexity argument and McDiarmid's inequality. Thederivation is based on the PAC-Bayes approach with second-order expansion andRademacher-complexity-based bounds for the residual term of the expansion.</description><author>Shogo Nakakita</author><pubDate>Mon, 14 Oct 2024 16:40:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18055v5</guid></item><item><title>TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model</title><link>http://arxiv.org/abs/2410.10696v1</link><description>Recently, 2D speaking avatars have increasingly participated in everydayscenarios due to the fast development of facial animation techniques. However,most existing works neglect the explicit control of human bodies. In thispaper, we propose to drive not only the faces but also the torso and gesturemovements of a speaking figure. Inspired by recent advances in diffusionmodels, we propose the Motion-Enhanced Textural-Aware ModeLing for SpeaKingAvatar Reenactment (TALK-Act) framework, which enables high-fidelity avatarreenactment from only short footage of monocular video. Our key idea is toenhance the textural awareness with explicit motion guidance in diffusionmodeling. Specifically, we carefully construct 2D and 3D structural informationas intermediate guidance. While recent diffusion models adopt a side networkfor control information injection, they fail to synthesize temporally stableresults even with person-specific fine-tuning. We propose a Motion-EnhancedTextural Alignment module to enhance the bond between driving and targetsignals. Moreover, we build a Memory-based Hand-Recovering module to help withthe difficulties in hand-shape preserving. After pre-training, our model canachieve high-fidelity 2D avatar reenactment with only 30 seconds ofperson-specific data. Extensive experiments demonstrate the effectiveness andsuperiority of our proposed framework. Resources can be found athttps://guanjz20.github.io/projects/TALK-Act.</description><author>Jiazhi Guan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Shengyi He, Zhiliang Xu, Haocheng Feng, Errui Ding, Jingdong Wang, Hongtao Xie, Youjian Zhao, Ziwei Liu</author><pubDate>Mon, 14 Oct 2024 16:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10696v1</guid></item><item><title>The Future of Large Language Model Pre-training is Federated</title><link>http://arxiv.org/abs/2405.10853v3</link><description>Generative pre-trained large language models (LLMs) have demonstratedimpressive performance over a wide range of tasks, thanks to the unprecedentedamount of data they have been trained on. As established scaling laws indicate,LLMs' future performance improvement depends on the amount of computing anddata sources they can leverage for pre-training. Federated learning (FL) hasthe potential to unleash the majority of the planet's data and computationalresources, which are underutilized by the data-center-focused trainingmethodology of current LLM practice. Our work presents a robust, flexible,reproducible FL approach that enables large-scale collaboration acrossinstitutions to train LLMs. We propose a scalable deployment system calledPhoton to enable the investigation and development of this new trainingparadigm for LLM pre-training. We show that Photon can be used by organizationsinterested in collaborating with their private data sources and computationalresources for pre-training LLMs with billions of parameters. This paradigmwould mobilize more computational and data resources while matching orpotentially exceeding centralized performance. We further show theeffectiveness of the federated training scales with model size and present ourapproach for training billion-scale federated LLMs using limited resources.Thus far, we have used Photon to train LLM models to the size of 7B parametersand anticipate larger models being completed in the near future. Finally, weshow that LLM training is highly resilient to the classical challenges offederated statistical and hardware heterogeneity. Furthermore, we show thatconvergence is robust to partial participation, opening the avenue forcompute-efficient collaborative training. Photon will help data-rich actors tobecome the protagonists of LLMs pre-training instead of leaving the stage tocompute-rich actors alone.</description><author>Lorenzo Sani, Alex Iacob, Zeyu Cao, Bill Marino, Yan Gao, Tomas Paulik, Wanru Zhao, William F. Shen, Preslav Aleksandrov, Xinchi Qiu, Nicholas D. Lane</author><pubDate>Mon, 14 Oct 2024 16:37:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10853v3</guid></item><item><title>Enhancing Performance of Point Cloud Completion Networks with Consistency Loss</title><link>http://arxiv.org/abs/2410.07298v2</link><description>Point cloud completion networks are conventionally trained to minimize thedisparities between the completed point cloud and the ground-truth counterpart.However, an incomplete object-level point cloud can have multiple validcompletion solutions when it is examined in isolation. This one-to-many mappingissue can cause contradictory supervision signals to the network because theloss function may produce different values for identical input-output pairs ofthe network. In many cases, this issue could adversely affect the networkoptimization process. In this work, we propose to enhance the conventionallearning objective using a novel completion consistency loss to mitigate theone-to-many mapping problem. Specifically, the proposed consistency loss ensurethat a point cloud completion network generates a coherent completion solutionfor incomplete objects originating from the same source point cloud.Experimental results across multiple well-established datasets and benchmarksdemonstrated the proposed completion consistency loss have excellent capabilityto enhance the completion performance of various existing networks without anymodification to the design of the networks. The proposed consistency lossenhances the performance of the point completion network without affecting theinference speed, thereby increasing the accuracy of point cloud completion.Notably, a state-of-the-art point completion network trained with the proposedconsistency loss can achieve state-of-the-art accuracy on the challenging newMVP dataset. The code and result of experiment various point completion modelsusing proposed consistency loss will be available at:https://github.com/kaist-avelab/ConsistencyLoss .</description><author>Christofel Rio Goenawan, Kevin Tirta Wijaya, Seung-Hyun Kong</author><pubDate>Mon, 14 Oct 2024 16:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07298v2</guid></item><item><title>C-Learner: Constrained Learning for Causal Inference and Semiparametric Statistics</title><link>http://arxiv.org/abs/2405.09493v3</link><description>Popular debiased causal estimation methods, e.g. for the average treatmenteffect -- such as one-step estimation (e.g., augmented inverse propensityweighting) and targeted maximum likelihood estimation -- enjoy desirableasymptotic properties such as statistical efficiency and double robustness.However, they often produce unstable estimates when there is limited overlapbetween treatment and control, and require ad hoc adjustments in practice(e.g., truncating propensity scores). In contrast, simple plug-in estimatorsare stable but lack good asymptotic properties. We propose a novel debiasedestimator that achieves the best of both worlds, producing stable plug-inestimates with desirable asymptotic properties. Our constrained learningframework solves for the best plug-in estimator under the constraint that thefirst-order error with respect to the plugged-in quantity is zero, and canleverage flexible model classes including neural networks and tree ensembles.In several experimental settings, including ones in which we handle text-basedcovariates by fine-tuning language models, our constrained learning-basedestimator outperforms one-step estimation and targeting in challenging settingswith limited overlap between treatment and control, and performs comparablyotherwise.</description><author>Tiffany Tianhui Cai, Yuri Fonseca, Kaiwen Hou, Hongseok Namkoong</author><pubDate>Mon, 14 Oct 2024 16:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09493v3</guid></item></channel></rss>