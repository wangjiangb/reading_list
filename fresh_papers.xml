<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 04 Jul 2023 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Online Heavy-tailed Change-point detection</title><link>http://arxiv.org/abs/2306.09548v2</link><description>We study algorithms for online change-point detection (OCPD), where samplesthat are potentially heavy-tailed, are presented one at a time and a change inthe underlying mean must be detected as early as possible. We present analgorithm based on clipped Stochastic Gradient Descent (SGD), that works evenif we only assume that the second moment of the data generating process isbounded. We derive guarantees on worst-case, finite-sample false-positive rate(FPR) over the family of all distributions with bounded second moment. Thus,our method is the first OCPD algorithm that guarantees finite-sample FPR, evenif the data is high dimensional and the underlying distributions areheavy-tailed. The technical contribution of our paper is to show thatclipped-SGD can estimate the mean of a random vector and simultaneously provideconfidence bounds at all confidence values. We combine this robust estimatewith a union bound argument and construct a sequential change-point algorithmwith finite-sample FPR guarantees. We show empirically that our algorithm workswell in a variety of situations, whether the underlying data are heavy-tailed,light-tailed, high dimensional or discrete. No other algorithm achieves boundedFPR theoretically or empirically, over all settings we study simultaneously.</description><author>Abishek Sankararaman, Balakrishnan, Narayanaswamy</author><pubDate>Mon, 03 Jul 2023 18:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09548v2</guid></item><item><title>ELQA: A Corpus of Metalinguistic Questions and Answers about English</title><link>http://arxiv.org/abs/2205.00395v2</link><description>We present ELQA, a corpus of questions and answers in and about the Englishlanguage. Collected from two online forums, the &gt;70k questions (from Englishlearners and others) cover wide-ranging topics including grammar, meaning,fluency, and etymology. The answers include descriptions of general propertiesof English vocabulary and grammar as well as explanations about specific(correct and incorrect) usage examples. Unlike most NLP datasets, this corpusis metalinguistic -- it consists of language about language. As such, it canfacilitate investigations of the metalinguistic capabilities of NLU models, aswell as educational applications in the language learning domain. To studythis, we define a free-form question answering task on our dataset and conductevaluations on multiple LLMs (Large Language Models) to analyze their capacityto generate metalinguistic answers.</description><author>Shabnam Behzad, Keisuke Sakaguchi, Nathan Schneider, Amir Zeldes</author><pubDate>Mon, 03 Jul 2023 18:42:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.00395v2</guid></item><item><title>Neural Algorithmic Reasoning with Causal Regularisation</title><link>http://arxiv.org/abs/2302.10258v2</link><description>Recent work on neural algorithmic reasoning has investigated the reasoningcapabilities of neural networks, effectively demonstrating they can learn toexecute classical algorithms on unseen data coming from the train distribution.However, the performance of existing neural reasoners significantly degrades onout-of-distribution (OOD) test data, where inputs have larger sizes. In thiswork, we make an important observation: there are many different inputs forwhich an algorithm will perform certain intermediate computations identically.This insight allows us to develop data augmentation procedures that, given analgorithm's intermediate trajectory, produce inputs for which the targetalgorithm would have exactly the same next trajectory step. We ensureinvariance in the next-step prediction across such inputs, by employing aself-supervised objective derived by our observation, formalised in a causalgraph. We prove that the resulting method, which we call Hint-ReLIC, improvesthe OOD generalisation capabilities of the reasoner. We evaluate our method onthe CLRS algorithmic reasoning benchmark, where we show up to 3$\times$improvements on the OOD test data.</description><author>Beatrice Bevilacqua, Kyriacos Nikiforou, Borja Ibarz, Ioana Bica, Michela Paganini, Charles Blundell, Jovana Mitrovic, Petar Veličković</author><pubDate>Mon, 03 Jul 2023 18:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10258v2</guid></item><item><title>Lower Complexity Adaptation for Empirical Entropic Optimal Transport</title><link>http://arxiv.org/abs/2306.13580v2</link><description>Entropic optimal transport (EOT) presents an effective and computationallyviable alternative to unregularized optimal transport (OT), offering diverseapplications for large-scale data analysis. In this work, we derive novelstatistical bounds for empirical plug-in estimators of the EOT cost and showthat their statistical performance in the entropy regularization parameter$\epsilon$ and the sample size $n$ only depends on the simpler of the twoprobability measures. For instance, under sufficiently smooth costs this yieldsthe parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is theminimum dimension of the two population measures. This confirms that empiricalEOT also adheres to the lower complexity adaptation principle, a hallmarkfeature only recently identified for unregularized OT. As a consequence of ourtheory, we show that the empirical entropic Gromov-Wasserstein distance and itsunregularized version for measures on Euclidean spaces also obey thisprinciple. Additionally, we comment on computational aspects and complement ourfindings with Monte Carlo simulations. Our techniques employ empirical processtheory and rely on a dual formulation of EOT over a single function class.Crucial to our analysis is the observation that the entropiccost-transformation of a function class does not increase its uniform metricentropy by much.</description><author>Michel Groppe, Shayan Hundrieser</author><pubDate>Mon, 03 Jul 2023 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13580v2</guid></item><item><title>Statler: State-Maintaining Language Models for Embodied Reasoning</title><link>http://arxiv.org/abs/2306.17840v2</link><description>Large language models (LLMs) provide a promising tool that enable robots toperform complex robot reasoning tasks. However, the limited context window ofcontemporary LLMs makes reasoning over long time horizons difficult. Embodiedtasks such as those that one might expect a household robot to performtypically require that the planner consider information acquired a long timeago (e.g., properties of the many objects that the robot previously encounteredin the environment). Attempts to capture the world state using an LLM'simplicit internal representation is complicated by the paucity of task- andenvironment-relevant information available in a robot's action history, whilemethods that rely on the ability to convey information via the prompt to theLLM are subject to its limited context window. In this paper, we proposeStatler, a framework that endows LLMs with an explicit representation of theworld state as a form of ``memory'' that is maintained over time. Integral toStatler is its use of two instances of general LLMs -- a world-model reader anda world-model writer -- that interface with and maintain the world state. Byproviding access to this world state ``memory'', Statler improves the abilityof existing LLMs to reason over longer time horizons without the constraint ofcontext length. We evaluate the effectiveness of our approach on threesimulated table-top manipulation domains and a real robot domain, and show thatit improves the state-of-the-art in LLM-based robot reasoning. Project website:https://statler-lm.github.io/</description><author>Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, Matthew R. Walter</author><pubDate>Mon, 03 Jul 2023 17:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17840v2</guid></item><item><title>SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design</title><link>http://arxiv.org/abs/2306.15656v2</link><description>This paper introduces SparseOptimizer, a novel deep learning optimizer thatexploits Moreau-Yosida regularization to naturally induce sparsity in largelanguage models such as BERT, ALBERT and GPT. Key to the design ofSparseOptimizer is an embedded shrinkage operator, which imparts sparsitydirectly within the optimization process. This operator, backed by a soundtheoretical framework, includes an analytical solution, thereby reinforcing theoptimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-playfunctionality eradicates the need for code modifications, making it auniversally adaptable tool for a wide array of large language models. Empiricalevaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2confirm that SparseBERT and SparseALBERT, when sparsified usingSparseOptimizer, achieve performance comparable to their dense counterparts,BERT and ALBERT, while significantly reducing their parameter count. Further,this work proposes an innovative optimizer-compiler co-design strategy,demonstrating the potential of inference acceleration (\textbf{3.37x},\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, andLLVM generic compile, respectively) in SparseBERT when paired with anappropriately designed compiler. This study represents a significant stepforward in the evolution of efficient, scalable, and high-performing largelanguage models, setting a precedent for future exploration and optimization inthis domain. The SparseOptimizer code and SparseALBERT model will be publiclyavailable upon paper acceptance.</description><author>Fu-Ming Guo</author><pubDate>Mon, 03 Jul 2023 17:25:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15656v2</guid></item><item><title>Networked Time Series Prediction with Incomplete Data</title><link>http://arxiv.org/abs/2110.02271v2</link><description>A networked time series (NETS) is a family of time series on a given graph,one for each node. It has found a wide range of applications from intelligenttransportation, environment monitoring to mobile network management. Animportant task in such applications is to predict the future values of a NETSbased on its historical values and the underlying graph. Most existing methodsrequire complete data for training. However, in real-world scenarios, it is notuncommon to have missing data due to sensor malfunction, incomplete sensingcoverage, etc. In this paper, we study the problem of NETS prediction withincomplete data. We propose NETS-ImpGAN, a novel deep learning framework thatcan be trained on incomplete data with missing values in both history andfuture. Furthermore, we propose novel Graph Temporal Attention Networks byincorporating the attention mechanism to capture both inter-time seriescorrelations and temporal correlations. We conduct extensive experiments onthree real-world datasets under different missing patterns and missing rates.The experimental results show that NETS-ImpGAN outperforms existing methodsexcept when data exhibit very low variance, in which case NETS-ImpGAN stillachieves competitive performance.</description><author>Yichen Zhu, Mengtian Zhang, Bo Jiang, Haiming Jin, Jianqiang Huang, Xinbing Wang</author><pubDate>Mon, 03 Jul 2023 17:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.02271v2</guid></item><item><title>Increasing Fairness via Combination with Learning Guarantees</title><link>http://arxiv.org/abs/2301.10813v2</link><description>The concern about underlying discrimination hidden in ML models isincreasing, as ML systems have been widely applied in more and more real-worldscenarios and any discrimination hidden in them will directly affect humanlife. Many techniques have been developed to enhance fairness includingcommonly-used group fairness measures and several fairness-aware methodscombining ensemble learning. However, existing fairness measures can only focuson one aspect -- either group or individual fairness, and the hardcompatibility among them indicates a possibility of remaining biases even ifone of them is satisfied. Moreover, existing mechanisms to boost fairnessusually present empirical results to show validity, yet few of them discusswhether fairness can be boosted with certain theoretical guarantees. To addressthese issues, we propose a fairness quality measure named discriminative riskin this paper to reflect both individual and group fairness aspects.Furthermore, we investigate the properties of the proposed measure and proposefirst- and second-order oracle bounds to show that fairness can be boosted viaensemble combination with theoretical learning guarantees. Note that theanalysis is suitable for both binary and multi-class classification. A pruningmethod is also proposed to utilise our proposed measure and comprehensiveexperiments are conducted to evaluate the effectiveness of the proposed methodsin this paper.</description><author>Yijun Bian, Kun Zhang, Anqi Qiu</author><pubDate>Mon, 03 Jul 2023 17:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10813v2</guid></item><item><title>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</title><link>http://arxiv.org/abs/2306.15195v2</link><description>In human conversations, individuals can indicate relevant regions within ascene while addressing others. In turn, the other person can then respond byreferring to specific regions if necessary. This natural referential ability indialogue remains absent in current Multimodal Large Language Models (MLLMs). Tofill this gap, this paper proposes an MLLM called Shikra, which can handlespatial coordinate inputs and outputs in natural language. Its architectureconsists of a vision encoder, an alignment layer, and a LLM. It is designed tobe straightforward and simple, without the need for extra vocabularies,position encoder, pre-/post-detection modules, or external plug-in models. Allinputs and outputs are in natural language form. Referential dialogue is asuperset of various vision-language (VL) tasks. Shikra can naturally handlelocation-related tasks like REC and PointQA, as well as conventional VL taskssuch as Image Captioning and VQA. Experimental results showcase Shikra'spromising performance. Furthermore, it enables numerous exciting applications,like providing mentioned objects' coordinates in chains of thoughts andcomparing user-pointed regions similarities. Our code, model and dataset areaccessed at https://github.com/shikras/shikra.</description><author>Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao</author><pubDate>Mon, 03 Jul 2023 17:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15195v2</guid></item><item><title>Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge</title><link>http://arxiv.org/abs/2210.05723v2</link><description>Various neural network architectures rely on pooling operators to aggregateinformation coming from different sources. It is often implicitly assumed insuch contexts that vectors encode epistemic states, i.e. that vectors capturethe evidence that has been obtained about some properties of interest, and thatpooling these vectors yields a vector that combines this evidence. We study,for a number of standard pooling operators, under what conditions they arecompatible with this idea, which we call the epistemic pooling principle. Whilewe find that all the considered pooling operators can satisfy the epistemicpooling principle, this only holds when embeddings are sufficientlyhigh-dimensional and, for most pooling operators, when the embeddings satisfyparticular constraints (e.g. having non-negative coordinates). We furthermoreshow that these constraints have important implications on how the embeddingscan be used in practice. In particular, we find that when the epistemic poolingprinciple is satisfied, in most cases it is impossible to verify thesatisfaction of propositional formulas using linear scoring functions, with twoexceptions: (i) max-pooling with embeddings that are upper-bounded and (ii)Hadamard pooling with non-negative embeddings. This finding helps to clarify,among others, why Graph Neural Networks sometimes under-perform in reasoningtasks. Finally, we also study an extension of the epistemic pooling principleto weighted epistemic states, which are important in the context ofnon-monotonic reasoning, where max-pooling emerges as the most suitableoperator.</description><author>Steven Schockaert</author><pubDate>Mon, 03 Jul 2023 16:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05723v2</guid></item><item><title>EVD Surgical Guidance with Retro-Reflective Tool Tracking and Spatial Reconstruction using Head-Mounted Augmented Reality Device</title><link>http://arxiv.org/abs/2306.15490v2</link><description>Augmented Reality (AR) has been used to facilitate surgical guidance duringExternal Ventricular Drain (EVD) surgery, reducing the risks of misplacement inmanual operations. During this procedure, the key challenge is accuratelyestimating the spatial relationship between pre-operative images and actualpatient anatomy in AR environment. This research proposes a novel frameworkutilizing Time of Flight (ToF) depth sensors integrated in commerciallyavailable AR Head Mounted Devices (HMD) for precise EVD surgical guidance. Asprevious studies have proven depth errors for ToF sensors, we first assessedtheir properties on AR-HMDs. Subsequently, a depth error model andpatient-specific parameter identification method are introduced for accuratesurface information. A tracking pipeline combining retro-reflective markers andpoint clouds is then proposed for accurate head tracking. The head surface isreconstructed using depth data for spatial registration, avoiding fixingtracking targets rigidly on the patient's skull. Firstly, $7.580\pm 1.488 mm$depth value error was revealed on human skin, indicating the significance ofdepth correction. Our results showed that the error was reduced by over $85\%$using proposed depth correction method on head phantoms in different materials.Meanwhile, the head surface reconstructed with corrected depth data achievedsub-millimetre accuracy. An experiment on sheep head revealed $0.79 mm$reconstruction error. Furthermore, a user study was conducted for theperformance in simulated EVD surgery, where five surgeons performed nine k-wireinjections on a head phantom with virtual guidance. Results of this studyrevealed $2.09 \pm 0.16 mm$ translational accuracy and $2.97\pm 0.91$ degreeorientational accuracy.</description><author>Haowei Li, Wenqing Yan, Du Liu, Long Qian, Yuxing Yang, Yihao Liu, Zhe Zhao, Hui Ding, Guangzhi Wang</author><pubDate>Mon, 03 Jul 2023 16:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15490v2</guid></item><item><title>A Survey on Generative Diffusion Model</title><link>http://arxiv.org/abs/2209.02646v9</link><description>Deep generative models are a prominent approach for data generation, and havebeen used to produce high quality samples in various domains. Diffusion models,an emerging class of deep generative models, have attracted considerableattention owing to their exceptional generative quality. Despite this, theyhave certain limitations, including a time-consuming iterative generationprocess and confinement to high-dimensional Euclidean space. This surveypresents a plethora of advanced techniques aimed at enhancing diffusion models,including sampling acceleration and the design of new diffusion processes. Inaddition, we delve into strategies for implementing diffusion models inmanifold and discrete spaces, maximum likelihood training for diffusion models,and methods for creating bridges between two arbitrary distributions. Theinnovations we discuss represent the efforts for improving the functionalityand efficiency of diffusion models in recent years. To examine the efficacy ofexisting models, a benchmark of FID score, IS, and NLL is presented in aspecific NFE. Furthermore, diffusion models are found to be useful in variousdomains such as computer vision, audio, sequence modeling, and AI for science.The paper concludes with a summary of this field, along with existinglimitations and future directions. Summation of existing well-classifiedmethods is in our Github:https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model</description><author>Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li</author><pubDate>Mon, 03 Jul 2023 16:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.02646v9</guid></item><item><title>Spatio-Angular Convolutions for Super-resolution in Diffusion MRI</title><link>http://arxiv.org/abs/2306.00854v2</link><description>Diffusion MRI (dMRI) is a widely used imaging modality, but requires longscanning times to acquire high resolution datasets. By leveraging the uniquegeometry present within this domain, we present a novel approach to dMRIangular super-resolution that extends upon the parametric continuousconvolution (PCConv) framework. We introduce several additions to the operationincluding a Fourier feature mapping, global coordinates, and domain specificcontext. Using this framework, we build a fully parametric continuousconvolution network (PCCNN) and compare against existing models. We demonstratethe PCCNN performs competitively while using significantly less parameters.Moreover, we show that this formulation generalises well to clinically relevantdownstream analyses such as fixel-based analysis, and neurite orientationdispersion and density imaging.</description><author>Matthew Lyon, Paul Armitage, Mauricio A Álvarez</author><pubDate>Mon, 03 Jul 2023 16:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00854v2</guid></item><item><title>ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis</title><link>http://arxiv.org/abs/2306.04527v2</link><description>Domain generalization is critical for real-world applications of machinelearning models to microscopy images, including histopathology and fluorescenceimaging. Artifacts in histopathology arise through a complex combination offactors relating to tissue collection and laboratory processing, as well asfactors intrinsic to patient samples. In fluorescence imaging, these artifactsstem from variations across experimental batches. The complexity and subtletyof these artifacts make the enumeration of data domains intractable. Therefore,augmentation-based methods of domain generalization that require domainidentifiers and manual fine-tuning are inadequate in this setting. To overcomethis challenge, we introduce ContriMix, a domain generalization technique thatlearns to generate synthetic images by disentangling and permuting thebiological content ("content") and technical variations ("attributes") inmicroscopy images. ContriMix does not rely on domain identifiers or handcraftedaugmentations and makes no assumptions about the input characteristics ofimages. We assess the performance of ContriMix on two pathology datasets(Camelyon17-WILDS and a prostate cell classification dataset) and onefluorescence microscopy dataset (RxRx1-WILDS). ContriMix outperforms currentstate-of-the-art methods in all datasets, motivating its usage for microscopyimage analysis in real-world settings where domain information is hard to comeby.</description><author>Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah, Chintan Shah, Sai Chowdary Gullapally, Michael Griffin, Anand Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner</author><pubDate>Mon, 03 Jul 2023 16:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04527v2</guid></item><item><title>Generalized iterated-sums signatures</title><link>http://arxiv.org/abs/2012.04597v3</link><description>We explore the algebraic properties of a generalized version of theiterated-sums signature, inspired by previous work of F.~Kir\'aly andH.~Oberhauser. In particular, we show how to recover the character property ofthe associated linear map over the tensor algebra by considering a deformedquasi-shuffle product of words on the latter. We introduce three non-lineartransformations on iterated-sums signatures, close in spirit to MachineLearning applications, and show some of their properties.</description><author>Joscha Diehl, Kurusch Ebrahimi-Fard, Nikolas Tapia</author><pubDate>Mon, 03 Jul 2023 16:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.04597v3</guid></item><item><title>Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation</title><link>http://arxiv.org/abs/2305.07609v2</link><description>The remarkable achievements of Large Language Models (LLMs) have led to theemergence of a novel recommendation paradigm -- Recommendation via LLM(RecLLM). Nevertheless, it is important to note that LLMs may contain socialprejudices, and therefore, the fairness of recommendations made by RecLLMrequires further investigation. To avoid the potential risks of RecLLM, it isimperative to evaluate the fairness of RecLLM with respect to various sensitiveattributes on the user side. Due to the differences between the RecLLM paradigmand the traditional recommendation paradigm, it is problematic to directly usethe fairness benchmark of traditional recommendation. To address the dilemma,we propose a novel benchmark called Fairness of Recommendation via LLM(FaiRLLM). This benchmark comprises carefully crafted metrics and a datasetthat accounts for eight sensitive attributes1 in two recommendation scenarios:music and movies. By utilizing our FaiRLLM benchmark, we conducted anevaluation of ChatGPT and discovered that it still exhibits unfairness to somesensitive attributes when generating recommendations. Our code and dataset canbe found at https://github.com/jizhi-zhang/FaiRLLM.</description><author>Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He</author><pubDate>Mon, 03 Jul 2023 16:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07609v2</guid></item><item><title>Variations of Squeeze and Excitation networks</title><link>http://arxiv.org/abs/2304.06502v2</link><description>Convolutional neural networks learns spatial features and are heavilyinterlinked within kernels. The SE module have broken the traditional route ofneural networks passing the entire result to next layer. Instead SE only passesimportant features to be learned with its squeeze and excitation (SE) module.We propose variations of the SE module which improvises the process of squeezeand excitation and enhances the performance. The proposed squeezing or excitingthe layer makes it possible for having a smooth transition of layer weights.These proposed variations also retain the characteristics of SE module. Theexperimented results are carried out on residual networks and the results aretabulated.</description><author>Mahendran NV</author><pubDate>Mon, 03 Jul 2023 16:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06502v2</guid></item><item><title>Att-KGCN: Tourist Attractions Recommendation System by using Attention mechanism and Knowledge Graph Convolution Network</title><link>http://arxiv.org/abs/2306.10946v4</link><description>The recommendation algorithm based on knowledge graphs is at a relativelymature stage. However, there are still some problems in the recommendation ofspecific areas. For example, in the tourism field, selecting suitable touristattraction attributes process is complicated as the recommendation basis fortourist attractions. In this paper, we propose the improved Attention KnowledgeGraph Convolution Network model, named ($Att-KGCN$), which automaticallydiscovers the neighboring entities of the target scenic spot semantically. Theattention layer aggregates relatively similar locations and represents themwith an adjacent vector. Then, according to the tourist's preferred choices,the model predicts the probability of similar spots as a recommendation system.A knowledge graph dataset of tourist attractions used based on tourism data onSocotra Island-Yemen. Through experiments, it is verified that the AttentionKnowledge Graph Convolution Network has a good effect on the recommendation oftourist attractions and can make more recommendations for tourists' choices.</description><author>Ahmad A. Mubarak, JingJing Li, Han Cao</author><pubDate>Mon, 03 Jul 2023 16:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10946v4</guid></item><item><title>Improving Online Continual Learning Performance and Stability with Temporal Ensembles</title><link>http://arxiv.org/abs/2306.16817v2</link><description>Neural networks are very effective when trained on large datasets for a largenumber of iterations. However, when they are trained on non-stationary streamsof data and in an online fashion, their performance is reduced (1) by theonline setup, which limits the availability of data, (2) due to catastrophicforgetting because of the non-stationary nature of the data. Furthermore,several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.13452showed that replay methods used in continual learning suffer from the stabilitygap, encountered when evaluating the model continually (rather than only ontask boundaries). In this article, we study the effect of model ensembling as away to improve performance and stability in online continual learning. Wenotice that naively ensembling models coming from a variety of training tasksincreases the performance in online continual learning considerably. Startingfrom this observation, and drawing inspirations from semi-supervised learningensembling methods, we use a lightweight temporal ensemble that computes theexponential moving average of the weights (EMA) at test time, and show that itcan drastically increase the performance and stability when used in combinationwith several methods from the literature.</description><author>Albin Soutif--Cormerais, Antonio Carta, Joost Van de Weijer</author><pubDate>Mon, 03 Jul 2023 15:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16817v2</guid></item><item><title>DataCI: A Platform for Data-Centric AI on Streaming Data</title><link>http://arxiv.org/abs/2306.15538v2</link><description>We introduce DataCI, a comprehensive open-source platform designedspecifically for data-centric AI in dynamic streaming data settings. DataCIprovides 1) an infrastructure with rich APIs for seamless streaming datasetmanagement, data-centric pipeline development and evaluation on streamingscenarios, 2) an carefully designed versioning control function to track thepipeline lineage, and 3) an intuitive graphical interface for a betterinteractive user experience. Preliminary studies and demonstrations attest tothe easy-to-use and effectiveness of DataCI, highlighting its potential torevolutionize the practice of data-centric AI in streaming data contexts.</description><author>Huaizheng Zhang, Yizheng Huang, Yuanming Li</author><pubDate>Mon, 03 Jul 2023 15:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15538v2</guid></item><item><title>A unified stochastic approximation framework for learning in games</title><link>http://arxiv.org/abs/2206.03922v2</link><description>We develop a flexible stochastic approximation framework for analyzing thelong-run behavior of learning in games (both continuous and finite). Theproposed analysis template incorporates a wide array of popular learningalgorithms, including gradient-based methods, the exponential/multiplicativeweights algorithm for learning in finite games, optimistic and bandit variantsof the above, etc. In addition to providing an integrated view of thesealgorithms, our framework further allows us to obtain several new convergenceresults, both asymptotic and in finite time, in both continuous and finitegames. Specifically, we provide a range of criteria for identifying classes ofNash equilibria and sets of action profiles that are attracting with highprobability, and we also introduce the notion of coherence, a game-theoreticproperty that includes strict and sharp equilibria, and which leads toconvergence in finite time. Importantly, our analysis applies to bothoracle-based and bandit, payoff-based methods - that is, when players onlyobserve their realized payoffs.</description><author>Panayotis Mertikopoulos, Ya-Ping Hsieh, Volkan Cevher</author><pubDate>Mon, 03 Jul 2023 15:51:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03922v2</guid></item><item><title>Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task</title><link>http://arxiv.org/abs/2211.05039v2</link><description>We introduce a challenging decision-making task that we call activeacquisition for multimodal temporal data (A2MT). In many real-world scenarios,input features are not readily available at test time and must instead beacquired at significant cost. With A2MT, we aim to learn agents that activelyselect which modalities of an input to acquire, trading off acquisition costand predictive performance. A2MT extends a previous task called active featureacquisition to temporal decision making about high-dimensional inputs. Wepropose a method based on the Perceiver IO architecture to address A2MT inpractice. Our agents are able to solve a novel synthetic scenario requiringpractically relevant cross-modal reasoning skills. On two large-scale,real-world datasets, Kinetics-700 and AudioSet, our agents successfully learncost-reactive acquisition behavior. However, an ablation reveals they areunable to learn adaptive acquisition strategies, emphasizing the difficulty ofthe task even for state-of-the-art models. Applications of A2MT may beimpactful in domains like medicine, robotics, or finance, where modalitiesdiffer in acquisition cost and informativeness.</description><author>Jannik Kossen, Cătălina Cangea, Eszter Vértes, Andrew Jaegle, Viorica Patraucean, Ira Ktena, Nenad Tomasev, Danielle Belgrave</author><pubDate>Mon, 03 Jul 2023 15:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05039v2</guid></item><item><title>A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design</title><link>http://arxiv.org/abs/2306.11768v3</link><description>Structure-based drug design (SBDD), which utilizes the three-dimensionalgeometry of proteins to identify potential drug candidates, is becomingincreasingly vital in drug discovery. However, traditional methods based onphysiochemical modeling and experts' domain knowledge are time-consuming andlaborious. The recent advancements in geometric deep learning, which integratesand processes 3D geometric data, coupled with the availability of accurateprotein 3D structure predictions from tools like AlphaFold, have significantlypropelled progress in structure-based drug design. In this paper, wesystematically review the recent progress of geometric deep learning forstructure-based drug design. We start with a brief discussion of the mainstreamtasks in structure-based drug design, commonly used 3D protein representationsand representative predictive/generative models. Then we delve into detailedreviews for each task (binding site prediction, binding pose generation,\emph{de novo} molecule generation, linker design, and binding affinityprediction), including the problem setup, representative methods, datasets, andevaluation metrics. Finally, we conclude this survey with the currentchallenges and highlight potential opportunities of geometric deep learning forstructure-based drug design.We curate a GitHub repo containing the relatedpapers \url{https://github.com/zaixizhang/Awesome-SBDD}.</description><author>Zaixi Zhang, Jiaxian Yan, Qi Liu, Enhong Chen</author><pubDate>Mon, 03 Jul 2023 15:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11768v3</guid></item><item><title>IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages</title><link>http://arxiv.org/abs/2212.10180v2</link><description>The rapid growth of machine translation (MT) systems has necessitatedcomprehensive studies to meta-evaluate evaluation metrics being used, whichenables a better selection of metrics that best reflect MT quality.Unfortunately, most of the research focuses on high-resource languages, mainlyEnglish, the observations for which may not always apply to other languages.Indian languages, having over a billion speakers, are linguistically differentfrom English, and to date, there has not been a systematic study of evaluatingMT systems from English into Indian languages. In this paper, we fill this gapby creating an MQM dataset consisting of 7000 fine-grained annotations,spanning 5 Indian languages and 7 MT systems, and use it to establishcorrelations between annotator scores and scores obtained using existingautomatic metrics. Our results show that pre-trained metrics, such as COMET,have the highest correlations with annotator scores. Additionally, we find thatthe metrics do not adequately capture fluency-based errors in Indian languages,and there is a need to develop metrics focused on Indian languages. We hopethat our dataset and analysis will help promote further research in this area.</description><author>Ananya B. Sai, Vignesh Nagarajan, Tanay Dixit, Raj Dabre, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra</author><pubDate>Mon, 03 Jul 2023 15:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10180v2</guid></item><item><title>Shapley Curves: A Smoothing Perspective</title><link>http://arxiv.org/abs/2211.13289v3</link><description>Originating from cooperative game theory, Shapley values have become one ofthe most widely used measures for variable importance in applied MachineLearning. However, the statistical understanding of Shapley values is stilllimited. In this paper, we take a nonparametric (or smoothing) perspective byintroducing Shapley curves as a local measure of variable importance. Wepropose two estimation strategies and derive the consistency and asymptoticnormality both under independence and dependence among the features. Thisallows us to construct confidence intervals and conduct inference on theestimated Shapley curves. We propose a novel version of the wild bootstrapprocedure, specifically adjusted to give good finite sample coverage of theShapley curves. The asymptotic results are validated in extensive experiments.In an empirical application, we analyze which attributes drive the prices ofvehicles.</description><author>Ratmir Miftachov, Georg Keilbar, Wolfgang Karl Härdle</author><pubDate>Mon, 03 Jul 2023 15:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13289v3</guid></item><item><title>Zero-Shot Cross-Lingual Summarization via Large Language Models</title><link>http://arxiv.org/abs/2302.14229v3</link><description>Given a document in a source language, cross-lingual summarization (CLS) aimsto generate a summary in a different target language. Recently, the emergenceof Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, hasattracted wide attention from the computational linguistics community. However,it is not yet known the performance of LLMs on CLS. In this report, weempirically use various prompts to guide LLMs to perform zero-shot CLS fromdifferent paradigms (i.e., end-to-end and pipeline), and provide a preliminaryevaluation on the generated summaries. We find that ChatGPT and GPT-4originally prefer to produce lengthy summaries with detailed information. Thesetwo LLMs can further balance informativeness and conciseness with the help ofan interactive prompt, significantly improving their CLS performance.Experimental results on three widely-used CLS datasets show that GPT-4 achievesstate-of-the-art zero-shot CLS performance, and performs competitively comparedwith the fine-tuned mBART-50. Moreover, we also find some multi-lingual andbilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limitedzero-shot CLS ability. Due to the composite nature of CLS, which requiresmodels to perform summarization and translation simultaneously, accomplishingthis task in a zero-shot manner is even a challenge for LLMs. Therefore, wesincerely hope and recommend future LLM research could use CLS as a testbed.</description><author>Jiaan Wang, Yunlong Liang, Fandong Meng, Beiqi Zou, Zhixu Li, Jianfeng Qu, Jie Zhou</author><pubDate>Mon, 03 Jul 2023 15:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14229v3</guid></item><item><title>Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets</title><link>http://arxiv.org/abs/2301.12139v2</link><description>We investigate five English NLP benchmark datasets (on the superGLUEleaderboard) and two Swedish datasets for bias, along multiple axes. Thedatasets are the following: Boolean Question (Boolq), CommitmentBank (CB),Winograd Schema Challenge (WSC), Wino-gender diagnostic (AXg), RecognisingTextual Entailment (RTE), Swedish CB, and SWEDN. Bias can be harmful and it isknown to be common in data, which ML models learn from. In order to mitigatebias in data, it is crucial to be able to estimate it objectively. We usebipol, a novel multi-axes bias metric with explainability, to estimate andexplain how much bias exists in these datasets. Multilingual, multi-axes biasevaluation is not very common. Hence, we also contribute a new, large Swedishbias-labelled dataset (of 2 million samples), translated from the Englishversion and train the SotA mT5 model on it. In addition, we contribute newmulti-axes lexica for bias detection in Swedish. We make the codes, model, andnew dataset publicly available.</description><author>Tosin Adewumi, Isabella Södergren, Lama Alkhaled, Sana Sabah Sabry, Foteini Liwicki, Marcus Liwicki</author><pubDate>Mon, 03 Jul 2023 15:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12139v2</guid></item><item><title>From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning</title><link>http://arxiv.org/abs/2302.12559v2</link><description>We study differentially private (DP) machine learning algorithms as instancesof noisy fixed-point iterations, in order to derive privacy and utility resultsfrom this well-studied framework. We show that this new perspective recoverspopular private gradient-based methods like DP-SGD and provides a principledway to design and analyze new private optimization algorithms in a flexiblemanner. Focusing on the widely-used Alternating Directions Method ofMultipliers (ADMM) method, we use our general framework to derive novel privateADMM algorithms for centralized, federated and fully decentralized learning.For these three algorithms, we establish strong privacy guarantees leveragingprivacy amplification by iteration and by subsampling. Finally, we provideutility guarantees using a unified analysis that exploits a recent linearconvergence result for noisy fixed-point iterations.</description><author>Edwige Cyffers, Aurélien Bellet, Debabrota Basu</author><pubDate>Mon, 03 Jul 2023 14:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12559v2</guid></item><item><title>Rethinking the U-Net, ResUnet, and U-Net3+ architectures with dual skip connections for building footprint extraction</title><link>http://arxiv.org/abs/2303.09064v3</link><description>The importance of building footprints and their inventory has been recognisedas foundational spatial information for multiple societal problems. Extractingcomplex urban buildings involves the segmentation of very high-resolution (VHR)earth observation (EO) images. U-Net is a common deep learning network andfoundation for its new incarnations like ResUnet, U-Net++ and U-Net3+ for suchsegmentation. The re-incarnations look for efficiency gain by re-designing theskip connection component and exploiting the multi-scale features in U-Net.However, skip connections do not always improve these networks and removingsome of them provides efficiency gains and reduced network parameters. In thispaper, we propose three dual skip connection mechanisms for U-Net, ResUnet, andU-Net3+. These mechanisms deepen the feature maps forwarded by the skipconnections and allow us to study which skip connections need to be denser toyield the highest efficiency gain. The mechanisms are evaluated on feature mapsof different scales in the three networks, producing nine new networkconfigurations. The networks are evaluated against their original vanillaversions using four building footprint datasets (three existing and one new) ofdifferent spatial resolutions: VHR (0.3m), high-resolution (1m and 1.2m), andmulti-resolution (0.3+0.6+1.2m). The proposed mechanisms report efficiency gainon four evaluation measures for U-Net and ResUnet, and up to 17.7% and 18.4%gain in F1 score and Intersection over Union (IoU) for U-Net3+. The codes willbe available in a GitHub link after peer review.</description><author>Bipul Neupane, Jagannath Aryal, Abbas Rajabifard</author><pubDate>Mon, 03 Jul 2023 14:46:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09064v3</guid></item><item><title>Deep Direct Discriminative Decoders for High-dimensional Time-series Data Analysis</title><link>http://arxiv.org/abs/2205.10947v2</link><description>The state-space models (SSMs) are widely utilized in the analysis oftime-series data. SSMs rely on an explicit definition of the state andobservation processes. Characterizing these processes is not always easy andbecomes a modeling challenge when the dimension of observed data grows or theobserved data distribution deviates from the normal distribution. Here, wepropose a new formulation of SSM for high-dimensional observation processes. Wecall this solution the deep direct discriminative decoder (D4). The D4 bringsdeep neural networks' expressiveness and scalability to the SSM formulationletting us build a novel solution that efficiently estimates the underlyingstate processes through high-dimensional observation signal. We demonstrate theD4 solutions in simulated and real data such as Lorenz attractors, Langevindynamics, random walk dynamics, and rat hippocampus spiking neural data andshow that the D4 performs better than traditional SSMs and RNNs. The D4 can beapplied to a broader class of time-series data where the connection betweenhigh-dimensional observation and the underlying latent process is hard tocharacterize.</description><author>Mohammad R. Rezaei, Milos R. Popovic, Milad Lankarany, Ali Yousefi</author><pubDate>Mon, 03 Jul 2023 14:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10947v2</guid></item><item><title>Extensible Motion-based Identification of XR Users using Non-Specific Motion Data</title><link>http://arxiv.org/abs/2302.07517v4</link><description>In this paper, we combine the strengths of distance-based andclassification-based approaches for the task of identifying extended realityusers by their movements. For this we explore an embedding-based model thatleverages deep metric learning. We train the model on a dataset of usersplaying the VR game ``Half-Life: Alyx'' and conduct multiple experiments andanalyses using a state of the art classification-based model as baseline. Theresults show that the embedding-based method 1) is able to identify new usersfrom non-specific movements using only a few minutes of enrollment data, 2) canenroll new users within seconds, while retraining the baseline approach takesalmost a day, 3) is more reliable than the baseline approach when only littleenrollment data is available, 4) can be used to identify new users from anotherdataset recorded with different VR devices. Altogether, our solution is a foundation for easily extensible XR useridentification systems, applicable to a wide range of user motions. It alsopaves the way for production-ready models that could be used by XRpractitioners without the requirements of expertise, hardware, or data fortraining deep learning models.</description><author>Christian Rack, Konstantin Kobs, Tamara Fernando, Andreas Hotho, Marc Erich Latoschik</author><pubDate>Mon, 03 Jul 2023 14:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07517v4</guid></item><item><title>Derandomized Novelty Detection with FDR Control via Conformal E-values</title><link>http://arxiv.org/abs/2302.07294v2</link><description>Conformal inference provides a general distribution-free method to rigorouslycalibrate the output of any machine learning algorithm for novelty detection.While this approach has many strengths, it has the limitation of beingrandomized, in the sense that it may lead to different results when analyzingtwice the same data, and this can hinder the interpretation of any findings. Wepropose to make conformal inferences more stable by leveraging suitableconformal e-values instead of p-values to quantify statistical significance.This solution allows the evidence gathered from multiple analyses of the samedata to be aggregated effectively while provably controlling the falsediscovery rate. Further, we show that the proposed method can reduce randomnesswithout much loss of power compared to standard conformal inference, partlythanks to an innovative way of weighting conformal e-values based on additionalside information carefully extracted from the same data. Simulations withsynthetic and real data confirm this solution can be effective at eliminatingrandom noise in the inferences obtained with state-of-the-art alternativetechniques, sometimes also leading to higher power.</description><author>Meshi Bashari, Amir Epstein, Yaniv Romano, Matteo Sesia</author><pubDate>Mon, 03 Jul 2023 14:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07294v2</guid></item><item><title>Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism</title><link>http://arxiv.org/abs/2305.18438v3</link><description>In this paper, we study offline Reinforcement Learning with Human Feedback(RLHF) where we aim to learn the human's underlying reward and the MDP'soptimal policy from a set of trajectories induced by human choices. RLHF ischallenging for multiple reasons: large state space but limited human feedback,the bounded rationality of human decisions, and the off-policy distributionshift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model formodeling and understanding human choices. DCC, rooted in econometrics anddecision theory, is widely used to model a human decision-making process withforward-looking and bounded rationality. We propose a\underline{D}ynamic-\underline{C}hoice-\underline{P}essimistic-\underline{P}olicy-\underline{O}ptimization(DCPPO) method. \ The method involves a three-stage process: The first step isto estimate the human behavior policy and the state-action value function viamaximum likelihood estimation (MLE); the second step recovers the human rewardfunction via minimizing Bellman mean squared error using the learned valuefunctions; the third step is to plug in the learned reward and invokepessimistic value iteration for finding a near-optimal policy. With onlysingle-policy coverage (i.e., optimal policy) of the dataset, we prove that thesuboptimality of DCPPO almost matches the classical pessimistic offline RLalgorithm in terms of suboptimality's dependency on distribution shift anddimension. To the best of our knowledge, this paper presents the firsttheoretical guarantees for off-policy offline RLHF with dynamic discrete choicemodel.</description><author>Zihao Li, Zhuoran Yang, Mengdi Wang</author><pubDate>Mon, 03 Jul 2023 14:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18438v3</guid></item><item><title>Cracking Double-Blind Review: Authorship Attribution with Deep Learning</title><link>http://arxiv.org/abs/2211.07467v3</link><description>Double-blind peer review is considered a pillar of academic research becauseit is perceived to ensure a fair, unbiased, and fact-centered scientificdiscussion. Yet, experienced researchers can often correctly guess from whichresearch group an anonymous submission originates, biasing the peer-reviewprocess. In this work, we present a transformer-based, neural-networkarchitecture that only uses the text content and the author names in thebibliography to attribute an anonymous manuscript to an author. To train andevaluate our method, we created the largest authorship identification datasetto date. It leverages all research papers publicly available on arXiv amountingto over 2 million manuscripts. In arXiv-subsets with up to 2,000 differentauthors, our method achieves an unprecedented authorship attribution accuracy,where up to 73% of papers are attributed correctly. We present a scalinganalysis to highlight the applicability of the proposed method to even largerdatasets when sufficient compute capabilities are more widely available to theacademic community. Furthermore, we analyze the attribution accuracy insettings where the goal is to identify all authors of an anonymous manuscript.Thanks to our method, we are not only able to predict the author of ananonymous work, but we also provide empirical evidence of the key aspects thatmake a paper attributable. We have open-sourced the necessary tools toreproduce our experiments.</description><author>Leonard Bauersfeld, Angel Romero, Manasi Muglikar, Davide Scaramuzza</author><pubDate>Mon, 03 Jul 2023 13:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07467v3</guid></item><item><title>Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation</title><link>http://arxiv.org/abs/2306.17115v2</link><description>We present a novel alignment-before-generation approach to tackle thechallenging task of generating general 3D shapes based on 2D images or texts.Directly learning a conditional generative model from images or texts to 3Dshapes is prone to producing inconsistent results with the conditions because3D shapes have an additional dimension whose distribution significantly differsfrom that of 2D images and texts. To bridge the domain gap among the threemodalities and facilitate multi-modal-conditioned 3D shape generation, weexplore representing 3D shapes in a shape-image-text-aligned space. Ourframework comprises two models: a Shape-Image-Text-Aligned VariationalAuto-Encoder (SITA-VAE) and a conditional Aligned Shape Latent Diffusion Model(ASLDM). The former model encodes the 3D shapes into the shape latent spacealigned to the image and text and reconstructs the fine-grained 3D neuralfields corresponding to given shape embeddings via the transformer-baseddecoder. The latter model learns a probabilistic mapping function from theimage or text space to the latent shape space. Our extensive experimentsdemonstrate that our proposed approach can generate higher-quality and morediverse 3D shapes that better semantically conform to the visual or texturalconditional inputs, validating the effectiveness of theshape-image-text-aligned space for cross-modality 3D shape generation.</description><author>Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, Shenghua Gao</author><pubDate>Mon, 03 Jul 2023 13:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17115v2</guid></item><item><title>Bivariate vine copula based regression, bivariate level and quantile curves</title><link>http://arxiv.org/abs/2205.02557v2</link><description>The statistical analysis of univariate quantiles is a well developed researchtopic. However, there is a need for research in multivariate quantiles. Weconstruct bivariate (conditional) quantiles using the level curves of vinecopula based bivariate regression model. Vine copulas are graph theoreticalmodels identified by a sequence of linked trees, which allow for separatemodelling of marginal distributions and the dependence structure. We introducea novel graph structure model (given by a tree sequence) specifically designedfor a symmetric treatment of two responses in a predictive regression setting.We establish computational tractability of the model and a straight forward wayof obtaining different conditional distributions. Using vine copulas thetypical shortfalls of regression, as the need for transformations orinteractions of predictors, collinearity or quantile crossings are avoided. Weillustrate the copula based bivariate level curves for different copuladistributions and show how they can be adjusted to form valid quantile curves.We apply our approach to weather measurements from Seoul, Korea. This dataexample emphasizes the benefits of the joint bivariate response modelling incontrast to two separate univariate regressions or by assuming conditionalindependence, for bivariate response data set in the presence of conditionaldependence.</description><author>Marija Tepegjozova, Claudia Czado</author><pubDate>Mon, 03 Jul 2023 13:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.02557v2</guid></item><item><title>TextSLAM: Visual SLAM with Semantic Planar Text Features</title><link>http://arxiv.org/abs/2305.10029v2</link><description>We propose a novel visual SLAM method that integrates text objects tightly bytreating them as semantic features via fully exploring their geometric andsemantic prior. The text object is modeled as a texture-rich planar patch whosesemantic meaning is extracted and updated on the fly for better dataassociation. With the full exploration of locally planar characteristics andsemantic meaning of text objects, the SLAM system becomes more accurate androbust even under challenging conditions such as image blurring, largeviewpoint changes, and significant illumination variations (day and night). Wetested our method in various scenes with the ground truth data. The resultsshow that integrating texture features leads to a more superior SLAM systemthat can match images across day and night. The reconstructed semantic 3D textmap could be useful for navigation and scene understanding in robotic and mixedreality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .</description><author>Boying Li, Danping Zou, Yuan Huang, Xinghan Niu, Ling Pei, Wenxian Yu</author><pubDate>Mon, 03 Jul 2023 13:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10029v2</guid></item><item><title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding</title><link>http://arxiv.org/abs/2304.14005v2</link><description>Although 3D-aware GANs based on neural radiance fields have achievedcompetitive performance, their applicability is still limited to objects orscenes with the ground-truths or prediction models for clearly definedcanonical camera poses. To extend the scope of applicable datasets, we proposea novel 3D-aware GAN optimization technique through contrastive learning withimplicit pose embeddings. To this end, we first revise the discriminator designand remove dependency on ground-truth camera poses. Then, to capture complexand challenging 3D scene structures more effectively, we make the discriminatorestimate a high-dimensional implicit pose embedding from a given image andperform contrastive learning on the pose embedding. The proposed approach canbe employed for the dataset, where the canonical camera pose is ill-definedbecause it does not look up or estimate camera poses. Experimental results showthat our algorithm outperforms existing methods by large margins on thedatasets with multiple object categories and inconsistent canonical cameraposes.</description><author>Mijeong Kim, Hyunjoon Lee, Bohyung Han</author><pubDate>Mon, 03 Jul 2023 12:34:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14005v2</guid></item><item><title>Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition</title><link>http://arxiv.org/abs/2304.01117v3</link><description>Symbolic regression searches for analytic expressions that accuratelydescribe studied phenomena. The main attraction of this approach is that itreturns an interpretable model that can be insightful to users. Historically,the majority of algorithms for symbolic regression have been based onevolutionary algorithms. However, there has been a recent surge of newproposals that instead utilize approaches such as enumeration algorithms, mixedlinear integer programming, neural networks, and Bayesian optimization. Inorder to assess how well these new approaches behave on a set of commonchallenges often faced in real-world data, we hosted a competition at the 2022Genetic and Evolutionary Computation Conference consisting of differentsynthetic and real-world datasets which were blind to entrants. For thereal-world track, we assessed interpretability in a realistic way by using adomain expert to judge the trustworthiness of candidate models.We present anin-depth analysis of the results obtained in this competition, discuss currentchallenges of symbolic regression algorithms and highlight possibleimprovements for future competitions.</description><author>F. O. de Franca, M. Virgolin, M. Kommenda, M. S. Majumder, M. Cranmer, G. Espada, L. Ingelse, A. Fonseca, M. Landajuela, B. Petersen, R. Glatt, N. Mundhenk, C. S. Lee, J. D. Hochhalter, D. L. Randall, P. Kamienny, H. Zhang, G. Dick, A. Simon, B. Burlacu, Jaan Kasak, Meera Machado, Casper Wilstrup, W. G. La Cava</author><pubDate>Mon, 03 Jul 2023 12:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01117v3</guid></item><item><title>LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection</title><link>http://arxiv.org/abs/2306.17408v2</link><description>As malicious actors employ increasingly advanced and widespread bots todisseminate misinformation and manipulate public opinion, the detection ofTwitter bots has become a crucial task. Though graph-based Twitter botdetection methods achieve state-of-the-art performance, we find that theirinference depends on the neighbor users multi-hop away from the targets, andfetching neighbors is time-consuming and may introduce bias. At the same time,we find that after finetuning on Twitter bot detection, pretrained languagemodels achieve competitive performance and do not require a graph structureduring deployment. Inspired by this finding, we propose a novel bot detectionframework LMBot that distills the knowledge of graph neural networks (GNNs)into language models (LMs) for graph-less deployment in Twitter bot detectionto combat the challenge of data dependency. Moreover, LMBot is compatible withgraph-based and graph-less datasets. Specifically, we first represent each useras a textual sequence and feed them into the LM for domain adaptation. Forgraph-based datasets, the output of LMs provides input features for the GNN,enabling it to optimize for bot detection and distill knowledge back to the LMin an iterative, mutually enhancing process. Armed with the LM, we can performgraph-less inference, which resolves the graph data dependency and samplingbias issues. For datasets without graph structure, we simply replace the GNNwith an MLP, which has also shown strong performance. Our experimentsdemonstrate that LMBot achieves state-of-the-art performance on four Twitterbot detection benchmarks. Extensive studies also show that LMBot is morerobust, versatile, and efficient compared to graph-based Twitter bot detectionmethods.</description><author>Zijian Cai, Zhaoxuan Tan, Zhenyu Lei, Zifeng Zhu, Hongrui Wang, Qinghua Zheng, Minnan Luo</author><pubDate>Mon, 03 Jul 2023 12:22:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17408v2</guid></item><item><title>Super-Resolution of BVOC Maps by Adapting Deep Learning Methods</title><link>http://arxiv.org/abs/2302.07570v4</link><description>Biogenic Volatile Organic Compounds (BVOCs) play a critical role inbiosphere-atmosphere interactions, being a key factor in the physical andchemical properties of the atmosphere and climate. Acquiring large andfine-grained BVOC emission maps is expensive and time-consuming, so mostavailable BVOC data are obtained on a loose and sparse sampling grid or onsmall regions. However, high-resolution BVOC data are desirable in manyapplications, such as air quality, atmospheric chemistry, and climatemonitoring. In this work, we investigate the possibility of enhancing BVOCacquisitions, further explaining the relationships between the environment andthese compounds. We do so by comparing the performances of severalstate-of-the-art neural networks proposed for image Super-Resolution (SR),adapting them to overcome the challenges posed by the large dynamic range ofthe emission and reduce the impact of outliers in the prediction. Moreover, wealso consider realistic scenarios, considering both temporal and geographicalconstraints. Finally, we present possible future developments regarding SRgeneralization, considering the scale-invariance property and super-resolvingemissions from unseen compounds.</description><author>Antonio Giganti, Sara Mandelli, Paolo Bestagini, Marco Marcon, Stefano Tubaro</author><pubDate>Mon, 03 Jul 2023 12:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07570v4</guid></item><item><title>A HRNet-based Rehabilitation Monitoring System</title><link>http://arxiv.org/abs/2306.10756v3</link><description>The rehabilitation treatment helps to heal minor sports and occupationalinjuries. In a traditional rehabilitation process, a therapist will assigncertain actions to a patient to perform in between hospital visits, and it willrely on the patient to remember actions correctly and the schedule to performthem. Unfortunately, many patients forget to perform actions or fail to recallactions in detail. As a consequence, the rehabilitation treatment is hamperedor, in the worst case, the patient may suffer from additional injury caused byperforming incorrect actions. To resolve these issues, we propose a HRNet-basedrehabilitation monitoring system, which can remind a patient when to performthe actions and display the actions for the patient to follow via the patient'ssmartphone. In addition, it helps the therapist to monitor the progress of therehabilitation for the patient. Our system consists of an iOS app and severalcomponents at the server side. The app is in charge of displaying andcollecting action videos. The server computes the similarity score between thetherapist's actions and the patient's in the videos to keep track of the numberof repetitions of each action. Theses stats will be shown to both of thepatient and therapist. The extensive experiments show that the F1-Score of thesimilarity calculation is as high as 0.9 and the soft accuracy of the number ofrepetitions is higher than 90%.</description><author>Yi-Ching Hung, Yu-Qing Jiang, Fong-Syuan Liou, Yu-Hsuan Tsao, Zi-Cing Chiang, MIn-Te Sun</author><pubDate>Mon, 03 Jul 2023 11:36:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10756v3</guid></item><item><title>V-LoL: A Diagnostic Dataset for Visual Logical Learning</title><link>http://arxiv.org/abs/2306.07743v2</link><description>Despite the successes of recent developments in visual AI, differentshortcomings still exist; from missing exact logical reasoning, to abstractgeneralization abilities, to understanding complex and noisy scenes.Unfortunately, existing benchmarks, were not designed to capture more than afew of these aspects. Whereas deep learning datasets focus on visually complexdata but simple visual reasoning tasks, inductive logic datasets involvecomplex logical learning tasks, however, lack the visual component. To addressthis, we propose the visual logical learning dataset, V-LoL, that seamlesslycombines visual and logical challenges. Notably, we introduce the firstinstantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classicbenchmark in symbolic AI, the Michalski train problem. By incorporatingintricate visual scenes and flexible logical reasoning tasks within a versatileframework, V-LoL-Trains provides a platform for investigating a wide range ofvisual logical learning challenges. We evaluate a variety of AI systemsincluding traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Ourevaluations demonstrate that even state-of-the-art AI faces difficulties indealing with visual logical learning challenges, highlighting unique advantagesand limitations specific to each methodology. Overall, V-LoL opens up newavenues for understanding and enhancing current abilities in visual logicallearning for AI systems.</description><author>Lukas Helff, Wolfgang Stammer, Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting</author><pubDate>Mon, 03 Jul 2023 11:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07743v2</guid></item><item><title>Separable Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2306.15969v2</link><description>Physics-informed neural networks (PINNs) have recently emerged as promisingdata-driven PDE solvers showing encouraging results on various PDEs. However,there is a fundamental limitation of training PINNs to solve multi-dimensionalPDEs and approximate highly complex solution functions. The number of trainingpoints (collocation points) required on these challenging PDEs growssubstantially, but it is severely limited due to the expensive computationalcosts and heavy memory overhead. To overcome this issue, we propose a networkarchitecture and training algorithm for PINNs. The proposed method, separablePINN (SPINN), operates on a per-axis basis to significantly reduce the numberof network propagations in multi-dimensional PDEs unlike point-wise processingin conventional PINNs. We also propose using forward-mode automaticdifferentiation to reduce the computational cost of computing PDE residuals,enabling a large number of collocation points (&gt;10^7) on a single commodityGPU. The experimental results show drastically reduced computational costs (62xin wall-clock time, 1,394x in FLOPs given the same number of collocationpoints) in multi-dimensional PDEs while achieving better accuracy. Furthermore,we present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equationsignificantly faster than the best-performing prior method (9 minutes vs 10hours in a single GPU), maintaining accuracy. Finally, we showcase that SPINNcan accurately obtain the solution of a highly nonlinear and multi-dimensionalPDE, a (3+1)-d Navier-Stokes equation. For visualized results and code, pleasesee https://jwcho5576.github.io/spinn.github.io/.</description><author>Junwoo Cho, Seungtae Nam, Hyunmo Yang, Seok-Bae Yun, Youngjoon Hong, Eunbyung Park</author><pubDate>Mon, 03 Jul 2023 11:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15969v2</guid></item><item><title>Language-agnostic Code-Switching in Sequence-To-Sequence Speech Recognition</title><link>http://arxiv.org/abs/2210.08992v2</link><description>Code-Switching (CS) is referred to the phenomenon of alternately using wordsand phrases from different languages. While today's neural end-to-end (E2E)models deliver state-of-the-art performances on the task of automatic speechrecognition (ASR) it is commonly known that these systems are verydata-intensive. However, there is only a few transcribed and aligned CS speechavailable. To overcome this problem and train multilingual systems which cantranscribe CS speech, we propose a simple yet effective data augmentation inwhich audio and corresponding labels of different source languages areconcatenated. By using this training data, our E2E model improves ontranscribing CS speech. It also surpasses monolingual models on monolingualtests. The results show that this augmentation technique can even improve themodel's performance on inter-sentential language switches not seen duringtraining by 5,03% WER.</description><author>Enes Yavuz Ugan, Christian Huber, Juan Hussain, Alexander Waibel</author><pubDate>Mon, 03 Jul 2023 11:01:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08992v2</guid></item><item><title>Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2203.00259v2</link><description>Density-based and classification-based methods have ruled unsupervisedanomaly detection in recent years, while reconstruction-based methods arerarely mentioned for the poor reconstruction ability and low performance.However, the latter requires no costly extra training samples for theunsupervised training that is more practical, so this paper focuses onimproving this kind of method and proposes a novel Omni-frequencyChannel-selection Reconstruction (OCR-GAN) network to handle anomaly detectiontask in a perspective of frequency. Concretely, we propose a FrequencyDecoupling (FD) module to decouple the input image into different frequencycomponents and model the reconstruction process as a combination of parallelomni-frequency image restorations, as we observe a significant difference inthe frequency distribution of normal and abnormal images. Given the correlationamong multiple frequencies, we further propose a Channel Selection (CS) modulethat performs frequency interaction among different encoders by adaptivelyselecting different channels. Abundant experiments demonstrate theeffectiveness and superiority of our approach over different kinds of methods,e.g., achieving a new state-of-the-art 98.3 detection AUC on the MVTec ADdataset without extra training data that markedly surpasses thereconstruction-based baseline by +38.1 and the current SOTA method by +0.3.Source code is available at https://github.com/zhangzjn/OCR-GAN.</description><author>Yufei Liang, Jiangning Zhang, Shiwei Zhao, Runze Wu, Yong Liu, Shuwen Pan</author><pubDate>Mon, 03 Jul 2023 10:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.00259v2</guid></item><item><title>Neural 3D Scene Reconstruction from Multi-view Images without 3D Supervision</title><link>http://arxiv.org/abs/2306.17643v2</link><description>Neural scene reconstruction methods have achieved impressive performance inreconstructing complex geometry and low-textured regions in large scenes.However, these methods heavily rely on 3D supervised information which iscostly and time-consuming to obtain in the real world. In this paper, wepropose a novel neural reconstruction method that reconstructs scenes without3D supervision. We perform differentiable volume rendering for scenereconstruction by using accessible 2D images as supervision. We impose geometryto improve the reconstruction quality of complex geometry regions in thescenes, and impose plane constraints to improve the reconstruction quality oflow-textured regions in the scenes. Specifically, we introduce a signeddistance function (SDF) field, a color field, and a probability field torepresent the scene, and optimize the fields under the differentiable raymarching to reconstruct the scene. Besides, we impose geometric constraintsthat project 3D points on the surface to similar-looking regions with similarfeatures in different views. We also impose plane constraints to make largeplanes keep parallel or vertical to the wall or floor. These two constraintshelp to reconstruct accurate and smooth geometry structures of the scene.Without 3D supervision information, our method achieves competitivereconstruction compared with some existing methods that use 3D information assupervision on the ScanNet dataset.</description><author>Yi Guo, Che Sun, Yunde Jia, Yuwei Wu</author><pubDate>Mon, 03 Jul 2023 10:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17643v2</guid></item><item><title>Data Poisoning Attack Aiming the Vulnerability of Continual Learning</title><link>http://arxiv.org/abs/2211.15875v2</link><description>Generally, regularization-based continual learning models limit access to theprevious task data to imitate the real-world constraints related to memory andprivacy. However, this introduces a problem in these models by not being ableto track the performance on each task. In essence, current continual learningmethods are susceptible to attacks on previous tasks. We demonstrate thevulnerability of regularization-based continual learning methods by presentinga simple task-specific data poisoning attack that can be used in the learningprocess of a new task. Training data generated by the proposed attack causesperformance degradation on a specific task targeted by the attacker. Weexperiment with the attack on the two representative regularization-basedcontinual learning methods, Elastic Weight Consolidation (EWC) and SynapticIntelligence (SI), trained with variants of MNIST dataset. The experimentresults justify the vulnerability proposed in this paper and demonstrate theimportance of developing continual learning models that are robust toadversarial attacks.</description><author>Gyojin Han, Jaehyun Choi, Hyeong Gwon Hong, Junmo Kim</author><pubDate>Mon, 03 Jul 2023 10:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15875v2</guid></item><item><title>Neural Extended Kalman Filters for Learning and Predicting Dynamics of Structural Systems</title><link>http://arxiv.org/abs/2210.04165v2</link><description>Accurate structural response prediction forms a main driver for structuralhealth monitoring and control applications. This often requires the proposedmodel to adequately capture the underlying dynamics of complex structuralsystems. In this work, we utilize a learnable Extended Kalman Filter (EKF),named the Neural Extended Kalman Filter (Neural EKF) throughout this paper, forlearning the latent evolution dynamics of complex physical systems. The NeuralEKF is a generalized version of the conventional EKF, where the modeling ofprocess dynamics and sensory observations can be parameterized by neuralnetworks, therefore learned by end-to-end training. The method is implementedunder the variational inference framework with the EKF conducting inferencefrom sensing measurements. Typically, conventional variational inference modelsare parameterized by neural networks independent of the latent dynamics models.This characteristic makes the inference and reconstruction accuracy weaklybased on the dynamics models and renders the associated training inadequate. Inthis work, we show that the structure imposed by the Neural EKF is beneficialto the learning process. We demonstrate the efficacy of the framework on bothsimulated and real-world structural monitoring datasets, with the resultsindicating significant predictive capabilities of the proposed scheme.</description><author>Wei Liu, Zhilu Lai, Kiran Bacsa, Eleni Chatzi</author><pubDate>Mon, 03 Jul 2023 10:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.04165v2</guid></item><item><title>1M parameters are enough? A lightweight CNN-based model for medical image segmentation</title><link>http://arxiv.org/abs/2306.16103v2</link><description>Convolutional neural networks (CNNs) and Transformer-based models are beingwidely applied in medical image segmentation thanks to their ability to extracthigh-level features and capture important aspects of the image. However, thereis often a trade-off between the need for high accuracy and the desire for lowcomputational cost. A model with higher parameters can theoretically achievebetter performance but also result in more computational complexity and highermemory usage, and thus is not practical to implement. In this paper, we lookfor a lightweight U-Net-based model which can remain the same or even achievebetter performance, namely U-Lite. We design U-Lite based on the principle ofDepthwise Separable Convolution so that the model can both leverage thestrength of CNNs and reduce a remarkable number of computing parameters.Specifically, we propose Axial Depthwise Convolutions with kernels 7x7 in boththe encoder and decoder to enlarge the model receptive field. To furtherimprove the performance, we use several Axial Dilated Depthwise Convolutionswith filters 3x3 for the bottleneck as one of our branches. Overall, U-Litecontains only 878K parameters, 35 times less than the traditional U-Net, andmuch more times less than other modern Transformer-based models. The proposedmodel cuts down a large amount of computational complexity while attaining animpressive performance on medical segmentation tasks compared to otherstate-of-the-art architectures. The code will be available at:https://github.com/duong-db/U-Lite.</description><author>Binh-Duong Dinh, Thanh-Thu Nguyen, Thi-Thao Tran, Van-Truong Pham</author><pubDate>Mon, 03 Jul 2023 10:38:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16103v2</guid></item><item><title>Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version)</title><link>http://arxiv.org/abs/2306.17323v2</link><description>Owing to their remarkable learning capabilities and performance in real-worldapplications, the use of machine learning systems based on Neural Networks(NNs) has been continuously increasing. However, various case studies andempirical findings in the literature suggest that slight variations to NNinputs can lead to erroneous and undesirable NN behavior. This has led toconsiderable interest in their formal analysis, aiming to provide guaranteesregarding a given NN's behavior. Existing frameworks provide robustness and/orsafety guarantees for the trained NNs, using satisfiability solving and linearprogramming. We proposed FANNet, the first model checking-based framework foranalyzing a broader range of NN properties. However, the state-space explosionassociated with model checking entails a scalability problem, making the FANNetapplicable only to small NNs. This work develops state-space reduction andinput segmentation approaches, to improve the scalability and timing efficiencyof formal NN analysis. Compared to the state-of-the-art FANNet, this enablesour new model checking-based framework to reduce the verification's timingoverhead by a factor of up to 8000, making the framework applicable to NNs evenwith approximately $80$ times more network parameters. This in turn allows theanalysis of NN safety properties using the new framework, in addition to allthe NN properties already included with FANNet. The framework is shown to beefficiently able to analyze properties of NNs trained on healthcare datasets aswell as the well--acknowledged ACAS Xu NNs.</description><author>Mahum Naseer, Osman Hasan, Muhammad Shafique</author><pubDate>Mon, 03 Jul 2023 10:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17323v2</guid></item><item><title>Motif Graph Neural Network</title><link>http://arxiv.org/abs/2112.14900v3</link><description>Graphs can model complicated interactions between entities, which naturallyemerge in many important applications. These applications can often be castinto standard graph learning tasks, in which a crucial step is to learnlow-dimensional graph representations. Graph neural networks (GNNs) arecurrently the most popular model in graph embedding approaches. However,standard GNNs in the neighborhood aggregation paradigm suffer from limiteddiscriminative power in distinguishing \emph{high-order} graph structures asopposed to \emph{low-order} structures. To capture high-order structures,researchers have resorted to motifs and developed motif-based GNNs. However,existing motif-based GNNs still often suffer from less discriminative power onhigh-order structures. To overcome the above limitations, we propose MotifGraph Neural Network (MGNN), a novel framework to better capture high-orderstructures, hinging on our proposed motif redundancy minimization operator andinjective motif combination. First, MGNN produces a set of node representationsw.r.t. each motif. The next phase is our proposed redundancy minimization amongmotifs which compares the motifs with each other and distills the featuresunique to each motif. Finally, MGNN performs the updating of noderepresentations by combining multiple representations from different motifs. Inparticular, to enhance the discriminative power, MGNN utilizes an injectivefunction to combine the representations w.r.t. different motifs. We furthershow that our proposed architecture increases the expressive power of GNNs witha theoretical analysis. We demonstrate that MGNN outperforms state-of-the-artmethods on seven public benchmarks on both node classification and graphclassification tasks.</description><author>Xuexin Chen, Ruichu Cai, Yuan Fang, Min Wu, Zijian Li, Zhifeng Hao</author><pubDate>Mon, 03 Jul 2023 10:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.14900v3</guid></item><item><title>BPF Algorithms for Multiple Source-Translation Computed Tomography Reconstruction</title><link>http://arxiv.org/abs/2305.18878v2</link><description>Micro-computed tomography (micro-CT) is a widely used state-of-the-artinstrument employed to study the morphological structures of objects in variousfields. However, its small field-of-view (FOV) cannot meet the pressing demandfor imaging relatively large objects at high spatial resolutions. Recently, wedevised a novel scanning mode called multiple source translation CT (mSTCT)that effectively enlarges the FOV of the micro-CT and correspondingly developeda virtual projection-based filtered backprojection (V-FBP) algorithm forreconstruction. Although V-FBP skillfully solves the truncation problem inmSTCT, it requires densely sampled projections to arrive at high-resolutionreconstruction, which reduces imaging efficiency. In this paper, we developedtwo backprojection-filtration (BPF)-based algorithms for mSTCT: S-BPF(derivatives along source) and D-BPF (derivatives along detector). D-BPF canachieve high-resolution reconstruction with fewer projections than V-FBP andS-BPF. Through simulated and real experiments conducted in this paper, wedemonstrate that D-BPF can reduce source sampling by 75% compared with V-FBP atthe same spatial resolution, which makes mSTCT more feasible in practice.Meanwhile, S-BPF can yield more stable results than D-BPF, which is similar toV-FBP.</description><author>Zhisheng Wang, Haijun Yu, Yixing Huang, Shunli Wang, Song Ni, Zongfeng Li, Fenglin Liu, Junning Cui</author><pubDate>Mon, 03 Jul 2023 10:27:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18878v2</guid></item><item><title>CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data</title><link>http://arxiv.org/abs/2210.17283v2</link><description>Causal inference is a vital aspect of multiple scientific disciplines and isroutinely applied to high-impact applications such as medicine. However,evaluating the performance of causal inference methods in real-worldenvironments is challenging due to the need for observations under bothinterventional and control conditions. Traditional evaluations conducted onsynthetic datasets do not reflect the performance in real-world systems. Toaddress this, we introduce CausalBench, a benchmark suite for evaluatingnetwork inference methods on real-world interventional data from large-scalesingle-cell perturbation experiments. CausalBench incorporatesbiologically-motivated performance metrics, including new distribution-basedinterventional metrics. A systematic evaluation of state-of-the-art causalinference methods using our CausalBench suite highlights how poor scalabilityof current methods limits performance. Moreover, methods that useinterventional information do not outperform those that only use observationaldata, contrary to what is observed on synthetic benchmarks. Thus, CausalBenchopens new avenues in causal network inference research and provides aprincipled and reliable way to track progress in leveraging real-worldinterventional data.</description><author>Mathieu Chevalley, Yusuf Roohani, Arash Mehrjou, Jure Leskovec, Patrick Schwab</author><pubDate>Mon, 03 Jul 2023 10:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.17283v2</guid></item><item><title>DRCFS: Doubly Robust Causal Feature Selection</title><link>http://arxiv.org/abs/2306.07024v2</link><description>Knowing the features of a complex system that are highly relevant to aparticular target variable is of fundamental interest in many areas of science.Existing approaches are often limited to linear settings, sometimes lackguarantees, and in most cases, do not scale to the problem at hand, inparticular to images. We propose DRCFS, a doubly robust feature selectionmethod for identifying the causal features even in nonlinear and highdimensional settings. We provide theoretical guarantees, illustrate necessaryconditions for our assumptions, and perform extensive experiments across a widerange of simulated and semi-synthetic datasets. DRCFS significantly outperformsexisting state-of-the-art methods, selecting robust features even inchallenging highly non-linear and high-dimensional problems.</description><author>Francesco Quinzan, Ashkan Soleymani, Patrik Jaillet, Cristian R. Rojas, Stefan Bauer</author><pubDate>Mon, 03 Jul 2023 10:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07024v2</guid></item><item><title>Feature-Based Time-Series Analysis in R using the theft Package</title><link>http://arxiv.org/abs/2208.06146v4</link><description>Time series are measured and analyzed across the sciences. One method ofquantifying the structure of time series is by calculating a set of summarystatistics or `features', and then representing a time series in terms of itsproperties as a feature vector. The resulting feature space is interpretableand informative, and enables conventional statistical learning approaches,including clustering, regression, and classification, to be applied totime-series datasets. Many open-source software packages for computing sets oftime-series features exist across multiple programming languages, includingcatch22 (22 features: Matlab, R, Python, Julia), feasts (42 features: R),tsfeatures (63 features: R), Kats (40 features: Python), tsfresh (779 features:Python), and TSFEL (390 features: Python). However, there are several issues:(i) a singular access point to these packages is not currently available; (ii)to access all feature sets, users must be fluent in multiple languages; and(iii) these feature-extraction packages lack extensive accompanyingmethodological pipelines for performing feature-based time-series analysis,such as applications to time-series classification. Here we introduce asolution to these issues in an R software package called theft: Tools forHandling Extraction of Features from Time series. theft is a unified andextendable framework for computing features from the six open-sourcetime-series feature sets listed above. It also includes a suite of functionsfor processing and interpreting the performance of extracted features,including extensive data-visualization templates, low-dimensional projections,and time-series classification operations. With an increasing volume andcomplexity of time-series datasets in the sciences and industry, theft providesa standardized framework for comprehensively quantifying and interpretinginformative structure in time series.</description><author>Trent Henderson, Ben D. Fulcher</author><pubDate>Mon, 03 Jul 2023 10:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.06146v4</guid></item><item><title>Learning When to Advise Human Decision Makers</title><link>http://arxiv.org/abs/2209.13578v2</link><description>Artificial intelligence (AI) systems are increasingly used for providingadvice to facilitate human decision making in a wide range of domains, such ashealthcare, criminal justice, and finance. Motivated by limitations of thecurrent practice where algorithmic advice is provided to human users as aconstant element in the decision-making pipeline, in this paper we raise thequestion of when should algorithms provide advice? We propose a novel design ofAI systems in which the algorithm interacts with the human user in a two-sidedmanner and aims to provide advice only when it is likely to be beneficial forthe user in making their decision. The results of a large-scale experiment showthat our advising approach manages to provide advice at times of need and tosignificantly improve human decision making compared to fixed, non-interactive,advising approaches. This approach has additional advantages in facilitatinghuman learning, preserving complementary strengths of human decision makers,and leading to more positive responsiveness to the advice.</description><author>Gali Noti, Yiling Chen</author><pubDate>Mon, 03 Jul 2023 10:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.13578v2</guid></item><item><title>Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Diagnosis</title><link>http://arxiv.org/abs/2003.06534v5</link><description>Medical diagnosis assistant (MDA) aims to build an interactive diagnosticagent to sequentially inquire about symptoms for discriminating diseases.However, since the dialogue records used to build a patient simulator arecollected passively, the data might be deteriorated by some task-unrelatedbiases, such as the preference of the collectors. These biases might hinder thediagnostic agent to capture transportable knowledge from the simulator. Thiswork attempts to address these critical issues in MDA by taking advantage ofthe causal diagram to identify and resolve two representative non-causalbiases, i.e., (i) default-answer bias and (ii) distributional inquiry bias.Specifically, Bias (i) originates from the patient simulator which tries toanswer the unrecorded inquiries with some biased default answers. Consequently,the diagnostic agents cannot fully demonstrate their advantages due to thebiased answers. To eliminate this bias and inspired by the propensity scorematching technique with causal diagram, we propose a propensity-based patientsimulator to effectively answer unrecorded inquiry by drawing knowledge fromthe other records; Bias (ii) inherently comes along with the passivelycollected data, and is one of the key obstacles for training the agent towards"learning how" rather than "remembering what". For example, within thedistribution of training data, if a symptom is highly coupled with a certaindisease, the agent might learn to only inquire about that symptom todiscriminate that disease, thus might not generalize to the out-of-distributioncases. To this end, we propose a progressive assurance agent, which includesthe dual processes accounting for symptom inquiry and disease diagnosisrespectively. The inquiry process is driven by the diagnosis process in atop-down manner to inquire about symptoms for enhancing diagnostic confidence.</description><author>Junfan Lin, Keze Wang, Ziliang Chen, Xiaodan Liang, Liang Lin</author><pubDate>Mon, 03 Jul 2023 09:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.06534v5</guid></item><item><title>Knowledge-Driven Robot Program Synthesis from Human VR Demonstrations</title><link>http://arxiv.org/abs/2306.02739v2</link><description>Aging societies, labor shortages and increasing wage costs call forassistance robots capable of autonomously performing a wide array of real-worldtasks. Such open-ended robotic manipulation requires not only powerfulknowledge representations and reasoning (KR&amp;R) algorithms, but also methods forhumans to instruct robots what tasks to perform and how to perform them. Inthis paper, we present a system for automatically generating executable robotcontrol programs from human task demonstrations in virtual reality (VR). Weleverage common-sense knowledge and game engine-based physics to semanticallyinterpret human VR demonstrations, as well as an expressive and general taskrepresentation and automatic path planning and code generation, embedded into astate-of-the-art cognitive architecture. We demonstrate our approach in thecontext of force-sensitive fetch-and-place for a robotic shopping assistant.The source code is available athttps://github.com/ease-crc/vr-program-synthesis.</description><author>Benjamin Alt, Franklin Kenghagho Kenfack, Andrei Haidu, Darko Katic, Rainer Jäkel, Michael Beetz</author><pubDate>Mon, 03 Jul 2023 09:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02739v2</guid></item><item><title>Attention Mixtures for Time-Aware Sequential Recommendation</title><link>http://arxiv.org/abs/2304.08158v2</link><description>Transformers emerged as powerful methods for sequential recommendation.However, existing architectures often overlook the complex dependencies betweenuser preferences and the temporal context. In this short paper, we introduceMOJITO, an improved Transformer sequential recommender system that addressesthis limitation. MOJITO leverages Gaussian mixtures of attention-based temporalcontext and item embedding representations for sequential modeling. Such anapproach permits to accurately predict which items should be recommended nextto users depending on past actions and the temporal context. We demonstrate therelevance of our approach, by empirically outperforming existing Transformersfor sequential recommendation on several real-world datasets.</description><author>Viet-Anh Tran, Guillaume Salha-Galvan, Bruno Sguerra, Romain Hennequin</author><pubDate>Mon, 03 Jul 2023 09:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08158v2</guid></item><item><title>Analysis of a Deep Learning Model for 12-Lead ECG Classification Reveals Learned Features Similar to Diagnostic Criteria</title><link>http://arxiv.org/abs/2211.01738v2</link><description>Despite their remarkable performance, deep neural networks remain unadoptedin clinical practice, which is considered to be partially due to their lack inexplainability. In this work, we apply attribution methods to a pre-traineddeep neural network (DNN) for 12-lead electrocardiography classification toopen this "black box" and understand the relationship between model predictionand learned features. We classify data from a public data set and theattribution methods assign a "relevance score" to each sample of the classifiedsignals. This allows analyzing what the network learned during training, forwhich we propose quantitative methods: average relevance scores over a)classes, b) leads, and c) average beats. The analyses of relevance scores foratrial fibrillation (AF) and left bundle branch block (LBBB) compared tohealthy controls show that their mean values a) increase with higherclassification probability and correspond to false classifications when aroundzero, and b) correspond to clinical recommendations regarding which lead toconsider. Furthermore, c) visible P-waves and concordant T-waves result inclearly negative relevance scores in AF and LBBB classification, respectively.In summary, our analysis suggests that the DNN learned features similar tocardiology textbook knowledge.</description><author>Theresa Bender, Jacqueline Michelle Beinecke, Dagmar Krefting, Carolin Müller, Henning Dathe, Tim Seidler, Nicolai Spicher, Anne-Christin Hauschild</author><pubDate>Mon, 03 Jul 2023 09:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01738v2</guid></item><item><title>Domain shifts in dermoscopic skin cancer datasets: Evaluation of essential limitations for clinical translation</title><link>http://arxiv.org/abs/2304.06968v3</link><description>The limited ability of Convolutional Neural Networks to generalize to imagesfrom previously unseen domains is a major limitation, in particular, forsafety-critical clinical tasks such as dermoscopic skin cancer classification.In order to translate CNN-based applications into the clinic, it is essentialthat they are able to adapt to domain shifts. Such new conditions can arisethrough the use of different image acquisition systems or varying lightingconditions. In dermoscopy, shifts can also occur as a change in patient age oroccurence of rare lesion localizations (e.g. palms). These are not prominentlyrepresented in most training datasets and can therefore lead to a decrease inperformance. In order to verify the generalizability of classification modelsin real world clinical settings it is crucial to have access to data whichmimics such domain shifts. To our knowledge no dermoscopic image dataset existswhere such domain shifts are properly described and quantified. We thereforegrouped publicly available images from ISIC archive based on their metadata(e.g. acquisition location, lesion localization, patient age) to generatemeaningful domains. To verify that these domains are in fact distinct, we usedmultiple quantification measures to estimate the presence and intensity ofdomain shifts. Additionally, we analyzed the performance on these domains withand without an unsupervised domain adaptation technique. We observed that inmost of our grouped domains, domain shifts in fact exist. Based on our results,we believe these datasets to be helpful for testing the generalizationcapabilities of dermoscopic skin cancer classifiers.</description><author>Katharina Fogelberg, Sireesha Chamarthi, Roman C. Maron, Julia Niebling, Titus J. Brinker</author><pubDate>Mon, 03 Jul 2023 09:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06968v3</guid></item><item><title>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</title><link>http://arxiv.org/abs/2306.06211v3</link><description>Segment anything model (SAM) developed by Meta AI Research has recentlyattracted significant attention. Trained on a large segmentation dataset ofover 1 billion masks, SAM is capable of segmenting any object on a certainimage. In the original SAM work, the authors turned to zero-short transfertasks (like edge detection) for evaluating the performance of SAM. Recently,numerous works have attempted to investigate the performance of SAM in variousscenarios to recognize and segment objects. Moreover, numerous projects haveemerged to show the versatility of SAM as a foundation model by combining itwith other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. Withthe relevant papers and projects increasing exponentially, it is challengingfor the readers to catch up with the development of SAM. To this end, this workconducts the first yet comprehensive survey on SAM. This is an ongoing projectand we intend to update the manuscript on a regular basis. Therefore, readersare welcome to contact us if they complete new works related to SAM so that wecan include them in our next version.</description><author>Chaoning Zhang, Fachrina Dewi Puspitasari, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, Francois Rameau, Lik-Hang Lee, Sung-Ho Bae, Choong Seon Hong</author><pubDate>Mon, 03 Jul 2023 09:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06211v3</guid></item><item><title>PCDNF: Revisiting Learning-based Point Cloud Denoising via Joint Normal Filtering</title><link>http://arxiv.org/abs/2209.00798v2</link><description>Recovering high quality surfaces from noisy point clouds, known as pointcloud denoising, is a fundamental yet challenging problem in geometryprocessing. Most of the existing methods either directly denoise the noisyinput or filter raw normals followed by updating point positions. Motivated bythe essential interplay between point cloud denoising and normal filtering, werevisit point cloud denoising from a multitask perspective, and propose anend-to-end network, named PCDNF, to denoise point clouds via joint normalfiltering. In particular, we introduce an auxiliary normal filtering task tohelp the overall network remove noise more effectively while preservinggeometric features more accurately. In addition to the overall architecture,our network has two novel modules. On one hand, to improve noise removalperformance, we design a shape-aware selector to construct the latent tangentspace representation of the specific point by comprehensively considering thelearned point and normal features and geometry priors. On the other hand, pointfeatures are more suitable for describing geometric details, and normalfeatures are more conducive for representing geometric structures (e.g., sharpedges and corners). Combining point and normal features allows us to overcometheir weaknesses. Thus, we design a feature refinement module to fuse point andnormal features for better recovering geometric information. Extensiveevaluations, comparisons, and ablation studies demonstrate that the proposedmethod outperforms state-of-the-arts for both point cloud denoising and normalfiltering.</description><author>Zheng Liu, Yaowu Zhao, Sijing Zhan, Yuanyuan Liu, Renjie Chen, Ying He</author><pubDate>Mon, 03 Jul 2023 09:24:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.00798v2</guid></item><item><title>SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs</title><link>http://arxiv.org/abs/2306.17842v2</link><description>In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enablingfrozen LLMs to perform both understanding and generation tasks involvingnon-linguistic modalities such as images or videos. SPAE converts between rawpixels and interpretable lexical tokens (or words) extracted from the LLM'svocabulary. The resulting tokens capture both the semantic meaning and thefine-grained details needed for visual reconstruction, effectively translatingthe visual content into a language comprehensible to the LLM, and empowering itto perform a wide array of multimodal tasks. Our approach is validated throughin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse setof image understanding and generation tasks. Our method marks the firstsuccessful attempt to enable a frozen LLM to generate image content whilesurpassing state-of-the-art performance in image understanding tasks, under thesame setting, by over 25%.</description><author>Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang</author><pubDate>Mon, 03 Jul 2023 09:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17842v2</guid></item><item><title>BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation</title><link>http://arxiv.org/abs/2305.18326v3</link><description>We present a large-scale video subtitle translation dataset, BigVideo, tofacilitate the study of multi-modality machine translation. Compared with thewidely used How2 and VaTeX datasets, BigVideo is more than 10 times larger,consisting of 4.5 million sentence pairs and 9,981 hours of videos. We alsointroduce two deliberately designed test sets to verify the necessity of visualinformation: Ambiguous with the presence of ambiguous words, and Unambiguous inwhich the text context is self-contained for translation. To better model thecommon semantics shared across texts and videos, we introduce a contrastivelearning method in the cross-modal encoder. Extensive experiments on theBigVideo show that: a) Visual information consistently improves the NMT modelin terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous testsets. b) Visual information helps disambiguation, compared to the strong textbaseline on terminology-targeted scores and human evaluation. Dataset and ourimplementations are available at https://github.com/DeepLearnXMU/BigVideo-VMT.</description><author>Liyan Kang, Luyang Huang, Ningxin Peng, Peihao Zhu, Zewei Sun, Shanbo Cheng, Mingxuan Wang, Degen Huang, Jinsong Su</author><pubDate>Mon, 03 Jul 2023 09:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18326v3</guid></item><item><title>Evaluating the Adversarial Robustness of Convolution-based Human Motion Prediction</title><link>http://arxiv.org/abs/2306.11990v2</link><description>Human motion prediction has achieved a brilliant performance with the help ofCNNs, which facilitates human-machine cooperation. However, currently, there isno work evaluating the potential risk in human motion prediction when facingadversarial attacks, which may cause danger in real applications. Theadversarial attack will face two problems against human motion prediction: 1.For naturalness, pose data is highly related to the physical dynamics of humanskeletons where Lp norm constraints cannot constrain the adversarial examplewell; 2. Unlike the pixel value in images, pose data is diverse at scalebecause of the different acquisition equipment and the data processing, whichmakes it hard to set fixed parameters to perform attacks. To solve the problemsabove, we propose a new adversarial attack method that perturbs the input humanmotion sequence by maximizing the prediction error with physical constraints.Specifically, we introduce a novel adaptable scheme that facilitates the attackto suit the scale of the target pose and two physical constraints to enhancethe imperceptibility of the adversarial example. The evaluating experiments onthree datasets show that the prediction errors of all target models areenlarged significantly, which means current convolution-based human motionprediction models can be easily disturbed under the proposed attack. Thequantitative analysis shows that prior knowledge and semantic informationmodeling can be the key to the adversarial robustness of human motionpredictors. The qualitative results indicate that the adversarial sample ishard to be noticed when compared frame by frame but is relatively easy to bedetected when the sample is animated.</description><author>Chengxu Duan, Zhicheng Zhang, Xiaoli Liu, Yonghao Dang, Jianqin Yin</author><pubDate>Mon, 03 Jul 2023 09:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11990v2</guid></item><item><title>Attention-Based Depth Distillation with 3D-Aware Positional Encoding for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2211.16779v2</link><description>Monocular 3D object detection is a low-cost but challenging task, as itrequires generating accurate 3D localization solely from a single image input.Recent developed depth-assisted methods show promising results by usingexplicit depth maps as intermediate features, which are either precomputed bymonocular depth estimation networks or jointly evaluated with 3D objectdetection. However, inevitable errors from estimated depth priors may lead tomisaligned semantic information and 3D localization, hence resulting in featuresmearing and suboptimal predictions. To mitigate this issue, we propose ADD, anAttention-based Depth knowledge Distillation framework with 3D-aware positionalencoding. Unlike previous knowledge distillation frameworks that adopt stereo-or LiDAR-based teachers, we build up our teacher with identical architecture asthe student but with extra ground-truth depth as input. Credit to our teacherdesign, our framework is seamless, domain-gap free, easily implementable, andis compatible with object-wise ground-truth depth. Specifically, we leverageintermediate features and responses for knowledge distillation. Consideringlong-range 3D dependencies, we propose \emph{3D-aware self-attention} and\emph{target-aware cross-attention} modules for student adaptation. Extensiveexperiments are performed to verify the effectiveness of our framework on thechallenging KITTI 3D object detection benchmark. We implement our framework onthree representative monocular detectors, and we achieve state-of-the-artperformance with no additional inference computational cost relative tobaseline models. Our code is available at https://github.com/rockywind/ADD.</description><author>Zizhang Wu, Yunzhe Wu, Jian Pu, Xianzhi Li, Xiaoquan Wang</author><pubDate>Mon, 03 Jul 2023 09:07:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16779v2</guid></item><item><title>Exploration by self-supervised exploitation</title><link>http://arxiv.org/abs/2302.11563v2</link><description>Reinforcement learning can solve decision-making problems and train an agentto behave in an environment according to a predesigned reward function.However, such an approach becomes very problematic if the reward is too sparseand the agent does not come across the reward during the environmentalexploration. The solution to such a problem may be in equipping the agent withan intrinsic motivation, which will provide informed exploration, during whichthe agent is likely to also encounter external reward. Novelty detection is oneof the promising branches of intrinsic motivation research. We presentSelf-supervised Network Distillation (SND), a class of internal motivationalgorithms based on the distillation error as a novelty indicator, where thetarget model is trained using self-supervised learning. We adapted threeexisting self-supervised methods for this purpose and experimentally testedthem on a set of ten environments that are considered difficult to explore. Theresults show that our approach achieves faster growth and higher externalreward for the same training time compared to the baseline models, whichimplies improved exploration in a very sparse reward environment.</description><author>Matej Pecháč, Michal Chovanec, Igor Farkaš</author><pubDate>Mon, 03 Jul 2023 08:52:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11563v2</guid></item><item><title>VSA: Learning Varied-Size Window Attention in Vision Transformers</title><link>http://arxiv.org/abs/2204.08446v2</link><description>Attention within windows has been widely explored in vision transformers tobalance the performance, computation complexity, and memory footprint. However,current models adopt a hand-crafted fixed-size window design, which restrictstheir capacity of modeling long-term dependencies and adapting to objects ofdifferent sizes. To address this drawback, we propose\textbf{V}aried-\textbf{S}ize Window \textbf{A}ttention (VSA) to learn adaptivewindow configurations from data. Specifically, based on the tokens within eachdefault window, VSA employs a window regression module to predict the size andlocation of the target window, i.e., the attention area where the key and valuetokens are sampled. By adopting VSA independently for each attention head, itcan model long-term dependencies, capture rich context from diverse windows,and promote information exchange among overlapped windows. VSA is aneasy-to-implement module that can replace the window attention instate-of-the-art representative models with minor modifications and negligibleextra computational cost while improving their performance by a large margin,e.g., 1.1\% for Swin-T on ImageNet classification. In addition, the performancegain increases when using larger images for training and test. Experimentalresults on more downstream tasks, including object detection, instancesegmentation, and semantic segmentation, further demonstrate the superiority ofVSA over the vanilla window attention in dealing with objects of differentsizes. The code will be releasedhttps://github.com/ViTAE-Transformer/ViTAE-VSA.</description><author>Qiming Zhang, Yufei Xu, Jing Zhang, Dacheng Tao</author><pubDate>Mon, 03 Jul 2023 08:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.08446v2</guid></item><item><title>Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks</title><link>http://arxiv.org/abs/2303.11338v3</link><description>Despite their immense success in numerous fields, machine and deep learningsystems have not yet been able to firmly establish themselves inmission-critical applications in healthcare. One of the main reasons lies inthe fact that when models are presented with previously unseen,Out-of-Distribution samples, their performance deteriorates significantly. Thisis known as the Domain Generalization (DG) problem. Our objective in this workis to propose a benchmark for evaluating DG algorithms, in addition tointroducing a novel architecture for tackling DG in biosignal classification.In this paper, we describe the Domain Generalization problem for biosignals,focusing on electrocardiograms (ECG) and electroencephalograms (EEG) andpropose and implement an open-source biosignal DG evaluation benchmark.Furthermore, we adapt state-of-the-art DG algorithms from computer vision tothe problem of 1D biosignal classification and evaluate their effectiveness.Finally, we also introduce a novel neural network architecture that leveragesmulti-layer representations for improved model generalizability. Byimplementing the above DG setup we are able to experimentally demonstrate thepresence of the DG problem in ECG and EEG datasets. In addition, our proposedmodel demonstrates improved effectiveness compared to the baseline algorithms,exceeding the state-of-the-art in both datasets. Recognizing the significanceof the distribution shift present in biosignal datasets, the presentedbenchmark aims at urging further research into the field of biomedical DG bysimplifying the evaluation process of proposed algorithms. To our knowledge,this is the first attempt at developing an open-source framework for evaluatingECG and EEG DG algorithms.</description><author>Aristotelis Ballas, Christos Diou</author><pubDate>Mon, 03 Jul 2023 08:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11338v3</guid></item><item><title>Deep Active Learning for Multi-Label Classification of Remote Sensing Images</title><link>http://arxiv.org/abs/2212.01165v2</link><description>In this letter, we introduce deep active learning (AL) for multi-labelclassification (MLC) problems in remote sensing (RS). In particular, weinvestigate the effectiveness of several AL query functions for MLC of RSimages. Unlike the existing AL query functions (which are defined forsingle-label classification or semantic segmentation problems), each queryfunction in this paper is based on the evaluation of two criteria: i)multi-label uncertainty; and ii) multi-label diversity. The multi-labeluncertainty criterion is associated to the confidence of the deep neuralnetworks (DNNs) in correctly assigning multi-labels to each image. To assessthis criterion, we investigate three strategies: i) learning multi-label lossordering; ii) measuring temporal discrepancy of multi-label predictions; andiii) measuring magnitude of approximated gradient embeddings. The multi-labeldiversity criterion is associated to the selection of a set of images that areas diverse as possible to each other that prevents redundancy among them. Toassess this criterion, we exploit a clustering based strategy. We combine eachof the above-mentioned uncertainty strategies with the clustering baseddiversity strategy, resulting in three different query functions. All theconsidered query functions are introduced for the first time in the frameworkof MLC problems in RS. Experimental results obtained on two benchmark archivesshow that these query functions result in the selection of a highly informativeset of samples at each iteration of the AL process.</description><author>Lars Möllenbrok, Gencer Sumbul, Begüm Demir</author><pubDate>Mon, 03 Jul 2023 08:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01165v2</guid></item><item><title>Adaptive Conditional Quantile Neural Processes</title><link>http://arxiv.org/abs/2305.18777v3</link><description>Neural processes are a family of probabilistic models that inherit theflexibility of neural networks to parameterize stochastic processes. Despiteproviding well-calibrated predictions, especially in regression problems, andquick adaptation to new tasks, the Gaussian assumption that is commonly used torepresent the predictive likelihood fails to capture more complicateddistributions such as multimodal ones. To overcome this limitation, we proposeConditional Quantile Neural Processes (CQNPs), a new member of the neuralprocesses family, which exploits the attractive properties of quantileregression in modeling the distributions irrespective of their form. Byintroducing an extension of quantile regression where the model learns to focuson estimating informative quantiles, we show that the sampling efficiency andprediction accuracy can be further enhanced. Our experiments with real andsynthetic datasets demonstrate substantial improvements in predictiveperformance compared to the baselines, and better modeling of heterogeneousdistributions' characteristics such as multimodality.</description><author>Peiman Mohseni, Nick Duffield, Bani Mallick, Arman Hasanzadeh</author><pubDate>Mon, 03 Jul 2023 08:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18777v3</guid></item><item><title>Layer-level activation mechanism</title><link>http://arxiv.org/abs/2306.04940v2</link><description>In this work, we propose a novel activation mechanism aimed at establishinglayer-level activation (LayerAct) functions. These functions are designed to bemore noise-robust compared to traditional element-level activation functions byreducing the layer-level fluctuation of the activation outputs due to shift ininputs. Moreover, the LayerAct functions achieve a zero-like mean activationoutput without restricting the activation output space. We present an analysisand experiments demonstrating that LayerAct functions exhibit superiornoise-robustness compared to element-level activation functions, andempirically show that these functions have a zero-like mean activation.Experimental results on three benchmark image classification tasks show thatLayerAct functions excel in handling noisy image datasets, outperformingelement-level activation functions, while the performance on clean datasets isalso superior in most cases.</description><author>Kihyuk Yoon, Chiehyeon Lim</author><pubDate>Mon, 03 Jul 2023 08:14:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04940v2</guid></item><item><title>CVB: A Video Dataset of Cattle Visual Behaviors</title><link>http://arxiv.org/abs/2305.16555v2</link><description>Existing image/video datasets for cattle behavior recognition are mostlysmall, lack well-defined labels, or are collected in unrealistic controlledenvironments. This limits the utility of machine learning (ML) models learnedfrom them. Therefore, we introduce a new dataset, called Cattle VisualBehaviors (CVB), that consists of 502 video clips, each fifteen seconds long,captured in natural lighting conditions, and annotated with eleven visuallyperceptible behaviors of grazing cattle. We use the Computer Vision AnnotationTool (CVAT) to collect our annotations. To make the procedure more efficient,we perform an initial detection and tracking of cattle in the videos usingappropriate pre-trained models. The results are corrected by domain expertsalong with cattle behavior labeling in CVAT. The pre-hoc detection and trackingstep significantly reduces the manual annotation time and effort. Moreover, weconvert CVB to the atomic visual action (AVA) format and train and evaluate thepopular SlowFast action recognition model on it. The associated preliminaryresults confirm that we can localize the cattle and recognize their frequentlyoccurring behaviors with confidence. By creating and sharing CVB, our aim is todevelop improved models capable of recognizing all important behaviorsaccurately and to assist other researchers and practitioners in developing andevaluating new ML models for cattle behavior classification using video data.</description><author>Ali Zia, Renuka Sharma, Reza Arablouei, Greg Bishop-Hurley, Jody McNally, Neil Bagnall, Vivien Rolland, Brano Kusy, Lars Petersson, Aaron Ingham</author><pubDate>Mon, 03 Jul 2023 08:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16555v2</guid></item><item><title>Towards Better Evaluation of GNN Expressiveness with BREC Dataset</title><link>http://arxiv.org/abs/2304.07702v3</link><description>Research on the theoretical expressiveness of Graph Neural Networks (GNNs)has developed rapidly, and many methods have been proposed to enhance theexpressiveness. However, most methods do not have a uniform expressivenessmeasure except for a few that strictly follow the $k$-dimensionalWeisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are oftenlimited to distinguishing certain families of non-isomorphic graphs, leading todifficulties in quantitatively comparing their expressiveness. In contrast totheoretical analysis, another way to measure expressiveness is by evaluatingmodel performance on certain datasets containing 1-WL-indistinguishable graphs.Previous datasets specifically designed for this purpose, however, faceproblems with difficulty (any model surpassing 1-WL has nearly 100% accuracy),granularity (models tend to be either 100% correct or near random guess), andscale (only a few essentially different graphs in each dataset). To addressthese limitations, we propose a new expressiveness dataset, $\textbf{BREC}$,which includes 400 pairs of non-isomorphic graphs carefully selected from fourprimary categories (Basic, Regular, Extension, and CFI). These graphs havehigher difficulty (up to 4-WL-indistinguishable), finer granularity (able tocompare models between 1-WL and 3-WL), and a larger scale (400 pairs). Further,we synthetically test 23 models with higher-than-1-WL expressiveness on ourBREC dataset. Our experiment gives the first thorough comparison of theexpressiveness of those state-of-the-art beyond-1-WL GNN models. We expect thisdataset to serve as a benchmark for testing the expressiveness of future GNNs.Our dataset and evaluation code are released at:https://github.com/GraphPKU/BREC.</description><author>Yanbo Wang, Muhan Zhang</author><pubDate>Mon, 03 Jul 2023 08:10:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07702v3</guid></item><item><title>Transfer learning for atomistic simulations using GNNs and kernel mean embeddings</title><link>http://arxiv.org/abs/2306.01589v3</link><description>Interatomic potentials learned using machine learning methods have beensuccessfully applied to atomistic simulations. However, deep learning pipelinesare notoriously data-hungry, while generating reference calculations iscomputationally demanding. To overcome this difficulty, we propose a transferlearning algorithm that leverages the ability of graph neural networks (GNNs)in describing chemical environments, together with kernel mean embeddings. Weextract a feature map from GNNs pre-trained on the OC20 dataset and use it tolearn the potential energy surface from system-specific datasets of catalyticprocesses. Our method is further enhanced by a flexible kernel function thatincorporates chemical species information, resulting in improved performanceand interpretability. We test our approach on a series of realistic datasets ofincreasing complexity, showing excellent generalization and transferabilityperformance, and improving on methods that rely on GNNs or ridge regressionalone, as well as similar fine-tuning approaches. We make the code available tothe community at https://github.com/IsakFalk/atomistic_transfer_mekrr.</description><author>John Falk, Luigi Bonati, Pietro Novelli, Michele Parrinello, Massimiliano Pontil</author><pubDate>Mon, 03 Jul 2023 08:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01589v3</guid></item><item><title>Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments</title><link>http://arxiv.org/abs/2212.09683v4</link><description>We present a human-in-the-loop evaluation framework for fact-checking novelmisinformation claims and identifying social media messages that support them.Our approach extracts check-worthy claims, which are aggregated and ranked forreview. Stance classifiers are then used to identify tweets supporting novelmisinformation claims, which are further reviewed to determine whether theyviolate relevant policies. To demonstrate the feasibility of our approach, wedevelop a baseline system based on modern NLP methods for human-in-the-loopfact-checking in the domain of COVID-19 treatments. We make our data anddetailed annotation guidelines available to support the evaluation ofhuman-in-the-loop systems that identify novel misinformation directly from rawuser-generated content.</description><author>Ethan Mendes, Yang Chen, Wei Xu, Alan Ritter</author><pubDate>Mon, 03 Jul 2023 08:04:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09683v4</guid></item><item><title>Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data</title><link>http://arxiv.org/abs/2302.06279v2</link><description>Deep neural networks (DNNs) have demonstrated remarkable performance acrossvarious tasks, including image and speech recognition. However, maximizing theeffectiveness of DNNs requires meticulous optimization of numeroushyperparameters and network parameters through training. Moreover,high-performance DNNs entail many parameters, which consume significant energyduring training. In order to overcome these challenges, researchers have turnedto spiking neural networks (SNNs), which offer enhanced energy efficiency andbiologically plausible data processing capabilities, rendering them highlysuitable for sensory data tasks, particularly in neuromorphic data. Despitetheir advantages, SNNs, like DNNs, are susceptible to various threats,including adversarial examples and backdoor attacks. Yet, the field of SNNsstill needs to be explored in terms of understanding and countering theseattacks. This paper delves into backdoor attacks in SNNs using neuromorphic datasetsand diverse triggers. Specifically, we explore backdoor triggers withinneuromorphic data that can manipulate their position and color, providing abroader scope of possibilities than conventional triggers in domains likeimages. We present various attack strategies, achieving an attack success rateof up to 100\% while maintaining a negligible impact on clean accuracy.Furthermore, we assess these attacks' stealthiness, revealing that our mostpotent attacks possess significant stealth capabilities. Lastly, we adaptseveral state-of-the-art defenses from the image domain, evaluating theirefficacy on neuromorphic data and uncovering instances where they fall short,leading to compromised performance.</description><author>Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta</author><pubDate>Mon, 03 Jul 2023 08:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06279v2</guid></item><item><title>Can We Trust AI-Generated Educational Content? Comparative Analysis of Human and AI-Generated Learning Resources</title><link>http://arxiv.org/abs/2306.10509v2</link><description>As an increasing number of students move to online learning platforms thatdeliver personalized learning experiences, there is a great need for theproduction of high-quality educational content. Large language models (LLMs)appear to offer a promising solution to the rapid creation of learningmaterials at scale, reducing the burden on instructors. In this study, weinvestigated the potential for LLMs to produce learning resources in anintroductory programming context, by comparing the quality of the resourcesgenerated by an LLM with those created by students as part of a learnersourcingactivity. Using a blind evaluation, students rated the correctness andhelpfulness of resources generated by AI and their peers, after both wereinitially provided with identical exemplars. Our results show that the qualityof AI-generated resources, as perceived by students, is equivalent to thequality of resources generated by their peers. This suggests that AI-generatedresources may serve as viable supplementary material in certain contexts.Resources generated by LLMs tend to closely mirror the given exemplars, whereasstudent-generated resources exhibit greater variety in terms of content lengthand specific syntax features used. The study highlights the need for furtherresearch exploring different types of learning resources and a broader range ofsubject areas, and understanding the long-term impact of AI-generated resourceson learning outcomes.</description><author>Paul Denny, Hassan Khosravi, Arto Hellas, Juho Leinonen, Sami Sarsa</author><pubDate>Mon, 03 Jul 2023 07:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10509v2</guid></item><item><title>Perceptual Quality Assessment of Virtual Reality Videos in the Wild</title><link>http://arxiv.org/abs/2206.08751v2</link><description>Investigating how people perceive virtual reality videos in the wild (\ie,those captured by everyday users) is a crucial and challenging task inVR-related applications due to complex \textit{authentic} distortions localizedin space and time. Existing panoramic video databases only consider syntheticdistortions, assume fixed viewing conditions, and are limited in size. Toovercome these shortcomings, we construct the VR Video Quality in the Wild(VRVQW) database, which is one of the first of its kind, and contains $502$user-generated videos with diverse content and distortion characteristics.Based on VRVQW, we conduct a formal psychophysical experiment to record thescanpaths and perceived quality scores from $139$ participants under twodifferent viewing conditions. We provide a thorough statistical analysis of therecorded data, observing significant impact of viewing conditions on both humanscanpaths and perceived quality. Moreover, we develop an objective qualityassessment model for VR videos based on pseudocylindrical representation andconvolution. Results on the proposed VRVQW show that our method is superior toexisting video quality assessment models, only underperforming viewport-basedmodels that otherwise rely on human scanpaths for projection. Last, we explorethe additional use of the VRVQW dataset to benchmark saliency detectiontechniques, highlighting the need for further research. We have made thedatabase and code available at\url{https://github.com/limuhit/VR-Video-Quality-in-the-Wild}.</description><author>Wen Wen, Mu Li, Yiru Yao, Xiangjie Sui, Yabin Zhang, Long Lan, Yuming Fang, Kede Ma</author><pubDate>Mon, 03 Jul 2023 07:41:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08751v2</guid></item><item><title>MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation</title><link>http://arxiv.org/abs/2306.15253v3</link><description>Humans talk in free-form while negotiating the expressed meanings or commonground. Despite the impressive conversational abilities of the large generativelanguage models, they do not consider the individual differences in contextualunderstanding in a shared situated environment. In this work, we proposeMindDial, a novel conversational framework that can generate situated free-formresponses to negotiate common ground. We design an explicit mind module thatcan track three-level beliefs -- the speaker's belief, the speaker's predictionof the listener's belief, and the common belief based on the gap between thefirst two. Then the speaking act classification head will decide to continue totalk, end this turn, or take task-related action. We augment a common groundalignment dataset MutualFriend with belief dynamics annotation, of which thegoal is to find a single mutual friend based on the free chat between twoagents. Experiments show that our model with mental state modeling can resemblehuman responses when aligning common ground meanwhile mimic the natural humanconversation flow. The ablation study further validates the third-level commonbelief can aggregate information of the first and second-order beliefs andalign common ground more efficiently.</description><author>Shuwen Qiu, Song-Chun Zhu, Zilong Zheng</author><pubDate>Mon, 03 Jul 2023 07:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15253v3</guid></item><item><title>SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic Retinopathy Grading</title><link>http://arxiv.org/abs/2210.10969v4</link><description>Self-supervised learning (SSL) has been widely applied to learn imagerepresentations through exploiting unlabeled images. However, it has not beenfully explored in the medical image analysis field. In this work, we proposeSaliency-guided Self-Supervised image Transformer (SSiT) for diabeticretinopathy (DR) grading from fundus images. We novelly introduce saliency mapsinto SSL, with a goal of guiding self-supervised pre-training withdomain-specific prior knowledge. Specifically, two saliency-guided learningtasks are employed in SSiT: (1) We conduct saliency-guided contrastive learningbased on the momentum contrast, wherein we utilize fundus images' saliency mapsto remove trivial patches from the input sequences of the momentum-updated keyencoder. And thus, the key encoder is constrained to provide targetrepresentations focusing on salient regions, guiding the query encoder tocapture salient features. (2) We train the query encoder to predict thesaliency segmentation, encouraging preservation of fine-grained information inthe learned representations. Extensive experiments are conducted on fourpublicly-accessible fundus image datasets. The proposed SSiT significantlyoutperforms other representative state-of-the-art SSL methods on all datasetsand under various evaluation settings, establishing the effectiveness of thelearned representations from SSiT. The source code is available athttps://github.com/YijinHuang/SSiT.</description><author>Yijin Huang, Junyan Lyu, Pujin Cheng, Roger Tam, Xiaoying Tang</author><pubDate>Mon, 03 Jul 2023 07:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10969v4</guid></item><item><title>Shaped Policy Search for Evolutionary Strategies using Waypoints</title><link>http://arxiv.org/abs/2105.14639v2</link><description>In this paper, we try to improve exploration in Blackbox methods,particularly Evolution strategies (ES), when applied to Reinforcement Learning(RL) problems where intermediate waypoints/subgoals are available. SinceEvolutionary strategies are highly parallelizable, instead of extracting just ascalar cumulative reward, we use the state-action pairs from the trajectoriesobtained during rollouts/evaluations, to learn the dynamics of the agent. Thelearnt dynamics are then used in the optimization procedure to speed-uptraining. Lastly, we show how our proposed approach is universally applicableby presenting results from experiments conducted on Carla driving and UR5robotic arm simulators.</description><author>Kiran Lekkala, Laurent Itti</author><pubDate>Mon, 03 Jul 2023 07:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.14639v2</guid></item><item><title>An Overview on Language Models: Recent Developments and Outlook</title><link>http://arxiv.org/abs/2303.05759v2</link><description>Language modeling studies the probability distributions over strings oftexts. It is one of the most fundamental tasks in natural language processing(NLP). It has been widely used in text generation, speech recognition, machinetranslation, etc. Conventional language models (CLMs) aim to predict theprobability of linguistic sequences in a causal manner, while pre-trainedlanguage models (PLMs) cover broader concepts and can be used in both causalsequential modeling and fine-tuning for downstream applications. PLMs havetheir own training paradigms (usually self-supervised) and serve as foundationmodels in modern NLP systems. This overview paper provides an introduction toboth CLMs and PLMs from five aspects, i.e., linguistic units, architectures,training methods, evaluation methods, and applications. Furthermore, we discussthe relationship between CLMs and PLMs and shed light on the future directionsof language modeling in the pre-trained era.</description><author>Chengwei Wei, Yun-Cheng Wang, Bin Wang, C. -C. Jay Kuo</author><pubDate>Mon, 03 Jul 2023 06:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05759v2</guid></item><item><title>Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks</title><link>http://arxiv.org/abs/2302.11628v2</link><description>Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subsetof the features. $\ell_0$ robustness analysis is particularly well-suited forheterogeneous (tabular) data where features have different types or scales.State-of-the-art $\ell_0$ certified defenses are based on randomized smoothingand apply to evasion attacks only. This paper proposes feature partitionaggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion,backdoor, and poisoning attacks. FPA generates its stronger robustnessguarantees via an ensemble whose submodels are trained on disjoint featuresets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to3,000${\times}$ faster and provides larger median robustness guarantees (e.g.,median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 forMNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaningFPA provides the additional dimensions of robustness essentially for free.</description><author>Zayd Hammoudeh, Daniel Lowd</author><pubDate>Mon, 03 Jul 2023 06:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11628v2</guid></item><item><title>Progressive Multi-task Learning Framework for Chinese Text Error Correction</title><link>http://arxiv.org/abs/2306.17447v2</link><description>Chinese Text Error Correction (CTEC) aims to detect and correct errors in theinput text, which benefits human's daily life and various downstream tasks.Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolveCTEC task and achieve tremendous success. However, previous approaches sufferfrom issues of over-correction and under-correction, and the former isespecially conspicuous in the precision-critical CTEC task. To mitigate theissue of overcorrection, we propose a novel model-agnostic progressivemultitask learning framework for CTEC, named ProTEC, which guides a CTEC modelto learn the task from easy to difficult. We divide CTEC task into threesub-tasks from easy to difficult: Error Detection, Error Type Identification,and Correction Result Generation. During the training process, ProTEC guidesthe model to learn text error correction progressively by incorporating thesesub-tasks into a multi-task training objective. During the inference process,the model completes these sub-tasks in turn to generate the correction results.Extensive experiments and detailed analyses fully demonstrate the effectivenessand efficiency of our proposed framework.</description><author>Shirong Ma, Yinghui Li, Haojing Huang, Shulin Huang, Yangning Li, Hai-Tao Zheng, Ying Shen</author><pubDate>Mon, 03 Jul 2023 06:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17447v2</guid></item><item><title>Machine Learning and Polymer Self-Consistent Field Theory in Two Spatial Dimensions</title><link>http://arxiv.org/abs/2212.10478v2</link><description>A computational framework that leverages data from self-consistent fieldtheory simulations with deep learning to accelerate the exploration ofparameter space for block copolymers is presented. This is a substantialtwo-dimensional extension of the framework introduced in [1]. Severalinnovations and improvements are proposed. (1) A Sobolev space-trained,convolutional neural network (CNN) is employed to handle the exponentialdimension increase of the discretized, local average monomer density fields andto strongly enforce both spatial translation and rotation invariance of thepredicted, field-theoretic intensive Hamiltonian. (2) A generative adversarialnetwork (GAN) is introduced to efficiently and accurately predict saddle point,local average monomer density fields without resorting to gradient descentmethods that employ the training set. This GAN approach yields importantsavings of both memory and computational cost. (3) The proposed machinelearning framework is successfully applied to 2D cell size optimization as aclear illustration of its broad potential to accelerate the exploration ofparameter space for discovering polymer nanostructures. Extensions tothree-dimensional phase discovery appear to be feasible.</description><author>Yao Xuan, Kris T. Delaney, Hector D. Ceniceros, Glenn H. Fredrickson</author><pubDate>Mon, 03 Jul 2023 06:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10478v2</guid></item><item><title>Online Bidding Algorithms for Return-on-Spend Constrained Advertisers</title><link>http://arxiv.org/abs/2208.13713v3</link><description>Online advertising has recently grown into a highly competitive and complexmulti-billion-dollar industry, with advertisers bidding for ad slots at largescales and high frequencies. This has resulted in a growing need for efficient"auto-bidding" algorithms that determine the bids for incoming queries tomaximize advertisers' targets subject to their specified constraints. This workexplores efficient online algorithms for a single value-maximizing advertiserunder an increasingly popular constraint: Return-on-Spend (RoS). We quantifyefficiency in terms of regret relative to the optimal algorithm, which knowsall queries a priori. We contribute a simple online algorithm that achieves near-optimal regret inexpectation while always respecting the specified RoS constraint when the inputsequence of queries are i.i.d. samples from some distribution. We alsointegrate our results with the previous work of Balseiro, Lu, and Mirrokni[BLM20] to achieve near-optimal regret while respecting both RoS and fixedbudget constraints. Our algorithm follows the primal-dual framework and uses online mirrordescent (OMD) for the dual updates. However, we need to use a non-canonicalsetup of OMD, and therefore the classic low-regret guarantee of OMD, which isfor the adversarial setting in online learning, no longer holds. Nonetheless,in our case and more generally where low-regret dynamics are applied inalgorithm design, the gradients encountered by OMD can be far from adversarialbut influenced by our algorithmic choices. We exploit this key insight to showour OMD setup achieves low regret in the realm of our algorithm.</description><author>Zhe Feng, Swati Padmanabhan, Di Wang</author><pubDate>Mon, 03 Jul 2023 06:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.13713v3</guid></item><item><title>SSD-MonoDETR: Supervised Scale-aware Deformable Transformer for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2305.07270v3</link><description>Transformer-based methods have demonstrated superior performance formonocular 3D object detection recently, which aims at predicting 3D attributesfrom a single 2D image. Most existing transformer-based methods leverage bothvisual and depth representations to explore valuable query points on objects,and the quality of the learned query points has a great impact on detectionaccuracy. Unfortunately, existing unsupervised attention mechanisms intransformers are prone to generate low-quality query features due to inaccuratereceptive fields, especially on hard objects. To tackle this problem, thispaper proposes a novel Supervised Scale-aware Deformable Attention (SSDA) formonocular 3D object detection. Specifically, SSDA presets several masks withdifferent scales and utilizes depth and visual features to adaptively learn ascale-aware filter for object query augmentation. Imposing the scale awareness,SSDA could well predict the accurate receptive field of an object query tosupport robust query feature generation. Aside from this, SSDA is assigned witha Weighted Scale Matching (WSM) loss to supervise scale prediction, whichpresents more confident results as compared to the unsupervised attentionmechanisms. Extensive experiments on the KITTI benchmark demonstrate that SSDAsignificantly improves the detection accuracy, especially on moderate and hardobjects, yielding state-of-the-art performance as compared to the existingapproaches. Our code will be made publicly available athttps://github.com/mikasa3lili/SSD-MonoDETR.</description><author>Xuan He, Fan Yang, Kailun Yang, Jiacheng Lin, Haolong Fu, Meng Wang, Jin Yuan, Zhiyong Li</author><pubDate>Mon, 03 Jul 2023 06:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07270v3</guid></item><item><title>Probabilistic Linguistic Knowledge and Token-level Text Augmentation</title><link>http://arxiv.org/abs/2306.16644v2</link><description>This paper investigates the effectiveness of token-level text augmentationand the role of probabilistic linguistic knowledge within alinguistically-motivated evaluation context. Two text augmentation programs,REDA and REDA$_{NG}$, were developed, both implementing five token-level textediting operations: Synonym Replacement (SR), Random Swap (RS), RandomInsertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$leverages pretrained $n$-gram language models to select the most likelyaugmented texts from REDA's output. Comprehensive and fine-grained experimentswere conducted on a binary question matching classification task in bothChinese and English. The results strongly refute the general effectiveness ofthe five token-level text augmentation techniques under investigation, whetherapplied together or separately, and irrespective of various commonclassification model types used, including transformers. Furthermore, the roleof probabilistic linguistic knowledge is found to be minimal.</description><author>Zhengxiang Wang</author><pubDate>Mon, 03 Jul 2023 06:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16644v2</guid></item><item><title>Scalable Video Object Segmentation with Identification Mechanism</title><link>http://arxiv.org/abs/2203.11442v6</link><description>This paper delves into the challenges of achieving scalable and effectivemulti-object modeling for semi-supervised Video Object Segmentation (VOS).Previous VOS methods decode features with a single positive object, limitingthe learning of multi-object representation as they must match and segment eachtarget separately under multi-object scenarios. Additionally, earliertechniques catered to specific application objectives and lacked theflexibility to fulfill different speed-accuracy requirements. To address theseproblems, we present two innovative approaches, Associating Objects withTransformers (AOT) and Associating Objects with Scalable Transformers (AOST).In pursuing effective multi-object modeling, AOT introduces the IDentification(ID) mechanism to allocate each object a unique identity. This approach enablesthe network to model the associations among all objects simultaneously, thusfacilitating the tracking and segmentation of objects in a single network pass.To address the challenge of inflexible deployment, AOST further integratesscalable long short-term transformers that incorporate layer-wise ID-basedattention and scalable supervision. This overcomes ID embeddings'representation limitations and enables online architecture scalability in VOSfor the first time. Given the absence of a benchmark for VOS involving denselymulti-object annotations, we propose a challenging Video Object Segmentation inthe Wild (VOSW) benchmark to validate our approaches. We evaluated various AOTand AOST variants using extensive experiments across VOSW and fivecommonly-used VOS benchmarks. Our approaches surpass the state-of-the-artcompetitors and display exceptional efficiency and scalability consistentlyacross all six benchmarks. Moreover, we notably achieved the 1st position inthe 3rd Large-scale Video Object Segmentation Challenge.</description><author>Zongxin Yang, Xiaohan Wang, Jiaxu Miao, Yunchao Wei, Wenguan Wang, Yi Yang</author><pubDate>Mon, 03 Jul 2023 05:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11442v6</guid></item><item><title>Towards Source-free Domain Adaptive Semantic Segmentation via Importance-aware and Prototype-contrast Learning</title><link>http://arxiv.org/abs/2306.01598v2</link><description>Domain adaptive semantic segmentation enables robust pixel-wise understandingin real-world driving scenes. Source-free domain adaptation, as a morepractical technique, addresses the concerns of data privacy and storagelimitations in typical unsupervised domain adaptation methods. It utilizes awell-trained source model and unlabeled target data to achieve adaptation inthe target domain. However, in the absence of source data and target labels,current solutions cannot sufficiently reduce the impact of domain shift andfully leverage the information from the target data. In this paper, we proposean end-to-end source-free domain adaptation semantic segmentation method viaImportance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPCframework effectively extracts domain-invariant knowledge from the well-trainedsource model and learns domain-specific knowledge from the unlabeled targetdomain. Specifically, considering the problem of domain shift in the predictionof the target domain by the source model, we put forward an importance-awaremechanism for the biased target prediction probability distribution to extractdomain-invariant knowledge from the source model. We further introduce aprototype-contrast strategy, which includes a prototype-symmetric cross-entropyloss and a prototype-enhanced cross-entropy loss, to learn target intra-domainknowledge without relying on labels. A comprehensive variety of experiments ontwo domain adaptive semantic segmentation benchmarks demonstrates that theproposed end-to-end IAPC solution outperforms existing state-of-the-artmethods. Code will be made publicly available athttps://github.com/yihong-97/Source-free_IAPC.</description><author>Yihong Cao, Hui Zhang, Xiao Lu, Zheng Xiao, Kailun Yang, Yaonan Wang</author><pubDate>Mon, 03 Jul 2023 05:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01598v2</guid></item><item><title>RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding</title><link>http://arxiv.org/abs/2304.00962v2</link><description>Existing 3D scene understanding tasks have achieved high performance onclose-set benchmarks but fail to handle novel categories in real-worldapplications. To this end, we propose a Regional Point-Language Contrastivelearning framework, namely RegionPLC, for open-world 3D scene understanding,which equips models trained on closed-set datasets with open-vocabularyrecognition capabilities. We propose dense visual prompts to elicitregion-level visual-language knowledge from 2D foundation models viacaptioning, which further allows us to build dense regional point-languageassociations. Then, we design a point-discriminative contrastive learningobjective to enable point-independent learning from captions for dense sceneunderstanding. We conduct extensive experiments on ScanNet, ScanNet200, andnuScenes datasets. Our RegionPLC significantly outperforms previousbase-annotated 3D open-world scene understanding approaches by an average of11.6\% and 6.6\% for semantic and instance segmentation, respectively. It alsoshows promising open-world results in absence of any human annotation with lowtraining and inference costs. Code will be released.</description><author>Jihan Yang, Runyu Ding, Zhe Wang, Xiaojuan Qi</author><pubDate>Mon, 03 Jul 2023 05:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00962v2</guid></item><item><title>Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism</title><link>http://arxiv.org/abs/2103.12021v2</link><description>Offline (or batch) reinforcement learning (RL) algorithms seek to learn anoptimal policy from a fixed dataset without active data collection. Based onthe composition of the offline dataset, two main categories of methods areused: imitation learning which is suitable for expert datasets and vanillaoffline RL which often requires uniform coverage datasets. From a practicalstandpoint, datasets often deviate from these two extremes and the exact datacomposition is usually unknown a priori. To bridge this gap, we present a newoffline RL framework that smoothly interpolates between the two extremes ofdata composition, hence unifying imitation learning and vanilla offline RL. Thenew framework is centered around a weak version of the concentrabilitycoefficient that measures the deviation from the behavior policy to the expertpolicy alone. Under this new framework, we further investigate the question on algorithmdesign: can one develop an algorithm that achieves a minimax optimal rate andalso adapts to unknown data composition? To address this question, we considera lower confidence bound (LCB) algorithm developed based on pessimism in theface of uncertainty in offline RL. We study finite-sample properties of LCB aswell as information-theoretic limits in multi-armed bandits, contextualbandits, and Markov decision processes (MDPs). Our analysis reveals surprisingfacts about optimality rates. In particular, in all three settings, LCBachieves a faster rate of $1/N$ for nearly-expert datasets compared to theusual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples inthe batch dataset. In the case of contextual bandits with at least twocontexts, we prove that LCB is adaptively optimal for the entire datacomposition range, achieving a smooth transition from imitation learning tooffline RL. We further show that LCB is almost adaptively optimal in MDPs.</description><author>Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, Stuart Russell</author><pubDate>Mon, 03 Jul 2023 05:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.12021v2</guid></item><item><title>Sample Efficient Deep Reinforcement Learning via Local Planning</title><link>http://arxiv.org/abs/2301.12579v2</link><description>The focus of this work is sample-efficient deep reinforcement learning (RL)with a simulator. One useful property of simulators is that it is typicallyeasy to reset the environment to a previously observed state. We propose analgorithmic framework, named uncertainty-first local planning (UFLP), thattakes advantage of this property. Concretely, in each data collectioniteration, with some probability, our meta-algorithm resets the environment toan observed state which has high uncertainty, instead of sampling according tothe initial-state distribution. The agent-environment interaction then proceedsas in the standard online RL setting. We demonstrate that this simple procedurecan dramatically improve the sample cost of several baseline RL algorithms ondifficult exploration tasks. Notably, with our framework, we can achievesuper-human performance on the notoriously hard Atari game, Montezuma'sRevenge, with a simple (distributional) double DQN. Our work can be seen as anefficient approximate implementation of an existing algorithm with theoreticalguarantees, which offers an interpretation of the positive empirical results.</description><author>Dong Yin, Sridhar Thiagarajan, Nevena Lazic, Nived Rajaraman, Botao Hao, Csaba Szepesvari</author><pubDate>Mon, 03 Jul 2023 05:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12579v2</guid></item><item><title>Improving Fast Auto-Focus with Event Polarity</title><link>http://arxiv.org/abs/2303.08611v2</link><description>Fast and accurate auto-focus in adverse conditions remains an arduous task.The emergence of event cameras has opened up new possibilities for addressingthe challenge. This paper presents a new high-speed and accurate event-basedfocusing algorithm. Specifically, the symmetrical relationship between theevent polarities in focusing is investigated, and the event-based focusevaluation function is proposed based on the principles of the event camerasand the imaging model in the focusing process. Comprehensive experiments on thepublic event-based autofocus dataset (EAD) show the robustness of the model.Furthermore, precise focus with less than one depth of focus is achieved within0.004 seconds on our self-built high-speed focusing platform. The dataset andcode will be made publicly available.</description><author>Yuhan Bao, Lei Sun, Yuqin Ma, Diyang Gu, Kaiwei Wang</author><pubDate>Mon, 03 Jul 2023 05:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08611v2</guid></item><item><title>DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification</title><link>http://arxiv.org/abs/2305.15957v2</link><description>Large pre-trained models have had a significant impact on computer vision byenabling multi-modal learning, where the CLIP model has achieved impressiveresults in image classification, object detection, and semantic segmentation.However, the model's performance on 3D point cloud processing tasks is limiteddue to the domain gap between depth maps from 3D projection and training imagesof CLIP. This paper proposes DiffCLIP, a new pre-training framework thatincorporates stable diffusion with ControlNet to minimize the domain gap in thevisual branch. Additionally, a style-prompt generation module is introduced forfew-shot tasks in the textual branch. Extensive experiments on the ModelNet10,ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilitiesfor 3D understanding. By using stable diffusion and style-prompt generation,DiffCLIP achieves an accuracy of 43.2\% for zero-shot classification on OBJ\_BGof ScanObjectNN, which is state-of-the-art performance, and an accuracy of80.6\% for zero-shot classification on ModelNet10, which is comparable tostate-of-the-art performance.</description><author>Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu</author><pubDate>Mon, 03 Jul 2023 05:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15957v2</guid></item><item><title>A Closer Look at the Intervention Procedure of Concept Bottleneck Models</title><link>http://arxiv.org/abs/2302.14260v3</link><description>Concept bottleneck models (CBMs) are a class of interpretable neural networkmodels that predict the target response of a given input based on itshigh-level concepts. Unlike the standard end-to-end models, CBMs enable domainexperts to intervene on the predicted concepts and rectify any mistakes at testtime, so that more accurate task predictions can be made at the end. While suchintervenability provides a powerful avenue of control, many aspects of theintervention procedure remain rather unexplored. In this work, we developvarious ways of selecting intervening concepts to improve the interventioneffectiveness and conduct an array of in-depth analyses as to how they evolveunder different circumstances. Specifically, we find that an informedintervention strategy can reduce the task error more than ten times compared tothe current baseline under the same amount of intervention counts in realisticsettings, and yet, this can vary quite significantly when taking into accountdifferent intervention granularity. We verify our findings throughcomprehensive evaluations, not only on the standard real datasets, but also onsynthetic datasets that we generate based on a set of different causal graphs.We further discover some major pitfalls of the current practices which, withouta proper addressing, raise concerns on reliability and fairness of theintervention procedure.</description><author>Sungbin Shin, Yohan Jo, Sungsoo Ahn, Namhoon Lee</author><pubDate>Mon, 03 Jul 2023 04:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14260v3</guid></item><item><title>Pseudo-Bag Mixup Augmentation for Multiple Instance Learning-Based Whole Slide Image Classification</title><link>http://arxiv.org/abs/2306.16180v2</link><description>Given the special situation of modeling gigapixel images, multiple instancelearning (MIL) has become one of the most important frameworks for Whole SlideImage (WSI) classification. In current practice, most MIL networks often facetwo unavoidable problems in training: i) insufficient WSI data, and ii) thesample memorization inclination inherent in neural networks. These problems mayhinder MIL models from adequate and efficient training, suppressing thecontinuous performance promotion of classification models on WSIs. Inspired bythe basic idea of Mixup, this paper proposes a new Pseudo-bag Mixup (PseMix)data augmentation scheme to improve the training of MIL models. This schemegeneralizes the Mixup strategy for general images to special WSIs viapseudo-bags so as to be applied in MIL-based WSI classification. Cooperated bypseudo-bags, our PseMix fulfills the critical size alignment and semanticalignment in Mixup strategy. Moreover, it is designed as an efficient anddecoupled method, neither involving time-consuming operations nor relying onMIL model predictions. Comparative experiments and ablation studies arespecially designed to evaluate the effectiveness and advantages of our PseMix.Experimental results show that PseMix could often assist state-of-the-art MILnetworks to refresh the classification performance on WSIs. Besides, it couldalso boost the generalization ability of MIL models, and promote theirrobustness to patch occlusion and noisy labels. Our source code is available athttps://github.com/liupei101/PseMix.</description><author>Pei Liu, Luping Ji, Xinyu Zhang, Feng Ye</author><pubDate>Mon, 03 Jul 2023 04:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16180v2</guid></item></channel></rss>