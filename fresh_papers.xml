<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 14 May 2024 06:00:38 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MambaOut: Do We Really Need Mamba for Vision?</title><link>http://arxiv.org/abs/2405.07992v1</link><description>Mamba, an architecture with RNN-like token mixer of state space model (SSM),was recently introduced to address the quadratic complexity of the attentionmechanism and subsequently applied to vision tasks. Nevertheless, theperformance of Mamba for vision is often underwhelming when compared withconvolutional and attention-based models. In this paper, we delve into theessence of Mamba, and conceptually conclude that Mamba is ideally suited fortasks with long-sequence and autoregressive characteristics. For vision tasks,as image classification does not align with either characteristic, wehypothesize that Mamba is not necessary for this task; Detection andsegmentation tasks are also not autoregressive, yet they adhere to thelong-sequence characteristic, so we believe it is still worthwhile to exploreMamba's potential for these tasks. To empirically verify our hypotheses, weconstruct a series of models named \emph{MambaOut} through stacking Mambablocks while removing their core token mixer, SSM. Experimental resultsstrongly support our hypotheses. Specifically, our MambaOut model surpasses allvisual Mamba models on ImageNet image classification, indicating that Mamba isindeed unnecessary for this task. As for detection and segmentation, MambaOutcannot match the performance of state-of-the-art visual Mamba models,demonstrating the potential of Mamba for long-sequence visual tasks. The codeis available at https://github.com/yuweihao/MambaOut</description><author>Weihao Yu, Xinchao Wang</author><pubDate>Mon, 13 May 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07992v1</guid></item><item><title>Auto-Linear Phenomenon in Subsurface Imaging</title><link>http://arxiv.org/abs/2305.13314v2</link><description>Subsurface imaging involves solving full waveform inversion (FWI) to predictgeophysical properties from measurements. This problem can be reframed as animage-to-image translation, with the usual approach being to train anencoder-decoder network using paired data from two domains: geophysicalproperty and measurement. A recent seminal work (InvLINT) demonstrates there isonly a linear mapping between the latent spaces of the two domains, and thedecoder requires paired data for training. This paper extends this direction by demonstrating that only linear mappingnecessitates paired data, while both the encoder and decoder can be learnedfrom their respective domains through self-supervised learning. This unveils anintriguing phenomenon (named Auto-Linear) where the self-learned features oftwo separate domains are automatically linearly correlated. Compared withexisting methods, our Auto-Linear has four advantages: (a) solving both forwardand inverse modeling simultaneously, (b) applicable to different subsurfaceimaging tasks and achieving markedly better results than previous methods,(c)enhanced performance, especially in scenarios with limited paired data andin the presence of noisy data, and (d) strong generalization ability of thetrained encoder and decoder.</description><author>Yinan Feng, Yinpeng Chen, Peng Jin, Shihang Feng, Zicheng Liu, Youzuo Lin</author><pubDate>Mon, 13 May 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13314v2</guid></item><item><title>SPIN: Simultaneous Perception, Interaction and Navigation</title><link>http://arxiv.org/abs/2405.07991v1</link><description>While there has been remarkable progress recently in the fields ofmanipulation and locomotion, mobile manipulation remains a long-standingchallenge. Compared to locomotion or static manipulation, a mobile system mustmake a diverse range of long-horizon tasks feasible in unstructured and dynamicenvironments. While the applications are broad and interesting, there are aplethora of challenges in developing these systems such as coordination betweenthe base and arm, reliance on onboard perception for perceiving and interactingwith the environment, and most importantly, simultaneously integrating allthese parts together. Prior works approach the problem using disentangledmodular skills for mobility and manipulation that are trivially tied together.This causes several limitations such as compounding errors, delays indecision-making, and no whole-body coordination. In this work, we present areactive mobile manipulation framework that uses an active visual system toconsciously perceive and react to its environment. Similar to how humansleverage whole-body and hand-eye coordination, we develop a mobile manipulatorthat exploits its ability to move and see, more specifically -- to move inorder to see and to see in order to move. This allows it to not only movearound and interact with its environment but also, choose "when" to perceive"what" using an active visual system. We observe that such an agent learns tonavigate around complex cluttered scenarios while displaying agile whole-bodycoordination using only ego-vision without needing to create environment maps.Results visualizations and videos at https://spin-robot.github.io/</description><author>Shagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, Deepak Pathak</author><pubDate>Mon, 13 May 2024 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07991v1</guid></item><item><title>Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots</title><link>http://arxiv.org/abs/2405.07990v1</link><description>The remarkable progress of Multi-modal Large Language Models (MLLMs) hasattracted significant attention due to their superior performance in visualcontexts. However, their capabilities in turning visual figure to executablecode, have not been evaluated thoroughly. To address this, we introducePlot2Code, a comprehensive visual coding benchmark designed for a fair andin-depth assessment of MLLMs. We carefully collect 132 manually selectedhigh-quality matplotlib plots across six plot types from publicly availablematplotlib galleries. For each plot, we carefully offer its source code, and andescriptive instruction summarized by GPT-4. This approach enables Plot2Code toextensively evaluate MLLMs' code capabilities across various input modalities.Furthermore, we propose three automatic evaluation metrics, including code passrate, text-match ratio, and GPT-4V overall rating, for a fine-grainedassessment of the output code and rendered images. Instead of simply judgingpass or fail, we employ GPT-4V to make an overall judgement between thegenerated and reference images, which has been shown to be consistent withhuman evaluation. The evaluation results, which include analyses of 14 MLLMssuch as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini,highlight the substantial challenges presented by Plot2Code. With Plot2Code, wereveal that most existing MLLMs struggle with visual coding for text-denseplots, heavily relying on textual instruction. We hope that the evaluationresults from Plot2Code on visual coding will guide the future development ofMLLMs. All data involved with Plot2Code are available athttps://huggingface.co/datasets/TencentARC/Plot2Code.</description><author>Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, Ping Luo</author><pubDate>Mon, 13 May 2024 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07990v1</guid></item><item><title>A Generalist Learner for Multifaceted Medical Image Interpretation</title><link>http://arxiv.org/abs/2405.07988v1</link><description>Current medical artificial intelligence systems are often limited to narrowapplications, hindering their widespread adoption in clinical practice. Toaddress this limitation, we propose MedVersa, a generalist learner that enablesflexible learning and tasking for medical image interpretation. By leveraging alarge language model as a learnable orchestrator, MedVersa can learn from bothvisual and linguistic supervision, support multimodal inputs, and performreal-time task specification. This versatility allows MedVersa to adapt tovarious clinical scenarios and perform multifaceted medical image analysis. Weintroduce MedInterp, the largest multimodal dataset to date for medical imageinterpretation, consisting of over 13 million annotated instances spanning 11tasks across 3 modalities, to support the development of MedVersa. Ourexperiments demonstrate that MedVersa achieves state-of-the-art performance in9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersais the first to showcase the viability of multimodal generative medical AI inimplementing multimodal outputs, inputs, and dynamic task specification,highlighting its potential as a multifunctional system for comprehensivemedical image analysis. This generalist approach to medical imageinterpretation paves the way for more adaptable and efficient AI-assistedclinical decision-making.</description><author>Hong-Yu Zhou, Subathra Adithan, Julián Nicolás Acosta, Eric J. Topol, Pranav Rajpurkar</author><pubDate>Mon, 13 May 2024 18:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07988v1</guid></item><item><title>The Platonic Representation Hypothesis</title><link>http://arxiv.org/abs/2405.07987v1</link><description>We argue that representations in AI models, particularly deep networks, areconverging. First, we survey many examples of convergence in the literature:over time and across multiple domains, the ways by which different neuralnetworks represent data are becoming more aligned. Next, we demonstrateconvergence across data modalities: as vision models and language models getlarger, they measure distance between datapoints in a more and more alike way.We hypothesize that this convergence is driving toward a shared statisticalmodel of reality, akin to Plato's concept of an ideal reality. We term such arepresentation the platonic representation and discuss several possibleselective pressures toward it. Finally, we discuss the implications of thesetrends, their limitations, and counterexamples to our analysis.</description><author>Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola</author><pubDate>Mon, 13 May 2024 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07987v1</guid></item><item><title>A Demographic-Conditioned Variational Autoencoder for fMRI Distribution Sampling and Removal of Confounds</title><link>http://arxiv.org/abs/2405.07977v1</link><description>Objective: fMRI and derived measures such as functional connectivity (FC)have been used to predict brain age, general fluid intelligence, psychiatricdisease status, and preclinical neurodegenerative disease. However, it is notalways clear that all demographic confounds, such as age, sex, and race, havebeen removed from fMRI data. Additionally, many fMRI datasets are restricted toauthorized researchers, making dissemination of these valuable data sourceschallenging. Methods: We create a variational autoencoder (VAE)-based model,DemoVAE, to decorrelate fMRI features from demographics and generatehigh-quality synthetic fMRI data based on user-supplied demographics. We trainand validate our model using two large, widely used datasets, the PhiladelphiaNeurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network forIntermediate Phenotypes (BSNIP). Results: We find that DemoVAE recapitulatesgroup differences in fMRI data while capturing the full breadth of individualvariations. Significantly, we also find that most clinical and computerizedbattery fields that are correlated with fMRI data are not correlated withDemoVAE latents. An exception are several fields related to schizophreniamedication and symptom severity. Conclusion: Our model generates fMRI data thatcaptures the full distribution of FC better than traditional VAE or GAN models.We also find that most prediction using fMRI data is dependent on correlationwith, and prediction of, demographics. Significance: Our DemoVAE model allowsfor generation of high quality synthetic data conditioned on subjectdemographics as well as the removal of the confounding effects of demographics.We identify that FC-based prediction tasks are highly influenced by demographicconfounds.</description><author>Anton Orlichenko, Gang Qu, Ziyu Zhou, Anqi Liu, Hong-Wen Deng, Zhengming Ding, Julia M. Stephen, Tony W. Wilson, Vince D. Calhoun, Yu-Ping Wang</author><pubDate>Mon, 13 May 2024 18:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07977v1</guid></item><item><title>Localized Adaptive Risk Control</title><link>http://arxiv.org/abs/2405.07976v1</link><description>Adaptive Risk Control (ARC) is an online calibration strategy based on setprediction that offers worst-case deterministic long-term risk control, as wellas statistical marginal coverage guarantees. ARC adjusts the size of theprediction set by varying a single scalar threshold based on feedback from pastdecisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC),an online calibration scheme that targets statistical localized risk guaranteesranging from conditional risk to marginal risk, while preserving the worst-caseperformance of ARC. L-ARC updates a threshold function within a reproducingkernel Hilbert space (RKHS), with the kernel determining the level oflocalization of the statistical risk guarantee. The theoretical resultshighlight a trade-off between localization of the statistical risk andconvergence speed to the long-term risk target. Thanks to localization, L-ARCis demonstrated via experiments to produce prediction sets with risk guaranteesacross different data subpopulations, significantly improving the fairness ofthe calibrated model for tasks such as image segmentation and beam selection inwireless networks.</description><author>Matteo Zecchin, Osvaldo Simeone</author><pubDate>Mon, 13 May 2024 18:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07976v1</guid></item><item><title>SignAvatar: Sign Language 3D Motion Reconstruction and Generation</title><link>http://arxiv.org/abs/2405.07974v1</link><description>Achieving expressive 3D motion reconstruction and automatic generation forisolated sign words can be challenging, due to the lack of real-world 3Dsign-word data, the complex nuances of signing motions, and the cross-modalunderstanding of sign language semantics. To address these challenges, weintroduce SignAvatar, a framework capable of both word-level sign languagereconstruction and generation. SignAvatar employs a transformer-basedconditional variational autoencoder architecture, effectively establishingrelationships across different semantic modalities. Additionally, this approachincorporates a curriculum learning strategy to enhance the model's robustnessand generalization, resulting in more realistic motions. Furthermore, wecontribute the ASL3DWord dataset, composed of 3D joint rotation data for thebody, hands, and face, for unique sign words. We demonstrate the effectivenessof SignAvatar through extensive experiments, showcasing its superiorreconstruction and automatic generation capabilities. The code and dataset areavailable on the project page.</description><author>Lu Dong, Lipisha Chaudhary, Fei Xu, Xiao Wang, Mason Lary, Ifeoma Nwogu</author><pubDate>Mon, 13 May 2024 18:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07974v1</guid></item><item><title>Sensitivity Analysis for Active Sampling, with Applications to the Simulation of Analog Circuits</title><link>http://arxiv.org/abs/2405.07971v1</link><description>We propose an active sampling flow, with the use-case of simulating theimpact of combined variations on analog circuits. In such a context, given thelarge number of parameters, it is difficult to fit a surrogate model and toefficiently explore the space of design features. By combining a drastic dimension reduction using sensitivity analysis andBayesian surrogate modeling, we obtain a flexible active sampling flow. Onsynthetic and real datasets, this flow outperforms the usual Monte-Carlosampling which often forms the foundation of design space exploration.</description><author>Reda Chhaibi, Fabrice Gamboa, Christophe Oger, Vinicius Oliveira, Clément Pellegrini, Damien Remot</author><pubDate>Mon, 13 May 2024 18:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07971v1</guid></item><item><title>Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation</title><link>http://arxiv.org/abs/2405.07969v1</link><description>Zero-shot anomaly segmentation using pre-trained foundation models is apromising approach that enables effective algorithms without expensive,domain-specific training or fine-tuning. Ensuring that these methods workacross various environmental conditions and are robust to distribution shiftsis an open problem. We investigate the performance of WinCLIP [14] zero-shotanomaly segmentation algorithm by perturbing test data using three semantictransformations: bounded angular rotations, bounded saturation shifts, and hueshifts. We empirically measure a lower performance bound by aggregating acrossper-sample worst-case perturbations and find that average performance drops byup to 20% in area under the ROC curve and 40% in area under the per-regionoverlap curve. We find that performance is consistently lowered on three CLIPbackbones, regardless of model architecture or learning objective,demonstrating a need for careful performance evaluation.</description><author>Kevin Stangl, Marius Arvinte, Weilin Xu, Cory Cornelius</author><pubDate>Mon, 13 May 2024 18:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07969v1</guid></item><item><title>OverlapMamba: Novel Shift State Space Model for LiDAR-based Place Recognition</title><link>http://arxiv.org/abs/2405.07966v1</link><description>Place recognition is the foundation for enabling autonomous systems toachieve independent decision-making and safe operations. It is also crucial intasks such as loop closure detection and global localization within SLAM.Previous methods utilize mundane point cloud representations as input and deeplearning-based LiDAR-based Place Recognition (LPR) approaches employingdifferent point cloud image inputs with convolutional neural networks (CNNs) ortransformer architectures. However, the recently proposed Mamba deep learningmodel, combined with state space models (SSMs), holds great potential for longsequence modeling. Therefore, we developed OverlapMamba, a novel network forplace recognition, which represents input range views (RVs) as sequences. In anovel way, we employ a stochastic reconstruction approach to build shift statespace models, compressing the visual representation. Evaluated on threedifferent public datasets, our method effectively detects loop closures,showing robustness even when traversing previously visited locations fromdifferent directions. Relying on raw range view inputs, it outperforms typicalLiDAR and multi-view combination methods in time complexity and speed,indicating strong place recognition capabilities and real-time efficiency.</description><author>Qiuchi Xiang, Jintao Cheng, Jiehao Luo, Jin Wu, Rui Fan, Xieyuanli Chen, Xiaoyu Tang</author><pubDate>Mon, 13 May 2024 18:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07966v1</guid></item><item><title>Fast Computation of Superquantile-Constrained Optimization Through Implicit Scenario Reduction</title><link>http://arxiv.org/abs/2405.07965v1</link><description>Superquantiles have recently gained significant interest as a risk-awaremetric for addressing fairness and distribution shifts in statistical learningand decision making problems. This paper introduces a fast, scalable and robustsecond-order computational framework to solve large-scale optimization problemswith superquantile-based constraints. Unlike empirical risk minimization,superquantile-based optimization requires ranking random functions evaluatedacross all scenarios to compute the tail conditional expectation. While thistail-based feature might seem computationally unfriendly, it provides anadvantageous setting for a semismooth-Newton-based augmented Lagrangian method.The superquantile operator effectively reduces the dimensions of the Newtonsystems since the tail expectation involves considerably fewer scenarios.Notably, the extra cost of obtaining relevant second-order information andperforming matrix inversions is often comparable to, and sometimes even lessthan, the effort required for gradient computation. Our developed solver isparticularly effective when the number of scenarios substantially exceeds thenumber of decision variables. In synthetic problems with linear and convexdiagonal quadratic objectives, numerical experiments demonstrate that ourmethod outperforms existing approaches by a large margin: It achieves speedsmore than 750 times faster for linear and quadratic objectives than thealternating direction method of multipliers as implemented by OSQP forcomputing low-accuracy solutions. Additionally, it is up to 25 times faster forlinear objectives and 70 times faster for quadratic objectives than thecommercial solver Gurobi, and 20 times faster for linear objectives and 30times faster for quadratic objectives than the Portfolio Safeguard optimizationsuite for high-accuracy solution computations.</description><author>Jake Roth, Ying Cui</author><pubDate>Mon, 13 May 2024 18:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07965v1</guid></item><item><title>AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments</title><link>http://arxiv.org/abs/2405.07960v1</link><description>Diagnosing and managing a patient is a complex, sequential decision makingprocess that requires physicians to obtain information -- such as which teststo perform -- and to act upon it. Recent advances in artificial intelligence(AI) and large language models (LLMs) promise to profoundly impact clinicalcare. However, current evaluation schemes overrely on static medicalquestion-answering benchmarks, falling short on interactive decision-makingthat is required in real-life clinical work. Here, we present AgentClinic: amultimodal benchmark to evaluate LLMs in their ability to operate as agents insimulated clinical environments. In our benchmark, the doctor agent mustuncover the patient's diagnosis through dialogue and active data collection. Wepresent two open benchmarks: a multimodal image and dialogue environment,AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embedcognitive and implicit biases both in patient and doctor agents to emulaterealistic interactions between biased agents. We find that introducing biasleads to large reductions in diagnostic accuracy of the doctor agents, as wellas reduced compliance, confidence, and follow-up consultation willingness inpatient agents. Evaluating a suite of state-of-the-art LLMs, we find thatseveral models that excel in benchmarks like MedQA are performing poorly inAgentClinic-MedQA. We find that the LLM used in the patient agent is animportant factor for performance in the AgentClinic benchmark. We show thatboth having limited interactions as well as too many interaction reducesdiagnostic accuracy in doctor agents. The code and data for this work ispublicly available at https://AgentClinic.github.io.</description><author>Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, Michael Moor</author><pubDate>Mon, 13 May 2024 18:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07960v1</guid></item><item><title>Kreyòl-MT: Building MT for Latin American, Caribbean and Colonial African Creole Languages</title><link>http://arxiv.org/abs/2405.05376v2</link><description>A majority of language technologies are tailored for a small number ofhigh-resource languages, while relatively many low-resource languages areneglected. One such group, Creole languages, have long been marginalized inacademic study, though their speakers could benefit from machine translation(MT). These languages are predominantly used in much of Latin America, Africaand the Caribbean. We present the largest cumulative dataset to date for Creolelanguage MT, including 14.5M unique Creole sentences with parallel translations-- 11.6M of which we release publicly, and the largest bitexts gathered to datefor 41 languages -- the first ever for 21. In addition, we provide MT modelssupporting all 41 Creole languages in 172 translation directions. Given ourdiverse dataset, we produce a model for Creole language MT exposed to moregenre diversity than ever before, which outperforms a genre-specific Creole MTmodel on its own benchmark for 26 of 34 translation directions.</description><author>Nathaniel R. Robinson, Raj Dabre, Ammon Shurtz, Rasul Dent, Onenamiyi Onesi, Claire Bizon Monroc, Loïc Grobol, Hasan Muhammad, Ashi Garg, Naome A. Etori, Vijay Murari Tiyyala, Olanrewaju Samuel, Matthew Dean Stutzman, Bismarck Bamfo Odoom, Sanjeev Khudanpur, Stephen D. Richardson, Kenton Murray</author><pubDate>Mon, 13 May 2024 18:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05376v2</guid></item><item><title>Hierarchical Decision Mamba</title><link>http://arxiv.org/abs/2405.07943v1</link><description>Recent advancements in imitation learning have been largely fueled by theintegration of sequence models, which provide a structured flow of informationto effectively mimic task behaviours. Currently, Decision Transformer (DT) andsubsequently, the Hierarchical Decision Transformer (HDT), presentedTransformer-based approaches to learn task policies. Recently, the Mambaarchitecture has shown to outperform Transformers across various task domains.In this work, we introduce two novel methods, Decision Mamba (DM) andHierarchical Decision Mamba (HDM), aimed at enhancing the performance of theTransformer models. Through extensive experimentation across diverseenvironments such as OpenAI Gym and D4RL, leveraging varying demonstration datasets, we demonstrate the superiority of Mamba models over their Transformercounterparts in a majority of tasks. Results show that HDM outperforms othermethods in most settings. The code can be found athttps://github.com/meowatthemoon/HierarchicalDecisionMamba.</description><author>André Correia, Luís A. Alexandre</author><pubDate>Mon, 13 May 2024 18:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07943v1</guid></item><item><title>RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors</title><link>http://arxiv.org/abs/2405.07940v1</link><description>Many commercial and open-source models claim to detect machine-generated textwith very high accuracy (99\% or higher). However, very few of these detectorsare evaluated on shared benchmark datasets and even when they are, the datasetsused for evaluation are insufficiently challenging -- lacking variations insampling strategy, adversarial attacks, and open-source generative models. Inthis work we present RAID: the largest and most challenging benchmark datasetfor machine-generated text detection. RAID includes over 6 million generationsspanning 11 models, 8 domains, 11 adversarial attacks and 4 decodingstrategies. Using RAID, we evaluate the out-of-domain and adversarialrobustness of 8 open- and 4 closed-source detectors and find that currentdetectors are easily fooled by adversarial attacks, variations in samplingstrategies, repetition penalties, and unseen generative models. We release ourdataset and tools to encourage further exploration into detector robustness.</description><author>Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch</author><pubDate>Mon, 13 May 2024 18:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07940v1</guid></item><item><title>EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning</title><link>http://arxiv.org/abs/2405.07938v1</link><description>In this paper, we introduce EconLogicQA, a rigorous benchmark designed toassess the sequential reasoning capabilities of large language models (LLMs)within the intricate realms of economics, business, and supply chainmanagement. Diverging from traditional benchmarks that predict subsequentevents individually, EconLogicQA poses a more challenging task: it requiresmodels to discern and sequence multiple interconnected events, capturing thecomplexity of economic logics. EconLogicQA comprises an array of multi-eventscenarios derived from economic articles, which necessitate an insightfulunderstanding of both temporal and logical event relationships. Throughcomprehensive evaluations, we exhibit that EconLogicQA effectively gauges aLLM's proficiency in navigating the sequential complexities inherent ineconomic contexts. We provide a detailed description of EconLogicQA dataset andshows the outcomes from evaluating the benchmark across various leading-edgeLLMs, thereby offering a thorough perspective on their sequential reasoningpotential in economic contexts. Our benchmark dataset is available athttps://huggingface.co/datasets/yinzhu-quan/econ_logic_qa.</description><author>Yinzhu Quan, Zefang Liu</author><pubDate>Mon, 13 May 2024 18:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07938v1</guid></item><item><title>Active Learning with Simple Questions</title><link>http://arxiv.org/abs/2405.07937v1</link><description>We consider an active learning setting where a learner is presented with apool S of n unlabeled examples belonging to a domain X and asks queries to findthe underlying labeling that agrees with a target concept h^* \in H. In contrast to traditional active learning that queries a single example forits label, we study more general region queries that allow the learner to picka subset of the domain T \subset X and a target label y and ask a labelerwhether h^*(x) = y for every example in the set T \cap S. Such more powerful queries allow us to bypass the limitations of traditionalactive learning and use significantly fewer rounds of interactions to learn butcan potentially lead to a significantly more complex query language. Our maincontribution is quantifying the trade-off between the number of queries and thecomplexity of the query language used by the learner. We measure the complexity of the region queries via the VC dimension of thefamily of regions. We show that given any hypothesis class H with VC dimensiond, one can design a region query family Q with VC dimension O(d) such that forevery set of n examples S \subset X and every h^* \in H, a learner can submitO(d log n) queries from Q to a labeler and perfectly label S. We show amatching lower bound by designing a hypothesis class H with VC dimension d anda dataset S \subset X of size n such that any learning algorithm using anyquery class with VC dimension O(d) must make poly(n) queries to label Sperfectly. Finally, we focus on well-studied hypothesis classes including unions ofintervals, high-dimensional boxes, and d-dimensional halfspaces, and obtainstronger results. In particular, we design learning algorithms that (i) arecomputationally efficient and (ii) work even when the queries are not answeredbased on the learner's pool of examples S but on some unknown superset L of S</description><author>Vasilis Kontonis, Mingchen Ma, Christos Tzamos</author><pubDate>Mon, 13 May 2024 18:13:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07937v1</guid></item><item><title>Authentic Hand Avatar from a Phone Scan via Universal Hand Model</title><link>http://arxiv.org/abs/2405.07933v1</link><description>The authentic 3D hand avatar with every identifiable information, such ashand shapes and textures, is necessary for immersive experiences in AR/VR. Inthis paper, we present a universal hand model (UHM), which 1) can universallyrepresent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) canbe adapted to each person with a short phone scan for the authentic handavatar. For effective universal hand modeling, we perform tracking and modelingat the same time, while previous 3D hand models perform them separately. Theconventional separate pipeline suffers from the accumulated errors from thetracking stage, which cannot be recovered in the modeling stage. On the otherhand, ours does not suffer from the accumulated errors while having a much moreconcise overall pipeline. We additionally introduce a novel image matching lossfunction to address a skin sliding during the tracking and modeling, whileexisting works have not focused on it much. Finally, using learned priors fromour UHM, we effectively adapt our UHM to each person's short phone scan for theauthentic hand avatar.</description><author>Gyeongsik Moon, Weipeng Xu, Rohan Joshi, Chenglei Wu, Takaaki Shiratori</author><pubDate>Mon, 13 May 2024 18:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07933v1</guid></item><item><title>PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition</title><link>http://arxiv.org/abs/2405.07932v1</link><description>Large language models (LLMs) have shown success in many natural languageprocessing tasks. Despite rigorous safety alignment processes, supposedlysafety-aligned LLMs like Llama 2 and Claude 2 are still susceptible tojailbreaks, leading to security risks and abuse of the models. One option tomitigate such risks is to augment the LLM with a dedicated "safeguard", whichchecks the LLM's inputs or outputs for undesired behaviour. A promisingapproach is to use the LLM itself as the safeguard. Nonetheless, baselinemethods, such as prompting the LLM to self-classify toxic content, demonstratelimited efficacy. We hypothesise that this is due to domain shift: thealignment training imparts a self-censoring behaviour to the model ("Sorry Ican't do that"), while the self-classify approach shifts it to a classificationformat ("Is this prompt malicious"). In this work, we propose PARDEN, whichavoids this domain shift by simply asking the model to repeat its own outputs.PARDEN neither requires finetuning nor white box access to the model. Weempirically verify the effectiveness of our method and show that PARDENsignificantly outperforms existing jailbreak detection baselines for Llama-2and Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN. We find that PARDEN is particularly powerful in the relevant regime of highTrue Positive Rate (TPR) and low False Positive Rate (FPR). For instance, forLlama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction inthe FPR from 24.8% to 2.0% on the harmful behaviours dataset.</description><author>Ziyang Zhang, Qizhen Zhang, Jakob Foerster</author><pubDate>Mon, 13 May 2024 18:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07932v1</guid></item><item><title>Look Once to Hear: Target Speech Hearing with Noisy Examples</title><link>http://arxiv.org/abs/2405.06289v2</link><description>In crowded settings, the human brain can focus on speech from a targetspeaker, given prior knowledge of how they sound. We introduce a novelintelligent hearable system that achieves this capability, enabling targetspeech hearing to ignore all interfering speech and noise, but the targetspeaker. A naive approach is to require a clean speech example to enroll thetarget speaker. This is however not well aligned with the hearable applicationdomain since obtaining a clean example is challenging in real world scenarios,creating a unique user interface problem. We present the first enrollmentinterface where the wearer looks at the target speaker for a few seconds tocapture a single, short, highly noisy, binaural example of the target speaker.This noisy example is used for enrollment and subsequent speech extraction inthe presence of interfering speakers and noise. Our system achieves a signalquality improvement of 7.01 dB using less than 5 seconds of noisy enrollmentaudio and can process 8 ms of audio chunks in 6.24 ms on an embedded CPU. Ouruser studies demonstrate generalization to real-world static and mobilespeakers in previously unseen indoor and outdoor multipath environments.Finally, our enrollment interface for noisy examples does not cause performancedegradation compared to clean examples, while being convenient anduser-friendly. Taking a step back, this paper takes an important step towardsenhancing the human auditory perception with artificial intelligence. Weprovide code and data at: https://github.com/vb000/LookOnceToHear.</description><author>Bandhav Veluri, Malek Itani, Tuochao Chen, Takuya Yoshioka, Shyamnath Gollakota</author><pubDate>Mon, 13 May 2024 18:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06289v2</guid></item><item><title>Improving Multimodal Learning with Multi-Loss Gradient Modulation</title><link>http://arxiv.org/abs/2405.07930v1</link><description>Learning from multiple modalities, such as audio and video, offersopportunities for leveraging complementary information, enhancing robustness,and improving contextual understanding and performance. However, combining suchmodalities presents challenges, especially when modalities differ in datastructure, predictive contribution, and the complexity of their learningprocesses. It has been observed that one modality can potentially dominate thelearning process, hindering the effective utilization of information from othermodalities and leading to sub-optimal model performance. To address this issuethe vast majority of previous works suggest to assess the unimodalcontributions and dynamically adjust the training to equalize them. We improveupon previous work by introducing a multi-loss objective and further refiningthe balancing process, allowing it to dynamically adjust the learning pace ofeach modality in both directions, acceleration and deceleration, with theability to phase out balancing effects upon convergence. We achieve superiorresults across three audio-video datasets: on CREMA-D, models with ResNetbackbone encoders surpass the previous best by 1.9% to 12.4%, and Conformerbackbone models deliver improvements ranging from 2.8% to 14.1% acrossdifferent fusion methods. On AVE, improvements range from 2.7% to 7.7%, whileon UCF101, gains reach up to 6.1%.</description><author>Konstantinos Kontras, Christos Chatzichristos, Matthew Blaschko, Maarten De Vos</author><pubDate>Mon, 13 May 2024 18:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07930v1</guid></item><item><title>Stable Diffusion-based Data Augmentation for Federated Learning with Non-IID Data</title><link>http://arxiv.org/abs/2405.07925v1</link><description>The proliferation of edge devices has brought Federated Learning (FL) to theforefront as a promising paradigm for decentralized and collaborative modeltraining while preserving the privacy of clients' data. However, FL struggleswith a significant performance reduction and poor convergence when confrontedwith Non-Independent and Identically Distributed (Non-IID) data distributionsamong participating clients. While previous efforts, such as client driftmitigation and advanced server-side model fusion techniques, have shown somesuccess in addressing this challenge, they often overlook the root cause of theperformance reduction - the absence of identical data accurately mirroring theglobal data distribution among clients. In this paper, we introduce Gen-FedSD,a novel approach that harnesses the powerful capability of state-of-the-arttext-to-image foundation models to bridge the significant Non-IID performancegaps in FL. In Gen-FedSD, each client constructs textual prompts for each classlabel and leverages an off-the-shelf state-of-the-art pre-trained StableDiffusion model to synthesize high-quality data samples. The generatedsynthetic data is tailored to each client's unique local data gaps anddistribution disparities, effectively making the final augmented local dataIID. Through extensive experimentation, we demonstrate that Gen-FedSD achievesstate-of-the-art performance and significant communication cost savings acrossvarious datasets and Non-IID settings.</description><author>Mahdi Morafah, Matthias Reisser, Bill Lin, Christos Louizos</author><pubDate>Mon, 13 May 2024 17:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07925v1</guid></item><item><title>Can Better Text Semantics in Prompt Tuning Improve VLM Generalization?</title><link>http://arxiv.org/abs/2405.07921v1</link><description>Going beyond mere fine-tuning of vision-language models (VLMs), learnableprompt tuning has emerged as a promising, resource-efficient alternative.Despite their potential, effectively learning prompts faces the followingchallenges: (i) training in a low-shot scenario results in overfitting,limiting adaptability and yielding weaker performance on newer classes ordatasets; (ii) prompt-tuning's efficacy heavily relies on the label space, withdecreased performance in large class spaces, signaling potential gaps inbridging image and class concepts. In this work, we ask the question if bettertext semantics can help address these concerns. In particular, we introduce aprompt-tuning method that leverages class descriptions obtained from largelanguage models (LLMs). Our approach constructs part-level description-guidedviews of both image and text features, which are subsequently aligned to learnmore generalizable prompts. Our comprehensive experiments, conducted across 11benchmark datasets, outperform established methods, demonstrating substantialimprovements.</description><author>Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian</author><pubDate>Mon, 13 May 2024 17:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07921v1</guid></item><item><title>Exploring the Low-Pass Filtering Behavior in Image Super-Resolution</title><link>http://arxiv.org/abs/2405.07919v1</link><description>Deep neural networks for image super-resolution have shown significantadvantages over traditional approaches like interpolation. However, they areoften criticized as `black boxes' compared to traditional approaches which havesolid mathematical foundations. In this paper, we attempt to interpret thebehavior of deep neural networks using theories from signal processingtheories. We first report an intriguing phenomenon, referred to as `the sincphenomenon,' which occurs when an impulse input is fed to a neural network.Building on this observation, we propose a method named Hybird ResponseAnalysis (HyRA) to analyze the behavior of neural networks in imagesuper-resolution tasks. In details, HyRA decomposes a neural network into aparallel connection of a linear system and a non-linear system, demonstratingthat the linear system functions as a low-pass filter, while the non-linearsystem injects high-frequency information. Furthermore, to quantify theinjected high-frequency information, we introduce a metric for image-to-imagetasks called Frequency Spectrum Distribution Similarity (FSDS). FSDS reflectsthe distribution similarity of different frequency components, capturingnuances that traditional metrics may overlook. Code for this work can be foundin: https://github.com/RisingEntropy/LPFInISR.</description><author>Haoyu Deng, Zijing Xu, Yule Duan, Xiao Wu, Wenjie Shu, Liang-Jian Deng</author><pubDate>Mon, 13 May 2024 17:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07919v1</guid></item><item><title>IMAFD: An Interpretable Multi-stage Approach to Flood Detection from time series Multispectral Data</title><link>http://arxiv.org/abs/2405.07916v1</link><description>In this paper, we address two critical challenges in the domain of flooddetection: the computational expense of large-scale time series changedetection and the lack of interpretable decision-making processes onexplainable AI (XAI). To overcome these challenges, we proposed aninterpretable multi-stage approach to flood detection, IMAFD has been proposed.It provides an automatic, efficient and interpretable solution suitable forlarge-scale remote sensing tasks and offers insight into the decision-makingprocess. The proposed IMAFD approach combines the analysis of the dynamic timeseries image sequences to identify images with possible flooding with thestatic, within-image semantic segmentation. It combines anomaly detection (atboth image and pixel level) with semantic segmentation. The flood detectionproblem is addressed through four stages: (1) at a sequence level: identifyingthe suspected images (2) at a multi-image level: detecting change withinsuspected images (3) at an image level: semantic segmentation of images intoLand, Water or Cloud class (4) decision making. Our contributions are twofolder. First, we efficiently reduced the number of frames to be processed fordense change detection by providing a multi-stage holistic approach to flooddetection. Second, the proposed semantic change detection method (stage 3)provides human users with an interpretable decision-making process, while mostof the explainable AI (XAI) methods provide post hoc explanations. Theevaluation of the proposed IMAFD framework was performed on three datasets,WorldFloods, RavAEn and MediaEval. For all the above datasets, the proposedframework demonstrates a competitive performance compared to other methodsoffering also interpretability and insight.</description><author>Ziyang Zhang, Plamen Angelov, Dmitry Kangin, Nicolas Longépé</author><pubDate>Mon, 13 May 2024 17:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07916v1</guid></item><item><title>Distribution Learning Meets Graph Structure Sampling</title><link>http://arxiv.org/abs/2405.07914v1</link><description>This work establishes a novel link between the problem of PAC-learninghigh-dimensional graphical models and the task of (efficient) counting andsampling of graph structures, using an online learning framework. We observe that if we apply the exponentially weighted average (EWA) orrandomized weighted majority (RWM) forecasters on a sequence of samples from adistribution P using the log loss function, the average regret incurred by theforecaster's predictions can be used to bound the expected KL divergencebetween P and the predictions. Known regret bounds for EWA and RWM then yieldnew sample complexity bounds for learning Bayes nets. Moreover, thesealgorithms can be made computationally efficient for several interestingclasses of Bayes nets. Specifically, we give a new sample-optimal andpolynomial time learning algorithm with respect to trees of unknown structureand the first polynomial sample and time algorithm for learning with respect toBayes nets over a given chordal skeleton.</description><author>Arnab Bhattacharyya, Sutanu Gayen, Philips George John, Sayantan Sen, N. V. Vinodchandran</author><pubDate>Mon, 13 May 2024 17:47:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07914v1</guid></item><item><title>CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control &amp; Altering of T2I Models</title><link>http://arxiv.org/abs/2405.07913v1</link><description>Text-to-image generative models have become a prominent and powerful toolthat excels at generating high-resolution realistic images. However, guidingthe generative process of these models to consider detailed forms ofconditioning reflecting style and/or structure information remains an openproblem. In this paper, we present LoRAdapter, an approach that unifies bothstyle and structure conditioning under the same formulation using a novelconditional LoRA block that enables zero-shot control. LoRAdapter is anefficient, powerful, and architecture-agnostic approach to conditiontext-to-image diffusion models, which enables fine-grained control conditioningduring generation and outperforms recent state-of-the-art approaches</description><author>Nick Stracke, Stefan Andreas Baumann, Joshua M. Susskind, Miguel Angel Bautista, Björn Ommer</author><pubDate>Mon, 13 May 2024 17:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07913v1</guid></item><item><title>MRSegmentator: Robust Multi-Modality Segmentation of 40 Classes in MRI and CT Sequences</title><link>http://arxiv.org/abs/2405.06463v2</link><description>Purpose: To introduce a deep learning model capable of multi-organsegmentation in MRI scans, offering a solution to the current limitations inMRI analysis due to challenges in resolution, standardized intensity values,and variability in sequences. Materials and Methods: he model was trained on 1,200 manually annotated MRIscans from the UK Biobank, 221 in-house MRI scans and 1228 CT scans, leveragingcross-modality transfer learning from CT segmentation models. Ahuman-in-the-loop annotation workflow was employed to efficiently createhigh-quality segmentations. The model's performance was evaluated on NAKO andthe AMOS22 dataset containing 600 and 60 MRI examinations. Dice SimilarityCoefficient (DSC) and Hausdorff Distance (HD) was used to assess segmentationaccuracy. The model will be open sourced. Results: The model showcased high accuracy in segmenting well-defined organs,achieving Dice Similarity Coefficient (DSC) scores of 0.97 for the right andleft lungs, and 0.95 for the heart. It also demonstrated robustness in organslike the liver (DSC: 0.96) and kidneys (DSC: 0.95 left, 0.95 right), whichpresent more variability. However, segmentation of smaller and complexstructures such as the portal and splenic veins (DSC: 0.54) and adrenal glands(DSC: 0.65 left, 0.61 right) revealed the need for further model optimization. Conclusion: The proposed model is a robust, tool for accurate segmentation of40 anatomical structures in MRI and CT images. By leveraging cross-modalitylearning and interactive annotation, the model achieves strong performance andgeneralizability across diverse datasets, making it a valuable resource forresearchers and clinicians. It is open source and can be downloaded fromhttps://github.com/hhaentze/MRSegmentator.</description><author>Hartmut Häntze, Lina Xu, Felix J. Dorfner, Leonhard Donle, Daniel Truhn, Hugo Aerts, Mathias Prokop, Bram van Ginneken, Alessa Hering, Lisa C. Adams, Keno K. Bressem</author><pubDate>Mon, 13 May 2024 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06463v2</guid></item><item><title>PLUTO: Pathology-Universal Transformer</title><link>http://arxiv.org/abs/2405.07905v1</link><description>Pathology is the study of microscopic inspection of tissue, and a pathologydiagnosis is often the medical gold standard to diagnose disease. Pathologyimages provide a unique challenge for computer-vision-based analysis: a singlepathology Whole Slide Image (WSI) is gigapixel-sized and often containshundreds of thousands to millions of objects of interest across multipleresolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO):a light-weight pathology FM that is pre-trained on a diverse dataset of 195million image tiles collected from multiple sites and extracts meaningfulrepresentations across multiple WSI scales that enable a large variety ofdownstream pathology tasks. In particular, we design task-specific adaptationheads that utilize PLUTO's output embeddings for tasks which span pathologyscales ranging from subcellular to slide-scale, including instancesegmentation, tile classification, and slide-level prediction. We comparePLUTO's performance to other state-of-the-art methods on a diverse set ofexternal and internal benchmarks covering multiple biologically relevant tasks,tissue types, resolutions, stains, and scanners. We find that PLUTO matches oroutperforms existing task-specific baselines and pathology-specific foundationmodels, some of which use orders-of-magnitude larger datasets and model sizeswhen compared to PLUTO. Our findings present a path towards a universalembedding to power pathology image analysis, and motivate further explorationaround pathology foundation models in terms of data diversity, architecturalimprovements, sample efficiency, and practical deployability in real-worldapplications.</description><author>Dinkar Juyal, Harshith Padigela, Chintan Shah, Daniel Shenker, Natalia Harguindeguy, Yi Liu, Blake Martin, Yibo Zhang, Michael Nercessian, Miles Markey, Isaac Finberg, Kelsey Luu, Daniel Borders, Syed Ashar Javed, Emma Krause, Raymond Biju, Aashish Sood, Allen Ma, Jackson Nyman, John Shamshoian, Guillaume Chhor, Darpan Sanghavi, Marc Thibault, Limin Yu, Fedaa Najdawi, Jennifer A. Hipp, Darren Fahy, Benjamin Glass, Eric Walk, John Abel, Harsha Pokkalla, Andrew H. Beck, Sean Grullon</author><pubDate>Mon, 13 May 2024 17:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07905v1</guid></item><item><title>Science based AI model certification for new operational environments with application in traffic state estimation</title><link>http://arxiv.org/abs/2405.07893v1</link><description>The expanding role of Artificial Intelligence (AI) in diverse engineeringdomains highlights the challenges associated with deploying AI models in newoperational environments, involving substantial investments in data collectionand model training. Rapid application of AI necessitates evaluating thefeasibility of utilizing pre-trained models in unobserved operational settingswith minimal or no additional data. However, interpreting the opaque nature ofAI's black-box models remains a persistent challenge. Addressing this issue,this paper proposes a science-based certification methodology to assess theviability of employing pre-trained data-driven models in new operationalenvironments. The methodology advocates a profound integration of domainknowledge, leveraging theoretical and analytical models from physics andrelated disciplines, with data-driven AI models. This novel approach introducestools to facilitate the development of secure engineering systems, providingdecision-makers with confidence in the trustworthiness and safety of AI-basedmodels across diverse environments characterized by limited training data anddynamic, uncertain conditions. The paper demonstrates the efficacy of thismethodology in real-world safety-critical scenarios, particularly in thecontext of traffic state estimation. Through simulation results, the studyillustrates how the proposed methodology efficiently quantifies physicalinconsistencies exhibited by pre-trained AI models. By utilizing analyticalmodels, the methodology offers a means to gauge the applicability ofpre-trained AI models in new operational environments. This researchcontributes to advancing the understanding and deployment of AI models,offering a robust certification framework that enhances confidence in theirreliability and safety across a spectrum of operational conditions.</description><author>Daryl Mupupuni, Anupama Guntu, Liang Hong, Kamrul Hasan, Leehyun Keel</author><pubDate>Mon, 13 May 2024 17:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07893v1</guid></item><item><title>All Nodes are created Not Equal: Node-Specific Layer Aggregation and Filtration for GNN</title><link>http://arxiv.org/abs/2405.07892v1</link><description>The ever-designed Graph Neural Networks, though opening a promising path forthe modeling of the graph-structure data, unfortunately introduce two dauntingobstacles to their deployment on devices. (I) Most of existing GNNs areshallow, due mostly to the over-smoothing and gradient-vanish problem as theygo deeper as convolutional architectures. (II) The vast majority of GNNs adhereto the homophily assumption, where the central node and its adjacent nodesshare the same label. This assumption often poses challenges for many GNNsworking with heterophilic graphs. Addressing the aforementioned issue hasbecome a looming challenge in enhancing the robustness and scalability of GNNapplications. In this paper, we take a comprehensive and systematic approach toovercoming the two aforementioned challenges for the first time. We propose aNode-Specific Layer Aggregation and Filtration architecture, termed NoSAF, aframework capable of filtering and processing information from each individualnodes. NoSAF introduces the concept of "All Nodes are Created Not Equal" intoevery layer of deep networks, aiming to provide a reliable information filterfor each layer's nodes to sieve out information beneficial for the subsequentlayer. By incorporating a dynamically updated codebank, NoSAF dynamicallyoptimizes the optimal information outputted downwards at each layer. Thiseffectively overcomes heterophilic issues and aids in deepening the network. Tocompensate for the information loss caused by the continuous filtering inNoSAF, we also propose NoSAF-D (Deep), which incorporates a compensationmechanism that replenishes information in every layer of the model, allowingNoSAF to perform meaningful computations even in very deep layers.</description><author>Shilong Wang, Hao Wu, Yifan Duan, Guibin Zhang, Guohao Li, Yuxuan Liang, Shirui Pan, Kun Wang, Yang Wang</author><pubDate>Mon, 13 May 2024 17:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07892v1</guid></item><item><title>Russian-Language Multimodal Dataset for Automatic Summarization of Scientific Papers</title><link>http://arxiv.org/abs/2405.07886v1</link><description>The paper discusses the creation of a multimodal dataset of Russian-languagescientific papers and testing of existing language models for the task ofautomatic text summarization. A feature of the dataset is its multimodal data,which includes texts, tables and figures. The paper presents the results ofexperiments with two language models: Gigachat from SBER and YandexGPT fromYandex. The dataset consists of 420 papers and is publicly available onhttps://github.com/iis-research-team/summarization-dataset.</description><author>Alena Tsanda, Elena Bruches</author><pubDate>Mon, 13 May 2024 17:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07886v1</guid></item><item><title>Soft Merging of Experts with Adaptive Routing</title><link>http://arxiv.org/abs/2306.03745v2</link><description>Sparsely activated neural networks with conditional computation learn toroute their inputs through different "expert" subnetworks, providing a form ofmodularity that densely activated models lack. Despite their possible benefits,models with learned routing often underperform their parameter-matched denselyactivated counterparts as well as models that use non-learned heuristic routingstrategies. In this paper, we hypothesize that these shortcomings stem from thegradient estimation techniques used to train sparsely activated models that usenon-differentiable discrete routing decisions. To address this issue, weintroduce Soft Merging of Experts with Adaptive Routing (SMEAR), which avoidsdiscrete routing by using a single "merged" expert constructed via a weightedaverage of all of the experts' parameters. By routing activations through asingle merged expert, SMEAR does not incur a significant increase incomputational costs and enables standard gradient-based training. Weempirically validate that models using SMEAR outperform models that route basedon metadata or learn sparse routing through gradient estimation. Furthermore,we provide qualitative analysis demonstrating that the experts learned viaSMEAR exhibit a significant amount of specialization. All of the code used inour experiments is publicly available.</description><author>Mohammed Muqeeth, Haokun Liu, Colin Raffel</author><pubDate>Mon, 13 May 2024 17:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03745v2</guid></item><item><title>Lai Loss: A Novel Loss Integrating Regularization</title><link>http://arxiv.org/abs/2405.07884v1</link><description>In the field of machine learning, traditional regularization methodsgenerally tend to directly add regularization terms to the loss function. Thispaper introduces the "Lai loss", a novel loss design that integrates theregularization terms (gradient component) into the traditional loss functionthrough a straightforward geometric ideation. This design innovativelypenalizes the gradient vectors through the loss, effectively controlling themodel's smoothness and offering the dual benefits of reducing overfitting andavoiding underfitting. Subsequently, we proposed a random sampling method thatsuccessfully addresses the challenges associated with its application underlarge sample conditions. We conducted preliminary experiments using publiclyavailable datasets from Kaggle, demonstrating that the design of Lai loss cancontrol the model's smoothness while ensuring maximum accuracy.</description><author>YuFei Lai</author><pubDate>Mon, 13 May 2024 17:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07884v1</guid></item><item><title>OMPGPT: A Generative Pre-trained Transformer Model for OpenMP</title><link>http://arxiv.org/abs/2401.16445v2</link><description>Large language models (LLMs)such as ChatGPT have significantly advanced thefield of Natural Language Processing (NLP). This trend led to the developmentof code-based large language models such as StarCoder, WizardCoder, andCodeLlama, which are trained extensively on vast repositories of code andprogramming languages. While the generic abilities of these code LLMs areuseful for many programmers in tasks like code generation, the area ofhigh-performance computing (HPC) has a narrower set of requirements that make asmaller and more domain-specific model a smarter choice. This paper presentsOMPGPT, a novel domain-specific model meticulously designed to harness theinherent strengths of language models for OpenMP pragma generation.Furthermore, we leverage prompt engineering techniques from the NLP domain tocreate Chain-of-OMP, an innovative strategy designed to enhance OMPGPT'seffectiveness. Our extensive evaluations demonstrate that OMPGPT outperformsexisting large language models specialized in OpenMP tasks and maintains anotably smaller size, aligning it more closely with the typical hardwareconstraints of HPC environments. We consider our contribution as a pivotalbridge, connecting the advantage of language models with the specific demandsof HPC tasks.</description><author>Le Chen, Arijit Bhattacharjee, Nesreen Ahmed, Niranjan Hasabnis, Gal Oren, Vy Vo, Ali Jannesari</author><pubDate>Mon, 13 May 2024 17:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16445v2</guid></item><item><title>Zero-Shot Tokenizer Transfer</title><link>http://arxiv.org/abs/2405.07883v1</link><description>Language models (LMs) are bound to their tokenizer, which maps raw text to asequence of vocabulary items (tokens). This restricts their flexibility: forexample, LMs trained primarily on English may still perform well in othernatural and programming languages, but have vastly decreased efficiency due totheir English-centric tokenizer. To mitigate this, we should be able to swapthe original LM tokenizer with an arbitrary one, on the fly, without degradingperformance. Hence, in this work we define a new problem: Zero-Shot TokenizerTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings forthe tokens in the vocabulary of the new tokenizer. Since prior heuristics forinitializing embeddings often perform at chance level in a ZeTT setting, wepropose a new solution: we train a hypernetwork taking a tokenizer as input andpredicting the corresponding embeddings. We empirically demonstrate that thehypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) anddecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'performance in cross-lingual and coding tasks while markedly reducing thelength of the tokenized sequence. We also find that the remaining gap can bequickly closed by continued training on less than 1B tokens. Finally, we showthat a ZeTT hypernetwork trained for a base (L)LM can also be applied tofine-tuned variants without extra training. Overall, our results makesubstantial strides toward detaching LMs from their tokenizer.</description><author>Benjamin Minixhofer, Edoardo Maria Ponti, Ivan Vulić</author><pubDate>Mon, 13 May 2024 17:17:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07883v1</guid></item><item><title>Prospects for AI-Enhanced ECG as a Unified Screening Tool for Cardiac and Non-Cardiac Conditions -- An Explorative Study in Emergency Care</title><link>http://arxiv.org/abs/2312.11050v2</link><description>Current deep learning algorithms designed for automatic ECG analysis haveexhibited notable accuracy. However, akin to traditional electrocardiography,they tend to be narrowly focused and typically address a singular diagnosticcondition. In this exploratory study, we specifically investigate thecapability of a single model to predict a diverse range of both cardiac andnon-cardiac discharge diagnoses based on a sole ECG collected in the emergencydepartment. We find that 253, 81 cardiac, and 172 non-cardiac, ICD codes can bereliably predicted in the sense of exceeding an AUROC score of 0.8 in astatistically significant manner. This underscores the model's proficiency inhandling a wide array of cardiac and non-cardiac diagnostic scenarios whichdemonstrates potential as a screening tool for diverse medical encounters.</description><author>Nils Strodthoff, Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp</author><pubDate>Mon, 13 May 2024 17:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11050v2</guid></item><item><title>Multi-scale Wasserstein Shortest-path Graph Kernels for Graph Classification</title><link>http://arxiv.org/abs/2206.00979v5</link><description>Graph kernels are conventional methods for computing graph similarities.However, the existing R-convolution graph kernels cannot resolve both of thetwo challenges: 1) Comparing graphs at multiple different scales, and 2)Considering the distributions of substructures when computing the kernelmatrix. These two challenges limit their performances. To mitigate both of thetwo challenges, we propose a novel graph kernel called the Multi-scaleWasserstein Shortest-Path graph kernel (MWSP), at the heart of which is themulti-scale shortest-path node feature map, of which each element denotes thenumber of occurrences of the shortest path around a node. The shortest path isrepresented by the concatenation of all the labels of nodes in it. Since theshortest-path node feature map can only compare graphs at local scales, weincorporate into it the multiple different scales of the graph structure, whichare captured by the truncated BFS trees of different depths rooted at each nodein a graph. We use the Wasserstein distance to compute the similarity betweenthe multi-scale shortest-path node feature maps of two graphs, considering thedistributions of shortest paths. We empirically validate MWSP on variousbenchmark graph datasets and demonstrate that it achieves state-of-the-artperformance on most datasets.</description><author>Wei Ye, Hao Tian, Qijun Chen</author><pubDate>Mon, 13 May 2024 17:14:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.00979v5</guid></item><item><title>Fully Embedded Time-Series Generative Adversarial Networks</title><link>http://arxiv.org/abs/2308.15730v2</link><description>Generative Adversarial Networks (GANs) should produce synthetic data thatfits the underlying distribution of the data being modeled. For real valuedtime-series data, this implies the need to simultaneously capture the staticdistribution of the data, but also the full temporal distribution of the datafor any potential time horizon. This temporal element produces a more complexproblem that can potentially leave current solutions under-constrained,unstable during training, or prone to varying degrees of mode collapse. InFETSGAN, entire sequences are translated directly to the generator's samplingspace using a seq2seq style adversarial auto encoder (AAE), where adversarialtraining is used to match the training distribution in both the feature spaceand the lower dimensional sampling space. This additional constraint provides aloose assurance that the temporal distribution of the synthetic samples willnot collapse. In addition, the First Above Threshold (FAT) operator isintroduced to supplement the reconstruction of encoded sequences, whichimproves training stability and the overall quality of the synthetic data beinggenerated. These novel contributions demonstrate a significant improvement tothe current state of the art for adversarial learners in qualitative measuresof temporal similarity and quantitative predictive ability of data generatedthrough FETSGAN.</description><author>Joe Beck, Subhadeep Chakraborty</author><pubDate>Mon, 13 May 2024 17:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15730v2</guid></item><item><title>On the Relation Between Autoencoders and Non-negative Matrix Factorization, and Their Application for Mutational Signature Extraction</title><link>http://arxiv.org/abs/2405.07879v1</link><description>The aim of this study is to provide a foundation to understand therelationship between non-negative matrix factorization (NMF) and non-negativeautoencoders enabling proper interpretation and understanding ofautoencoder-based alternatives to NMF. Since its introduction, NMF has been apopular tool for extracting interpretable, low-dimensional representations ofhigh-dimensional data. However, recently, several studies have proposed toreplace NMF with autoencoders. This increasing popularity of autoencoderswarrants an investigation on whether this replacement is in general valid andreasonable. Moreover, the exact relationship between non-negative autoencodersand NMF has not been thoroughly explored. Thus, a main aim of this study is toinvestigate in detail the relationship between non-negative autoencoders andNMF. We find that the connection between the two models can be establishedthrough convex NMF, which is a restricted case of NMF. In particular, convexNMF is a special case of an autoencoder. The performance of NMF andautoencoders is compared within the context of extraction of mutationalsignatures from cancer genomics data. We find that the reconstructions based onNMF are more accurate compared to autoencoders, while the signatures extractedusing both methods show comparable consistencies and values when externallyvalidated. These findings suggest that the non-negative autoencodersinvestigated in this article do not provide an improvement of NMF in the fieldof mutational signature extraction.</description><author>Ida Egendal, Rasmus Froberg Brøndum, Marta Pelizzola, Asger Hobolth, Martin Bøgsted</author><pubDate>Mon, 13 May 2024 17:09:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07879v1</guid></item><item><title>Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs</title><link>http://arxiv.org/abs/2310.02619v2</link><description>Generating realistic time series data is important for many engineering andscientific applications. Existing work tackles this problem using generativeadversarial networks (GANs). However, GANs are unstable during training, andthey can suffer from mode collapse. While variational autoencoders (VAEs) areknown to be more robust to the these issues, they are (surprisingly) lessconsidered for time series generation. In this work, we introduce Koopman VAE(KoVAE), a new generative framework that is based on a novel design for themodel prior, and that can be optimized for either regular and irregulartraining data. Inspired by Koopman theory, we represent the latent conditionalprior dynamics using a linear map. Our approach enhances generative modelingwith two desired features: (i) incorporating domain knowledge can be achievedby leveraging spectral tools that prescribe constraints on the eigenvalues ofthe linear map; and (ii) studying the qualitative behavior and stability of thesystem can be performed using tools from dynamical systems theory. Our resultsshow that KoVAE outperforms state-of-the-art GAN and VAE methods across severalchallenging synthetic and real-world time series generation benchmarks. Whethertrained on regular or irregular data, KoVAE generates time series that improveboth discriminative and predictive metrics. We also present visual evidencesuggesting that KoVAE learns probability density functions that betterapproximate the empirical ground truth distribution.</description><author>Ilan Naiman, N. Benjamin Erichson, Pu Ren, Michael W. Mahoney, Omri Azencot</author><pubDate>Mon, 13 May 2024 17:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02619v2</guid></item><item><title>Reproducing the Metric-Based Evaluation of a Set of Controllable Text Generation Techniques</title><link>http://arxiv.org/abs/2405.07875v1</link><description>Rerunning a metric-based evaluation should be more straightforward, andresults should be closer, than in a human-based evaluation, especially wherecode and model checkpoints are made available by the original authors. As thisreport of our efforts to rerun a metric-based evaluation of a set ofsingle-attribute and multiple-attribute controllable text generation (CTG)techniques shows however, such reruns of evaluations do not always produceresults that are the same as the original results, and can reveal errors in thereporting of the original work.</description><author>Michela Lorandi, Anya Belz</author><pubDate>Mon, 13 May 2024 17:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07875v1</guid></item><item><title>Enhancing Clinically Significant Prostate Cancer Prediction in T2-weighted Images through Transfer Learning from Breast Cancer</title><link>http://arxiv.org/abs/2405.07869v1</link><description>In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting inover 375,000 deaths. The accurate identification of clinically significantprostate cancer is crucial for delivering effective treatment to patients.Consequently, there has been a surge in research exploring the application ofdeep neural networks to predict clinical significance based on magneticresonance images. However, these networks demand extensive datasets to attainoptimal performance. Recently, transfer learning emerged as a technique thatleverages acquired features from a domain with richer data to enhance theperformance of a domain with limited data. In this paper, we investigate theimprovement of clinically significant prostate cancer prediction in T2-weightedimages through transfer learning from breast cancer. The results demonstrate aremarkable improvement of over 30% in leave-one-out cross-validation accuracy.</description><author>Chi-en Amy Tai, Alexander Wong</author><pubDate>Mon, 13 May 2024 16:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07869v1</guid></item><item><title>Boostlet.js: Image processing plugins for the web via JavaScript injection</title><link>http://arxiv.org/abs/2405.07868v1</link><description>Can web-based image processing and visualization tools easily integrate intoexisting websites without significant time and effort? Our Boostlet.js libraryaddresses this challenge by providing an open-source, JavaScript-based webframework to enable additional image processing functionalities. Boostletexamples include kernel filtering, image captioning, data visualization,segmentation, and web-optimized machine-learning models. To achieve this,Boostlet.js uses a browser bookmark to inject a user-friendly plugin selectiontool called PowerBoost into any host website. Boostlet also provides on-siteaccess to a standard API independent of any visualization framework for pixeldata and scene manipulation. Web-based Boostlets provide a modular architectureand client-side processing capabilities to apply advanced image-processingtechniques using consumer-level hardware. The code is open-source andavailable.</description><author>Edward Gaibor, Shruti Varade, Rohini Deshmukh, Tim Meyer, Mahsa Geshvadi, SangHyuk Kim, Vidhya Sree Narayanappa, Daniel Haehn</author><pubDate>Mon, 13 May 2024 16:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07868v1</guid></item><item><title>Approximating Numerical Fluxes Using Fourier Neural Operators for Hyperbolic Conservation Laws</title><link>http://arxiv.org/abs/2401.01783v4</link><description>Traditionally, classical numerical schemes have been employed to solvepartial differential equations (PDEs) using computational methods. Recently,neural network-based methods have emerged. Despite these advancements, neuralnetwork-based methods, such as physics-informed neural networks (PINNs) andneural operators, exhibit deficiencies in robustness and generalization. Toaddress these issues, numerous studies have integrated classical numericalframeworks with machine learning techniques, incorporating neural networks intoparts of traditional numerical methods. In this study, we focus on hyperbolicconservation laws by replacing traditional numerical fluxes with neuraloperators. To this end, we developed loss functions inspired by establishednumerical schemes related to conservation laws and approximated numericalfluxes using Fourier neural operators (FNOs). Our experiments demonstrated thatour approach combines the strengths of both traditional numerical schemes andFNOs, outperforming standard FNO methods in several respects. For instance, wedemonstrate that our method is robust, has resolution invariance, and isfeasible as a data-driven method. In particular, our method can make continuouspredictions over time and exhibits superior generalization capabilities without-of-distribution (OOD) samples, which are challenges that existing neuraloperator methods encounter.</description><author>Taeyoung Kim, Myungjoo Kang</author><pubDate>Mon, 13 May 2024 16:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01783v4</guid></item><item><title>AnoVox: A Benchmark for Multimodal Anomaly Detection in Autonomous Driving</title><link>http://arxiv.org/abs/2405.07865v1</link><description>The scale-up of autonomous vehicles depends heavily on their ability to dealwith anomalies, such as rare objects on the road. In order to handle suchsituations, it is necessary to detect anomalies in the first place. Anomalydetection for autonomous driving has made great progress in the past years butsuffers from poorly designed benchmarks with a strong focus on camera data. Inthis work, we propose AnoVox, the largest benchmark for ANOmaly detection inautonomous driving to date. AnoVox incorporates large-scale multimodal sensordata and spatial VOXel ground truth, allowing for the comparison of methodsindependent of their used sensor. We propose a formal definition of normalityand provide a compliant training dataset. AnoVox is the first benchmark tocontain both content and temporal anomalies.</description><author>Daniel Bogdoll, Iramm Hamdard, Lukas Namgyu Rößler, Felix Geisler, Muhammed Bayram, Felix Wang, Jan Imhof, Miguel de Campos, Anushervon Tabarov, Yitian Yang, Hanno Gottschalk, J. Marius Zöllner</author><pubDate>Mon, 13 May 2024 16:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07865v1</guid></item><item><title>RLHF Workflow: From Reward Modeling to Online RLHF</title><link>http://arxiv.org/abs/2405.07863v1</link><description>We present the workflow of Online Iterative Reinforcement Learning from HumanFeedback (RLHF) in this technical report, which is widely reported tooutperform its offline counterpart by a large margin in the recent largelanguage model (LLM) literature. However, existing open-source RLHF projectsare still largely confined to the offline learning setting. In this technicalreport, we aim to fill in this gap and provide a detailed recipe that is easyto reproduce for online iterative RLHF. In particular, since online humanfeedback is usually infeasible for open-source communities with limitedresources, we start by constructing preference models using a diverse set ofopen-source datasets and use the constructed proxy preference model toapproximate human feedback. Then, we discuss the theoretical insights andalgorithmic principles behind online iterative RLHF, followed by a detailedpractical implementation. Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R,achieves impressive performance on LLM chatbot benchmarks, includingAlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarkssuch as HumanEval and TruthfulQA. We have shown that supervised fine-tuning(SFT) and iterative RLHF can obtain state-of-the-art performance with fullyopen-source datasets. Further, we have made our models, curated datasets, andcomprehensive step-by-step code guidebooks publicly available. Please refer tohttps://github.com/RLHFlow/RLHF-Reward-Modeling andhttps://github.com/RLHFlow/Online-RLHF for more detailed information.</description><author>Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang</author><pubDate>Mon, 13 May 2024 16:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07863v1</guid></item><item><title>Improving Breast Cancer Grade Prediction with Multiparametric MRI Created Using Optimized Synthetic Correlated Diffusion Imaging</title><link>http://arxiv.org/abs/2405.07861v1</link><description>Breast cancer was diagnosed for over 7.8 million women between 2015 to 2020.Grading plays a vital role in breast cancer treatment planning. However, thecurrent tumor grading method involves extracting tissue from patients, leadingto stress, discomfort, and high medical costs. A recent paper leveragingvolumetric deep radiomic features from synthetic correlated diffusion imaging(CDI$^s$) for breast cancer grade prediction showed immense promise fornoninvasive methods for grading. Motivated by the impact of CDI$^s$optimization for prostate cancer delineation, this paper examines usingoptimized CDI$^s$ to improve breast cancer grade prediction. We fuse theoptimized CDI$^s$ signal with diffusion-weighted imaging (DWI) to create amultiparametric MRI for each patient. Using a larger patient cohort andtraining across all the layers of a pretrained MONAI model, we achieve aleave-one-out cross-validation accuracy of 95.79%, over 8% higher compared tothat previously reported.</description><author>Chi-en Amy Tai, Alexander Wong</author><pubDate>Mon, 13 May 2024 16:48:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07861v1</guid></item><item><title>GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</title><link>http://arxiv.org/abs/2310.08529v3</link><description>In recent times, the generation of 3D assets from text prompts has shownimpressive results. Both 2D and 3D diffusion models can help generate decent 3Dobjects based on prompts. 3D diffusion models have good 3D consistency, buttheir quality and generalization are limited as trainable 3D data is expensiveand hard to obtain. 2D diffusion models enjoy strong abilities ofgeneralization and fine generation, but 3D consistency is hard to guarantee.This paper attempts to bridge the power from the two types of diffusion modelsvia the recent explicit and efficient 3D Gaussian splatting representation. Afast 3D object generation framework, named as GaussianDreamer, is proposed,where the 3D diffusion model provides priors for initialization and the 2Ddiffusion model enriches the geometry and appearance. Operations of noisy pointgrowing and color perturbation are introduced to enhance the initializedGaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3Davatar within 15 minutes on one GPU, much faster than previous methods, whilethe generated instances can be directly rendered in real time. Demos and codeare available at https://taoranyi.com/gaussiandreamer/.</description><author>Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang</author><pubDate>Mon, 13 May 2024 15:12:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08529v3</guid></item><item><title>Synthetic Test Collections for Retrieval Evaluation</title><link>http://arxiv.org/abs/2405.07767v1</link><description>Test collections play a vital role in evaluation of information retrieval(IR) systems. Obtaining a diverse set of user queries for test collectionconstruction can be challenging, and acquiring relevance judgments, whichindicate the appropriateness of retrieved documents to a query, is often costlyand resource-intensive. Generating synthetic datasets using Large LanguageModels (LLMs) has recently gained significant attention in variousapplications. In IR, while previous work exploited the capabilities of LLMs togenerate synthetic queries or documents to augment training data and improvethe performance of ranking models, using LLMs for constructing synthetic testcollections is relatively unexplored. Previous studies demonstrate that LLMshave the potential to generate synthetic relevance judgments for use in theevaluation of IR systems. In this paper, we comprehensively investigate whetherit is possible to use LLMs to construct fully synthetic test collections bygenerating not only synthetic judgments but also synthetic queries. Inparticular, we analyse whether it is possible to construct reliable synthetictest collections and the potential risks of bias such test collections mayexhibit towards LLM-based models. Our experiments indicate that using LLMs itis possible to construct synthetic test collections that can reliably be usedfor retrieval evaluation.</description><author>Hossein A. Rahmani, Nick Craswell, Emine Yilmaz, Bhaskar Mitra, Daniel Campos</author><pubDate>Mon, 13 May 2024 15:11:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07767v1</guid></item><item><title>Challenges and Opportunities of NLP for HR Applications: A Discussion Paper</title><link>http://arxiv.org/abs/2405.07766v1</link><description>Over the course of the recent decade, tremendous progress has been made inthe areas of machine learning and natural language processing, which opened upvast areas of potential application use cases, including hiring and humanresource management. We review the use cases for text analytics in the realm ofhuman resources/personnel management, including actually realized as well aspotential but not yet implemented ones, and we analyze the opportunities andrisks of these.</description><author>Jochen L. Leidner, Mark Stevenson</author><pubDate>Mon, 13 May 2024 15:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07766v1</guid></item><item><title>TANQ: An open domain dataset of table answered questions</title><link>http://arxiv.org/abs/2405.07765v1</link><description>Language models, potentially augmented with tool usage such as retrieval arebecoming the go-to means of answering questions. Understanding and answeringquestions in real-world settings often requires retrieving information fromdifferent sources, processing and aggregating data to extract insights, andpresenting complex findings in form of structured artifacts such as noveltables, charts, or infographics. In this paper, we introduce TANQ, the firstopen domain question answering dataset where the answers require buildingtables from information across multiple sources. We release the full sourceattribution for every cell in the resulting table and benchmarkstate-of-the-art language models in open, oracle, and closed book setups. Ourbest-performing baseline, GPT4 reaches an overall F1 score of 29.1, laggingbehind human performance by 19.7 points. We analyse baselines' performanceacross different dataset attributes such as different skills required for thistask, including multi-hop reasoning, math operations, and unit conversions. Wefurther discuss common failures in model-generated answers, suggesting thatTANQ is a complex task with many challenges ahead.</description><author>Mubashara Akhtar, Chenxi Pang, Andreea Marzoca, Yasemin Altun, Julian Martin Eisenschlos</author><pubDate>Mon, 13 May 2024 15:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07765v1</guid></item><item><title>LGDE: Local Graph-based Dictionary Expansion</title><link>http://arxiv.org/abs/2405.07764v1</link><description>Expanding a dictionary of pre-selected keywords is crucial for tasks ininformation retrieval, such as database query and online data collection. Herewe propose Local Graph-based Dictionary Expansion (LGDE), a method that usestools from manifold learning and network science for the data-driven discoveryof keywords starting from a seed dictionary. At the heart of LGDE lies thecreation of a word similarity graph derived from word embeddings and theapplication of local community detection based on graph diffusion to discoversemantic neighbourhoods of pre-defined seed keywords. The diffusion in thelocal graph manifold allows the exploration of the complex nonlinear geometryof word embeddings and can capture word similarities based on paths of semanticassociation. We validate our method on a corpus of hate speech-related postsfrom Reddit and Gab and show that LGDE enriches the list of keywords andachieves significantly better performance than threshold methods based ondirect word similarities. We further demonstrate the potential of our methodthrough a real-world use case from communication science, where LGDE isevaluated quantitatively on data collected and analysed by domain experts byexpanding a conspiracy-related dictionary.</description><author>Dominik J. Schindler, Sneha Jha, Xixuan Zhang, Kilian Buehling, Annett Heft, Mauricio Barahona</author><pubDate>Mon, 13 May 2024 15:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07764v1</guid></item><item><title>Archetypal Analysis++: Rethinking the Initialization Strategy</title><link>http://arxiv.org/abs/2301.13748v4</link><description>Archetypal analysis is a matrix factorization method with convexityconstraints. Due to local minima, a good initialization is essential, butfrequently used initialization methods yield either sub-optimal starting pointsor are prone to get stuck in poor local minima. In this paper, we proposearchetypal analysis++ (AA++), a probabilistic initialization strategy forarchetypal analysis that sequentially samples points based on their influenceon the objective function, similar to $k$-means++. In fact, we argue that$k$-means++ already approximates the proposed initialization method.Furthermore, we suggest to adapt an efficient Monte Carlo approximation of$k$-means++ to AA++. In an extensive empirical evaluation of 15 real-world datasets of varying sizes and dimensionalities and considering two pre-processingstrategies, we show that AA++ almost always outperforms all baselines,including the most frequently used ones.</description><author>Sebastian Mair, Jens Sjölund</author><pubDate>Mon, 13 May 2024 15:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13748v4</guid></item><item><title>Fast Timing-Conditioned Latent Audio Diffusion</title><link>http://arxiv.org/abs/2402.04825v3</link><description>Generating long-form 44.1kHz stereo audio from text prompts can becomputationally demanding. Further, most previous works do not tackle thatmusic and sound effects naturally vary in their duration. Our research focuseson the efficient generation of long-form, variable-length stereo music andsounds at 44.1kHz using text prompts with a generative model. Stable Audio isbased on latent diffusion, with its latent defined by a fully-convolutionalvariational autoencoder. It is conditioned on text prompts as well as timingembeddings, allowing for fine control over both the content and length of thegenerated music and sounds. Stable Audio is capable of rendering stereo signalsof up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its computeefficiency and fast inference, it is one of the best in two publictext-to-music and -audio benchmarks and, differently from state-of-the-artmodels, can generate music with structure and stereo sounds.</description><author>Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, Jordi Pons</author><pubDate>Mon, 13 May 2024 15:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04825v3</guid></item><item><title>A method for supervoxel-wise association studies of age and other non-imaging variables from coronary computed tomography angiograms</title><link>http://arxiv.org/abs/2405.07762v1</link><description>The study of associations between an individual's age and imaging andnon-imaging data is an active research area that attempts to aid understandingof the effects and patterns of aging. In this work we have conducted asupervoxel-wise association study between both volumetric and tissue densityfeatures in coronary computed tomography angiograms and the chronological ageof a subject, to understand the localized changes in morphology and tissuedensity with age. To enable a supervoxel-wise study of volume and tissuedensity, we developed a novel method based on image segmentation, inter-subjectimage registration, and robust supervoxel-based correlation analysis, toachieve a statistical association study between the images and age. We evaluatethe registration methodology in terms of the Dice coefficient for the heartchambers and myocardium, and the inverse consistency of the transformations,showing that the method works well in most cases with high overlap and inverseconsistency. In a sex-stratified study conducted on a subset of $n=1388$ imagesfrom the SCAPIS study, the supervoxel-wise analysis was able to find localizedassociations with age outside of the commonly segmented and analyzedsub-regions, and several substantial differences between the sexes inassociation of age and volume.</description><author>Johan Öfverstedt, Elin Lundström, Göran Bergström, Joel Kullberg, Håkan Ahlström</author><pubDate>Mon, 13 May 2024 15:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07762v1</guid></item><item><title>LLM4ED: Large Language Models for Automatic Equation Discovery</title><link>http://arxiv.org/abs/2405.07761v1</link><description>Equation discovery is aimed at directly extracting physical laws from dataand has emerged as a pivotal research domain. Previous methods based onsymbolic mathematics have achieved substantial advancements, but often requirethe design of implementation of complex algorithms. In this paper, we introducea new framework that utilizes natural language-based prompts to guide largelanguage models (LLMs) in automatically mining governing equations from data.Specifically, we first utilize the generation capability of LLMs to generatediverse equations in string form, and then evaluate the generated equationsbased on observations. In the optimization phase, we propose two alternatelyiterated strategies to optimize generated equations collaboratively. The firststrategy is to take LLMs as a black-box optimizer and achieve equationself-improvement based on historical samples and their performance. The secondstrategy is to instruct LLMs to perform evolutionary operators for globalsearch. Experiments are extensively conducted on both partial differentialequations and ordinary differential equations. Results demonstrate that ourframework can discover effective equations to reveal the underlying physicallaws under various nonlinear dynamic systems. Further comparisons are made withstate-of-the-art models, demonstrating good stability and usability. Ourframework substantially lowers the barriers to learning and applying equationdiscovery techniques, demonstrating the application potential of LLMs in thefield of knowledge discovery.</description><author>Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, Dongxiao Zhang</author><pubDate>Mon, 13 May 2024 15:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07761v1</guid></item><item><title>CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling</title><link>http://arxiv.org/abs/2310.17347v4</link><description>While conditional diffusion models are known to have good coverage of thedata distribution, they still face limitations in output diversity,particularly when sampled with a high classifier-free guidance scale foroptimal image quality or when trained on small datasets. We attribute thisproblem to the role of the conditioning signal in inference and offer animproved sampling strategy for diffusion models that can increase generationdiversity, especially at high guidance scales, with minimal loss of samplequality. Our sampling strategy anneals the conditioning signal by addingscheduled, monotonically decreasing Gaussian noise to the conditioning vectorduring inference to balance diversity and condition alignment. OurCondition-Annealed Diffusion Sampler (CADS) can be used with any pretrainedmodel and sampling algorithm, and we show that it boosts the diversity ofdiffusion models in various conditional generation tasks. Further, using anexisting pretrained diffusion model, CADS achieves a new state-of-the-art FIDof 1.70 and 2.31 for class-conditional ImageNet generation at 256$\times$256and 512$\times$512 respectively.</description><author>Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber</author><pubDate>Mon, 13 May 2024 15:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17347v4</guid></item><item><title>CAGES: Cost-Aware Gradient Entropy Search for Efficient Local Multi-Fidelity Bayesian Optimization</title><link>http://arxiv.org/abs/2405.07760v1</link><description>Bayesian optimization (BO) is a popular approach for optimizingexpensive-to-evaluate black-box objective functions. An important challenge inBO is its application to high-dimensional search spaces due in large part tothe curse of dimensionality. One way to overcome this challenge is to focus onlocal BO methods that aim to efficiently learn gradients, which have shownstrong empirical performance on a variety of high-dimensional problemsincluding policy search in reinforcement learning (RL). However, current localBO methods assume access to only a single high-fidelity information sourcewhereas, in many engineering and control problems, one has access to multiplecheaper approximations of the objective. We propose a novel algorithm,Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelityblack-box functions. CAGES makes no assumption about the relationship betweendifferent information sources, making it more flexible than othermulti-fidelity methods. It also employs a new type of information-theoreticacquisition function, which enables systematic identification of samples thatmaximize the information gain about the unknown gradient per cost of theevaluation. We demonstrate CAGES can achieve significant performanceimprovements compared to other state-of-the-art methods on a variety ofsynthetic and benchmark RL problems.</description><author>Wei-Ting Tang, Joel A. Paulson</author><pubDate>Mon, 13 May 2024 15:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07760v1</guid></item><item><title>MADRL-Based Rate Adaptation for 360$\degree$ Video Streaming with Multi-Viewpoint Prediction</title><link>http://arxiv.org/abs/2405.07759v1</link><description>Over the last few years, 360$\degree$ video traffic on the network has grownsignificantly. A key challenge of 360$\degree$ video playback is ensuring ahigh quality of experience (QoE) with limited network bandwidth. Currently,most studies focus on tile-based adaptive bitrate (ABR) streaming based onsingle viewport prediction to reduce bandwidth consumption. However, theperformance of models for single-viewpoint prediction is severely limited bythe inherent uncertainty in head movement, which can not cope with the suddenmovement of users very well. This paper first presents a multimodalspatial-temporal attention transformer to generate multiple viewpointtrajectories with their probabilities given a historical trajectory. Theproposed method models viewpoint prediction as a classification problem anduses attention mechanisms to capture the spatial and temporal characteristicsof input video frames and viewpoint trajectories for multi-viewpointprediction. After that, a multi-agent deep reinforcement learning (MADRL)-basedABR algorithm utilizing multi-viewpoint prediction for 360$\degree$ videostreaming is proposed for maximizing different QoE objectives under variousnetwork conditions. We formulate the ABR problem as a decentralized partiallyobservable Markov decision process (Dec-POMDP) problem and present a MAPPOalgorithm based on centralized training and decentralized execution (CTDE)framework to solve the problem. The experimental results show that our proposedmethod improves the defined QoE metric by up to 85.5\% compared to existing ABRmethods.</description><author>Haopeng Wang, Zijian Long, Haiwei Dong, Abdulmotaleb El Saddik</author><pubDate>Mon, 13 May 2024 14:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07759v1</guid></item><item><title>NeuroNet: A Novel Hybrid Self-Supervised Learning Framework for Sleep Stage Classification Using Single-Channel EEG</title><link>http://arxiv.org/abs/2404.17585v2</link><description>The classification of sleep stages is a pivotal aspect of diagnosing sleepdisorders and evaluating sleep quality. However, the conventional manualscoring process, conducted by clinicians, is time-consuming and prone to humanbias. Recent advancements in deep learning have substantially propelled theautomation of sleep stage classification. Nevertheless, challenges persist,including the need for large datasets with labels and the inherent biases inhuman-generated annotations. This paper introduces NeuroNet, a self-supervisedlearning (SSL) framework designed to effectively harness unlabeledsingle-channel sleep electroencephalogram (EEG) signals by integratingcontrastive learning tasks and masked prediction tasks. NeuroNet demonstratessuperior performance over existing SSL methodologies through extensiveexperimentation conducted across three polysomnography (PSG) datasets.Additionally, this study proposes a Mamba-based temporal context module tocapture the relationships among diverse EEG epochs. Combining NeuroNet with theMamba-based temporal context module has demonstrated the capability to achieve,or even surpass, the performance of the latest supervised learningmethodologies, even with a limited amount of labeled data. This study isexpected to establish a new benchmark in sleep stage classification, promisingto guide future research and applications in the field of sleep analysis.</description><author>Cheol-Hui Lee, Hakseung Kim, Hyun-jee Han, Min-Kyung Jung, Byung C. Yoon, Dong-Joo Kim</author><pubDate>Mon, 13 May 2024 14:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17585v2</guid></item><item><title>Integrating supervised and unsupervised learning approaches to unveil critical process inputs</title><link>http://arxiv.org/abs/2405.07751v1</link><description>This study introduces a machine learning framework tailored to large-scaleindustrial processes characterized by a plethora of numerical and categoricalinputs. The framework aims to (i) discern critical parameters influencing theoutput and (ii) generate accurate out-of-sample qualitative and quantitativepredictions of production outcomes. Specifically, we address the pivotalquestion of the significance of each input in shaping the process outcome,using an industrial Chemical Vapor Deposition (CVD) process as an example. Theinitial objective involves merging subject matter expertise and clusteringtechniques exclusively on the process output, here, coating thicknessmeasurements at various positions in the reactor. This approach identifiesgroups of production runs that share similar qualitative characteristics, suchas film mean thickness and standard deviation. In particular, the differencesof the outcomes represented by the different clusters can be attributed todifferences in specific inputs, indicating that these inputs are critical forthe production outcome. Leveraging this insight, we subsequently implementsupervised classification and regression methods using the identified criticalprocess inputs. The proposed methodology proves to be valuable in scenarioswith a multitude of inputs and insufficient data for the direct application ofdeep learning techniques, providing meaningful insights into the underlyingprocesses.</description><author>Paris Papavasileiou, Dimitrios G. Giovanis, Gabriele Pozzetti, Martin Kathrein, Christoph Czettl, Ioannis G. Kevrekidis, Andreas G. Boudouvis, Stéphane P. A. Bordas, Eleni D. Koronaki</author><pubDate>Mon, 13 May 2024 14:50:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07751v1</guid></item><item><title>DeepHYDRA: Resource-Efficient Time-Series Anomaly Detection in Dynamically-Configured Systems</title><link>http://arxiv.org/abs/2405.07749v1</link><description>Anomaly detection in distributed systems such as High-Performance Computing(HPC) clusters is vital for early fault detection, performance optimisation,security monitoring, reliability in general but also operational insights. DeepNeural Networks have seen successful use in detecting long-term anomalies inmultidimensional data, originating for instance from industrial or medicalsystems, or weather prediction. A downside of such methods is that they requirea static input size, or lose data through cropping, sampling, or otherdimensionality reduction methods, making deployment on systems with variabilityon monitored data channels, such as computing clusters difficult. To addressthese problems, we present DeepHYDRA (Deep Hybrid DBSCAN/Reduction-BasedAnomaly Detection) which combines DBSCAN and learning-based anomaly detection.DBSCAN clustering is used to find point anomalies in time-series data,mitigating the risk of missing outliers through loss of information whenreducing input data to a fixed number of channels. A deep learning-basedtime-series anomaly detection method is then applied to the reduced data inorder to identify long-term outliers. This hybrid approach reduces the chancesof missing anomalies that might be made indistinguishable from normal data bythe reduction process, and likewise enables the algorithm to be scalable andtolerate partial system failures while retaining its detection capabilities.Using a subset of the well-known SMD dataset family, a modified variant of theEclipse dataset, as well as an in-house dataset with a large variability inactive data channels, made publicly available with this work, we furthermoreanalyse computational intensity, memory footprint, and activation counts.DeepHYDRA is shown to reliably detect different types of anomalies in bothlarge and complex datasets.</description><author>Franz Kevin Stehle, Wainer Vandelli, Giuseppe Avolio, Felix Zahn, Holger Fröning</author><pubDate>Mon, 13 May 2024 14:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07749v1</guid></item><item><title>Neural Network Compression for Reinforcement Learning Tasks</title><link>http://arxiv.org/abs/2405.07748v1</link><description>In real applications of Reinforcement Learning (RL), such as robotics, lowlatency and energy efficient inference is very desired. The use of sparsity andpruning for optimizing Neural Network inference, and particularly to improveenergy and latency efficiency, is a standard technique. In this work, weperform a systematic investigation of applying these optimization techniquesfor different RL algorithms in different RL environments, yielding up to a400-fold reduction in the size of neural networks.</description><author>Dmitry A. Ivanov, Denis A. Larionov, Oleg V. Maslennikov, Vladimir V. Voevodin</author><pubDate>Mon, 13 May 2024 14:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07748v1</guid></item><item><title>LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language</title><link>http://arxiv.org/abs/2405.07745v1</link><description>Despite advancements in English-dominant generative large language models,further development is needed for low-resource languages to enhance globalaccessibility. The primary methods for representing these languages aremonolingual and multilingual pretraining. Monolingual pretraining is expensivedue to hardware requirements, and multilingual models often have unevenperformance across languages. This study explores an alternative solution byadapting large language models, primarily trained on English, to low-resourcelanguages. We assess various strategies, including continual training,instruction fine-tuning, task-specific fine-tuning, and vocabulary extension.The results show that continual training improves language comprehension, asreflected in perplexity scores, and task-specific tuning generally enhancesperformance of downstream tasks. However, extending the vocabulary shows nosubstantial benefits. Additionally, while larger models improve taskperformance with few-shot tuning, multilingual models perform worse than theirmonolingual counterparts when adapted.</description><author>Cagri Toraman</author><pubDate>Mon, 13 May 2024 14:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07745v1</guid></item><item><title>Federated Hierarchical Tensor Networks: a Collaborative Learning Quantum AI-Driven Framework for Healthcare</title><link>http://arxiv.org/abs/2405.07735v1</link><description>Healthcare industries frequently handle sensitive and proprietary data, anddue to strict privacy regulations, they are often reluctant to share datadirectly. In today's context, Federated Learning (FL) stands out as a crucialremedy, facilitating the rapid advancement of distributed machine learningwhile effectively managing critical concerns regarding data privacy andgovernance. The fusion of federated learning and quantum computing represents agroundbreaking interdisciplinary approach with immense potential torevolutionize various industries, from healthcare to finance. In this work, weproposed a federated learning framework based on quantum tensor networks, whichleverages the principles of many-body quantum physics. Currently, there are noknown classical tensor networks implemented in federated settings. Furthermore,we investigated the effectiveness and feasibility of the proposed framework byconducting a differential privacy analysis to ensure the security of sensitivedata across healthcare institutions. Experiments on popular medical imagedatasets show that the federated quantum tensor network model achieved a meanreceiver-operator characteristic area under the curve (ROC-AUC) between0.91-0.98. Experimental results demonstrate that the quantum federated globalmodel, consisting of highly entangled tensor network structures, showed bettergeneralization and robustness and achieved higher testing accuracy, surpassingthe performance of locally trained clients under unbalanced data distributionsamong healthcare institutions.</description><author>Amandeep Singh Bhatia, David E. Bernal Neira</author><pubDate>Mon, 13 May 2024 14:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07735v1</guid></item><item><title>Revisiting the Power of Prompt for Visual Tuning</title><link>http://arxiv.org/abs/2402.02382v2</link><description>Visual prompt tuning (VPT) is a promising solution incorporating learnableprompt tokens to customize pre-trained models for downstream tasks. However,VPT and its variants often encounter challenges like prompt initialization,prompt length, and subpar performance in self-supervised pretraining, hinderingsuccessful contextual adaptation. This study commences by exploring thecorrelation evolvement between prompts and patch tokens during proficienttraining. Inspired by the observation that the prompt tokens tend to share highmutual information with patch tokens, we propose initializing prompts withdownstream token prototypes. The strategic initialization, a stand-in for theprevious initialization, substantially improves performance in fine-tuning. Torefine further, we optimize token construction with a streamlined pipeline thatmaintains excellent performance with almost no increase in computationalexpenses compared to VPT. Exhaustive experiments show our proposed approachoutperforms existing methods by a remarkable margin. For instance, it surpassesfull fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnableparameters on the FGVC and VTAB-1K benchmarks. Notably, our methodsignificantly advances the adaptation for self-supervised pretraining,achieving impressive task performance gains of at least 10% to 30%. Besides,the experimental results demonstrate the proposed SPT is robust to promptlengths and scales well with model capacity and training data size. We finallyprovide an insightful exploration into the amount of target data facilitatingthe adaptation of pre-trained models to downstream tasks. The code is availableat https://github.com/WangYZ1608/Self-Prompt-Tuning.</description><author>Yuzhu Wang, Lechao Cheng, Chaowei Fang, Dingwen Zhang, Manni Duan, Meng Wang</author><pubDate>Mon, 13 May 2024 14:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02382v2</guid></item><item><title>Language Imbalance Can Boost Cross-lingual Generalisation</title><link>http://arxiv.org/abs/2404.07982v3</link><description>Multilinguality is crucial for extending recent advancements in languagemodelling to diverse linguistic communities. To maintain high performance whilerepresenting multiple languages, multilingual models ideally alignrepresentations, allowing what is learned in one language to generalise toothers. Prior research has emphasised the importance of parallel data andshared vocabulary elements as key factors for such alignment. In this study, weinvestigate an unintuitive novel driver of cross-lingual generalisation:language imbalance. In controlled experiments on perfectly equivalent clonedlanguages, we observe that the existence of a predominant language duringtraining boosts the performance of less frequent languages and leads tostronger alignment of model representations across languages. Furthermore, wefind that this trend is amplified with scale: with large enough models or longenough training, we observe that bilingual training data with a 90/10 languagesplit yields better performance on both languages than a balanced 50/50 split.Building on these insights, we design training schemes that can improveperformance in all cloned languages, even without altering the training data.As we extend our analysis to real languages, we find that infrequent languagesstill benefit from frequent ones, yet whether language imbalance causescross-lingual generalisation there is not conclusive.</description><author>Anton Schäfer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, Imanol Schlag</author><pubDate>Mon, 13 May 2024 14:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07982v3</guid></item><item><title>Does Dependency Locality Predict Non-canonical Word Order in Hindi?</title><link>http://arxiv.org/abs/2405.07730v1</link><description>Previous work has shown that isolated non-canonical sentences withObject-before-Subject (OSV) order are initially harder to process than theircanonical counterparts with Subject-before-Object (SOV) order. Although thisdifficulty diminishes with appropriate discourse context, the underlyingcognitive factors responsible for alleviating processing challenges in OSVsentences remain a question. In this work, we test the hypothesis thatdependency length minimization is a significant predictor of non-canonical(OSV) syntactic choices, especially when controlling for information statussuch as givenness and surprisal measures. We extract sentences from theHindi-Urdu Treebank corpus (HUTB) that contain clearly-defined subjects andobjects, systematically permute the preverbal constituents of those sentences,and deploy a classifier to distinguish between original corpus sentences andartificially generated alternatives. The classifier leverages variousdiscourse-based and cognitive features, including dependency length, surprisal,and information status, to inform its predictions. Our results suggest that,although there exists a preference for minimizing dependency length innon-canonical corpus sentences amidst the generated variants, this factor doesnot significantly contribute in identifying corpus sentences above and beyondsurprisal and givenness measures. Notably, discourse predictability emerges asthe primary determinant of constituent-order preferences. These findings arefurther supported by human evaluations involving 44 native Hindi speakers.Overall, this work sheds light on the role of expectation adaptation inword-ordering decisions. We conclude by situating our results within thetheories of discourse production and information locality.</description><author>Sidharth Ranjan, Marten van Schijndel</author><pubDate>Mon, 13 May 2024 14:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07730v1</guid></item><item><title>Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing</title><link>http://arxiv.org/abs/2405.07726v1</link><description>Persona-driven role-playing (PRP) aims to build AI characters that canrespond to user queries by faithfully sticking with all persona statements.Unfortunately, existing faithfulness criteria for PRP are limited tocoarse-grained LLM-based scoring without a clear definition or formulation.This paper presents a pioneering exploration to quantify PRP faithfulness as afine-grained and explainable criterion, which also serves as a reliablereference for optimization. Our criterion first discriminates personastatements into active and passive constraints by identifying thequery-statement relevance. Then, we incorporate all constraints following theprinciple that the AI character's response should be (a) entailed by active(relevant) constraints and (b) not contradicted by passive (irrelevant)constraints. We translate this principle mathematically into a novelActive-Passive-Constraint (APC) score, a constraint-wise sum of naturallanguage inference (NLI) scores weighted by relevance scores. In practice, webuild the APC scoring system by symbolically distilling small discriminatorsfrom GPT-4 for efficiency. We validate the quality of the APC score againsthuman evaluation based on example personas with tens of statements, and theresults show a high correlation. We further leverage it as a reward system indirect preference optimization (DPO) for better AI characters. Our experimentsoffer a fine-grained and explainable comparison between existing PRPtechniques, revealing their advantages and limitations. We further findAPC-based DPO to be one of the most competitive techniques for sticking withall constraints and can be well incorporated with other techniques. We thenextend the scale of the experiments to real persons with hundreds of statementsand reach a consistent conclusion.</description><author>Letian Peng, Jingbo Shang</author><pubDate>Mon, 13 May 2024 14:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07726v1</guid></item><item><title>Coarse or Fine? Recognising Action End States without Labels</title><link>http://arxiv.org/abs/2405.07723v1</link><description>We focus on the problem of recognising the end state of an action in animage, which is critical for understanding what action is performed and inwhich manner. We study this focusing on the task of predicting the coarsenessof a cut, i.e., deciding whether an object was cut "coarsely" or "finely". Nodataset with these annotated end states is available, so we propose anaugmentation method to synthesise training data. We apply this method tocutting actions extracted from an existing action recognition dataset. Ourmethod is object agnostic, i.e., it presupposes the location of the object butnot its identity. Starting from less than a hundred images of a whole object,we can generate several thousands images simulating visually diverse cuts ofdifferent coarseness. We use our synthetic data to train a model based on UNetand test it on real images showing coarsely/finely cut objects. Resultsdemonstrate that the model successfully recognises the end state of the cuttingaction despite the domain gap between training and testing, and that the modelgeneralises well to unseen objects.</description><author>Davide Moltisanti, Hakan Bilen, Laura Sevilla-Lara, Frank Keller</author><pubDate>Mon, 13 May 2024 14:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07723v1</guid></item><item><title>Temporal Interest Network for User Response Prediction</title><link>http://arxiv.org/abs/2308.08487v3</link><description>User response prediction is essential in industrial recommendation systems,such as online display advertising. Among all the features in recommendationmodels, user behaviors are among the most critical. Many works have revealedthat a user's behavior reflects her interest in the candidate item, owing tothe semantic or temporal correlation between behaviors and the candidate. Whilethe literature has individually examined each of these correlations,researchers have yet to analyze them in combination, that is, thesemantic-temporal correlation. We empirically measure this correlation andobserve intuitive yet robust patterns. We then examine several popular userinterest models and find that, surprisingly, none of them learn suchcorrelation well. To fill this gap, we propose a Temporal Interest Network (TIN) to capture thesemantic-temporal correlation simultaneously between behaviors and the target.We achieve this by incorporating target-aware temporal encoding, in addition tosemantic encoding, to represent behaviors and the target. Furthermore, weconduct explicit 4-way interaction by deploying target-aware attention andtarget-aware representation to capture both semantic and temporal correlation.We conduct comprehensive evaluations on two popular public datasets, and ourproposed TIN outperforms the best-performing baselines by 0.43% and 0.29% onGAUC, respectively. During online A/B testing in Tencent's advertisingplatform, TIN achieves 1.65% cost lift and 1.93% GMV lift over the base model.It has been successfully deployed in production since October 2023, serving theWeChat Moments traffic. We have released our code athttps://github.com/zhouxy1003/TIN.</description><author>Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng Gao, Guihai Chen</author><pubDate>Mon, 13 May 2024 14:18:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08487v3</guid></item><item><title>Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders</title><link>http://arxiv.org/abs/2306.05023v3</link><description>The posterior collapse phenomenon in variational autoencoder (VAE), where thevariational posterior distribution closely matches the prior distribution, canhinder the quality of the learned latent variables. As a consequence ofposterior collapse, the latent variables extracted by the encoder in VAEpreserve less information from the input data and thus fail to producemeaningful representations as input to the reconstruction process in thedecoder. While this phenomenon has been an actively addressed topic related toVAE performance, the theory for posterior collapse remains underdeveloped,especially beyond the standard VAE. In this work, we advance the theoreticalunderstanding of posterior collapse to two important and prevalent yet lessstudied classes of VAE: conditional VAE and hierarchical VAE. Specifically, viaa non-trivial theoretical analysis of linear conditional VAE and hierarchicalVAE with two levels of latent, we prove that the cause of posterior collapsesin these models includes the correlation between the input and output of theconditional VAE and the effect of learnable encoder variance in thehierarchical VAE. We empirically validate our theoretical findings for linearconditional and hierarchical VAE and demonstrate that these results are alsopredictive for non-linear cases with extensive experiments.</description><author>Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho</author><pubDate>Mon, 13 May 2024 14:10:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05023v3</guid></item><item><title>A Unified Sequence Parallelism Approach for Long Context Generative AI</title><link>http://arxiv.org/abs/2405.07719v1</link><description>Sequence parallelism (SP), which divides the sequence dimension of inputtensors across multiple computational devices, is becoming key to unlocking thelong-context capabilities of generative AI models. This paper investigates thestate-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, andproposes a unified SP approach, which is more robust to transformer modelarchitectures and network hardware topology. This paper compares thecommunication and memory cost of SP and existing parallelism, includingdata/tensor/zero/expert/pipeline parallelism, and discusses the best practicesfor designing hybrid 4D parallelism involving SP. We achieved 86\% MFU on two8xA800 nodes using SP for sequence length 208K for the LLAMA3-8B model. Ourcode is publicly available on\url{https://github.com/feifeibear/long-context-attention}.</description><author>Jiarui Fang, Shangchun Zhao</author><pubDate>Mon, 13 May 2024 14:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07719v1</guid></item><item><title>How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses</title><link>http://arxiv.org/abs/2310.03031v3</link><description>With the introduction of ChatGPT, OpenAI made large language models (LLM)accessible to users with limited IT expertise. However, users with nobackground in natural language processing (NLP) might lack a properunderstanding of LLMs. Thus the awareness of their inherent limitations, andtherefore will take the systems' output at face value. In this paper, wesystematically analyse prompts and the generated responses to identify possibleproblematic issues with a special focus on gender biases, which users need tobe aware of when processing the system's output. We explore how ChatGPT reactsin English and German if prompted to answer from a female, male, or neutralperspective. In an in-depth investigation, we examine selected prompts andanalyse to what extent responses differ if the system is prompted several timesin an identical way. On this basis, we show that ChatGPT is indeed useful forhelping non-IT users draft texts for their daily work. However, it isabsolutely crucial to thoroughly check the system's responses for biases aswell as for syntactic and grammatical mistakes.</description><author>Stefanie Urchs, Veronika Thurner, Matthias Aßenmacher, Christian Heumann, Stephanie Thiemichen</author><pubDate>Mon, 13 May 2024 14:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03031v3</guid></item><item><title>Generalization with data-dependent quantum geometry</title><link>http://arxiv.org/abs/2303.13462v2</link><description>Generalization is the ability of machine learning models to make accuratepredictions on new data by learning from training data. However, understandinggeneralization of quantum machine learning models has been a major challenge.Here, we introduce the data quantum Fisher information metric (DQFIM). Itdescribes the capacity of variational quantum algorithms depending onvariational ansatz, training data and their symmetries. We apply the DQFIM toquantify circuit parameters and training data needed to successfully train andgeneralize. Using the dynamical Lie algebra, we explain how to generalize usinga low number of training states. Counter-intuitively, breaking symmetries ofthe training data can help to improve generalization. Finally, we find thatout-of-distribution generalization, where training and testing data are drawnfrom different data distributions, can be better than using the samedistribution. Our work provides a useful framework to explore the power ofquantum machine learning models.</description><author>Tobias Haug, M. S. Kim</author><pubDate>Mon, 13 May 2024 14:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13462v2</guid></item><item><title>Frequency-Time Diffusion with Neural Cellular Automata</title><link>http://arxiv.org/abs/2401.06291v2</link><description>Despite considerable success, large Denoising Diffusion Models (DDMs) withUNet backbone pose practical challenges, particularly on limited hardware andin processing gigapixel images. To address these limitations, we introduce twoNeural Cellular Automata (NCA)-based DDMs: Diff-NCA and FourierDiff-NCA.Capitalizing on the local communication capabilities of NCA, Diff-NCAsignificantly reduces the parameter counts of NCA-based DDMs. IntegratingFourier-based diffusion enables global communication early in the diffusionprocess. This feature is particularly valuable in synthesizing complex imageswith important global features, such as the CelebA dataset. We demonstrate thateven a 331k parameter Diff-NCA can generate 512x512 pathology slices, whileFourierDiff-NCA (1.1m parameters) reaches a three times lower FID score of43.86, compared to the four times bigger UNet (3.94m parameters) with a scoreof 128.2. Additionally, FourierDiff-NCA can perform diverse tasks such assuper-resolution, out-of-distribution image synthesis, and inpainting withoutexplicit training.</description><author>John Kalkhof, Arlene Kühn, Yannik Frisch, Anirban Mukhopadhyay</author><pubDate>Mon, 13 May 2024 13:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06291v2</guid></item><item><title>DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for Single-cell Clustering</title><link>http://arxiv.org/abs/2311.03410v2</link><description>Single-cell RNA sequencing (scRNA-seq) is important to transcriptomicanalysis of gene expression. Recently, deep learning has facilitated theanalysis of high-dimensional single-cell data. Unfortunately, deep learningmodels may leak sensitive information about users. As a result, DifferentialPrivacy (DP) is increasingly used to protect privacy. However, existing DPmethods usually perturb whole neural networks to achieve differential privacy,and hence result in great performance overheads. To address this challenge, inthis paper, we take advantage of the uniqueness of the autoencoder that itoutputs only the dimension-reduced vector in the middle of the network, anddesign a Differentially Private Deep Contrastive Autoencoder Network (DP-DCAN)by partial network perturbation for single-cell clustering. Since only partialnetwork is added with noise, the performance improvement is obvious andtwofold: one part of network is trained with less noise due to a bigger privacybudget, and the other part is trained without any noise. Experimental resultsof six datasets have verified that DP-DCAN is superior to the traditional DPscheme with whole network perturbation. Moreover, DP-DCAN demonstrates strongrobustness to adversarial attacks.</description><author>Huifa Li, Jie Fu, Zhili Chen, Xiaomin Yang, Haitao Liu, Xinpeng Ling</author><pubDate>Mon, 13 May 2024 13:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03410v2</guid></item><item><title>Secure Aggregation Meets Sparsification in Decentralized Learning</title><link>http://arxiv.org/abs/2405.07708v1</link><description>Decentralized learning (DL) faces increased vulnerability to privacy breachesdue to sophisticated attacks on machine learning (ML) models. Secureaggregation is a computationally efficient cryptographic technique that enablesmultiple parties to compute an aggregate of their private data while keepingtheir individual inputs concealed from each other and from any centralaggregator. To enhance communication efficiency in DL, sparsificationtechniques are used, selectively sharing only the most crucial parameters orgradients in a model, thereby maintaining efficiency without notablycompromising accuracy. However, applying secure aggregation to sparsifiedmodels in DL is challenging due to the transmission of disjoint parameter setsby distinct nodes, which can prevent masks from canceling out effectively. Thispaper introduces CESAR, a novel secure aggregation protocol for DL designed tobe compatible with existing sparsification mechanisms. CESAR provably defendsagainst honest-but-curious adversaries and can be formally adapted tocounteract collusion between them. We provide a foundational understanding ofthe interaction between the sparsification carried out by the nodes and theproportion of the parameters shared under CESAR in both colluding andnon-colluding environments, offering analytical insight into the working andapplicability of the protocol. Experiments on a network with 48 nodes in a3-regular topology show that with random subsampling, CESAR is always within0.5% accuracy of decentralized parallel stochastic gradient descent (D-PSGD),while adding only 11% of data overhead. Moreover, it surpasses the accuracy onTopK by up to 0.3% on independent and identically distributed (IID) data.</description><author>Sayan Biswas, Anne-Marie Kermarrec, Rafael Pires, Rishi Sharma, Milos Vujasinovic</author><pubDate>Mon, 13 May 2024 13:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07708v1</guid></item><item><title>Are Sounds Sound for Phylogenetic Reconstruction?</title><link>http://arxiv.org/abs/2402.02807v2</link><description>In traditional studies on language evolution, scholars often emphasize theimportance of sound laws and sound correspondences for phylogenetic inferenceof language family trees. However, to date, computational approaches havetypically not taken this potential into account. Most computational studiesstill rely on lexical cognates as major data source for phylogeneticreconstruction in linguistics, although there do exist a few studies in whichauthors praise the benefits of comparing words at the level of sound sequences.Building on (a) ten diverse datasets from different language families, and (b)state-of-the-art methods for automated cognate and sound correspondencedetection, we test, for the first time, the performance of sound-based versuscognate-based approaches to phylogenetic reconstruction. Our results show thatphylogenies reconstructed from lexical cognates are topologically closer, byapproximately one third with respect to the generalized quartet distance onaverage, to the gold standard phylogenies than phylogenies reconstructed fromsound correspondences.</description><author>Luise Häuser, Gerhard Jäger, Taraka Rama, Johann-Mattis List, Alexandros Stamatakis</author><pubDate>Mon, 13 May 2024 13:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02807v2</guid></item><item><title>OpenLLM-Ro -- Technical Report on Open-source Romanian LLMs trained starting from Llama 2</title><link>http://arxiv.org/abs/2405.07703v1</link><description>In recent years, Large Language Models (LLMs) have achieved almost human-likeperformance on various tasks. While some LLMs have been trained on multilingualdata, most of the training data is in English. Hence, their performance inEnglish greatly exceeds their performance in other languages. This documentpresents our approach to training and evaluating the first foundational andchat LLM specialized for Romanian.</description><author>Mihai Masala, Denis C. Ilie-Ablachim, Dragos Corlatescu, Miruna Zavelca, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea</author><pubDate>Mon, 13 May 2024 13:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07703v1</guid></item><item><title>FORESEE: Multimodal and Multi-view Representation Learning for Robust Prediction of Cancer Survival</title><link>http://arxiv.org/abs/2405.07702v1</link><description>Integrating the different data modalities of cancer patients cansignificantly improve the predictive performance of patient survival. However,most existing methods ignore the simultaneous utilization of rich semanticfeatures at different scales in pathology images. When collecting multimodaldata and extracting features, there is a likelihood of encounteringintra-modality missing data, introducing noise into the multimodal data. Toaddress these challenges, this paper proposes a new end-to-end framework,FORESEE, for robustly predicting patient survival by mining multimodalinformation. Specifically, the cross-fusion transformer effectively utilizesfeatures at the cellular level, tissue level, and tumor heterogeneity level tocorrelate prognosis through a cross-scale feature cross-fusion method. Thisenhances the ability of pathological image feature representation. Secondly,the hybrid attention encoder (HAE) uses the denoising contextual attentionmodule to obtain the contextual relationship features and local detail featuresof the molecular data. HAE's channel attention module obtains global featuresof molecular data. Furthermore, to address the issue of missing informationwithin modalities, we propose an asymmetrically masked triplet maskedautoencoder to reconstruct lost information within modalities. Extensiveexperiments demonstrate the superiority of our method over state-of-the-artmethods on four benchmark datasets in both complete and missing settings.</description><author>Liangrui Pan, Yijun Peng, Yan Li, Yiyi Liang, Liwen Xu, Qingchun Liang, Shaoliang Peng</author><pubDate>Mon, 13 May 2024 13:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07702v1</guid></item><item><title>Age-Dependent Analysis and Stochastic Generation of Child-Directed Speech</title><link>http://arxiv.org/abs/2405.07700v1</link><description>Child-directed speech (CDS) is a particular type of speech that adults usewhen addressing young children. Its properties also change as a function ofextralinguistic factors, such as age of the child being addressed. Access tolarge amounts of representative and varied CDS would be useful for childlanguage research, as this would enable controlled computational modelingexperiments of infant language acquisition with realistic input in terms ofquality and quantity. In this study, we describe an approach to modelage-dependent linguistic properties of CDS using a language model (LM) trainedon CDS transcripts and ages of the recipient children, as obtained from NorthAmerican English corpora of the CHILDES database. The created LM can then beused to stochastically generate synthetic CDS transcripts in an age-appropriatemanner, thereby scaling beyond the original datasets in size. We comparecharacteristics of the generated CDS against the real speech addressed atchildren of different ages, showing that the LM manages to captureage-dependent changes in CDS, except for a slight difference in the effectivevocabulary size. As a side product, we also provide a systematiccharacterization of age-dependent linguistic properties of CDS in CHILDES,illustrating how all measured aspects of the CDS change with children's age.</description><author>Okko Räsänen, Daniil Kocharov</author><pubDate>Mon, 13 May 2024 13:35:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07700v1</guid></item><item><title>oTTC: Object Time-to-Contact for Motion Estimation in Autonomous Driving</title><link>http://arxiv.org/abs/2405.07698v1</link><description>Autonomous driving systems require a quick and robust perception of thenearby environment to carry out their routines effectively. With the aim toavoid collisions and drive safely, autonomous driving systems rely heavily onobject detection. However, 2D object detections alone are insufficient; moreinformation, such as relative velocity and distance, is required for saferplanning. Monocular 3D object detectors try to solve this problem by directlypredicting 3D bounding boxes and object velocities given a camera image. Recentresearch estimates time-to-contact in a per-pixel manner and suggests that itis more effective measure than velocity and depth combined. However, per-pixeltime-to-contact requires object detection to serve its purpose effectively andhence increases overall computational requirements as two different models needto run. To address this issue, we propose per-object time-to-contact estimationby extending object detection models to additionally predict thetime-to-contact attribute for each object. We compare our proposed approachwith existing time-to-contact methods and provide benchmarking results onwell-known datasets. Our proposed approach achieves higher precision comparedto prior art while using a single image.</description><author>Abdul Hannan Khan, Syed Tahseen Raza Rizvi, Dheeraj Varma Chittari Macharavtu, Andreas Dengel</author><pubDate>Mon, 13 May 2024 13:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07698v1</guid></item><item><title>MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders</title><link>http://arxiv.org/abs/2405.07696v1</link><description>Monocular 3D object detection aims for precise 3D localization andidentification of objects from a single-view image. Despite its recentprogress, it often struggles while handling pervasive object occlusions thattend to complicate and degrade the prediction of object dimensions, depths, andorientations. We design MonoMAE, a monocular 3D detector inspired by MaskedAutoencoders that addresses the object occlusion issue by masking andreconstructing objects in the feature space. MonoMAE consists of two noveldesigns. The first is depth-aware masking that selectively masks certain partsof non-occluded object queries in the feature space for simulating occludedobject queries for network training. It masks non-occluded object queries bybalancing the masked and preserved query portions adaptively according to thedepth information. The second is lightweight query completion that works withthe depth-aware masking to learn to reconstruct and complete the masked objectqueries. With the proposed object occlusion and completion, MonoMAE learnsenriched 3D representations that achieve superior monocular 3D detectionperformance qualitatively and quantitatively for both occluded and non-occludedobjects. Additionally, MonoMAE learns generalizable representations that canwork well in new domains.</description><author>Xueying Jiang, Sheng Jin, Xiaoqin Zhang, Ling Shao, Shijian Lu</author><pubDate>Mon, 13 May 2024 13:32:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07696v1</guid></item><item><title>Simultaneous Tri-Modal Medical Image Fusion and Super-Resolution using Conditional Diffusion Model</title><link>http://arxiv.org/abs/2404.17357v2</link><description>In clinical practice, tri-modal medical image fusion, compared to theexisting dual-modal technique, can provide a more comprehensive view of thelesions, aiding physicians in evaluating the disease's shape, location, andbiological activity. However, due to the limitations of imaging equipment andconsiderations for patient safety, the quality of medical images is usuallylimited, leading to sub-optimal fusion performance, and affecting the depth ofimage analysis by the physician. Thus, there is an urgent need for a technologythat can both enhance image resolution and integrate multi-modal information.Although current image processing methods can effectively address image fusionand super-resolution individually, solving both problems synchronously remainsextremely challenging. In this paper, we propose TFS-Diff, a simultaneouslyrealize tri-modal medical image fusion and super-resolution model. Specially,TFS-Diff is based on the diffusion model generation of a random iterativedenoising process. We also develop a simple objective function and the proposedfusion super-resolution loss, effectively evaluates the uncertainty in thefusion and ensures the stability of the optimization process. And the channelattention module is proposed to effectively integrate key information fromdifferent modalities for clinical diagnosis, avoiding information loss causedby multiple image processing. Extensive experiments on public Harvard datasetsshow that TFS-Diff significantly surpass the existing state-of-the-art methodsin both quantitative and visual evaluations. The source code will be availableat GitHub.</description><author>Yushen Xu, Xiaosong Li, Yuchan Jie, Haishu Tan</author><pubDate>Mon, 13 May 2024 13:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17357v2</guid></item><item><title>It Couldn't Help But Overhear: On the Limits of Modelling Meta-Communicative Grounding Acts with Supervised Learning</title><link>http://arxiv.org/abs/2405.01139v2</link><description>Active participation in a conversation is key to building common ground,since understanding is jointly tailored by producers and recipients.Overhearers are deprived of the privilege of performing grounding acts and canonly conjecture about intended meanings. Still, data generation and annotation,modelling, training and evaluation of NLP dialogue models place reliance on theoverhearing paradigm. How much of the underlying grounding processes arethereby forfeited? As we show, there is evidence pointing to the impossibilityof properly modelling human meta-communicative acts with data-driven learningmodels. In this paper, we discuss this issue and provide a preliminary analysison the variability of human decisions for requesting clarification. Mostimportantly, we wish to bring this topic back to the community's table,encouraging discussion on the consequences of having models designed to only"listen in".</description><author>Brielen Madureira, David Schlangen</author><pubDate>Mon, 13 May 2024 13:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01139v2</guid></item><item><title>FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation</title><link>http://arxiv.org/abs/2405.07682v1</link><description>Singing Accompaniment Generation (SAG), which generates instrumental music toaccompany input vocals, is crucial to developing human-AI symbiotic artcreation systems. The state-of-the-art method, SingSong, utilizes a multi-stageautoregressive (AR) model for SAG, however, this method is extremely slow as itgenerates semantic and acoustic tokens recursively, and this makes itimpossible for real-time applications. In this paper, we aim to develop a FastSAG method that can create high-quality and coherent accompaniments. A non-ARdiffusion-based framework is developed, which by carefully designing theconditions inferred from the vocal signals, generates the Mel spectrogram ofthe target accompaniment directly. With diffusion and Mel spectrogram modeling,the proposed method significantly simplifies the AR token-based SingSongframework, and largely accelerates the generation. We also design semanticprojection, prior projection blocks as well as a set of loss functions, toensure the generated accompaniment has semantic and rhythm coherence with thevocal signal. By intensive experimental studies, we demonstrate that theproposed method can generate better samples than SingSong, and accelerate thegeneration by at least 30 times. Audio samples and code are available athttps://fastsag.github.io/.</description><author>Jianyi Chen, Wei Xue, Xu Tan, Zhen Ye, Qifeng Liu, Yike Guo</author><pubDate>Mon, 13 May 2024 13:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07682v1</guid></item><item><title>Establishing a Unified Evaluation Framework for Human Motion Generation: A Comparative Analysis of Metrics</title><link>http://arxiv.org/abs/2405.07680v1</link><description>The development of generative artificial intelligence for human motiongeneration has expanded rapidly, necessitating a unified evaluation framework.This paper presents a detailed review of eight evaluation metrics for humanmotion generation, highlighting their unique features and shortcomings. Wepropose standardized practices through a unified evaluation setup to facilitateconsistent model comparisons. Additionally, we introduce a novel metric thatassesses diversity in temporal distortion by analyzing warping diversity,thereby enhancing the evaluation of temporal data. We also conduct experimentalanalyses of three generative models using a publicly available dataset,offering insights into the interpretation of each metric in specific casescenarios. Our goal is to offer a clear, user-friendly evaluation framework fornewcomers, complemented by publicly accessible code.</description><author>Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier</author><pubDate>Mon, 13 May 2024 13:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07680v1</guid></item><item><title>Class-wise Activation Unravelling the Engima of Deep Double Descent</title><link>http://arxiv.org/abs/2405.07679v1</link><description>Double descent presents a counter-intuitive aspect within the machinelearning domain, and researchers have observed its manifestation in variousmodels and tasks. While some theoretical explanations have been proposed forthis phenomenon in specific contexts, an accepted theory for its occurringmechanism in deep learning remains yet to be established. In this study, werevisited the phenomenon of double descent and discussed the conditions of itsoccurrence. This paper introduces the concept of class-activation matrices anda methodology for estimating the effective complexity of functions, on which weunveil that over-parameterized models exhibit more distinct and simpler classpatterns in hidden activations compared to under-parameterized ones. We furtherlooked into the interpolation of noisy labelled data among cleanrepresentations and demonstrated overfitting w.r.t. expressive capacity. Bycomprehensively analysing hypotheses and presenting corresponding empiricalevidence that either validates or contradicts these hypotheses, we aim toprovide fresh insights into the phenomenon of double descent and benignover-parameterization and facilitate future explorations. By comprehensivelystudying different hypotheses and the corresponding empirical evidence eithersupports or challenges these hypotheses, our goal is to offer new insights intothe phenomena of double descent and benign over-parameterization, therebyenabling further explorations in the field. The source code is available athttps://github.com/Yufei-Gu-451/sparse-generalization.git.</description><author>Yufei Gu</author><pubDate>Mon, 13 May 2024 13:07:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07679v1</guid></item><item><title>CoVScreen: Pitfalls and recommendations for screening COVID-19 using Chest X-rays</title><link>http://arxiv.org/abs/2405.07674v1</link><description>The novel coronavirus (COVID-19), a highly infectious respiratory diseasecaused by the SARS-CoV-2 has emerged as an unprecedented healthcare crisis. Thepandemic had a devastating impact on the health, well-being, and economy of theglobal population. Early screening and diagnosis of symptomatic patients playscrucial role in isolation of patient to help stop community transmission aswell as providing early treatment helping in reducing the mortality rate.Although, the RT-PCR test is the gold standard for COVID-19 testing, it is amanual, laborious, time consuming, uncomfortable, and invasive process. Due toits accessibility, availability, lower-cost, ease of sanitisation, and portablesetup, chest X-Ray imaging can serve as an effective screening and diagnostictool. In this study, we first highlight limitations of existing datasets andstudies in terms of data quality, data imbalance, and evaluation strategy.Second, we curated a large-scale COVID-19 chest X-ray dataset from manypublicly available COVID-19 imaging databases and proposed a pre-processingpipeline to improve quality of the dataset. We proposed CoVScreen, an CNNarchitecture to train and test the curated dataset. The experimental resultsapplying different classification scenarios on the curated dataset in terms ofvarious evaluation metrics demonstrate the effectiveness of proposedmethodology in the screening of COVID-19 infection.</description><author>Sonit Singh</author><pubDate>Mon, 13 May 2024 13:03:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07674v1</guid></item><item><title>An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation</title><link>http://arxiv.org/abs/2405.07673v1</link><description>Massively multilingual neural machine translation (MMNMT) has been proven toenhance the translation quality of low-resource languages. In this paper, weempirically investigate the translation robustness of Indonesian-Chinesetranslation in the face of various naturally occurring noise. To assess this,we create a robustness evaluation benchmark dataset for Indonesian-Chinesetranslation. This dataset is automatically translated into Chinese using fourNLLB-200 models of different sizes. We conduct both automatic and humanevaluations. Our in-depth analysis reveal the correlations between translationerror types and the types of noise present, how these correlations changeacross different model sizes, and the relationships between automaticevaluation indicators and human evaluation indicators. The dataset is publiclyavailable at https://github.com/tjunlp-lab/ID-ZH-MTRobustEval.</description><author>Supryadi, Leiyu Pan, Deyi Xiong</author><pubDate>Mon, 13 May 2024 13:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07673v1</guid></item><item><title>Constructing a BPE Tokenization DFA</title><link>http://arxiv.org/abs/2405.07671v1</link><description>Many natural language processing systems operate over tokenizations of textto address the open-vocabulary problem. In this paper, we give and analyze analgorithm for the efficient construction of deterministic finite automatadesigned to operate directly on tokenizations produced by the popular byte pairencoding technique. This makes it possible to apply many existing techniquesand algorithms to the tokenized case, such as pattern matching, equivalencechecking of tokenization dictionaries, and composing tokenized languages invarious ways.</description><author>Martin Berglund, Willeke Martens, Brink van der Merwe</author><pubDate>Mon, 13 May 2024 12:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07671v1</guid></item><item><title>Impact of white Gaussian internal noise on analog echo-state neural networks</title><link>http://arxiv.org/abs/2405.07670v1</link><description>In recent years, more and more works have appeared devoted to the analog(hardware) implementation of artificial neural networks, in which neurons andthe connection between them are based not on computer calculations, but onphysical principles. Such networks offer improved energy efficiency and, insome cases, scalability, but may be susceptible to internal noise. This paperstudies the influence of noise on the functioning of recurrent networks usingthe example of trained echo state networks (ESNs). The most common reservoirconnection matrices were chosen as various topologies of ESNs: random uniformand band matrices with different connectivity. White Gaussian noise was chosenas the influence, and according to the way of its introducing it was additiveor multiplicative, as well as correlated or uncorrelated. In the paper, we showthat the propagation of noise in reservoir is mainly controlled by thestatistical properties of the output connection matrix, namely the mean and themean square. Depending on these values, more correlated or uncorrelated noiseaccumulates in the network. We also show that there are conditions under whicheven noise with an intensity of $10^{-20}$ is already enough to completely losethe useful signal. In the article we show which types of noise are mostcritical for networks with different activation functions (hyperbolic tangent,sigmoid and linear) and if the network is self-closed.</description><author>Nadezhda Semenova</author><pubDate>Mon, 13 May 2024 12:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07670v1</guid></item><item><title>Conversational Disease Diagnosis via External Planner-Controlled Large Language Models</title><link>http://arxiv.org/abs/2404.04292v4</link><description>The development of large language models (LLMs) has brought unprecedentedpossibilities for artificial intelligence (AI) based medical diagnosis.However, the application perspective of LLMs in real diagnostic scenarios isstill unclear because they are not adept at collecting patient dataproactively. This study presents a LLM-based diagnostic system that enhancesplanning capabilities by emulating doctors. Our system involves two externalplanners to handle planning tasks. The first planner employs a reinforcementlearning approach to formulate disease screening questions and conduct initialdiagnoses. The second planner uses LLMs to parse medical guidelines and conductdifferential diagnoses. By utilizing real patient electronic medical recorddata, we constructed simulated dialogues between virtual patients and doctorsand evaluated the diagnostic abilities of our system. We demonstrate that oursystem significantly surpasses existing models, including GPT-4 Turbo, in bothdisease screening and differential diagnoses. This research represents a steptowards more seamlessly integrating AI into clinical settings, potentiallyenhancing the accuracy and accessibility of medical diagnostics.</description><author>Zhoujian Sun, Cheng Luo, Ziyi Liu, Zhengxing Huang</author><pubDate>Mon, 13 May 2024 12:58:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04292v4</guid></item><item><title>CrossCert: A Cross-Checking Detection Approach to Patch Robustness Certification for Deep Learning Models</title><link>http://arxiv.org/abs/2405.07668v1</link><description>Patch robustness certification is an emerging kind of defense techniqueagainst adversarial patch attacks with provable guarantees. There are tworesearch lines: certified recovery and certified detection. They aim to labelmalicious samples with provable guarantees correctly and issue warnings formalicious samples predicted to non-benign labels with provable guarantees,respectively. However, existing certified detection defenders suffer fromprotecting labels subject to manipulation, and existing certified recoverydefenders cannot systematically warn samples about their labels. A certifieddefense that simultaneously offers robust labels and systematic warningprotection against patch attacks is desirable. This paper proposes a novelcertified defense technique called CrossCert. CrossCert formulates a novelapproach by cross-checking two certified recovery defenders to provideunwavering certification and detection certification. Unwavering certificationensures that a certified sample, when subjected to a patched perturbation, willalways be returned with a benign label without triggering any warnings with aprovable guarantee. To our knowledge, CrossCert is the first certifieddetection technique to offer this guarantee. Our experiments show that, with aslightly lower performance than ViP and comparable performance with PatchCensorin terms of detection certification, CrossCert certifies a significantproportion of samples with the guarantee of unwavering certification.</description><author>Qilin Zhou, Zhengyuan Wei, Haipeng Wang, Bo Jiang, W. K. Chan</author><pubDate>Mon, 13 May 2024 12:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07668v1</guid></item><item><title>Backdoor Removal for Generative Large Language Models</title><link>http://arxiv.org/abs/2405.07667v1</link><description>With rapid advances, generative large language models (LLMs) dominate variousNatural Language Processing (NLP) tasks from understanding to reasoning. Yet,language models' inherent vulnerabilities may be exacerbated due to increasedaccessibility and unrestricted model training on massive textual data from theInternet. A malicious adversary may publish poisoned data online and conductbackdoor attacks on the victim LLMs pre-trained on the poisoned data.Backdoored LLMs behave innocuously for normal queries and generate harmfulresponses when the backdoor trigger is activated. Despite significant effortspaid to LLMs' safety issues, LLMs are still struggling against backdoorattacks. As Anthropic recently revealed, existing safety training strategies,including supervised fine-tuning (SFT) and Reinforcement Learning from HumanFeedback (RLHF), fail to revoke the backdoors once the LLM is backdoored duringthe pre-training stage. In this paper, we present Simulate and Eliminate(SANDE) to erase the undesired backdoored mappings for generative LLMs. Weinitially propose Overwrite Supervised Fine-tuning (OSFT) for effectivebackdoor removal when the trigger is known. Then, to handle the scenarios wherethe trigger patterns are unknown, we integrate OSFT into our two-stageframework, SANDE. Unlike previous works that center on the identification ofbackdoors, our safety-enhanced LLMs are able to behave normally even when theexact triggers are activated. We conduct comprehensive experiments to show thatour proposed SANDE is effective against backdoor attacks while bringing minimalharm to LLMs' powerful capability without any additional access to unbackdooredclean models. We will release the reproducible code.</description><author>Haoran Li, Yulin Chen, Zihao Zheng, Qi Hu, Chunkit Chan, Heshan Liu, Yangqiu Song</author><pubDate>Mon, 13 May 2024 12:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07667v1</guid></item><item><title>Partial information decomposition as information bottleneck</title><link>http://arxiv.org/abs/2405.07665v1</link><description>The partial information decomposition (PID) aims to quantify the amount ofredundant information that a set of sources provide about a target. Here weshow that this goal can be formulated as a type of information bottleneck (IB)problem, which we term the "redundancy bottleneck" (RB). The RB formalizes atradeoff between prediction and compression: it extracts information from thesources that predicts the target, without revealing which source provided theinformation. It can be understood as a generalization "Blackwell redundancy",which we previously proposed as a principled measure of PID redundancy. The "RBcurve" quantifies the prediction/compression tradeoff at multiple scales. Thiscurve can also be quantified for individual sources, allowing subsets ofredundant sources to be identified without combinatorial optimization. Weprovide an efficient iterative algorithm for computing the RB curve.</description><author>Artemy Kolchinsky</author><pubDate>Mon, 13 May 2024 12:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07665v1</guid></item></channel></rss>