<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 20 Jul 2024 01:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model</title><link>http://arxiv.org/abs/2407.13772v1</link><description>Recent advancements in state-space models (SSMs) have showcased effectiveperformance in modeling long-range dependencies with subquadratic complexity.However, pure SSM-based models still face challenges related to stability andachieving optimal performance on computer vision tasks. Our paper addresses thechallenges of scaling SSM-based models for computer vision, particularly theinstability and inefficiency of large model sizes. To address this, weintroduce a Modulated Group Mamba layer which divides the input channels intofour groups and applies our proposed SSM-based efficient Visual SingleSelective Scanning (VSSS) block independently to each group, with each VSSSblock scanning in one of the four spatial directions. The Modulated Group Mambalayer also wraps the four VSSS blocks into a channel modulation operator toimprove cross-channel communication. Furthermore, we introduce adistillation-based training objective to stabilize the training of largemodels, leading to consistent performance gains. Our comprehensive experimentsdemonstrate the merits of the proposed contributions, leading to superiorperformance over existing methods for image classification on ImageNet-1K,object detection, instance segmentation on MS-COCO, and semantic segmentationon ADE20K. Our tiny variant with 23M parameters achieves state-of-the-artperformance with a classification top-1 accuracy of 83.3% on ImageNet-1K, whilebeing 26% efficient in terms of parameters, compared to the best existing Mambadesign of same model size. Our code and models are available at:https://github.com/Amshaker/GroupMamba.</description><author>Abdelrahman Shaker, Syed Talal Wasim, Salman Khan, Juergen Gall, Fahad Shahbaz Khan</author><pubDate>Thu, 18 Jul 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13772v1</guid></item><item><title>Training-Free Model Merging for Multi-target Domain Adaptation</title><link>http://arxiv.org/abs/2407.13771v1</link><description>In this paper, we study multi-target domain adaptation of scene understandingmodels. While previous methods achieved commendable results throughinter-domain consistency losses, they often assumed unrealistic simultaneousaccess to images from all target domains, overlooking constraints such as datatransfer bandwidth limitations and data privacy concerns. Given thesechallenges, we pose the question: How to merge models adapted independently ondistinct domains while bypassing the need for direct access to training data?Our solution to this problem involves two components, merging model parametersand merging model buffers (i.e., normalization layer statistics). For mergingmodel parameters, empirical analyses of mode connectivity surprisingly revealthat linear merging suffices when employing the same pretrained backboneweights for adapting separate models. For merging model buffers, we model thereal-world distribution with a Gaussian prior and estimate new statistics fromthe buffers of separately trained models. Our method is simple yet effective,achieving comparable performance with data combination training baselines,while eliminating the need for accessing training data. Project page:https://air-discover.github.io/ModelMerging</description><author>Wenyi Li, Huan-ang Gao, Mingju Gao, Beiwen Tian, Rong Zhi, Hao Zhao</author><pubDate>Thu, 18 Jul 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13771v1</guid></item><item><title>NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields</title><link>http://arxiv.org/abs/2404.01300v3</link><description>Neural fields excel in computer vision and robotics due to their ability tounderstand the 3D visual world such as inferring semantics, geometry, anddynamics. Given the capabilities of neural fields in densely representing a 3Dscene from 2D images, we ask the question: Can we scale their self-supervisedpretraining, specifically using masked autoencoders, to generate effective 3Drepresentations from posed RGB images. Owing to the astounding success ofextending transformers to novel data modalities, we employ standard 3D VisionTransformers to suit the unique formulation of NeRFs. We leverage NeRF'svolumetric grid as a dense input to the transformer, contrasting it with other3D representations such as pointclouds where the information density can beuneven, and the representation is irregular. Due to the difficulty of applyingmasked autoencoders to an implicit representation, such as NeRF, we opt forextracting an explicit representation that canonicalizes scenes across domainsby employing the camera trajectory for sampling. Our goal is made possible bymasking random patches from NeRF's radiance and density grid and employing astandard 3D Swin Transformer to reconstruct the masked patches. In doing so,the model can learn the semantic and spatial structure of complete scenes. Wepretrain this representation at scale on our proposed curated posed-RGB data,totaling over 1.8 million images. Once pretrained, the encoder is used foreffective 3D transfer learning. Our novel self-supervised pretraining forNeRFs, NeRF-MAE, scales remarkably well and improves performance on variouschallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRFscene understanding baselines on Front3D and ScanNet datasets with an absoluteperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.</description><author>Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</author><pubDate>Thu, 18 Jul 2024 17:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01300v3</guid></item><item><title>Addressing Imbalance for Class Incremental Learning in Medical Image Classification</title><link>http://arxiv.org/abs/2407.13768v1</link><description>Deep convolutional neural networks have made significant breakthroughs inmedical image classification, under the assumption that training samples fromall classes are simultaneously available. However, in real-world medicalscenarios, there's a common need to continuously learn about new diseases,leading to the emerging field of class incremental learning (CIL) in themedical domain. Typically, CIL suffers from catastrophic forgetting whentrained on new classes. This phenomenon is mainly caused by the imbalancebetween old and new classes, and it becomes even more challenging withimbalanced medical datasets. In this work, we introduce two simple yeteffective plug-in methods to mitigate the adverse effects of the imbalance.First, we propose a CIL-balanced classification loss to mitigate the classifierbias toward majority classes via logit adjustment. Second, we propose adistribution margin loss that not only alleviates the inter-class overlap inembedding space but also enforces the intra-class compactness. We evaluate theeffectiveness of our method with extensive experiments on three benchmarkdatasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that ourapproach outperforms state-of-the-art methods.</description><author>Xuze Hao, Wenqian Ni, Xuhao Jiang, Weimin Tan, Bo Yan</author><pubDate>Thu, 18 Jul 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13768v1</guid></item><item><title>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</title><link>http://arxiv.org/abs/2403.09635v2</link><description>In spite of their huge success, transformer models remain difficult to scalein depth. In this work, we develop a unified signal propagation theory andprovide formulae that govern the moments of the forward and backward signalthrough the transformer model. Our framework can be used to understand andmitigate vanishing/exploding gradients, rank collapse, and instabilityassociated with high attention scores. We also propose DeepScaleLM, aninitialization and scaling scheme that conserves unit output/gradient momentsthroughout the model, enabling the training of very deep models with 1000layers. We find that transformer models could be much deeper - our deep modelswith fewer parameters outperform shallow models in Language Modeling, SpeechTranslation, and Image Classification, across encoder-only, decoder-only andencoder-decoder variants, for both Pre-LN and Post-LN transformers, formultiple datasets and model sizes. These improvements also translate intoimproved performance on downstream Question Answering tasks and improvedrobustness for Image Classification.</description><author>Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee</author><pubDate>Thu, 18 Jul 2024 17:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09635v2</guid></item><item><title>Visual Haystacks: Answering Harder Questions About Sets of Images</title><link>http://arxiv.org/abs/2407.13766v1</link><description>Recent advancements in Large Multimodal Models (LMMs) have made significantprogress in the field of single-image visual question answering. However, thesemodels face substantial challenges when tasked with queries that span extensivecollections of images, similar to real-world scenarios like searching throughlarge photo albums, finding specific information across the internet, ormonitoring environmental changes through satellite imagery. This paper exploresthe task of Multi-Image Visual Question Answering (MIQA): given a large set ofimages and a natural language query, the task is to generate a relevant andgrounded response. We propose a new public benchmark, dubbed "Visual Haystacks(VHs)," specifically designed to evaluate LMMs' capabilities in visualretrieval and reasoning over sets of unrelated images, where we performcomprehensive evaluations demonstrating that even robust closed-source modelsstruggle significantly. Towards addressing these shortcomings, we introduceMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QAframework tailored for LMMs that confronts the challenges of MIQA with markedefficiency and accuracy improvements over baseline methods. Our evaluationshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHsbenchmark and offers up to 3.4x improvements in efficiency over text-focusedmulti-stage approaches.</description><author>Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E. Gonzalez, Trevor Darrell, David M. Chan</author><pubDate>Thu, 18 Jul 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13766v1</guid></item><item><title>Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data</title><link>http://arxiv.org/abs/2407.13765v1</link><description>As language models (LMs) deliver increasing performance on a range of NLPtasks, probing classifiers have become an indispensable technique in the effortto better understand their inner workings. A typical setup involves (1)defining an auxiliary task consisting of a dataset of text annotated withlabels, then (2) supervising small classifiers to predict the labels from therepresentations of a pretrained LM as it processed the dataset. A high probingaccuracy is interpreted as evidence that the LM has learned to perform theauxiliary task as an unsupervised byproduct of its original pretrainingobjective. Despite the widespread usage of probes, however, the robust designand analysis of probing experiments remains a challenge. We develop a formalperspective on probing using structural causal models (SCM). Specifically,given an SCM which explains the distribution of tokens observed duringtraining, we frame the central hypothesis as whether the LM has learned torepresent the latent variables of the SCM. Empirically, we extend a recentstudy of LMs in the context of a synthetic grid-world navigation task, wherehaving an exact model of the underlying causal structure allows us to drawstrong inferences from the result of probing experiments. Our techniquesprovide robust empirical evidence for the ability of LMs to learn the latentcausal concepts underlying text.</description><author>Charles Jin</author><pubDate>Thu, 18 Jul 2024 17:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13765v1</guid></item><item><title>Shape of Motion: 4D Reconstruction from a Single Video</title><link>http://arxiv.org/abs/2407.13764v1</link><description>Monocular dynamic reconstruction is a challenging and long-standing visionproblem due to the highly ill-posed nature of the task. Existing approaches arelimited in that they either depend on templates, are effective only inquasi-static scenes, or fail to model 3D motion explicitly. In this work, weintroduce a method capable of reconstructing generic dynamic scenes, featuringexplicit, full-sequence-long 3D motion, from casually captured monocularvideos. We tackle the under-constrained nature of the problem with two keyinsights: First, we exploit the low-dimensional structure of 3D motion byrepresenting scene motion with a compact set of SE3 motion bases. Each point'smotion is expressed as a linear combination of these bases, facilitating softdecomposition of the scene into multiple rigidly-moving groups. Second, weutilize a comprehensive set of data-driven priors, including monocular depthmaps and long-range 2D tracks, and devise a method to effectively consolidatethese noisy supervisory signals, resulting in a globally consistentrepresentation of the dynamic scene. Experiments show that our method achievesstate-of-the-art performance for both long-range 3D/2D motion estimation andnovel view synthesis on dynamic scenes. Project Page:https://shape-of-motion.github.io/</description><author>Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, Angjoo Kanazawa</author><pubDate>Thu, 18 Jul 2024 17:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13764v1</guid></item><item><title>Scalable Spatiotemporal Prediction with Bayesian Neural Fields</title><link>http://arxiv.org/abs/2403.07657v2</link><description>Spatiotemporal datasets, which consist of spatially-referenced time series,are ubiquitous in many scientific and business-intelligence applications, suchas air pollution monitoring, disease tracking, and cloud-demand forecasting. Asmodern datasets continue to increase in size and complexity, there is a growingneed for new statistical methods that are flexible enough to capture complexspatiotemporal dynamics and scalable enough to handle large predictionproblems. This work presents the Bayesian Neural Field (BayesNF), adomain-general statistical model for inferring rich probability distributionsover a spatiotemporal domain, which can be used for data-analysis tasksincluding forecasting, interpolation, and variography. BayesNF integrates anovel deep neural network architecture for high-capacity function estimationwith hierarchical Bayesian inference for robust uncertainty quantification. Bydefining the prior through a sequence of smooth differentiable transforms,posterior inference is conducted on large-scale data using variationallylearned surrogates trained via stochastic gradient descent. We evaluate BayesNFagainst prominent statistical and machine-learning baselines, showingconsiderable improvements on diverse prediction problems from climate andpublic health datasets that contain tens to hundreds of thousands ofmeasurements. The paper is accompanied with an open-source software package(https://github.com/google/bayesnf) that is easy-to-use and compatible withmodern GPU and TPU accelerators on the JAX machine learning platform.</description><author>Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs Köster, Rif A. Saurous, Matthew Hoffman</author><pubDate>Thu, 18 Jul 2024 17:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07657v2</guid></item><item><title>SegPoint: Segment Any Point Cloud via Large Language Model</title><link>http://arxiv.org/abs/2407.13761v1</link><description>Despite significant progress in 3D point cloud segmentation, existing methodsprimarily address specific tasks and depend on explicit instructions toidentify targets, lacking the capability to infer and understand implicit userintentions in a unified framework. In this work, we propose a model, calledSegPoint, that leverages the reasoning capabilities of a multi-modal LargeLanguage Model (LLM) to produce point-wise segmentation masks across a diverserange of tasks: 1) 3D instruction segmentation, 2) 3D referring segmentation,3) 3D semantic segmentation, and 4) 3D open-vocabulary semantic segmentation.To advance 3D instruction research, we introduce a new benchmark, Instruct3D,designed to evaluate segmentation performance from complex and implicitinstructional texts, featuring 2,565 point cloud-instruction pairs. Ourexperimental results demonstrate that SegPoint achieves competitive performanceon established benchmarks such as ScanRefer for referring segmentation andScanNet for semantic segmentation, while delivering outstanding outcomes on theInstruct3D dataset. To our knowledge, SegPoint is the first model to addressthese varied segmentation tasks within a single framework, achievingsatisfactory performance.</description><author>Shuting He, Henghui Ding, Xudong Jiang, Bihan Wen</author><pubDate>Thu, 18 Jul 2024 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13761v1</guid></item><item><title>Neural Network Tire Force Modeling for Automated Drifting</title><link>http://arxiv.org/abs/2407.13760v1</link><description>Automated drifting presents a challenge problem for vehicle control,requiring models and control algorithms that can precisely handle nonlinear,coupled tire forces at the friction limits. We present a neural networkarchitecture for predicting front tire lateral force as a drop-in replacementfor physics-based approaches. With a full-scale automated vehicle purpose-builtfor the drifting application, we deploy these models in a nonlinear modelpredictive controller tuned for tracking a reference drifting trajectory, fordirect comparisons of model performance. The neural network tire model exhibitssignificantly improved path tracking performance over the brush tire model incases where front-axle braking force is applied, suggesting the neuralnetwork's ability to express previously unmodeled, latent dynamics in thedrifting condition.</description><author>Nicholas Drake Broadbent, Trey Weber, Daiki Mori, J. Christian Gerdes</author><pubDate>Thu, 18 Jul 2024 17:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13760v1</guid></item><item><title>Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion</title><link>http://arxiv.org/abs/2407.13759v1</link><description>We present a method for generating Streetscapes-long sequences of viewsthrough an on-the-fly synthesized city-scale scene. Our generation isconditioned by language input (e.g., city name, weather), as well as anunderlying map/layout hosting the desired trajectory. Compared to recent modelsfor video generation or 3D view synthesis, our method can scale to muchlonger-range camera trajectories, spanning several city blocks, whilemaintaining visual quality and consistency. To achieve this goal, we build onrecent work on video diffusion, used within an autoregressive framework thatcan easily scale to long sequences. In particular, we introduce a new temporalimputation method that prevents our autoregressive approach from drifting fromthe distribution of realistic city imagery. We train our Streetscapes system ona compelling source of data-posed imagery from Google Street View, along withcontextual map data-which allows users to generate city views conditioned onany desired city layout, with controllable camera poses. Please see moreresults at our project page at https://boyangdeng.com/streetscapes.</description><author>Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas Guibas, Noah Snavely, Gordon Wetzstein</author><pubDate>Thu, 18 Jul 2024 17:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13759v1</guid></item><item><title>Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models</title><link>http://arxiv.org/abs/2407.13757v1</link><description>Retrieval-Augmented Generation (RAG) is applied to solve hallucinationproblems and real-time constraints of large language models, but it alsoinduces vulnerabilities against retrieval corruption attacks. Existing researchmainly explores the unreliability of RAG in white-box and closed-domain QAtasks. In this paper, we aim to reveal the vulnerabilities ofRetrieval-Enhanced Generative (RAG) models when faced with black-box attacksfor opinion manipulation. We explore the impact of such attacks on usercognition and decision-making, providing new insight to enhance the reliabilityand security of RAG models. We manipulate the ranking results of the retrievalmodel in RAG with instruction and use these results as data to train asurrogate model. By employing adversarial retrieval attack methods to thesurrogate model, black-box transfer attacks on RAG are further realized.Experiments conducted on opinion datasets across multiple topics show that theproposed attack strategy can significantly alter the opinion polarity of thecontent generated by RAG. This demonstrates the model's vulnerability and, moreimportantly, reveals the potential negative impact on user cognition anddecision-making, making it easier to mislead users into accepting incorrect orbiased information.</description><author>Zhuo Chen, Jiawei Liu, Haotan Liu, Qikai Cheng, Fan Zhang, Wei Lu, Xiaozhong Liu</author><pubDate>Thu, 18 Jul 2024 17:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13757v1</guid></item><item><title>Random Latent Exploration for Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2407.13755v1</link><description>The ability to efficiently explore high-dimensional state spaces is essentialfor the practical success of deep Reinforcement Learning (RL). This paperintroduces a new exploration technique called Random Latent Exploration (RLE),that combines the strengths of bonus-based and noise-based (two popularapproaches for effective exploration in deep RL) exploration strategies. RLEleverages the idea of perturbing rewards by adding structured random rewards tothe original task rewards in certain (random) states of the environment, toencourage the agent to explore the environment during training. RLE isstraightforward to implement and performs well in practice. To demonstrate thepractical effectiveness of RLE, we evaluate it on the challenging Atari andIsaacGym benchmarks and show that RLE exhibits higher overall scores across allthe tasks than other approaches.</description><author>Srinath Mahankali, Zhang-Wei Hong, Ayush Sekhari, Alexander Rakhlin, Pulkit Agrawal</author><pubDate>Thu, 18 Jul 2024 17:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13755v1</guid></item><item><title>Exploring Facial Biomarkers for Depression through Temporal Analysis of Action Units</title><link>http://arxiv.org/abs/2407.13753v1</link><description>Depression is characterized by persistent sadness and loss of interest,significantly impairing daily functioning and now a widespread mental disorder.Traditional diagnostic methods rely on subjective assessments, necessitatingobjective approaches for accurate diagnosis. Our study investigates the use offacial action units (AUs) and emotions as biomarkers for depression. Weanalyzed facial expressions from video data of participants classified with orwithout depression. Our methodology involved detailed feature extraction, meanintensity comparisons of key AUs, and the application of time seriesclassification models. Furthermore, we employed Principal Component Analysis(PCA) and various clustering algorithms to explore the variability in emotionalexpression patterns. Results indicate significant differences in theintensities of AUs associated with sadness and happiness between the groups,highlighting the potential of facial analysis in depression assessment.</description><author>Aditya Parikh, Misha Sadeghi, Bjorn Eskofier</author><pubDate>Thu, 18 Jul 2024 17:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13753v1</guid></item><item><title>LogoSticker: Inserting Logos into Diffusion Models for Customized Generation</title><link>http://arxiv.org/abs/2407.13752v1</link><description>Recent advances in text-to-image model customization have underscored theimportance of integrating new concepts with a few examples. Yet, theseprogresses are largely confined to widely recognized subjects, which can belearned with relative ease through models' adequate shared prior knowledge. Incontrast, logos, characterized by unique patterns and textual elements, arehard to establish shared knowledge within diffusion models, thus presenting aunique challenge. To bridge this gap, we introduce the task of logo insertion.Our goal is to insert logo identities into diffusion models and enable theirseamless synthesis in varied contexts. We present a novel two-phase pipelineLogoSticker to tackle this task. First, we propose the actor-critic relationpre-training algorithm, which addresses the nontrivial gaps in models'understanding of the potential spatial positioning of logos and interactionswith other objects. Second, we propose a decoupled identity learning algorithm,which enables precise localization and identity extraction of logos.LogoSticker can generate logos accurately and harmoniously in diverse contexts.We comprehensively validate the effectiveness of LogoSticker over customizationmethods and large models such as DALLE~3.\href{https://mingkangz.github.io/logosticker}{Project page}.</description><author>Mingkang Zhu, Xi Chen, Zhongdao Wang, Hengshuang Zhao, Jiaya Jia</author><pubDate>Thu, 18 Jul 2024 17:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13752v1</guid></item><item><title>Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management</title><link>http://arxiv.org/abs/2407.13751v1</link><description>In the era of rapid globalization and digitalization, accurate identificationof similar stocks has become increasingly challenging due to the non-stationarynature of financial markets and the ambiguity in conventional regional andsector classifications. To address these challenges, we examine SimStock, anovel temporal self-supervised learning framework that combines techniques fromself-supervised learning (SSL) and temporal domain generalization to learnrobust and informative representations of financial time series data. Theprimary focus of our study is to understand the similarities between stocksfrom a broader perspective, considering the complex dynamics of the globalfinancial landscape. We conduct extensive experiments on four real-worlddatasets with thousands of stocks and demonstrate the effectiveness of SimStockin finding similar stocks, outperforming existing methods. The practicalutility of SimStock is showcased through its application to various investmentstrategies, such as pairs trading, index tracking, and portfolio optimization,where it leads to superior performance compared to conventional methods. Ourfindings empirically examine the potential of data-driven approach to enhanceinvestment decision-making and risk management practices by leveraging thepower of temporal self-supervised learning in the face of the ever-changingglobal financial landscape.</description><author>Yoontae Hwang, Stefan Zohren, Yongjae Lee</author><pubDate>Thu, 18 Jul 2024 17:54:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13751v1</guid></item><item><title>Pose-guided multi-task video transformer for driver action recognition</title><link>http://arxiv.org/abs/2407.13750v1</link><description>We investigate the task of identifying situations of distracted drivingthrough analysis of in-car videos. To tackle this challenge we introduce amulti-task video transformer that predicts both distracted actions and driverpose. Leveraging VideoMAEv2, a large pre-trained architecture, our approachincorporates semantic information from human keypoint locations to enhanceaction recognition and decrease computational overhead by minimizing the numberof spatio-temporal tokens. By guiding token selection with pose and classinformation, we notably reduce the model's computational requirements whilepreserving the baseline accuracy. Our model surpasses existing state-of-the artresults in driver action recognition while exhibiting superior efficiencycompared to current video transformer-based approaches.</description><author>Ricardo Pizarro, Roberto Valle, Luis Miguel Bergasa, José M. Buenaposada, Luis Baumela</author><pubDate>Thu, 18 Jul 2024 17:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13750v1</guid></item><item><title>General Geometry-aware Weakly Supervised 3D Object Detection</title><link>http://arxiv.org/abs/2407.13748v1</link><description>3D object detection is an indispensable component for scene understanding.However, the annotation of large-scale 3D datasets requires significant humaneffort. To tackle this problem, many methods adopt weakly supervised 3D objectdetection that estimates 3D boxes by leveraging 2D boxes andscene/class-specific priors. However, these approaches generally depend onsophisticated manual priors, which is hard to generalize to novel categoriesand scenes. In this paper, we are motivated to propose a general approach,which can be easily adapted to new scenes and/or classes. A unified frameworkis developed for learning 3D object detectors from RGB images and associated 2Dboxes. In specific, we propose three general components: prior injection moduleto obtain general object geometric priors from LLM model, 2D space projectionconstraint to minimize the discrepancy between the boundaries of projected 3Dboxes and their corresponding 2D boxes on the image plane, and 3D spacegeometry constraint to build a Point-to-Box alignment loss to further refinethe pose of estimated 3D boxes. Experiments on KITTI and SUN-RGBD datasetsdemonstrate that our method yields surprisingly high-quality 3D bounding boxeswith only 2D annotation. The source code is available athttps://github.com/gwenzhang/GGA.</description><author>Guowen Zhang, Junsong Fan, Liyi Chen, Zhaoxiang Zhang, Zhen Lei, Lei Zhang</author><pubDate>Thu, 18 Jul 2024 17:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13748v1</guid></item><item><title>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</title><link>http://arxiv.org/abs/2306.00978v5</link><description>Large language models (LLMs) have transformed numerous AI applications.On-device LLM is becoming increasingly important: running LLMs locally on edgedevices can reduce the cloud computing cost and protect users' privacy.However, the astronomical model size and the limited hardware resource posesignificant deployment challenges. We propose Activation-aware WeightQuantization (AWQ), a hardware-friendly approach for LLM low-bit weight-onlyquantization. AWQ finds that not all weights in an LLM are equally important.Protecting only 1% salient weights can greatly reduce quantization error. Toidentify salient weight channels, we should refer to the activationdistribution, not weights. To avoid the hardware-inefficient mix-precisionquantization, we mathematically derive that scaling up the salient channels canreduce the quantization error. AWQ employs an equivalent transformation toscale the salient weight channels to protect them. The scale is determined bycollecting the activation statistics offline. AWQ does not rely on anybackpropagation or reconstruction, so it generalizes to different domains andmodalities without overfitting the calibration set. AWQ outperforms existingwork on various language modeling and domain-specific benchmarks (coding andmath). Thanks to better generalization, it achieves excellent quantizationperformance for instruction-tuned LMs and, for the first time, multi-modal LMs.Alongside AWQ, we implement TinyChat, an efficient and flexible inferenceframework tailored for 4-bit on-device LLM/VLMs. With kernel fusion andplatform-aware weight packing, TinyChat offers more than 3x speedup over theHuggingface FP16 implementation on both desktop and mobile GPUs. It alsodemocratizes the deployment of the 70B Llama-2 model on mobile GPUs.</description><author>Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han</author><pubDate>Thu, 18 Jul 2024 17:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00978v5</guid></item><item><title>Multi-Label Learning with Stronger Consistency Guarantees</title><link>http://arxiv.org/abs/2407.13746v1</link><description>We present a detailed study of surrogate losses and algorithms formulti-label learning, supported by $H$-consistency bounds. We first show that,for the simplest form of multi-label loss (the popular Hamming loss), thewell-known consistent binary relevance surrogate suffers from a sub-optimaldependency on the number of labels in terms of $H$-consistency bounds, whenusing smooth losses such as logistic losses. Furthermore, this loss functionfails to account for label correlations. To address these drawbacks, weintroduce a novel surrogate loss, multi-label logistic loss, that accounts forlabel correlations and benefits from label-independent $H$-consistency bounds.We then broaden our analysis to cover a more extensive family of multi-labellosses, including all common ones and a new extension defined based onlinear-fractional functions with respect to the confusion matrix. We alsoextend our multi-label logistic losses to more comprehensive multi-labelcomp-sum losses, adapting comp-sum losses from standard classification to themulti-label learning. We prove that this family of surrogate losses benefitsfrom $H$-consistency bounds, and thus Bayes-consistency, across any generalmulti-label loss. Our work thus proposes a unified surrogate loss frameworkbenefiting from strong consistency guarantees for any multi-label loss,significantly expanding upon previous work which only establishedBayes-consistency and for specific loss functions. Additionally, we adaptconstrained losses from standard classification to multi-label constrainedlosses in a similar way, which also benefit from $H$-consistency bounds andthus Bayes-consistency for any multi-label loss. We further describe efficientgradient computation algorithms for minimizing the multi-label logistic loss.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Thu, 18 Jul 2024 17:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13746v1</guid></item><item><title>MaRINeR: Enhancing Novel Views by Matching Rendered Images with Nearby References</title><link>http://arxiv.org/abs/2407.13745v1</link><description>Rendering realistic images from 3D reconstruction is an essential task ofmany Computer Vision and Robotics pipelines, notably for mixed-realityapplications as well as training autonomous agents in simulated environments.However, the quality of novel views heavily depends of the sourcereconstruction which is often imperfect due to noisy or missing geometry andappearance. Inspired by the recent success of reference-based super-resolutionnetworks, we propose MaRINeR, a refinement method that leverages information ofa nearby mapping image to improve the rendering of a target viewpoint. We firstestablish matches between the raw rendered image of the scene geometry from thetarget viewpoint and the nearby reference based on deep features, followed byhierarchical detail transfer. We show improved renderings in quantitativemetrics and qualitative examples from both explicit and implicit scenerepresentations. We further employ our method on the downstream tasks ofpseudo-ground-truth validation, synthetic data enhancement and detail recoveryfor renderings of reduced 3D reconstructions.</description><author>Lukas Bösiger, Mihai Dusmanu, Marc Pollefeys, Zuria Bauer</author><pubDate>Thu, 18 Jul 2024 17:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13745v1</guid></item><item><title>LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation</title><link>http://arxiv.org/abs/2407.13744v1</link><description>Natural Language Processing has moved rather quickly from modelling specifictasks to taking more general pre-trained models and fine-tuning them forspecific tasks, to a point where we now have what appear to be inherentlygeneralist models. This paper argues that the resultant loss of clarity on whatthese models model leads to metaphors like "artificial general intelligences"that are not helpful for evaluating their strengths and weaknesses. Theproposal is to see their generality, and their potential value, in theirability to approximate specialist function, based on a natural languagespecification. This framing brings to the fore questions of the quality of theapproximation, but beyond that, also questions of discoverability, stability,and protectability of these functions. As the paper will show, this framinghence brings together in one conceptual framework various aspects ofevaluation, both from a practical and a theoretical perspective, as well asquestions often relegated to a secondary status (such as "prompt injection" and"jailbreaking").</description><author>David Schlangen</author><pubDate>Thu, 18 Jul 2024 17:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13744v1</guid></item><item><title>Benchmarking Vision Language Models for Cultural Understanding</title><link>http://arxiv.org/abs/2407.10920v2</link><description>Foundation models and vision-language pre-training have notably advancedVision Language Models (VLMs), enabling multimodal processing of visual andlinguistic data. However, their performance has been typically assessed ongeneral scene understanding - recognizing objects, attributes, and actions -rather than cultural comprehension. This study introduces CulturalVQA, a visualquestion-answering benchmark aimed at assessing VLM's geo-diverse culturalunderstanding. We curate a collection of 2,378 image-question pairs with 1-5answers per question representing cultures from 11 countries across 5continents. The questions probe understanding of various facets of culture suchas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs onCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level ofcultural understanding across regions, with strong cultural understandingcapabilities for North America while significantly lower performance forAfrica. We observe disparity in their performance across cultural facets too,with clothing, rituals, and traditions seeing higher performances than food anddrink. These disparities help us identify areas where VLMs lack culturalunderstanding and demonstrate the potential of CulturalVQA as a comprehensiveevaluation set for gauging VLM progress in understanding diverse cultures.</description><author>Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal</author><pubDate>Thu, 18 Jul 2024 17:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10920v2</guid></item><item><title>Optimistic Q-learning for average reward and episodic reinforcement learning</title><link>http://arxiv.org/abs/2407.13743v1</link><description>We present an optimistic Q-learning algorithm for regret minimization inaverage reward reinforcement learning under an additional assumption on theunderlying MDP that for all policies, the expected time to visit some frequentstate $s_0$ is finite and upper bounded by $H$. Our setting strictlygeneralizes the episodic setting and is significantly less restrictive than theassumption of bounded hitting time {\it for all states} made by most previousliterature on model-free algorithms in average reward settings. We demonstratea regret bound of $\tilde{O}(H^5 S\sqrt{AT})$, where $S$ and $A$ are thenumbers of states and actions, and $T$ is the horizon. A key technical noveltyof our work is to introduce an $\overline{L}$ operator defined as $\overline{L}v = \frac{1}{H} \sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. Weshow that under the given assumption, the $\overline{L}$ operator has a strictcontraction (in span) even in the average reward setting. Our algorithm designthen uses ideas from episodic Q-learning to estimate and apply this operatoriteratively. Therefore, we provide a unified view of regret minimization inepisodic and non-episodic settings that may be of independent interest.</description><author>Priyank Agrawal, Shipra Agrawal</author><pubDate>Thu, 18 Jul 2024 17:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13743v1</guid></item><item><title>CellularLint: A Systematic Approach to Identify Inconsistent Behavior in Cellular Network Specifications</title><link>http://arxiv.org/abs/2407.13742v1</link><description>In recent years, there has been a growing focus on scrutinizing the securityof cellular networks, often attributing security vulnerabilities to issues inthe underlying protocol design descriptions. These protocol designspecifications, typically extensive documents that are thousands of pages long,can harbor inaccuracies, underspecifications, implicit assumptions, andinternal inconsistencies. In light of the evolving landscape, we introduceCellularLint--a semi-automatic framework for inconsistency detection within thestandards of 4G and 5G, capitalizing on a suite of natural language processingtechniques. Our proposed method uses a revamped few-shot learning mechanism ondomain-adapted large language models. Pre-trained on a vast corpus of cellularnetwork protocols, this method enables CellularLint to simultaneously detectinconsistencies at various levels of semantics and practical use cases. Indoing so, CellularLint significantly advances the automated analysis ofprotocol specifications in a scalable fashion. In our investigation, we focusedon the Non-Access Stratum (NAS) and the security specifications of 4G and 5Gnetworks, ultimately uncovering 157 inconsistencies with 82.67% accuracy. Afterverification of these inconsistencies on open-source implementations and 17commercial devices, we confirm that they indeed have a substantial impact ondesign decisions, potentially leading to concerns related to privacy,integrity, availability, and interoperability.</description><author>Mirza Masfiqur Rahman, Imtiaz Karim, Elisa Bertino</author><pubDate>Thu, 18 Jul 2024 17:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13742v1</guid></item><item><title>Scaling Granite Code Models to 128K Context</title><link>http://arxiv.org/abs/2407.13739v1</link><description>This paper introduces long-context Granite code models that support effectivecontext windows of up to 128K tokens. Our solution for scaling context lengthof Granite 3B/8B code models from 2K/4K to 128K consists of a light-weightcontinual pretraining by gradually increasing its RoPE base frequency withrepository-level file packing and length-upsampled long-context data.Additionally, we also release instruction-tuned models with long-contextsupport which are derived by further finetuning the long context base models ona mix of permissively licensed short and long-context instruction-responsepairs. While comparing to the original short-context Granite code models, ourlong-context models achieve significant improvements on long-context taskswithout any noticeable performance degradation on regular code completionbenchmarks (e.g., HumanEval). We release all our long-context Granite codemodels under an Apache 2.0 license for both research and commercial use.</description><author>Matt Stallone, Vaibhav Saxena, Leonid Karlinsky, Bridget McGinn, Tim Bula, Mayank Mishra, Adriana Meza Soria, Gaoyuan Zhang, Aditya Prasad, Yikang Shen, Saptha Surendran, Shanmukha Guttula, Hima Patel, Parameswaran Selvam, Xuan-Hong Dang, Yan Koyfman, Atin Sood, Rogerio Feris, Nirmit Desai, David D. Cox, Ruchir Puri, Rameswar Panda</author><pubDate>Thu, 18 Jul 2024 17:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13739v1</guid></item><item><title>A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation</title><link>http://arxiv.org/abs/2406.07320v2</link><description>Model performance evaluation is a critical and expensive task in machinelearning and computer vision. Without clear guidelines, practitioners oftenestimate model accuracy using a one-time completely random selection of thedata. However, by employing tailored sampling and estimation strategies, onecan obtain more precise estimates and reduce annotation costs. In this paper,we propose a statistical framework for model evaluation that includesstratification, sampling, and estimation components. We examine the statisticalproperties of each component and evaluate their efficiency (precision). One keyresult of our work is that stratification via k-means clustering based onaccurate predictions of model performance yields efficient estimators. Ourexperiments on computer vision datasets show that this method consistentlyprovides more precise accuracy estimates than the traditional simple randomsampling, even with substantial efficiency gains of 10x. We also find thatmodel-assisted estimators, which leverage predictions of model accuracy on theunlabeled portion of the dataset, are generally more efficient than thetraditional estimates based solely on the labeled data.</description><author>Riccardo Fogliato, Pratik Patil, Mathew Monfort, Pietro Perona</author><pubDate>Thu, 18 Jul 2024 17:43:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07320v2</guid></item><item><title>Dynamic Pricing in Securities Lending Market: Application in Revenue Optimization for an Agent Lender Portfolio</title><link>http://arxiv.org/abs/2407.13687v1</link><description>Securities lending is an important part of the financial market structure,where agent lenders help long term institutional investors to lend out theirsecurities to short sellers in exchange for a lending fee. Agent lenders withinthe market seek to optimize revenue by lending out securities at the highestrate possible. Typically, this rate is set by hard-coded business rules orstandard supervised machine learning models. These approaches are oftendifficult to scale and are not adaptive to changing market conditions. Unlike atraditional stock exchange with a centralized limit order book, the securitieslending market is organized similarly to an e-commerce marketplace, where agentlenders and borrowers can transact at any agreed price in a bilateral fashion.This similarity suggests that the use of typical methods for addressing dynamicpricing problems in e-commerce could be effective in the securities lendingmarket. We show that existing contextual bandit frameworks can be successfullyutilized in the securities lending market. Using offline evaluation on realhistorical data, we show that the contextual bandit approach can consistentlyoutperform typical approaches by at least 15% in terms of total revenuegenerated.</description><author>Jing Xu, Yung Cheng Hsu, William Biscarri</author><pubDate>Thu, 18 Jul 2024 17:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13687v1</guid></item><item><title>FBChain: A Blockchain-based Federated Learning Model with Efficiency and Secure Communication</title><link>http://arxiv.org/abs/2312.00035v2</link><description>Privacy and security in the parameter transmission process of federatedlearning are currently among the most prominent concerns. However, there aretwo thorny problems caused by unprotected communication methods:"parameter-leakage" and "inefficient-communication". This article proposesBlockchain-based Federated Learning (FBChain) model for federated learningparameter communication to overcome the above two problems. First, we utilizethe immutability of blockchain to store the global model and hash value oflocal model parameters in case of tampering during the communication process,protect data privacy by encrypting parameters, and verify data consistency bycomparing the hash values of local parameters, thus addressing the"parameter-leakage" problem. Second, the Proof of Weighted Link Speed (PoWLS)consensus algorithm comprehensively selects nodes with the higher weighted linkspeed to aggregate global model and package blocks, thereby solving the"inefficient-communication" problem. Experimental results demonstrate theeffectiveness of our proposed FBChain model and its ability to improve modelcommunication efficiency in federated learning.</description><author>Yang Li, Chunhe Xia, Wei Liu, Chen Chen, Tianbo Wang</author><pubDate>Thu, 18 Jul 2024 17:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00035v2</guid></item><item><title>Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</title><link>http://arxiv.org/abs/2402.09327v2</link><description>In this work, we investigate the interplay between memorization and learningin the context of \emph{stochastic convex optimization} (SCO). We definememorization via the information a learning algorithm reveals about itstraining data points. We then quantify this information using the framework ofconditional mutual information (CMI) proposed by Steinke and Zakynthinou(2020). Our main result is a precise characterization of the tradeoff betweenthe accuracy of a learning algorithm and its CMI, answering an open questionposed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded settingand under strong convexity, every learner with an excess error $\varepsilon$has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,respectively. We further demonstrate the essential role of memorization inlearning problems in SCO by designing an adversary capable of accuratelyidentifying a significant fraction of the training samples in specific SCOproblems. Finally, we enumerate several implications of our results, such as alimitation of generalization bounds based on CMI and the incompressibility ofsamples in SCO problems.</description><author>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy</author><pubDate>Thu, 18 Jul 2024 17:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09327v2</guid></item><item><title>Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review</title><link>http://arxiv.org/abs/2407.13734v1</link><description>This tutorial provides a comprehensive survey of methods for fine-tuningdiffusion models to optimize downstream reward functions. While diffusionmodels are widely known to provide excellent generative modeling capability,practical applications in domains such as biology require generating samplesthat maximize some desired metric (e.g., translation efficiency in RNA, dockingscore in molecules, stability in protein). In these cases, the diffusion modelcan be optimized not only to generate realistic samples but also to explicitlymaximize the measure of interest. Such methods are based on concepts fromreinforcement learning (RL). We explain the application of various RLalgorithms, including PPO, differentiable optimization, reward-weighted MLE,value-weighted sampling, and path consistency learning, tailored specificallyfor fine-tuning diffusion models. We aim to explore fundamental aspects such asthe strengths and limitations of different RL-based fine-tuning algorithmsacross various scenarios, the benefits of RL-based fine-tuning compared tonon-RL-based approaches, and the formal objectives of RL-based fine-tuning(target distributions). Additionally, we aim to examine their connections withrelated topics such as classifier guidance, Gflownets, flow-based diffusionmodels, path integral control theory, and sampling from unnormalizeddistributions such as MCMC. The code of this tutorial is available athttps://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq</description><author>Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, Sergey Levine</author><pubDate>Thu, 18 Jul 2024 17:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13734v1</guid></item><item><title>Realizable $H$-Consistent and Bayes-Consistent Loss Functions for Learning to Defer</title><link>http://arxiv.org/abs/2407.13732v1</link><description>We present a comprehensive study of surrogate loss functions for learning todefer. We introduce a broad family of surrogate losses, parameterized by anon-increasing function $\Psi$, and establish their realizable $H$-consistencyunder mild conditions. For cost functions based on classification error, wefurther show that these losses admit $H$-consistency bounds when the hypothesisset is symmetric and complete, a property satisfied by common neural networkand linear function hypothesis sets. Our results also resolve an open questionraised in previous work (Mozannar et al., 2023) by proving the realizable$H$-consistency and Bayes-consistency of a specific surrogate loss.Furthermore, we identify choices of $\Psi$ that lead to $H$-consistentsurrogate losses for any general cost function, thus achievingBayes-consistency, realizable $H$-consistency, and $H$-consistency boundssimultaneously. We also investigate the relationship between $H$-consistencybounds and realizable $H$-consistency in learning to defer, highlighting keydifferences from standard classification. Finally, we empirically evaluate ourproposed surrogate losses and compare them with existing baselines.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Thu, 18 Jul 2024 17:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13732v1</guid></item><item><title>Predictive Low Rank Matrix Learning under Partial Observations: Mixed-Projection ADMM</title><link>http://arxiv.org/abs/2407.13731v1</link><description>We study the problem of learning a partially observed matrix under the lowrank assumption in the presence of fully observed side information that dependslinearly on the true underlying matrix. This problem consists of an importantgeneralization of the Matrix Completion problem, a central problem inStatistics, Operations Research and Machine Learning, that arises inapplications such as recommendation systems, signal processing, systemidentification and image denoising. We formalize this problem as anoptimization problem with an objective that balances the strength of the fit ofthe reconstruction to the observed entries with the ability of thereconstruction to be predictive of the side information. We derive amixed-projection reformulation of the resulting optimization problem andpresent a strong semidefinite cone relaxation. We design an efficient, scalablealternating direction method of multipliers algorithm that produces highquality feasible solutions to the problem of interest. Our numerical resultsdemonstrate that in the small rank regime ($k \leq 15$), our algorithm outputssolutions that achieve on average $79\%$ lower objective value and $90.1\%$lower $\ell_2$ reconstruction error than the solutions returned by theexperiment-wise best performing benchmark method. The runtime of our algorithmis competitive with and often superior to that of the benchmark methods. Ouralgorithm is able to solve problems with $n = 10000$ rows and $m = 10000$columns in less than a minute.</description><author>Dimitris Bertsimas, Nicholas A. G. Johnson</author><pubDate>Thu, 18 Jul 2024 17:33:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13731v1</guid></item><item><title>Lessons from a human-in-the-loop machine learning approach for identifying vacant, abandoned, and deteriorated properties in Savannah, Georgia</title><link>http://arxiv.org/abs/2407.11138v2</link><description>Addressing strategies for managing vacant, abandoned, and deteriorated (VAD)properties is important for maintaining healthy communities. Yet, the processof identifying these properties can be difficult. Here, we create ahuman-in-the-loop machine learning (HITLML) model called VADecide and apply itto a parcel-level case study in Savannah, Georgia. The results show a higherprediction accuracy than was achieved when using a machine learning modelwithout human input in the training. The HITLML approach also revealsdifferences between machine vs. human-generated results. Our findingscontribute to knowledge about the advantages and challenges of HITLML in urbanplanning. [Accepted for Publication at a Peer Review Journal]</description><author>Xiaofan Liang, Brian Brainerd, Tara Hicks, Clio Andris</author><pubDate>Thu, 18 Jul 2024 17:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11138v2</guid></item><item><title>An Intrinsic Vector Heat Network</title><link>http://arxiv.org/abs/2406.09648v2</link><description>Vector fields are widely used to represent and model flows for many scienceand engineering applications. This paper introduces a novel neural networkarchitecture for learning tangent vector fields that are intrinsically definedon manifold surfaces embedded in 3D. Previous approaches to learning vectorfields on surfaces treat vectors as multi-dimensional scalar fields, usingtraditional scalar-valued architectures to process channels individually, thusfail to preserve fundamental intrinsic properties of the vector field. The coreidea of this work is to introduce a trainable vector heat diffusion module tospatially propagate vector-valued feature data across the surface, which weincorporate into our proposed architecture that consists of vector-valuedneurons. Our architecture is invariant to rigid motion of the input, isometricdeformation, and choice of local tangent bases, and is robust todiscretizations of the surface. We evaluate our Vector Heat Network on trianglemeshes, and empirically validate its invariant properties. We also demonstratethe effectiveness of our method on the useful industrial application ofquadrilateral mesh generation.</description><author>Alexander Gao, Maurice Chu, Mubbasir Kapadia, Ming C. Lin, Hsueh-Ti Derek Liu</author><pubDate>Thu, 18 Jul 2024 17:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09648v2</guid></item><item><title>Baba Is AI: Break the Rules to Beat the Benchmark</title><link>http://arxiv.org/abs/2407.13729v1</link><description>Humans solve problems by following existing rules and procedures, and also byleaps of creativity to redefine those rules and objectives. To probe theseabilities, we developed a new benchmark based on the game Baba Is You where anagent manipulates both objects in the environment and rules, represented bymovable tiles with words written on them, to reach a specified goal and win thegame. We test three state-of-the-art multi-modal large language models (OpenAIGPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they faildramatically when generalization requires that the rules of the game must bemanipulated and combined.</description><author>Nathan Cloos, Meagan Jens, Michelangelo Naim, Yen-Ling Kuo, Ignacio Cases, Andrei Barbu, Christopher J. Cueva</author><pubDate>Thu, 18 Jul 2024 17:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13729v1</guid></item><item><title>Compressing Structured Tensor Algebra</title><link>http://arxiv.org/abs/2407.13726v1</link><description>Tensor algebra is a crucial component for data-intensive workloads such asmachine learning and scientific computing. As the complexity of data grows,scientists often encounter a dilemma between the highly specialized densetensor algebra and efficient structure-aware algorithms provided by sparsetensor algebra. In this paper, we introduce DASTAC, a framework to propagatethe tensors's captured high-level structure down to low-level code generationby incorporating techniques such as automatic data layout compression,polyhedral analysis, and affine code generation. Our methodology reduces memoryfootprint by automatically detecting the best data layout, heavily benefitsfrom polyhedral optimizations, leverages further optimizations, and enablesparallelization through MLIR. Through extensive experimentation, we show thatDASTAC achieves 1 to 2 orders of magnitude speedup over TACO, astate-of-the-art sparse tensor compiler, and StructTensor, a state-of-the-artstructured tensor algebra compiler, with a significantly lower memoryfootprint.</description><author>Mahdi Ghorbani, Emilien Bauer, Tobias Grosser, Amir Shaikhha</author><pubDate>Thu, 18 Jul 2024 17:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13726v1</guid></item><item><title>On Learning to Summarize with Large Language Models as References</title><link>http://arxiv.org/abs/2305.14239v3</link><description>Recent studies have found that summaries generated by large language models(LLMs) are favored by human annotators over the original reference summaries incommonly used summarization datasets. Therefore, we study an LLM-as-referencelearning setting for smaller text summarization models to investigate whethertheir performance can be substantially improved. To this end, we use LLMs asboth oracle summary generators for standard supervised fine-tuning and oraclesummary evaluators for efficient contrastive learning that leverages the LLMs'supervision signals. We conduct comprehensive experiments with source newsarticles and find that (1) summarization models trained under theLLM-as-reference setting achieve significant performance improvement in bothLLM and human evaluations; (2) contrastive learning outperforms standardsupervised fine-tuning under both low and high resource settings. Ourexperimental results also enable a meta-analysis of LLMs' summary evaluationcapacities under a challenging setting, showing that LLMs are not well-alignedwith human evaluators. Particularly, our expert human evaluation revealsremaining nuanced performance gaps between LLMs and our fine-tuned models,which LLMs fail to capture. Thus, we call for further studies into both thepotential and challenges of using LLMs in summarization model development.</description><author>Yixin Liu, Kejian Shi, Katherine S He, Longtian Ye, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman Cohan</author><pubDate>Thu, 18 Jul 2024 17:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14239v3</guid></item><item><title>Enhanced $H$-Consistency Bounds</title><link>http://arxiv.org/abs/2407.13722v1</link><description>Recent research has introduced a key notion of $H$-consistency bounds forsurrogate losses. These bounds offer finite-sample guarantees, quantifying therelationship between the zero-one estimation error (or other target loss) andthe surrogate loss estimation error for a specific hypothesis set. However,previous bounds were derived under the condition that a lower bound of thesurrogate loss conditional regret is given as a convex function of the targetconditional regret, without non-constant factors depending on the predictor orinput instance. Can we derive finer and more favorable $H$-consistency bounds?In this work, we relax this condition and present a general framework forestablishing enhanced $H$-consistency bounds based on more general inequalitiesrelating conditional regrets. Our theorems not only subsume existing results asspecial cases but also enable the derivation of more favorable bounds invarious scenarios. These include standard multi-class classification, binaryand multi-class classification under Tsybakov noise conditions, and bipartiteranking.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Thu, 18 Jul 2024 17:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13722v1</guid></item><item><title>HazeCLIP: Towards Language Guided Real-World Image Dehazing</title><link>http://arxiv.org/abs/2407.13719v1</link><description>Existing methods have achieved remarkable performance in single imagedehazing, particularly on synthetic datasets. However, they often struggle withreal-world hazy images due to domain shift, limiting their practicalapplicability. This paper introduces HazeCLIP, a language-guided adaptationframework designed to enhance the real-world performance of pre-traineddehazing networks. Inspired by the Contrastive Language-Image Pre-training(CLIP) model's ability to distinguish between hazy and clean images, we utilizeit to evaluate dehazing results. Combined with a region-specific dehazingtechnique and tailored prompt sets, CLIP model accurately identifies hazyareas, providing a high-quality, human-like prior that guides the fine-tuningprocess of pre-trained networks. Extensive experiments demonstrate thatHazeCLIP achieves the state-of-the-art performance in real-word image dehazing,evaluated through both visual quality and no-reference quality assessments. Thecode is available: https://github.com/Troivyn/HazeCLIP .</description><author>Ruiyi Wang, Wenhao Li, Xiaohong Liu, Chunyi Li, Zicheng Zhang, Xiongkuo Min, Guangtao Zhai</author><pubDate>Thu, 18 Jul 2024 17:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13719v1</guid></item><item><title>CoDefeater: Using LLMs To Find Defeaters in Assurance Cases</title><link>http://arxiv.org/abs/2407.13717v1</link><description>Constructing assurance cases is a widely used, and sometimes required,process toward demonstrating that safety-critical systems will operate safelyin their planned environment. To mitigate the risk of errors and missing edgecases, the concept of defeaters - arguments or evidence that challenge claimsin an assurance case - has been introduced. Defeaters can provide timelydetection of weaknesses in the arguments, prompting further investigation andtimely mitigations. However, capturing defeaters relies on expert judgment,experience, and creativity and must be done iteratively due to evolvingrequirements and regulations. This paper proposes CoDefeater, an automatedprocess to leverage large language models (LLMs) for finding defeaters. Initialresults on two systems show that LLMs can efficiently find known and unforeseenfeasible defeaters to support safety analysts in enhancing the completeness andconfidence of assurance cases.</description><author>Usman Gohar, Michael C. Hunter, Robyn R. Lutz, Myra B. Cohen</author><pubDate>Thu, 18 Jul 2024 17:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13717v1</guid></item><item><title>Attention Based Simple Primitives for Open World Compositional Zero-Shot Learning</title><link>http://arxiv.org/abs/2407.13715v1</link><description>Compositional Zero-Shot Learning (CZSL) aims to predict unknown compositionsmade up of attribute and object pairs. Predicting compositions unseen duringtraining is a challenging task. We are exploring Open World CompositionalZero-Shot Learning (OW-CZSL) in this study, where our test space encompassesall potential combinations of attributes and objects. Our approach involvesutilizing the self-attention mechanism between attributes and objects toachieve better generalization from seen to unseen compositions. Utilizing aself-attention mechanism facilitates the model's ability to identifyrelationships between attribute and objects. The similarity between theself-attended textual and visual features is subsequently calculated togenerate predictions during the inference phase. The potential test space mayencompass implausible object-attribute combinations arising from unrestrictedattribute-object pairings. To mitigate this issue, we leverage externalknowledge from ConceptNet to restrict the test space to realistic compositions.Our proposed model, Attention-based Simple Primitives (ASP), demonstratescompetitive performance, achieving results comparable to the state-of-the-art.</description><author>Ans Munir, Faisal Z. Qureshi, Muhammad Haris Khan, Mohsen Ali</author><pubDate>Thu, 18 Jul 2024 17:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13715v1</guid></item><item><title>FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning</title><link>http://arxiv.org/abs/2407.13711v1</link><description>Laplace approximations are popular techniques for endowing deep networks withepistemic uncertainty estimates as they can be applied without altering thepredictions of the neural network, and they scale to large models and datasets.While the choice of prior strongly affects the resulting posteriordistribution, computational tractability and lack of interpretability of weightspace typically limit the Laplace approximation to isotropic Gaussian priors,which are known to cause pathological behavior as depth increases. As a remedy,we directly place a prior on function space. More precisely, since Lebesguedensities do not exist on infinite-dimensional function spaces, we have torecast training as finding the so-called weak mode of the posterior measureunder a Gaussian process (GP) prior restricted to the space of functionsrepresentable by the neural network. Through the GP prior, one can expressstructured and interpretable inductive biases, such as regularity orperiodicity, directly in function space, while still exploiting the implicitinductive biases that allow deep networks to generalize. After modellinearization, the training objective induces a negative log-posterior densityto which we apply a Laplace approximation, leveraging highly scalable methodsfrom matrix-free linear algebra. Our method provides improved results whereprior knowledge is abundant, e.g., in many scientific inference tasks. At thesame time, it stays competitive for black-box regression and classificationtasks where neural networks typically excel.</description><author>Tristan Cinquin, Marvin Pförtner, Vincent Fortuin, Philipp Hennig, Robert Bamler</author><pubDate>Thu, 18 Jul 2024 17:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13711v1</guid></item><item><title>Understanding Reference Policies in Direct Preference Optimization</title><link>http://arxiv.org/abs/2407.13709v1</link><description>Direct Preference Optimization (DPO) has become a widely used training methodfor the instruction fine-tuning of large language models (LLMs). In this work,we explore an under-investigated aspect of DPO - its dependency on thereference model or policy. Such reference policies, typically instantiated asthe model to be further fine-tuned, are important since they can impose anupper limit on DPO's effectiveness. Therefore, we address three relatedresearch questions in this work. First, we explore the optimal strength of theKL-divergence constraint in DPO, which penalizes deviations from the referencepolicy, and find that DPO is sensitive to this strength. Next, we examine thenecessity of reference policies for instruction fine-tuning by providing boththeoretical and empirical comparisons between DPO and related learningobjectives, demonstrating DPO's superiority. Additionally, we investigatewhether DPO benefits from stronger reference policies, finding that a strongerreference policy can lead to improved performance, but only when it is similarto the model being fine-tuned. Our findings highlight the confounding role ofreference policies in DPO and offer insights for best practices, while alsoidentifying open research questions for future studies.</description><author>Yixin Liu, Pengfei Liu, Arman Cohan</author><pubDate>Thu, 18 Jul 2024 17:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13709v1</guid></item><item><title>Are We Ready for Out-of-Distribution Detection in Digital Pathology?</title><link>http://arxiv.org/abs/2407.13708v1</link><description>The detection of semantic and covariate out-of-distribution (OOD) examples isa critical yet overlooked challenge in digital pathology (DP). Recently,substantial insight and methods on OOD detection were presented by the MLcommunity, but how do they fare in DP applications? To this end, we establish abenchmark study, our highlights being: 1) the adoption of proper evaluationprotocols, 2) the comparison of diverse detectors in both a single andmulti-model setting, and 3) the exploration into advanced ML settings liketransfer learning (ImageNet vs. DP pre-training) and choice of architecture(CNNs vs. transformers). Through our comprehensive experiments, we contributenew insights and guidelines, paving the way for future research and discussion.</description><author>Ji-Hun Oh, Kianoush Falahkheirkhah, Rohit Bhargava</author><pubDate>Thu, 18 Jul 2024 17:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13708v1</guid></item><item><title>Localizing Anomalies via Multiscale Score Matching Analysis</title><link>http://arxiv.org/abs/2407.00148v2</link><description>Anomaly detection and localization in medical imaging remain criticalchallenges in healthcare. This paper introduces Spatial-MSMA (Multiscale ScoreMatching Analysis), a novel unsupervised method for anomaly localization involumetric brain MRIs. Building upon the MSMA framework, our approachincorporates spatial information and conditional likelihoods to enhance anomalydetection capabilities. We employ a flexible normalizing flow model conditionedon patch positions and global image features to estimate patch-wise anomalyscores. The method is evaluated on a dataset of 1,650 T1- and T2-weighted brainMRIs from typically developing children, with simulated lesions added to thetest set. Spatial-MSMA significantly outperforms existing methods, includingreconstruction-based, generative-based, and interpretation-based approaches, inlesion detection and segmentation tasks. Our model achieves superiorperformance in both distance-based metrics (99th percentile Hausdorff Distance:$7.05 \pm 0.61$, Mean Surface Distance: $2.10 \pm 0.43$) and component-wisemetrics (True Positive Rate: $0.83 \pm 0.01$, Positive Predictive Value: $0.96\pm 0.01$). These results demonstrate Spatial-MSMA's potential for accurate andinterpretable anomaly localization in medical imaging, with implications forimproved diagnosis and treatment planning in clinical settings. Our code isavailable at~\url{https://github.com/ahsanMah/sade/}.</description><author>Ahsan Mahmood, Junier Oliva, Martin Styner</author><pubDate>Thu, 18 Jul 2024 17:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00148v2</guid></item><item><title>Discovering governing equation in structural dynamics from acceleration-only measurements</title><link>http://arxiv.org/abs/2407.13704v1</link><description>Over the past few years, equation discovery has gained popularity indifferent fields of science and engineering. However, existing equationdiscovery algorithms rely on the availability of noisy measurements of thestate variables (i.e., displacement {and velocity}). This is a major bottleneckin structural dynamics, where we often only have access to accelerationmeasurements. To that end, this paper introduces a novel equation discoveryalgorithm for discovering governing equations of dynamical systems fromacceleration-only measurements. The proposed algorithm employs a library-basedapproach for equation discovery. To enable equation discovery fromacceleration-only measurements, we propose a novel Approximate BayesianComputation (ABC) model that prioritizes parsimonious models. The efficacy ofthe proposed algorithm is illustrated using {four} structural dynamics examplesthat include both linear and nonlinear dynamical systems. The case studiespresented illustrate the possible application of the proposed approach forequation discovery of dynamical systems from acceleration-only measurements.</description><author>Calvin Alvares, Souvik Chakraborty</author><pubDate>Thu, 18 Jul 2024 17:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13704v1</guid></item><item><title>ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free Hallucination Detection</title><link>http://arxiv.org/abs/2407.13702v1</link><description>Research on token-level reference-free hallucination detection haspredominantly focused on English, primarily due to the scarcity of robustdatasets in other languages. This has hindered systematic investigations intothe effectiveness of cross-lingual transfer for this important NLP application.To address this gap, we introduce ANHALTEN, a new evaluation dataset thatextends the English hallucination detection dataset to German. To the best ofour knowledge, this is the first work that explores cross-lingual transfer fortoken-level reference-free hallucination detection. ANHALTEN contains goldannotations in German that are parallel (i.e., directly comparable to theoriginal English instances). We benchmark several prominent cross-lingualtransfer approaches, demonstrating that larger context length leads to betterhallucination detection in German, even without succeeding context.Importantly, we show that the sample-efficient few-shot transfer is the mosteffective approach in most setups. This highlights the practical benefits ofminimal annotation effort in the target language for reference-freehallucination detection. Aiming to catalyze future research on cross-lingualtoken-level reference-free hallucination detection, we make ANHALTEN publiclyavailable: https://github.com/janekh24/anhalten</description><author>Janek Herrlein, Chia-Chien Hung, Goran Glavaš</author><pubDate>Thu, 18 Jul 2024 17:01:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13702v1</guid></item><item><title>Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift</title><link>http://arxiv.org/abs/2407.13700v1</link><description>Studying adversarial attacks on artificial intelligence (AI) systems helpsdiscover model shortcomings, enabling the construction of a more robust system.Most existing adversarial attack methods only concentrate on single-tasksingle-model or single-task cross-model scenarios, overlooking the multi-taskcharacteristic of artificial intelligence systems. As a result, most of theexisting attacks do not pose a practical threat to a comprehensive andcollaborative AI system. However, implementing cross-task attacks is highlydemanding and challenging due to the difficulty in obtaining the real labels ofdifferent tasks for the same picture and harmonizing the loss functions acrossdifferent tasks. To address this issue, we propose a self-supervised Cross-TaskAttack framework (CTA), which utilizes co-attention and anti-attention maps togenerate cross-task adversarial perturbation. Specifically, the co-attentionmap reflects the area to which different visual task models pay attention,while the anti-attention map reflects the area that different visual taskmodels neglect. CTA generates cross-task perturbations by shifting theattention area of samples away from the co-attention map and closer to theanti-attention map. We conduct extensive experiments on multiple vision tasksand the experimental results confirm the effectiveness of the proposed designfor adversarial attacks.</description><author>Qingyuan Zeng, Yunpeng Gong, Min Jiang</author><pubDate>Thu, 18 Jul 2024 17:01:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13700v1</guid></item><item><title>A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice</title><link>http://arxiv.org/abs/2407.13699v1</link><description>Recommender Systems (RS) play an integral role in enhancing user experiencesby providing personalized item suggestions. This survey reviews the progress inRS inclusively from 2017 to 2024, effectively connecting theoretical advanceswith practical applications. We explore the development from traditional RStechniques like content-based and collaborative filtering to advanced methodsinvolving deep learning, graph-based models, reinforcement learning, and largelanguage models. We also discuss specialized systems such as context-aware,review-based, and fairness-aware RS. The primary goal of this survey is tobridge theory with practice. It addresses challenges across various sectors,including e-commerce, healthcare, and finance, emphasizing the need forscalable, real-time, and trustworthy solutions. Through this survey, we promotestronger partnerships between academic research and industry practices. Theinsights offered by this survey aim to guide industry professionals inoptimizing RS deployment and to inspire future research directions, especiallyin addressing emerging technological and societal trends</description><author>Shaina Raza, Mizanur Rahman, Safiullah Kamawal, Armin Toroghi, Ananya Raval, Farshad Navah, Amirmohammad Kazemeini</author><pubDate>Thu, 18 Jul 2024 17:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13699v1</guid></item><item><title>Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark Evaluation</title><link>http://arxiv.org/abs/2407.13696v1</link><description>Recent advancements in Language Models (LMs) have catalyzed the creation ofmultiple benchmarks, designed to assess these models' general capabilities. Acrucial task, however, is assessing the validity of the benchmarks themselves.This is most commonly done via Benchmark Agreement Testing (BAT), where newbenchmarks are validated against established ones using some agreement metric(e.g., rank correlation). Despite the crucial role of BAT for benchmarkbuilders and consumers, there are no standardized procedures for such agreementtesting. This deficiency can lead to invalid conclusions, fostering mistrust inbenchmarks and upending the ability to properly choose the appropriatebenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate howsome overlooked methodological choices can significantly influence BAT results,potentially undermining the validity of conclusions. To address theseinconsistencies, we propose a set of best practices for BAT and demonstrate howutilizing these methodologies greatly improves BAT robustness and validity. Tofoster adoption and facilitate future research,, we introduce BenchBench, apython package for BAT, and release the BenchBench-leaderboard, ameta-benchmark designed to evaluate benchmarks using their peers. Our findingsunderscore the necessity for standardized BAT, ensuring the robustness andvalidity of benchmark evaluations in the evolving landscape of language modelresearch. BenchBench Package: https://github.com/IBM/BenchBench Leaderboard: https://huggingface.co/spaces/per/BenchBench</description><author>Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel, Eyal Shnarch, Michal Shmueli-Scheuer, Leshem Choshen</author><pubDate>Thu, 18 Jul 2024 17:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13696v1</guid></item><item><title>Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following</title><link>http://arxiv.org/abs/2406.02774v2</link><description>Training gaze following models requires a large number of images with gazetarget coordinates annotated by human annotators, which is a laborious andinherently ambiguous process. We propose the first semi-supervised method forgaze following by introducing two novel priors to the task. We obtain the firstprior using a large pretrained Visual Question Answering (VQA) model, where wecompute Grad-CAM heatmaps by `prompting' the VQA model with a gaze followingquestion. These heatmaps can be noisy and not suited for use in training. Theneed to refine these noisy annotations leads us to incorporate a second prior.We utilize a diffusion model trained on limited human annotations and modifythe reverse sampling process to refine the Grad-CAM heatmaps. By tuning thediffusion process we achieve a trade-off between the human annotation prior andthe VQA heatmap prior, which retains the useful VQA prior information whileexhibiting similar properties to the training data distribution. Our methodoutperforms simple pseudo-annotation generation baselines on the GazeFollowimage dataset. More importantly, our pseudo-annotation strategy, applied to awidely used supervised gaze following model (VAT), reduces the annotation needby 50%. Our method also performs the best on the VideoAttentionTarget dataset.</description><author>Qiaomu Miao, Alexandros Graikos, Jingwei Zhang, Sounak Mondal, Minh Hoai, Dimitris Samaras</author><pubDate>Thu, 18 Jul 2024 16:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02774v2</guid></item><item><title>Prover-Verifier Games improve legibility of LLM outputs</title><link>http://arxiv.org/abs/2407.13692v1</link><description>One way to increase confidence in the outputs of Large Language Models (LLMs)is to support them with reasoning that is clear and easy to check -- a propertywe call legibility. We study legibility in the context of solving grade-schoolmath problems and show that optimizing chain-of-thought solutions only foranswer correctness can make them less legible. To mitigate the loss inlegibility, we propose a training algorithm inspired by Prover-Verifier Gamefrom Anil et al. (2021). Our algorithm iteratively trains small verifiers topredict solution correctness, "helpful" provers to produce correct solutionsthat the verifier accepts, and "sneaky" provers to produce incorrect solutionsthat fool the verifier. We find that the helpful prover's accuracy and theverifier's robustness to adversarial attacks increase over the course oftraining. Furthermore, we show that legibility training transfers totime-constrained humans tasked with verifying solution correctness. Over courseof LLM training human accuracy increases when checking the helpful prover'ssolutions, and decreases when checking the sneaky prover's solutions. Hence,training for checkability by small verifiers is a plausible technique forincreasing output legibility. Our results suggest legibility training againstsmall verifiers as a practical avenue for increasing legibility of large LLMsto humans, and thus could help with alignment of superhuman models.</description><author>Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, Yuri Burda</author><pubDate>Thu, 18 Jul 2024 16:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13692v1</guid></item><item><title>Shaded Route Planning Using Active Segmentation and Identification of Satellite Images</title><link>http://arxiv.org/abs/2407.13689v1</link><description>Heatwaves pose significant health risks, particularly due to prolongedexposure to high summer temperatures. Vulnerable groups, especially pedestriansand cyclists on sun-exposed sidewalks, motivate the development of a routeplanning method that incorporates somatosensory temperature effects throughshade ratio consideration. This paper is the first to introduce a pipeline thatutilizes segmentation foundation models to extract shaded areas fromhigh-resolution satellite images. These areas are then integrated into amulti-layered road map, enabling users to customize routes based on a balancebetween distance and shade exposure, thereby enhancing comfort and healthduring outdoor activities. Specifically, we construct a graph-basedrepresentation of the road map, where links indicate connectivity and areupdated with shade ratio data for dynamic route planning. This system isalready implemented online, with a video demonstration, and will bespecifically adapted to assist travelers during the 2024 Olympic Games inParis.</description><author>Longchao Da, Rohan Chhibba, Rushabh Jaiswal, Ariane Middel, Hua Wei</author><pubDate>Thu, 18 Jul 2024 16:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13689v1</guid></item><item><title>HPix: Generating Vector Maps from Satellite Images</title><link>http://arxiv.org/abs/2407.13680v1</link><description>Vector maps find widespread utility across diverse domains due to theircapacity to not only store but also represent discrete data boundaries such asbuilding footprints, disaster impact analysis, digitization, urban planning,location points, transport links, and more. Although extensive research existson identifying building footprints and road types from satellite imagery, thegeneration of vector maps from such imagery remains an area with limitedexploration. Furthermore, conventional map generation techniques rely onlabor-intensive manual feature extraction or rule-based approaches, whichimpose inherent limitations. To surmount these limitations, we propose a novelmethod called HPix, which utilizes modified Generative Adversarial Networks(GANs) to generate vector tile map from satellite images. HPix incorporates twohierarchical frameworks: one operating at the global level and the other at thelocal level, resulting in a comprehensive model. Through empirical evaluations,our proposed approach showcases its effectiveness in producing highly accurateand visually captivating vector tile maps derived from satellite images. Wefurther extend our study's application to include mapping of road intersectionsand building footprints cluster based on their area.</description><author>Aditya Taparia, Keshab Nath</author><pubDate>Thu, 18 Jul 2024 16:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13680v1</guid></item><item><title>PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers</title><link>http://arxiv.org/abs/2407.13677v1</link><description>The increased demand for tools that automate the 3D content creation processled to tremendous progress in deep generative models that can generate diverse3D objects of high fidelity. In this paper, we present PASTA, an autoregressivetransformer architecture for generating high quality 3D shapes. PASTA comprisestwo main components: An autoregressive transformer that generates objects as asequence of cuboidal primitives and a blending network, implemented with atransformer decoder that composes the sequences of cuboids and synthesizes highquality meshes for each object. Our model is trained in two stages: First wetrain our autoregressive generative model using only annotated cuboidal partsas supervision and next, we train our blending network using explicit 3Dsupervision, in the form of watertight meshes. Evaluations on various ShapeNetobjects showcase the ability of our model to perform shape generation fromdiverse inputs \eg from scratch, from a partial object, from text and images,as well size-guided generation, by explicitly conditioning on a bounding boxthat defines the object's boundaries. Moreover, as our model considers theunderlying part-based structure of a 3D object, we are able to select aspecific part and produce shapes with meaningful variations of this part. Asevidenced by our experiments, our model generates 3D shapes that are both morerealistic and diverse than existing part-based and non part-based methods,while at the same time is simpler to implement and train.</description><author>Songlin Li, Despoina Paschalidou, Leonidas Guibas</author><pubDate>Thu, 18 Jul 2024 16:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13677v1</guid></item><item><title>Aligning Sight and Sound: Advanced Sound Source Localization Through Audio-Visual Alignment</title><link>http://arxiv.org/abs/2407.13676v1</link><description>Recent studies on learning-based sound source localization have mainlyfocused on the localization performance perspective. However, prior work andexisting benchmarks overlook a crucial aspect: cross-modal interaction, whichis essential for interactive sound source localization. Cross-modal interactionis vital for understanding semantically matched or mismatched audio-visualevents, such as silent objects or off-screen sounds. In this paper, we firstcomprehensively examine the cross-modal interaction of existing methods,benchmarks, evaluation metrics, and cross-modal understanding tasks. Then, weidentify the limitations of previous studies and make several contributions toovercome the limitations. First, we introduce a new synthetic benchmark forinteractive sound source localization. Second, we introduce new evaluationmetrics to rigorously assess sound source localization methods, focusing onaccurately evaluating both localization performance and cross-modal interactionability. Third, we propose a learning framework with a cross-modal alignmentstrategy to enhance cross-modal interaction. Lastly, we evaluate bothinteractive sound source localization and auxiliary cross-modal retrieval taskstogether to thoroughly assess cross-modal interaction capabilities andbenchmark competing methods. Our new benchmarks and evaluation metrics revealpreviously overlooked issues in sound source localization studies. Our proposednovel method, with enhanced cross-modal alignment, shows superior sound sourcelocalization performance. This work provides the most comprehensive analysis ofsound source localization to date, with extensive validation of competingmethods on both existing and new benchmarks using new and standard evaluationmetrics.</description><author>Arda Senocak, Hyeonggon Ryu, Junsik Kim, Tae-Hyun Oh, Hanspeter Pfister, Joon Son Chung</author><pubDate>Thu, 18 Jul 2024 16:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13676v1</guid></item><item><title>MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture Synthesis</title><link>http://arxiv.org/abs/2407.13675v1</link><description>We present MeshSegmenter, a simple yet effective framework designed forzero-shot 3D semantic segmentation. This model successfully extends thepowerful capabilities of 2D segmentation models to 3D meshes, deliveringaccurate 3D segmentation across diverse meshes and segment descriptions.Specifically, our model leverages the Segment Anything Model (SAM) model tosegment the target regions from images rendered from the 3D shape. In light ofthe importance of the texture for segmentation, we also leverage the pretrainedstable diffusion model to generate images with textures from 3D shape, andleverage SAM to segment the target regions from images with textures. Texturessupplement the shape for segmentation and facilitate accurate 3D segmentationeven in geometrically non-prominent areas, such as segmenting a car door withina car mesh. To achieve the 3D segments, we render 2D images from differentviews and conduct segmentation for both textured and untextured images. Lastly,we develop a multi-view revoting scheme that integrates 2D segmentation resultsand confidence scores from various views onto the 3D mesh, ensuring the 3Dconsistency of segmentation results and eliminating inaccuracies from specificperspectives. Through these innovations, MeshSegmenter offers stable andreliable 3D segmentation results both quantitatively and qualitatively,highlighting its potential as a transformative tool in the field of 3Dzero-shot segmentation. The code is available at\url{https://github.com/zimingzhong/MeshSegmenter}.</description><author>Ziming Zhong, Yanxu Xu, Jing Li, Jiale Xu, Zhengxin Li, Chaohui Yu, Shenghua Gao</author><pubDate>Thu, 18 Jul 2024 16:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13675v1</guid></item><item><title>I Can't Believe It's Not Scene Flow!</title><link>http://arxiv.org/abs/2403.04739v2</link><description>Current scene flow methods broadly fail to describe motion on small objects,and current scene flow evaluation protocols hide this failure by averaging overmany points, with most drawn larger objects. To fix this evaluation failure, wepropose a new evaluation protocol, Bucket Normalized EPE, which is class-awareand speed-normalized, enabling contextualized error comparisons between objecttypes that move at vastly different speeds. To highlight current methodfailures, we propose a frustratingly simple supervised scene flow baseline,TrackFlow, built by bolting a high-quality pretrained detector (trained usingmany class rebalancing techniques) onto a simple tracker, that producesstate-of-the-art performance on current standard evaluations and largeimprovements over prior art on our new evaluation. Our results make it clearthat all scene flow evaluations must be class and speed aware, and supervisedscene flow methods must address point class imbalances. We release theevaluation code publicly athttps://github.com/kylevedder/BucketedSceneFlowEval.</description><author>Ishan Khatri, Kyle Vedder, Neehar Peri, Deva Ramanan, James Hays</author><pubDate>Thu, 18 Jul 2024 16:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04739v2</guid></item><item><title>Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning</title><link>http://arxiv.org/abs/2407.13666v1</link><description>Uncertainty quantification (UQ) is a crucial but challenging task in manyhigh-dimensional regression or learning problems to increase the confidence ofa given predictor. We develop a new data-driven approach for UQ in regressionthat applies both to classical regression approaches such as the LASSO as wellas to neural networks. One of the most notable UQ techniques is the debiasedLASSO, which modifies the LASSO to allow for the construction of asymptoticconfidence intervals by decomposing the estimation error into a Gaussian and anasymptotically vanishing bias component. However, in real-world problems withfinite-dimensional data, the bias term is often too significant to beneglected, resulting in overly narrow confidence intervals. Our work rigorouslyaddresses this issue and derives a data-driven adjustment that corrects theconfidence intervals for a large class of predictors by estimating the meansand variances of the bias terms from training data, exploiting high-dimensionalconcentration phenomena. This gives rise to non-asymptotic confidenceintervals, which can help avoid overestimating uncertainty in criticalapplications such as MRI diagnosis. Importantly, our analysis extends beyondsparse regression to data-driven predictors like neural networks, enhancing thereliability of model-based deep learning. Our findings bridge the gap betweenestablished theory and the practical applicability of such debiased methods.</description><author>Frederik Hoppe, Claudio Mayrink Verdun, Hannah Laus, Felix Krahmer, Holger Rauhut</author><pubDate>Thu, 18 Jul 2024 16:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13666v1</guid></item><item><title>Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization</title><link>http://arxiv.org/abs/2407.13664v1</link><description>Marketing optimization plays an important role to enhance user engagement inonline Internet platforms. Existing studies usually formulate this problem as abudget allocation problem and solve it by utilizing two fully decoupled stages,i.e., machine learning (ML) and operation research (OR). However, the learningobjective in ML does not take account of the downstream optimization task inOR, which causes that the prediction accuracy in ML may be not positivelyrelated to the decision quality. Decision Focused Learning (DFL) integrates ML and OR into an end-to-endframework, which takes the objective of the downstream task as the decisionloss function and guarantees the consistency of the optimization directionbetween ML and OR. However, deploying DFL in marketing is non-trivial due tomultiple technological challenges. Firstly, the budget allocation problem inmarketing is a 0-1 integer stochastic programming problem and the budget isuncertain and fluctuates a lot in real-world settings, which is beyond thegeneral problem background in DFL. Secondly, the counterfactual in marketingcauses that the decision loss cannot be directly computed and the optimalsolution can never be obtained, both of which disable the commongradient-estimation approaches in DFL. Thirdly, the OR solver is calledfrequently to compute the decision loss during model training in DFL, whichproduces huge computational cost and cannot support large-scale training data.In this paper, we propose a decision focused causal learning framework (DFCL)for direct counterfactual marketing optimization, which overcomes the abovetechnological challenges. Both offline experiments and online A/B testingdemonstrate the effectiveness of DFCL over the state-of-the-art methods.Currently, DFCL has been deployed in several marketing scenarios in Meituan,one of the largest online food delivery platform in the world.</description><author>Hao Zhou, Rongxiao Huang, Shaoming Li, Guibin Jiang, Jiaqi Zheng, Bing Cheng, Wei Lin</author><pubDate>Thu, 18 Jul 2024 16:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13664v1</guid></item><item><title>Studying the Performance of the Jellyfish Search Optimiser for the Application of Projection Pursuit</title><link>http://arxiv.org/abs/2407.13663v1</link><description>The projection pursuit (PP) guided tour interactively optimises a criteriafunction known as the PP index, to explore high-dimensional data by revealinginteresting projections. The optimisation in PP can be non-trivial, involvingnon-smooth functions and optima with a small squint angle, detectable only fromclose proximity. To address these challenges, this study investigates theperformance of a recently introduced swarm-based algorithm, Jellyfish SearchOptimiser (JSO), for optimising PP indexes. The performance of JSO forvisualising data is evaluated across various hyper-parameter settings andcompared with existing optimisers. Additionally, this work proposes novelmethods to quantify two properties of the PP index, smoothness andsquintability that capture the complexities inherent in PP optimisationproblems. These two metrics are evaluated along with JSO hyper-parameters todetermine their effects on JSO success rate. Our numerical results confirm thepositive impact of these metrics on the JSO success rate, with squintabilitybeing the most significant. The JSO algorithm has been implemented in the tourrpackage and functions to calculate smoothness and squintability are availablein the ferrn package.</description><author>H. Sherry Zhang, Dianne Cook, Nicolas Langrené, Jessica Wai Yin Leung</author><pubDate>Thu, 18 Jul 2024 16:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13663v1</guid></item><item><title>CogniVoice: Multimodal and Multilingual Fusion Networks for Mild Cognitive Impairment Assessment from Spontaneous Speech</title><link>http://arxiv.org/abs/2407.13660v1</link><description>Mild Cognitive Impairment (MCI) is a medical condition characterized bynoticeable declines in memory and cognitive abilities, potentially affectingindividual's daily activities. In this paper, we introduce CogniVoice, a novelmultilingual and multimodal framework to detect MCI and estimate Mini-MentalState Examination (MMSE) scores by analyzing speech data and its textualtranscriptions. The key component of CogniVoice is an ensemble multimodal andmultilingual network based on ``Product of Experts'' that mitigates reliance onshortcut solutions. Using a comprehensive dataset containing both English andChinese languages from TAUKADIAL challenge, CogniVoice outperforms the bestperforming baseline model on MCI classification and MMSE regression tasks by2.8 and 4.1 points in F1 and RMSE respectively, and can effectively reduce theperformance gap across different language groups by 0.7 points in F1.</description><author>Jiali Cheng, Mohamed Elgaar, Nidhi Vakil, Hadi Amiri</author><pubDate>Thu, 18 Jul 2024 16:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13660v1</guid></item><item><title>Probabilistic Image-Driven Traffic Modeling via Remote Sensing</title><link>http://arxiv.org/abs/2403.05521v2</link><description>This work addresses the task of modeling spatiotemporal traffic patternsdirectly from overhead imagery, which we refer to as image-driven trafficmodeling. We extend this line of work and introduce a multi-modal, multi-tasktransformer-based segmentation architecture that can be used to create densecity-scale traffic models. Our approach includes a geo-temporal positionalencoding module for integrating geo-temporal context and a probabilisticobjective function for estimating traffic speeds that naturally models temporalvariations. We evaluate our method extensively using the Dynamic Traffic Speeds(DTS) benchmark dataset and significantly improve the state-of-the-art.Finally, we introduce the DTS++ dataset to support mobility-related locationadaptation experiments.</description><author>Scott Workman, Armin Hadzic</author><pubDate>Thu, 18 Jul 2024 16:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05521v2</guid></item><item><title>MVSBoost: An Efficient Point Cloud-based 3D Reconstruction</title><link>http://arxiv.org/abs/2406.13515v2</link><description>Efficient and accurate 3D reconstruction is crucial for various applications,including augmented and virtual reality, medical imaging, and cinematic specialeffects. While traditional Multi-View Stereo (MVS) systems have beenfundamental in these applications, using neural implicit fields in implicit 3Dscene modeling has introduced new possibilities for handling complex topologiesand continuous surfaces. However, neural implicit fields often suffer fromcomputational inefficiencies, overfitting, and heavy reliance on data quality,limiting their practical use. This paper presents an enhanced MVS frameworkthat integrates multi-view 360-degree imagery with robust camera poseestimation via Structure from Motion (SfM) and advanced image processing forpoint cloud densification, mesh reconstruction, and texturing. Our approachsignificantly improves upon traditional MVS methods, offering superior accuracyand precision as validated using Chamfer distance metrics on the RealisticSynthetic 360 dataset. The developed MVS technique enhances the detail andclarity of 3D reconstructions and demonstrates superior computationalefficiency and robustness in complex scene reconstruction, effectively handlingocclusions and varying viewpoints. These improvements suggest that our MVSframework can compete with and potentially exceed current state-of-the-artneural implicit field methods, especially in scenarios requiring real-timeprocessing and scalability.</description><author>Umair Haroon, Ahmad AlMughrabi, Ricardo Marques, Petia Radeva</author><pubDate>Thu, 18 Jul 2024 16:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13515v2</guid></item><item><title>FuLG: 150B Romanian Corpus for Language Model Pretraining</title><link>http://arxiv.org/abs/2407.13657v1</link><description>Research in the field of language models is rapidly evolving, with many openmodels being released to the public. Openly available pretraining corporausually focus on only a handful of languages, with many others either missingcompletely or extremely underrepresented. In this report, we introduce FuLG, ahundred-fifty-billion-token Romanian corpus extracted from CommonCrawl. Wepresent our methodology for filtering FuLG and compare it via ablation studiesagainst existing Romanian corpora.</description><author>Vlad-Andrei Bădoiu, Mihai-Valentin Dumitru, Alexandru M. Gherghescu, Alexandru Agache, Costin Raiciu</author><pubDate>Thu, 18 Jul 2024 16:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13657v1</guid></item><item><title>MinD-3D: Reconstruct High-quality 3D objects in Human Brain</title><link>http://arxiv.org/abs/2312.07485v3</link><description>In this paper, we introduce Recon3DMind, an innovative task aimed atreconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)signals, marking a significant advancement in the fields of cognitiveneuroscience and computer vision. To support this pioneering task, we presentthe fMRI-Shape dataset, which includes data from 14 participants and features360-degree videos of 3D objects to enable comprehensive fMRI signal captureacross various settings, thereby laying a foundation for future research.Furthermore, we propose MinD-3D, a novel and effective three-stage frameworkspecifically designed to decode the brain's 3D visual information from fMRIsignals, demonstrating the feasibility of this challenging task. The frameworkbegins by extracting and aggregating features from fMRI frames through aneuro-fusion encoder, subsequently employs a feature bridge diffusion model togenerate visual features, and ultimately recovers the 3D object via agenerative transformer decoder. We assess the performance of MinD-3D using asuite of semantic and structural metrics and analyze the correlation betweenthe features extracted by our model and the visual regions of interest (ROIs)in fMRI signals. Our findings indicate that MinD-3D not only reconstructs 3Dobjects with high semantic relevance and spatial similarity but alsosignificantly enhances our understanding of the human brain's capabilities inprocessing 3D visual information. Project page at:https://jianxgao.github.io/MinD-3D.</description><author>Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu</author><pubDate>Thu, 18 Jul 2024 16:31:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07485v3</guid></item><item><title>QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead</title><link>http://arxiv.org/abs/2406.03482v2</link><description>Serving LLMs requires substantial memory due to the storage requirements ofKey-Value (KV) embeddings in the KV cache, which grows with sequence length. Aneffective approach to compress KV cache is quantization. However, traditionalquantization methods face significant memory overhead due to the need to storequantization constants (at least a zero point and a scale) in full precisionper data block. Depending on the block size, this overhead can add 1 or 2 bitsper quantized number. We introduce QJL, a new quantization approach thatconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bitquantization. In contrast to existing methods, QJL eliminates memory overheadsby removing the need for storing quantization constants. We propose anasymmetric estimator for the inner product of two vectors and demonstrate thatapplying QJL to one vector and a standard JL transform without quantization tothe other provides an unbiased estimator with minimal distortion. We havedeveloped an efficient implementation of the QJL sketch and its correspondinginner product estimator, incorporating a lightweight CUDA kernel for optimizedcomputation. When applied across various LLMs and NLP tasks to quantize the KVcache to only 3 bits, QJL demonstrates a more than fivefold reduction in KVcache memory usage without compromising accuracy, all while achieving fasterruntime. Codes are available at \url{https://github.com/amirzandieh/QJL}.</description><author>Amir Zandieh, Majid Daliri, Insu Han</author><pubDate>Thu, 18 Jul 2024 16:31:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03482v2</guid></item><item><title>Weak-to-Strong Reasoning</title><link>http://arxiv.org/abs/2407.13647v1</link><description>When large language models (LLMs) exceed human-level capabilities, it becomesincreasingly challenging to provide full-scale and accurate supervisions forthese models. Weak-to-strong learning, which leverages a less capable model tounlock the latent abilities of a stronger model, proves valuable in thiscontext. Yet, the efficacy of this approach for complex reasoning tasks isstill untested. Furthermore, tackling reasoning tasks under the weak-to-strongsetting currently lacks efficient methods to avoid blindly imitating the weaksupervisor including its errors. In this paper, we introduce a progressivelearning framework that enables the strong model to autonomously refine itstraining data, without requiring input from either a more advanced model orhuman-annotated data. This framework begins with supervised fine-tuning on aselective small but high-quality dataset, followed by preference optimizationon contrastive samples identified by the strong model itself. Extensiveexperiments on the GSM8K and MATH datasets demonstrate that our methodsignificantly enhances the reasoning capabilities of Llama2-70b using threeseparate weak models. This method is further validated in a forward-lookingexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70bon the highly challenging OlympicArena dataset. This work paves the way for amore scalable and sophisticated strategy to enhance AI reasoning powers. Allrelevant code and resources are available in\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.</description><author>Yuqing Yang, Yan Ma, Pengfei Liu</author><pubDate>Thu, 18 Jul 2024 16:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13647v1</guid></item><item><title>Beyond Dropout: Robust Convolutional Neural Networks Based on Local Feature Masking</title><link>http://arxiv.org/abs/2407.13646v1</link><description>In the contemporary of deep learning, where models often grapple with thechallenge of simultaneously achieving robustness against adversarial attacksand strong generalization capabilities, this study introduces an innovativeLocal Feature Masking (LFM) strategy aimed at fortifying the performance ofConvolutional Neural Networks (CNNs) on both fronts. During the training phase,we strategically incorporate random feature masking in the shallow layers ofCNNs, effectively alleviating overfitting issues, thereby enhancing the model'sgeneralization ability and bolstering its resilience to adversarial attacks.LFM compels the network to adapt by leveraging remaining features to compensatefor the absence of certain semantic features, nurturing a more elastic featurelearning mechanism. The efficacy of LFM is substantiated through a series ofquantitative and qualitative assessments, collectively showcasing a consistentand significant improvement in CNN's generalization ability and resistanceagainst adversarial attacks--a phenomenon not observed in current and priormethodologies. The seamless integration of LFM into established CNN frameworksunderscores its potential to advance both generalization and adversarialrobustness within the deep learning paradigm. Through comprehensiveexperiments, including robust person re-identification baseline generalizationexperiments and adversarial attack experiments, we demonstrate the substantialenhancements offered by LFM in addressing the aforementioned challenges. Thiscontribution represents a noteworthy stride in advancing robust neural networkarchitectures.</description><author>Yunpeng Gong, Chuangliang Zhang, Yongjie Hou, Lifei Chen, Min Jiang</author><pubDate>Thu, 18 Jul 2024 16:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13646v1</guid></item><item><title>Private Mean Estimation with Person-Level Differential Privacy</title><link>http://arxiv.org/abs/2405.20405v2</link><description>We study person-level differentially private (DP) mean estimation in the casewhere each person holds multiple samples. DP here requires the usual notion ofdistributional stability when $\textit{all}$ of a person's datapoints can bemodified. Informally, if $n$ people each have $m$ samples from an unknown$d$-dimensional distribution with bounded $k$-th moments, we show that peopleare necessary and sufficient to estimate the mean up to distance $\alpha$ in$\ell_2$-norm under $\varepsilon$-differential privacy (and its commonrelaxations). In the multivariate setting, we give computationally efficientalgorithms under approximate-DP and computationally inefficient algorithmsunder pure DP, and our nearly matching lower bounds hold for the mostpermissive case of approximate DP. Our computationally efficient estimators arebased on the standard clip-and-noise framework, but the analysis for oursetting requires both new algorithmic techniques and new analyses. Inparticular, our new bounds on the tails of sums of independent, vector-valued,bounded-moments random variables may be of interest. \[n = \tilde \Theta\left(\frac{d}{\alpha^2 m} + \frac{d}{\alpha m^{1/2}\varepsilon} + \frac{d}{\alpha^{k/(k-1)} m \varepsilon} +\frac{d}{\varepsilon}\right)\]</description><author>Sushant Agarwal, Gautam Kamath, Mahbod Majid, Argyris Mouzakis, Rose Silver, Jonathan Ullman</author><pubDate>Thu, 18 Jul 2024 16:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20405v2</guid></item><item><title>Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2407.13642v1</link><description>In this paper, we investigate the use of diffusion models which arepre-trained on large-scale image-caption pairs for open-vocabulary 3D semanticunderstanding. We propose a novel method, namely Diff2Scene, which leveragesfrozen representations from text-image generative models, along withsalient-aware and geometric-aware masks, for open-vocabulary 3D semanticsegmentation and visual grounding tasks. Diff2Scene gets rid of any labeled 3Ddata and effectively identifies objects, appearances, materials, locations andtheir compositions in 3D scenes. We show that it outperforms competitivebaselines and achieves significant improvements over state-of-the-art methods.In particular, Diff2Scene improves the state-of-the-art method on ScanNet200 by12%.</description><author>Xiaoyu Zhu, Hao Zhou, Pengfei Xing, Long Zhao, Hao Xu, Junwei Liang, Alexander Hauptmann, Ting Liu, Andrew Gallagher</author><pubDate>Thu, 18 Jul 2024 16:20:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13642v1</guid></item><item><title>Beyond Augmentation: Empowering Model Robustness under Extreme Capture Environments</title><link>http://arxiv.org/abs/2407.13640v1</link><description>Person Re-identification (re-ID) in computer vision aims to recognize andtrack individuals across different cameras. While previous research has mainlyfocused on challenges like pose variations and lighting changes, the impact ofextreme capture conditions is often not adequately addressed. These extremeconditions, including varied lighting, camera styles, angles, and imagedistortions, can significantly affect data distribution and re-ID accuracy. Current research typically improves model generalization under normalshooting conditions through data augmentation techniques such as adjustingbrightness and contrast. However, these methods pay less attention to therobustness of models under extreme shooting conditions. To tackle this, wepropose a multi-mode synchronization learning (MMSL) strategy . This approachinvolves dividing images into grids, randomly selecting grid blocks, andapplying data augmentation methods like contrast and brightness adjustments.This process introduces diverse transformations without altering the originalimage structure, helping the model adapt to extreme variations. This methodimproves the model's generalization under extreme conditions and enableslearning diverse features, thus better addressing the challenges in re-ID.Extensive experiments on a simulated test set under extreme conditions havedemonstrated the effectiveness of our method. This approach is crucial forenhancing model robustness and adaptability in real-world scenarios, supportingthe future development of person re-identification technology.</description><author>Yunpeng Gong, Yongjie Hou, Chuangliang Zhang, Min Jiang</author><pubDate>Thu, 18 Jul 2024 16:18:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13640v1</guid></item><item><title>A Comparative Study on Automatic Coding of Medical Letters with Explainability</title><link>http://arxiv.org/abs/2407.13638v1</link><description>This study aims to explore the implementation of Natural Language Processing(NLP) and machine learning (ML) techniques to automate the coding of medicalletters with visualised explainability and light-weighted local computersettings. Currently in clinical settings, coding is a manual process thatinvolves assigning codes to each condition, procedure, and medication in apatient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). Thereare preliminary research on automatic coding in this field usingstate-of-the-art ML models; however, due to the complexity and size of themodels, the real-world deployment is not achieved. To further facilitate thepossibility of automatic coding practice, we explore some solutions in a localcomputer setting; in addition, we explore the function of explainability fortransparency of AI models. We used the publicly available MIMIC-III databaseand the HAN/HLAN network models for ICD code prediction purposes. We alsoexperimented with the mapping between ICD and SNOMED CT knowledge bases. In ourexperiments, the models provided useful information for 97.98\% of codes. Theresult of this investigation can shed some light on implementing automaticclinical coding in practice, such as in hospital settings, on the localcomputers used by clinicians , project page\url{https://github.com/Glenj01/Medical-Coding}.</description><author>Jamie Glen, Lifeng Han, Paul Rayson, Goran Nenadic</author><pubDate>Thu, 18 Jul 2024 16:12:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13638v1</guid></item><item><title>IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields</title><link>http://arxiv.org/abs/2407.11921v2</link><description>Neural Radiance Field (NeRF) represents a significant advancement in computervision, offering implicit neural network-based scene representation and novelview synthesis capabilities. Its applications span diverse fields includingrobotics, urban mapping, autonomous navigation, virtual reality/augmentedreality, etc., some of which are considered high-risk AI applications. However,despite its widespread adoption, the robustness and security of NeRF remainlargely unexplored. In this study, we contribute to this area by introducingthe Illusory Poisoning Attack against Neural Radiance Fields (IPA-NeRF). Thisattack involves embedding a hidden backdoor view into NeRF, allowing it toproduce predetermined outputs, i.e. illusory, when presented with the specifiedbackdoor view while maintaining normal performance with standard inputs. Ourattack is specifically designed to deceive users or downstream models at aparticular position while ensuring that any abnormalities in NeRF remainundetectable from other viewpoints. Experimental results demonstrate theeffectiveness of our Illusory Poisoning Attack, successfully presenting thedesired illusory on the specified viewpoint without impacting other views.Notably, we achieve this attack by introducing small perturbations solely tothe training set. The code can be found athttps://github.com/jiang-wenxiang/IPA-NeRF.</description><author>Wenxiang Jiang, Hanwei Zhang, Shuo Zhao, Zhongwen Guo, Hao Wang</author><pubDate>Thu, 18 Jul 2024 16:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11921v2</guid></item><item><title>Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data</title><link>http://arxiv.org/abs/2403.10663v2</link><description>With the increasing prevalence of Machine Learning as a Service (MLaaS)platforms, there is a growing focus on deep neural network (DNN) watermarkingtechniques. These methods are used to facilitate the verification of ownershipfor a target DNN model to protect intellectual property. One of the most widelyemployed watermarking techniques involves embedding a trigger set into thesource model. Unfortunately, existing methodologies based on trigger sets arestill susceptible to functionality-stealing attacks, potentially enablingadversaries to steal the functionality of the source model without a reliablemeans of verifying ownership. In this paper, we first introduce a novelperspective on trigger set-based watermarking methods from a feature learningperspective. Specifically, we demonstrate that by selecting data exhibitingmultiple features, also referred to as \emph{multi-view data}, it becomesfeasible to effectively defend functionality stealing attacks. Based on thisperspective, we introduce a novel watermarking technique based on Multi-viewdATa, called MAT, for efficiently embedding watermarks within DNNs. Thisapproach involves constructing a trigger set with multi-view data andincorporating a simple feature-based regularization method for training thesource model. We validate our method across various benchmarks and demonstrateits efficacy in defending against model extraction attacks, surpassing relevantbaselines by a significant margin. The code is available at:\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.</description><author>Yuxuan Li, Sarthak Kumar Maharana, Yunhui Guo</author><pubDate>Thu, 18 Jul 2024 16:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10663v2</guid></item><item><title>Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2404.03613v2</link><description>As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel viewsynthesis, it is a natural extension to deform a canonical 3DGS to multipleframes for representing a dynamic scene. However, previous works fail toaccurately reconstruct complex dynamic scenes. We attribute the failure to thedesign of the deformation field, which is built as a coordinate-based function.This approach is problematic because 3DGS is a mixture of multiple fieldscentered at the Gaussians, not just a single coordinate-based framework. Toresolve this problem, we define the deformation as a function of per-Gaussianembeddings and temporal embeddings. Moreover, we decompose deformations ascoarse and fine deformations to model slow and fast movements, respectively.Also, we introduce a local smoothness regularization for per-Gaussian embeddingto improve the details in dynamic regions. Project page:https://jeongminb.github.io/e-d3dgs/</description><author>Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</author><pubDate>Thu, 18 Jul 2024 16:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03613v2</guid></item><item><title>Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth Completion</title><link>http://arxiv.org/abs/2312.00844v3</link><description>It is widely believed that sparse supervision is worse than dense supervisionin the field of depth completion, but the underlying reasons for this arerarely discussed. To this end, we revisit the task of radar-camera depthcompletion and present a new method with sparse LiDAR supervision to outperformprevious dense LiDAR supervision methods in both accuracy and speed.Specifically, when trained by sparse LiDAR supervision, depth completion modelsusually output depth maps containing significant stripe-like artifacts. We findthat such a phenomenon is caused by the implicitly learned positionaldistribution pattern from sparse LiDAR supervision, termed as LiDARDistribution Leakage (LDL) in this paper. Based on such understanding, wepresent a novel Disruption-Compensation radar-camera depth completion frameworkto address this issue. The Disruption part aims to deliberately disrupt thelearning of LiDAR distribution from sparse supervision, while the Compensationpart aims to leverage 3D spatial and 2D semantic information to compensate forthe information loss of previous disruptions. Extensive experimental resultsdemonstrate that by reducing the impact of LDL, our framework with sparsesupervision outperforms the state-of-the-art dense supervision methods with11.6% improvement in Mean Absolute Error (MAE)} and 1.6x speedup in Frame PerSecond (FPS)}. The code is available athttps://github.com/megvii-research/Sparse-Beats-Dense.</description><author>Huadong Li, Minhao Jing, Jiajun Liang, Haoqiang Fan, Renhe Ji</author><pubDate>Thu, 18 Jul 2024 16:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00844v3</guid></item><item><title>Pyramid Diffusion for Fine 3D Large Scene Generation</title><link>http://arxiv.org/abs/2311.12085v2</link><description>Diffusion models have shown remarkable results in generating 2D images andsmall-scale 3D objects. However, their application to the synthesis oflarge-scale 3D scenes has been rarely explored. This is mainly due to theinherent complexity and bulky size of 3D scenery data, particularly outdoorscenes, and the limited availability of comprehensive real-world datasets,which makes training a stable scene diffusion model challenging. In this work,we explore how to effectively generate large-scale 3D scenes using thecoarse-to-fine paradigm. We introduce a framework, the Pyramid DiscreteDiffusion model (PDD), which employs scale-varied diffusion models toprogressively generate high-quality outdoor scenes. Experimental results of PDDdemonstrate our successful exploration in generating 3D scenes bothunconditionally and conditionally. We further showcase the data compatibilityof the PDD model, due to its multi-scale architecture: a PDD model trained onone dataset can be easily fine-tuned with another dataset. Code is available athttps://github.com/yuhengliu02/pyramid-discrete-diffusion.</description><author>Yuheng Liu, Xinke Li, Xueting Li, Lu Qi, Chongshou Li, Ming-Hsuan Yang</author><pubDate>Thu, 18 Jul 2024 16:04:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12085v2</guid></item><item><title>Data Alchemy: Mitigating Cross-Site Model Variability Through Test Time Data Calibration</title><link>http://arxiv.org/abs/2407.13632v1</link><description>Deploying deep learning-based imaging tools across various clinical sitesposes significant challenges due to inherent domain shifts and regulatoryhurdles associated with site-specific fine-tuning. For histopathology, stainnormalization techniques can mitigate discrepancies, but they often fall shortof eliminating inter-site variations. Therefore, we present Data Alchemy, anexplainable stain normalization method combined with test time data calibrationvia a template learning framework to overcome barriers in cross-site analysis.Data Alchemy handles shifts inherent to multi-site data and minimizes themwithout needing to change the weights of the normalization or classifiernetworks. Our approach extends to unseen sites in various clinical settingswhere data domain discrepancies are unknown. Extensive experiments highlightthe efficacy of our framework in tumor classification in hematoxylin andeosin-stained patches. Our explainable normalization method boostsclassification tasks' area under the precision-recall curve(AUPR) by 0.165,0.545 to 0.710. Additionally, Data Alchemy further reduces the multisiteclassification domain gap, by improving the 0.710 AUPR an additional 0.142,elevating classification performance further to 0.852, from 0.545. Our DataAlchemy framework can popularize precision medicine with minimal operationaloverhead by allowing for the seamless integration of pre-trained deeplearning-based clinical tools across multiple sites.</description><author>Abhijeet Parida, Antonia Alomar, Zhifan Jiang, Pooneh Roshanitabrizi, Austin Tapp, Maria Ledesma-Carbayo, Ziyue Xu, Syed Muhammed Anwar, Marius George Linguraru, Holger R. Roth</author><pubDate>Thu, 18 Jul 2024 16:03:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13632v1</guid></item><item><title>SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models</title><link>http://arxiv.org/abs/2404.17912v2</link><description>Radiology Report Generation (R2Gen) demonstrates how Multi-modal LargeLanguage Models (MLLMs) can automate the creation of accurate and coherentradiological reports. Existing methods often hallucinate details in text-basedreports that don't accurately reflect the image content. To mitigate this, weintroduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePortGENeraTion using Vision Language Models), which improves the R2Gen task byintegrating a self-refining mechanism into the MLLM framework. We employ aunique self-supervised loss that leverages similarity between pooled imagerepresentations and the contextual representations of the generatedradiological text, alongside the standard Causal Language Modeling objective,to refine image-text representations. This allows the model to scrutinize andalign the generated text through dynamic interaction between a given image andthe generated text, therefore reducing hallucination and continuously enhancingnuanced report generation. SERPENT-VLM outperforms existing baselines such asLLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray andRadiology Objects in COntext (ROCO) datasets, and also proves to be robustagainst noisy images. A qualitative case study emphasizes the significantadvancements towards more sophisticated MLLM frameworks for R2Gen, openingpaths for further research into self-supervised refinement in the medicalimaging domain.</description><author>Manav Nitin Kapadnis, Sohan Patnaik, Abhilash Nandy, Sourjyadip Ray, Pawan Goyal, Debdoot Sheet</author><pubDate>Thu, 18 Jul 2024 16:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17912v2</guid></item><item><title>Distributionally and Adversarially Robust Logistic Regression via Intersecting Wasserstein Balls</title><link>http://arxiv.org/abs/2407.13625v1</link><description>Empirical risk minimization often fails to provide robustness againstadversarial attacks in test data, causing poor out-of-sample performance.Adversarially robust optimization (ARO) has thus emerged as the de factostandard for obtaining models that hedge against such attacks. However, whilethese models are robust against adversarial attacks, they tend to sufferseverely from overfitting. To address this issue for logistic regression, westudy the Wasserstein distributionally robust (DR) counterpart of ARO and showthat this problem admits a tractable reformulation. Furthermore, we develop aframework to reduce the conservatism of this problem by utilizing an auxiliarydataset (e.g., synthetic, external, or out-of-domain data), whenever available,with instances independently sampled from a nonidentical but related groundtruth. In particular, we intersect the ambiguity set of the DR problem withanother Wasserstein ambiguity set that is built using the auxiliary dataset. Weanalyze the properties of the underlying optimization problem, developefficient solution algorithms, and demonstrate that the proposed methodconsistently outperforms benchmark approaches on real-world datasets.</description><author>Aras Selvi, Eleonora Kreacic, Mohsen Ghassemi, Vamsi Potluru, Tucker Balch, Manuela Veloso</author><pubDate>Thu, 18 Jul 2024 15:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13625v1</guid></item><item><title>Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies</title><link>http://arxiv.org/abs/2407.13623v1</link><description>Research on scaling large language models (LLMs) has primarily focused onmodel parameters and training data size, overlooking the role of vocabularysize. % Intuitively, larger vocabularies enable more efficient tokenization byrepresenting sentences with fewer tokens, but they also increase the risk ofunder-fitting representations for rare tokens. We investigate how vocabularysize impacts LLM scaling laws by training models ranging from 33M to 3Bparameters on up to 500B characters with various vocabulary configurations. Wepropose three complementary approaches for predicting the compute-optimalvocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fitof the loss function. Our approaches converge on the same result that theoptimal vocabulary size depends on the available compute budget and that largermodels deserve larger vocabularies. However, most LLMs use too small vocabularysizes. For example, we predict that the optimal vocabulary size of Llama2-70Bshould have been at least 216K, 7 times larger than its vocabulary of 32K. Wevalidate our predictions empirically by training models with 3B parametersacross different FLOPs budgets. Adopting our predicted optimal vocabulary sizeconsistently improves downstream performance over commonly used vocabularysizes. By increasing the vocabulary size from the conventional 32K to 43K, weimprove performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21FLOPs. Our work emphasizes the necessity of jointly considering modelparameters and vocabulary size for efficient scaling.</description><author>Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, Ngai Wong</author><pubDate>Thu, 18 Jul 2024 15:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13623v1</guid></item><item><title>Misspecified $Q$-Learning with Sparse Linear Function Approximation: Tight Bounds on Approximation Error</title><link>http://arxiv.org/abs/2407.13622v1</link><description>The recent work by Dong &amp; Yang (2023) showed for misspecified sparse linearbandits, one can obtain an $O\left(\epsilon\right)$-optimal policy using apolynomial number of samples when the sparsity is a constant, where $\epsilon$is the misspecification error. This result is in sharp contrast to misspecifiedlinear bandits without sparsity, which require an exponential number of samplesto get the same guarantee. In order to study whether the analog result ispossible in the reinforcement learning setting, we consider the followingproblem: assuming the optimal $Q$-function is a $d$-dimensional linear functionwith sparsity $k$ and misspecification error $\epsilon$, whether we can obtainan $O\left(\epsilon\right)$-optimal policy using number of samples polynomiallyin the feature dimension $d$. We first demonstrate why the standard approachbased on Bellman backup or the existing optimistic value function eliminationapproach such as OLIVE (Jiang et al., 2017) achieves suboptimal guarantees forthis problem. We then design a novel elimination-based algorithm to show onecan obtain an $O\left(H\epsilon\right)$-optimal policy with sample complexitypolynomially in the feature dimension $d$ and planning horizon $H$. Lastly, wecomplement our upper bound with an $\widetilde{\Omega}\left(H\epsilon\right)$suboptimality lower bound, giving a complete picture of this problem.</description><author>Ally Yalei Du, Lin F. Yang, Ruosong Wang</author><pubDate>Thu, 18 Jul 2024 15:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13622v1</guid></item><item><title>Differential Privacy Mechanisms in Neural Tangent Kernel Regression</title><link>http://arxiv.org/abs/2407.13621v1</link><description>Training data privacy is a fundamental problem in modern ArtificialIntelligence (AI) applications, such as face recognition, recommendationsystems, language generation, and many others, as it may contain sensitive userinformation related to legal issues. To fundamentally understand how privacymechanisms work in AI applications, we study differential privacy (DP) in theNeural Tangent Kernel (NTK) regression setting, where DP is one of the mostpowerful tools for measuring privacy under statistical learning, and NTK is oneof the most popular analysis frameworks for studying the learning mechanisms ofdeep neural networks. In our work, we can show provable guarantees for bothdifferential privacy and test accuracy of our NTK regression. Furthermore, weconduct experiments on the basic image classification dataset CIFAR10 todemonstrate that NTK regression can preserve good accuracy under a modestprivacy budget, supporting the validity of our analysis. To our knowledge, thisis the first work to provide a DP guarantee for NTK regression.</description><author>Jiuxiang Gu, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</author><pubDate>Thu, 18 Jul 2024 15:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13621v1</guid></item><item><title>Explicit-NeRF-QA: A Quality Assessment Database for Explicit NeRF Model Compression</title><link>http://arxiv.org/abs/2407.08165v2</link><description>In recent years, Neural Radiance Fields (NeRF) have demonstrated significantadvantages in representing and synthesizing 3D scenes. Explicit NeRF modelsfacilitate the practical NeRF applications with faster rendering speed, andalso attract considerable attention in NeRF compression due to its huge storagecost. To address the challenge of the NeRF compression study, in this paper, weconstruct a new dataset, called Explicit-NeRF-QA. We use 22 3D objects withdiverse geometries, textures, and material complexities to train four typicalexplicit NeRF models across five parameter levels. Lossy compression isintroduced during the model generation, pivoting the selection of keyparameters such as hash table size for InstantNGP and voxel grid resolution forPlenoxels. By rendering NeRF samples to processed video sequences (PVS), alarge scale subjective experiment with lab environment is conducted to collectsubjective scores from 21 viewers. The diversity of content, accuracy of meanopinion scores (MOS), and characteristics of NeRF distortion arecomprehensively presented, establishing the heterogeneity of the proposeddataset. The state-of-the-art objective metrics are tested in the new dataset.Best Person correlation, which is around 0.85, is collected from thefull-reference objective metric. All tested no-reference metrics report verypoor results with 0.4 to 0.6 correlations, demonstrating the need for furtherdevelopment of more robust no-reference metrics. The dataset, including NeRFsamples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, ismade publicly available at the following location:https://github.com/LittlericeChloe/Explicit_NeRF_QA.</description><author>Yuke Xing, Qi Yang, Kaifa Yang, Yilin Xu, Zhu Li</author><pubDate>Thu, 18 Jul 2024 15:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08165v2</guid></item><item><title>Training-free Composite Scene Generation for Layout-to-Image Synthesis</title><link>http://arxiv.org/abs/2407.13609v1</link><description>Recent breakthroughs in text-to-image diffusion models have significantlyadvanced the generation of high-fidelity, photo-realistic images from textualdescriptions. Yet, these models often struggle with interpreting spatialarrangements from text, hindering their ability to produce images with precisespatial configurations. To bridge this gap, layout-to-image generation hasemerged as a promising direction. However, training-based approaches arelimited by the need for extensively annotated datasets, leading to high dataacquisition costs and a constrained conceptual scope. Conversely, training-freemethods face challenges in accurately locating and generating semanticallysimilar objects within complex compositions. This paper introduces a noveltraining-free approach designed to overcome adversarial semantic intersectionsduring the diffusion conditioning phase. By refining intra-token loss withselective sampling and enhancing the diffusion process with attentionredistribution, we propose two innovative constraints: 1) an inter-tokenconstraint that resolves token conflicts to ensure accurate concept synthesis;and 2) a self-attention constraint that improves pixel-to-pixel relationships.Our evaluations confirm the effectiveness of leveraging layout information forguiding the diffusion process, generating content-rich images with enhancedfidelity and complexity. Code is available athttps://github.com/Papple-F/csg.git.</description><author>Jiaqi Liu, Tao Huang, Chang Xu</author><pubDate>Thu, 18 Jul 2024 15:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13609v1</guid></item><item><title>dzNLP at NADI 2024 Shared Task: Multi-Classifier Ensemble with Weighted Voting and TF-IDF Features</title><link>http://arxiv.org/abs/2407.13608v1</link><description>This paper presents the contribution of our dzNLP team to the NADI 2024shared task, specifically in Subtask 1 - Multi-label Country-level DialectIdentification (MLDID) (Closed Track). We explored various configurations toaddress the challenge: in Experiment 1, we utilized a union of n-gram analyzers(word, character, character with word boundaries) with different n-gram values;in Experiment 2, we combined a weighted union of Term Frequency-InverseDocument Frequency (TF-IDF) features with various weights; and in Experiment 3,we implemented a weighted major voting scheme using three classifiers: LinearSupport Vector Classifier (LSVC), Random Forest (RF), and K-Nearest Neighbors(KNN). Our approach, despite its simplicity and reliance on traditional machinelearning techniques, demonstrated competitive performance in terms of F1-scoreand precision. Notably, we achieved the highest precision score of 63.22% amongthe participating teams. However, our overall F1 score was approximately 21%,significantly impacted by a low recall rate of 12.87%. This indicates thatwhile our models were highly precise, they struggled to recall a broad range ofdialect labels, highlighting a critical area for improvement in handlingdiverse dialectal variations.</description><author>Mohamed Lichouri, Khaled Lounnas, Boualem Nadjib Zahaf, Mehdi Ayoub Rabiai</author><pubDate>Thu, 18 Jul 2024 15:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13608v1</guid></item><item><title>Physics-guided Active Sample Reweighting for Urban Flow Prediction</title><link>http://arxiv.org/abs/2407.13605v1</link><description>Urban flow prediction is a spatio-temporal modeling task that estimates thethroughput of transportation services like buses, taxis, and ride-sharing,where data-driven models have become the most popular solution in the pastdecade. Meanwhile, the implicitly learned mapping between historicalobservations to the prediction targets tend to over-simplify the dynamics ofreal-world urban flows, leading to suboptimal predictions. Some recentspatio-temporal prediction solutions bring remedies with the notion ofphysics-guided machine learning (PGML), which describes spatio-temporal datawith nuanced and principled physics laws, thus enhancing both the predictionaccuracy and interpretability. However, these spatio-temporal PGML methods arebuilt upon a strong assumption that the observed data fully conforms to thedifferential equations that define the physical system, which can quicklybecome ill-posed in urban flow prediction tasks. The observed urban flow data,especially when sliced into time-dependent snapshots to facilitate predictions,is typically incomplete and sparse, and prone to inherent noise incurred in thecollection process. As a result, such physical inconsistency between the dataand PGML model significantly limits the predictive power and robustness of thesolution. Moreover, due to the interval-based predictions and intermittentnature of data filing in many transportation services, the instantaneousdynamics of urban flows can hardly be captured, rendering differentialequation-based continuous modeling a loose fit for this setting. To overcomethe challenges, we develop a discretized physics-guided network (PN), andpropose a data-aware framework Physics-guided Active Sample Reweighting(P-GASR) to enhance PN. Experimental results in four real-world datasetsdemonstrate that our method achieves state-of-the-art performance with ademonstrable improvement in robustness.</description><author>Wei Jiang, Tong Chen, Guanhua Ye, Wentao Zhang, Lizhen Cui, Zi Huang, Hongzhi Yin</author><pubDate>Thu, 18 Jul 2024 15:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13605v1</guid></item><item><title>dzStance at StanceEval2024: Arabic Stance Detection based on Sentence Transformers</title><link>http://arxiv.org/abs/2407.13603v1</link><description>This study compares Term Frequency-Inverse Document Frequency (TF-IDF)features with Sentence Transformers for detecting writers' stances--favorable,opposing, or neutral--towards three significant topics: COVID-19 vaccine,digital transformation, and women empowerment. Through empirical evaluation, wedemonstrate that Sentence Transformers outperform TF-IDF features acrossvarious experimental setups. Our team, dzStance, participated in a stancedetection competition, achieving the 13th position (74.91%) among 15 teams inWomen Empowerment, 10th (73.43%) in COVID Vaccine, and 12th (66.97%) in DigitalTransformation. Overall, our team's performance ranked 13th (71.77%) among allparticipants. Notably, our approach achieved promising F1-scores, highlightingits effectiveness in identifying writers' stances on diverse topics. Theseresults underscore the potential of Sentence Transformers to enhance stancedetection models for addressing critical societal issues.</description><author>Mohamed Lichouri, Khaled Lounnas, Khelil Rafik Ouaras, Mohamed Abi, Anis Guechtouli</author><pubDate>Thu, 18 Jul 2024 15:43:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13603v1</guid></item><item><title>Efficient Image Denoising by Low-Rank Singular Vector Approximations of Geodesics' Gramian Matrix</title><link>http://arxiv.org/abs/2209.13094v4</link><description>With the advent of sophisticated cameras, the urge to capture high-qualityimages has grown enormous. However, the noise contamination of the imagesresults in substandard expectations among the people; thus, image denoising isan essential pre-processing step. While the algebraic image processingframeworks are sometimes inefficient for this denoising task as they mayrequire processing of matrices of order equivalent to some power of the orderof the original image, the neural network image processing frameworks aresometimes not robust as they require a lot of similar training samples. Thus,here we present a manifold-based noise filtering method that mainly exploits afew prominent singular vectors of the geodesics' Gramian matrix. Especially,the framework partitions an image, say that of size $n \times n$, into $n^2$overlapping patches of known size such that one patch is centered at eachpixel. Then, the prominent singular vectors, of the Gramian matrix of size $n^2\times n^2$ of the geodesic distances computed over the patch space, areutilized to denoise the image. Here, the prominent singular vectors arerevealed by efficient, but diverse, approximation techniques, rather thanexplicitly computing them using frameworks like Singular Value Decomposition(SVD) which encounters $\mathcal{O}(n^6)$ operations. Finally, we compare bothcomputational time and the noise filtration performance of the proposeddenoising algorithm with and without singular vector approximation techniques.</description><author>Kelum Gajamannage, Yonggi Park, S. M. Mallikarjunaiah, Sunil Mathur</author><pubDate>Thu, 18 Jul 2024 15:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.13094v4</guid></item><item><title>RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model</title><link>http://arxiv.org/abs/2406.10157v4</link><description>Minigolf is an exemplary real-world game for examining embodied intelligence,requiring challenging spatial and kinodynamic understanding to putt the ball.Additionally, reflective reasoning is required if the feasibility of achallenge is not ensured. We introduce RoboGolf, a VLM-based framework thatcombines dual-camera perception with closed-loop action refinement, augmentedby a reflective equilibrium loop. The core of both loops is powered byfinetuned VLMs. We analyze the capabilities of the framework in an offlineinference setting, relying on an extensive set of recorded trajectories.Exemplary demonstrations of the analyzed problem domain are available athttps://jity16.github.io/RoboGolf/</description><author>Hantao Zhou, Tianying Ji, Lukas Sommerhalder, Michael Goerner, Norman Hendrich, Jianwei Zhang, Fuchun Sun, Huazhe Xu</author><pubDate>Thu, 18 Jul 2024 15:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10157v4</guid></item><item><title>PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like (PL) Tasks</title><link>http://arxiv.org/abs/2407.13597v1</link><description>Text summarization is a well-studied problem that deals with derivinginsights from unstructured text consumed by humans, and it has found extensivebusiness applications. However, many real-life tasks involve generating aseries of actions to achieve specific goals, such as workflows, recipes,dialogs, and travel plans. We refer to them as planning-like (PL) tasks notingthat the main commonality they share is control flow information. which may bepartially specified. Their structure presents an opportunity to create morepractical summaries to help users make quick decisions. We investigate thisobservation by introducing a novel plan summarization problem, presenting adataset, and providing a baseline method for generating PL summaries. Usingquantitative metrics and qualitative user studies to establish baselines, weevaluate the plan summaries from our method and large language models. Webelieve the novel problem and dataset can reinvigorate research insummarization, which some consider as a solved problem.</description><author>Vishal Pallagani, Biplav Srivastava, Nitin Gupta</author><pubDate>Thu, 18 Jul 2024 15:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13597v1</guid></item><item><title>EarthMarker: A Visual Prompt Learning Framework for Region-level and Point-level Remote Sensing Imagery Comprehension</title><link>http://arxiv.org/abs/2407.13596v1</link><description>Recent advances in visual prompting in the natural image area have allowedusers to interact with artificial intelligence (AI) tools through variousvisual marks such as box, point, and free-form shapes. However, due to thesignificant difference between the natural and remote sensing (RS) images,existing visual prompting models face challenges in RS scenarios. Moreover, RSMLLMs mainly focus on interpreting image-level RS data and only supportinteraction with language instruction, restricting flexibility applications inthe real world. To address those limitations, a novel visual prompting modelnamed EarthMarker is proposed, which excels in image-level, region-level, andpoint-level RS imagery interpretation. Specifically, the visual promptsalongside images and text instruction input into the large language model(LLM), adapt models toward specific predictions and tasks. Subsequently, asharing visual encoding method is introduced to refine multi-scale imagefeatures and visual prompt information uniformly. Furthermore, to endow theEarthMarker with versatile multi-granularity visual perception abilities, thecross-domain phased learning strategy is developed, and the disjoint parametersare optimized in a lightweight manner by leveraging both the natural and RSdomain-specific knowledge. In addition, to tackle the lack of RS visualprompting data, a dataset named RSVP featuring multi-modal fine-grained visualprompting instruction is constructed. Extensive experiments are conducted todemonstrate the proposed EarthMarker's competitive performance, representing asignificant advance in multi-granularity RS imagery interpretation under thevisual prompting learning framework.</description><author>Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Xuerui Mao</author><pubDate>Thu, 18 Jul 2024 15:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13596v1</guid></item><item><title>Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning</title><link>http://arxiv.org/abs/2407.06642v2</link><description>Personalized text-to-image models allow users to generate varied styles ofimages (specified with a sentence) for an object (specified with a set ofreference images). While remarkable results have been achieved usingdiffusion-based generation models, the visual structure and details of theobject are often unexpectedly changed during the diffusion process. One majorreason is that these diffusion-based approaches typically adopt a simplereconstruction objective during training, which can hardly enforce appropriatestructural consistency between the generated and the reference images. To thisend, in this paper, we design a novel reinforcement learning framework byutilizing the deterministic policy gradient method for personalizedtext-to-image generation, with which various objectives, differential or evennon-differential, can be easily incorporated to supervise the diffusion modelsto improve the quality of the generated images. Experimental results onpersonalized text-to-image generation benchmark datasets demonstrate that ourproposed approach outperforms existing state-of-the-art methods by a largemargin on visual fidelity while maintaining text-alignment. Our code isavailable at: \url{https://github.com/wfanyue/DPG-T2I-Personalization}.</description><author>Fanyue Wei, Wei Zeng, Zhenyang Li, Dawei Yin, Lixin Duan, Wen Li</author><pubDate>Thu, 18 Jul 2024 15:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06642v2</guid></item><item><title>Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach</title><link>http://arxiv.org/abs/2407.13594v1</link><description>Mechanistic interpretability aims to reverse engineer the computationperformed by a neural network in terms of its internal components. Althoughthere is a growing body of research on mechanistic interpretation of neuralnetworks, the notion of a mechanistic interpretation itself is often ad-hoc.Inspired by the notion of abstract interpretation from the program analysisliterature that aims to develop approximate semantics for programs, we give aset of axioms that formally characterize a mechanistic interpretation as adescription that approximately captures the semantics of the neural networkunder analysis in a compositional manner. We use these axioms to guide themechanistic interpretability analysis of a Transformer-based model trained tosolve the well-known 2-SAT problem. We are able to reverse engineer thealgorithm learned by the model -- the model first parses the input formulas andthen evaluates their satisfiability via enumeration of different possiblevaluations of the Boolean input variables. We also present evidence to supportthat the mechanistic interpretation of the analyzed model indeed satisfies thestated axioms.</description><author>Nils Palumbo, Ravi Mangal, Zifan Wang, Saranya Vijayakumar, Corina S. Pasareanu, Somesh Jha</author><pubDate>Thu, 18 Jul 2024 15:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13594v1</guid></item><item><title>No More Sliding-Windows: Dynamic Functional Connectivity Based On Random Convolutions Without Learning</title><link>http://arxiv.org/abs/2406.16619v2</link><description>Compared to static functional connectivity, dynamic functional connectivityprovides more detailed temporal information. The traditional sliding windowmethod constructs functional connectivity matrices by applying a moving timewindow across the entire time series to calculate correlations between brainregions. However, as a method of feature extraction, it exhibits significantlimitations, such as the dependency of feature dimensions on the window lengthand the generation of features lacking information from other time pointswithin the window. This paper presents RandCon, a novel method for calculatingdynamic functional connectivity (DFC), which employs randomly generatedmulti-dimensional convolution kernels. This method performs convolutionoperations directly on the BOLD signal without the need for learning,extracting functional connectivity features. Compared to the sliding windowmethod, RandCon shows notable improvements in performance on simulated data,particularly in terms of temporal accuracy and noise resistance. Results fromreal data indicate that this method maintains stability within short timewindows and better identifies gender differences. Furthermore, we propose amore comprehensive theoretical framework, the multi-dimensional convolutionmethod, where the sliding window method and its variants are specific cases ofthis method. The proposed method is straightforward and efficient,significantly broadening the scope of dynamic functional connectivity researchand offering substantial theoretical and practical potential.</description><author>Yongjie Duan, Zhiying Long</author><pubDate>Thu, 18 Jul 2024 15:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16619v2</guid></item><item><title>LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning Approaches and Exploring its Applications</title><link>http://arxiv.org/abs/2401.17029v2</link><description>We investigate the prospect of reconstructing the ''cosmic distance ladder''of the Universe using a novel deep learning framework called LADDER - LearningAlgorithm for Deep Distance Estimation and Reconstruction. LADDER is trained onthe apparent magnitude data from the Pantheon Type Ia supernovae compilation,incorporating the full covariance information among data points, to producepredictions along with corresponding errors. After employing several validationtests with a number of deep learning models, we pick LADDER as the bestperforming one. We then demonstrate applications of our method in thecosmological context, including serving as a model-independent tool forconsistency checks for other datasets like baryon acoustic oscillations,calibration of high-redshift datasets such as gamma ray bursts, and use as amodel-independent mock catalog generator for future probes. Our analysisadvocates for careful consideration of machine learning techniques applied tocosmological contexts.</description><author>Rahul Shah, Soumadeep Saha, Purba Mukherjee, Utpal Garain, Supratik Pal</author><pubDate>Thu, 18 Jul 2024 15:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17029v2</guid></item><item><title>MeshFeat: Multi-Resolution Features for Neural Fields on Meshes</title><link>http://arxiv.org/abs/2407.13592v1</link><description>Parametric feature grid encodings have gained significant attention as anencoding approach for neural fields since they allow for much smaller MLPs,which significantly decreases the inference time of the models. In this work,we propose MeshFeat, a parametric feature encoding tailored to meshes, forwhich we adapt the idea of multi-resolution feature grids from Euclidean space.We start from the structure provided by the given vertex topology and use amesh simplification algorithm to construct a multi-resolution featurerepresentation directly on the mesh. The approach allows the usage of smallMLPs for neural fields on meshes, and we show a significant speed-up comparedto previous representations while maintaining comparable reconstruction qualityfor texture reconstruction and BRDF representation. Given its intrinsiccoupling to the vertices, the method is particularly well-suited forrepresentations on deforming meshes, making it a good fit for object animation.</description><author>Mihir Mahajan, Florian Hofherr, Daniel Cremers</author><pubDate>Thu, 18 Jul 2024 15:29:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13592v1</guid></item></channel></rss>