<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 05 Jun 2024 06:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors</title><link>http://arxiv.org/abs/2406.02552v1</link><description>We present a stereo-matching method for depth estimation from high-resolutionimages using visual hulls as priors, and a memory-efficient technique for thecorrelation computation. Our method uses object masks extracted fromsupplementary views of the scene to guide the disparity estimation, effectivelyreducing the search space for matches. This approach is specifically tailoredto stereo rigs in volumetric capture systems, where an accurate depth plays akey role in the downstream reconstruction task. To enable training andregression at high resolutions targeted by recent systems, our approach extendsa sparse correlation computation into a hybrid sparse-dense scheme suitable forapplication in leading recurrent network architectures. We evaluate theperformance-efficiency trade-off of our method compared to state-of-the-artmethods, and demonstrate the efficacy of the visual hull guidance. In addition,we propose a training scheme for a further reduction of memory requirementsduring optimization, facilitating training on high-resolution data.</description><author>Markus Plack, Hannah Dröge, Leif Van Holland, Matthias B. Hullin</author><pubDate>Tue, 04 Jun 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02552v1</guid></item><item><title>Identifying Equivalent Training Dynamics</title><link>http://arxiv.org/abs/2302.09160v2</link><description>Study of the nonlinear evolution deep neural network (DNN) parameters undergoduring training has uncovered regimes of distinct dynamical behavior. While adetailed understanding of these phenomena has the potential to advanceimprovements in training efficiency and robustness, the lack of methods foridentifying when DNN models have equivalent dynamics limits the insight thatcan be gained from prior work. Topological conjugacy, a notion from dynamicalsystems theory, provides a precise definition of dynamical equivalence,offering a possible route to address this need. However, topologicalconjugacies have historically been challenging to compute. By leveragingadvances in Koopman operator theory, we develop a framework for identifyingconjugate and non-conjugate training dynamics. To validate our approach, wedemonstrate that it can correctly identify a known equivalence between onlinemirror descent and online gradient descent. We then utilize it to: identifynon-conjugate training dynamics between shallow and wide fully connected neuralnetworks; characterize the early phase of training dynamics in convolutionalneural networks; uncover non-conjugate training dynamics in Transformers thatdo and do not undergo grokking. Our results, across a range of DNNarchitectures, illustrate the flexibility of our framework and highlight itspotential for shedding new light on training dynamics.</description><author>William T. Redman, Juan M. Bello-Rivas, Maria Fonoberova, Ryan Mohr, Ioannis G. Kevrekidis, Igor Mezić</author><pubDate>Tue, 04 Jun 2024 16:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09160v2</guid></item><item><title>WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo Collections</title><link>http://arxiv.org/abs/2406.02407v1</link><description>Novel View Synthesis (NVS) from unconstrained photo collections ischallenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) hasshown promise for photorealistic and real-time NVS of static scenes. Buildingon 3DGS, we propose an efficient point-based differentiable rendering frameworkfor scene reconstruction from photo collections. Our key innovation is aresidual-based spherical harmonic coefficients transfer module that adapts 3DGSto varying lighting conditions and photometric post-processing. Thislightweight module can be pre-computed and ensures efficient gradientpropagation from rendered images to 3D Gaussian attributes. Additionally, weobserve that the appearance encoder and the transient mask predictor, the twomost critical parts of NVS from unconstrained photo collections, can bemutually beneficial. We introduce a plug-and-play lightweight spatial attentionmodule to simultaneously predict transient occluders and latent appearancerepresentation for each image. After training and preprocessing, our methodaligns with the standard 3DGS format and rendering pipeline, facilitatingseamlessly integration into various 3DGS applications. Extensive experiments ondiverse datasets show our approach outperforms existing approaches on therendering quality of novel view and appearance synthesis with high converge andrendering speed.</description><author>Yuze Wang, Junyi Wang, Yue Qi</author><pubDate>Tue, 04 Jun 2024 16:17:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02407v1</guid></item><item><title>Alternative Methods to SHAP Derived from Properties of Kernels: A Note on Theoretical Analysis</title><link>http://arxiv.org/abs/2406.00371v2</link><description>This study first derives a general and analytical expression of AFA (AdditiveFeature Attribution) in terms of the kernel in LIME (Local InterpretableModel-agnostic Explanations). Then, we propose some new AFAs that haveappropriate properties of kernels or that coincide with the LS prenucleolus incooperative game theory. We also revisit existing AFAs such as SHAP (SHapleyAdditive exPlanations) and re-examine the properties of their kernels.</description><author>Kazuhiro Hiraki, Shinichi Ishihara, Junnosuke Shino</author><pubDate>Tue, 04 Jun 2024 16:16:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00371v2</guid></item><item><title>Online Fair Allocation of Perishable Resources</title><link>http://arxiv.org/abs/2406.02402v1</link><description>We consider a practically motivated variant of the canonical online fairallocation problem: a decision-maker has a budget of perishable resources toallocate over a fixed number of rounds. Each round sees a random number ofarrivals, and the decision-maker must commit to an allocation for theseindividuals before moving on to the next round. The goal is to construct asequence of allocations that is envy-free and efficient. Our work makes twoimportant contributions toward this problem: we first derive strong lowerbounds on the optimal envy-efficiency trade-off that demonstrate that adecision-maker is fundamentally limited in what she can hope to achieverelative to the no-perishing setting; we then design an algorithm achievingthese lower bounds which takes as input $(i)$ a prediction of the perishingorder, and $(ii)$ a desired bound on envy. Given the remaining budget in eachperiod, the algorithm uses forecasts of future demand and perishing toadaptively choose one of two carefully constructed guardrail quantities. Wedemonstrate our algorithm's strong numerical performance - andstate-of-the-art, perishing-agnostic algorithms' inefficacy - on simulationscalibrated to a real-world dataset.</description><author>Siddhartha Banerjee, Chamsi Hssaine, Sean R. Sinclair</author><pubDate>Tue, 04 Jun 2024 16:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02402v1</guid></item><item><title>Inhomogeneous graph trend filtering via a l2,0 cardinality penalty</title><link>http://arxiv.org/abs/2304.05223v3</link><description>We study estimation of piecewise smooth signals over a graph. We propose a$\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimatepiecewise smooth graph signals that exhibit inhomogeneous levels of smoothnessacross the nodes. We prove that the proposed GTF model is simultaneously ak-means clustering on the signal over the nodes and a minimum graph cut on theedges of the graph, where the clustering and the cut share the same assignmentmatrix. We propose two methods to solve the proposed GTF model: a spectraldecomposition method and a method based on simulated annealing. In theexperiment on synthetic and real-world datasets, we show that the proposed GTFmodel has a better performances compared with existing approaches on the tasksof denoising, support recovery and semi-supervised classification. We also showthat the proposed GTF model can be solved more efficiently than existing modelsfor the dataset with a large edge set.</description><author>Xiaoqing Huang, Andersen Ang, Kun Huang, Jie Zhang, Yijie Wang</author><pubDate>Tue, 04 Jun 2024 16:13:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05223v3</guid></item><item><title>Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning</title><link>http://arxiv.org/abs/2305.01206v4</link><description>Solving Constrained Horn Clauses (CHCs) is a fundamental challenge behind awide range of verification and analysis tasks. Data-driven approaches showgreat promise in improving CHC solving without the painstaking manual effort ofcreating and tuning various heuristics. However, a large performance gap existsbetween data-driven CHC solvers and symbolic reasoning-based solvers. In thiswork, we develop a simple but effective framework, "Chronosymbolic Learning",which unifies symbolic information and numerical data points to solve a CHCsystem efficiently. We also present a simple instance of ChronosymbolicLearning with a data-driven learner and a BMC-styled reasoner. Despite itsrelative simplicity, experimental results show the efficacy and robustness ofour tool. It outperforms state-of-the-art CHC solvers on a dataset consistingof 288 benchmarks, including many instances with non-linear integerarithmetics.</description><author>Ziyan Luo, Xujie Si</author><pubDate>Tue, 04 Jun 2024 16:11:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01206v4</guid></item><item><title>The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding</title><link>http://arxiv.org/abs/2406.02396v1</link><description>The evaluation of English text embeddings has transitioned from evaluating ahandful of datasets to broad coverage across many tasks through benchmarks suchas MTEB. However, this is not the case for multilingual text embeddings due toa lack of available benchmarks. To address this problem, we introduce theScandinavian Embedding Benchmark (SEB). SEB is a comprehensive framework thatenables text embedding evaluation for Scandinavian languages across 24 tasks,10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26models, uncovering significant performance disparities between public andcommercial solutions not previously captured by MTEB. We open-source SEB andintegrate it with MTEB, thus bridging the text embedding evaluation gap forScandinavian languages.</description><author>Kenneth Enevoldsen, Márton Kardos, Niklas Muennighoff, Kristoffer Laigaard Nielbo</author><pubDate>Tue, 04 Jun 2024 16:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02396v1</guid></item><item><title>Majority Vote for Distributed Differentially Private Sign Selection</title><link>http://arxiv.org/abs/2209.04419v2</link><description>Privacy-preserving data analysis has become more prevalent in recent years.In this study, we propose a distributed group differentially private MajorityVote mechanism, for the sign selection problem in a distributed setup. Toachieve this, we apply the iterative peeling to the stability function and usethe exponential mechanism to recover the signs. For enhanced applicability, westudy the private sign selection for mean estimation and linear regressionproblems, in distributed systems. Our method recovers the support and signswith the optimal signal-to-noise ratio as in the non-private scenario, which isbetter than contemporary works of private variable selections. Moreover, thesign selection consistency is justified by theoretical guarantees. Simulationstudies are conducted to demonstrate the effectiveness of the proposed method.</description><author>Weidong Liu, Jiyuan Tu, Xiaojun Mao, Xi Chen</author><pubDate>Tue, 04 Jun 2024 16:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.04419v2</guid></item><item><title>GrootVL: Tree Topology is All You Need in State Space Model</title><link>http://arxiv.org/abs/2406.02395v1</link><description>The state space models, employing recursively propagated features,demonstrate strong representation capabilities comparable to Transformer modelsand superior efficiency. However, constrained by the inherent geometricconstraints of sequences, it still falls short in modeling long-rangedependencies. To address this issue, we propose the GrootVL network, whichfirst dynamically generates a tree topology based on spatial relationships andinput features. Then, feature propagation is performed based on this graph,thereby breaking the original sequence constraints to achieve strongerrepresentation capabilities. Additionally, we introduce a linear complexitydynamic programming algorithm to enhance long-range interactions withoutincreasing computational cost. GrootVL is a versatile multimodal framework thatcan be applied to both visual and textual tasks. Extensive experimentsdemonstrate that our method significantly outperforms existing structured statespace models on image classification, object detection and segmentation.Besides, by fine-tuning large language models, our approach achieves consistentimprovements in multiple textual tasks at minor training cost.</description><author>Yicheng Xiao, Lin Song, Shaoli Huang, Jiangshan Wang, Siyu Song, Yixiao Ge, Xiu Li, Ying Shan</author><pubDate>Tue, 04 Jun 2024 16:09:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02395v1</guid></item><item><title>Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data</title><link>http://arxiv.org/abs/2406.02394v1</link><description>Large Language Models (LLMs) like ChatGPT demonstrate significant potentialin the medical field, often evaluated using multiple-choice questions (MCQs)similar to those found on the USMLE. Despite their prevalence in medicaleducation, MCQs have limitations that might be exacerbated when assessing LLMs.To evaluate the effectiveness of MCQs in assessing the performance of LLMs, wedeveloped a fictional medical benchmark focused on a non-existent gland, theGlianorex. This approach allowed us to isolate the knowledge of the LLM fromits test-taking abilities. We used GPT-4 to generate a comprehensive textbookon the Glianorex in both English and French and developed correspondingmultiple-choice questions in both languages. We evaluated various open-source,proprietary, and domain-specific LLMs using these questions in a zero-shotsetting. The models achieved average scores around 67%, with minor performancedifferences between larger and smaller models. Performance was slightly higherin English than in French. Fine-tuned medical models showed some improvementover their base versions in English but not in French. The uniformly highperformance across models suggests that traditional MCQ-based benchmarks maynot accurately measure LLMs' clinical knowledge and reasoning abilities,instead highlighting their pattern recognition skills. This study underscoresthe need for more robust evaluation methods to better assess the truecapabilities of LLMs in medical contexts.</description><author>Maxime Griot, Jean Vanderdonckt, Demet Yuksel, Coralie Hemptinne</author><pubDate>Tue, 04 Jun 2024 16:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02394v1</guid></item><item><title>DiarizationLM: Speaker Diarization Post-Processing with Large Language Models</title><link>http://arxiv.org/abs/2401.03506v5</link><description>In this paper, we introduce DiarizationLM, a framework to leverage largelanguage models (LLM) to post-process the outputs from a speaker diarizationsystem. Various goals can be achieved with the proposed framework, such asimproving the readability of the diarized transcript, or reducing the worddiarization error rate (WDER). In this framework, the outputs of the automaticspeech recognition (ASR) and speaker diarization systems are represented as acompact textual format, which is included in the prompt to an optionallyfinetuned LLM. The outputs of the LLM can be used as the refined diarizationresults with the desired enhancement. As a post-processing step, this frameworkcan be easily applied to any off-the-shelf ASR and speaker diarization systemswithout retraining existing components. Our experiments show that a finetunedPaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephoneconversation dataset, and rel. 44.9% on the Callhome English dataset.</description><author>Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, Hank Liao</author><pubDate>Tue, 04 Jun 2024 16:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03506v5</guid></item><item><title>Taxi1500: A Multilingual Dataset for Text Classification in 1500 Languages</title><link>http://arxiv.org/abs/2305.08487v2</link><description>While natural language processing tools have been developed extensively forsome of the world's languages, a significant portion of the world's over 7000languages are still neglected. One reason for this is that evaluation datasetsdo not yet cover a wide range of languages, including low-resource andendangered ones. We aim to address this issue by creating a text classificationdataset encompassing a large number of languages, many of which currently havelittle to no annotated data available. We leverage parallel translations of theBible to construct such a dataset by first developing applicable topics andemploying a crowdsourcing tool to collect annotated data. By annotating theEnglish side of the data and projecting the labels onto other languages throughaligned verses, we generate text classification datasets for more than 1500languages. We extensively benchmark several existing multilingual languagemodels using our dataset. To facilitate the advancement of research in thisarea, we will release our dataset and code.</description><author>Chunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, Renhao Pei, Ehsaneddin Asgari, Hinrich Schütze</author><pubDate>Tue, 04 Jun 2024 16:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08487v2</guid></item><item><title>PASOA- PArticle baSed Bayesian Optimal Adaptive design</title><link>http://arxiv.org/abs/2402.07160v2</link><description>We propose a new procedure named PASOA, for Bayesian experimental design,that performs sequential design optimization by simultaneously providingaccurate estimates of successive posterior distributions for parameterinference. The sequential design process is carried out via a contrastiveestimation principle, using stochastic optimization and Sequential Monte Carlo(SMC) samplers to maximise the Expected Information Gain (EIG). As largerinformation gains are obtained for larger distances between successiveposterior distributions, this EIG objective may worsen classical SMCperformance. To handle this issue, tempering is proposed to have both a largeinformation gain and an accurate SMC sampling, that we show is crucial forperformance. This novel combination of stochastic optimization and tempered SMCallows to jointly handle design optimization and parameter inference. Weprovide a proof that the obtained optimal design estimators benefit from someconsistency property. Numerical experiments confirm the potential of theapproach, which outperforms other recent existing procedures.</description><author>Jacopo Iollo, Christophe Heinkelé, Pierre Alliez, Florence Forbes</author><pubDate>Tue, 04 Jun 2024 16:01:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07160v2</guid></item><item><title>Low-Rank Adaption on Transformer-based Oriented Object Detector for Satellite Onboard Processing of Remote Sensing Images</title><link>http://arxiv.org/abs/2406.02385v1</link><description>Deep learning models in satellite onboard enable real-time interpretation ofremote sensing images, reducing the need for data transmission to the groundand conserving communication resources. As satellite numbers and observationfrequencies increase, the demand for satellite onboard real-time imageinterpretation grows, highlighting the expanding importance and development ofthis technology. However, updating the extensive parameters of models deployedon the satellites for spaceborne object detection model is challenging due tothe limitations of uplink bandwidth in wireless satellite communications. Toaddress this issue, this paper proposes a method based on parameter-efficientfine-tuning technology with low-rank adaptation (LoRA) module. It involvestraining low-rank matrix parameters and integrating them with the originalmodel's weight matrix through multiplication and summation, thereby fine-tuningthe model parameters to adapt to new data distributions with minimal weightupdates. The proposed method combines parameter-efficient fine-tuning with fullfine-tuning in the parameter update strategy of the oriented object detectionalgorithm architecture. This strategy enables model performance improvementsclose to full fine-tuning effects with minimal parameter updates. In addition,low rank approximation is conducted to pick an optimal rank value for LoRAmatrices. Extensive experiments verify the effectiveness of the proposedmethod. By fine-tuning and updating only 12.4$\%$ of the model's totalparameters, it is able to achieve 97$\%$ to 100$\%$ of the performance of fullfine-tuning models. Additionally, the reduced number of trainable parametersaccelerates model training iterations and enhances the generalization androbustness of the oriented object detection model. The source code is availableat: \url{https://github.com/fudanxu/LoRA-Det}.</description><author>Xinyang Pu, Feng Xu</author><pubDate>Tue, 04 Jun 2024 16:00:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02385v1</guid></item><item><title>Learning to Edit Visual Programs with Self-Supervision</title><link>http://arxiv.org/abs/2406.02383v1</link><description>We design a system that learns how to edit visual programs. Our edit networkconsumes a complete input program and a visual target. From this input, we taskour network with predicting a local edit operation that could be applied to theinput program to improve its similarity to the target. In order to apply thisscheme for domains that lack program annotations, we develop a self-supervisedlearning approach that integrates this edit network into a bootstrappedfinetuning loop along with a network that predicts entire programs in one-shot.Our joint finetuning scheme, when coupled with an inference procedure thatinitializes a population from the one-shot model and evolves members of thispopulation with the edit network, helps to infer more accurate visual programs.Over multiple domains, we experimentally compare our method against thealternative of using only the one-shot model, and find that even under equalsearch-time budgets, our editing-based paradigm provides significantadvantages.</description><author>R. Kenny Jones, Renhao Zhang, Aditya Ganeshan, Daniel Ritchie</author><pubDate>Tue, 04 Jun 2024 15:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02383v1</guid></item><item><title>Kirigami: large convolutional kernels improve deep learning-based RNA secondary structure prediction</title><link>http://arxiv.org/abs/2406.02381v1</link><description>We introduce a novel fully convolutional neural network (FCN) architecturefor predicting the secondary structure of ribonucleic acid (RNA) molecules.Interpreting RNA structures as weighted graphs, we employ deep learning toestimate the probability of base pairing between nucleotide residues. Unique toour model are its massive 11-pixel kernels, which we argue provide a distinctadvantage for FCNs on the specialized domain of RNA secondary structures. On awidely adopted, standardized test set comprised of 1,305 molecules, theaccuracy of our method exceeds that of current state-of-the-art (SOTA)secondary structure prediction software, achieving a Matthews CorrelationCoefficient (MCC) over 11-40% higher than that of other leading methods onoverall structures and 58-400% higher on pseudoknots specifically.</description><author>Marc Harary, Chengxin Zhang, Anna Marie Pyle</author><pubDate>Tue, 04 Jun 2024 15:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02381v1</guid></item><item><title>Dynamical Survival Analysis with Controlled Latent States</title><link>http://arxiv.org/abs/2401.17077v2</link><description>We consider the task of learning individual-specific intensities of countingprocesses from a set of static variables and irregularly sampled time series.We introduce a novel modelization approach in which the intensity is thesolution to a controlled differential equation. We first design a neuralestimator by building on neural controlled differential equations. In a secondtime, we show that our model can be linearized in the signature space undersufficient regularity conditions, yielding a signature-based estimator which wecall CoxSig. We provide theoretical learning guarantees for both estimators,before showcasing the performance of our models on a vast array of simulatedand real-world datasets from finance, predictive maintenance and food supplychain management.</description><author>Linus Bleistein, Van-Tuan Nguyen, Adeline Fermanian, Agathe Guilloux</author><pubDate>Tue, 04 Jun 2024 15:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17077v2</guid></item><item><title>EUFCC-340K: A Faceted Hierarchical Dataset for Metadata Annotation in GLAM Collections</title><link>http://arxiv.org/abs/2406.02380v1</link><description>In this paper, we address the challenges of automatic metadata annotation inthe domain of Galleries, Libraries, Archives, and Museums (GLAMs) byintroducing a novel dataset, EUFCC340K, collected from the Europeana portal.Comprising over 340,000 images, the EUFCC340K dataset is organized acrossmultiple facets: Materials, Object Types, Disciplines, and Subjects, followinga hierarchical structure based on the Art &amp; Architecture Thesaurus (AAT). Wedeveloped several baseline models, incorporating multiple heads on a ConvNeXTbackbone for multi-label image tagging on these facets, and fine-tuning a CLIPmodel with our image text pairs. Our experiments to evaluate model robustnessand generalization capabilities in two different test scenarios demonstrate theutility of the dataset in improving multi-label classification tools that havethe potential to alleviate cataloging tasks in the cultural heritage sector.</description><author>Francesc Net, Marc Folia, Pep Casals, Andrew D. Bagdanov, Lluis Gomez</author><pubDate>Tue, 04 Jun 2024 15:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02380v1</guid></item><item><title>On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept</title><link>http://arxiv.org/abs/2406.02378v1</link><description>Large Language Models (LLMs) can improve their responses when instructed todo so, a capability known as self-correction. When these instructions lackspecific details about the issues in the response, this is referred to asleveraging the intrinsic self-correction capability. The empirical success ofself-correction can be found in various applications, e.g., text detoxificationand social bias mitigation. However, leveraging this self-correction capabilitymay not always be effective, as it has the potential to revise an initiallycorrect response into an incorrect one. In this paper, we endeavor tounderstand how and why leveraging the self-correction capability is effective.We identify that appropriate instructions can guide LLMs to a convergencestate, wherein additional self-correction steps do not yield furtherperformance improvements. We empirically demonstrate that model uncertainty andactivated latent concepts jointly characterize the effectiveness ofself-correction. Furthermore, we provide a mathematical formulation indicatingthat the activated latent concept drives the convergence of the modeluncertainty and self-correction performance. Our analysis can also begeneralized to the self-correction behaviors observed in Vision-Language Models(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit fromour principle in terms of selecting effective fine-tuning samples. Such initialsuccess demonstrates the potential extensibility for better instruction tuningand safety alignment.</description><author>Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Kristen Johnson, Jiliang Tang, Rongrong Wang</author><pubDate>Tue, 04 Jun 2024 15:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02378v1</guid></item><item><title>XRec: Large Language Models for Explainable Recommendation</title><link>http://arxiv.org/abs/2406.02377v1</link><description>Recommender systems help users navigate information overload by providingpersonalized recommendations aligned with their preferences. CollaborativeFiltering (CF) is a widely adopted approach, but while advanced techniques likegraph neural networks (GNNs) and self-supervised learning (SSL) have enhancedCF models for better user representations, they often lack the ability toprovide explanations for the recommended items. Explainable recommendations aimto address this gap by offering transparency and insights into therecommendation decision-making process, enhancing users' understanding. Thiswork leverages the language capabilities of Large Language Models (LLMs) topush the boundaries of explainable recommender systems. We introduce amodel-agnostic framework called XRec, which enables LLMs to providecomprehensive explanations for user behaviors in recommender systems. Byintegrating collaborative signals and designing a lightweight collaborativeadaptor, the framework empowers LLMs to understand complex patterns inuser-item interactions and gain a deeper understanding of user preferences. Ourextensive experiments demonstrate the effectiveness of XRec, showcasing itsability to generate comprehensive and meaningful explanations that outperformbaseline approaches in explainable recommender systems. We open-source ourmodel implementation at https://github.com/HKUDS/XRec.</description><author>Qiyao Ma, Xubin Ren, Chao Huang</author><pubDate>Tue, 04 Jun 2024 15:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02377v1</guid></item><item><title>Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs</title><link>http://arxiv.org/abs/2406.02376v1</link><description>The growing popularity of Large Language Models has sparked interest incontext compression for Large Language Models (LLMs). However, the performanceof previous methods degrades dramatically as compression ratios increase,sometimes even falling to the closed-book level. This decline can be attributedto the loss of key information during the compression process. Our preliminarystudy supports this hypothesis, emphasizing the significance of retaining keyinformation to maintain model performance under high compression ratios. As aresult, we introduce Query-Guided Compressor (QGC), which leverages queries toguide the context compression process, effectively preserving key informationwithin the compressed context. Additionally, we employ a dynamic compressionstrategy. We validate the effectiveness of our proposed QGC on the QuestionAnswering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets.Experimental results show that QGC can consistently perform well even at highcompression ratios, which also offers significant benefits in terms ofinference cost and throughput.</description><author>Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, Jinsong Su</author><pubDate>Tue, 04 Jun 2024 15:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02376v1</guid></item><item><title>An Empirical Analysis on Large Language Models in Debate Evaluation</title><link>http://arxiv.org/abs/2406.00050v2</link><description>In this study, we investigate the capabilities and inherent biases ofadvanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the contextof debate evaluation. We discover that LLM's performance exceeds humans andsurpasses the performance of state-of-the-art methods fine-tuned on extensivedatasets in debate evaluation. We additionally explore and analyze biasespresent in LLMs, including positional bias, lexical bias, order bias, which mayaffect their evaluative judgments. Our findings reveal a consistent bias inboth GPT-3.5 and GPT-4 towards the second candidate response presented,attributed to prompt design. We also uncover lexical biases in both GPT-3.5 andGPT-4, especially when label sets carry connotations such as numerical orsequential, highlighting the critical need for careful label verbalizerselection in prompt design. Additionally, our analysis indicates a tendency ofboth models to favor the debate's concluding side as the winner, suggesting anend-of-discussion bias.</description><author>Xinyi Liu, Pinxin Liu, Hangfeng He</author><pubDate>Tue, 04 Jun 2024 15:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00050v2</guid></item><item><title>NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism</title><link>http://arxiv.org/abs/2403.00862v4</link><description>We present NewsBench, a novel evaluation framework to systematically assessthe capabilities of Large Language Models (LLMs) for editorial capabilities inChinese journalism. Our constructed benchmark dataset is focused on four facetsof writing proficiency and six facets of safety adherence, and it comprisesmanually and carefully designed 1,267 test samples in the types of multiplechoice questions and short answer questions for five editorial tasks in 24 newsdomains. To measure performances, we propose different GPT-4 based automaticevaluation protocols to assess LLM generations for short answer questions interms of writing proficiency and safety adherence, and both are validated bythe high correlations with human evaluations. Based on the systematicevaluation framework, we conduct a comprehensive analysis of ten popular LLMswhich can handle Chinese. The experimental results highlight GPT-4 and ERNIEBot as top performers, yet reveal a relative deficiency in journalistic safetyadherence in creative writing tasks. Our findings also underscore the need forenhanced ethical guidance in machine-generated journalistic content, marking astep forward in aligning LLMs with journalistic standards and safetyconsiderations.</description><author>Miao Li, Ming-Bin Chen, Bo Tang, Shengbin Hou, Pengyu Wang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Keming Mao, Peng Cheng, Yi Luo</author><pubDate>Tue, 04 Jun 2024 15:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00862v4</guid></item><item><title>Improving Transformers with Dynamically Composable Multi-Head Attention</title><link>http://arxiv.org/abs/2405.08553v2</link><description>Multi-Head Attention (MHA) is a key component of Transformer. In MHA,attention heads work independently, causing problems such as low-rankbottleneck of attention score matrices and head redundancy. We proposeDynamically Composable Multi-Head Attention (DCMHA), a parameter andcomputation efficient attention architecture that tackles the shortcomings ofMHA and increases the expressive power of the model by dynamically composingattention heads. At the core of DCMHA is a $\it{Compose}$ function thattransforms the attention score and weight matrices in an input-dependent way.DCMHA can be used as a drop-in replacement of MHA in any transformerarchitecture to obtain the corresponding DCFormer. DCFormer significantlyoutperforms Transformer on different architectures and model scales in languagemodeling, matching the performance of models with ~1.7x-2.0x compute. Forexample, DCPythia-6.9B outperforms open source Pythia-12B on both pretrainingperplexity and downstream task evaluation. The code and models are available athttps://github.com/Caiyun-AI/DCFormer.</description><author>Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan</author><pubDate>Tue, 04 Jun 2024 15:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08553v2</guid></item><item><title>SuperGaussian: Repurposing Video Models for 3D Super Resolution</title><link>http://arxiv.org/abs/2406.00609v2</link><description>We present a simple, modular, and generic method that upsamples coarse 3Dmodels by adding geometric and appearance details. While generative 3D modelsnow exist, they do not yet match the quality of their counterparts in image andvideo domains. We demonstrate that it is possible to directly repurposeexisting (pretrained) video models for 3D super-resolution and thus sidestepthe problem of the shortage of large repositories of high-quality 3D trainingmodels. We describe how to repurpose video upsampling models, which are not 3Dconsistent, and combine them with 3D consolidation to produce 3D-consistentresults. As output, we produce high quality Gaussian Splat models, which areobject centric and effective. Our method is category agnostic and can be easilyincorporated into existing 3D workflows. We evaluate our proposed SuperGaussianon a variety of 3D inputs, which are diverse both in terms of complexity andrepresentation (e.g., Gaussian Splats or NeRFs), and demonstrate that oursimple method significantly improves the fidelity of the final 3D models. Checkour project website for details: supergaussian.github.io</description><author>Yuan Shen, Duygu Ceylan, Paul Guerrero, Zexiang Xu, Niloy J. Mitra, Shenlong Wang, Anna Frühstück</author><pubDate>Tue, 04 Jun 2024 15:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00609v2</guid></item><item><title>Large Language Models Make Sample-Efficient Recommender Systems</title><link>http://arxiv.org/abs/2406.02368v1</link><description>Large language models (LLMs) have achieved remarkable progress in the fieldof natural language processing (NLP), demonstrating remarkable abilities inproducing text that resembles human language for various tasks. This opens upnew opportunities for employing them in recommender systems (RSs). In thispaper, we specifically examine the sample efficiency of LLM-enhancedrecommender systems, which pertains to the model's capacity to attain superiorperformance with a limited quantity of training data. Conventionalrecommendation models (CRMs) often need a large amount of training data becauseof the sparsity of features and interactions. Hence, we propose and verify ourcore viewpoint: Large Language Models Make Sample-Efficient RecommenderSystems. We propose a simple yet effective framework (i.e., Laser) to validatethe viewpoint from two aspects: (1) LLMs themselves are sample-efficientrecommenders; and (2) LLMs, as feature generators and encoders, make CRMs moresample-efficient. Extensive experiments on two public datasets show that Laserrequires only a small fraction of training samples to match or even surpassCRMs that are trained on the entire training set, demonstrating superior sampleefficiency.</description><author>Jianghao Lin, Xinyi Dai, Rong Shan, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang</author><pubDate>Tue, 04 Jun 2024 15:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02368v1</guid></item><item><title>Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models</title><link>http://arxiv.org/abs/2406.02366v1</link><description>Diffusion models (DMs) produce very detailed and high-quality images. Theirpower results from extensive training on large amounts of data, usually scrapedfrom the internet without proper attribution or consent from content creators.Unfortunately, this practice raises privacy and intellectual property concerns,as DMs can memorize and later reproduce their potentially sensitive orcopyrighted training images at inference time. Prior efforts prevent this issueby either changing the input to the diffusion process, thereby preventing theDM from generating memorized samples during inference, or removing thememorized data from training altogether. While those are viable solutions whenthe DM is developed and deployed in a secure and constantly monitoredenvironment, they hold the risk of adversaries circumventing the safeguards andare not effective when the DM itself is publicly released. To solve theproblem, we introduce NeMo, the first method to localize memorization ofindividual data samples down to the level of neurons in DMs' cross-attentionlayers. Through our experiments, we make the intriguing finding that in manycases, single neurons are responsible for memorizing particular trainingsamples. By deactivating these memorization neurons, we can avoid thereplication of training data at inference time, increase the diversity in thegenerated outputs, and mitigate the leakage of private and copyrighted data. Inthis way, our NeMo contributes to a more responsible deployment of DMs.</description><author>Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, Franziska Boenisch</author><pubDate>Tue, 04 Jun 2024 15:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02366v1</guid></item><item><title>Temporal Graph Rewiring with Expander Graphs</title><link>http://arxiv.org/abs/2406.02362v1</link><description>Evolving relations in real-world networks are often modelled by temporalgraphs. Graph rewiring techniques have been utilised on Graph Neural Networks(GNNs) to improve expressiveness and increase model performance. In this work,we propose Temporal Graph Rewiring (TGR), the first approach for graph rewiringon temporal graphs. TGR enables communication between temporally distant nodesin a continuous time dynamic graph by utilising expander graph propagation toconstruct a message passing highway for message passing between distant nodes.Expander graphs are suitable candidates for rewiring as they help overcome theoversquashing problem often observed in GNNs. On the public tgbl-wikibenchmark, we show that TGR improves the performance of a widely used TGN modelby a significant margin. Our code repository is accessible athttps://anonymous.4open.science/r/TGR-254C.</description><author>Katarina Petrović, Shenyang Huang, Farimah Poursafaei, Petar Veličković</author><pubDate>Tue, 04 Jun 2024 15:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02362v1</guid></item><item><title>Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models</title><link>http://arxiv.org/abs/2402.18099v2</link><description>Model editing aims to precisely alter the behaviors of large language models(LLMs) in relation to specific knowledge, while leaving unrelated knowledgeintact. This approach has proven effective in addressing issues ofhallucination and outdated information in LLMs. However, the potential of usingmodel editing to modify knowledge in the medical field remains largelyunexplored, even though resolving hallucination is a pressing need in thisarea. Our observations indicate that current methods face significantchallenges in dealing with specialized and complex knowledge in medical domain.Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy formedical model editing. MedLaSA harnesses the strengths of both adding extraparameters and locate-then-edit methods for medical model editing. We utilizecausal tracing to identify the association of knowledge in neurons acrossdifferent layers, and generate a corresponding scale set from the associationvalue for each piece of knowledge. Subsequently, we incorporate scalableadapters into the dense layers of LLMs. These adapters are assigned scalingvalues based on the corresponding specific knowledge, which allows for theadjustment of the adapter's weight and rank. The more similar the content, themore consistent the scale between them. This ensures precise editing ofsemantically identical knowledge while avoiding impact on unrelated knowledge.To evaluate the editing impact on the behaviours of LLMs, we propose two modelediting studies for medical domain: (1) editing factual knowledge for medicalspecialization and (2) editing the explanatory ability for complex knowledge.We build two novel medical benchmarking datasets and introduce a series ofchallenging and comprehensive metrics. Extensive experiments on medical LLMsdemonstrate the editing efficiency of MedLaSA, without affecting unrelatedknowledge.</description><author>Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu, Tong Xu, Wanyu Wang, Yuyang Ye, Xiangyu Zhao, Yefeng Zheng, Enhong Chen</author><pubDate>Tue, 04 Jun 2024 15:38:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18099v2</guid></item><item><title>Using Self-supervised Learning Can Improve Model Fairness</title><link>http://arxiv.org/abs/2406.02361v1</link><description>Self-supervised learning (SSL) has become the de facto training paradigm oflarge models, where pre-training is followed by supervised fine-tuning usingdomain-specific data and labels. Despite demonstrating comparable performancewith supervised methods, comprehensive efforts to assess SSL's impact onmachine learning fairness (i.e., performing equally on different demographicbreakdowns) are lacking. Hypothesizing that SSL models would learn moregeneric, hence less biased representations, this study explores the impact ofpre-training and fine-tuning strategies on fairness. We introduce a fairnessassessment framework for SSL, comprising five stages: defining datasetrequirements, pre-training, fine-tuning with gradual unfreezing, assessingrepresentation similarity conditioned on demographics, and establishingdomain-specific evaluation processes. We evaluate our method's generalizabilityon three real-world human-centric datasets (i.e., MIMIC, MESA, and GLOBEM) bysystematically comparing hundreds of SSL and fine-tuned models on variousdimensions spanning from the intermediate representations to appropriateevaluation metrics. Our findings demonstrate that SSL can significantly improvemodel fairness, while maintaining performance on par with supervisedmethods-exhibiting up to a 30% increase in fairness with minimal loss inperformance through self-supervision. We posit that such differences can beattributed to representation dissimilarities found between the best- and theworst-performing demographics across models-up to x13 greater for protectedattributes with larger performance discrepancies between segments.</description><author>Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Athena Vakali, Daniele Quercia, Fahim Kawsar</author><pubDate>Tue, 04 Jun 2024 15:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02361v1</guid></item><item><title>The complexity of approximate (coarse) correlated equilibrium for incomplete information games</title><link>http://arxiv.org/abs/2406.02357v1</link><description>We study the iteration complexity of decentralized learning of approximatecorrelated equilibria in incomplete information games. On the negative side, we prove that in $\mathit{extensive}$-$\mathit{form}$$\mathit{games}$, assuming $\mathsf{PPAD} \not\subset\mathsf{TIME}(n^{\mathsf{polylog}(n)})$, any polynomial-time learningalgorithms must take at least $2^{\log_2^{1-o(1)}(|\mathcal{I}|)}$ iterationsto converge to the set of $\epsilon$-approximate correlated equilibrium, where$|\mathcal{I}|$ is the number of nodes in the game and $\epsilon &gt; 0$ is anabsolute constant. This nearly matches, up to the $o(1)$ term, the algorithmsof [PR'24, DDFG'24] for learning $\epsilon$-approximate correlated equilibrium,and resolves an open question of Anagnostides, Kalavasis, Sandholm, andZampetakis [AKSZ'24]. Our lower bound holds even for the easier solutionconcept of $\epsilon$-approximate $\mathit{coarse}$ correlated equilibrium On the positive side, we give uncoupled dynamics that reach$\epsilon$-approximate correlated equilibria of a $\mathit{Bayesian}$$\mathit{game}$ in polylogarithmic iterations, without any dependence of thenumber of types. This demonstrates a separation between Bayesian games andextensive-form games.</description><author>Binghui Peng, Aviad Rubinstein</author><pubDate>Tue, 04 Jun 2024 15:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02357v1</guid></item><item><title>Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks</title><link>http://arxiv.org/abs/2406.02356v1</link><description>The ability (and inability) of large language models (LLMs) to performarithmetic tasks has been the subject of much theoretical and practical debate.We show that LLMs are frequently able to correctly and confidently predict thefirst digit of n-digit by m-digit multiplication tasks without using chain ofthought reasoning, despite these tasks require compounding operations to solve.Simultaneously, LLMs in practice often fail to correctly or confidently predictthe last digit of an n-digit by m-digit multiplication, a task equivalent to1-digit by 1-digit multiplication which can be easily learned or memorized. Weshow that the latter task can be solved more robustly when the LLM isconditioned on all of the correct higher-order digits, which on averageincreases the confidence of the correct last digit on 5-digit by 5-digitmultiplication tasks using Llama 2-13B by over 230% (0.13 to 0.43) andMistral-7B by 150% (0.22 to 0.55).</description><author>Andrew Gambardella, Yusuke Iwasawa, Yutaka Matsuo</author><pubDate>Tue, 04 Jun 2024 15:34:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02356v1</guid></item><item><title>Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback</title><link>http://arxiv.org/abs/2404.10271v2</link><description>Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwiseproblematic behavior, such as helping to commit crimes or producing racisttext. One approach to fine-tuning, called reinforcement learning from humanfeedback, learns from humans' expressed preferences over multiple outputs.Another approach is constitutional AI, in which the input from humans is a listof high-level principles. But how do we deal with potentially diverging inputfrom humans? How can we aggregate the input into consistent data about"collective" preferences or otherwise use it to make collective choices aboutmodel behavior? In this paper, we argue that the field of social choice is wellpositioned to address these questions, and we discuss ways forward for thisagenda, drawing on discussions in a recent workshop on Social Choice for AIEthics and Safety held in Berkeley, CA, USA in December 2023.</description><author>Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, William S. Zwicker</author><pubDate>Tue, 04 Jun 2024 15:34:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10271v2</guid></item><item><title>FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning</title><link>http://arxiv.org/abs/2406.02355v1</link><description>Federated Learning (FL) has emerged as a pivotal framework for thedevelopment of effective global models (global FL) or personalized models(personalized FL) across clients with heterogeneous, non-iid data distribution.A key challenge in FL is client drift, where data heterogeneity impedes theaggregation of scattered knowledge. Recent studies have tackled the clientdrift issue by identifying significant divergence in the last classifier layer.To mitigate this divergence, strategies such as freezing the classifier weightsand aligning the feature extractor accordingly have proven effective. Althoughthe local alignment between classifier and feature extractor has been studiedas a crucial factor in FL, we observe that it may lead the model tooveremphasize the observed classes within each client. Thus, our objectives aretwofold: (1) enhancing local alignment while (2) preserving the representationof unseen class samples. This approach aims to effectively integrate knowledgefrom individual clients, thereby improving performance for both global andpersonalized FL. To achieve this, we introduce a novel algorithm named FedDr+,which empowers local model alignment using dot-regression loss. FedDr+ freezesthe classifier as a simplex ETF to align the features and improves aggregatedglobal models by employing a feature distillation mechanism to retaininformation about unseen/missing classes. Consequently, we provide empiricalevidence demonstrating that our algorithm surpasses existing methods that use afrozen classifier to boost alignment across the diverse distribution.</description><author>Seongyoon Kim, Minchan Jeong, Sungnyun Kim, Sungwoo Cho, Sumyeong Ahn, Se-Young Yun</author><pubDate>Tue, 04 Jun 2024 15:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02355v1</guid></item><item><title>Label-wise Aleatoric and Epistemic Uncertainty Quantification</title><link>http://arxiv.org/abs/2406.02354v1</link><description>We present a novel approach to uncertainty quantification in classificationtasks based on label-wise decomposition of uncertainty measures. Thislabel-wise perspective allows uncertainty to be quantified at the individualclass level, thereby improving cost-sensitive decision-making and helpingunderstand the sources of uncertainty. Furthermore, it allows to define total,aleatoric, and epistemic uncertainty on the basis of non-categorical measuressuch as variance, going beyond common entropy-based measures. In particular,variance-based measures address some of the limitations associated withestablished methods that have recently been discussed in the literature. Weshow that our proposed measures adhere to a number of desirable properties.Through empirical evaluation on a variety of benchmark data sets -- includingapplications in the medical domain where accurate uncertainty quantification iscrucial -- we establish the effectiveness of label-wise uncertaintyquantification.</description><author>Yusuf Sale, Paul Hofman, Timo Löhr, Lisa Wimmer, Thomas Nagler, Eyke Hüllermeier</author><pubDate>Tue, 04 Jun 2024 15:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02354v1</guid></item><item><title>System-Aware Neural ODE Processes for Few-Shot Bayesian Optimization</title><link>http://arxiv.org/abs/2406.02352v1</link><description>We consider the problem of optimizing initial conditions and timing indynamical systems governed by unknown ordinary differential equations (ODEs),where evaluating different initial conditions is costly and there areconstraints on observation times. To identify the optimal conditions withinseveral trials, we introduce a few-shot Bayesian Optimization (BO) frameworkbased on the system's prior information. At the core of our approach is theSystem-Aware Neural ODE Processes (SANODEP), an extension of Neural ODEProcesses (NODEP) designed to meta-learn ODE systems from multiple trajectoriesusing a novel context embedding block. Additionally, we propose amulti-scenario loss function specifically for optimization purposes. Ourtwo-stage BO framework effectively incorporates search space constraints,enabling efficient optimization of both initial conditions and observationtimings. We conduct extensive experiments showcasing SANODEP's potential forfew-shot BO. We also explore SANODEP's adaptability to varying levels of priorinformation, highlighting the trade-off between prior flexibility and modelfitting accuracy.</description><author>Jixiang Qing, Becky D Langdon, Robert M Lee, Behrang Shafei, Mark van der Wilk, Calvin Tsay, Ruth Misener</author><pubDate>Tue, 04 Jun 2024 15:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02352v1</guid></item><item><title>On the Identifiability of Switching Dynamical Systems</title><link>http://arxiv.org/abs/2305.15925v4</link><description>The identifiability of latent variable models has received increasingattention due to its relevance in interpretability and out-of-distributiongeneralisation. In this work, we study the identifiability of SwitchingDynamical Systems, taking an initial step toward extending identifiabilityanalysis to sequential latent variable models. We first prove theidentifiability of Markov Switching Models, which commonly serve as the priordistribution for the continuous latent variables in Switching DynamicalSystems. We present identification conditions for first-order Markov dependencystructures, whose transition distribution is parametrised via non-linearGaussians. We then establish the identifiability of the latent variables andnon-linear mappings in Switching Dynamical Systems up to affinetransformations, by leveraging identifiability analysis techniques fromidentifiable deep latent variable models. We finally develop estimationalgorithms for identifiable Switching Dynamical Systems. Throughout empiricalstudies, we demonstrate the practicality of identifiable Switching DynamicalSystems for segmenting high-dimensional time series such as videos, andshowcase the use of identifiable Markov Switching Models for regime-dependentcausal discovery in climate data.</description><author>Carles Balsells-Rodas, Yixin Wang, Yingzhen Li</author><pubDate>Tue, 04 Jun 2024 15:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15925v4</guid></item><item><title>CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models</title><link>http://arxiv.org/abs/2402.13109v2</link><description>The advancement of large language models (LLMs) has enhanced the ability togeneralize across a wide range of unseen natural language processing (NLP)tasks through instruction-following. Yet, their effectiveness often diminishesin low-resource languages like Chinese, exacerbated by biased evaluations fromdata leakage, casting doubt on their true generalizability to new linguisticterritories. In response, we introduce the Chinese Instruction-FollowingBenchmark (CIF-Bench), designed to evaluate the zero-shot generalizability ofLLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000input-output pairs, developed by native speakers to test complex reasoning andChinese cultural nuances across 20 categories. To mitigate data contamination,we release only half of the dataset publicly, with the remainder kept private,and introduce diversified instructions to minimize score variance, totaling45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeableperformance gap, with the best model scoring only 52.9%, highlighting thelimitations of LLMs in less familiar language and task contexts. This work notonly uncovers the current limitations of LLMs in handling Chinese languagetasks but also sets a new standard for future LLM generalizability research,pushing towards the development of more adaptable, culturally informed, andlinguistically diverse models.</description><author>Yizhi LI, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Stephen W. Huang, Chenghua Lin, Jie Fu</author><pubDate>Tue, 04 Jun 2024 15:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13109v2</guid></item><item><title>A-SDM: Accelerating Stable Diffusion through Model Assembly and Feature Inheritance Strategies</title><link>http://arxiv.org/abs/2406.00210v2</link><description>The Stable Diffusion Model (SDM) is a prevalent and effective model fortext-to-image (T2I) and image-to-image (I2I) generation. Despite variousattempts at sampler optimization, model distillation, and networkquantification, these approaches typically maintain the original networkarchitecture. The extensive parameter scale and substantial computationaldemands have limited research into adjusting the model architecture. This studyfocuses on reducing redundant computation in SDM and optimizes the modelthrough both tuning and tuning-free methods. 1) For the tuning method, wedesign a model assembly strategy to reconstruct a lightweight model whilepreserving performance through distillation. Second, to mitigate performanceloss due to pruning, we incorporate multi-expert conditional convolution(ME-CondConv) into compressed UNets to enhance network performance byincreasing capacity without sacrificing speed. Third, we validate theeffectiveness of the multi-UNet switching method for improving network speed.2) For the tuning-free method, we propose a feature inheritance strategy toaccelerate inference by skipping local computations at the block, layer, orunit level within the network structure. We also examine multiple samplingmodes for feature inheritance at the time-step level. Experiments demonstratethat both the proposed tuning and the tuning-free methods can improve the speedand performance of the SDM. The lightweight model reconstructed by the modelassembly strategy increases generation speed by $22.4%$, while the featureinheritance strategy enhances the SDM generation speed by $40.0%$.</description><author>Jinchao Zhu, Yuxuan Wang, Siyuan Pan, Pengfei Wan, Di Zhang, Gao Huang</author><pubDate>Tue, 04 Jun 2024 15:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00210v2</guid></item><item><title>Evaluating ChatGPT as a Recommender System: A Rigorous Approach</title><link>http://arxiv.org/abs/2309.03613v2</link><description>Large Language Models (LLMs) have recently shown impressive abilities inhandling various natural language-related tasks. Among different LLMs, currentstudies have assessed ChatGPT's superior performance across manifold tasks,especially under the zero/few-shot prompting conditions. Given such successes,the Recommender Systems (RSs) research community have started investigating itspotential applications within the recommendation scenario. However, althoughvarious methods have been proposed to integrate ChatGPT's capabilities intoRSs, current research struggles to comprehensively evaluate such models whileconsidering the peculiarities of generative models. Often, evaluations do notconsider hallucinations, duplications, and out-of-the-closed domainrecommendations and solely focus on accuracy metrics, neglecting the impact onbeyond-accuracy facets. To bridge this gap, we propose a robust evaluationpipeline to assess ChatGPT's ability as an RS and post-process ChatGPTrecommendations to account for these aspects. Through this pipeline, weinvestigate ChatGPT-3.5 and ChatGPT-4 performance in the recommendation taskunder the zero-shot condition employing the role-playing prompt. We analyze themodel's functionality in three settings: the Top-N Recommendation, thecold-start recommendation, and the re-ranking of a list of recommendations, andin three domains: movies, music, and books. The experiments reveal that ChatGPTexhibits higher accuracy than the baselines on books domain. It also excels inre-ranking and cold-start scenarios while maintaining reasonablebeyond-accuracy metrics. Furthermore, we measure the similarity between theChatGPT recommendations and the other recommenders, providing insights abouthow ChatGPT could be categorized in the realm of recommender systems. Theevaluation pipeline is publicly released for future research.</description><author>Dario Di Palma, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, Eugenio Di Sciascio</author><pubDate>Tue, 04 Jun 2024 15:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03613v2</guid></item><item><title>LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing</title><link>http://arxiv.org/abs/2406.02350v1</link><description>Large language models (LLMs) have shown amazing capabilities in knowledgememorization and present. However, when it comes to domain-specific knowledgeand downstream tasks like medical, general LLMs are often unable to giveprecise answers. In addition, when people want LLMs to answer classificationquestions, they usually go through instruction tuning first, however, LLMs donot always give a direct index of the categorization after instruction tuning.In this paper, we proposed LlamaCare, a fine-tuned medical language model, andExtended Classification Integration(ECI), a module to handle classificationproblems of LLMs. Our contributions are : (i) We fine-tuned a large languagemodel of medical knowledge with very low carbon emissions and achieved similarperformance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundantcategorical answers and improved the performance of LLMs by proposing a newmodule called Extended Classification Integration. (iii) We released ourprocessed data for one-shot and few-shot training for some benchmarks such asPubMedQA and USMLE 1-3 step. Our method achieves a close effect with thestate-of-the-art model in benchmarks while costing lower GPU resources comparedto LLMs with the same quantity of parameters. Our models, codes, and datasetscan be found in https://github.com/Stephen-SMJ/LLamaCare</description><author>Maojun Sun</author><pubDate>Tue, 04 Jun 2024 15:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02350v1</guid></item><item><title>CADE: Cosine Annealing Differential Evolution for Spiking Neural Network</title><link>http://arxiv.org/abs/2406.02349v1</link><description>Spiking neural networks (SNNs) have gained prominence for their potential inneuromorphic computing and energy-efficient artificial intelligence, yetoptimizing them remains a formidable challenge for gradient-based methods dueto their discrete, spike-based computation. This paper attempts to tackle thechallenges by introducing Cosine Annealing Differential Evolution (CADE),designed to modulate the mutation factor (F) and crossover rate (CR) ofdifferential evolution (DE) for the SNN model, i.e., Spiking Element Wise (SEW)ResNet. Extensive empirical evaluations were conducted to analyze CADE. CADEshowed a balance in exploring and exploiting the search space, resulting inaccelerated convergence and improved accuracy compared to existinggradient-based and DE-based methods. Moreover, an initialization method basedon a transfer learning setting was developed, pretraining on a source dataset(i.e., CIFAR-10) and fine-tuning the target dataset (i.e., CIFAR-100), toimprove population diversity. It was found to further enhance CADE for SNN.Remarkably, CADE elevates the performance of the highest accuracy SEW model byan additional 0.52 percentage points, underscoring its effectiveness infine-tuning and enhancing SNNs. These findings emphasize the pivotal role of ascheduler for F and CR adjustment, especially for DE-based SNN. Source Code onGithub: https://github.com/Tank-Jiang/CADE4SNN.</description><author>Runhua Jiang, Guodong Du, Shuyang Yu, Yifei Guo, Sim Kuan Goh, Ho-Kin Tang</author><pubDate>Tue, 04 Jun 2024 15:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02349v1</guid></item><item><title>AMOSL: Adaptive Modality-wise Structure Learning in Multi-view Graph Neural Networks For Enhanced Unified Representation</title><link>http://arxiv.org/abs/2406.02348v1</link><description>While Multi-view Graph Neural Networks (MVGNNs) excel at leveraging diversemodalities for learning object representation, existing methods assumeidentical local topology structures across modalities that overlook real-worlddiscrepancies. This leads MVGNNs straggles in modality fusion andrepresentations denoising. To address these issues, we propose adaptivemodality-wise structure learning (AMoSL). AMoSL captures node correspondencesbetween modalities via optimal transport, and jointly learning with graphembedding. To enable efficient end-to-end training, we employ an efficientsolution for the resulting complex bilevel optimization problem. Furthermore,AMoSL adapts to downstream tasks through unsupervised learning oninter-modality distances. The effectiveness of AMoSL is demonstrated by itsability to train more accurate graph classifiers on six benchmark datasets.</description><author>Peiyu Liang, Hongchang Gao, Xubin He</author><pubDate>Tue, 04 Jun 2024 15:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02348v1</guid></item><item><title>Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models</title><link>http://arxiv.org/abs/2402.14007v2</link><description>Text watermarking technology aims to tag and identify content produced bylarge language models (LLMs) to prevent misuse. In this study, we introduce theconcept of cross-lingual consistency in text watermarking, which assesses theability of text watermarks to maintain their effectiveness after beingtranslated into other languages. Preliminary empirical results from two LLMsand three watermarking methods reveal that current text watermarkingtechnologies lack consistency when texts are translated into various languages.Based on this observation, we propose a Cross-lingual Watermark Removal Attack(CWRA) to bypass watermarking by first obtaining a response from an LLM in apivot language, which is then translated into the target language. CWRA caneffectively remove watermarks, decreasing the AUCs to a random-guessing levelwithout performance loss. Furthermore, we analyze two key factors thatcontribute to the cross-lingual consistency in text watermarking and proposeX-SIR as a defense method against CWRA. Code: https://github.com/zwhe99/X-SIR.</description><author>Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang</author><pubDate>Tue, 04 Jun 2024 15:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14007v2</guid></item><item><title>Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation</title><link>http://arxiv.org/abs/2406.02347v1</link><description>In this paper, we propose an efficient, fast, and versatile distillationmethod to accelerate the generation of pre-trained diffusion models: FlashDiffusion. The method reaches state-of-the-art performances in terms of FID andCLIP-Score for few steps image generation on the COCO2014 and COCO2017datasets, while requiring only several GPU hours of training and fewertrainable parameters than existing methods. In addition to its efficiency, theversatility of the method is also exposed across several tasks such astext-to-image, inpainting, face-swapping, super-resolution and using differentbackbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\alpha$),as well as adapters. In all cases, the method allowed to reduce drastically thenumber of sampling steps while maintaining very high-quality image generation.The official implementation is available athttps://github.com/gojasper/flash-diffusion.</description><author>Clement Chadebec, Onur Tasar, Eyal Benaroche, Benjamin Aubin</author><pubDate>Tue, 04 Jun 2024 15:23:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02347v1</guid></item><item><title>Progressive Confident Masking Attention Network for Audio-Visual Segmentation</title><link>http://arxiv.org/abs/2406.02345v1</link><description>Audio and visual signals typically occur simultaneously, and humans possessan innate ability to correlate and synchronize information from these twomodalities. Recently, a challenging problem known as Audio-Visual Segmentation(AVS) has emerged, intending to produce segmentation maps for sounding objectswithin a scene. However, the methods proposed so far have not sufficientlyintegrated audio and visual information, and the computational costs have beenextremely high. Additionally, the outputs of different stages have not beenfully utilized. To facilitate this research, we introduce a novel ProgressiveConfident Masking Attention Network (PMCANet). It leverages attentionmechanisms to uncover the intrinsic correlations between audio signals andvisual frames. Furthermore, we design an efficient and effectivecross-attention module to enhance semantic perception by selecting querytokens. This selection is determined through confidence-driven units based onthe network's multi-stage predictive outputs. Experiments demonstrate that ournetwork outperforms other AVS methods while requiring less computationalresources.</description><author>Yuxuan Wang, Feng Dong, Jinchao Zhu</author><pubDate>Tue, 04 Jun 2024 15:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02345v1</guid></item><item><title>PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning</title><link>http://arxiv.org/abs/2310.00901v3</link><description>Instruction tuning enhances the instruction following ability of largelanguage models by finetuning with supervised instruction data. Previous workproposes in-context instruction tuning (ICIT) where specific positive ornegative examples are incorporated into the prompt for better performance. Inthis work, we propose PACIT, a simple and effective in-context instructiontuning method, inspired by the pedagogical concept of desirable difficulty. ThePACIT method unlocks the power of examples by encouraging the model to activelylearn to grasp the distinctions between the positive and negative examplesinstead of merely reading. The model is expected to first verify thecorrectness of the provided example according to the task description, which isthen set as the condition for generating a better response to the taskinstance. Our extensive experiments prove the effectiveness of PACIT,outperforming ICIT baseline on both in-domain and out-domain tasks up to 9.16and 3.14 average ROUGE-L scores, respectively. Moreover, PACIT can notablyenhance the performance of instruction tuning even when all positive andnegative examples are generated with a self-instruct method.</description><author>Tianci Xue, Ziqi Wang, Yixia Li, Yun Chen, Guanhua Chen</author><pubDate>Tue, 04 Jun 2024 15:21:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00901v3</guid></item><item><title>Incorporating Navigation Context into Inland Vessel Trajectory Prediction: A Gaussian Mixture Model and Transformer Approach</title><link>http://arxiv.org/abs/2406.02344v1</link><description>Using data sources beyond the Automatic Identification System to representthe context a vessel is navigating in and consequently improve situationawareness is still rare in machine learning approaches to vessel trajectoryprediction (VTP). In inland shipping, where vessel movement is constrainedwithin fairways, navigational context information is indispensable. In thiscontribution targeting inland VTP, Gaussian Mixture Models (GMMs) are applied,on a fused dataset of AIS and discharge measurements, to generate multi-modaldistribution curves, capturing typical lateral vessel positioning in thefairway and dislocation speeds along the waterway. By sampling the probabilitydensity curves of the GMMs, feature vectors are derived which are used,together with spatio-temporal vessel features and fairway geometries, as inputto a VTP transformer model. The incorporation of these distribution features ofboth the current and forthcoming navigation context improves predictionaccuracy. The superiority of the model over a previously proposed transformermodel for inland VTP is shown. The novelty lies in the provision ofpreprocessed, statistics-based features representing the conditioned spatialcontext, rather than relying on the model to extract relevant features for theVTP task from contextual data. Oversimplification of the complexity of inlandnavigation patterns by assuming a single typical route or selecting specificclusters prior to model application is avoided by giving the model access tothe entire distribution information. The methodology's generalizability isdemonstrated through the usage of data of 3 distinct river sections. It can beintegrated into an interaction-aware prediction framework, where insights intothe positioning of the actual vessel behavior in the overall distribution atthe current location and discharge can enhance trajectory prediction accuracy.</description><author>Kathrin Donandt, Dirk Söffker</author><pubDate>Tue, 04 Jun 2024 15:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02344v1</guid></item><item><title>Cluster-Aware Similarity Diffusion for Instance Retrieval</title><link>http://arxiv.org/abs/2406.02343v1</link><description>Diffusion-based re-ranking is a common method used for retrieving instancesby performing similarity propagation in a nearest neighbor graph. However,existing techniques that construct the affinity graph based on pairwiseinstances can lead to the propagation of misinformation from outliers and othermanifolds, resulting in inaccurate results. To overcome this issue, we proposea novel Cluster-Aware Similarity (CAS) diffusion for instance retrieval. Theprimary concept of CAS is to conduct similarity diffusion within localclusters, which can reduce the influence from other manifolds explicitly. Toobtain a symmetrical and smooth similarity matrix, our Bidirectional SimilarityDiffusion strategy introduces an inverse constraint term to the optimizationobjective of local cluster diffusion. Additionally, we have optimized aNeighbor-guided Similarity Smoothing approach to ensure similarity consistencyamong the local neighbors of each instance. Evaluations in instance retrievaland object re-identification validate the effectiveness of the proposed CAS,our code is publicly available.</description><author>Jifei Luo, Hantao Yao, Changsheng Xu</author><pubDate>Tue, 04 Jun 2024 15:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02343v1</guid></item><item><title>Understanding Heterophily for Graph Neural Networks</title><link>http://arxiv.org/abs/2401.09125v2</link><description>Graphs with heterophily have been regarded as challenging scenarios for GraphNeural Networks (GNNs), where nodes are connected with dissimilar neighborsthrough various patterns. In this paper, we present theoretical understandingsof the impacts of different heterophily patterns for GNNs by incorporating thegraph convolution (GC) operations into fully connected networks via theproposed Heterophilous Stochastic Block Models (HSBM), a general random graphmodel that can accommodate diverse heterophily patterns. Firstly, we show thatby applying a GC operation, the separability gains are determined by twofactors, i.e., the Euclidean distance of the neighborhood distributions and$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where$\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. Itreveals that the impact of heterophily on classification needs to be evaluatedalongside the averaged node degree. Secondly, we show that the topologicalnoise has a detrimental impact on separability, which is equivalent todegrading $\mathbb{E}\left[\operatorname{deg}\right]$. Finally, when applyingmultiple GC operations, we show that the separability gains are determined bythe normalized distance of the $l$-powered neighborhood distributions. Itindicates that the nodes still possess separability as $l$ goes to infinity ina wide range of regimes. Extensive experiments on both synthetic and real-worlddata verify the effectiveness of our theory.</description><author>Junfu Wang, Yuanfang Guo, Liang Yang, Yunhong Wang</author><pubDate>Tue, 04 Jun 2024 15:15:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09125v2</guid></item><item><title>More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime validity</title><link>http://arxiv.org/abs/2306.12214v4</link><description>In this paper, we present new high-probability PAC-Bayes bounds for differenttypes of losses. Firstly, for losses with a bounded range, we recover astrengthened version of Catoni's bound that holds uniformly for all parametervalues. This leads to new fast-rate and mixed-rate bounds that areinterpretable and tighter than previous bounds in the literature. Inparticular, the fast-rate bound is equivalent to the Seeger--Langford bound.Secondly, for losses with more general tail behaviors, we introduce two newparameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulativegenerating function is bounded, and a bound when the loss' second moment isbounded. These two bounds are obtained using a new technique based on adiscretization of the space of possible events for the ``in probability''parameter optimization problem. This technique is both simpler and more generalthan previous approaches optimizing over a grid on the parameters' space.Finally, using a simple technique that is applicable to any existing bound, weextend all previous results to anytime-valid bounds.</description><author>Borja Rodríguez-Gálvez, Ragnar Thobaben, Mikael Skoglund</author><pubDate>Tue, 04 Jun 2024 15:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12214v4</guid></item><item><title>Linguistic Fingerprint in Transformer Models: How Language Variation Influences Parameter Selection in Irony Detection</title><link>http://arxiv.org/abs/2406.02338v1</link><description>This paper explores the correlation between linguistic diversity, sentimentanalysis and transformer model architectures. We aim to investigate howdifferent English variations impact transformer-based models for ironydetection. To conduct our study, we used the EPIC corpus to extract fivediverse English variation-specific datasets and applied the KEN pruningalgorithm on five different architectures. Our results reveal severalsimilarities between optimal subnetworks, which provide insights into thelinguistic variations that share strong resemblances and those that exhibitgreater dissimilarities. We discovered that optimal subnetworks across modelsshare at least 60% of their parameters, emphasizing the significance ofparameter values in capturing and interpreting linguistic variations. Thisstudy highlights the inherent structural similarities between models trained ondifferent variants of the same language and also the critical role of parametervalues in capturing these nuances.</description><author>Michele Mastromattei, Fabio Massimo Zanzotto</author><pubDate>Tue, 04 Jun 2024 15:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02338v1</guid></item><item><title>Polynomial-Augmented Neural Networks (PANNs) with Weak Orthogonality Constraints for Enhanced Function and PDE Approximation</title><link>http://arxiv.org/abs/2406.02336v1</link><description>We present polynomial-augmented neural networks (PANNs), a novel machinelearning architecture that combines deep neural networks (DNNs) with apolynomial approximant. PANNs combine the strengths of DNNs (flexibility andefficiency in higher-dimensional approximation) with those of polynomialapproximation (rapid convergence rates for smooth functions). To aid in bothstable training and enhanced accuracy over a variety of problems, we present(1) a family of orthogonality constraints that impose mutual orthogonalitybetween the polynomial and the DNN within a PANN; (2) a simple basis pruningapproach to combat the curse of dimensionality introduced by the polynomialcomponent; and (3) an adaptation of a polynomial preconditioning strategy toboth DNNs and polynomials. We test the resulting architecture for itspolynomial reproduction properties, ability to approximate both smoothfunctions and functions of limited smoothness, and as a method for the solutionof partial differential equations (PDEs). Through these experiments, wedemonstrate that PANNs offer superior approximation properties to DNNs for bothregression and the numerical solution of PDEs, while also offering enhancedaccuracy over both polynomial and DNN-based regression (each) when regressingfunctions with limited smoothness.</description><author>Madison Cooley, Shandian Zhe, Robert M. Kirby, Varun Shankar</author><pubDate>Tue, 04 Jun 2024 15:06:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02336v1</guid></item><item><title>Probing the Category of Verbal Aspect in Transformer Language Models</title><link>http://arxiv.org/abs/2406.02335v1</link><description>We investigate how pretrained language models (PLM) encode the grammaticalcategory of verbal aspect in Russian. Encoding of aspect in transformer LMs hasnot been studied previously in any language. A particular challenge is posed by"alternative contexts": where either the perfective or the imperfective aspectis suitable grammatically and semantically. We perform probing using BERT andRoBERTa on alternative and non-alternative contexts. First, we assess themodels' performance on aspect prediction, via behavioral probing. Next, weexamine the models' performance when their contextual representations aresubstituted with counterfactual representations, via causal probing. Thesecounterfactuals alter the value of the "boundedness" feature--a semanticfeature, which characterizes the action in the context. Experiments show thatBERT and RoBERTa do encode aspect--mostly in their final layers. Thecounterfactual interventions affect perfective and imperfective in oppositeways, which is consistent with grammar: perfective is positively affected byadding the meaning of boundedness, and vice versa. The practical implicationsof our probing results are that fine-tuning only the last layers of BERT onpredicting aspect is faster and more effective than fine-tuning the wholemodel. The model has high predictive uncertainty about aspect in alternativecontexts, which tend to lack explicit hints about the boundedness of thedescribed action.</description><author>Anisia Katinskaia, Roman Yangarber</author><pubDate>Tue, 04 Jun 2024 15:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02335v1</guid></item><item><title>Towards Neural Architecture Search for Transfer Learning in 6G Networks</title><link>http://arxiv.org/abs/2406.02333v1</link><description>The future 6G network is envisioned to be AI-native, and as such, ML modelswill be pervasive in support of optimizing performance, reducing energyconsumption, and in coping with increasing complexity and heterogeneity. A keychallenge is automating the process of finding optimal model architecturessatisfying stringent requirements stemming from varying tasks, dynamicity andavailable resources in the infrastructure and deployment positions. In thispaper, we describe and review the state-of-the-art in Neural ArchitectureSearch and Transfer Learning and their applicability in networking. Further, weidentify open research challenges and set directions with a specific focus onthree main requirements with elements unique to the future network, namelycombining NAS and TL, multi-objective search, and tabular data. Finally, weoutline and discuss both near-term and long-term work ahead.</description><author>Adam Orucu, Farnaz Moradi, Masoumeh Ebrahimi, Andreas Johnsson</author><pubDate>Tue, 04 Jun 2024 15:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02333v1</guid></item><item><title>Extended Mind Transformers</title><link>http://arxiv.org/abs/2406.02332v1</link><description>Pre-trained language models demonstrate general intelligence and commonsense, but long inputs quickly become a bottleneck for memorizing informationat inference time. We resurface a simple method, Memorizing Transformers (Wu etal., 2022), that gives the model access to a bank of pre-computed memories. Weshow that it is possible to fix many of the shortcomings of the originalmethod, such as the need for fine-tuning, by critically assessing howpositional encodings should be updated for the keys and values retrieved. Thisintuitive method uses the model's own key/query system to select and attend tothe most relevant memories at each generation step, rather than using externalembeddings. We demonstrate the importance of external information beingretrieved in a majority of decoder layers, contrary to previous work. We opensource a new counterfactual long-range retrieval benchmark, and show thatExtended Mind Transformers outperform today's state of the art by 6% onaverage.</description><author>Phoebe Klett, Thomas Ahle</author><pubDate>Tue, 04 Jun 2024 15:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02332v1</guid></item><item><title>Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering</title><link>http://arxiv.org/abs/2406.02331v1</link><description>Building a reliable visual question answering~(VQA) system across differentlanguages is a challenging problem, primarily due to the lack of abundantsamples for training. To address this challenge, recent studies have employedmachine translation systems for the cross-lingual VQA task. This involvestranslating the evaluation samples into a source language (usually English) andusing monolingual models (i.e., translate-test). However, our analysis revealsthat translated texts contain unique characteristics distinct fromhuman-written ones, referred to as translation artifacts. We find that theseartifacts can significantly affect the models, confirmed by extensiveexperiments across diverse models, languages, and translation processes. Inlight of this, we present a simple data augmentation strategy that canalleviate the adverse impacts of translation artifacts.</description><author>ChaeHun Park, Koanho Lee, Hyesu Lim, Jaeseok Kim, Junmo Park, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo</author><pubDate>Tue, 04 Jun 2024 15:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02331v1</guid></item><item><title>Practical Performance Guarantees for Pipelined DNN Inference</title><link>http://arxiv.org/abs/2311.03703v3</link><description>We optimize pipeline parallelism for deep neural network (DNN) inference bypartitioning model graphs into $k$ stages and minimizing the running time ofthe bottleneck stage, including communication. We give practical and effectivealgorithms for this NP-hard problem, but our emphasis is on tackling thepractitioner's dilemma of deciding when a solution is good enough. To this end,we design novel mixed-integer programming (MIP) relaxations for proving lowerbounds. Applying these methods to a diverse testbed of 369 production models,for $k \in \{2, 4, 8, 16, 32, 64\}$, we empirically show that these lowerbounds are strong enough to be useful in practice. Our lower bounds aresubstantially stronger than standard combinatorial bounds. For example,evaluated via geometric means across a production testbed with $k = 16$pipeline stages, our MIP formulations raise the lower bound from 0.4598 to0.9452, expressed as a fraction of the best partition found. In other words,our improved lower bounds close the optimality gap by a factor of 9.855x.</description><author>Aaron Archer, Matthew Fahrbach, Kuikui Liu, Prakash Prabhu</author><pubDate>Tue, 04 Jun 2024 14:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03703v3</guid></item><item><title>On Affine Homotopy between Language Encoders</title><link>http://arxiv.org/abs/2406.02329v1</link><description>Pre-trained language encoders -- functions that represent text as vectors --are an integral component of many NLP tasks. We tackle a natural question inlanguage encoder analysis: What does it mean for two encoders to be similar? Wecontend that a faithful measure of similarity needs to be \emph{intrinsic},that is, task-independent, yet still be informative of \emph{extrinsic}similarity -- the performance on downstream tasks. It is common to consider twoencoders similar if they are \emph{homotopic}, i.e., if they can be alignedthrough some transformation. In this spirit, we study the properties of\emph{affine} alignment of language encoders and its implications on extrinsicsimilarity. We find that while affine alignment is fundamentally an asymmetricnotion of similarity, it is still informative of extrinsic similarity. Weconfirm this on datasets of natural language representations. Beyond providinguseful bounds on extrinsic similarity, affine intrinsic similarity also allowsus to begin uncovering the structure of the space of pre-trained encoders bydefining an order over them.</description><author>Robin SM Chan, Reda Boumasmoud, Anej Svete, Yuxin Ren, Qipeng Guo, Zhijing Jin, Shauli Ravfogel, Mrinmaya Sachan, Bernhard Schölkopf, Mennatallah El-Assady, Ryan Cotterell</author><pubDate>Tue, 04 Jun 2024 14:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02329v1</guid></item><item><title>Continual Unsupervised Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2406.02327v1</link><description>Deep learning models excel when the data distribution during training alignswith testing data. Yet, their performance diminishes when faced without-of-distribution (OOD) samples, leading to great interest in the field ofOOD detection. Current approaches typically assume that OOD samples originatefrom an unconcentrated distribution complementary to the training distribution.While this assumption is appropriate in the traditional unsupervised OOD(U-OOD) setting, it proves inadequate when considering the place of deploymentof the underlying deep learning model. To better reflect this real-worldscenario, we introduce the novel setting of continual U-OOD detection. Totackle this new setting, we propose a method that starts from a U-OOD detector,which is agnostic to the OOD distribution, and slowly updates during deploymentto account for the actual OOD distribution. Our method uses a new U-OOD scoringfunction that combines the Mahalanobis distance with a nearest-neighborapproach. Furthermore, we design a confidence-scaled few-shot OOD detector thatoutperforms previous methods. We show our method greatly improves upon strongbaselines from related fields.</description><author>Lars Doorenbos, Raphael Sznitman, Pablo Márquez-Neila</author><pubDate>Tue, 04 Jun 2024 14:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02327v1</guid></item><item><title>Technical Language Processing for Telecommunications Specifications</title><link>http://arxiv.org/abs/2406.02325v1</link><description>Large Language Models (LLMs) are continuously being applied in a more diverseset of contexts. At their current state, however, even state-of-the-art LLMssuch as Generative Pre-Trained Transformer 4 (GTP-4) have challenges whenextracting information from real-world technical documentation without a heavypreprocessing. One such area with real-world technical documentation istelecommunications engineering, which could greatly benefit fromdomain-specific LLMs. The unique format and overall structure oftelecommunications internal specifications differs greatly from standardEnglish and thus it is evident that the application of out-of-the-box NaturalLanguage Processing (NLP) tools is not a viable option. In this article, weoutline the limitations of out-of-the-box NLP tools for processing technicalinformation generated by telecommunications experts, and expand the concept ofTechnical Language Processing (TLP) to the telecommunication domain.Additionally, we explore the effect of domain-specific LLMs in the work ofSpecification Engineers, emphasizing the potential benefits of adoptingdomain-specific LLMs to speed up the training of experts in differenttelecommunications fields.</description><author>Felipe A. Rodriguez Y.</author><pubDate>Tue, 04 Jun 2024 14:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02325v1</guid></item><item><title>MEDIQ: Question-Asking LLMs for Adaptive and Reliable Clinical Reasoning</title><link>http://arxiv.org/abs/2406.00922v2</link><description>In high-stakes domains like clinical reasoning, AI assistants powered bylarge language models (LLMs) are yet to be reliable and safe. We identify a keyobstacle towards reliability: existing LLMs are trained to answer any question,even with incomplete context in the prompt or insufficient parametricknowledge. We propose to change this paradigm to develop more careful LLMs thatask follow-up questions to gather necessary and sufficient information andrespond reliably. We introduce MEDIQ, a framework to simulate realisticclinical interactions, which incorporates a Patient System and an adaptiveExpert System. The Patient may provide incomplete information in the beginning;the Expert refrains from making diagnostic decisions when unconfident, andinstead elicits missing details from the Patient via follow-up questions. Toevaluate MEDIQ, we convert MEDQA and CRAFT-MD -- medical benchmarks fordiagnostic question answering -- into an interactive setup. We develop areliable Patient system and prototype several Expert systems, first showingthat directly prompting state-of-the-art LLMs to ask questions degrades thequality of clinical reasoning, indicating that adapting LLMs to interactiveinformation-seeking settings is nontrivial. We then augment the Expert with anovel abstention module to better estimate model confidence and decide whetherto ask more questions, thereby improving diagnostic accuracy by 20.3%; however,performance still lags compared to an (unrealistic in practice) upper boundwhen full information is given upfront. Further analyses reveal thatinteractive performance can be improved by filtering irrelevant contexts andreformatting conversations. Overall, our paper introduces a novel problemtowards LLM reliability, a novel MEDIQ framework, and highlights importantfuture directions to extend the information-seeking abilities of LLM assistantsin critical domains.</description><author>Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov</author><pubDate>Tue, 04 Jun 2024 14:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00922v2</guid></item><item><title>A Survey of Transformer Enabled Time Series Synthesis</title><link>http://arxiv.org/abs/2406.02322v1</link><description>Generative AI has received much attention in the image and language domains,with the transformer neural network continuing to dominate the state of theart. Application of these models to time series generation is less explored,however, and is of great utility to machine learning, privacy preservation, andexplainability research. The present survey identifies this gap at theintersection of the transformer, generative AI, and time series data, andreviews works in this sparsely populated subdomain. The reviewed works showgreat variety in approach, and have not yet converged on a conclusive answer tothe problems the domain poses. GANs, diffusion models, state space models, andautoencoders were all encountered alongside or surrounding the transformerswhich originally motivated the survey. While too open a domain to offerconclusive insights, the works surveyed are quite suggestive, and severalrecommendations for best practice, and suggestions of valuable future work, areprovided.</description><author>Alexander Sommers, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure, Thomas Arnold</author><pubDate>Tue, 04 Jun 2024 14:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02322v1</guid></item><item><title>Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption</title><link>http://arxiv.org/abs/2405.19014v2</link><description>Dyna-style model-based reinforcement learning (MBRL) combines model-freeagents with predictive transition models through model-based rollouts. Thiscombination raises a critical question: 'When to trust your model?'; i.e.,which rollout length results in the model providing useful data? Janner et al.(2019) address this question by gradually increasing rollout lengths throughoutthe training. While theoretically tempting, uniform model accuracy is a fallacythat collapses at the latest when extrapolating. Instead, we propose asking thequestion 'Where to trust your model?'. Using inherent model uncertainty toconsider local accuracy, we obtain the Model-Based Actor-Critic withUncertainty-Aware Rollout Adaption (MACURA) algorithm. We propose aneasy-to-tune rollout mechanism and demonstrate substantial improvements in dataefficiency and performance compared to state-of-the-art deep MBRL methods onthe MuJoCo benchmark.</description><author>Bernd Frauenknecht, Artur Eisele, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe</author><pubDate>Tue, 04 Jun 2024 14:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19014v2</guid></item><item><title>PeFAD: A Parameter-Efficient Federated Framework for Time Series Anomaly Detection</title><link>http://arxiv.org/abs/2406.02318v1</link><description>With the proliferation of mobile sensing techniques, huge amounts of timeseries data are generated and accumulated in various domains, fueling plenty ofreal-world applications. In this setting, time series anomaly detection ispractically important. It endeavors to identify deviant samples from the normalsample distribution in time series. Existing approaches generally assume thatall the time series is available at a central location. However, we arewitnessing the decentralized collection of time series due to the deployment ofvarious edge devices. To bridge the gap between the decentralized time seriesdata and the centralized anomaly detection algorithms, we propose aParameter-efficient Federated Anomaly Detection framework named PeFAD with theincreasing privacy concerns. PeFAD for the first time employs the pre-trainedlanguage model (PLM) as the body of the client's local model, which can benefitfrom its cross-modality knowledge transfer capability. To reduce thecommunication overhead and local model adaptation cost, we propose aparameter-efficient federated training module such that clients only need tofine-tune small-scale parameters and transmit them to the server for update.PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate theimpact of neglected anomalies during training. A knowledge distillationoperation on a synthetic privacy-preserving dataset that is shared by all theclients is also proposed to address the data heterogeneity issue acrossclients. We conduct extensive evaluations on four real datasets, where PeFADoutperforms existing state-of-the-art baselines by up to 28.74\%.</description><author>Ronghui Xu, Hao Miao, Senzhang Wang, Philip S. Yu, Jianxin Wang</author><pubDate>Tue, 04 Jun 2024 14:51:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02318v1</guid></item><item><title>Generative Conditional Distributions by Neural (Entropic) Optimal Transport</title><link>http://arxiv.org/abs/2406.02317v1</link><description>Learning conditional distributions is challenging because the desired outcomeis not a single distribution but multiple distributions that correspond tomultiple instances of the covariates. We introduce a novel neural entropicoptimal transport method designed to effectively learn generative models ofconditional distributions, particularly in scenarios characterized by limitedsample sizes. Our method relies on the minimax training of two neural networks:a generative network parametrizing the inverse cumulative distributionfunctions of the conditional distributions and another network parametrizingthe conditional Kantorovich potential. To prevent overfitting, we regularizethe objective function by penalizing the Lipschitz constant of the networkoutput. Our experiments on real-world datasets show the effectiveness of ouralgorithm compared to state-of-the-art conditional distribution learningtechniques. Our implementation can be found athttps://github.com/nguyenngocbaocmt02/GENTLE.</description><author>Bao Nguyen, Binh Nguyen, Hieu Trung Nguyen, Viet Anh Nguyen</author><pubDate>Tue, 04 Jun 2024 14:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02317v1</guid></item><item><title>An Independence-promoting Loss for Music Generation with Language Models</title><link>http://arxiv.org/abs/2406.02315v1</link><description>Music generation schemes using language modeling rely on a vocabulary ofaudio tokens, generally provided as codes in a discrete latent space learnt byan auto-encoder. Multi-stage quantizers are often employed to produce thesetokens, therefore the decoding strategy used for token prediction must beadapted to account for multiple codebooks: either it should model the jointdistribution over all codebooks, or fit the product of the codebook marginaldistributions. Modelling the joint distribution requires a costly increase inthe number of auto-regressive steps, while fitting the product of the marginalsyields an inexact model unless the codebooks are mutually independent. In thiswork, we introduce an independence-promoting loss to regularize theauto-encoder used as the tokenizer in language models for music generation. Theproposed loss is a proxy for mutual information based on the maximum meandiscrepancy principle, applied in reproducible kernel Hilbert spaces. Ourcriterion is simple to implement and train, and it is generalizable to othermulti-stream codecs. We show that it reduces the statistical dependence betweencodebooks during auto-encoding. This leads to an increase in the generatedmusic quality when modelling the product of the marginal distributions, whilegenerating audio much faster than the joint distribution model.</description><author>Jean-Marie Lemercier, Simon Rouard, Jade Copet, Yossi Adi, Alexandre Déffosez</author><pubDate>Tue, 04 Jun 2024 14:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02315v1</guid></item><item><title>Neural Thermodynamic Integration: Free Energies from Energy-based Diffusion Models</title><link>http://arxiv.org/abs/2406.02313v1</link><description>Thermodynamic integration (TI) offers a rigorous method for estimatingfree-energy differences by integrating over a sequence of interpolatingconformational ensembles. However, TI calculations are computationallyexpensive and typically limited to coupling a small number of degrees offreedom due to the need to sample numerous intermediate ensembles withsufficient conformational-space overlap. In this work, we propose to perform TIalong an alchemical pathway represented by a trainable neural network, which weterm Neural TI. Critically, we parametrize a time-dependent Hamiltonianinterpolating between the interacting and non-interacting systems, and optimizeits gradient using a denoising-diffusion objective. The ability of theresulting energy-based diffusion model to sample all intermediate ensembles,allows us to perform TI from a single reference calculation. We apply ourmethod to Lennard-Jones fluids, where we report accurate calculations of theexcess chemical potential, demonstrating that Neural TI is capable of couplinghundreds of degrees of freedom at once.</description><author>Bálint Máté, François Fleuret, Tristan Bereau</author><pubDate>Tue, 04 Jun 2024 14:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02313v1</guid></item><item><title>In value-based deep reinforcement learning, a pruned network is a good network</title><link>http://arxiv.org/abs/2402.12479v2</link><description>Recent work has shown that deep reinforcement learning agents have difficultyin effectively using their network parameters. We leverage prior insights intothe advantages of sparse training techniques and demonstrate that gradualmagnitude pruning enables value-based agents to maximize parametereffectiveness. This results in networks that yield dramatic performanceimprovements over traditional networks, using only a small fraction of the fullnetwork parameters.</description><author>Johan Obando-Ceron, Aaron Courville, Pablo Samuel Castro</author><pubDate>Tue, 04 Jun 2024 14:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12479v2</guid></item><item><title>Disentangled Representation via Variational AutoEncoder for Continuous Treatment Effect Estimation</title><link>http://arxiv.org/abs/2406.02310v1</link><description>Continuous treatment effect estimation holds significant practical importanceacross various decision-making and assessment domains, such as healthcare andthe military. However, current methods for estimating dose-response curveshinge on balancing the entire representation by treating all covariates asconfounding variables. Although various approaches disentangle covariates intodifferent factors for treatment effect estimation, they are confined to binarytreatment settings. Moreover, observational data are often tainted withnon-causal noise information that is imperceptible to the human. Hence, in thispaper, we propose a novel Dose-Response curve estimator via VariationalAutoEncoder (DRVAE) disentangled covariates representation. Our model isdedicated to disentangling covariates into instrumental factors, confoundingfactors, adjustment factors, and external noise factors, thereby facilitatingthe estimation of treatment effects under continuous treatment settings bybalancing the disentangled confounding factors. Extensive results on syntheticand semi-synthetic datasets demonstrate that our model outperforms the currentstate-of-the-art methods.</description><author>Ruijing Cui, Jianbin Sun, Bingyu He, Kewei Yang, Bingfeng Ge</author><pubDate>Tue, 04 Jun 2024 14:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02310v1</guid></item><item><title>Effects of Exponential Gaussian Distribution on (Double Sampling) Randomized Smoothing</title><link>http://arxiv.org/abs/2406.02309v1</link><description>Randomized Smoothing (RS) is currently a scalable certified defense methodproviding robustness certification against adversarial examples. Althoughsignificant progress has been achieved in providing defenses against $\ell_p$adversaries, the interaction between the smoothing distribution and therobustness certification still remains vague. In this work, we comprehensivelystudy the effect of two families of distributions, named Exponential StandardGaussian (ESG) and Exponential General Gaussian (EGG) distributions, onRandomized Smoothing and Double Sampling Randomized Smoothing (DSRS). We derivean analytic formula for ESG's certified radius, which converges to the originformula of RS as the dimension $d$ increases. Additionally, we prove that EGGcan provide tighter constant factors than DSRS in providing $\Omega(\sqrt{d})$lower bounds of $\ell_2$ certified radius, and thus further addresses the curseof dimensionality in RS. Our experiments on real-world datasets confirm ourtheoretical analysis of the ESG distributions, that they provide almost thesame certification under different exponents $\eta$ for both RS and DSRS. Inaddition, EGG</description><author>Youwei Shu, Xi Xiao, Derui Wang, Yuxin Cao, Siji Chen, Jason Xue, Linyi Li, Bo Li</author><pubDate>Tue, 04 Jun 2024 14:41:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02309v1</guid></item><item><title>Dynamics Harmonic Analysis of Robotic Systems: Application in Data-Driven Koopman Modelling</title><link>http://arxiv.org/abs/2312.07457v3</link><description>We introduce the use of harmonic analysis to decompose the state space ofsymmetric robotic systems into orthogonal isotypic subspaces. These arelower-dimensional spaces that capture distinct, symmetric, and synergisticmotions. For linear dynamics, we characterize how this decomposition leads to asubdivision of the dynamics into independent linear systems on each subspace, aproperty we term dynamics harmonic analysis (DHA). To exploit this property, weuse Koopman operator theory to propose an equivariant deep-learningarchitecture that leverages the properties of DHA to learn a global linearmodel of the system dynamics. Our architecture, validated on synthetic systemsand the dynamics of locomotion of a quadrupedal robot, exhibits enhancedgeneralization, sample efficiency, and interpretability, with fewer trainableparameters and computational costs.</description><author>Daniel Ordoñez-Apraez, Vladimir Kostic, Giulio Turrisi, Pietro Novelli, Carlos Mastalli, Claudio Semini, Massimiliano Pontil</author><pubDate>Tue, 04 Jun 2024 14:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07457v3</guid></item><item><title>Mixtures of Experts Unlock Parameter Scaling for Deep RL</title><link>http://arxiv.org/abs/2402.08609v2</link><description>The recent rapid progress in (self) supervised learning models is in largepart predicted by empirical scaling laws: a model's performance scalesproportionally to its size. Analogous scaling laws remain elusive forreinforcement learning domains, however, where increasing the parameter countof a model often hurts its final performance. In this paper, we demonstratethat incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs(Puigcerver et al., 2023), into value-based networks results in moreparameter-scalable models, evidenced by substantial performance increasesacross a variety of training regimes and model sizes. This work thus providesstrong empirical evidence towards developing scaling laws for reinforcementlearning.</description><author>Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, Pablo Samuel Castro</author><pubDate>Tue, 04 Jun 2024 14:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08609v2</guid></item><item><title>InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification</title><link>http://arxiv.org/abs/2401.16475v2</link><description>Text simplification aims to make technical texts more accessible to laypeoplebut often results in deletion of information and vagueness. This work proposesInfoLossQA, a framework to characterize and recover simplification-inducedinformation loss in form of question-and-answer (QA) pairs. Building on thetheory of Question Under Discussion, the QA pairs are designed to help readersdeepen their knowledge of a text. We conduct a range of experiments with thisframework. First, we collect a dataset of 1,000 linguist-curated QA pairsderived from 104 LLM simplifications of scientific abstracts of medicalstudies. Our analyses of this data reveal that information loss occursfrequently, and that the QA pairs give a high-level overview of whatinformation was lost. Second, we devise two methods for this task: end-to-endprompting of open-source and commercial language models, and a natural languageinference pipeline. With a novel evaluation framework considering thecorrectness of QA pairs and their linguistic suitability, our expert evaluationreveals that models struggle to reliably identify information loss and applyingsimilar standards as humans at what constitutes information loss.</description><author>Jan Trienes, Sebastian Joseph, Jörg Schlötterer, Christin Seifert, Kyle Lo, Wei Xu, Byron C. Wallace, Junyi Jessy Li</author><pubDate>Tue, 04 Jun 2024 14:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16475v2</guid></item><item><title>CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data</title><link>http://arxiv.org/abs/2403.11346v2</link><description>Neural Machine Translation (NMT) for low-resource languages is still achallenging task in front of NLP researchers. In this work, we deploy astandard data augmentation methodology by back-translation to a new languagetranslation direction Cantonese-to-English. We present the models we fine-tunedusing the limited amount of real data and the synthetic data we generated usingback-translation including OpusMT, NLLB, and mBART. We carried out automaticevaluation using a range of different metrics including lexical-based andembedding-based. Furthermore. we create a user-friendly interface for themodels we included in this\textsc{ CantonMT} research project and make itavailable to facilitate Cantonese-to-English MT research. Researchers can addmore models into this platform via our open-source\textsc{ CantonMT} toolkit\url{https://github.com/kenrickkung/CantoneseTranslation}.</description><author>Kung Yin Hong, Lifeng Han, Riza Batista-Navarro, Goran Nenadic</author><pubDate>Tue, 04 Jun 2024 14:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11346v2</guid></item><item><title>mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models</title><link>http://arxiv.org/abs/2406.02301v1</link><description>Large language models (LLMs) with Chain-of-thought (CoT) have recentlyemerged as a powerful technique for eliciting reasoning to improve variousdownstream tasks. As most research mainly focuses on English, with fewexplorations in a multilingual context, the question of how reliable thisreasoning capability is in different languages is still open. To address itdirectly, we study multilingual reasoning consistency across multiplelanguages, using popular open-source LLMs. First, we compile the firstlarge-scale multilingual math reasoning dataset, mCoT-MATH, covering elevendiverse languages. Then, we introduce multilingual CoT instruction tuning toboost reasoning capability across languages, thereby improving modelconsistency. While existing LLMs show substantial variation across thelanguages we consider, and especially low performance for lesser resourcedlanguages, our 7B parameter model mCoT achieves impressive consistency acrosslanguages, and superior or comparable performance to close- and open-sourcemodels even of much larger sizes.</description><author>Huiyuan Lai, Malvina Nissim</author><pubDate>Tue, 04 Jun 2024 14:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02301v1</guid></item><item><title>Node-Level Topological Representation Learning on Point Clouds</title><link>http://arxiv.org/abs/2406.02300v1</link><description>Topological Data Analysis (TDA) allows us to extract powerful topological andhigher-order information on the global shape of a data set or point cloud.Tools like Persistent Homology or the Euler Transform give a single complexdescription of the global structure of the point cloud. However, common machinelearning applications like classification require point-level information andfeatures to be available. In this paper, we bridge this gap and propose a novelmethod to extract node-level topological features from complex point cloudsusing discrete variants of concepts from algebraic topology and differentialgeometry. We verify the effectiveness of these topological point features(TOPF) on both synthetic and real-world data and study their robustness undernoise.</description><author>Vincent P. Grande, Michael T. Schaub</author><pubDate>Tue, 04 Jun 2024 14:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02300v1</guid></item><item><title>Text Embedding Inversion Security for Multilingual Language Models</title><link>http://arxiv.org/abs/2401.12192v3</link><description>Textual data is often represented as real-numbered embeddings in NLP,particularly with the popularity of large language models (LLMs) and Embeddingsas a Service (EaaS). However, storing sensitive information as embeddings canbe susceptible to security breaches, as research shows that text can bereconstructed from embeddings, even without knowledge of the underlying model.While defence mechanisms have been explored, these are exclusively focused onEnglish, leaving other languages potentially exposed to attacks. This workexplores LLM security through multilingual embedding inversion. We define theproblem of black-box multilingual and cross-lingual inversion attacks, andexplore their potential implications. Our findings suggest that multilingualLLMs may be more vulnerable to inversion attacks, in part because English-baseddefences may be ineffective. To alleviate this, we propose a simple maskingdefense effective for both monolingual and multilingual models. This study isthe first to investigate multilingual inversion attacks, shedding light on thedifferences in attacks and defenses across monolingual and multilingualsettings.</description><author>Yiyi Chen, Heather Lent, Johannes Bjerva</author><pubDate>Tue, 04 Jun 2024 14:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12192v3</guid></item><item><title>Large Language Models for Generative Information Extraction: A Survey</title><link>http://arxiv.org/abs/2312.17617v2</link><description>Information extraction (IE) aims to extract structural knowledge (such asentities, relations, and events) from plain natural language texts. Recently,generative Large Language Models (LLMs) have demonstrated remarkablecapabilities in text understanding and generation, allowing for generalizationacross various domains and tasks. As a result, numerous works have beenproposed to harness abilities of LLMs and offer viable solutions for IE tasksbased on a generative paradigm. To conduct a comprehensive systematic reviewand exploration of LLM efforts for IE tasks, in this study, we survey the mostrecent advancements in this field. We first present an extensive overview bycategorizing these works in terms of various IE subtasks and learningparadigms, then we empirically analyze the most advanced methods and discoverthe emerging trend of IE tasks with LLMs. Based on thorough review conducted,we identify several insights in technique and promising research directionsthat deserve further exploration in future studies. We maintain a publicrepository and consistently update related resources at:\url{https://github.com/quqxui/Awesome-LLM4IE-Papers}.</description><author>Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, Enhong Chen</author><pubDate>Tue, 04 Jun 2024 14:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17617v2</guid></item><item><title>Solving Partial Differential Equations in Different Domains by Operator Learning method Based on Boundary Integral Equations</title><link>http://arxiv.org/abs/2406.02298v1</link><description>This article explores operator learning models that can deduce solutions topartial differential equations (PDEs) on arbitrary domains without requiringretraining. We introduce two innovative models rooted in boundary integralequations (BIEs): the Boundary Integral Type Deep Operator Network(BI-DeepONet) and the Boundary Integral Trigonometric Deep Operator NeuralNetwork (BI-TDONet), which are crafted to address PDEs across diverse domains.Once fully trained, these BIE-based models adeptly predict the solutions ofPDEs in any domain without the need for additional training. BI-TDONet notablyenhances its performance by employing the singular value decomposition (SVD) ofbounded linear operators, allowing for the efficient distribution of inputfunctions across its modules. Furthermore, to tackle the issue of functionsampling values that do not effectively capture oscillatory and impulse signalcharacteristics, trigonometric coefficients are utilized as both inputs andoutputs in BI-TDONet. Our numerical experiments robustly support and confirmthe efficacy of this theoretical framework.</description><author>Bin Meng, Yutong Lu, Ying Jiang</author><pubDate>Tue, 04 Jun 2024 14:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02298v1</guid></item><item><title>Learning-Rate-Free Stochastic Optimization over Riemannian Manifolds</title><link>http://arxiv.org/abs/2406.02296v1</link><description>In recent years, interest in gradient-based optimization over Riemannianmanifolds has surged. However, a significant challenge lies in the reliance onhyperparameters, especially the learning rate, which requires meticulous tuningby practitioners to ensure convergence at a suitable rate. In this work, weintroduce innovative learning-rate-free algorithms for stochastic optimizationover Riemannian manifolds, eliminating the need for hand-tuning and providing amore robust and user-friendly approach. We establish high probabilityconvergence guarantees that are optimal, up to logarithmic factors, compared tothe best-known optimally tuned rate in the deterministic setting. Our approachis validated through numerical experiments, demonstrating competitiveperformance against learning-rate-dependent algorithms.</description><author>Daniel Dodd, Louis Sharrock, Christopher Nemeth</author><pubDate>Tue, 04 Jun 2024 14:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02296v1</guid></item><item><title>How to Explore with Belief: State Entropy Maximization in POMDPs</title><link>http://arxiv.org/abs/2406.02295v1</link><description>Recent works have studied *state entropy maximization* in reinforcementlearning, in which the agent's objective is to learn a policy inducing highentropy over states visitation (Hazan et al., 2019). They typically assume fullobservability of the state of the system, so that the entropy of theobservations is maximized. In practice, the agent may only get *partial*observations, e.g., a robot perceiving the state of a physical space throughproximity sensors and cameras. A significant mismatch between the entropy overobservations and true states of the system can arise in those settings. In thispaper, we address the problem of entropy maximization over the *true states*with a decision policy conditioned on partial observations *only*. The latteris a generalization of POMDPs, which is intractable in general. We develop amemory and computationally efficient *policy gradient* method to address afirst-order relaxation of the objective defined on *belief* states, providingvarious formal characterizations of approximation gaps, the optimizationlandscape, and the *hallucination* problem. This paper aims to generalize stateentropy maximization to more realistic domains that meet the challenges ofapplications.</description><author>Riccardo Zamboni, Duilio Cirino, Marcello Restelli, Mirco Mutti</author><pubDate>Tue, 04 Jun 2024 14:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02295v1</guid></item><item><title>Smaller Batches, Bigger Gains? Investigating the Impact of Batch Sizes on Reinforcement Learning Based Real-World Production Scheduling</title><link>http://arxiv.org/abs/2406.02294v1</link><description>Production scheduling is an essential task in manufacturing, withReinforcement Learning (RL) emerging as a key solution. In a previous work, RLwas utilized to solve an extended permutation flow shop scheduling problem(PFSSP) for a real-world production line with two stages, linked by a centralbuffer. The RL agent was trained to sequence equallysized product batches tominimize setup efforts and idle times. However, the substantial impact causedby varying the size of these product batches has not yet been explored. In thisfollow-up study, we investigate the effects of varying batch sizes, exploringboth the quality of solutions and the training dynamics of the RL agent. Theresults demonstrate that it is possible to methodically identify reasonableboundaries for the batch size. These boundaries are determined on one side bythe increasing sample complexity associated with smaller batch sizes, and onthe other side by the decreasing flexibility of the agent when dealing withlarger batch sizes. This provides the practitioner the ability to make aninformed decision regarding the selection of an appropriate batch size.Moreover, we introduce and investigate two new curriculum learning strategiesto enable the training with small batch sizes. The findings of this work offerthe potential for application in several industrial use cases with comparablescheduling problems.</description><author>Arthur Müller, Felix Grumbach, Matthia Sabatelli</author><pubDate>Tue, 04 Jun 2024 14:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02294v1</guid></item><item><title>Active Label Correction for Semantic Segmentation with Foundation Models</title><link>http://arxiv.org/abs/2403.10820v2</link><description>Training and validating models for semantic segmentation require datasetswith pixel-wise annotations, which are notoriously labor-intensive. Althoughuseful priors such as foundation models or crowdsourced datasets are available,they are error-prone. We hence propose an effective framework of active labelcorrection (ALC) based on a design of correction query to rectify pseudo labelsof pixels, which in turn is more annotator-friendly than the standard oneinquiring to classify a pixel directly according to our theoretical analysisand user study. Specifically, leveraging foundation models providing usefulzero-shot predictions on pseudo labels and superpixels, our method comprisestwo key techniques: (i) an annotator-friendly design of correction query withthe pseudo labels, and (ii) an acquisition function looking ahead labelexpansions based on the superpixels. Experimental results on PASCAL,Cityscapes, and Kvasir-SEG datasets demonstrate the effectiveness of our ALCframework, outperforming prior methods for active semantic segmentation andlabel correction. Notably, utilizing our method, we obtained a revised datasetof PASCAL by rectifying errors in 2.6 million pixels in PASCAL dataset.</description><author>Hoyoung Kim, Sehyun Hwang, Suha Kwak, Jungseul Ok</author><pubDate>Tue, 04 Jun 2024 14:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10820v2</guid></item><item><title>Composite Quantile Regression With XGBoost Using the Novel Arctan Pinball Loss</title><link>http://arxiv.org/abs/2406.02293v1</link><description>This paper explores the use of XGBoost for composite quantile regression.XGBoost is a highly popular model renowned for its flexibility, efficiency, andcapability to deal with missing data. The optimization uses a second orderapproximation of the loss function, complicating the use of loss functions witha zero or vanishing second derivative. Quantile regression -- a popularapproach to obtain conditional quantiles when point estimates alone areinsufficient -- unfortunately uses such a loss function, the pinball loss.Existing workarounds are typically inefficient and can result in severequantile crossings. In this paper, we present a smooth approximation of thepinball loss, the arctan pinball loss, that is tailored to the needs ofXGBoost. Specifically, contrary to other smooth approximations, the arctanpinball loss has a relatively large second derivative, which makes it moresuitable to use in the second order approximation. Using this loss functionenables the simultaneous prediction of multiple quantiles, which is moreefficient and results in far fewer quantile crossings.</description><author>Laurens Sluijterman, Frank Kreuwel, Eric Cator, Tom Heskes</author><pubDate>Tue, 04 Jun 2024 14:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02293v1</guid></item><item><title>An Axiomatic Approach to Loss Aggregation and an Adapted Aggregating Algorithm</title><link>http://arxiv.org/abs/2406.02292v1</link><description>Supervised learning has gone beyond the expected risk minimization framework.Central to most of these developments is the introduction of more generalaggregation functions for losses incurred by the learner. In this paper, weturn towards online learning under expert advice. Via easily justifiedassumptions we characterize a set of reasonable loss aggregation functions asquasi-sums. Based upon this insight, we suggest a variant of the AggregatingAlgorithm tailored to these more general aggregation functions. This variantinherits most of the nice theoretical properties of the AA, such as recovery ofBayes' updating and a time-independent bound on quasi-sum regret. Finally, weargue that generalized aggregations express the attitude of the learner towardslosses.</description><author>Armando J. Cabrera Pacheco, Rabanus Derr, Robert C. Williamson</author><pubDate>Tue, 04 Jun 2024 14:11:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02292v1</guid></item><item><title>How to discretize continuous state-action spaces in Q-learning: A symbolic control approach</title><link>http://arxiv.org/abs/2406.01548v2</link><description>Q-learning is widely recognized as an effective approach for synthesizingcontrollers to achieve specific goals. However, handling challenges posed bycontinuous state-action spaces remains an ongoing research focus. This paperpresents a systematic analysis that highlights a major drawback in spacediscretization methods. To address this challenge, the paper proposes asymbolic model that represents behavioral relations, such as alternatingsimulation from abstraction to the controlled system. This relation allows forseamless application of the synthesized controller based on abstraction to theoriginal system. Introducing a novel Q-learning technique for symbolic models,the algorithm yields two Q-tables encoding optimal policies. Theoreticalanalysis demonstrates that these Q-tables serve as both upper and lower boundson the Q-values of the original system with continuous spaces. Additionally,the paper explores the correlation between the parameters of the spaceabstraction and the loss in Q-values. The resulting algorithm facilitatesachieving optimality within an arbitrary accuracy, providing control over thetrade-off between accuracy and computational complexity. The obtained resultsprovide valuable insights for selecting appropriate learning parameters andrefining the controller. The engineering relevance of the proposed Q-learningbased symbolic model is illustrated through two case studies.</description><author>Sadek Belamfedel Alaoui, Adnane Saoud</author><pubDate>Tue, 04 Jun 2024 14:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01548v2</guid></item><item><title>A Study of Optimizations for Fine-tuning Large Language Models</title><link>http://arxiv.org/abs/2406.02290v1</link><description>Fine-tuning large language models is a popular choice among users trying toadapt them for specific applications. However, fine-tuning these models is ademanding task because the user has to examine several factors, such asresource budget, runtime, model size and context length among others. Aspecific challenge is that fine-tuning is memory intensive, imposingconstraints on the required hardware memory and context length of training datathat can be handled. In this work, we share a detailed study on a variety offine-tuning optimizations across different fine-tuning scenarios. Inparticular, we assess Gradient Checkpointing, Low Rank Adaptation, DeepSpeed'sZeRO Redundancy Optimizer and Flash Attention. With a focus on memory andruntime, we examine the impact of different optimization combinations on GPUmemory usage and execution runtime during fine-tuning phase. We providerecommendation on best default optimization for balancing memory and runtimeacross diverse model sizes. We share effective strategies for fine-tuning verylarge models with tens or hundreds of billions of parameters and enabling largecontext lengths during fine-tuning. Furthermore, we propose the appropriateoptimization mixtures for fine-tuning under GPU resource limitations.</description><author>Arjun Singh, Nikhil Pandey, Anup Shirgaonkar, Pavan Manoj, Vijay Aski</author><pubDate>Tue, 04 Jun 2024 14:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02290v1</guid></item><item><title>Vertical Federated Learning for Effectiveness, Security, Applicability: A Survey</title><link>http://arxiv.org/abs/2405.17495v2</link><description>Vertical Federated Learning (VFL) is a privacy-preserving distributedlearning paradigm where different parties collaboratively learn models usingpartitioned features of shared samples, without leaking private data. Recentresearch has shown promising results addressing various challenges in VFL,highlighting its potential for practical applications in cross-domaincollaboration. However, the corresponding research is scattered and lacksorganization. To advance VFL research, this survey offers a systematic overviewof recent developments. First, we provide a history and backgroundintroduction, along with a summary of the general training protocol of VFL. Wethen revisit the taxonomy in recent reviews and analyze limitations in-depth.For a comprehensive and structured discussion, we synthesize recent researchfrom three fundamental perspectives: effectiveness, security, andapplicability. Finally, we discuss several critical future research directionsin VFL, which will facilitate the developments in this field. We provide acollection of research lists and periodically update them athttps://github.com/shentt67/VFL_Survey.</description><author>Mang Ye, Wei Shen, Bo Du, Eduard Snezhko, Vassili Kovalev, Pong C. Yuen</author><pubDate>Tue, 04 Jun 2024 14:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17495v2</guid></item><item><title>Optimised ProPainter for Video Diminished Reality Inpainting</title><link>http://arxiv.org/abs/2406.02287v1</link><description>In this paper, part of the DREAMING Challenge - Diminished Reality forEmerging Applications in Medicine through Inpainting, we introduce a refinedvideo inpainting technique optimised from the ProPainter method to meet thespecialised demands of medical imaging, specifically in the context of oral andmaxillofacial surgery. Our enhanced algorithm employs the zero-shot ProPainter,featuring optimized parameters and pre-processing, to adeptly manage thecomplex task of inpainting surgical video sequences, without requiring anytraining process. It aims to produce temporally coherent and detail-richreconstructions of occluded regions, facilitating clearer views of operativefields. The efficacy of our approach is evaluated using comprehensive metrics,positioning it as a significant advancement in the application of diminishedreality for medical purposes.</description><author>Pengze Li, Lihao Liu, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero</author><pubDate>Tue, 04 Jun 2024 14:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02287v1</guid></item><item><title>LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization</title><link>http://arxiv.org/abs/2401.06034v3</link><description>Pretrained language models (PLMs) have become remarkably adept at task andlanguage generalization. Nonetheless, they often fail when faced with unseenlanguages. In this work, we present LinguAlchemy, a regularization method thatincorporates various linguistic information covering typological, geographical,and phylogenetic features to align PLMs representation to the correspondinglinguistic information on each language. Our LinguAlchemy significantlyimproves the performance of mBERT and XLM-R on low-resource languages inmultiple downstream tasks such as intent classification, news classification,and semantic relatedness compared to fully finetuned models and displaying ahigh degree of unseen language generalization. We further introduceAlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts thelinguistic regularization weights automatically, alleviating the need forhyperparameter search.</description><author>Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Alham Fikri Aji, Genta Indra Winata, Ayu Purwarianti</author><pubDate>Tue, 04 Jun 2024 13:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06034v3</guid></item><item><title>Towards Supervised Performance on Speaker Verification with Self-Supervised Learning by Leveraging Large-Scale ASR Models</title><link>http://arxiv.org/abs/2406.02285v1</link><description>Recent advancements in Self-Supervised Learning (SSL) have shown promisingresults in Speaker Verification (SV). However, narrowing the performance gapwith supervised systems remains an ongoing challenge. Several studies haveobserved that speech representations from large-scale ASR models containvaluable speaker information. This work explores the limitations of fine-tuningthese models for SV using an SSL contrastive objective in an end-to-endapproach. Then, we propose a framework to learn speaker representations in anSSL context by fine-tuning a pre-trained WavLM with a supervised loss usingpseudo-labels. Initial pseudo-labels are derived from an SSL DINO-based modeland are iteratively refined by clustering the model embeddings. Our methodachieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art onself-supervised SV. As this performance is close to our supervised baseline of0.94% EER, this contribution is a step towards supervised performance on SVwith SSL.</description><author>Victor Miara, Theo Lepage, Reda Dehak</author><pubDate>Tue, 04 Jun 2024 13:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02285v1</guid></item><item><title>VITS : Variational Inference Thomson Sampling for contextual bandits</title><link>http://arxiv.org/abs/2307.10167v2</link><description>In this paper, we introduce and analyze a variant of the Thompson sampling(TS) algorithm for contextual bandits. At each round, traditional TS requiressamples from the current posterior distribution, which is usually intractable.To circumvent this issue, approximate inference techniques can be used andprovide samples with distribution close to the posteriors. However, currentapproximate techniques yield to either poor estimation (Laplace approximation)or can be computationally expensive (MCMC methods, Ensemble sampling...). Inthis paper, we propose a new algorithm, Varational Inference Thompson samplingVITS, based on Gaussian Variational Inference. This scheme provides powerfulposterior approximations which are easy to sample from, and is computationallyefficient, making it an ideal choice for TS. In addition, we show that VITSachieves a sub-linear regret bound of the same order in the dimension andnumber of round as traditional TS for linear contextual bandit. Finally, wedemonstrate experimentally the effectiveness of VITS on both synthetic and realworld datasets.</description><author>Pierre Clavier, Tom Huix, Alain Durmus</author><pubDate>Tue, 04 Jun 2024 13:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10167v2</guid></item><item><title>Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds</title><link>http://arxiv.org/abs/2311.03760v3</link><description>Among various acquisition functions (AFs) in Bayesian optimization (BO),Gaussian process upper confidence bound (GP-UCB) and Thompson sampling (TS) arewell-known options with established theoretical properties regarding Bayesiancumulative regret (BCR). Recently, it has been shown that a randomized variantof GP-UCB achieves a tighter BCR bound compared with GP-UCB, which we call thetighter BCR bound for brevity. Inspired by this study, this paper first showsthat TS achieves the tighter BCR bound. On the other hand, GP-UCB and TS oftenpractically suffer from manual hyperparameter tuning and over-explorationissues, respectively. Therefore, we analyze yet another AF called a probabilityof improvement from the maximum of a sample path (PIMS). We show that PIMSachieves the tighter BCR bound and avoids the hyperparameter tuning, unlikeGP-UCB. Furthermore, we demonstrate a wide range of experiments, focusing onthe effectiveness of PIMS that mitigates the practical issues of GP-UCB and TS.</description><author>Shion Takeno, Yu Inatsu, Masayuki Karasuyama, Ichiro Takeuchi</author><pubDate>Tue, 04 Jun 2024 13:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03760v3</guid></item><item><title>Test-Time Regret Minimization in Meta Reinforcement Learning</title><link>http://arxiv.org/abs/2406.02282v1</link><description>Meta reinforcement learning sets a distribution over a set of tasks on whichthe agent can train at will, then is asked to learn an optimal policy for anytest task efficiently. In this paper, we consider a finite set of tasks modeledthrough Markov decision processes with various dynamics. We assume to haveendured a long training phase, from which the set of tasks is perfectlyrecovered, and we focus on regret minimization against the optimal policy inthe unknown test task. Under a separation condition that states the existenceof a state-action pair revealing a task against another, Chen et al. (2022)show that $O(M^2 \log(H))$ regret can be achieved, where $M, H$ are the numberof tasks in the set and test episodes, respectively. In our first contribution,we demonstrate that the latter rate is nearly optimal by developing a novellower bound for test-time regret minimization under separation, showing that alinear dependence with $M$ is unavoidable. Then, we present a family ofstronger yet reasonable assumptions beyond separation, which we call strongidentifiability, enabling algorithms achieving fast rates $\log (H)$ andsublinear dependence with $M$ simultaneously. Our paper provides a newunderstanding of the statistical barriers of test-time regret minimization andwhen fast rates can be achieved.</description><author>Mirco Mutti, Aviv Tamar</author><pubDate>Tue, 04 Jun 2024 13:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02282v1</guid></item><item><title>A KL-based Analysis Framework with Applications to Non-Descent Optimization Methods</title><link>http://arxiv.org/abs/2406.02273v1</link><description>We propose a novel analysis framework for non-descent-type optimizationmethodologies in nonconvex scenarios based on the Kurdyka-Lojasiewicz property.Our framework allows covering a broad class of algorithms, including thosecommonly employed in stochastic and distributed optimization. Specifically, itenables the analysis of first-order methods that lack a sufficient descentproperty and do not require access to full (deterministic) gradientinformation. We leverage this framework to establish, for the first time,iterate convergence and the corresponding rates for the decentralized gradientmethod and federated averaging under mild assumptions. Furthermore, based onthe new analysis techniques, we show the convergence of the random reshufflingand stochastic gradient descent method without necessitating typical a prioribounded iterates assumptions.</description><author>Junwen Qiu, Bohao Ma, Xiao Li, Andre Milzarek</author><pubDate>Tue, 04 Jun 2024 13:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02273v1</guid></item><item><title>Graph Neural Networks Do Not Always Oversmooth</title><link>http://arxiv.org/abs/2406.02269v1</link><description>Graph neural networks (GNNs) have emerged as powerful tools for processingrelational data in applications. However, GNNs suffer from the problem ofoversmoothing, the property that the features of all nodes exponentiallyconverge to the same vector over layers, prohibiting the design of deep GNNs.In this work we study oversmoothing in graph convolutional networks (GCNs) byusing their Gaussian process (GP) equivalence in the limit of infinitely manyhidden features. By generalizing methods from conventional deep neural networks(DNNs), we can describe the distribution of features at the output layer ofdeep GCNs in terms of a GP: as expected, we find that typical parameter choicesfrom the literature lead to oversmoothing. The theory, however, allows us toidentify a new, nonoversmoothing phase: if the initial weights of the networkhave sufficiently large variance, GCNs do not oversmooth, and node featuresremain informative even at large depth. We demonstrate the validity of thisprediction in finite-size GCNs by training a linear classifier on their output.Moreover, using the linearization of the GCN GP, we generalize the concept ofpropagation depth of information from DNNs to GCNs. This propagation depthdiverges at the transition between the oversmoothing and non-oversmoothingphase. We test the predictions of our approach and find good agreement withfinite-size GCNs. Initializing GCNs near the transition to thenon-oversmoothing phase, we obtain networks which are both deep and expressive.</description><author>Bastian Epping, Alexandre René, Moritz Helias, Michael T. Schaub</author><pubDate>Tue, 04 Jun 2024 13:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02269v1</guid></item><item><title>Analyzing the Benefits of Prototypes for Semi-Supervised Category Learning</title><link>http://arxiv.org/abs/2406.02268v1</link><description>Categories can be represented at different levels of abstraction, fromprototypes focused on the most typical members to remembering all observedexemplars of the category. These representations have been explored in thecontext of supervised learning, where stimuli are presented with known categorylabels. We examine the benefits of prototype-based representations in aless-studied domain: semi-supervised learning, where agents must formunsupervised representations of stimuli before receiving category labels. Westudy this problem in a Bayesian unsupervised learning model called avariational auto-encoder, and we draw on recent advances in machine learning toimplement a prior that encourages the model to use abstract prototypes torepresent data. We apply this approach to image datasets and show that formingprototypes can improve semi-supervised category learning. Additionally, westudy the latent embeddings of the models and show that these prototypes allowthe models to form clustered representations without supervision, contributingto their success in downstream categorization performance.</description><author>Liyi Zhang, Logan Nelson, Thomas L. Griffiths</author><pubDate>Tue, 04 Jun 2024 13:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02268v1</guid></item><item><title>PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models</title><link>http://arxiv.org/abs/2401.15042v4</link><description>Large Language Models (LLMs) have succeeded remarkably in understandinglong-form contents. However, exploring their capability for generatinglong-form contents, such as reports and articles, has been relativelyunexplored and inadequately assessed by existing benchmarks. The prevalentevaluation methods, which predominantly rely on crowdsourcing, are recognizedfor their labor-intensive nature and lack of efficiency, whereas automatedmetrics, such as the ROUGE score, demonstrate discordance with human judgmentcriteria. In this paper, we propose ProxyQA, an innovative framework dedicatedto assessing long-text generation. ProxyQA comprises in-depth human-curatedmeta-questions spanning various domains, each accompanied by specificproxy-questions with pre-annotated answers. LLMs are tasked to generateextensive content in response to these meta-questions, by engaging an evaluatorand incorporating the generated texts as contextual background, ProxyQAassesses the generated content's quality through the evaluator's accuracy inaddressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA'sdemanding nature as a high-quality assessment tool. Human evaluationdemonstrates that the proxy-question method is notably self-consistent andaligns closely with human evaluative standards. The dataset and leaderboard isavailable at \url{https://proxy-qa.com}.</description><author>Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song</author><pubDate>Tue, 04 Jun 2024 13:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15042v4</guid></item></channel></rss>