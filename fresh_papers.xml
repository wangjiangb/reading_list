<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 25 Sep 2023 06:00:30 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>E(2)-Equivariant Graph Planning for Navigation</title><link>http://arxiv.org/abs/2309.13043v1</link><description>Learning for robot navigation presents a critical and challenging task. Thescarcity and costliness of real-world datasets necessitate efficient learningapproaches. In this letter, we exploit Euclidean symmetry in planning for 2Dnavigation, which originates from Euclidean transformations between referenceframes and enables parameter sharing. To address the challenges of unstructuredenvironments, we formulate the navigation problem as planning on a geometricgraph and develop an equivariant message passing network to perform valueiteration. Furthermore, to handle multi-camera input, we propose a learnableequivariant layer to lift features to a desired space. We conduct comprehensiveevaluations across five diverse tasks encompassing structured and unstructuredenvironments, along with maps of known and unknown, given point goals orsemantic goals. Our experiments confirm the substantial benefits on trainingefficiency, stability, and generalization.</description><author>Linfeng Zhao, Hongyu Li, Taskin Padir, Huaizu Jiang, Lawson L. S. Wong</author><pubDate>Fri, 22 Sep 2023 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13043v1</guid></item><item><title>MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation</title><link>http://arxiv.org/abs/2309.13042v1</link><description>We present MosaicFusion, a simple yet effective diffusion-based dataaugmentation approach for large vocabulary instance segmentation. Our method istraining-free and does not rely on any label supervision. Two key designsenable us to employ an off-the-shelf text-to-image diffusion model as a usefuldataset generator for object instances and mask annotations. First, we dividean image canvas into several regions and perform a single round of diffusionprocess to generate multiple instances simultaneously, conditioning ondifferent text prompts. Second, we obtain corresponding instance masks byaggregating cross-attention maps associated with object prompts across layersand diffusion time steps, followed by simple thresholding and edge-awarerefinement processing. Without bells and whistles, our MosaicFusion can producea significant amount of synthetic labeled data for both rare and novelcategories. Experimental results on the challenging LVIS long-tailed andopen-vocabulary benchmarks demonstrate that MosaicFusion can significantlyimprove the performance of existing instance segmentation models, especiallyfor rare and novel categories. Code will be released athttps://github.com/Jiahao000/MosaicFusion.</description><author>Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, Chen Change Loy</author><pubDate>Fri, 22 Sep 2023 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13042v1</guid></item><item><title>Robotic Offline RL from Internet Videos via Value-Function Pre-Training</title><link>http://arxiv.org/abs/2309.13041v1</link><description>Pre-training on Internet data has proven to be a key ingredient for broadgeneralization in many modern ML systems. What would it take to enable suchcapabilities in robotic reinforcement learning (RL)? Offline RL methods, whichlearn from datasets of robot experience, offer one way to leverage prior datainto the robotic learning pipeline. However, these methods have a "typemismatch" with video data (such as Ego4D), the largest prior datasets availablefor robotics, since video offers observation-only experience without the actionor reward annotations needed for RL methods. In this paper, we develop a systemfor leveraging large-scale human video datasets in robotic offline RL, basedentirely on learning value functions via temporal-difference learning. We showthat value learning on video datasets learns representations that are moreconducive to downstream robotic offline RL than other approaches for learningfrom video data. Our system, called V-PTR, combines the benefits ofpre-training on video data with robotic offline RL approaches that train ondiverse robot data, resulting in value functions and policies for manipulationtasks that perform better, act robustly, and generalize broadly. On severalmanipulation tasks on a real WidowX robot, our framework produces policies thatgreatly improve over prior methods. Our video and additional details can befound at https://dibyaghosh.com/vptr/</description><author>Chethan Bhateja, Derek Guo, Dibya Ghosh, Anikait Singh, Manan Tomar, Quan Vuong, Yevgen Chebotar, Sergey Levine, Aviral Kumar</author><pubDate>Fri, 22 Sep 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13041v1</guid></item><item><title>NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular Objects with Neural Refractive-Reflective Fields</title><link>http://arxiv.org/abs/2309.13039v1</link><description>Neural radiance fields (NeRF) have revolutionized the field of image-basedview synthesis. However, NeRF uses straight rays and fails to deal withcomplicated light path changes caused by refraction and reflection. Thisprevents NeRF from successfully synthesizing transparent or specular objects,which are ubiquitous in real-world robotics and A/VR applications. In thispaper, we introduce the refractive-reflective field. Taking the objectsilhouette as input, we first utilize marching tetrahedra with a progressiveencoding to reconstruct the geometry of non-Lambertian objects and then modelrefraction and reflection effects of the object in a unified framework usingFresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, wepropose a virtual cone supersampling technique. We benchmark our method ondifferent shapes, backgrounds and Fresnel terms on both real-world andsynthetic datasets. We also qualitatively and quantitatively benchmark therendering results of various editing applications, including material editing,object replacement/insertion, and environment illumination estimation. Codesand data are publicly available at https://github.com/dawning77/NeRRF.</description><author>Xiaoxue Chen, Junchen Liu, Hao Zhao, Guyue Zhou, Ya-Qin Zhang</author><pubDate>Fri, 22 Sep 2023 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13039v1</guid></item><item><title>Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?</title><link>http://arxiv.org/abs/2309.13038v1</link><description>Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly usedto evaluate model privacy risk under reconstruction attacks. Under thesemetrics, reconstructed images that are determined to resemble the original onegenerally indicate more privacy leakage. Images determined as overalldissimilar, on the other hand, indicate higher robustness against attack.However, there is no guarantee that these metrics well reflect human opinions,which, as a judgement for model privacy leakage, are more trustworthy. In thispaper, we comprehensively study the faithfulness of these hand-crafted metricsto human perception of privacy information from the reconstructed images. On 5datasets ranging from natural images, faces, to fine-grained classes, we use 4existing attack methods to reconstruct images from many differentclassification models and, for each reconstructed image, we ask multiple humanannotators to assess whether this image is recognizable. Our studies revealthat the hand-crafted metrics only have a weak correlation with the humanevaluation of privacy leakage and that even these metrics themselves oftencontradict each other. These observations suggest risks of current metrics inthe community. To address this potential risk, we propose a learning-basedmeasure called SemSim to evaluate the Semantic Similarity between the originaland reconstructed images. SemSim is trained with a standard triplet loss, usingan original image as an anchor, one of its recognizable reconstructed images asa positive sample, and an unrecognizable one as a negative. By training onhuman annotations, SemSim exhibits a greater reflection of privacy leakage onthe semantic level. We show that SemSim has a significantly higher correlationwith human judgment compared with existing metrics. Moreover, this strongcorrelation generalizes to unseen datasets, models and attack methods.</description><author>Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng</author><pubDate>Fri, 22 Sep 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13038v1</guid></item><item><title>Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations</title><link>http://arxiv.org/abs/2303.15065v2</link><description>Clinical routine and retrospective cohorts commonly include multi-parametricMagnetic Resonance Imaging; however, they are mostly acquired in differentanisotropic 2D views due to signal-to-noise-ratio and scan-time constraints.Thus acquired views suffer from poor out-of-plane resolution and affectdownstream volumetric image analysis that typically requires isotropic 3Dscans. Combining different views of multi-contrast scans into high-resolutionisotropic 3D scans is challenging due to the lack of a large training cohort,which calls for a subject-specific framework. This work proposes a novelsolution to this problem leveraging Implicit Neural Representations (INR). Ourproposed INR jointly learns two different contrasts of complementary views in acontinuous spatial function and benefits from exchanging anatomical informationbetween them. Trained within minutes on a single commodity GPU, our modelprovides realistic super-resolution across different pairs of contrasts in ourexperiments with three datasets. Using Mutual Information (MI) as a metric, wefind that our model converges to an optimum MI amongst sequences, achievinganatomically faithful reconstruction. Code is available at:https://github.com/jqmcginnis/multi_contrast_inr/</description><author>Julian McGinnis, Suprosanna Shit, Hongwei Bran Li, Vasiliki Sideri-Lampretsa, Robert Graf, Maik Dannecker, Jiazhen Pan, Nil Stolt Ansó, Mark Mühlau, Jan S. Kirschke, Daniel Rueckert, Benedikt Wiestler</author><pubDate>Fri, 22 Sep 2023 18:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15065v2</guid></item><item><title>A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference</title><link>http://arxiv.org/abs/2212.12393v3</link><description>We study the problem of combining neural networks with symbolic reasoning.Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL),such as DeepProbLog, perform exponential-time exact inference, limiting thescalability of PNL solutions. We introduce Approximate Neurosymbolic Inference(A-NeSI): a new framework for PNL that uses neural networks for scalableapproximate inference. A-NeSI 1) performs approximate inference in polynomialtime without changing the semantics of probabilistic logics; 2) is trainedusing data generated by the background knowledge; 3) can generate symbolicexplanations of predictions; and 4) can guarantee the satisfaction of logicalconstraints at test time, which is vital in safety-critical applications. Ourexperiments show that A-NeSI is the first end-to-end method to solve threeneurosymbolic tasks with exponential combinatorial scaling. Finally, ourexperiments show that A-NeSI achieves explainability and safety without apenalty in performance.</description><author>Emile van Krieken, Thiviyan Thanapalasingam, Jakub M. Tomczak, Frank van Harmelen, Annette ten Teije</author><pubDate>Fri, 22 Sep 2023 18:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12393v3</guid></item><item><title>Memory-augmented conformer for improved end-to-end long-form ASR</title><link>http://arxiv.org/abs/2309.13029v1</link><description>Conformers have recently been proposed as a promising modelling approach forautomatic speech recognition (ASR), outperforming recurrent neuralnetwork-based approaches and transformers. Nevertheless, in general, theperformance of these end-to-end models, especially attention-based models, isparticularly degraded in the case of long utterances. To address thislimitation, we propose adding a fully-differentiable memory-augmented neuralnetwork between the encoder and decoder of a conformer. This external memorycan enrich the generalization for longer utterances since it allows the systemto store and retrieve more information recurrently. Notably, we explore theneural Turing machine (NTM) that results in our proposed Conformer-NTM modelarchitecture for ASR. Experimental results using Librispeech train-clean-100and train-960 sets show that the proposed system outperforms the baselineconformer without memory for long utterances.</description><author>Carlos Carvalho, Alberto Abad</author><pubDate>Fri, 22 Sep 2023 18:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13029v1</guid></item><item><title>Learning high-level visual representations from a child's perspective without strong inductive biases</title><link>http://arxiv.org/abs/2305.15372v2</link><description>Young children develop sophisticated internal models of the world based ontheir visual experience. Can such models be learned from a child's visualexperience without strong inductive biases? To investigate this, we trainstate-of-the-art neural networks on a realistic proxy of a child's visualexperience without any explicit supervision or domain-specific inductivebiases. Specifically, we train both embedding models and generative models on200 hours of headcam video from a single child collected over two years andcomprehensively evaluate their performance in downstream tasks using variousreference models as yardsticks. On average, the best embedding models performat a respectable 70% of a high-performance ImageNet-trained model, despitesubstantial differences in training data. They also learn broad semanticcategories and object localization capabilities without explicit supervision,but they are less object-centric than models trained on all of ImageNet.Generative models trained with the same data successfully extrapolate simpleproperties of partially masked objects, like their rough outline, texture,color, or orientation, but struggle with finer object details. We replicate ourexperiments with two other children and find remarkably consistent results.Broadly useful high-level visual representations are thus robustly learnablefrom a representative sample of a child's visual experience without stronginductive biases.</description><author>A. Emin Orhan, Brenden M. Lake</author><pubDate>Fri, 22 Sep 2023 18:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15372v2</guid></item><item><title>Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement</title><link>http://arxiv.org/abs/2303.08983v3</link><description>We propose Dataset Reinforcement, a strategy to improve a dataset once suchthat the accuracy of any model architecture trained on the reinforced datasetis improved at no additional training cost for users. We propose a DatasetReinforcement strategy based on data augmentation and knowledge distillation.Our generic strategy is designed based on extensive analysis across CNN- andtransformer-based models and performing large-scale study of distillation withstate-of-the-art models with various data augmentations. We create a reinforcedversion of the ImageNet training dataset, called ImageNet+, as well asreinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trainedwith ImageNet+ are more accurate, robust, and calibrated, and transfer well todownstream tasks (e.g., segmentation and detection). As an example, theaccuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% onImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on theImageNet validation set is also reduced by 9.9%. Using this backbone withMask-RCNN for object detection on MS-COCO, the mean average precision improvesby 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers.For MobileNetV3 and Swin-Tiny, we observe significant improvements onImageNet-R/A/C of up to 20% improved robustness. Models pretrained on ImageNet+and fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4%improved accuracy. The code, datasets, and pretrained models are available athttps://github.com/apple/ml-dr.</description><author>Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, Oncel Tuzel</author><pubDate>Fri, 22 Sep 2023 18:36:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08983v3</guid></item><item><title>Graph Neural Network for Stress Predictions in Stiffened Panels Under Uniform Loading</title><link>http://arxiv.org/abs/2309.13022v1</link><description>Machine learning (ML) and deep learning (DL) techniques have gainedsignificant attention as reduced order models (ROMs) to computationallyexpensive structural analysis methods, such as finite element analysis (FEA).Graph neural network (GNN) is a particular type of neural network whichprocesses data that can be represented as graphs. This allows for efficientrepresentation of complex geometries that can change during conceptual designof a structure or a product. In this study, we propose a novel graph embeddingtechnique for efficient representation of 3D stiffened panels by consideringseparate plate domains as vertices. This approach is considered using GraphSampling and Aggregation (GraphSAGE) to predict stress distributions instiffened panels with varying geometries. A comparison between afinite-element-vertex graph representation is conducted to demonstrate theeffectiveness of the proposed approach. A comprehensive parametric study isperformed to examine the effect of structural geometry on the predictionperformance. Our results demonstrate the immense potential of graph neuralnetworks with the proposed graph embedding method as robust reduced-ordermodels for 3D structures.</description><author>Yuecheng Cai, Jasmin Jelovica</author><pubDate>Fri, 22 Sep 2023 18:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13022v1</guid></item><item><title>A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection</title><link>http://arxiv.org/abs/2309.13021v1</link><description>Precise crop yield prediction is essential for improving agriculturalpractices and ensuring crop resilience in varying climates. Integrating weatherdata across the growing season, especially for different crop varieties, iscrucial for understanding their adaptability in the face of climate change. Inthe MLCAS2021 Crop Yield Prediction Challenge, we utilized a dataset comprising93,028 training records to forecast yields for 10,337 test records, covering159 locations across 28 U.S. states and Canadian provinces over 13 years(2003-2015). This dataset included details on 5,838 distinct genotypes anddaily weather data for a 214-day growing season, enabling comprehensiveanalysis. As one of the winning teams, we developed two novel convolutionalneural network (CNN) architectures: the CNN-DNN model, combining CNN andfully-connected networks, and the CNN-LSTM-DNN model, with an added LSTM layerfor weather variables. Leveraging the Generalized Ensemble Method (GEM), wedetermined optimal model weights, resulting in superior performance compared tobaseline models. The GEM model achieved lower RMSE (5.55% to 39.88%), reducedMAE (5.34% to 43.76%), and higher correlation coefficients (1.1% to 10.79%)when evaluated on test data. We applied the CNN-DNN model to identifytop-performing genotypes for various locations and weather conditions, aidinggenotype selection based on weather variables. Our data-driven approach isvaluable for scenarios with limited testing years. Additionally, a featureimportance analysis using RMSE change highlighted the significance of location,MG, year, and genotype, along with the importance of weather variables MDNI andAP.</description><author>Zahra Khalilzadeh, Motahareh Kashanian, Saeed Khaki, Lizhi Wang</author><pubDate>Fri, 22 Sep 2023 18:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13021v1</guid></item><item><title>Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model</title><link>http://arxiv.org/abs/2309.13018v1</link><description>Neural network pruning offers an effective method for compressing amultilingual automatic speech recognition (ASR) model with minimal performanceloss. However, it entails several rounds of pruning and re-training needed tobe run for each language. In this work, we propose the use of an adaptivemasking approach in two scenarios for pruning a multilingual ASR modelefficiently, each resulting in sparse monolingual models or a sparsemultilingual model (named as Dynamic ASR Pathways). Our approach dynamicallyadapts the sub-network, avoiding premature decisions about a fixed sub-networkstructure. We show that our approach outperforms existing pruning methods whentargeting sparse monolingual models. Further, we illustrate that Dynamic ASRPathways jointly discovers and trains better sub-networks (pathways) of asingle multilingual model by adapting from different sub-networkinitializations, thereby reducing the need for language-specific pruning.</description><author>Jiamin Xie, Ke Li, Jinxi Guo, Andros Tjandra, Yuan Shangguan, Leda Sari, Chunyang Wu, Junteng Jia, Jay Mahadeokar, Ozlem Kalinli</author><pubDate>Fri, 22 Sep 2023 18:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13018v1</guid></item><item><title>Improving Generative Model-based Unfolding with Schrödinger Bridges</title><link>http://arxiv.org/abs/2308.12351v2</link><description>Machine learning-based unfolding has enabled unbinned and high-dimensionaldifferential cross section measurements. Two main approaches have emerged inthis research area: one based on discriminative models and one based ongenerative models. The main advantage of discriminative models is that theylearn a small correction to a starting simulation while generative models scalebetter to regions of phase space with little data. We propose to useSchroedinger Bridges and diffusion models to create SBUnfold, an unfoldingapproach that combines the strengths of both discriminative and generativemodels. The key feature of SBUnfold is that its generative model maps one setof events into another without having to go through a known probability densityas is the case for normalizing flows and standard diffusion models. We showthat SBUnfold achieves excellent performance compared to state of the artmethods on a synthetic Z+jets dataset.</description><author>Sascha Diefenbacher, Guan-Horng Liu, Vinicius Mikuni, Benjamin Nachman, Weili Nie</author><pubDate>Fri, 22 Sep 2023 18:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12351v2</guid></item><item><title>Understanding Deep Gradient Leakage via Inversion Influence Functions</title><link>http://arxiv.org/abs/2309.13016v1</link><description>Deep Gradient Leakage (DGL) is a highly effective attack that recoversprivate training images from gradient vectors. This attack casts significantprivacy challenges on distributed learning from clients with sensitive data,where clients are required to share gradients. Defending against such attacksrequires but lacks an understanding of when and how privacy leakage happens,mostly because of the black-box nature of deep networks. In this paper, wepropose a novel Inversion Influence Function (I$^2$F) that establishes aclosed-form connection between the recovered images and the private gradientsby implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$Fis scalable for analyzing deep networks, requiring only oracle access togradients and Jacobian-vector products. We empirically demonstrate that I$^2$Feffectively approximated the DGL generally on different model architectures,datasets, attack implementations, and noise-based defenses. With this noveltool, we provide insights into effective gradient perturbation directions, theunfairness of privacy protection, and privacy-preferred model initialization.Our codes are provided inhttps://github.com/illidanlab/inversion-influence-function.</description><author>Haobo Zhang, Junyuan Hong, Yuyang Deng, Mehrdad Mahdavi, Jiayu Zhou</author><pubDate>Fri, 22 Sep 2023 18:26:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13016v1</guid></item><item><title>Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design</title><link>http://arxiv.org/abs/2309.13015v1</link><description>Sparse training is one of the promising techniques to reduce thecomputational cost of DNNs while retaining high accuracy. In particular, N:Mfine-grained structured sparsity, where only N out of consecutive M elementscan be nonzero, has attracted attention due to its hardware-friendly patternand capability of achieving a high sparse ratio. However, the potential toaccelerate N:M sparse DNN training has not been fully exploited, and there is alack of efficient hardware supporting N:M sparse training. To tackle thesechallenges, this paper presents a computation-efficient training scheme for N:Msparse DNNs using algorithm, architecture, and dataflow co-design. At thealgorithm level, a bidirectional weight pruning method, dubbed BDWP, isproposed to leverage the N:M sparsity of weights during both forward andbackward passes of DNN training, which can significantly reduce thecomputational cost while maintaining model accuracy. At the architecture level,a sparse accelerator for DNN training, namely SAT, is developed to neatlysupport both the regular dense operations and the computation-efficient N:Msparse operations. At the dataflow level, multiple optimization methods rangingfrom interleave mapping, pre-generation of N:M sparse weights, and offlinescheduling, are proposed to boost the computational efficiency of SAT. Finally,the effectiveness of our training scheme is evaluated on a Xilinx VCU1525 FPGAcard using various DNN models and datasets. Experimental results show the SATaccelerator with the BDWP sparse training method under 2:8 sparse ratioachieves an average speedup of 1.75x over that with the dense training,accompanied by a negligible accuracy loss of 0.56% on average. Furthermore, ourproposed training scheme significantly improves the training throughput by2.97~25.22x and the energy efficiency by 1.36~3.58x over prior FPGA-basedaccelerators.</description><author>Chao Fang, Wei Sun, Aojun Zhou, Zhongfeng Wang</author><pubDate>Fri, 22 Sep 2023 18:26:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13015v1</guid></item><item><title>Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning</title><link>http://arxiv.org/abs/2307.01849v2</link><description>Sequence modeling approaches have shown promising results in robot imitationlearning. Recently, diffusion models have been adopted for behavioral cloningin a sequence modeling fashion, benefiting from their exceptional capabilitiesin modeling complex data distributions. The standard diffusion-based policyiteratively generates action sequences from random noise conditioned on theinput states. Nonetheless, the model for diffusion policy can be furtherimproved in terms of visual representations. In this work, we propose CrosswayDiffusion, a simple yet effective method to enhance diffusion-based visuomotorpolicy learning via a carefully designed state decoder and an auxiliaryself-supervised learning (SSL) objective. The state decoder reconstructs rawimage pixels and other state information from the intermediate representationsof the reverse diffusion process. The whole model is jointly optimized by theSSL objective and the original diffusion loss. Our experiments demonstrate theeffectiveness of Crossway Diffusion in various simulated and real-world robottasks, confirming its consistent advantages over the standard diffusion-basedpolicy and substantial improvements over the baselines.</description><author>Xiang Li, Varun Belagali, Jinghuan Shang, Michael S. Ryoo</author><pubDate>Fri, 22 Sep 2023 18:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01849v2</guid></item><item><title>Performance Analysis of UNet and Variants for Medical Image Segmentation</title><link>http://arxiv.org/abs/2309.13013v1</link><description>Medical imaging plays a crucial role in modern healthcare by providingnon-invasive visualisation of internal structures and abnormalities, enablingearly disease detection, accurate diagnosis, and treatment planning. This studyaims to explore the application of deep learning models, particularly focusingon the UNet architecture and its variants, in medical image segmentation. Weseek to evaluate the performance of these models across various challengingmedical image segmentation tasks, addressing issues such as imagenormalization, resizing, architecture choices, loss function design, andhyperparameter tuning. The findings reveal that the standard UNet, whenextended with a deep network layer, is a proficient medical image segmentationmodel, while the Res-UNet and Attention Res-UNet architectures demonstratesmoother convergence and superior performance, particularly when handling fineimage details. The study also addresses the challenge of high class imbalancethrough careful preprocessing and loss function definitions. We anticipate thatthe results of this study will provide useful insights for researchers seekingto apply these models to new medical imaging problems and offer guidance andbest practices for their implementation.</description><author>Walid Ehab, Yongmin Li</author><pubDate>Fri, 22 Sep 2023 18:20:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13013v1</guid></item><item><title>ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs</title><link>http://arxiv.org/abs/2309.13007v1</link><description>Large Language Models (LLMs) still struggle with complex reasoning tasks.Motivated by the society of minds (Minsky, 1988), we propose ReConcile, amulti-model multi-agent framework designed as a round table conference amongdiverse LLM agents to foster diverse thoughts and discussion for improvedconsensus. ReConcile enhances the reasoning capabilities of LLMs by holdingmultiple rounds of discussion, learning to convince other agents to improvetheir answers, and employing a confidence-weighted voting mechanism. In eachround, ReConcile initiates discussion between agents via a 'discussion prompt'that consists of (a) grouped answers and explanations generated by each agentin the previous round, (b) their uncertainties, and (c) demonstrations ofanswer-rectifying human explanations, used for convincing other agents. Thisdiscussion prompt enables each agent to revise their responses in light ofinsights from other agents. Once a consensus is reached and the discussionends, ReConcile determines the final answer by leveraging the confidence ofeach agent in a weighted voting scheme. We implement ReConcile with ChatGPT,Bard, and Claude2 as the three agents. Our experimental results on variousbenchmarks demonstrate that ReConcile significantly enhances the reasoningperformance of the agents (both individually and as a team), surpassing priorsingle-agent and multi-agent baselines by 7.7% and also outperforming GPT-4 onsome of these datasets. We also experiment with GPT-4 itself as one of theagents in ReConcile and demonstrate that its initial performance also improvesby absolute 10.0% through discussion and feedback from other agents. Finally,we also analyze the accuracy after every round and observe that ReConcileachieves better and faster consensus between agents, compared to a multi-agentdebate baseline. Our code is available at: https://github.com/dinobby/ReConcile</description><author>Justin Chih-Yao Chen, Swarnadeep Saha, Mohit Bansal</author><pubDate>Fri, 22 Sep 2023 18:12:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13007v1</guid></item><item><title>Deep3DSketch+: Rapid 3D Modeling from Single Free-hand Sketches</title><link>http://arxiv.org/abs/2309.13006v1</link><description>The rapid development of AR/VR brings tremendous demands for 3D content.While the widely-used Computer-Aided Design (CAD) method requires atime-consuming and labor-intensive modeling process, sketch-based 3D modelingoffers a potential solution as a natural form of computer-human interaction.However, the sparsity and ambiguity of sketches make it challenging to generatehigh-fidelity content reflecting creators' ideas. Precise drawing from multipleviews or strategic step-by-step drawings is often required to tackle thechallenge but is not friendly to novice users. In this work, we introduce anovel end-to-end approach, Deep3DSketch+, which performs 3D modeling using onlya single free-hand sketch without inputting multiple sketches or viewinformation. Specifically, we introduce a lightweight generation network forefficient inference in real-time and a structural-aware adversarial trainingapproach with a Stroke Enhancement Module (SEM) to capture the structuralinformation to facilitate learning of the realistic and fine-detailed shapestructures for high-fidelity performance. Extensive experiments demonstratedthe effectiveness of our approach with the state-of-the-art (SOTA) performanceon both synthetic and real datasets.</description><author>Tianrun Chen, Chenglong Fu, Ying Zang, Lanyun Zhu, Jia Zhang, Papa Mao, Lingyun Sun</author><pubDate>Fri, 22 Sep 2023 18:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13006v1</guid></item><item><title>Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains</title><link>http://arxiv.org/abs/2309.13005v1</link><description>Recognizing the prevalence of domain shift as a common challenge in machinelearning, various domain generalization (DG) techniques have been developed toenhance the performance of machine learning systems when dealing without-of-distribution (OOD) data. Furthermore, in real-world scenarios, datadistributions can gradually change across a sequence of sequential domains.While current methodologies primarily focus on improving model effectivenesswithin these new domains, they often overlook fairness issues throughout thelearning process. In response, we introduce an innovative framework calledCounterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder(CDSAE). This approach effectively separates environmental information andsensitive attributes from the embedded representation of classificationfeatures. This concurrent separation not only greatly improves modelgeneralization across diverse and unfamiliar domains but also effectivelyaddresses challenges related to unfair classification. Our strategy is rootedin the principles of causal inference to tackle these dual issues. To examinethe intricate relationship between semantic information, sensitive attributes,and environmental cues, we systematically categorize exogenous uncertaintyfactors into four latent variables: 1) semantic information influenced bysensitive attributes, 2) semantic information unaffected by sensitiveattributes, 3) environmental cues influenced by sensitive attributes, and 4)environmental cues unaffected by sensitive attributes. By incorporatingfairness regularization, we exclusively employ semantic information forclassification purposes. Empirical validation on synthetic and real-worlddatasets substantiates the effectiveness of our approach, demonstratingimproved accuracy levels while ensuring the preservation of fairness in theevolving landscape of continuous domains.</description><author>Yujie Lin, Chen Zhao, Minglai Shao, Baoluo Meng, Xujiang Zhao, Haifeng Chen</author><pubDate>Fri, 22 Sep 2023 18:08:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13005v1</guid></item><item><title>Expressive variational quantum circuits provide inherent privacy in federated learning</title><link>http://arxiv.org/abs/2309.13002v1</link><description>Federated learning has emerged as a viable distributed solution to trainmachine learning models without the actual need to share data with the centralaggregator. However, standard neural network-based federated learning modelshave been shown to be susceptible to data leakage from the gradients sharedwith the server. In this work, we introduce federated learning with variationalquantum circuit model built using expressive encoding maps coupled withoverparameterized ans\"atze. We show that expressive maps lead to inherentprivacy against gradient inversion attacks, while overparameterization ensuresmodel trainability. Our privacy framework centers on the complexity of solvingthe system of high-degree multivariate Chebyshev polynomials generated by thegradients of quantum circuit. We present compelling arguments highlighting theinherent difficulty in solving these equations, both in exact and approximatescenarios. Additionally, we delve into machine learning-based attack strategiesand establish a direct connection between overparameterization in the originalfederated learning model and underparameterization in the attack model.Furthermore, we provide numerical scaling arguments showcasing thatunderparameterization of the expressive map in the attack model leads to theloss landscape being swamped with exponentially many spurious local minimapoints, thus making it extremely hard to realize a successful attack. Thisprovides a strong claim, for the first time, that the nature of quantum machinelearning models inherently helps prevent data leakage in federated learning.</description><author>Niraj Kumar, Jamie Heredge, Changhao Li, Shaltiel Eloul, Shree Hari Sureshbabu, Marco Pistoia</author><pubDate>Fri, 22 Sep 2023 18:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13002v1</guid></item><item><title>"Task-relevant autoencoding" enhances machine learning for human neuroscience</title><link>http://arxiv.org/abs/2208.08478v2</link><description>In human neuroscience, machine learning can help reveal lower-dimensionalneural representations relevant to subjects' behavior. However,state-of-the-art models typically require large datasets to train, so are proneto overfitting on human neuroimaging data that often possess few samples butmany input dimensions. Here, we capitalized on the fact that the features weseek in human neuroscience are precisely those relevant to subjects' behavior.We thus developed a Task-Relevant Autoencoder via Classifier Enhancement(TRACE), and tested its ability to extract behaviorally-relevant, separablerepresentations compared to a standard autoencoder, a variational autoencoder,and principal component analysis for two severely truncated machine learningdatasets. We then evaluated all models on fMRI data from 59 subjects whoobserved animals and objects. TRACE outperformed all models nearlyunilaterally, showing up to 12% increased classification accuracy and up to 56%improvement in discovering "cleaner", task-relevant representations. Theseresults showcase TRACE's potential for a wide variety of data related to humanbehavior.</description><author>Seyedmehdi Orouji, Vincent Taschereau-Dumouchel, Aurelio Cortese, Brian Odegaard, Cody Cushing, Mouslim Cherkaoui, Mitsuo Kawato, Hakwan Lau, Megan A. K. Peters</author><pubDate>Fri, 22 Sep 2023 18:04:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08478v2</guid></item><item><title>Audience-specific Explanations for Machine Translation</title><link>http://arxiv.org/abs/2309.12998v1</link><description>In machine translation, a common problem is that the translation of certainwords even if translated can cause incomprehension of the target languageaudience due to different cultural backgrounds. A solution to solve thisproblem is to add explanations for these words. In a first step, we thereforeneed to identify these words or phrases. In this work we explore techniques toextract example explanations from a parallel corpus. However, the sparsity ofsentences containing words that need to be explained makes building thetraining dataset extremely difficult. In this work, we propose a semi-automatictechnique to extract these explanations from a large parallel corpus.Experiments on English-&gt;German language pair show that our method is able toextract sentence so that more than 10% of the sentences contain explanation,while only 1.9% of the original sentences contain explanations. In addition,experiments on English-&gt;French and English-&gt;Chinese language pairs also showsimilar conclusions. This is therefore an essential first automatic step tocreate a explanation dataset. Furthermore we show that the technique is robustfor all three language pairs.</description><author>Renhan Lou, Jan Niehues</author><pubDate>Fri, 22 Sep 2023 18:00:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12998v1</guid></item><item><title>Scaling Limits of the Wasserstein information matrix on Gaussian Mixture Models</title><link>http://arxiv.org/abs/2309.12997v1</link><description>We consider the Wasserstein metric on the Gaussian mixture models (GMMs),which is defined as the pullback of the full Wasserstein metric on the space ofsmooth probability distributions with finite second moment. It derives a classof Wasserstein metrics on probability simplices over one-dimensional boundedhomogeneous lattices via a scaling limit of the Wasserstein metric on GMMs.Specifically, for a sequence of GMMs whose variances tend to zero, we provethat the limit of the Wasserstein metric exists after certain renormalization.Generalizations of this metric in general GMMs are established, includinginhomogeneous lattice models whose lattice gaps are not the same, extended GMMswhose mean parameters of Gaussian components can also change, and thesecond-order metric containing high-order information of the scaling limit. Wefurther study the Wasserstein gradient flows on GMMs for three typicalfunctionals: potential, internal, and interaction energies. Numerical examplesdemonstrate the effectiveness of the proposed GMM models for approximatingWasserstein gradient flows.</description><author>Wuchen Li, Jiaxi Zhao</author><pubDate>Fri, 22 Sep 2023 17:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12997v1</guid></item><item><title>Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count</title><link>http://arxiv.org/abs/2309.12996v1</link><description>This paper introduces the Point Cloud Network (PCN) architecture, a novelimplementation of linear layers in deep learning networks, and providesempirical evidence to advocate for its preference over the MultilayerPerceptron (MLP) in linear layers. We train several models, including theoriginal AlexNet, using both MLP and PCN architectures for direct comparison oflinear layers (Krizhevsky et al., 2012). The key results collected are modelparameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100datasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet,achieves comparable efficacy (test accuracy) to the original architecture witha 99.5% reduction of parameters in its linear layers. All training is done oncloud RTX 4090 GPUs, leveraging pytorch for model construction and training.Code is provided for anyone to reproduce the trials from this paper.</description><author>Charles Hetterich</author><pubDate>Fri, 22 Sep 2023 17:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12996v1</guid></item><item><title>The Topology and Geometry of Neural Representations</title><link>http://arxiv.org/abs/2309.11028v2</link><description>A central question for neuroscience is how to characterize brainrepresentations of perceptual and cognitive content. An ideal characterizationshould distinguish different functional regions with robustness to noise andidiosyncrasies of individual brains that do not correspond to computationaldifferences. Previous studies have characterized brain representations by theirrepresentational geometry, which is defined by the representationaldissimilarity matrix (RDM), a summary statistic that abstracts from the rolesof individual neurons (or responses channels) and characterizes thediscriminability of stimuli. Here we explore a further step of abstraction:from the geometry to the topology of brain representations. We proposetopological representational similarity analysis (tRSA), an extension ofrepresentational similarity analysis (RSA) that uses a family ofgeo-topological summary statistics that generalizes the RDM to characterize thetopology while de-emphasizing the geometry. We evaluate this new family ofstatistics in terms of the sensitivity and specificity for model selectionusing both simulations and functional MRI (fMRI) data. In the simulations, theground truth is a data-generating layer representation in a neural networkmodel and the models are the same and other layers in different model instances(trained from different random seeds). In fMRI, the ground truth is a visualarea and the models are the same and other areas measured in differentsubjects. Results show that topology-sensitive characterizations of populationcodes are robust to noise and interindividual variability and maintainexcellent sensitivity to the unique representational signatures of differentneural network layers and brain regions.</description><author>Baihan Lin, Nikolaus Kriegeskorte</author><pubDate>Fri, 22 Sep 2023 17:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11028v2</guid></item><item><title>How to Index Item IDs for Recommendation Foundation Models</title><link>http://arxiv.org/abs/2305.06569v5</link><description>Recommendation foundation model utilizes large language models (LLM) forrecommendation by converting recommendation tasks into natural language tasks.It enables generative recommendation which directly generates the item(s) torecommend rather than calculating a ranking score for each and every candidateitem in traditional recommendation models, simplifying the recommendationpipeline from multi-stage filtering to single-stage filtering. To avoidgenerating excessively long text and hallucinated recommendation when decidingwhich item(s) to recommend, creating LLM-compatible item IDs to uniquelyidentify each item is essential for recommendation foundation models. In thisstudy, we systematically examine the item indexing problem for recommendationfoundation models, using P5 as an example of backbone model. To emphasize theimportance of item indexing, we first discuss the issues of several trivialitem indexing methods, such as independent indexing, title indexing, and randomindexing. We then propose four simple yet effective solutions, includingsequential indexing, collaborative indexing, semantic (content-based) indexing,and hybrid indexing. Our study highlights the significant influence of itemindexing methods on the performance of LLM-based recommendation, and ourresults on real-world datasets validate the effectiveness of our proposedsolutions. The research also demonstrates how recent advances on languagemodeling and traditional IR principles such as indexing can help each other forbetter learning and inference.</description><author>Wenyue Hua, Shuyuan Xu, Yingqiang Ge, Yongfeng Zhang</author><pubDate>Fri, 22 Sep 2023 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06569v5</guid></item><item><title>Deep learning probability flows and entropy production rates in active matter</title><link>http://arxiv.org/abs/2309.12991v1</link><description>Active matter systems, from self-propelled colloids to motile bacteria, arecharacterized by the conversion of free energy into useful work at themicroscopic scale. These systems generically involve physics beyond the reachof equilibrium statistical mechanics, and a persistent challenge has been tounderstand the nature of their nonequilibrium states. The entropy productionrate and the magnitude of the steady-state probability current providequantitative ways to do so by measuring the breakdown of time-reversal symmetryand the strength of nonequilibrium transport of measure. Yet, their efficientcomputation has remained elusive, as they depend on the system's unknown andhigh-dimensional probability density. Here, building upon recent advances ingenerative modeling, we develop a deep learning framework that estimates thescore of this density. We show that the score, together with the microscopicequations of motion, gives direct access to the entropy production rate, theprobability current, and their decomposition into local contributions fromindividual particles, spatial regions, and degrees of freedom. To represent thescore, we introduce a novel, spatially-local transformer-based networkarchitecture that learns high-order interactions between particles whilerespecting their underlying permutation symmetry. We demonstrate the broadutility and scalability of the method by applying it to severalhigh-dimensional systems of interacting active particles undergoingmotility-induced phase separation (MIPS). We show that a single instance of ournetwork trained on a system of 4096 particles at one packing fraction cangeneralize to other regions of the phase diagram, including systems with asmany as 32768 particles. We use this observation to quantify the spatialstructure of the departure from equilibrium in MIPS as a function of the numberof particles and the packing fraction.</description><author>Nicholas M. Boffi, Eric Vanden-Eijnden</author><pubDate>Fri, 22 Sep 2023 17:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12991v1</guid></item><item><title>Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence</title><link>http://arxiv.org/abs/2301.11476v2</link><description>Many policy optimization approaches in reinforcement learning incorporate aKullback-Leilbler (KL) divergence to the previous policy, to prevent the policyfrom changing too quickly. This idea was initially proposed in a seminal paperon Conservative Policy Iteration, with approximations given by algorithms likeTRPO and Munchausen Value Iteration (MVI). We continue this line of work byinvestigating a generalized KL divergence -- called the Tsallis KL divergence-- which use the $q$-logarithm in the definition. The approach is a strictgeneralization, as $q = 1$ corresponds to the standard KL divergence; $q &gt; 1$provides a range of new options. We characterize the types of policies learnedunder the Tsallis KL, and motivate when $q &gt;1$ could be beneficial. To obtain apractical algorithm that incorporates Tsallis KL regularization, we extend MVI,which is one of the simplest approaches to incorporate KL regularization. Weshow that this generalized MVI($q$) obtains significant improvements over thestandard MVI($q = 1$) across 35 Atari games.</description><author>Lingwei Zhu, Zheng Chen, Matthew Schlegel, Martha White</author><pubDate>Fri, 22 Sep 2023 17:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11476v2</guid></item><item><title>The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement</title><link>http://arxiv.org/abs/2306.09633v6</link><description>Reinforcement learning (RL) for physical design of silicon chips in a Google2021 Nature paper stirred controversy due to poorly documented claims thatraised eyebrows and drew critical media coverage. The paper withheld criticalmethodology steps and most inputs needed to reproduce results. Ourmeta-analysis shows how two separate evaluations filled in the gaps anddemonstrated that Google RL lags behind (i) human designers, (ii) a well-knownalgorithm (Simulated Annealing), and (iii) generally-available commercialsoftware, while being slower; and in a 2023 open research contest, RL methodsweren't in top 5. Crosschecked data indicate that the integrity of the Naturepaper is substantially undermined owing to errors in conduct, analysis andreporting. Before publishing, Google rebuffed internal allegations of fraud. Wenote policy implications and conclusions for chip design.</description><author>Igor L. Markov</author><pubDate>Fri, 22 Sep 2023 17:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09633v6</guid></item><item><title>Efficient Reinforcement Learning for Jumping Monopods</title><link>http://arxiv.org/abs/2309.07038v2</link><description>In this work, we consider the complex control problem of making a monopodreach a target with a jump. The monopod can jump in any direction and theterrain underneath its foot can be uneven. This is a template of a much largerclass of problems, which are extremely challenging and computationallyexpensive to solve using standard optimisation-based techniques. ReinforcementLearning (RL) could be an interesting alternative, but the application of anend-to-end approach in which the controller must learn everything from scratch,is impractical. The solution advocated in this paper is to guide the learningprocess within an RL framework by injecting physical knowledge. This expedientbrings to widespread benefits, such as a drastic reduction of the learningtime, and the ability to learn and compensate for possible errors in thelow-level controller executing the motion. We demonstrate the advantage of ourapproach with respect to both optimization-based and end-to-end RL approaches.</description><author>Riccardo Bussola, Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Luigi Palopoli</author><pubDate>Fri, 22 Sep 2023 17:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07038v2</guid></item><item><title>License Plate Recognition Based On Multi-Angle View Model</title><link>http://arxiv.org/abs/2309.12972v1</link><description>In the realm of research, the detection/recognition of text withinimages/videos captured by cameras constitutes a highly challenging problem forresearchers. Despite certain advancements achieving high accuracy, currentmethods still require substantial improvements to be applicable in practicalscenarios. Diverging from text detection in images/videos, this paper addressesthe issue of text detection within license plates by amalgamating multipleframes of distinct perspectives. For each viewpoint, the proposed methodextracts descriptive features characterizing the text components of the licenseplate, specifically corner points and area. Concretely, we present threeviewpoints: view-1, view-2, and view-3, to identify the nearest neighboringcomponents facilitating the restoration of text components from the samelicense plate line based on estimations of similarity levels and distancemetrics. Subsequently, we employ the CnOCR method for text recognition withinlicense plates. Experimental results on the self-collected dataset(PTITPlates), comprising pairs of images in various scenarios, and the publiclyavailable Stanford Cars Dataset, demonstrate the superiority of the proposedmethod over existing approaches.</description><author>Dat Tran-Anh, Khanh Linh Tran, Hoai-Nam Vu</author><pubDate>Fri, 22 Sep 2023 17:12:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12972v1</guid></item><item><title>Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes</title><link>http://arxiv.org/abs/2309.12971v1</link><description>Despite the recent successes of vanilla Graph Neural Networks (GNNs) on manytasks, their foundation on pairwise interaction networks inherently limitstheir capacity to discern latent higher-order interactions in complex systems.To bridge this capability gap, we propose a novel approach exploiting the richmathematical theory of simplicial complexes (SCs) - a robust tool for modelinghigher-order interactions. Current SC-based GNNs are burdened by highcomplexity and rigidity, and quantifying higher-order interaction strengthsremains challenging. Innovatively, we present a higher-order Flower-Petals (FP)model, incorporating FP Laplacians into SCs. Further, we introduce aHigher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians,capable of discerning intrinsic features across varying topological scales. Byemploying learnable graph filters, a parameter group within each FP Laplaciandomain, we can identify diverse patterns where the filters' weights serve as aquantifiable measure of higher-order interaction strengths. The theoreticalunderpinnings of HiGCN's advanced expressiveness are rigorously demonstrated.Additionally, our empirical investigations reveal that the proposed modelaccomplishes state-of-the-art (SOTA) performance on a range of graph tasks andprovides a scalable and flexible solution to explore higher-order interactionsin graphs.</description><author>Yiming Huang, Yujie Zeng, Qiang Wu, Linyuan Lü</author><pubDate>Fri, 22 Sep 2023 17:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12971v1</guid></item><item><title>PI-RADS v2 Compliant Automated Segmentation of Prostate Zones Using co-training Motivated Multi-task Dual-Path CNN</title><link>http://arxiv.org/abs/2309.12970v1</link><description>The detailed images produced by Magnetic Resonance Imaging (MRI) providelife-critical information for the diagnosis and treatment of prostate cancer.To provide standardized acquisition, interpretation and usage of the complexMRI images, the PI-RADS v2 guideline was proposed. An automated segmentationfollowing the guideline facilitates consistent and precise lesion detection,staging and treatment. The guideline recommends a division of the prostate intofour zones, PZ (peripheral zone), TZ (transition zone), DPU (distal prostaticurethra) and AFS (anterior fibromuscular stroma). Not every zone shares aboundary with the others and is present in every slice. Further, therepresentations captured by a single model might not suffice for all zones.This motivated us to design a dual-branch convolutional neural network (CNN),where each branch captures the representations of the connected zonesseparately. Further, the representations from different branches actcomplementary to each other at the second stage of training, where they arefine-tuned through an unsupervised loss. The loss penalises the difference inpredictions from the two branches for the same class. We also incorporatemulti-task learning in our framework to further improve the segmentationaccuracy. The proposed approach improves the segmentation accuracy of thebaseline (mean absolute symmetric distance) by 7.56%, 11.00%, 58.43% and 19.67%for PZ, TZ, DPU and AFS zones respectively.</description><author>Arnab Das, Suhita Ghosh, Sebastian Stober</author><pubDate>Fri, 22 Sep 2023 17:10:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12970v1</guid></item><item><title>Detect Every Thing with Few Examples</title><link>http://arxiv.org/abs/2309.12969v1</link><description>Open-set object detection aims at detecting arbitrary categories beyond thoseseen during training. Most recent advancements have adopted the open-vocabularyparadigm, utilizing vision-language backbones to represent categories withlanguage. In this paper, we introduce DE-ViT, an open-set object detector thatemploys vision-only DINOv2 backbones and learns new categories through exampleimages instead of language. To improve general detection ability, we transformmulti-classification tasks into binary classification tasks while bypassingper-class inference, and propose a novel region propagation technique forlocalization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shotobject detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms theopen-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViTsurpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot andone-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabularySoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available athttps://github.com/mlzxy/devit.</description><author>Xinyu Zhang, Yuting Wang, Abdeslam Boularias</author><pubDate>Fri, 22 Sep 2023 17:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12969v1</guid></item><item><title>Assessment of the Reliablity of a Model's Decision by Generalizing Attribution to the Wavelet Domain</title><link>http://arxiv.org/abs/2305.14979v2</link><description>Neural networks have shown remarkable performance in computer vision, buttheir deployment in numerous scientific and technical fields is challenging dueto their black-box nature. Scientists and practitioners need to evaluate thereliability of a decision, i.e., to know simultaneously if a model relies onthe relevant features and whether these features are robust to imagecorruptions. Existing attribution methods aim to provide human-understandableexplanations by highlighting important regions in the image domain, but fail tofully characterize a decision process's reliability. To bridge this gap, weintroduce the Wavelet sCale Attribution Method (WCAM), a generalization ofattribution from the pixel domain to the space-scale domain using wavelettransforms. Attribution in the wavelet domain reveals where {\it and} on whatscales the model focuses, thus enabling us to assess whether a decision isreliable.</description><author>Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint Drenan, Philippe Blanc</author><pubDate>Fri, 22 Sep 2023 17:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14979v2</guid></item><item><title>Nested Event Extraction upon Pivot Element Recogniton</title><link>http://arxiv.org/abs/2309.12960v1</link><description>Nested Event Extraction (NEE) aims to extract complex event structures wherean event contains other events as its arguments recursively. Nested eventsinvolve a kind of Pivot Elements (PEs) that simultaneously act as arguments ofouter events and as triggers of inner events, and thus connect them into nestedstructures. This special characteristic of PEs brings challenges to existingNEE methods, as they cannot well cope with the dual identities of PEs.Therefore, this paper proposes a new model, called PerNee, which extractsnested events mainly based on recognizing PEs. Specifically, PerNee firstrecognizes the triggers of both inner and outer events and further recognizesthe PEs via classifying the relation type between trigger pairs. In order toobtain better representations of triggers and arguments to further improve NEEperformance, it incorporates the information of both event types and argumentroles into PerNee through prompt learning. Since existing NEE datasets (e.g.,Genia11) are limited to specific domains and contain a narrow range of eventtypes with nested structures, we systematically categorize nested events ingeneric domain and construct a new NEE dataset, namely ACE2005-Nest.Experimental results demonstrate that PerNee consistently achievesstate-of-the-art performance on ACE2005-Nest, Genia11 and Genia13.</description><author>Weicheng Ren, Zixuan Li, Xiaolong Jin, Long Bai, Miao Su, Yantao Liu, Saiping Guan, Jiafeng Guo, Xueqi Cheng</author><pubDate>Fri, 22 Sep 2023 16:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12960v1</guid></item><item><title>On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures</title><link>http://arxiv.org/abs/2309.12955v1</link><description>Collaborative perception, which greatly enhances the sensing capability ofconnected and autonomous vehicles (CAVs) by incorporating data from externalresources, also brings forth potential security risks. CAVs' driving decisionsrely on remote untrusted data, making them susceptible to attacks carried outby malicious participants in the collaborative perception system. However,security analysis and countermeasures for such threats are absent. Tounderstand the impact of the vulnerability, we break the ground by proposingvarious real-time data fabrication attacks in which the attacker deliverscrafted malicious data to victims in order to perturb their perception results,leading to hard brakes or increased collision risks. Our attacks demonstrate ahigh success rate of over 86\% on high-fidelity simulated scenarios and arerealizable in real-world experiments. To mitigate the vulnerability, we presenta systematic anomaly detection approach that enables benign vehicles to jointlyreveal malicious fabrication. It detects 91.5% of attacks with a false positiverate of 3% in simulated scenarios and significantly mitigates attack impacts inreal-world scenarios.</description><author>Qingzhao Zhang, Shuowei Jin, Jiachen Sun, Xumiao Zhang, Ruiyang Zhu, Qi Alfred Chen, Z. Morley Mao</author><pubDate>Fri, 22 Sep 2023 16:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12955v1</guid></item><item><title>Inter-vendor harmonization of Computed Tomography (CT) reconstruction kernels using unpaired image translation</title><link>http://arxiv.org/abs/2309.12953v1</link><description>The reconstruction kernel in computed tomography (CT) generation determinesthe texture of the image. Consistency in reconstruction kernels is important asthe underlying CT texture can impact measurements during quantitative imageanalysis. Harmonization (i.e., kernel conversion) minimizes differences inmeasurements due to inconsistent reconstruction kernels. Existing methodsinvestigate harmonization of CT scans in single or multiple manufacturers.However, these methods require paired scans of hard and soft reconstructionkernels that are spatially and anatomically aligned. Additionally, a largenumber of models need to be trained across different kernel pairs withinmanufacturers. In this study, we adopt an unpaired image translation approachto investigate harmonization between and across reconstruction kernels fromdifferent manufacturers by constructing a multipath cycle generativeadversarial network (GAN). We use hard and soft reconstruction kernels from theSiemens and GE vendors from the National Lung Screening Trial dataset. We use50 scans from each reconstruction kernel and train a multipath cycle GAN. Toevaluate the effect of harmonization on the reconstruction kernels, weharmonize 50 scans each from Siemens hard kernel, GE soft kernel and GE hardkernel to a reference Siemens soft kernel (B30f) and evaluate percentemphysema. We fit a linear model by considering the age, smoking status, sexand vendor and perform an analysis of variance (ANOVA) on the emphysema scores.Our approach minimizes differences in emphysema measurement and highlights theimpact of age, sex, smoking status and vendor on emphysema quantification.</description><author>Aravind R. Krishnan, Kaiwen Xu, Thomas Li, Chenyu Gao, Lucas W. Remedios, Praitayini Kanakaraj, Ho Hin Lee, Shunxing Bao, Kim L. Sandler, Fabien Maldonado, Ivana Isgum, Bennett A. Landman</author><pubDate>Fri, 22 Sep 2023 16:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12953v1</guid></item><item><title>MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</title><link>http://arxiv.org/abs/2309.12284v2</link><description>Large language models (LLMs) have pushed the limits of natural languageunderstanding and exhibited excellent problem-solving ability. Despite thegreat success, most existing open-source LLMs (e.g., LLaMA-2) are still faraway from satisfactory for solving mathematical problem due to the complexreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tunedlanguage model that specializes in mathematical reasoning. Specifically, westart by bootstrapping mathematical questions by rewriting the question frommultiple perspectives without extra knowledge, which results in a new datasetcalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.Experimental results on two popular benchmarks (i.e., GSM8K and MATH) formathematical reasoning demonstrate that MetaMath outperforms a suite ofopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the samesize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQAdataset, the MetaMath models with different model sizes and the training codefor public use.</description><author>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu</author><pubDate>Fri, 22 Sep 2023 16:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12284v2</guid></item><item><title>Background Activation Suppression for Weakly Supervised Object Localization and Semantic Segmentation</title><link>http://arxiv.org/abs/2309.12943v1</link><description>Weakly supervised object localization and semantic segmentation aim tolocalize objects using only image-level labels. Recently, a new paradigm hasemerged by generating a foreground prediction map (FPM) to achieve pixel-levellocalization. While existing FPM-based methods use cross-entropy to evaluatethe foreground prediction map and to guide the learning of the generator, thispaper presents two astonishing experimental observations on the objectlocalization learning process: For a trained network, as the foreground maskexpands, 1) the cross-entropy converges to zero when the foreground mask coversonly part of the object region. 2) The activation value continuously increasesuntil the foreground mask expands to the object boundary. Therefore, to achievea more effective localization performance, we argue for the usage of activationvalue to learn more object regions. In this paper, we propose a BackgroundActivation Suppression (BAS) method. Specifically, an Activation Map Constraint(AMC) module is designed to facilitate the learning of generator by suppressingthe background activation value. Meanwhile, by using foreground region guidanceand area constraint, BAS can learn the whole region of the object. In theinference phase, we consider the prediction maps of different categoriestogether to obtain the final localization results. Extensive experiments showthat BAS achieves significant and consistent improvement over the baselinemethods on the CUB-200-2011 and ILSVRC datasets. In addition, our method alsoachieves state-of-the-art weakly supervised semantic segmentation performanceon the PASCAL VOC 2012 and MS COCO 2014 datasets. Code and models are availableat https://github.com/wpy1999/BAS-Extension.</description><author>Wei Zhai, Pingyu Wu, Kai Zhu, Yang Cao, Feng Wu, Zheng-Jun Zha</author><pubDate>Fri, 22 Sep 2023 16:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12943v1</guid></item><item><title>SLAM for Visually Impaired Navigation: A Systematic Literature Review of the Current State of Research</title><link>http://arxiv.org/abs/2212.04745v2</link><description>In recent decades, several assistive technologies have been developed forvisually impaired and blind (VIB) individuals to improve their ability tonavigate independently and safely. At the same time, simultaneous localizationand mapping (SLAM) techniques have become sufficiently robust and efficient tobe adopted in the development of these assistive technologies. In this paper,we first report the results of an anonymous worldwide survey conducted with VIBpeople to understand their experiences, needs, and challenges in navigation,differentiating our approach from prior work that often has a limitedgeographic scope and focuses on specific challenges. We then present asystematic literature review of recent studies on SLAM-based solutions for VIBpeople. This review explores various SLAM techniques employed in this context.We discuss the advantages and limitations of these techniques for VIBnavigation. Moreover, we examined a range of challenging situations addressedin the studies included in this review. We explain how SLAM-based solutionsoffer potential to improve the ability of visually impaired individuals tonavigate effectively. Finally, we present future opportunities and challengesin this domain.</description><author>Marziyeh Bamdad, Davide Scaramuzza, Alireza Darvishy</author><pubDate>Fri, 22 Sep 2023 16:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04745v2</guid></item><item><title>Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models</title><link>http://arxiv.org/abs/2309.12941v1</link><description>Assurance cases can be used to argue for the safety of products in safetyengineering. In safety-critical areas, the construction of assurance cases isindispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance casesby incorporating formal methods, rendering it possible for automatic reasoningabout assurance cases. We present Trustworthiness Derivation Tree Analyzer(Trusta), a desktop application designed to automatically construct and verifyTDTs. The tool has a built-in Prolog interpreter in its backend, and issupported by the constraint solvers Z3 and MONA. Therefore, it can solveconstraints about logical formulas involving arithmetic, sets, Horn clausesetc. Trusta also utilizes large language models to make the creation andevaluation of assurance cases more convenient. It allows for interactive humanexamination and modification. We evaluated top language models likeChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our testsshowed a 50%-80% similarity between machine-generated and human-created cases.In addition, Trusta can extract formal constraints from text in naturallanguages, facilitating an easier interpretation and validation process. Thisextraction is subject to human review and correction, blending the best ofautomated efficiency with human insight. To our knowledge, this marks the firstintegration of large language models in automatic creating and reasoning aboutassurance cases, bringing a novel approach to a traditional challenge. Throughseveral industrial case studies, Trusta has proven to quickly find some subtleissues that are typically missed in manual inspection, demonstrating itspractical value in enhancing the assurance case development process.</description><author>Zezhong Chen, Yuxin Deng, Wenjie Du</author><pubDate>Fri, 22 Sep 2023 16:42:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12941v1</guid></item><item><title>Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models</title><link>http://arxiv.org/abs/2309.12940v1</link><description>Task-oriented dialogue (TOD) systems facilitate users in executing variousactivities via multi-turn dialogues, but Large Language Models (LLMs) oftenstruggle to comprehend these intricate contexts. In this study, we propose anovel "Self-Explanation" prompting strategy to enhance the comprehensionabilities of LLMs in multi-turn dialogues. This task-agnostic approach requiresthe model to analyze each dialogue utterance before task execution, therebyimproving performance across various dialogue-centric tasks. Experimentalresults from six benchmark datasets confirm that our method consistentlyoutperforms other zero-shot prompts and matches or exceeds the efficacy offew-shot prompts, demonstrating its potential as a powerful tool in enhancingLLMs' comprehension in complex dialogue tasks.</description><author>Haoyu Gao, Ting-En Lin, Hangyu Li, Min Yang, Yuchuan Wu, Wentao Ma, Yongbin Li</author><pubDate>Fri, 22 Sep 2023 16:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12940v1</guid></item><item><title>Frustrated with Code Quality Issues? LLMs can Help!</title><link>http://arxiv.org/abs/2309.12938v1</link><description>As software projects progress, quality of code assumes paramount importanceas it affects reliability, maintainability and security of software. For thisreason, static analysis tools are used in developer workflows to flag codequality issues. However, developers need to spend extra efforts to revise theircode to improve code quality based on the tool findings. In this work, weinvestigate the use of (instruction-following) large language models (LLMs) toassist developers in revising code to resolve code quality issues. We present atool, CORE (short for COde REvisions), architected using a pair of LLMsorganized as a duo comprised of a proposer and a ranker. Providers of staticanalysis tools recommend ways to mitigate the tool warnings and developersfollow them to revise their code. The \emph{proposer LLM} of CORE takes thesame set of recommendations and applies them to generate candidate coderevisions. The candidates which pass the static quality checks are retained.However, the LLM may introduce subtle, unintended functionality changes whichmay go un-detected by the static analysis. The \emph{ranker LLM} evaluates thechanges made by the proposer using a rubric that closely follows the acceptancecriteria that a developer would enforce. CORE uses the scores assigned by theranker LLM to rank the candidate revisions before presenting them to thedeveloper. CORE could revise 59.2% Python files (across 52 quality checks) sothat they pass scrutiny by both a tool and a human reviewer. The ranker LLM isable to reduce false positives by 25.8% in these cases. CORE produced revisionsthat passed the static analysis tool in 76.8% Java files (across 10 qualitychecks) comparable to 78.3% of a specialized program repair tool, withsignificantly much less engineering efforts.</description><author>Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Aditya Kanade, Suresh Parthasarathy, Sriram Rajamani</author><pubDate>Fri, 22 Sep 2023 16:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12938v1</guid></item><item><title>TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts</title><link>http://arxiv.org/abs/2309.12934v1</link><description>Recent advances in Large Language Models (LLMs) have enabled the generationof open-ended high-quality texts, that are non-trivial to distinguish fromhuman-written texts. We refer to such LLM-generated texts as \emph{deepfaketexts}. There are currently over 11K text generation models in the huggingfacemodel repo. As such, users with malicious intent can easily use theseopen-sourced LLMs to generate harmful texts and misinformation at scale. Tomitigate this problem, a computational method to determine if a given text is adeepfake text or not is desired--i.e., Turing Test (TT). In particular, in thiswork, we investigate the more general version of the problem, known as\emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not onlydetermining if a given text is a deepfake text or not but also being able topinpoint which LLM is the author. We propose \textbf{TopRoBERTa} to improveexisting AA solutions by capturing more linguistic patterns in deepfake textsby including a Topological Data Analysis (TDA) layer in the RoBERTa model. Weshow the benefits of having a TDA layer when dealing with noisy, imbalanced,and heterogeneous datasets, by extracting TDA features from the reshaped$pooled\_output$ of RoBERTa as input. We use RoBERTa to capture contextualrepresentations (i.e., semantic and syntactic linguistic features), while usingTDA to capture the shape and structure of data (i.e., linguistic structures).Finally, \textbf{TopRoBERTa}, outperforms the vanilla RoBERTa in 2/3 datasets,achieving up to 7\% increase in Macro F1 score.</description><author>Adaku Uchendu, Thai Le, Dongwon Lee</author><pubDate>Fri, 22 Sep 2023 16:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12934v1</guid></item><item><title>On Separate Normalization in Self-supervised Transformers</title><link>http://arxiv.org/abs/2309.12931v1</link><description>Self-supervised training methods for transformers have demonstratedremarkable performance across various domains. Previous transformer-basedmodels, such as masked autoencoders (MAE), typically utilize a singlenormalization layer for both the [CLS] symbol and the tokens. We propose inthis paper a simple modification that employs separate normalization layers forthe tokens and the [CLS] symbol to better capture their distinctcharacteristics and enhance downstream task performance. Our method aims toalleviate the potential negative effects of using the same normalizationstatistics for both token types, which may not be optimally aligned with theirindividual roles. We empirically show that by utilizing a separatenormalization layer, the [CLS] embeddings can better encode the globalcontextual information and are distributed more uniformly in its anisotropicspace. When replacing the conventional normalization layer with the twoseparate layers, we observe an average 2.7% performance improvement over theimage, natural language, and graph domains.</description><author>Xiaohui Chen, Yinkai Wang, Yuanqi Du, Soha Hassoun, Li-Ping Liu</author><pubDate>Fri, 22 Sep 2023 16:30:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12931v1</guid></item><item><title>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</title><link>http://arxiv.org/abs/2308.12066v2</link><description>Large language models (LLMs) based on transformers have made significantstrides in recent years, the success of which is driven by scaling up theirmodel size. Despite their high algorithmic performance, the computational andmemory requirements of LLMs present unprecedented challenges. To tackle thehigh compute requirements of LLMs, the Mixture-of-Experts (MoE) architecturewas introduced which is able to scale its model size without proportionallyscaling up its computational requirements. Unfortunately, MoE's high memorydemands and dynamic activation of sparse experts restrict its applicability toreal-world problems. Previous solutions that offload MoE's memory-hungry expertparameters to CPU memory fall short because the latency to migrate activatedexperts from CPU to GPU incurs high performance overhead. Our proposedPre-gated MoE system effectively tackles the compute and memory challenges ofconventional MoE architectures using our algorithm-system co-design. Pre-gatedMoE employs our novel pre-gating function which alleviates the dynamic natureof sparse expert activation, allowing our proposed system to address the largememory footprint of MoEs while also achieving high performance. We demonstratethat Pre-gated MoE is able to improve performance, reduce GPU memoryconsumption, while also maintaining the same level of model quality. Thesefeatures allow our Pre-gated MoE system to cost-effectively deploy large-scaleLLMs using just a single GPU with high performance.</description><author>Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang</author><pubDate>Fri, 22 Sep 2023 16:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12066v2</guid></item><item><title>BayesDLL: Bayesian Deep Learning Library</title><link>http://arxiv.org/abs/2309.12928v1</link><description>We release a new Bayesian neural network library for PyTorch for large-scaledeep networks. Our library implements mainstream approximate Bayesian inferencealgorithms: variational inference, MC-dropout, stochastic-gradient MCMC, andLaplace approximation. The main differences from other existing Bayesian neuralnetwork libraries are as follows: 1) Our library can deal with very large-scaledeep networks including Vision Transformers (ViTs). 2) We need virtually zerocode modifications for users (e.g., the backbone network definition codes donot neet to be modified at all). 3) Our library also allows the pre-trainedmodel weights to serve as a prior mean, which is very useful for performingBayesian inference with the large-scale foundation models like ViTs that arehard to optimise from scratch with the downstream data alone. Our code ispublicly available at: \url{https://github.com/SamsungLabs/BayesDLL}\footnote{Amirror repository is also available at:\url{https://github.com/minyoungkim21/BayesDLL}.}.</description><author>Minyoung Kim, Timothy Hospedales</author><pubDate>Fri, 22 Sep 2023 16:27:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12928v1</guid></item><item><title>Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting</title><link>http://arxiv.org/abs/2307.15299v4</link><description>Accurate load forecasting plays a vital role in numerous sectors, butaccurately capturing the complex dynamics of dynamic power systems remains achallenge for traditional statistical models. For these reasons, time-seriesmodels (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonlydeployed and often experience higher success. In this paper, we analyze theefficacy of the recently developed Transformer-based Neural Network model inLoad forecasting. Transformer models have the potential to improve Loadforecasting because of their ability to learn long-range dependencies derivedfrom their Attention Mechanism. We apply several metaheuristics namelyDifferential Evolution to find the optimal hyperparameters of theTransformer-based Neural Network to produce accurate forecasts. DifferentialEvolution provides scalable, robust, global solutions to non-differentiable,multi-objective, or constrained optimization problems. Our work compares theproposed Transformer based Neural Network model integrated with differentmetaheuristic algorithms by their performance in Load forecasting based onnumerical metrics such as Mean Squared Error (MSE) and Mean Absolute PercentageError (MAPE). Our findings demonstrate the potential of metaheuristic-enhancedTransformer-based Neural Network models in Load forecasting accuracy andprovide optimal hyperparameters for each model.</description><author>Anuvab Sen, Arul Rhik Mazumder, Udayon Sen</author><pubDate>Fri, 22 Sep 2023 16:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15299v4</guid></item><item><title>Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks</title><link>http://arxiv.org/abs/2309.12927v1</link><description>Recurrent neural networks (RNNs) in the brain and in silico excel at solvingtasks with intricate temporal dependencies. Long timescales required forsolving such tasks can arise from properties of individual neurons(single-neuron timescale, $\tau$, e.g., membrane time constant in biologicalneurons) or recurrent interactions among them (network-mediated timescale).However, the contribution of each mechanism for optimally solvingmemory-dependent tasks remains poorly understood. Here, we train RNNs to solve$N$-parity and $N$-delayed match-to-sample tasks with increasing memoryrequirements controlled by $N$ by simultaneously optimizing recurrent weightsand $\tau$s. We find that for both tasks RNNs develop longer timescales withincreasing $N$, but depending on the learning objective, they use differentmechanisms. Two distinct curricula define learning objectives: sequentiallearning of a single-$N$ (single-head) or simultaneous learning of multiple$N$s (multi-head). Single-head networks increase their $\tau$ with $N$ and areable to solve tasks for large $N$, but they suffer from catastrophicforgetting. However, multi-head networks, which are explicitly required to holdmultiple concurrent memories, keep $\tau$ constant and develop longertimescales through recurrent connectivity. Moreover, we show that themulti-head curriculum increases training speed and network stability toablations and perturbations, and allows RNNs to generalize better to tasksbeyond their training regime. This curriculum also significantly improvestraining GRUs and LSTMs for large-$N$ tasks. Our results suggest that adaptingtimescales to task requirements via recurrent interactions allows learning morecomplex objectives and improves the RNN's performance.</description><author>Sina Khajehabdollahi, Roxana Zeraati, Emmanouil Giannakakis, Tim Jakob Schäfer, Georg Martius, Anna Levina</author><pubDate>Fri, 22 Sep 2023 16:26:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12927v1</guid></item><item><title>Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders</title><link>http://arxiv.org/abs/2302.00662v2</link><description>Offline reinforcement learning is important in domains such as medicine,economics, and e-commerce where online experimentation is costly, dangerous orunethical, and where the true model is unknown. However, most methods assumeall covariates used in the behavior policy's action decisions are observed.Though this assumption, sequential ignorability/unconfoundedness, likely doesnot hold in observational data, most of the data that accounts for selectioninto treatment may be observed, motivating sensitivity analysis. We studyrobust policy evaluation and policy optimization in the presence ofsequentially-exogenous unobserved confounders under a sensitivity model. Wepropose and analyze orthogonalized robust fitted-Q-iteration that usesclosed-form solutions of the robust Bellman operator to derive a lossminimization problem for the robust Q function, and adds a bias-correction toquantile estimation. Our algorithm enjoys the computational ease offitted-Q-iteration and statistical improvements (reduced dependence on quantileestimation error) from orthogonalization. We provide sample complexity bounds,insights, and show effectiveness both in simulations and on real-worldlongitudinal healthcare data of treating sepsis. In particular, our model ofsequential unobserved confounders yields an online Markov decision process,rather than partially observed Markov decision process: we illustrate how thiscan enable warm-starting optimistic reinforcement learning algorithms withvalid robust bounds from observational data.</description><author>David Bruns-Smith, Angela Zhou</author><pubDate>Fri, 22 Sep 2023 16:15:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00662v2</guid></item><item><title>Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation</title><link>http://arxiv.org/abs/2305.17558v3</link><description>Stein Variational Gradient Descent (SVGD) is a popular variational inferencealgorithm which simulates an interacting particle system to approximatelysample from a target distribution, with impressive empirical performance acrossvarious domains. Theoretically, its population (i.e, infinite-particle) limitdynamics is well studied but the behavior of SVGD in the finite-particle regimeis much less understood. In this work, we design two computationally efficientvariants of SVGD, namely VP-SVGD and GB-SVGD, with provably fastfinite-particle convergence rates. We introduce the notion of virtual particlesand develop novel stochastic approximations of population-limit SVGD dynamicsin the space of probability measures, which are exactly implementable using afinite number of particles. Our algorithms can be viewed as specificrandom-batch approximations of SVGD, which are computationally more efficientthan ordinary SVGD. We show that the $n$ particles output by VP-SVGD andGB-SVGD, run for $T$ steps with batch-size $K$, are at-least as good as i.i.dsamples from a distribution whose Kernel Stein Discrepancy to the target is atmost $O\left(\tfrac{d^{1/3}}{(KT)^{1/6}}\right)$ under standard assumptions.Our results also hold under a mild growth condition on the potential function,which is much weaker than the isoperimetric (e.g. Poincare Inequality) orinformation-transport conditions (e.g. Talagrand's Inequality $\mathsf{T}_1$)generally considered in prior works. As a corollary, we consider theconvergence of the empirical measure (of the particles output by VP-SVGD andGB-SVGD) to the target distribution and demonstrate a double exponentialimprovement over the best known finite-particle analysis of SVGD. Beyond this,our results present the first known oracle complexities for this setting withpolynomial dimension dependence.</description><author>Aniket Das, Dheeraj Nagaraj</author><pubDate>Fri, 22 Sep 2023 16:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17558v3</guid></item><item><title>What Makes a Language Easy to Deep-Learn?</title><link>http://arxiv.org/abs/2302.12239v2</link><description>Neural networks drive the success of natural language processing. Afundamental property of language is its compositional structure, allowinghumans to produce forms for new meanings systematically. However, unlikehumans, neural networks notoriously struggle with systematic generalization,and do not necessarily benefit from compositional structure in emergentcommunication simulations. This poses a problem for using neural networks tosimulate human language learning and evolution, and suggests crucialdifferences in the biases of the different learning systems. Here, we directlytest how neural networks compare to humans in learning and generalizingdifferent input languages that vary in their degree of structure. We evaluatethe memorization and generalization capabilities of a pre-trained languagemodel GPT-3.5 (analagous to an adult second language learner) and recurrentneural networks trained from scratch (analaogous to a child first languagelearner). Our results show striking similarities between deep neural networksand adult human learners, with more structured linguistic input leading to moresystematic generalization and to better convergence between neural networks andhumans. These findings suggest that all the learning systems are sensitive tothe structure of languages in similar ways with compositionality beingadvantageous for learning. Our findings draw a clear prediction regardingchildren's learning biases, as well as highlight the challenges of automatedprocessing of languages spoken by small communities. Notably, the similaritybetween humans and machines opens new avenues for research on language learningand evolution.</description><author>Lukas Galke, Yoav Ram, Limor Raviv</author><pubDate>Fri, 22 Sep 2023 16:02:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12239v2</guid></item><item><title>A matter of attitude: Focusing on positive and active gradients to boost saliency maps</title><link>http://arxiv.org/abs/2309.12913v1</link><description>Saliency maps have become one of the most widely used interpretabilitytechniques for convolutional neural networks (CNN) due to their simplicity andthe quality of the insights they provide. However, there are still some doubtsabout whether these insights are a trustworthy representation of what CNNs useto come up with their predictions. This paper explores how rescuing the sign ofthe gradients from the saliency map can lead to a deeper understanding ofmulti-class classification problems. Using both pretrained and trained fromscratch CNNs we unveil that considering the sign and the effect not only of thecorrect class, but also the influence of the other classes, allows to betteridentify the pixels of the image that the network is really focusing on.Furthermore, how occluding or altering those pixels is expected to affect theoutcome also becomes clearer.</description><author>Oscar Llorente, Jaime Boal, Eugenio F. Sánchez-Úbeda</author><pubDate>Fri, 22 Sep 2023 16:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12913v1</guid></item><item><title>Self-Supervised Training with Autoencoders for Visual Anomaly Detection</title><link>http://arxiv.org/abs/2206.11723v5</link><description>Deep autoencoders provide an effective tool for learning non-lineardimensionality reduction in an unsupervised way. Recently, they have been usedfor the task of anomaly detection in the visual domain. By optimizing for thereconstruction error using anomaly-free examples, the common belief is that acorresponding network should fail to accurately reconstruct anomalous regionsin the application phase. This goal is typically addressed by controlling thecapacity of the network, either by reducing the size of the bottleneck layer orby enforcing sparsity constraints on the activations. However, neither of thesetechniques does explicitly penalize reconstruction of anomalous signals oftenresulting in poor detection. We tackle this problem by adapting aself-supervised learning regime that allows the use of discriminativeinformation during training but focuses on the data manifold of normalexamples. We emphasize that inference with our approach is very efficientduring training and prediction requiring a single forward pass for each inputimage. Our experiments on the MVTec AD dataset demonstrate high detection andlocalization performance. On the texture-subset, in particular, our approachconsistently outperforms recent anomaly detection methods by a significantmargin.</description><author>Alexander Bauer, Shinichi Nakajima, Klaus-Robert Müller</author><pubDate>Fri, 22 Sep 2023 15:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11723v5</guid></item><item><title>OpenApePose: a database of annotated ape photographs for pose estimation</title><link>http://arxiv.org/abs/2212.00741v2</link><description>Because of their close relationship with humans, non-human apes (chimpanzees,bonobos, gorillas, orangutans, and gibbons, including siamangs) are of greatscientific interest. The goal of understanding their complex behavior would begreatly advanced by the ability to perform video-based pose tracking. Tracking,however, requires high-quality annotated datasets of ape photographs. Here wepresent OpenApePose, a new public dataset of 71,868 photographs, annotated with16 body landmarks, of six ape species in naturalistic contexts. We show that astandard deep net (HRNet-W48) trained on ape photos can reliably trackout-of-sample ape photos better than networks trained on monkeys (specifically,the OpenMonkeyPose dataset) and on humans (COCO) can. This trained network cantrack apes almost as well as the other networks can track their respectivetaxa, and models trained without one of the six ape species can track the heldout species better than the monkey and human models can. Ultimately, theresults of our analyses highlight the importance of large specialized databasesfor animal tracking systems and confirm the utility of our new ape database.</description><author>Nisarg Desai, Praneet Bala, Rebecca Richardson, Jessica Raper, Jan Zimmermann, Benjamin Hayden</author><pubDate>Fri, 22 Sep 2023 15:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00741v2</guid></item><item><title>KG-MDL: Mining Graph Patterns in Knowledge Graphs with the MDL Principle</title><link>http://arxiv.org/abs/2309.12908v1</link><description>Nowadays, increasingly more data are available as knowledge graphs (KGs).While this data model supports advanced reasoning and querying, they remaindifficult to mine due to their size and complexity. Graph mining approaches canbe used to extract patterns from KGs. However this presents two main issues.First, graph mining approaches tend to extract too many patterns for a humananalyst to interpret (pattern explosion). Second, real-life KGs tend to differfrom the graphs usually treated in graph mining: they are multigraphs, theirvertex degrees tend to follow a power-law, and the way in which they modelknowledge can produce spurious patterns. Recently, a graph mining approachnamed GraphMDL+ has been proposed to tackle the problem of pattern explosion,using the Minimum Description Length (MDL) principle. However, GraphMDL+, likeother graph mining approaches, is not suited for KGs without adaptations. Inthis paper we propose KG-MDL, a graph pattern mining approach based on the MDLprinciple that, given a KG, generates a human-sized and descriptive set ofgraph patterns, and so in a parameter-less and anytime way. We report onexperiments on medium-sized KGs showing that our approach generates sets ofpatterns that are both small enough to be interpreted by humans and descriptiveof the KG. We show that the extracted patterns highlight relevantcharacteristics of the data: both of the schema used to create the data, and ofthe concrete facts it contains. We also discuss the issues related to mininggraph patterns on knowledge graphs, as opposed to other types of graph data.</description><author>Francesco Bariatti, Peggy Cellier, Sébastien Ferré</author><pubDate>Fri, 22 Sep 2023 15:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12908v1</guid></item><item><title>An Improved Encoder-Decoder Framework for Food Energy Estimation</title><link>http://arxiv.org/abs/2309.00468v3</link><description>Dietary assessment is essential to maintaining a healthy lifestyle. Automaticimage-based dietary assessment is a growing field of research due to theincreasing prevalence of image capturing devices (e.g. mobile phones). In thiswork, we estimate food energy from a single monocular image, a difficult taskdue to the limited hard-to-extract amount of energy information present in animage. To do so, we employ an improved encoder-decoder framework for energyestimation; the encoder transforms the image into a representation embeddedwith food energy information in an easier-to-extract format, which the decoderthen extracts the energy information from. To implement our method, we compilea high-quality food image dataset verified by registered dietitians containingeating scene images, food-item segmentation masks, and ground truth calorievalues. Our method improves upon previous caloric estimation methods by over10\% and 30 kCal in terms of MAPE and MAE respectively.</description><author>Jack Ma, Jiangpeng He, Fengqing Zhu</author><pubDate>Fri, 22 Sep 2023 15:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00468v3</guid></item><item><title>Point spread function modelling for astronomical telescopes: a review focused on weak gravitational lensing studies</title><link>http://arxiv.org/abs/2306.07996v3</link><description>The accurate modelling of the Point Spread Function (PSF) is of paramountimportance in astronomical observations, as it allows for the correction ofdistortions and blurring caused by the telescope and atmosphere. PSF modellingis crucial for accurately measuring celestial objects' properties. The lastdecades brought us a steady increase in the power and complexity ofastronomical telescopes and instruments. Upcoming galaxy surveys like Euclidand LSST will observe an unprecedented amount and quality of data. Modellingthe PSF for these new facilities and surveys requires novel modellingtechniques that can cope with the ever-tightening error requirements. Thepurpose of this review is three-fold. First, we introduce the opticalbackground required for a more physically-motivated PSF modelling and proposean observational model that can be reused for future developments. Second, weprovide an overview of the different physical contributors of the PSF,including the optic- and detector-level contributors and the atmosphere. Weexpect that the overview will help better understand the modelled effects.Third, we discuss the different methods for PSF modelling from the parametricand non-parametric families for ground- and space-based telescopes, with theiradvantages and limitations. Validation methods for PSF models are thenaddressed, with several metrics related to weak lensing studies discussed indetail. Finally, we explore current challenges and future directions in PSFmodelling for astronomical telescopes.</description><author>Tobias Liaudat, Jean-Luc Starck, Martin Kilbinger</author><pubDate>Fri, 22 Sep 2023 15:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07996v3</guid></item><item><title>Few-shot Link Prediction on N-ary Facts</title><link>http://arxiv.org/abs/2305.06104v2</link><description>N-ary facts composed of a primary triple (head entity, relation, tail entity)and an arbitrary number of auxiliary attribute-value pairs, are prevalent inreal-world knowledge graphs (KGs). Link prediction on n-ary facts is to predicta missing element in an n-ary fact. This helps populate and enrich KGs andfurther promotes numerous downstream applications. Previous studies usuallyrequire a substantial amount of high-quality data to understand the elements inn-ary facts. However, these studies overlook few-shot relations, which havelimited labeled instances, yet are common in real-world scenarios. Thus, thispaper introduces a new task, few-shot link prediction on n-ary facts. It aimsto predict a missing entity in an n-ary fact with limited labeled instances. Wefurther propose a model for Few-shot Link prEdict on N-ary facts, thus calledFLEN, which consists of three modules: the relation learning, support-specificadjusting, and query inference modules. FLEN captures relation meta informationfrom limited instances to predict a missing entity in a query instance. Tovalidate the effectiveness of FLEN, we construct three datasets based onexisting benchmark data. Our experimental results show that FLEN significantlyoutperforms existing related models in both few-shot link prediction on n-aryfacts and binary facts.</description><author>Jiyao Wei, Saiping Guan, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng</author><pubDate>Fri, 22 Sep 2023 15:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06104v2</guid></item><item><title>ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction</title><link>http://arxiv.org/abs/2309.12892v1</link><description>Event Relation Extraction (ERE) aims to extract multiple kinds of relationsamong events in texts. However, existing methods singly categorize eventrelations as different classes, which are inadequately capturing the intrinsicsemantics of these relations. To comprehensively understand their intrinsicsemantics, in this paper, we obtain prototype representations for each type ofevent relation and propose a Prototype-Enhanced Matching (ProtoEM) frameworkfor the joint extraction of multiple kinds of event relations. Specifically,ProtoEM extracts event relations in a two-step manner, i.e., prototyperepresenting and prototype matching. In the first step, to capture theconnotations of different event relations, ProtoEM utilizes examples torepresent the prototypes corresponding to these relations. Subsequently, tocapture the interdependence among event relations, it constructs a dependencygraph for the prototypes corresponding to these relations and utilized a GraphNeural Network (GNN)-based module for modeling. In the second step, it obtainsthe representations of new event pairs and calculates their similarity withthose prototypes obtained in the first step to evaluate which types of eventrelations they belong to. Experimental results on the MAVEN-ERE datasetdemonstrate that the proposed ProtoEM framework can effectively represent theprototypes of event relations and further obtain a significant improvement overbaseline models.</description><author>Zhilei Hu, Zixuan Li, Daozhu Xu, Long Bai, Cheng Jin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng</author><pubDate>Fri, 22 Sep 2023 15:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12892v1</guid></item><item><title>Deep Imbalanced Time-series Forecasting via Local Discrepancy Density</title><link>http://arxiv.org/abs/2302.13563v2</link><description>Time-series forecasting models often encounter abrupt changes in a givenperiod of time which generally occur due to unexpected or unknown events.Despite their scarce occurrences in the training set, abrupt changes incur lossthat significantly contributes to the total loss. Therefore, they act as noisytraining samples and prevent the model from learning generalizable patterns,namely the normal states. Based on our findings, we propose a reweightingframework that down-weights the losses incurred by abrupt changes andup-weights those by normal states. For the reweighting framework, we firstdefine a measurement termed Local Discrepancy (LD) which measures the degree ofabruptness of a change in a given period of time. Since a training set ismostly composed of normal states, we then consider how frequently the temporalchanges appear in the training set based on LD. Our reweighting framework isapplicable to existing time-series forecasting models regardless of thearchitectures. Through extensive experiments on 12 time-series forecastingmodels over eight datasets with various in-output sequence lengths, wedemonstrate that applying our reweighting framework reduces MSE by 10.1% onaverage and by up to 18.6% in the state-of-the-art model.</description><author>Junwoo Park, Jungsoo Lee, Youngin Cho, Woncheol Shin, Dongmin Kim, Jaegul Choo, Edward Choi</author><pubDate>Fri, 22 Sep 2023 15:21:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13563v2</guid></item><item><title>Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing</title><link>http://arxiv.org/abs/2303.15585v3</link><description>The field of mobile and wearable computing is undergoing a revolutionaryintegration of machine learning. Devices can now diagnose diseases, predictheart irregularities, and unlock the full potential of human cognition.However, the underlying algorithms powering these predictions are not immune tobiases with respect to sensitive attributes (e.g., gender, race), leading todiscriminatory outcomes. The goal of this work is to explore the extent towhich the mobile and wearable computing community has adopted ways of reportinginformation about datasets and models to surface and, eventually, counterbiases. Our systematic review of papers published in the Proceedings of the ACMInteractive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal from2018-2022 indicates that, while there has been progress made on algorithmicfairness, there is still ample room for growth. Our findings show that only asmall portion (5%) of published papers adheres to modern fairness reporting,while the overwhelming majority thereof focuses on accuracy or error metrics.To generalize these results across venues of similar scope, we analyzed recentproceedings of ACM MobiCom, MobiSys, and SenSys, IEEE Pervasive, and IEEETransactions on Mobile Computing Computing, and found no deviation from ourprimary result. In light of these findings, our work provides practicalguidelines for the design and development of mobile and wearable technologiesthat not only strive for accuracy but also fairness.</description><author>Sofia Yfantidou, Marios Constantinides, Dimitris Spathis, Athena Vakali, Daniele Quercia, Fahim Kawsar</author><pubDate>Fri, 22 Sep 2023 15:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15585v3</guid></item><item><title>Affect Recognition in Conversations Using Large Language Models</title><link>http://arxiv.org/abs/2309.12881v1</link><description>Affect recognition, encompassing emotions, moods, and feelings, plays apivotal role in human communication. In the realm of conversational artificialintelligence (AI), the ability to discern and respond to human affective cuesis a critical factor for creating engaging and empathetic interactions. Thisstudy delves into the capacity of large language models (LLMs) to recognisehuman affect in conversations, with a focus on both open-domain chit-chatdialogues and task-oriented dialogues. Leveraging three diverse datasets,namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues fromcasual conversations to clinical interviews, we evaluated and compared LLMs'performance in affect recognition. Our investigation explores the zero-shot andfew-shot capabilities of LLMs through in-context learning (ICL) as well astheir model capacities through task-specific fine-tuning. Additionally, thisstudy takes into account the potential impact of automatic speech recognition(ASR) errors on LLM predictions. With this work, we aim to shed light on theextent to which LLMs can replicate human-like affect recognition capabilitiesin conversations.</description><author>Shutong Feng, Guangzhi Sun, Nurul Lubis, Chao Zhang, Milica Gašić</author><pubDate>Fri, 22 Sep 2023 15:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12881v1</guid></item><item><title>FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing</title><link>http://arxiv.org/abs/2309.12877v1</link><description>How can we ensure that Ubiquitous Computing (UbiComp) research outcomes areboth ethical and fair? While fairness in machine learning (ML) has gainedtraction in recent years, fairness in UbiComp remains unexplored. This workshopaims to discuss fairness in UbiComp research and its social, technical, andlegal implications. From a social perspective, we will examine the relationshipbetween fairness and UbiComp research and identify pathways to ensure thatubiquitous technologies do not cause harm or infringe on individual rights.From a technical perspective, we will initiate a discussion on data practicesto develop bias mitigation approaches tailored to UbiComp research. From alegal perspective, we will examine how new policies shape our community's workand future research. We aim to foster a vibrant community centered around thetopic of responsible UbiComp, while also charting a clear path for futureresearch endeavours in this field.</description><author>Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Tong Xia, Niels van Berkel</author><pubDate>Fri, 22 Sep 2023 15:04:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12877v1</guid></item><item><title>MiVOLO: Multi-input Transformer for Age and Gender Estimation</title><link>http://arxiv.org/abs/2307.04616v2</link><description>Age and gender recognition in the wild is a highly challenging task: apartfrom the variability of conditions, pose complexities, and varying imagequality, there are cases where the face is partially or completely occluded. Wepresent MiVOLO (Multi Input VOLO), a straightforward approach for age andgender estimation using the latest vision transformer. Our method integratesboth tasks into a unified dual input/output model, leveraging not only facialinformation but also person image data. This improves the generalizationability of our model and enables it to deliver satisfactory results even whenthe face is not visible in the image. To evaluate our proposed model, weconduct experiments on four popular benchmarks and achieve state-of-the-artperformance, while demonstrating real-time processing capabilities.Additionally, we introduce a novel benchmark based on images from the OpenImages Dataset. The ground truth annotations for this benchmark have beenmeticulously generated by human annotators, resulting in high accuracy answersdue to the smart aggregation of votes. Furthermore, we compare our model's agerecognition performance with human-level accuracy and demonstrate that itsignificantly outperforms humans across a majority of age ranges. Finally, wegrant public access to our models, along with the code for validation andinference. In addition, we provide extra annotations for used datasets andintroduce our new benchmark.</description><author>Maksim Kuprashevich, Irina Tolstykh</author><pubDate>Fri, 22 Sep 2023 15:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04616v2</guid></item><item><title>Gravity Network for end-to-end small lesion detection</title><link>http://arxiv.org/abs/2309.12876v1</link><description>This paper introduces a novel one-stage end-to-end detector specificallydesigned to detect small lesions in medical images. Precise localization ofsmall lesions presents challenges due to their appearance and the diversecontextual backgrounds in which they are found. To address this, our approachintroduces a new type of pixel-based anchor that dynamically moves towards thetargeted lesion for detection. We refer to this new architecture as GravityNet,and the novel anchors as gravity points since they appear to be "attracted" bythe lesions. We conducted experiments on two well-established medical problemsinvolving small lesions to evaluate the performance of the proposed approach:microcalcifications detection in digital mammograms and microaneurysmsdetection in digital fundus images. Our method demonstrates promising resultsin effectively detecting small lesions in these medical imaging tasks.</description><author>Ciro Russo, Alessandro Bria, Claudio Marrocco</author><pubDate>Fri, 22 Sep 2023 15:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12876v1</guid></item><item><title>Incentivizing Honesty among Competitors in Collaborative Learning and Optimization</title><link>http://arxiv.org/abs/2305.16272v2</link><description>Collaborative learning techniques have the potential to enable trainingmachine learning models that are superior to models trained on a singleentity's data. However, in many cases, potential participants in suchcollaborative schemes are competitors on a downstream task, such as firms thateach aim to attract customers by providing the best recommendations. This canincentivize dishonest updates that damage other participants' models,potentially undermining the benefits of collaboration. In this work, weformulate a game that models such interactions and study two learning taskswithin this framework: single-round mean estimation and multi-round SGD onstrongly-convex objectives. For a natural class of player actions, we show thatrational clients are incentivized to strongly manipulate their updates,preventing learning. We then propose mechanisms that incentivize honestcommunication and ensure learning quality comparable to full cooperation.Lastly, we empirically demonstrate the effectiveness of our incentive scheme ona standard non-convex federated learning benchmark. Our work shows thatexplicitly modeling the incentives and actions of dishonest clients, ratherthan assuming them malicious, can enable strong robustness guarantees forcollaborative learning.</description><author>Florian E. Dorner, Nikola Konstantinov, Georgi Pashaliev, Martin Vechev</author><pubDate>Fri, 22 Sep 2023 14:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16272v2</guid></item><item><title>AnglE-Optimized Text Embeddings</title><link>http://arxiv.org/abs/2309.12871v1</link><description>High-quality text embedding is pivotal in improving semantic textualsimilarity (STS) tasks, which are crucial components in Large Language Model(LLM) applications. However, a common challenge existing text embedding modelsface is the problem of vanishing gradients, primarily due to their reliance onthe cosine function in the optimization objective, which has saturation zones.To address this issue, this paper proposes a novel angle-optimized textembedding model called AnglE. The core idea of AnglE is to introduce angleoptimization in a complex space. This novel approach effectively mitigates theadverse effects of the saturation zone in the cosine function, which can impedegradient and hinder optimization processes. To set up a comprehensive STSevaluation, we experimented on existing short-text STS datasets and a newlycollected long-text STS dataset from GitHub Issues. Furthermore, we examinedomain-specific STS scenarios with limited labeled data and explore how AnglEworks with LLM-annotated data. Extensive experiments were conducted on varioustasks including short-text STS, long-text STS, and domain-specific STS tasks.The results show that AnglE outperforms the state-of-the-art (SOTA) STS modelsthat ignore the cosine saturation zone. These findings demonstrate the abilityof AnglE to generate high-quality text embeddings and the usefulness of angleoptimization in STS.</description><author>Xianming Li, Jing Li</author><pubDate>Fri, 22 Sep 2023 14:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12871v1</guid></item><item><title>OccupancyDETR: Making Semantic Scene Completion as Straightforward as Object Detection</title><link>http://arxiv.org/abs/2309.08504v2</link><description>Visual-based 3D semantic occupancy perception (also known as 3D semanticscene completion) is a new perception paradigm for robotic applications likeautonomous driving. Compared with Bird's Eye View (BEV) perception, it extendsthe vertical dimension, significantly enhancing the ability of robots tounderstand their surroundings. However, due to this very reason, thecomputational demand for current 3D semantic occupancy perception methodsgenerally surpasses that of BEV perception methods and 2D perception methods.We propose a novel 3D semantic occupancy perception method, OccupancyDETR,which consists of a DETR-like object detection module and a 3D occupancydecoder module. The integration of object detection simplifies our methodstructurally - instead of predicting the semantics of each voxels, itidentifies objects in the scene and their respective 3D occupancy grids. Thisspeeds up our method, reduces required resources, and leverages objectdetection algorithm, giving our approach notable performance on small objects.We demonstrate the effectiveness of our proposed method on the SemanticKITTIdataset, showcasing an mIoU of 23 and a processing speed of 6 frames persecond, thereby presenting a promising solution for real-time 3D semantic scenecompletion.</description><author>Yupeng Jia, Jie He, Runze Chen, Fang Zhao, Haiyong Luo</author><pubDate>Fri, 22 Sep 2023 14:52:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08504v2</guid></item><item><title>Accurate and Fast Compressed Video Captioning</title><link>http://arxiv.org/abs/2309.12867v1</link><description>Existing video captioning approaches typically require to first sample videoframes from a decoded video and then conduct a subsequent process (e.g.,feature extraction and/or captioning model learning). In this pipeline, manualframe sampling may ignore key information in videos and thus degradeperformance. Additionally, redundant information in the sampled frames mayresult in low efficiency in the inference of video captioning. Addressing this,we study video captioning from a different perspective in compressed domain,which brings multi-fold advantages over the existing pipeline: 1) Compared toraw images from the decoded video, the compressed video, consisting ofI-frames, motion vectors and residuals, is highly distinguishable, which allowsus to leverage the entire video for learning without manual sampling through aspecialized model design; 2) The captioning model is more efficient ininference as smaller and less redundant information is processed. We propose asimple yet effective end-to-end transformer in the compressed domain for videocaptioning that enables learning from the compressed video for captioning. Weshow that even with a simple design, our method can achieve state-of-the-artperformance on different benchmarks while running almost 2x faster thanexisting approaches. Code is available at https://github.com/acherstyx/CoCap.</description><author>Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, Libo Zhang</author><pubDate>Fri, 22 Sep 2023 14:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12867v1</guid></item><item><title>Bridging Sensor Gaps via Single-Direction Tuning for Hyperspectral Image Classification</title><link>http://arxiv.org/abs/2309.12865v1</link><description>Recently, some researchers started exploring the use of ViTs in tackling HSIclassification and achieved remarkable results. However, the training of ViTmodels requires a considerable number of training samples, while hyperspectraldata, due to its high annotation costs, typically has a relatively small numberof training samples. This contradiction has not been effectively addressed. Inthis paper, aiming to solve this problem, we propose the single-directiontuning (SDT) strategy, which serves as a bridge, allowing us to leverageexisting labeled HSI datasets even RGB datasets to enhance the performance onnew HSI datasets with limited samples. The proposed SDT inherits the idea ofprompt tuning, aiming to reuse pre-trained models with minimal modificationsfor adaptation to new tasks. But unlike prompt tuning, SDT is custom-designedto accommodate the characteristics of HSIs. The proposed SDT utilizes aparallel architecture, an asynchronous cold-hot gradient update strategy, andunidirectional interaction. It aims to fully harness the potent representationlearning capabilities derived from training on heterologous, even cross-modaldatasets. In addition, we also introduce a novel Triplet-structured transformer(Tri-Former), where spectral attention and spatial attention modules are mergedin parallel to construct the token mixing component for reducing computationcost and a 3D convolution-based channel mixer module is integrated to enhancestability and keep structure information. Comparison experiments conducted onthree representative HSI datasets captured by different sensors demonstrate theproposed Tri-Former achieves better performance compared to severalstate-of-the-art methods. Homologous, heterologous and cross-modal tuningexperiments verified the effectiveness of the proposed SDT.</description><author>Xizhe Xue, Haokui Zhang, Ying Li, Liuwei Wan, Zongwen Bai, Mike Zheng Shou</author><pubDate>Fri, 22 Sep 2023 14:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12865v1</guid></item><item><title>Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts</title><link>http://arxiv.org/abs/2309.12863v1</link><description>Neural machine translation (NMT) has shown impressive performance whentrained on large-scale corpora. However, generic NMT systems have demonstratedpoor performance on out-of-domain translation. To mitigate this issue, severaldomain adaptation methods have recently been proposed which often lead tobetter translation quality than genetic NMT systems. While there has been somecontinuous progress in NMT for English and other European languages, domainadaption in Arabic has received little attention in the literature. The currentstudy, therefore, aims to explore the effectiveness of domain-specificadaptation for Arabic MT (AMT), in yet unexplored domain, financial newsarticles. To this end, we developed carefully a parallel corpus forArabic-English (AR- EN) translation in the financial domain for benchmarkingdifferent domain adaptation methods. We then fine-tuned several pre-trained NMTand Large Language models including ChatGPT-3.5 Turbo on our dataset. Theresults showed that the fine-tuning is successful using just a few well-alignedin-domain AR-EN segments. The quality of ChatGPT translation was superior thanother models based on automatic and human evaluations. To the best of ourknowledge, this is the first work on fine-tuning ChatGPT towards financialdomain transfer learning. To contribute to research in domain translation, wemade our datasets and fine-tuned models available athttps://huggingface.co/asas-ai/.</description><author>Emad A. Alghamdi, Jezia Zakraoui, Fares A. Abanmy</author><pubDate>Fri, 22 Sep 2023 14:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12863v1</guid></item><item><title>Associative Transformer Is A Sparse Representation Learner</title><link>http://arxiv.org/abs/2309.12862v1</link><description>Emerging from the monolithic pairwise attention mechanism in conventionalTransformer models, there is a growing interest in leveraging sparseinteractions that align more closely with biological principles. Approachesincluding the Set Transformer and the Perceiver employ cross-attentionconsolidated with a latent space that forms an attention bottleneck withlimited capacity. Building upon recent neuroscience studies of Global WorkspaceTheory and associative memory, we propose the Associative Transformer (AiT).AiT induces low-rank explicit memory that serves as both priors to guidebottleneck attention in the shared workspace and attractors within associativememory of a Hopfield network. Through joint end-to-end training, these priorsnaturally develop module specialization, each contributing a distinct inductivebias to form attention bottlenecks. A bottleneck can foster competition amonginputs for writing information into the memory. We show that AiT is a sparserepresentation learner, learning distinct priors through the bottlenecks thatare complexity-invariant to input quantities and dimensions. AiT demonstratesits superiority over methods such as the Set Transformer, Vision Transformer,and Coordination in various vision tasks.</description><author>Yuwei Sun, Hideya Ochiai, Zhirong Wu, Stephen Lin, Ryota Kanai</author><pubDate>Fri, 22 Sep 2023 14:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12862v1</guid></item><item><title>Strategic Data Sharing between Competitors</title><link>http://arxiv.org/abs/2305.16052v2</link><description>Collaborative learning techniques have significantly advanced in recentyears, enabling private model training across multiple organizations. Despitethis opportunity, firms face a dilemma when considering data sharing withcompetitors -- while collaboration can improve a company's machine learningmodel, it may also benefit competitors and hence reduce profits. In this work,we introduce a general framework for analyzing this data-sharing trade-off. Theframework consists of three components, representing the firms' productiondecisions, the effect of additional data on model quality, and the data-sharingnegotiation process, respectively. We then study an instantiation of theframework, based on a conventional market model from economic theory, toidentify key factors that affect collaboration incentives. Our findingsindicate a profound impact of market conditions on the data-sharing incentives.In particular, we find that reduced competition, in terms of the similaritiesbetween the firms' products, and harder learning tasks foster collaboration.</description><author>Nikita Tsoy, Nikola Konstantinov</author><pubDate>Fri, 22 Sep 2023 14:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16052v2</guid></item><item><title>Diffusion Augmentation for Sequential Recommendation</title><link>http://arxiv.org/abs/2309.12858v1</link><description>Sequential recommendation (SRS) has become the technical foundation in manyapplications recently, which aims to recommend the next item based on theuser's historical interactions. However, sequential recommendation often facesthe problem of data sparsity, which widely exists in recommender systems.Besides, most users only interact with a few items, but existing SRS modelsoften underperform these users. Such a problem, named the long-tail userproblem, is still to be resolved. Data augmentation is a distinct way toalleviate these two problems, but they often need fabricated trainingstrategies or are hindered by poor-quality generated interactions. To addressthese problems, we propose a Diffusion Augmentation for SequentialRecommendation (DiffuASR) for a higher quality generation. The augmenteddataset by DiffuASR can be used to train the sequential recommendation modelsdirectly, free from complex training procedures. To make the best of thegeneration ability of the diffusion model, we first propose a diffusion-basedpseudo sequence generation framework to fill the gap between image and sequencegeneration. Then, a sequential U-Net is designed to adapt the diffusion noiseprediction model U-Net to the discrete sequence generation task. At last, wedevelop two guide strategies to assimilate the preference between generated andorigin sequences. To validate the proposed DiffuASR, we conduct extensiveexperiments on three real-world datasets with three sequential recommendationmodels. The experimental results illustrate the effectiveness of DiffuASR. Asfar as we know, DiffuASR is one pioneer that introduce the diffusion model tothe recommendation.</description><author>Qidong Liu, Fan Yan, Xiangyu Zhao, Zhaocheng Du, Huifeng Guo, Ruiming Tang, Feng Tian</author><pubDate>Fri, 22 Sep 2023 14:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12858v1</guid></item><item><title>Robotic Handling of Compliant Food Objects by Robust Learning from Demonstration</title><link>http://arxiv.org/abs/2309.12856v1</link><description>The robotic handling of compliant and deformable food raw materials,characterized by high biological variation, complex geometrical 3D shapes, andmechanical structures and texture, is currently in huge demand in the oceanspace, agricultural, and food industries. Many tasks in these industries areperformed manually by human operators who, due to the laborious and tediousnature of their tasks, exhibit high variability in execution, with variableoutcomes. The introduction of robotic automation for most complex processingtasks has been challenging due to current robot learning policies. A moreconsistent learning policy involving skilled operators is desired. In thispaper, we address the problem of robot learning when presented withinconsistent demonstrations. To this end, we propose a robust learning policybased on Learning from Demonstration (LfD) for robotic grasping of foodcompliant objects. The approach uses a merging of RGB-D images and tactile datain order to estimate the necessary pose of the gripper, gripper fingerconfiguration and forces exerted on the object in order to achieve effectiverobot handling. During LfD training, the gripper pose, finger configurationsand tactile values for the fingers, as well as RGB-D images are saved. Wepresent an LfD learning policy that automatically removes inconsistentdemonstrations, and estimates the teacher's intended policy. The performance ofour approach is validated and demonstrated for fragile and compliant foodobjects with complex 3D shapes. The proposed approach has a vast range ofpotential applications in the aforementioned industry sectors.</description><author>Ekrem Misimi, Alexander Olofsson, Aleksander Eilertsen, Elling Ruud Øye, John Reidar Mathiassen</author><pubDate>Fri, 22 Sep 2023 14:30:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12856v1</guid></item><item><title>Cross-Modal Translation and Alignment for Survival Analysis</title><link>http://arxiv.org/abs/2309.12855v1</link><description>With the rapid advances in high-throughput sequencing technologies, the focusof survival analysis has shifted from examining clinical indicators toincorporating genomic profiles with pathological images. However, existingmethods either directly adopt a straightforward fusion of pathological featuresand genomic profiles for survival prediction, or take genomic profiles asguidance to integrate the features of pathological images. The former wouldoverlook intrinsic cross-modal correlations. The latter would discardpathological information irrelevant to gene expression. To address theseissues, we present a Cross-Modal Translation and Alignment (CMTA) framework toexplore the intrinsic cross-modal correlations and transfer potentialcomplementary information. Specifically, we construct two parallelencoder-decoder structures for multi-modal data to integrate intra-modalinformation and generate cross-modal representation. Taking the generatedcross-modal representation to enhance and recalibrate intra-modalrepresentation can significantly improve its discrimination for comprehensivesurvival analysis. To explore the intrinsic crossmodal correlations, we furtherdesign a cross-modal attention module as the information bridge betweendifferent modalities to perform cross-modal interactions and transfercomplementary information. Our extensive experiments on five public TCGAdatasets demonstrate that our proposed framework outperforms thestate-of-the-art methods.</description><author>Fengtao Zhou, Hao Chen</author><pubDate>Fri, 22 Sep 2023 14:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12855v1</guid></item><item><title>ThinResNet: A New Baseline for Structured Convolutional Networks Pruning</title><link>http://arxiv.org/abs/2309.12854v1</link><description>Pruning is a compression method which aims to improve the efficiency ofneural networks by reducing their number of parameters while maintaining a goodperformance, thus enhancing the performance-to-cost ratio in nontrivial ways.Of particular interest are structured pruning techniques, in which wholeportions of parameters are removed altogether, resulting in easier to leverageshrunk architectures. Since its growth in popularity in the recent years,pruning gave birth to countless papers and contributions, resulting first incritical inconsistencies in the way results are compared, and then to acollective effort to establish standardized benchmarks. However, saidbenchmarks are based on training practices that date from several years ago anddo not align with current practices. In this work, we verify how results in therecent literature of pruning hold up against networks that underwent bothstate-of-the-art training methods and trivial model scaling. We find that thelatter clearly and utterly outperform all the literature we compared to,proving that updating standard pruning benchmarks and re-evaluating classicalmethods in their light is an absolute necessity. We thus introduce a newchallenging baseline to compare structured pruning to: ThinResNet.</description><author>Hugo Tessier, Ghouti Boukli Hacene, Vincent Gripon</author><pubDate>Fri, 22 Sep 2023 14:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12854v1</guid></item><item><title>Ensemble Differential Evolution with Simulation-Based Hybridization and Self-Adaptation for Inventory Management Under Uncertainty</title><link>http://arxiv.org/abs/2309.12852v1</link><description>This study proposes an Ensemble Differential Evolution with Simula-tion-BasedHybridization and Self-Adaptation (EDESH-SA) approach for inven-tory management(IM) under uncertainty. In this study, DE with multiple runs is combined with asimulation-based hybridization method that includes a self-adaptive mechanismthat dynamically alters mutation and crossover rates based on the success orfailure of each iteration. Due to its adaptability, the algorithm is able tohandle the complexity and uncertainty present in IM. Utilizing Monte CarloSimulation (MCS), the continuous review (CR) inventory strategy is ex-aminedwhile accounting for stochasticity and various demand scenarios. Thissimulation-based approach enables a realistic assessment of the proposedalgo-rithm's applicability in resolving the challenges faced by IM in practicalsettings. The empirical findings demonstrate the potential of the proposedmethod to im-prove the financial performance of IM and optimize large searchspaces. The study makes use of performance testing with the Ackley function andSensitivity Analysis with Perturbations to investigate how changes in variablesaffect the objective value. This analysis provides valuable insights into thebehavior and robustness of the algorithm.</description><author>Sarit Maitra, Vivek Mishra, Sukanya Kundu</author><pubDate>Fri, 22 Sep 2023 14:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12852v1</guid></item><item><title>Differentiable graph-structured models for inverse design of lattice materials</title><link>http://arxiv.org/abs/2304.05422v2</link><description>Architected materials possessing physico-chemical properties adaptable todisparate environmental conditions embody a disruptive new domain of materialsscience. Fueled by advances in digital design and fabrication, materials shapedinto lattice topologies enable a degree of property customization not affordedto bulk materials. A promising venue for inspiration toward their design is inthe irregular micro-architectures of nature. However, the immense designvariability unlocked by such irregularity is challenging to probe analytically.Here, we propose a new computational approach using graph-based representationfor regular and irregular lattice materials. Our method uses differentiablemessage passing algorithms to calculate mechanical properties, thereforeallowing automatic differentiation with surrogate derivatives to adjust bothgeometric structure and local attributes of individual lattice elements toachieve inversely designed materials with desired properties. We furtherintroduce a graph neural network surrogate model for structural analysis atscale. The methodology is generalizable to any system representable asheterogeneous graphs.</description><author>Dominik Dold, Derek Aranguren van Egmond</author><pubDate>Fri, 22 Sep 2023 14:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05422v2</guid></item><item><title>DeepOPF-U: A Unified Deep Neural Network to Solve AC Optimal Power Flow in Multiple Networks</title><link>http://arxiv.org/abs/2309.12849v1</link><description>The traditional machine learning models to solve optimal power flow (OPF) aremostly trained for a given power network and lack generalizability to today'spower networks with varying topologies and growing plug-and-play distributedenergy resources (DERs). In this paper, we propose DeepOPF-U, which uses oneunified deep neural network (DNN) to solve alternating-current (AC) OPFproblems in different power networks, including a set of power networks that issuccessively expanding. Specifically, we design elastic input and output layersfor the vectors of given loads and OPF solutions with varying lengths indifferent networks. The proposed method, using a single unified DNN, can dealwith different and growing numbers of buses, lines, loads, and DERs.Simulations of IEEE 57/118/300-bus test systems and a network growing from 73to 118 buses verify the improved performance of DeepOPF-U compared to existingDNN-based solution methods.</description><author>Heng Liang, Changhong Zhao</author><pubDate>Fri, 22 Sep 2023 14:22:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12849v1</guid></item><item><title>Metrics reloaded: Recommendations for image analysis validation</title><link>http://arxiv.org/abs/2206.01653v7</link><description>Increasing evidence shows that flaws in machine learning (ML) algorithmvalidation are an underestimated global problem. Particularly in automaticbiomedical image analysis, chosen performance metrics often do not reflect thedomain interest, thus failing to adequately measure scientific progress andhindering translation of ML techniques into practice. To overcome this, ourlarge international expert consortium created Metrics Reloaded, a comprehensiveframework guiding researchers in the problem-aware selection of metrics.Following the convergence of ML methodology across application domains, MetricsReloaded fosters the convergence of validation methodology. The framework wasdeveloped in a multi-stage Delphi process and is based on the novel concept ofa problem fingerprint - a structured representation of the given problem thatcaptures all aspects that are relevant for metric selection, from the domaininterest to the properties of the target structure(s), data set and algorithmoutput. Based on the problem fingerprint, users are guided through the processof choosing and applying appropriate validation metrics while being made awareof potential pitfalls. Metrics Reloaded targets image analysis problems thatcan be interpreted as a classification task at image, object or pixel level,namely image-level classification, object detection, semantic segmentation, andinstance segmentation tasks. To improve the user experience, we implemented theframework in the Metrics Reloaded online tool, which also provides a point ofaccess to explore weaknesses, strengths and specific recommendations for themost common validation metrics. The broad applicability of our framework acrossdomains is demonstrated by an instantiation for various biological and medicalimage analysis use cases.</description><author>Lena Maier-Hein, Annika Reinke, Patrick Godau, Minu D. Tizabi, Florian Buettner, Evangelia Christodoulou, Ben Glocker, Fabian Isensee, Jens Kleesiek, Michal Kozubek, Mauricio Reyes, Michael A. Riegler, Manuel Wiesenfarth, A. Emre Kavur, Carole H. Sudre, Michael Baumgartner, Matthias Eisenmann, Doreen Heckmann-Nötzel, A. Tim Rädsch, Laura Acion, Michela Antonelli, Tal Arbel, Spyridon Bakas, Arriel Benis, Matthew Blaschko, M. Jorge Cardoso, Veronika Cheplygina, Beth A. Cimini, Gary S. Collins, Keyvan Farahani, Luciana Ferrer, Adrian Galdran, Bram van Ginneken, Robert Haase, Daniel A. Hashimoto, Michael M. Hoffman, Merel Huisman, Pierre Jannin, Charles E. Kahn, Dagmar Kainmueller, Bernhard Kainz, Alexandros Karargyris, Alan Karthikesalingam, Hannes Kenngott, Florian Kofler, Annette Kopp-Schne</author><pubDate>Fri, 22 Sep 2023 14:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.01653v7</guid></item><item><title>Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction</title><link>http://arxiv.org/abs/2308.10820v2</link><description>Hyperspectral Image (HSI) reconstruction has made gratifying progress withthe deep unfolding framework by formulating the problem into a data module anda prior module. Nevertheless, existing methods still face the problem ofinsufficient matching with HSI data. The issues lie in three aspects: 1) fixedgradient descent step in the data module while the degradation of HSI isagnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3)stage interaction ignoring the differences in features at different stages. Toaddress these issues, in this work, we propose a Pixel Adaptive Deep UnfoldingTransformer (PADUT) for HSI reconstruction. In the data module, a pixeladaptive descent step is employed to focus on pixel-level agnostic degradation.In the prior module, we introduce the Non-local Spectral Transformer (NST) toemphasize the 3D characteristics of HSI for recovering. Moreover, inspired bythe diverse expression of features in different stages and depths, the stageinteraction is improved by the Fast Fourier Transform (FFT). Experimentalresults on both simulated and real scenes exhibit the superior performance ofour method compared to state-of-the-art HSI reconstruction methods. The code isreleased at: https://github.com/MyuLi/PADUT.</description><author>Miaoyu Li, Ying Fu, Ji Liu, Yulun Zhang</author><pubDate>Fri, 22 Sep 2023 14:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10820v2</guid></item><item><title>Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive?</title><link>http://arxiv.org/abs/2306.09281v3</link><description>Motion forecasting is crucial in enabling autonomous vehicles to anticipatethe future trajectories of surrounding agents. To do so, it requires solvingmapping, detection, tracking, and then forecasting problems, in a multi-steppipeline. In this complex system, advances in conventional forecasting methodshave been made using curated data, i.e., with the assumption of perfect maps,detection, and tracking. This paradigm, however, ignores any errors fromupstream modules. Meanwhile, an emerging end-to-end paradigm, that tightlyintegrates the perception and forecasting architectures into joint training,promises to solve this issue. So far, however, the evaluation protocols betweenthe two methods were incompatible and their comparison was not possible. Infact, and perhaps surprisingly, conventional forecasting methods are usuallynot trained nor tested in real-world pipelines (e.g., with upstream detection,tracking, and mapping modules). In this work, we aim to bring forecastingmodels closer to real-world deployment. First, we propose a unified evaluationpipeline for forecasting methods with real-world perception inputs, allowing usto compare the performance of conventional and end-to-end methods for the firsttime. Second, our in-depth study uncovers a substantial performance gap whentransitioning from curated to perception-based data. In particular, we showthat this gap (1) stems not only from differences in precision but also fromthe nature of imperfect inputs provided by perception modules, and that (2) isnot trivially reduced by simply finetuning on perception outputs. Based onextensive experiments, we provide recommendations for critical areas thatrequire improvement and guidance towards more robust motion forecasting in thereal world. We will release an evaluation library to benchmark models understandardized and practical conditions.</description><author>Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Alexandre Alahi, Matthieu Cord, Patrick Pérez</author><pubDate>Fri, 22 Sep 2023 14:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09281v3</guid></item><item><title>SRFNet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events</title><link>http://arxiv.org/abs/2309.12842v1</link><description>Monocular depth estimation is a crucial task to measure distance relative toa camera, which is important for applications, such as robot navigation andself-driving. Traditional frame-based methods suffer from performance drops dueto the limited dynamic range and motion blur. Therefore, recent works leveragenovel event cameras to complement or guide the frame modality via frame-eventfeature fusion. However, event streams exhibit spatial sparsity, leaving someareas unperceived, especially in regions with marginal light changes.Therefore, direct fusion methods, e.g., RAMNet, often ignore the contributionof the most confident regions of each modality. This leads to structuralambiguity in the modality fusion process, thus degrading the depth estimationperformance. In this paper, we propose a novel Spatial Reliability-orientedFusion Network (SRFNet), that can estimate depth with fine-grained structure atboth daytime and nighttime. Our method consists of two key technicalcomponents. Firstly, we propose an attention-based interactive fusion (AIF)module that applies spatial priors of events and frames as the initial masksand learns the consensus regions to guide the inter-modal feature fusion. Thefused feature are then fed back to enhance the frame and event featurelearning. Meanwhile, it utilizes an output head to generate a fused mask, whichis iteratively updated for learning consensual spatial priors. Secondly, wepropose the Reliability-oriented Depth Refinement (RDR) module to estimatedense depth with the fine-grained structure based on the fused features andmasks. We evaluate the effectiveness of our method on the synthetic andreal-world datasets, which shows that, even without pretraining, our methodoutperforms the prior methods, e.g., RAMNet, especially in night scenes. Ourproject homepage: https://vlislab22.github.io/SRFNet.</description><author>Tianbo Pan, Zidong Cao, Lin Wang</author><pubDate>Fri, 22 Sep 2023 13:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12842v1</guid></item><item><title>Reward Function Design for Crowd Simulation via Reinforcement Learning</title><link>http://arxiv.org/abs/2309.12841v1</link><description>Crowd simulation is important for video-games design, since it enables topopulate virtual worlds with autonomous avatars that navigate in a human-likemanner. Reinforcement learning has shown great potential in simulating virtualcrowds, but the design of the reward function is critical to achievingeffective and efficient results. In this work, we explore the design of rewardfunctions for reinforcement learning-based crowd simulation. We providetheoretical insights on the validity of certain reward functions according totheir analytical properties, and evaluate them empirically using a range ofscenarios, using the energy efficiency as the metric. Our experiments show thatdirectly minimizing the energy usage is a viable strategy as long as it ispaired with an appropriately scaled guiding potential, and enable us to studythe impact of the different reward components on the behavior of the simulatedcrowd. Our findings can inform the development of new crowd simulationtechniques, and contribute to the wider study of human-like navigation.</description><author>Ariel Kwiatkowski, Vicky Kalogeiton, Julien Pettré, Marie-Paule Cani</author><pubDate>Fri, 22 Sep 2023 13:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12841v1</guid></item><item><title>Model-based causal feature selection for general response types</title><link>http://arxiv.org/abs/2309.12833v1</link><description>Discovering causal relationships from observational data is a fundamental yetchallenging task. In some applications, it may suffice to learn the causalfeatures of a given response variable, instead of learning the entireunderlying causal structure. Invariant causal prediction (ICP, Peters et al.,2016) is a method for causal feature selection which requires data fromheterogeneous settings. ICP assumes that the mechanism for generating theresponse from its direct causes is the same in all settings and exploits thisinvariance to output a subset of the causal features. The framework of ICP hasbeen extended to general additive noise models and to nonparametric settingsusing conditional independence testing. However, nonparametric conditionalindependence testing often suffers from low power (or poor type I errorcontrol) and the aforementioned parametric models are not suitable forapplications in which the response is not measured on a continuous scale, butrather reflects categories or counts. To bridge this gap, we develop ICP in thecontext of transformation models (TRAMs), allowing for continuous, categorical,count-type, and uninformatively censored responses (we show that, in general,these model classes do not allow for identifiability when there is no exogenousheterogeneity). We propose TRAM-GCM, a test for invariance of a subset ofcovariates, based on the expected conditional covariance between environmentsand score residuals which satisfies uniform asymptotic level guarantees. Forthe special case of linear shift TRAMs, we propose an additional invariancetest, TRAM-Wald, based on the Wald statistic. We implement both proposedmethods in the open-source R package "tramicp" and show in simulations thatunder the correct model specification, our approach empirically yields higherpower than nonparametric ICP based on conditional independence testing.</description><author>Lucas Kook, Sorawit Saengkyongam, Anton Rask Lundborg, Torsten Hothorn, Jonas Peters</author><pubDate>Fri, 22 Sep 2023 13:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12833v1</guid></item><item><title>Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion</title><link>http://arxiv.org/abs/2211.16247v2</link><description>Deep 3D point cloud models are sensitive to adversarial attacks, which posesthreats to safety-critical applications such as autonomous driving. Robusttraining and defend-by-denoising are typical strategies for defendingadversarial perturbations. However, they either induce massive computationaloverhead or rely heavily upon specified priors, limiting generalized robustnessagainst attacks of all kinds. To remedy it, this paper introduces a noveldistortion-aware defense framework that can rebuild the pristine datadistribution with a tailored intensity estimator and a diffusion model. Toperform distortion-aware forward diffusion, we design a distortion estimationalgorithm that is obtained by summing the distance of each point to thebest-fitting plane of its local neighboring points, which is based on theobservation of the local spatial properties of the adversarial point cloud. Byiterative diffusion and reverse denoising, the perturbed point cloud undervarious distortions can be restored back to a clean distribution. This approachenables effective defense against adaptive attacks with varying noise budgets,enhancing the robustness of existing 3D deep recognition models.</description><author>Kui Zhang, Hang Zhou, Jie Zhang, Qidong Huang, Weiming Zhang, Nenghai Yu</author><pubDate>Fri, 22 Sep 2023 13:42:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16247v2</guid></item><item><title>Synthetic Experience Replay</title><link>http://arxiv.org/abs/2303.06614v3</link><description>A key theme in the past decade has been that when large neural networks andlarge datasets combine they can produce remarkable results. In deepreinforcement learning (RL), this paradigm is commonly made possible throughexperience replay, whereby a dataset of past experiences is used to train apolicy or value function. However, unlike in supervised or self-supervisedlearning, an RL agent has to collect its own data, which is often limited.Thus, it is challenging to reap the benefits of deep learning, and even smallneural networks can overfit at the start of training. In this work, we leveragethe tremendous recent progress in generative modeling and propose SyntheticExperience Replay (SynthER), a diffusion-based approach to flexibly upsample anagent's collected experience. We show that SynthER is an effective method fortraining RL agents across offline and online settings, in both proprioceptiveand pixel-based environments. In offline settings, we observe drasticimprovements when upsampling small offline datasets and see that additionalsynthetic data also allows us to effectively train larger networks.Furthermore, SynthER enables online agents to train with a much higherupdate-to-data ratio than before, leading to a significant increase in sampleefficiency, without any algorithmic changes. We believe that synthetic trainingdata could open the door to realizing the full potential of deep learning forreplay-based RL algorithms from limited data. Finally, we open-source our codeat https://github.com/conglu1997/SynthER.</description><author>Cong Lu, Philip J. Ball, Yee Whye Teh, Jack Parker-Holder</author><pubDate>Fri, 22 Sep 2023 13:41:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06614v3</guid></item><item><title>AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling</title><link>http://arxiv.org/abs/2309.12830v1</link><description>The rising usage of AI and ML-based processing across application domains hasexacerbated the need for low-cost ML implementation, specifically forresource-constrained embedded systems. To this end, approximate computing, anapproach that explores the power, performance, area (PPA), and behavioralaccuracy (BEHAV) trade-offs, has emerged as a possible solution forimplementing embedded machine learning. Due to the predominance of MACoperations in ML, designing platform-specific approximate arithmetic operatorsforms one of the major research problems in approximate computing. Recentlythere has been a rising usage of AI/ML-based design space explorationtechniques for implementing approximate operators. However, most of theseapproaches are limited to using ML-based surrogate functions for predicting thePPA and BEHAV impact of a set of related design decisions. While this approachleverages the regression capabilities of ML methods, it does not exploit themore advanced approaches in ML. To this end, we propose AxOCS, a methodologyfor designing approximate arithmetic operators through ML-based supersampling.Specifically, we present a method to leverage the correlation of PPA and BEHAVmetrics across operators of varying bit-widths for generating larger bit-widthoperators. The proposed approach involves traversing the relatively smallerdesign space of smaller bit-width operators and employing its associatedDesign-PPA-BEHAV relationship to generate initial solutions formetaheuristics-based optimization for larger operators. The experimentalevaluation of AxOCS for FPGA-optimized approximate operators shows that theproposed approach significantly improves the quality-resulting hypervolume formulti-objective optimization-of 8x8 signed approximate multipliers.</description><author>Siva Satyendra Sahoo, Salim Ullah, Soumyo Bhattacharjee, Akash Kumar</author><pubDate>Fri, 22 Sep 2023 13:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12830v1</guid></item><item><title>Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography</title><link>http://arxiv.org/abs/2309.12829v1</link><description>Accurate segmentation is essential for echocardiography-based assessment ofcardiovascular diseases (CVDs). However, the variability among sonographers andthe inherent challenges of ultrasound images hinder precise segmentation. Byleveraging the joint representation of image and text modalities,Vision-Language Segmentation Models (VLSMs) can incorporate rich contextualinformation, potentially aiding in accurate and explainable segmentation.However, the lack of readily available data in echocardiography hampers thetraining of VLSMs. In this study, we explore using synthetic datasets fromSemantic Diffusion Models (SDMs) to enhance VLSMs for echocardiographysegmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS)using seven different kinds of language prompts derived from severalattributes, automatically extracted from echocardiography images, segmentationmasks, and their metadata. Our results show improved metrics and fasterconvergence when pretraining VLSMs on SDM-generated synthetic images beforefinetuning on real images. The code, configs, and prompts are available athttps://github.com/naamiinepal/synthetic-boost.</description><author>Rabin Adhikari, Manish Dhakal, Safal Thapaliya, Kanchan Poudel, Prasiddha Bhandari, Bishesh Khanal</author><pubDate>Fri, 22 Sep 2023 13:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12829v1</guid></item><item><title>AceGPT, Localizing Large Language Models in Arabic</title><link>http://arxiv.org/abs/2309.12053v2</link><description>This paper explores the imperative need and methodology for developing alocalized Large Language Model (LLM) tailored for Arabic, a language withunique cultural characteristics that are not adequately addressed by currentmainstream models like ChatGPT. Key concerns additionally arise whenconsidering cultural sensitivity and local values. To this end, the paperoutlines a packaged solution, including further pre-training with Arabic texts,supervised fine-tuning (SFT) using native Arabic instructions and GPT-4responses in Arabic, and reinforcement learning with AI feedback (RLAIF) usinga reward model that is sensitive to local culture and values. The objective isto train culturally aware and value-aligned Arabic LLMs that can serve thediverse application-specific needs of Arabic-speaking communities. Extensive evaluations demonstrated that the resulting LLM called `AceGPT' isthe SOTA open Arabic LLM in various benchmarks, including instruction-followingbenchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval), knowledge benchmark(i.e., Arabic MMLU and EXAMs), as well as the newly-proposed Arabic cultural \&amp;value alignment benchmark. Notably, AceGPT outperforms ChatGPT in the popularVicuna-80 benchmark when evaluated with GPT-4, despite the benchmark's limitedscale. % Natural Language Understanding (NLU) benchmark (i.e., ALUE) Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.</description><author>Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, Jinchao Xu</author><pubDate>Fri, 22 Sep 2023 13:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12053v2</guid></item><item><title>OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control</title><link>http://arxiv.org/abs/2309.12825v1</link><description>In this work, we introduce OmniDrones, an efficient and flexible platformtailored for reinforcement learning in drone control, built on Nvidia'sOmniverse Isaac Sim. It employs a bottom-up design approach that allows usersto easily design and experiment with various application scenarios on top ofGPU-parallelized simulations. It also offers a range of benchmark tasks,presenting challenges ranging from single-drone hovering to over-actuatedsystem tracking. In summary, we propose an open-sourced drone simulationplatform, equipped with an extensive suite of tools for drone learning. Itincludes 4 drone models, 5 sensor modalities, 4 control modes, over 10benchmark tasks, and a selection of widely used RL baselines. To showcase thecapabilities of OmniDrones and to support future research, we also providepreliminary results on these benchmark tasks. We hope this platform willencourage further studies on applying RL to practical drone systems.</description><author>Botian Xu, Feng Gao, Chao Yu, Ruize Zhang, Yi Wu, Yu Wang</author><pubDate>Fri, 22 Sep 2023 13:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12825v1</guid></item><item><title>A Spectral Theory of Neural Prediction and Alignment</title><link>http://arxiv.org/abs/2309.12821v1</link><description>The representations of neural networks are often compared to those ofbiological systems by performing regression between the neural networkresponses and those measured from biological systems. Many differentstate-of-the-art deep neural networks yield similar neural predictions, but itremains unclear how to differentiate among models that perform equally well atpredicting neural responses. To gain insight into this, we use a recenttheoretical framework that relates the generalization error from regression tothe spectral bias of the model activations and the alignment of the neuralresponses onto the learnable subspace of the model. We extend this theory tothe case of regression between model activations and neural responses, anddefine geometrical properties describing the error embedding geometry. We testa large number of deep neural networks that predict visual cortical activityand show that there are multiple types of geometries that result in low neuralprediction error as measured via regression. The work demonstrates thatcarefully decomposing representational metrics can provide interpretability ofhow models are capturing neural activity and points the way towards improvedmodels of neural activity.</description><author>Abdulkadir Canatar, Jenelle Feather, Albert Wakhloo, SueYeon Chung</author><pubDate>Fri, 22 Sep 2023 13:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12821v1</guid></item><item><title>Robust Ellipsoid Fitting Using Axial Distance and Combination</title><link>http://arxiv.org/abs/2304.00517v2</link><description>In random sample consensus (RANSAC), the problem of ellipsoid fitting can beformulated as a problem of minimization of point-to-model distance, which isrealized by maximizing model score. Hence, the performance of ellipsoid fittingis affected by distance metric. In this paper, we proposed a novel distancemetric called the axial distance, which is converted from the algebraicdistance by introducing a scaling factor to solve nongeometric problems of thealgebraic distance. There is complementarity between the axial distance andSampson distance because their combination is a stricter metric whencalculating the model score of sample consensus and the weight of the weightedleast squares (WLS) fitting. Subsequently, a novel sample-consensus-basedellipsoid fitting method is proposed by using the combination between the axialdistance and Sampson distance (CAS). We compare the proposed method withseveral representative fitting methods through experiments on synthetic andreal datasets. The results show that the proposed method has a higherrobustness against outliers, consistently high accuracy, and a speed close tothat of the method based on sample consensus.</description><author>Min Han, Jiangming Kan, Gongping Yang, Xinghui Li</author><pubDate>Fri, 22 Sep 2023 13:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00517v2</guid></item><item><title>FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy</title><link>http://arxiv.org/abs/2305.10307v3</link><description>Measuring the distance between machine-produced and human language is acritical open problem. Inspired by empirical findings from psycholinguistics onthe periodicity of entropy in language, we propose FACE, a set of metrics basedon Fourier Analysis of the estimated Cross-Entropy of language, for measuringthe similarity between model-generated and human-written languages. Based on anopen-ended generation task and the experimental data from previous studies, wefind that FACE can effectively identify the human-model gap, scales with modelsize, reflects the outcomes of different sampling methods for decoding,correlates well with other evaluation metrics and with human judgment scores.FACE is computationally efficient and provides intuitive interpretations.</description><author>Zuhao Yang, Yingfang Yuan, Yang Xu, Shuo Zhan, Huajun Bai, Kefan Chen</author><pubDate>Fri, 22 Sep 2023 13:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10307v3</guid></item><item><title>AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial</title><link>http://arxiv.org/abs/2306.03753v3</link><description>Art curatorial practice is characterized by the presentation of an artcollection in a knowledgeable way. Machine processes are characterized by theircapacity to manage and analyze large amounts of data. This paper envisages AIcuration and audience interaction to explore the implications of contemporarymachine learning models for the curatorial world. This project was developedfor the occasion of the 2023 Helsinki Art Biennial, entitled New Directions MayEmerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the cityof Helsinki through the lens of machine perception. We use visual-textualmodels to place indoor artworks in public spaces, assigning fictionalcoordinates based on similarity scores. We transform the space that eachartwork inhabits in the city by generating synthetic 360 art panoramas. Weguide the generation estimating depth values from 360 panoramas at each artworklocation, and machine-generated prompts of the artworks. The result of thisproject is an AI curation that places the artworks in their imagined physicalspace, blurring the lines of artwork, context, and machine perception. The workis virtually presented as a web-based installation on this linkhttp://newlyformedcity.net/, where users can navigate an alternative version ofthe city while exploring and interacting with its cultural heritage at scale.</description><author>Ludovica Schaerf, Pepe Ballesteros, Valentine Bernasconi, Iacopo Neri, Dario Negueruela del Castillo</author><pubDate>Fri, 22 Sep 2023 13:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03753v3</guid></item></channel></rss>