<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 05 Sep 2024 13:00:23 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</title><link>http://arxiv.org/abs/2409.02920v1</link><description>Effective collaboration of dual-arm robots and their tool use capabilitiesare increasingly important areas in the advancement of robotics. These skillsplay a significant role in expanding robots' ability to operate in diversereal-world environments. However, progress is impeded by the scarcity ofspecialized training data. This paper introduces RoboTwin, a novel benchmarkdataset combining real-world teleoperated data with synthetic data from digitaltwins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform,we have collected diverse data on tool usage and human-robot interaction. Wepresent a innovative approach to creating digital twins using AI-generatedcontent, transforming 2D images into detailed 3D models. Furthermore, weutilize large language models to generate expert-level training data andtask-specific pose sequences oriented toward functionality. Our keycontributions are: 1) the RoboTwin benchmark dataset, 2) an efficientreal-to-simulation pipeline, and 3) the use of language models for automaticexpert-level data generation. These advancements are designed to address theshortage of robotic training data, potentially accelerating the development ofmore capable and versatile robotic systems for a wide range of real-worldapplications. The project page is available athttps://robotwin-benchmark.github.io/early-version/</description><author>Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen, Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie, Ping Luo</author><pubDate>Wed, 04 Sep 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02920v1</guid></item><item><title>HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts</title><link>http://arxiv.org/abs/2409.02919v1</link><description>The potential for higher-resolution image generation using pretraineddiffusion models is immense, yet these models often struggle with issues ofobject repetition and structural artifacts especially when scaling to 4Kresolution and higher. We figure out that the problem is caused by that, asingle prompt for the generation of multiple scales provides insufficientefficacy. In response, we propose HiPrompt, a new tuning-free solution thattackles the above problems by introducing hierarchical prompts. Thehierarchical prompts offer both global and local guidance. Specifically, theglobal guidance comes from the user input that describes the overall content,while the local guidance utilizes patch-wise descriptions from MLLMs toelaborately guide the regional structure and texture generation. Furthermore,during the inverse denoising process, the generated noise is decomposed intolow- and high-frequency spatial components. These components are conditioned onmultiple prompt levels, including detailed patch-wise descriptions and broaderimage-level prompts, facilitating prompt-guided denoising under hierarchicalsemantic guidance. It further allows the generation to focus more on localspatial regions and ensures the generated images maintain coherent local andglobal semantics, structures, and textures with high definition. Extensiveexperiments demonstrate that HiPrompt outperforms state-of-the-art works inhigher-resolution image generation, significantly reducing object repetitionand enhancing structural quality.</description><author>Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, Wenhan Luo, Qingfeng Liu, Yike Guo</author><pubDate>Wed, 04 Sep 2024 17:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02919v1</guid></item><item><title>Decentralized Health Intelligence Network (DHIN)</title><link>http://arxiv.org/abs/2408.06240v4</link><description>Decentralized Health Intelligence Network (DHIN) extends the DecentralizedIntelligence Network (DIN) framework to address challenges in healthcare datasovereignty and AI utilization. Building upon DIN's core principles, DHINintroduces healthcare-specific components to tackle data fragmentation acrossproviders and institutions, establishing a sovereign architecture forhealthcare provision. It facilitates effective AI utilization by overcomingbarriers to accessing diverse health data sources. This comprehensive frameworkleverages: 1) self-sovereign identity architecture coupled with a personalhealth record (PHR), extending DIN's personal data stores concept to ensurehealth data sovereignty; 2) a scalable federated learning (FL) protocolimplemented on a public blockchain for decentralized AI training in healthcare,tailored for medical data; and 3) a scalable, trustless rewards mechanismadapted from DIN to incentivize participation in healthcare AI development.DHIN operates on a public blockchain with an immutable record, ensuring that noentity can control access to health data or determine financial benefits. Itsupports effective AI training while allowing patients to maintain control overtheir health data, benefit financially, and contribute to a decentralizedecosystem. Unique to DHIN, patients receive rewards in digital wallets as anincentive to opt into the FL protocol, with a long-term roadmap to funddecentralized insurance solutions. This approach introduces a novel,self-financed healthcare model that adapts to individual needs, complementsexisting systems, and redefines universal coverage, showcasing how DINprinciples can transform healthcare data management and AI utilization whileempowering patients.</description><author>Abraham Nash</author><pubDate>Wed, 04 Sep 2024 17:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06240v4</guid></item><item><title>UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views</title><link>http://arxiv.org/abs/2409.02917v1</link><description>Visualizing surgical scenes is crucial for revealing internal anatomicalstructures during minimally invasive procedures. Novel View Synthesis is avital technique that offers geometry and appearance reconstruction, enhancingunderstanding, planning, and decision-making in surgical scenes. Despite theimpressive achievements of Neural Radiance Field (NeRF), its direct applicationto surgical scenes produces unsatisfying results due to two challenges:endoscopic sparse views and significant photometric inconsistencies. In thispaper, we propose uncertainty-aware conditional NeRF for novel view synthesisto tackle the severe shape-radiance ambiguity from sparse surgical views. Thecore of UC-NeRF is to incorporate the multi-view uncertainty estimation tocondition the neural radiance field for modeling the severe photometricinconsistencies adaptively. Specifically, our UC-NeRF first builds aconsistency learner in the form of multi-view stereo network, to establish thegeometric correspondence from sparse views and generate uncertainty estimationand feature priors. In neural rendering, we design a base-adaptive NeRF networkto exploit the uncertainty estimation for explicitly handling the photometricinconsistencies. Furthermore, an uncertainty-guided geometry distillation isemployed to enhance geometry learning. Experiments on the SCARED and Hamlyndatasets demonstrate our superior performance in rendering appearance andgeometry, consistently outperforming the current state-of-the-art approaches.Our code will be released at \url{https://github.com/wrld/UC-NeRF}.</description><author>Jiaxin Guo, Jiangliu Wang, Ruofeng Wei, Di Kang, Qi Dou, Yun-hui Liu</author><pubDate>Wed, 04 Sep 2024 17:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02917v1</guid></item><item><title>Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving</title><link>http://arxiv.org/abs/2409.02914v1</link><description>Large Vision-Language Models (LVLMs) have recently garnered significantattention, with many efforts aimed at harnessing their general knowledge toenhance the interpretability and robustness of autonomous driving models.However, LVLMs typically rely on large, general-purpose datasets and lack thespecialized expertise required for professional and safe driving. Existingvision-language driving datasets focus primarily on scene understanding anddecision-making, without providing explicit guidance on traffic rules anddriving skills, which are critical aspects directly related to driving safety.To bridge this gap, we propose IDKB, a large-scale dataset containing over onemillion data items collected from various countries, including drivinghandbooks, theory test data, and simulated road test data. Much like theprocess of obtaining a driver's license, IDKB encompasses nearly all theexplicit knowledge needed for driving from theory to practice. In particular,we conducted comprehensive tests on 15 LVLMs using IDKB to assess theirreliability in the context of autonomous driving and provided extensiveanalysis. We also fine-tuned popular models, achieving notable performanceimprovements, which further validate the significance of our dataset. Theproject page can be found at:\url{https://4dvlab.github.io/project_page/idkb.html}</description><author>Yuhang Lu, Yichen Yao, Jiadong Tu, Jiangnan Shao, Yuexin Ma, Xinge Zhu</author><pubDate>Wed, 04 Sep 2024 17:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02914v1</guid></item><item><title>Enhancing Graph Neural Networks with Limited Labeled Data by Actively Distilling Knowledge from Large Language Models</title><link>http://arxiv.org/abs/2407.13989v3</link><description>Graphs are pervasive in the real-world, such as social network analysis,bioinformatics, and knowledge graphs. Graph neural networks (GNNs) have greatability in node classification, a fundamental task on graphs. Unfortunately,conventional GNNs still face challenges in scenarios with few labeled nodes,despite the prevalence of few-shot node classification tasks in real-worldapplications. To address this challenge, various approaches have been proposed,including graph meta-learning, transfer learning, and methods based on LargeLanguage Models (LLMs). However, traditional meta-learning and transferlearning methods often require prior knowledge from base classes or fail toexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-basedmethods may overlook the zero-shot capabilities of LLMs and rely heavily on thequality of generated contexts. In this paper, we propose a novel approach thatintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoningcapabilities of LLMs and employing a Graph-LLM-based active learning paradigmto enhance GNNs' performance. Extensive experiments demonstrate theeffectiveness of our model in improving node classification accuracy withconsiderably limited labeled data, surpassing state-of-the-art baselines bysignificant margins.</description><author>Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, Suhang Wang</author><pubDate>Wed, 04 Sep 2024 17:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13989v3</guid></item><item><title>SITAR: Semi-supervised Image Transformer for Action Recognition</title><link>http://arxiv.org/abs/2409.02910v1</link><description>Recognizing actions from a limited set of labeled videos remains a challengeas annotating visual data is not only tedious but also can be expensive due toclassified nature. Moreover, handling spatio-temporal data using deep $3$Dtransformers for this can introduce significant computational complexity. Inthis paper, our objective is to address video action recognition in asemi-supervised setting by leveraging only a handful of labeled videos alongwith a collection of unlabeled videos in a compute efficient manner.Specifically, we rearrange multiple frames from the input videos in row-columnform to construct super images. Subsequently, we capitalize on the vast pool ofunlabeled samples and employ contrastive learning on the encoded super images.Our proposed approach employs two pathways to generate representations fortemporally augmented super images originating from the same video.Specifically, we utilize a 2D image-transformer to generate representations andapply a contrastive loss function to minimize the similarity betweenrepresentations from different videos while maximizing the representations ofidentical videos. Our method demonstrates superior performance compared toexisting state-of-the-art approaches for semi-supervised action recognitionacross various benchmark datasets, all while significantly reducingcomputational costs.</description><author>Owais Iqbal, Omprakash Chakraborty, Aftab Hussain, Rameswar Panda, Abir Das</author><pubDate>Wed, 04 Sep 2024 17:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02910v1</guid></item><item><title>Decentralized Intelligence Network (DIN)</title><link>http://arxiv.org/abs/2407.02461v5</link><description>Decentralized Intelligence Network (DIN) is a theoretical framework designedto address challenges in AI development, particularly focusing on datafragmentation and siloing issues. It facilitates effective AI training withinsovereign data networks by overcoming barriers to accessing diverse datasources, leveraging: 1) personal data stores to ensure data sovereignty, wheredata remains securely within Participants' control; 2) a scalable federatedlearning protocol implemented on a public blockchain for decentralized AItraining, where only model parameter updates are shared, keeping data withinthe personal data stores; and 3) a scalable, trustless cryptographic rewardsmechanism on a public blockchain to incentivize participation and ensure fairreward distribution through a decentralized auditing protocol. This approachguarantees that no entity can prevent or control access to training data orinfluence financial benefits, as coordination and reward distribution aremanaged on the public blockchain with an immutable record. The frameworksupports effective AI training by allowing Participants to maintain controlover their data, benefit financially, and contribute to a decentralized,scalable ecosystem that leverages collective AI to develop beneficialalgorithms.</description><author>Abraham Nash</author><pubDate>Wed, 04 Sep 2024 17:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02461v5</guid></item><item><title>Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling</title><link>http://arxiv.org/abs/2409.02908v1</link><description>Masked diffusion models (MDMs) have emerged as a popular research topic forgenerative modeling of discrete data, thanks to their superior performance overother discrete diffusion models, and are rivaling the auto-regressive models(ARMs) for language modeling tasks. The recent effort in simplifying the maskeddiffusion framework further leads to alignment with continuous-space diffusionmodels and more principled training and sampling recipes. In this paper,however, we reveal that both training and sampling of MDMs are theoreticallyfree from the time variable, arguably the key signature of diffusion models,and are instead equivalent to masked models. The connection on the samplingaspect is drawn by our proposed first-hitting sampler (FHS). Specifically, weshow that the FHS is theoretically equivalent to MDMs' original generationprocess while significantly alleviating the time-consuming categorical samplingand achieving a 20$\times$ speedup. In addition, our investigation challengesprevious claims that MDMs can surpass ARMs in generative perplexity. Weidentify, for the first time, an underlying numerical issue, even with the32-bit floating-point precision, which results in inaccurate categoricalsampling. We show that the numerical issue lowers the effective temperatureboth theoretically and empirically, leading to unfair assessments of MDMs'generation results in the previous literature.</description><author>Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang</author><pubDate>Wed, 04 Sep 2024 17:48:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02908v1</guid></item><item><title>Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning (PIML) Methods: Towards Robust Metrics</title><link>http://arxiv.org/abs/2402.11126v2</link><description>Physics-informed machine learning (PIML) as a means of solving partialdifferential equations (PDE) has garnered much attention in the ComputationalScience and Engineering (CS&amp;E) world. This topic encompasses a broad array ofmethods and models aimed at solving a single or a collection of PDE problems,called multitask learning. PIML is characterized by the incorporation ofphysical laws into the training process of machine learning models in lieu oflarge data when solving PDE problems. Despite the overall success of thiscollection of methods, it remains incredibly difficult to analyze, benchmark,and generally compare one approach to another. Using Kolmogorov n-widths as ameasure of effectiveness of approximating functions, we judiciously apply thismetric in the comparison of various multitask PIML architectures. We computelower accuracy bounds and analyze the model's learned basis functions onvarious PDE problems. This is the first objective metric for comparingmultitask PIML architectures and helps remove uncertainty in model validationfrom selective sampling and overfitting. We also identify avenues ofimprovement for model architectures, such as the choice of activation function,which can drastically affect model generalization to "worst-case" scenarios,which is not observed when reporting task-specific errors. We also incorporatethis metric into the optimization process through regularization, whichimproves the models' generalizability over the multitask PDE problem.</description><author>Michael Penwarden, Houman Owhadi, Robert M. Kirby</author><pubDate>Wed, 04 Sep 2024 17:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11126v2</guid></item><item><title>Hybrid Decentralized Optimization: Leveraging Both First- and Zeroth-Order Optimizers for Faster Convergence</title><link>http://arxiv.org/abs/2210.07703v2</link><description>Distributed optimization is the standard way of speeding up machine learningtraining, and most of the research in the area focuses on distributedfirst-order, gradient-based methods. Yet, there are settings where somecomputationally-bounded nodes may not be able to implement first-order,gradient-based optimization, while they could still contribute to jointoptimization tasks. In this paper, we initiate the study of hybriddecentralized optimization, studying settings where nodes with zeroth-order andfirst-order optimization capabilities co-exist in a distributed system, andattempt to jointly solve an optimization task over some data distribution. Weessentially show that, under reasonable parameter settings, such a system cannot only withstand noisier zeroth-order agents but can even benefit fromintegrating such agents into the optimization process, rather than ignoringtheir information. At the core of our approach is a new analysis of distributedoptimization with noisy and possibly-biased gradient estimators, which may beof independent interest. Our results hold for both convex and non-convexobjectives. Experimental results on standard optimization tasks confirm ouranalysis, showing that hybrid first-zeroth order optimization can be practical,even when training deep neural networks.</description><author>Matin Ansaripour, Shayan Talaei, Giorgi Nadiradze, Dan Alistarh</author><pubDate>Wed, 04 Sep 2024 17:45:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07703v2</guid></item><item><title>Topological Methods in Machine Learning: A Tutorial for Practitioners</title><link>http://arxiv.org/abs/2409.02901v1</link><description>Topological Machine Learning (TML) is an emerging field that leveragestechniques from algebraic topology to analyze complex data structures in waysthat traditional machine learning methods may not capture. This tutorialprovides a comprehensive introduction to two key TML techniques, persistenthomology and the Mapper algorithm, with an emphasis on practical applications.Persistent homology captures multi-scale topological features such as clusters,loops, and voids, while the Mapper algorithm creates an interpretable graphsummarizing high-dimensional data. To enhance accessibility, we adopt adata-centric approach, enabling readers to gain hands-on experience applyingthese techniques to relevant tasks. We provide step-by-step explanations,implementations, hands-on examples, and case studies to demonstrate how thesetools can be applied to real-world problems. The goal is to equip researchersand practitioners with the knowledge and resources to incorporate TML intotheir work, revealing insights often hidden from conventional machine learningmethods. The tutorial code is available athttps://github.com/cakcora/TopologyForML</description><author>Baris Coskunuzer, Cüneyt Gürcan Akçora</author><pubDate>Wed, 04 Sep 2024 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02901v1</guid></item><item><title>LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA</title><link>http://arxiv.org/abs/2409.02897v1</link><description>Though current long-context large language models (LLMs) have demonstratedimpressive capacities in answering user questions based on extensive text, thelack of citations in their responses makes user verification difficult, leadingto concerns about their trustworthiness due to their potential hallucinations.In this work, we aim to enable long-context LLMs to generate responses withfine-grained sentence-level citations, improving their faithfulness andverifiability. We first introduce LongBench-Cite, an automated benchmark forassessing current LLMs' performance in Long-Context Question Answering withCitations (LQAC), revealing considerable room for improvement. To this end, wepropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMsto automatically generate long-context QA instances with precise sentence-levelcitations, and leverage this pipeline to construct LongCite-45k, a large-scaleSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using theLongCite-45k dataset, successfully enabling their generation of accurateresponses and fine-grained sentence-level citations in a single output. Theevaluation results on LongBench-Cite show that our trained models achievestate-of-the-art citation quality, surpassing advanced proprietary modelsincluding GPT-4o.</description><author>jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li</author><pubDate>Wed, 04 Sep 2024 17:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02897v1</guid></item><item><title>Regional data-driven weather modeling with a global stretched-grid</title><link>http://arxiv.org/abs/2409.02891v1</link><description>A data-driven model (DDM) suitable for regional weather forecastingapplications is presented. The model extends the Artificial IntelligenceForecasting System by introducing a stretched-grid architecture that dedicateshigher resolution over a regional area of interest and maintains a lowerresolution elsewhere on the globe. The model is based on graph neural networks,which naturally affords arbitrary multi-resolution grid configurations. The model is applied to short-range weather prediction for the Nordics,producing forecasts at 2.5 km spatial and 6 h temporal resolution. The model ispre-trained on 43 years of global ERA5 data at 31 km resolution and is furtherrefined using 3.3 years of 2.5 km resolution operational analyses from theMetCoOp Ensemble Prediction System (MEPS). The performance of the model isevaluated using surface observations from measurement stations across Norwayand is compared to short-range weather forecasts from MEPS. The DDM outperformsboth the control run and the ensemble mean of MEPS for 2 m temperature. Themodel also produces competitive precipitation and wind speed forecasts, but isshown to underestimate extreme events.</description><author>Thomas Nils Nipen, Håvard Homleid Haugen, Magnus Sikora Ingstad, Even Marius Nordhagen, Aram Farhad Shafiq Salihi, Paulina Tedesco, Ivar Ambjørn Seierstad, Jørn Kristiansen, Simon Lang, Mihai Alexe, Jesper Dramsch, Baudouin Raoult, Gert Mertes, Matthew Chantry</author><pubDate>Wed, 04 Sep 2024 17:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02891v1</guid></item><item><title>LADDER: Language Driven Slice Discovery and Error Rectification</title><link>http://arxiv.org/abs/2408.07832v3</link><description>Error slice discovery associates structured patterns with model errors.Existing methods discover error slices by clustering the error-prone sampleswith similar patterns or assigning discrete attributes to each sample forpost-hoc analysis. While these methods aim for interpretability and easiermitigation through reweighting or rebalancing, they may not capture the fullcomplexity of error patterns due to incomplete or missing attributes. Contraryto the existing approach, this paper utilizes the reasoning capabilities of theLarge Language Model (LLM) to analyze complex error patterns and generatetestable hypotheses. This paper proposes LADDER: Language Driven sliceDiscovery and Error Rectification. It first projects the model's representationinto a language-aligned feature space (eg CLIP) to preserve semantics in theoriginal model feature space. This ensures the accurate retrieval of sentencesthat highlight the model's errors. Next, the LLM utilizes the sentences andgenerates hypotheses to discover error slices. Finally, we mitigate the errorby fine-tuning the classification head by creating a group-balanced datasetusing the hypotheses. Our entire method does not require any attributeannotation, either explicitly or through external tagging models. We validateour method with \textbf{five} image classification datasets. The code isavailable (https://github.com/batmanlab/Ladder).</description><author>Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Kayhan Batmanghelich</author><pubDate>Wed, 04 Sep 2024 17:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07832v3</guid></item><item><title>Quantifying uncertainty in lung cancer segmentation with foundation models applied to mixed-domain datasets</title><link>http://arxiv.org/abs/2403.13113v2</link><description>Medical image foundation models have shown the ability to segment organs andtumors with minimal fine-tuning. These models are typically evaluated ontask-specific in-distribution (ID) datasets. However, reliable performance onID dataset does not guarantee robust generalization on out-of-distribution(OOD) datasets. Importantly, once deployed for clinical use, it is impracticalto have `ground truth' delineations to assess ongoing performance drifts,especially when images fall into OOD category due to different imagingprotocols. Hence, we introduced a comprehensive set of computationally fastmetrics to evaluate the performance of multiple foundation models (Swin UNETR,SimMIM, iBOT, SMIT) trained with self-supervised learning (SSL). SSLpretraining was selected as this approach is applicable for large, diverse, andunlabeled image sets. All models were fine-tuned on identical datasets for lungtumor segmentation from computed tomography (CT) scans. SimMIM, iBOT, and SMITused identical architecture, pretraining, and fine-tuning datasets to assessperformance variations with the choice of pretext tasks used in SSL. Evaluationwas performed on two public lung cancer datasets (LRAD: n = 140, 5Rater: n =21) with different image acquisitions and tumor stage compared to training data(n = 317 public resource with stage III-IV lung cancers) and a publicnon-cancer dataset containing volumetric CT scans of patients with pulmonaryembolism (n = 120). All models produced similarly accurate tumor segmentationon the lung cancer testing datasets. SMIT produced a highest F1-score (LRAD:0.60, 5Rater: 0.64) and lowest entropy (LRAD: 0.06, 5Rater: 0.12), indicatinghigher tumor detection rate and confident segmentations. In the OOD dataset,SMIT misdetected least number of tumors, indicated by median volume occupancyof 5.67 cc compared to second best method SimMIM of 9.97 cc.</description><author>Aneesh Rangnekar, Nishant Nadkarni, Jue Jiang, Harini Veeraraghavan</author><pubDate>Wed, 04 Sep 2024 17:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13113v2</guid></item><item><title>LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture</title><link>http://arxiv.org/abs/2409.02889v1</link><description>Expanding the long-context capabilities of Multi-modal Large LanguageModels~(MLLMs) is crucial for video understanding, high-resolution imageunderstanding, and multi-modal agents. This involves a series of systematicoptimizations, including model architecture, data construction and trainingstrategy, particularly addressing challenges such as \textit{degradedperformance with more images} and \textit{high computational costs}. In thispaper, we adapt the model architecture to a hybrid of Mamba and Transformerblocks, approach data construction with both temporal and spatial dependenciesamong multiple images and employ a progressive training strategy. The releasedmodel \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge\textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the firsthybrid MLLM, which achieved a better balance between efficiency andeffectiveness. LongLLaVA not only achieves competitive results across variousbenchmarks, but also maintains high throughput and low memory consumption.Especially, it could process nearly a thousand images on a single A100 80GBGPU, showing promising application prospects for a wide range of tasks.</description><author>Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang</author><pubDate>Wed, 04 Sep 2024 17:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02889v1</guid></item><item><title>Kinetic Interacting Particle Langevin Monte Carlo</title><link>http://arxiv.org/abs/2407.05790v2</link><description>This paper introduces and analyses interacting underdamped Langevinalgorithms, termed Kinetic Interacting Particle Langevin Monte Carlo (KIPLMC)methods, for statistical inference in latent variable models. We propose adiffusion process that evolves jointly in the space of parameters and latentvariables and exploit the fact that the stationary distribution of thisdiffusion concentrates around the maximum marginal likelihood estimate of theparameters. We then provide two explicit discretisations of this diffusion aspractical algorithms to estimate parameters of statistical models. For eachalgorithm, we obtain nonasymptotic rates of convergence for the case where thejoint log-likelihood is strongly concave with respect to latent variables andparameters. In particular, we provide convergence analysis for the diffusiontogether with the discretisation error, providing convergence rate estimatesfor the algorithms in Wasserstein-2 distance. We achieve acceleratedconvergence rates clearly demonstrating improvement in dimension dependence,similar to the underdamped samplers. To demonstrate the utility of theintroduced methodology, we provide numerical experiments that demonstrate theeffectiveness of the proposed diffusion for statistical inference and thestability of the numerical integrators utilised for discretisation. Our settingcovers a broad number of applications, including unsupervised learning,statistical inference, and inverse problems.</description><author>Paul Felix Valsecchi Oliva, O. Deniz Akyildiz</author><pubDate>Wed, 04 Sep 2024 17:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05790v2</guid></item><item><title>The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem</title><link>http://arxiv.org/abs/2407.18322v2</link><description>Large language models (LLMs) are useful tools with the capacity forperforming specific types of knowledge work at an effective scale. However, LLMdeployments in high-risk and safety-critical domains pose unique challenges,notably the issue of ``hallucination,'' where LLMs can generate fabricatedinformation. This is particularly concerning in settings such as drug safety,where inaccuracies could lead to patient harm. To mitigate these risks, we havedeveloped and demonstrated a proof of concept suite of guardrails specificallydesigned to mitigate certain types of hallucinations and errors for drugsafety, and potentially applicable to other medical safety-critical contexts.These guardrails include mechanisms to detect anomalous documents to preventthe ingestion of inappropriate data, identify incorrect drug names or adverseevent terms, and convey uncertainty in generated content. We integrated theseguardrails with an LLM fine-tuned for a text-to-text task, which involvesconverting both structured and unstructured data within adverse event reportsinto natural language. This method was applied to translate individual casesafety reports, demonstrating effective application in a pharmacovigilanceprocessing task. Our guardrail framework offers a set of tools with broadapplicability across various domains, ensuring LLMs can be safely used inhigh-risk situations by eliminating the occurrence of key errors, including thegeneration of incorrect pharmacovigilance-related terms, thus adhering tostringent regulatory and quality standards in medical safety-criticalenvironments.</description><author>Joe B Hakim, Jeffery L Painter, Darmendra Ramcharran, Vijay Kara, Greg Powell, Paulina Sobczak, Chiho Sato, Andrew Bate, Andrew Beam</author><pubDate>Wed, 04 Sep 2024 17:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18322v2</guid></item><item><title>CanvOI, an Oncology Intelligence Foundation Model: Scaling FLOPS Differently</title><link>http://arxiv.org/abs/2409.02885v1</link><description>The rapidly evolving field of digital oncopathology faces significantchallenges, including the need to address diverse and complex clinicalquestions, often involving rare conditions, with limited availability oflabeled data. These limitations hinder the development of robust AI-driventools in the biomedical space, where accuracy in probabilistic determinationsis of utmost importance. To address this, digital pathology foundation modelshave begun to emerge, typically developed with the size and diversity of thepre-training dataset and model parameters in mind. Here, we present CanvOI, aViT-g/10-based foundation model designed to enhance the capabilities of digitalpathology by addressing these challenges through a different approach.Considering the unique nature of oncologic histopathological images and therequirements from the embeddings to provide meaningful representations forMultiple Instance Learning (MIL) downstream models, we chose to modify theinput image characteristics. By introducing larger tile sizes (380 x 380pixels) and smaller patch sizes (10 x 10 pixels), we were able to optimize themodel's performance, pushing computational resources in a new direction andachieving state-of-the-art performance on cancer-related benchmarks. CanvOIdemonstrated a 1.5-7.4% improvement in averaged AUC compared to other leadingfoundation models built for digital pathology. Moreover, our resultsdemonstrate that CanvOI significantly outperformed the other models, with theperformance gap widening substantially when trained on just 10% of the initialcohort. This work highlights an alternative approach that, if integrated withtraditional development approaches, has the potential to advance OncologyIntelligence (OI), overcome some of the current barriers and ultimately improvethe clinical outcome of cancer patients.</description><author>Jonathan Zalach, Inbal Gazy, Assaf Avinoam, Ron Sinai, Eran Shmuel, Inbar Gilboa, Christine Swisher, Naim Matasci, Reva Basho, David B. Agus</author><pubDate>Wed, 04 Sep 2024 17:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02885v1</guid></item><item><title>Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test</title><link>http://arxiv.org/abs/2409.02883v1</link><description>Drawing tests like the Rey Complex Figure Test (RCFT) are widely used toassess cognitive functions such as visuospatial skills and memory, making themvaluable tools for detecting mild cognitive impairment (MCI). Despite theirutility, existing predictive models based on these tests often suffer fromlimitations like small sample sizes and lack of external validation, whichundermine their reliability. We developed a multi-stream deep learningframework that integrates two distinct processing streams: a multi-headself-attention based spatial stream using raw RCFT images and a scoring streamemploying a previously developed automated scoring system. Our model wastrained on data from 1,740 subjects in the Korean cohort and validated on anexternal hospital dataset of 222 subjects from Korea. The proposed multi-streammodel demonstrated superior performance over baseline models (AUC = 0.872,Accuracy = 0.781) in external validation. The integration of both spatial andscoring streams enables the model to capture intricate visual details from theraw images while also incorporating structured scoring data, which togetherenhance its ability to detect subtle cognitive impairments. This dual approachnot only improves predictive accuracy but also increases the robustness of themodel, making it more reliable in diverse clinical settings. Our model haspractical implications for clinical settings, where it could serve as acost-effective tool for early MCI screening.</description><author>Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won</author><pubDate>Wed, 04 Sep 2024 17:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02883v1</guid></item><item><title>Benchmarking Spurious Bias in Few-Shot Image Classifiers</title><link>http://arxiv.org/abs/2409.02882v1</link><description>Few-shot image classifiers are designed to recognize and classify new datawith minimal supervision and limited data but often show reliance on spuriouscorrelations between classes and spurious attributes, known as spurious bias.Spurious correlations commonly hold in certain samples and few-shot classifierscan suffer from spurious bias induced from them. There is an absence of anautomatic benchmarking system to assess the robustness of few-shot classifiersagainst spurious bias. In this paper, we propose a systematic and rigorousbenchmark framework, termed FewSTAB, to fairly demonstrate and quantify varieddegrees of robustness of few-shot classifiers to spurious bias. FewSTAB createsfew-shot evaluation tasks with biased attributes so that using them forpredictions can demonstrate poor performance. To construct these tasks, wepropose attribute-based sample selection strategies based on a pre-trainedvision-language model, eliminating the need for manual dataset curation. Thisallows FewSTAB to automatically benchmark spurious bias using any existing testdata. FewSTAB offers evaluation results in a new dimension along with a newdesign guideline for building robust classifiers. Moreover, it can benchmarkspurious bias in varied degrees and enable designs for varied degrees ofrobustness. Its effectiveness is demonstrated through experiments on tenfew-shot learning methods across three datasets. We hope our framework caninspire new designs of robust few-shot classifiers. Our code is available athttps://github.com/gtzheng/FewSTAB.</description><author>Guangtao Zheng, Wenqian Ye, Aidong Zhang</author><pubDate>Wed, 04 Sep 2024 17:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02882v1</guid></item><item><title>GenoCraft: A Comprehensive, User-Friendly Web-Based Platform for High-Throughput Omics Data Analysis and Visualization</title><link>http://arxiv.org/abs/2312.14249v2</link><description>The surge in high-throughput omics data has reshaped the landscape ofbiological research, underlining the need for powerful, user-friendly dataanalysis and interpretation tools. This paper presents GenoCraft, a web-basedcomprehensive software solution designed to handle the entire pipeline of omicsdata processing. GenoCraft offers a unified platform featuring advancedbioinformatics tools, covering all aspects of omics data analysis. Itencompasses a range of functionalities, such as normalization, quality control,differential analysis, network analysis, pathway analysis, and diversevisualization techniques. This software makes state-of-the-art omics dataanalysis more accessible to a wider range of users. With GenoCraft, researchersand data scientists have access to an array of cutting-edge bioinformaticstools under a user-friendly interface, making it a valuable resource formanaging and analyzing large-scale omics data. The API with an interactive webinterface is publicly available at https://genocraft.stanford. edu/. We alsorelease all the codes in https://github.com/futianfan/GenoCraft.</description><author>Yingzhou Lu, Minjie Shen, Ling Yue, Chenhao Li, Fan Meng, Xiao Wang, David Herrington, Yue Wang, Yue Zhao, Tianfan Fu, Capucine Van Rechem</author><pubDate>Wed, 04 Sep 2024 17:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14249v2</guid></item><item><title>Configurable Foundation Models: Building LLMs from a Modular Perspective</title><link>http://arxiv.org/abs/2409.02877v1</link><description>Advancements in LLMs have recently unveiled challenges tied to computationalefficiency and continual scalability due to their requirements of hugeparameters, making the applications and evolution of these models on deviceswith limited computation resources and scenarios requiring various abilitiesincreasingly cumbersome. Inspired by modularity within the human brain, thereis a growing tendency to decompose LLMs into numerous functional modules,allowing for inference with part of modules and dynamic assembly of modules totackle complex tasks, such as mixture-of-experts. To highlight the inherentefficiency and composability of the modular approach, we coin the term brick torepresent each functional module, designating the modularized structure asconfigurable foundation models. In this paper, we offer a comprehensiveoverview and investigation of the construction, utilization, and limitation ofconfigurable foundation models. We first formalize modules into emergent bricks- functional neuron partitions that emerge during the pre-training phase, andcustomized bricks - bricks constructed via additional post-training to improvethe capabilities and knowledge of LLMs. Based on diverse functional bricks, wefurther present four brick-oriented operations: retrieval and routing, merging,updating, and growing. These operations allow for dynamic configuration of LLMsbased on instructions to handle complex tasks. To verify our perspective, weconduct an empirical analysis on widely-used LLMs. We find that the FFN layersfollow modular patterns with functional specialization of neurons andfunctional neuron partitions. Finally, we highlight several open issues anddirections for future research. Overall, this paper aims to offer a freshmodular perspective on existing LLM research and inspire the future creation ofmore efficient and scalable foundational models.</description><author>Chaojun Xiao, Zhengyan Zhang, Chenyang Song, Dazhi Jiang, Feng Yao, Xu Han, Xiaozhi Wang, Shuo Wang, Yufei Huang, Guanyu Lin, Yingfa Chen, Weilin Zhao, Yuge Tu, Zexuan Zhong, Ao Zhang, Chenglei Si, Khai Hao Moo, Chenyang Zhao, Huimin Chen, Yankai Lin, Zhiyuan Liu, Jingbo Shang, Maosong Sun</author><pubDate>Wed, 04 Sep 2024 17:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02877v1</guid></item><item><title>$μ$GUIDE: a framework for quantitative imaging via generalized uncertainty-driven inference using deep learning</title><link>http://arxiv.org/abs/2312.17293v4</link><description>This work proposes $\mu$GUIDE: a general Bayesian framework to estimateposterior distributions of tissue microstructure parameters from any givenbiophysical model or MRI signal representation, with exemplar demonstration indiffusion-weighted MRI. Harnessing a new deep learning architecture forautomatic signal feature selection combined with simulation-based inference andefficient sampling of the posterior distributions, $\mu$GUIDE bypasses the highcomputational and time cost of conventional Bayesian approaches and does notrely on acquisition constraints to define model-specific summary statistics.The obtained posterior distributions allow to highlight degeneracies present inthe model definition and quantify the uncertainty and ambiguity of theestimated parameters.</description><author>Maëliss Jallais, Marco Palombo</author><pubDate>Wed, 04 Sep 2024 16:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17293v4</guid></item><item><title>Automating Pharmacovigilance Evidence Generation: Using Large Language Models to Produce Context-Aware SQL</title><link>http://arxiv.org/abs/2406.10690v3</link><description>Objective: To enhance the efficiency and accuracy of information retrievalfrom pharmacovigilance (PV) databases by employing Large Language Models (LLMs)to convert natural language queries (NLQs) into Structured Query Language (SQL)queries, leveraging a business context document. Materials and Methods: We utilized OpenAI's GPT-4 model within aretrieval-augmented generation (RAG) framework, enriched with a businesscontext document, to transform NLQs into syntactically precise SQL queries.Each NLQ was presented to the LLM randomly and independently to preventmemorization. The study was conducted in three phases, varying querycomplexity, and assessing the LLM's performance both with and without thebusiness context document. Results: Our approach significantly improved NLQ-to-SQL accuracy, increasingfrom 8.3\% with the database schema alone to 78.3\% with the business contextdocument. This enhancement was consistent across low, medium, and highcomplexity queries, indicating the critical role of contextual knowledge inquery generation. Discussion: The integration of a business context document markedly improvedthe LLM's ability to generate accurate and contextually relevant SQL queries.Performance achieved a maximum of 85\% when high complexity queries areexcluded, suggesting promise for routine deployment. Conclusion: This study presents a novel approach to employing LLMs for safetydata retrieval and analysis, demonstrating significant advancements in querygeneration accuracy. The methodology offers a framework applicable to variousdata-intensive domains, enhancing the accessibility and efficiency ofinformation retrieval for non-technical users.</description><author>Jeffery L. Painter, Venkateswara Rao Chalamalasetti, Raymond Kassekert, Andrew Bate</author><pubDate>Wed, 04 Sep 2024 16:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10690v3</guid></item><item><title>Hybrid Imitation-Learning Motion Planner for Urban Driving</title><link>http://arxiv.org/abs/2409.02871v1</link><description>With the release of open source datasets such as nuPlan and Argoverse, theresearch around learning-based planners has spread a lot in the last years.Existing systems have shown excellent capabilities in imitating the humandriver behaviour, but they struggle to guarantee safe closed-loop driving.Conversely, optimization-based planners offer greater security in short-termplanning scenarios. To confront this challenge, in this paper we propose anovel hybrid motion planner that integrates both learning-based andoptimization-based techniques. Initially, a multilayer perceptron (MLP)generates a human-like trajectory, which is then refined by anoptimization-based component. This component not only minimizes tracking errorsbut also computes a trajectory that is both kinematically feasible andcollision-free with obstacles and road boundaries. Our model effectivelybalances safety and human-likeness, mitigating the trade-off inherent in theseobjectives. We validate our approach through simulation experiments and furtherdemonstrate its efficacy by deploying it in real-world self-driving vehicles.</description><author>Cristian Gariboldi, Matteo Corno, Beng Jin</author><pubDate>Wed, 04 Sep 2024 16:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02871v1</guid></item><item><title>Look Into the LITE in Deep Learning for Time Series Classification</title><link>http://arxiv.org/abs/2409.02869v1</link><description>Deep learning models have been shown to be a powerful solution for TimeSeries Classification (TSC). State-of-the-art architectures, while producingpromising results on the UCR and the UEA archives , present a high number oftrainable parameters. This can lead to long training with high CO2 emission,power consumption and possible increase in the number of FLoating-pointOperation Per Second (FLOPS). In this paper, we present a new architecture forTSC, the Light Inception with boosTing tEchnique (LITE) with only 2.34% of thenumber of parameters of the state-of-the-art InceptionTime model, whilepreserving performance. This architecture, with only 9, 814 trainableparameters due to the usage of DepthWise Separable Convolutions (DWSC), isboosted by three techniques: multiplexing, custom filters, and dilatedconvolution. The LITE architecture, trained on the UCR, is 2.78 times fasterthan InceptionTime and consumes 2.79 times less CO2 and power. To evaluate theperformance of the proposed architecture on multivariate time series data, weadapt LITE to handle multivariate time series, we call this version LITEMV. Tobring theory into application, we also conducted experiments using LITEMV onmultivariate time series representing human rehabilitation movements, showingthat LITEMV not only is the most efficient model but also the best performingfor this application on the Kimore dataset, a skeleton based humanrehabilitation exercises dataset. Moreover, to address the interpretability ofLITEMV, we present a study using Class Activation Maps to understand theclassification decision taken by the model during evaluation.</description><author>Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier</author><pubDate>Wed, 04 Sep 2024 16:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02869v1</guid></item><item><title>The Impact of Balancing Real and Synthetic Data on Accuracy and Fairness in Face Recognition</title><link>http://arxiv.org/abs/2409.02867v1</link><description>Over the recent years, the advancements in deep face recognition have fueledan increasing demand for large and diverse datasets. Nevertheless, theauthentic data acquired to create those datasets is typically sourced from theweb, which, in many cases, can lead to significant privacy issues due to thelack of explicit user consent. Furthermore, obtaining a demographicallybalanced, large dataset is even more difficult because of the natural imbalancein the distribution of images from different demographic groups. In this paper,we investigate the impact of demographically balanced authentic and syntheticdata, both individually and in combination, on the accuracy and fairness offace recognition models. Initially, several generative methods were used tobalance the demographic representations of the corresponding syntheticdatasets. Then a state-of-the-art face encoder was trained and evaluated using(combinations of) synthetic and authentic images. Our findings emphasized twomain points: (i) the increased effectiveness of training data generated bydiffusion-based models in enhancing accuracy, whether used alone or combinedwith subsets of authentic data, and (ii) the minimal impact of incorporatingbalanced data from pre-trained generative methods on fairness (in nearly alltested scenarios using combined datasets, fairness scores remained eitherunchanged or worsened, even when compared to unbalanced authentic datasets).Source code and data are available at \url{https://cutt.ly/AeQy1K5G} forreproducibility.</description><author>Andrea Atzori, Pietro Cosseddu, Gianni Fenu, Mirko Marras</author><pubDate>Wed, 04 Sep 2024 16:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02867v1</guid></item><item><title>Hybrid-Segmentor: A Hybrid Approach to Automated Fine-Grained Crack Segmentation in Civil Infrastructure</title><link>http://arxiv.org/abs/2409.02866v1</link><description>Detecting and segmenting cracks in infrastructure, such as roads andbuildings, is crucial for safety and cost-effective maintenance. In spite ofthe potential of deep learning, there are challenges in achieving preciseresults and handling diverse crack types. With the proposed dataset and model,we aim to enhance crack detection and infrastructure maintenance. We introduceHybrid-Segmentor, an encoder-decoder based approach that is capable ofextracting both fine-grained local and global crack features. This allows themodel to improve its generalization capabilities in distinguish various type ofshapes, surfaces and sizes of cracks. To keep the computational performanceslow for practical purposes, while maintaining the high the generalizationcapabilities of the model, we incorporate a self-attention model at the encoderlevel, while reducing the complexity of the decoder component. The proposedmodel outperforms existing benchmark models across 5 quantitative metrics(accuracy 0.971, precision 0.804, recall 0.744, F1-score 0.770, and IoU score0.630), achieving state-of-the-art status.</description><author>June Moh Goo, Xenios Milidonis, Alessandro Artusi, Jan Boehm, Carlo Ciliberto</author><pubDate>Wed, 04 Sep 2024 16:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02866v1</guid></item><item><title>Partially Observable Multi-Agent Reinforcement Learning with Information Sharing</title><link>http://arxiv.org/abs/2308.08705v3</link><description>We study provable multi-agent reinforcement learning (RL) in the generalframework of partially observable stochastic games (POSGs). To circumvent theknown hardness results and the use of computationally intractable oracles, weadvocate leveraging the potential \emph{information-sharing} among agents, acommon practice in empirical multi-agent RL, and a standard model formulti-agent control systems with communications. We first establish severalcomputational complexity results to justify the necessity ofinformation-sharing, as well as the observability assumption that has enabledquasi-efficient single-agent RL with partial observations, for efficientlysolving POSGs. {Inspired by the inefficiency of planning in the ground-truthmodel,} we then propose to further \emph{approximate} the shared commoninformation to construct an {approximate model} of the POSG, in which planningan approximate \emph{equilibrium} (in terms of solving the original POSG) canbe quasi-efficient, i.e., of quasi-polynomial-time, under the aforementionedassumptions. Furthermore, we develop a partially observable multi-agent RLalgorithm that is \emph{both} statistically and computationallyquasi-efficient. {Finally, beyond equilibrium learning, we extend ouralgorithmic framework to finding the \emph{team-optimal solution} incooperative POSGs, i.e., decentralized partially observable Markov decisionprocesses, a much more challenging goal. We establish concrete computationaland sample complexities under several common structural assumptions of themodel.} We hope our study could open up the possibilities of leveraging andeven designing different \emph{information structures}, a well-studied notionin control theory, for developing both sample- and computation-efficientpartially observable multi-agent RL.</description><author>Xiangyu Liu, Kaiqing Zhang</author><pubDate>Wed, 04 Sep 2024 16:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08705v3</guid></item><item><title>Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant</title><link>http://arxiv.org/abs/2409.02864v1</link><description>We present a prototype for a Bioinformatics Retrieval Augmentation Data(BRAD) digital assistant. BRAD integrates a suite of tools to handle a widerange of bioinformatics tasks, from code execution to online search. Wedemonstrate BRAD's capabilities through (1) improved question-and-answeringwith retrieval augmented generation (RAG), (2) BRAD's ability to run and writecomplex software pipelines, and (3) BRAD's ability to organize and distributetasks across individual and teams of agents. We use BRAD for automation ofbioinformatics workflows, performing tasks ranging from gene enrichment andsearching the archive to automatic code generation and running biomarkeridentification pipelines. BRAD is a step toward the ultimate goal to develop adigital twin of laboratories driven by self-contained loops for hypothesisgeneration and testing of digital biology experiments.</description><author>Joshua Pickard, Marc Andrew Choi, Natalie Oliven, Cooper Stansbury, Jillian Cwycyshyn, Nicholas Galioto, Alex Gorodetsky, Alvaro Velasquez, Indika Rajapakse</author><pubDate>Wed, 04 Sep 2024 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02864v1</guid></item><item><title>The Design of an LLM-powered Unstructured Analytics System</title><link>http://arxiv.org/abs/2409.00847v2</link><description>LLMs demonstrate an uncanny ability to process unstructured data, and assuch, have the potential to go beyond search and run complex, semantic analysesat scale. We describe the design of an unstructured analytics system, Aryn, andthe tenets and use cases that motivate its design. With Aryn, users can specifyqueries in natural language and the system automatically determines a semanticplan and executes it to compute an answer from a large collection ofunstructured documents using LLMs. At the core of Aryn is Sycamore, adeclarative document processing engine, built using Ray, that provides areliable distributed abstraction called DocSets. Sycamore allows users toanalyze, enrich, and transform complex documents at scale. Aryn also comprisesLuna, a query planner that translates natural language queries to Sycamorescripts, and the Aryn Partitioner, which takes raw PDFs and document images,and converts them to DocSets for downstream processing. Using Aryn, wedemonstrate a real world use case for analyzing accident reports from theNational Transportation Safety Board (NTSB), and discuss some of the majorchallenges we encountered in deploying Aryn in the wild.</description><author>Eric Anderson, Jonathan Fritz, Austin Lee, Bohou Li, Mark Lindblad, Henry Lindeman, Alex Meyer, Parth Parmar, Tanvi Ranade, Mehul A. Shah, Benjamin Sowell, Dan Tecuci, Vinayak Thapliyal, Matt Welsh</author><pubDate>Wed, 04 Sep 2024 16:39:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00847v2</guid></item><item><title>Building a Scalable, Effective, and Steerable Search and Ranking Platform</title><link>http://arxiv.org/abs/2409.02856v1</link><description>Modern e-commerce platforms offer vast product selections, making itdifficult for customers to find items that they like and that are relevant totheir current session intent. This is why it is key for e-commerce platforms tohave near real-time scalable and adaptable personalized ranking and searchsystems. While numerous methods exist in the scientific literature for buildingsuch systems, many are unsuitable for large-scale industrial use due tocomplexity and performance limitations. Consequently, industrial rankingsystems often resort to computationally efficient yet simplistic retrieval orcandidate generation approaches, which overlook near real-time andheterogeneous customer signals, which results in a less personalized andrelevant experience. Moreover, related customer experiences are served bycompletely different systems, which increases complexity, maintenance, andinconsistent experiences. In this paper, we present a personalized, adaptable near real-time rankingplatform that is reusable across various use cases, such as browsing andsearch, and that is able to cater to millions of items and customers underheavy load (thousands of requests per second). We employ transformer-basedmodels through different ranking layers which can learn complex behaviorpatterns directly from customer action sequences while being able toincorporate temporal (e.g. in-session) and contextual information. We validateour system through a series of comprehensive offline and online real-worldexperiments at a large online e-commerce platform, and we demonstrate itssuperiority when compared to existing systems, both in terms of customerexperience as well as in net revenue. Finally, we share the lessons learnedfrom building a comprehensive, modern ranking platform for use in a large-scalee-commerce environment.</description><author>Marjan Celikik, Jacek Wasilewski, Ana Peleteiro Ramallo, Alexey Kurennoy, Evgeny Labzin, Danilo Ascione, Tural Gurbanov, Géraud Le Falher, Andrii Dzhoha, Ian Harris</author><pubDate>Wed, 04 Sep 2024 16:29:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02856v1</guid></item><item><title>Domain Decomposition-based coupling of Operator Inference reduced order models via the Schwarz alternating method</title><link>http://arxiv.org/abs/2409.01433v2</link><description>This paper presents and evaluates an approach for coupling togethersubdomain-local reduced order models (ROMs) constructed via non-intrusiveoperator inference (OpInf) with each other and with subdomain-local full ordermodels (FOMs), following a domain decomposition of the spatial geometry onwhich a given partial differential equation (PDE) is posed. Joiningsubdomain-local models is accomplished using the overlapping Schwarzalternating method, a minimally-intrusive multiscale coupling technique thatworks by transforming a monolithic problem into a sequence of subdomain-localproblems, which communicate through transmission boundary conditions imposed onthe subdomain interfaces. After formulating the overlapping Schwarz alternatingmethod for OpInf ROMs, termed OpInf-Schwarz, we evaluate the method's accuracyand efficiency on several test cases involving the heat equation in two spatialdimensions. We demonstrate that the method is capable of coupling togetherarbitrary combinations of OpInf ROMs and FOMs, and that speed-ups over amonolithic FOM are possible when performing OpInf ROM coupling.</description><author>Ian Moore, Christopher Wentland, Anthony Gruber, Irina Tezaur</author><pubDate>Wed, 04 Sep 2024 16:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01433v2</guid></item><item><title>Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models</title><link>http://arxiv.org/abs/2409.02851v1</link><description>Generating lifelike 3D humans from a single RGB image remains a challengingtask in computer vision, as it requires accurate modeling of geometry,high-quality texture, and plausible unseen parts. Existing methods typicallyuse multi-view diffusion models for 3D generation, but they often faceinconsistent view issues, which hinder high-quality 3D human generation. Toaddress this, we propose Human-VDM, a novel method for generating 3D human froma single RGB image using Video Diffusion Models. Human-VDM provides temporallyconsistent views for 3D human generation using Gaussian Splatting. It consistsof three modules: a view-consistent human video diffusion module, a videoaugmentation module, and a Gaussian Splatting module. First, a single image isfed into a human video diffusion module to generate a coherent human video.Next, the video augmentation module applies super-resolution and videointerpolation to enhance the textures and geometric smoothness of the generatedvideo. Finally, the 3D Human Gaussian Splatting module learns lifelike humansunder the guidance of these high-resolution and view-consistent images.Experiments demonstrate that Human-VDM achieves high-quality 3D human from asingle image, outperforming state-of-the-art methods in both generation qualityand quantity. Project page: https://human-vdm.github.io/Human-VDM/</description><author>Zhibin Liu, Haoye Dong, Aviral Chharia, Hefeng Wu</author><pubDate>Wed, 04 Sep 2024 16:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02851v1</guid></item><item><title>Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning</title><link>http://arxiv.org/abs/2409.02850v1</link><description>The predominant method for computing confidence intervals (CI) in few-shotlearning (FSL) is based on sampling the tasks with replacement, i.e.\ allowingthe same samples to appear in multiple tasks. This makes the CI misleading inthat it takes into account the randomness of the sampler but not the dataitself. To quantify the extent of this problem, we conduct a comparativeanalysis between CIs computed with and without replacement. These reveal anotable underestimation by the predominant method. This observation calls for areevaluation of how we interpret confidence intervals and the resultingconclusions in FSL comparative studies. Our research demonstrates that the useof paired tests can partially address this issue. Additionally, we exploremethods to further reduce the (size of the) CI by strategically sampling tasksof a specific size. We also introduce a new optimized benchmark, which can beaccessed at https://github.com/RafLaf/FSL-benchmark-again</description><author>Raphael Lafargue, Luke Smith, Franck Vermet, Mathias Löwe, Ian Reid, Vincent Gripon, Jack Valmadre</author><pubDate>Wed, 04 Sep 2024 16:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02850v1</guid></item><item><title>MaDis-Stereo: Enhanced Stereo Matching via Distilled Masked Image Modeling</title><link>http://arxiv.org/abs/2409.02846v1</link><description>In stereo matching, CNNs have traditionally served as the predominantarchitectures. Although Transformer-based stereo models have been studiedrecently, their performance still lags behind CNN-based stereo models due tothe inherent data scarcity issue in the stereo matching task. In this paper, wepropose Masked Image Modeling Distilled Stereo matching model, termedMaDis-Stereo, that enhances locality inductive bias by leveraging Masked ImageModeling (MIM) in training Transformer-based stereo model. Given randomlymasked stereo images as inputs, our method attempts to conduct both imagereconstruction and depth prediction tasks. While this strategy is beneficial toresolving the data scarcity issue, the dual challenge of reconstructing maskedtokens and subsequently performing stereo matching poses significantchallenges, particularly in terms of training stability. To address this, wepropose to use an auxiliary network (teacher), updated via Exponential MovingAverage (EMA), along with the original stereo model (student), where teacherpredictions serve as pseudo supervisory signals to effectively distillknowledge into the student model. State-of-the-arts performance is achievedwith the proposed method on several stereo matching such as ETH3D and KITTI2015. Additionally, to demonstrate that our model effectively leverageslocality inductive bias, we provide the attention distance measurement.</description><author>Jihye Ahn, Hyesong Choi, Soomin Kim, Dongbo Min</author><pubDate>Wed, 04 Sep 2024 16:17:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02846v1</guid></item><item><title>SNNAX -- Spiking Neural Networks in JAX</title><link>http://arxiv.org/abs/2409.02842v1</link><description>Spiking Neural Networks (SNNs) simulators are essential tools to prototypebiologically inspired models and neuromorphic hardware architectures andpredict their performance. For such a tool, ease of use and flexibility arecritical, but so is simulation speed especially given the complexity inherentto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulatingand training such models with PyTorch-like intuitiveness and JAX-like executionspeed. SNNAX models are easily extended and customized to fit the desired modelspecifications and target neuromorphic hardware. Additionally, SNNAX offers keyfeatures for optimizing the training and deployment of SNNs such as flexibleautomatic differentiation and just-in-time compilation. We evaluate and compareSNNAX to other commonly used machine learning (ML) frameworks used forprogramming SNNs. We provide key performance metrics, best practices,documented examples for simulating SNNs in SNNAX, and implement severalbenchmarks used in the literature.</description><author>Jamie Lohoff, Jan Finkbeiner, Emre Neftci</author><pubDate>Wed, 04 Sep 2024 16:14:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02842v1</guid></item><item><title>Historical German Text Normalization Using Type- and Token-Based Language Modeling</title><link>http://arxiv.org/abs/2409.02841v1</link><description>Historic variations of spelling poses a challenge for full-text search ornatural language processing on historical digitized texts. To minimize the gapbetween the historic orthography and contemporary spelling, usually anautomatic orthographic normalization of the historical source material ispursued. This report proposes a normalization system for German literary textsfrom c. 1700-1900, trained on a parallel corpus. The proposed system makes useof a machine learning approach using Transformer language models, combining anencoder-decoder model to normalize individual word types, and a pre-trainedcausal language model to adjust these normalizations within their context. Anextensive evaluation shows that the proposed system provides state-of-the-artaccuracy, comparable with a much larger fully end-to-end sentence-basednormalization system, fine-tuning a pre-trained Transformer large languagemodel. However, the normalization of historical text remains a challenge due todifficulties for models to generalize, and the lack of extensive high-qualityparallel data.</description><author>Anton Ehrmanntraut</author><pubDate>Wed, 04 Sep 2024 16:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02841v1</guid></item><item><title>Simple and Scalable Strategies to Continually Pre-train Large Language Models</title><link>http://arxiv.org/abs/2403.08763v4</link><description>Large language models (LLMs) are routinely pre-trained on billions of tokens,only to start the process over again once new data becomes available. A muchmore efficient solution is to continually pre-train these models, savingsignificant compute compared to re-training. However, the distribution shiftinduced by new data typically results in degraded performance on previous dataor poor adaptation to the new data. In this work, we show that a simple andscalable combination of learning rate (LR) re-warming, LR re-decaying, andreplay of previous data is sufficient to match the performance of fullyre-training from scratch on all available data, as measured by the final lossand the average score on several language model (LM) evaluation benchmarks.Specifically, we show this for a weak but realistic distribution shift betweentwo commonly used LLM pre-training datasets (English$\rightarrow$English) and astronger distribution shift (English$\rightarrow$German) at the $405$Mparameter model scale with large dataset sizes (hundreds of billions oftokens). Selecting the weak but realistic shift for larger-scale experiments,we also find that our continual learning strategies match the re-trainingbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can besuccessfully updated via simple and scalable continual learning strategies,matching the re-training baseline using only a fraction of the compute.Finally, inspired by previous work, we propose alternatives to the cosinelearning rate schedule that help circumvent forgetting induced by LR re-warmingand that are not bound to a fixed token budget.</description><author>Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish</author><pubDate>Wed, 04 Sep 2024 16:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08763v4</guid></item><item><title>R2GQA: Retriever-Reader-Generator Question Answering System to Support Students Understanding Legal Regulations in Higher Education</title><link>http://arxiv.org/abs/2409.02840v1</link><description>In this article, we propose the R2GQA system, a Retriever-Reader-GeneratorQuestion Answering system, consisting of three main components: DocumentRetriever, Machine Reader, and Answer Generator. The Retriever module employsadvanced information retrieval techniques to extract the context of articlesfrom a dataset of legal regulation documents. The Machine Reader moduleutilizes state-of-the-art natural language understanding algorithms tocomprehend the retrieved documents and extract answers. Finally, the Generatormodule synthesizes the extracted answers into concise and informative responsesto questions of students regarding legal regulations. Furthermore, we built theViRHE4QA dataset in the domain of university training regulations, comprising9,758 question-answer pairs with a rigorous construction process. This is thefirst Vietnamese dataset in the higher regulations domain with various types ofanswers, both extractive and abstractive. In addition, the R2GQA system is thefirst system to offer abstractive answers in Vietnamese. This paper discussesthe design and implementation of each module within the R2GQA system on theViRHE4QA dataset, highlighting their functionalities and interactions.Furthermore, we present experimental results demonstrating the effectivenessand utility of the proposed system in supporting the comprehension of studentsof legal regulations in higher education settings. In general, the R2GQA systemand the ViRHE4QA dataset promise to contribute significantly to relatedresearch and help students navigate complex legal documents and regulations,empowering them to make informed decisions and adhere to institutional policieseffectively. Our dataset is available for research purposes.</description><author>Phuc-Tinh Pham Do, Duy-Ngoc Dinh Cao, Khanh Quoc Tran, Kiet Van Nguyen</author><pubDate>Wed, 04 Sep 2024 16:12:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02840v1</guid></item><item><title>iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation</title><link>http://arxiv.org/abs/2409.02838v1</link><description>Transfer learning based on full fine-tuning (FFT) of the pre-trained encoderand task-specific decoder becomes increasingly complex as deep models growexponentially. Parameter efficient fine-tuning (PEFT) approaches using adaptersconsisting of small learnable layers have emerged as an alternative to FFT,achieving comparable performance while maintaining high training efficiency.However, the inflexibility of the adapter with respect to input instanceslimits its capability of learning task-specific information in diversedownstream tasks. In this paper, we propose a novel PEFT approach,input-Conditioned transFormer, termed iConFormer, that leverages a dynamicadapter conditioned on the input instances. To secure flexible learning abilityon input instances in various downstream tasks, we introduce aninput-Conditioned Network (iCoN) in the dynamic adapter that enablesinstance-level feature transformation. To be specific, iCoN generateschannel-wise convolutional kernels for each feature and transform it usingadaptive convolution process to effectively capture task-specific andfine-grained details tailor to downstream tasks. Experimental resultsdemonstrate that by tuning just 1.6% to 2.8% of the Transformer backboneparameters, iConFormer achieves performance comparable to FFT in monoculardepth estimation and semantic segmentation, while outperforming it in imageclassification and instance segmentation. Also, the proposed methodconsistently outperforms recent PEFT methods for all the tasks mentioned above.</description><author>Hayeon Jo, Hyesong Choi, Minhee Cho, Dongbo Min</author><pubDate>Wed, 04 Sep 2024 16:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02838v1</guid></item><item><title>Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency Discussions by Few-Shot Learning with Large Language Models</title><link>http://arxiv.org/abs/2409.02836v1</link><description>This study performs analysis of Predictive statements, Hope speech, andRegret Detection behaviors within cryptocurrency-related discussions,leveraging advanced natural language processing techniques. We introduce anovel classification scheme named "Prediction statements," categorizingcomments into Predictive Incremental, Predictive Decremental, PredictiveNeutral, or Non-Predictive categories. Employing GPT-4o, a cutting-edge largelanguage model, we explore sentiment dynamics across five prominentcryptocurrencies: Cardano, Binance, Matic, Fantom, and Ripple. Our analysisreveals distinct patterns in predictive sentiments, with Matic demonstrating anotably higher propensity for optimistic predictions. Additionally, weinvestigate hope and regret sentiments, uncovering nuanced interplay betweenthese emotions and predictive behaviors. Despite encountering limitationsrelated to data volume and resource availability, our study reports valuablediscoveries concerning investor behavior and sentiment trends within thecryptocurrency market, informing strategic decision-making and future researchendeavors.</description><author>Moein Shahiki Tash, Zahra Ahani, Mohim Tash, Olga Kolesnikova, Grigori Sidorov</author><pubDate>Wed, 04 Sep 2024 16:02:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02836v1</guid></item><item><title>CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models</title><link>http://arxiv.org/abs/2409.02834v1</link><description>Large language models (LLMs) have obtained promising results in mathematicalreasoning, which is a foundational skill for human intelligence. Most previousstudies focus on improving and measuring the performance of LLMs based ontextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a fewresearchers have released English multimodal math datasets (e.g., MATHVISTA andMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). Inthis paper, we release a Chinese multimodal math (CMM-Math) dataset, includingbenchmark and training parts, to evaluate and enhance the mathematicalreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank,and so on) with detailed solutions across 12 grade levels from elementary tohigh school in China. Specifically, the visual context may be present in thequestions or opinions, which makes this dataset more challenging. Throughcomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Mathdataset face challenges, emphasizing the necessity for further improvements inLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) tohandle the problems with mixed input of multiple images and text segments. Wetrain our model using three stages, including foundational pre-training,foundational fine-tuning, and mathematical fine-tuning. The extensiveexperiments indicate that our model effectively improves math reasoningperformance by comparing it with the SOTA LMMs over three multimodalmathematical datasets.</description><author>Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, Liang He</author><pubDate>Wed, 04 Sep 2024 16:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02834v1</guid></item><item><title>CoLaNET -- A Spiking Neural Network with Columnar Layered Architecture for Classification</title><link>http://arxiv.org/abs/2409.01230v2</link><description>In the present paper, I describe a spiking neural network (SNN) architecturewhich, can be used in wide range of supervised learning classification tasks.It is assumed, that all participating signals (the classified objectdescription, correct class label and SNN decision) have spiking nature. Thedistinctive feature of this architecture is a combination of prototypicalnetwork structures corresponding to different classes and significantlydistinctive instances of one class (=columns) and functionally differingpopulations of neurons inside columns (=layers). The other distinctive featureis a novel combination of anti-Hebbian and dopamine-modulated plasticity. Theplasticity rules are local and do not use the backpropagation principle.Besides that, as in my previous studies, I was guided by the requirement thatthe all neuron/plasticity models should be easily implemented on modernneurochips. I illustrate the high performance of my network on the MNISTbenchmark.</description><author>Mikhail Kiselev</author><pubDate>Wed, 04 Sep 2024 15:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01230v2</guid></item><item><title>Convolutional L2LFlows: Generating Accurate Showers in Highly Granular Calorimeters Using Convolutional Normalizing Flows</title><link>http://arxiv.org/abs/2405.20407v3</link><description>In the quest to build generative surrogate models as computationallyefficient alternatives to rule-based simulations, the quality of the generatedsamples remains a crucial frontier. So far, normalizing flows have been amongthe models with the best fidelity. However, as the latent space in such modelsis required to have the same dimensionality as the data space, scaling upnormalizing flows to high dimensional datasets is not straightforward. Theprior L2LFlows approach successfully used a series of separate normalizingflows and sequence of conditioning steps to circumvent this problem. In thiswork, we extend L2LFlows to simulate showers with a 9-times larger profile inthe lateral direction. To achieve this, we introduce convolutional layers andU-Net-type connections, move from masked autoregressive flows to couplinglayers, and demonstrate the successful modelling of showers in the ILDElectromagnetic Calorimeter as well as Dataset 3 from the public CaloChallengedataset.</description><author>Thorsten Buss, Frank Gaede, Gregor Kasieczka, Claudius Krause, David Shih</author><pubDate>Wed, 04 Sep 2024 15:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20407v3</guid></item><item><title>LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models</title><link>http://arxiv.org/abs/2409.00509v2</link><description>Large language models (LLMs) face significant challenges in handlinglong-context tasks because of their limited effective context window sizeduring pretraining, which restricts their ability to generalize over extendedsequences. Meanwhile, extending the context window in LLMs throughpost-pretraining is highly resource-intensive. To address this, we introduceLongRecipe, an efficient training strategy for extending the context window ofLLMs, including impactful token analysis, position index transformation, andtraining optimization strategies. It simulates long-sequence inputs whilemaintaining training efficiency and significantly improves the model'sunderstanding of long-range dependencies. Experiments on three types of LLMsshow that LongRecipe can utilize long sequences while requiring only 30% of thetarget context window size, and reduces computational training resource over85% compared to full sequence training. Furthermore, LongRecipe also preservesthe original LLM's capabilities in general tasks. Ultimately, we can extend theeffective context window of open-source LLMs from 8k to 128k, achievingperformance close to GPT-4 with just one day of dedicated training using asingle GPU with 80G memory. Our code is released athttps://github.com/zhiyuanhubj/LongRecipe.</description><author>Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh Tuan Luu, See-Kiong Ng, Zhiwei Jiang, Bryan Hooi</author><pubDate>Wed, 04 Sep 2024 15:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00509v2</guid></item><item><title>Multi-task Learning Approach for Intracranial Hemorrhage Prognosis</title><link>http://arxiv.org/abs/2408.08784v2</link><description>Prognosis after intracranial hemorrhage (ICH) is influenced by a complexinterplay between imaging and tabular data. Rapid and reliable prognosis arecrucial for effective patient stratification and informed treatmentdecision-making. In this study, we aim to enhance image-based prognosis bylearning a robust feature representation shared between prognosis and theclinical and demographic variables most highly correlated with it. Our approachmimics clinical decision-making by reinforcing the model to learn valuableprognostic data embedded in the image. We propose a 3D multi-task image modelto predict prognosis, Glasgow Coma Scale and age, improving accuracy andinterpretability. Our method outperforms current state-of-the-art baselineimage models, and demonstrates superior performance in ICH prognosis comparedto four board-certified neuroradiologists using only CT scans as input. Wefurther validate our model with interpretability saliency maps. Code isavailable at https://github.com/MiriamCobo/MultitaskLearning_ICH_Prognosis.git.</description><author>Miriam Cobo, Amaia Pérez del Barrio, Pablo Menéndez Fernández-Miranda, Pablo Sanz Bellón, Lara Lloret Iglesias, Wilson Silva</author><pubDate>Wed, 04 Sep 2024 15:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08784v2</guid></item><item><title>Multi-Agent Reinforcement Learning from Human Feedback: Data Coverage and Algorithmic Techniques</title><link>http://arxiv.org/abs/2409.00717v2</link><description>We initiate the study of Multi-Agent Reinforcement Learning from HumanFeedback (MARLHF), exploring both theoretical foundations and empiricalvalidations. We define the task as identifying Nash equilibrium from apreference-only offline dataset in general-sum games, a problem marked by thechallenge of sparse feedback signals. Our theory establishes the uppercomplexity bounds for Nash Equilibrium in effective MARLHF, demonstrating thatsingle-policy coverage is inadequate and highlighting the importance ofunilateral dataset coverage. These theoretical insights are verified throughcomprehensive experiments. To enhance the practical performance, we furtherintroduce two algorithmic techniques. (1) We propose a Mean Squared Error (MSE)regularization along the time axis to achieve a more uniform rewarddistribution and improve reward learning outcomes. (2) We utilize imitationlearning to approximate the reference policy, ensuring stability andeffectiveness in training. Our findings underscore the multifaceted approachrequired for MARLHF, paving the way for effective preference-based multi-agentsystems.</description><author>Natalia Zhang, Xinqi Wang, Qiwen Cui, Runlong Zhou, Sham M. Kakade, Simon S. Du</author><pubDate>Wed, 04 Sep 2024 15:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00717v2</guid></item><item><title>ExpLLM: Towards Chain of Thought for Facial Expression Recognition</title><link>http://arxiv.org/abs/2409.02828v1</link><description>Facial expression recognition (FER) is a critical task in multimedia withsignificant implications across various domains. However, analyzing the causesof facial expressions is essential for accurately recognizing them. Currentapproaches, such as those based on facial action units (AUs), typically provideAU names and intensities but lack insight into the interactions andrelationships between AUs and the overall expression. In this paper, we proposea novel method called ExpLLM, which leverages large language models to generatean accurate chain of thought (CoT) for facial expression recognition.Specifically, we have designed the CoT mechanism from three key perspectives:key observations, overall emotional interpretation, and conclusion. The keyobservations describe the AU's name, intensity, and associated emotions. Theoverall emotional interpretation provides an analysis based on multiple AUs andtheir interactions, identifying the dominant emotions and their relationships.Finally, the conclusion presents the final expression label derived from thepreceding analysis. Furthermore, we also introduce the Exp-CoT Engine, designedto construct this expression CoT and generate instruction-description data fortraining our ExpLLM. Extensive experiments on the RAF-DB and AffectNet datasetsdemonstrate that ExpLLM outperforms current state-of-the-art FER methods.ExpLLM also surpasses the latest GPT-4o in expression CoT generation,particularly in recognizing micro-expressions where GPT-4o frequently fails.</description><author>Xing Lan, Jian Xue, Ji Qi, Dongmei Jiang, Ke Lu, Tat-Seng Chua</author><pubDate>Wed, 04 Sep 2024 15:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02828v1</guid></item><item><title>Revisiting Character-level Adversarial Attacks for Language Models</title><link>http://arxiv.org/abs/2405.04346v2</link><description>Adversarial attacks in Natural Language Processing apply perturbations in thecharacter or token levels. Token-level attacks, gaining prominence for theiruse of gradient-based methods, are susceptible to altering sentence semantics,leading to invalid adversarial examples. While character-level attacks easilymaintain semantics, they have received less attention as they cannot easilyadopt popular gradient-based methods, and are thought to be easy to defend.Challenging these beliefs, we introduce Charmer, an efficient query-basedadversarial attack capable of achieving high attack success rate (ASR) whilegenerating highly similar adversarial examples. Our method successfully targetsboth small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2,Charmer improves the ASR in 4.84% points and the USE similarity in 8% pointswith respect to the previous art. Our implementation is available inhttps://github.com/LIONS-EPFL/Charmer.</description><author>Elias Abad Rocamora, Yongtao Wu, Fanghui Liu, Grigorios G. Chrysos, Volkan Cevher</author><pubDate>Wed, 04 Sep 2024 15:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04346v2</guid></item><item><title>Automatic facial axes standardization of 3D fetal ultrasound images</title><link>http://arxiv.org/abs/2409.02826v1</link><description>Craniofacial anomalies indicate early developmental disturbances and areusually linked to many genetic syndromes. Early diagnosis is critical, yetultrasound (US) examinations often fail to identify these features. This studypresents an AI-driven tool to assist clinicians in standardizing fetal facialaxes/planes in 3D US, reducing sonographer workload and facilitating the facialevaluation. Our network, structured into three blocks-feature extractor,rotation and translation regression, and spatial transformer-processes threeorthogonal 2D slices to estimate the necessary transformations forstandardizing the facial planes in the 3D US. These transformations are appliedto the original 3D US using a differentiable module (the spatial transformerblock), yielding a standardized 3D US and the corresponding 2D facial standardplanes. The dataset used consists of 1180 fetal facial 3D US images acquiredbetween weeks 20 and 35 of gestation. Results show that our networkconsiderably reduces inter-observer rotation variability in the test set, witha mean geodesic angle difference of 14.12$^{\circ}$ $\pm$ 18.27$^{\circ}$ andan Euclidean angle error of 7.45$^{\circ}$ $\pm$ 14.88$^{\circ}$. Thesefindings demonstrate the network's ability to effectively standardize facialaxes, crucial for consistent fetal facial assessments. In conclusion, theproposed network demonstrates potential for improving the consistency andaccuracy of fetal facial assessments in clinical settings, facilitating earlyevaluation of craniofacial anomalies.</description><author>Antonia Alomar, Ricardo Rubio, Laura Salort, Gerard Albaiges, Antoni Payà, Gemma Piella, Federico Sukno</author><pubDate>Wed, 04 Sep 2024 15:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02826v1</guid></item><item><title>Deep Learning Meets Satellite Images -- An Evaluation on Handcrafted and Learning-based Features for Multi-date Satellite Stereo Images</title><link>http://arxiv.org/abs/2409.02825v1</link><description>A critical step in the digital surface models(DSM) generation is featurematching. Off-track (or multi-date) satellite stereo images, in particular, canchallenge the performance of feature matching due to spectral distortionsbetween images, long baseline, and wide intersection angles. Feature matchingmethods have evolved over the years from handcrafted methods (e.g., SIFT) tolearning-based methods (e.g., SuperPoint and SuperGlue). In this paper, wecompare the performance of different features, also known as feature extractionand matching methods, applied to satellite imagery. A wide range of stereopairs(~500) covering two separate study sites are used. SIFT, as a widely usedclassic feature extraction and matching algorithm, is compared with sevendeep-learning matching methods: SuperGlue, LightGlue, LoFTR, ASpanFormer, DKM,GIM-LightGlue, and GIM-DKM. Results demonstrate that traditional matchingmethods are still competitive in this age of deep learning, although forparticular scenarios learning-based methods are very promising.</description><author>Shuang Song, Luca Morelli, Xinyi Wu, Rongjun Qin, Hessah Albanwan, Fabio Remondino</author><pubDate>Wed, 04 Sep 2024 15:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02825v1</guid></item><item><title>The Future of Open Human Feedback</title><link>http://arxiv.org/abs/2408.16961v2</link><description>Human feedback on conversations with language language models (LLMs) iscentral to how these systems learn about the world, improve their capabilities,and are steered toward desirable and safe behaviors. However, this feedback ismostly collected by frontier AI labs and kept behind closed doors. In thiswork, we bring together interdisciplinary experts to assess the opportunitiesand challenges to realizing an open ecosystem of human feedback for AI. Wefirst look for successful practices in peer production, open source, andcitizen science communities. We then characterize the main challenges for openhuman feedback. For each, we survey current approaches and offerrecommendations. We end by envisioning the components needed to underpin asustainable and open human feedback ecosystem. In the center of this ecosystemare mutually beneficial feedback loops, between users and specialized models,incentivizing a diverse stakeholders community of model trainers and feedbackproviders to support a general open feedback pool.</description><author>Shachar Don-Yehiya, Ben Burtenshaw, Ramon Fernandez Astudillo, Cailean Osborne, Mimansa Jaiswal, Tzu-Sheng Kuo, Wenting Zhao, Idan Shenfeld, Andi Peng, Mikhail Yurochkin, Atoosa Kasirzadeh, Yangsibo Huang, Tatsunori Hashimoto, Yacine Jernite, Daniel Vila-Suero, Omri Abend, Jennifer Ding, Sara Hooker, Hannah Rose Kirk, Leshem Choshen</author><pubDate>Wed, 04 Sep 2024 15:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16961v2</guid></item><item><title>SDE-based Multiplicative Noise Removal</title><link>http://arxiv.org/abs/2408.10283v2</link><description>Multiplicative noise, also known as speckle or pepper noise, commonly affectsimages produced by synthetic aperture radar (SAR), lasers, or optical lenses.Unlike additive noise, which typically arises from thermal processes orexternal factors, multiplicative noise is inherent to the system, originatingfrom the fluctuation in diffuse reflections. These fluctuations result inmultiple copies of the same signal with varying magnitudes being combined.Consequently, despeckling, or removing multiplicative noise, necessitatesdifferent techniques compared to those used for additive noise removal. In this paper, we propose a novel approach using Stochastic DifferentialEquations based diffusion models to address multiplicative noise. Wedemonstrate that multiplicative noise can be effectively modeled as a GeometricBrownian Motion process in the logarithmic domain. Utilizing the Fokker-Planckequation, we derive the corresponding reverse process for image denoising. Tovalidate our method, we conduct extensive experiments on two differentdatasets, comparing our approach to both classical signal processing techniquesand contemporary CNN-based noise removal models. Our results indicate that theproposed method significantly outperforms existing methods on perception-basedmetrics such as FID and LPIPS, while maintaining competitive performance ontraditional metrics like PSNR and SSIM.</description><author>An Vuong, Thinh Nguyen</author><pubDate>Wed, 04 Sep 2024 15:36:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10283v2</guid></item><item><title>Obsidian: Cooperative State-Space Exploration for Performant Inference on Secure ML Accelerators</title><link>http://arxiv.org/abs/2409.02817v1</link><description>Trusted execution environments (TEEs) for machine learning accelerators areindispensable in secure and efficient ML inference. Optimizing workloadsthrough state-space exploration for the accelerator architectures improvesperformance and energy consumption. However, such explorations are expensiveand slow due to the large search space. Current research has to use fastanalytical models that forego critical hardware details and cross-layeropportunities unique to the hardware security primitives. While cycle-accuratemodels can theoretically reach better designs, their high runtime costrestricts them to a smaller state space. We present Obsidian, an optimization framework for finding the optimalmapping from ML kernels to a secure ML accelerator. Obsidian addresses theabove challenge by exploring the state space using analytical andcycle-accurate models cooperatively. The two main exploration componentsinclude: (1) A secure accelerator analytical model, that includes the effect ofsecure hardware while traversing the large mapping state space and produce thebest m model mappings; (2) A compiler profiling step on a cycle-accurate model,that captures runtime bottlenecks to further improve execution runtime, energyand resource utilization and find the optimal model mapping. We compare our results to a baseline secure accelerator, comprising of thestate-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11].The analytical model reduces the inference latency by 20.5% for a cloud and8.4% for an edge deployment with an energy improvement of 24% and 19%respectively. The cycle-accurate model, further reduces the latency by 9.1% fora cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%.</description><author>Sarbartha Banerjee, Shijia Wei, Prakash Ramrakhyani, Mohit Tiwari</author><pubDate>Wed, 04 Sep 2024 15:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02817v1</guid></item><item><title>LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models</title><link>http://arxiv.org/abs/2408.15778v2</link><description>Large Language Models (LLMs) have demonstrated notable capabilities acrossvarious tasks, showcasing complex problem-solving abilities. Understanding andexecuting complex rules, along with multi-step planning, are fundamental tological reasoning and critical for practical LLM agents and decision-makingsystems. However, evaluating LLMs as effective rule-based executors andplanners remains underexplored. In this paper, we introduce LogicGame, a novelbenchmark designed to evaluate the comprehensive rule understanding, execution,and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGameprovides diverse games that contain a series of rules with an initial state,requiring models to comprehend and apply predefined regulations to solveproblems. We create simulated scenarios in which models execute or planoperations to achieve specific outcomes. These game scenarios are specificallydesigned to distinguish logical reasoning from mere knowledge by relyingexclusively on predefined rules. This separation allows for a pure assessmentof rule-based reasoning capabilities. The evaluation considers not only finaloutcomes but also intermediate steps, providing a comprehensive assessment ofmodel performance. Moreover, these intermediate steps are deterministic and canbe automatically verified. LogicGame defines game scenarios with varyingdifficulty levels, from simple rule applications to complex reasoning chains,in order to offer a precise evaluation of model performance on ruleunderstanding and multi-step execution. Utilizing LogicGame, we test variousLLMs and identify notable shortcomings in their rule-based logical reasoningabilities.</description><author>Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang</author><pubDate>Wed, 04 Sep 2024 15:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15778v2</guid></item><item><title>Learning Local Pattern Modularization for Point Cloud Reconstruction from Unseen Classes</title><link>http://arxiv.org/abs/2408.14279v2</link><description>It is challenging to reconstruct 3D point clouds in unseen classes fromsingle 2D images. Instead of object-centered coordinate system, current methodsgeneralized global priors learned in seen classes to reconstruct 3D shapes fromunseen classes in viewer-centered coordinate system. However, thereconstruction accuracy and interpretability are still eager to get improved.To resolve this issue, we introduce to learn local pattern modularization forreconstructing 3D shapes in unseen classes, which achieves both goodgeneralization ability and high reconstruction accuracy. Our insight is tolearn a local prior which is class-agnostic and easy to generalize inobject-centered coordinate system. Specifically, the local prior is learned viaa process of learning and customizing local pattern modularization in seenclasses. During this process, we first learn a set of patterns in localregions, which is the basis in the object-centered coordinate system torepresent an arbitrary region on shapes across different classes. Then, wemodularize each region on an initially reconstructed shape using the learnedlocal patterns. Based on that, we customize the local pattern modularizationusing the input image by refining the reconstruction with more details. Ourmethod enables to reconstruct high fidelity point clouds from unseen classes inobject-centered coordinate system without requiring a large number of patternsor any additional information, such as segmentation supervision or cameraposes. Our experimental results under widely used benchmarks show that ourmethod achieves the state-of-the-art reconstruction accuracy for shapes fromunseen classes. The code is available at https://github.com/chenchao15/Unseen.</description><author>Chao Chen, Yu-Shen Liu, Zhizhong Han</author><pubDate>Wed, 04 Sep 2024 15:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14279v2</guid></item><item><title>MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark</title><link>http://arxiv.org/abs/2409.02813v1</link><description>This paper introduces MMMU-Pro, a robust version of the MassiveMulti-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.MMMU-Pro rigorously assesses multimodal models' true understanding andreasoning capabilities through a three-step process based on MMMU: (1)filtering out questions answerable by text-only models, (2) augmentingcandidate options, and (3) introducing a vision-only input setting wherequestions are embedded within images. This setting challenges AI to truly "see"and "read" simultaneously, testing a fundamental human cognitive skill ofseamlessly integrating visual and textual information. Results show that modelperformance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%to 26.9% across models. We explore the impact of OCR prompts and Chain ofThought (CoT) reasoning, finding that OCR prompts have minimal effect while CoTgenerally improves performance. MMMU-Pro provides a more rigorous evaluationtool, closely mimicking real-world scenarios and offering valuable directionsfor future research in multimodal AI.</description><author>Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, Graham Neubig</author><pubDate>Wed, 04 Sep 2024 15:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02813v1</guid></item><item><title>A hybrid FEM-PINN method for time-dependent partial differential equations</title><link>http://arxiv.org/abs/2409.02810v1</link><description>In this work, we present a hybrid numerical method for solving evolutionpartial differential equations (PDEs) by merging the time finite element methodwith deep neural networks. In contrast to the conventional deep learning-basedformulation where the neural network is defined on a spatiotemporal domain, ourmethodology utilizes finite element basis functions in the time direction wherethe space-dependent coefficients are defined as the output of a neural network.We then apply the Galerkin or collocation projection in the time direction toobtain a system of PDEs for the space-dependent coefficients which isapproximated in the framework of PINN. The advantages of such a hybridformulation are twofold: statistical errors are avoided for the integral in thetime direction, and the neural network's output can be regarded as a set ofreduced spatial basis functions. To further alleviate the difficulties fromhigh dimensionality and low regularity, we have developed an adaptive samplingstrategy that refines the training set. More specifically, we use an explicitdensity model to approximate the distribution induced by the PDE residual andthen augment the training set with new time-dependent random samples given bythe learned density model. The effectiveness and efficiency of our proposedmethod have been demonstrated through a series of numerical experiments.</description><author>Xiaodong Feng, Haojiong Shangguan, Tao Tang, Xiaoliang Wan, Tao Zhou</author><pubDate>Wed, 04 Sep 2024 15:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02810v1</guid></item><item><title>Towards Edge-Based Data Lake Architecture for Intelligent Transportation System</title><link>http://arxiv.org/abs/2409.02808v1</link><description>The rapid urbanization growth has underscored the need for innovativesolutions to enhance transportation efficiency and safety. IntelligentTransportation Systems (ITS) have emerged as a promising solution in thiscontext. However, analyzing and processing the massive and intricate datagenerated by ITS presents significant challenges for traditional dataprocessing systems. This work proposes an Edge-based Data Lake Architecture tointegrate and analyze the complex data from ITS efficiently. The architectureoffers scalability, fault tolerance, and performance, improving decision-makingand enhancing innovative services for a more intelligent transportationecosystem. We demonstrate the effectiveness of the architecture through ananalysis of three different use cases: (i) Vehicular Sensor Network, (ii)Mobile Network, and (iii) Driver Identification applications.</description><author>Danilo Fernandes, Douglas L. L. Moura, Gean Santos, Geymerson S. Ramos, Fabiane Queiroz, Andre L. L. Aquino</author><pubDate>Wed, 04 Sep 2024 15:25:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02808v1</guid></item><item><title>CONDA: Condensed Deep Association Learning for Co-Salient Object Detection</title><link>http://arxiv.org/abs/2409.01021v2</link><description>Inter-image association modeling is crucial for co-salient object detection.Despite satisfactory performance, previous methods still have limitations onsufficient inter-image association modeling. Because most of them focus onimage feature optimization under the guidance of heuristically calculated rawinter-image associations. They directly rely on raw associations which are notreliable in complex scenarios, and their image feature optimization approach isnot explicit for inter-image association modeling. To alleviate theselimitations, this paper proposes a deep association learning strategy thatdeploys deep networks on raw associations to explicitly transform them intodeep association features. Specifically, we first create hyperassociations tocollect dense pixel-pair-wise raw associations and then deploys deepaggregation networks on them. We design a progressive association generationmodule for this purpose with additional enhancement of the hyperassociationcalculation. More importantly, we propose a correspondence-induced associationcondensation module that introduces a pretext task, i.e. semanticcorrespondence estimation, to condense the hyperassociations for computationalburden reduction and noise elimination. We also design an object-aware cycleconsistency loss for high-quality correspondence estimations. Experimentalresults in three benchmark datasets demonstrate the remarkable effectiveness ofour proposed method with various training settings.</description><author>Long Li, Nian Liu, Dingwen Zhang, Zhongyu Li, Salman Khan, Rao Anwer, Hisham Cholakkal, Junwei Han, Fahad Shahbaz Khan</author><pubDate>Wed, 04 Sep 2024 15:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01021v2</guid></item><item><title>Boosting Certificate Robustness for Time Series Classification with Efficient Self-Ensemble</title><link>http://arxiv.org/abs/2409.02802v1</link><description>Recently, the issue of adversarial robustness in the time series domain hasgarnered significant attention. However, the available defense mechanismsremain limited, with adversarial training being the predominant approach,though it does not provide theoretical guarantees. Randomized Smoothing hasemerged as a standout method due to its ability to certify a provable lowerbound on robustness radius under $\ell_p$-ball attacks. Recognizing itssuccess, research in the time series domain has started focusing on theseaspects. However, existing research predominantly focuses on time seriesforecasting, or under the non-$\ell_p$ robustness in statistic featureaugmentation for time series classification~(TSC). Our review found thatRandomized Smoothing performs modestly in TSC, struggling to provide effectiveassurances on datasets with poor robustness. Therefore, we propose aself-ensemble method to enhance the lower bound of the probability confidenceof predicted labels by reducing the variance of classification margins, therebycertifying a larger radius. This approach also addresses the computationaloverhead issue of Deep Ensemble~(DE) while remaining competitive and, in somecases, outperforming it in terms of robustness. Both theoretical analysis andexperimental results validate the effectiveness of our method, demonstratingsuperior performance in robustness testing compared to baseline approaches.</description><author>Chang Dong, Zhengyang Li, Liangwei Zheng, Weitong Chen, Wei Emma Zhang</author><pubDate>Wed, 04 Sep 2024 15:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02802v1</guid></item><item><title>AI-generated text boundary detection with RoFT</title><link>http://arxiv.org/abs/2311.08349v3</link><description>Due to the rapid development of large language models, people increasinglyoften encounter texts that may start as written by a human but continue asmachine-generated. Detecting the boundary between human-written andmachine-generated parts of such texts is a challenging problem that has notreceived much attention in literature. We attempt to bridge this gap andexamine several ways to adapt state of the art artificial text detectionclassifiers to the boundary detection setting. We push all detectors to theirlimits, using the Real or Fake text benchmark that contains short texts onseveral topics and includes generations of various language models. We use thisdiversity to deeply examine the robustness of all detectors in cross-domain andcross-model settings to provide baselines and insights for future research. Inparticular, we find that perplexity-based approaches to boundary detection tendto be more robust to peculiarities of domain-specific data than supervisedfine-tuning of the RoBERTa model; we also find which features of the textconfuse boundary detection algorithms and negatively influence theirperformance in cross-domain settings.</description><author>Laida Kushnareva, Tatiana Gaintseva, German Magai, Serguei Barannikov, Dmitry Abulkhanov, Kristian Kuznetsov, Eduard Tulchinskii, Irina Piontkovskaya, Sergey Nikolenko</author><pubDate>Wed, 04 Sep 2024 15:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08349v3</guid></item><item><title>Privacy-aware Berrut Approximated Coded Computing for Federated Learning</title><link>http://arxiv.org/abs/2405.01704v2</link><description>Federated Learning (FL) is an interesting strategy that enables thecollaborative training of an AI model among different data owners withoutrevealing their private datasets. Even so, FL has some privacy vulnerabilitiesthat have been tried to be overcome by applying some techniques likeDifferential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-PartyComputation (SMPC). However, these techniques have some important drawbacksthat might narrow their range of application: problems to work with non-linearfunctions and to operate large matrix multiplications and high communicationand computational costs to manage semi-honest nodes. In this context, wepropose a solution to guarantee privacy in FL schemes that simultaneouslysolves the previously mentioned problems. Our proposal is based on the BerrutApproximated Coded Computing, a technique from the Coded Distributed Computingparadigm, adapted to a Secret Sharing configuration, to provide input privacyto FL in a scalable way. It can be applied for computing non-linear functionsand treats the special case of distributed matrix multiplication, a keyprimitive at the core of many automated learning tasks. Because of thesecharacteristics, it could be applied in a wide range of FL scenarios, since itis independent of the machine learning models or aggregation algorithms used inthe FL scheme. We provide analysis of the achieved privacy and complexity ofour solution and, due to the extensive numerical results performed, a goodtrade-off between privacy and precision can be observed.</description><author>Xavier Martínez Luaña, Rebeca P. Díaz Redondo, Manuel Fernández Veiga</author><pubDate>Wed, 04 Sep 2024 15:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01704v2</guid></item><item><title>Open Gaze: Open Source eye tracker for smartphone devices using Deep Learning</title><link>http://arxiv.org/abs/2308.13495v3</link><description>Eye tracking has been a pivotal tool in diverse fields such as visionresearch, language analysis, and usability assessment. The majority of priorinvestigations, however, have concentrated on expansive desktop displaysemploying specialized, costly eye tracking hardware that lacks scalability.Remarkably little insight exists into ocular movement patterns on smartphones,despite their widespread adoption and significant usage. In this manuscript, wepresent an open-source implementation of a smartphone-based gaze tracker thatemulates the methodology proposed by a GooglePaper (whose source code remainsproprietary). Our focus is on attaining accuracy comparable to that attainedthrough the GooglePaper's methodology, without the necessity for supplementaryhardware. Through the integration of machine learning techniques, we unveil anaccurate eye tracking solution that is native to smartphones. Our approachdemonstrates precision akin to the state-of-the-art mobile eye trackers, whichare characterized by a cost that is two orders of magnitude higher. Leveragingthe vast MIT GazeCapture dataset, which is available through registration onthe dataset's website, we successfully replicate crucial findings from previousstudies concerning ocular motion behavior in oculomotor tasks and saliencyanalyses during natural image observation. Furthermore, we emphasize theapplicability of smartphone-based gaze tracking in discerning readingcomprehension challenges. Our findings exhibit the inherent potential toamplify eye movement research by significant proportions, accommodatingparticipation from thousands of subjects with explicit consent. Thisscalability not only fosters advancements in vision research, but also extendsits benefits to domains such as accessibility enhancement and healthcareapplications.</description><author>Sushmanth reddy, Jyothi Swaroop Reddy</author><pubDate>Wed, 04 Sep 2024 15:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13495v3</guid></item><item><title>Towards a Unified View of Preference Learning for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2409.02795v1</link><description>Large Language Models (LLMs) exhibit remarkably powerful capabilities. One ofthe crucial factors to achieve success is aligning the LLM's output with humanpreferences. This alignment process often requires only a small amount of datato efficiently enhance the LLM's performance. While effective, research in thisarea spans multiple domains, and the methods involved are relatively complex tounderstand. The relationships between different methods have beenunder-explored, limiting the development of the preference alignment. In lightof this, we break down the existing popular alignment strategies into differentcomponents and provide a unified framework to study the current alignmentstrategies, thereby establishing connections among them. In this survey, wedecompose all the strategies in preference learning into four components:model, data, feedback, and algorithm. This unified view offers an in-depthunderstanding of existing alignment algorithms and also opens up possibilitiesto synergize the strengths of different strategies. Furthermore, we presentdetailed working examples of prevalent existing algorithms to facilitate acomprehensive understanding for the readers. Finally, based on our unifiedperspective, we explore the challenges and future research directions foraligning large language models with human preferences.</description><author>Bofei Gao, Feifan Song, Yibo Miao, Zefan Cai, Zhe Yang, Liang Chen, Helan Hu, Runxin Xu, Qingxiu Dong, Ce Zheng, Wen Xiao, Ge Zhang, Daoguang Zan, Keming Lu, Bowen Yu, Dayiheng Liu, Zeyu Cui, Jian Yang, Lei Sha, Houfeng Wang, Zhifang Sui, Peiyi Wang, Tianyu Liu, Baobao Chang</author><pubDate>Wed, 04 Sep 2024 15:11:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02795v1</guid></item><item><title>Q-Seg: Quantum Annealing-Based Unsupervised Image Segmentation</title><link>http://arxiv.org/abs/2311.12912v3</link><description>We present Q-Seg, a novel unsupervised image segmentation method based onquantum annealing, tailored for existing quantum hardware. We formulate thepixel-wise segmentation problem, which assimilates spectral and spatialinformation of the image, as a graph-cut optimization task. Our methodefficiently leverages the interconnected qubit topology of the D-Wave Advantagedevice, offering superior scalability over existing quantum approaches andoutperforming several tested state-of-the-art classical methods. Empiricalevaluations on synthetic datasets have shown that Q-Seg has better runtimeperformance than the state-of-the-art classical optimizer Gurobi. The methodhas also been tested on earth observation image segmentation, a critical areawith noisy and unreliable annotations. In the era of noisy intermediate-scalequantum, Q-Seg emerges as a reliable contender for real-world applications incomparison to advanced techniques like Segment Anything. Consequently, Q-Segoffers a promising solution using available quantum hardware, especially insituations constrained by limited labeled data and the need for efficientcomputational runtime.</description><author>Supreeth Mysore Venkatesh, Antonio Macaluso, Marlon Nuske, Matthias Klusch, Andreas Dengel</author><pubDate>Wed, 04 Sep 2024 15:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12912v3</guid></item><item><title>A Systematic Bias of Machine Learning Regression Models and Its Correction: an Application to Imaging-based Brain Age Prediction</title><link>http://arxiv.org/abs/2405.15950v2</link><description>Machine learning models for continuous outcomes often yield systematicallybiased predictions, particularly for values that largely deviate from the mean.Specifically, predictions for large-valued outcomes tend to be negativelybiased (underestimating actual values), while those for small-valued outcomesare positively biased (overestimating actual values). We refer to this linearcentral tendency warped bias as the "systematic bias of machine learningregression". In this paper, we first demonstrate that this systematicprediction bias persists across various machine learning regression models, andthen delve into its theoretical underpinnings. To address this issue, wepropose a general constrained optimization approach designed to correct thisbias and develop computationally efficient implementation algorithms.Simulation results indicate that our correction method effectively eliminatesthe bias from the predicted outcomes. We apply the proposed approach to theprediction of brain age using neuroimaging data. In comparison to competingmachine learning regression models, our method effectively addresses thelongstanding issue of "systematic bias of machine learning regression" inneuroimaging-based brain age calculation, yielding unbiased predictions ofbrain age.</description><author>Hwiyoung Lee, Shuo Chen</author><pubDate>Wed, 04 Sep 2024 15:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15950v2</guid></item><item><title>UnLearning from Experience to Avoid Spurious Correlations</title><link>http://arxiv.org/abs/2409.02792v1</link><description>While deep neural networks can achieve state-of-the-art performance in manytasks, these models are more fragile than they appear. They are prone tolearning spurious correlations in their training data, leading to surprisingfailure cases. In this paper, we propose a new approach that addresses theissue of spurious correlations: UnLearning from Experience (ULE). Our method isbased on using two classification models trained in parallel: student andteacher models. Both models receive the same batches of training data. Thestudent model is trained with no constraints and pursues the spuriouscorrelations in the data. The teacher model is trained to solve the sameclassification problem while avoiding the mistakes of the student model. Astraining is done in parallel, the better the student model learns the spuriouscorrelations, the more robust the teacher model becomes. The teacher model usesthe gradient of the student's output with respect to its input to unlearnmistakes made by the student. We show that our method is effective on theWaterbirds, CelebA, Spawrious and UrbanCars datasets.</description><author>Jeff Mitchell, Jesús Martínez del Rincón, Niall McLaughlin</author><pubDate>Wed, 04 Sep 2024 15:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02792v1</guid></item><item><title>Enhancing the vision-language foundation model with key semantic knowledge-emphasized report refinement</title><link>http://arxiv.org/abs/2401.11421v2</link><description>Recently, vision-language representation learning has made remarkableadvancements in building up medical foundation models, holding immensepotential for transforming the landscape of clinical research and medical care.The underlying hypothesis is that the rich knowledge embedded in radiologyreports can effectively assist and guide the learning process, reducing theneed for additional labels. However, these reports tend to be complex andsometimes even consist of redundant descriptions that make the representationlearning too challenging to capture the key semantic information. This paperdevelops a novel iterative vision-language representation learning framework byproposing a key semantic knowledge-emphasized report refinement method.Particularly, raw radiology reports are refined to highlight the keyinformation according to a constructed clinical dictionary and twomodel-optimized knowledge-enhancement metrics. The iterative framework isdesigned to progressively learn, starting from gaining a general understandingof the patient's condition based on raw reports and gradually refines andextracts critical information essential to the fine-grained analysis tasks. Theeffectiveness of the proposed framework is validated on various downstreammedical image analysis tasks, including disease classification,region-of-interest segmentation, and phrase grounding. Our framework surpassesseven state-of-the-art methods in both fine-tuning and zero-shot settings,demonstrating its encouraging potential for different clinical applications.</description><author>Weijian Huang, Cheng Li, Hao Yang, Jiarun Liu, Yong Liang, Hairong Zheng, Shanshan Wang</author><pubDate>Wed, 04 Sep 2024 15:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11421v2</guid></item><item><title>Governing dual-use technologies: Case studies of international security agreements and lessons for AI governance</title><link>http://arxiv.org/abs/2409.02779v1</link><description>International AI governance agreements and institutions may play an importantrole in reducing global security risks from advanced AI. To inform the designof such agreements and institutions, we conducted case studies of historicaland contemporary international security agreements. We focused specifically onthose arrangements around dual-use technologies, examining agreements innuclear security, chemical weapons, biosecurity, and export controls. For eachagreement, we examined four key areas: (a) purpose, (b) core powers, (c)governance structure, and (d) instances of non-compliance. From these casestudies, we extracted lessons for the design of international AI agreements andgovernance institutions. We discuss the importance of robust verificationmethods, strategies for balancing power between nations, mechanisms foradapting to rapid technological change, approaches to managing trade-offsbetween transparency and security, incentives for participation, and effectiveenforcement mechanisms.</description><author>Akash R. Wasil, Peter Barnett, Michael Gerovitch, Roman Hauksson, Tom Reed, Jack William Miller</author><pubDate>Wed, 04 Sep 2024 14:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02779v1</guid></item><item><title>Regularized Multi-output Gaussian Convolution Process with Domain Adaptation</title><link>http://arxiv.org/abs/2409.02778v1</link><description>Multi-output Gaussian process (MGP) has been attracting increasing attentionas a transfer learning method to model multiple outputs. Despite its highflexibility and generality, MGP still faces two critical challenges whenapplied to transfer learning. The first one is negative transfer, which occurswhen there exists no shared information among the outputs. The second challengeis the input domain inconsistency, which is commonly studied in transferlearning yet not explored in MGP. In this paper, we propose a regularized MGPmodeling framework with domain adaptation to overcome these challenges. Morespecifically, a sparse covariance matrix of MGP is proposed by usingconvolution process, where penalization terms are added to adaptively selectthe most informative outputs for knowledge transfer. To deal with the domaininconsistency, a domain adaptation method is proposed by marginalizinginconsistent features and expanding missing features to align the input domainsamong different outputs. Statistical properties of the proposed method areprovided to guarantee the performance practically and asymptotically. Theproposed framework outperforms state-of-the-art benchmarks in comprehensivesimulation studies and one real case study of a ceramic manufacturing process.The results demonstrate the effectiveness of our method in dealing with boththe negative transfer and the domain inconsistency.</description><author>Wang Xinming, Wang Chao, Song Xuan, Kirby Levi, Wu Jianguo</author><pubDate>Wed, 04 Sep 2024 14:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02778v1</guid></item><item><title>Pre-processing and Compression: Understanding Hidden Representation Refinement Across Imaging Domains via Intrinsic Dimension</title><link>http://arxiv.org/abs/2408.08381v2</link><description>In recent years, there has been interest in how geometric properties such asintrinsic dimension (ID) of a neural network's hidden representations changethrough its layers, and how such properties are predictive of important modelbehavior such as generalization ability. However, evidence has begun to emergethat such behavior can change significantly depending on the domain of thenetwork's training data, such as natural versus medical images. Here, wefurther this inquiry by exploring how the ID of a network's learnedrepresentations changes through its layers, in essence, characterizing how thenetwork successively refines the information content of input data to be usedfor predictions. Analyzing eleven natural and medical image datasets across sixnetwork architectures, we find that how ID changes through the network differsnoticeably between natural and medical image models. Specifically, medicalimage models peak in representation ID earlier in the network, implying adifference in the image features and their abstractness that are typically usedfor downstream tasks in these domains. Additionally, we discover a strongcorrelation of this peak representation ID with the ID of the data in its inputspace, implying that the intrinsic information content of a model's learnedrepresentations is guided by that of the data it was trained on. Overall, ourfindings emphasize notable discrepancies in network behavior between naturaland non-natural imaging domains regarding hidden representation informationcontent, and provide further insights into how a network's learned features areshaped by its training data.</description><author>Nicholas Konz, Maciej A. Mazurowski</author><pubDate>Wed, 04 Sep 2024 14:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08381v2</guid></item><item><title>Unifying Causal Representation Learning with the Invariance Principle</title><link>http://arxiv.org/abs/2409.02772v1</link><description>Causal representation learning aims at recovering latent causal variablesfrom high-dimensional observations to solve causal downstream tasks, such aspredicting the effect of new interventions or more robust classification. Aplethora of methods have been developed, each tackling carefully craftedproblem settings that lead to different types of identifiability. The folkloreis that these different settings are important, as they are often linked todifferent rungs of Pearl's causal hierarchy, although not all neatly fit. Ourmain contribution is to show that many existing causal representation learningapproaches methodologically align the representation to known data symmetries.Identification of the variables is guided by equivalence classes acrossdifferent data pockets that are not necessarily causal. This result suggestsimportant implications, allowing us to unify many existing approaches in asingle method that can mix and match different assumptions, includingnon-causal ones, based on the invariances relevant to our application. It alsosignificantly benefits applicability, which we demonstrate by improvingtreatment effect estimation on real-world high-dimensional ecological data.Overall, this paper clarifies the role of causality assumptions in thediscovery of causal variables and shifts the focus to preserving datasymmetries.</description><author>Dingling Yao, Dario Rancati, Riccardo Cadei, Marco Fumero, Francesco Locatello</author><pubDate>Wed, 04 Sep 2024 14:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02772v1</guid></item><item><title>Validation of musculoskeletal segmentation model with uncertainty estimation for bone and muscle assessment in hip-to-knee clinical CT images</title><link>http://arxiv.org/abs/2409.02770v1</link><description>Deep learning-based image segmentation has allowed for the fully automated,accurate, and rapid analysis of musculoskeletal (MSK) structures from medicalimages. However, current approaches were either applied only to 2Dcross-sectional images, addressed few structures, or were validated on smalldatasets, which limit the application in large-scale databases. This studyaimed to validate an improved deep learning model for volumetric MSKsegmentation of the hip and thigh with uncertainty estimation from clinicalcomputed tomography (CT) images. Databases of CT images from multiplemanufacturers/scanners, disease status, and patient positioning were used. Thesegmentation accuracy, and accuracy in estimating the structures volume anddensity, i.e., mean HU, were evaluated. An approach for segmentation failuredetection based on predictive uncertainty was also investigated. The model hasshown an overall improvement with respect to all segmentation accuracy andstructure volume/density evaluation metrics. The predictive uncertainty yieldedlarge areas under the receiver operating characteristic (AUROC) curves(AUROCs&gt;=.95) in detecting inaccurate and failed segmentations. The highsegmentation and muscle volume/density estimation accuracy, along with the highaccuracy in failure detection based on the predictive uncertainty, exhibitedthe model's reliability for analyzing individual MSK structures in large-scaleCT databases.</description><author>Mazen Soufi, Yoshito Otake, Makoto Iwasa, Keisuke Uemura, Tomoki Hakotani, Masahiro Hashimoto, Yoshitake Yamada, Minoru Yamada, Yoichi Yokoyama, Masahiro Jinzaki, Suzushi Kusano, Masaki Takao, Seiji Okada, Nobuhiko Sugano, Yoshinobu Sato</author><pubDate>Wed, 04 Sep 2024 14:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02770v1</guid></item><item><title>Energy-Efficient Channel Decoding for Wireless Federated Learning: Convergence Analysis and Adaptive Design</title><link>http://arxiv.org/abs/2407.13703v3</link><description>One of the most critical challenges for deploying distributed learningsolutions, such as federated learning (FL), in wireless networks is the limitedbattery capacity of mobile clients. While it is a common belief that the majorenergy consumption of mobile clients comes from the uplink data transmission,this paper presents a novel finding, namely channel decoding also contributessignificantly to the overall energy consumption of mobile clients in FL.Motivated by this new observation, we propose an energy-efficient adaptivechannel decoding scheme that leverages the intrinsic robustness of FL to modelerrors. In particular, the robustness is exploited to reduce the energyconsumption of channel decoders at mobile clients by adaptively adjusting thenumber of decoding iterations. We theoretically prove that wireless FL withcommunication errors can converge at the same rate as the case with error-freecommunication provided the bit error rate (BER) is properly constrained. Anadaptive channel decoding scheme is then proposed to improve the energyefficiency of wireless FL systems. Experimental results demonstrate that theproposed method maintains the same learning accuracy while reducing the channeldecoding energy consumption by ~20% when compared to an existing approach.</description><author>Linping Qu, Yuyi Mao, Shenghui Song, Chi-Ying Tsui</author><pubDate>Wed, 04 Sep 2024 14:41:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13703v3</guid></item><item><title>Negation Blindness in Large Language Models: Unveiling the NO Syndrome in Image Generation</title><link>http://arxiv.org/abs/2409.00105v2</link><description>Foundational Large Language Models (LLMs) have changed the way we perceivetechnology. They have been shown to excel in tasks ranging from poem writingand coding to essay generation and puzzle solving. With the incorporation ofimage generation capability, they have become more comprehensive and versatileAI tools. At the same time, researchers are striving to identify thelimitations of these tools to improve them further. Currently identified flawsinclude hallucination, biases, and bypassing restricted commands to generateharmful content. In the present work, we have identified a fundamentallimitation related to the image generation ability of LLMs, and termed it TheNO Syndrome. This negation blindness refers to LLMs inability to correctlycomprehend NO related natural language prompts to generate the desired images.Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were foundto be suffering from this syndrome. To demonstrate the generalization of thislimitation, we carried out simulation experiments and conducted entropy-basedand benchmark statistical analysis tests on various LLMs in multiple languages,including English, Hindi, and French. We conclude that the NO syndrome is asignificant flaw in current LLMs that needs to be addressed. A related findingof this study showed a consistent discrepancy between image and textualresponses as a result of this NO syndrome. We posit that the introduction of anegation context-aware reinforcement learning based feedback loop between theLLMs textual response and generated image could help ensure the generated textis based on both the LLMs correct contextual understanding of the negationquery and the generated visual output.</description><author>Mohammad Nadeem, Shahab Saquib Sohail, Erik Cambria, Björn W. Schuller, Amir Hussain</author><pubDate>Wed, 04 Sep 2024 14:40:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00105v2</guid></item><item><title>CHOTA: A Higher Order Accuracy Metric for Cell Tracking</title><link>http://arxiv.org/abs/2408.11571v2</link><description>The evaluation of cell tracking results steers the development of trackingmethods, significantly impacting biomedical research. This is quantitativelyachieved by means of evaluation metrics. Unfortunately, current metrics favorlocal correctness and weakly reward global coherence, impeding high-levelbiological analysis. To also foster global coherence, we propose the CHOTAmetric (Cell-specific Higher Order Tracking Accuracy) which unifies theevaluation of all relevant aspects of cell tracking: cell detections and localassociations, global coherence, and lineage tracking. We achieve this byintroducing a new definition of the term 'trajectory' that includes the entirecell lineage and by including this into the well-established HOTA metric fromgeneral multiple object tracking. Furthermore, we provide a detailed survey ofcontemporary cell tracking metrics to compare our novel CHOTA metric and toshow its advantages. All metrics are extensively evaluated on state-of-the-artreal-data cell tracking results and synthetic results that simulate specifictracking errors. We show that CHOTA is sensitive to all tracking errors andgives a good indication of the biologically relevant capability of a method toreconstruct the full lineage of cells. It introduces a robust and comprehensivealternative to the currently used metrics in cell tracking. Python code isavailable at https://github.com/CellTrackingChallenge/py-ctcmetrics .</description><author>Timo Kaiser, Vladimir Ulman, Bodo Rosenhahn</author><pubDate>Wed, 04 Sep 2024 14:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11571v2</guid></item><item><title>An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting</title><link>http://arxiv.org/abs/2409.02760v1</link><description>This paper introduces a novel incremental preference elicitation-basedapproach to learning potentially non-monotonic preferences in multi-criteriasorting (MCS) problems, enabling decision makers to progressively provideassignment example preference information. Specifically, we first construct amax-margin optimization-based model to model potentially non-monotonicpreferences and inconsistent assignment example preference information in eachiteration of the incremental preference elicitation process. Using the optimalobjective function value of the max-margin optimization-based model, we deviseinformation amount measurement methods and question selection strategies topinpoint the most informative alternative in each iteration within theframework of uncertainty sampling in active learning. Once the terminationcriterion is satisfied, the sorting result for non-reference alternatives canbe determined through the use of two optimization models, i.e., the max-marginoptimization-based model and the complexity controlling optimization model.Subsequently, two incremental preference elicitation-based algorithms aredeveloped to learn potentially non-monotonic preferences, considering differenttermination criteria. Ultimately, we apply the proposed approach to a creditrating problem to elucidate the detailed implementation steps, and performcomputational experiments on both artificial and real-world data sets tocompare the proposed question selection strategies with several benchmarkstrategies.</description><author>Zhuolin Li, Zhen Zhang, Witold Pedrycz</author><pubDate>Wed, 04 Sep 2024 14:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02760v1</guid></item><item><title>A Comparative Study of Pre-training and Self-training</title><link>http://arxiv.org/abs/2409.02751v1</link><description>Pre-training and self-training are two approaches to semi-supervisedlearning. The comparison between pre-training and self-training has beenexplored. However, the previous works led to confusing findings: self-trainingoutperforms pre-training experienced on some tasks in computer vision, andcontrarily, pre-training outperforms self-training experienced on some tasks innatural language processing, under certain conditions of incomparable settings.We propose, comparatively and exhaustively, an ensemble method to empiricalstudy all feasible training paradigms combining pre-training, self-training,and fine-tuning within consistent foundational settings comparable to dataaugmentation. We conduct experiments on six datasets, four data augmentation,and imbalanced data for sentiment analysis and natural language inferencetasks. Our findings confirm that the pre-training and fine-tuning paradigmyields the best overall performances. Moreover, self-training offers noadditional benefits when combined with semi-supervised pre-training.</description><author>Yiheng Wang, Jiayu Lin, Zuoquan Lin</author><pubDate>Wed, 04 Sep 2024 14:30:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02751v1</guid></item><item><title>Tractable Offline Learning of Regular Decision Processes</title><link>http://arxiv.org/abs/2409.02747v1</link><description>This work studies offline Reinforcement Learning (RL) in a class ofnon-Markovian environments called Regular Decision Processes (RDPs). In RDPs,the unknown dependency of future observations and rewards from the pastinteractions can be captured by some hidden finite-state automaton. For thisreason, many RDP algorithms first reconstruct this unknown dependency usingautomata learning techniques. In this paper, we show that it is possible toovercome two strong limitations of previous offline RL algorithms for RDPs,notably RegORL. This can be accomplished via the introduction of two originaltechniques: the development of a new pseudometric based on formal languages,which removes a problematic dependency on$L_\infty^\mathsf{p}$-distinguishability parameters, and the adoption ofCount-Min-Sketch (CMS), instead of naive counting. The former reduces thenumber of samples required in environments that are characterized by a lowcomplexity in language-theoretic terms. The latter alleviates the memoryrequirements for long planning horizons. We derive the PAC sample complexitybounds associated to each of these techniques, and we validate the approachexperimentally.</description><author>Ahana Deb, Roberto Cipollone, Anders Jonsson, Alessandro Ronca, Mohammad Sadegh Talebi</author><pubDate>Wed, 04 Sep 2024 14:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02747v1</guid></item><item><title>Different Victims, Same Layout: Email Visual Similarity Detection for Enhanced Email Protection</title><link>http://arxiv.org/abs/2408.16945v3</link><description>In the pursuit of an effective spam detection system, the focus has oftenbeen on identifying known spam patterns either through rule-based detectionsystems or machine learning (ML) solutions that rely on keywords. However, bothsystems are susceptible to evasion techniques and zero-day attacks that can beachieved at low cost. Therefore, an email that bypassed the defense system oncecan do it again in the following days, even though rules are updated or the MLmodels are retrained. The recurrence of failures to detect emails that exhibitlayout similarities to previously undetected spam is concerning for customersand can erode their trust in a company. Our observations show that threatactors reuse email kits extensively and can bypass detection with littleeffort, for example, by making changes to the content of emails. In this work,we propose an email visual similarity detection approach, named Pisco, toimprove the detection capabilities of an email threat defense system. We applyour proof of concept to some real-world samples received from differentsources. Our results show that email kits are being reused extensively andvisually similar emails are sent to our customers at various time intervals.Therefore, this method could be very helpful in situations where detectionengines that rely on textual features and keywords are bypassed, an occurrenceour observations show happens frequently.</description><author>Sachin Shukla, Omid Mirzaei</author><pubDate>Wed, 04 Sep 2024 14:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16945v3</guid></item><item><title>Convolutional Neural Networks for Automated Cellular Automaton Classification</title><link>http://arxiv.org/abs/2409.02740v1</link><description>The emergent dynamics in spacetime diagrams of cellular automata (CAs) isoften organised by means of a number of behavioural classes. Whilstclassification of elementary CAs is feasible and well-studied, non-elementaryCAs are generally too diverse and numerous to exhaustively classify manually.In this chapter we treat the spacetime diagram as a digital image, andimplement simple computer vision techniques to perform an automatedclassification of elementary cellular automata into the five Li-Packardclasses. In particular, we present a supervised learning task to aconvolutional neural network, in such a way that it may be generalised tonon-elementary CAs. If we want to do so, we must divert the algorithm's focusaway from the underlying 'microscopic' local updates. We first show thatpreviously developed deep learning approaches have in fact been trained toidentify the local update rule, rather than directly focus on the mesoscopicpatterns that are associated with the particular behavioural classes. By meansof a well-argued neural network design, as well as a number of dataaugmentation techniques, we then present a convolutional neural network thatperforms nearly perfectly at identifying the behavioural class, withoutnecessarily first identifying the underlying microscopic dynamics.</description><author>Michiel Rollier, Aisling J. Daly, Jan M. Baetens</author><pubDate>Wed, 04 Sep 2024 14:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02740v1</guid></item><item><title>Multi-Modal Experience Inspired AI Creation</title><link>http://arxiv.org/abs/2209.02427v2</link><description>AI creation, such as poem or lyrics generation, has attracted increasingattention from both industry and academic communities, with many promisingmodels proposed in the past few years. Existing methods usually estimate theoutputs based on single and independent visual or textual information. However,in reality, humans usually make creations according to their experiences, whichmay involve different modalities and be sequentially correlated. To model suchhuman capabilities, in this paper, we define and solve a novel AI creationproblem based on human experiences. More specifically, we study how to generatetexts based on sequential multi-modal information. Compared with the previousworks, this task is much more difficult because the designed model has to wellunderstand and adapt the semantics among different modalities and effectivelyconvert them into the output in a sequential manner. To alleviate thesedifficulties, we firstly design a multi-channel sequence-to-sequencearchitecture equipped with a multi-modal attention network. For more effectiveoptimization, we then propose a curriculum negative sampling strategy tailoredfor the sequential inputs. To benchmark this problem and demonstrate theeffectiveness of our model, we manually labeled a new multi-modal experiencedataset. With this dataset, we conduct extensive experiments by comparing ourmodel with a series of representative baselines, where we can demonstratesignificant improvements in our model based on both automatic andhuman-centered metrics. The code and data are available at:\url{https://github.com/Aman-4-Real/MMTG}.</description><author>Qian Cao, Xu Chen, Ruihua Song, Hao Jiang, Guang Yang, Zhao Cao</author><pubDate>Wed, 04 Sep 2024 14:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.02427v2</guid></item><item><title>Fast and interpretable Support Vector Classification based on the truncated ANOVA decomposition</title><link>http://arxiv.org/abs/2402.02438v2</link><description>Support Vector Machines (SVMs) are an important tool for performingclassification on scattered data, where one usually has to deal with many datapoints in high-dimensional spaces. We propose solving SVMs in primal form usingfeature maps based on trigonometric functions or wavelets. In small dimensionalsettings the Fast Fourier Transform (FFT) and related methods are a powerfultool in order to deal with the considered basis functions. For growingdimensions the classical FFT-based methods become inefficient due to the curseof dimensionality. Therefore, we restrict ourselves to multivariate basisfunctions, each of which only depends on a small number of dimensions. This ismotivated by the well-known sparsity of effects and recent results regardingthe reconstruction of functions from scattered data in terms of truncatedanalysis of variance (ANOVA) decompositions, which makes the resulting modeleven interpretable in terms of importance of the features as well as theircouplings. The usage of small superposition dimensions has the consequence thatthe computational effort no longer grows exponentially but only polynomiallywith respect to the dimension. In order to enforce sparsity regarding the basiscoefficients, we use the frequently applied $\ell_2$-norm and, in addition,$\ell_1$-norm regularization. The found classifying function, which is thelinear combination of basis functions, and its variance can then be analyzed interms of the classical ANOVA decomposition of functions. Based on numericalexamples we show that we are able to recover the signum of a function thatperfectly fits our model assumptions. Furthermore, we perform classification ondifferent artificial and real-world data sets. We obtain better results with$\ell_1$-norm regularization, both in terms of accuracy and clarity ofinterpretability.</description><author>Kseniya Akhalaya, Franziska Nestler, Daniel Potts</author><pubDate>Wed, 04 Sep 2024 14:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02438v2</guid></item><item><title>The future of cosmological likelihood-based inference: accelerated high-dimensional parameter estimation and model comparison</title><link>http://arxiv.org/abs/2405.12965v2</link><description>We advocate for a new paradigm of cosmological likelihood-based inference,leveraging recent developments in machine learning and its underlyingtechnology, to accelerate Bayesian inference in high-dimensional settings.Specifically, we combine (i) emulation, where a machine learning model istrained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii)differentiable and probabilistic programming, e.g. JAX and NumPyro,respectively; (iii) scalable Markov chain Monte Carlo (MCMC) samplingtechniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv)decoupled and scalable Bayesian model selection techniques that compute theBayesian evidence purely from posterior samples, e.g. the learned harmonic meanimplemented in harmonic. This paradigm allows us to carry out a completeBayesian analysis, including both parameter estimation and model selection, ina fraction of the time of traditional approaches. First, we demonstrate theapplication of this paradigm on a simulated cosmic shear analysis for a StageIV survey in 37- and 39-dimensional parameter spaces, comparing $\Lambda$CDMand a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contoursand evidence estimates that are in excellent agreement with those computed bythe traditional nested sampling approach while reducing the computational costfrom 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a jointanalysis between three simulated next-generation surveys, each performing a3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces.Standard nested sampling techniques are simply unlikely to be feasible in thishigh-dimensional setting, requiring a projected 12 years of compute time on 48CPU cores; on the other hand, the proposed approach only requires 8 days ofcompute time on 24 GPUs. All packages used in our analyses are publiclyavailable.</description><author>Davide Piras, Alicja Polanska, Alessio Spurio Mancini, Matthew A. Price, Jason D. McEwen</author><pubDate>Wed, 04 Sep 2024 14:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12965v2</guid></item><item><title>Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms</title><link>http://arxiv.org/abs/2407.04183v2</link><description>Large language models (LLMs) are trained on broad corpora and then used incommunities with specialized norms. Is providing LLMs with community rulesenough for models to follow these norms? We evaluate LLMs' capacity to detect(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia'sNeutral Point of View (NPOV) policy. LLMs struggled with bias detection,achieving only 64% accuracy on a balanced dataset. Models exhibited contrastingbiases (some under- and others over-predicted bias), suggesting distinct priorsabout neutrality. LLMs performed better at generation, removing 79% of wordsremoved by Wikipedia editors. However, LLMs made additional changes beyondWikipedia editors' simpler neutralizations, resulting in high-recall butlow-precision editing. Interestingly, crowdworkers rated AI rewrites as moreneutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitativeanalysis found LLMs sometimes applied NPOV more comprehensively than Wikipediaeditors but often made extraneous non-NPOV-related changes (such as grammar).LLMs may apply rules in ways that resonate with the public but diverge fromcommunity experts. While potentially effective for generation, LLMs may reduceeditor agency and increase moderation workload (e.g., verifying additions).Even when rules are easy to articulate, having LLMs apply them like communitymembers may still be difficult.</description><author>Joshua Ashkinaze, Ruijia Guan, Laura Kurek, Eytan Adar, Ceren Budak, Eric Gilbert</author><pubDate>Wed, 04 Sep 2024 14:07:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04183v2</guid></item><item><title>Complete and Efficient Covariants for 3D Point Configurations with Application to Learning Molecular Quantum Properties</title><link>http://arxiv.org/abs/2409.02730v1</link><description>When modeling physical properties of molecules with machine learning, it isdesirable to incorporate $SO(3)$-covariance. While such models based on lowbody order features are not complete, we formulate and prove generalcompleteness properties for higher order methods, and show that $6k-5$ of thesefeatures are enough for up to $k$ atoms. We also find that the Clebsch--Gordanoperations commonly used in these methods can be replaced by matrixmultiplications without sacrificing completeness, lowering the scaling from$O(l^6)$ to $O(l^3)$ in the degree of the features. We apply this to quantumchemistry, but the proposed methods are generally applicable for problemsinvolving 3D point configurations.</description><author>Hartmut Maennel, Oliver T. Unke, Klaus-Robert Müller</author><pubDate>Wed, 04 Sep 2024 14:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02730v1</guid></item><item><title>Task-Oriented Communication for Graph Data: A Graph Information Bottleneck Approach</title><link>http://arxiv.org/abs/2409.02728v1</link><description>Graph data, essential in fields like knowledge representation and socialnetworks, often involves large networks with many nodes and edges. Transmittingthese graphs can be highly inefficient due to their size and redundancy forspecific tasks. This paper introduces a method to extract a smaller,task-focused subgraph that maintains key information while reducingcommunication overhead. Our approach utilizes graph neural networks (GNNs) andthe graph information bottleneck (GIB) principle to create a compact,informative, and robust graph representation suitable for transmission. Thechallenge lies in the irregular structure of graph data, making GIBoptimization complex. We address this by deriving a tractable variational upperbound for the objective function. Additionally, we propose the VQ-GIBmechanism, integrating vector quantization (VQ) to convert subgraphrepresentations into a discrete codebook sequence, compatible with existingdigital communication systems. Our experiments show that this GIB-based methodsignificantly lowers communication costs while preserving essentialtask-related information. The approach demonstrates robust performance acrossvarious communication channels, suitable for both continuous and discretesystems.</description><author>Shujing Li, Yanhu Wang, Shuaishuai Guo, Chenyuan Feng</author><pubDate>Wed, 04 Sep 2024 14:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02728v1</guid></item><item><title>Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models?</title><link>http://arxiv.org/abs/2409.02727v1</link><description>The significant advancements of Large Language Models (LLMs) in generativetasks have led to a growing body of work exploring LLM-based embedding models.While these models, employing different pooling and attention strategies, haveachieved state-of-the-art performance on public embedding benchmarks, questionsstill arise about what constitutes an effective design for LLM-based embeddingmodels. However, these models are often trained on different datasets, usingdifferent LLM base models or training settings. Moreover, evaluations on publicembedding benchmarks often fail to report statistical significance, making itdifficult to determine which designs truly contribute to final performance.This complicates the process for practitioners seeking optimal training recipesfor LLM-based embedding models. In this study, we conduct a large-scaleexperiment by training a series of LLM-based embedding models using the sametraining data and base model but differing in their pooling and attentionstrategies. The results show that there is no one-size-fits-all solution: whilebidirectional attention and an additional trainable pooling layer outperform intext similarity and information retrieval tasks, they do not significantlysurpass simpler designs like EOS-last token pooling and default causalattention in clustering and classification tasks. Furthermore, we propose a newpooling strategy, Multi-Layers Trainable Pooling, which transforms the outputsof all hidden layers, rather than just the last layer, using a cross-attentionnetwork. This method proves to be statistically superior in text similarity andretrieval tasks compared to existing pooling methods. Overall, this paper shedslight on effective training strategies for LLM-based embedding models.</description><author>Yixuan Tang, Yi Yang</author><pubDate>Wed, 04 Sep 2024 14:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02727v1</guid></item><item><title>Multimodal Recommender Systems: A Survey</title><link>http://arxiv.org/abs/2302.03883v2</link><description>The recommender system (RS) has been an integral toolkit of online services.They are equipped with various deep learning techniques to model userpreference based on identifier and attribute information. With the emergence ofmultimedia services, such as short videos, news and etc., understanding thesecontents while recommending becomes critical. Besides, multimodal features arealso helpful in alleviating the problem of data sparsity in RS. Thus,Multimodal Recommender System (MRS) has attracted much attention from bothacademia and industry recently. In this paper, we will give a comprehensivesurvey of the MRS models, mainly from technical views. First, we conclude thegeneral procedures and major challenges for MRS. Then, we introduce theexisting MRS models according to four categories, i.e., Modality Encoder,Feature Interaction, Feature Enhancement and Model Optimization. Besides, tomake it convenient for those who want to research this field, we also summarizethe dataset and code resources. Finally, we discuss some promising futuredirections of MRS and conclude this paper. To access more details of thesurveyed papers, such as implementation code, we open source a repository.</description><author>Qidong Liu, Jiaxi Hu, Yutian Xiao, Xiangyu Zhao, Jingtong Gao, Wanyu Wang, Qing Li, Jiliang Tang</author><pubDate>Wed, 04 Sep 2024 14:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03883v2</guid></item><item><title>Pre-training data selection for biomedical domain adaptation using journal impact metrics</title><link>http://arxiv.org/abs/2409.02725v1</link><description>Domain adaptation is a widely used method in natural language processing(NLP) to improve the performance of a language model within a specific domain.This method is particularly common in the biomedical domain, which sees regularpublication of numerous scientific articles. PubMed, a significant corpus oftext, is frequently used in the biomedical domain. The primary objective ofthis study is to explore whether refining a pre-training dataset using specificquality metrics for scientific papers can enhance the performance of theresulting model. To accomplish this, we employ two straightforward journalimpact metrics and conduct experiments by continually pre-training BERT onvarious subsets of the complete PubMed training set, we then evaluate theresulting models on biomedical language understanding tasks from the BLURBbenchmark. Our results show that pruning using journal impact metrics is notefficient. But we also show that pre-training using fewer abstracts (but withthe same number of training steps) does not necessarily decrease the resultingmodel's performance.</description><author>Mathieu Laï-king, Patrick Paroubek</author><pubDate>Wed, 04 Sep 2024 13:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02725v1</guid></item><item><title>Alignment-Aware Model Extraction Attacks on Large Language Models</title><link>http://arxiv.org/abs/2409.02718v1</link><description>Model extraction attacks (MEAs) on large language models (LLMs) have receivedincreasing research attention lately. Existing attack methods on LLMs inheritthe extraction strategies from those designed for deep neural networks (DNNs)yet neglect the inconsistency of training tasks between MEA and LLMs'alignments. As such, they result in poor attack performances. To tackle thisissue, we present Locality Reinforced Distillation (LoRD), a novel modelextraction attack algorithm specifically for LLMs. In particular, we design apolicy-gradient-style training task, which utilizes victim models' responses asa signal to guide the crafting of preference for the local model. Theoreticalanalysis has shown that i) LoRD's convergence procedure in MEAs is consistentwith the alignments of LLMs, and ii) LoRD can reduce query complexity whilemitigating watermark protection through exploration-based stealing. Extensiveexperiments on domain-specific extractions demonstrate the superiority of ourmethod by examining the extraction of various state-of-the-art commercial LLMs.</description><author>Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu</author><pubDate>Wed, 04 Sep 2024 13:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02718v1</guid></item><item><title>A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations</title><link>http://arxiv.org/abs/2409.02712v1</link><description>Machine translation in low-resource language pairs faces significantchallenges due to the scarcity of parallel corpora and linguistic resources.This study focuses on the case of English-Marathi language pairs, whereexisting datasets are notably noisy, impeding the performance of machinetranslation models. To mitigate the impact of data quality issues, we propose adata filtering approach based on cross-lingual sentence representations. Ourmethodology leverages a multilingual SBERT model to filter out problematictranslations in the training data. Specifically, we employ an IndicSBERTsimilarity model to assess the semantic equivalence between original andtranslated sentences, allowing us to retain linguistically correct translationswhile discarding instances with substantial deviations. The results demonstratea significant improvement in translation quality over the baselinepost-filtering with IndicSBERT. This illustrates how cross-lingual sentencerepresentations can reduce errors in machine translation scenarios with limitedresources. By integrating multilingual sentence BERT models into thetranslation pipeline, this research contributes to advancing machinetranslation techniques in low-resource environments. The proposed method notonly addresses the challenges in English-Marathi language pairs but alsoprovides a valuable framework for enhancing translation quality in otherlow-resource language translation tasks.</description><author>Nidhi Kowtal, Tejas Deshpande, Raviraj Joshi</author><pubDate>Wed, 04 Sep 2024 13:49:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02712v1</guid></item><item><title>Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for PostNL</title><link>http://arxiv.org/abs/2409.02711v1</link><description>The developments in the field of generative AI has brought a lot ofopportunities for companies, for instance to improve efficiency in customerservice and automating tasks. PostNL, the biggest parcel and E-commercecorporation of the Netherlands wants to use generative AI to enhance thecommunication around track and trace of parcels. During the internship aMinimal Viable Product (MVP) is created to showcase the value of usinggenerative AI technologies, to enhance parcel tracking, analyzing the parcel'sjourney and being able to communicate about it in an easy to understand manner.The primary goal was to develop an in-house LLM-based system, reducingdependency on external platforms and establishing the feasibility of adedicated generative AI team within the company. This multi-agent LLM basedsystem aimed to construct parcel journey stories and identify logisticaldisruptions with heightened efficiency and accuracy. The research involveddeploying a sophisticated AI-driven communication system, employingRetrieval-Augmented Generation (RAG) for enhanced response precision, andoptimizing large language models (LLMs) tailored to domain specific tasks. The MVP successfully implemented a multi-agent open-source LLM system, calledSuperTracy. SuperTracy is capable of autonomously managing a broad spectrum ofuser inquiries and improving internal knowledge handling. Results andevaluation demonstrated technological innovation and feasibility, notably incommunication about the track and trace of a parcel, which exceeded initialexpectations. These advancements highlight the potential of AI-driven solutionsin logistics, suggesting many opportunities for further refinement and broaderimplementation within PostNL operational framework.</description><author>Mohammad Reshadati</author><pubDate>Wed, 04 Sep 2024 13:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02711v1</guid></item><item><title>Few-shot Multi-Task Learning of Linear Invariant Features with Meta Subspace Pursuit</title><link>http://arxiv.org/abs/2409.02708v1</link><description>Data scarcity poses a serious threat to modern machine learning andartificial intelligence, as their practical success typically relies on theavailability of big datasets. One effective strategy to mitigate the issue ofinsufficient data is to first harness information from other data sourcespossessing certain similarities in the study design stage, and then employ themulti-task or meta learning framework in the analysis stage. In this paper, wefocus on multi-task (or multi-source) linear models whose coefficients acrosstasks share an invariant low-rank component, a popular structural assumptionconsidered in the recent multi-task or meta learning literature. Under thisassumption, we propose a new algorithm, called Meta Subspace Pursuit(abbreviated as Meta-SP), that provably learns this invariant subspace sharedby different tasks. Under this stylized setup for multi-task or meta learning,we establish both the algorithmic and statistical guarantees of the proposedmethod. Extensive numerical experiments are conducted, comparing Meta-SPagainst several competing methods, including popular, off-the-shelfmodel-agnostic meta learning algorithms such as ANIL. These experimentsdemonstrate that Meta-SP achieves superior performance over the competingmethods in various aspects.</description><author>Chaozhi Zhang, Lin Liu, Xiaoqun Zhang</author><pubDate>Wed, 04 Sep 2024 13:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02708v1</guid></item><item><title>Incorporating Like-Minded Peers to Overcome Friend Data Sparsity in Session-Based Social Recommendations</title><link>http://arxiv.org/abs/2409.02702v1</link><description>Session-based Social Recommendation (SSR) leverages social relationshipswithin online networks to enhance the performance of Session-basedRecommendation (SR). However, existing SSR algorithms often encounter thechallenge of ``friend data sparsity''. Moreover, significant discrepancies canexist between the purchase preferences of social network friends and those ofthe target user, reducing the influence of friends relative to the targetuser's own preferences. To address these challenges, this paper introduces theconcept of ``Like-minded Peers'' (LMP), representing users whose preferencesalign with the target user's current session based on their historicalsessions. This is the first work, to our knowledge, that uses LMP to enhancethe modeling of social influence in SSR. This approach not only alleviates theproblem of friend data sparsity but also effectively incorporates users withsimilar preferences to the target user. We propose a novel model namedTransformer Encoder with Graph Attention Aggregator Recommendation (TEGAARec),which includes the TEGAA module and the GAT-based social aggregation module.The TEGAA module captures and merges both long-term and short-term interestsfor target users and LMP users. Concurrently, the GAT-based social aggregationmodule is designed to aggregate the target users' dynamic interests and socialinfluence in a weighted manner. Extensive experiments on four real-worlddatasets demonstrate the efficacy and superiority of our proposed model andablation studies are done to illustrate the contributions of each component inTEGAARec.</description><author>Chunyan An, Yunhan Li, Qiang Yang, Winston K. G. Seah, Zhixu Li, Conghao Yanga</author><pubDate>Wed, 04 Sep 2024 13:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02702v1</guid></item><item><title>CLDA: Collaborative Learning for Enhanced Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2409.02699v1</link><description>Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between amodel trained on a labeled source domain and its deployment in an unlabeledtarget domain. However, current high-performance models demand significantresources, resulting in prohibitive deployment costs and highlighting the needfor small yet effective models. For UDA of lightweight models, KnowledgeDistillation (KD) in a Teacher-Student framework can be a common approach, butwe find that domain shift in UDA leads to a significant increase in non-salientparameters in the teacher model, degrading model's generalization ability andtransferring misleading information to the student model. Interestingly, weobserved that this phenomenon occurs considerably less in the student model.Driven by this insight, we introduce Collaborative Learning, a method thatupdates the teacher's non-salient parameters using the student model and at thesame time enhance the student's performance using the updated teacher model.Experiments across various tasks and datasets show consistent performanceimprovements for both student and teacher models. For example, in semanticsegmentation, CLDA achieves an improvement of +0.7% mIoU for teacher and +1.4%mIoU for student compared to the baseline model in the GTA to Cityscapes. Inthe Synthia to Cityscapes, it achieves an improvement of +0.8% mIoU for teacherand +2.0% mIoU for student.</description><author>Minhee Cho, Hyesong Choi, Hayeon Jo, Dongbo Min</author><pubDate>Wed, 04 Sep 2024 13:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02699v1</guid></item></channel></rss>