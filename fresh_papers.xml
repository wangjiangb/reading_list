<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 14 Jan 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning</title><link>http://arxiv.org/abs/2308.03234v2</link><description>Multiple-choice questions (MCQs) are ubiquitous in almost all levels ofeducation since they are easy to administer, grade, and are a reliable form ofassessment. An important aspect of MCQs is the distractors, i.e., incorrectoptions that are designed to target specific misconceptions or insufficientknowledge among students. To date, the task of crafting high-qualitydistractors has largely remained a labor-intensive process for teachers andlearning content designers, which has limited scalability. In this work, weexplore the task of automated distractor and corresponding feedback messagegeneration in math MCQs using large language models. We establish a formulationof these two tasks and propose a simple, in-context learning-based solution.Moreover, we propose generative AI-based metrics for evaluating the quality ofthe feedback messages. We conduct extensive experiments on these tasks using areal-world MCQ dataset. Our findings suggest that there is a lot of room forimprovement in automated distractor and feedback generation; based on thesefindings, we outline several directions for future work.</description><author>Hunter McNichols, Wanyong Feng, Jaewook Lee, Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan</author><pubDate>Thu, 11 Jan 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03234v2</guid></item><item><title>Distilling Vision-Language Models on Millions of Videos</title><link>http://arxiv.org/abs/2401.06129v1</link><description>The recent advance in vision-language models is largely attributed to theabundance of image-text data. We aim to replicate this success forvideo-language models, but there simply is not enough human-curated video-textdata available. We thus resort to fine-tuning a video-language model from astrong image-language baseline with synthesized instructional data. Theresulting video-language model is then used to auto-label millions of videos togenerate high-quality captions. We show the adapted video-language modelperforms well on a wide range of video-language benchmarks. For instance, itsurpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, ourmodel generates detailed descriptions for previously unseen videos, whichprovide better textual supervision than existing methods. Experiments show thata video-language dual-encoder model contrastively trained on theseauto-generated captions is 3.8% better than the strongest baseline that alsoleverages vision-language models. Our best model outperforms state-of-the-artmethods on MSR-VTT zero-shot text-to-video retrieval by 6%.</description><author>Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, Philipp Krähenbühl, Liangzhe Yuan</author><pubDate>Thu, 11 Jan 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06129v1</guid></item><item><title>ARMA Cell: A Modular and Effective Approach for Neural Autoregressive Modeling</title><link>http://arxiv.org/abs/2208.14919v2</link><description>The autoregressive moving average (ARMA) model is a classical, and arguablyone of the most studied approaches to model time series data. It has compellingtheoretical properties and is widely used among practitioners. More recent deeplearning approaches popularize recurrent neural networks (RNNs) and, inparticular, Long Short-Term Memory (LSTM) cells that have become one of thebest performing and most common building blocks in neural time series modeling.While advantageous for time series data or sequences with long-term effects,complex RNN cells are not always a must and can sometimes even be inferior tosimpler recurrent approaches. In this work, we introduce the ARMA cell, asimpler, modular, and effective approach for time series modeling in neuralnetworks. This cell can be used in any neural network architecture whererecurrent structures are present and naturally handles multivariate time seriesusing vector autoregression. We also introduce the ConvARMA cell as a naturalsuccessor for spatially-correlated time series. Our experiments show that theproposed methodology is competitive with popular alternatives in terms ofperformance while being more robust and compelling due to its simplicity</description><author>Philipp Schiele, Christoph Berninger, David Rügamer</author><pubDate>Thu, 11 Jan 2024 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.14919v2</guid></item><item><title>E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation</title><link>http://arxiv.org/abs/2401.06127v1</link><description>One highly promising direction for enabling flexible real-time on-deviceimage editing is utilizing data distillation by leveraging large-scaletext-to-image diffusion models, such as Stable Diffusion, to generate paireddatasets used for training generative adversarial networks (GANs). Thisapproach notably alleviates the stringent requirements typically imposed byhigh-end commercial GPUs for performing image editing with diffusion models.However, unlike text-to-image diffusion models, each distilled GAN isspecialized for a specific image editing task, necessitating costly trainingefforts to obtain models for various concepts. In this work, we introduce andaddress a novel research direction: can the process of distilling GANs fromdiffusion models be made significantly more efficient? To achieve this goal, wepropose a series of innovative techniques. First, we construct a base GAN modelwith generalized features, adaptable to different concepts through fine-tuning,eliminating the need for training from scratch. Second, we identify cruciallayers within the base GAN model and employ Low-Rank Adaptation (LoRA) with asimple yet effective rank search process, rather than fine-tuning the entirebase model. Third, we investigate the minimal amount of data necessary forfine-tuning, further reducing the overall training time. Extensive experimentsshow that we can efficiently empower GANs with the ability to perform real-timehigh-quality image editing on mobile devices with remarkable reduced trainingcost and storage for each concept.</description><author>Yifan Gong, Zheng Zhan, Qing Jin, Yanyu Li, Yerlan Idelbayev, Xian Liu, Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, Jian Ren</author><pubDate>Thu, 11 Jan 2024 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06127v1</guid></item><item><title>Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors</title><link>http://arxiv.org/abs/2401.06126v1</link><description>Visual dubbing is the process of generating lip motions of an actor in avideo to synchronise with given audio. Recent advances have made progresstowards this goal but have not been able to produce an approach suitable formass adoption. Existing methods are split into either person-generic orperson-specific models. Person-specific models produce results almostindistinguishable from reality but rely on long training times using largesingle-person datasets. Person-generic works have allowed for the visualdubbing of any video to any audio without further training, but these fail tocapture the person-specific nuances and often suffer from visual artefacts. Ourmethod, based on data-efficient neural rendering priors, overcomes thelimitations of existing approaches. Our pipeline consists of learning adeferred neural rendering prior network and actor-specific adaptation usingneural textures. This method allows for $\textbf{high-quality visual dubbingwith just a few seconds of data}$, that enables video dubbing for any actor -from A-list celebrities to background actors. We show that we achievestate-of-the-art in terms of $\textbf{visual quality}$ and$\textbf{recognisability}$ both quantitatively, and qualitatively through twouser studies. Our prior learning and adaptation method $\textbf{generalises tolimited data}$ better and is more $\textbf{scalable}$ than existingperson-specific models. Our experiments on real-world, limited data scenariosfind that our model is preferred over all others. The project page may be foundat https://dubbingforeveryone.github.io/</description><author>Jack Saunders, Vinay Namboodiri</author><pubDate>Thu, 11 Jan 2024 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06126v1</guid></item><item><title>Manipulating Feature Visualizations with Gradient Slingshots</title><link>http://arxiv.org/abs/2401.06122v1</link><description>Deep Neural Networks (DNNs) are capable of learning complex and versatilerepresentations, however, the semantic nature of the learned concepts remainsunknown. A common method used to explain the concepts learned by DNNs isActivation Maximization (AM), which generates a synthetic input signal thatmaximally activates a particular neuron in the network. In this paper, weinvestigate the vulnerability of this approach to adversarial modelmanipulations and introduce a novel method for manipulating featurevisualization without altering the model architecture or significantlyimpacting the model's decision-making process. We evaluate the effectiveness ofour method on several neural network models and demonstrate its capabilities tohide the functionality of specific neurons by masking the original explanationsof neurons with chosen target explanations during model auditing. As a remedy,we propose a protective measure against such manipulations and providequantitative evidence which substantiates our findings.</description><author>Dilyara Bareeva, Marina M. -C. Höhne, Alexander Warnecke, Lukas Pirch, Klaus-Robert Müller, Konrad Rieck, Kirill Bykov</author><pubDate>Thu, 11 Jan 2024 18:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06122v1</guid></item><item><title>TOFU: A Task of Fictitious Unlearning for LLMs</title><link>http://arxiv.org/abs/2401.06121v1</link><description>Large language models trained on massive corpora of data from the web canmemorize and reproduce sensitive or private data raising both legal and ethicalconcerns. Unlearning, or tuning models to forget information present in theirtraining data, provides us with a way to protect private data after training.Although several methods exist for such unlearning, it is unclear to whatextent they result in models equivalent to those where the data to be forgottenwas never learned in the first place. To address this challenge, we presentTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepenour understanding of unlearning. We offer a dataset of 200 diverse syntheticauthor profiles, each consisting of 20 question-answer pairs, and a subset ofthese profiles called the forget set that serves as the target for unlearning.We compile a suite of metrics that work together to provide a holistic pictureof unlearning efficacy. Finally, we provide a set of baseline results fromexisting unlearning algorithms. Importantly, none of the baselines we considershow effective unlearning motivating continued efforts to develop approachesfor unlearning that effectively tune models so that they truly behave as ifthey were never trained on the forget data at all.</description><author>Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, J. Zico Kolter</author><pubDate>Thu, 11 Jan 2024 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06121v1</guid></item><item><title>Extreme Compression of Large Language Models via Additive Quantization</title><link>http://arxiv.org/abs/2401.06118v1</link><description>The emergence of accurate open large language models (LLMs) has led to a racetowards quantization techniques for such models enabling execution on end-userdevices. In this paper, we revisit the problem of "extreme" LLMcompression--defined as targeting extremely low bit counts, such as 2 to 3 bitsper parameter, from the point of view of classic methods in Multi-CodebookQuantization (MCQ). Our work builds on top of Additive Quantization, a classicalgorithm from the MCQ family, and adapts it to the quantization of languagemodels. The resulting algorithm advances the state-of-the-art in LLMcompression, outperforming all recently-proposed techniques in terms ofaccuracy at a given compression budget. For instance, when compressing Llama 2models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93perplexity (a 1.29 improvement relative to the best prior work, and 1.81 pointsfrom FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70Bmodel to 3.94 perplexity (a .22 improvement) on WikiText2. We release ourimplementation of Additive Quantization for Language Models AQLM as a baselineto facilitate future research in LLM quantization.</description><author>Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan Alistarh</author><pubDate>Thu, 11 Jan 2024 18:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06118v1</guid></item><item><title>Gaussian Shadow Casting for Neural Characters</title><link>http://arxiv.org/abs/2401.06116v1</link><description>Neural character models can now reconstruct detailed geometry and texturefrom video, but they lack explicit shadows and shading, leading to artifactswhen generating novel views and poses or during relighting. It is particularlydifficult to include shadows as they are a global effect and the requiredcasting of secondary rays is costly. We propose a new shadow model using aGaussian density proxy that replaces sampling with a simple analytic formula.It supports dynamic motion and is tailored for shadow computation, therebyavoiding the affine projection approximation and sorting required by theclosely related Gaussian splatting. Combined with a deferred neural renderingmodel, our Gaussian shadows enable Lambertian shading and shadow casting withminimal overhead. We demonstrate improved reconstructions, with betterseparation of albedo, shading, and shadows in challenging outdoor scenes withdirect sun light and hard shadows. Our method is able to optimize the lightdirection without any input from the user. As a result, novel poses have fewershadow artifacts and relighting in novel scenes is more realistic compared tothe state-of-the-art methods, providing new ways to pose neural characters innovel environments, increasing their applicability.</description><author>Luis Bolanos, Shih-Yang Su, Helge Rhodin</author><pubDate>Thu, 11 Jan 2024 18:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06116v1</guid></item><item><title>RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</title><link>http://arxiv.org/abs/2401.04679v3</link><description>We investigate parameter-efficient fine-tuning (PEFT) methods that canprovide good accuracy under limited computational and memory budgets in thecontext of large language models (LLMs). We present a new PEFT method calledRobust Adaptation (RoSA) inspired by robust principal component analysis (PCA)that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ componentson top of a set of fixed pretrained weights to efficiently approximate theperformance of a full-fine-tuning (FFT) solution. Across a series ofchallenging generative tasks such as grade-school math and SQL querygeneration, which require fine-tuning for good performance, we show that RoSAoutperforms both LoRA and pure sparse fine-tuning, at the same parameterbudget. We provide system support for RoSA to complement the trainingalgorithm, specifically in the form of sparse GPU kernels which enable memory-and computationally-efficient training. Our code will be made available at$\href{https://github.com/IST-DASLab/RoSA}{\text{our github page}}$.</description><author>Mahdi Nikdan, Soroush Tabesh, Dan Alistarh</author><pubDate>Thu, 11 Jan 2024 18:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04679v3</guid></item><item><title>Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings</title><link>http://arxiv.org/abs/2401.06112v1</link><description>Word embedding is one of the most important components in natural languageprocessing, but interpreting high-dimensional embeddings remains a challengingproblem. To address this problem, Independent Component Analysis (ICA) isidentified as an effective solution. ICA-transformed word embeddings revealinterpretable semantic axes; however, the order of these axes are arbitrary. Inthis study, we focus on this property and propose a novel method, Axis Tour,which optimizes the order of the axes. Inspired by Word Tour, a one-dimensionalword embedding method, we aim to improve the clarity of the word embeddingspace by maximizing the semantic continuity of the axes. Furthermore, we showthrough experiments on downstream tasks that Axis Tour constructs betterlow-dimensional embeddings compared to both PCA and ICA.</description><author>Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira</author><pubDate>Thu, 11 Jan 2024 18:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06112v1</guid></item><item><title>Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning</title><link>http://arxiv.org/abs/2307.01849v3</link><description>Sequence modeling approaches have shown promising results in robot imitationlearning. Recently, diffusion models have been adopted for behavioral cloningin a sequence modeling fashion, benefiting from their exceptional capabilitiesin modeling complex data distributions. The standard diffusion-based policyiteratively generates action sequences from random noise conditioned on theinput states. Nonetheless, the model for diffusion policy can be furtherimproved in terms of visual representations. In this work, we propose CrosswayDiffusion, a simple yet effective method to enhance diffusion-based visuomotorpolicy learning via a carefully designed state decoder and an auxiliaryself-supervised learning (SSL) objective. The state decoder reconstructs rawimage pixels and other state information from the intermediate representationsof the reverse diffusion process. The whole model is jointly optimized by theSSL objective and the original diffusion loss. Our experiments demonstrate theeffectiveness of Crossway Diffusion in various simulated and real-world robottasks, confirming its consistent advantages over the standard diffusion-basedpolicy and substantial improvements over the baselines.</description><author>Xiang Li, Varun Belagali, Jinghuan Shang, Michael S. Ryoo</author><pubDate>Thu, 11 Jan 2024 18:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01849v3</guid></item><item><title>Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review</title><link>http://arxiv.org/abs/2303.06471v2</link><description>Cancer has relational information residing at varying scales, modalities, andresolutions of the acquired data, such as radiology, pathology, genomics,proteomics, and clinical records. Integrating diverse data types can improvethe accuracy and reliability of cancer diagnosis and treatment. There can bedisease-related information that is too subtle for humans or existingtechnological tools to discern visually. Traditional methods typically focus onpartial or unimodal information about biological systems at individual scalesand fail to encapsulate the complete spectrum of the heterogeneous nature ofdata. Deep neural networks have facilitated the development of sophisticatedmultimodal data fusion approaches that can extract and integrate relevantinformation from multiple sources. Recent deep learning frameworks such asGraph Neural Networks (GNNs) and Transformers have shown remarkable success inmultimodal learning. This review article provides an in-depth analysis of thestate-of-the-art in GNNs and Transformers for multimodal data fusion inoncology settings, highlighting notable research studies and their findings. Wealso discuss the foundations of multimodal learning, inherent challenges, andopportunities for integrative learning in oncology. By examining the currentstate and potential future developments of multimodal data integration inoncology, we aim to demonstrate the promising role that multimodal neuralnetworks can play in cancer prevention, early detection, and treatment throughinformed oncology practices in personalized settings.</description><author>Asim Waqas, Aakash Tripathi, Ravi P. Ramachandran, Paul Stewart, Ghulam Rasool</author><pubDate>Thu, 11 Jan 2024 18:38:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06471v2</guid></item><item><title>PALP: Prompt Aligned Personalization of Text-to-Image Models</title><link>http://arxiv.org/abs/2401.06105v1</link><description>Content creators often aim to create personalized images using personalsubjects that go beyond the capabilities of conventional text-to-image models.Additionally, they may want the resulting image to encompass a specificlocation, style, ambiance, and more. Existing personalization methods maycompromise personalization ability or the alignment to complex textual prompts.This trade-off can impede the fulfillment of user prompts and subject fidelity.We propose a new approach focusing on personalization methods for a\emph{single} prompt to address this issue. We term our approach prompt-alignedpersonalization. While this may seem restrictive, our method excels inimproving text alignment, enabling the creation of images with complex andintricate prompts, which may pose a challenge for current techniques. Inparticular, our method keeps the personalized model aligned with a targetprompt using an additional score distillation sampling term. We demonstrate theversatility of our method in multi- and single-shot settings and further showthat it can compose multiple subjects or use inspiration from reference images,such as artworks. We compare our approach quantitatively and qualitatively withexisting baselines and state-of-the-art techniques.</description><author>Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, Ariel Shamir</author><pubDate>Thu, 11 Jan 2024 18:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06105v1</guid></item><item><title>Transformers are Multi-State RNNs</title><link>http://arxiv.org/abs/2401.06104v1</link><description>Transformers are considered conceptually different compared to the previousgeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).In this work, we demonstrate that decoder-only transformers can in fact beconceptualized as infinite multi-state RNNs - an RNN variant with unlimitedhidden state size. We further show that pretrained transformers can beconverted into $\textit{finite}$ multi-state RNNs by fixing the size of theirhidden state. We observe that several existing transformers cache compressiontechniques can be framed as such conversion policies, and introduce a novelpolicy, TOVA, which is simpler compared to these policies. Our experiments withseveral long range tasks indicate that TOVA outperforms all other baselinepolicies, while being nearly on par with the full (infinite) model, and usingin some cases only $\frac{1}{8}$ of the original cache size. Our resultsindicate that transformer decoder LLMs often behave in practice as RNNs. Theyalso lay out the option of mitigating one of their most painful computationalbottlenecks - the size of their cache memory. We publicly release our code athttps://github.com/schwartz-lab-NLP/TOVA.</description><author>Matanel Oren, Michael Hassid, Yossi Adi, Roy Schwartz</author><pubDate>Thu, 11 Jan 2024 18:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06104v1</guid></item><item><title>Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models</title><link>http://arxiv.org/abs/2401.06102v1</link><description>Inspecting the information encoded in hidden representations of largelanguage models (LLMs) can explain models' behavior and verify their alignmentwith human values. Given the capabilities of LLMs in generatinghuman-understandable text, we propose leveraging the model itself to explainits internal representations in natural language. We introduce a frameworkcalled Patchscopes and show how it can be used to answer a wide range ofresearch questions about an LLM's computation. We show that priorinterpretability methods based on projecting representations into thevocabulary space and intervening on the LLM computation, can be viewed asspecial instances of this framework. Moreover, several of their shortcomingssuch as failure in inspecting early layers or lack of expressivity can bemitigated by a Patchscope. Beyond unifying prior inspection techniques,Patchscopes also opens up new possibilities such as using a more capable modelto explain the representations of a smaller model, and unlocks new applicationssuch as self-correction in multi-hop reasoning.</description><author>Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva</author><pubDate>Thu, 11 Jan 2024 18:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06102v1</guid></item><item><title>Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning</title><link>http://arxiv.org/abs/2305.14062v3</link><description>Photoplethysmography (PPG) refers to the measurement of variations in bloodvolume using light and is a feature of most wearable devices. The PPG signalsprovide insight into the body's circulatory system and can be employed toextract various bio-features, such as heart rate and vascular ageing. Althoughseveral algorithms have been proposed for this purpose, many exhibitlimitations, including heavy reliance on human calibration, high signal qualityrequirements, and a lack of generalisation. In this paper, we introduce a PPGsignal processing framework that integrates graph theory and computer visionalgorithms, to provide an analysis framework which is amplitude-independent andinvariant to affine transformations. It also requires minimal preprocessing,fuses information through RGB channels and exhibits robust generalisationacross tasks and datasets. The proposed VGTL-net achieves state-of-the-artperformance in the prediction of vascular ageing and demonstrates robustestimation of continuous blood pressure waveforms.</description><author>Yuyang Miao, Harry J. Davies, Danilo P. Mandic</author><pubDate>Thu, 11 Jan 2024 18:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14062v3</guid></item><item><title>Linear Spaces of Meanings: Compositional Structures in Vision-Language Models</title><link>http://arxiv.org/abs/2302.14383v3</link><description>We investigate compositional structures in data embeddings from pre-trainedvision-language models (VLMs). Traditionally, compositionality has beenassociated with algebraic operations on embeddings of words from a pre-existingvocabulary. In contrast, we seek to approximate representations from an encoderas combinations of a smaller set of vectors in the embedding space. Thesevectors can be seen as "ideal words" for generating concepts directly withinthe embedding space of the model. We first present a framework forunderstanding compositional structures from a geometric perspective. We thenexplain what these compositional structures entail probabilistically in thecase of VLM embeddings, providing intuitions for why they arise in practice.Finally, we empirically explore these structures in CLIP's embeddings and weevaluate their usefulness for solving different vision-language tasks such asclassification, debiasing, and retrieval. Our results show that simple linearalgebraic operations on embedding vectors can be used as compositional andinterpretable methods for regulating the behavior of VLMs.</description><author>Matthew Trager, Pramuditha Perera, Luca Zancato, Alessandro Achille, Parminder Bhatia, Stefano Soatto</author><pubDate>Thu, 11 Jan 2024 18:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14383v3</guid></item><item><title>Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series</title><link>http://arxiv.org/abs/2401.03955v2</link><description>Large Pretrained models for zero/few-shot learning excel in language andvision domains but encounter challenges in multivariate time series (TS) due tothe diverse nature and scarcity of publicly available pretraining data.Consequently, there has been a recent surge in utilizing pretrained largelanguage models (LLMs) with various adaptations for time series forecasting.These approaches employ cross-domain transfer learning and surprisingly yieldimpressive results. However, these models are typically very slow and large($\sim$billion parameters) and do not consider cross-channel correlations. Toaddress this, we present Multi-level Tiny Time Mixers (TTM), a significantlysmall model based on the lightweight TSMixer architecture. TTM marks the firstsuccess in developing tiny general-pretrained models ($\le$1 millionparameters), exclusively trained on public TS datasets in a flash of just 4-8hrs with effective transfer learning capabilities for forecasting. To tacklethe complexity of pretraining on multiple datasets with varied temporalresolutions, we introduce several novel enhancements such as adaptive patching,dataset augmentation via downsampling, and resolution prefix tuning. Moreover,we employ a multi-level modeling strategy to effectively model channelcorrelations and incorporate exogenous signals during fine-tuning, a crucialcapability lacking in existing benchmarks. TTM excels in few/zero-shotforecasting, demonstrating significant accuracy gains (12-38%) over existingbenchmarks. Further, it achieves a remarkable 14-106X reduction in modelparameters, enabling 54-65X faster finetuning/inference as compared to theLLM-TS benchmarks. In fact, TTM's zero-shot often surpasses the few-shotresults in many popular benchmarks, highlighting the efficacy of our approach.Code and Pretrained Models will be open-sourced.</description><author>Vijay Ekambaram, Arindam Jati, Nam H. Nguyen, Pankaj Dayama, Chandra Reddy, Wesley M. Gifford, Jayant Kalagnanam</author><pubDate>Thu, 11 Jan 2024 18:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03955v2</guid></item><item><title>A Closer Look at AUROC and AUPRC under Class Imbalance</title><link>http://arxiv.org/abs/2401.06091v1</link><description>In machine learning (ML), a widespread adage is that the area under theprecision-recall curve (AUPRC) is a superior metric for model comparison to thearea under the receiver operating characteristic (AUROC) for binaryclassification tasks with class imbalance. This paper challenges this notionthrough novel mathematical analysis, illustrating that AUROC and AUPRC can beconcisely related in probabilistic terms. We demonstrate that AUPRC, contraryto popular belief, is not superior in cases of class imbalance and might evenbe a harmful metric, given its inclination to unduly favor model improvementsin subpopulations with more frequent positive labels. This bias caninadvertently heighten algorithmic disparities. Prompted by these insights, athorough review of existing ML literature was conducted, utilizing largelanguage models to analyze over 1.5 million papers from arXiv. Ourinvestigation focused on the prevalence and substantiation of the purportedAUPRC superiority. The results expose a significant deficit in empiricalbacking and a trend of misattributions that have fuelled the widespreadacceptance of AUPRC's supposed advantages. Our findings represent a dualcontribution: a significant technical advancement in understanding metricbehaviors and a stark warning about unchecked assumptions in the ML community.All experiments are accessible athttps://github.com/mmcdermott/AUC_is_all_you_need.</description><author>Matthew B. A. McDermott, Lasse Hyldig Hansen, Haoran Zhang, Giovanni Angelotti, Jack Gallifant</author><pubDate>Thu, 11 Jan 2024 18:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06091v1</guid></item><item><title>Designing Ecosystems of Intelligence from First Principles</title><link>http://arxiv.org/abs/2212.01354v2</link><description>This white paper lays out a vision of research and development in the fieldof artificial intelligence for the next decade (and beyond). Its denouement isa cyber-physical ecosystem of natural and synthetic sense-making, in whichhumans are integral participants -- what we call ''shared intelligence''. Thisvision is premised on active inference, a formulation of adaptive behavior thatcan be read as a physics of intelligence, and which inherits from the physicsof self-organization. In this context, we understand intelligence as thecapacity to accumulate evidence for a generative model of one's sensed world --also known as self-evidencing. Formally, this corresponds to maximizing(Bayesian) model evidence, via belief updating over several scales: i.e.,inference, learning, and model selection. Operationally, this self-evidencingcan be realized via (variational) message passing or belief propagation on afactor graph. Crucially, active inference foregrounds an existential imperativeof intelligent systems; namely, curiosity or the resolution of uncertainty.This same imperative underwrites belief sharing in ensembles of agents, inwhich certain aspects (i.e., factors) of each agent's generative world modelprovide a common ground or frame of reference. Active inference plays afoundational role in this ecology of belief sharing -- leading to a formalaccount of collective intelligence that rests on shared narratives and goals.We also consider the kinds of communication protocols that must be developed toenable such an ecosystem of intelligences and motivate the development of ashared hyper-spatial modeling language and transaction protocol, as a first --and key -- step towards such an ecology.</description><author>Karl J Friston, Maxwell J D Ramstead, Alex B Kiefer, Alexander Tschantz, Christopher L Buckley, Mahault Albarracin, Riddhi J Pitliya, Conor Heins, Brennan Klein, Beren Millidge, Dalton A R Sakthivadivel, Toby St Clere Smithe, Magnus Koudahl, Safae Essafi Tremblay, Capm Petersen, Kaiser Fung, Jason G Fox, Steven Swanson, Dan Mapes, Gabriel René</author><pubDate>Thu, 11 Jan 2024 18:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01354v2</guid></item><item><title>PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage Clustering on GPU</title><link>http://arxiv.org/abs/2401.06089v1</link><description>This paper presents \pandora, a novel parallel algorithm for efficientlyconstructing dendrograms for single-linkage hierarchical clustering, including\hdbscan. Traditional dendrogram construction methods from a minimum spanningtree (MST), such as agglomerative or divisive techniques, often fail toefficiently parallelize, especially with skewed dendrograms common inreal-world data. \pandora addresses these challenges through a unique recursive treecontraction method, which simplifies the tree for initial dendrogramconstruction and then progressively reconstructs the complete dendrogram. Thisprocess makes \pandora asymptotically work-optimal, independent of dendrogramskewness. All steps in \pandora are fully parallel and suitable for massivelythreaded accelerators such as GPUs. Our implementation is written in Kokkos, providing support for both CPUs andmulti-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \pandora is2.2$\times$ faster than the current best-multithreaded implementation, whilethe GPU \pandora implementation achieved 6-20$\times$ on \amdgpu and10-37$\times$ on \nvidiagpu speed-up over multithreaded \pandora. Theseadvancements lead to up to a 6-fold speedup for \hdbscan on GPUs over thecurrent best, which only offload MST construction to GPUs and performmultithreaded dendrogram construction.</description><author>Piyush Sao, Andrey Prokopenko, Damien Lebrun-Grandié</author><pubDate>Thu, 11 Jan 2024 18:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06089v1</guid></item><item><title>Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models</title><link>http://arxiv.org/abs/2401.06088v1</link><description>The Chief Complaint (CC) is a crucial component of a patient's medical recordas it describes the main reason or concern for seeking medical care. Itprovides critical information for healthcare providers to make informeddecisions about patient care. However, documenting CCs can be time-consumingfor healthcare providers, especially in busy emergency departments. To addressthis issue, an autocompletion tool that suggests accurate and well-formattedphrases or sentences for clinical notes can be a valuable resource for triagenurses. In this study, we utilized text generation techniques to developmachine learning models using CC data. In our proposed work, we train a LongShort-Term Memory (LSTM) model and fine-tune three different variants ofBiomedical Generative Pretrained Transformers (BioGPT), namelymicrosoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.Additionally, we tune a prompt by incorporating exemplar CC sentences,utilizing the OpenAI API of GPT-4. We evaluate the models' performance based onthe perplexity score, modified BERTScore, and cosine similarity score. Theresults show that BioGPT-Large exhibits superior performance compared to theother models. It consistently achieves a remarkably low perplexity score of1.65 when generating CC, whereas the baseline LSTM model achieves the bestperplexity score of 170. Further, we evaluate and assess the proposed models'performance and the outcome of GPT-4.0. Our study demonstrates that utilizingLLMs such as BioGPT, leads to the development of an effective autocompletiontool for generating CC documentation in healthcare settings.</description><author>K M Sajjadul Islam, Ayesha Siddika Nipu, Praveen Madiraju, Priya Deshpande</author><pubDate>Thu, 11 Jan 2024 18:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06088v1</guid></item><item><title>XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange</title><link>http://arxiv.org/abs/2401.06086v1</link><description>We present first results from the use of XGBoost, a highly effective machinelearning (ML) method, within the Bristol Betting Exchange (BBE), an open-sourceagent-based model (ABM) designed to simulate a contemporary sports-bettingexchange with in-play betting during track-racing events such as horse races.We use the BBE ABM and its array of minimally-simple bettor-agents as asynthetic data generator which feeds into our XGBoost ML system, with theintention that XGBoost discovers profitable dynamic betting strategies bylearning from the more profitable bets made by the BBE bettor-agents. Afterthis XGBoost training, which results in one or more decision trees, abettor-agent with a betting strategy determined by the XGBoost-learned decisiontree(s) is added to the BBE ABM and made to bet on a sequence of races undervarious conditions and betting-market scenarios, with profitability serving asthe primary metric of comparison and evaluation. Our initial findings presentedhere show that XGBoost trained in this way can indeed learn profitable bettingstrategies, and can generalise to learn strategies that outperform each of theset of strategies used for creation of the training data. To foster furtherresearch and enhancements, the complete version of our extended BBE, includingthe XGBoost integration, has been made freely available as an open-sourcerelease on GitHub.</description><author>Chawin Terawong, Dave Cliff</author><pubDate>Thu, 11 Jan 2024 18:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06086v1</guid></item><item><title>Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint</title><link>http://arxiv.org/abs/2401.06081v1</link><description>Reinforcement learning (RL) has been widely used in training large languagemodels~(LLMs) for preventing unexpected outputs, \eg reducing harmfulness anderrors. However, existing RL methods mostly adopt the instance-level reward,which is unable to provide fine-grained supervision for complex reasoningtasks, and can not focus on the few key tokens that lead to the incorrectness.To address it, we propose a new RL method named \textbf{RLMEC} thatincorporates a generative model as the reward model, which is trained by theerroneous solution rewriting task under the minimum editing constraint, and canproduce token-level rewards for RL training. Based on the generative rewardmodel, we design the token-level RL objective for training and animitation-based regularization for stabilizing RL process. And the bothobjectives focus on the learning of the key tokens for the erroneous solution,reducing the effect of other unimportant tokens. The experiment results onmathematical tasks and question-answering tasks have demonstrated theeffectiveness of our approach. Our code and data are available at\url{https://github.com/RUCAIBox/RLMEC}.</description><author>Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, Ji-Rong Wen</author><pubDate>Thu, 11 Jan 2024 17:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06081v1</guid></item><item><title>Secrets of RLHF in Large Language Models Part II: Reward Modeling</title><link>http://arxiv.org/abs/2401.06080v1</link><description>Reinforcement Learning from Human Feedback (RLHF) has become a crucialtechnology for aligning language models with human values and intentions,enabling models to produce more helpful and harmless responses. Reward modelsare trained as proxies for human preferences to drive reinforcement learningoptimization. While reward models are often considered central to achievinghigh performance, they face the following challenges in practical applications:(1) Incorrect and ambiguous preference pairs in the dataset may hinder thereward model from accurately capturing human intent. (2) Reward models trainedon data from a specific distribution often struggle to generalize to examplesoutside that distribution and are not suitable for iterative RLHF training. In this report, we attempt to address these two issues. (1) From a dataperspective, we propose a method to measure the strength of preferences withinthe data, based on a voting mechanism of multiple reward models. Experimentalresults confirm that data with varying preference strengths have differentimpacts on reward model performance. We introduce a series of novel methods tomitigate the influence of incorrect and ambiguous preferences in the datasetand fully leverage high-quality preference data. (2) From an algorithmicstandpoint, we introduce contrastive learning to enhance the ability of rewardmodels to distinguish between chosen and rejected responses, thereby improvingmodel generalization. Furthermore, we employ meta-learning to enable the rewardmodel to maintain the ability to differentiate subtle differences inout-of-distribution samples, and this approach can be utilized for iterativeRLHF optimization.</description><author>Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Thu, 11 Jan 2024 17:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06080v1</guid></item><item><title>Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion</title><link>http://arxiv.org/abs/2401.06072v1</link><description>Temporal Knowledge Graph Completion (TKGC) is a challenging task ofpredicting missing event links at future timestamps by leveraging establishedtemporal structural knowledge. Given the formidable generative capabilitiesinherent in LLMs (LLMs), this paper proposes a novel approach to conceptualizetemporal link prediction as an event generation task within the context of ahistorical event chain. We employ efficient fine-tuning methods to make LLMsadapt to specific graph textual information and patterns discovered in temporaltimelines. Furthermore, we introduce structure-based historical dataaugmentation and the integration of reverse knowledge to emphasize LLMs'awareness of structural information, thereby enhancing their reasoningcapabilities. We conduct thorough experiments on multiple widely used datasetsand find that our fine-tuned model outperforms existing embedding-based modelson multiple metrics, achieving SOTA results. We also carry out sufficientablation experiments to explore the key influencing factors when LLMs performstructured temporal knowledge inference tasks.</description><author>Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li, Yujiu Yang</author><pubDate>Thu, 11 Jan 2024 17:42:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06072v1</guid></item><item><title>LEGO:Language Enhanced Multi-modal Grounding Model</title><link>http://arxiv.org/abs/2401.06071v1</link><description>Multi-modal large language models have demonstrated impressive performanceacross various tasks in different modalities. However, existing multi-modalmodels primarily emphasize capturing global information within each modalitywhile neglecting the importance of perceiving local information acrossmodalities. Consequently, these models lack the ability to effectivelyunderstand the fine-grained details of input data, limiting their performancein tasks that require a more nuanced understanding. To address this limitation,there is a compelling need to develop models that enable fine-grainedunderstanding across multiple modalities, thereby enhancing their applicabilityto a wide range of tasks. In this paper, we propose LEGO, a language enhancedmulti-modal grounding model. Beyond capturing global information like othermulti-modal models, our proposed model excels at tasks demanding a detailedunderstanding of local information within the input. It demonstrates preciseidentification and localization of specific regions in images or moments invideos. To achieve this objective, we design a diversified dataset constructionpipeline, resulting in a multi-modal, multi-granularity dataset for modeltraining. The code, dataset, and demo of our model can be found at https://github.com/lzw-lzw/LEGO.</description><author>Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang</author><pubDate>Thu, 11 Jan 2024 17:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06071v1</guid></item><item><title>Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model for Complex Material Responses</title><link>http://arxiv.org/abs/2401.06070v1</link><description>Neural operators, which can act as implicit solution operators of hiddengoverning equations, have recently become popular tools for learning theresponses of complex real-world physical systems. Nevertheless, most neuraloperator applications have thus far been data-driven and neglect the intrinsicpreservation of fundamental physical laws in data. In this work, we introduce anovel integral neural operator architecture called the Peridynamic NeuralOperator (PNO) that learns a nonlocal constitutive law from data. This neuraloperator provides a forward model in the form of state-based peridynamics, withobjectivity and momentum balance laws automatically guaranteed. Asapplications, we demonstrate the expressivity and efficacy of our model inlearning complex material behaviors from both synthetic and experimental datasets. We show that, owing to its ability to capture complex responses, ourlearned neural operator achieves improved accuracy and efficiency compared tobaseline models that use predefined constitutive laws. Moreover, by preservingthe essential physical laws within the neural network architecture, the PNO isrobust in treating noisy data. The method shows generalizability to differentdomain configurations, external loadings, and discretizations.</description><author>Siavash Jafarzadeh, Stewart Silling, Ning Liu, Zhongqiang Zhang, Yue Yu</author><pubDate>Thu, 11 Jan 2024 17:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06070v1</guid></item><item><title>Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures</title><link>http://arxiv.org/abs/2311.00636v2</link><description>The core components of many modern neural network architectures, such astransformers, convolutional, or graph neural networks, can be expressed aslinear layers with $\textit{weight-sharing}$. Kronecker-Factored ApproximateCurvature (K-FAC), a second-order optimisation method, has shown promise tospeed up neural network training and thereby reduce computational costs.However, there is currently no framework to apply it to generic architectures,specifically ones with linear weight-sharing layers. In this work, we identifytwo different settings of linear weight-sharing layers which motivate twoflavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that theyare exact for deep linear networks with weight-sharing in their respectivesetting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which weleverage to speed up automatic hyperparameter selection via optimising themarginal likelihood for a Wide ResNet. Finally, we observe little differencebetween these two K-FAC variations when using them to train both a graph neuralnetwork and a vision transformer. However, both variations are able to reach afixed validation metric target in $50$-$75\%$ of the number of steps of afirst-order reference run, which translates into a comparable improvement inwall-clock time. This highlights the potential of applying K-FAC to modernneural network architectures.</description><author>Runa Eschenhagen, Alexander Immer, Richard E. Turner, Frank Schneider, Philipp Hennig</author><pubDate>Thu, 11 Jan 2024 17:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00636v2</guid></item><item><title>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</title><link>http://arxiv.org/abs/2401.06066v1</link><description>In the era of large language models, Mixture-of-Experts (MoE) is a promisingarchitecture for managing computational costs when scaling up model parameters.However, conventional MoE architectures like GShard, which activate the top-$K$out of $N$ experts, face challenges in ensuring expert specialization, i.e.each expert acquires non-overlapping and focused knowledge. In response, wepropose the DeepSeekMoE architecture towards ultimate expert specialization. Itinvolves two principal strategies: (1) finely segmenting the experts into $mN$ones and activating $mK$ from them, allowing for a more flexible combination ofactivated experts; (2) isolating $K_s$ experts as shared ones, aiming atcapturing common knowledge and mitigating redundancy in routed experts.Starting from a modest scale with 2B parameters, we demonstrate thatDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5times the expert parameters and computation. In addition, DeepSeekMoE 2B nearlyapproaches the performance of its dense counterpart with the same number oftotal parameters, which set the upper bound of MoE models. Subsequently, wescale up DeepSeekMoE to 16B parameters and show that it achieves comparableperformance with LLaMA2 7B, with only about 40% of computations. Further, ourpreliminary efforts to scale up DeepSeekMoE to 145B parameters consistentlyvalidate its substantial advantages over the GShard architecture, and show itsperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)of computations.</description><author>Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang</author><pubDate>Thu, 11 Jan 2024 17:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06066v1</guid></item><item><title>Investigating Data Contamination for Pre-training Language Models</title><link>http://arxiv.org/abs/2401.06059v1</link><description>Language models pre-trained on web-scale corpora demonstrate impressivecapabilities on diverse downstream tasks. However, there is increasing concernwhether such capabilities might arise from evaluation datasets being includedin the pre-training corpus -- a phenomenon known as \textit{data contamination}-- in a manner that artificially increases performance. There has been littleunderstanding of how this potential contamination might influence LMs'performance on downstream tasks. In this paper, we explore the impact of datacontamination at the pre-training stage by pre-training a series of GPT-2models \textit{from scratch}. We highlight the effect of both textcontamination (\textit{i.e.}\ input text of the evaluation samples) andground-truth contamination (\textit{i.e.}\ the prompts asked on the input andthe desired outputs) from evaluation data. We also investigate the effects ofrepeating contamination for various downstream tasks. Additionally, we examinethe prevailing n-gram-based definitions of contamination within current LLMreports, pinpointing their limitations and inadequacy. Our findings offer newinsights into data contamination's effects on language model capabilities andunderscore the need for independent, comprehensive contamination assessments inLLM studies.</description><author>Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo</author><pubDate>Thu, 11 Jan 2024 17:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06059v1</guid></item><item><title>MatSynth: A Modern PBR Materials Dataset</title><link>http://arxiv.org/abs/2401.06056v1</link><description>We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBRmaterials. Materials are crucial components of virtual relightable assets,defining the interaction of light at the surface of geometries. Given theirimportance, significant research effort was dedicated to their representation,creation and acquisition. However, in the past 6 years, most research inmaterial acquisiton or generation relied either on the same unique dataset, oron company-owned huge library of procedural materials. With this dataset wepropose a significantly larger, more diverse, and higher resolution set ofmaterials than previously publicly available. We carefully discuss the datacollection process and demonstrate the benefits of this dataset on materialacquisition and generation applications. The complete data further containsmetadata with each material's origin, license, category, tags, creation methodand, when available, descriptions and physical size, as well as 3M+ renderingsof the augmented materials, in 1K, under various environment lightings. TheMatSynth dataset is released through the project page at:https://www.gvecchio.com/matsynth.</description><author>Giuseppe Vecchio, Valentin Deschaintre</author><pubDate>Thu, 11 Jan 2024 17:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06056v1</guid></item><item><title>Fast High Dynamic Range Radiance Fields for Dynamic Scenes</title><link>http://arxiv.org/abs/2401.06052v1</link><description>Neural Radiances Fields (NeRF) and their extensions have shown great successin representing 3D scenes and synthesizing novel-view images. However, mostNeRF methods take in low-dynamic-range (LDR) images, which may lose details,especially with nonuniform illumination. Some previous NeRF methods attempt tointroduce high-dynamic-range (HDR) techniques but mainly target static scenes.To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRFframework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D imagescaptured with various exposures. A learnable exposure mapping function isconstructed to obtain adaptive exposure values for each image. Based on themonotonically increasing prior, a camera response function is designed forstable learning. With the proposed model, high-quality novel-view images at anytime point can be rendered with any desired exposure. We further construct adataset containing multiple dynamic scenes captured with diverse exposures forevaluation. All the datasets and code are available at\url{https://guanjunwu.github.io/HDR-HexPlane/}.</description><author>Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang</author><pubDate>Thu, 11 Jan 2024 17:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06052v1</guid></item><item><title>On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks</title><link>http://arxiv.org/abs/2401.06048v1</link><description>This paper studies four Graph Neural Network architectures (GNNs) for a graphclassification task on a synthetic dataset created using classic generativemodels of Network Science. Since the synthetic networks do not contain (node oredge) features, five different augmentation strategies (artificial featuretypes) are applied to nodes. All combinations of the 4 GNNs (GCN withHierarchical and Global aggregation, GIN and GATv2) and the 5 feature types(constant 1, noise, degree, normalized degree and ID -- a vector of the numberof cycles of various lengths) are studied and their performances compared as afunction of the hidden dimension of artificial neural networks used in theGNNs. The generalisation ability of these models is also analysed using asecond synthetic network dataset (containing networks of different sizes).Ourresults point towards the balanced importance of the computational power of theGNN architecture and the the information level provided by the artificialfeatures. GNN architectures with higher computational power, like GIN andGATv2, perform well for most augmentation strategies. On the other hand,artificial features with higher information content, like ID or degree, notonly consistently outperform other augmentation strategies, but can also helpGNN architectures with lower computational power to achieve good performance.</description><author>Walid Guettala, László Gulyás</author><pubDate>Thu, 11 Jan 2024 17:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06048v1</guid></item><item><title>Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) for Automated Structural Condition Assessment in Visual Inspection</title><link>http://arxiv.org/abs/2307.07643v4</link><description>Efficiently monitoring the condition of civil infrastructure requiresautomating the structural condition assessment in visual inspection. This paperproposes an Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) forautomatic structural condition assessment in visual bridge inspection.AECIF-Net can simultaneously parse structural elements and segment surfacedefects on the elements in inspection images. It integrates two task-specificrelearning subnets to extract task-specific features from an overall featureembedding. A co-interactive feature fusion module further captures the spatialcorrelation and facilitates information sharing between tasks. Experimentalresults demonstrate that the proposed AECIF-Net outperforms the currentstate-of-the-art approaches, achieving promising performance with 92.11% mIoUfor element segmentation and 87.16% mIoU for corrosion segmentation on the testset of the new benchmark dataset Steel Bridge Condition Inspection Visual(SBCIV). An ablation study verifies the merits of the designs for AECIF-Net,and a case study demonstrates its capability to automate structural conditionassessment.</description><author>Chenyu Zhang, Zhaozheng Yin, Ruwen Qin</author><pubDate>Thu, 11 Jan 2024 17:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07643v4</guid></item><item><title>Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting</title><link>http://arxiv.org/abs/2401.06040v1</link><description>Traffic forecasting is the foundation for intelligent transportation systems.Spatiotemporal graph neural networks have demonstrated state-of-the-artperformance in traffic forecasting. However, these methods do not explicitlymodel some of the natural characteristics in traffic data, such as themultiscale structure that encompasses spatial and temporal variations atdifferent levels of granularity or scale. To that end, we propose aWavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combinesmultiscale analysis (MSA)-based method with Deep Learning (DL)-based method. InWavGCRN, the traffic data is decomposed into time-frequency components withDiscrete Wavelet Transformation (DWT), constructing a multi-stream inputstructure; then Graph Convolutional Recurrent networks (GCRNs) are employed asencoders for each stream, extracting spatiotemporal features in differentscales; and finally the learnable Inversed DWT and GCRN are combined as thedecoder, fusing the information from all streams for traffic metricsreconstruction and prediction. Furthermore, road-network-informed graphs anddata-driven graph learning are combined to accurately capture spatialcorrelation. The proposed method can offer well-defined interpretability,powerful learning capability, and competitive forecasting performance onreal-world traffic data sets.</description><author>Qipeng Qian, Tanwi Mallick</author><pubDate>Thu, 11 Jan 2024 16:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06040v1</guid></item><item><title>Heterogeneous Value Alignment Evaluation for Large Language Models</title><link>http://arxiv.org/abs/2305.17147v3</link><description>The emergent capabilities of Large Language Models (LLMs) have made itcrucial to align their values with those of humans. However, currentmethodologies typically attempt to assign value as an attribute to LLMs, yetlack attention to the ability to pursue value and the importance oftransferring heterogeneous values in specific practical applications. In thispaper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,designed to assess the success of aligning LLMs with heterogeneous values.Specifically, our approach first brings the Social Value Orientation (SVO)framework from social psychology, which corresponds to how much weight a personattaches to the welfare of others in relation to their own. We then assign theLLMs with different social values and measure whether their behaviors alignwith the inducing values. We conduct evaluations with new auto-metric\textit{value rationality} to represent the ability of LLMs to align withspecific values. Evaluating the value rationality of five mainstream LLMs, wediscern a propensity in LLMs towards neutral values over pronounced personalvalues. By examining the behavior of these LLMs, we contribute to a deeperinsight into the value alignment of LLMs within a heterogeneous value system.</description><author>Zhaowei Zhang, Ceyao Zhang, Nian Liu, Siyuan Qi, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, Yaodong Yang</author><pubDate>Thu, 11 Jan 2024 16:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17147v3</guid></item><item><title>RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks</title><link>http://arxiv.org/abs/2401.06035v1</link><description>We present a novel unconditional video generative model designed to addresslong-term spatial and temporal dependencies. To capture these dependencies, ourapproach incorporates a hybrid explicit-implicit tri-plane representationinspired by 3D-aware generative frameworks developed for three-dimensionalobject representation and employs a singular latent code to model an entirevideo sequence. Individual video frames are then synthesized from anintermediate tri-plane representation, which itself is derived from the primarylatent code. This novel strategy reduces computational complexity by a factorof $2$ as measured in FLOPs. Consequently, our approach facilitates theefficient and temporally coherent generation of videos. Moreover, our jointframe modeling approach, in contrast to autoregressive methods, mitigates thegeneration of visual artifacts. We further enhance the model's capabilities byintegrating an optical flow-based module within our Generative AdversarialNetwork (GAN) based generator architecture, thereby compensating for theconstraints imposed by a smaller generator size. As a result, our model iscapable of synthesizing high-fidelity video clips at a resolution of$256\times256$ pixels, with durations extending to more than $5$ seconds at aframe rate of 30 fps. The efficacy and versatility of our approach areempirically validated through qualitative and quantitative assessments acrossthree different datasets comprising both synthetic and real video clips.</description><author>Partha Ghosh, Soubhik Sanyal, Cordelia Schmid, Bernhard Schölkopf</author><pubDate>Thu, 11 Jan 2024 16:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06035v1</guid></item><item><title>LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization</title><link>http://arxiv.org/abs/2401.06034v1</link><description>Pretrained language models (PLMs) have shown remarkable generalization towardmultiple tasks and languages. Nonetheless, the generalization of PLMs towardsunseen languages is poor, resulting in significantly worse languageperformance, or even generating nonsensical responses that are comparable to arandom baseline. This limitation has been a longstanding problem of PLMsraising the problem of diversity and equal access to language modelingtechnology. In this work, we solve this limitation by introducing LinguAlchemy,a regularization technique that incorporates various aspects of languagescovering typological, geographical, and phylogenetic constraining the resultingrepresentation of PLMs to better characterize the corresponding linguisticsconstraints. LinguAlchemy significantly improves the accuracy performance ofmBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared tofully finetuned models and displaying a high degree of unseen languagegeneralization. We further introduce AlchemyScale and AlchemyTune, extension ofLinguAlchemy which adjusts the linguistic regularization weights automatically,alleviating the need for hyperparameter search. LinguAlchemy enables bettercross-lingual generalization to unseen languages which is vital for betterinclusivity and accessibility of PLMs.</description><author>Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Alham Fikri Aji, Genta Indra Winata, Ayu Purwarianti</author><pubDate>Thu, 11 Jan 2024 16:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06034v1</guid></item><item><title>CORAL: Expert-Curated medical Oncology Reports to Advance Language Model Inference</title><link>http://arxiv.org/abs/2308.03853v3</link><description>Both medical care and observational studies in oncology require a thoroughunderstanding of a patient's disease progression and treatment history, oftenelaborately documented in clinical notes. Despite their vital role, no currentoncology information representation and annotation schema fully encapsulatesthe diversity of information recorded within these notes. Although largelanguage models (LLMs) have recently exhibited impressive performance onvarious medical natural language processing tasks, due to the current lack ofcomprehensively annotated oncology datasets, an extensive evaluation of LLMs inextracting and reasoning with the complex rhetoric in oncology notes remainsunderstudied. We developed a detailed schema for annotating textual oncologyinformation, encompassing patient characteristics, tumor characteristics,tests, treatments, and temporality. Using a corpus of 40 de-identified breastand pancreatic cancer progress notes at University of California, SanFrancisco, we applied this schema to assess the zero-shot abilities of threerecent LLMs (GPT-4, GPT-3.5-turbo, and FLAN-UL2) to extract detailedoncological history from two narrative sections of clinical progress notes. Ourteam annotated 9028 entities, 9986 modifiers, and 5312 relationships. The GPT-4model exhibited overall best performance, with an average BLEU score of 0.73,an average ROUGE score of 0.72, an exact-match F1-score of 0.51, and an averageaccuracy of 68% on complex tasks (expert manual evaluation on subset). Notably,it was proficient in tumor characteristic and medication extraction, anddemonstrated superior performance in relational inference like adverse eventdetection. However, further improvements are needed before using it to reliablyextract important facts from cancer progress notes needed for clinicalresearch, complex population management, and documenting quality patient care.</description><author>Madhumita Sushil, Vanessa E. Kennedy, Divneet Mandair, Brenda Y. Miao, Travis Zack, Atul J. Butte</author><pubDate>Thu, 11 Jan 2024 16:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03853v3</guid></item><item><title>GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model</title><link>http://arxiv.org/abs/2401.06031v1</link><description>Adversarial generative models, such as Generative Adversarial Networks(GANs), are widely applied for generating various types of data, i.e., images,text, and audio. Accordingly, its promising performance has led to theGAN-based adversarial attack methods in the white-box and black-box attackscenarios. The importance of transferable black-box attacks lies in theirability to be effective across different models and settings, more closelyaligning with real-world applications. However, it remains challenging toretain the performance in terms of transferable adversarial examples for suchmethods. Meanwhile, we observe that some enhanced gradient-based transferableadversarial attack algorithms require prolonged time for adversarial samplegeneration. Thus, in this work, we propose a novel algorithm named GE-AdvGAN toenhance the transferability of adversarial samples whilst improving thealgorithm's efficiency. The main approach is via optimising the trainingprocess of the generator parameters. With the functional and characteristicsimilarity analysis, we introduce a novel gradient editing (GE) mechanism andverify its feasibility in generating transferable samples on various models.Moreover, by exploring the frequency domain information to determine thegradient editing direction, GE-AdvGAN can generate highly transferableadversarial samples while minimizing the execution time in comparison to thestate-of-the-art transferable adversarial attack algorithms. The performance ofGE-AdvGAN is comprehensively evaluated by large-scale experiments on differentdatasets, which results demonstrate the superiority of our algorithm. The codefor our algorithm is available at: https://github.com/LMBTough/GE-advGAN</description><author>Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo</author><pubDate>Thu, 11 Jan 2024 16:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06031v1</guid></item><item><title>Automatic UAV-based Airport Pavement Inspection Using Mixed Real and Virtual Scenarios</title><link>http://arxiv.org/abs/2401.06019v1</link><description>Runway and taxiway pavements are exposed to high stress during theirprojected lifetime, which inevitably leads to a decrease in their conditionover time. To make sure airport pavement condition ensure uninterrupted andresilient operations, it is of utmost importance to monitor their condition andconduct regular inspections. UAV-based inspection is recently gainingimportance due to its wide range monitoring capabilities and reduced cost. Inthis work, we propose a vision-based approach to automatically identifypavement distress using images captured by UAVs. The proposed method is basedon Deep Learning (DL) to segment defects in the image. The DL architectureleverages the low computational capacities of embedded systems in UAVs by usingan optimised implementation of EfficientNet feature extraction and FeaturePyramid Network segmentation. To deal with the lack of annotated data fortraining we have developed a synthetic dataset generation methodology to extendavailable distress datasets. We demonstrate that the use of a mixed datasetcomposed of synthetic and real training images yields better results whentesting the training models in real application scenarios.</description><author>Pablo Alonso, Jon Ander Iñiguez de Gordoa, Juan Diego Ortega, Sara García, Francisco Javier Iriarte, Marcos Nieto</author><pubDate>Thu, 11 Jan 2024 16:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06019v1</guid></item><item><title>Optimal Chaining of Vehicle Plans with Time Windows</title><link>http://arxiv.org/abs/2401.02873v2</link><description>For solving problems from the domain of Mobility-on-Demand (MoD), we oftenneed to connect vehicle plans into plans spanning longer time, a process wecall plan chaining. As we show in this work, chaining of the plans can be usedto reduce the size of MoD providers' fleet (fleet-sizing problem) but also toreduce the total driven distance by providing high-quality vehicle dispatchingsolutions in MoD systems. Recently, a solution that uses this principle hasbeen proposed to solve the fleet-sizing problem. The method does not considerthe time flexibility of the plans. Instead, plans are fixed in time and cannotbe delayed. However, time flexibility is an essential property of all vehicleproblems with time windows. This work presents a new plan chaining formulationthat considers delays as allowed by the time windows and a solution method forsolving it. Moreover, we prove that the proposed plan chaining method isoptimal, and we analyze its complexity. Finally, we list some practicalapplications and perform a demonstration for one of them: a new heuristicvehicle dispatching method for solving the static dial-a-ride problem. Thedemonstration results show that our proposed method provides a better solutionthan the two heuristic baselines for the majority of instances that cannot besolved optimally. At the same time, our method does not have the largestcomputational time requirements compared to the baselines. Therefore, weconclude that the proposed optimal chaining method provides not onlytheoretically sound results but is also practically applicable.</description><author>David Fiedler, Fabio V. Difonzo, Jan Mrkos</author><pubDate>Thu, 11 Jan 2024 16:26:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02873v2</guid></item><item><title>ProAgent: Building Proactive Cooperative Agents with Large Language Models</title><link>http://arxiv.org/abs/2308.11339v3</link><description>Building agents with adaptive behavior in cooperative tasks stands as aparamount goal in the realm of multi-agent systems. Current approaches todeveloping cooperative agents rely primarily on learning-based methods, whosepolicy generalization depends heavily on the diversity of teammates theyinteract with during the training phase. Such reliance, however, constrains theagents' capacity for strategic adaptation when cooperating with unfamiliarteammates, which becomes a significant challenge in zero-shot coordinationscenarios. To address this challenge, we propose ProAgent, a novel frameworkthat harnesses large language models (LLMs) to create proactive agents capableof dynamically adapting their behavior to enhance cooperation with teammates.ProAgent can analyze the present state, and infer the intentions of teammatesfrom observations. It then updates its beliefs in alignment with the teammates'subsequent actual behaviors. Moreover, ProAgent exhibits a high degree ofmodularity and interpretability, making it easily integrated into various ofcoordination scenarios. Experimental evaluations conducted within theOvercooked-AI environment unveil the remarkable performance superiority ofProAgent, outperforming five methods based on self-play and population-basedtraining when cooperating with AI agents. Furthermore, in partnered with humanproxy models, its performance exhibits an average improvement exceeding 10%compared to the current state-of-the-art method. For more information about ourproject, please visit~\url{https://pku-proagent.github.io}.</description><author>Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, Yaodong Yang</author><pubDate>Thu, 11 Jan 2024 16:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11339v3</guid></item><item><title>Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery</title><link>http://arxiv.org/abs/2401.06013v1</link><description>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,surgical navigation and augmented reality visualization. Although thefoundation model exhibits outstanding performance in many vision tasks,including depth estimation (e.g., DINOv2), recent works observed itslimitations in medical and surgical domain-specific applications. This workpresents a low-ranked adaptation (LoRA) of the foundation model for surgicaldepth estimation. Methods: We design a foundation model-based depth estimationmethod, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 fordepth estimation in endoscopic surgery. We build LoRA layers and integrate theminto DINO to adapt with surgery-specific domain knowledge instead ofconventional fine-tuning. During training, we freeze the DINO image encoder,which shows excellent visual representation capacity, and only optimize theLoRA layers and depth decoder to integrate features from the surgical scene.Results: Our model is extensively validated on a MICCAI challenge dataset ofSCARED, which is collected from da Vinci Xi endoscope surgery. We empiricallyshow that Surgical-DINO significantly outperforms all the state-of-the-artmodels in endoscopic depth estimation tasks. The analysis with ablation studieshas shown evidence of the remarkable effect of our LoRA layers and adaptation.Conclusion: Surgical-DINO shed some light on the successful adaptation of thefoundation models into the surgical domain for depth estimation. There is clearevidence in the results that zero-shot prediction on pre-trained weights incomputer vision datasets or naive fine-tuning is not sufficient to use thefoundation model in the surgical domain directly. Code is available athttps://github.com/BeileiCui/SurgicalDINO.</description><author>Cui Beilei, Islam Mobarakol, Bai Long, Ren Hongliang</author><pubDate>Thu, 11 Jan 2024 16:22:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06013v1</guid></item><item><title>Attention to detail: inter-resolution knowledge distillation</title><link>http://arxiv.org/abs/2401.06010v1</link><description>The development of computer vision solutions for gigapixel images in digitalpathology is hampered by significant computational limitations due to the largesize of whole slide images. In particular, digitizing biopsies at highresolutions is a time-consuming process, which is necessary due to theworsening results from the decrease in image detail. To alleviate this issue,recent literature has proposed using knowledge distillation to enhance themodel performance at reduced image resolutions. In particular, soft labels andfeatures extracted at the highest magnification level are distilled into amodel that takes lower-magnification images as input. However, this approachfails to transfer knowledge about the most discriminative image regions in theclassification process, which may be lost when the resolution is decreased. Inthis work, we propose to distill this information by incorporating attentionmaps during training. In particular, our formulation leverages saliency maps ofthe target class via grad-CAMs, which guides the lower-resolution Student modelto match the Teacher distribution by minimizing the l2 distance between them.Comprehensive experiments on prostate histology image grading demonstrate thatthe proposed approach substantially improves the model performance acrossdifferent image resolutions compared to previous literature.</description><author>Rocío del Amor, Julio Silva-Rodríguez, Adrián Colomer, Valery Naranjo</author><pubDate>Thu, 11 Jan 2024 16:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06010v1</guid></item><item><title>Sea ice detection using concurrent multispectral and synthetic aperture radar imagery</title><link>http://arxiv.org/abs/2401.06009v1</link><description>Synthetic Aperture Radar (SAR) imagery is the primary data type used for seaice mapping due to its spatio-temporal coverage and the ability to detect seaice independent of cloud and lighting conditions. Automatic sea ice detectionusing SAR imagery remains problematic due to the presence of ambiguous signaland noise within the image. Conversely, ice and water are easilydistinguishable using multispectral imagery (MSI), but in the polar regions theocean's surface is often occluded by cloud or the sun may not appear above thehorizon for many months. To address some of these limitations, this paperproposes a new tool trained using concurrent multispectral Visible and SARimagery for sea Ice Detection (ViSual\_IceD). ViSual\_IceD is a convolutionneural network (CNN) that builds on the classic U-Net architecture bycontaining two parallel encoder stages, enabling the fusion and concatenationof MSI and SAR imagery containing different spatial resolutions. Theperformance of ViSual\_IceD is compared with U-Net models trained usingconcatenated MSI and SAR imagery as well as models trained exclusively on MSIor SAR imagery. ViSual\_IceD outperforms the other networks, with a F1 score1.60\% points higher than the next best network, and results indicate thatViSual\_IceD is selective in the image type it uses during image segmentation.Outputs from ViSual\_IceD are compared to sea ice concentration productsderived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight howViSual\_IceD is a useful tool to use in conjunction with PMW data, particularlyin coastal regions. As the spatial-temporal coverage of MSI and SAR imagerycontinues to increase, ViSual\_IceD provides a new opportunity for robust,accurate sea ice coverage detection in polar regions.</description><author>Martin S J Rogers, Maria Fox, Andrew Fleming, Louisa van Zeeland, Jeremy Wilkinson, J. Scott Hosking</author><pubDate>Thu, 11 Jan 2024 16:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06009v1</guid></item><item><title>Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model</title><link>http://arxiv.org/abs/2306.01424v3</link><description>Counterfactual inference aims to answer retrospective "what if" questions andthus belongs to the most fine-grained type of inference in Pearl's causalityladder. Existing methods for counterfactual inference with continuous outcomesaim at point identification and thus make strong and unnatural assumptionsabout the underlying structural causal model. In this paper, we relax theseassumptions and aim at partial counterfactual identification of continuousoutcomes, i.e., when the counterfactual query resides in an ignorance intervalwith informative bounds. We prove that, in general, the ignorance interval ofthe counterfactual queries has non-informative bounds, already when functionsof structural causal models are continuously differentiable. As a remedy, wepropose a novel sensitivity model called Curvature Sensitivity Model. Thisallows us to obtain informative bounds by bounding the curvature of level setsof the functions. We further show that existing point counterfactualidentification methods are special cases of our Curvature Sensitivity Modelwhen the bound of the curvature is set to zero. We then propose animplementation of our Curvature Sensitivity Model in the form of a novel deepgenerative model, which we call Augmented Pseudo-Invertible Decoder. Ourimplementation employs (i) residual normalizing flows with (ii) variationalaugmentations. We empirically demonstrate the effectiveness of our AugmentedPseudo-Invertible Decoder. To the best of our knowledge, ours is the firstpartial identification model for Markovian structural causal models withcontinuous outcomes.</description><author>Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</author><pubDate>Thu, 11 Jan 2024 16:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01424v3</guid></item><item><title>How does the primate brain combine generative and discriminative computations in vision?</title><link>http://arxiv.org/abs/2401.06005v1</link><description>Vision is widely understood as an inference problem. However, two contrastingconceptions of the inference process have each been influential in research onbiological vision as well as the engineering of machine vision. The firstemphasizes bottom-up signal flow, describing vision as a largely feedforward,discriminative inference process that filters and transforms the visualinformation to remove irrelevant variation and represent behaviorally relevantinformation in a format suitable for downstream functions of cognition andbehavioral control. In this conception, vision is driven by the sensory data,and perception is direct because the processing proceeds from the data to thelatent variables of interest. The notion of "inference" in this conception isthat of the engineering literature on neural networks, where feedforwardconvolutional neural networks processing images are said to perform inference.The alternative conception is that of vision as an inference process inHelmholtz's sense, where the sensory evidence is evaluated in the context of agenerative model of the causal processes giving rise to it. In this conception,vision inverts a generative model through an interrogation of the evidence in aprocess often thought to involve top-down predictions of sensory data toevaluate the likelihood of alternative hypotheses. The authors includescientists rooted in roughly equal numbers in each of the conceptions andmotivated to overcome what might be a false dichotomy between them and engagethe other perspective in the realm of theory and experiment. The primate brainemploys an unknown algorithm that may combine the advantages of bothconceptions. We explain and clarify the terminology, review the key empiricalevidence, and propose an empirical research program that transcends thedichotomy and sets the stage for revealing the mysterious hybrid algorithm ofprimate vision.</description><author>Benjamin Peters, James J. DiCarlo, Todd Gureckis, Ralf Haefner, Leyla Isik, Joshua Tenenbaum, Talia Konkle, Thomas Naselaris, Kimberly Stachenfeld, Zenna Tavares, Doris Tsao, Ilker Yildirim, Nikolaus Kriegeskorte</author><pubDate>Thu, 11 Jan 2024 16:07:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06005v1</guid></item><item><title>TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering</title><link>http://arxiv.org/abs/2401.06003v1</link><description>Point-based radiance field rendering has demonstrated impressive results fornovel view synthesis, offering a compelling blend of rendering quality andcomputational efficiency. However, also latest approaches in this domain arenot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.2023] struggles when tasked with rendering highly detailed scenes, due toblurring and cloudy artifacts. On the other hand, ADOP [R\"uckert et al. 2022]can accommodate crisper images, but the neural reconstruction network decreasesperformance, it grapples with temporal instability and it is unable toeffectively address large gaps in the point cloud. In this paper, we present TRIPS (Trilinear Point Splatting), an approach thatcombines ideas from both Gaussian Splatting and ADOP. The fundamental conceptbehind our novel technique involves rasterizing points into a screen-spaceimage pyramid, with the selection of the pyramid layer determined by theprojected point size. This approach allows rendering arbitrarily large pointsusing a single trilinear write. A lightweight neural network is then used toreconstruct a hole-free image including detail beyond splat resolution.Importantly, our render pipeline is entirely differentiable, allowing forautomatic optimization of both point sizes and positions. Our evaluation demonstrate that TRIPS surpasses existing state-of-the-artmethods in terms of rendering quality while maintaining a real-time frame rateof 60 frames per second on readily available hardware. This performance extendsto challenging scenarios, such as scenes featuring intricate geometry,expansive landscapes, and auto-exposed footage.</description><author>Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger</author><pubDate>Thu, 11 Jan 2024 16:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06003v1</guid></item><item><title>Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges</title><link>http://arxiv.org/abs/2308.00031v3</link><description>Generative Artificial Intelligence (AI) is one of the most excitingdevelopments in Computer Science of the last decade. At the same time,Reinforcement Learning (RL) has emerged as a very successful paradigm for avariety of machine learning tasks. In this survey, we discuss the state of theart, opportunities and open research questions in applying RL to generative AI.In particular, we will discuss three types of applications, namely, RL as analternative way for generation without specified objectives; as a way forgenerating outputs while concurrently maximizing an objective function; and,finally, as a way of embedding desired characteristics, which cannot be easilycaptured by means of an objective function, into the generative process. Weconclude the survey with an in-depth discussion of the opportunities andchallenges in this fascinating emerging area.</description><author>Giorgio Franceschelli, Mirco Musolesi</author><pubDate>Thu, 11 Jan 2024 16:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00031v3</guid></item><item><title>Body-Area Capacitive or Electric Field Sensing for Human Activity Recognition and Human-Computer Interaction: A Comprehensive Survey</title><link>http://arxiv.org/abs/2401.06000v1</link><description>Due to the fact that roughly sixty percent of the human body is essentiallycomposed of water, the human body is inherently a conductive object, being ableto, firstly, form an inherent electric field from the body to the surroundingsand secondly, deform the distribution of an existing electric field near thebody. Body-area capacitive sensing, also called body-area electric fieldsensing, is becoming a promising alternative for wearable devices to accomplishcertain tasks in human activity recognition and human-computer interaction.Over the last decade, researchers have explored plentiful novel sensing systemsbacked by the body-area electric field. On the other hand, despite thepervasive exploration of the body-area electric field, a comprehensive surveydoes not exist for an enlightening guideline. Moreover, the various hardwareimplementations, applied algorithms, and targeted applications result in achallenging task to achieve a systematic overview of the subject. This paperaims to fill in the gap by comprehensively summarizing the existing works onbody-area capacitive sensing so that researchers can have a better view of thecurrent exploration status. To this end, we first sorted the explorations intothree domains according to the involved body forms: body-part electric field,whole-body electric field, and body-to-body electric field, and enumerated thestate-of-art works in the domains with a detailed survey of the backed sensingtricks and targeted applications. We then summarized the three types of sensingfrontends in circuit design, which is the most critical part in body-areacapacitive sensing, and analyzed the data processing pipeline categorized intothree kinds of approaches. Finally, we described the challenges and outlooks ofbody-area electric sensing.</description><author>Sizhen Bian, Mengxi Liu, Bo Zhou, Paul Lukowicz, Michele Magno</author><pubDate>Thu, 11 Jan 2024 16:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06000v1</guid></item><item><title>Combating Adversarial Attacks with Multi-Agent Debate</title><link>http://arxiv.org/abs/2401.05998v1</link><description>While state-of-the-art language models have achieved impressive results, theyremain susceptible to inference-time adversarial attacks, such as adversarialprompts generated by red teams arXiv:2209.07858. One approach proposed toimprove the general quality of language model generations is multi-agentdebate, where language models self-evaluate through discussion and feedbackarXiv:2305.14325. We implement multi-agent debate between currentstate-of-the-art language models and evaluate models' susceptibility to redteam attacks in both single- and multi-agent settings. We find that multi-agentdebate can reduce model toxicity when jailbroken or less capable models areforced to debate with non-jailbroken or more capable models. We also findmarginal improvements through the general usage of multi-agent interactions. Wefurther perform adversarial prompt content classification via embeddingclustering, and analyze the susceptibility of different models to differenttypes of attack topics.</description><author>Steffi Chern, Zhen Fan, Andy Liu</author><pubDate>Thu, 11 Jan 2024 15:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05998v1</guid></item><item><title>MGARD: A multigrid framework for high-performance, error-controlled data compression and refactoring</title><link>http://arxiv.org/abs/2401.05994v1</link><description>We describe MGARD, a software providing MultiGrid Adaptive Reduction forfloating-point scientific data on structured and unstructured grids. Withexceptional data compression capability and precise error control, MGARDaddresses a wide range of requirements, including storage reduction,high-performance I/O, and in-situ data analysis. It features a unifiedapplication programming interface (API) that seamlessly operates across diversecomputing architectures. MGARD has been optimized with highly-tuned GPU kernelsand efficient memory and device management mechanisms, ensuring scalable andrapid operations.</description><author>Qian Gong, Jieyang Chen, Ben Whitney, Xin Liang, Viktor Reshniak, Tania Banerjee, Jaemoon Lee, Anand Rangarajan, Lipeng Wan, Nicolas Vidal, Qing Liu, Ana Gainaru, Norbert Podhorszki, Richard Archibald, Sanjay Ranka, Scott Klasky</author><pubDate>Thu, 11 Jan 2024 15:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05994v1</guid></item><item><title>Automated speech audiometry: Can it work using open-source pre-trained Kaldi-NL automatic speech recognition?</title><link>http://arxiv.org/abs/2312.12269v2</link><description>A practical speech audiometry tool is the digits-in-noise (DIN) test forhearing screening of populations of varying ages and hearing status. The testis usually conducted by a human supervisor (e.g., clinician), who scores theresponses spoken by the listener, or online, where a software scores theresponses entered by the listener. The test has 24 digit-triplets presented inan adaptive staircase procedure, resulting in a speech reception threshold(SRT). We propose an alternative automated DIN test setup that can evaluatespoken responses whilst conducted without a human supervisor, using theopen-source automatic speech recognition toolkit, Kaldi-NL. Thirtyself-reported normal-hearing Dutch adults (19-64 years) completed oneDIN+Kaldi-NL test. Their spoken responses were recorded, and used forevaluating the transcript of decoded responses by Kaldi-NL. Study 1 evaluatedthe Kaldi-NL performance through its word error rate (WER), percentage ofsummed decoding errors regarding only digits found in the transcript comparedto the total number of digits present in the spoken responses. Average WERacross participants was 5.0% (range 0 - 48%, SD = 8.8%), with average decodingerrors in three triplets per participant. Study 2 analysed the effect thattriplets with decoding errors from Kaldi-NL had on the DIN test output (SRT),using bootstrapping simulations. Previous research indicated 0.70 dB as thetypical within-subject SRT variability for normal-hearing adults. Study 2showed that up to four triplets with decoding errors produce SRT variationswithin this range, suggesting that our proposed setup could be feasible forclinical applications.</description><author>Gloria Araiza-Illan, Luke Meyer, Khiet P. Truong, Deniz Baskent</author><pubDate>Thu, 11 Jan 2024 15:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12269v2</guid></item><item><title>A tree-based varying coefficient model</title><link>http://arxiv.org/abs/2401.05982v1</link><description>The paper introduces a tree-based varying coefficient model (VCM) where thevarying coefficients are modelled using the cyclic gradient boosting machine(CGBM) from Delong et al. (2023). Modelling the coefficient functions using aCGBM allows for dimension-wise early stopping and feature importance scores.The dimension-wise early stopping not only reduces the risk ofdimension-specific overfitting, but also reveals differences in modelcomplexity across dimensions. The use of feature importance scores allows forsimple feature selection and easy model interpretation. The model is evaluatedon the same simulated and real data examples as those used in Richman andW\"uthrich (2023), and the results show that it produces results in terms ofout of sample loss that are comparable to those of their neural network-basedVCM called LocalGLMnet.</description><author>Henning Zakrisson, Mathias Lindholm</author><pubDate>Thu, 11 Jan 2024 15:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05982v1</guid></item><item><title>Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination</title><link>http://arxiv.org/abs/2401.05254v2</link><description>Although affective expressions of individuals have been extensively studiedusing social media, research has primarily focused on the Western context.There are substantial differences among cultures that contribute to theiraffective expressions. This paper examines the differences between Twitter (X)in the United States and Sina Weibo posts in China on two primary dimensions ofaffect - valence and arousal. We study the difference in the functionalrelationship between arousal and valence (so-called V-shaped) among individualsin the US and China and explore the associated content differences.Furthermore, we correlate word usage and topics in both platforms to interprettheir differences. We observe that for Twitter users, the variation inemotional intensity is less distinct between negative and positive emotionscompared to Weibo users, and there is a sharper escalation in arousalcorresponding with heightened emotions. From language features, we discoverthat affective expressions are associated with personal life and feelings onTwitter, while on Weibo such discussions are about socio-political topics inthe society. These results suggest a West-East difference in the V-shapedrelationship between valence and arousal of affective expressions on socialmedia influenced by content differences. Our findings have implications forapplications and theories related to cultural differences in affectiveexpressions.</description><author>Young-Min Cho, Dandan Pang, Stuti Thapa, Garrick Sherman, Lyle Ungar, Louis Tay, Sharath Chandra Guntuku</author><pubDate>Thu, 11 Jan 2024 15:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05254v2</guid></item><item><title>Resilient Constrained Learning</title><link>http://arxiv.org/abs/2306.02426v4</link><description>When deploying machine learning solutions, they must satisfy multiplerequirements beyond accuracy, such as fairness, robustness, or safety. Theserequirements are imposed during training either implicitly, using penalties, orexplicitly, using constrained optimization methods based on Lagrangian duality.Either way, specifying requirements is hindered by the presence of compromisesand limited prior knowledge about the data. Furthermore, their impact onperformance can often only be evaluated by actually solving the learningproblem. This paper presents a constrained learning approach that adapts therequirements while simultaneously solving the learning task. To do so, itrelaxes the learning constraints in a way that contemplates how much theyaffect the task at hand by balancing the performance gains obtained from therelaxation against a user-defined cost of that relaxation. We call thisapproach resilient constrained learning after the term used to describeecological systems that adapt to disruptions by modifying their operation. Weshow conditions under which this balance can be achieved and introduce apractical algorithm to compute it, for which we derive approximation andgeneralization guarantees. We showcase the advantages of this resilientlearning method in image classification tasks involving multiple potentialinvariances and in heterogeneous federated learning.</description><author>Ignacio Hounie, Alejandro Ribeiro, Luiz F. O. Chamon</author><pubDate>Thu, 11 Jan 2024 15:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02426v4</guid></item><item><title>End-to-end Learnable Clustering for Intent Learning in Recommendation</title><link>http://arxiv.org/abs/2401.05975v1</link><description>Mining users' intents plays a crucial role in sequential recommendation. Therecent approach, ICLRec, was introduced to extract underlying users' intentsusing contrastive learning and clustering. While it has shown effectiveness,the existing method suffers from complex and cumbersome alternatingoptimization, leading to two main issues. Firstly, the separation ofrepresentation learning and clustering optimization within a generalizedexpectation maximization (EM) framework often results in sub-optimalperformance. Secondly, performing clustering on the entire dataset hampersscalability for large-scale industry data. To address these challenges, wepropose a novel intent learning method called \underline{ELCRec}, whichintegrates representation learning into an \underline{E}nd-to-end\underline{L}earnable \underline{C}lustering framework for\underline{Rec}ommendation. Specifically, we encode users' behavior sequencesand initialize the cluster centers as learnable network parameters.Additionally, we design a clustering loss that guides the networks todifferentiate between different cluster centers and pull similar samplestowards their respective cluster centers. This allows simultaneous optimizationof recommendation and clustering using mini-batch data. Moreover, we leveragethe learned cluster centers as self-supervision signals for representationlearning, resulting in further enhancement of recommendation performance.Extensive experiments conducted on open benchmarks and industry data validatethe superiority, effectiveness, and efficiency of our proposed ELCRec method.Code is available at: https://github.com/yueliu1999/ELCRec.</description><author>Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Wenliang Zhong, Guannan Zhang, Kejun Zhang, Xinwang Liu</author><pubDate>Thu, 11 Jan 2024 15:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05975v1</guid></item><item><title>Learning physics-based reduced models from data for the Hasegawa-Wakatani equations</title><link>http://arxiv.org/abs/2401.05972v1</link><description>This paper focuses on the construction of non-intrusive Scientific MachineLearning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasmaturbulence simulations. In particular, we propose using Operator Inference(OpInf) to build low-cost physics-based ROMs from data for such simulations. Asa representative example, we focus on the Hasegawa-Wakatani (HW) equations usedfor modeling two-dimensional electrostatic drift-wave plasma turbulence. For acomprehensive perspective of the potential of OpInf to construct accurate ROMsfor this model, we consider a setup for the HW equations that leads to theformation of complex, nonlinear, and self-driven dynamics, and perform two setsof experiments. We first use the data obtained via a direct numericalsimulation of the HW equations starting from a specific initial condition andtrain OpInf ROMs for predictions beyond the training time horizon. In thesecond, more challenging set of experiments, we train ROMs using the samedataset as before but this time perform predictions for six other initialconditions. Our results show that the OpInf ROMs capture the important featuresof the turbulent dynamics and generalize to new and unseen initial conditionswhile reducing the evaluation time of the high-fidelity model by up to fiveorders of magnitude in single-core performance. In the broader context offusion research, this shows that non-intrusive SciML ROMs have the potential todrastically accelerate numerical studies, which can ultimately enable taskssuch as the design and real-time control of optimized fusion devices.</description><author>Constatin Gahr, Ionut-Gabriel Farcas, Frank Jenko</author><pubDate>Thu, 11 Jan 2024 15:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05972v1</guid></item><item><title>UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization</title><link>http://arxiv.org/abs/2401.05971v1</link><description>Despite significant progress in global localization of Unmanned AerialVehicles (UAVs) in GPS-denied environments, existing methods remain constrainedby the availability of datasets. Current datasets often focus on small-scalescenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAVbuild-in sensor data. To address these limitations, we introduce a large-scale6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoFlocalization pipeline (UAVLoc), which consists of offline synthetic datageneration and online visual localization. Additionally, based on the 6-DoFestimator, we design a hierarchical system for tracking ground target in 3Dspace. Experimental results on the new dataset demonstrate the effectiveness ofthe proposed approach. Code and dataset are available athttps://github.com/RingoWRW/UAVD4L</description><author>Rouwan Wu, Xiaoya Cheng, Juelin Zhu, Xuxiang Liu, Maojun Zhang, Shen Yan</author><pubDate>Thu, 11 Jan 2024 15:19:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05971v1</guid></item><item><title>Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem</title><link>http://arxiv.org/abs/2401.05969v1</link><description>The traveling officer problem (TOP) is a challenging stochastic optimizationtask. In this problem, a parking officer is guided through a city equipped withparking sensors to fine as many parking offenders as possible. A majorchallenge in TOP is the dynamic nature of parking offenses, which randomlyappear and disappear after some time, regardless of whether they have beenfined. Thus, solutions need to dynamically adjust to currently fineable parkingoffenses while also planning ahead to increase the likelihood that the officerarrives during the offense taking place. Though various solutions exist, thesemethods often struggle to take the implications of actions on the ability tofine future parking violations into account. This paper proposes SATOP, a novelspatial-aware deep reinforcement learning approach for TOP. Our novel stateencoder creates a representation of each action, leveraging the spatialrelationships between parking spots, the agent, and the action. Furthermore, wepropose a novel message-passing module for learning future inter-actioncorrelations in the given environment. Thus, the agent can estimate thepotential to fine further parking violations after executing an action. Weevaluate our method using an environment based on real-world data fromMelbourne. Our results show that SATOP consistently outperformsstate-of-the-art TOP agents and is able to fine up to 22% more parkingoffenses.</description><author>Niklas Strauß, Matthias Schubert</author><pubDate>Thu, 11 Jan 2024 15:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05969v1</guid></item><item><title>A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd Counting</title><link>http://arxiv.org/abs/2401.05968v1</link><description>Crowd counting finds direct applications in real-world situations, makingcomputational efficiency and performance crucial. However, most of the previousmethods rely on a heavy backbone and a complex downstream architecture thatrestricts the deployment. To address this challenge and enhance the versatilityof crowd-counting models, we introduce two lightweight models. These modelsmaintain the same downstream architecture while incorporating two distinctbackbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion toextract diverse scale features from a Pre-Trained Model (PTM) and subsequentlycombine these features seamlessly. This approach empowers our models to achieveimproved performance while maintaining a compact and efficient design. With thecomparison of our proposed models with previously available state-of-the-art(SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, itachieves comparable results while being the most computationally efficientmodel. Finally, we present a comparative study, an extensive ablation study,along with pruning to show the effectiveness of our models.</description><author>Yashwardhan Chaudhuri, Ankit Kumar, Orchid Chetia Phukan, Arun Balaji Buduru</author><pubDate>Thu, 11 Jan 2024 15:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05968v1</guid></item><item><title>Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding</title><link>http://arxiv.org/abs/2401.05967v1</link><description>The primary aim of Knowledge Graph embeddings (KGE) is to learnlow-dimensional representations of entities and relations for predictingmissing facts. While rotation-based methods like RotatE and QuatE perform wellin KGE, they face two challenges: limited model flexibility requiringproportional increases in relation size with entity dimension, and difficultiesin generalizing the model for higher-dimensional rotations. To address theseissues, we introduce OrthogonalE, a novel KGE model employing matrices forentities and block-diagonal orthogonal matrices with Riemannian optimizationfor relations. This approach enhances the generality and flexibility of KGEmodels. The experimental results indicate that our new KGE model, OrthogonalE,is both general and flexible, significantly outperforming state-of-the-art KGEmodels while substantially reducing the number of relation parameters.</description><author>Yihua Zhu, Hidetoshi Shimodaira</author><pubDate>Thu, 11 Jan 2024 15:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05967v1</guid></item><item><title>Interaction Region Visual Transformer for Egocentric Action Anticipation</title><link>http://arxiv.org/abs/2211.14154v7</link><description>Human-object interaction is one of the most important visual cues and wepropose a novel way to represent human-object interactions for egocentricaction anticipation. We propose a novel transformer variant to modelinteractions by computing the change in the appearance of objects and humanhands due to the execution of the actions and use those changes to refine thevideo representation. Specifically, we model interactions between hands andobjects using Spatial Cross-Attention (SCA) and further infuse contextualinformation using Trajectory Cross-Attention to obtain environment-refinedinteraction tokens. Using these tokens, we construct an interaction-centricvideo representation for action anticipation. We term our model InAViT whichachieves state-of-the-art action anticipation performance on large-scaleegocentric datasets EPICKTICHENS100 (EK100) and EGTEA Gaze+. InAViT outperformsother visual transformer-based methods including object-centric videorepresentation. On the EK100 evaluation server, InAViT is the top-performingmethod on the public leaderboard (at the time of submission) where itoutperforms the second-best model by 3.3% on mean-top5 recall.</description><author>Debaditya Roy, Ramanathan Rajendiran, Basura Fernando</author><pubDate>Thu, 11 Jan 2024 15:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14154v7</guid></item><item><title>An attempt to generate new bridge types from latent space of PixelCNN</title><link>http://arxiv.org/abs/2401.05964v1</link><description>Try to generate new bridge types using generative artificial intelligencetechnology. Using symmetric structured image dataset of three-span beam bridge,arch bridge, cable-stayed bridge and suspension bridge , based on Pythonprogramming language, TensorFlow and Keras deep learning platform framework ,PixelCNN is constructed and trained. The model can capture the statisticalstructure of the images and calculate the probability distribution of the nextpixel when the previous pixels are given. From the obtained latent spacesampling, new bridge types different from the training dataset can begenerated. PixelCNN can organically combine different structural components onthe basis of human original bridge types, creating new bridge types that have acertain degree of human original ability. Autoregressive models cannotunderstand the meaning of the sequence, while multimodal models combineregression and autoregressive models to understand the sequence. Multimodalmodels should be the way to achieve artificial general intelligence in thefuture.</description><author>Hongjun Zhang</author><pubDate>Thu, 11 Jan 2024 15:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05964v1</guid></item><item><title>MS23D: : A 3D Object Detection Method Using Multi-Scale Semantic Feature Points to Construct 3D Feature Layer</title><link>http://arxiv.org/abs/2308.16518v5</link><description>LiDAR point clouds can effectively depict the motion and posture of objectsin three-dimensional space. Many studies accomplish the 3D object detection byvoxelizing point clouds. However, in autonomous driving scenarios, the sparsityand hollowness of point clouds create some difficulties for voxel-basedmethods. The sparsity of point clouds makes it challenging to describe thegeometric features of objects. The hollowness of point clouds posesdifficulties for the aggregation of 3D features. We propose a two-stage 3Dobject detection framework, called MS23D. (1) We propose a method using voxelfeature points from multi-branch to construct the 3D feature layer. Using voxelfeature points from different branches, we construct a relatively compact 3Dfeature layer with rich semantic features. Additionally, we propose adistance-weighted sampling method, reducing the loss of foreground pointscaused by downsampling and allowing the 3D feature layer to retain moreforeground points. (2) In response to the hollowness of point clouds, wepredict the offsets between deep-level feature points and the object'scentroid, making them as close as possible to the object's centroid. Thisenables the aggregation of these feature points with abundant semanticfeatures. For feature points from shallow-level, we retain them on the object'ssurface to describe the geometric features of the object. To validate ourapproach, we evaluated its effectiveness on both the KITTI and ONCE datasets.</description><author>Yongxin Shao, Aihong Tan, Binrui Wang, Tianhong Yan, Zhetao Sun, Yiyang Zhang, Jiaxin Liu</author><pubDate>Thu, 11 Jan 2024 15:02:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16518v5</guid></item><item><title>Machine Learning Insides OptVerse AI Solver: Design Principles and Applications</title><link>http://arxiv.org/abs/2401.05960v1</link><description>In an era of digital ubiquity, efficient resource management anddecision-making are paramount across numerous industries. To this end, wepresent a comprehensive study on the integration of machine learning (ML)techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate thescarcity of real-world mathematical programming instances, and to surpass thecapabilities of traditional optimization techniques. We showcase our methodsfor generating complex SAT and MILP instances utilizing generative models thatmirror multifaceted structures of real-world problem. Furthermore, we introducea training framework leveraging augmentation policies to maintain solvers'utility in dynamic environments. Besides the data generation and augmentation,our proposed approaches also include novel ML-driven policies for personalizedsolver strategies, with an emphasis on applications like graph convolutionalnetworks for initial basis selection and reinforcement learning for advancedpresolving and cut selection. Additionally, we detail the incorporation ofstate-of-the-art parameter tuning algorithms which markedly elevate solverperformance. Compared with traditional solvers such as Gurobi and SCIP, ourML-augmented OptVerse AI Solver demonstrates superior speed and precisionacross both established benchmarks and real-world scenarios, reinforcing thepractical imperative and effectiveness of machine learning techniques inmathematical programming solvers.</description><author>Xijun Li, Fangzhou Zhu, Hui-Ling Zhen, Weilin Luo, Meng Lu, Yimin Huang, Zhenan Fan, Zirui Zhou, Yufei Kuang, Zhihai Wang, Zijie Geng, Yang Li, Haoyang Liu, Zhiwu An, Muming Yang, Jianshu Li, Jie Wang, Junchi Yan, Defeng Sun, Tao Zhong, Yong Zhang, Jia Zeng, Mingxuan Yuan, Jianye Hao, Jun Yao, Kun Mao</author><pubDate>Thu, 11 Jan 2024 15:02:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05960v1</guid></item><item><title>Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal</title><link>http://arxiv.org/abs/2212.13023v2</link><description>Most deep-learning-based continuous sign language recognition (CSLR) modelsshare a similar backbone consisting of a visual module, a sequential module,and an alignment module. However, due to limited training samples, aconnectionist temporal classification loss may not train such CSLR backbonessufficiently. In this work, we propose three auxiliary tasks to enhance theCSLR backbones. The first task enhances the visual module, which is sensitiveto the insufficient training problem, from the perspective of consistency.Specifically, since the information of sign languages is mainly included insigners' facial expressions and hand movements, a keypoint-guided spatialattention module is developed to enforce the visual module to focus oninformative regions, i.e., spatial attention consistency. Second, noticing thatboth the output features of the visual and sequential modules represent thesame sentence, to better exploit the backbone's power, a sentence embeddingconsistency constraint is imposed between the visual and sequential modules toenhance the representation power of both features. We name the CSLR modeltrained with the above auxiliary tasks as consistency-enhanced CSLR, whichperforms well on signer-dependent datasets in which all signers appear duringboth training and testing. To make it more robust for the signer-independentsetting, a signer removal module based on feature disentanglement is furtherproposed to remove signer information from the backbone. Extensive ablationstudies are conducted to validate the effectiveness of these auxiliary tasks.More remarkably, with a transformer-based backbone, our model achievesstate-of-the-art or competitive performance on five benchmarks, PHOENIX-2014,PHOENIX-2014-T, PHOENIX-2014-SI, CSL, and CSL-Daily. Code and Models areavailable at https://github.com/2000ZRL/LCSA_C2SLR_SRM.</description><author>Ronglai Zuo, Brian Mak</author><pubDate>Thu, 11 Jan 2024 14:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.13023v2</guid></item><item><title>Transparency in Sleep Staging: Deep Learning Method for EEG Sleep Stage Classification with Model Interpretability</title><link>http://arxiv.org/abs/2309.07156v3</link><description>Automated Sleep stage classification using raw single channel EEG is acritical tool for sleep quality assessment and disorder diagnosis. However,modelling the complexity and variability inherent in this signal is achallenging task, limiting their practicality and effectiveness in clinicalsettings. To mitigate these challenges, this study presents an end-to-end deeplearning (DL) model which integrates squeeze and excitation blocks within theresidual network to extract features and stacked Bi-LSTM to understand complextemporal dependencies. A distinctive aspect of this study is the adaptation ofGradCam for sleep staging, marking the first instance of an explainable DLmodel in this domain with alignment of its decision-making with sleep expert'sinsights. We evaluated our model on the publically available datasets(SleepEDF-20, SleepEDF-78, and SHHS), achieving Macro-F1 scores of 82.5, 78.9,and 81.9, respectively. Additionally, a novel training efficiency enhancementstrategy was implemented by increasing stride size, leading to 8x fastertraining times with minimal impact on performance. Comparative analysesunderscore our model outperforms all existing baselines, indicating itspotential for clinical usage.</description><author>Shivam Sharma, Suvadeep Maiti, S. Mythirayee, Srijithesh Rajendran, Raju Surampudi Bapi</author><pubDate>Thu, 11 Jan 2024 14:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07156v3</guid></item><item><title>Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning</title><link>http://arxiv.org/abs/2309.03581v3</link><description>Hyperparameter optimization (HPO) is important to leverage the full potentialof machine learning (ML). In practice, users are often interested inmulti-objective (MO) problems, i.e., optimizing potentially conflictingobjectives, like accuracy and energy consumption. To tackle this, the vastmajority of MO-ML algorithms return a Pareto front of non-dominated machinelearning models to the user. Optimizing the hyperparameters of such algorithmsis non-trivial as evaluating a hyperparameter configuration entails evaluatingthe quality of the resulting Pareto front. In literature, there are knownindicators that assess the quality of a Pareto front (e.g., hypervolume, R2) byquantifying different properties (e.g., volume, proximity to a referencepoint). However, choosing the indicator that leads to the desired Pareto frontmight be a hard task for a user. In this paper, we propose a human-centeredinteractive HPO approach tailored towards multi-objective ML leveragingpreference learning to extract desiderata from users that guide theoptimization. Instead of relying on the user guessing the most suitableindicator for their needs, our approach automatically learns an appropriateindicator. Concretely, we leverage pairwise comparisons of distinct Paretofronts to learn such an appropriate quality indicator. Then, we optimize thehyperparameters of the underlying MO-ML algorithm towards this learnedindicator using a state-of-the-art HPO approach. In an experimental studytargeting the environmental impact of ML, we demonstrate that our approachleads to substantially better Pareto fronts compared to optimizing based on awrong indicator pre-selected by the user, and performs comparable in the caseof an advanced user knowing which indicator to pick.</description><author>Joseph Giovanelli, Alexander Tornede, Tanja Tornede, Marius Lindauer</author><pubDate>Thu, 11 Jan 2024 14:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03581v3</guid></item><item><title>Towards Redundancy-Free Sub-networks in Continual Learning</title><link>http://arxiv.org/abs/2312.00840v2</link><description>Catastrophic Forgetting (CF) is a prominent issue in continual learning.Parameter isolation addresses this challenge by masking a sub-network for eachtask to mitigate interference with old tasks. However, these sub-networks areconstructed relying on weight magnitude, which does not necessarily correspondto the importance of weights, resulting in maintaining unimportant weights andconstructing redundant sub-networks. To overcome this limitation, inspired byinformation bottleneck, which removes redundancy between adjacent networklayers, we propose \textbf{\underline{I}nformation \underline{B}ottleneck\underline{M}asked sub-network (IBM)} to eliminate redundancy withinsub-networks. Specifically, IBM accumulates valuable information into essentialweights to construct redundancy-free sub-networks, not only effectivelymitigating CF by freezing the sub-networks but also facilitating new taskstraining through the transfer of valuable knowledge. Additionally, IBMdecomposes hidden representations to automate the construction process and makeit flexible. Extensive experiments demonstrate that IBM consistentlyoutperforms state-of-the-art methods. Notably, IBM surpasses thestate-of-the-art parameter isolation method with a 70\% reduction in the numberof parameters within sub-networks and an 80\% decrease in training time.</description><author>Cheng Chen, Jingkuan Song, LianLi Gao, Heng Tao Shen</author><pubDate>Thu, 11 Jan 2024 14:44:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00840v2</guid></item><item><title>Human-Inspired Framework to Accelerate Reinforcement Learning</title><link>http://arxiv.org/abs/2303.08115v2</link><description>Reinforcement learning (RL) is crucial for data science decision-making butsuffers from sample inefficiency, particularly in real-world scenarios withcostly physical interactions. This paper introduces a novel human-inspiredframework to enhance RL algorithm sample efficiency. It achieves this byinitially exposing the learning agent to simpler tasks that progressivelyincrease in complexity, ultimately leading to the main task. This methodrequires no pre-training and involves learning simpler tasks for just oneiteration. The resulting knowledge can facilitate various transfer learningapproaches, such as value and policy transfer, without increasing computationalcomplexity. It can be applied across different goals, environments, and RLalgorithms, including value-based, policy-based, tabular, and deep RL methods.Experimental evaluations demonstrate the framework's effectiveness in enhancingsample efficiency, especially in challenging main tasks, demonstrated throughboth a simple Random Walk and more complex optimal control problems withconstraints.</description><author>Ali Beikmohammadi, Sindri Magnússon</author><pubDate>Thu, 11 Jan 2024 14:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08115v2</guid></item><item><title>LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase</title><link>http://arxiv.org/abs/2401.05952v1</link><description>With the remarkable development and widespread applications of large languagemodels (LLMs), the use of machine-generated text (MGT) is becoming increasinglycommon. This trend brings potential risks, particularly to the quality andcompleteness of information in fields such as news and education. Currentresearch predominantly addresses the detection of pure MGT without adequatelyaddressing mixed scenarios including AI-revised Human-Written Text (HWT) orhuman-revised MGT. To confront this challenge, we introduce mixcase, a novelconcept representing a hybrid text form involving both machine-generated andhuman-generated content. We collected mixcase instances generated from multipledaily text-editing scenarios and composed MixSet, the first dataset dedicatedto studying these mixed modification scenarios. We conduct experiments toevaluate the efficacy of popular MGT detectors, assessing their effectiveness,robustness, and generalization performance. Our findings reveal that existingdetectors struggle to identify mixcase as a separate class or MGT, particularlyin dealing with subtle modifications and style adaptability. This researchunderscores the urgent need for more fine-grain detectors tailored for mixcase,offering valuable insights for future research. Code and Models are availableat https://github.com/Dongping-Chen/MixSet.</description><author>Chujie Gao, Dongping Chen, Qihui Zhang, Yue Huang, Yao Wan, Lichao Sun</author><pubDate>Thu, 11 Jan 2024 14:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05952v1</guid></item><item><title>Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks</title><link>http://arxiv.org/abs/2401.05949v1</link><description>In-context learning, a paradigm bridging the gap between pre-training andfine-tuning, has demonstrated high efficacy in several NLP tasks, especially infew-shot settings. Unlike traditional fine-tuning methods, in-context learningadapts pre-trained models to unseen tasks without updating any parameters.Despite being widely applied, in-context learning is vulnerable to maliciousattacks. In this work, we raise security concerns regarding this paradigm. Ourstudies demonstrate that an attacker can manipulate the behavior of largelanguage models by poisoning the demonstration context, without the need forfine-tuning the model. Specifically, we have designed a new backdoor attackmethod, named ICLAttack, to target large language models based on in-contextlearning. Our method encompasses two types of attacks: poisoning demonstrationexamples and poisoning prompts, which can make models behave in accordance withpredefined intentions. ICLAttack does not require additional fine-tuning toimplant a backdoor, thus preserving the model's generality. Furthermore, thepoisoned examples are correctly labeled, enhancing the natural stealth of ourattack method. Extensive experimental results across several language models,ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness ofour attack method, exemplified by a high average attack success rate of 95.0%across the three datasets on OPT models. Our findings highlight thevulnerabilities of language models, and we hope this work will raise awarenessof the possible security threats associated with in-context learning.</description><author>Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen</author><pubDate>Thu, 11 Jan 2024 14:38:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05949v1</guid></item><item><title>UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation</title><link>http://arxiv.org/abs/2312.08952v2</link><description>Multi-object tracking (MOT) in video sequences remains a challenging task,especially in scenarios with significant camera movements. This is becausetargets can drift considerably on the image plane, leading to erroneoustracking outcomes. Addressing such challenges typically requires supplementaryappearance cues or Camera Motion Compensation (CMC). While these strategies areeffective, they also introduce a considerable computational burden, posingchallenges for real-time MOT. In response to this, we introduce UCMCTrack, anovel motion model-based tracker robust to camera movements. Unlikeconventional CMC that computes compensation parameters frame-by-frame,UCMCTrack consistently applies the same compensation parameters throughout avideo sequence. It employs a Kalman filter on the ground plane and introducesthe Mapped Mahalanobis Distance (MMD) as an alternative to the traditionalIntersection over Union (IoU) distance measure. By leveraging projectedprobability distributions on the ground plane, our approach efficientlycaptures motion patterns and adeptly manages uncertainties introduced byhomography projections. Remarkably, UCMCTrack, relying solely on motion cues,achieves state-of-the-art performance across a variety of challenging datasets,including MOT17, MOT20, DanceTrack and KITTI. More details and code areavailable at https://github.com/corfyi/UCMCTrack</description><author>Kefu Yi, Kai Luo, Xiaolei Luo, Jiangui Huang, Hao Wu, Rongdong Hu, Wei Hao</author><pubDate>Thu, 11 Jan 2024 14:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08952v2</guid></item><item><title>EarthPT: a time series foundation model for Earth Observation</title><link>http://arxiv.org/abs/2309.07207v2</link><description>We introduce EarthPT -- an Earth Observation (EO) pretrained transformer.EarthPT is a 700 million parameter decoding transformer foundation modeltrained in an autoregressive self-supervised manner and developed specificallywith EO use-cases in mind. We demonstrate that EarthPT is an effectiveforecaster that can accurately predict future pixel-level surface reflectancesacross the 400-2300 nm range well into the future. For example, forecasts ofthe evolution of the Normalised Difference Vegetation Index (NDVI) have atypical error of approximately 0.05 (over a natural range of -1 -&gt; 1) at thepixel level over a five month test set horizon, out-performing simplephase-folded models based on historical averaging. We also demonstrate thatembeddings learnt by EarthPT hold semantically meaningful information and couldbe exploited for downstream tasks such as highly granular, dynamic land useclassification. Excitingly, we note that the abundance of EO data provides uswith -- in theory -- quadrillions of training tokens. Therefore, if we assumethat EarthPT follows neural scaling laws akin to those derived for LargeLanguage Models (LLMs), there is currently no data-imposed limit to scalingEarthPT and other similar `Large Observation Models.'</description><author>Michael J. Smith, Luke Fleming, James E. Geach</author><pubDate>Thu, 11 Jan 2024 14:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07207v2</guid></item><item><title>A Unified Approach to Controlling Implicit Regularization via Mirror Descent</title><link>http://arxiv.org/abs/2306.13853v2</link><description>Inspired by the remarkable success of large neural networks, there has beensignificant interest in understanding the generalization performance ofover-parameterized models. Substantial efforts have been invested incharacterizing how optimization algorithms impact generalization through their"preferred" solutions, a phenomenon commonly referred to as implicitregularization. In particular, it has been argued that gradient descent (GD)induces an implicit $\ell_2$-norm regularization in regression andclassification problems. However, the implicit regularization of differentalgorithms are confined to either a specific geometry or a particular class oflearning problems, indicating a gap in a general approach for controlling theimplicit regularization. To address this, we present a unified approach usingmirror descent (MD), a notable generalization of GD, to control implicitregularization in both regression and classification settings. Morespecifically, we show that MD with the general class of homogeneous potentialfunctions converges in direction to a generalized maximum-margin solution forlinear classification problems, thereby answering a long-standing question inthe classification setting. Further, we show that MD can be implementedefficiently and enjoys fast convergence under suitable conditions. Throughcomprehensive experiments, we demonstrate that MD is a versatile method toproduce learned models with different regularizers, which in turn havedifferent generalization performances.</description><author>Haoyuan Sun, Khashayar Gatmiry, Kwangjun Ahn, Navid Azizan</author><pubDate>Thu, 11 Jan 2024 14:35:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13853v2</guid></item><item><title>Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments</title><link>http://arxiv.org/abs/2401.05946v1</link><description>Despite their stellar performance on a wide range of tasks, includingin-context tasks only revealed during inference, vanilla transformers andvariants trained for next-token predictions (a) do not learn an explicit worldmodel of their environment which can be flexibly queried and (b) cannot be usedfor planning or navigation. In this paper, we consider partially observedenvironments (POEs), where an agent receives perceptually aliased observationsas it navigates, which makes path planning hard. We introduce a transformerwith (multiple) discrete bottleneck(s), TDB, whose latent codes learn acompressed representation of the history of observations and actions. Aftertraining a TDB to predict the future observation(s) given the history, weextract interpretable cognitive maps of the environment from its activebottleneck(s) indices. These maps are then paired with an external solver tosolve (constrained) path planning problems. First, we show that a TDB trainedon POEs (a) retains the near perfect predictive performance of a vanillatransformer or an LSTM while (b) solving shortest path problems exponentiallyfaster. Second, a TDB extracts interpretable representations from textdatasets, while reaching higher in-context accuracy than vanilla sequencemodels. Finally, in new POEs, a TDB (a) reaches near-perfect in-contextaccuracy, (b) learns accurate in-context cognitive maps (c) solves in-contextpath planning problems.</description><author>Antoine Dedieu, Wolfgang Lehrach, Guangyao Zhou, Dileep George, Miguel Lázaro-Gredilla</author><pubDate>Thu, 11 Jan 2024 14:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05946v1</guid></item><item><title>An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival</title><link>http://arxiv.org/abs/2203.09438v2</link><description>To compare alternative taxi schedules and to compute them, as well as toprovide insights into an upcoming taxi trip to drivers and passengers, theduration of a trip or its Estimated Time of Arrival (ETA) is predicted. Toreach a high prediction precision, machine learning models for ETA are state ofthe art. One yet unexploited option to further increase prediction precision isto combine multiple ETA models into an ensemble. While an increase ofprediction precision is likely, the main drawback is that the predictions madeby such an ensemble become less transparent due to the sophisticated ensemblearchitecture. One option to remedy this drawback is to apply eXplainableArtificial Intelligence (XAI). The contribution of this paper is three-fold.First, we combine multiple machine learning models from our previous work forETA into a two-level ensemble model - a stacked ensemble model - which on itsown is novel; therefore, we can outperform previous state-of-the-art staticroute-free ETA approaches. Second, we apply existing XAI methods to explain thefirst- and second-level models of the ensemble. Third, we propose three joiningmethods for combining the first-level explanations with the second-level ones.Those joining methods enable us to explain stacked ensembles for regressiontasks. An experimental evaluation shows that the ETA models correctly learnedthe importance of those input features driving the prediction.</description><author>Sören Schleibaum, Jörg P. Müller, Monika Sester</author><pubDate>Thu, 11 Jan 2024 14:29:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.09438v2</guid></item><item><title>Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs</title><link>http://arxiv.org/abs/2401.05940v1</link><description>Large Language Models (LLMs) have shown remarkable capabilities in processingboth natural and programming languages, which have enabled various applicationsin software engineering, such as requirement engineering, code generation, andsoftware testing. However, existing code generation benchmarks do notnecessarily assess the code understanding performance of LLMs, especially forthe subtle inconsistencies that may arise between code and its semanticsdescribed in natural language. In this paper, we propose a novel method to systematically assess the codeunderstanding performance of LLMs, particularly focusing on subtle differencesbetween code and its descriptions, by introducing code mutations to existingcode generation datasets. Code mutations are small changes that alter thesemantics of the original code, creating a mismatch with the natural languagedescription. We apply different types of code mutations, such as operatorreplacement and statement deletion, to generate inconsistent code-descriptionpairs. We then use these pairs to test the ability of LLMs to correctly detectthe inconsistencies. We propose a new LLM testing method, called Mutation-based ConsistencyTesting (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 andGPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, whichconsists of six programming languages (Python, C++, Java, Go, JavaScript, andRust). We compare the performance of the LLMs across different types of codemutations and programming languages and analyze the results. We find that theLLMs show significant variation in their code understanding performance andthat they have different strengths and weaknesses depending on the mutationtype and language.</description><author>Ziyu Li, Donghwan Shin</author><pubDate>Thu, 11 Jan 2024 14:27:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05940v1</guid></item><item><title>DREQ: Document Re-Ranking Using Entity-based Query Understanding</title><link>http://arxiv.org/abs/2401.05939v1</link><description>While entity-oriented neural IR models have advanced significantly, theyoften overlook a key nuance: the varying degrees of influence individualentities within a document have on its overall relevance. Addressing this gap,we present DREQ, an entity-oriented dense document re-ranking model. Uniquely,we emphasize the query-relevant entities within a document's representationwhile simultaneously attenuating the less relevant ones, thus obtaining aquery-specific entity-centric document representation. We then combine thisentity-centric document representation with the text-centric representation ofthe document to obtain a "hybrid" representation of the document. We learn arelevance score for the document using this hybrid representation. Using fourlarge-scale benchmarks, we show that DREQ outperforms state-of-the-art neuraland non-neural re-ranking methods, highlighting the effectiveness of ourentity-oriented representation approach.</description><author>Shubham Chatterjee, Iain Mackie, Jeff Dalton</author><pubDate>Thu, 11 Jan 2024 14:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05939v1</guid></item><item><title>Fovea Transformer: Efficient Long-Context Modeling with Structured Fine-to-Coarse Attention</title><link>http://arxiv.org/abs/2311.07102v2</link><description>The quadratic complexity of self-attention in Transformers has hindered theprocessing of long text. To alleviate this problem, previous works haveproposed to sparsify the attention matrix, taking advantage of the observationthat crucial information about a token can be derived from its neighbors. Thesemethods typically combine one or another form of local attention and globalattention. Such combinations introduce abrupt changes in contextual granularitywhen going from local to global, which may be undesirable. We believe that asmoother transition could potentially enhance model's ability to capturelong-context dependencies. In this study, we introduce Fovea Transformer, along-context focused transformer that addresses the challenges of capturingglobal dependencies while maintaining computational efficiency. To achievethis, we construct a multi-scale tree from the input sequence, and userepresentations of context tokens with a progressively coarser granularity inthe tree, as their distance to the query token increases. We evaluate our modelon three long-context summarization tasks\footnote{Our code is publiclyavailable at: \textit{https://github.com/ZiweiHe/Fovea-Transformer}}. Itachieves state-of-the-art performance on two of them, and competitive resultson the third with mixed improvement and setback of the evaluation metrics.</description><author>Ziwei He, Jian Yuan, Le Zhou, Jingwen Leng, Bo Jiang</author><pubDate>Thu, 11 Jan 2024 14:24:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07102v2</guid></item><item><title>Trinary Decision Trees for handling missing data</title><link>http://arxiv.org/abs/2309.03561v2</link><description>This paper introduces the Trinary decision tree, an algorithm designed toimprove the handling of missing data in decision tree regressors andclassifiers. Unlike other approaches, the Trinary decision tree does not assumethat missing values contain any information about the response. Boththeoretical calculations on estimator bias and numerical illustrations usingreal data sets are presented to compare its performance with establishedalgorithms in different missing data scenarios (Missing Completely at Random(MCAR), and Informative Missingness (IM)). Notably, the Trinary treeoutperforms its peers in MCAR settings, especially when data is only missingout-of-sample, while lacking behind in IM settings. A hybrid model, theTrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes(MIA) approach, shows robust performance in all types of missingness. Despitethe potential drawback of slower training speed, the Trinary tree offers apromising and more accurate method of handling missing data in decision treealgorithms.</description><author>Henning Zakrisson</author><pubDate>Thu, 11 Jan 2024 14:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03561v2</guid></item><item><title>Combining Normalizing Flows and Quasi-Monte Carlo</title><link>http://arxiv.org/abs/2401.05934v1</link><description>Recent advances in machine learning have led to the development of newmethods for enhancing Monte Carlo methods such as Markov chain Monte Carlo(MCMC) and importance sampling (IS). One such method is normalizing flows,which use a neural network to approximate a distribution by evaluating itpointwise. Normalizing flows have been shown to improve the performance of MCMCand IS. On the other side, (randomized) quasi-Monte Carlo methods are used toperform numerical integration. They replace the random sampling of Monte Carloby a sequence which cover the hypercube more uniformly, resulting in betterconvergence rates for the error that plain Monte Carlo. In this work, wecombine these two methods by using quasi-Monte Carlo to sample the initialdistribution that is transported by the flow. We demonstrate through numericalexperiments that this combination can lead to an estimator with significantlylower variance than if the flow was sampled with a classic Monte Carlo.</description><author>Charly Andral</author><pubDate>Thu, 11 Jan 2024 14:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05934v1</guid></item><item><title>Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?</title><link>http://arxiv.org/abs/2401.05933v1</link><description>With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDSepidemic in the Philippines is the one that is spreading the quickest in thewestern Pacific. Although the full effects of COVID-19 on HIV services anddevelopment are still unknown, it is predicted that such disruptions could leadto a significant increase in HIV casualties. Therefore, the nation needs somemodeling and forecasting techniques to foresee the spread pattern and enhancethe governments prevention, treatment, testing, and care program. In thisstudy, the researcher uses Multilayer Perceptron Neural Network to forecasttime series during the period when the COVID-19 pandemic strikes the nation,using statistics taken from the HIV/AIDS and ART Registry of the Philippines.After training, validation, and testing of data, the study finds that thepredicted cumulative cases in the nation by 2030 will reach 145,273.Additionally, there is very little difference between observed and anticipatedHIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as wellas a greater coefficient of determination. Further research revealed that thePhilippines seems far from achieving Sustainable Development Goal 3 of Project2030 due to an increase in the nations rate of new HIV infections. Despite thedetrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, thePhilippine government, under the Marcos administration, must continue to adhereto the United Nations 90-90-90 targets by enhancing its ART program andensuring that all vital health services are readily accessible and available.</description><author>Sales G. Aribe Jr., Bobby D. Gerardo, Ruji P. Medina</author><pubDate>Thu, 11 Jan 2024 14:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05933v1</guid></item><item><title>DiffDA: a diffusion model for weather-scale data assimilation</title><link>http://arxiv.org/abs/2401.05932v1</link><description>The generation of initial conditions via accurate data assimilation iscrucial for reliable weather forecasting and climate modeling. We propose theDiffDA as a machine learning based data assimilation method capable ofassimilating atmospheric variables using predicted states and sparseobservations. We adapt the pretrained GraphCast weather forecast model as adenoising diffusion model. Our method applies two-phase conditioning: on thepredicted state during both training and inference, and on sparse observationsduring inference only. As a byproduct, this strategy also enables thepost-processing of predictions into the future, for which no observations areavailable.Through experiments based on a reanalysis dataset, we have verifiedthat our method can produce assimilated global atmospheric data consistent withobservations at 0.25degree resolution. The experiments also show that theinitial conditions that are generated via our approach can be used for forecastmodels with a loss of lead time of at most 24 hours when compared to initialconditions of state-of-the-art data assimilation suites. This enables to applythe method to real world applications such as the creation of reanalysisdatasets with autoregressive data assimilation.</description><author>Langwen Huang, Lukas Gianinazzi, Yuejiang Yu, Peter D. Dueben, Torsten Hoefler</author><pubDate>Thu, 11 Jan 2024 14:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05932v1</guid></item><item><title>SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully</title><link>http://arxiv.org/abs/2401.05930v1</link><description>Large language models (LLMs) demonstrate great performance in textgeneration. However, LLMs are still suffering from hallucinations. In thiswork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),to help LLMs decode more truthfully. SH2 is based on a simple fact rooted ininformation theory that for an LLM, the tokens predicted with lowerprobabilities are prone to be more informative than others. Our analysis showsthat the tokens assigned with lower probabilities by an LLM are more likely tobe closely related to factual information, such as nouns, proper nouns, andadjectives. Therefore, we propose to ''highlight'' the factual information byselecting the tokens with the lowest probabilities and concatenating them tothe original context, thus forcing the model to repeatedly read and hesitate onthese tokens before generation. During decoding, we also adopt contrastivedecoding to emphasize the difference in the output probabilities brought by thehesitation. Experimental results demonstrate that our SH2, requiring noadditional data or models, can effectively help LLMs elicit factual knowledgeand distinguish hallucinated contexts. Significant and consistent improvementsare achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.</description><author>Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin</author><pubDate>Thu, 11 Jan 2024 14:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05930v1</guid></item><item><title>Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback</title><link>http://arxiv.org/abs/2401.05928v1</link><description>An emotional support conversation system aims to alleviate users' emotionaldistress and assist them in addressing their challenges. To generate supportiveresponses, it is critical to consider multiple factors such as empathy, supportstrategies, and response coherence, as established in prior methods.Nonetheless, previous models occasionally generate unhelpful responses, whichintend to provide support but display counterproductive effects. According topsychology and communication theories, poor performance in just onecontributing factor might cause a response to be unhelpful. From the modeltraining perspective, since these models have not been exposed to unhelpfulresponses during their training phase, they are unable to distinguish if thetokens they generate might result in unhelpful responses during inference. Toaddress this issue, we introduce a novel model-agnostic framework namedmitigating unhelpfulness with multifaceted AI feedback for emotional support(Muffin). Specifically, Muffin employs a multifaceted AI feedback module toassess the helpfulness of responses generated by a specific model withconsideration of multiple factors. Using contrastive learning, it then reducesthe likelihood of the model generating unhelpful responses compared to thehelpful ones. Experimental results demonstrate that Muffin effectivelymitigates the generation of unhelpful responses while slightly increasingresponse fluency and relevance.</description><author>Jiashuo Wang, Chunpu Xu, Chak Tou Leong, Wenjie Li, Jing Li</author><pubDate>Thu, 11 Jan 2024 14:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05928v1</guid></item><item><title>Heterogeneous Generative Knowledge Distillation with Masked Image Modeling</title><link>http://arxiv.org/abs/2309.09571v2</link><description>Small CNN-based models usually require transferring knowledge from a largemodel before they are deployed in computationally resource-limited edgedevices. Masked image modeling (MIM) methods achieve great success in variousvisual tasks but remain largely unexplored in knowledge distillation forheterogeneous deep models. The reason is mainly due to the significantdiscrepancy between the Transformer-based large model and the CNN-based smallnetwork. In this paper, we develop the first Heterogeneous Generative KnowledgeDistillation (H-GKD) based on MIM, which can efficiently transfer knowledgefrom large Transformer models to small CNN-based models in a generativeself-supervised fashion. Our method builds a bridge between Transformer-basedmodels and CNNs by training a UNet-style student with sparse convolution, whichcan effectively mimic the visual representation inferred by a teacher overmasked modeling. Our method is a simple yet effective learning paradigm tolearn the visual representation and distribution of data from heterogeneousteacher models, which can be pre-trained using advanced generative methods.Extensive experiments show that it adapts well to various models and sizes,consistently achieving state-of-the-art performance in image classification,object detection, and semantic segmentation tasks. For example, in the Imagenet1K dataset, H-GKD improves the accuracy of Resnet50 (sparse) from 76.98% to80.01%.</description><author>Ziming Wang, Shumin Han, Xiaodi Wang, Jing Hao, Xianbin Cao, Baochang Zhang</author><pubDate>Thu, 11 Jan 2024 14:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09571v2</guid></item><item><title>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians</title><link>http://arxiv.org/abs/2401.05925v1</link><description>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), amethod for compact 3D-consistent scene segmentation at fast rendering speedwith only RGB images input. Previous NeRF-based 3D segmentation methods haverelied on implicit or voxel neural scene representation and ray-marching volumerendering which are time consuming. Recent 3D Gaussian Splatting significantlyimproves the rendering speed, however, existing Gaussians-based segmentationmethods(eg: Gaussian Grouping) fail to provide compact segmentation masksespecially in zero-shot segmentation, which is mainly caused by the lack ofrobustness and compactness for straightforwardly assigning learnable parametersto each Gaussian when encountering inconsistent 2D machine-generated labels.Our method aims to achieve compact and reliable zero-shot scene segmentationswiftly by mapping fused spatial and semantically meaningful features for eachGaussian point with a shallow decoding network. Specifically, our methodfirstly optimizes Gaussian points' position, convariance and color attributesunder the supervision of RGB images. After Gaussian Locating, we distillmulti-scale DINO features extracted from images through unprojection to eachGaussian, which is then incorporated with spatial features from the fast pointfeatures processing network, i.e. RandLA-Net. Then the shallow decoding MLP isapplied to the multi-scale fused features to obtain compact segmentation.Experimental results show that our model can perform high-quality zero-shotscene segmentation, as our model outperforms other segmentation methods on bothsemantic and panoptic segmentation task, meanwhile consumes approximately only10% segmenting time compared to NeRF-based segmentation. Code and more resultswill be available at https://David-Dou.github.io/CoSSegGaussians</description><author>Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</author><pubDate>Thu, 11 Jan 2024 14:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05925v1</guid></item><item><title>Fine-Tuning Language Models with Just Forward Passes</title><link>http://arxiv.org/abs/2305.17333v3</link><description>Fine-tuning language models (LMs) has yielded success on diverse downstreamtasks, but as LMs grow in size, backpropagation requires a prohibitively largeamount of memory. Zeroth-order (ZO) methods can in principle estimate gradientsusing only two forward passes but are theorized to be catastrophically slow foroptimizing large models. In this work, we propose a memory-efficientzerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operatein-place, thereby fine-tuning LMs with the same memory footprint as inference.For example, with a single A100 80GB GPU, MeZO can train a 30-billion parametermodel, whereas fine-tuning with backpropagation can train only a 2.7B LM withthe same budget. We conduct comprehensive experiments across model types(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks(classification, multiple-choice, and generation). Our results demonstrate that(1) MeZO significantly outperforms in-context learning and linear probing; (2)MeZO achieves comparable performance to fine-tuning with backpropagation acrossmultiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reductionin our implementation; (3) MeZO is compatible with both full-parameter andparameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZOcan effectively optimize non-differentiable objectives (e.g., maximizingaccuracy or F1). We support our empirical findings with theoretical insights,highlighting how adequate pre-training and task prompts enable MeZO tofine-tune huge models, despite classical ZO analyses suggesting otherwise.</description><author>Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, Sanjeev Arora</author><pubDate>Thu, 11 Jan 2024 13:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17333v3</guid></item><item><title>Holistic Processing of Colour Images Using Novel Quaternion-Valued Wavelets on the Plane</title><link>http://arxiv.org/abs/2308.16875v2</link><description>Recently, novel quaternion-valued wavelets on the plane were constructedusing an optimisation approach. These wavelets are compactly supported, smooth,orthonormal, non-separable and truly quaternionic. However, they have not beentested in application. In this paper, we introduce a methodology fordecomposing and reconstructing colour images using quaternionic wavelet filtersassociated to recently developed quaternion-valued wavelets on the plane. Weinvestigate its applicability in compression, enhancement, segmentation, anddenoising of colour images. Our results demonstrate these wavelets as promisingtools for an end-to-end quaternion processing of colour images.</description><author>Neil D. Dizon, Jeffrey A. Hogan</author><pubDate>Thu, 11 Jan 2024 13:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16875v2</guid></item><item><title>Style Aligned Image Generation via Shared Attention</title><link>http://arxiv.org/abs/2312.02133v2</link><description>Large-scale Text-to-Image (T2I) models have rapidly gained prominence acrosscreative fields, generating visually compelling outputs from textual prompts.However, controlling these models to ensure consistent style remainschallenging, with existing methods necessitating fine-tuning and manualintervention to disentangle content and style. In this paper, we introduceStyleAligned, a novel technique designed to establish style alignment among aseries of generated images. By employing minimal `attention sharing' during thediffusion process, our method maintains style consistency across images withinT2I models. This approach allows for the creation of style-consistent imagesusing a reference style through a straightforward inversion operation. Ourmethod's evaluation across diverse styles and text prompts demonstrateshigh-quality synthesis and fidelity, underscoring its efficacy in achievingconsistent style across various inputs.</description><author>Amir Hertz, Andrey Voynov, Shlomi Fruchter, Daniel Cohen-Or</author><pubDate>Thu, 11 Jan 2024 13:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02133v2</guid></item><item><title>How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes</title><link>http://arxiv.org/abs/2401.05914v1</link><description>Question generation (QG) is a natural language processing task with anabundance of potential benefits and use cases in the educational domain. Inorder for this potential to be realized, QG systems must be designed andvalidated with pedagogical needs in mind. However, little research has assessedor designed QG approaches with the input from real teachers or students. Thispaper applies a large language model-based QG approach where questions aregenerated with learning goals derived from Bloom's taxonomy. The automaticallygenerated questions are used in multiple experiments designed to assess howteachers use them in practice. The results demonstrate that teachers prefer towrite quizzes with automatically generated questions, and that such quizzeshave no loss in quality compared to handwritten versions. Further, severalmetrics indicate that automatically generated questions can even improve thequality of the quizzes created, showing the promise for large scale use of QGin the classroom setting.</description><author>Sabina Elkins, Ekaterina Kochmar, Jackie C. K. Cheung, Iulian Serban</author><pubDate>Thu, 11 Jan 2024 13:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05914v1</guid></item><item><title>Prompt-based mental health screening from social media text</title><link>http://arxiv.org/abs/2401.05912v1</link><description>This article presents a method for prompt-based mental health screening froma large and noisy dataset of social media text. Our method uses GPT 3.5.prompting to distinguish publications that may be more relevant to the task,and then uses a straightforward bag-of-words text classifier to predict actualuser labels. Results are found to be on pair with a BERT mixture of expertsclassifier, and incurring only a fraction of its computational costs.</description><author>Wesley Ramos dos Santos, Ivandre Paraboni</author><pubDate>Thu, 11 Jan 2024 13:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05912v1</guid></item><item><title>EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge</title><link>http://arxiv.org/abs/2401.05908v1</link><description>With large training datasets and massive amounts of computing sources, largelanguage models (LLMs) achieve remarkable performance in comprehensive andgenerative ability. Based on those powerful LLMs, the model fine-tuned withdomain-specific datasets posseses more specialized knowledge and thus is morepractical like medical LLMs. However, the existing fine-tuned medical LLMs arelimited to general medical knowledge with English language. Fordisease-specific problems, the model's response is inaccurate and sometimeseven completely irrelevant, especially when using a language other thanEnglish. In this work, we focus on the particular disease of Epilepsy withJapanese language and introduce a customized LLM termed as EpilepsyLLM. Ourmodel is trained from the pre-trained LLM by fine-tuning technique usingdatasets from the epilepsy domain. The datasets contain knowledge of basicinformation about disease, common treatment methods and drugs, and importantnotes in life and work. The experimental results demonstrate that EpilepsyLLMcan provide more reliable and specialized medical knowledge responses.</description><author>Xuyang Zhao, Qibin Zhao, Toshihisa Tanaka</author><pubDate>Thu, 11 Jan 2024 13:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05908v1</guid></item><item><title>Riesz feature representation: scale equivariant scattering network for classification tasks</title><link>http://arxiv.org/abs/2307.08467v2</link><description>Scattering networks yield powerful and robust hierarchical image descriptorswhich do not require lengthy training and which work well with very fewtraining data. However, they rely on sampling the scale dimension. Hence, theybecome sensitive to scale variations and are unable to generalize to unseenscales. In this work, we define an alternative feature representation based onthe Riesz transform. We detail and analyze the mathematical foundations behindthis representation. In particular, it inherits scale equivariance from theRiesz transform and completely avoids sampling of the scale dimension.Additionally, the number of features in the representation is reduced by afactor four compared to scattering networks. Nevertheless, our representationperforms comparably well for texture classification with an interestingaddition: scale equivariance. Our method yields superior performance whendealing with scales outside of those covered by the training dataset. Theusefulness of the equivariance property is demonstrated on the digitclassification task, where accuracy remains stable even for scales four timeslarger than the one chosen for training. As a second example, we considerclassification of textures.</description><author>Tin Barisin, Jesus Angulo, Katja Schladitz, Claudia Redenbach</author><pubDate>Thu, 11 Jan 2024 13:38:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08467v2</guid></item><item><title>Efficient Image Deblurring Networks based on Diffusion Models</title><link>http://arxiv.org/abs/2401.05907v1</link><description>This article introduces a sliding window model for defocus deblurring thatachieves the best performance to date with extremely low memory usage. NamedSwintormer, the method utilizes a diffusion model to generate latent priorfeatures that assist in restoring more detailed images. It also extends thesliding window strategy to specialized Transformer blocks for efficientinference. Additionally, we have further optimized Multiply-Accumulateoperations (Macs). Compared to the currently top-performing GRL method, ourSwintormer model drastically reduces computational complexity from 140.35 GMACsto 8.02 GMacs, while also improving the Signal-to-Noise Ratio (SNR) for defocusdeblurring from 27.04 dB to 27.07 dB. This new method allows for the processingof higher resolution images on devices with limited memory, significantlyexpanding potential application scenarios. The article concludes with anablation study that provides an in-depth analysis of the impact of each networkmodule on final performance. The source code and model will be available at thefollowing website: https://github.com/bnm6900030/swintormer.</description><author>Kang Chen, Yuanjie Liu</author><pubDate>Thu, 11 Jan 2024 13:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05907v1</guid></item></channel></rss>