<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 13 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Learning Disentangled Avatars with Hybrid 3D Representations</title><link>http://arxiv.org/abs/2309.06441v1</link><description>Tremendous efforts have been made to learn animatable and photorealistichuman avatars. Towards this end, both explicit and implicit 3D representationsare heavily studied for a holistic modeling and capture of the whole human(e.g., body, clothing, face and hair), but neither representation is an optimalchoice in terms of representation efficacy since different parts of the humanavatar have different modeling desiderata. For example, meshes are generallynot suitable for modeling clothing and hair. Motivated by this, we presentDisentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit3D representations. DELTA takes a monocular RGB video as input, and produces ahuman avatar with separate body and clothing/hair layers. Specifically, wedemonstrate two important applications for DELTA. For the first one, weconsider the disentanglement of the human body and clothing and in the second,we disentangle the face and hair. To do so, DELTA represents the body or facewith an explicit mesh-based parametric 3D model and the clothing or hair withan implicit neural radiance field. To make this possible, we design anend-to-end differentiable renderer that integrates meshes into volumetricrendering, enabling DELTA to learn directly from monocular videos without any3D supervision. Finally, we show that how these two applications can be easilycombined to model full-body avatars, such that the hair, face, body andclothing can be fully disentangled yet jointly rendered. Such a disentanglementenables hair and clothing transfer to arbitrary body shapes. We empiricallyvalidate the effectiveness of DELTA's disentanglement by demonstrating itspromising performance on disentangled reconstruction, virtual clothing try-onand hairstyle transfer. To facilitate future research, we also release anopen-sourced pipeline for the study of hybrid human avatar modeling.</description><author>Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, Michael J. Black</author><pubDate>Tue, 12 Sep 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06441v1</guid></item><item><title>LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning</title><link>http://arxiv.org/abs/2309.06440v1</link><description>Dexterous manipulation has been a long-standing challenge in robotics. Whilemachine learning techniques have shown some promise, results have largely beencurrently limited to simulation. This can be mostly attributed to the lack ofsuitable hardware. In this paper, we present LEAP Hand, a low-cost dexterousand anthropomorphic hand for machine learning research. In contrast to previoushands, LEAP Hand has a novel kinematic structure that allows maximal dexterityregardless of finger pose. LEAP Hand is low-cost and can be assembled in 4hours at a cost of 2000 USD from readily available parts. It is capable ofconsistently exerting large torques over long durations of time. We show thatLEAP Hand can be used to perform several manipulation tasks in the real world-- from visual teleoperation to learning from passive video data and sim2real.LEAP Hand significantly outperforms its closest competitor Allegro Hand in allour experiments while being 1/8th of the cost. We release detailed assemblyinstructions, the Sim2Real pipeline and a development platform with useful APIson our website at https://leap-hand.github.io/</description><author>Kenneth Shaw, Ananye Agarwal, Deepak Pathak</author><pubDate>Tue, 12 Sep 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06440v1</guid></item><item><title>Attention De-sparsification Matters: Inducing Diversity in Digital Pathology Representation Learning</title><link>http://arxiv.org/abs/2309.06439v1</link><description>We propose DiRL, a Diversity-inducing Representation Learning technique forhistopathology imaging. Self-supervised learning techniques, such ascontrastive and non-contrastive approaches, have been shown to learn rich andeffective representations of digitized tissue samples with limited pathologistsupervision. Our analysis of vanilla SSL-pretrained models' attentiondistribution reveals an insightful observation: sparsity in attention, i.e,models tends to localize most of their attention to some prominent patterns inthe image. Although attention sparsity can be beneficial in natural images dueto these prominent patterns being the object of interest itself, this can besub-optimal in digital pathology; this is because, unlike natural images,digital pathology scans are not object-centric, but rather a complex phenotypeof various spatially intermixed biological components. Inadequatediversification of attention in these complex images could result in crucialinformation loss. To address this, we leverage cell segmentation to denselyextract multiple histopathology-specific representations, and then propose aprior-guided dense pretext task for SSL, designed to match the multiplecorresponding representations between the views. Through this, the model learnsto attend to various components more closely and evenly, thus inducing adequatediversification in attention for capturing context rich representations.Through quantitative and qualitative analysis on multiple tasks across cancertypes, we demonstrate the efficacy of our method and observe that the attentionis more globally distributed.</description><author>Saarthak Kapse, Srijan Das, Jingwei Zhang, Rajarsi R. Gupta, Joel Saltz, Dimitris Samaras, Prateek Prasanna</author><pubDate>Tue, 12 Sep 2023 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06439v1</guid></item><item><title>Exploring Non-additive Randomness on ViT against Query-Based Black-Box Attacks</title><link>http://arxiv.org/abs/2309.06438v1</link><description>Deep Neural Networks can be easily fooled by small and imperceptibleperturbations. The query-based black-box attack (QBBA) is able to create theperturbations using model output probabilities of image queries requiring noaccess to the underlying models. QBBA poses realistic threats to real-worldapplications. Recently, various types of robustness have been explored todefend against QBBA. In this work, we first taxonomize the stochastic defensestrategies against QBBA. Following our taxonomy, we propose to explorenon-additive randomness in models to defend against QBBA. Specifically, wefocus on underexplored Vision Transformers based on their flexiblearchitectures. Extensive experiments show that the proposed defense approachachieves effective defense, without much sacrifice in performance.</description><author>Jindong Gu, Fangyun Wei, Philip Torr, Han Hu</author><pubDate>Tue, 12 Sep 2023 18:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06438v1</guid></item><item><title>Unveiling the potential of large language models in generating semantic and cross-language clones</title><link>http://arxiv.org/abs/2309.06424v1</link><description>Semantic and Cross-language code clone generation may be useful for codereuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model haspotential in such clone generation as GPT is used for text generation. Whendevelopers copy/paste codes from Stack Overflow (SO) or within a system, theremight be inconsistent changes leading to unexpected behaviours. Similarly, ifsomeone possesses a code snippet in a particular programming language but seeksequivalent functionality in a different language, a semantic cross-languagecode clone generation approach could provide valuable assistance.In this study,using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 modelcould help generate semantic and cross-language clone variants for a givenfragment.We have comprised a diverse set of code fragments and assessed GPT-3sperformance in generating code variants.Through extensive experimentation andanalysis, where 9 judges spent 158 hours to validate, we investigate themodel's ability to produce accurate and semantically correct variants. Ourfindings shed light on GPT-3's strengths in code generation, offering insightsinto the potential applications and challenges of using advanced languagemodels in software development. Our quantitative analysis yields compellingresults. In the realm of semantic clones, GPT-3 attains an impressive accuracyof 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering.Furthermore, the model shines in transcending linguistic confines, boasting anexceptional 91.25% accuracy in generating cross-language clones</description><author>Palash R. Roy, Ajmain I. Alam, Farouq Al-omari, Banani Roy, Chanchal K. Roy, Kevin A. Schneider</author><pubDate>Tue, 12 Sep 2023 18:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06424v1</guid></item><item><title>Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey</title><link>http://arxiv.org/abs/2212.04634v3</link><description>Storytelling and narrative are fundamental to human experience, intertwinedwith our social and cultural engagement. As such, researchers have longattempted to create systems that can generate stories automatically. In recentyears, powered by deep learning and massive data resources, automatic storygeneration has shown significant advances. However, considerable challenges,like the need for global coherence in generated stories, still hampergenerative models from reaching the same storytelling ability as humannarrators. To tackle these challenges, many studies seek to inject structuredknowledge into the generation process, which is referred to as structuredknowledge-enhanced story generation. Incorporating external knowledge canenhance the logical coherence among story events, achieve better knowledgegrounding, and alleviate over-generalization and repetition problems instories. This survey provides the latest and comprehensive review of thisresearch field: (i) we present a systematic taxonomy regarding how existingmethods integrate structured knowledge into story generation; (ii) we summarizeinvolved story corpora, structured knowledge datasets, and evaluation metrics;(iii) we give multidimensional insights into the challenges ofknowledge-enhanced story generation and cast light on promising directions forfuture study.</description><author>Yuxin Wang, Jieru Lin, Zhiwei Yu, Wei Hu, Börje F. Karlsson</author><pubDate>Tue, 12 Sep 2023 18:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04634v3</guid></item><item><title>AGMDT: Virtual Staining of Renal Histology Images with Adjacency-Guided Multi-Domain Transfer</title><link>http://arxiv.org/abs/2309.06421v1</link><description>Renal pathology, as the gold standard of kidney disease diagnosis, requiresdoctors to analyze a serial of tissue slices stained by H\&amp;E staining andspecial staining like Masson, PASM, and PAS, respectively. These specialstaining methods are costly, time-consuming, and hard to standardize for wideuse especially in primary hospitals. Advances of supervised learning methodscan virtually convert H\&amp;E images into special staining images, but thepixel-to-pixel alignment is hard to achieve for training. As contrast,unsupervised learning methods regarding different stains as different styletransferring domains can use unpaired data, but they ignore the spatialinter-domain correlations and thus decrease the trustworthiness of structuraldetails for diagnosis. In this paper, we propose a novel virtual stainingframework AGMDT to translate images into other domains by avoiding pixel-levelalignment and meanwhile utilizing the correlations among adjacent tissueslices. We first build a high-quality multi-domain renal histological datasetwhere each specimen case comprises a series of slices stained in various ways.Based on it, the proposed framework AGMDT discovers patch-level aligned pairsacross the serial slices of multi-domains through glomerulus detection andbipartite graph matching, and utilizes such correlations to supervise theend-to-end model for multi-domain staining transformation. Experimental resultsshow that the proposed AGMDT achieves a good balance between the precisepixel-level alignment and unpaired domain transfer by exploiting correlationsacross multi-domain serial pathological slices, and outperforms thestate-of-the-art methods in both quantitative measure and morphologicaldetails.</description><author>Tao Ma, Chao Zhang, Min Lu, Lin Luo</author><pubDate>Tue, 12 Sep 2023 18:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06421v1</guid></item><item><title>On Computationally Efficient Learning of Exponential Family Distributions</title><link>http://arxiv.org/abs/2309.06413v1</link><description>We consider the classical problem of learning, with arbitrary accuracy, thenatural parameters of a $k$-parameter truncated \textit{minimal} exponentialfamily from i.i.d. samples in a computationally and statistically efficientmanner. We focus on the setting where the support as well as the naturalparameters are appropriately bounded. While the traditional maximum likelihoodestimator for this class of exponential family is consistent, asymptoticallynormal, and asymptotically efficient, evaluating it is computationally hard. Inthis work, we propose a novel loss function and a computationally efficientestimator that is consistent as well as asymptotically normal under mildconditions. We show that, at the population level, our method can be viewed asthe maximum likelihood estimation of a re-parameterized distribution belongingto the same class of exponential family. Further, we show that our estimatorcan be interpreted as a solution to minimizing a particular Bregman score aswell as an instance of minimizing the \textit{surrogate} likelihood. We alsoprovide finite sample guarantees to achieve an error (in $\ell_2$-norm) of$\alpha$ in the parameter estimation with sample complexity $O({\sfpoly}(k)/\alpha^2)$. Our method achives the order-optimal sample complexity of$O({\sf log}(k)/\alpha^2)$ when tailored for node-wise-sparse Markov randomfields. Finally, we demonstrate the performance of our estimator via numericalexperiments.</description><author>Abhin Shah, Devavrat Shah, Gregory W. Wornell</author><pubDate>Tue, 12 Sep 2023 18:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06413v1</guid></item><item><title>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</title><link>http://arxiv.org/abs/2203.01881v5</link><description>Self-supervised learning (SSL) has shown impressive results in downstreamclassification tasks. However, there is limited work in understanding theirfailure modes and interpreting their learned representations. In this paper, westudy the representation space of state-of-the-art self-supervised modelsincluding SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins.Without the use of class label information, we discover discriminative featuresthat correspond to unique physical attributes in images, present mostly incorrectly-classified representations. Using these features, we can compress therepresentation space by up to 40% without significantly affecting linearclassification performance. We then propose Self-Supervised RepresentationQuality Score (or Q-Score), an unsupervised score that can reliably predict ifa given sample is likely to be mis-classified during linear evaluation,achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score canalso be used as a regularization term on pre-trained encoders to remedylow-quality representations. Fine-tuning with Q-Score regularization can boostthe linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and3.7% on ImageNet-1K compared to their baselines. Finally, using gradientheatmaps and Salient ImageNet masks, we define a metric to quantify theinterpretability of each representation. We show that discriminative featuresare strongly correlated to core attributes and, enhancing these featuresthrough Q-score regularization makes SSL representations more interpretable.</description><author>Neha Kalibhat, Kanika Narang, Hamed Firooz, Maziar Sanjabi, Soheil Feizi</author><pubDate>Tue, 12 Sep 2023 17:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.01881v5</guid></item><item><title>Point-SLAM: Dense Neural Point Cloud-based SLAM</title><link>http://arxiv.org/abs/2304.04278v3</link><description>We propose a dense neural simultaneous localization and mapping (SLAM)approach for monocular RGBD input which anchors the features of a neural scenerepresentation in a point cloud that is iteratively generated in aninput-dependent data-driven manner. We demonstrate that both tracking andmapping can be performed with the same point-based neural scene representationby minimizing an RGBD-based re-rendering loss. In contrast to recent denseneural SLAM methods which anchor the scene features in a sparse grid, ourpoint-based approach allows dynamically adapting the anchor point density tothe information density of the input. This strategy reduces runtime and memoryusage in regions with fewer details and dedicates higher point density toresolve fine details. Our approach performs either better or competitive toexisting dense neural RGBD SLAM methods in tracking, mapping and renderingaccuracy on the Replica, TUM-RGBD and ScanNet datasets. The source code isavailable at https://github.com/eriksandstroem/Point-SLAM.</description><author>Erik Sandström, Yue Li, Luc Van Gool, Martin R. Oswald</author><pubDate>Tue, 12 Sep 2023 17:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04278v3</guid></item><item><title>You Only Label Once: 3D Box Adaptation from Point Cloud to Image via Semi-Supervised Learning</title><link>http://arxiv.org/abs/2211.09302v2</link><description>The image-based 3D object detection task expects that the predicted 3Dbounding box has a ``tightness'' projection (also referred to as cuboid), whichfits the object contour well on the image while still keeping the geometricattribute on the 3D space, e.g., physical dimension, pairwise orthogonal, etc.These requirements bring significant challenges to the annotation. Simplyprojecting the Lidar-labeled 3D boxes to the image leads to non-trivialmisalignment, while directly drawing a cuboid on the image cannot access theoriginal 3D information. In this work, we propose a learning-based 3D boxadaptation approach that automatically adjusts minimum parameters of the360$^{\circ}$ Lidar 3D bounding box to perfectly fit the image appearance ofpanoramic cameras. With only a few 2D boxes annotation as guidance during thetraining phase, our network can produce accurate image-level cuboid annotationswith 3D properties from Lidar boxes. We call our method ``you only labelonce'', which means labeling on the point cloud once and automatically adaptingto all surrounding cameras. As far as we know, we are the first to focus onimage-level cuboid refinement, which balances the accuracy and efficiency welland dramatically reduces the labeling effort for accurate cuboid annotation.Extensive experiments on the public Waymo and NuScenes datasets show that ourmethod can produce human-level cuboid annotation on the image without needingmanual adjustment.</description><author>Jieqi Shi, Peiliang Li, Xiaozhi Chen, Shaojie Shen</author><pubDate>Tue, 12 Sep 2023 17:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09302v2</guid></item><item><title>Ensemble Mask Networks</title><link>http://arxiv.org/abs/2309.06382v1</link><description>Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learnmatrix-vector multiplication? This study introduces two mechanisms - flexiblemasking to take matrix inputs, and a unique network pruning to respect themask's dependency structure. Networks can approximate fixed operations such asmatrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating themechanisms introduced with applications towards litmus-testing dependencies orinteraction order in graph-based models.</description><author>Jonny Luntzel</author><pubDate>Tue, 12 Sep 2023 17:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06382v1</guid></item><item><title>Leveraging Large Language Models for Exploiting ASR Uncertainty</title><link>http://arxiv.org/abs/2309.04842v2</link><description>While large language models excel in a variety of natural language processing(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, theymust either rely on off-the-shelf automatic speech recognition (ASR) systemsfor transcription, or be equipped with an in-built speech modality. This workfocuses on the former scenario, where LLM's accuracy on SLU tasks isconstrained by the accuracy of a fixed ASR system on the spoken input.Specifically, we tackle speech-intent classification task, where a highword-error-rate can limit the LLM's ability to understand the spoken intent.Instead of chasing a high accuracy by designing complex or specializedarchitectures regardless of deployment costs, we seek to answer how far we cango without substantially changing the underlying ASR and LLM, which canpotentially be shared by multiple unrelated tasks. To this end, we proposeprompting the LLM with an n-best list of ASR hypotheses instead of only theerror-prone 1-best hypothesis. We explore prompt-engineering to explain theconcept of n-best lists to the LLM; followed by the finetuning of Low-RankAdapters on the downstream tasks. Our approach using n-best lists proves to beeffective on a device-directed speech detection task as well as on a keywordspotting task, where systems using n-best list prompts outperform those using1-best ASR hypothesis; thus paving the way for an efficient method to exploitASR uncertainty via LLMs for speech-based applications.</description><author>Pranay Dighe, Yi Su, Shangshang Zheng, Yunshu Liu, Vineet Garg, Xiaochuan Niu, Ahmed Tewfik</author><pubDate>Tue, 12 Sep 2023 17:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04842v2</guid></item><item><title>InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation</title><link>http://arxiv.org/abs/2309.06380v1</link><description>Diffusion models have revolutionized text-to-image generation with itsexceptional quality and creativity. However, its multi-step sampling process isknown to be slow, often requiring tens of inference steps to obtainsatisfactory results. Previous attempts to improve its sampling speed andreduce computational costs through distillation have been unsuccessful inachieving a functional one-step model. In this paper, we explore a recentmethod called Rectified Flow, which, thus far, has only been applied to smalldatasets. The core of Rectified Flow lies in its \emph{reflow} procedure, whichstraightens the trajectories of probability flows, refines the coupling betweennoises and images, and facilitates the distillation process with studentmodels. We propose a novel text-conditioned pipeline to turn Stable Diffusion(SD) into an ultra-fast one-step model, in which we find reflow plays acritical role in improving the assignment between noise and images. Leveragingour new pipeline, we create, to the best of our knowledge, the first one-stepdiffusion-based text-to-image generator with SD-level image quality, achievingan FID (Frechet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassingthe previous state-of-the-art technique, progressive distillation, by asignificant margin ($37.2$ $\rightarrow$ $23.3$ in FID). By utilizing anexpanded network with 1.7B parameters, we further improve the FID to $22.4$. Wecall our one-step models \emph{InstaFlow}. On MS COCO 2014-30k, InstaFlowyields an FID of $13.1$ in just $0.09$ second, the best in $\leq 0.1$ secondregime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably,the training of InstaFlow only costs 199 A100 GPU days. Projectpage:~\url{https://github.com/gnobitab/InstaFlow}.</description><author>Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, Qiang Liu</author><pubDate>Tue, 12 Sep 2023 17:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06380v1</guid></item><item><title>Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI</title><link>http://arxiv.org/abs/2309.06379v1</link><description>With recent advances in Generative AI, it is becoming easier to automaticallymanipulate 3D models. However, current methods tend to apply edits to modelsglobally, which risks compromising the intended functionality of the 3D modelwhen fabricated in the physical world. For example, modifying functionalsegments in 3D models, such as the base of a vase, could break the originalfunctionality of the model, thus causing the vase to fall over. We introduce amethod for automatically segmenting 3D models into functional and aestheticelements. This method allows users to selectively modify aesthetic segments of3D models, without affecting the functional segments. To develop this method wefirst create a taxonomy of functionality in 3D models by qualitativelyanalyzing 1000 models sourced from a popular 3D printing repository,Thingiverse. With this taxonomy, we develop a semi-automatic classificationmethod to decompose 3D models into functional and aesthetic elements. Wepropose a system called Style2Fab that allows users to selectively stylize 3Dmodels without compromising their functionality. We evaluate the effectivenessof our classification method compared to human-annotated data, and demonstratethe utility of Style2Fab with a user study to show that functionality-awaresegmentation helps preserve model functionality.</description><author>Faraz Faruqi, Ahmed Katary, Tarik Hasic, Amira Abdel-Rahman, Nayeemur Rahman, Leandra Tejedor, Mackenzie Leake, Megan Hofmann, Stefanie Mueller</author><pubDate>Tue, 12 Sep 2023 17:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06379v1</guid></item><item><title>Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates</title><link>http://arxiv.org/abs/2210.08494v2</link><description>K-FAC (arXiv:1503.05671, arXiv:1602.01407) is a tractable implementation ofNatural Gradient (NG) for Deep Learning (DL), whose bottleneck is computing theinverses of the so-called ``Kronecker-Factors'' (K-factors). RS-KFAC(arXiv:2206.15397) is a K-FAC improvement which provides a cheap way ofestimating the K-factors inverses. In this paper, we exploit the exponential-average construction paradigm ofthe K-factors, and use online numerical linear algebra techniques to propose aneven cheaper (but less accurate) way of estimating the K-factors inverses. Inparticular, we propose a K-factor inverse update which scales linearly in layersize. We also propose an inverse application procedure which scales linearly aswell (the one of K-FAC scales cubically and the one of RS-KFAC scalesquadratically). Overall, our proposed algorithm gives an approximate K-FACimplementation whose preconditioning part scales linearly in layer size(compare to cubic for K-FAC and quadratic for RS-KFAC). Importantly however,this update is only applicable in some circumstances (typically for all FClayers), unlike the RS-KFAC approach (arXiv:2206.15397). Numerical results show RS-KFAC's inversion error can be reduced with minimalCPU overhead by adding our proposed update to it. Based on the proposedprocedure, a correction to it, and RS-KFAC, we propose three practicalalgorithms for optimizing generic Deep Neural Nets. Numerical results show thattwo of these outperform RS-KFAC for any target test accuracy on CIFAR10classification with a slightly modified version of VGG16_bn. Our proposedalgorithms achieve 91$\%$ test accuracy faster than SENG (the state of artimplementation of empirical NG for DL; arXiv:2006.05924) but underperform itfor higher test-accuracy.</description><author>Constantin Octavian Puiu</author><pubDate>Tue, 12 Sep 2023 17:41:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08494v2</guid></item><item><title>Padding-free Convolution based on Preservation of Differential Characteristics of Kernels</title><link>http://arxiv.org/abs/2309.06370v1</link><description>Convolution is a fundamental operation in image processing and machinelearning. Aimed primarily at maintaining image size, padding is a keyingredient of convolution, which, however, can introduce undesirable boundaryeffects. We present a non-padding-based method for size-keeping convolutionbased on the preservation of differential characteristics of kernels. The mainidea is to make convolution over an incomplete sliding window "collapse" to alinear differential operator evaluated locally at its central pixel, which nolonger requires information from the neighbouring missing pixels. While theunderlying theory is rigorous, our final formula turns out to be simple: theconvolution over an incomplete window is achieved by convolving its nearestcomplete window with a transformed kernel. This formula is computationallylightweight, involving neither interpolation or extrapolation nor restrictionson image and kernel sizes. Our method favours data with smooth boundaries, suchas high-resolution images and fields from physics. Our experiments include: i)filtering analytical and non-analytical fields from computational physics and,ii) training convolutional neural networks (CNNs) for the tasks of imageclassification, semantic segmentation and super-resolution reconstruction. Inall these experiments, our method has exhibited visible superiority over thecompared ones.</description><author>Kuangdai Leng, Jeyan Thiyagalingam</author><pubDate>Tue, 12 Sep 2023 17:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06370v1</guid></item><item><title>Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages</title><link>http://arxiv.org/abs/2303.13592v4</link><description>While code-mixing is a common linguistic practice in many parts of the world,collecting high-quality and low-cost code-mixed data remains a challenge fornatural language processing (NLP) research. The recent proliferation of LargeLanguage Models (LLMs) compels one to ask: how capable are these systems ingenerating code-mixed data? In this paper, we explore prompting multilingualLLMs in a zero-shot manner to generate code-mixed data for seven languages inSouth East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,Tamil, and Singlish. We find that publicly available multilingualinstruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable ofproducing texts with phrases or clauses from different languages. ChatGPTexhibits inconsistent capabilities in generating code-mixed texts, wherein itsperformance varies depending on the prompt template and language pairing. Forinstance, ChatGPT generates fluent and natural Singlish texts (an English-basedcreole spoken in Singapore), but for English-Tamil language pair, the systemmostly produces grammatically incorrect or semantically meaningless utterances.Furthermore, it may erroneously introduce languages not specified in theprompt. Based on our investigation, existing multilingual LLMs exhibit a widerange of proficiency in code-mixed data generation for SEA languages. As such,we advise against using LLMs in this context without extensive human checks.</description><author>Zheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde, Skyler Wang, Arjun Subramonian, Holy Lovenia, Samuel Cahyawijaya, Genta Indra Winata, Lintang Sutawika, Jan Christian Blaise Cruz, Yin Lin Tan, Long Phan, Rowena Garcia, Thamar Solorio, Alham Fikri Aji</author><pubDate>Tue, 12 Sep 2023 17:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13592v4</guid></item><item><title>Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?</title><link>http://arxiv.org/abs/2308.01936v2</link><description>A hallmark of intelligence is the ability to use a familiar domain to makeinferences about a less familiar domain, known as analogical reasoning. In thisarticle, we delve into the performance of Large Language Models (LLMs) indealing with progressively complex analogies expressed in unstructured text. Wediscuss analogies at four distinct levels of complexity: lexical analogies,syntactic analogies, semantic analogies, and pragmatic analogies. As theanalogies become more complex, they require increasingly extensive, diverseknowledge beyond the textual content, unlikely to be found in the lexicalco-occurrence statistics that power LLMs. To address this, we discuss thenecessity of employing Neuro-symbolic AI techniques that combine statisticaland symbolic AI, informing the representation of unstructured text to highlightand augment relevant content, provide abstraction and guide the mappingprocess. Our knowledge-informed approach maintains the efficiency of LLMs whilepreserving the ability to explain analogies for pedagogical applications.</description><author>Thilini Wijesiriwardene, Amit Sheth, Valerie L. Shalin, Amitava Das</author><pubDate>Tue, 12 Sep 2023 17:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01936v2</guid></item><item><title>Tracking Everything Everywhere All at Once</title><link>http://arxiv.org/abs/2306.05422v2</link><description>We present a new test-time optimization method for estimating dense andlong-range motion from a video sequence. Prior optical flow or particle videotracking algorithms typically operate within limited temporal windows,struggling to track through occlusions and maintain global consistency ofestimated motion trajectories. We propose a complete and globally consistentmotion representation, dubbed OmniMotion, that allows for accurate, full-lengthmotion estimation of every pixel in a video. OmniMotion represents a videousing a quasi-3D canonical volume and performs pixel-wise tracking viabijections between local and canonical space. This representation allows us toensure global consistency, track through occlusions, and model any combinationof camera and object motion. Extensive evaluations on the TAP-Vid benchmark andreal-world footage show that our approach outperforms prior state-of-the-artmethods by a large margin both quantitatively and qualitatively. See ourproject page for more results: http://omnimotion.github.io/</description><author>Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, Noah Snavely</author><pubDate>Tue, 12 Sep 2023 17:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05422v2</guid></item><item><title>Cited Text Spans for Citation Text Generation</title><link>http://arxiv.org/abs/2309.06365v1</link><description>Automatic related work generation must ground their outputs to the content ofthe cited papers to avoid non-factual hallucinations, but due to the length ofscientific documents, existing abstractive approaches have conditioned only onthe cited paper \textit{abstracts}. We demonstrate that the abstract is notalways the most appropriate input for citation generation and that modelstrained in this way learn to hallucinate. We propose to condition instead onthe \textit{cited text span} (CTS) as an alternative to the abstract. Becausemanual CTS annotation is extremely time- and labor-intensive, we experimentwith automatic, ROUGE-based labeling of candidate CTS sentences, achievingsufficiently strong performance to substitute for expensive human annotations,and we propose a human-in-the-loop, keyword-based CTS retrieval approach thatmakes generating citation texts grounded in the full text of cited papers bothpromising and practical.</description><author>Xiangci Li, Yi-Hui Lee, Jessica Ouyang</author><pubDate>Tue, 12 Sep 2023 17:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06365v1</guid></item><item><title>PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</title><link>http://arxiv.org/abs/2304.11277v2</link><description>It is widely acknowledged that large models have the potential to deliversuperior performance across a broad range of domains. Despite the remarkableprogress made in the field of machine learning systems research, which hasenabled the development and exploration of large models, such abilities remainconfined to a small group of advanced users and industry leaders, resulting inan implicit technical barrier for the wider community to access and leveragethese technologies. In this paper, we introduce PyTorch Fully Sharded DataParallel (FSDP) as an industry-grade solution for large model training. FSDPhas been closely co-designed with several key PyTorch core components includingTensor implementation, dispatcher system, and CUDA memory caching allocator, toprovide non-intrusive user experiences and high training efficiency.Additionally, FSDP natively incorporates a range of techniques and settings tooptimize resource utilization across a variety of hardware configurations. Theexperimental results demonstrate that FSDP is capable of achieving comparableperformance to Distributed Data Parallel while providing support forsignificantly larger models with near-linear scalability in terms of TFLOPS.</description><author>Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, Shen Li</author><pubDate>Tue, 12 Sep 2023 17:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11277v2</guid></item><item><title>Learning to Predict Concept Ordering for Common Sense Generation</title><link>http://arxiv.org/abs/2309.06363v1</link><description>Prior work has shown that the ordering in which concepts are shown to acommonsense generator plays an important role, affecting the quality of thegenerated sentence. However, it remains a challenge to determine the optimalordering of a given set of concepts such that a natural sentence covering allthe concepts could be generated from a pretrained generator. To understand therelationship between the ordering of the input concepts and the quality of thegenerated sentences, we conduct a systematic study considering multiplelanguage models (LMs) and concept ordering strategies. We find that BART-largemodel consistently outperforms all other LMs considered in this study whenfine-tuned using the ordering of concepts as they appear in CommonGen trainingdata as measured using multiple evaluation metrics. Moreover, the largerGPT3-based large language models (LLMs) variants do not necessarily outperformmuch smaller LMs on this task, even when fine-tuned on task-specific trainingdata. Interestingly, human annotators significantly reorder input concept setswhen manually writing sentences covering those concepts, and this orderingprovides the best sentence generations independently of the LM used for thegeneration, outperforming a probabilistic concept ordering baseline</description><author>Tianhui Zhang, Danushka Bollegala, Bei Peng</author><pubDate>Tue, 12 Sep 2023 17:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06363v1</guid></item><item><title>High-Fidelity Eye Animatable Neural Radiance Fields for Human Face</title><link>http://arxiv.org/abs/2308.00773v3</link><description>Face rendering using neural radiance fields (NeRF) is a rapidly developingresearch area in computer vision. While recent methods primarily focus oncontrolling facial attributes such as identity and expression, they oftenoverlook the crucial aspect of modeling eyeball rotation, which holdsimportance for various downstream tasks. In this paper, we aim to learn a faceNeRF model that is sensitive to eye movements from multi-view images. Weaddress two key challenges in eye-aware face NeRF learning: how to effectivelycapture eyeball rotation for training and how to construct a manifold forrepresenting eyeball rotation. To accomplish this, we first fit FLAME, awell-established parametric face model, to the multi-view images consideringmulti-view consistency. Subsequently, we introduce a new Dynamic Eye-aware NeRF(DeNeRF). DeNeRF transforms 3D points from different views into a canonicalspace to learn a unified face NeRF model. We design an eye deformation fieldfor the transformation, including rigid transformation, e.g., eyeball rotation,and non-rigid transformation. Through experiments conducted on the ETH-XGazedataset, we demonstrate that our model is capable of generating high-fidelityimages with accurate eyeball rotation and non-rigid periocular deformation,even under novel viewing angles. Furthermore, we show that utilizing therendered images can effectively enhance gaze estimation performance.</description><author>Hengfei Wang, Zhongqun Zhang, Yihua Cheng, Hyung Jin Chang</author><pubDate>Tue, 12 Sep 2023 17:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00773v3</guid></item><item><title>GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization</title><link>http://arxiv.org/abs/2009.01745v3</link><description>This paper deals with a network of computing agents aiming to solve an onlineoptimization problem in a distributed fashion, i.e., by means of localcomputation and communication, without any central coordinator. We propose thegradient tracking with adaptive momentum estimation (GTAdam) distributedalgorithm, which combines a gradient tracking mechanism with first and secondorder momentum estimates of the gradient. The algorithm is analyzed in theonline setting for strongly convex cost functions with Lipschitz continuousgradients. We provide an upper bound for the dynamic regret given by a termrelated to the initial conditions and another term related to the temporalvariations of the objective functions. Moreover, a linear convergence rate isguaranteed in the static setup. The algorithm is tested on a time-varyingclassification problem, on a (moving) target localization problem, and in astochastic optimization setup from image classification. In these numericalexperiments from multi-agent learning, GTAdam outperforms state-of-the-artdistributed optimization methods.</description><author>Guido Carnevale, Francesco Farina, Ivano Notarnicola, Giuseppe Notarstefano</author><pubDate>Tue, 12 Sep 2023 17:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.01745v3</guid></item><item><title>Using Reed-Muller Codes for Classification with Rejection and Recovery</title><link>http://arxiv.org/abs/2309.06359v1</link><description>When deploying classifiers in the real world, users expect them to respond toinputs appropriately. However, traditional classifiers are not equipped tohandle inputs which lie far from the distribution they were trained on.Malicious actors can exploit this defect by making adversarial perturbationsdesigned to cause the classifier to give an incorrect output.Classification-with-rejection methods attempt to solve this problem by allowingnetworks to refuse to classify an input in which they have low confidence. Thisworks well for strongly adversarial examples, but also leads to the rejectionof weakly perturbed images, which intuitively could be correctly classified. Toaddress these issues, we propose Reed-Muller Aggregation Networks (RMAggNet), aclassifier inspired by Reed-Muller error-correction codes which can correct andreject inputs. This paper shows that RMAggNet can minimise incorrectness whilemaintaining good correctness over multiple adversarial attacks at differentperturbation budgets by leveraging the ability to correct errors in theclassification process. This provides an alternativeclassification-with-rejection method which can reduce the amount of additionalprocessing in situations where a small number of incorrect classifications arepermissible.</description><author>Daniel Fentham, David Parker, Mark Ryan</author><pubDate>Tue, 12 Sep 2023 17:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06359v1</guid></item><item><title>Robust Markov Decision Processes without Model Estimation</title><link>http://arxiv.org/abs/2302.01248v2</link><description>Robust Markov Decision Processes (MDPs) are receiving much attention inlearning a robust policy which is less sensitive to environment changes. Thereare an increasing number of works analyzing sample-efficiency of robust MDPs.However, there are two major barriers to applying robust MDPs in practice.First, most works study robust MDPs in a model-based regime, where thetransition probability needs to be estimated and requires a large amount ofmemories $\mathcal{O}(|\mathcal{S}|^2|\mathcal{A}|)$. Second, prior worktypically assumes a strong oracle to obtain the optimal solution as anintermediate step to solve robust MDPs. However, in practice, such an oracledoes not exist usually. To remove the oracle, we transform the original robustMDPs into an alternative form, which allows us to use stochastic gradientmethods to solve the robust MDPs. Moreover, we prove the alternative form stillplays a similar role as the original form. With this new formulation, we devisea sample-efficient algorithm to solve the robust MDPs in a model-free regime,which does not require an oracle and trades off a lower storage requirement$\mathcal{O}(|\mathcal{S}||\mathcal{A}|)$ with being able to generate samplesfrom a generative model or Markovian chain. Finally, we validate ourtheoretical findings via numerical experiments, showing the efficiency with thealternative form of robust MDPs.</description><author>Wenhao Yang, Han Wang, Tadashi Kozuno, Scott M. Jordan, Zhihua Zhang</author><pubDate>Tue, 12 Sep 2023 17:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01248v2</guid></item><item><title>Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors</title><link>http://arxiv.org/abs/2309.06349v1</link><description>Thompson sampling (TS) is one of the most popular and earliest algorithms tosolve stochastic multi-armed bandit problems. We consider a variant of TS,named $\alpha$-TS, where we use a fractional or $\alpha$-posterior($\alpha\in(0,1)$) instead of the standard posterior distribution. To computean $\alpha$-posterior, the likelihood in the definition of the standardposterior is tempered with a factor $\alpha$. For $\alpha$-TS we obtain bothinstance-dependent $\mathcal{O}\left(\sum_{k \neq i^*}\Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$and instance-independent $\mathcal{O}(\sqrt{KT\log K})$ frequentist regretbounds under very mild conditions on the prior and reward distributions, where$\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and thebest arms, and $C(\alpha)$ is a known constant. Both the sub-Gaussian andexponential family models satisfy our general conditions on the rewarddistribution. Our conditions on the prior distribution just require its densityto be positive, continuous, and bounded. We also establish anotherinstance-dependent regret upper bound that matches (up to constants) to that ofimproved UCB [Auer and Ortner, 2010]. Our regret analysis carefully combinesrecent theoretical developments in the non-asymptotic concentration analysisand Bernstein-von Mises type results for the $\alpha$-posterior distribution.Moreover, our analysis does not require additional structural properties suchas closed-form posteriors or conjugate priors.</description><author>Prateek Jaiswal, Debdeep Pati, Anirban Bhattacharya, Bani K. Mallick</author><pubDate>Tue, 12 Sep 2023 17:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06349v1</guid></item><item><title>Plant Disease Detection using Region-Based Convolutional Neural Network</title><link>http://arxiv.org/abs/2303.09063v2</link><description>Agriculture plays an important role in the food and economy of Bangladesh.The rapid growth of population over the years also has increased the demand forfood production. One of the major reasons behind low crop production isnumerous bacteria, virus and fungal plant diseases. Early detection of plantdiseases and proper usage of pesticides and fertilizers are vital forpreventing the diseases and boost the yield. Most of the farmers usegeneralized pesticides and fertilizers in the entire fields withoutspecifically knowing the condition of the plants. Thus the production costoftentimes increases, and, not only that, sometimes this becomes detrimental tothe yield. Deep Learning models are found to be very effective to automaticallydetect plant diseases from images of plants, thereby reducing the need forhuman specialists. This paper aims at building a lightweight deep learningmodel for predicting leaf disease in tomato plants. By modifying theregion-based convolutional neural network, we design an efficient and effectivemodel that demonstrates satisfactory empirical performance on a benchmarkdataset. Our proposed model can easily be deployed in a larger system wheredrones take images of leaves and these images will be fed into our model toknow the health condition.</description><author>Hasin Rehana, Muhammad Ibrahim, Md. Haider Ali</author><pubDate>Tue, 12 Sep 2023 17:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09063v2</guid></item><item><title>Band-gap regression with architecture-optimized message-passing neural networks</title><link>http://arxiv.org/abs/2309.06348v1</link><description>Graph-based neural networks and, specifically, message-passing neuralnetworks (MPNNs) have shown great potential in predicting physical propertiesof solids. In this work, we train an MPNN to first classify materials throughdensity functional theory data from the AFLOW database as being metallic orsemiconducting/insulating. We then perform a neural-architecture search toexplore the model architecture and hyperparameter space of MPNNs to predict theband gaps of the materials identified as non-metals. The parameters in thesearch include the number of message-passing steps, latent size, andactivation-function, among others. The top-performing models from the searchare pooled into an ensemble that significantly outperforms existing models fromthe literature. Uncertainty quantification is evaluated with Monte-CarloDropout and ensembling, with the ensemble method proving superior. The domainof applicability of the ensemble model is analyzed with respect to the crystalsystems, the inclusion of a Hubbard parameter in the density functionalcalculations, and the atomic species building up the materials.</description><author>Tim Bechtel, Daniel T. Speckhard, Jonathan Godwin, Claudia Draxl</author><pubDate>Tue, 12 Sep 2023 17:13:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06348v1</guid></item><item><title>Tradeoff of generalization error in unsupervised learning</title><link>http://arxiv.org/abs/2303.05718v2</link><description>Finding the optimal model complexity that minimizes the generalization error(GE) is a key issue of machine learning. For the conventional supervisedlearning, this task typically involves the bias-variance tradeoff: lowering thebias by making the model more complex entails an increase in the variance.Meanwhile, little has been studied about whether the same tradeoff exists forunsupervised learning. In this study, we propose that unsupervised learninggenerally exhibits a two-component tradeoff of the GE, namely the model errorand the data error -- using a more complex model reduces the model error at thecost of the data error, with the data error playing a more significant role fora smaller training dataset. This is corroborated by training the restrictedBoltzmann machine to generate the configurations of the two-dimensional Isingmodel at a given temperature and the totally asymmetric simple exclusionprocess with given entry and exit rates. Our results also indicate that theoptimal model tends to be more complex when the data to be learned are morecomplex.</description><author>Gilhan Kim, Hojun Lee, Junghyo Jo, Yongjoo Baek</author><pubDate>Tue, 12 Sep 2023 17:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05718v2</guid></item><item><title>JOSA: Joint surface-based registration with atlas construction enables accurate alignment of the brain geometry and function</title><link>http://arxiv.org/abs/2303.01592v3</link><description>Surface-based cortical registration is an important topic in medical imageanalysis and facilitates many downstream applications. Current approaches forcortical registration are mainly driven by geometric features, such as sulcaldepth and curvature, and often assume that registration of folding patternsleads to alignment of brain function. However, functional variability ofanatomically corresponding areas across subjects has been widely reported,particularly in higher-order cognitive areas. In this work, we present JOSA, anovel cortical registration framework that jointly models the mismatch betweengeometry and function while simultaneously learning an unbiasedpopulation-specific atlas. Using a semi-supervised training strategy, JOSAachieves superior registration performance in both geometry and functionwithout requiring functional data at inference. This learning framework can beextended to any auxiliary data to guide spherical registration that isavailable during training but is difficult or impossible to obtain duringinference, such as parcellations, architectonic identity, transcriptomicinformation, and molecular profiles.</description><author>Jian Li, Greta Tuckute, Evelina Fedorenko, Brian L. Edlow, Adrian V. Dalca, Bruce Fischl</author><pubDate>Tue, 12 Sep 2023 16:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01592v3</guid></item><item><title>Exploring Flat Minima for Domain Generalization with Large Learning Rates</title><link>http://arxiv.org/abs/2309.06337v1</link><description>Domain Generalization (DG) aims to generalize to arbitrary unseen domains. Apromising approach to improve model generalization in DG is the identificationof flat minima. One typical method for this task is SWAD, which involvesaveraging weights along the training trajectory. However, the success of weightaveraging depends on the diversity of weights, which is limited when trainingwith a small learning rate. Instead, we observe that leveraging a largelearning rate can simultaneously promote weight diversity and facilitate theidentification of flat regions in the loss landscape. However, employing alarge learning rate suffers from the convergence problem, which cannot beresolved by simply averaging the training weights. To address this issue, weintroduce a training strategy called Lookahead which involves the weightinterpolation, instead of average, between fast and slow weights. The fastweight explores the weight space with a large learning rate, which is notconverged while the slow weight interpolates with it to ensure the convergence.Besides, weight interpolation also helps identify flat minima by implicitlyoptimizing the local entropy loss that measures flatness. To further preventoverfitting during training, we propose two variants to regularize the trainingweight with weighted averaged weight or with accumulated history weight. Takingadvantage of this new perspective, our methods achieve state-of-the-artperformance on both classification and semantic segmentation domaingeneralization benchmarks. The code is available athttps://github.com/koncle/DG-with-Large-LR.</description><author>Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao</author><pubDate>Tue, 12 Sep 2023 16:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06337v1</guid></item><item><title>Grounded Language Acquisition From Object and Action Imagery</title><link>http://arxiv.org/abs/2309.06335v1</link><description>Deep learning approaches to natural language processing have made greatstrides in recent years. While these models produce symbols that convey vastamounts of diverse knowledge, it is unclear how such symbols are grounded indata from the world. In this paper, we explore the development of a privatelanguage for visual data representation by training emergent language (EL)encoders/decoders in both i) a traditional referential game environment and ii)a contrastive learning environment utilizing a within-class matching trainingparadigm. An additional classification layer utilizing neural machinetranslation and random forest classification was used to transform symbolicrepresentations (sequences of integer symbols) to class labels. These methodswere applied in two experiments focusing on object recognition and actionrecognition. For object recognition, a set of sketches produced by humanparticipants from real imagery was used (Sketchy dataset) and for actionrecognition, 2D trajectories were generated from 3D motion capture systems(MOVI dataset). In order to interpret the symbols produced for data in eachexperiment, gradient-weighted class activation mapping (Grad-CAM) methods wereused to identify pixel regions indicating semantic features which contributeevidence towards symbols in learned languages. Additionally, a t-distributedstochastic neighbor embedding (t-SNE) method was used to investigate embeddingslearned by CNN feature extractors.</description><author>James Robert Kubricht, Zhaoyuan Yang, Jianwei Qiu, Peter Henry Tu</author><pubDate>Tue, 12 Sep 2023 16:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06335v1</guid></item><item><title>ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models</title><link>http://arxiv.org/abs/2307.00398v2</link><description>Large-scale vision-language models (VLMs) like CLIP successfully findcorrespondences between images and text. Through the standard deterministicmapping process, an image or a text sample is mapped to a single vector in theembedding space. This is problematic: as multiple samples (images or text) canabstract the same concept in the physical world, deterministic embeddings donot reflect the inherent ambiguity in the embedding space. We propose ProbVLM,a probabilistic adapter that estimates probability distributions for theembeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hocmanner without needing large-scale datasets or computing. On four challengingdatasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate themulti-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantifythe calibration of embedding uncertainties in retrieval tasks and show thatProbVLM outperforms other methods. Furthermore, we propose active learning andmodel selection as two real-world downstream tasks for VLMs and show that theestimated uncertainty aids both tasks. Lastly, we present a novel technique forvisualizing the embedding distributions using a large-scale pre-trained latentdiffusion model. Code is available at https://github.com/ExplainableML/ProbVLM.</description><author>Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano Mancini, Zeynep Akata</author><pubDate>Tue, 12 Sep 2023 16:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00398v2</guid></item><item><title>Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction</title><link>http://arxiv.org/abs/2309.05406v2</link><description>Diffuse gliomas are malignant brain tumors that grow widespread through thebrain. The complex interactions between neoplastic cells and normal tissue, aswell as the treatment-induced changes often encountered, make glioma tumorgrowth modeling challenging. In this paper, we present a novel end-to-endnetwork capable of generating future tumor masks and realistic MRIs of how thetumor will look at any future time points for different treatment plans. Ourmodel is built upon cutting-edge diffusion probabilistic models anddeep-segmentation neural networks. We extended a diffusion model to includesequential multi-parametric MRI and treatment information as conditioning inputto guide the generative diffusion process. This allows us to estimate tumorgrowth at any given time point. We trained the model using real-worldpostoperative longitudinal MRI data with glioma tumor growth trajectoriesrepresented as tumor segmentation maps over time. The model has demonstratedpromising performance across a range of tasks, including the generation ofhigh-quality synthetic MRIs with tumor masks, time-series tumor segmentations,and uncertainty estimation. Combined with the treatment-aware generated MRIs,the tumor growth predictions with uncertainty estimates can provide usefulinformation for clinical decision-making.</description><author>Qinghui Liu, Elies Fuster-Garcia, Ivar Thokle Hovden, Donatas Sederevicius, Karoline Skogen, Bradley J MacIntosh, Edvard Grødem, Till Schellhorn, Petter Brandal, Atle Bjørnerud, Kyrre Eeg Emblem</author><pubDate>Tue, 12 Sep 2023 16:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05406v2</guid></item><item><title>Stability to Deformations of Manifold Filters and Manifold Neural Networks</title><link>http://arxiv.org/abs/2106.03725v4</link><description>The paper defines and studies manifold (M) convolutional filters and neuralnetworks (NNs). \emph{Manifold} filters and MNNs are defined in terms of theLaplace-Beltrami operator exponential and are such that \emph{graph} (G)filters and neural networks (NNs) are recovered as discrete approximations whenthe manifold is sampled. These filters admit a spectral representation which isa generalization of both the spectral representation of graph filters and thefrequency response of standard convolutional filters in continuous time. Themain technical contribution of the paper is to analyze the stability ofmanifold filters and MNNs to smooth deformations of the manifold. This analysisgeneralizes known stability properties of graph filters and GNNs and it is alsoa generalization of known stability properties of standard convolutionalfilters and neural networks in continuous time. The most important observationthat follows from this analysis is that manifold filters, same as graph filtersand standard continuous time filters, have difficulty discriminating highfrequency components in the presence of deformations. This is a challenge thatcan be ameliorated with the use of manifold, graph, or continuous time neuralnetworks. The most important practical consequence of this analysis is to shedlight on the behavior of graph filters and GNNs in large scale graphs.</description><author>Zhiyang Wang, Luana Ruiz, Alejandro Ribeiro</author><pubDate>Tue, 12 Sep 2023 16:45:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.03725v4</guid></item><item><title>FreeMan: Towards Benchmarking 3D Human Pose Estimation in the Wild</title><link>http://arxiv.org/abs/2309.05073v2</link><description>Estimating the 3D structure of the human body from natural scenes is afundamental aspect of visual perception. This task carries great importance forfields like AIGC and human-robot interaction. In practice, 3D human poseestimation in real-world settings is a critical initial step in solving thisproblem. However, the current datasets, often collected under controlledlaboratory conditions using complex motion capture equipment and unvaryingbackgrounds, are insufficient. The absence of real-world datasets is stallingthe progress of this crucial task. To facilitate the development of 3D poseestimation, we present FreeMan, the first large-scale, real-world multi-viewdataset. FreeMan was captured by synchronizing 8 smartphones across diversescenarios. It comprises 11M frames from 8000 sequences, viewed from differentperspectives. These sequences cover 40 subjects across 10 different scenarios,each with varying lighting conditions. We have also established an automated,precise labeling pipeline that allows for large-scale processing efficiently.We provide comprehensive evaluation baselines for a range of tasks, underliningthe significant challenges posed by FreeMan. Further evaluations of standardindoor/outdoor human sensing datasets reveal that FreeMan offers robustrepresentation transferability in real and complex scenes. FreeMan is nowpublicly available at https://wangjiongw.github.io/freeman.</description><author>Jiong Wang, Fengyu Yang, Wenbo Gou, Bingliang Li, Danqi Yan, Ailing Zeng, Yijun Gao, Junle Wang, Ruimao Zhang</author><pubDate>Tue, 12 Sep 2023 16:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05073v2</guid></item><item><title>SAMPLING: Scene-adaptive Hierarchical Multiplane Images Representation for Novel View Synthesis from a Single Image</title><link>http://arxiv.org/abs/2309.06323v1</link><description>Recent novel view synthesis methods obtain promising results for relativelysmall scenes, e.g., indoor environments and scenes with a few objects, but tendto fail for unbounded outdoor scenes with a single image as input. In thispaper, we introduce SAMPLING, a Scene-adaptive Hierarchical Multiplane ImagesRepresentation for Novel View Synthesis from a Single Image based on improvedmultiplane images (MPI). Observing that depth distribution varies significantlyfor unbounded outdoor scenes, we employ an adaptive-bins strategy for MPI toarrange planes in accordance with each scene image. To represent intricategeometry and multi-scale details, we further introduce a hierarchicalrefinement branch, which results in high-quality synthesized novel views. Ourmethod demonstrates considerable performance gains in synthesizing large-scaleunbounded outdoor scenes using a single image on the KITTI dataset andgeneralizes well to the unseen Tanks and Temples dataset. The code and modelswill be made public.</description><author>Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</author><pubDate>Tue, 12 Sep 2023 16:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06323v1</guid></item><item><title>Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks</title><link>http://arxiv.org/abs/2303.05445v2</link><description>Multi-armed bandits are extensively used to model sequential decision-making,making them ubiquitous in many real-life applications such as onlinerecommender systems and wireless networking. We consider a multi-agent settingwhere each agent solves their own bandit instance endowed with a different setof arms. Their goal is to minimize their group regret while collaborating viasome communication protocol over a given network. Previous literature on thisproblem only considered arm heterogeneity and networked agents separately. Inthis work, we introduce a setting that encompasses both features. For thisnovel setting, we first provide a rigorous regret analysis for a standardflooding protocol combined with the classic UCB policy. Then, to mitigate theissue of high communication costs incurred by flooding in complex networks, wepropose a new protocol called Flooding with Absorption (FwA). We provide atheoretical analysis of the resulting regret bound and discuss the advantagesof using FwA over flooding. Lastly, we experimentally verify on variousscenarios, including dynamic networks, that FwA leads to significantly lowercommunication costs despite minimal regret performance loss compared to othernetwork protocols.</description><author>Junghyun Lee, Laura Schmid, Se-Young Yun</author><pubDate>Tue, 12 Sep 2023 16:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05445v2</guid></item><item><title>Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning</title><link>http://arxiv.org/abs/2309.06315v1</link><description>A set of variables is the Markov blanket of a random variable if it containsall the information needed for predicting the variable. If the blanket cannotbe reduced without losing useful information, it is called a Markov boundary.Identifying the Markov boundary of a random variable is advantageous becauseall variables outside the boundary are superfluous. Hence, the Markov boundaryprovides an optimal feature set. However, learning the Markov boundary fromdata is challenging for two reasons. If one or more variables are removed fromthe Markov boundary, variables outside the boundary may start providinginformation. Conversely, variables within the boundary may stop providinginformation. The true role of each candidate variable is only manifesting whenthe Markov boundary has been identified. In this paper, we propose a newTsetlin Machine (TM) feedback scheme that supplements Type I and Type IIfeedback. The scheme introduces a novel Finite State Automaton - aContext-Specific Independence Automaton. The automaton learns which featuresare outside the Markov boundary of the target, allowing them to be pruned fromthe TM during learning. We investigate the new scheme empirically, showing howit is capable of exploiting context-specific independence to find Markovboundaries. Further, we provide a theoretical analysis of convergence. Ourapproach thus connects the field of Bayesian networks (BN) with TMs,potentially opening up for synergies when it comes to inference and learning,including TM-produced Bayesian knowledge bases and TM-based Bayesian inference.</description><author>Ole-Christoffer Granmo, Per-Arne Andersen, Lei Jiao, Xuan Zhang, Christian Blakely, Tor Tveit</author><pubDate>Tue, 12 Sep 2023 16:27:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06315v1</guid></item><item><title>Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle</title><link>http://arxiv.org/abs/2309.06313v1</link><description>It is difficult to perform 3D reconstruction from on-vehicle gathered videodue to the large forward motion of the vehicle. Even object detection and humansensing models perform significantly worse on onboard videos when compared tostandard benchmarks because objects often appear far away from the cameracompared to the standard object detection benchmarks, image quality is oftendecreased by motion blur and occlusions occur often. This has led to thepopularisation of traffic data-specific benchmarks. Recently Light DetectionAnd Ranging (LiDAR) sensors have become popular to directly estimate depthswithout the need to perform 3D reconstructions. However, LiDAR-based methodsstill lack in articulated human detection at a distance when compared toimage-based methods. We hypothesize that benchmarks targeted at articulatedhuman sensing from LiDAR data could bring about increased research in humansensing and prediction in traffic and could lead to improved traffic safety forpedestrians.</description><author>Maria Priisalu</author><pubDate>Tue, 12 Sep 2023 16:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06313v1</guid></item><item><title>AI4Food-NutritionFW: A Novel Framework for the Automatic Synthesis and Analysis of Eating Behaviours</title><link>http://arxiv.org/abs/2309.06308v1</link><description>Nowadays millions of images are shared on social media and web platforms. Inparticular, many of them are food images taken from a smartphone over time,providing information related to the individual's diet. On the other hand,eating behaviours are directly related to some of the most prevalent diseasesin the world. Exploiting recent advances in image processing and ArtificialIntelligence (AI), this scenario represents an excellent opportunity to: i)create new methods that analyse the individuals' health from what they eat, andii) develop personalised recommendations to improve nutrition and diet underspecific circumstances (e.g., obesity or COVID). Having tunable tools forcreating food image datasets that facilitate research in both lines is verymuch needed. This paper proposes AI4Food-NutritionFW, a framework for the creation of foodimage datasets according to configurable eating behaviours. AI4Food-NutritionFWsimulates a user-friendly and widespread scenario where images are taken usinga smartphone. In addition to the framework, we also provide and describe aunique food image dataset that includes 4,800 different weekly eatingbehaviours from 15 different profiles and 1,200 subjects. Specifically, weconsider profiles that comply with actual lifestyles from healthy eatingbehaviours (according to established knowledge), variable profiles (e.g.,eating out, holidays), to unhealthy ones (e.g., excess of fast food or sweets).Finally, we automatically evaluate a healthy index of the subject's eatingbehaviours using multidimensional metrics based on guidelines for healthy dietsproposed by international organisations, achieving promising results (99.53%and 99.60% accuracy and sensitivity, respectively). We also release to theresearch community a software implementation of our proposedAI4Food-NutritionFW and the mentioned food image dataset created with it.</description><author>Sergio Romero-Tapiador, Ruben Tolosana, Aythami Morales, Isabel Espinosa-Salinas, Gala Freixer, Julian Fierrez, Ruben Vera-Rodriguez, Enrique Carrillo de Santa Pau, Ana Ramírez de Molina, Javier Ortega-Garcia</author><pubDate>Tue, 12 Sep 2023 16:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06308v1</guid></item><item><title>Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility</title><link>http://arxiv.org/abs/2309.04296v2</link><description>In traditional deep learning algorithms, one of the key assumptions is thatthe data distribution remains constant during both training and deployment.However, this assumption becomes problematic when faced withOut-of-Distribution periods, such as the COVID-19 lockdowns, where the datadistribution significantly deviates from what the model has seen duringtraining. This paper employs a two-fold strategy: utilizing continual learningtechniques to update models with new data and harnessing human mobility datacollected from privacy-preserving pedestrian counters located outsidebuildings. In contrast to online learning, which suffers from 'catastrophicforgetting' as newly acquired knowledge often erases prior information,continual learning offers a holistic approach by preserving past insights whileintegrating new data. This research applies FSNet, a powerful continuallearning algorithm, to real-world data from 13 building complexes in Melbourne,Australia, a city which had the second longest total lockdown duration globallyduring the pandemic. Results underscore the crucial role of continual learningin accurate energy forecasting, particularly during Out-of-Distributionperiods. Secondary data such as mobility and temperature provided ancillarysupport to the primary forecasting model. More importantly, while traditionalmethods struggled to adapt during lockdowns, models featuring at least onlinelearning demonstrated resilience, with lockdown periods posing fewer challengesonce armed with adaptive learning techniques. This study contributes valuablemethodologies and insights to the ongoing effort to improve energy loadforecasting during future Out-of-Distribution periods.</description><author>Arian Prabowo, Kaixuan Chen, Hao Xue, Subbu Sethuvenkatraman, Flora D. Salim</author><pubDate>Tue, 12 Sep 2023 16:18:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04296v2</guid></item><item><title>RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain</title><link>http://arxiv.org/abs/2306.04054v2</link><description>Despite the recent advancements in speech recognition, there are stilldifficulties in accurately transcribing conversational and emotional speech innoisy and reverberant acoustic environments. This poses a particular challengein the search and rescue (SAR) domain, where transcribing conversations amongrescue team members is crucial to support real-time decision-making. Thescarcity of speech data and associated background noise in SAR scenarios makeit difficult to deploy robust speech recognition systems. To address thisissue, we have created and made publicly available a German speech datasetcalled RescueSpeech. This dataset includes real speech recordings fromsimulated rescue exercises. Additionally, we have released competitive trainingrecipes and pre-trained models. Our study highlights that the performanceattained by state-of-the-art methods in this challenging scenario is still farfrom reaching an acceptable level.</description><author>Sangeet Sagar, Mirco Ravanelli, Bernd Kiefer, Ivana Kruijff Korbayova, Josef van Genabith</author><pubDate>Tue, 12 Sep 2023 16:11:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04054v2</guid></item><item><title>Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic Data</title><link>http://arxiv.org/abs/2309.06302v1</link><description>This paper aims to remove specular highlights from a single object-levelimage. Although previous methods have made some progresses, their performanceremains somewhat limited, particularly for real images with complex specularhighlights. To this end, we propose a three-stage network to address them.Specifically, given an input image, we first decompose it into the albedo,shading, and specular residue components to estimate a coarse specular-freeimage. Then, we further refine the coarse result to alleviate its visualartifacts such as color distortion. Finally, we adjust the tone of the refinedresult to match that of the input as closely as possible. In addition, tofacilitate network training and quantitative evaluation, we present alarge-scale synthetic dataset of object-level images, covering diverse objectsand illumination conditions. Extensive experiments illustrate that our networkis able to generalize well to unseen real object-level images, and even producegood results for scene-level images with multiple background objects andcomplex lighting.</description><author>Gang Fu, Qing Zhang, Lei Zhu, Chunxia Xiao, Ping Li</author><pubDate>Tue, 12 Sep 2023 16:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06302v1</guid></item><item><title>ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning</title><link>http://arxiv.org/abs/2212.07919v2</link><description>Large language models show improved downstream task performance when promptedto generate step-by-step reasoning to justify their final answers. Thesereasoning steps greatly improve model interpretability and verification, butobjectively studying their correctness (independent of the final answer) isdifficult without reliable methods for automatic evaluation. We simply do notknow how often the stated reasoning steps actually support the final end taskpredictions. In this work, we present ROSCOE, a suite of interpretable,unsupervised automatic scores that improve and extend previous text generationevaluation metrics. To evaluate ROSCOE against baseline metrics, we design atypology of reasoning errors and collect synthetic and human evaluation scoreson commonly used reasoning datasets. In contrast with existing metrics, ROSCOEcan measure semantic consistency, logicality, informativeness, fluency, andfactuality - among other traits - by leveraging properties of step-by-steprationales. We empirically verify the strength of our metrics on five humanannotated and six programmatically perturbed diagnostics datasets - covering adiverse set of tasks that require reasoning skills and show that ROSCOE canconsistently outperform baseline metrics.</description><author>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz</author><pubDate>Tue, 12 Sep 2023 16:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07919v2</guid></item><item><title>Modeling Supply and Demand in Public Transportation Systems</title><link>http://arxiv.org/abs/2309.06299v1</link><description>The Harrisonburg Department of Public Transportation (HDPT) aims to leveragetheir data to improve the efficiency and effectiveness of their operations. Weconstruct two supply and demand models that help the department identify gapsin their service. The models take many variables into account, including theway that the HDPT reports to the federal government and the areas with the mostvulnerable populations in Harrisonburg City. We employ data analysis andmachine learning techniques to make our predictions.</description><author>Miranda Bihler, Hala Nelson, Erin Okey, Noe Reyes Rivas, John Webb, Anna White</author><pubDate>Tue, 12 Sep 2023 16:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06299v1</guid></item><item><title>Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models</title><link>http://arxiv.org/abs/2308.11890v2</link><description>Ligand-based drug design aims to identify novel drug candidates of similarshapes with known active molecules. In this paper, we formulated an in silicoshape-conditioned molecule generation problem to generate 3D moleculestructures conditioned on the shape of a given molecule. To address thisproblem, we developed a translation- and rotation-equivariant shape-guidedgenerative model ShapeMol. ShapeMol consists of an equivariant shape encoderthat maps molecular surface shapes into latent embeddings, and an equivariantdiffusion model that generates 3D molecules based on these embeddings.Experimental results show that ShapeMol can generate novel, diverse, drug-likemolecules that retain 3D molecular shapes similar to the given shape condition.These results demonstrate the potential of ShapeMol in designing drugcandidates of desired 3D shapes binding to protein target pockets.</description><author>Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning</author><pubDate>Tue, 12 Sep 2023 16:02:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11890v2</guid></item><item><title>Testing the limits of natural language models for predicting human language judgments</title><link>http://arxiv.org/abs/2204.03592v3</link><description>Neural network language models can serve as computational hypotheses abouthow humans process language. We compared the model-human consistency of diverselanguage models using a novel experimental approach: controversial sentencepairs. For each controversial sentence pair, two language models disagree aboutwhich sentence is more likely to occur in natural text. Considering ninelanguage models (including n-gram, recurrent neural networks, and transformermodels), we created hundreds of such controversial sentence pairs by eitherselecting sentences from a corpus or synthetically optimizing sentence pairs tobe highly controversial. Human subjects then provided judgments indicating foreach pair which of the two sentences is more likely. Controversial sentencepairs proved highly effective at revealing model failures and identifyingmodels that aligned most closely with human judgments. The mosthuman-consistent model tested was GPT-2, although experiments also revealedsignificant shortcomings of its alignment with human perception.</description><author>Tal Golan, Matthew Siegelman, Nikolaus Kriegeskorte, Christopher Baldassano</author><pubDate>Tue, 12 Sep 2023 16:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.03592v3</guid></item><item><title>TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines</title><link>http://arxiv.org/abs/2309.04801v2</link><description>Tsetlin Machines (TMs) provide a fundamental shift from arithmetic-based tologic-based machine learning. Supporting convolution, they deal successfullywith image classification datasets like MNIST, Fashion-MNIST, and CIFAR-2.However, the TM struggles with getting state-of-the-art performance on CIFAR-10and CIFAR-100, representing more complex tasks. This paper introducesplug-and-play collaboration between specialized TMs, referred to as TMComposites. The collaboration relies on a TM's ability to specialize duringlearning and to assess its competence during inference. When teaming up, themost confident TMs make the decisions, relieving the uncertain ones. In thismanner, a TM Composite becomes more competent than its members, benefiting fromtheir specializations. The collaboration is plug-and-play in that members canbe combined in any way, at any time, without fine-tuning. We implement three TMspecializations in our empirical evaluation: Histogram of Gradients, AdaptiveGaussian Thresholding, and Color Thermometers. The resulting TM Compositeincreases accuracy on Fashion-MNIST by two percentage points, CIFAR-10 bytwelve points, and CIFAR-100 by nine points, yielding new state-of-the-artresults for TMs. Overall, we envision that TM Composites will enable anultra-low energy and transparent alternative to state-of-the-art deep learningon more tasks and datasets.</description><author>Ole-Christoffer Granmo</author><pubDate>Tue, 12 Sep 2023 16:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04801v2</guid></item><item><title>Fidelity of Interpretability Methods and Perturbation Artifacts in Neural Networks</title><link>http://arxiv.org/abs/2203.02928v4</link><description>Despite excellent performance of deep neural networks (DNNs) in imageclassification, detection, and prediction, characterizing how DNNs make a givendecision remains an open problem, resulting in a number of interpretabilitymethods. Post-hoc interpretability methods primarily aim to quantify theimportance of input features with respect to the class probabilities. However,due to the lack of ground truth and the existence of interpretability methodswith diverse operating characteristics, evaluating these methods is a crucialchallenge. A popular approach to evaluate interpretability methods is toperturb input features deemed important for a given prediction and observe thedecrease in accuracy. However, perturbation itself may introduce artifacts. Wepropose a method for estimating the impact of such artifacts on the fidelityestimation by utilizing model accuracy curves from perturbing input featuresaccording to the Most Import First (MIF) and Least Import First (LIF) orders.Using the ResNet-50 trained on the ImageNet, we demonstrate the proposedfidelity estimation of four popular post-hoc interpretability methods.</description><author>Lennart Brocki, Neo Christopher Chung</author><pubDate>Tue, 12 Sep 2023 16:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.02928v4</guid></item><item><title>Graph Barlow Twins: A self-supervised representation learning framework for graphs</title><link>http://arxiv.org/abs/2106.02466v3</link><description>The self-supervised learning (SSL) paradigm is an essential exploration area,which tries to eliminate the need for expensive data labeling. Despite thegreat success of SSL methods in computer vision and natural languageprocessing, most of them employ contrastive learning objectives that requirenegative samples, which are hard to define. This becomes even more challengingin the case of graphs and is a bottleneck for achieving robust representations.To overcome such limitations, we propose a framework for self-supervised graphrepresentation learning - Graph Barlow Twins, which utilizes across-correlation-based loss function instead of negative samples. Moreover, itdoes not rely on non-symmetric neural network architectures - in contrast tostate-of-the-art self-supervised graph representation learning method BGRL. Weshow that our method achieves as competitive results as the bestself-supervised methods and fully supervised ones while requiring fewerhyperparameters and substantially shorter computation time (ca. 30 times fasterthan BGRL).</description><author>Piotr Bielak, Tomasz Kajdanowicz, Nitesh V. Chawla</author><pubDate>Tue, 12 Sep 2023 15:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.02466v3</guid></item><item><title>Self-Training and Multi-Task Learning for Limited Data: Evaluation Study on Object Detection</title><link>http://arxiv.org/abs/2309.06288v1</link><description>Self-training allows a network to learn from the predictions of a morecomplicated model, thus often requires well-trained teacher models and mixtureof teacher-student data while multi-task learning jointly optimizes differenttargets to learn salient interrelationship and requires multi-task annotationsfor each training example. These frameworks, despite being particularly datademanding have potentials for data exploitation if such assumptions can berelaxed. In this paper, we compare self-training object detection under thedeficiency of teacher training data where students are trained on unseenexamples by the teacher, and multi-task learning with partially annotated data,i.e. single-task annotation per training example. Both scenarios have their ownlimitation but potentially helpful with limited annotated data. Experimentalresults show the improvement of performance when using a weak teacher withunseen data for training a multi-task student. Despite the limited setup webelieve the experimental results show the potential of multi-task knowledgedistillation and self-training, which could be beneficial for future study.Source code is at https://lhoangan.github.io/multas.</description><author>Hoàng-Ân Lê, Minh-Tan Pham</author><pubDate>Tue, 12 Sep 2023 15:50:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06288v1</guid></item><item><title>Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition</title><link>http://arxiv.org/abs/2309.06286v1</link><description>Data-driven research in Additive Manufacturing (AM) has gained significantsuccess in recent years. This has led to a plethora of scientific literature toemerge. The knowledge in these works consists of AM and Artificial Intelligence(AI) contexts that have not been mined and formalized in an integrated way.Moreover, no tools or guidelines exist to support data-driven knowledgetransfer from one context to another. As a result, data-driven solutions usingspecific AI techniques are being developed and validated only for specific AMprocess technologies. There is a potential to exploit the inherent similaritiesacross various AM technologies and adapt the existing solutions from oneprocess or problem to another using AI, such as Transfer Learning. We propose athree-step knowledge transferability analysis framework in AM to supportdata-driven AM knowledge transfer. As a prerequisite to transferabilityanalysis, AM knowledge is featurized into identified knowledge components. Theframework consists of pre-transfer, transfer, and post-transfer steps toaccomplish knowledge transfer. A case study is conducted between flagship metalAM processes. Laser Powder Bed Fusion (LPBF) is the source of knowledgemotivated by its relative matureness in applying AI over Directed EnergyDeposition (DED), which drives the need for knowledge transfer as the lessexplored target process. We show successful transfer at different levels of thedata-driven solution, including data representation, model architecture, andmodel parameters. The pipeline of AM knowledge transfer can be automated in thefuture to allow efficient cross-context or cross-process knowledge exchange.</description><author>Mutahar Safdar, Jiarui Xie, Hyunwoong Ko, Yan Lu, Guy Lamouche, Yaoyao Fiona Zhao</author><pubDate>Tue, 12 Sep 2023 15:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06286v1</guid></item><item><title>Fairness and robustness in anti-causal prediction</title><link>http://arxiv.org/abs/2209.09423v2</link><description>Robustness to distribution shift and fairness have independently emerged astwo important desiderata required of modern machine learning models. Whilethese two desiderata seem related, the connection between them is often unclearin practice. Here, we discuss these connections through a causal lens, focusingon anti-causal prediction tasks, where the input to a classifier (e.g., animage) is assumed to be generated as a function of the target label and theprotected attribute. By taking this perspective, we draw explicit connectionsbetween a common fairness criterion - separation - and a common notion ofrobustness - risk invariance. These connections provide new motivation forapplying the separation criterion in anticausal settings, and inform olddiscussions regarding fairness-performance tradeoffs. In addition, our findingssuggest that robustness-motivated approaches can be used to enforce separation,and that they often work better in practice than methods designed to directlyenforce separation. Using a medical dataset, we empirically validate ourfindings on the task of detecting pneumonia from X-rays, in a setting wheredifferences in prevalence across sex groups motivates a fairness mitigation.Our findings highlight the importance of considering causal structure whenchoosing and enforcing fairness criteria.</description><author>Maggie Makar, Alexander D'Amour</author><pubDate>Tue, 12 Sep 2023 15:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.09423v2</guid></item><item><title>Jersey Number Recognition using Keyframe Identification from Low-Resolution Broadcast Videos</title><link>http://arxiv.org/abs/2309.06285v1</link><description>Player identification is a crucial component in vision-driven socceranalytics, enabling various downstream tasks such as player assessment, in-gameanalysis, and broadcast production. However, automatically detecting jerseynumbers from player tracklets in videos presents challenges due to motion blur,low resolution, distortions, and occlusions. Existing methods, utilizingSpatial Transformer Networks, CNNs, and Vision Transformers, have shown successin image data but struggle with real-world video data, where jersey numbers arenot visible in most of the frames. Hence, identifying frames that contain thejersey number is a key sub-problem to tackle. To address these issues, wepropose a robust keyframe identification module that extracts frames containingessential high-level information about the jersey number. A spatio-temporalnetwork is then employed to model spatial and temporal context and predict theprobabilities of jersey numbers in the video. Additionally, we adopt amulti-task loss function to predict the probability distribution of each digitseparately. Extensive evaluations on the SoccerNet dataset demonstrate thatincorporating our proposed keyframe identification module results in asignificant 37.81% and 37.70% increase in the accuracies of 2 different testsets with domain gaps. These results highlight the effectiveness and importanceof our approach in tackling the challenges of automatic jersey number detectionin sports videos.</description><author>Bavesh Balaji, Jerrin Bright, Harish Prakash, Yuhao Chen, David A Clausi, John Zelek</author><pubDate>Tue, 12 Sep 2023 15:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06285v1</guid></item><item><title>Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</title><link>http://arxiv.org/abs/2309.06284v1</link><description>Text-driven human motion generation in computer vision is both significantand challenging. However, current methods are limited to producing eitherdeterministic or imprecise motion sequences, failing to effectively control thetemporal and spatial relationships required to conform to a given textdescription. In this work, we propose a fine-grained method for generatinghigh-quality, conditional human motion sequences supporting precise textdescription. Our approach consists of two key components: 1) alinguistics-structure assisted module that constructs accurate and completelanguage feature to fully utilize text information; and 2) a context-awareprogressive reasoning module that learns neighborhood and overall semanticlinguistics features from shallow and deep graph neural networks to achieve amulti-step inference. Experiments show that our approach outperformstext-driven motion generation methods on HumanML3D and KIT test sets andgenerates better visually confirmed motion to the text conditions.</description><author>Yin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng Wu, Xiaohui Liang</author><pubDate>Tue, 12 Sep 2023 15:43:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06284v1</guid></item><item><title>IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation</title><link>http://arxiv.org/abs/2309.06282v1</link><description>Domain generalized semantic segmentation (DGSS) is a critical yet challengingtask, where the model is trained only on source data without access to anytarget data. Despite the proposal of numerous DGSS strategies, thegeneralization capability remains limited in CNN architectures. Though someTransformer-based segmentation models show promising performance, theyprimarily focus on capturing intra-sample attentive relationships, disregardinginter-sample correlations which can potentially benefit DGSS. To this end, weenhance the attention modules in Transformer networks for improving DGSS byincorporating information from other independent samples in the same batch,enriching contextual information, and diversifying the training data for eachattention block. Specifically, we propose two alternative intra-batch attentionmechanisms, namely mean-based intra-batch attention (MIBA) and element-wiseintra-batch attention (EIBA), to capture correlations between differentsamples, enhancing feature representation and generalization capabilities.Building upon intra-batch attention, we introduce IBAFormer, which integratesself-attention modules with the proposed intra-batch attention for DGSS.Extensive experiments demonstrate that IBAFormer achieves SOTA performance inDGSS, and ablation studies further confirm the effectiveness of each introducedcomponent.</description><author>Qiyu Sun, Huilin Chen, Meng Zheng, Ziyan Wu, Michael Felsberg, Yang Tang</author><pubDate>Tue, 12 Sep 2023 15:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06282v1</guid></item><item><title>Revealing the preference for correcting separated aberrations in joint optic-image design</title><link>http://arxiv.org/abs/2309.04342v2</link><description>The joint design of the optical system and the downstream algorithm is achallenging and promising task. Due to the demand for balancing the globaloptimal of imaging systems and the computational cost of physical simulation,existing methods cannot achieve efficient joint design of complex systems suchas smartphones and drones. In this work, starting from the perspective of theoptical design, we characterize the optics with separated aberrations.Additionally, to bridge the hardware and software without gradients, an imagesimulation system is presented to reproduce the genuine imaging procedure oflenses with large field-of-views. As for aberration correction, we propose anetwork to perceive and correct the spatially varying aberrations and validateits superiority over state-of-the-art methods. Comprehensive experiments revealthat the preference for correcting separated aberrations in joint design is asfollows: longitudinal chromatic aberration, lateral chromatic aberration,spherical aberration, field curvature, and coma, with astigmatism coming last.Drawing from the preference, a 10% reduction in the total track length of theconsumer-level mobile phone lens module is accomplished. Moreover, thisprocedure spares more space for manufacturing deviations, realizingextreme-quality enhancement of computational photography. The optimizationparadigm provides innovative insight into the practical joint design ofsophisticated optical systems and post-processing algorithms.</description><author>Jingwen Zhou, Shiqi Chen, Zheng Ren, Wenguan Zhang, Jiapu Yan, Huajun Feng, Qi Li, Yueting Chen</author><pubDate>Tue, 12 Sep 2023 15:39:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04342v2</guid></item><item><title>OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action Segmentation</title><link>http://arxiv.org/abs/2309.06276v1</link><description>Temporal action segmentation is typically achieved by discovering thedramatic variances in global visual descriptors. In this paper, we explore themerits of local features by proposing the unsupervised framework ofObject-centric Temporal Action Segmentation (OTAS). Broadly speaking, OTASconsists of self-supervised global and local feature extraction modules as wellas a boundary selection module that fuses the features and detects salientboundaries for action segmentation. As a second contribution, we discuss thepros and cons of existing frame-level and boundary-level evaluation metrics.Through extensive experiments, we find OTAS is superior to the previousstate-of-the-art method by $41\%$ on average in terms of our recommended F1score. Surprisingly, OTAS even outperforms the ground-truth human annotationsin the user study. Moreover, OTAS is efficient enough to allow real-timeinference.</description><author>Yuerong Li, Zhengrong Xue, Huazhe Xu</author><pubDate>Tue, 12 Sep 2023 15:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06276v1</guid></item><item><title>Re-Reading Improves Reasoning in Language Models</title><link>http://arxiv.org/abs/2309.06275v1</link><description>Reasoning presents a significant and challenging issue for Large LanguageModels (LLMs). The predominant focus of research has revolved around developingdiverse prompting strategies to guide and structure the reasoning processes ofLLMs. However, these approaches based on decoder-only causal language modelsoften operate the input question in a single forward pass, potentially missingthe rich, back-and-forth interactions inherent in human reasoning. Scantattention has been paid to a critical dimension, i.e., the input questionitself embedded within the prompts. In response, we introduce a deceptivelysimple yet highly effective prompting strategy, termed question "re-reading".Drawing inspiration from human learning and problem-solving, re-reading entailsrevisiting the question information embedded within input prompts. Thisapproach aligns seamlessly with the cognitive principle of reinforcement,enabling LLMs to extract deeper insights, identify intricate patterns,establish more nuanced connections, and ultimately enhance their reasoningcapabilities across various tasks. Experiments conducted on a series ofreasoning benchmarks serve to underscore the effectiveness and generality ofour method. Moreover, our findings demonstrate that our approach seamlesslyintegrates with various language models, though-eliciting prompting methods,and ensemble techniques, further underscoring its versatility and compatibilityin the realm of LLMs.</description><author>Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-guang Lou</author><pubDate>Tue, 12 Sep 2023 15:36:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06275v1</guid></item><item><title>ELRA: Exponential learning rate adaption gradient descent optimization method</title><link>http://arxiv.org/abs/2309.06274v1</link><description>We present a novel, fast (exponential rate adaption), ab initio(hyper-parameter-free) gradient based optimizer algorithm. The main idea of themethod is to adapt the learning rate $\alpha$ by situational awareness, mainlystriving for orthogonal neighboring gradients. The method has a high successand fast convergence rate and does not rely on hand-tuned parameters giving itgreater universality. It can be applied to problems of any dimensions n andscales only linearly (of order O(n)) with the dimension of the problem. Itoptimizes convex and non-convex continuous landscapes providing some kind ofgradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.)the method is rotation invariant: optimization path and performance areindependent of coordinate choices. The impressive performance is demonstratedby extensive experiments on the MNIST benchmark data-set againststate-of-the-art optimizers. We name this new class of optimizers after itscore idea Exponential Learning Rate Adaption - ELRA. We present it in twovariants c2min and p2min with slightly different control. The authors stronglybelieve that ELRA will open a completely new research direction for gradientdescent optimize.</description><author>Alexander Kleinsorge, Stefan Kupper, Alexander Fauck, Felix Rothe</author><pubDate>Tue, 12 Sep 2023 15:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06274v1</guid></item><item><title>A prediction and behavioural analysis of machine learning methods for modelling travel mode choice</title><link>http://arxiv.org/abs/2301.04404v3</link><description>The emergence of a variety of Machine Learning (ML) approaches for travelmode choice prediction poses an interesting question to transport modellers:which models should be used for which applications? The answer to this questiongoes beyond simple predictive performance, and is instead a balance of manyfactors, including behavioural interpretability and explainability,computational complexity, and data efficiency. There is a growing body ofresearch which attempts to compare the predictive performance of different MLclassifiers with classical random utility models. However, existing studiestypically analyse only the disaggregate predictive performance, ignoring otheraspects affecting model choice. Furthermore, many studies are affected bytechnical limitations, such as the use of inappropriate validation schemes,incorrect sampling for hierarchical data, lack of external validation, and theexclusive use of discrete metrics. We address these limitations by conducting asystematic comparison of different modelling approaches, across multiplemodelling problems, in terms of the key factors likely to affect model choice(out-of-sample predictive performance, accuracy of predicted market shares,extraction of behavioural indicators, and computational efficiency). We combineseveral real world datasets with synthetic datasets, where the data generationfunction is known. The results indicate that the models with the highestdisaggregate predictive performance (namely extreme gradient boosting andrandom forests) provide poorer estimates of behavioural indicators andaggregate mode shares, and are more expensive to estimate, than other models,including deep neural networks and Multinomial Logit (MNL). It is furtherobserved that the MNL model performs robustly in a variety of situations,though ML techniques can improve the estimates of behavioural indices such asWillingness to Pay.</description><author>José Ángel Martín-Baos, Julio Alberto López-Gómez, Luis Rodriguez-Benitez, Tim Hillel, Ricardo García-Ródenas</author><pubDate>Tue, 12 Sep 2023 15:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04404v3</guid></item><item><title>FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning</title><link>http://arxiv.org/abs/2309.04663v2</link><description>Learning paradigms for large language models (LLMs) currently tend to fallwithin either in-context learning (ICL) or full fine-tuning. Each of thesecomes with their own trade-offs based on available data, model size, computecost, ease-of-use, and final quality with neither solution performing wellacross-the-board. In this article, we first describe ICL and fine-tuningparadigms in a way that highlights their natural connections. Based on theseconnections, we propose a new learning paradigm called FIAT that fuses the bestof these paradigms together, enabling prompt-engineered instructions andchain-of-thought reasoning with the very largest models while also usingsimilar methods to perform parameter updates on a modestly-sized LLM withparameter-efficient tuning. We evaluate FIAT's effectiveness on a variety ofmultilingual tasks and observe that FIAT performs better than both ICL andfine-tuning at scales ranging from 100-10,000 training examples. We hope thatFIAT provides a practical way of harnessing the full potential of LLMs withoutneeding to make a hard choice between learning paradigms.</description><author>Xinyi Wang, John Wieting, Jonathan H. Clark</author><pubDate>Tue, 12 Sep 2023 15:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04663v2</guid></item><item><title>ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation</title><link>http://arxiv.org/abs/2309.06268v1</link><description>MRI is increasingly being used in the diagnosis of prostate cancer (PCa),with diffusion MRI (dMRI) playing an integral role. When combined withcomputational models, dMRI can estimate microstructural information such ascell size. Conventionally, such models are fit with a nonlinear least squares(NLLS) curve fitting approach, associated with a high computational cost.Supervised deep neural networks (DNNs) are an efficient alternative, howevertheir performance is significantly affected by the underlying distribution ofthe synthetic training data. Self-supervised learning is an attractivealternative, where instead of using a separate training dataset, the networklearns the features of the input data itself. This approach has only beenapplied to fitting of trivial dMRI models thus far. Here, we introduce aself-supervised DNN to estimate the parameters of the VERDICT (Vascular,Extracellular and Restricted DIffusion for Cytometry in Tumours) model forprostate. We demonstrate, for the first time, fitting of a complexthree-compartment biophysical model with machine learning without therequirement of explicit training labels. We compare the estimation performanceto baseline NLLS and supervised DNN methods, observing improvement inestimation accuracy and reduction in bias with respect to ground truth values.Our approach also achieves a higher confidence level for discrimination betweencancerous and benign prostate tissue in comparison to the other methods on adataset of 20 PCa patients, indicating potential for accurate tumourcharacterisation.</description><author>Snigdha Sen, Saurabh Singh, Hayley Pye, Caroline Moore, Hayley Whitaker, Shonit Punwani, David Atkinson, Eleftheria Panagiotaki, Paddy J. Slator</author><pubDate>Tue, 12 Sep 2023 15:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06268v1</guid></item><item><title>PSO-Convolutional Neural Networks with Heterogeneous Learning Rate</title><link>http://arxiv.org/abs/2205.10456v3</link><description>Convolutional Neural Networks (ConvNets or CNNs) have been candidly deployedin the scope of computer vision and related fields. Nevertheless, the dynamicsof training of these neural networks lie still elusive: it is hard andcomputationally expensive to train them. A myriad of architectures and trainingstrategies have been proposed to overcome this challenge and address severalproblems in image processing such as speech, image and action recognition aswell as object detection. In this article, we propose a novel Particle SwarmOptimization (PSO) based training for ConvNets. In such framework, the vectorof weights of each ConvNet is typically cast as the position of a particle inphase space whereby PSO collaborative dynamics intertwines with StochasticGradient Descent (SGD) in order to boost training performance andgeneralization. Our approach goes as follows: i) [regular phase] each ConvNetis trained independently via SGD; ii) [collaborative phase] ConvNets shareamong themselves their current vector of weights (or particle-position) alongwith their gradient estimates of the Loss function. Distinct step sizes arecoined by distinct ConvNets. By properly blending ConvNets with large (possiblyrandom) step-sizes along with more conservative ones, we propose an algorithmwith competitive performance with respect to other PSO-based approaches onCifar-10 and Cifar-100 (accuracy of 98.31% and 87.48%). These accuracy levelsare obtained by resorting to only four ConvNets -- such results are expected toscale with the number of collaborative ConvNets accordingly. We make our sourcecodes available for download https://github.com/leonlha/PSO-ConvNet-Dynamics.</description><author>Nguyen Huu Phong, Augusto Santos, Bernardete Ribeiro</author><pubDate>Tue, 12 Sep 2023 15:22:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10456v3</guid></item><item><title>Modality Unifying Network for Visible-Infrared Person Re-Identification</title><link>http://arxiv.org/abs/2309.06262v1</link><description>Visible-infrared person re-identification (VI-ReID) is a challenging task dueto large cross-modality discrepancies and intra-class variations. Existingmethods mainly focus on learning modality-shared representations by embeddingdifferent modalities into the same feature space. As a result, the learnedfeature emphasizes the common patterns across modalities while suppressingmodality-specific and identity-aware information that is valuable for Re-ID. Toaddress these issues, we propose a novel Modality Unifying Network (MUN) toexplore a robust auxiliary modality for VI-ReID. First, the auxiliary modalityis generated by combining the proposed cross-modality learner andintra-modality learner, which can dynamically model the modality-specific andmodality-shared representations to alleviate both cross-modality andintra-modality variations. Second, by aligning identity centres across thethree modalities, an identity alignment loss function is proposed to discoverthe discriminative feature representations. Third, a modality alignment loss isintroduced to consistently reduce the distribution distance of visible andinfrared images by modality prototype modeling. Extensive experiments onmultiple public datasets demonstrate that the proposed method surpasses thecurrent state-of-the-art methods by a significant margin.</description><author>Hao Yu, Xu Cheng, Wei Peng, Weihao Liu, Guoying Zhao</author><pubDate>Tue, 12 Sep 2023 15:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06262v1</guid></item><item><title>Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning</title><link>http://arxiv.org/abs/2309.06260v1</link><description>We propose a novel method for developing discretization-consistent closureschemes for implicitly filtered Large Eddy Simulation (LES). In implicitlyfiltered LES, the induced filter kernel, and thus the closure terms, aredetermined by the properties of the grid and the discretization operator,leading to additional computational subgrid terms that are generally unknown ina priori analysis. Therefore, the task of adapting the coefficients of LESclosure models is formulated as a Markov decision process and solved in an aposteriori manner with Reinforcement Learning (RL). This allows to adjust themodel to the actual discretization as it also incorporates the interactionbetween the discretization and the model itself. This optimization framework isapplied to both explicit and implicit closure models. An element-local eddyviscosity model is optimized as the explicit model. For the implicit modeling,RL is applied to identify an optimal blending strategy for a hybriddiscontinuous Galerkin (DG) and finite volume scheme. All newly derived modelsachieve accurate and consistent results, either matching or outperformingclassical state-of-the-art models for different discretizations andresolutions. Moreover, the explicit model is demonstrated to adapt itsdistribution of viscosity within the DG elements to the inhomogeneousdiscretization properties of the operator. In the implicit case, the optimizedhybrid scheme renders itself as a viable modeling ansatz that could initiate anew class of high order schemes for compressible turbulence. Overall, theresults demonstrate that the proposed RL optimization can providediscretization-consistent closures that could reduce the uncertainty inimplicitly filtered LES.</description><author>Andrea Beck, Marius Kurz</author><pubDate>Tue, 12 Sep 2023 15:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06260v1</guid></item><item><title>BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Search and Recommendation Models on Commodity CPU Hardware</title><link>http://arxiv.org/abs/2303.17727v4</link><description>Efficient large-scale neural network training and inference on commodity CPUhardware is of immense practical significance in democratizing deep learning(DL) capabilities. Presently, the process of training massive models consistingof hundreds of millions to billions of parameters requires the extensive use ofspecialized hardware accelerators, such as GPUs, which are only accessible to alimited number of institutions with considerable financial resources. Moreover,there is often an alarming carbon footprint associated with training anddeploying these models. In this paper, we take a step towards addressing thesechallenges by introducing BOLT, a sparse deep learning library for traininglarge-scale search and recommendation models on standard CPU hardware. BOLTprovides a flexible, high-level API for constructing models that will befamiliar to users of existing popular DL frameworks. By automatically tuningspecialized hyperparameters, BOLT also abstracts away the algorithmic detailsof sparse network training. We evaluate BOLT on a number of informationretrieval tasks including product recommendations, text classification, graphneural networks, and personalization. We find that our proposed system achievescompetitive performance with state-of-the-art techniques at a fraction of thecost and energy consumption and an order-of-magnitude faster inference time.BOLT has also been successfully deployed by multiple businesses to addresscritical problems, and we highlight one customer case study in the field ofe-commerce.</description><author>Nicholas Meisburger, Vihan Lakshman, Benito Geordie, Joshua Engels, David Torres Ramos, Pratik Pranav, Benjamin Coleman, Benjamin Meisburger, Shubh Gupta, Yashwanth Adunukota, Tharun Medini, Anshumali Shrivastava</author><pubDate>Tue, 12 Sep 2023 15:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17727v4</guid></item><item><title>Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models</title><link>http://arxiv.org/abs/2309.06256v1</link><description>Foundation models, including Vision Language Models (VLMs) and Large LanguageModels (LLMs), possess the $generality$ to handle diverse distributions andtasks, which stems from their extensive pre-training datasets. The fine-tuningof foundation models is a common practice to enhance task performance or alignthe model's behavior with human expectations, allowing them to gain$speciality$. However, the small datasets used for fine-tuning may notadequately cover the diverse distributions and tasks encountered duringpre-training. Consequently, the pursuit of speciality during fine-tuning canlead to a loss of {generality} in the model, which is related to catastrophicforgetting (CF) in deep learning. In this study, we demonstrate this phenomenonin both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNetresults in a loss of generality in handling diverse distributions, andfine-tuning LLMs like Galactica in the medical domain leads to a loss infollowing instructions and common sense. To address the trade-off between the speciality and generality, weinvestigate multiple regularization methods from continual learning, the weightaveraging method (Wise-FT) from out-of-distributional (OOD) generalization,which interpolates parameters between pre-trained and fine-tuned models, andparameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Ourfindings show that both continual learning and Wise-ft methods effectivelymitigate the loss of generality, with Wise-FT exhibiting the strongestperformance in balancing speciality and generality.</description><author>Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, Tong Zhang</author><pubDate>Tue, 12 Sep 2023 15:16:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06256v1</guid></item><item><title>Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation</title><link>http://arxiv.org/abs/2309.06255v1</link><description>One primary topic of multi-modal learning is to jointly incorporateheterogeneous information from different modalities. However, most models oftensuffer from unsatisfactory multi-modal cooperation, which could not jointlyutilize all modalities well. Some methods are proposed to identify and enhancethe worse learnt modality, but are often hard to provide the fine-grainedobservation of multi-modal cooperation at sample-level with theoreticalsupport. Hence, it is essential to reasonably observe and improve thefine-grained cooperation between modalities, especially when facing realisticscenarios where the modality discrepancy could vary across different samples.To this end, we introduce a fine-grained modality valuation metric to evaluatethe contribution of each modality at sample-level. Via modality valuation, weregretfully observe that the multi-modal model tends to rely on one specificmodality, resulting in other modalities being low-contributing. We furtheranalyze this issue and improve cooperation between modalities by enhancing thediscriminative ability of low-contributing modalities in a targeted manner.Overall, our methods reasonably observe the fine-grained uni-modal contributionat sample-level and achieve considerable improvement on different multi-modalmodels.</description><author>Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu</author><pubDate>Tue, 12 Sep 2023 15:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06255v1</guid></item><item><title>Federated Learning for Medical Image Analysis: A Survey</title><link>http://arxiv.org/abs/2306.05980v3</link><description>Machine learning in medical imaging often faces a fundamental dilemma, namelythe small sample size problem. Many recent studies suggest using multi-domaindata pooled from different acquisition sites/datasets to improve statisticalpower. However, medical images from different sites cannot be easily shared tobuild large datasets for model training due to privacy protection reasons. As apromising solution, federated learning, which enables collaborative training ofmachine learning models based on data from different sites without cross-sitedata sharing, has attracted considerable attention recently. In this paper, weconduct a comprehensive survey of the recent development of federated learningmethods in medical image analysis. We first introduce the background andmotivation of federated learning for dealing with privacy protection andcollaborative learning issues in medical imaging. We then present acomprehensive review of recent advances in federated learning methods formedical image analysis. Specifically, existing methods are categorized based onthree critical aspects of a federated learning system, including client end,server end, and communication techniques. In each category, we summarize theexisting federated learning methods according to specific research problems inmedical image analysis and also provide insights into the motivations ofdifferent approaches. In addition, we provide a review of existing benchmarkmedical imaging datasets and software platforms for current federated learningresearch. We also conduct an experimental study to empirically evaluate typicalfederated learning methods for medical image analysis. This survey can help tobetter understand the current research status, challenges and potentialresearch opportunities in this promising research field.</description><author>Hao Guan, Pew-Thian Yap, Andrea Bozoki, Mingxia Liu</author><pubDate>Tue, 12 Sep 2023 15:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05980v3</guid></item><item><title>RoDia: A New Dataset for Romanian Dialect Identification from Speech</title><link>http://arxiv.org/abs/2309.03378v2</link><description>Dialect identification is a critical task in speech processing and languagetechnology, enhancing various applications such as speech recognition, speakerverification, and many others. While most research studies have been dedicatedto dialect identification in widely spoken languages, limited attention hasbeen given to dialect identification in low-resource languages, such asRomanian. To address this research gap, we introduce RoDia, the first datasetfor Romanian dialect identification from speech. The RoDia dataset includes avaried compilation of speech samples from five distinct regions of Romania,covering both urban and rural environments, totaling 2 hours of manuallyannotated speech data. Along with our dataset, we introduce a set ofcompetitive models to be used as baselines for future research. The top scoringmodel achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%,indicating that the task is challenging. We thus believe that RoDia is avaluable resource that will stimulate research aiming to address the challengesof Romanian dialect identification. We publicly release our dataset and code athttps://github.com/codrut2/RoDia.</description><author>Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu</author><pubDate>Tue, 12 Sep 2023 15:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03378v2</guid></item><item><title>Leveraging Automatic Personalised Nutrition: Food Image Recognition Benchmark and Dataset based on Nutrition Taxonomy</title><link>http://arxiv.org/abs/2211.07440v2</link><description>Leading a healthy lifestyle has become one of the most challenging goals intoday's society due to our sedentary lifestyle and poor eating habits. As aresult, national and international organisms have made numerous efforts topromote healthier food diets and physical activity habits. However, theserecommendations are sometimes difficult to follow in our daily life and theyare also based on a general population. As a consequence, a new area ofresearch, personalised nutrition, has been conceived focusing on individualsolutions through smart devices and Artificial Intelligence (AI) methods. This study presents the AI4Food-NutritionDB database, the first nutritiondatabase that considers food images and a nutrition taxonomy based onrecommendations by national and international organisms. In addition, fourdifferent categorisation levels are considered following nutrition experts: 6nutritional levels, 19 main categories (e.g., "Meat"), 73 subcategories (e.g.,"White Meat"), and 893 final food products (e.g., "Chicken"). TheAI4Food-NutritionDB opens the doors to new food computing approaches in termsof food intake frequency, quality, and categorisation. Also, in addition to thedatabase, we propose a standard experimental protocol and benchmark includingthree tasks based on the nutrition taxonomy (i.e., category, subcategory, andfinal product) to be used for the research community. Finally, we also releaseour Deep Learning models trained with the AI4Food-NutritionDB, which can beused as pre-trained models, achieving accurate recognition results withchallenging food image databases.</description><author>Sergio Romero-Tapiador, Ruben Tolosana, Aythami Morales, Isabel Espinosa-Salinas, Gala Freixer, Julian Fierrez, Ruben Vera-Rodriguez, Enrique Carrillo de Santa Pau, Ana Ramírez de Molina, Javier Ortega-Garcia</author><pubDate>Tue, 12 Sep 2023 15:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07440v2</guid></item><item><title>Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data</title><link>http://arxiv.org/abs/2309.06248v1</link><description>Probability estimation models play an important role in various fields, suchas weather forecasting, recommendation systems, and sports analysis. Amongseveral models estimating probabilities, it is difficult to evaluate whichmodel gives reliable probabilities since the ground-truth probabilities are notavailable. The win probability estimation model for esports, which calculatesthe win probability under a certain game state, is also one of the fields beingactively studied in probability estimation. However, most of the previous worksevaluated their models using accuracy, a metric that only can measure theperformance of discrimination. In this work, we firstly investigate the Brierscore and the Expected Calibration Error (ECE) as a replacement of accuracyused as a performance evaluation metric for win probability estimation modelsin esports field. Based on the analysis, we propose a novel metric calledBalance score which is a simple yet effective metric in terms of six goodproperties that probability estimation metric should have. Under the generalcondition, we also found that the Balance score can be an effectiveapproximation of the true expected calibration error which has been imperfectlyapproximated by ECE using the binning technique. Extensive evaluations usingsimulation studies and real game snapshot data demonstrate the promisingpotential to adopt the proposed metric not only for the win probabilityestimation model for esports but also for evaluating general probabilityestimation models.</description><author>Euihyeon Choi, Jooyoung Kim, Wonkyung Lee</author><pubDate>Tue, 12 Sep 2023 15:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06248v1</guid></item><item><title>Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking</title><link>http://arxiv.org/abs/2308.05911v2</link><description>Multi-object tracking (MOT) at low frame rates can reduce computational,storage and power overhead to better meet the constraints of edge devices. Manyexisting MOT methods suffer from significant performance degradation inlow-frame-rate videos due to significant location and appearance changesbetween adjacent frames. To this end, we propose to explore collaborativetracking learning (ColTrack) for frame-rate-insensitive MOT in a query-basedend-to-end manner. Multiple historical queries of the same target jointly trackit with richer temporal descriptions. Meanwhile, we insert an informationrefinement module between every two temporal blocking decoders to better fusetemporal clues and refine features. Moreover, a tracking object consistencyloss is proposed to guide the interaction between historical queries. Extensiveexperimental results demonstrate that in high-frame-rate videos, ColTrackobtains higher performance than state-of-the-art methods on large-scaledatasets Dancetrack and BDD100K, and outperforms the existing end-to-endmethods on MOT17. More importantly, ColTrack has a significant advantage overstate-of-the-art methods in low-frame-rate videos, which allows it to obtainfaster processing speeds by reducing frame-rate requirements while maintaininghigher performance. Code will be released athttps://github.com/yolomax/ColTrack</description><author>Yiheng Liu, Junta Wu, Yi Fu</author><pubDate>Tue, 12 Sep 2023 15:01:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05911v2</guid></item><item><title>Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks</title><link>http://arxiv.org/abs/2309.06240v1</link><description>Reliable uncertainty quantification (UQ) in machine learning (ML) regressiontasks is becoming the focus of many studies in materials and chemical science.It is now well understood that average calibration is insufficient, and moststudies implement additional methods testing the conditional calibration withrespect to uncertainty, i.e. consistency. Consistency is assessed mostly byso-called reliability diagrams. There exists however another way beyond averagecalibration, which is conditional calibration with respect to input features,i.e. adaptivity. In practice, adaptivity is the main concern of the final usersof a ML-UQ method, seeking for the reliability of predictions and uncertaintiesfor any point in features space. This article aims to show that consistency andadaptivity are complementary validation targets, and that a good consistencydoes not imply a good adaptivity. Adapted validation methods are proposed andillustrated on a representative example.</description><author>Pascal Pernot</author><pubDate>Tue, 12 Sep 2023 14:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06240v1</guid></item><item><title>A survey on deep learning in medical image registration: new technologies, uncertainty, evaluation metrics, and beyond</title><link>http://arxiv.org/abs/2307.15615v2</link><description>Deep learning technologies have dramatically reshaped the field of medicalimage registration over the past decade. The initial developments, such asResNet-based and U-Net-based networks, established the foundation for deeplearning in image registration. Subsequent progress has been made in variousaspects of deep learning-based registration, including similarity measures,deformation regularizations, and uncertainty estimation. These advancementshave not only enriched the field of image registration but have alsofacilitated its application in a wide range of tasks, including atlasconstruction, multi-atlas segmentation, motion estimation, and 2D-3Dregistration. In this paper, we present a comprehensive overview of the mostrecent advancements in deep learning-based image registration. We begin with aconcise introduction to the core concepts of deep learning-based imageregistration. Then, we delve into innovative network architectures, lossfunctions specific to registration, and methods for estimating registrationuncertainty. Additionally, this paper explores appropriate evaluation metricsfor assessing the performance of deep learning models in registration tasks.Finally, we highlight the practical applications of these novel techniques inmedical imaging and discuss the future prospects of deep learning-based imageregistration.</description><author>Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du</author><pubDate>Tue, 12 Sep 2023 14:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15615v2</guid></item><item><title>Risk-Aware Reinforcement Learning through Optimal Transport Theory</title><link>http://arxiv.org/abs/2309.06239v1</link><description>In the dynamic and uncertain environments where reinforcement learning (RL)operates, risk management becomes a crucial factor in ensuring reliabledecision-making. Traditional RL approaches, while effective in rewardoptimization, often overlook the landscape of potential risks. In response,this paper pioneers the integration of Optimal Transport (OT) theory with RL tocreate a risk-aware framework. Our approach modifies the objective function,ensuring that the resulting policy not only maximizes expected rewards but alsorespects risk constraints dictated by OT distances between state visitationdistributions and the desired risk profiles. By leveraging the mathematicalprecision of OT, we offer a formulation that elevates risk considerationsalongside conventional RL objectives. Our contributions are substantiated witha series of theorems, mapping the relationships between risk distributions,optimal value functions, and policy behaviors. Through the lens of OT, thiswork illuminates a promising direction for RL, ensuring a balanced fusion ofreward pursuit and risk awareness.</description><author>Ali Baheri</author><pubDate>Tue, 12 Sep 2023 14:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06239v1</guid></item><item><title>Hierarchical Optimization-Derived Learning</title><link>http://arxiv.org/abs/2302.05587v2</link><description>In recent years, by utilizing optimization techniques to formulate thepropagation of deep model, a variety of so-called Optimization-Derived Learning(ODL) approaches have been proposed to address diverse learning and visiontasks. Although having achieved relatively satisfying practical performance,there still exist fundamental issues in existing ODL methods. In particular,current ODL methods tend to consider model construction and learning as twoseparate phases, and thus fail to formulate their underlying coupling anddepending relationship. In this work, we first establish a new framework, namedHierarchical ODL (HODL), to simultaneously investigate the intrinsic behaviorsof optimization-derived model construction and its corresponding learningprocess. Then we rigorously prove the joint convergence of these two sub-tasks,from the perspectives of both approximation quality and stationary analysis. Toour best knowledge, this is the first theoretical guarantee for these twocoupled ODL components: optimization and learning. We further demonstrate theflexibility of our framework by applying HODL to challenging learning tasks,which have not been properly addressed by existing ODL methods. Finally, weconduct extensive experiments on both synthetic data and real applications invision and other learning tasks to verify the theoretical properties andpractical performance of HODL in various application scenarios.</description><author>Risheng Liu, Xuan Liu, Shangzhi Zeng, Jin Zhang, Yixuan Zhang</author><pubDate>Tue, 12 Sep 2023 14:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05587v2</guid></item><item><title>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models</title><link>http://arxiv.org/abs/2309.06236v1</link><description>Large Language Models (LLMs) have demonstrated remarkable generalizationacross diverse tasks, leading individuals to increasingly use them as personalassistants and universal computing engines. Nevertheless, a notable obstacleemerges when feeding numerical/temporal data into these models, such as datasourced from wearables or electronic health records. LLMs employ tokenizers intheir input that break down text into smaller units. However, tokenizers arenot designed to represent numerical values and might struggle to understandrepetitive patterns and context, treating consecutive values as separate tokensand disregarding their temporal relationships. Here, we discuss recent worksthat employ LLMs for human-centric tasks such as in mobile health sensing andpresent a case study showing that popular LLMs tokenize temporal dataincorrectly. To address that, we highlight potential solutions such as prompttuning with lightweight embedding layers as well as multimodal adapters, thatcan help bridge this "modality gap". While the capability of language models togeneralize to other modalities with minimal or no finetuning is exciting, thispaper underscores the fact that their outputs cannot be meaningful if theystumble over input nuances.</description><author>Dimitris Spathis, Fahim Kawsar</author><pubDate>Tue, 12 Sep 2023 14:51:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06236v1</guid></item><item><title>The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature</title><link>http://arxiv.org/abs/2309.04198v2</link><description>The application of Large Language Models (LLMs) to the medical domain hasstimulated the interest of researchers. Recent studies have focused onconstructing Instruction Fine-Tuning (IFT) data through medical knowledgegraphs to enrich the interactive medical knowledge of LLMs. However, themedical literature serving as a rich source of medical knowledge remainsunexplored. Our work introduces the CALLA dataset to probe LLMs' interactiveknowledge acquisition from Chinese medical literature. It assesses theproficiency of LLMs in mastering medical knowledge through a free-dialoguefact-checking task. We identify a phenomenon called the ``fact-followingresponse``, where LLMs tend to affirm facts mentioned in questions and displaya reluctance to challenge them. To eliminate the inaccurate evaluation causedby this phenomenon, for the golden fact, we artificially construct test datafrom two perspectives: one consistent with the fact and one inconsistent withthe fact. Drawing from the probing experiment on the CALLA dataset, we concludethat IFT data highly correlated with the medical literature corpus serves as apotent catalyst for LLMs, enabling themselves to skillfully employ the medicalknowledge acquired during the pre-training phase within interactive scenarios,enhancing accuracy. Furthermore, we design a framework for automaticallyconstructing IFT data based on medical literature and discuss some real-worldapplications.</description><author>Yanrui Du, Sendong Zhao, Muzhen Cai, Jianyu Chen, Haochun Wang, Yuhan Chen, Haoqiang Guo, Bing Qin</author><pubDate>Tue, 12 Sep 2023 14:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04198v2</guid></item><item><title>A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models</title><link>http://arxiv.org/abs/2309.06230v1</link><description>Analysis of high-dimensional data has led to increased interest in bothsingle index models (SIMs) and best subset selection. SIMs provide aninterpretable and flexible modeling framework for high-dimensional data, whilebest subset selection aims to find a sparse model from a large set ofpredictors. However, best subset selection in high-dimensional models is knownto be computationally intractable. Existing methods tend to relax theselection, but do not yield the best subset solution. In this paper, wedirectly tackle the intractability by proposing the first provably scalablealgorithm for best subset selection in high-dimensional SIMs. Our algorithmicsolution enjoys the subset selection consistency and has the oracle propertywith a high probability. The algorithm comprises a generalized informationcriterion to determine the support size of the regression coefficients,eliminating the model selection tuning. Moreover, our method does not assume anerror distribution or a specific link function and hence is flexible to apply.Extensive simulation results demonstrate that our method is not onlycomputationally efficient but also able to exactly recover the best subset invarious settings (e.g., linear regression, Poisson regression, heteroscedasticmodels).</description><author>Borui Tang, Jin Zhu, Junxian Zhu, Xueqin Wang, Heping Zhang</author><pubDate>Tue, 12 Sep 2023 14:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06230v1</guid></item><item><title>On the Injunction of XAIxArt</title><link>http://arxiv.org/abs/2309.06227v1</link><description>The position paper highlights the range of concerns that are engulfed in theinjunction of explainable artificial intelligence in art (XAIxArt). Through aseries of quick sub-questions, it points towards the ambiguities concerning'explanation' and the postpositivist tradition of 'relevant explanation'.Rejecting both 'explanation' and 'relevant explanation', the paper takes astance that XAIxArt is a symptom of insecurity of the anthropocentric notion ofart and a nostalgic desire to return to outmoded notions of authorship andhuman agency. To justify this stance, the paper makes a distinction between anornamentation model of explanation to a model of explanation as sense-making.</description><author>Cheshta Arora, Debarun Sarkar</author><pubDate>Tue, 12 Sep 2023 14:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06227v1</guid></item><item><title>Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification</title><link>http://arxiv.org/abs/2302.02070v2</link><description>Existing image augmentation methods consist of two categories:perturbation-based methods and generative methods. Perturbation-based methodsapply pre-defined perturbations to augment an original image, but only locallyvary the image, thus lacking image diversity. In contrast, generative methodsbring more image diversity in the augmented images but may not preservesemantic consistency, thus incorrectly changing the essential semantics of theoriginal image. To balance image diversity and semantic consistency inaugmented images, we propose SGID, a Semantic-guided Generative Imageaugmentation method with Diffusion models for image classification.Specifically, SGID employs diffusion models to generate augmented images withgood image diversity. More importantly, SGID takes image labels and captions asguidance to maintain semantic consistency between the augmented and originalimages. Experimental results show that SGID outperforms the best augmentationbaseline by 1.72% on ResNet-50 (from scratch), 0.33% on ViT (ImageNet-21k), and0.14% on CLIP-ViT (LAION-2B). Moreover, SGID can be combined with other imageaugmentation baselines and further improves the overall performance. Wedemonstrate the semantic consistency and image diversity of SGID throughquantitative human and automated evaluations, as well as qualitative casestudies.</description><author>Bohan Li, Xiao Xu, Xinghao Wang, Yutai Hou, Yunlong Feng, Feng Wang, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che</author><pubDate>Tue, 12 Sep 2023 14:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02070v2</guid></item><item><title>Unveiling Signle-Bit-Flip Attacks on DNN Executables</title><link>http://arxiv.org/abs/2309.06223v1</link><description>Recent research has shown that bit-flip attacks (BFAs) can manipulate deepneural networks (DNNs) via DRAM Rowhammer exploitations. Existing attacks areprimarily launched over high-level DNN frameworks like PyTorch and flip bits inmodel weight files. Nevertheless, DNNs are frequently compiled into low-levelexecutables by deep learning (DL) compilers to fully leverage low-levelhardware primitives. The compiled code is usually high-speed and manifestsdramatically distinct execution paradigms from high-level DNN frameworks. In this paper, we launch the first systematic study on the attack surface ofBFA specifically for DNN executables compiled by DL compilers. We design anautomated search tool to identify vulnerable bits in DNN executables andidentify practical attack vectors that exploit the model structure in DNNexecutables with BFAs (whereas prior works make likely strong assumptions toattack model weights). DNN executables appear more "opaque" than models inhigh-level DNN frameworks. Nevertheless, we find that DNN executables containextensive, severe (e.g., single-bit flip), and transferrable attack surfacesthat are not present in high-level DNN models and can be exploited to depletefull model intelligence and control output labels. Our finding calls forincorporating security mechanisms in future DNN compilation toolchains.</description><author>Yanzuo Chen, Zhibo Liu, Yuanyuan Yuan, Sihang Hu, Tianxiang Li, Shuai Wang</author><pubDate>Tue, 12 Sep 2023 14:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06223v1</guid></item><item><title>Use neural networks to recognize students' handwritten letters and incorrect symbols</title><link>http://arxiv.org/abs/2309.06221v1</link><description>Correcting students' multiple-choice answers is a repetitive and mechanicaltask that can be considered an image multi-classification task. Assumingpossible options are 'abcd' and the correct option is one of the four, somestudents may write incorrect symbols or options that do not exist. In thispaper, five classifications were set up - four for possible correct options andone for other incorrect writing. This approach takes into account thepossibility of non-standard writing options.</description><author>JiaJun Zhu, Zichuan Yang, Binjie Hong, Jiacheng Song, Jiwei Wang, Tianhao Chen, Shuilan Yang, Zixun Lan, Fei Ma</author><pubDate>Tue, 12 Sep 2023 14:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06221v1</guid></item><item><title>Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction</title><link>http://arxiv.org/abs/2309.06219v1</link><description>We introduce the task of automatic human action co-occurrence identification,i.e., determine whether two human actions can co-occur in the same interval oftime. We create and make publicly available the ACE (Action Co-occurrencE)dataset, consisting of a large graph of ~12k co-occurring pairs of visualactions and their corresponding video clips. We describe graph link predictionmodels that leverage visual and textual information to automatically infer iftwo actions are co-occurring. We show that graphs are particularly well suitedto capture relations between human actions, and the learned graphrepresentations are effective for our task and capture novel and relevantinformation across different data domains. The ACE dataset and the codeintroduced in this paper are publicly available athttps://github.com/MichiganNLP/vlog_action_co-occurrence.</description><author>Oana Ignat, Santiago Castro, Weiji Li, Rada Mihalcea</author><pubDate>Tue, 12 Sep 2023 14:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06219v1</guid></item><item><title>Zero-Resource Hallucination Prevention for Large Language Models</title><link>http://arxiv.org/abs/2309.02654v2</link><description>The prevalent use of large language models (LLMs) in various domains hasdrawn attention to the issue of "hallucination," which refers to instanceswhere LLMs generate factually inaccurate or ungrounded information. Existingtechniques for hallucination detection in language assistants rely on intricatefuzzy, specific free-language-based chain of thought (CoT) techniques orparameter-based methods that suffer from interpretability issues. Additionally,the methods that identify hallucinations post-generation could not preventtheir occurrence and suffer from inconsistent performance due to the influenceof the instruction format and model style. In this paper, we introduce a novelpre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, whichfocuses on evaluating the model's familiarity with the concepts present in theinput instruction and withholding the generation of response in case ofunfamiliar concepts. This approach emulates the human ability to refrain fromresponding to unfamiliar topics, thus reducing hallucinations. We validateSELF-FAMILIARITY across four different large language models, demonstratingconsistently superior performance compared to existing techniques. Our findingspropose a significant shift towards preemptive strategies for hallucinationmitigation in LLM assistants, promising improvements in reliability,applicability, and interpretability.</description><author>Junyu Luo, Cao Xiao, Fenglong Ma</author><pubDate>Tue, 12 Sep 2023 14:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02654v2</guid></item><item><title>Long-term drought prediction using deep neural networks based on geospatial weather data</title><link>http://arxiv.org/abs/2309.06212v1</link><description>The accurate prediction of drought probability in specific regions is crucialfor informed decision-making in agricultural practices. It is important to makepredictions one year in advance, particularly for long-term decisions. However,forecasting this probability presents challenges due to the complex interplayof various factors within the region of interest and neighboring areas. In thisstudy, we propose an end-to-end solution to address this issue based on variousspatiotemporal neural networks. The models considered focus on predicting thedrought intensity based on the Palmer Drought Severity Index (PDSI) forsubregions of interest, leveraging intrinsic factors and insights from climatemodels to enhance drought predictions. Comparative evaluations demonstrate the superior accuracy of ConvolutionalLSTM (ConvLSTM) and transformer models compared to baseline gradient boostingand logistic regression solutions. The two former models achieved impressiveROC AUC scores from 0.90 to 0.70 for forecast horizons from one to six months,outperforming baseline models. The transformer showed superiority for shorterhorizons, while ConvLSTM did so for longer horizons. Thus, we recommendselecting the models accordingly for long-term drought forecasting. To ensure the broad applicability of the considered models, we conductextensive validation across regions worldwide, considering differentenvironmental conditions. We also run several ablation and sensitivity studiesto challenge our findings and provide additional information on how to solvethe problem.</description><author>Vsevolod Grabar, Alexander Marusov, Alexey Zaytsev, Yury Maximov, Nazar Sotiriadi, Alexander Bulkin</author><pubDate>Tue, 12 Sep 2023 14:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06212v1</guid></item><item><title>SGFeat: Salient Geometric Feature for Point Cloud Registration</title><link>http://arxiv.org/abs/2309.06207v1</link><description>Point Cloud Registration (PCR) is a critical and challenging task in computervision. One of the primary difficulties in PCR is identifying salient andmeaningful points that exhibit consistent semantic and geometric propertiesacross different scans. Previous methods have encountered challenges withambiguous matching due to the similarity among patch blocks throughout theentire point cloud and the lack of consideration for efficient global geometricconsistency. To address these issues, we propose a new framework that includesseveral novel techniques. Firstly, we introduce a semantic-aware geometricencoder that combines object-level and patch-level semantic information. Thisencoder significantly improves registration recall by reducing ambiguity inpatch-level superpoint matching. Additionally, we incorporate a prior knowledgeapproach that utilizes an intrinsic shape signature to identify salient points.This enables us to extract the most salient super points and meaningful densepoints in the scene. Secondly, we introduce an innovative transformer thatencodes High-Order (HO) geometric features. These features are crucial foridentifying salient points within initial overlap regions while consideringglobal high-order geometric consistency. To optimize this high-ordertransformer further, we introduce an anchor node selection strategy. Byencoding inter-frame triangle or polyhedron consistency features based on theseanchor nodes, we can effectively learn high-order geometric features of salientsuper points. These high-order features are then propagated to dense points andutilized by a Sinkhorn matching module to identify key correspondences forsuccessful registration. In our experiments conducted on well-known datasetssuch as 3DMatch/3DLoMatch and KITTI, our approach has shown promising results,highlighting the effectiveness of our novel method.</description><author>Qianliang Wu, Yaqing Ding, Lei Luo, Chuanwei Zhou, Jin Xie, Jian Yang</author><pubDate>Tue, 12 Sep 2023 14:21:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06207v1</guid></item><item><title>Neural Fields for Interactive Visualization of Statistical Dependencies in 3D Simulation Ensembles</title><link>http://arxiv.org/abs/2307.02203v4</link><description>We present the first neural network that has learned to compactly representand can efficiently reconstruct the statistical dependencies between the valuesof physical variables at different spatial locations in large 3D simulationensembles. Going beyond linear dependencies, we consider mutual information asa measure of non-linear dependence. We demonstrate learning and reconstructionwith a large weather forecast ensemble comprising 1000 members, each storingmultiple physical variables at a 250 x 352 x 20 simulation grid. Bycircumventing compute-intensive statistical estimators at runtime, wedemonstrate significantly reduced memory and computation requirements forreconstructing the major dependence structures. This enables embedding theestimator into a GPU-accelerated direct volume renderer and interactivelyvisualizing all mutual dependencies for a selected domain point.</description><author>Fatemeh Farokhmanesh, Kevin Höhlein, Christoph Neuhauser, Rüdiger Westermann</author><pubDate>Tue, 12 Sep 2023 14:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02203v4</guid></item><item><title>Fast Sparse PCA via Positive Semidefinite Projection for Unsupervised Feature Selection</title><link>http://arxiv.org/abs/2309.06202v1</link><description>In the field of unsupervised feature selection, sparse principal componentanalysis (SPCA) methods have attracted more and more attention recently.Compared to spectral-based methods, SPCA methods don't rely on the constructionof a similarity matrix and show better feature selection ability on real-worlddata. The original SPCA formulates a nonconvex optimization problem. Existingconvex SPCA methods reformulate SPCA as a convex model by regarding thereconstruction matrix as an optimization variable. However, they are lack ofconstraints equivalent to the orthogonality restriction in SPCA, leading tolarger solution space. In this paper, it's proved that the optimal solution toa convex SPCA model falls onto the Positive Semidefinite (PSD) cone. A standardconvex SPCA-based model with PSD constraint for unsupervised feature selectionis proposed. Further, a two-step fast optimization algorithm via PSD projectionis presented to solve the proposed model. Two other existing convex SPCA-basedmodels are also proven to have their solutions optimized on the PSD cone inthis paper. Therefore, the PSD versions of these two models are proposed toaccelerate their convergence as well. We also provide a regularizationparameter setting strategy for our proposed method. Experiments on syntheticand real-world datasets demonstrate the effectiveness and efficiency of theproposed methods.</description><author>Junjing Zheng, Xinyu Zhang, Yongxiang Liu, Weidong Jiang, Kai Huo, Li Liu</author><pubDate>Tue, 12 Sep 2023 14:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06202v1</guid></item><item><title>ShaRPy: Shape Reconstruction and Hand Pose Estimation from RGB-D with Uncertainty</title><link>http://arxiv.org/abs/2303.10042v2</link><description>Despite their potential, markerless hand tracking technologies are not yetapplied in practice to the diagnosis or monitoring of the activity ininflammatory musculoskeletal diseases. One reason is that the focus of mostmethods lies in the reconstruction of coarse, plausible poses, whereas in theclinical context, accurate, interpretable, and reliable results are required.Therefore, we propose ShaRPy, the first RGB-D Shape Reconstruction and handPose tracking system, which provides uncertainty estimates of the computedpose, e.g., when a finger is hidden or its estimate is inconsistent with theobservations in the input, to guide clinical decision-making. Besides pose,ShaRPy approximates a personalized hand shape, promoting a more realistic andintuitive understanding of its digital twin. Our method requires only alight-weight setup with a single consumer-level RGB-D camera yet it is able todistinguish similar poses with only small joint angle deviations in ametrically accurate space. This is achieved by combining a data-driven densecorrespondence predictor with traditional energy minimization. To bridge thegap between interactive visualization and biomedical simulation we leverage aparametric hand model in which we incorporate biomedical constraints andoptimize for both, its pose and hand shape. We evaluate ShaRPy on a keypointdetection benchmark and show qualitative results of hand function assessmentsfor activity monitoring of musculoskeletal diseases.</description><author>Vanessa Wirth, Anna-Maria Liphardt, Birte Coppers, Johanna Bräunig, Simon Heinrich, Sigrid Leyendecker, Arnd Kleyer, Georg Schett, Martin Vossiek, Bernhard Egger, Marc Stamminger</author><pubDate>Tue, 12 Sep 2023 14:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10042v2</guid></item><item><title>SCP: Scene Completion Pre-training for 3D Object Detection</title><link>http://arxiv.org/abs/2309.06199v1</link><description>3D object detection using LiDAR point clouds is a fundamental task in thefields of computer vision, robotics, and autonomous driving. However, existing3D detectors heavily rely on annotated datasets, which are both time-consumingand prone to errors during the process of labeling 3D bounding boxes. In thispaper, we propose a Scene Completion Pre-training (SCP) method to enhance theperformance of 3D object detectors with less labeled data. SCP offers three keyadvantages: (1) Improved initialization of the point cloud model. By completingthe scene point clouds, SCP effectively captures the spatial and semanticrelationships among objects within urban environments. (2) Elimination of theneed for additional datasets. SCP serves as a valuable auxiliary network thatdoes not impose any additional efforts or data requirements on the 3Ddetectors. (3) Reduction of the amount of labeled data for detection. With thehelp of SCP, the existing state-of-the-art 3D detectors can achieve comparableperformance while only relying on 20% labeled data.</description><author>Yiming Shan, Yan Xia, Yuhong Chen, Daniel Cremers</author><pubDate>Tue, 12 Sep 2023 14:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06199v1</guid></item><item><title>360$^\circ$ from a Single Camera: A Few-Shot Approach for LiDAR Segmentation</title><link>http://arxiv.org/abs/2309.06197v1</link><description>Deep learning applications on LiDAR data suffer from a strong domain gap whenapplied to different sensors or tasks. In order for these methods to obtainsimilar accuracy on different data in comparison to values reported on publicbenchmarks, a large scale annotated dataset is necessary. However, in practicalapplications labeled data is costly and time consuming to obtain. Such factorshave triggered various research in label-efficient methods, but a large gapremains to their fully-supervised counterparts. Thus, we propose ImageTo360, aneffective and streamlined few-shot approach to label-efficient LiDARsegmentation. Our method utilizes an image teacher network to generate semanticpredictions for LiDAR data within a single camera view. The teacher is used topretrain the LiDAR segmentation student network, prior to optional fine-tuningon 360$^\circ$ data. Our method is implemented in a modular manner on the pointlevel and as such is generalizable to different architectures. We improve overthe current state-of-the-art results for label-efficient methods and evensurpass some traditional fully-supervised segmentation networks.</description><author>Laurenz Reichardt, Nikolas Ebert, Oliver Wasenmüller</author><pubDate>Tue, 12 Sep 2023 14:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06197v1</guid></item><item><title>Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding</title><link>http://arxiv.org/abs/2309.06195v1</link><description>Solving linear inverse problems plays a crucial role in numerousapplications. Algorithm unfolding based, model-aware data-driven approacheshave gained significant attention for effectively addressing these problems.Learned iterative soft-thresholding algorithm (LISTA) and alternating directionmethod of multipliers compressive sensing network (ADMM-CSNet) are two widelyused such approaches, based on ISTA and ADMM algorithms, respectively. In thiswork, we study optimization guarantees, i.e., achieving near-zero training losswith the increase in the number of learning epochs, for finite-layer unfoldednetworks such as LISTA and ADMM-CSNet with smooth soft-thresholding in anover-parameterized (OP) regime. We achieve this by leveraging a modifiedversion of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying thePL$^*$ condition within a specific region of the loss landscape ensures theexistence of a global minimum and exponential convergence from initializationusing gradient descent based methods. Hence, we provide conditions, in terms ofthe network width and the number of training samples, on these unfoldednetworks for the PL$^*$ condition to hold. We achieve this by deriving theHessian spectral norm of these networks. Additionally, we show that thethreshold on the number of training samples increases with the increase in thenetwork width. Furthermore, we compare the threshold on training samples ofunfolded networks with that of a standard fully-connected feed-forward network(FFNN) with smooth soft-thresholding non-linearity. We prove that unfoldednetworks have a higher threshold value than FFNN. Consequently, one can expecta better expected error for unfolded networks than FFNN.</description><author>Shaik Basheeruddin Shah, Pradyumna Pradhan, Wei Pu, Ramunaidu Randhi, Miguel R. D. Rodrigues, Yonina C. Eldar</author><pubDate>Tue, 12 Sep 2023 14:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06195v1</guid></item><item><title>A 3M-Hybrid Model for the Restoration of Unique Giant Murals: A Case Study on the Murals of Yongle Palace</title><link>http://arxiv.org/abs/2309.06194v1</link><description>The Yongle Palace murals, as valuable cultural heritage, have sufferedvarying degrees of damage, making their restoration of significant importance.However, the giant size and unique data of Yongle Palace murals presentchallenges for existing deep-learning based restoration methods: 1) Thedistinctive style introduces domain bias in traditional transfer learning-basedrestoration methods, while the scarcity of mural data further limits theapplicability of these methods. 2) Additionally, the giant size of these muralsresults in a wider range of defect types and sizes, necessitating models withgreater adaptability. Consequently, there is a lack of focus on deeplearning-based restoration methods for the unique giant murals of YonglePalace. Here, a 3M-Hybrid model is proposed to address these challenges.Firstly, based on the characteristic that the mural data frequency is prominentin the distribution of low and high frequency features, high and low frequencyfeatures are separately abstracted for complementary learning. Furthermore, weintegrate a pre-trained Vision Transformer model (VIT) into the CNN module,allowing us to leverage the benefits of a large model while mitigating domainbias. Secondly, we mitigate seam and structural distortion issues resultingfrom the restoration of large defects by employing a multi-scale andmulti-perspective strategy, including data segmentation and fusion.Experimental results demonstrate the efficacy of our proposed model. Inregular-sized mural restoration, it improves SSIM and PSNR by 14.61% and 4.73%,respectively, compared to the best model among four representative CNN models.Additionally, it achieves favorable results in the final restoration of giantmurals.</description><author>Jing Yang, Nur Intan Raihana Ruhaiyem, Chichun Zhou</author><pubDate>Tue, 12 Sep 2023 14:03:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06194v1</guid></item><item><title>Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains</title><link>http://arxiv.org/abs/2309.06192v1</link><description>News recommender systems play an increasingly influential role in shapinginformation access within democratic societies. However, tailoringrecommendations to users' specific interests can result in the divergence ofinformation streams. Fragmented access to information poses challenges to theintegrity of the public sphere, thereby influencing democracy and publicdiscourse. The Fragmentation metric quantifies the degree of fragmentation ofinformation streams in news recommendations. Accurate measurement of thismetric requires the application of Natural Language Processing (NLP) toidentify distinct news events, stories, or timelines. This paper presents anextensive investigation of various approaches for quantifying Fragmentation innews recommendations. These approaches are evaluated both intrinsically, bymeasuring performance on news story clustering, and extrinsically, by assessingthe Fragmentation scores of different simulated news recommender scenarios. Ourfindings demonstrate that agglomerative hierarchical clustering coupled withSentenceBERT text representation is substantially better at detectingFragmentation than earlier implementations. Additionally, the analysis ofsimulated scenarios yields valuable insights and recommendations forstakeholders concerning the measurement and interpretation of Fragmentation.</description><author>Alessandra Polimeno, Myrthe Reuver, Sanne Vrijenhoek, Antske Fokkens</author><pubDate>Tue, 12 Sep 2023 14:01:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06192v1</guid></item></channel></rss>