<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 16 Dec 2024 13:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction</title><link>http://arxiv.org/abs/2412.10373v1</link><description>3D occupancy prediction is important for autonomous driving due to itscomprehensive perception of the surroundings. To incorporate sequential inputs,most existing methods fuse representations from previous frames to infer thecurrent 3D occupancy. However, they fail to consider the continuity of drivingscenarios and ignore the strong prior provided by the evolution of 3D scenes(e.g., only dynamic objects move). In this paper, we propose aworld-model-based framework to exploit the scene evolution for perception. Wereformulate 3D occupancy prediction as a 4D occupancy forecasting problemconditioned on the current sensor input. We decompose the scene evolution intothree factors: 1) ego motion alignment of static scenes; 2) local movements ofdynamic objects; and 3) completion of newly-observed scenes. We then employ aGaussian world model (GaussianWorld) to explicitly exploit these priors andinfer the scene evolution in the 3D Gaussian space considering the current RGBobservation. We evaluate the effectiveness of our framework on the widely usednuScenes dataset. Our GaussianWorld improves the performance of thesingle-frame counterpart by over 2% in mIoU without introducing additionalcomputations. Code: https://github.com/zuosc19/GaussianWorld.</description><author>Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, Jiwen Lu</author><pubDate>Fri, 13 Dec 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10373v1</guid></item><item><title>UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities</title><link>http://arxiv.org/abs/2412.10372v1</link><description>Vision-Language Models (VLMs) trained via contrastive learning have achievednotable success in natural image tasks. However, their application in themedical domain remains limited due to the scarcity of openly accessible,large-scale medical image-text datasets. Existing medical VLMs either train onclosed-source proprietary or relatively small open-source datasets that do notgeneralize well. Similarly, most models remain specific to a single or limitednumber of medical imaging domains, again restricting their applicability toother modalities. To address this gap, we introduce UniMed, a large-scale,open-source multi-modal medical dataset comprising over 5.3 million image-textpairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,Pathology, and Fundus. UniMed is developed using a data-collection frameworkthat leverages Large Language Models (LLMs) to transform modality-specificclassification datasets into image-text formats while incorporating existingimage-text data from the medical domain, facilitating scalable VLM pretraining.Using UniMed, we trained UniMed-CLIP, a unified VLM for six modalities thatsignificantly outperforms existing generalist VLMs and matchesmodality-specific medical VLMs, achieving notable gains in zero-shotevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained onproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,while using 3x less training data. To facilitate future research, we releaseUniMed dataset, training codes, and models athttps://github.com/mbzuai-oryx/UniMed-CLIP.</description><author>Muhammad Uzair Khattak, Shahina Kunhimon, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Fri, 13 Dec 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10372v1</guid></item><item><title>GaussianAD: Gaussian-Centric End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2412.10371v1</link><description>Vision-based autonomous driving shows great potential due to its satisfactoryperformance and low costs. Most existing methods adopt dense representations(e.g., bird's eye view) or sparse representations (e.g., instance boxes) fordecision-making, which suffer from the trade-off between comprehensiveness andefficiency. This paper explores a Gaussian-centric end-to-end autonomousdriving (GaussianAD) framework and exploits 3D semantic Gaussians toextensively yet sparsely describe the scene. We initialize the scene withuniform 3D Gaussians and use surrounding-view images to progressively refinethem to obtain the 3D Gaussian scene representation. We then use sparseconvolutions to efficiently perform 3D perception (e.g., 3D detection, semanticmap construction). We predict 3D flows for the Gaussians with dynamic semanticsand plan the ego trajectory accordingly with an objective of future sceneforecasting. Our GaussianAD can be trained in an end-to-end manner withoptional perception labels when available. Extensive experiments on the widelyused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD onvarious tasks including motion planning, 3D occupancy prediction, and 4Doccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.</description><author>Wenzhao Zheng, Junjie Wu, Yao Zheng, Sicheng Zuo, Zixun Xie, Longchao Yang, Yong Pan, Zhihui Hao, Peng Jia, Xianpeng Lang, Shanghang Zhang</author><pubDate>Fri, 13 Dec 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10371v1</guid></item><item><title>Inverse Reinforcement Learning by Estimating Expertise of Demonstrators</title><link>http://arxiv.org/abs/2402.01886v2</link><description>In Imitation Learning (IL), utilizing suboptimal and heterogeneousdemonstrations presents a substantial challenge due to the varied nature ofreal-world data. However, standard IL algorithms consider these datasets ashomogeneous, thereby inheriting the deficiencies of suboptimal demonstrators.Previous approaches to this issue rely on impractical assumptions likehigh-quality data subsets, confidence rankings, or explicit environmentalknowledge. This paper introduces IRLEED, Inverse Reinforcement Learning byEstimating Expertise of Demonstrators, a novel framework that overcomes thesehurdles without prior knowledge of demonstrator expertise. IRLEED enhancesexisting Inverse Reinforcement Learning (IRL) algorithms by combining a generalmodel for demonstrator suboptimality to address reward bias and actionvariance, with a Maximum Entropy IRL framework to efficiently derive theoptimal policy from diverse, suboptimal demonstrations. Experiments in bothonline and offline IL settings, with simulated and human-generated data,demonstrate IRLEED's adaptability and effectiveness, making it a versatilesolution for learning from suboptimal demonstrations.</description><author>Mark Beliaev, Ramtin Pedarsani</author><pubDate>Fri, 13 Dec 2024 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01886v2</guid></item><item><title>A Grounded Typology of Word Classes</title><link>http://arxiv.org/abs/2412.10369v1</link><description>We propose a grounded approach to meaning in language typology. We treat datafrom perceptual modalities, such as images, as a language-agnosticrepresentation of meaning. Hence, we can quantify the function--formrelationship between images and captions across languages. Inspired byinformation theory, we define "groundedness", an empirical measure ofcontextual semantic contentfulness (formulated as a difference in surprisal)which can be computed with multilingual multimodal language models. As a proofof concept, we apply this measure to the typology of word classes. Our measurecaptures the contentfulness asymmetry between functional (grammatical) andlexical (content) classes across languages, but contradicts the view thatfunctional classes do not convey content. Moreover, we find universal trends inthe hierarchy of groundedness (e.g., nouns &gt; adjectives &gt; verbs), and show thatour measure partly correlates with psycholinguistic concreteness norms inEnglish. We release a dataset of groundedness scores for 30 languages. Ourresults suggest that the grounded typology approach can provide quantitativeevidence about semantic function in language.</description><author>Coleman Haley, Sharon Goldwater, Edoardo Ponti</author><pubDate>Fri, 13 Dec 2024 18:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10369v1</guid></item><item><title>OP-LoRA: The Blessing of Dimensionality</title><link>http://arxiv.org/abs/2412.10362v1</link><description>Low-rank adapters enable fine-tuning of large models with only a small numberof parameters, thus reducing storage costs and minimizing the risk ofcatastrophic forgetting. However, they often pose optimization challenges, withpoor convergence. To overcome these challenges, we introduce anover-parameterized approach that accelerates training without increasinginference costs. This method reparameterizes low-rank adaptation by employing aseparate MLP and learned embedding for each layer. The learned embedding isinput to the MLP, which generates the adapter parameters. Suchoverparamaterization has been shown to implicitly function as an adaptivelearning rate and momentum, accelerating optimization. At inference time, theMLP can be discarded, leaving behind a standard low-rank adapter. To study theeffect of MLP overparameterization on a small yet difficult proxy task, weimplement it for matrix factorization, and find it achieves faster convergenceand lower final loss. Extending this approach to larger-scale tasks, we observeconsistent performance gains across domains. We achieve improvements invision-language tasks and especially notable increases in image generation,with CMMD scores improving by up to 15 points.</description><author>Piotr Teterwak, Kate Saenko, Bryan A. Plummer, Ser-Nam Lim</author><pubDate>Fri, 13 Dec 2024 18:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10362v1</guid></item><item><title>Apollo: An Exploration of Video Understanding in Large Multimodal Models</title><link>http://arxiv.org/abs/2412.10360v1</link><description>Despite the rapid integration of video perception capabilities into LargeMultimodal Models (LMMs), the underlying mechanisms driving their videounderstanding remain poorly understood. Consequently, many design decisions inthis domain are made without proper justification or analysis. The highcomputational cost of training and evaluating such models, coupled with limitedopen research, hinders the development of video-LMMs. To address this, wepresent a comprehensive study that helps uncover what effectively drives videounderstanding in LMMs. We begin by critically examining the primary contributors to the highcomputational requirements associated with video-LMM research and discoverScaling Consistency, wherein design and training decisions made on smallermodels and datasets (up to a critical size) effectively transfer to largermodels. Leveraging these insights, we explored many video-specific aspects ofvideo-LMMs, including video sampling, architectures, data composition, trainingschedules, and more. For example, we demonstrated that fps sampling duringtraining is vastly preferable to uniform frame sampling and which visionencoders are the best for video representation. Guided by these findings, we introduce Apollo, a state-of-the-art family ofLMMs that achieve superior performance across different model sizes. Our modelscan perceive hour-long videos efficiently, with Apollo-3B outperforming mostexisting $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B isstate-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 onVideo-MME.</description><author>Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, Xide Xia</author><pubDate>Fri, 13 Dec 2024 18:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10360v1</guid></item><item><title>The Correlated Gaussian Sparse Histogram Mechanism</title><link>http://arxiv.org/abs/2412.10357v1</link><description>We consider the problem of releasing a sparse histogram under $(\varepsilon,\delta)$-differential privacy. The stability histogram independently adds noisefrom a Laplace or Gaussian distribution to the non-zero entries and removesthose noisy counts below a threshold. Thereby, the introduction of new non-zero values between neighboringhistograms is only revealed with probability at most $\delta$, and typically,the value of the threshold dominates the error of the mechanism. We considerthe variant of the stability histogram with Gaussian noise. Recent works ([Joseph and Yu, COLT '24] and [Lebeda, SOSA '25]) reduced theerror for private histograms using correlated Gaussian noise. However, thesetechniques can not be directly applied in the very sparse setting. Instead, weadopt Lebeda's technique and show that adding correlated noise to the non-zerocounts only allows us to reduce the magnitude of noise when we have a sparsitybound. This, in turn, allows us to use a lower threshold by up to a factor of$1/2$ compared to the non-correlated noise mechanism. We then extend ourmechanism to a setting without a known bound on sparsity. Additionally, we showthat correlated noise can give a similar improvement for the more practicaldiscrete Gaussian mechanism.</description><author>Christian Janos Lebeda, Lukas Retschmeier</author><pubDate>Fri, 13 Dec 2024 18:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10357v1</guid></item><item><title>A Library for Learning Neural Operators</title><link>http://arxiv.org/abs/2412.10354v1</link><description>We present NeuralOperator, an open-source Python library for operatorlearning. Neural operators generalize neural networks to maps between functionspaces instead of finite-dimensional Euclidean spaces. They can be trained andinferenced on input and output functions given at various discretizations,satisfying a discretization convergence properties. Built on top of PyTorch,NeuralOperator provides all the tools for training and deploying neuraloperator models, as well as developing new ones, in a high-quality, tested,open-source package. It combines cutting-edge models and customizability with agentle learning curve and simple user interface for newcomers.</description><author>Jean Kossaifi, Nikola Kovachki, Zongyi Li, Davit Pitt, Miguel Liu-Schiaffini, Robert Joseph George, Boris Bonev, Kamyar Azizzadenesheli, Julius Berner, Anima Anandkumar</author><pubDate>Fri, 13 Dec 2024 18:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10354v1</guid></item><item><title>Robust image classification with multi-modal large language models</title><link>http://arxiv.org/abs/2412.10353v1</link><description>Deep Neural Networks are vulnerable to adversarial examples, i.e., carefullycrafted input samples that can cause models to make incorrect predictions withhigh confidence. To mitigate these vulnerabilities, adversarial training anddetection-based defenses have been proposed to strengthen models in advance.However, most of these approaches focus on a single data modality, overlookingthe relationships between visual patterns and textual descriptions of theinput. In this paper, we propose a novel defense, Multi-Shield, designed tocombine and complement these defenses with multi-modal information to furtherenhance their robustness. Multi-Shield leverages multi-modal large languagemodels to detect adversarial examples and abstain from uncertainclassifications when there is no alignment between textual and visualrepresentations of the input. Extensive evaluations on CIFAR-10 and ImageNetdatasets, using robust and non-robust image classification models, demonstratethat Multi-Shield can be easily integrated to detect and reject adversarialexamples, outperforming the original defenses.</description><author>Francesco Villani, Igor Maljkovic, Dario Lazzaro, Angelo Sotgiu, Antonio Emanuele Cinà, Fabio Roli</author><pubDate>Fri, 13 Dec 2024 18:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10353v1</guid></item><item><title>VibrantVS: A high-resolution multi-task transformer for forest canopy height estimation</title><link>http://arxiv.org/abs/2412.10351v1</link><description>This paper explores the application of a novel multi-task vision transformer(ViT) model for the estimation of canopy height models (CHMs) using 4-bandNational Agriculture Imagery Program (NAIP) imagery across the western UnitedStates. We compare the effectiveness of this model in terms of accuracy andprecision aggregated across ecoregions and class heights versus three otherbenchmark peer-reviewed models. Key findings suggest that, while otherbenchmark models can provide high precision in localized areas, the VibrantVSmodel has substantial advantages across a broad reach of ecoregions in thewestern United States with higher accuracy, higher precision, the ability togenerate updated inference at a cadence of three years or less, and highspatial resolution. The VibrantVS model provides significant value forecological monitoring and land management decisions for wildfire mitigation.</description><author>Tony Chang, Kiarie Ndegwa, Andreas Gros, Vincent A. Landau, Luke J. Zachmann, Bogdan State, Mitchell A. Gritts, Colton W. Miller, Nathan E. Rutenbeck, Scott Conway, Guy Bayes</author><pubDate>Fri, 13 Dec 2024 18:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10351v1</guid></item><item><title>Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration</title><link>http://arxiv.org/abs/2412.10349v1</link><description>In dynamic environments, robots often encounter constrained movementtrajectories when manipulating objects with specific properties, such as doors.Therefore, applying the appropriate force is crucial to prevent damage to boththe robots and the objects. However, current vision-guided robot stategeneration methods often falter in this regard, as they lack the integration oftactile perception. To tackle this issue, this paper introduces a novel statediffusion framework termed SafeDiff. It generates a prospective state sequencefrom the current robot state and visual context observation while incorporatingreal-time tactile feedback to refine the sequence. As far as we know, this isthe first study specifically focused on ensuring force safety in roboticmanipulation. It significantly enhances the rationality of state planning, andthe safe action trajectory is derived from inverse dynamics based on thisrefined planning. In practice, unlike previous approaches that concatenatevisual and tactile data to generate future robot state sequences, our methodemploys tactile data as a calibration signal to adjust the robot's state withinthe state space implicitly. Additionally, we've developed a large-scalesimulation dataset called SafeDoorManip50k, offering extensive multimodal datato train and evaluate the proposed method. Extensive experiments show that ourvisual-tactile model substantially mitigates the risk of harmful forces in thedoor opening, across both simulated and real-world settings.</description><author>Lai Wei, Jiahua Ma, Yibo Hu, Ruimao Zhang</author><pubDate>Fri, 13 Dec 2024 18:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10349v1</guid></item><item><title>A dual contrastive framework</title><link>http://arxiv.org/abs/2412.10348v1</link><description>In current multimodal tasks, models typically freeze the encoder and decoderwhile adapting intermediate layers to task-specific goals, such as regioncaptioning. Region-level visual understanding presents significant challengesfor large-scale vision-language models. While limited spatial awareness is aknown issue, coarse-grained pretraining, in particular, exacerbates thedifficulty of optimizing latent representations for effective encoder-decoderalignment. We propose AlignCap, a framework designed to enhance region-levelunderstanding through fine-grained alignment of latent spaces. Our approachintroduces a novel latent feature refinement module that enhances conditionedlatent space representations to improve region-level captioning performance. Wealso propose an innovative alignment strategy, the semantic space alignmentmodule, which boosts the quality of multimodal representations. Additionally,we incorporate contrastive learning in a novel manner within both modules tofurther enhance region-level captioning performance. To address spatiallimitations, we employ a General Object Detection (GOD) method as a datapreprocessing pipeline that enhances spatial reasoning at the regional level.Extensive experiments demonstrate that our approach significantly improvesregion-level captioning performance across various tasks</description><author>Yuan Sun, Zhao Zhang, Jorge Ortiz</author><pubDate>Fri, 13 Dec 2024 18:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10348v1</guid></item><item><title>COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models</title><link>http://arxiv.org/abs/2412.10347v1</link><description>As key elements within the central dogma, DNA, RNA, and proteins play crucialroles in maintaining life by guaranteeing accurate genetic expression andimplementation. Although research on these molecules has profoundly impactedfields like medicine, agriculture, and industry, the diversity of machinelearning approaches-from traditional statistical methods to deep learningmodels and large language models-poses challenges for researchers in choosingthe most suitable models for specific tasks, especially for cross-omics andmulti-omics tasks due to the lack of comprehensive benchmarks. To address this,we introduce the first comprehensive multi-omics benchmark COMET (Benchmark forBiological COmprehensive Multi-omics Evaluation Tasks and Language Models),designed to evaluate models across single-omics, cross-omics, and multi-omicstasks. First, we curate and develop a diverse collection of downstream tasksand datasets covering key structural and functional aspects in DNA, RNA, andproteins, including tasks that span multiple omics levels. Then, we evaluateexisting foundational language models for DNA, RNA, and proteins, as well asthe newly proposed multi-omics method, offering valuable insights into theirperformance in integrating and analyzing data from different biologicalmodalities. This benchmark aims to define critical issues in multi-omicsresearch and guide future directions, ultimately promoting advancements inunderstanding biological processes through integrated and different omics dataanalysis.</description><author>Yuchen Ren, Wenwei Han, Qianyuan Zhang, Yining Tang, Weiqiang Bai, Yuchen Cai, Lifeng Qiao, Hao Jiang, Dong Yuan, Tao Chen, Siqi Sun, Pan Tan, Wanli Ouyang, Nanqing Dong, Xinzhu Ma, Peng Ye</author><pubDate>Fri, 13 Dec 2024 18:42:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10347v1</guid></item><item><title>TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</title><link>http://arxiv.org/abs/2412.10345v1</link><description>Although large vision-language-action (VLA) models pretrained on extensiverobot datasets offer promising generalist policies for robotic learning, theystill struggle with spatial-temporal dynamics in interactive robotics, makingthem less effective in handling complex tasks, such as manipulation. In thiswork, we introduce visual trace prompting, a simple yet effective approach tofacilitate VLA models' spatial-temporal awareness for action prediction byencoding state-action trajectories visually. We develop a new TraceVLA model byfinetuning OpenVLA on our own collected dataset of 150K robot manipulationtrajectories using visual trace prompting. Evaluations of TraceVLA across 137configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstratestate-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and3.5x on real-robot tasks and exhibiting robust generalization across diverseembodiments and scenarios. To further validate the effectiveness and generalityof our method, we present a compact VLA model based on 4B Phi-3-Vision,pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7BOpenVLA baseline while significantly improving inference efficiency.</description><author>Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, Jianwei Yang</author><pubDate>Fri, 13 Dec 2024 18:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10345v1</guid></item><item><title>Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining</title><link>http://arxiv.org/abs/2412.10342v1</link><description>Digital agents are increasingly employed to automate tasks in interactivedigital environments such as web pages, software applications, and operatingsystems. While text-based agents built on Large Language Models (LLMs) oftenrequire frequent updates due to platform-specific APIs, visual agentsleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptabilityby interacting directly with Graphical User Interfaces (GUIs). However, theseagents face significant challenges in visual perception, particularly whenhandling high-resolution, visually complex digital environments. This paperintroduces Iris, a foundational visual agent that addresses these challengesthrough two key innovations: Information-Sensitive Cropping (ISC) andSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizesvisually dense regions using a edge detection algorithm, enabling efficientprocessing by allocating more computational resources to areas with higherinformation density. SRDL enhances the agent's ability to handle complex tasksby leveraging a dual-learning loop, where improvements in referring (describingUI elements) reinforce grounding (locating elements) and vice versa, allwithout requiring additional annotated data. Empirical evaluations demonstratethat Iris achieves state-of-the-art performance across multiple benchmarks withonly 850K GUI annotations, outperforming methods using 10x more training data.These improvements further translate to significant gains in both web and OSagent downstream tasks.</description><author>Zhiqi Ge, Juncheng Li, Xinglei Pang, Minghe Gao, Kaihang Pan, Wang Lin, Hao Fei, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Fri, 13 Dec 2024 18:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10342v1</guid></item><item><title>Shape error prediction in 5-axis machining using graph neural networks</title><link>http://arxiv.org/abs/2412.10341v1</link><description>This paper presents an innovative method for predicting shape errors in5-axis machining using graph neural networks. The graph structure is definedwith nodes representing workpiece surface points and edges denoting theneighboring relationships. The dataset encompasses data from a material removalsimulation, process data, and post-machining quality information. Experimentalresults show that the presented approach can generalize the shape errorprediction for the investigated workpiece geometry. Moreover, by modellingspatial and temporal connections within the workpiece, the approach handles alow number of labels compared to non-graphical methods such as Support VectorMachines.</description><author>Julia Huuk, Abheek Dhingra, Eirini Ntoutsi, Bernd Denkena</author><pubDate>Fri, 13 Dec 2024 18:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10341v1</guid></item><item><title>A Universal Degradation-based Bridging Technique for Domain Adaptive Semantic Segmentation</title><link>http://arxiv.org/abs/2412.10339v1</link><description>Semantic segmentation often suffers from significant performance degradationwhen the trained network is applied to a different domain. To address thisissue, unsupervised domain adaptation (UDA) has been extensively studied.Existing methods introduce the domain bridging techniques to mitigatesubstantial domain gap, which construct intermediate domains to facilitate thegradual transfer of knowledge across different domains. However, thesestrategies often require dataset-specific designs and may generate unnaturalintermediate distributions that lead to semantic shift. In this paper, wepropose DiDA, a universal degradation-based bridging technique formalized as adiffusion forward process. DiDA consists of two key modules: (1)Degradation-based Intermediate Domain Construction, which creates continuousintermediate domains through simple image degradation operations to encouragelearning domain-invariant features as domain differences gradually diminish;(2) Semantic Shift Compensation, which leverages a diffusion encoder to encodeand compensate for semantic shift information with degraded time-steps,preserving discriminative representations in the intermediate domains. As aplug-and-play solution, DiDA supports various degradation operations andseamlessly integrates with existing UDA methods. Extensive experiments onprevalent synthetic-to-real semantic segmentation benchmarks demonstrate thatDiDA consistently improves performance across different settings and achievesnew state-of-the-art results when combined with existing methods.</description><author>Wangkai Li, Rui Sun, Tianzhu Zhang</author><pubDate>Fri, 13 Dec 2024 18:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10339v1</guid></item><item><title>XYScanNet: An Interpretable State Space Model for Perceptual Image Deblurring</title><link>http://arxiv.org/abs/2412.10338v1</link><description>Deep state-space models (SSMs), like recent Mamba architectures, are emergingas a promising alternative to CNN and Transformer networks. ExistingMamba-based restoration methods process the visual data by leveraging aflatten-and-scan strategy that converts image patches into a 1D sequence beforescanning. However, this scanning paradigm ignores local pixel dependencies andintroduces spatial misalignment by positioning distant pixels incorrectlyadjacent, which reduces local noise-awareness and degrades image sharpness inlow-level vision tasks. To overcome these issues, we propose a novelslice-and-scan strategy that alternates scanning along intra- and inter-slices.We further design a new Vision State Space Module (VSSM) for image deblurring,and tackle the inefficiency challenges of the current Mamba-based visionmodule. Building upon this, we develop XYScanNet, an SSM architectureintegrated with a lightweight feature fusion module for enhanced imagedeblurring. XYScanNet, maintains competitive distortion metrics andsignificantly improves perceptual performance. Experimental results show thatXYScanNet enhances KID by $17\%$ compared to the nearest competitor. Our codewill be released soon.</description><author>Hanzhou Liu, Chengkai Liu, Jiacong Xu, Peng Jiang, Mi Lu</author><pubDate>Fri, 13 Dec 2024 18:33:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10338v1</guid></item><item><title>Generative AI in Medicine</title><link>http://arxiv.org/abs/2412.10337v1</link><description>The increased capabilities of generative AI have dramatically expanded itspossible use cases in medicine. We provide a comprehensive overview ofgenerative AI use cases for clinicians, patients, clinical trial organizers,researchers, and trainees. We then discuss the many challenges -- includingmaintaining privacy and security, improving transparency and interpretability,upholding equity, and rigorously evaluating models -- which must be overcome torealize this potential, and the open research directions they give rise to.</description><author>Divya Shanmugam, Monica Agrawal, Rajiv Movva, Irene Y. Chen, Marzyeh Ghassemi, Emma Pierson</author><pubDate>Fri, 13 Dec 2024 18:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10337v1</guid></item><item><title>Geometric sparsification in recurrent neural networks</title><link>http://arxiv.org/abs/2406.06290v2</link><description>A common technique for ameliorating the computational costs of running largeneural models is sparsification, or the pruning of neural connections duringtraining. Sparse models are capable of maintaining the high accuracy of stateof the art models, while functioning at the cost of more parsimonious models.The structures which underlie sparse architectures are, however, poorlyunderstood and not consistent between differently trained models andsparsification schemes. In this paper, we propose a new technique forsparsification of recurrent neural nets (RNNs), called moduli regularization,in combination with magnitude pruning. Moduli regularization leverages thedynamical system induced by the recurrent structure to induce a geometricrelationship between neurons in the hidden state of the RNN. By making ourregularizing term explicitly geometric, we provide the first, to our knowledge,a priori description of the desired sparse architecture of our neural net, aswell as explicit end-to-end learning of RNN geometry. We verify theeffectiveness of our scheme under diverse conditions, testing in navigation,natural language processing, and addition RNNs. Navigation is a structurallygeometric task, for which there are known moduli spaces, and we show thatregularization can be used to reach 90% sparsity while maintaining modelperformance only when coefficients are chosen in accordance with a suitablemoduli space. Natural language processing and addition, however, have no knownmoduli space in which computations are performed. Nevertheless, we show thatmoduli regularization induces more stable recurrent neural nets, and achieveshigh fidelity models above 90% sparsity.</description><author>Wyatt Mackey, Ioannis Schizas, Jared Deighton, David L. Boothe, Jr., Vasileios Maroulas</author><pubDate>Fri, 13 Dec 2024 18:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06290v2</guid></item><item><title>AdvPrefix: An Objective for Nuanced LLM Jailbreaks</title><link>http://arxiv.org/abs/2412.10321v1</link><description>Many jailbreak attacks on large language models (LLMs) rely on a commonobjective: making the model respond with the prefix "Sure, here is (harmfulrequest)". While straightforward, this objective has two limitations: limitedcontrol over model behaviors, often resulting in incomplete or unrealisticresponses, and a rigid format that hinders optimization. To address theselimitations, we introduce AdvPrefix, a new prefix-forcing objective thatenables more nuanced control over model behavior while being easy to optimize.Our objective leverages model-dependent prefixes, automatically selected basedon two criteria: high prefilling attack success rates and low negativelog-likelihood. It can further simplify optimization by using multiple prefixesfor a single user request. AdvPrefix can integrate seamlessly into existingjailbreak attacks to improve their performance for free. For example, simplyreplacing GCG attack's target prefixes with ours on Llama-3 improves nuancedattack success rates from 14% to 80%, suggesting that current alignmentstruggles to generalize to unseen prefixes. Our work demonstrates theimportance of jailbreak objectives in achieving nuanced jailbreaks.</description><author>Sicheng Zhu, Brandon Amos, Yuandong Tian, Chuan Guo, Ivan Evtimov</author><pubDate>Fri, 13 Dec 2024 18:00:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10321v1</guid></item><item><title>MeshA*: Efficient Path Planing With Motion Primitives</title><link>http://arxiv.org/abs/2412.10320v1</link><description>We study a path planning problem where the possible move actions arerepresented as a finite set of motion primitives aligned with the gridrepresentation of the environment. That is, each primitive corresponds to ashort kinodynamically-feasible motion of an agent and is represented as asequence of the swept cells of a grid. Typically heuristic search, i.e. A*, isconducted over the lattice induced by these primitives (lattice-based planning)to find a path. However due to the large branching factor such search may beinefficient in practice. To this end we suggest a novel technique rooted in theidea of searching over the grid cells (as in vanilla A*) simultaneously fittingthe possible sequences of the motion primitives into these cells. The resultantalgorithm, MeshA*, provably preserves the guarantees on completeness andoptimality, on the one hand, and is shown to notably outperform conventionallattice-based planning (x1.5 decrease in the runtime), on the other hand.Moreover, we suggest an additional pruning technique that additionallydecreases the search space of MeshA*. The resultant planner is combined withthe regular A* to retain completeness and is shown to further increase thesearch performance at the cost of negligible decrease of the solution quality.</description><author>Marat Agranovskiy, Konstantin Yakovlev</author><pubDate>Fri, 13 Dec 2024 18:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10320v1</guid></item><item><title>SCBench: A KV Cache-Centric Analysis of Long-Context Methods</title><link>http://arxiv.org/abs/2412.10319v1</link><description>Long-context LLMs have enabled numerous downstream applications but alsointroduced significant challenges related to computational and memoryefficiency. To address these challenges, optimizations for long-contextinference have been developed, centered around the KV cache. However, existingbenchmarks often evaluate in single-request, neglecting the full lifecycle ofthe KV cache in real-world use. This oversight is particularly critical, as KVcache reuse has become widely adopted in LLMs inference frameworks, such asvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,Google, and Anthropic. To address this gap, we introduceSCBench(SharedContextBench), a comprehensive benchmark for evaluatinglong-context methods from a KV cachecentric perspective: 1) KV cachegeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cacheloading. Specifically, SCBench uses test examples with shared context, ranging12 tasks with two shared context modes, covering four categories oflong-context capabilities: string retrieval, semantic retrieval, globalinformation, and multi-task. With it, we provide an extensive KV cache-centricanalysis of eight categories long-context solutions, including Gated LinearRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,KV cache dropping, quantization, retrieval, loading, and prompt compression.The evaluation is conducted on 8 long-context LLMs. Our findings show thatsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encodingwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.Dynamic sparsity yields more expressive KV caches than static patterns, andlayer-level sparsity in hybrid architectures reduces memory usage with strongperformance. Additionally, we identify attention distribution shift issues inlong-generation scenarios. https://aka.ms/SCBench.</description><author>Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu</author><pubDate>Fri, 13 Dec 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10319v1</guid></item><item><title>BrushEdit: All-In-One Image Inpainting and Editing</title><link>http://arxiv.org/abs/2412.10316v1</link><description>Image editing has advanced significantly with the development of diffusionmodels using both inversion-based and instruction-based methods. However,current inversion-based approaches struggle with big modifications (e.g.,adding or removing objects) due to the structured nature of inversion noise,which hinders substantial changes. Meanwhile, instruction-based methods oftenconstrain users to black-box operations, limiting direct interaction forspecifying editing regions and intensity. To address these limitations, wepropose BrushEdit, a novel inpainting-based instruction-guided image editingparadigm, which leverages multimodal large language models (MLLMs) and imageinpainting models to enable autonomous, user-friendly, and interactivefree-form instruction editing. Specifically, we devise a system enablingfree-form instruction editing by integrating MLLMs and a dual-branch imageinpainting model in an agent-cooperative framework to perform editing categoryclassification, main object identification, mask acquisition, and editing areainpainting. Extensive experiments show that our framework effectively combinesMLLMs and inpainting models, achieving superior performance across sevenmetrics including mask region preservation and editing effect coherence.</description><author>Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Ying Shan, Qiang Xu</author><pubDate>Fri, 13 Dec 2024 17:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10316v1</guid></item><item><title>MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation</title><link>http://arxiv.org/abs/2412.10313v1</link><description>Regulatory documents are rich in nuanced terminology and specializedsemantics. FRAG systems: Frozen retrieval-augmented generators utilizingpre-trained (or, frozen) components face consequent challenges with bothretriever and answering performance. We present a system that adapts theretriever performance to the target domain using a multi-stage tuning (MST)strategy. Our retrieval approach, called MST-R (a) first fine-tunes encodersused in vector stores using hard negative mining, (b) then uses a hybridretriever, combining sparse and dense retrievers using reciprocal rank fusion,and then (c) adapts the cross-attention encoder by fine-tuning only the top-kretrieved results. We benchmark the system performance on the dataset releasedfor the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). Weachieve significant performance gains obtaining a top rank on the RegNLPchallenge leaderboard. We also show that a trivial answering approach games theRePASs metric outscoring all baselines and a pre-trained Llama model. Analyzingthis anomaly, we present important takeaways for future research.</description><author>Yash Malviya, Karan Dhingra, Maneesh Singh</author><pubDate>Fri, 13 Dec 2024 17:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10313v1</guid></item><item><title>DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving</title><link>http://arxiv.org/abs/2411.02820v2</link><description>Large Language Models (LLMs) are increasingly employed in complex workflows,where different LLMs and fine-tuned variants collaboratively address complextasks. However, these systems face significant inefficiencies due to redundantcontext processing of the shared context. We propose DroidSpeak, a frameworkthat optimizes context sharing between fine-tuned LLMs derived from the samefoundational model. DroidSpeak identifies critical layers in the KV cache andselectively recomputes them, enabling effective reuse of intermediate datawhile maintaining high accuracy. Our approach balances computational efficiency and task fidelity,significantly reducing inference latency and throughput bottlenecks.Experiments on diverse datasets and model pairs demonstrate that DroidSpeakachieves up to 3x higher throughputs and 2.6x faster prefill times withnegligible accuracy loss compared to full recomputation.</description><author>Yuhan Liu, Yuyang Huang, Jiayi Yao, Zhuohan Gu, Kuntai Du, Hanchen Li, Yihua Cheng, Junchen Jiang, Shan Lu, Madan Musuvathi, Esha Choukse</author><pubDate>Fri, 13 Dec 2024 17:53:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02820v2</guid></item><item><title>Interlocking-free Selective Rationalization Through Genetic-based Learning</title><link>http://arxiv.org/abs/2412.10312v1</link><description>A popular end-to-end architecture for selective rationalization is theselect-then-predict pipeline, comprising a generator to extract highlights fedto a predictor. Such a cooperative system suffers from suboptimal equilibriumminima due to the dominance of one of the two modules, a phenomenon known asinterlocking. While several contributions aimed at addressing interlocking,they only mitigate its effect, often by introducing feature-based heuristics,sampling, and ad-hoc regularizations. We present GenSPP, the firstinterlocking-free architecture for selective rationalization that does notrequire any learning overhead, as the above-mentioned. GenSPP avoidsinterlocking by performing disjoint training of the generator and predictor viagenetic global search. Experiments on a synthetic and a real-world benchmarkshow that our model outperforms several state-of-the-art competitors.</description><author>Federico Ruggeri, Gaetano Signorelli</author><pubDate>Fri, 13 Dec 2024 17:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10312v1</guid></item><item><title>TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes</title><link>http://arxiv.org/abs/2412.10308v1</link><description>We tackle the problem of localizing the traffic surveillance cameras incooperative perception. To overcome the lack of large-scale real-worldintersection datasets, we introduce Carla Intersection, a new simulated datasetwith 75 urban and rural intersections in Carla. Moreover, we introduce a novelneural network, TrafficLoc, localizing traffic cameras within a 3D referencemap. TrafficLoc employs a coarse-to-fine matching pipeline. For image-pointcloud feature fusion, we propose a novel Geometry-guided Attention Loss toaddress cross-modal viewpoint inconsistencies. During coarse matching, wepropose an Inter-Intra Contrastive Learning to achieve precise alignment whilepreserving distinctiveness among local intra-features within image patch-pointgroup pairs. Besides, we introduce Dense Training Alignment with a soft-argmaxoperator to consider additional features when regressing the final position.Extensive experiments show that our TrafficLoc improves the localizationaccuracy over the state-of-the-art Image-to-point cloud registration methods bya large margin (up to 86%) on Carla Intersection and generalizes well toreal-world data. TrafficLoc also achieves new SOTA performance on KITTI andNuScenes datasets, demonstrating strong localization ability across bothin-vehicle and traffic cameras. Our project page is publicly available athttps://tum-luk.github.io/projects/trafficloc/.</description><author>Yan Xia, Yunxiang Lu, Rui Song, Oussema Dhaouadi, João F. Henriques, Daniel Cremers</author><pubDate>Fri, 13 Dec 2024 17:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10308v1</guid></item><item><title>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</title><link>http://arxiv.org/abs/2412.10302v1</link><description>We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)Vision-Language Models that significantly improves upon its predecessor,DeepSeek-VL, through two key major upgrades. For the vision component, weincorporate a dynamic tiling vision encoding strategy designed for processinghigh-resolution images with different aspect ratios. For the languagecomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attentionmechanism, which compresses Key-Value cache into latent vectors, to enableefficient inference and high throughput. Trained on an improved vision-languagedataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,including but not limited to visual question answering, optical characterrecognition, document/table/chart understanding, and visual grounding. Ourmodel series is composed of three variants: DeepSeek-VL2-Tiny,DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activatedparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-artperformance with similar or fewer activated parameters compared to existingopen-source dense and MoE-based models. Codes and pre-trained models arepublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.</description><author>Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, Chong Ruan</author><pubDate>Fri, 13 Dec 2024 17:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10302v1</guid></item><item><title>Iterating the Transient Light Transport Matrix for Non-Line-of-Sight Imaging</title><link>http://arxiv.org/abs/2412.10300v1</link><description>Active imaging systems sample the Transient Light Transport Matrix (TLTM) fora scene by sequentially illuminating various positions in this scene using acontrollable light source, and then measuring the resulting spatiotemporallight transport with time of flight (ToF) sensors. Time-resolvedNon-line-of-sight (NLOS) imaging employs an active imaging system that measurespart of the TLTM of an intermediary relay surface, and uses the indirectreflections of light encoded within this TLTM to "see around corners". Suchimaging systems have applications in diverse areas such as disaster response,remote surveillance, and autonomous navigation. While existing NLOS imagingsystems usually measure a subset of the full TLTM, development of customizedgated Single Photon Avalanche Diode (SPAD) arrays\cite{riccardo_fast-gated_2022} has made it feasible to probe the fullmeasurement space. In this work, we demonstrate that the full TLTM on the relaysurface can be processed with efficient algorithms to computationally focus anddetect our illumination in different parts of the hidden scene, turning therelay surface into a second-order active imaging system. These algorithms allowus to iterate on the measured, first-order TLTM, and extract a \textbf{secondorder TLTM for surfaces in the hidden scene}. We showcase three applications ofTLTMs in NLOS imaging: (1) Scene Relighting with novel illumination, (2)Separation of direct and indirect components of light transport in the hiddenscene, and (3) Dual Photography. Additionally, we empirically demonstrate thatSPAD arrays enable parallel acquisition of photons, effectively mitigating longacquisition times.</description><author>Talha Sultan, Eric Brandt, Khadijeh Masumnia-Bisheh, Simone Riccardo, Pavel Polynkin, Alberto Tosi, Andreas Velten</author><pubDate>Fri, 13 Dec 2024 17:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10300v1</guid></item><item><title>Buzz to Broadcast: Predicting Sports Viewership Using Social Media Engagement</title><link>http://arxiv.org/abs/2412.10298v1</link><description>Accurately predicting sports viewership is crucial for optimizing ad salesand revenue forecasting. Social media platforms, such as Reddit, provide awealth of user-generated content that reflects audience engagement andinterest. In this study, we propose a regression-based approach to predictsports viewership using social media metrics, including post counts, comments,scores, and sentiment analysis from TextBlob and VADER. Through iterativeimprovements, such as focusing on major sports subreddits, incorporatingcategorical features, and handling outliers by sport, the model achieved an$R^2$ of 0.99, a Mean Absolute Error (MAE) of 1.27 million viewers, and a RootMean Squared Error (RMSE) of 2.33 million viewers on the full dataset. Theseresults demonstrate the model's ability to accurately capture patterns inaudience behavior, offering significant potential for pre-event revenueforecasting and targeted advertising strategies.</description><author>Anakin Trotter</author><pubDate>Fri, 13 Dec 2024 17:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10298v1</guid></item><item><title>Enhancing Temporal Understanding in Audio Question Answering for Large Audio Language Models</title><link>http://arxiv.org/abs/2409.06223v3</link><description>The Audio Question Answering (AQA) task includes audio event classification,audio captioning, and open-ended reasoning. Recently, AQA has garneredattention due to the advent of Large Audio Language Models (LALMs). Currentliterature focuses on constructing LALMs by integrating audio encoders withtext-only Large Language Models (LLMs) through a projection module. While LALMsexcel in general audio understanding, they are limited in temporal reasoning,which may hinder their commercial applications and on-device deployment. Thispaper addresses these challenges and limitations in audio temporal reasoning.First, we introduce a data augmentation technique for generating reliable audiotemporal questions and answers using an LLM. Second, we perform a furtherfine-tuning of an existing baseline using curriculum learning strategy tospecialize in temporal reasoning without compromising performance on fine-tunedtasks. We demonstrate the performance of our model using state-of-the-art LALMson public audio benchmark datasets. Third, we implement our AQA model on-devicelocally and investigate its CPU inference for edge applications.</description><author>Arvind Krishna Sridhar, Yinyi Guo, Erik Visser</author><pubDate>Fri, 13 Dec 2024 17:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06223v3</guid></item><item><title>Coherent 3D Scene Diffusion From a Single RGB Image</title><link>http://arxiv.org/abs/2412.10294v1</link><description>We present a novel diffusion-based approach for coherent 3D scenereconstruction from a single RGB image. Our method utilizes animage-conditioned 3D scene diffusion model to simultaneously denoise the 3Dposes and geometries of all objects within the scene. Motivated by theill-posed nature of the task and to obtain consistent scene reconstructionresults, we learn a generative scene prior by conditioning on all scene objectssimultaneously to capture the scene context and by allowing the model to learninter-object relationships throughout the diffusion process. We further proposean efficient surface alignment loss to facilitate training even in the absenceof full ground-truth annotation, which is common in publicly availabledatasets. This loss leverages an expressive shape representation, which enablesdirect point sampling from intermediate shape predictions. By framing the taskof single RGB image 3D scene reconstruction as a conditional diffusion process,our approach surpasses current state-of-the-art methods, achieving a 12.04%improvement in AP3D on SUN RGB-D and a 13.43% increase in F-Score on Pix3D.</description><author>Manuel Dahnert, Angela Dai, Norman Müller, Matthias Nießner</author><pubDate>Fri, 13 Dec 2024 17:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10294v1</guid></item><item><title>Prompt-Guided Mask Proposal for Two-Stage Open-Vocabulary Segmentation</title><link>http://arxiv.org/abs/2412.10292v1</link><description>We tackle the challenge of open-vocabulary segmentation, where we need toidentify objects from a wide range of categories in different environments,using text prompts as our input. To overcome this challenge, existing methodsoften use multi-modal models like CLIP, which combine image and text featuresin a shared embedding space to bridge the gap between limited and extensivevocabulary recognition, resulting in a two-stage approach: In the first stage,a mask generator takes an input image to generate mask proposals, and the inthe second stage the target mask is picked based on the query. However, theexpected target mask may not exist in the generated mask proposals, which leadsto an unexpected output mask. In our work, we propose a novel approach namedPrompt-guided Mask Proposal (PMP) where the mask generator takes the input textprompts and generates masks guided by these prompts. Compared with maskproposals generated without input prompts, masks generated by PMP are betteraligned with the input prompts. To realize PMP, we designed a cross-attentionmechanism between text tokens and query tokens which is capable of generatingprompt-guided mask proposals after each decoding. We combined our PMP withseveral existing works employing a query-based segmentation backbone and theexperiments on five benchmark datasets demonstrate the effectiveness of thisapproach, showcasing significant improvements over the current two-stage models(1% ~ 3% absolute performance gain in terms of mIOU). The steady improvement inperformance across these benchmarks indicates the effective generalization ofour proposed lightweight prompt-aware method.</description><author>Yu-Jhe Li, Xinyang Zhang, Kun Wan, Lantao Yu, Ajinkya Kale, Xin Lu</author><pubDate>Fri, 13 Dec 2024 17:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10292v1</guid></item><item><title>Still "Talking About Large Language Models": Some Clarifications</title><link>http://arxiv.org/abs/2412.10291v1</link><description>My paper "Talking About Large Language Models" has more than once beeninterpreted as advocating a reductionist stance towards large language models.But the paper was not intended that way, and I do not endorse such positions.This short note situates the paper in the context of a larger philosophicalproject that is concerned with the (mis)use of words rather than metaphysics,in the spirit of Wittgenstein's later writing.</description><author>Murray Shanahan</author><pubDate>Fri, 13 Dec 2024 17:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10291v1</guid></item><item><title>Performance evaluation of predictive AI models to support medical decisions: Overview and guidance</title><link>http://arxiv.org/abs/2412.10288v1</link><description>A myriad of measures to illustrate performance of predictive artificialintelligence (AI) models have been proposed in the literature. Selectingappropriate performance measures is essential for predictive AI models that aredeveloped to be used in medical practice, because poorly performing models mayharm patients and lead to increased costs. We aim to assess the merits ofclassic and contemporary performance measures when validating predictive AImodels for use in medical practice. We focus on models with a binary outcome.We discuss 32 performance measures covering five performance domains(discrimination, calibration, overall, classification, and clinical utility)along with accompanying graphical assessments. The first four domains coverstatistical performance, the fifth domain covers decision-analytic performance.We explain why two key characteristics are important when selecting whichperformance measures to assess: (1) whether the measure's expected value isoptimized when it is calculated using the correct probabilities (i.e., a"proper" measure), and (2) whether they reflect either purely statisticalperformance or decision-analytic performance by properly consideringmisclassification costs. Seventeen measures exhibit both characteristics,fourteen measures exhibited one characteristic, and one measure possessedneither characteristic (the F1 measure). All classification measures (such asclassification accuracy and F1) are improper for clinically relevant decisionthresholds other than 0.5 or the prevalence. We recommend the followingmeasures and plots as essential to report: AUROC, calibration plot, a clinicalutility measure such as net benefit with decision curve analysis, and a plotwith probability distributions per outcome category.</description><author>Ben Van Calster, Gary S. Collins, Andrew J. Vickers, Laure Wynants, Kathleen F. Kerr, Lasai Barreñada, Gael Varoquaux, Karandeep Singh, Karel G. M. Moons, Tina Hernandez-boussard, Dirk Timmerman, David J. Mclernon, Maarten Van Smeden, Ewout W. Steyerberg</author><pubDate>Fri, 13 Dec 2024 17:11:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10288v1</guid></item><item><title>Utilizing Multi-step Loss for Single Image Reflection Removal</title><link>http://arxiv.org/abs/2412.08582v2</link><description>Image reflection removal is crucial for restoring image quality. Distortedimages can negatively impact tasks like object detection and imagesegmentation. In this paper, we present a novel approach for image reflectionremoval using a single image. Instead of focusing on model architecture, weintroduce a new training technique that can be generalized to image-to-imageproblems, with input and output being similar in nature. This technique isembodied in our multi-step loss mechanism, which has proven effective in thereflection removal task. Additionally, we address the scarcity of reflectionremoval training data by synthesizing a high-quality, non-linear syntheticdataset called RefGAN using Pix2Pix GAN. This dataset significantly enhancesthe model's ability to learn better patterns for reflection removal. We alsoutilize a ranged depth map, extracted from the depth estimation of the ambientimage, as an auxiliary feature, leveraging its property of lacking depthestimations for reflections. Our approach demonstrates superior performance onthe SIR^2 benchmark and other real-world datasets, proving its effectiveness byoutperforming other state-of-the-art models.</description><author>Abdelrahman Elnenaey, Marwan Torki</author><pubDate>Fri, 13 Dec 2024 17:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08582v2</guid></item><item><title>One world, one opinion? The superstar effect in LLM responses</title><link>http://arxiv.org/abs/2412.10281v1</link><description>As large language models (LLMs) are shaping the way information is shared andaccessed online, their opinions have the potential to influence a wideaudience. This study examines who the LLMs view as the most prominent figuresacross various fields, using prompts in ten different languages to explore theinfluence of linguistic diversity. Our findings reveal low diversity inresponses, with a small number of figures dominating recognition acrosslanguages (also known as the "superstar effect"). These results highlight therisk of narrowing global knowledge representation when LLMs retrieve subjectiveinformation.</description><author>Sofie Goethals, Lauren Rhue</author><pubDate>Fri, 13 Dec 2024 17:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10281v1</guid></item><item><title>Envisioning National Resources for Artificial Intelligence Research: NSF Workshop Report</title><link>http://arxiv.org/abs/2412.10278v1</link><description>This is a report of an NSF workshop titled "Envisioning National Resourcesfor Artificial Intelligence Research" held in Alexandria, Virginia, in May2024. The workshop aimed to identify initial challenges and opportunities fornational resources for AI research (e.g., compute, data, models, etc.) and tofacilitate planning for the envisioned National AI Research Resource.Participants included AI and cyberinfrastructure (CI) experts. The reportoutlines significant findings and identifies needs and recommendations from theworkshop.</description><author>Shantenu Jha, Yolanda Gil</author><pubDate>Fri, 13 Dec 2024 17:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10278v1</guid></item><item><title>Targeted Hard Sample Synthesis Based on Estimated Pose and Occlusion Error for Improved Object Pose Estimation</title><link>http://arxiv.org/abs/2412.04279v2</link><description>6D Object pose estimation is a fundamental component in robotics enablingefficient interaction with the environment. It is particularly challenging inbin-picking applications, where objects may be textureless and in difficultposes, and occlusion between objects of the same type may cause confusion evenin well-trained models. We propose a novel method of hard example synthesisthat is model-agnostic, using existing simulators and the modeling of poseerror in both the camera-to-object viewsphere and occlusion space. Throughevaluation of the model performance with respect to the distribution of objectposes and occlusions, we discover regions of high error and generate realistictraining samples to specifically target these regions. With our trainingapproach, we demonstrate an improvement in correct detection rate of up to 20%across several ROBI-dataset objects using state-of-the-art pose estimationmodels.</description><author>Alan Li, Angela P. Schoellig</author><pubDate>Fri, 13 Dec 2024 16:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04279v2</guid></item><item><title>NLP Cluster Analysis of Common Core State Standards and NAEP Item Specifications</title><link>http://arxiv.org/abs/2412.04482v2</link><description>Camilli (2024) proposed a methodology using natural language processing (NLP)to map the relationship of a set of content standards to item specifications.This study provided evidence that NLP can be used to improve the mappingprocess. As part of this investigation, the nominal classifications ofstandards and items specifications were used to examine construct equivalence.In the current paper, we determine the strength of empirical support for thesemantic distinctiveness of these classifications, which are known as "domains"for Common Core standards, and "strands" for National Assessment of EducationalProgress (NAEP) item specifications. This is accomplished by separate k-meansclustering for standards and specifications of their corresponding embeddingvectors. We then briefly illustrate an application of these findings.</description><author>Gregory Camilli, Larry Suter</author><pubDate>Fri, 13 Dec 2024 16:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04482v2</guid></item><item><title>MOREL: Enhancing Adversarial Robustness through Multi-Objective Representation Learning</title><link>http://arxiv.org/abs/2410.01697v3</link><description>Extensive research has shown that deep neural networks (DNNs) are vulnerableto slight adversarial perturbations$-$small changes to the input data thatappear insignificant but cause the model to produce drastically differentoutputs. In addition to augmenting training data with adversarial examplesgenerated from a specific attack method, most of the current defense strategiesnecessitate modifying the original model architecture components to improverobustness or performing test-time data purification to handle adversarialattacks. In this work, we demonstrate that strong feature representationlearning during training can significantly enhance the original model'srobustness. We propose MOREL, a multi-objective feature representation learningapproach, encouraging classification models to produce similar features forinputs within the same class, despite perturbations. Our training methodinvolves an embedding space where cosine similarity loss and multi-positivecontrastive loss are used to align natural and adversarial features from themodel encoder and ensure tight clustering. Concurrently, the classifier ismotivated to achieve accurate predictions. Through extensive experiments, wedemonstrate that our approach significantly enhances the robustness of DNNsagainst white-box and black-box adversarial attacks, outperforming othermethods that similarly require no architectural changes or test-time datapurification. Our code is available at https://github.com/salomonhotegni/MOREL</description><author>Sedjro Salomon Hotegni, Sebastian Peitz</author><pubDate>Fri, 13 Dec 2024 16:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01697v3</guid></item><item><title>TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation</title><link>http://arxiv.org/abs/2412.10275v1</link><description>Text-driven Image to Video Generation (TI2V) aims to generate controllablevideo given the first frame and corresponding textual description. The primarychallenges of this task lie in two parts: (i) how to identify the targetobjects and ensure the consistency between the movement trajectory and thetextual description. (ii) how to improve the subjective quality of generatedvideos. To tackle the above challenges, we propose a new diffusion-based TI2Vframework, termed TIV-Diffusion, via object-centric textual-visual alignment,intending to achieve precise control and high-quality video generation based ontextual-described motion for different objects. Concretely, we enable ourTIV-Diffuion model to perceive the textual-described objects and their motiontrajectory by incorporating the fused textual and visual knowledge throughscale-offset modulation. Moreover, to mitigate the problems of objectdisappearance and misaligned objects and motion, we introduce an object-centrictextual-visual alignment module, which reduces the risk of misalignedobjects/motion by decoupling the objects in the reference image and aligningtextual features with each object individually. Based on the above innovations,our TIV-Diffusion achieves state-of-the-art high-quality video generationcompared with existing TI2V methods.</description><author>Xingrui Wang, Xin Li, Yaosi Hu, Hanxin Zhu, Chen Hou, Cuiling Lan, Zhibo Chen</author><pubDate>Fri, 13 Dec 2024 16:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10275v1</guid></item><item><title>Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry</title><link>http://arxiv.org/abs/2412.10273v1</link><description>We introduce a hierarchical probabilistic approach to go from a 2D image tomultiview 3D: a diffusion "prior" models the unseen 3D geometry, which thenconditions a diffusion "decoder" to generate novel views of the subject. We usea pointmap-based geometric representation in a multiview image format tocoordinate the generation of multiple target views simultaneously. Wefacilitate correspondence between views by assuming fixed target camera posesrelative to the source camera, and constructing a predictable distribution ofgeometric features per target. Our modular, geometry-driven approach tonovel-view synthesis (called "unPIC") beats SoTA baselines such as CAT3D andOne-2-3-45 on held-out objects from ObjaverseXL, as well as real-world objectsranging from Google Scanned Objects, Amazon Berkeley Objects, to the DigitalTwin Catalog.</description><author>Rishabh Kabra, Drew A. Hudson, Sjoerd van Steenkiste, Joao Carreira, Niloy J. Mitra</author><pubDate>Fri, 13 Dec 2024 16:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10273v1</guid></item><item><title>Trustworthy and Explainable Decision-Making for Workforce allocation</title><link>http://arxiv.org/abs/2412.10272v1</link><description>In industrial contexts, effective workforce allocation is crucial foroperational efficiency. This paper presents an ongoing project focused ondeveloping a decision-making tool designed for workforce allocation,emphasising the explainability to enhance its trustworthiness. Our objective isto create a system that not only optimises the allocation of teams to scheduledtasks but also provides clear, understandable explanations for its decisions,particularly in cases where the problem is infeasible. By incorporatinghuman-in-the-loop mechanisms, the tool aims to enhance user trust andfacilitate interactive conflict resolution. We implemented our approach on aprototype tool/digital demonstrator intended to be evaluated on a realindustrial scenario both in terms of performance and user acceptability.</description><author>Guillaume Povéda, Ryma Boumazouza, Andreas Strahl, Mark Hall, Santiago Quintana-Amate, Nahum Alvarez, Ignace Bleukx, Dimos Tsouros, Hélène Verhaeghe, Tias Guns</author><pubDate>Fri, 13 Dec 2024 16:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10272v1</guid></item><item><title>Benchmarking Linguistic Diversity of Large Language Models</title><link>http://arxiv.org/abs/2412.10271v1</link><description>The development and evaluation of Large Language Models (LLMs) has primarilyfocused on their task-solving capabilities, with recent models even surpassinghuman performance in some areas. However, this focus often neglects whethermachine-generated language matches the human level of diversity, in terms ofvocabulary choice, syntactic construction, and expression of meaning, raisingquestions about whether the fundamentals of language generation have been fullyaddressed. This paper emphasizes the importance of examining the preservationof human linguistic richness by language models, given the concerning surge inonline content produced or aided by LLMs. We propose a comprehensive frameworkfor evaluating LLMs from various linguistic diversity perspectives includinglexical, syntactic, and semantic dimensions. Using this framework, we benchmarkseveral state-of-the-art LLMs across all diversity dimensions, and conduct anin-depth case study for syntactic diversity. Finally, we analyze how differentdevelopment and deployment choices impact the linguistic diversity of LLMoutputs.</description><author>Yanzhu Guo, Guokan Shang, Chloé Clavel</author><pubDate>Fri, 13 Dec 2024 16:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10271v1</guid></item><item><title>Cultural Evolution of Cooperation among LLM Agents</title><link>http://arxiv.org/abs/2412.10270v1</link><description>Large language models (LLMs) provide a compelling foundation for buildinggenerally-capable AI agents. These agents may soon be deployed at scale in thereal world, representing the interests of individual humans (e.g., AIassistants) or groups of humans (e.g., AI-accelerated corporations). Atpresent, relatively little is known about the dynamics of multiple LLM agentsinteracting over many generations of iterative deployment. In this paper, weexamine whether a "society" of LLM agents can learn mutually beneficial socialnorms in the face of incentives to defect, a distinctive feature of humansociality that is arguably crucial to the success of civilization. Inparticular, we study the evolution of indirect reciprocity across generationsof LLM agents playing a classic iterated Donor Game in which agents can observethe recent behavior of their peers. We find that the evolution of cooperationdiffers markedly across base models, with societies of Claude 3.5 Sonnet agentsachieving significantly higher average scores than Gemini 1.5 Flash, which, inturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of anadditional mechanism for costly punishment to achieve yet higher scores, whileGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we alsoobserve variation in emergent behavior across random seeds, suggesting anunderstudied sensitive dependence on initial conditions. We suggest that ourevaluation regime could inspire an inexpensive and informative new class of LLMbenchmarks, focussed on the implications of LLM agent deployment for thecooperative infrastructure of society.</description><author>Aron Vallinder, Edward Hughes</author><pubDate>Fri, 13 Dec 2024 16:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10270v1</guid></item><item><title>MVC-VPR: Mutual Learning of Viewpoint Classification and Visual Place Recognition</title><link>http://arxiv.org/abs/2412.09199v2</link><description>Visual Place Recognition (VPR) aims to robustly identify locations byleveraging image retrieval based on descriptors encoded from environmentalimages. However, drastic appearance changes of images captured from differentviewpoints at the same location pose incoherent supervision signals fordescriptor learning, which severely hinder the performance of VPR. Previouswork proposes classifying images based on manually defined rules or groundtruth labels for viewpoints, followed by descriptor training based on theclassification results. However, not all datasets have ground truth labels ofviewpoints and manually defined rules may be suboptimal, leading to degradeddescriptor performance.To address these challenges, we introduce the mutuallearning of viewpoint self-classification and VPR. Starting from coarseclassification based on geographical coordinates, we progress to finerclassification of viewpoints using simple clustering techniques. The dataset ispartitioned in an unsupervised manner while simultaneously training adescriptor extractor for place recognition. Experimental results show that thisapproach almost perfectly partitions the dataset based on viewpoints, thusachieving mutually reinforcing effects. Our method even excels state-of-the-art(SOTA) methods that partition datasets using ground truth labels.</description><author>Qiwen Gu, Xufei Wang, Fenglin Zhang, Junqiao Zhao, Siyue Tao, Chen Ye, Tiantian Feng, Changjun Jiang</author><pubDate>Fri, 13 Dec 2024 16:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09199v2</guid></item><item><title>Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT</title><link>http://arxiv.org/abs/2412.10267v1</link><description>The role of multiple-choice questions (MCQs) as effective learning tools hasbeen debated in past research. While MCQs are widely used due to their ease ingrading, open response questions are increasingly used for instruction, givenadvances in large language models (LLMs) for automated grading. This studyevaluates MCQs effectiveness relative to open-response questions, bothindividually and in combination, on learning. These activities are embeddedwithin six tutor lessons on advocacy. Using a posttest-only randomized controldesign, we compare the performance of 234 tutors (790 lesson completions)across three conditions: MCQ only, open response only, and a combination ofboth. We find no significant learning differences across conditions atposttest, but tutors in the MCQ condition took significantly less time tocomplete instruction. These findings suggest that MCQs are as effective, andmore efficient, than open response tasks for learning when practice time islimited. To further enhance efficiency, we autograded open responses usingGPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes oflow-stakes assessment, though further research is needed for broader use. Thisstudy contributes a dataset of lesson log data, human annotation rubrics, andLLM prompts to promote transparency and reproducibility.</description><author>Danielle R. Thomas, Conrad Borchers, Sanjit Kakarla, Jionghao Lin, Shambhavi Bhushan, Boyuan Guo, Erin Gatz, Kenneth R. Koedinger</author><pubDate>Fri, 13 Dec 2024 16:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10267v1</guid></item><item><title>On the Power of Adaptive Weighted Aggregation in Heterogeneous Federated Learning and Beyond</title><link>http://arxiv.org/abs/2310.02702v4</link><description>Federated averaging (FedAvg) is the most fundamental algorithm in Federatedlearning (FL). Previous theoretical results assert that FedAvg convergence andgeneralization degenerate under heterogeneous clients. However, recentempirical results show that FedAvg can perform well in many real-worldheterogeneous tasks. These results reveal an inconsistency between FL theoryand practice that is not fully explained. In this paper, we show that commonheterogeneity measures contribute to this inconsistency based on rigorousconvergence analysis. Furthermore, we introduce a new measure \textit{clientconsensus dynamics} and prove that \textit{FedAvg can effectively handle clientheterogeneity when an appropriate aggregation strategy is used}. Building onthis theoretical insight, we present a simple and effective FedAvg varianttermed FedAWARE. Extensive experiments on three datasets and two modern neuralnetwork architectures demonstrate that FedAWARE ensures faster convergence andbetter generalization in heterogeneous client settings. Moreover, our resultsshow that FedAWARE can significantly enhance the generalization performance ofadvanced FL algorithms when used as a plug-in module.</description><author>Dun Zeng, Zenglin Xu, Shiyu Liu, Yu Pan, Qifan Wang, Xiaoying Tang</author><pubDate>Fri, 13 Dec 2024 16:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02702v4</guid></item><item><title>Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media</title><link>http://arxiv.org/abs/2412.10266v1</link><description>Stance detection is crucial for fostering a human-centric Web by analyzinguser-generated content to identify biases and harmful narratives that underminetrust. With the development of Large Language Models (LLMs), existingapproaches treat stance detection as a classification problem, providing robustmethodologies for modeling complex group interactions and advancingcapabilities in natural language tasks. However, these methods often lackinterpretability, limiting their ability to offer transparent andunderstandable justifications for predictions. This study adopts a generativeapproach, where stance predictions include explicit, interpretable rationales,and integrates them into smaller language models through single-task andmultitask learning. We find that incorporating reasoning into stance detectionenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shotperformance, achieving an improvement of up to 9.57%. Moreover, our resultsshow that reasoning capabilities enhance multitask learning performance but mayreduce effectiveness in single-task settings. Crucially, we demonstrate thatfaithful rationales improve rationale distillation into SLMs, advancing effortsto build interpretable, trustworthy systems for addressing discrimination,fostering trust, and promoting equitable engagement on social media.</description><author>Jiaqing Yuan, Ruijie Xi, Munindar P. Singh</author><pubDate>Fri, 13 Dec 2024 16:34:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10266v1</guid></item><item><title>Adversarial Robustness of Bottleneck Injected Deep Neural Networks for Task-Oriented Communication</title><link>http://arxiv.org/abs/2412.10265v1</link><description>This paper investigates the adversarial robustness of Deep Neural Networks(DNNs) using Information Bottleneck (IB) objectives for task-orientedcommunication systems. We empirically demonstrate that while IB-basedapproaches provide baseline resilience against attacks targeting downstreamtasks, the reliance on generative models for task-oriented communicationintroduces new vulnerabilities. Through extensive experiments on severaldatasets, we analyze how bottleneck depth and task complexity influenceadversarial robustness. Our key findings show that Shallow VariationalBottleneck Injection (SVBI) provides less adversarial robustness compared toDeep Variational Information Bottleneck (DVIB) approaches, with the gapwidening for more complex tasks. Additionally, we reveal that IB-basedobjectives exhibit stronger robustness against attacks focusing on salientpixels with high intensity compared to those perturbing many pixels with lowerintensity. Lastly, we demonstrate that task-oriented communication systems thatrely on generative models to extract and recover salient information have anincreased attack surface. The results highlight important securityconsiderations for next-generation communication systems that leverage neuralnetworks for goal-oriented compression.</description><author>Alireza Furutanpey, Pantelis A. Frangoudis, Patrik Szabo, Schahram Dustdar</author><pubDate>Fri, 13 Dec 2024 16:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10265v1</guid></item><item><title>MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization</title><link>http://arxiv.org/abs/2412.10261v1</link><description>Vector quantization(VQ) is a hardware-friendly DNN compression method thatcan reduce the storage cost and weight-loading datawidth of hardwareaccelerators. However, conventional VQ techniques lead to significant accuracyloss because the important weights are not well preserved. To tackle thisproblem, a novel approach called MVQ is proposed, which aims at betterapproximating important weights with a limited number of codewords. At thealgorithm level, our approach removes the less important weights through N:Mpruning and then minimizes the vector clustering error between the remainingweights and codewords by the masked k-means algorithm. Only distances betweenthe unpruned weights and the codewords are computed, which are then used toupdate the codewords. At the architecture level, our accelerator implementsvector quantization on an EWS (Enhanced weight stationary) CNN accelerator andproposes a sparse systolic array design to maximize the benefits brought bymasked vector quantization.\\ Our algorithm is validated on various models forimage classification, object detection, and segmentation tasks. Experimentalresults demonstrate that MVQ not only outperforms conventional vectorquantization methods at comparable compression ratios but also reduces FLOPs.Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by2.3$\times$ and reduces the size of the systolic array by 55\% when comparedwith the base EWS accelerator. Compared to the previous sparse accelerators,MVQ achieves 1.73$\times$ higher energy efficiency.</description><author>Shuaiting Li, Chengxuan Wang, Juncan Deng, Zeyu Wang, Zewen Ye, Zongsheng Wang, Haibin Shen, Kejie Huang</author><pubDate>Fri, 13 Dec 2024 16:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10261v1</guid></item><item><title>Copy-Move Detection in Optical Microscopy: A Segmentation Network and A Dataset</title><link>http://arxiv.org/abs/2412.10258v1</link><description>With increasing revelations of academic fraud, detecting forged experimentalimages in the biomedical field has become a public concern. The challenge liesin the fact that copy-move targets can include background tissue, smallforeground objects, or both, which may be out of the training domain andsubject to unseen attacks, rendering standard object-detection-based approachesless effective. To address this, we reformulate the problem of detectingbiomedical copy-move forgery regions as an intra-image co-saliency detectiontask and propose CMSeg-Net, a copy-move forgery segmentation network capable ofidentifying unseen duplicated areas. Built on a multi-resolutionencoder-decoder architecture, CMSeg-Net incorporates self-correlation andcorrelation-assisted spatial-attention modules to detect intra-image regionalsimilarities within feature tensors at each observation scale. This designhelps distinguish even small copy-move targets in complex microscopic imagesfrom other similar objects. Furthermore, we created a copy-move forgery datasetof optical microscopic images, named FakeParaEgg, using open data from the ICIP2022 Challenge to support CMSeg-Net's development and verify its performance.Extensive experiments demonstrate that our approach outperforms previousstate-of-the-art methods on the FakeParaEgg dataset and other open copy-movedetection datasets, including CASIA-CMFD, CoMoFoD, and CMF. The FakeParaEggdataset, our source code, and the CMF dataset with our manually definedsegmentation ground truths available at``https://github.com/YoursEver/FakeParaEgg''.</description><author>Hao-Chiang Shao, Yuan-Rong Liao, Tse-Yu Tseng, Yen-Liang Chuo, Fong-Yi Lin</author><pubDate>Fri, 13 Dec 2024 16:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10258v1</guid></item><item><title>Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models</title><link>http://arxiv.org/abs/2412.10257v1</link><description>The sheer scale of data required to train modern large language models (LLMs)poses significant risks, as models are likely to gain knowledge of sensitivetopics such as bio-security, as well the ability to replicate copyrightedworks. Methods designed to remove such knowledge must do so from all promptdirections, in a multi-lingual capacity and without degrading general modelperformance. To this end, we introduce the targeted angular reversal (TARS)method of knowledge removal from LLMs. The TARS method firstly leverages theLLM in combination with a detailed prompt to aggregate information about aselected concept in the internal representation space of the LLM. It thenrefines this approximate concept vector to trigger the concept token with highprobability, by perturbing the approximate concept vector with noise andtransforming it into token scores with the language model head. The feedforwardweight vectors in the LLM which operate directly on the internal representationspace, and have the highest cosine similarity with this targeting vector, arethen replaced by a reversed targeting vector, thus limiting the ability of theconcept to propagate through the model. The modularity of the TARS methodallows for a sequential removal of concepts from Llama 3.1 8B, such as thefamous literary detective Sherlock Holmes, and the planet Saturn. It isdemonstrated that the probability of triggering target concepts can be reducedto 0.00 with as few as 1 TARS edit, whilst simultaneously removing theknowledge bi-directionally. Moreover, knowledge is shown to be removed acrossall languages despite only being targeted in English. Importantly, TARS hasminimal impact on the general model capabilities, as after removing 5 diverseconcepts in a modular fashion, there is minimal KL divergence in the next tokenprobabilities of the LLM on large corpora of Wikipedia text (median of 0.002).</description><author>Harry J. Davies, Giorgos Iacovides, Danilo P. Mandic</author><pubDate>Fri, 13 Dec 2024 16:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10257v1</guid></item><item><title>Exploring the Frontiers of Animation Video Generation in the Sora Era: Method, Dataset and Benchmark</title><link>http://arxiv.org/abs/2412.10255v1</link><description>Animation has gained significant interest in the recent film and TV industry.Despite the success of advanced video generation models like Sora, Kling, andCogVideoX in generating natural videos, they lack the same effectiveness inhandling animation videos. Evaluating animation video generation is also agreat challenge due to its unique artist styles, violating the laws of physicsand exaggerated motions. In this paper, we present a comprehensive system,AniSora, designed for animation video generation, which includes a dataprocessing pipeline, a controllable generation model, and an evaluationdataset. Supported by the data processing pipeline with over 10M high-qualitydata, the generation model incorporates a spatiotemporal mask module tofacilitate key animation production functions such as image-to-videogeneration, frame interpolation, and localized image-guided animation. We alsocollect an evaluation benchmark of 948 various animation videos, the evaluationon VBench and human double-blind test demonstrates consistency in character andmotion, achieving state-of-the-art results in animation video generation. %Wealso collect an evaluation benchmark of 948 various animation videos, withspecifically developed metrics for animation video generation. Our model accessAPI and evaluation benchmark will be publicly available.</description><author>Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Jixuan Xu, Yue Zhang, Jinlong Hou, Huyang Sun</author><pubDate>Fri, 13 Dec 2024 16:24:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10255v1</guid></item><item><title>Controlling dynamical systems into unseen target states using machine learning</title><link>http://arxiv.org/abs/2412.10251v1</link><description>We present a novel, model-free, and data-driven methodology for controllingcomplex dynamical systems into previously unseen target states, including thosewith significantly different and complex dynamics. Leveraging a parameter-awarerealization of next-generation reservoir computing, our approach accuratelypredicts system behavior in unobserved parameter regimes, enabling control overtransitions to arbitrary target states. Crucially, this includes states withdynamics that differ fundamentally from known regimes, such as shifts fromperiodic to intermittent or chaotic behavior. The method's parameter-awarenessfacilitates non-stationary control, ensuring smooth transitions between states.By extending the applicability of machine learning-based control mechanisms topreviously inaccessible target dynamics, this methodology opens the door totransformative new applications while maintaining exceptional efficiency. Ourresults highlight reservoir computing as a powerful alternative to traditionalmethods for dynamic system control.</description><author>Daniel Köglmayr, Alexander Haluszczynski, Christoph Räth</author><pubDate>Fri, 13 Dec 2024 16:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10251v1</guid></item><item><title>Detecting LLM Hallucination Through Layer-wise Information Deficiency: Analysis of Unanswerable Questions and Ambiguous Prompts</title><link>http://arxiv.org/abs/2412.10246v1</link><description>Large language models (LLMs) frequently generate confident yet inaccurateresponses, introducing significant risks for deployment in safety-criticaldomains. We present a novel approach to detecting model hallucination throughsystematic analysis of information flow across model layers when processinginputs with insufficient or ambiguous context. Our investigation reveals thathallucination manifests as usable information deficiencies in inter-layertransmissions. While existing approaches primarily focus on final-layer outputanalysis, we demonstrate that tracking cross-layer information dynamics($\mathcal{L}$I) provides robust indicators of model reliability, accountingfor both information gain and loss during computation. $\mathcal{L}$I improvesmodel reliability by immediately integrating with universal LLMs withoutadditional training or architectural modifications.</description><author>Hazel Kim, Adel Bibi, Philip Torr, Yarin Gal</author><pubDate>Fri, 13 Dec 2024 16:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10246v1</guid></item><item><title>Efficient Continual Pre-training of LLMs for Low-resource Languages</title><link>http://arxiv.org/abs/2412.10244v1</link><description>Open-source Large Language models (OsLLMs) propel the democratization ofnatural language research by giving the flexibility to augment or update modelparameters for performance improvement. Nevertheless, like proprietary LLMs,Os-LLMs offer poorer performance on low-resource languages (LRLs) thanhigh-resource languages (HRLs), owing to smaller amounts of training data andunderrepresented vocabulary. On the other hand, continual pre-training (CPT)with large amounts of language-specific data is a costly proposition in termsof data acquisition and computational resources. Our goal is to drasticallyreduce CPT cost. To that end, we first develop a new algorithm to select asubset of texts from a larger corpus. We show the effectiveness of ourtechnique using very little CPT data. In search of further improvement, wedesign a new algorithm to select tokens to include in the LLM vocabulary. Weexperiment with the recent Llama-3 model and nine Indian languages with diversescripts and extent of resource availability. For evaluation, we useIndicGenBench, a generation task benchmark dataset for Indic languages. Weexperiment with various CPT corpora and augmented vocabulary size and offerinsights across language families.</description><author>Arijit Nag, Soumen Chakrabarti, Animesh Mukherjee, Niloy Ganguly</author><pubDate>Fri, 13 Dec 2024 16:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10244v1</guid></item><item><title>Physics Instrument Design with Reinforcement Learning</title><link>http://arxiv.org/abs/2412.10237v1</link><description>We present a case for the use of Reinforcement Learning (RL) for the designof physics instrument as an alternative to gradient-basedinstrument-optimization methods. It's applicability is demonstrated using twoempirical studies. One is longitudinal segmentation of calorimeters and thesecond is both transverse segmentation as well longitudinal placement oftrackers in a spectrometer. Based on these experiments, we propose analternative approach that offers unique advantages over differentiableprogramming and surrogate-based differentiable design optimization methods.First, Reinforcement Learning (RL) algorithms possess inherent exploratorycapabilities, which help mitigate the risk of convergence to local optima.Second, this approach eliminates the necessity of constraining the design to apredefined detector model with fixed parameters. Instead, it allows for theflexible placement of a variable number of detector components and facilitatesdiscrete decision-making. We then discuss the road map of how this idea can beextended into designing very complex instruments. The presented study sets thestage for a novel framework in physics instrument design, offering a scalableand efficient framework that can be pivotal for future projects such as theFuture Circular Collider (FCC), where most optimized detectors are essentialfor exploring physics at unprecedented energy scales.</description><author>Shah Rukh Qasim, Patrick Owen, Nicola Serra</author><pubDate>Fri, 13 Dec 2024 16:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10237v1</guid></item><item><title>EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling</title><link>http://arxiv.org/abs/2412.10235v1</link><description>Estimating full-body motion using the tracking signals of head and hands fromVR devices holds great potential for various applications. However, thesparsity and unique distribution of observations present a significantchallenge, resulting in an ill-posed problem with multiple feasible solutions(i.e., hypotheses). This amplifies uncertainty and ambiguity in full-bodymotion estimation, especially for the lower-body joints. Therefore, we proposea new method, EnvPoser, that employs a two-stage framework to perform full-bodymotion estimation using sparse tracking signals and pre-scanned environmentfrom VR devices. EnvPoser models the multi-hypothesis nature of human motionthrough an uncertainty-aware estimation module in the first stage. In thesecond stage, we refine these multi-hypothesis estimates by integratingsemantic and geometric environmental constraints, ensuring that the finalmotion estimation aligns realistically with both the environmental context andphysical interactions. Qualitative and quantitative experiments on two publicdatasets demonstrate that our method achieves state-of-the-art performance,highlighting significant improvements in human motion estimation withinmotion-environment interaction scenarios.</description><author>Songpengcheng Xia, Yu Zhang, Zhuo Su, Xiaozheng Zheng, Zheng Lv, Guidong Wang, Yongjie Zhang, Qi Wu, Lei Chu, Ling Pei</author><pubDate>Fri, 13 Dec 2024 16:06:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10235v1</guid></item><item><title>Solving Epistemic Logic Programs using Generate-and-Test with Propagation</title><link>http://arxiv.org/abs/2410.22130v2</link><description>This paper introduces a general framework for generate-and-test-based solversfor epistemic logic programs that can be instantiated with different generatorand tester programs, and we prove sufficient conditions on those programs forthe correctness of the solvers built using this framework. It also introduces anew generator program that incorporates the propagation of epistemicconsequences and shows that this can exponentially reduce the number ofcandidates that need to be tested while only incurring a linear overhead. Weimplement a new solver based on these theoretical findings and experimentallyshow that it outperforms existing solvers by achieving a ~3.3x speed-up andsolving 91% more instances on well-known benchmarks.</description><author>Jorge Fandinno, Lute Lillo</author><pubDate>Fri, 13 Dec 2024 16:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22130v2</guid></item><item><title>SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians</title><link>http://arxiv.org/abs/2412.10231v1</link><description>3D Gaussian Splatting has recently gained traction for its efficient trainingand real-time rendering. While the vanilla Gaussian Splatting representation ismainly designed for view synthesis, more recent works investigated how toextend it with scene understanding and language features. However, existingmethods lack a detailed comprehension of scenes, limiting their ability tosegment and interpret complex structures. To this end, We introduce SuperGSeg,a novel approach that fosters cohesive, context-aware scene representation bydisentangling segmentation and language field distillation. SuperGSeg firstemploys neural Gaussians to learn instance and hierarchical segmentationfeatures from multi-view images with the aid of off-the-shelf 2D masks. Thesefeatures are then leveraged to create a sparse set of what we callSuper-Gaussians. Super-Gaussians facilitate the distillation of 2D languagefeatures into 3D space. Through Super-Gaussians, our method enableshigh-dimensional language feature rendering without extreme increases in GPUmemory. Extensive experiments demonstrate that SuperGSeg outperforms priorworks on both open-vocabulary object localization and semantic segmentationtasks.</description><author>Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari</author><pubDate>Fri, 13 Dec 2024 16:01:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10231v1</guid></item><item><title>Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce</title><link>http://arxiv.org/abs/2410.12691v4</link><description>Language is a symbolic capital that affects people's lives in many ways(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,cultures, traditions, and societies in general. Hence, data in a given languageshould be viewed as more than a collection of tokens. Good data collection andlabeling practices are key to building more human-centered and socially awaretechnologies. While there has been a rising interest in mid- to low-resourcelanguages within the NLP community, work in this space has to overcome uniquechallenges such as data scarcity and access to suitable annotators. In thispaper, we collect feedback from those directly involved in and impacted by NLPartefacts for mid- to low-resource languages. We conduct a quantitative andqualitative analysis of the responses and highlight the main issues related to(1) data quality such as linguistic and cultural data suitability; and (2) theethics of common annotation practices such as the misuse of online communityservices. Based on these findings, we make several recommendations for thecreation of high-quality language artefacts that reflect the cultural milieu ofits speakers, while simultaneously respecting the dignity and labor of dataworkers.</description><author>Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad</author><pubDate>Fri, 13 Dec 2024 15:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12691v4</guid></item><item><title>Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models</title><link>http://arxiv.org/abs/2409.12435v2</link><description>We introduce a novel analysis that leverages linguistic minimal pairs toprobe the internal linguistic representations of Large Language Models (LLMs).By measuring the similarity between LLM activation differences across minimalpairs, we quantify the and gain insight into the linguistic knowledge capturedby LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairsin three languages, reveal properties of linguistic similarity from four keyaspects: consistency across LLMs, relation to theoretical categorizations,dependency to semantic context, and cross-lingual alignment of relevantphenomena. Our findings suggest that 1) linguistic similarity is significantlyinfluenced by training data exposure, leading to higher cross-LLM agreement inhigher-resource languages. 2) Linguistic similarity strongly aligns withfine-grained theoretical linguistic categories but weakly with broader ones. 3)Linguistic similarity shows a weak correlation with semantic similarity,showing its context-dependent nature. 4) LLMs exhibit limited cross-lingualalignment in their understanding of relevant linguistic phenomena. This workdemonstrates the potential of minimal pairs as a window into the neuralrepresentations of language in LLMs, shedding light on the relationship betweenLLMs and linguistic theory. Codes and data are available athttps://github.com/ChenDelong1999/Linguistic-Similarity</description><author>Xinyu Zhou, Delong Chen, Samuel Cahyawijaya, Xufeng Duan, Zhenguang G. Cai</author><pubDate>Fri, 13 Dec 2024 15:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12435v2</guid></item><item><title>SPT: Sequence Prompt Transformer for Interactive Image Segmentation</title><link>http://arxiv.org/abs/2412.10224v1</link><description>Interactive segmentation aims to extract objects of interest from an imagebased on user-provided clicks. In real-world applications, there is often aneed to segment a series of images featuring the same target object. However,existing methods typically process one image at a time, failing to consider thesequential nature of the images. To overcome this limitation, we propose anovel method called Sequence Prompt Transformer (SPT), the first to utilizesequential image information for interactive segmentation. Our model comprisestwo key components: (1) Sequence Prompt Transformer (SPT) for acquiringinformation from sequence of images, clicks and masks to improve accurate. (2)Top-k Prompt Selection (TPS) selects precise prompts for SPT to further enhancethe segmentation effect. Additionally, we create the ADE20K-Seq benchmark tobetter evaluate model performance. We evaluate our approach on multiplebenchmark datasets and show that our model surpasses state-of-the-art methodsacross all datasets.</description><author>Senlin Cheng, Haopeng Sun</author><pubDate>Fri, 13 Dec 2024 15:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10224v1</guid></item><item><title>CAS-GAN for Contrast-free Angiography Synthesis</title><link>http://arxiv.org/abs/2410.08490v3</link><description>Iodinated contrast agents are widely utilized in numerous interventionalprocedures, yet posing substantial health risks to patients. This paperpresents CAS-GAN, a novel GAN framework that serves as a "virtual contrastagent" to synthesize X-ray angiographies via disentanglement representationlearning and vessel semantic guidance, thereby reducing the reliance oniodinated contrast agents during interventional procedures. Specifically, ourapproach disentangles X-ray angiographies into background and vesselcomponents, leveraging medical prior knowledge. A specialized predictor thenlearns to map the interrelationships between these components. Additionally, avessel semantic-guided generator and a corresponding loss function areintroduced to enhance the visual fidelity of generated images. Experimentalresults on the XCAD dataset demonstrate the state-of-the-art performance of ourCAS-GAN, achieving a FID of 5.87 and a MMD of 0.016. These promising resultshighlight CAS-GAN's potential for clinical applications.</description><author>De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Hao Li, Tian-Yu Xiang, Zeng-Guang Hou</author><pubDate>Fri, 13 Dec 2024 15:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08490v3</guid></item><item><title>Multi-Feature Fusion and Compressed Bi-LSTM for Memory-Efficient Heartbeat Classification on Wearable Devices</title><link>http://arxiv.org/abs/2405.15312v2</link><description>In this article, we present a resource-efficient approach forelectrocardiogram (ECG) based heartbeat classification using multi-featurefusion and bidirectional long short-term memory (Bi-LSTM). The datasetcomprises five original classes from the MIT-BIH Arrhythmia Database: Normal(N), Left Bundle Branch Block (LBBB), Right Bundle Branch Block (RBBB),Premature Ventricular Contraction (PVC), and Paced Beat (PB). Preprocessingmethods including the discrete wavelet transform and dual moving averagewindows are used to reduce noise and artifacts in the raw ECG signal, andextract the main points (PQRST) of the ECG waveform. Multi-feature fusion isachieved by utilizing time intervals and the proposed under-the-curve areas,which are inherently robust against noise, as input features. Simulationsdemonstrated that incorporating under-the-curve area features improved theclassification accuracy for the challenging RBBB and LBBB classes from 31.4\%to 84.3\% for RBBB, and from 69.6\% to 87.0\% for LBBB. Using a Bi-LSTMnetwork, rather than a conventional LSTM network, resulted in higher accuracy(33.8\% vs 21.8\%) with a 28\% reduction in required network parameters for theRBBB class. Multiple neural network models with varying parameter sizes,including tiny (84k), small (150k), medium (478k), and large (1.25M) models,are developed to achieve high accuracy \textit{across all classes}, a morecrucial and challenging goal than overall classification accuracy.</description><author>Reza Nikandish, Jiayu He, Benyamin Haghi</author><pubDate>Fri, 13 Dec 2024 15:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15312v2</guid></item><item><title>Fair Decentralized Learning</title><link>http://arxiv.org/abs/2410.02541v2</link><description>Decentralized learning (DL) is an emerging approach that enables nodes tocollaboratively train a machine learning model without sharing raw data. Inmany application domains, such as healthcare, this approach faces challengesdue to the high level of heterogeneity in the training data's feature space.Such feature heterogeneity lowers model utility and negatively impactsfairness, particularly for nodes with under-represented training data. In thispaper, we introduce \textsc{Facade}, a clustering-based DL algorithmspecifically designed for fair model training when the training data exhibitsseveral distinct features. The challenge of \textsc{Facade} is to assign nodesto clusters, one for each feature, based on the similarity in the features oftheir local data, without requiring individual nodes to know apriori whichcluster they belong to. \textsc{Facade} (1) dynamically assigns nodes to theirappropriate clusters over time, and (2) enables nodes to collaboratively traina specialized model for each cluster in a fully decentralized manner. Wetheoretically prove the convergence of \textsc{Facade}, implement ouralgorithm, and compare it against three state-of-the-art baselines. Ourexperimental results on three datasets demonstrate the superiority of ourapproach in terms of model accuracy and fairness compared to all threecompetitors. Compared to the best-performing baseline, \textsc{Facade} on theCIFAR-10 dataset also reduces communication costs by 32.3\% to reach a targetaccuracy when cluster sizes are imbalanced.</description><author>Sayan Biswas, Anne-Marie Kermarrec, Rishi Sharma, Thibaud Trinca, Martijn de Vos</author><pubDate>Fri, 13 Dec 2024 15:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02541v2</guid></item><item><title>How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives</title><link>http://arxiv.org/abs/2412.10220v1</link><description>A rapidly developing application of LLMs in XAI is to convert quantitativeexplanations such as SHAP into user-friendly narratives to explain thedecisions made by smaller prediction models. Evaluating the narratives withoutrelying on human preference studies or surveys is becoming increasinglyimportant in this field. In this work we propose a framework and exploreseveral automated metrics to evaluate LLM-generated narratives for explanationsof tabular classification tasks. We apply our approach to compare severalstate-of-the-art LLMs across different datasets and prompt types. As ademonstration of their utility, these metrics allow us to identify newchallenges related to LLM hallucinations for XAI narratives.</description><author>Timour Ichmoukhamedov, James Hinns, David Martens</author><pubDate>Fri, 13 Dec 2024 15:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10220v1</guid></item><item><title>Learning Complex Non-Rigid Image Edits from Multimodal Conditioning</title><link>http://arxiv.org/abs/2412.10219v1</link><description>In this paper we focus on inserting a given human (specifically, a singleimage of a person) into a novel scene. Our method, which builds on top ofStable Diffusion, yields natural looking images while being highly controllablewith text and pose. To accomplish this we need to train on pairs of images, thefirst a reference image with the person, the second a "target image" showingthe same person (with a different pose and possibly in a different background).Additionally we require a text caption describing the new pose relative to thatin the reference image. In this paper we present a novel dataset following thiscriteria, which we create using pairs of frames from human-centric andaction-rich videos and employing a multimodal LLM to automatically summarizethe difference in human pose for the text captions. We demonstrate thatidentity preservation is a more challenging task in scenes "in-the-wild", andespecially scenes where there is an interaction between persons and objects.Combining the weak supervision from noisy captions, with robust 2D poseimproves the quality of person-object interactions.</description><author>Nikolai Warner, Jack Kolb, Meera Hahn, Vighnesh Birodkar, Jonathan Huang, Irfan Essa</author><pubDate>Fri, 13 Dec 2024 15:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10219v1</guid></item><item><title>Low-Latency Scalable Streaming for Event-Based Vision</title><link>http://arxiv.org/abs/2412.07889v2</link><description>Recently, we have witnessed the rise of novel ``event-based'' camera sensorsfor high-speed, low-power video capture. Rather than recording discrete imageframes, these sensors output asynchronous ``event'' tuples with microsecondprecision, only when the brightness change of a given pixel exceeds a certainthreshold. Although these sensors have enabled compelling new computer visionapplications, these applications often require expensive, power-hungry GPUsystems, rendering them incompatible for deployment on the low-power devicesfor which event cameras are optimized. Whereas receiver-driven rate adaptationis a crucial feature of modern video streaming solutions, this topic isunderexplored in the realm of event-based vision systems. On a real-world eventcamera dataset, we first demonstrate that a state-of-the-art object detectionapplication is resilient to dramatic data loss, and that this loss may beweighted towards the end of each temporal window. We then propose a scalablestreaming method for event-based data based on Media Over QUIC, prioritizingobject detection performance and low latency. The application server canreceive complementary event data across several streams simultaneously, anddrop streams as needed to maintain a certain latency. With a latency target of5 ms for end-to-end transmission across a small network, we observe an averagereduction in detection mAP as low as 0.36. With a more relaxed latency targetof 50 ms, we observe an average mAP reduction as low as 0.19.</description><author>Andrew Hamara, Benjamin Kilpatrick, Alex Baratta, Brendon Kofink, Andrew C. Freeman</author><pubDate>Fri, 13 Dec 2024 15:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07889v2</guid></item><item><title>Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Preference Optimization</title><link>http://arxiv.org/abs/2409.12741v3</link><description>Large Language Model (LLM) fine tuning is underutilized in the field ofmedicine. Two of the most common methods of fine tuning are Supervised FineTuning (SFT) and Direct Preference Optimization (DPO), but there is littleguidance informing users when to use either technique. In this investigation,we compare the performance of SFT and DPO for five common natural languagetasks in medicine: Classification with text data, Classification with numericdata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFTalone is sufficient for Classification with text data, whereas DPO improvesperformance for the more complex tasks of Clinical Reasoning, Summarization andClinical Triage. Our results establish the role and importance of DPO finetuning within medicine, and consequently call attention to current softwaregaps that prevent widespread deployment of this technique.</description><author>Thomas Savage, Stephen Ma, Abdessalem Boukil, Vishwesh Patel, Ekanath Rangan, Ivan Lopez, Jonathan H Chen</author><pubDate>Fri, 13 Dec 2024 15:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12741v3</guid></item><item><title>RAID-Database: human Responses to Affine Image Distortions</title><link>http://arxiv.org/abs/2412.10211v1</link><description>Image quality databases are used to train models for predicting subjectivehuman perception. However, most existing databases focus on distortionscommonly found in digital media and not in natural conditions. Affinetransformations are particularly relevant to study, as they are among the mostcommonly encountered by human observers in everyday life. This Data Descriptorpresents a set of human responses to suprathreshold affine image transforms(rotation, translation, scaling) and Gaussian noise as convenient reference tocompare with previously existing image quality databases. The responses weremeasured using well established psychophysics: the Maximum LikelihoodDifference Scaling method. The set contains responses to 864 distorted images.The experiments involved 105 observers and more than 20000 comparisons ofquadruples of images. The quality of the dataset is ensured because (a) itreproduces the classical Pi\'eron's law, (b) it reproduces classical absolutedetection thresholds, and (c) it is consistent with conventional image qualitydatabases but improves them according to Group-MAD experiments.</description><author>Paula Daudén-Oliver, David Agost-Beltran, Emilio Sansano-Sansano, Valero Laparra, Jesús Malo, Marina Martínez-Garcia</author><pubDate>Fri, 13 Dec 2024 15:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10211v1</guid></item><item><title>Exact gradients for linear optics with single photons</title><link>http://arxiv.org/abs/2409.16369v2</link><description>Though parameter shift rules have drastically improved gradient estimationmethods for several types of quantum circuits, leading to improved performancein downstream tasks, so far they have not been transferable to linear opticswith single photons. In this work, we derive an analytical formula for thegradients in these circuits with respect to phaseshifters via a generalizedparameter shift rule, where the number of parameter shifts depends linearly onthe total number of photons. Experimentally, this enables access to derivativesin photonic systems without the need for finite difference approximations.Building on this, we propose two strategies through which one can reduce thenumber of shifts in the expression, and hence reduce the overall samplecomplexity. Numerically, we show that this generalized parameter-shift rule canconverge to the minimum of a cost function with fewer parameter update stepsthan alternative techniques. We anticipate that this method will open up newavenues to solving optimization problems with photonic systems, as well asprovide new techniques for the experimental characterization and control oflinear optical systems.</description><author>Giorgio Facelli, David D. Roberts, Hugo Wallner, Alexander Makarovskiy, Zoë Holmes, William R. Clements</author><pubDate>Fri, 13 Dec 2024 15:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16369v2</guid></item><item><title>GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion</title><link>http://arxiv.org/abs/2412.10209v1</link><description>We propose a novel approach for reconstructing animatable 3D Gaussian avatarsfrom monocular videos captured by commodity devices like smartphones.Photorealistic 3D head avatar reconstruction from such recordings ischallenging due to limited observations, which leaves unobserved regionsunder-constrained and can lead to artifacts in novel views. To address thisproblem, we introduce a multi-view head diffusion model, leveraging its priorsto fill in missing regions and ensure view consistency in Gaussian splattingrenderings. To enable precise viewpoint control, we use normal maps renderedfrom FLAME-based head reconstruction, which provides pixel-aligned inductivebiases. We also condition the diffusion model on VAE features extracted fromthe input image to preserve details of facial identity and appearance. ForGaussian avatar reconstruction, we distill multi-view diffusion priors by usingiteratively denoised images as pseudo-ground truths, effectively mitigatingover-saturation issues. To further improve photorealism, we apply latentupsampling to refine the denoised latent before decoding it into an image. Weevaluate our method on the NeRSemble dataset, showing that GAF outperforms theprevious state-of-the-art methods in novel view synthesis by a 5.34\% higherSSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructionsfrom monocular videos captured on commodity devices.</description><author>Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner</author><pubDate>Fri, 13 Dec 2024 15:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10209v1</guid></item><item><title>Efficient Generative Modeling with Residual Vector Quantization-Based Tokens</title><link>http://arxiv.org/abs/2412.10208v1</link><description>We explore the use of Residual Vector Quantization (RVQ) for high-fidelitygeneration in vector-quantized generative models. This quantization techniquemaintains higher data fidelity by employing more in-depth tokens. However,increasing the token number in generative models leads to slower inferencespeeds. To this end, we introduce ResGen, an efficient RVQ-based discretediffusion model that generates high-fidelity samples without compromisingsampling speed. Our key idea is a direct prediction of vector embedding ofcollective tokens rather than individual ones. Moreover, we demonstrate thatour proposed token masking and multi-token prediction method can be formulatedwithin a principled probabilistic framework using a discrete diffusion processand variational inference. We validate the efficacy and generalizability of theproposed method on two challenging tasks across different modalities:conditional image generation} on ImageNet 256x256 and zero-shot text-to-speechsynthesis. Experimental results demonstrate that ResGen outperformsautoregressive counterparts in both tasks, delivering superior performancewithout compromising sampling speed. Furthermore, as we scale the depth of RVQ,our generative models exhibit enhanced generation fidelity or faster samplingspeeds compared to similarly sized baseline models. The project page can befound at https://resgen-genai.github.io</description><author>Jaehyeon Kim, Taehong Moon, Keon Lee, Jaewoong Cho</author><pubDate>Fri, 13 Dec 2024 15:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10208v1</guid></item><item><title>Retrieval-Augmented Semantic Parsing: Using Large Language Models to Improve Generalization</title><link>http://arxiv.org/abs/2412.10207v1</link><description>Open-domain semantic parsing remains a challenging task, as models often relyon heuristics and struggle to handle unseen concepts. In this paper, weinvestigate the potential of large language models (LLMs) for this task andintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effectiveapproach that integrates external lexical knowledge into the parsing process.Our experiments not only show that LLMs outperform previous encoder-decoderbaselines for semantic parsing, but that RASP further enhances their ability topredict unseen concepts, nearly doubling the performance of previous models onout-of-distribution concepts. These findings highlight the promise ofleveraging large language models and retrieval mechanisms for robust andopen-domain semantic parsing.</description><author>Xiao Zhang, Qianru Meng, Johan Bos</author><pubDate>Fri, 13 Dec 2024 15:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10207v1</guid></item><item><title>ADA-Track++: End-to-End Multi-Camera 3D Multi-Object Tracking with Alternating Detection and Association</title><link>http://arxiv.org/abs/2405.08909v2</link><description>Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt thetracking-by-attention paradigm, utilizing track queries for identity-consistentdetection and object queries for identity-agnostic track spawning.Tracking-by-attention, however, entangles detection and tracking queries in oneembedding for both the detection and tracking task, which is sub-optimal. Otherapproaches resemble the tracking-by-detection paradigm and detect objects usingdecoupled track and detection queries followed by a subsequent association.These methods, however, do not leverage synergies between the detection andassociation task. Combining the strengths of both paradigms, we introduceADA-Track++, a novel end-to-end framework for 3D MOT from multi-view cameras.We introduce a learnable data association module based on edge-augmentedcross-attention, leveraging appearance and geometric features. We also proposean auxiliary token in this attention-based association module, which helpsmitigate disproportionately high attention to incorrect association targetscaused by attention normalization. Furthermore, we integrate this associationmodule into the decoder layer of a DETR-based 3D detector, enablingsimultaneous DETR-like query-to-image cross-attention for detection andquery-to-query cross-attention for data association. By stacking these decoderlayers, queries are refined for the detection and association task alternately,effectively harnessing the task dependencies. We evaluate our method on thenuScenes dataset and demonstrate the advantage of our approach compared to thetwo previous paradigms.</description><author>Shuxiao Ding, Lukas Schneider, Marius Cordts, Juergen Gall</author><pubDate>Fri, 13 Dec 2024 15:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08909v2</guid></item><item><title>A logic for reasoning with inconsistent knowledge -- A reformulation using nowadays terminology (2024)</title><link>http://arxiv.org/abs/2411.10197v2</link><description>In many situations humans have to reason with inconsistent knowledge. Theseinconsistencies may occur due to not fully reliable sources of information. Inorder to reason with inconsistent knowledge, it is not possible to view a setof premisses as absolute truths as is done in predicate logic. Viewing the setof premisses as a set of assumptions, however, it is possible to deduce usefulconclusions from an inconsistent set of premisses. In this paper a logic forreasoning with inconsistent knowledge is described. This logic is ageneralization of the work of N. Rescher [15]. In the logic a reliabilityrelation is used to choose between incompatible assumptions. These choices areonly made when a contradiction is derived. As long as no contradiction isderived, the knowledge is assumed to be consistent. This makes it possible todefine an argumentation-based deduction process for the logic. For the logic asemantics based on the ideas of Y. Shoham [22, 23], is defined. It turns outthat the semantics for the logic is a preferential semantics according to thedefinition S. Kraus, D. Lehmann and M. Magidor [12]. Therefore the logic is alogic of system P and possesses all the properties of an ideal non-monotoniclogic.</description><author>Nico Roos</author><pubDate>Fri, 13 Dec 2024 15:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10197v2</guid></item><item><title>Towards the Characterization of Representations Learned via Capsule-based Network Architectures</title><link>http://arxiv.org/abs/2305.05349v2</link><description>Capsule Networks (CapsNets) have been re-introduced as a more compact andinterpretable alternative to standard deep neural networks. While recentefforts have proved their compression capabilities, to date, theirinterpretability properties have not been fully assessed. Here, we conduct asystematic and principled study towards assessing the interpretability of thesetypes of networks. Moreover, we pay special attention towards analyzing thelevel to which part-whole relationships are indeed encoded within the learnedrepresentation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebAdatasets suggest that the representations encoded in CapsNets might not be asdisentangled nor strictly related to parts-whole relationships as is commonlystated in the literature.</description><author>Saja Tawalbeh, José Oramas</author><pubDate>Fri, 13 Dec 2024 15:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05349v2</guid></item><item><title>Integrative Analysis of Financial Market Sentiment Using CNN and GRU for Risk Prediction and Alert Systems</title><link>http://arxiv.org/abs/2412.10199v1</link><description>This document presents an in-depth examination of stock market sentimentthrough the integration of Convolutional Neural Networks (CNN) and GatedRecurrent Units (GRU), enabling precise risk alerts. The robust featureextraction capability of CNN is utilized to preprocess and analyze extensivenetwork text data, identifying local features and patterns. The extractedfeature sequences are then input into the GRU model to understand theprogression of emotional states over time and their potential impact on futuremarket sentiment and risk. This approach addresses the order dependence andlong-term dependencies inherent in time series data, resulting in a detailedanalysis of stock market sentiment and effective early warnings of futurerisks.</description><author>You Wu, Mengfang Sun, Hongye Zheng, Jinxin Hu, Yingbin Liang, Zhenghao Lin</author><pubDate>Fri, 13 Dec 2024 15:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10199v1</guid></item><item><title>TrustUQA: A Trustful Framework for Unified Structured Data Question Answering</title><link>http://arxiv.org/abs/2406.18916v2</link><description>Natural language question answering (QA) over structured data sources such astables and knowledge graphs have been widely investigated, especially withLarge Language Models (LLMs) in recent years. The main solutions includequestion to formal query parsing and retrieval-based answer generation.However, current methods of the former often suffer from weak generalization,failing to dealing with multi-types of sources, while the later is limited intrustfulness. In this paper, we propose TrustUQA, a trustful QA framework thatcan simultaneously support multiple types of structured data in a unified way.To this end, it adopts an LLM-friendly and unified knowledge representationmethod called Condition Graph(CG), and uses an LLM and demonstration-basedtwo-level method for CG querying. For enhancement, it is also equipped withdynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarkscovering 3 types of structured data. It outperforms 2 existing unifiedstructured data QA methods. In comparison with the baselines that are specificto one data type, it achieves state-of-the-art on 2 of the datasets. Furthermore, we have demonstrated the potential of our method for more general QAtasks, QA over mixed structured data and QA across structured data. The code isavailable at https://github.com/zjukg/TrustUQA.</description><author>Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang, Huajun Chen</author><pubDate>Fri, 13 Dec 2024 15:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18916v2</guid></item><item><title>From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection</title><link>http://arxiv.org/abs/2412.10198v1</link><description>Tool-calling has changed Large Language Model (LLM) applications byintegrating external tools, significantly enhancing their functionality acrossdiverse tasks. However, this integration also introduces new securityvulnerabilities, particularly in the tool scheduling mechanisms of LLM, whichhave not been extensively studied. To fill this gap, we present ToolCommander,a novel framework designed to exploit vulnerabilities in LLM tool-callingsystems through adversarial tool injection. Our framework employs awell-designed two-stage attack strategy. Firstly, it injects malicious tools tocollect user queries, then dynamically updates the injected tools based on thestolen information to enhance subsequent attacks. These stages enableToolCommander to execute privacy theft, launch denial-of-service attacks, andeven manipulate business competition by triggering unscheduled tool-calling.Notably, the ASR reaches 91.67% for privacy theft and hits 100% fordenial-of-service and unscheduled tool calling in certain cases. Our workdemonstrates that these vulnerabilities can lead to severe consequences beyondsimple misuse of tool-calling systems, underscoring the urgent need for robustdefensive strategies to secure LLM Tool-calling systems.</description><author>Haowei Wang, Rupeng Zhang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, Qing Wang</author><pubDate>Fri, 13 Dec 2024 15:15:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10198v1</guid></item><item><title>High-dimensional Statistics Applications to Batch Effects in Metabolomics</title><link>http://arxiv.org/abs/2412.10196v1</link><description>Batch effects are inevitable in large-scale metabolomics. Prior to formaldata analysis, batch effect correction (BEC) is applied to prevent fromobscuring biological variations, and batch effect evaluation (BEE) is used forcorrection assessment. However, existing BEE algorithms neglect covariancesbetween the variables, and existing BEC algorithms might fail to adequatelycorrect the covariances. Therefore, we resort to recent advancements inhigh-dimensional statistics, and respectively propose "quality control-basedsimultaneous tests (QC-ST)" and "covariance correction (CoCo)". Validated bythe simulation data, QC-ST can simultaneously detect the statisticalsignificance of QC samples' mean vectors and covariance matrices acrossdifferent batches, and has a satisfactory statistical performance in empiricalsizes, empirical powers, and computational speed. Then, we apply four QC-basedBEC algorithms to two large cohort datasets, and find that extreme gradientboost (XGBoost) performs best in relative standard deviation (RSD) anddispersion-ratio (D-ratio). After prepositive BEC, if QC-ST still suggests thatbatch effects between some two batches are significant, CoCo should beimplemented. And after CoCo (if necessary), the four metrics (i.e., RSD,D-ratio, classification performance, and QC-ST) might be further improved. Insummary, under the guidance of QC-ST, we can develop a matching strategy tointegrate multiple BEC algorithms more rationally and flexibly, and minimizebatch effects for reliable biological conclusions.</description><author>Zhendong Guo</author><pubDate>Fri, 13 Dec 2024 15:10:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10196v1</guid></item><item><title>SoK: Decentralized AI (DeAI)</title><link>http://arxiv.org/abs/2411.17461v2</link><description>The centralization of Artificial Intelligence (AI) poses significantchallenges, including single points of failure, inherent biases, data privacyconcerns, and scalability issues. These problems are especially prevalent inclosed-source large language models (LLMs), where user data is collected andused without transparency. To mitigate these issues, blockchain-baseddecentralized AI (DeAI) has emerged as a promising solution. DeAI combines thestrengths of both blockchain and AI technologies to enhance the transparency,security, decentralization, and trustworthiness of AI systems. However, acomprehensive understanding of state-of-the-art DeAI development, particularlyfor active industry solutions, is still lacking. In this work, we present aSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. Wepropose a taxonomy to classify existing DeAI protocols based on the modellifecycle. Based on this taxonomy, we provide a structured way to clarify thelandscape of DeAI protocols and identify their similarities and differences. Weanalyze the functionalities of blockchain in DeAI, investigating how blockchainfeatures contribute to enhancing the security, transparency, andtrustworthiness of AI processes, while also ensuring fair incentives for AIdata and model contributors. In addition, we identify key insights and researchgaps in developing DeAI protocols, highlighting several critical avenues forfuture research.</description><author>Zhipeng Wang, Rui Sun, Elizabeth Lui, Vatsal Shah, Xihan Xiong, Jiahao Sun, Davide Crapis, William Knottenbelt</author><pubDate>Fri, 13 Dec 2024 15:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17461v2</guid></item><item><title>Simple Guidance Mechanisms for Discrete Diffusion Models</title><link>http://arxiv.org/abs/2412.10193v1</link><description>Diffusion models for continuous data gained widespread adoption owing totheir high quality generation and control mechanisms. However, controllablediffusion on discrete data faces challenges given that continuous guidancemethods do not directly apply to discrete diffusion. Here, we provide astraightforward derivation of classifier-free and classifier-based guidance fordiscrete diffusion, as well as a new class of diffusion models that leverageuniform noise and that are more guidable because they can continuously edittheir outputs. We improve the quality of these models with a novelcontinuous-time variational lower bound that yields state-of-the-artperformance, especially in settings involving guidance or fast generation.Empirically, we demonstrate that our guidance mechanisms combined with uniformnoise diffusion improve controllable generation relative to autoregressive anddiffusion baselines on several discrete data domains, including genomicsequences, small molecule design, and discretized image generation.</description><author>Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo P. de Almeida, Alexander Rush, Thomas Pierrot, Volodymyr Kuleshov</author><pubDate>Fri, 13 Dec 2024 15:08:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10193v1</guid></item><item><title>A Gated Residual Kolmogorov-Arnold Networks for Mixtures of Experts</title><link>http://arxiv.org/abs/2409.15161v2</link><description>This paper introduces KAMoE, a novel Mixture of Experts (MoE) framework basedon Gated Residual Kolmogorov-Arnold Networks (GRKAN). We propose GRKAN as analternative to the traditional gating function, aiming to enhance efficiencyand interpretability in MoE modeling. Through extensive experiments on digitalasset markets and real estate valuation, we demonstrate that KAMoE consistentlyoutperforms traditional MoE architectures across various tasks and model types.Our results show that GRKAN exhibits superior performance compared to standardGating Residual Networks, particularly in LSTM-based models for sequentialtasks. We also provide insights into the trade-offs between model complexityand performance gains in MoE and KAMoE architectures.</description><author>Hugo Inzirillo, Remi Genet</author><pubDate>Fri, 13 Dec 2024 15:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15161v2</guid></item><item><title>Bridging Sequence-Structure Alignment in RNA Foundation Models</title><link>http://arxiv.org/abs/2407.11242v3</link><description>The alignment between RNA sequences and structures in foundation models (FMs)has yet to be thoroughly investigated. Existing FMs have struggled to establishsequence-structure alignment, hindering the free flow of genomic informationbetween RNA sequences and structures. In this study, we introduce OmniGenome,an RNA FM trained to align RNA sequences with respect to secondary structuresbased on structure-contextualised modelling. The alignment enables free andbidirectional mappings between sequences and structures by utilising theflexible RNA modelling paradigm that supports versatile input and outputmodalities, i.e., sequence and/or structure as input/output. We implement RNAdesign and zero-shot secondary structure prediction as case studies to evaluatethe Seq2Str and Str2Seq mapping capacity of OmniGenome. Results on the EternaV2benchmark show that OmniGenome solved 74% of puzzles, whereas existing FMs onlysolved up to 3% of the puzzles due to the oversight of sequence-structurealignment. We leverage four comprehensive in-silico genome modelling benchmarksto evaluate performance across a diverse set of genome downstream tasks, wherethe results show that OmniGenome achieves state-of-the-art performance on RNAand DNA benchmarks, even without any training on DNA genomes.</description><author>Heng Yang, Renzhi Chen, Ke Li</author><pubDate>Fri, 13 Dec 2024 14:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11242v3</guid></item><item><title>BiCert: A Bilinear Mixed Integer Programming Formulation for Precise Certified Bounds Against Data Poisoning Attacks</title><link>http://arxiv.org/abs/2412.10186v1</link><description>Data poisoning attacks pose one of the biggest threats to modern AI systems,necessitating robust defenses. While extensive efforts have been made todevelop empirical defenses, attackers continue to evolve, creatingsophisticated methods to circumvent these measures. To address this, we mustmove beyond empirical defenses and establish provable certification methodsthat guarantee robustness. This paper introduces a novel certificationapproach, BiCert, using Bilinear Mixed Integer Programming (BMIP) to computesound deterministic bounds that provide such provable robustness. Using BMIP,we compute the reachable set of parameters that could result from training withpotentially manipulated data. A key element to make this computation feasibleis to relax the reachable parameter set to a convex set between trainingiterations. At test time, this parameter set allows us to predict all possibleoutcomes, guaranteeing robustness. BiCert is more precise than previousmethods, which rely solely on interval and polyhedral bounds. Crucially, ourapproach overcomes the fundamental limitation of prior approaches whereparameter bounds could only grow, often uncontrollably. We show that BiCert'stighter bounds eliminate a key source of divergence issues, resulting in morestable training and higher certified accuracy.</description><author>Tobias Lorenz, Marta Kwiatkowska, Mario Fritz</author><pubDate>Fri, 13 Dec 2024 14:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10186v1</guid></item><item><title>Solving Robust Markov Decision Processes: Generic, Reliable, Efficient</title><link>http://arxiv.org/abs/2412.10185v1</link><description>Markov decision processes (MDP) are a well-established model for sequentialdecision-making in the presence of probabilities. In robust MDP (RMDP), everyaction is associated with an uncertainty set of probability distributions,modelling that transition probabilities are not known precisely. Based on theknown theoretical connection to stochastic games, we provide a framework forsolving RMDPs that is generic, reliable, and efficient. It is *generic* bothwith respect to the model, allowing for a wide range of uncertainty sets,including but not limited to intervals, $L^1$- or $L^2$-balls, and polytopes;and with respect to the objective, including long-run average reward,undiscounted total reward, and stochastic shortest path. It is *reliable*, asour approach not only converges in the limit, but provides precision guaranteesat any time during the computation. It is *efficient* because -- in contrast tostate-of-the-art approaches -- it avoids explicitly constructing the underlyingstochastic game. Consequently, our prototype implementation outperformsexisting tools by several orders of magnitude and can solve RMDPs with amillion states in under a minute.</description><author>Tobias Meggendorfer, Maximilian Weininger, Patrick Wienhöft</author><pubDate>Fri, 13 Dec 2024 14:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10185v1</guid></item><item><title>Sims: An Interactive Tool for Geospatial Matching and Clustering</title><link>http://arxiv.org/abs/2412.10184v1</link><description>Acquiring, processing, and visualizing geospatial data requires significantcomputing resources, especially for large spatio-temporal domains. Thischallenge hinders the rapid discovery of predictive features, which isessential for advancing geospatial modeling. To address this, we developedSimilarity Search (Sims), a no-code web tool that allows users to visualize,compare, cluster, and perform similarity search over defined regions ofinterest using Google Earth Engine as a backend. Sims is designed to complementexisting modeling tools by focusing on feature exploration rather than modelcreation. We demonstrate the utility of Sims through a case study analyzingsimulated maize yield data in Rwanda, where we evaluate how differentcombinations of soil, weather, and agronomic features affect the clustering ofyield response zones. Sims is open source and available athttps://github.com/microsoft/Sims</description><author>Akram Zaytar, Girmaw Abebe Tadesse, Caleb Robinson, Eduardo G. Bendito, Medha Devare, Meklit Chernet, Gilles Q. Hacheme, Rahul Dodhia, Juan M. Lavista Ferres</author><pubDate>Fri, 13 Dec 2024 14:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10184v1</guid></item><item><title>Multi-Head Encoding for Extreme Label Classification</title><link>http://arxiv.org/abs/2412.10182v1</link><description>The number of categories of instances in the real world is normally huge, andeach instance may contain multiple labels. To distinguish these massive labelsutilizing machine learning, eXtreme Label Classification (XLC) has beenestablished. However, as the number of categories increases, the number ofparameters and nonlinear operations in the classifier also rises. This resultsin a Classifier Computational Overload Problem (CCOP). To address this, wepropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanillaclassifier with a multi-head classifier. During the training process, MHEdecomposes extreme labels into the product of multiple short local labels, witheach head trained on these local labels. During testing, the predicted labelscan be directly calculated from the local predictions of each head. Thisreduces the computational load geometrically. Then, according to thecharacteristics of different XLC tasks, e.g., single-label, multi-label, andmodel pretraining tasks, three MHE-based implementations, i.e., Multi-HeadProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to moreeffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE canachieve performance approximately equivalent to that of the vanilla classifierby generalizing the low-rank approximation problem from Frobenius-norm toCross-Entropy. Experimental results show that the proposed methods achievestate-of-the-art performance while significantly streamlining the training andinference processes of XLC tasks. The source code has been made public athttps://github.com/Anoise/MHE.</description><author>Daojun Liang, Haixia Zhang, Dongfeng Yuan, Minggao Zhang</author><pubDate>Fri, 13 Dec 2024 14:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10182v1</guid></item><item><title>Ultra-High Resolution Segmentation via Boundary-Enhanced Patch-Merging Transformer</title><link>http://arxiv.org/abs/2412.10181v1</link><description>Segmentation of ultra-high resolution (UHR) images is a critical task withnumerous applications, yet it poses significant challenges due to high spatialresolution and rich fine details. Recent approaches adopt a dual-brancharchitecture, where a global branch learns long-range contextual informationand a local branch captures fine details. However, they struggle to handle theconflict between global and local information while adding significant extracomputational cost. Inspired by the human visual system's ability to rapidlyorient attention to important areas with fine details and filter out irrelevantinformation, we propose a novel UHR segmentation method calledBoundary-enhanced Patch-merging Transformer (BPT). BPT consists of two keycomponents: (1) Patch-Merging Transformer (PMT) for dynamically allocatingtokens to informative regions to acquire global and local representations, and(2) Boundary-Enhanced Module (BEM) that leverages boundary information toenrich fine details. Extensive experiments on multiple UHR image segmentationbenchmarks demonstrate that our BPT outperforms previous state-of-the-artmethods without introducing extra computational overhead. Codes will bereleased to facilitate research.</description><author>Haopeng Sun</author><pubDate>Fri, 13 Dec 2024 14:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10181v1</guid></item><item><title>SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models</title><link>http://arxiv.org/abs/2412.10178v1</link><description>Given an input video of a person and a new garment, the objective of thispaper is to synthesize a new video where the person is wearing the specifiedgarment while maintaining spatiotemporal consistency. While significantadvances have been made in image-based virtual try-ons, extending thesesuccesses to video often results in frame-to-frame inconsistencies. Someapproaches have attempted to address this by increasing the overlap of framesacross multiple video chunks, but this comes at a steep computational cost dueto the repeated processing of the same frames, especially for long videosequence. To address these challenges, we reconceptualize video virtual try-onas a conditional video inpainting task, with garments serving as inputconditions. Specifically, our approach enhances image diffusion models byincorporating temporal attention layers to improve temporal coherence. Toreduce computational overhead, we introduce ShiftCaching, a novel techniquethat maintains temporal consistency while minimizing redundant computations.Furthermore, we introduce the \dataname~dataset, a new video try-on datasetfeaturing more complex backgrounds, challenging movements, and higherresolution compared to existing public datasets. Extensive experiments showthat our approach outperforms current baselines, particularly in terms of videoconsistency and inference speed. Data and code are available athttps://github.com/VinAIResearch/swift-try</description><author>Hung Nguyen, Quang Qui-Vinh Nguyen, Khoi Nguyen, Rang Nguyen</author><pubDate>Fri, 13 Dec 2024 14:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10178v1</guid></item><item><title>UN-DETR: Promoting Objectness Learning via Joint Supervision for Unknown Object Detection</title><link>http://arxiv.org/abs/2412.10176v1</link><description>Unknown Object Detection (UOD) aims to identify objects of unseen categories,differing from the traditional detection paradigm limited by the closed-worldassumption. A key component of UOD is learning a generalized representation,i.e. objectness for both known and unknown categories to distinguish andlocalize objects from the background in a class-agnostic manner. However,previous methods obtain supervision signals for learning objectness inisolation from either localization or classification information, leading topoor performance for UOD. To address this issue, we propose a transformer-basedUOD framework, UN-DETR. Based on this, we craft Instance Presence Score (IPS)to represent the probability of an object's presence. For the purpose ofinformation complementarity, IPS employs a strategy of joint supervisedlearning, integrating attributes representing general objectness from thepositional and the categorical latent space as supervision signals. To enhanceIPS learning, we introduce a one-to-many assignment strategy to incorporatemore supervision. Then, we propose Unbiased Query Selection to provide premiuminitial query vectors for the decoder. Additionally, we propose an IPS-guidedpost-process strategy to filter redundant boxes and correct classificationpredictions for known and unknown objects. Finally, we pretrain the entireUN-DETR in an unsupervised manner, in order to obtain objectness prior. OurUN-DETR is comprehensively evaluated on multiple UOD and known detectionbenchmarks, demonstrating its effectiveness and achieving state-of-the-artperformance.</description><author>Haomiao Liu, Hao Xu, Chuhuai Yue, Bo Ma</author><pubDate>Fri, 13 Dec 2024 14:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10176v1</guid></item><item><title>Learning payoffs while routing in skill-based queues</title><link>http://arxiv.org/abs/2412.10168v1</link><description>Motivated by applications in service systems, we consider queueing systemswhere each customer must be handled by a server with the right skill set. Wefocus on optimizing the routing of customers to servers in order to maximizethe total payoff of customer--server matches. In addition, customer--serverdependent payoff parameters are assumed to be unknown a priori. We construct amachine learning algorithm that adaptively learns the payoff parameters whilemaximizing the total payoff and prove that it achieves polylogarithmic regret.Moreover, we show that the algorithm is asymptotically optimal up tologarithmic terms by deriving a regret lower bound. The algorithm leverages thebasic feasible solutions of a static linear program as the action space. Theregret analysis overcomes the complex interplay between queueing and learningby analyzing the convergence of the queue length process to its stationarybehavior. We also demonstrate the performance of the algorithm numerically, andhave included an experiment with time-varying parameters highlighting thepotential of the algorithm in non-static environments.</description><author>Sanne van Kempen, Jaron Sanders, Fiona Sloothaak, Maarten G. Wolf</author><pubDate>Fri, 13 Dec 2024 14:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10168v1</guid></item><item><title>Robust Monocular Visual Odometry using Curriculum Learning</title><link>http://arxiv.org/abs/2411.13438v2</link><description>Curriculum Learning (CL), drawing inspiration from natural learning patternsobserved in humans and animals, employs a systematic approach of graduallyintroducing increasingly complex training data during model development. Ourwork applies innovative CL methodologies to address the challenging geometricproblem of monocular Visual Odometry (VO) estimation, which is essential forrobot navigation in constrained environments. The primary objective of ourresearch is to push the boundaries of current state-of-the-art (SOTA)benchmarks in monocular VO by investigating various curriculum learningstrategies. We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO)framework through the integration of novel CL approaches, with the goal ofdeveloping more resilient models capable of maintaining high performance acrosschallenging environments and complex motion scenarios. Our research encompassesseveral distinctive CL strategies. We develop methods to evaluate sampledifficulty based on trajectory motion characteristics, implement sophisticatedadaptive scheduling through self-paced weighted loss mechanisms, and utilizereinforcement learning agents for dynamic adjustment of training emphasis.Through comprehensive evaluation on the diverse synthetic TartanAir dataset andcomplex real-world benchmarks such as EuRoC and TUM-RGBD, our CurriculumLearning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superiorperformance compared to existing SOTA methods, including both feature-based andlearning-based VO approaches. The results validate the effectiveness ofintegrating curriculum learning principles into visual odometry systems.</description><author>Assaf Lahiany, Oren Gal</author><pubDate>Fri, 13 Dec 2024 14:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13438v2</guid></item><item><title>Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation</title><link>http://arxiv.org/abs/2412.10163v1</link><description>We introduce Limited Rollout Beam Search (LRBS), a beam search strategy fordeep reinforcement learning (DRL) based combinatorial optimization improvementheuristics. Utilizing pre-trained models on the Euclidean Traveling SalespersonProblem, LRBS significantly enhances both in-distribution performance andgeneralization to larger problem instances, achieving optimality gaps thatoutperform existing improvement heuristics and narrowing the gap withstate-of-the-art constructive methods. We also extend our analysis to twopickup and delivery TSP variants to validate our results. Finally, we employour search strategy for offline and online adaptation of the pre-trainedimprovement policy, leading to improved search performance and surpassingrecent adaptive methods for constructive heuristics.</description><author>Federico Julian Camerota Verdù, Lorenzo Castelli, Luca Bortolussi</author><pubDate>Fri, 13 Dec 2024 14:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10163v1</guid></item></channel></rss>